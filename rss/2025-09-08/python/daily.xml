<rss version="2.0">
  <channel>
    <title>GitHub Python Daily Trending</title>
    <description>Daily Trending of Python in GitHub</description>
    <pubDate>Sun, 07 Sep 2025 01:36:26 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>microsoft/BitNet</title>
      <link>https://github.com/microsoft/BitNet</link>
      <description>&lt;p&gt;Official inference framework for 1-bit LLMs&lt;/p&gt;&lt;hr&gt;</description>
    </item>
    
    <item>
      <title>stanfordnlp/dspy</title>
      <link>https://github.com/stanfordnlp/dspy</link>
      <description>&lt;p&gt;DSPy: The framework for programming‚Äînot prompting‚Äîlanguage models&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img align="center" src="https://raw.githubusercontent.com/stanfordnlp/dspy/main/docs/docs/static/img/dspy_logo.png" width="460px" /&gt; &lt;/p&gt; 
&lt;p align="left"&gt; &lt;/p&gt;
&lt;h2&gt;DSPy: &lt;em&gt;Programming&lt;/em&gt;‚Äînot prompting‚ÄîFoundation Models&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Documentation:&lt;/strong&gt; &lt;a href="https://dspy.ai/"&gt;DSPy Docs&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://pepy.tech/projects/dspy"&gt;&lt;img src="https://static.pepy.tech/badge/dspy/month" alt="PyPI Downloads" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;DSPy is the framework for &lt;em&gt;programming‚Äîrather than prompting‚Äîlanguage models&lt;/em&gt;. It allows you to iterate fast on &lt;strong&gt;building modular AI systems&lt;/strong&gt; and offers algorithms for &lt;strong&gt;optimizing their prompts and weights&lt;/strong&gt;, whether you're building simple classifiers, sophisticated RAG pipelines, or Agent loops.&lt;/p&gt; 
&lt;p&gt;DSPy stands for Declarative Self-improving Python. Instead of brittle prompts, you write compositional &lt;em&gt;Python code&lt;/em&gt; and use DSPy to &lt;strong&gt;teach your LM to deliver high-quality outputs&lt;/strong&gt;. Learn more via our &lt;a href="https://dspy.ai/"&gt;official documentation site&lt;/a&gt; or meet the community, seek help, or start contributing via this GitHub repo and our &lt;a href="https://discord.gg/XCGy2WDCQB"&gt;Discord server&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Documentation: &lt;a href="https://dspy.ai"&gt;dspy.ai&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Please go to the &lt;a href="https://dspy.ai"&gt;DSPy Docs at dspy.ai&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install dspy
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To install the very latest from &lt;code&gt;main&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install git+https://github.com/stanfordnlp/dspy.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;üìú Citation &amp;amp; Reading More&lt;/h2&gt; 
&lt;p&gt;If you're looking to understand the framework, please go to the &lt;a href="https://dspy.ai"&gt;DSPy Docs at dspy.ai&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you're looking to understand the underlying research, this is a set of our papers:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;[Jul'25] &lt;a href="https://arxiv.org/abs/2507.19457"&gt;GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning&lt;/a&gt;&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;[Jun'24] &lt;a href="https://arxiv.org/abs/2406.11695"&gt;Optimizing Instructions and Demonstrations for Multi-Stage Language Model Programs&lt;/a&gt;&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;[Oct'23] &lt;a href="https://arxiv.org/abs/2310.03714"&gt;DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines&lt;/a&gt;&lt;/strong&gt;&lt;br /&gt; [Jul'24] &lt;a href="https://arxiv.org/abs/2407.10930"&gt;Fine-Tuning and Prompt Optimization: Two Great Steps that Work Better Together&lt;/a&gt;&lt;br /&gt; [Jun'24] &lt;a href="https://arxiv.org/abs/2406.11706"&gt;Prompts as Auto-Optimized Training Hyperparameters&lt;/a&gt;&lt;br /&gt; [Feb'24] &lt;a href="https://arxiv.org/abs/2402.14207"&gt;Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models&lt;/a&gt;&lt;br /&gt; [Jan'24] &lt;a href="https://arxiv.org/abs/2401.12178"&gt;In-Context Learning for Extreme Multi-Label Classification&lt;/a&gt;&lt;br /&gt; [Dec'23] &lt;a href="https://arxiv.org/abs/2312.13382"&gt;DSPy Assertions: Computational Constraints for Self-Refining Language Model Pipelines&lt;/a&gt;&lt;br /&gt; [Dec'22] &lt;a href="https://arxiv.org/abs/2212.14024.pdf"&gt;Demonstrate-Search-Predict: Composing Retrieval &amp;amp; Language Models for Knowledge-Intensive NLP&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;To stay up to date or learn more, follow &lt;a href="https://twitter.com/DSPyOSS"&gt;@DSPyOSS&lt;/a&gt; on Twitter or the DSPy page on LinkedIn.&lt;/p&gt; 
&lt;p&gt;The &lt;strong&gt;DSPy&lt;/strong&gt; logo is designed by &lt;strong&gt;Chuyi Zhang&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;If you use DSPy or DSP in a research paper, please cite our work as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@inproceedings{khattab2024dspy,
  title={DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines},
  author={Khattab, Omar and Singhvi, Arnav and Maheshwari, Paridhi and Zhang, Zhiyuan and Santhanam, Keshav and Vardhamanan, Sri and Haq, Saiful and Sharma, Ashutosh and Joshi, Thomas T. and Moazam, Hanna and Miller, Heather and Zaharia, Matei and Potts, Christopher},
  journal={The Twelfth International Conference on Learning Representations},
  year={2024}
}
@article{khattab2022demonstrate,
  title={Demonstrate-Search-Predict: Composing Retrieval and Language Models for Knowledge-Intensive {NLP}},
  author={Khattab, Omar and Santhanam, Keshav and Li, Xiang Lisa and Hall, David and Liang, Percy and Potts, Christopher and Zaharia, Matei},
  journal={arXiv preprint arXiv:2212.14024},
  year={2022}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;!-- You can also read more about the evolution of the framework from Demonstrate-Search-Predict to DSPy:

* [**DSPy Assertions: Computational Constraints for Self-Refining Language Model Pipelines**](https://arxiv.org/abs/2312.13382)   (Academic Paper, Dec 2023) 
* [**DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines**](https://arxiv.org/abs/2310.03714) (Academic Paper, Oct 2023) 
* [**Releasing DSPy, the latest iteration of the framework**](https://twitter.com/lateinteraction/status/1694748401374490946) (Twitter Thread, Aug 2023)
* [**Releasing the DSP Compiler (v0.1)**](https://twitter.com/lateinteraction/status/1625231662849073160)  (Twitter Thread, Feb 2023)
* [**Introducing DSP**](https://twitter.com/lateinteraction/status/1617953413576425472)  (Twitter Thread, Jan 2023)
* [**Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP**](https://arxiv.org/abs/2212.14024.pdf) (Academic Paper, Dec 2022) --&gt;</description>
    </item>
    
    <item>
      <title>jlowin/fastmcp</title>
      <link>https://github.com/jlowin/fastmcp</link>
      <description>&lt;p&gt;üöÄ The fast, Pythonic way to build MCP servers and clients&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;!-- omit in toc --&gt; 
 &lt;h1&gt;FastMCP v2 üöÄ&lt;/h1&gt; 
 &lt;p&gt;&lt;strong&gt;The fast, Pythonic way to build MCP servers and clients.&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;Made with ‚òïÔ∏è by &lt;a href="https://www.prefect.io/"&gt;Prefect&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://gofastmcp.com"&gt;&lt;img src="https://img.shields.io/badge/docs-gofastmcp.com-blue" alt="Docs" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/fastmcp"&gt;&lt;img src="https://img.shields.io/pypi/v/fastmcp.svg?sanitize=true" alt="PyPI - Version" /&gt;&lt;/a&gt; &lt;a href="https://github.com/jlowin/fastmcp/actions/workflows/run-tests.yml"&gt;&lt;img src="https://github.com/jlowin/fastmcp/actions/workflows/run-tests.yml/badge.svg?sanitize=true" alt="Tests" /&gt;&lt;/a&gt; &lt;a href="https://github.com/jlowin/fastmcp/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/jlowin/fastmcp.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/13266" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/13266" alt="jlowin%2Ffastmcp | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Note]&lt;/p&gt; 
 &lt;h4&gt;Beyond the Protocol&lt;/h4&gt; 
 &lt;p&gt;FastMCP is the standard framework for working with the Model Context Protocol. FastMCP 1.0 was incorporated into the &lt;a href="https://github.com/modelcontextprotocol/python-sdk"&gt;official MCP Python SDK&lt;/a&gt; in 2024.&lt;/p&gt; 
 &lt;p&gt;This is FastMCP 2.0, the &lt;strong&gt;actively maintained version&lt;/strong&gt; that provides a complete toolkit for working with the MCP ecosystem.&lt;/p&gt; 
 &lt;p&gt;FastMCP 2.0 has a comprehensive set of features that go far beyond the core MCP specification, all in service of providing &lt;strong&gt;the simplest path to production&lt;/strong&gt;. These include deployment, auth, clients, server proxying and composition, generating servers from REST APIs, dynamic tool rewriting, built-in testing tools, integrations, and more.&lt;/p&gt; 
 &lt;p&gt;Ready to upgrade or get started? Follow the &lt;a href="https://gofastmcp.com/getting-started/installation"&gt;installation instructions&lt;/a&gt;, which include steps for upgrading from the official MCP SDK.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr /&gt; 
&lt;p&gt;The &lt;a href="https://modelcontextprotocol.io"&gt;Model Context Protocol (MCP)&lt;/a&gt; is a new, standardized way to provide context and tools to your LLMs, and FastMCP makes building MCP servers and clients simple and intuitive. Create tools, expose resources, define prompts, and connect components with clean, Pythonic code.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# server.py
from fastmcp import FastMCP

mcp = FastMCP("Demo üöÄ")

@mcp.tool
def add(a: int, b: int) -&amp;gt; int:
    """Add two numbers"""
    return a + b

if __name__ == "__main__":
    mcp.run()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Run the server locally:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;fastmcp run server.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;üìö Documentation&lt;/h3&gt; 
&lt;p&gt;FastMCP's complete documentation is available at &lt;strong&gt;&lt;a href="https://gofastmcp.com"&gt;gofastmcp.com&lt;/a&gt;&lt;/strong&gt;, including detailed guides, API references, and advanced patterns. This readme provides only a high-level overview.&lt;/p&gt; 
&lt;p&gt;Documentation is also available in &lt;a href="https://llmstxt.org/"&gt;llms.txt format&lt;/a&gt;, which is a simple markdown standard that LLMs can consume easily.&lt;/p&gt; 
&lt;p&gt;There are two ways to access the LLM-friendly documentation:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://gofastmcp.com/llms.txt"&gt;&lt;code&gt;llms.txt&lt;/code&gt;&lt;/a&gt; is essentially a sitemap, listing all the pages in the documentation.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://gofastmcp.com/llms-full.txt"&gt;&lt;code&gt;llms-full.txt&lt;/code&gt;&lt;/a&gt; contains the entire documentation. Note this may exceed the context window of your LLM.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;!-- omit in toc --&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/jlowin/fastmcp/main/#what-is-mcp"&gt;What is MCP?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/jlowin/fastmcp/main/#why-fastmcp"&gt;Why FastMCP?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/jlowin/fastmcp/main/#installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/jlowin/fastmcp/main/#core-concepts"&gt;Core Concepts&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/jlowin/fastmcp/main/#the-fastmcp-server"&gt;The &lt;code&gt;FastMCP&lt;/code&gt; Server&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/jlowin/fastmcp/main/#tools"&gt;Tools&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/jlowin/fastmcp/main/#resources--templates"&gt;Resources &amp;amp; Templates&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/jlowin/fastmcp/main/#prompts"&gt;Prompts&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/jlowin/fastmcp/main/#context"&gt;Context&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/jlowin/fastmcp/main/#mcp-clients"&gt;MCP Clients&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/jlowin/fastmcp/main/#advanced-features"&gt;Advanced Features&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/jlowin/fastmcp/main/#proxy-servers"&gt;Proxy Servers&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/jlowin/fastmcp/main/#composing-mcp-servers"&gt;Composing MCP Servers&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/jlowin/fastmcp/main/#openapi--fastapi-generation"&gt;OpenAPI &amp;amp; FastAPI Generation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/jlowin/fastmcp/main/#authentication--security"&gt;Authentication &amp;amp; Security&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/jlowin/fastmcp/main/#running-your-server"&gt;Running Your Server&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/jlowin/fastmcp/main/#contributing"&gt;Contributing&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/jlowin/fastmcp/main/#prerequisites"&gt;Prerequisites&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/jlowin/fastmcp/main/#setup"&gt;Setup&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/jlowin/fastmcp/main/#unit-tests"&gt;Unit Tests&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/jlowin/fastmcp/main/#static-checks"&gt;Static Checks&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/jlowin/fastmcp/main/#pull-requests"&gt;Pull Requests&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;What is MCP?&lt;/h2&gt; 
&lt;p&gt;The &lt;a href="https://modelcontextprotocol.io"&gt;Model Context Protocol (MCP)&lt;/a&gt; lets you build servers that expose data and functionality to LLM applications in a secure, standardized way. It is often described as "the USB-C port for AI", providing a uniform way to connect LLMs to resources they can use. It may be easier to think of it as an API, but specifically designed for LLM interactions. MCP servers can:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Expose data through &lt;strong&gt;Resources&lt;/strong&gt; (think of these sort of like GET endpoints; they are used to load information into the LLM's context)&lt;/li&gt; 
 &lt;li&gt;Provide functionality through &lt;strong&gt;Tools&lt;/strong&gt; (sort of like POST endpoints; they are used to execute code or otherwise produce a side effect)&lt;/li&gt; 
 &lt;li&gt;Define interaction patterns through &lt;strong&gt;Prompts&lt;/strong&gt; (reusable templates for LLM interactions)&lt;/li&gt; 
 &lt;li&gt;And more!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;FastMCP provides a high-level, Pythonic interface for building, managing, and interacting with these servers.&lt;/p&gt; 
&lt;h2&gt;Why FastMCP?&lt;/h2&gt; 
&lt;p&gt;The MCP protocol is powerful but implementing it involves a lot of boilerplate - server setup, protocol handlers, content types, error management. FastMCP handles all the complex protocol details and server management, so you can focus on building great tools. It's designed to be high-level and Pythonic; in most cases, decorating a function is all you need.&lt;/p&gt; 
&lt;p&gt;FastMCP 2.0 has evolved into a comprehensive platform that goes far beyond basic protocol implementation. While 1.0 provided server-building capabilities (and is now part of the official MCP SDK), 2.0 offers a complete ecosystem including client libraries, authentication systems, deployment tools, integrations with major AI platforms, testing frameworks, and production-ready infrastructure patterns.&lt;/p&gt; 
&lt;p&gt;FastMCP aims to be:&lt;/p&gt; 
&lt;p&gt;üöÄ &lt;strong&gt;Fast:&lt;/strong&gt; High-level interface means less code and faster development&lt;/p&gt; 
&lt;p&gt;üçÄ &lt;strong&gt;Simple:&lt;/strong&gt; Build MCP servers with minimal boilerplate&lt;/p&gt; 
&lt;p&gt;üêç &lt;strong&gt;Pythonic:&lt;/strong&gt; Feels natural to Python developers&lt;/p&gt; 
&lt;p&gt;üîç &lt;strong&gt;Complete:&lt;/strong&gt; A comprehensive platform for all MCP use cases, from dev to prod&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;We recommend installing FastMCP with &lt;a href="https://docs.astral.sh/uv/"&gt;uv&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv pip install fastmcp
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For full installation instructions, including verification, upgrading from the official MCPSDK, and developer setup, see the &lt;a href="https://gofastmcp.com/getting-started/installation"&gt;&lt;strong&gt;Installation Guide&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Core Concepts&lt;/h2&gt; 
&lt;p&gt;These are the building blocks for creating MCP servers and clients with FastMCP.&lt;/p&gt; 
&lt;h3&gt;The &lt;code&gt;FastMCP&lt;/code&gt; Server&lt;/h3&gt; 
&lt;p&gt;The central object representing your MCP application. It holds your tools, resources, and prompts, manages connections, and can be configured with settings like authentication.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from fastmcp import FastMCP

# Create a server instance
mcp = FastMCP(name="MyAssistantServer")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Learn more in the &lt;a href="https://gofastmcp.com/servers/fastmcp"&gt;&lt;strong&gt;FastMCP Server Documentation&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Tools&lt;/h3&gt; 
&lt;p&gt;Tools allow LLMs to perform actions by executing your Python functions (sync or async). Ideal for computations, API calls, or side effects (like &lt;code&gt;POST&lt;/code&gt;/&lt;code&gt;PUT&lt;/code&gt;). FastMCP handles schema generation from type hints and docstrings. Tools can return various types, including text, JSON-serializable objects, and even images or audio aided by the FastMCP media helper classes.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;@mcp.tool
def multiply(a: float, b: float) -&amp;gt; float:
    """Multiplies two numbers."""
    return a * b
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Learn more in the &lt;a href="https://gofastmcp.com/servers/tools"&gt;&lt;strong&gt;Tools Documentation&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Resources &amp;amp; Templates&lt;/h3&gt; 
&lt;p&gt;Resources expose read-only data sources (like &lt;code&gt;GET&lt;/code&gt; requests). Use &lt;code&gt;@mcp.resource("your://uri")&lt;/code&gt;. Use &lt;code&gt;{placeholders}&lt;/code&gt; in the URI to create dynamic templates that accept parameters, allowing clients to request specific data subsets.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Static resource
@mcp.resource("config://version")
def get_version(): 
    return "2.0.1"

# Dynamic resource template
@mcp.resource("users://{user_id}/profile")
def get_profile(user_id: int):
    # Fetch profile for user_id...
    return {"name": f"User {user_id}", "status": "active"}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Learn more in the &lt;a href="https://gofastmcp.com/servers/resources"&gt;&lt;strong&gt;Resources &amp;amp; Templates Documentation&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Prompts&lt;/h3&gt; 
&lt;p&gt;Prompts define reusable message templates to guide LLM interactions. Decorate functions with &lt;code&gt;@mcp.prompt&lt;/code&gt;. Return strings or &lt;code&gt;Message&lt;/code&gt; objects.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;@mcp.prompt
def summarize_request(text: str) -&amp;gt; str:
    """Generate a prompt asking for a summary."""
    return f"Please summarize the following text:\n\n{text}"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Learn more in the &lt;a href="https://gofastmcp.com/servers/prompts"&gt;&lt;strong&gt;Prompts Documentation&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Context&lt;/h3&gt; 
&lt;p&gt;Access MCP session capabilities within your tools, resources, or prompts by adding a &lt;code&gt;ctx: Context&lt;/code&gt; parameter. Context provides methods for:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Logging:&lt;/strong&gt; Log messages to MCP clients with &lt;code&gt;ctx.info()&lt;/code&gt;, &lt;code&gt;ctx.error()&lt;/code&gt;, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;LLM Sampling:&lt;/strong&gt; Use &lt;code&gt;ctx.sample()&lt;/code&gt; to request completions from the client's LLM.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;HTTP Request:&lt;/strong&gt; Use &lt;code&gt;ctx.http_request()&lt;/code&gt; to make HTTP requests to other servers.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Resource Access:&lt;/strong&gt; Use &lt;code&gt;ctx.read_resource()&lt;/code&gt; to access resources on the server&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Progress Reporting:&lt;/strong&gt; Use &lt;code&gt;ctx.report_progress()&lt;/code&gt; to report progress to the client.&lt;/li&gt; 
 &lt;li&gt;and more...&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To access the context, add a parameter annotated as &lt;code&gt;Context&lt;/code&gt; to any mcp-decorated function. FastMCP will automatically inject the correct context object when the function is called.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from fastmcp import FastMCP, Context

mcp = FastMCP("My MCP Server")

@mcp.tool
async def process_data(uri: str, ctx: Context):
    # Log a message to the client
    await ctx.info(f"Processing {uri}...")

    # Read a resource from the server
    data = await ctx.read_resource(uri)

    # Ask client LLM to summarize the data
    summary = await ctx.sample(f"Summarize: {data.content[:500]}")

    # Return the summary
    return summary.text
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Learn more in the &lt;a href="https://gofastmcp.com/servers/context"&gt;&lt;strong&gt;Context Documentation&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;MCP Clients&lt;/h3&gt; 
&lt;p&gt;Interact with &lt;em&gt;any&lt;/em&gt; MCP server programmatically using the &lt;code&gt;fastmcp.Client&lt;/code&gt;. It supports various transports (Stdio, SSE, In-Memory) and often auto-detects the correct one. The client can also handle advanced patterns like server-initiated &lt;strong&gt;LLM sampling requests&lt;/strong&gt; if you provide an appropriate handler.&lt;/p&gt; 
&lt;p&gt;Critically, the client allows for efficient &lt;strong&gt;in-memory testing&lt;/strong&gt; of your servers by connecting directly to a &lt;code&gt;FastMCP&lt;/code&gt; server instance via the &lt;code&gt;FastMCPTransport&lt;/code&gt;, eliminating the need for process management or network calls during tests.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from fastmcp import Client

async def main():
    # Connect via stdio to a local script
    async with Client("my_server.py") as client:
        tools = await client.list_tools()
        print(f"Available tools: {tools}")
        result = await client.call_tool("add", {"a": 5, "b": 3})
        print(f"Result: {result.content[0].text}")

    # Connect via SSE
    async with Client("http://localhost:8000/sse") as client:
        # ... use the client
        pass
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To use clients to test servers, use the following pattern:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from fastmcp import FastMCP, Client

mcp = FastMCP("My MCP Server")

async def main():
    # Connect via in-memory transport
    async with Client(mcp) as client:
        # ... use the client
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;FastMCP also supports connecting to multiple servers through a single unified client using the standard MCP configuration format:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from fastmcp import Client

# Standard MCP configuration with multiple servers
config = {
    "mcpServers": {
        "weather": {"url": "https://weather-api.example.com/mcp"},
        "assistant": {"command": "python", "args": ["./assistant_server.py"]}
    }
}

# Create a client that connects to all servers
client = Client(config)

async def main():
    async with client:
        # Access tools and resources with server prefixes
        forecast = await client.call_tool("weather_get_forecast", {"city": "London"})
        answer = await client.call_tool("assistant_answer_question", {"query": "What is MCP?"})
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Learn more in the &lt;a href="https://gofastmcp.com/clients/client"&gt;&lt;strong&gt;Client Documentation&lt;/strong&gt;&lt;/a&gt; and &lt;a href="https://gofastmcp.com/clients/transports"&gt;&lt;strong&gt;Transports Documentation&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Advanced Features&lt;/h2&gt; 
&lt;p&gt;FastMCP introduces powerful ways to structure and deploy your MCP applications.&lt;/p&gt; 
&lt;h3&gt;Proxy Servers&lt;/h3&gt; 
&lt;p&gt;Create a FastMCP server that acts as an intermediary for another local or remote MCP server using &lt;code&gt;FastMCP.as_proxy()&lt;/code&gt;. This is especially useful for bridging transports (e.g., remote SSE to local Stdio) or adding a layer of logic to a server you don't control.&lt;/p&gt; 
&lt;p&gt;Learn more in the &lt;a href="https://gofastmcp.com/patterns/proxy"&gt;&lt;strong&gt;Proxying Documentation&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Composing MCP Servers&lt;/h3&gt; 
&lt;p&gt;Build modular applications by mounting multiple &lt;code&gt;FastMCP&lt;/code&gt; instances onto a parent server using &lt;code&gt;mcp.mount()&lt;/code&gt; (live link) or &lt;code&gt;mcp.import_server()&lt;/code&gt; (static copy).&lt;/p&gt; 
&lt;p&gt;Learn more in the &lt;a href="https://gofastmcp.com/patterns/composition"&gt;&lt;strong&gt;Composition Documentation&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;OpenAPI &amp;amp; FastAPI Generation&lt;/h3&gt; 
&lt;p&gt;Automatically generate FastMCP servers from existing OpenAPI specifications (&lt;code&gt;FastMCP.from_openapi()&lt;/code&gt;) or FastAPI applications (&lt;code&gt;FastMCP.from_fastapi()&lt;/code&gt;), instantly bringing your web APIs to the MCP ecosystem.&lt;/p&gt; 
&lt;p&gt;Learn more: &lt;a href="https://gofastmcp.com/integrations/openapi"&gt;&lt;strong&gt;OpenAPI Integration&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://gofastmcp.com/integrations/fastapi"&gt;&lt;strong&gt;FastAPI Integration&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Authentication &amp;amp; Security&lt;/h3&gt; 
&lt;p&gt;FastMCP provides built-in authentication support to secure both your MCP servers and clients in production environments. Protect your server endpoints from unauthorized access and authenticate your clients against secured MCP servers using industry-standard protocols.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Server Protection&lt;/strong&gt;: Secure your FastMCP server endpoints with configurable authentication providers&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Client Authentication&lt;/strong&gt;: Connect to authenticated MCP servers with automatic credential management&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Production Ready&lt;/strong&gt;: Support for common authentication patterns used in enterprise environments&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Learn more in the &lt;strong&gt;Authentication Documentation&lt;/strong&gt; for &lt;a href="https://gofastmcp.com/servers/auth"&gt;servers&lt;/a&gt; and &lt;a href="https://gofastmcp.com/clients/auth"&gt;clients&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Running Your Server&lt;/h2&gt; 
&lt;p&gt;The main way to run a FastMCP server is by calling the &lt;code&gt;run()&lt;/code&gt; method on your server instance:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# server.py
from fastmcp import FastMCP

mcp = FastMCP("Demo üöÄ")

@mcp.tool
def hello(name: str) -&amp;gt; str:
    return f"Hello, {name}!"

if __name__ == "__main__":
    mcp.run()  # Default: uses STDIO transport
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;FastMCP supports three transport protocols:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;STDIO (Default)&lt;/strong&gt;: Best for local tools and command-line scripts.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;mcp.run(transport="stdio")  # Default, so transport argument is optional
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Streamable HTTP&lt;/strong&gt;: Recommended for web deployments.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;mcp.run(transport="http", host="127.0.0.1", port=8000, path="/mcp")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;SSE&lt;/strong&gt;: For compatibility with existing SSE clients.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;mcp.run(transport="sse", host="127.0.0.1", port=8000)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See the &lt;a href="https://gofastmcp.com/deployment/running-server"&gt;&lt;strong&gt;Running Server Documentation&lt;/strong&gt;&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Contributions are the core of open source! We welcome improvements and features.&lt;/p&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.10+&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.astral.sh/uv/"&gt;uv&lt;/a&gt; (Recommended for environment management)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Setup&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Clone the repository:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/jlowin/fastmcp.git 
cd fastmcp
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Create and sync the environment:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;uv sync
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This installs all dependencies, including dev tools.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Activate the virtual environment (e.g., &lt;code&gt;source .venv/bin/activate&lt;/code&gt; or via your IDE).&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Unit Tests&lt;/h3&gt; 
&lt;p&gt;FastMCP has a comprehensive unit test suite. All PRs must introduce or update tests as appropriate and pass the full suite.&lt;/p&gt; 
&lt;p&gt;Run tests using pytest:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pytest
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or if you want an overview of the code coverage&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run pytest --cov=src --cov=examples --cov-report=html
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Static Checks&lt;/h3&gt; 
&lt;p&gt;FastMCP uses &lt;code&gt;pre-commit&lt;/code&gt; for code formatting, linting, and type-checking. All PRs must pass these checks (they run automatically in CI).&lt;/p&gt; 
&lt;p&gt;Install the hooks locally:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run pre-commit install
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The hooks will now run automatically on &lt;code&gt;git commit&lt;/code&gt;. You can also run them manually at any time:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pre-commit run --all-files
# or via uv
uv run pre-commit run --all-files
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Pull Requests&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Fork the repository on GitHub.&lt;/li&gt; 
 &lt;li&gt;Create a feature branch from &lt;code&gt;main&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Make your changes, including tests and documentation updates.&lt;/li&gt; 
 &lt;li&gt;Ensure tests and pre-commit hooks pass.&lt;/li&gt; 
 &lt;li&gt;Commit your changes and push to your fork.&lt;/li&gt; 
 &lt;li&gt;Open a pull request against the &lt;code&gt;main&lt;/code&gt; branch of &lt;code&gt;jlowin/fastmcp&lt;/code&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Please open an issue or discussion for questions or suggestions before starting significant work!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Vector-Wangel/XLeRobot</title>
      <link>https://github.com/Vector-Wangel/XLeRobot</link>
      <description>&lt;p&gt;XLeRobot: Practical Dual-Arm Mobile Home Robot for $660&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;XLeRobot ü§ñ&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/Vector-Wangel/XLeRobot/main/README.md"&gt;&lt;img src="https://img.shields.io/badge/lang-en-blue.svg?sanitize=true" alt="en" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Vector-Wangel/XLeRobot/main/README_CN.md"&gt;&lt;img src="https://img.shields.io/badge/lang-%E4%B8%AD%E6%96%87-brown.svg?sanitize=true" alt="‰∏≠Êñá" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;a href="https://xlerobot.readthedocs.io/en/latest/index.html"&gt; &lt;img width="1725" height="1140" alt="front" src="https://github.com/user-attachments/assets/f9c454ee-2c46-42b4-a5d7-88834a1c95ab" /&gt; &lt;/a&gt; 
&lt;h2&gt;&lt;a href="https://opensource.org/licenses/Apache-2.0"&gt;&lt;img src="https://img.shields.io/badge/License-Apache%202.0-blue.svg?sanitize=true" alt="Apache License" /&gt;&lt;/a&gt; &lt;a href="https://twitter.com/VectorWang2"&gt;&lt;img src="https://img.shields.io/twitter/follow/VectorWang?style=social" alt="Twitter/X" /&gt;&lt;/a&gt; &lt;a href="https://xlerobot.readthedocs.io/en/latest/"&gt;&lt;img src="https://img.shields.io/badge/docs-passing-brightgreen.svg?sanitize=true" alt="Docs status" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/bjZveEUh6F"&gt;&lt;img src="https://img.shields.io/badge/Discord-XLeRobot-7289da?style=flat&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;üöÄ Bringing Embodied AI to Everyone - Cheaper Than an iPhone! üì±&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;üíµ Starts from $660 cost and ‚è∞ &amp;lt;4hrs total assembly time!!&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Built upon the giants: &lt;a href="https://github.com/huggingface/lerobot"&gt;LeRobot&lt;/a&gt;, &lt;a href="https://github.com/TheRobotStudio/SO-ARM100"&gt;SO-100/SO-101&lt;/a&gt;, &lt;a href="https://github.com/SIGRobotics-UIUC/LeKiwi"&gt;Lekiwi&lt;/a&gt;, &lt;a href="https://github.com/timqian/bambot"&gt;Bambot&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src="https://github.com/user-attachments/assets/17e31979-bd5e-4790-be70-566ea8bb181e" width="250" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/user-attachments/assets/96ff4a3e-3402-47a2-bc6b-b45137ee3fdd" width="250" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/user-attachments/assets/f6d52acc-bc8d-46f6-b3cd-8821f0306a7f" width="250" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src="https://github.com/user-attachments/assets/59086300-3e6f-4a3c-b5e0-db893eeabc0c" width="250" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/user-attachments/assets/4ddbc0ff-ca42-4ad0-94c6-4e0f4047fd01" width="250" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/user-attachments/assets/7abc890e-9c9c-4983-8b25-122573028de5" width="250" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src="https://github.com/user-attachments/assets/e74a602b-0146-49c4-953d-3fa3b038a7f7" width="250" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/user-attachments/assets/d8090b15-97f3-4abc-98c8-208ae79894d5" width="250" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/user-attachments/assets/8b54adc3-d61b-42a0-8985-ea28f2e8f64c" width="250" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;üíµ Total Cost üíµ&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Cost excludes 3D printing, tools, shipping, and taxes.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Price (Buy all the parts yourself)&lt;/th&gt; 
   &lt;th&gt;US&lt;/th&gt; 
   &lt;th&gt;EU&lt;/th&gt; 
   &lt;th&gt;CN&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Basic&lt;/strong&gt; (use your laptop, single RGB head cam)&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;~$660&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;~‚Ç¨680&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;~¬•3999&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;‚Üë Stereo dual-eye RGB head cam&lt;/td&gt; 
   &lt;td&gt;+$30&lt;/td&gt; 
   &lt;td&gt;+‚Ç¨30&lt;/td&gt; 
   &lt;td&gt;+¬•199&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;+ RasberryPi&lt;/td&gt; 
   &lt;td&gt;+$79&lt;/td&gt; 
   &lt;td&gt;+‚Ç¨79&lt;/td&gt; 
   &lt;td&gt;+¬•399&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;‚Üë RealSense RGBD head cam&lt;/td&gt; 
   &lt;td&gt;+$220&lt;/td&gt; 
   &lt;td&gt;+‚Ç¨230&lt;/td&gt; 
   &lt;td&gt;+¬•1499&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h1&gt;üì∞ News&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;2025-08-30: XLeRobot 0.3.0 Release with final outfit touch up and household chores showcase demos. Assembly kit ready for purchase soon, stay tuned!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025-07-30: &lt;a href="https://xlerobot.readthedocs.io/en/latest/software/index.html"&gt;Control XLeRobot in real life&lt;/a&gt; with &lt;strong&gt;keyboard/Xbox controller/Switch joycon&lt;/strong&gt; in the wild anywhere. All bluetooth, no wifi needed and zero latency.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/de8f50ad-a370-406c-97fb-fc01638d5624" alt="rea" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025-07-08: &lt;a href="https://xlerobot.readthedocs.io/en/latest/simulation/index.html"&gt;&lt;strong&gt;Simulation&lt;/strong&gt;&lt;/a&gt; with updated urdfs, control scripts (support Quest3 VR, keyboard, Xbox controller, switch joycon), support for new hardware and cameras, RL environment. Get started in 15 min.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/68b77bea-fdcf-4f42-9cf0-efcf1b188358" alt="vr" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025-07-01: &lt;a href="https://xlerobot.readthedocs.io/en/latest/index.html"&gt;&lt;strong&gt;Documentation&lt;/strong&gt; website&lt;/a&gt; out for more orgainized turotials, demos and resources.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025-06-13: &lt;a href="https://xlerobot.readthedocs.io"&gt;&lt;strong&gt;XLeRobot 0.2.0&lt;/strong&gt;&lt;/a&gt; hardware setup, the 1st version fully capable for autonomous household tasks, starts from 660$.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üöÄ Get Started üöÄ&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] If you are totally new to programming, please spend at least a day to get yourself familiar with basic Python, Ubuntu and Github (with the help of Google and AI). At least you should know how to set up ubuntu system, git clone, pip install, use intepreters (VS Code, Cursor, Pycharm, etc.) and directly run commands in the terminals.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ol&gt; 
 &lt;li&gt;üíµ &lt;strong&gt;Buy your parts&lt;/strong&gt;: &lt;a href="https://xlerobot.readthedocs.io/en/latest/hardware/getting_started/material.html"&gt;Bill of Materials&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üñ®Ô∏è &lt;strong&gt;Print your stuff&lt;/strong&gt;: &lt;a href="https://xlerobot.readthedocs.io/en/latest/hardware/getting_started/3d.html"&gt;3D printing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üî® &lt;del&gt;Avengers&lt;/del&gt;: &lt;a href="https://xlerobot.readthedocs.io/en/latest/hardware/getting_started/assemble.html"&gt;&lt;strong&gt;Assemble&lt;/strong&gt;!&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üíª &lt;strong&gt;Software&lt;/strong&gt;: &lt;a href="https://xlerobot.readthedocs.io/en/latest/software/index.html"&gt;Get your robot moving!&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Contribute&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;üëã Want to contribute to XLeRobot?&lt;/strong&gt; Please refer to &lt;a href="https://raw.githubusercontent.com/Vector-Wangel/XLeRobot/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for guidance on how to get involved!&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Main Contributors&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Zhuoyi Lu: RL sim2real deploy, teleop on real robot (Xbox, VR, Joycon)&lt;/li&gt; 
 &lt;li&gt;Nicole Yue: Documentation website setup&lt;/li&gt; 
 &lt;li&gt;Yuesong Wang: Mujoco simulation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This is just a small brick in the pyramid, made possible by&amp;nbsp;&lt;a href="https://github.com/huggingface/lerobot"&gt;LeRobot&lt;/a&gt;,&amp;nbsp;&lt;a href="https://github.com/TheRobotStudio/SO-ARM100"&gt;SO-100&lt;/a&gt;,&amp;nbsp;&lt;a href="https://github.com/SIGRobotics-UIUC/LeKiwi"&gt;Lekiwi&lt;/a&gt;, and&amp;nbsp;&lt;a href="https://github.com/timqian/bambot"&gt;Bambot&lt;/a&gt;. Thanks to all the talented contributors behind these detailed and professional projects.&lt;/p&gt; 
&lt;p&gt;Looking forward to collaborating with anyone interested in contributing to this project!&lt;/p&gt; 
&lt;h2&gt;About me&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://vector-wangel.github.io/"&gt;Gaotian/Vector Wang&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;I am a CS graduate student at Rice University, focusing on robust object manipulation, where we propse virtual cages and funnels and physics-aware world models to close the Sim2real gap and achieve robust manipulation under uncertainties. One of my papers, Caging in Time, has recently been accepted by International Journal of Robotics Research (IJRR).&lt;/p&gt; 
&lt;p&gt;I built XLeRobot as a personal hobby to instantiate my research theory, also to provide a low-cost platform for people who are interested in robotics and embodied AI to work with.&lt;/p&gt; 
&lt;h2&gt;&lt;a href="https://star-history.com/#Vector-Wangel/XLeRobot&amp;amp;Timeline"&gt;&lt;img src="https://api.star-history.com/svg?repos=Vector-Wangel/XLeRobot&amp;amp;type=Timeline" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you want, you can cite this work with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{wang2025xlerobot,
    author = {Wang, Gaotian and Lu, Zhuoyi},
    title = {XLeRobot: A Practical Low-cost Household Dual-Arm Mobile Robot Design for General Manipulation},
    howpublished = "\url{https://github.com/Vector-Wangel/XLeRobot}",
    year = {2025}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;---&lt;img src="https://github.com/user-attachments/assets/682ef049-bb42-4b50-bf98-74d6311e774d" alt="Generated Image August 27, 2025 - 4_58PM" /&gt;&lt;/p&gt; 
&lt;h2&gt;ü™ß Disclaimer ü™ß&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] If you build, buy, or develop a XLeRobot based on this repo, you will be fully responsible for all the physical and mental damages it does to you or others.&lt;/p&gt; 
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>coleam00/ottomator-agents</title>
      <link>https://github.com/coleam00/ottomator-agents</link>
      <description>&lt;p&gt;All the open source AI Agents hosted on the oTTomator Live Agent Studio platform!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;What is the Live Agent Studio?&lt;/h1&gt; 
&lt;p&gt;The &lt;a href="https://studio.ottomator.ai"&gt;Live Agent Studio&lt;/a&gt; is a community-driven platform developed by &lt;a href="https://ottomator.ai"&gt;oTTomator&lt;/a&gt; for you to explore cutting-edge AI agents and learn how to implement them for yourself or your business! All agents on this platform are open source and, over time, will cover a very large variety of use cases.&lt;/p&gt; 
&lt;p&gt;The goal with the studio is to build an educational platform for you to learn how to do incredible things with AI, while still providing practical value so that you‚Äôll want to use the agents just for the sake of what they can do for you!&lt;/p&gt; 
&lt;p&gt;This platform is still in beta ‚Äì expect longer response times under load, a rapidly growing agent library over the coming months, and a lot more content on this platform soon on Cole Medin‚Äôs YouTube channel!&lt;/p&gt; 
&lt;h1&gt;What is this Repository for?&lt;/h1&gt; 
&lt;p&gt;This repository contains the source code/workflow JSON for all the agents on the Live Agent Studio! Every agent being added to the platform is currently be open sourced here so we can not only create a curated collection of cutting-edge agents together as a community, but also learn from one another!&lt;/p&gt; 
&lt;h2&gt;Tokens&lt;/h2&gt; 
&lt;p&gt;Most agents on the Live Agent Studio cost tokens to use, which are purchasable on the platform. However, when you first sign in you are given some tokens to start so you can use the agents free of charge! The biggest reason agents cost tokens is that we pay for the LLM usage since we host all the agents developed by you and the rest of the community!&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://studio.ottomator.ai/pricing"&gt;Purchase Tokens&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Future Plans&lt;/h2&gt; 
&lt;p&gt;As the Live Agent Studio develops, it will become the go-to place to stay on top of what is possible with AI agents! Anytime there is a new AI technology, groundbreaking agent research, or a new tool/library to build agents with, it‚Äôll be featured through agents on the platform. It‚Äôs a tall order, but we have big plans for the oTTomator community, and we‚Äôre confident we can grow to accomplish this!&lt;/p&gt; 
&lt;h2&gt;FAQ&lt;/h2&gt; 
&lt;h3&gt;I want to build an agent to showcase in the Live Agent Studio! How do I do that?&lt;/h3&gt; 
&lt;p&gt;Head on over here to learn how to build an agent for the platform:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://studio.ottomator.ai/guide"&gt;Developer Guide&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Also check out &lt;a href="https://raw.githubusercontent.com/coleam00/ottomator-agents/main/~sample-n8n-agent~"&gt;the sample n8n agent&lt;/a&gt; for a starting point of building an n8n agent for the Live Agent Studio, and &lt;a href="https://raw.githubusercontent.com/coleam00/ottomator-agents/main/~sample-python-agent~"&gt;the sample Python agent&lt;/a&gt; for Python.&lt;/p&gt; 
&lt;h3&gt;How many tokens does it cost to use an agent?&lt;/h3&gt; 
&lt;p&gt;Each agent will charge tokens per prompt. The number of tokens depends on the agent, as some agents use larger LLMs, some call LLMs multiple times, and some use paid APIs.&lt;/p&gt; 
&lt;h3&gt;Where can I go to talk about all these agents and get help implementing them myself?&lt;/h3&gt; 
&lt;p&gt;Head on over to our Think Tank community and feel free to make a post!&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://thinktank.ottomator.ai"&gt;Think Tank Community&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;¬© 2024 Live Agent Studio. All rights reserved.&lt;br /&gt; Created by oTTomator&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>resemble-ai/chatterbox</title>
      <link>https://github.com/resemble-ai/chatterbox</link>
      <description>&lt;p&gt;SoTA open-source TTS&lt;/p&gt;&lt;hr&gt;&lt;img width="1200" height="600" alt="Chatterbox-Multilingual" src="https://www.resemble.ai/wp-content/uploads/2025/09/Chatterbox-Multilingual-1.png" /&gt; 
&lt;h1&gt;Chatterbox TTS&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://resemble-ai.github.io/chatterbox_demopage/"&gt;&lt;img src="https://img.shields.io/badge/listen-demo_samples-blue" alt="Alt Text" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/spaces/ResembleAI/Chatterbox"&gt;&lt;img src="https://huggingface.co/datasets/huggingface/badges/resolve/main/open-in-hf-spaces-sm.svg?sanitize=true" alt="Alt Text" /&gt;&lt;/a&gt; &lt;a href="https://podonos.com/resembleai/chatterbox"&gt;&lt;img src="https://static-public.podonos.com/badges/insight-on-pdns-sm-dark.svg?sanitize=true" alt="Alt Text" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/rJq9cRJBJ6"&gt;&lt;img src="https://img.shields.io/discord/1377773249798344776?label=join%20discord&amp;amp;logo=discord&amp;amp;style=flat" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;_Made with ‚ô•Ô∏è by &lt;a href="https://resemble.ai" target="_blank"&gt;&lt;img width="100" alt="resemble-logo-horizontal" src="https://github.com/user-attachments/assets/35cf756b-3506-4943-9c72-c05ddfa4e525" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;We're excited to introduce &lt;strong&gt;Chatterbox Multilingual&lt;/strong&gt;, &lt;a href="https://resemble.ai"&gt;Resemble AI's&lt;/a&gt; first production-grade open source TTS model supporting &lt;strong&gt;23 languages&lt;/strong&gt; out of the box. Licensed under MIT, Chatterbox has been benchmarked against leading closed-source systems like ElevenLabs, and is consistently preferred in side-by-side evaluations.&lt;/p&gt; 
&lt;p&gt;Whether you're working on memes, videos, games, or AI agents, Chatterbox brings your content to life across languages. It's also the first open source TTS model to support &lt;strong&gt;emotion exaggeration control&lt;/strong&gt; with robust &lt;strong&gt;multilingual zero-shot voice cloning&lt;/strong&gt;. Try the english only version now on our &lt;a href="https://huggingface.co/spaces/ResembleAI/Chatterbox"&gt;English Hugging Face Gradio app.&lt;/a&gt;. Or try the multilingual version on our &lt;a href="https://huggingface.co/spaces/ResembleAI/Chatterbox-Multilingual-TTS"&gt;Multilingual Hugging Face Gradio app.&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you like the model but need to scale or tune it for higher accuracy, check out our competitively priced TTS service (&lt;a href="https://resemble.ai"&gt;link&lt;/a&gt;). It delivers reliable performance with ultra-low latency of sub 200ms‚Äîideal for production use in agents, applications, or interactive media.&lt;/p&gt; 
&lt;h1&gt;Key Details&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;Multilingual, zero-shot TTS supporting 23 languages&lt;/li&gt; 
 &lt;li&gt;SoTA zeroshot English TTS&lt;/li&gt; 
 &lt;li&gt;0.5B Llama backbone&lt;/li&gt; 
 &lt;li&gt;Unique exaggeration/intensity control&lt;/li&gt; 
 &lt;li&gt;Ultra-stable with alignment-informed inference&lt;/li&gt; 
 &lt;li&gt;Trained on 0.5M hours of cleaned data&lt;/li&gt; 
 &lt;li&gt;Watermarked outputs&lt;/li&gt; 
 &lt;li&gt;Easy voice conversion script&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://podonos.com/resembleai/chatterbox"&gt;Outperforms ElevenLabs&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Supported Languages&lt;/h1&gt; 
&lt;p&gt;Arabic (ar) ‚Ä¢ Danish (da) ‚Ä¢ German (de) ‚Ä¢ Greek (el) ‚Ä¢ English (en) ‚Ä¢ Spanish (es) ‚Ä¢ Finnish (fi) ‚Ä¢ French (fr) ‚Ä¢ Hebrew (he) ‚Ä¢ Hindi (hi) ‚Ä¢ Italian (it) ‚Ä¢ Japanese (ja) ‚Ä¢ Korean (ko) ‚Ä¢ Malay (ms) ‚Ä¢ Dutch (nl) ‚Ä¢ Norwegian (no) ‚Ä¢ Polish (pl) ‚Ä¢ Portuguese (pt) ‚Ä¢ Russian (ru) ‚Ä¢ Swedish (sv) ‚Ä¢ Swahili (sw) ‚Ä¢ Turkish (tr) ‚Ä¢ Chinese (zh)&lt;/p&gt; 
&lt;h1&gt;Tips&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;General Use (TTS and Voice Agents):&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Ensure that the reference clip matches the specified language tag. Otherwise, language transfer outputs may inherit the accent of the reference clip‚Äôs language. To mitigate this, set &lt;code&gt;cfg_weight&lt;/code&gt; to &lt;code&gt;0&lt;/code&gt;.&lt;/li&gt; 
   &lt;li&gt;The default settings (&lt;code&gt;exaggeration=0.5&lt;/code&gt;, &lt;code&gt;cfg_weight=0.5&lt;/code&gt;) work well for most prompts across all languages.&lt;/li&gt; 
   &lt;li&gt;If the reference speaker has a fast speaking style, lowering &lt;code&gt;cfg_weight&lt;/code&gt; to around &lt;code&gt;0.3&lt;/code&gt; can improve pacing.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Expressive or Dramatic Speech:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Try lower &lt;code&gt;cfg_weight&lt;/code&gt; values (e.g. &lt;code&gt;~0.3&lt;/code&gt;) and increase &lt;code&gt;exaggeration&lt;/code&gt; to around &lt;code&gt;0.7&lt;/code&gt; or higher.&lt;/li&gt; 
   &lt;li&gt;Higher &lt;code&gt;exaggeration&lt;/code&gt; tends to speed up speech; reducing &lt;code&gt;cfg_weight&lt;/code&gt; helps compensate with slower, more deliberate pacing.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Installation&lt;/h1&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip install chatterbox-tts
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, you can install from source:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# conda create -yn chatterbox python=3.11
# conda activate chatterbox

git clone https://github.com/resemble-ai/chatterbox.git
cd chatterbox
pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;We developed and tested Chatterbox on Python 3.11 on Debian 11 OS; the versions of the dependencies are pinned in &lt;code&gt;pyproject.toml&lt;/code&gt; to ensure consistency. You can modify the code or dependencies in this installation mode.&lt;/p&gt; 
&lt;h1&gt;Usage&lt;/h1&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import torchaudio as ta
from chatterbox.tts import ChatterboxTTS
from chatterbox.mtl_tts import ChatterboxMultilingualTTS

# English example
model = ChatterboxTTS.from_pretrained(device="cuda")

text = "Ezreal and Jinx teamed up with Ahri, Yasuo, and Teemo to take down the enemy's Nexus in an epic late-game pentakill."
wav = model.generate(text)
ta.save("test-english.wav", wav, model.sr)

# Multilingual examples
multilingual_model = ChatterboxMultilingualTTS.from_pretrained(device=device)

french_text = "Bonjour, comment √ßa va? Ceci est le mod√®le de synth√®se vocale multilingue Chatterbox, il prend en charge 23 langues."
wav_french = multilingual_model.generate(spanish_text, language_id="fr")
ta.save("test-french.wav", wav_french, model.sr)

chinese_text = "‰Ω†Â•ΩÔºå‰ªäÂ§©Â§©Ê∞îÁúü‰∏çÈîôÔºåÂ∏åÊúõ‰Ω†Êúâ‰∏Ä‰∏™ÊÑâÂø´ÁöÑÂë®Êú´„ÄÇ"
wav_chinese = multilingual_model.generate(chinese_text, language_id="zh")
ta.save("test-chinese.wav", wav_chinese, model.sr)

# If you want to synthesize with a different voice, specify the audio prompt
AUDIO_PROMPT_PATH = "YOUR_FILE.wav"
wav = model.generate(text, audio_prompt_path=AUDIO_PROMPT_PATH)
ta.save("test-2.wav", wav, model.sr)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;code&gt;example_tts.py&lt;/code&gt; and &lt;code&gt;example_vc.py&lt;/code&gt; for more examples.&lt;/p&gt; 
&lt;h1&gt;Acknowledgements&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/FunAudioLLM/CosyVoice"&gt;Cosyvoice&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning"&gt;Real-Time-Voice-Cloning&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/yl4579/HiFTNet"&gt;HiFT-GAN&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/meta-llama/llama3"&gt;Llama 3&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/xingchensong/S3Tokenizer"&gt;S3Tokenizer&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Built-in PerTh Watermarking for Responsible AI&lt;/h1&gt; 
&lt;p&gt;Every audio file generated by Chatterbox includes &lt;a href="https://github.com/resemble-ai/perth"&gt;Resemble AI's Perth (Perceptual Threshold) Watermarker&lt;/a&gt; - imperceptible neural watermarks that survive MP3 compression, audio editing, and common manipulations while maintaining nearly 100% detection accuracy.&lt;/p&gt; 
&lt;h2&gt;Watermark extraction&lt;/h2&gt; 
&lt;p&gt;You can look for the watermark using the following script.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import perth
import librosa

AUDIO_PATH = "YOUR_FILE.wav"

# Load the watermarked audio
watermarked_audio, sr = librosa.load(AUDIO_PATH, sr=None)

# Initialize watermarker (same as used for embedding)
watermarker = perth.PerthImplicitWatermarker()

# Extract watermark
watermark = watermarker.get_watermark(watermarked_audio, sample_rate=sr)
print(f"Extracted watermark: {watermark}")
# Output: 0.0 (no watermark) or 1.0 (watermarked)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;Official Discord&lt;/h1&gt; 
&lt;p&gt;üëã Join us on &lt;a href="https://discord.gg/rJq9cRJBJ6"&gt;Discord&lt;/a&gt; and let's build something awesome together!&lt;/p&gt; 
&lt;h1&gt;Citation&lt;/h1&gt; 
&lt;p&gt;If you find this model useful, please consider citing.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@misc{chatterboxtts2025,
  author       = {{Resemble AI}},
  title        = {{Chatterbox-TTS}},
  year         = {2025},
  howpublished = {\url{https://github.com/resemble-ai/chatterbox}},
  note         = {GitHub repository}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;Disclaimer&lt;/h1&gt; 
&lt;p&gt;Don't use this model to do bad things. Prompts are sourced from freely available data on the internet.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>emcie-co/parlant</title>
      <link>https://github.com/emcie-co/parlant</link>
      <description>&lt;p&gt;LLM agents built for control. Designed for real-world use. Deployed in minutes.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://github.com/emcie-co/parlant/blob/develop/docs/LogoTransparentLight.png?raw=true" /&gt; 
  &lt;img alt="Parlant - AI Agent Framework" src="https://github.com/emcie-co/parlant/raw/develop/docs/LogoTransparentDark.png?raw=true" width="400" /&gt; 
 &lt;/picture&gt; 
 &lt;h3&gt;Finally, LLM agents that actually follow instructions&lt;/h3&gt; 
 &lt;p&gt; &lt;a href="https://www.parlant.io/" target="_blank"&gt;üåê Website&lt;/a&gt; ‚Ä¢ &lt;a href="https://www.parlant.io/docs/quickstart/installation" target="_blank"&gt;‚ö° Quick Start&lt;/a&gt; ‚Ä¢ &lt;a href="https://discord.gg/duxWqxKk6J" target="_blank"&gt;üí¨ Discord&lt;/a&gt; ‚Ä¢ &lt;a href="https://www.parlant.io/docs/quickstart/examples" target="_blank"&gt;üìñ Examples&lt;/a&gt; &lt;/p&gt; 
 &lt;p&gt; 
  &lt;!-- Keep these links. Translations will automatically update with the README. --&gt; &lt;a href="https://zdoc.app/de/emcie-co/parlant"&gt;Deutsch&lt;/a&gt; | &lt;a href="https://zdoc.app/es/emcie-co/parlant"&gt;Espa√±ol&lt;/a&gt; | &lt;a href="https://zdoc.app/fr/emcie-co/parlant"&gt;fran√ßais&lt;/a&gt; | &lt;a href="https://zdoc.app/ja/emcie-co/parlant"&gt;Êó•Êú¨Ë™û&lt;/a&gt; | &lt;a href="https://zdoc.app/ko/emcie-co/parlant"&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt; | &lt;a href="https://zdoc.app/pt/emcie-co/parlant"&gt;Portugu√™s&lt;/a&gt; | &lt;a href="https://zdoc.app/ru/emcie-co/parlant"&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt; | &lt;a href="https://zdoc.app/zh/emcie-co/parlant"&gt;‰∏≠Êñá&lt;/a&gt; &lt;/p&gt; 
 &lt;p&gt; &lt;a href="https://pypi.org/project/parlant/"&gt;&lt;img alt="PyPI" src="https://img.shields.io/pypi/v/parlant?color=blue" /&gt;&lt;/a&gt; &lt;img alt="Python 3.10+" src="https://img.shields.io/badge/python-3.10+-blue" /&gt; &lt;a href="https://opensource.org/licenses/Apache-2.0"&gt;&lt;img alt="License" src="https://img.shields.io/badge/license-Apache%202.0-green" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/duxWqxKk6J"&gt;&lt;img alt="Discord" src="https://img.shields.io/discord/1312378700993663007?color=7289da&amp;amp;logo=discord&amp;amp;logoColor=white" /&gt;&lt;/a&gt; &lt;img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/emcie-co/parlant?style=social" /&gt; &lt;/p&gt; 
 &lt;a href="https://trendshift.io/repositories/12768" target="_blank"&gt; &lt;img src="https://trendshift.io/api/badge/repositories/12768" alt="Trending on TrendShift" style="width: 250px; height: 55px;" width="250" height="55" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;üéØ The Problem Every AI Developer Faces&lt;/h2&gt; 
&lt;p&gt;You build an AI agent. It works great in testing. Then real users start talking to it and...&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;‚ùå It ignores your carefully crafted system prompts&lt;/li&gt; 
 &lt;li&gt;‚ùå It hallucinates responses in critical moments&lt;/li&gt; 
 &lt;li&gt;‚ùå It can't handle edge cases consistently&lt;/li&gt; 
 &lt;li&gt;‚ùå Each conversation feels like a roll of the dice&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Sound familiar?&lt;/strong&gt; You're not alone. This is the #1 pain point for developers building production AI agents.&lt;/p&gt; 
&lt;h2&gt;‚ö° The Solution: Stop Fighting Prompts, Teach Principles&lt;/h2&gt; 
&lt;p&gt;Parlant flips the script on AI agent development. Instead of hoping your LLM will follow instructions, &lt;strong&gt;Parlant ensures it&lt;/strong&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Traditional approach: Cross your fingers ü§û
system_prompt = "You are a helpful assistant. Please follow these 47 rules..."

# Parlant approach: Ensured compliance ‚úÖ
await agent.create_guideline(
    condition="Customer asks about refunds",
    action="Check order status first to see if eligible",
    tools=[check_order_status],
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Parlant gives you all the structure you need to build customer-facing agents that behave exactly as your business requires:&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://parlant.io/docs/concepts/customization/journeys"&gt;Journeys&lt;/a&gt;&lt;/strong&gt;: Define clear customer journeys and how your agent should respond at each step.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://parlant.io/docs/concepts/customization/guidelines"&gt;Behavioral Guidelines&lt;/a&gt;&lt;/strong&gt;: Easily craft agent behavior; Parlant will match the relevant elements contextually.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://parlant.io/docs/concepts/customization/tools"&gt;Tool Use&lt;/a&gt;&lt;/strong&gt;: Attach external APIs, data fetchers, or backend services to specific interaction events.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://parlant.io/docs/concepts/customization/glossary"&gt;Domain Adaptation&lt;/a&gt;&lt;/strong&gt;: Teach your agent domain-specific terminology and craft personalized responses.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://parlant.io/docs/concepts/customization/canned-responses"&gt;Canned Responses&lt;/a&gt;&lt;/strong&gt;: Use response templates to eliminate hallucinations and guarantee style consistency.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://parlant.io/docs/advanced/explainability"&gt;Explainability&lt;/a&gt;&lt;/strong&gt;: Understand why and when each guideline was matched and followed.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align="center"&gt; 
 &lt;h2&gt;üöÄ Get Your Agent Running in 60 Seconds&lt;/h2&gt; 
&lt;/div&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install parlant
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import parlant.sdk as p

@p.tool
async def get_weather(context: p.ToolContext, city: str) -&amp;gt; p.ToolResult:
    # Your weather API logic here
    return p.ToolResult(f"Sunny, 72¬∞F in {city}")

@p.tool
async def get_datetime(context: p.ToolContext) -&amp;gt; p.ToolResult:
    from datetime import datetime
    return p.ToolResult(datetime.now())

async def main():
    async with p.Server() as server:
        agent = await server.create_agent(
            name="WeatherBot",
            description="Helpful weather assistant"
        )

        # Have the agent's context be updated on every response (though
        # update interval is customizable) using a context variable.
        await agent.create_variable(name="current-datetime", tool=get_datetime)

        # Control and guide agent behavior with natural language
        await agent.create_guideline(
            condition="User asks about weather",
            action="Get current weather and provide a friendly response with suggestions",
            tools=[get_weather]
        )

        # Add other (reliably enforced) behavioral modeling elements
        # ...

        # üéâ Test playground ready at http://localhost:8800
        # Integrate the official React widget into your app,
        # or follow the tutorial to build your own frontend!

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;That's it!&lt;/strong&gt; Your agent is running with ensured rule-following behavior.&lt;/p&gt; 
&lt;h2&gt;üé¨ See It In Action&lt;/h2&gt; 
&lt;img alt="Parlant Demo" src="https://github.com/emcie-co/parlant/raw/develop/docs/demo.gif?raw=true" width="100%" /&gt; 
&lt;h2&gt;üî• Why Developers Are Switching to Parlant&lt;/h2&gt; 
&lt;table width="100%"&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;üèóÔ∏è &lt;strong&gt;Traditional AI Frameworks&lt;/strong&gt;&lt;/h3&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;‚ö° &lt;strong&gt;Parlant&lt;/strong&gt;&lt;/h3&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%"&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Write complex system prompts&lt;/li&gt; 
     &lt;li&gt;Hope the LLM follows them&lt;/li&gt; 
     &lt;li&gt;Debug unpredictable behaviors&lt;/li&gt; 
     &lt;li&gt;Scale by prompt engineering&lt;/li&gt; 
     &lt;li&gt;Cross fingers for reliability&lt;/li&gt; 
    &lt;/ul&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Define rules in natural language&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Ensured&lt;/strong&gt; rule compliance&lt;/li&gt; 
     &lt;li&gt;Predictable, consistent behavior&lt;/li&gt; 
     &lt;li&gt;Scale by adding guidelines&lt;/li&gt; 
     &lt;li&gt;Production-ready from day one&lt;/li&gt; 
    &lt;/ul&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;üéØ Perfect For Your Use Case&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;Financial Services&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;Healthcare&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;E-commerce&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;Legal Tech&lt;/strong&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;Compliance-first design&lt;/td&gt; 
    &lt;td align="center"&gt;HIPAA-ready agents&lt;/td&gt; 
    &lt;td align="center"&gt;Customer service at scale&lt;/td&gt; 
    &lt;td align="center"&gt;Precise legal guidance&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;Built-in risk management&lt;/td&gt; 
    &lt;td align="center"&gt;Patient data protection&lt;/td&gt; 
    &lt;td align="center"&gt;Order processing automation&lt;/td&gt; 
    &lt;td align="center"&gt;Document review assistance&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h2&gt;üõ†Ô∏è Enterprise-Grade Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;üß≠ Conversational Journeys&lt;/strong&gt; - Lead the customer step-by-step to a goal&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üéØ Dynamic Guideline Matching&lt;/strong&gt; - Context-aware rule application&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üîß Reliable Tool Integration&lt;/strong&gt; - APIs, databases, external services&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üìä Conversation Analytics&lt;/strong&gt; - Deep insights into agent behavior&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üîÑ Iterative Refinement&lt;/strong&gt; - Continuously improve agent responses&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üõ°Ô∏è Built-in Guardrails&lt;/strong&gt; - Prevent hallucination and off-topic responses&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üì± React Widget&lt;/strong&gt; - &lt;a href="https://github.com/emcie-co/parlant-chat-react"&gt;Drop-in chat UI for any web app&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üîç Full Explainability&lt;/strong&gt; - Understand every decision your agent makes&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üìà Join 8,000+ Developers Building Better AI&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;strong&gt;Companies using Parlant:&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;Financial institutions ‚Ä¢ Healthcare providers ‚Ä¢ Legal firms ‚Ä¢ E-commerce platforms&lt;/em&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://star-history.com/#emcie-co/parlant&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=emcie-co/parlant&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;üåü What Developers Are Saying&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;em&gt;"By far the most elegant conversational AI framework that I've come across! Developing with Parlant is pure joy."&lt;/em&gt; &lt;strong&gt;‚Äî Vishal Ahuja, Senior Lead, Customer-Facing Conversational AI @ JPMorgan Chase&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;üèÉ‚Äç‚ôÇÔ∏è Quick Start Paths&lt;/h2&gt; 
&lt;table border="0"&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üéØ I want to test it myself&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.parlant.io/docs/quickstart/installation"&gt;‚Üí 5-minute quickstart&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üõ†Ô∏è I want to see an example&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.parlant.io/docs/quickstart/examples"&gt;‚Üí Healthcare agent example&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üöÄ I want to get involved&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://discord.gg/duxWqxKk6J"&gt;‚Üí Join our Discord community&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;ü§ù Community &amp;amp; Support&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;üí¨ &lt;strong&gt;&lt;a href="https://discord.gg/duxWqxKk6J"&gt;Discord Community&lt;/a&gt;&lt;/strong&gt; - Get help from the team and community&lt;/li&gt; 
 &lt;li&gt;üìñ &lt;strong&gt;&lt;a href="https://parlant.io/docs/quickstart/installation"&gt;Documentation&lt;/a&gt;&lt;/strong&gt; - Comprehensive guides and examples&lt;/li&gt; 
 &lt;li&gt;üêõ &lt;strong&gt;&lt;a href="https://github.com/emcie-co/parlant/issues"&gt;GitHub Issues&lt;/a&gt;&lt;/strong&gt; - Bug reports and feature requests&lt;/li&gt; 
 &lt;li&gt;üìß &lt;strong&gt;&lt;a href="https://parlant.io/contact"&gt;Direct Support&lt;/a&gt;&lt;/strong&gt; - Direct line to our engineering team&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üìÑ License&lt;/h2&gt; 
&lt;p&gt;Apache 2.0 - Use it anywhere, including commercial projects.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;strong&gt;Ready to build AI agents that actually work?&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;‚≠ê &lt;strong&gt;Star this repo&lt;/strong&gt; ‚Ä¢ üöÄ &lt;strong&gt;&lt;a href="https://parlant.io/"&gt;Try Parlant now&lt;/a&gt;&lt;/strong&gt; ‚Ä¢ üí¨ &lt;strong&gt;&lt;a href="https://discord.gg/duxWqxKk6J"&gt;Join Discord&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;Built with ‚ù§Ô∏è by the team at &lt;a href="https://emcie.co"&gt;Emcie&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>ansible/ansible</title>
      <link>https://github.com/ansible/ansible</link>
      <description>&lt;p&gt;Ansible is a radically simple IT automation platform that makes your applications and systems easier to deploy and maintain. Automate everything from code deployment to network configuration to cloud management, in a language that approaches plain English, using SSH, with no agents to install on remote systems. https://docs.ansible.com.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href="https://pypi.org/project/ansible-core"&gt;&lt;img src="https://img.shields.io/pypi/v/ansible-core.svg?sanitize=true" alt="PyPI version" /&gt;&lt;/a&gt; &lt;a href="https://docs.ansible.com/ansible/latest/"&gt;&lt;img src="https://img.shields.io/badge/docs-latest-brightgreen.svg?sanitize=true" alt="Docs badge" /&gt;&lt;/a&gt; &lt;a href="https://docs.ansible.com/ansible/devel/community/communication.html"&gt;&lt;img src="https://img.shields.io/badge/chat-IRC-brightgreen.svg?sanitize=true" alt="Chat badge" /&gt;&lt;/a&gt; &lt;a href="https://dev.azure.com/ansible/ansible/_build/latest?definitionId=20&amp;amp;branchName=devel"&gt;&lt;img src="https://dev.azure.com/ansible/ansible/_apis/build/status/CI?branchName=devel" alt="Build Status" /&gt;&lt;/a&gt; &lt;a href="https://docs.ansible.com/ansible/devel/community/code_of_conduct.html"&gt;&lt;img src="https://img.shields.io/badge/code%20of%20conduct-Ansible-silver.svg?sanitize=true" alt="Ansible Code of Conduct" /&gt;&lt;/a&gt; &lt;a href="https://docs.ansible.com/ansible/devel/community/communication.html#mailing-list-information"&gt;&lt;img src="https://img.shields.io/badge/mailing%20lists-Ansible-orange.svg?sanitize=true" alt="Ansible mailing lists" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/ansible/ansible/devel/COPYING"&gt;&lt;img src="https://img.shields.io/badge/license-GPL%20v3.0-brightgreen.svg?sanitize=true" alt="Repository License" /&gt;&lt;/a&gt; &lt;a href="https://bestpractices.coreinfrastructure.org/projects/2372"&gt;&lt;img src="https://bestpractices.coreinfrastructure.org/projects/2372/badge" alt="Ansible CII Best Practices certification" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Ansible&lt;/h1&gt; 
&lt;p&gt;Ansible is a radically simple IT automation system. It handles configuration management, application deployment, cloud provisioning, ad-hoc task execution, network automation, and multi-node orchestration. Ansible makes complex changes like zero-downtime rolling updates with load balancers easy. More information on the Ansible &lt;a href="https://ansible.com/"&gt;website&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Design Principles&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Have an extremely simple setup process with a minimal learning curve.&lt;/li&gt; 
 &lt;li&gt;Manage machines quickly and in parallel.&lt;/li&gt; 
 &lt;li&gt;Avoid custom-agents and additional open ports, be agentless by leveraging the existing SSH daemon.&lt;/li&gt; 
 &lt;li&gt;Describe infrastructure in a language that is both machine and human friendly.&lt;/li&gt; 
 &lt;li&gt;Focus on security and easy auditability/review/rewriting of content.&lt;/li&gt; 
 &lt;li&gt;Manage new remote machines instantly, without bootstrapping any software.&lt;/li&gt; 
 &lt;li&gt;Allow module development in any dynamic language, not just Python.&lt;/li&gt; 
 &lt;li&gt;Be usable as non-root.&lt;/li&gt; 
 &lt;li&gt;Be the easiest IT automation system to use, ever.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Use Ansible&lt;/h2&gt; 
&lt;p&gt;You can install a released version of Ansible with &lt;code&gt;pip&lt;/code&gt; or a package manager. See our &lt;a href="https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html"&gt;installation guide&lt;/a&gt; for details on installing Ansible on a variety of platforms.&lt;/p&gt; 
&lt;p&gt;Power users and developers can run the &lt;code&gt;devel&lt;/code&gt; branch, which has the latest features and fixes, directly. Although it is reasonably stable, you are more likely to encounter breaking changes when running the &lt;code&gt;devel&lt;/code&gt; branch. We recommend getting involved in the Ansible community if you want to run the &lt;code&gt;devel&lt;/code&gt; branch.&lt;/p&gt; 
&lt;h2&gt;Communication&lt;/h2&gt; 
&lt;p&gt;Join the Ansible forum to ask questions, get help, and interact with the community.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://forum.ansible.com/c/help/6"&gt;Get Help&lt;/a&gt;: Find help or share your Ansible knowledge to help others. Use tags to filter and subscribe to posts, such as the following: 
  &lt;ul&gt; 
   &lt;li&gt;Posts tagged with &lt;a href="https://forum.ansible.com/tag/ansible"&gt;ansible&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Posts tagged with &lt;a href="https://forum.ansible.com/tag/ansible-core"&gt;ansible-core&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Posts tagged with &lt;a href="https://forum.ansible.com/tag/playbook"&gt;playbook&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://forum.ansible.com/c/chat/4"&gt;Social Spaces&lt;/a&gt;: Meet and interact with fellow enthusiasts.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://forum.ansible.com/c/news/5"&gt;News &amp;amp; Announcements&lt;/a&gt;: Track project-wide announcements including social events.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.ansible.com/ansible/devel/community/communication.html#the-bullhorn"&gt;Bullhorn newsletter&lt;/a&gt;: Get release announcements and important changes.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For more ways to get in touch, see &lt;a href="https://docs.ansible.com/ansible/devel/community/communication.html"&gt;Communicating with the Ansible community&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contribute to Ansible&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Check out the &lt;a href="https://raw.githubusercontent.com/ansible/ansible/devel/.github/CONTRIBUTING.md"&gt;Contributor's Guide&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Read &lt;a href="https://docs.ansible.com/ansible/devel/community"&gt;Community Information&lt;/a&gt; for all kinds of ways to contribute to and interact with the project, including how to submit bug reports and code to Ansible.&lt;/li&gt; 
 &lt;li&gt;Submit a proposed code update through a pull request to the &lt;code&gt;devel&lt;/code&gt; branch.&lt;/li&gt; 
 &lt;li&gt;Talk to us before making larger changes to avoid duplicate efforts. This not only helps everyone know what is going on, but it also helps save time and effort if we decide some changes are needed.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Coding Guidelines&lt;/h2&gt; 
&lt;p&gt;We document our Coding Guidelines in the &lt;a href="https://docs.ansible.com/ansible/devel/dev_guide/"&gt;Developer Guide&lt;/a&gt;. We particularly suggest you review:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.ansible.com/ansible/devel/dev_guide/developing_modules_checklist.html"&gt;Contributing your module to Ansible&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.ansible.com/ansible/devel/dev_guide/developing_modules_best_practices.html"&gt;Conventions, tips, and pitfalls&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Branch Info&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;The &lt;code&gt;devel&lt;/code&gt; branch corresponds to the release actively under development.&lt;/li&gt; 
 &lt;li&gt;The &lt;code&gt;stable-2.X&lt;/code&gt; branches correspond to stable releases.&lt;/li&gt; 
 &lt;li&gt;Create a branch based on &lt;code&gt;devel&lt;/code&gt; and set up a &lt;a href="https://docs.ansible.com/ansible/devel/dev_guide/developing_modules_general.html#common-environment-setup"&gt;dev environment&lt;/a&gt; if you want to open a PR.&lt;/li&gt; 
 &lt;li&gt;See the &lt;a href="https://docs.ansible.com/ansible/devel/reference_appendices/release_and_maintenance.html"&gt;Ansible release and maintenance&lt;/a&gt; page for information about active branches.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Roadmap&lt;/h2&gt; 
&lt;p&gt;Based on team and community feedback, an initial roadmap will be published for a major or minor version (ex: 2.7, 2.8). The &lt;a href="https://docs.ansible.com/ansible/devel/roadmap/"&gt;Ansible Roadmap page&lt;/a&gt; details what is planned and how to influence the roadmap.&lt;/p&gt; 
&lt;h2&gt;Authors&lt;/h2&gt; 
&lt;p&gt;Ansible was created by &lt;a href="https://github.com/mpdehaan"&gt;Michael DeHaan&lt;/a&gt; and has contributions from over 5000 users (and growing). Thanks everyone!&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.ansible.com"&gt;Ansible&lt;/a&gt; is sponsored by &lt;a href="https://www.redhat.com"&gt;Red Hat, Inc.&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;GNU General Public License v3.0 or later&lt;/p&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/ansible/ansible/devel/COPYING"&gt;COPYING&lt;/a&gt; to see the full text.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>pytorch/ao</title>
      <link>https://github.com/pytorch/ao</link>
      <description>&lt;p&gt;PyTorch native quantization and sparsity for training and inference&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h1&gt;TorchAO&lt;/h1&gt; 
&lt;/div&gt; 
&lt;h3&gt;PyTorch-Native Training-to-Serving Model Optimization&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Pre-train Llama-3.1-70B &lt;strong&gt;1.5x faster&lt;/strong&gt; with float8 training&lt;/li&gt; 
 &lt;li&gt;Recover &lt;strong&gt;77% of quantized perplexity degradation&lt;/strong&gt; on Llama-3.2-3B with QAT&lt;/li&gt; 
 &lt;li&gt;Quantize Llama-3-8B to int4 for &lt;strong&gt;1.89x faster&lt;/strong&gt; inference with &lt;strong&gt;58% less memory&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://openreview.net/attachment?id=HpqH0JakHf&amp;amp;name=pdf"&gt;&lt;img src="https://img.shields.io/badge/CodeML_%40_ICML-2025-blue" alt="" /&gt;&lt;/a&gt; &lt;a href="https://discord.com/channels/1189498204333543425/1205223658021458100"&gt;&lt;img src="https://dcbadge.vercel.app/api/server/gpumode?style=flat&amp;amp;label=TorchAO%20in%20GPU%20Mode" alt="" /&gt;&lt;/a&gt; &lt;a href="https://github.com/pytorch/ao/graphs/contributors"&gt;&lt;img src="https://img.shields.io/github/contributors-anon/pytorch/ao?color=yellow&amp;amp;style=flat-square" alt="" /&gt;&lt;/a&gt; &lt;a href="https://docs.pytorch.org/ao/stable/index.html"&gt;&lt;img src="https://img.shields.io/badge/torchao-documentation-blue?color=DE3412" alt="" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/pytorch/ao/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/license-BSD_3--Clause-lightgrey.svg?sanitize=true" alt="license" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/pytorch/ao/main/#-latest-news"&gt;Latest News&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/pytorch/ao/main/#-overview"&gt;Overview&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/pytorch/ao/main/#-quick-start"&gt;Quick Start&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/pytorch/ao/main/#-installation"&gt;Installation&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/pytorch/ao/main/#-integrations"&gt;Integrations&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/pytorch/ao/main/#-inference"&gt;Inference&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/pytorch/ao/main/#-training"&gt;Training&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/pytorch/ao/main/#-videos"&gt;Videos&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/pytorch/ao/main/#-citation"&gt;Citation&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;üì£ Latest News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;[Jun 25] Our &lt;a href="https://openreview.net/attachment?id=HpqH0JakHf&amp;amp;name=pdf"&gt;TorchAO paper&lt;/a&gt; was accepted to CodeML @ ICML 2025!&lt;/li&gt; 
 &lt;li&gt;[May 25] QAT is now integrated into &lt;a href="https://github.com/axolotl-ai-cloud/axolotl"&gt;Axolotl&lt;/a&gt; for fine-tuning (&lt;a href="https://docs.axolotl.ai/docs/qat.html"&gt;docs&lt;/a&gt;)!&lt;/li&gt; 
 &lt;li&gt;[Apr 25] Float8 rowwise training yielded &lt;a href="https://pytorch.org/blog/accelerating-large-scale-training-and-convergence-with-pytorch-float8-rowwise-on-crusoe-2k-h200s/"&gt;1.34-1.43x training speedup&lt;/a&gt; at 2k H100 GPU scale&lt;/li&gt; 
 &lt;li&gt;[Apr 25] TorchAO is added as a &lt;a href="https://docs.vllm.ai/en/latest/features/quantization/torchao.html"&gt;quantization backend to vLLM&lt;/a&gt; (&lt;a href="https://docs.vllm.ai/en/latest/features/quantization/torchao.html"&gt;docs&lt;/a&gt;)!&lt;/li&gt; 
 &lt;li&gt;[Mar 25] Our &lt;a href="https://openreview.net/pdf?id=O5feVk7p6Y"&gt;2:4 Sparsity paper&lt;/a&gt; was accepted to SLLM @ ICLR 2025!&lt;/li&gt; 
 &lt;li&gt;[Jan 25] Our &lt;a href="https://pytorch.org/blog/accelerating-llm-inference/"&gt;integration with GemLite and SGLang&lt;/a&gt; yielded 1.1-2x faster inference with int4 and float8 quantization across different batch sizes and tensor parallel sizes&lt;/li&gt; 
 &lt;li&gt;[Jan 25] We added &lt;a href="https://pytorch.org/blog/hi-po-low-bit-operators/"&gt;1-8 bit ARM CPU kernels&lt;/a&gt; for linear and embedding ops&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;Older news&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;[Nov 24] We achieved &lt;a href="https://pytorch.org/blog/training-using-float8-fsdp2/"&gt;1.43-1.51x faster pre-training&lt;/a&gt; on Llama-3.1-70B and 405B using float8 training&lt;/li&gt; 
  &lt;li&gt;[Oct 24] TorchAO is added as a quantization backend to HF Transformers!&lt;/li&gt; 
  &lt;li&gt;[Sep 24] We officially launched TorchAO. Check out our blog &lt;a href="https://pytorch.org/blog/pytorch-native-architecture-optimization/"&gt;here&lt;/a&gt;!&lt;/li&gt; 
  &lt;li&gt;[Jul 24] QAT &lt;a href="https://pytorch.org/blog/quantization-aware-training/"&gt;recovered up to 96% accuracy degradation&lt;/a&gt; from quantization on Llama-3-8B&lt;/li&gt; 
  &lt;li&gt;[Jun 24] Semi-structured 2:4 sparsity &lt;a href="https://pytorch.org/blog/accelerating-neural-network-training/"&gt;achieved 1.1x inference speedup and 1.3x training speedup&lt;/a&gt; on the SAM and ViT models respectively&lt;/li&gt; 
  &lt;li&gt;[Jun 24] Block sparsity &lt;a href="https://pytorch.org/blog/speeding-up-vits/"&gt;achieved 1.46x training speeedup&lt;/a&gt; on the ViT model with &amp;lt;2% drop in accuracy&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;üåÖ Overview&lt;/h2&gt; 
&lt;p&gt;TorchAO is a PyTorch-native model optimization framework leveraging quantization and sparsity to provide an end-to-end, training-to-serving workflow for AI models. TorchAO works out-of-the-box with &lt;code&gt;torch.compile()&lt;/code&gt; and &lt;code&gt;FSDP2&lt;/code&gt; across most HuggingFace PyTorch models. Key features include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Float8 &lt;a href="https://raw.githubusercontent.com/pytorch/ao/main/torchao/float8/README.md"&gt;training&lt;/a&gt; and &lt;a href="https://docs.pytorch.org/ao/main/generated/torchao.quantization.Float8DynamicActivationFloat8WeightConfig.html"&gt;inference&lt;/a&gt; for speedups without compromising accuracy&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/pytorch/ao/main/torchao/prototype/mx_formats/README.md"&gt;MX training and inference&lt;/a&gt;, provides MX tensor formats based on native PyTorch MX dtypes (prototype)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/pytorch/ao/main/torchao/quantization/qat/README.md"&gt;Quantization-Aware Training (QAT)&lt;/a&gt; for mitigating quantization degradation&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/pytorch/ao/main/torchao/quantization/README.md"&gt;Post-Training Quantization (PTQ)&lt;/a&gt; for int4, int8, fp6 etc, with matching kernels targeting a variety of backends including CUDA, ARM CPU, and XNNPACK&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/pytorch/ao/main/torchao/sparsity/README.md"&gt;Sparsity&lt;/a&gt;, includes different techniques such as 2:4 sparsity and block sparsity&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Check out our &lt;a href="https://docs.pytorch.org/ao/main/"&gt;docs&lt;/a&gt; for more details!&lt;/p&gt; 
&lt;p&gt;From the team that brought you the fast series:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;9.5x inference speedups for Image segmentation models with &lt;a href="https://pytorch.org/blog/accelerating-generative-ai"&gt;sam-fast&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;10x inference speedups for Language models with &lt;a href="https://pytorch.org/blog/accelerating-generative-ai-2"&gt;gpt-fast&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;3x inference speedup for Diffusion models with &lt;a href="https://pytorch.org/blog/accelerating-generative-ai-3"&gt;sd-fast&lt;/a&gt; (new: &lt;a href="https://pytorch.org/blog/presenting-flux-fast-making-flux-go-brrr-on-h100s/"&gt;flux-fast&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;2.7x inference speedup for FAIR‚Äôs Seamless M4T-v2 model with &lt;a href="https://pytorch.org/blog/accelerating-generative-ai-4/"&gt;seamlessv2-fast&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üöÄ Quick Start&lt;/h2&gt; 
&lt;p&gt;First, install TorchAO. We recommend installing the latest stable version:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install torchao
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Quantize your model weights to int4!&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;from torchao.quantization import Int4WeightOnlyConfig, quantize_
quantize_(model, Int4WeightOnlyConfig(group_size=32))
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Compared to a &lt;code&gt;torch.compiled&lt;/code&gt; bf16 baseline, your quantized model should be significantly smaller and faster on a single A100 GPU:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;int4 model size: 1.25 MB
bfloat16 model size: 4.00 MB
compression ratio: 3.2

bf16 mean time: 30.393 ms
int4 mean time: 4.410 ms
speedup: 6.9x
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For the full model setup and benchmark details, check out our &lt;a href="https://docs.pytorch.org/ao/stable/quick_start.html"&gt;quick start guide&lt;/a&gt;. Alternatively, try quantizing your favorite model using our &lt;a href="https://huggingface.co/spaces/pytorch/torchao-my-repo"&gt;HuggingFace space&lt;/a&gt;!&lt;/p&gt; 
&lt;h2&gt;üõ† Installation&lt;/h2&gt; 
&lt;p&gt;To install the latest stable version:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install torchao
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;Other installation options&lt;/summary&gt; 
 &lt;pre&gt;&lt;code&gt;# Nightly
pip install --pre torchao --index-url https://download.pytorch.org/whl/nightly/cu126

# Different CUDA versions
pip install torchao --index-url https://download.pytorch.org/whl/cu126  # CUDA 12.6
pip install torchao --index-url https://download.pytorch.org/whl/cpu    # CPU only

# For developers
USE_CUDA=1 python setup.py develop
USE_CPP=0 python setup.py develop
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p&gt;Please see the &lt;a href="https://github.com/pytorch/ao/issues/2919"&gt;torchao compability table&lt;/a&gt; for version requirements for dependencies.&lt;/p&gt; 
&lt;h2&gt;üîó Integrations&lt;/h2&gt; 
&lt;p&gt;TorchAO is integrated into some of the leading open-source libraries including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;HuggingFace transformers with a &lt;a href="https://huggingface.co/docs/transformers/main/quantization/torchao"&gt;builtin inference backend&lt;/a&gt; and &lt;a href="https://github.com/huggingface/transformers/pull/31865"&gt;low bit optimizers&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;HuggingFace diffusers best practices with &lt;code&gt;torch.compile&lt;/code&gt; and TorchAO in a standalone repo &lt;a href="https://github.com/huggingface/diffusers/raw/main/docs/source/en/quantization/torchao.md"&gt;diffusers-torchao&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;HuggingFace PEFT for LoRA using TorchAO as their &lt;a href="https://huggingface.co/docs/peft/en/developer_guides/quantization#torchao-pytorch-architecture-optimization"&gt;quantization backend&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Mobius HQQ backend leveraged our int4 kernels to get &lt;a href="https://github.com/mobiusml/hqq#faster-inference"&gt;195 tok/s on a 4090&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;TorchTune for our NF4 &lt;a href="https://docs.pytorch.org/torchtune/main/tutorials/qlora_finetune.html"&gt;QLoRA&lt;/a&gt;, &lt;a href="https://docs.pytorch.org/torchtune/main/recipes/qat_distributed.html"&gt;QAT&lt;/a&gt;, and &lt;a href="https://github.com/pytorch/torchtune/pull/2546"&gt;float8 quantized fine-tuning&lt;/a&gt; recipes&lt;/li&gt; 
 &lt;li&gt;TorchTitan for &lt;a href="https://github.com/pytorch/torchtitan/raw/main/docs/float8.md"&gt;float8 pre-training&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;VLLM for LLM serving: &lt;a href="https://docs.vllm.ai/en/latest/features/quantization/torchao.html"&gt;usage&lt;/a&gt;, &lt;a href="https://docs.pytorch.org/ao/main/torchao_vllm_integration.html"&gt;detailed docs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;SGLang for LLM serving: &lt;a href="https://docs.sglang.ai/backend/server_arguments.html#server-arguments"&gt;usage&lt;/a&gt; and the major &lt;a href="https://github.com/sgl-project/sglang/pull/1341"&gt;PR&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Axolotl for &lt;a href="https://docs.axolotl.ai/docs/qat.html"&gt;QAT&lt;/a&gt; and &lt;a href="https://docs.axolotl.ai/docs/quantize.html"&gt;PTQ&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üîé Inference&lt;/h2&gt; 
&lt;p&gt;TorchAO delivers substantial performance gains with minimal code changes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Int4 weight-only&lt;/strong&gt;: &lt;a href="https://raw.githubusercontent.com/pytorch/ao/main/torchao/quantization/README.md"&gt;1.89x throughput with 58.1% less memory&lt;/a&gt; on Llama-3-8B&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Float8 dynamic quantization&lt;/strong&gt;: &lt;a href="https://github.com/sayakpaul/diffusers-torchao"&gt;1.54x and 1.27x speedup on Flux.1-Dev* and CogVideoX-5b respectively&lt;/a&gt; on H100 with preserved quality&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Int4 + 2:4 Sparsity&lt;/strong&gt;: &lt;a href="https://raw.githubusercontent.com/pytorch/ao/main/torchao/sparsity/README.md"&gt;2.37x throughput with 67.7% memory reduction&lt;/a&gt; on Llama-3-8B&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Quantize any model with &lt;code&gt;nn.Linear&lt;/code&gt; layers in just one line (Option 1), or load the quantized model directly from HuggingFace using our integration with HuggingFace transformers (Option 2):&lt;/p&gt; 
&lt;h4&gt;Option 1: Direct TorchAO API&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from torchao.quantization.quant_api import quantize_, Int4WeightOnlyConfig
quantize_(model, Int4WeightOnlyConfig(group_size=128, use_hqq=True))
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Option 2: HuggingFace Integration&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from transformers import TorchAoConfig, AutoModelForCausalLM
from torchao.quantization.quant_api import Int4WeightOnlyConfig

# Create quantization configuration
quantization_config = TorchAoConfig(quant_type=Int4WeightOnlyConfig(group_size=128, use_hqq=True))

# Load and automatically quantize
quantized_model = AutoModelForCausalLM.from_pretrained(
    "microsoft/Phi-4-mini-instruct",
    torch_dtype="auto",
    device_map="auto",
    quantization_config=quantization_config
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Deploy quantized models in vLLM with one command:&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;vllm serve pytorch/Phi-4-mini-instruct-int4wo-hqq --tokenizer microsoft/Phi-4-mini-instruct -O3
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;With this quantization flow, we achieve &lt;strong&gt;67% VRAM reduction and 12-20% speedup&lt;/strong&gt; on A100 GPUs while maintaining model quality. For more detail, see this &lt;a href="https://huggingface.co/pytorch/Phi-4-mini-instruct-int4wo-hqq#quantization-recipe"&gt;step-by-step quantization guide&lt;/a&gt;. We also release some pre-quantized models &lt;a href="https://huggingface.co/pytorch"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;üöÖ Training&lt;/h2&gt; 
&lt;h3&gt;Quantization-Aware Training&lt;/h3&gt; 
&lt;p&gt;Post-training quantization can result in a fast and compact model, but may also lead to accuracy degradation. We recommend exploring Quantization-Aware Training (QAT) to overcome this limitation, especially for lower bit-width dtypes such as int4. In collaboration with &lt;a href="https://github.com/pytorch/torchtune/raw/main/recipes/quantization.md#quantization-aware-training-qat"&gt;TorchTune&lt;/a&gt;, we've developed a QAT recipe that demonstrates significant accuracy improvements over traditional PTQ, recovering &lt;strong&gt;96% of the accuracy degradation on hellaswag and 68% of the perplexity degradation on wikitext&lt;/strong&gt; for Llama3 compared to post-training quantization (PTQ). For more details, please refer to the &lt;a href="https://raw.githubusercontent.com/pytorch/ao/main/torchao/quantization/qat/README.md"&gt;QAT README&lt;/a&gt; and the &lt;a href="https://pytorch.org/blog/quantization-aware-training/"&gt;original blog&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from torchao.quantization import quantize_, Int8DynamicActivationInt4WeightConfig
from torchao.quantization.qat import QATConfig

# prepare
base_config = Int8DynamicActivationInt4WeightConfig(group_size=32)
quantize_(my_model, QATConfig(base_config, step="prepare"))

# train model (not shown)

# convert
quantize_(my_model, QATConfig(base_config, step="convert"))
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Users can also combine LoRA + QAT to speed up training by &lt;a href="https://dev-discuss.pytorch.org/t/speeding-up-qat-by-1-89x-with-lora/2700"&gt;1.89x&lt;/a&gt; compared to vanilla QAT using this &lt;a href="https://github.com/pytorch/torchtune/raw/main/recipes/qat_lora_finetune_distributed.py"&gt;fine-tuning recipe&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Float8&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/pytorch/ao/main/torchao/float8"&gt;torchao.float8&lt;/a&gt; implements training recipes with the scaled float8 dtypes, as laid out in &lt;a href="https://arxiv.org/abs/2209.05433"&gt;https://arxiv.org/abs/2209.05433&lt;/a&gt;. With &lt;code&gt;torch.compile&lt;/code&gt; on, current results show throughput speedups of up to &lt;strong&gt;1.5x on up to 512 GPU / 405B parameter count scale&lt;/strong&gt; (&lt;a href="https://pytorch.org/blog/training-using-float8-fsdp2/"&gt;details&lt;/a&gt;):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from torchao.float8 import convert_to_float8_training
convert_to_float8_training(m)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Our float8 training is integrated into &lt;a href="https://github.com/pytorch/torchtitan/raw/main/docs/float8.md"&gt;TorchTitan's pre-training flows&lt;/a&gt; so users can easily try it out. For more details, check out these blog posts about our float8 training support:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://pytorch.org/blog/accelerating-large-scale-training-and-convergence-with-pytorch-float8-rowwise-on-crusoe-2k-h200s/"&gt;Accelerating Large Scale Training and Convergence with PyTorch Float8 Rowwise on Crusoe 2K H200s&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pytorch.org/blog/training-using-float8-fsdp2/"&gt;Supercharging Training using float8 and FSDP2&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aws.amazon.com/blogs/machine-learning/efficient-pre-training-of-llama-3-like-model-architectures-using-torchtitan-on-amazon-sagemaker/"&gt;Efficient Pre-training of Llama 3-like model architectures using torchtitan on Amazon SageMaker&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://dev-discuss.pytorch.org/t/float8-in-pytorch-1-x/1815"&gt;Float8 in PyTorch&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Sparse Training&lt;/h3&gt; 
&lt;p&gt;We've added support for semi-structured 2:4 sparsity with &lt;strong&gt;6% end-to-end speedups on ViT-L&lt;/strong&gt;. Full blog &lt;a href="https://pytorch.org/blog/accelerating-neural-network-training/"&gt;here&lt;/a&gt;. The code change is a 1 liner with the full example available &lt;a href="https://raw.githubusercontent.com/pytorch/ao/main/torchao/sparsity/training/"&gt;here&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from torchao.sparsity.training import SemiSparseLinear, swap_linear_with_semi_sparse_linear
swap_linear_with_semi_sparse_linear(model, {"seq.0": SemiSparseLinear})
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Memory-efficient optimizers&lt;/h3&gt; 
&lt;p&gt;Optimizers like ADAM can consume substantial GPU memory - 2x as much as the model parameters themselves. TorchAO provides two approaches to reduce this overhead:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;1. Quantized optimizers&lt;/strong&gt;: Reduce optimizer state memory by 2-4x by quantizing to lower precision&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from torchao.optim import AdamW8bit, AdamW4bit, AdamWFp8
optim = AdamW8bit(model.parameters()) # replace with Adam4bit and AdamFp8 for the 4 / fp8 versions
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Our quantized optimizers are implemented in just a few hundred lines of PyTorch code and compiled for efficiency. While slightly slower than specialized kernels, they offer an excellent balance of memory savings and performance. See detailed &lt;a href="https://github.com/pytorch/ao/tree/main/torchao/optim"&gt;benchmarks here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;2. CPU offloading&lt;/strong&gt;: Move optimizer state and gradients to CPU memory&lt;/p&gt; 
&lt;p&gt;For maximum memory savings, we support &lt;a href="https://github.com/pytorch/ao/tree/main/torchao/optim#optimizer-cpu-offload"&gt;single GPU CPU offloading&lt;/a&gt; that efficiently moves both gradients and optimizer state to CPU memory. This approach can &lt;strong&gt;reduce your VRAM requirements by 60%&lt;/strong&gt; with minimal impact on training speed:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;optim = CPUOffloadOptimizer(model.parameters(), torch.optim.AdamW, fused=True)
optim.load_state_dict(ckpt["optim"])
&lt;/code&gt;&lt;/pre&gt; 
&lt;!--
## For Developers

### Composability
`torch.compile`: A key design principle for us is composability - any custom dtype or memory layout should work with our compiler. We enable kernel implementations in PyTorch, CUDA, C++, or Triton. This allows researchers and engineers to start with high-level dtype and layout logic in pure PyTorch, then progressively optimize performance by implementing lower-level kernels as needed, while maintaining compatibility with the compile infrastructure.

[FSDP2](https://github.com/pytorch/torchtitan/blob/main/docs/fsdp.md): Historically most quantization has been done for inference, there is now a thriving area of research combining distributed algorithms and quantization.

The best example we have combining the composability of lower bit dtype with compile and fsdp is [NF4](torchao/dtypes/nf4tensor.py) which we used to implement the [QLoRA](https://www.youtube.com/watch?v=UvRl4ansfCg) algorithm. So if you're doing research at the intersection of this area we'd love to hear from you.

Our framework makes it straightforward to add tensor parallel support to your custom quantized tensor subclass. Check out our [tensor parallel tutorial](tutorials/developer_api_guide/tensor_parallel.py) to see how a quantized tensor subclass can be extended to support column and row-wise tensor sharding while maintaining compatibility with `torch.compile`.

### Custom Kernels

We've added support for authoring and releasing [custom ops](./torchao/csrc/) that do not graph break with `torch.compile()`. We have a few examples you can follow

1. [fp6](torchao/dtypes/floatx/README.md) for 2x faster inference over fp16 with an easy to use API `quantize_(model, fpx_weight_only(3, 2))`
2. [2:4 Sparse Marlin GEMM](https://github.com/pytorch/ao/pull/733) 2x speedups for FP16xINT4 kernels even at batch sizes up to 256
3. [int4 tinygemm unpacker](https://github.com/pytorch/ao/pull/415) which makes it easier to switch quantized backends for inference

If you believe there's other CUDA kernels we should be taking a closer look at please leave a comment on [this issue](https://github.com/pytorch/ao/issues/697) or feel free to contribute directly to the repo.
--&gt; 
&lt;h2&gt;üé• Videos&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://youtu.be/FH5wiwOyPX4?si=VZK22hHz25GRzBG1&amp;amp;t=1009"&gt;Keynote talk at GPU MODE IRL&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://youtu.be/xcKwEZ77Cps?si=7BS6cXMGgYtFlnrA"&gt;Low precision dtypes at PyTorch conference&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=UvRl4ansfCg"&gt;Slaying OOMs at the Mastering LLM's course&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://youtu.be/1u9xUK3G4VM?si=4JcPlw2w8chPXW8J"&gt;Advanced Quantization at CUDA MODE&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/live/v_q2JTIqE20?si=mf7HeZ63rS-uYpS6"&gt;Chip Huyen's GPU Optimization Workshop&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=lVgrE36ZUw0"&gt;Cohere for AI community talk&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üí¨ Citation&lt;/h2&gt; 
&lt;p&gt;If you find the torchao library useful, please cite it in your work as below.&lt;/p&gt; 
&lt;!-- TODO: update to cite CodeML paper after Jul 2025 --&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@software{torchao,
  title={TorchAO: PyTorch-Native Training-to-Serving Model Optimization},
  author={torchao},
  url={https://github.com/pytorch/ao},
  license={BSD-3-Clause},
  month={oct},
  year={2024}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>eosphoros-ai/DB-GPT</title>
      <link>https://github.com/eosphoros-ai/DB-GPT</link>
      <description>&lt;p&gt;AI Native Data App Development framework with AWEL(Agentic Workflow Expression Language) and Agents&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;img src="https://raw.githubusercontent.com/eosphoros-ai/DB-GPT/main/assets/LOGO_SMALL.png" alt="Logo" style="vertical-align: middle; height: 24px;" /&gt; DB-GPT: AI Native Data App Development framework with AWEL and Agents&lt;/h1&gt; 
&lt;p align="left"&gt; &lt;img src="https://raw.githubusercontent.com/eosphoros-ai/DB-GPT/main/assets/Twitter_LOGO.png" width="100%" /&gt; &lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;a href="https://github.com/eosphoros-ai/DB-GPT"&gt; &lt;img alt="stars" src="https://img.shields.io/github/stars/eosphoros-ai/db-gpt?style=social" /&gt; &lt;/a&gt; &lt;a href="https://github.com/eosphoros-ai/DB-GPT"&gt; &lt;img alt="forks" src="https://img.shields.io/github/forks/eosphoros-ai/db-gpt?style=social" /&gt; &lt;/a&gt; &lt;a href="http://dbgpt.cn/"&gt; &lt;img alt="Official Website" src="https://img.shields.io/badge/Official%20website-DB--GPT-blue?style=flat&amp;amp;labelColor=3366CC" /&gt; &lt;/a&gt; &lt;a href="https://opensource.org/licenses/MIT"&gt; &lt;img alt="License: MIT" src="https://img.shields.io/github/license/eosphoros-ai/db-gpt?style=flat&amp;amp;labelColor=009966&amp;amp;color=009933" /&gt; &lt;/a&gt; &lt;a href="https://github.com/eosphoros-ai/DB-GPT/releases"&gt; &lt;img alt="Release Notes" src="https://img.shields.io/github/v/release/eosphoros-ai/db-gpt?style=flat&amp;amp;labelColor=FF9933&amp;amp;color=FF6633" /&gt; &lt;/a&gt; &lt;a href="https://github.com/eosphoros-ai/DB-GPT/issues"&gt; &lt;img alt="Open Issues" src="https://img.shields.io/github/issues-raw/eosphoros-ai/db-gpt?style=flat&amp;amp;labelColor=666666&amp;amp;color=333333" /&gt; &lt;/a&gt; &lt;a href="https://x.com/DBGPT_AI"&gt; &lt;img alt="X (formerly Twitter) Follow" src="https://img.shields.io/twitter/follow/DBGPT_AI" /&gt; &lt;/a&gt; &lt;a href="https://medium.com/@dbgpt0506"&gt; &lt;img alt="Medium Follow" src="https://badgen.net/badge/Medium/DB-GPT/333333?icon=medium&amp;amp;labelColor=666666" /&gt; &lt;/a&gt; &lt;a href="https://space.bilibili.com/3537113070963392"&gt; &lt;img alt="Bilibili Space" src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Frelation%2Fstat%3Fvmid%3D3537113070963392&amp;amp;query=data.follower&amp;amp;style=flat&amp;amp;logo=bilibili&amp;amp;logoColor=white&amp;amp;label=Bilibili%20Fans&amp;amp;labelColor=F37697&amp;amp;color=6495ED" /&gt; &lt;/a&gt; &lt;a href="https://join.slack.com/t/slack-inu2564/shared_invite/zt-29rcnyw2b-N~ubOD9kFc7b7MDOAM1otA"&gt; &lt;img alt="Slack" src="https://img.shields.io/badge/Slack-Join%20us-5d6b98?style=flat&amp;amp;logo=slack&amp;amp;labelColor=7d89b0" /&gt; &lt;/a&gt; &lt;a href="https://codespaces.new/eosphoros-ai/DB-GPT"&gt; &lt;img alt="Open in GitHub Codespaces" src="https://github.com/codespaces/badge.svg?sanitize=true" /&gt; &lt;/a&gt; &lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/eosphoros-ai/DB-GPT/main/README.md"&gt;&lt;img src="https://img.shields.io/badge/English-d9d9d9?style=flat-square" alt="English" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/eosphoros-ai/DB-GPT/main/README.zh.md"&gt;&lt;img src="https://img.shields.io/badge/%E7%AE%80%E4%BD%93%E4%B8%AD%E6%96%87-d9d9d9?style=flat-square" alt="ÁÆÄ‰Ωì‰∏≠Êñá" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/eosphoros-ai/DB-GPT/main/README.ja.md"&gt;&lt;img src="https://img.shields.io/badge/%E6%97%A5%E6%9C%AC%E8%AA%9E-d9d9d9?style=flat-square" alt="Êó•Êú¨Ë™û" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="http://docs.dbgpt.cn/docs/overview/"&gt;&lt;strong&gt;Documents&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://github.com/eosphoros-ai/DB-GPT/raw/main/README.zh.md#%E8%81%94%E7%B3%BB%E6%88%91%E4%BB%AC"&gt;&lt;strong&gt;Contact Us&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://github.com/eosphoros-ai/community"&gt;&lt;strong&gt;Community&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://arxiv.org/pdf/2312.17449.pdf"&gt;&lt;strong&gt;Paper&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;What is DB-GPT?&lt;/h2&gt; 
&lt;p&gt;ü§ñ &lt;strong&gt;DB-GPT is an open source AI native data app development framework with AWEL(Agentic Workflow Expression Language) and agents&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;The purpose is to build infrastructure in the field of large models, through the development of multiple technical capabilities such as multi-model management (SMMF), Text2SQL effect optimization, RAG framework and optimization, Multi-Agents framework collaboration, AWEL (agent workflow orchestration), etc. Which makes large model applications with data simpler and more convenient.&lt;/p&gt; 
&lt;p&gt;üöÄ &lt;strong&gt;In the Data 3.0 era, based on models and databases, enterprises and developers can build their own bespoke applications with less code.&lt;/strong&gt;&lt;/p&gt; 
&lt;h3&gt;Introduction&lt;/h3&gt; 
&lt;p&gt;The architecture of DB-GPT is shown in the following figure:&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/eosphoros-ai/DB-GPT/main/assets/dbgpt.png" width="800" /&gt; &lt;/p&gt; 
&lt;p&gt;The core capabilities include the following parts:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;RAG (Retrieval Augmented Generation)&lt;/strong&gt;: RAG is currently the most practically implemented and urgently needed domain. DB-GPT has already implemented a framework based on RAG, allowing users to build knowledge-based applications using the RAG capabilities of DB-GPT.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;GBI (Generative Business Intelligence)&lt;/strong&gt;: Generative BI is one of the core capabilities of the DB-GPT project, providing the foundational data intelligence technology to build enterprise report analysis and business insights.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Fine-tuning Framework&lt;/strong&gt;: Model fine-tuning is an indispensable capability for any enterprise to implement in vertical and niche domains. DB-GPT provides a complete fine-tuning framework that integrates seamlessly with the DB-GPT project. In recent fine-tuning efforts, an accuracy rate based on the Spider dataset has been achieved at 82.5%.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Data-Driven Multi-Agents Framework&lt;/strong&gt;: DB-GPT offers a data-driven self-evolving multi-agents framework, aiming to continuously make decisions and execute based on data.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Data Factory&lt;/strong&gt;: The Data Factory is mainly about cleaning and processing trustworthy knowledge and data in the era of large models.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Data Sources&lt;/strong&gt;: Integrating various data sources to seamlessly connect production business data to the core capabilities of DB-GPT.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;SubModule&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/eosphoros-ai/DB-GPT-Hub"&gt;DB-GPT-Hub&lt;/a&gt; Text-to-SQL workflow with high performance by applying Supervised Fine-Tuning (SFT) on Large Language Models (LLMs).&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/eosphoros-ai/dbgpts"&gt;dbgpts&lt;/a&gt; dbgpts is the official repository which contains some data apps„ÄÅAWEL operators„ÄÅAWEL workflow templates and agents which build upon DB-GPT.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Text2SQL Finetune&lt;/h4&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;LLM&lt;/th&gt; 
   &lt;th align="center"&gt;Supported&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;LLaMA&lt;/td&gt; 
   &lt;td align="center"&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;LLaMA-2&lt;/td&gt; 
   &lt;td align="center"&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;BLOOM&lt;/td&gt; 
   &lt;td align="center"&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;BLOOMZ&lt;/td&gt; 
   &lt;td align="center"&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Falcon&lt;/td&gt; 
   &lt;td align="center"&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Baichuan&lt;/td&gt; 
   &lt;td align="center"&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Baichuan2&lt;/td&gt; 
   &lt;td align="center"&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;InternLM&lt;/td&gt; 
   &lt;td align="center"&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Qwen&lt;/td&gt; 
   &lt;td align="center"&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;XVERSE&lt;/td&gt; 
   &lt;td align="center"&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;ChatGLM2&lt;/td&gt; 
   &lt;td align="center"&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;a href="https://github.com/eosphoros-ai/DB-GPT-Hub"&gt;More Information about Text2SQL finetune&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/eosphoros-ai/DB-GPT-Plugins"&gt;DB-GPT-Plugins&lt;/a&gt; DB-GPT Plugins that can run Auto-GPT plugin directly&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/eosphoros-ai/GPT-Vis"&gt;GPT-Vis&lt;/a&gt; Visualization protocol&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;AI-Native Data App&lt;/h3&gt; 
&lt;hr /&gt; 
&lt;ul&gt; 
 &lt;li&gt;üî•üî•üî• &lt;a href="http://docs.dbgpt.cn/blog/db-gpt-v070-release"&gt;Released V0.7.0 | A set of significant upgrades&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://github.com/eosphoros-ai/DB-GPT/pull/2497"&gt;Support MCP Protocol&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/deepseek-ai/DeepSeek-R1"&gt;Support DeepSeek R1&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://huggingface.co/Qwen/QwQ-32B"&gt;Support QwQ-32B&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=""&gt;Refactor the basic modules&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eosphoros-ai/DB-GPT/main/packages/dbgpt-app"&gt;dbgpt-app&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eosphoros-ai/DB-GPT/main/packages/dbgpt-core"&gt;dbgpt-core&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eosphoros-ai/DB-GPT/main/packages/dbgpt-serve"&gt;dbgpt-serve&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eosphoros-ai/DB-GPT/main/packages/dbgpt-client"&gt;dbgpt-client&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eosphoros-ai/DB-GPT/main/packages/dbgpt-accelerator"&gt;dbgpt-accelerator&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eosphoros-ai/DB-GPT/main/packages/dbgpt-ext"&gt;dbgpt-ext&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/a2f0a875-df8c-4f0d-89a3-eed321c02113" alt="app_chat_v0 6" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/c8cc85bb-e3c2-4fab-8fb9-7b4b469d0611" alt="app_manage_chat_data_v0 6" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/b15d6ebe-54c4-4527-a16d-02fbbaf20dc9" alt="chat_dashboard_display_v0 6" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/40761507-a1e1-49d4-b49a-3dd9a5ea41cc" alt="agent_prompt_awel_v0 6" /&gt;&lt;/p&gt; 
&lt;h2&gt;Installation / Quick Start&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://img.shields.io/badge/docker-%230db7ed.svg?style=for-the-badge&amp;amp;logo=docker&amp;amp;logoColor=white" alt="Docker" /&gt; &lt;img src="https://img.shields.io/badge/Linux-FCC624?style=for-the-badge&amp;amp;logo=linux&amp;amp;logoColor=black" alt="Linux" /&gt; &lt;img src="https://img.shields.io/badge/mac%20os-000000?style=for-the-badge&amp;amp;logo=macos&amp;amp;logoColor=F0F0F0" alt="macOS" /&gt; &lt;img src="https://img.shields.io/badge/Windows-0078D6?style=for-the-badge&amp;amp;logo=windows&amp;amp;logoColor=white" alt="Windows" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="http://docs.dbgpt.cn/docs/overview"&gt;&lt;strong&gt;Usage Tutorial&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="http://docs.dbgpt.cn/docs/installation"&gt;&lt;strong&gt;Install&lt;/strong&gt;&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="http://docs.dbgpt.cn/docs/installation/docker"&gt;Docker&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="http://docs.dbgpt.cn/docs/installation/sourcecode"&gt;Source Code&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://docs.dbgpt.cn/docs/quickstart"&gt;&lt;strong&gt;Quickstart&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://docs.dbgpt.cn/docs/operation_manual"&gt;&lt;strong&gt;Application&lt;/strong&gt;&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="http://docs.dbgpt.cn/docs/cookbook/app/data_analysis_app_develop"&gt;Development Guide&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="http://docs.dbgpt.cn/docs/application/app_usage"&gt;App Usage&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="http://docs.dbgpt.cn/docs/application/awel_flow_usage"&gt;AWEL Flow Usage&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://docs.dbgpt.cn/docs/operation_manual/advanced_tutorial/debugging"&gt;&lt;strong&gt;Debugging&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://docs.dbgpt.cn/docs/application/advanced_tutorial/cli"&gt;&lt;strong&gt;Advanced Usage&lt;/strong&gt;&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="http://docs.dbgpt.cn/docs/application/advanced_tutorial/smmf"&gt;SMMF&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="http://docs.dbgpt.cn/docs/application/fine_tuning_manual/dbgpt_hub"&gt;Finetune&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="http://docs.dbgpt.cn/docs/awel/tutorial"&gt;AWEL&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;p&gt;At present, we have introduced several key features to showcase our current capabilities:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Private Domain Q&amp;amp;A &amp;amp; Data Processing&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The DB-GPT project offers a range of functionalities designed to improve knowledge base construction and enable efficient storage and retrieval of both structured and unstructured data. These functionalities include built-in support for uploading multiple file formats, the ability to integrate custom data extraction plug-ins, and unified vector storage and retrieval capabilities for effectively managing large volumes of information.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Multi-Data Source &amp;amp; GBI(Generative Business intelligence)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The DB-GPT project facilitates seamless natural language interaction with diverse data sources, including Excel, databases, and data warehouses. It simplifies the process of querying and retrieving information from these sources, empowering users to engage in intuitive conversations and gain insights. Moreover, DB-GPT supports the generation of analytical reports, providing users with valuable data summaries and interpretations.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Multi-Agents&amp;amp;Plugins&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;It offers support for custom plug-ins to perform various tasks and natively integrates the Auto-GPT plug-in model. The Agents protocol adheres to the Agent Protocol standard.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Automated Fine-tuning text2SQL&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We've also developed an automated fine-tuning lightweight framework centred on large language models (LLMs), Text2SQL datasets, LoRA/QLoRA/Pturning, and other fine-tuning methods. This framework simplifies Text-to-SQL fine-tuning, making it as straightforward as an assembly line process. &lt;a href="https://github.com/eosphoros-ai/DB-GPT-Hub"&gt;DB-GPT-Hub&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;SMMF(Service-oriented Multi-model Management Framework)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We offer extensive model support, including dozens of large language models (LLMs) from both open-source and API agents, such as LLaMA/LLaMA2, Baichuan, ChatGLM, Wenxin, Tongyi, Zhipu, and many more.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;News&lt;/p&gt; 
    &lt;table&gt; 
     &lt;thead&gt; 
      &lt;tr&gt; 
       &lt;th&gt;Provider&lt;/th&gt; 
       &lt;th&gt;Supported&lt;/th&gt; 
       &lt;th&gt;Models&lt;/th&gt; 
      &lt;/tr&gt; 
     &lt;/thead&gt; 
     &lt;tbody&gt; 
      &lt;tr&gt; 
       &lt;td align="center" valign="middle"&gt;DeepSeek&lt;/td&gt; 
       &lt;td align="center" valign="middle"&gt;‚úÖ&lt;/td&gt; 
       &lt;td&gt; üî•üî•üî• &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-0528"&gt;DeepSeek-R1-0528&lt;/a&gt;&lt;br /&gt; üî•üî•üî• &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3-0324"&gt;DeepSeek-V3-0324&lt;/a&gt;&lt;br /&gt; üî•üî•üî• &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1"&gt;DeepSeek-R1&lt;/a&gt;&lt;br /&gt; üî•üî•üî• &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3"&gt;DeepSeek-V3&lt;/a&gt;&lt;br /&gt; üî•üî•üî• &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B"&gt;DeepSeek-R1-Distill-Llama-70B&lt;/a&gt;&lt;br /&gt; üî•üî•üî• &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"&gt;DeepSeek-R1-Distill-Qwen-32B&lt;/a&gt;&lt;br /&gt; üî•üî•üî• &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Instruct"&gt;DeepSeek-Coder-V2-Instruct&lt;/a&gt;&lt;br /&gt; &lt;/td&gt; 
      &lt;/tr&gt; 
      &lt;tr&gt; 
       &lt;td align="center" valign="middle"&gt;Qwen&lt;/td&gt; 
       &lt;td align="center" valign="middle"&gt;‚úÖ&lt;/td&gt; 
       &lt;td&gt; üî•üî•üî• &lt;a href="https://huggingface.co/Qwen/Qwen3-235B-A22B"&gt;Qwen3-235B-A22B&lt;/a&gt;&lt;br /&gt; üî•üî•üî• &lt;a href="https://huggingface.co/Qwen/Qwen3-30B-A3B"&gt;Qwen3-30B-A3B&lt;/a&gt;&lt;br /&gt; üî•üî•üî• &lt;a href="https://huggingface.co/Qwen/Qwen3-32B"&gt;Qwen3-32B&lt;/a&gt;&lt;br /&gt; üî•üî•üî• &lt;a href="https://huggingface.co/Qwen/QwQ-32B"&gt;QwQ-32B&lt;/a&gt;&lt;br /&gt; üî•üî•üî• &lt;a href="https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct"&gt;Qwen2.5-Coder-32B-Instruct&lt;/a&gt;&lt;br /&gt; üî•üî•üî• &lt;a href="https://huggingface.co/Qwen/Qwen2.5-Coder-14B-Instruct"&gt;Qwen2.5-Coder-14B-Instruct&lt;/a&gt;&lt;br /&gt; üî•üî•üî• &lt;a href="https://huggingface.co/Qwen/Qwen2.5-72B-Instruct"&gt;Qwen2.5-72B-Instruct&lt;/a&gt;&lt;br /&gt; üî•üî•üî• &lt;a href="https://huggingface.co/Qwen/Qwen2.5-32B-Instruct"&gt;Qwen2.5-32B-Instruct&lt;/a&gt;&lt;br /&gt; &lt;/td&gt; 
      &lt;/tr&gt; 
      &lt;tr&gt; 
       &lt;td align="center" valign="middle"&gt;GLM&lt;/td&gt; 
       &lt;td align="center" valign="middle"&gt;‚úÖ&lt;/td&gt; 
       &lt;td&gt; üî•üî•üî• &lt;a href="https://huggingface.co/THUDM/GLM-Z1-32B-0414"&gt;GLM-Z1-32B-0414&lt;/a&gt;&lt;br /&gt; üî•üî•üî• &lt;a href="https://huggingface.co/THUDM/GLM-4-32B-0414"&gt;GLM-4-32B-0414&lt;/a&gt;&lt;br /&gt; üî•üî•üî• &lt;a href="https://huggingface.co/THUDM/glm-4-9b-chat"&gt;Glm-4-9b-chat&lt;/a&gt; &lt;/td&gt; 
      &lt;/tr&gt; 
      &lt;tr&gt; 
       &lt;td align="center" valign="middle"&gt;Llama&lt;/td&gt; 
       &lt;td align="center" valign="middle"&gt;‚úÖ&lt;/td&gt; 
       &lt;td&gt; üî•üî•üî• &lt;a href="https://huggingface.co/meta-llama/Meta-Llama-3.1-405B-Instruct"&gt;Meta-Llama-3.1-405B-Instruct&lt;/a&gt;&lt;br /&gt; üî•üî•üî• &lt;a href="https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct"&gt;Meta-Llama-3.1-70B-Instruct&lt;/a&gt;&lt;br /&gt; üî•üî•üî• &lt;a href="https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct"&gt;Meta-Llama-3.1-8B-Instruct&lt;/a&gt;&lt;br /&gt; üî•üî•üî• &lt;a href="https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct"&gt;Meta-Llama-3-70B-Instruct&lt;/a&gt;&lt;br /&gt; üî•üî•üî• &lt;a href="https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct"&gt;Meta-Llama-3-8B-Instruct&lt;/a&gt; &lt;/td&gt; 
      &lt;/tr&gt; 
      &lt;tr&gt; 
       &lt;td align="center" valign="middle"&gt;Gemma&lt;/td&gt; 
       &lt;td align="center" valign="middle"&gt;‚úÖ&lt;/td&gt; 
       &lt;td&gt; üî•üî•üî• &lt;a href="https://huggingface.co/google/gemma-2-27b-it"&gt;gemma-2-27b-it&lt;/a&gt;&lt;br /&gt; üî•üî•üî• &lt;a href="https://huggingface.co/google/gemma-2-9b-it"&gt;gemma-2-9b-it&lt;/a&gt;&lt;br /&gt; üî•üî•üî• &lt;a href="https://huggingface.co/google/gemma-7b-it"&gt;gemma-7b-it&lt;/a&gt;&lt;br /&gt; üî•üî•üî• &lt;a href="https://huggingface.co/google/gemma-2b-it"&gt;gemma-2b-it&lt;/a&gt; &lt;/td&gt; 
      &lt;/tr&gt; 
      &lt;tr&gt; 
       &lt;td align="center" valign="middle"&gt;Yi&lt;/td&gt; 
       &lt;td align="center" valign="middle"&gt;‚úÖ&lt;/td&gt; 
       &lt;td&gt; üî•üî•üî• &lt;a href="https://huggingface.co/01-ai/Yi-1.5-34B-Chat"&gt;Yi-1.5-34B-Chat&lt;/a&gt;&lt;br /&gt; üî•üî•üî• &lt;a href="https://huggingface.co/01-ai/Yi-1.5-9B-Chat"&gt;Yi-1.5-9B-Chat&lt;/a&gt;&lt;br /&gt; üî•üî•üî• &lt;a href="https://huggingface.co/01-ai/Yi-1.5-6B-Chat"&gt;Yi-1.5-6B-Chat&lt;/a&gt;&lt;br /&gt; üî•üî•üî• &lt;a href="https://huggingface.co/01-ai/Yi-34B-Chat"&gt;Yi-34B-Chat&lt;/a&gt; &lt;/td&gt; 
      &lt;/tr&gt; 
      &lt;tr&gt; 
       &lt;td align="center" valign="middle"&gt;Starling&lt;/td&gt; 
       &lt;td align="center" valign="middle"&gt;‚úÖ&lt;/td&gt; 
       &lt;td&gt; üî•üî•üî• &lt;a href="https://huggingface.co/Nexusflow/Starling-LM-7B-beta"&gt;Starling-LM-7B-beta&lt;/a&gt; &lt;/td&gt; 
      &lt;/tr&gt; 
      &lt;tr&gt; 
       &lt;td align="center" valign="middle"&gt;SOLAR&lt;/td&gt; 
       &lt;td align="center" valign="middle"&gt;‚úÖ&lt;/td&gt; 
       &lt;td&gt; üî•üî•üî• &lt;a href="https://huggingface.co/upstage/SOLAR-10.7B-Instruct-v1.0"&gt;SOLAR-10.7B&lt;/a&gt; &lt;/td&gt; 
      &lt;/tr&gt; 
      &lt;tr&gt; 
       &lt;td align="center" valign="middle"&gt;Mixtral&lt;/td&gt; 
       &lt;td align="center" valign="middle"&gt;‚úÖ&lt;/td&gt; 
       &lt;td&gt; üî•üî•üî• &lt;a href="https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1"&gt;Mixtral-8x7B&lt;/a&gt; &lt;/td&gt; 
      &lt;/tr&gt; 
      &lt;tr&gt; 
       &lt;td align="center" valign="middle"&gt;Phi&lt;/td&gt; 
       &lt;td align="center" valign="middle"&gt;‚úÖ&lt;/td&gt; 
       &lt;td&gt; üî•üî•üî• &lt;a href="https://huggingface.co/collections/microsoft/phi-3-6626e15e9585a200d2d761e3"&gt;Phi-3&lt;/a&gt; &lt;/td&gt; 
      &lt;/tr&gt; 
     &lt;/tbody&gt; 
    &lt;/table&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;a href="http://docs.dbgpt.site/docs/modules/smmf"&gt;More Supported LLMs&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Privacy and Security&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We ensure the privacy and security of data through the implementation of various technologies, including privatized large models and proxy desensitization.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Support Datasources&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="http://docs.dbgpt.cn/docs/modules/connections"&gt;Datasources&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Image&lt;/h2&gt; 
&lt;p&gt;üåê &lt;a href="https://www.codewithgpu.com/i/eosphoros-ai/DB-GPT/dbgpt"&gt;AutoDL Image&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Contribution&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;To check detailed guidelines for new contributions, please refer &lt;a href="https://github.com/eosphoros-ai/DB-GPT/raw/main/CONTRIBUTING.md"&gt;how to contribute&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Contributors Wall&lt;/h3&gt; 
&lt;a href="https://github.com/eosphoros-ai/DB-GPT/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=eosphoros-ai/DB-GPT&amp;amp;max=200" /&gt; &lt;/a&gt; 
&lt;h2&gt;Licence&lt;/h2&gt; 
&lt;p&gt;The MIT License (MIT)&lt;/p&gt; 
&lt;h2&gt;DISCKAIMER&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eosphoros-ai/DB-GPT/main/DISCKAIMER.md"&gt;disckaimer&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you want to understand the overall architecture of DB-GPT, please cite &lt;a href="https://arxiv.org/abs/2312.17449" target="_blank"&gt;Paper&lt;/a&gt; and &lt;a href="https://arxiv.org/abs/2404.10209" target="_blank"&gt;Paper&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;If you want to learn about using DB-GPT for Agent development, please cite the &lt;a href="https://arxiv.org/abs/2412.13520" target="_blank"&gt;Paper&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@article{xue2023dbgpt,
      title={DB-GPT: Empowering Database Interactions with Private Large Language Models}, 
      author={Siqiao Xue and Caigao Jiang and Wenhui Shi and Fangyin Cheng and Keting Chen and Hongjun Yang and Zhiping Zhang and Jianshan He and Hongyang Zhang and Ganglin Wei and Wang Zhao and Fan Zhou and Danrui Qi and Hong Yi and Shaodong Liu and Faqiang Chen},
      year={2023},
      journal={arXiv preprint arXiv:2312.17449},
      url={https://arxiv.org/abs/2312.17449}
}
@misc{huang2024romasrolebasedmultiagentdatabase,
      title={ROMAS: A Role-Based Multi-Agent System for Database monitoring and Planning}, 
      author={Yi Huang and Fangyin Cheng and Fan Zhou and Jiahui Li and Jian Gong and Hongjun Yang and Zhidong Fan and Caigao Jiang and Siqiao Xue and Faqiang Chen},
      year={2024},
      eprint={2412.13520},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2412.13520}, 
}
@inproceedings{xue2024demonstration,
      title={Demonstration of DB-GPT: Next Generation Data Interaction System Empowered by Large Language Models}, 
      author={Siqiao Xue and Danrui Qi and Caigao Jiang and Wenhui Shi and Fangyin Cheng and Keting Chen and Hongjun Yang and Zhiping Zhang and Jianshan He and Hongyang Zhang and Ganglin Wei and Wang Zhao and Fan Zhou and Hong Yi and Shaodong Liu and Hongjun Yang and Faqiang Chen},
      year={2024},
      booktitle = "Proceedings of the VLDB Endowment",
      url={https://arxiv.org/abs/2404.10209}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Contact Information&lt;/h2&gt; 
&lt;p&gt;Thanks to everyone who has contributed to DB-GPT! Your ideas, code, comments, and even sharing them at events and on social platforms can make DB-GPT better. We are working on building a community, if you have any ideas for building the community, feel free to contact us.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/eosphoros-ai/DB-GPT/issues"&gt;Github Issues&lt;/a&gt; ‚≠êÔ∏èÔºöFor questions about using GB-DPT, see the CONTRIBUTING.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/orgs/eosphoros-ai/discussions"&gt;Github Discussions&lt;/a&gt; ‚≠êÔ∏èÔºöShare your experience or unique apps.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://x.com/DBGPT_AI"&gt;Twitter&lt;/a&gt; ‚≠êÔ∏èÔºöPlease feel free to talk to us.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#csunny/DB-GPT"&gt;&lt;img src="https://api.star-history.com/svg?repos=csunny/DB-GPT&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>socfortress/Wazuh-Rules</title>
      <link>https://github.com/socfortress/Wazuh-Rules</link>
      <description>&lt;p&gt;Advanced Wazuh Rules for more accurate threat detection. Feel free to implement within your own Wazuh environment, contribute, or fork!&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href="https://www.socfortress.co/"&gt;&lt;img src="https://raw.githubusercontent.com/socfortress/Wazuh-Rules/main/images/logo_orange.svg?sanitize=true" align="right" width="100" height="100" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Advanced Wazuh Detection Rules &lt;a href="https://www.socfortress.co/trial.html"&gt;&lt;img src="https://img.shields.io/badge/SOCFortress-Worlds%20First%20Free%20Cloud%20SOC-orange" alt="Awesome" /&gt;&lt;/a&gt;&lt;/h1&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;The SOCFortress Team has committed to contributing to the Open Source community. We hope you find these rulesets helpful and robust as you work to keep your networks secure.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;a href="https://github.com/socfortress/Wazuh-Rules/network/members"&gt;&lt;img src="https://img.shields.io/github/forks/socfortress/Wazuh-Rules" alt="Forks" /&gt;&lt;/a&gt; &lt;a href="https://github.com/socfortress/Wazuh-Rules/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/socfortress/Wazuh-Rules" alt="Stargazers" /&gt;&lt;/a&gt; &lt;a href="https://socfortress.supportbench.net"&gt;&lt;img src="https://img.shields.io/badge/Help%20Desk-Help%20Desk-blue" alt="MIT License" /&gt;&lt;/a&gt; &lt;a href="https://www.socfortress.co/"&gt;&lt;img src="https://img.shields.io/badge/Visit%20Us-www.socfortress.co-orange" alt="LinkedIn" /&gt;&lt;/a&gt; &lt;a href="https://www.socfortress.co/trial.html"&gt;&lt;img src="https://img.shields.io/badge/Get%20Started-FREE%20FOR%20LIFE%20TIER-orange" alt="your-own-soc-free-for-life-tier" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;!-- PROJECT LOGO --&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/socfortress/Wazuh-Rules"&gt; &lt;img src="https://raw.githubusercontent.com/socfortress/Wazuh-Rules/main/images/logo_orange.svg?sanitize=true" alt="Logo" width="100" height="100" /&gt; &lt;img src="https://raw.githubusercontent.com/socfortress/Wazuh-Rules/main/images/wazuh_logo.png" alt="Logo" /&gt; &lt;/a&gt; 
 &lt;h3 align="center"&gt;Advanced Wazuh Detection Rules&lt;/h3&gt; 
 &lt;p align="center"&gt; Have Wazuh deployed and ingesting your logs but looking for some better detection rules? Look no further. The objective for this repo is to provide the Wazuh community with rulesets that are more accurate, descriptive, and enriched from various sources and integrations. &lt;br /&gt; &lt;a href="https://www.socfortress.co/index.html"&gt;&lt;strong&gt;Worlds First Open Source Cloud SOC ¬ª&lt;/strong&gt;&lt;/a&gt; &lt;br /&gt; &lt;br /&gt; &lt;a href="https://documentation.wazuh.com/current/index.html"&gt;Wazuh Docs&lt;/a&gt; ¬∑ &lt;a href="https://www.socfortress.co/trial.html"&gt;FREE FOR LIFE TIER&lt;/a&gt; ¬∑ &lt;a href="https://socfortress.medium.com/"&gt;Our Blog&lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;!-- TABLE OF CONTENTS --&gt; 
&lt;details&gt; 
 &lt;summary&gt;Table of Contents&lt;/summary&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;a href="https://raw.githubusercontent.com/socfortress/Wazuh-Rules/main/#about-this-repo"&gt;About This Repo&lt;/a&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/socfortress/Wazuh-Rules/main/#supported-rules-and-integrations"&gt;Supported Rules and Integrations&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;a href="https://raw.githubusercontent.com/socfortress/Wazuh-Rules/main/#getting-started"&gt;Getting Started&lt;/a&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/socfortress/Wazuh-Rules/main/#prerequisites"&gt;Prerequisites&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/socfortress/Wazuh-Rules/main/#installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/socfortress/Wazuh-Rules/main/#contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/socfortress/Wazuh-Rules/main/#contact"&gt;Contact&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/socfortress/Wazuh-Rules/main/#acknowledgments"&gt;Acknowledgments&lt;/a&gt;&lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/details&gt; 
&lt;!-- ABOUT THE PROJECT --&gt; 
&lt;h2&gt;About This Repo&lt;/h2&gt; 
&lt;p&gt;The objective for this repo is to provide the Wazuh community with rulesets that are more accurate, descriptive, and enriched from various sources and integrations.&lt;/p&gt; 
&lt;p&gt;Here's why:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Detection rules can be a tricky business and we believe everyone should have access to a strong and growing ruleset.&lt;/li&gt; 
 &lt;li&gt;Wazuh serves as a great EDR agent, however the default rulesets are rather laxed (in our opinion). We wanted to start building a strong repo of Wazuh rules for the community to implement themselves and expand upon as new threats arise.&lt;/li&gt; 
 &lt;li&gt;Cybersecurity is hard enough, let's work together &lt;span&gt;üòÑ&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="right"&gt;(&lt;a href="https://raw.githubusercontent.com/socfortress/Wazuh-Rules/main/#readme-top"&gt;back to top&lt;/a&gt;)&lt;/p&gt; 
&lt;h3&gt;Supported Rules and Integrations&lt;/h3&gt; 
&lt;p&gt;Below are the current rules and integrations currently contained within this repo. Integrations, such as Office365, Trend Micro, etc. will have scripts provided within their respective folders for use. Feel free to build upon these scripts and contribute back &lt;span&gt;üòÑ&lt;/span&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/socfortress/Wazuh-Rules/tree/main/Windows_Sysmon"&gt;Sysmon for Windows&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/socfortress/Wazuh-Rules/tree/main/Sysmon%20Linux"&gt;Sysmon for Linux&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/socfortress/Wazuh-Rules/tree/main/Office%20365"&gt;Office365&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/socfortress/Wazuh-Rules/tree/main/Office%20Defender"&gt;Microsoft Defender&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/socfortress/Wazuh-Rules/tree/main/Sophos"&gt;Sophos&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/socfortress/Wazuh-Rules/tree/main/MISP"&gt;MISP&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/socfortress/Wazuh-Rules/tree/main/Osquery"&gt;Osquery&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/socfortress/Wazuh-Rules/tree/main/Yara"&gt;Yara&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/socfortress/Wazuh-Rules/tree/main/Suricata"&gt;Suricata&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/socfortress/Wazuh-Rules/tree/main/Packetbeat"&gt;Packetbeat&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/socfortress/Wazuh-Rules/tree/main/Falco"&gt;Falco&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/socfortress/Wazuh-Rules/tree/main/Modsecurity"&gt;Modsecurity&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/socfortress/Wazuh-Rules/tree/main/F-Secure"&gt;F-Secure&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/socfortress/Wazuh-Rules/tree/main/Domain%20Stats"&gt;Domain Stats&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/socfortress/Wazuh-Rules/tree/main/Snyk"&gt;Snyk&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/socfortress/Wazuh-Rules/tree/main/Windows%20Autoruns"&gt;Autoruns&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/socfortress/Wazuh-Rules/tree/main/Windows%20Sysinternals%20Sigcheck"&gt;Sigcheck&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/socfortress/Wazuh-Rules/tree/main/Windows%20Powershell"&gt;Powershell&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/socfortress/Wazuh-Rules/tree/main/Crowdstrike"&gt;Crowdstrike&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/socfortress/Wazuh-Rules/tree/main/Domain%20Stats"&gt;Alienvault&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Tessian - WIP&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Roadmap&lt;/h3&gt; 
&lt;p&gt;Have an Integration already configured that you'd like to share? Or have an idea for an Integration that you would like help on? Feel free to add it to the Roadmap.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Feel free to bring ideas &lt;span&gt;üòÑ&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="right"&gt;(&lt;a href="https://raw.githubusercontent.com/socfortress/Wazuh-Rules/main/#readme-top"&gt;back to top&lt;/a&gt;)&lt;/p&gt; 
&lt;!-- GETTING STARTED --&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;Feel free to implement all of the rules that are contained within this repo, or pick and choose as you see fit. See our Installation section below for a bash script that can be ran on your Wazuh Manager to quickly put these rules to work!&lt;/p&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;p&gt;Wazuh-Manager Version 4.x Required.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://documentation.wazuh.com/current/index.html"&gt;Wazuh Install Docs&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.socfortress.co/contact_form.html"&gt;Need Assitance? - Hire SOCFortress&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;p&gt;&lt;em&gt;You can either manually download the .xml rule files onto your Wazuh Manager or make use of our wazuh_socfortress_rules.sh script&lt;/em&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;span&gt;‚ö†&lt;/span&gt; &lt;strong&gt;USE AT OWN RISK&lt;/strong&gt;: If you already have custom rules built out, there is a good chance duplicate Rule IDs will exists. This will casue the Wazuh-Manager service to fail! Ensure there are no conflicting Rule IDs and your custom rules are backed up prior to running the wazuh_socfortress_rules.sh script!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ol&gt; 
 &lt;li&gt;Become Root User&lt;/li&gt; 
 &lt;li&gt;Run the Script &lt;pre&gt;&lt;code class="language-sh"&gt;curl -so ~/wazuh_socfortress_rules.sh https://raw.githubusercontent.com/socfortress/Wazuh-Rules/main/wazuh_socfortress_rules.sh &amp;amp;&amp;amp; bash ~/wazuh_socfortress_rules.sh
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;img src="https://github.com/socfortress/Wazuh-Rules/raw/main/images/run%20install.gif" alt="Alt Text" /&gt;&lt;/p&gt; 
&lt;p align="right"&gt;(&lt;a href="https://raw.githubusercontent.com/socfortress/Wazuh-Rules/main/#readme-top"&gt;back to top&lt;/a&gt;)&lt;/p&gt; 
&lt;!-- CONTRIBUTING --&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Contributions are what make the open source community such an amazing place to learn, inspire, and create. Any contributions you make are &lt;strong&gt;greatly appreciated&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;If you have a suggestion that would make this better, please fork the repo and create a pull request. You can also simply open an issue with the tag "enhancement". Don't forget to give the project a star! Thanks again!&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Fork the Project&lt;/li&gt; 
 &lt;li&gt;Create your Feature Branch (&lt;code&gt;git checkout -b ruleCategory/DetectionRule&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Commit your Changes (&lt;code&gt;git commit -m 'Add some DetectionRules'&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Push to the Branch (&lt;code&gt;git push origin ruleCategory/DetectionRule&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Open a Pull Request&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p align="right"&gt;(&lt;a href="https://raw.githubusercontent.com/socfortress/Wazuh-Rules/main/#readme-top"&gt;back to top&lt;/a&gt;)&lt;/p&gt; 
&lt;!-- CONTACT --&gt; 
&lt;h2&gt;Contact&lt;/h2&gt; 
&lt;p&gt;SOCFortress - &lt;a href="https://www.socfortress.co/"&gt;&lt;img src="https://img.shields.io/badge/Visit%20Us-www.socfortress.co-orange" alt="LinkedIn" /&gt;&lt;/a&gt; - &lt;a href="mailto:info@socfortress.co"&gt;info@socfortress.co&lt;/a&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;h2 align="center"&gt;Let SOCFortress Take Your Open Source SIEM to the Next Level&lt;/h2&gt; 
 &lt;a href="https://www.socfortress.co/contact_form.html"&gt; &lt;img src="https://raw.githubusercontent.com/socfortress/Wazuh-Rules/main/images/Email%20Banner.png" alt="Banner" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;p align="right"&gt;(&lt;a href="https://raw.githubusercontent.com/socfortress/Wazuh-Rules/main/#readme-top"&gt;back to top&lt;/a&gt;)&lt;/p&gt; 
&lt;!-- ACKNOWLEDGMENTS --&gt; 
&lt;h2&gt;Acknowledgments&lt;/h2&gt; 
&lt;p&gt;Security is best when we work together! Huge thank you to those supporting and those future supporters!&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://documentation.wazuh.com/current/index.html"&gt;Wazuh Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/channel/UC4EUQtTxeC8wGrKRafI6pZg"&gt;Taylor Walton&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/juaromu"&gt;Juan Romero&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- MARKDOWN LINKS &amp; IMAGES --&gt; 
&lt;!-- https://www.markdownguide.org/basic-syntax/#reference-style-links --&gt;</description>
    </item>
    
    <item>
      <title>apache/airflow</title>
      <link>https://github.com/apache/airflow</link>
      <description>&lt;p&gt;Apache Airflow - A platform to programmatically author, schedule, and monitor workflows&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Apache Airflow&lt;/h1&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Badges&lt;/th&gt; 
   &lt;th&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;License&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.apache.org/licenses/LICENSE-2.0.txt"&gt;&lt;img src="https://img.shields.io/:license-Apache%202-blue.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;PyPI&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://badge.fury.io/py/apache-airflow"&gt;&lt;img src="https://badge.fury.io/py/apache-airflow.svg?sanitize=true" alt="PyPI version" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/apache-airflow/"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/apache-airflow.svg?sanitize=true" alt="PyPI - Python Version" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/apache-airflow/"&gt;&lt;img src="https://img.shields.io/pypi/dm/apache-airflow" alt="PyPI - Downloads" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Containers&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://hub.docker.com/r/apache/airflow"&gt;&lt;img src="https://img.shields.io/docker/pulls/apache/airflow.svg?sanitize=true" alt="Docker Pulls" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/apache/airflow"&gt;&lt;img src="https://img.shields.io/docker/stars/apache/airflow.svg?sanitize=true" alt="Docker Stars" /&gt;&lt;/a&gt; &lt;a href="https://artifacthub.io/packages/search?repo=apache-airflow"&gt;&lt;img src="https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/apache-airflow" alt="Artifact HUB" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Community&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/apache/airflow/graphs/contributors"&gt;&lt;img src="https://img.shields.io/github/contributors/apache/airflow" alt="Contributors" /&gt;&lt;/a&gt; &lt;a href="https://s.apache.org/airflow-slack"&gt;&lt;img src="https://img.shields.io/badge/slack-join_chat-white.svg?logo=slack&amp;amp;style=social" alt="Slack Status" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/github/commit-activity/m/apache/airflow" alt="Commit Activity" /&gt; &lt;a href="https://ossrank.com/p/6"&gt;&lt;img src="https://shields.io/endpoint?url=https://ossrank.com/shield/6" alt="OSSRank" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Version&lt;/th&gt; 
   &lt;th&gt;Build Status&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Main&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/apache/airflow/actions"&gt;&lt;img src="https://github.com/apache/airflow/actions/workflows/ci-amd.yml/badge.svg?sanitize=true" alt="GitHub Build main" /&gt;&lt;/a&gt; &lt;a href="https://github.com/apache/airflow/actions"&gt;&lt;img src="https://github.com/apache/airflow/actions/workflows/ci-arm.yml/badge.svg?sanitize=true" alt="GitHub Build main" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;3.x&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/apache/airflow/actions"&gt;&lt;img src="https://github.com/apache/airflow/actions/workflows/ci-amd.yml/badge.svg?branch=v3-0-test" alt="GitHub Build 3.0" /&gt;&lt;/a&gt; &lt;a href="https://github.com/apache/airflow/actions"&gt;&lt;img src="https://github.com/apache/airflow/actions/workflows/ci-arm.yml/badge.svg?branch=v3-0-test" alt="GitHub Build 3.0" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;2.x&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/apache/airflow/actions"&gt;&lt;img src="https://github.com/apache/airflow/actions/workflows/ci.yml/badge.svg?branch=v2-11-test" alt="GitHub Build 2.11" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;picture width="500"&gt; 
 &lt;img src="https://github.com/apache/airflow/raw/19ebcac2395ef9a6b6ded3a2faa29dc960c1e635/docs/apache-airflow/img/logos/wordmark_1.png?raw=true" alt="Apache Airflow logo" /&gt; 
&lt;/picture&gt; 
&lt;p&gt;&lt;a href="https://airflow.apache.org/docs/apache-airflow/stable/"&gt;Apache Airflow&lt;/a&gt; (or simply Airflow) is a platform to programmatically author, schedule, and monitor workflows.&lt;/p&gt; 
&lt;p&gt;When workflows are defined as code, they become more maintainable, versionable, testable, and collaborative.&lt;/p&gt; 
&lt;p&gt;Use Airflow to author workflows (Dags) that orchestrate tasks. The Airflow scheduler executes your tasks on an array of workers while following the specified dependencies. Rich command line utilities make performing complex surgeries on DAGs a snap. The rich user interface makes it easy to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed.&lt;/p&gt; 
&lt;!-- END Apache Airflow, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;!-- START doctoc generated TOC please keep comment here to allow auto update --&gt; 
&lt;!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE --&gt; 
&lt;p&gt;&lt;strong&gt;Table of contents&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#project-focus"&gt;Project Focus&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#principles"&gt;Principles&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#requirements"&gt;Requirements&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#getting-started"&gt;Getting started&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#installing-from-pypi"&gt;Installing from PyPI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#official-source-code"&gt;Official source code&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#convenience-packages"&gt;Convenience packages&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#user-interface"&gt;User Interface&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#semantic-versioning"&gt;Semantic versioning&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#version-life-cycle"&gt;Version Life Cycle&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#support-for-python-and-kubernetes-versions"&gt;Support for Python and Kubernetes versions&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#base-os-support-for-reference-airflow-images"&gt;Base OS support for reference Airflow images&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#approach-to-dependencies-of-airflow"&gt;Approach to dependencies of Airflow&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#voting-policy"&gt;Voting Policy&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#who-uses-apache-airflow"&gt;Who uses Apache Airflow?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#who-maintains-apache-airflow"&gt;Who maintains Apache Airflow?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#what-goes-into-the-next-release"&gt;What goes into the next release?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#can-i-use-the-apache-airflow-logo-in-my-presentation"&gt;Can I use the Apache Airflow logo in my presentation?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#links"&gt;Links&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#sponsors"&gt;Sponsors&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- END doctoc generated TOC please keep comment here to allow auto update --&gt; 
&lt;h2&gt;Project Focus&lt;/h2&gt; 
&lt;p&gt;Airflow works best with workflows that are mostly static and slowly changing. When the DAG structure is similar from one run to the next, it clarifies the unit of work and continuity. Other similar projects include &lt;a href="https://github.com/spotify/luigi"&gt;Luigi&lt;/a&gt;, &lt;a href="https://oozie.apache.org/"&gt;Oozie&lt;/a&gt; and &lt;a href="https://azkaban.github.io/"&gt;Azkaban&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Airflow is commonly used to process data, but has the opinion that tasks should ideally be idempotent (i.e., results of the task will be the same, and will not create duplicated data in a destination system), and should not pass large quantities of data from one task to the next (though tasks can pass metadata using Airflow's &lt;a href="https://airflow.apache.org/docs/apache-airflow/stable/concepts/xcoms.html"&gt;XCom feature&lt;/a&gt;). For high-volume, data-intensive tasks, a best practice is to delegate to external services specializing in that type of work.&lt;/p&gt; 
&lt;p&gt;Airflow is not a streaming solution, but it is often used to process real-time data, pulling data off streams in batches.&lt;/p&gt; 
&lt;h2&gt;Principles&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Dynamic&lt;/strong&gt;: Pipelines are defined in code, enabling dynamic dag generation and parameterization.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Extensible&lt;/strong&gt;: The Airflow framework includes a wide range of built-in operators and can be extended to fit your needs.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Flexible&lt;/strong&gt;: Airflow leverages the &lt;a href="https://jinja.palletsprojects.com"&gt;&lt;strong&gt;Jinja&lt;/strong&gt;&lt;/a&gt; templating engine, allowing rich customizations.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- START Requirements, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;p&gt;Apache Airflow is tested with:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th&gt;Main version (dev)&lt;/th&gt; 
   &lt;th&gt;Stable version (3.0.6)&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Python&lt;/td&gt; 
   &lt;td&gt;3.10, 3.11, 3.12, 3.13&lt;/td&gt; 
   &lt;td&gt;3.9, 3.10, 3.11, 3.12&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Platform&lt;/td&gt; 
   &lt;td&gt;AMD64/ARM64(*)&lt;/td&gt; 
   &lt;td&gt;AMD64/ARM64(*)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Kubernetes&lt;/td&gt; 
   &lt;td&gt;1.30, 1.31, 1.32, 1.33&lt;/td&gt; 
   &lt;td&gt;1.30, 1.31, 1.32, 1.33&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;PostgreSQL&lt;/td&gt; 
   &lt;td&gt;13, 14, 15, 16, 17&lt;/td&gt; 
   &lt;td&gt;13, 14, 15, 16, 17&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MySQL&lt;/td&gt; 
   &lt;td&gt;8.0, 8.4, Innovation&lt;/td&gt; 
   &lt;td&gt;8.0, 8.4, Innovation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;SQLite&lt;/td&gt; 
   &lt;td&gt;3.15.0+&lt;/td&gt; 
   &lt;td&gt;3.15.0+&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;* Experimental&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: MariaDB is not tested/recommended.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: SQLite is used in Airflow tests. Do not use it in production. We recommend using the latest stable version of SQLite for local development.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Airflow currently can be run on POSIX-compliant Operating Systems. For development, it is regularly tested on fairly modern Linux Distros and recent versions of macOS. On Windows you can run it via WSL2 (Windows Subsystem for Linux 2) or via Linux Containers. The work to add Windows support is tracked via &lt;a href="https://github.com/apache/airflow/issues/10388"&gt;#10388&lt;/a&gt;, but it is not a high priority. You should only use Linux-based distros as "Production" execution environment as this is the only environment that is supported. The only distro that is used in our CI tests and that is used in the &lt;a href="https://hub.docker.com/p/apache/airflow"&gt;Community managed DockerHub image&lt;/a&gt; is &lt;code&gt;Debian Bookworm&lt;/code&gt;.&lt;/p&gt; 
&lt;!-- END Requirements, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;!-- START Getting started, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;h2&gt;Getting started&lt;/h2&gt; 
&lt;p&gt;Visit the official Airflow website documentation (latest &lt;strong&gt;stable&lt;/strong&gt; release) for help with &lt;a href="https://airflow.apache.org/docs/apache-airflow/stable/installation/"&gt;installing Airflow&lt;/a&gt;, &lt;a href="https://airflow.apache.org/docs/apache-airflow/stable/start.html"&gt;getting started&lt;/a&gt;, or walking through a more complete &lt;a href="https://airflow.apache.org/docs/apache-airflow/stable/tutorial/"&gt;tutorial&lt;/a&gt;.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Note: If you're looking for documentation for the main branch (latest development branch): you can find it on &lt;a href="https://s.apache.org/airflow-docs/"&gt;s.apache.org/airflow-docs&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;For more information on Airflow Improvement Proposals (AIPs), visit the &lt;a href="https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals"&gt;Airflow Wiki&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Documentation for dependent projects like provider distributions, Docker image, Helm Chart, you'll find it in &lt;a href="https://airflow.apache.org/docs/"&gt;the documentation index&lt;/a&gt;.&lt;/p&gt; 
&lt;!-- END Getting started, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;!-- START Installing from PyPI, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;h2&gt;Installing from PyPI&lt;/h2&gt; 
&lt;p&gt;We publish Apache Airflow as &lt;code&gt;apache-airflow&lt;/code&gt; package in PyPI. Installing it however might be sometimes tricky because Airflow is a bit of both a library and application. Libraries usually keep their dependencies open, and applications usually pin them, but we should do neither and both simultaneously. We decided to keep our dependencies as open as possible (in &lt;code&gt;pyproject.toml&lt;/code&gt;) so users can install different versions of libraries if needed. This means that &lt;code&gt;pip install apache-airflow&lt;/code&gt; will not work from time to time or will produce unusable Airflow installation.&lt;/p&gt; 
&lt;p&gt;To have repeatable installation, however, we keep a set of "known-to-be-working" constraint files in the orphan &lt;code&gt;constraints-main&lt;/code&gt; and &lt;code&gt;constraints-2-0&lt;/code&gt; branches. We keep those "known-to-be-working" constraints files separately per major/minor Python version. You can use them as constraint files when installing Airflow from PyPI. Note that you have to specify correct Airflow tag/version/branch and Python versions in the URL.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Installing just Airflow:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Note: Only &lt;code&gt;pip&lt;/code&gt; installation is currently officially supported.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;While it is possible to install Airflow with tools like &lt;a href="https://python-poetry.org"&gt;Poetry&lt;/a&gt; or &lt;a href="https://pypi.org/project/pip-tools"&gt;pip-tools&lt;/a&gt;, they do not share the same workflow as &lt;code&gt;pip&lt;/code&gt; - especially when it comes to constraint vs. requirements management. Installing via &lt;code&gt;Poetry&lt;/code&gt; or &lt;code&gt;pip-tools&lt;/code&gt; is not currently supported.&lt;/p&gt; 
&lt;p&gt;There are known issues with &lt;code&gt;bazel&lt;/code&gt; that might lead to circular dependencies when using it to install Airflow. Please switch to &lt;code&gt;pip&lt;/code&gt; if you encounter such problems. &lt;code&gt;Bazel&lt;/code&gt; community works on fixing the problem in &lt;code&gt;this PR &amp;lt;https://github.com/bazelbuild/rules_python/pull/1166&amp;gt;&lt;/code&gt;_ so it might be that newer versions of &lt;code&gt;bazel&lt;/code&gt; will handle it.&lt;/p&gt; 
&lt;p&gt;If you wish to install Airflow using those tools, you should use the constraint files and convert them to the appropriate format and workflow that your tool requires.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install 'apache-airflow==3.0.6' \
 --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-3.0.6/constraints-3.10.txt"
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Installing with extras (i.e., postgres, google)&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install 'apache-airflow[postgres,google]==3.0.6' \
 --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-3.0.6/constraints-3.10.txt"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For information on installing provider distributions, check &lt;a href="http://airflow.apache.org/docs/apache-airflow-providers/index.html"&gt;providers&lt;/a&gt;.&lt;/p&gt; 
&lt;!-- END Installing from PyPI, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;For comprehensive instructions on setting up your local development environment and installing Apache Airflow, please refer to the &lt;a href="https://raw.githubusercontent.com/apache/airflow/main/INSTALLING.md"&gt;INSTALLING.md&lt;/a&gt; file.&lt;/p&gt; 
&lt;!-- START Official source code, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;h2&gt;Official source code&lt;/h2&gt; 
&lt;p&gt;Apache Airflow is an &lt;a href="https://www.apache.org"&gt;Apache Software Foundation&lt;/a&gt; (ASF) project, and our official source code releases:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Follow the &lt;a href="https://www.apache.org/legal/release-policy.html"&gt;ASF Release Policy&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Can be downloaded from &lt;a href="https://downloads.apache.org/airflow"&gt;the ASF Distribution Directory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Are cryptographically signed by the release manager&lt;/li&gt; 
 &lt;li&gt;Are officially voted on by the PMC members during the &lt;a href="https://www.apache.org/legal/release-policy.html#release-approval"&gt;Release Approval Process&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Following the ASF rules, the source packages released must be sufficient for a user to build and test the release provided they have access to the appropriate platform and tools.&lt;/p&gt; 
&lt;!-- END Official source code, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;h2&gt;Convenience packages&lt;/h2&gt; 
&lt;p&gt;There are other ways of installing and using Airflow. Those are "convenience" methods - they are not "official releases" as stated by the &lt;code&gt;ASF Release Policy&lt;/code&gt;, but they can be used by the users who do not want to build the software themselves.&lt;/p&gt; 
&lt;p&gt;Those are - in the order of most common ways people install Airflow:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://pypi.org/project/apache-airflow/"&gt;PyPI releases&lt;/a&gt; to install Airflow using standard &lt;code&gt;pip&lt;/code&gt; tool&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://hub.docker.com/r/apache/airflow"&gt;Docker Images&lt;/a&gt; to install airflow via &lt;code&gt;docker&lt;/code&gt; tool, use them in Kubernetes, Helm Charts, &lt;code&gt;docker-compose&lt;/code&gt;, &lt;code&gt;docker swarm&lt;/code&gt;, etc. You can read more about using, customizing, and extending the images in the &lt;a href="https://airflow.apache.org/docs/docker-stack/index.html"&gt;Latest docs&lt;/a&gt;, and learn details on the internals in the &lt;a href="https://airflow.apache.org/docs/docker-stack/index.html"&gt;images&lt;/a&gt; document.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/apache/airflow/tags"&gt;Tags in GitHub&lt;/a&gt; to retrieve the git project sources that were used to generate official source packages via git&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;All those artifacts are not official releases, but they are prepared using officially released sources. Some of those artifacts are "development" or "pre-release" ones, and they are clearly marked as such following the ASF Policy.&lt;/p&gt; 
&lt;h2&gt;User Interface&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;DAGs&lt;/strong&gt;: Overview of all DAGs in your environment.&lt;/p&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/ui-dark/dags.png" alt="DAGs" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Assets&lt;/strong&gt;: Overview of Assets with dependencies.&lt;/p&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/ui-dark/assets_graph.png" alt="Asset Dependencies" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Grid&lt;/strong&gt;: Grid representation of a DAG that spans across time.&lt;/p&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/ui-dark/grid.png" alt="Grid" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Graph&lt;/strong&gt;: Visualization of a DAG's dependencies and their current status for a specific run.&lt;/p&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/ui-dark/graph.png" alt="Graph" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Home&lt;/strong&gt;: Summary statistics of your Airflow environment.&lt;/p&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/ui-dark/home.png" alt="Home" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Backfill&lt;/strong&gt;: Backfilling a DAG for a specific date range.&lt;/p&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/ui-dark/backfill.png" alt="Backfill" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Code&lt;/strong&gt;: Quick way to view source code of a DAG.&lt;/p&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/ui-dark/code.png" alt="Code" /&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Semantic versioning&lt;/h2&gt; 
&lt;p&gt;As of Airflow 2.0.0, we support a strict &lt;a href="https://semver.org/"&gt;SemVer&lt;/a&gt; approach for all packages released.&lt;/p&gt; 
&lt;p&gt;There are few specific rules that we agreed to that define details of versioning of the different packages:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Airflow&lt;/strong&gt;: SemVer rules apply to core airflow only (excludes any changes to providers). Changing limits for versions of Airflow dependencies is not a breaking change on its own.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Airflow Providers&lt;/strong&gt;: SemVer rules apply to changes in the particular provider's code only. SemVer MAJOR and MINOR versions for the packages are independent of the Airflow version. For example, &lt;code&gt;google 4.1.0&lt;/code&gt; and &lt;code&gt;amazon 3.0.6&lt;/code&gt; providers can happily be installed with &lt;code&gt;Airflow 2.1.2&lt;/code&gt;. If there are limits of cross-dependencies between providers and Airflow packages, they are present in providers as &lt;code&gt;install_requires&lt;/code&gt; limitations. We aim to keep backwards compatibility of providers with all previously released Airflow 2 versions but there will sometimes be breaking changes that might make some, or all providers, have minimum Airflow version specified.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Airflow Helm Chart&lt;/strong&gt;: SemVer rules apply to changes in the chart only. SemVer MAJOR and MINOR versions for the chart are independent of the Airflow version. We aim to keep backwards compatibility of the Helm Chart with all released Airflow 2 versions, but some new features might only work starting from specific Airflow releases. We might however limit the Helm Chart to depend on minimal Airflow version.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Airflow API clients&lt;/strong&gt;: Their versioning is independent from Airflow versions. They follow their own SemVer rules for breaking changes and new features - which for example allows to change the way we generate the clients.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Version Life Cycle&lt;/h2&gt; 
&lt;p&gt;Apache Airflow version life cycle:&lt;/p&gt; 
&lt;!-- This table is automatically updated by prek scripts/ci/prek/supported_versions.py --&gt; 
&lt;!-- Beginning of auto-generated table --&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Version&lt;/th&gt; 
   &lt;th&gt;Current Patch/Minor&lt;/th&gt; 
   &lt;th&gt;State&lt;/th&gt; 
   &lt;th&gt;First Release&lt;/th&gt; 
   &lt;th&gt;Limited Maintenance&lt;/th&gt; 
   &lt;th&gt;EOL/Terminated&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;3&lt;/td&gt; 
   &lt;td&gt;3.0.6&lt;/td&gt; 
   &lt;td&gt;Supported&lt;/td&gt; 
   &lt;td&gt;Apr 22, 2025&lt;/td&gt; 
   &lt;td&gt;TBD&lt;/td&gt; 
   &lt;td&gt;TBD&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;2.11.0&lt;/td&gt; 
   &lt;td&gt;Supported&lt;/td&gt; 
   &lt;td&gt;Dec 17, 2020&lt;/td&gt; 
   &lt;td&gt;Oct 22, 2025&lt;/td&gt; 
   &lt;td&gt;Apr 22, 2026&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;1.10&lt;/td&gt; 
   &lt;td&gt;1.10.15&lt;/td&gt; 
   &lt;td&gt;EOL&lt;/td&gt; 
   &lt;td&gt;Aug 27, 2018&lt;/td&gt; 
   &lt;td&gt;Dec 17, 2020&lt;/td&gt; 
   &lt;td&gt;June 17, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;1.9&lt;/td&gt; 
   &lt;td&gt;1.9.0&lt;/td&gt; 
   &lt;td&gt;EOL&lt;/td&gt; 
   &lt;td&gt;Jan 03, 2018&lt;/td&gt; 
   &lt;td&gt;Aug 27, 2018&lt;/td&gt; 
   &lt;td&gt;Aug 27, 2018&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;1.8&lt;/td&gt; 
   &lt;td&gt;1.8.2&lt;/td&gt; 
   &lt;td&gt;EOL&lt;/td&gt; 
   &lt;td&gt;Mar 19, 2017&lt;/td&gt; 
   &lt;td&gt;Jan 03, 2018&lt;/td&gt; 
   &lt;td&gt;Jan 03, 2018&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;1.7&lt;/td&gt; 
   &lt;td&gt;1.7.1.2&lt;/td&gt; 
   &lt;td&gt;EOL&lt;/td&gt; 
   &lt;td&gt;Mar 28, 2016&lt;/td&gt; 
   &lt;td&gt;Mar 19, 2017&lt;/td&gt; 
   &lt;td&gt;Mar 19, 2017&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;!-- End of auto-generated table --&gt; 
&lt;p&gt;Limited support versions will be supported with security and critical bug fix only. EOL versions will not get any fixes nor support. We always recommend that all users run the latest available minor release for whatever major version is in use. We &lt;strong&gt;highly&lt;/strong&gt; recommend upgrading to the latest Airflow major release at the earliest convenient time and before the EOL date.&lt;/p&gt; 
&lt;h2&gt;Support for Python and Kubernetes versions&lt;/h2&gt; 
&lt;p&gt;As of Airflow 2.0, we agreed to certain rules we follow for Python and Kubernetes support. They are based on the official release schedule of Python and Kubernetes, nicely summarized in the &lt;a href="https://devguide.python.org/#status-of-python-branches"&gt;Python Developer's Guide&lt;/a&gt; and &lt;a href="https://kubernetes.io/docs/setup/release/version-skew-policy/"&gt;Kubernetes version skew policy&lt;/a&gt;.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;We drop support for Python and Kubernetes versions when they reach EOL. Except for Kubernetes, a version stays supported by Airflow if two major cloud providers still provide support for it. We drop support for those EOL versions in main right after EOL date, and it is effectively removed when we release the first new MINOR (Or MAJOR if there is no new MINOR version) of Airflow. For example, for Python 3.10 it means that we will drop support in main right after 27.06.2023, and the first MAJOR or MINOR version of Airflow released after will not have it.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;We support a new version of Python/Kubernetes in main after they are officially released, as soon as we make them work in our CI pipeline (which might not be immediate due to dependencies catching up with new versions of Python mostly) we release new images/support in Airflow based on the working CI setup.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;This policy is best-effort which means there may be situations where we might terminate support earlier if circumstances require it.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Base OS support for reference Airflow images&lt;/h2&gt; 
&lt;p&gt;The Airflow Community provides conveniently packaged container images that are published whenever we publish an Apache Airflow release. Those images contain:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Base OS with necessary packages to install Airflow (stable Debian OS)&lt;/li&gt; 
 &lt;li&gt;Base Python installation in versions supported at the time of release for the MINOR version of Airflow released (so there could be different versions for 2.3 and 2.2 line for example)&lt;/li&gt; 
 &lt;li&gt;Libraries required to connect to supported Databases (again the set of databases supported depends on the MINOR version of Airflow)&lt;/li&gt; 
 &lt;li&gt;Predefined set of popular providers (for details see the &lt;a href="https://raw.githubusercontent.com/apache/airflow/main/Dockerfile"&gt;Dockerfile&lt;/a&gt;).&lt;/li&gt; 
 &lt;li&gt;Possibility of building your own, custom image where the user can choose their own set of providers and libraries (see &lt;a href="https://airflow.apache.org/docs/docker-stack/build.html"&gt;Building the image&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;In the future Airflow might also support a "slim" version without providers nor database clients installed&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The version of the base OS image is the stable version of Debian. Airflow supports using all currently active stable versions - as soon as all Airflow dependencies support building, and we set up the CI pipeline for building and testing the OS version. Approximately 6 months before the end-of-regular support of a previous stable version of the OS, Airflow switches the images released to use the latest supported version of the OS.&lt;/p&gt; 
&lt;p&gt;For example switch from &lt;code&gt;Debian Bullseye&lt;/code&gt; to &lt;code&gt;Debian Bookworm&lt;/code&gt; has been implemented before 2.8.0 release in October 2023 and &lt;code&gt;Debian Bookworm&lt;/code&gt; will be the only option supported as of Airflow 2.10.0.&lt;/p&gt; 
&lt;p&gt;Users will continue to be able to build their images using stable Debian releases until the end of regular support and building and verifying of the images happens in our CI but no unit tests were executed using this image in the &lt;code&gt;main&lt;/code&gt; branch.&lt;/p&gt; 
&lt;h2&gt;Approach to dependencies of Airflow&lt;/h2&gt; 
&lt;p&gt;Airflow has a lot of dependencies - direct and transitive, also Airflow is both - library and application, therefore our policies to dependencies has to include both - stability of installation of application, but also ability to install newer version of dependencies for those users who develop DAGs. We developed the approach where &lt;code&gt;constraints&lt;/code&gt; are used to make sure airflow can be installed in a repeatable way, while we do not limit our users to upgrade most of the dependencies. As a result we decided not to upper-bound version of Airflow dependencies by default, unless we have good reasons to believe upper-bounding them is needed because of importance of the dependency as well as risk it involves to upgrade specific dependency. We also upper-bound the dependencies that we know cause problems.&lt;/p&gt; 
&lt;p&gt;The constraint mechanism of ours takes care about finding and upgrading all the non-upper bound dependencies automatically (providing that all the tests pass). Our &lt;code&gt;main&lt;/code&gt; build failures will indicate in case there are versions of dependencies that break our tests - indicating that we should either upper-bind them or that we should fix our code/tests to account for the upstream changes from those dependencies.&lt;/p&gt; 
&lt;p&gt;Whenever we upper-bound such a dependency, we should always comment why we are doing it - i.e. we should have a good reason why dependency is upper-bound. And we should also mention what is the condition to remove the binding.&lt;/p&gt; 
&lt;h3&gt;Approach for dependencies for Airflow Core&lt;/h3&gt; 
&lt;p&gt;Those dependencies are maintained in &lt;code&gt;pyproject.toml&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;There are few dependencies that we decided are important enough to upper-bound them by default, as they are known to follow predictable versioning scheme, and we know that new versions of those are very likely to bring breaking changes. We commit to regularly review and attempt to upgrade to the newer versions of the dependencies as they are released, but this is manual process.&lt;/p&gt; 
&lt;p&gt;The important dependencies are:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;SQLAlchemy&lt;/code&gt;: upper-bound to specific MINOR version (SQLAlchemy is known to remove deprecations and introduce breaking changes especially that support for different Databases varies and changes at various speed)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Alembic&lt;/code&gt;: it is important to handle our migrations in predictable and performant way. It is developed together with SQLAlchemy. Our experience with Alembic is that it very stable in MINOR version&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Flask&lt;/code&gt;: We are using Flask as the back-bone of our web UI and API. We know major version of Flask are very likely to introduce breaking changes across those so limiting it to MAJOR version makes sense&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;werkzeug&lt;/code&gt;: the library is known to cause problems in new versions. It is tightly coupled with Flask libraries, and we should update them together&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;celery&lt;/code&gt;: Celery is a crucial component of Airflow as it used for CeleryExecutor (and similar). Celery &lt;a href="https://docs.celeryq.dev/en/stable/contributing.html?highlight=semver#versions"&gt;follows SemVer&lt;/a&gt;, so we should upper-bound it to the next MAJOR version. Also, when we bump the upper version of the library, we should make sure Celery Provider minimum Airflow version is updated.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;kubernetes&lt;/code&gt;: Kubernetes is a crucial component of Airflow as it is used for the KubernetesExecutor (and similar). Kubernetes Python library &lt;a href="https://github.com/kubernetes-client/python#compatibility"&gt;follows SemVer&lt;/a&gt;, so we should upper-bound it to the next MAJOR version. Also, when we bump the upper version of the library, we should make sure Kubernetes Provider minimum Airflow version is updated.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Approach for dependencies in Airflow Providers and extras&lt;/h3&gt; 
&lt;p&gt;The main part of the Airflow is the Airflow Core, but the power of Airflow also comes from a number of providers that extend the core functionality and are released separately, even if we keep them (for now) in the same monorepo for convenience. You can read more about the providers in the &lt;a href="https://airflow.apache.org/docs/apache-airflow-providers/index.html"&gt;Providers documentation&lt;/a&gt;. We also have set of policies implemented for maintaining and releasing community-managed providers as well as the approach for community vs. 3rd party providers in the &lt;a href="https://github.com/apache/airflow/raw/main/PROVIDERS.rst"&gt;providers&lt;/a&gt; document.&lt;/p&gt; 
&lt;p&gt;Those &lt;code&gt;extras&lt;/code&gt; and &lt;code&gt;providers&lt;/code&gt; dependencies are maintained in &lt;code&gt;provider.yaml&lt;/code&gt; of each provider.&lt;/p&gt; 
&lt;p&gt;By default, we should not upper-bound dependencies for providers, however each provider's maintainer might decide to add additional limits (and justify them with comment).&lt;/p&gt; 
&lt;!-- START Contributing, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Want to help build Apache Airflow? Check out our &lt;a href="https://github.com/apache/airflow/raw/main/contributing-docs/README.rst"&gt;contributors' guide&lt;/a&gt; for a comprehensive overview of how to contribute, including setup instructions, coding standards, and pull request guidelines.&lt;/p&gt; 
&lt;p&gt;If you can't wait to contribute, and want to get started asap, check out the &lt;a href="https://github.com/apache/airflow/raw/main/contributing-docs/03_contributors_quick_start.rst"&gt;contribution quickstart&lt;/a&gt; here!&lt;/p&gt; 
&lt;p&gt;Official Docker (container) images for Apache Airflow are described in &lt;a href="https://github.com/apache/airflow/raw/main/dev/breeze/doc/ci/02_images.md"&gt;images&lt;/a&gt;.&lt;/p&gt; 
&lt;!-- END Contributing, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;!-- START Who uses Apache Airflow, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;h2&gt;Voting Policy&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Commits need a +1 vote from a committer who is not the author&lt;/li&gt; 
 &lt;li&gt;When we do AIP voting, both PMC member's and committer's &lt;code&gt;+1s&lt;/code&gt; are considered a binding vote.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Who uses Apache Airflow?&lt;/h2&gt; 
&lt;p&gt;We know about around 500 organizations that are using Apache Airflow (but there are likely many more) &lt;a href="https://github.com/apache/airflow/raw/main/INTHEWILD.md"&gt;in the wild&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you use Airflow - feel free to make a PR to add your organisation to the list.&lt;/p&gt; 
&lt;!-- END Who uses Apache Airflow, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;!-- START Who maintains Apache Airflow, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;h2&gt;Who maintains Apache Airflow?&lt;/h2&gt; 
&lt;p&gt;Airflow is the work of the &lt;a href="https://github.com/apache/airflow/graphs/contributors"&gt;community&lt;/a&gt;, but the &lt;a href="https://people.apache.org/committers-by-project.html#airflow"&gt;core committers/maintainers&lt;/a&gt; are responsible for reviewing and merging PRs as well as steering conversations around new feature requests. If you would like to become a maintainer, please review the Apache Airflow &lt;a href="https://github.com/apache/airflow/raw/main/COMMITTERS.rst#guidelines-to-become-an-airflow-committer"&gt;committer requirements&lt;/a&gt;.&lt;/p&gt; 
&lt;!-- END Who maintains Apache Airflow, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;h2&gt;What goes into the next release?&lt;/h2&gt; 
&lt;p&gt;Often you will see an issue that is assigned to specific milestone with Airflow version, or a PR that gets merged to the main branch and you might wonder which release the merged PR(s) will be released in or which release the fixed issues will be in. The answer to this is as usual - it depends on various scenarios. The answer is different for PRs and Issues.&lt;/p&gt; 
&lt;p&gt;To add a bit of context, we are following the &lt;a href="https://semver.org/"&gt;Semver&lt;/a&gt; versioning scheme as described in &lt;a href="https://airflow.apache.org/docs/apache-airflow/stable/release-process.html"&gt;Airflow release process&lt;/a&gt;. More details are explained in detail in this README under the &lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#semantic-versioning"&gt;Semantic versioning&lt;/a&gt; chapter, but in short, we have &lt;code&gt;MAJOR.MINOR.PATCH&lt;/code&gt; versions of Airflow.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;MAJOR&lt;/code&gt; version is incremented in case of breaking changes&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;MINOR&lt;/code&gt; version is incremented when there are new features added&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;PATCH&lt;/code&gt; version is incremented when there are only bug-fixes and doc-only changes&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Generally we release &lt;code&gt;MINOR&lt;/code&gt; versions of Airflow from a branch that is named after the MINOR version. For example &lt;code&gt;2.7.*&lt;/code&gt; releases are released from &lt;code&gt;v2-7-stable&lt;/code&gt; branch, &lt;code&gt;2.8.*&lt;/code&gt; releases are released from &lt;code&gt;v2-8-stable&lt;/code&gt; branch, etc.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Most of the time in our release cycle, when the branch for next &lt;code&gt;MINOR&lt;/code&gt; branch is not yet created, all PRs merged to &lt;code&gt;main&lt;/code&gt; (unless they get reverted), will find their way to the next &lt;code&gt;MINOR&lt;/code&gt; release. For example if the last release is &lt;code&gt;2.7.3&lt;/code&gt; and &lt;code&gt;v2-8-stable&lt;/code&gt; branch is not created yet, the next &lt;code&gt;MINOR&lt;/code&gt; release is &lt;code&gt;2.8.0&lt;/code&gt; and all PRs merged to main will be released in &lt;code&gt;2.8.0&lt;/code&gt;. However, some PRs (bug-fixes and doc-only changes) when merged, can be cherry-picked to current &lt;code&gt;MINOR&lt;/code&gt; branch and released in the next &lt;code&gt;PATCHLEVEL&lt;/code&gt; release. For example, if &lt;code&gt;2.8.1&lt;/code&gt; is already released and we are working on &lt;code&gt;2.9.0dev&lt;/code&gt;, then marking a PR with &lt;code&gt;2.8.2&lt;/code&gt; milestone means that it will be cherry-picked to &lt;code&gt;v2-8-test&lt;/code&gt; branch and released in &lt;code&gt;2.8.2rc1&lt;/code&gt;, and eventually in &lt;code&gt;2.8.2&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;When we prepare for the next &lt;code&gt;MINOR&lt;/code&gt; release, we cut new &lt;code&gt;v2-*-test&lt;/code&gt; and &lt;code&gt;v2-*-stable&lt;/code&gt; branch and prepare &lt;code&gt;alpha&lt;/code&gt;, &lt;code&gt;beta&lt;/code&gt; releases for the next &lt;code&gt;MINOR&lt;/code&gt; version, the PRs merged to main will still be released in the next &lt;code&gt;MINOR&lt;/code&gt; release until &lt;code&gt;rc&lt;/code&gt; version is cut. This is happening because the &lt;code&gt;v2-*-test&lt;/code&gt; and &lt;code&gt;v2-*-stable&lt;/code&gt; branches are rebased on top of main when next &lt;code&gt;beta&lt;/code&gt; and &lt;code&gt;rc&lt;/code&gt; releases are prepared. For example, when we cut &lt;code&gt;2.10.0beta1&lt;/code&gt; version, anything merged to main before &lt;code&gt;2.10.0rc1&lt;/code&gt; is released, will find its way to 2.10.0rc1.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Then, once we prepare the first RC candidate for the MINOR release, we stop moving the &lt;code&gt;v2-*-test&lt;/code&gt; and &lt;code&gt;v2-*-stable&lt;/code&gt; branches and the PRs merged to main will be released in the next &lt;code&gt;MINOR&lt;/code&gt; release. However, some PRs (bug-fixes and doc-only changes) when merged, can be cherry-picked to current &lt;code&gt;MINOR&lt;/code&gt; branch and released in the next &lt;code&gt;PATCHLEVEL&lt;/code&gt; release - for example when the last released version from &lt;code&gt;v2-10-stable&lt;/code&gt; branch is &lt;code&gt;2.10.0rc1&lt;/code&gt;, some of the PRs from main can be marked as &lt;code&gt;2.10.0&lt;/code&gt; milestone by committers, the release manager will try to cherry-pick them into the release branch. If successful, they will be released in &lt;code&gt;2.10.0rc2&lt;/code&gt; and subsequently in &lt;code&gt;2.10.0&lt;/code&gt;. This also applies to subsequent &lt;code&gt;PATCHLEVEL&lt;/code&gt; versions. When for example &lt;code&gt;2.10.1&lt;/code&gt; is already released, marking a PR with &lt;code&gt;2.10.2&lt;/code&gt; milestone will mean that it will be cherry-picked to &lt;code&gt;v2-10-stable&lt;/code&gt; branch and released in &lt;code&gt;2.10.2rc1&lt;/code&gt; and eventually in &lt;code&gt;2.10.2&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;The final decision about cherry-picking is made by the release manager.&lt;/p&gt; 
&lt;p&gt;Marking issues with a milestone is a bit different. Maintainers do not mark issues with a milestone usually, normally they are only marked in PRs. If PR linked to the issue (and "fixing it") gets merged and released in a specific version following the process described above, the issue will be automatically closed, no milestone will be set for the issue, you need to check the PR that fixed the issue to see which version it was released in.&lt;/p&gt; 
&lt;p&gt;However, sometimes maintainers mark issues with specific milestone, which means that the issue is important to become a candidate to take a look when the release is being prepared. Since this is an Open-Source project, where basically all contributors volunteer their time, there is no guarantee that specific issue will be fixed in specific version. We do not want to hold the release because some issue is not fixed, so in such case release manager will reassign such unfixed issues to the next milestone in case they are not fixed in time for the current release. Therefore, the milestone for issue is more of an intent that it should be looked at, than promise it will be fixed in the version.&lt;/p&gt; 
&lt;p&gt;More context and &lt;strong&gt;FAQ&lt;/strong&gt; about the patchlevel release can be found in the &lt;a href="https://raw.githubusercontent.com/apache/airflow/main/dev/WHAT_GOES_INTO_THE_NEXT_RELEASE.md"&gt;What goes into the next release&lt;/a&gt; document in the &lt;code&gt;dev&lt;/code&gt; folder of the repository.&lt;/p&gt; 
&lt;h2&gt;Can I use the Apache Airflow logo in my presentation?&lt;/h2&gt; 
&lt;p&gt;Yes! Be sure to abide by the Apache Foundation &lt;a href="https://www.apache.org/foundation/marks/#books"&gt;trademark policies&lt;/a&gt; and the Apache Airflow &lt;a href="https://cwiki.apache.org/confluence/display/AIRFLOW/Brandbook"&gt;Brandbook&lt;/a&gt;. The most up-to-date logos are found in &lt;a href="https://github.com/apache/airflow/tree/main/airflow-core/docs/img/logos/"&gt;this repo&lt;/a&gt; and on the Apache Software Foundation &lt;a href="https://www.apache.org/logos/about.html"&gt;website&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Links&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://airflow.apache.org/docs/apache-airflow/stable/"&gt;Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://s.apache.org/airflow-slack"&gt;Chat&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://airflow.apache.org/community/"&gt;Community Information&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Sponsors&lt;/h2&gt; 
&lt;p&gt;The CI infrastructure for Apache Airflow has been sponsored by:&lt;/p&gt; 
&lt;!-- Ordered by most recently "funded" --&gt; 
&lt;p&gt;&lt;a href="https://astronomer.io"&gt;&lt;img src="https://assets2.astronomer.io/logos/logoForLIGHTbackground.png" alt="astronomer.io" width="250px" /&gt;&lt;/a&gt; &lt;a href="https://aws.amazon.com/opensource/"&gt;&lt;img src="https://github.com/apache/airflow/raw/main/providers/amazon/docs/integration-logos/AWS-Cloud-alt_light-bg@4x.png?raw=true" alt="AWS OpenSource" width="130px" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>philz1337x/clarity-upscaler</title>
      <link>https://github.com/philz1337x/clarity-upscaler</link>
      <description>&lt;p&gt;Clarity AI | AI Image Upscaler &amp; Enhancer - free and open-source Magnific Alternative&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h1&gt; Clarity AI | AI Image Upscaler &amp;amp; Enhancer - free and open-source Magnific Alternative &lt;/h1&gt; 
 &lt;p&gt;&lt;a href="https://ClarityAI.co"&gt;&lt;img src="https://img.shields.io/badge/App-ClarityAI.co-blueviolet" alt="App" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://ClarityAI.co/api"&gt;&lt;img src="https://img.shields.io/badge/API-ClarityAI.co/api-green" alt="API" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://twitter.com/philz1337x"&gt;&lt;img src="https://img.shields.io/twitter/follow/philz1337x?style=social" alt="Twitter Follow" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/github/stars/philz1337x/clarity-upscaler?style=social&amp;amp;label=Star" alt="GitHub stars" /&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/philz1337x/clarity-upscaler/main/example.gif" alt="Example video" /&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://x.com/philz1337x/status/1768679154726359128?s=20"&gt;Full Video on X/Twitter&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h1&gt;üëã Hello&lt;/h1&gt; 
&lt;p&gt;I build open source AI apps. To finance my work i also build paid versions of my code. But feel free to use the free code. I post features and new projects on &lt;a href="https://twitter.com/philz1337x"&gt;https://twitter.com/philz1337x&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;üóûÔ∏è Updates&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;03/05/2025: Flux upscaling only via my app and api: (&lt;a href="http://clarityai.co/flux-upscaler"&gt;http://clarityai.co/flux-upscaler&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;06/19/2024: Pattern upscaling&lt;/li&gt; 
 &lt;li&gt;05/24/2024: Increased Resolution to 13kx13k (&lt;a href="https://x.com/philz1337x/status/1793983581636690379"&gt;https://x.com/philz1337x/status/1793983581636690379&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;05/16/2024: Output file format: jpg/png/webp (&lt;a href="https://x.com/philz1337x/status/1791431093641457824"&gt;https://x.com/philz1337x/status/1791431093641457824&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;05/02/2024: Sharpen image&lt;/li&gt; 
 &lt;li&gt;05/07/2024: ComfyUI node (&lt;a href="https://x.com/philz1337x/status/1787905308439826920"&gt;https://x.com/philz1337x/status/1787905308439826920&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;04/12/2024: Multi-step upscaling (&lt;a href="https://x.com/philz1337x/status/1785269458304442565"&gt;https://x.com/philz1337x/status/1785269458304442565&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;04/07/2024: Resemblance fixed (&lt;a href="https://x.com/levelsio/status/1776729356120797265"&gt;https://x.com/levelsio/status/1776729356120797265&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;04/05/2024: Speed Improvements (&lt;a href="https://x.com/philz1337x/status/1776121175195975888"&gt;https://x.com/philz1337x/status/1776121175195975888&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;04/01/2024: Support custom safetensors checkpoints (&lt;a href="https://x.com/philz1337x/status/1774772572632338435"&gt;https://x.com/philz1337x/status/1774772572632338435&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;03/28/2024: Anime upscaling (&lt;a href="https://x.com/philz1337x/status/1773342568543346738"&gt;https://x.com/philz1337x/status/1773342568543346738&lt;/a&gt;) (&lt;a href="https://clarityai.co/anime-image-upscaling"&gt;https://clarityai.co/anime-image-upscaling&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;03/26/2024: LoRa Support (&lt;a href="https://x.com/philz1337x/status/1772575319871959180"&gt;https://x.com/philz1337x/status/1772575319871959180&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;03/21/2024: Pre downscaling (&lt;a href="https://x.com/philz1337x/status/1770680096031961351"&gt;https://x.com/philz1337x/status/1770680096031961351&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;03/18/2024: Fractality (&lt;a href="https://x.com/philz1337x/status/1769756654533485050"&gt;https://x.com/philz1337x/status/1769756654533485050&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;03/15/2024: Code release (&lt;a href="https://x.com/philz1337x/status/1768679154726359128"&gt;https://x.com/philz1337x/status/1768679154726359128&lt;/a&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;üîé Upscaling with Flux&lt;/h1&gt; 
&lt;p&gt;Flux Upscaling is now available at &lt;a href="http://clarityai.co/flux-upscaler"&gt;ClarityAI.co/flux-upscaler&lt;/a&gt; and is not open-source&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;It supports Flux LoRas with a style or a face&lt;/li&gt; 
 &lt;li&gt;It's very good at faces, text, art, and generating error-free images&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;üöÄ Options to use Clarity-Upscaler&lt;/h1&gt; 
&lt;p&gt;Note that this repository is an implementation for cog. If you are not familiar with cog, I recommend the easier solutions. The free options are ComfyUI and A1111, while the paid but easy-to-use options are my app ClarityAI.co and the ComfyUI API Node.&lt;/p&gt; 
&lt;h2&gt;üßë‚Äçüíª App&lt;/h2&gt; 
&lt;p&gt;The simplest option to use Clarity is with the app at &lt;a href="https://ClarityAI.co"&gt;ClarityAI.co&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;‚öôÔ∏è API&lt;/h2&gt; 
&lt;p&gt;To integrate Clarity Upscaler with an API into your application use: &lt;a href="https://ClarityAI.co/api"&gt;ClarityAI.co/API&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;üê∞ ComfyUI&lt;/h2&gt; 
&lt;h3&gt;1. API node&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Open ComfyUI Manager, search for Clarity AI, and install the node.&lt;/li&gt; 
 &lt;li&gt;Create an API key at: &lt;a href="https://ClarityAI.co/comfyui"&gt;ClarityAI.co/ComfyUI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Add the API key to the node as a) envirement variable &lt;code&gt;CAI_API_KEY&lt;/code&gt; OR b) to a &lt;code&gt;cai_platform_key.txt&lt;/code&gt; text file OR c) in &lt;code&gt;api_key_override&lt;/code&gt; field of the node.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Full instructions: &lt;a href="https://github.com/philz1337x/ComfyUI-ClarityAI"&gt;https://github.com/philz1337x/ComfyUI-ClarityAI&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;2. Free workflow&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Download the repo &lt;a href="https://github.com/philz1337x/ComfyUI-ClarityAI"&gt;https://github.com/philz1337x/ComfyUI-ClarityAI&lt;/a&gt; and use the file free-wokflow.json&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;üïµÔ∏è‚Äç‚ôÇÔ∏è Cog&lt;/h2&gt; 
&lt;p&gt;If you are not familiar with cog read: &lt;a href="https://github.com/replicate/cog/raw/main/docs/getting-started-own-model.md"&gt;cog docs&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;run &lt;code&gt;download_weights.py&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;predict with cog:&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-su"&gt;cog predict -i image="link-to-image"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ü§π‚Äç‚ôÇÔ∏è A1111 webUI&lt;/h2&gt; 
&lt;p&gt;For a detailed explanation, use the tutorial in this post: &lt;a href="https://x.com/philz1337x/status/1830504764389380466"&gt;https://x.com/philz1337x/status/1830504764389380466&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui"&gt;https://github.com/AUTOMATIC1111/stable-diffusion-webui&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Use these params:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-Prompt:"&gt;masterpiece, best quality, highres, &amp;lt;lora:more_details:0.5&amp;gt; &amp;lt;lora:SDXLrender_v2.0:1&amp;gt; Negative prompt: (worst quality, low quality, normal quality:2) JuggernautNegative-neg Steps: 18, Sampler: DPM++ 3M SDE Karras, CFG scale: 6.0, Seed: 1337, Size: 1024x1024, Model hash: 338b85bc4f, Model: juggernaut_reborn, Denoising strength: 0.35, Tiled Diffusion upscaler: 4x-UltraSharp, Tiled Diffusion scale factor: 2, Tiled Diffusion: {"Method": "MultiDiffusion", "Tile tile width": 112, "Tile tile height": 144, "Tile Overlap": 4, "Tile batch size": 8, "Upscaler": "4x-UltraSharp", "Upscale factor": 2, "Keep input size": true}, ControlNet 0: "Module: tile_resample, Model: control_v11f1e_sd15_tile, Weight: 0.6, Resize Mode: 1, Low Vram: False, Processor Res: 512, Threshold A: 1, Threshold B: 1, Guidance Start: 0.0, Guidance End: 1.0, Pixel Perfect: True, Control Mode: 1, Hr Option: HiResFixOption.BOTH, Save Detected Map: False", Lora hashes: "more_details: 3b8aa1d351ef, SDXLrender_v2.0: 3925cf4759af"
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>weaviate/elysia</title>
      <link>https://github.com/weaviate/elysia</link>
      <description>&lt;p&gt;Python package and backend for the Elysia platform app.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Elysia: Agentic Framework Powered by Decision Trees&lt;/h1&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;‚ö†Ô∏è Elysia is in beta!&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;If you encounter any issues, please &lt;a href="https://github.com/weaviate/elysia/issues"&gt;open an issue on GitHub&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;a href="https://pepy.tech/projects/elysia-ai"&gt;&lt;img src="https://static.pepy.tech/badge/elysia-ai" alt="PyPI Downloads" /&gt;&lt;/a&gt; &lt;a href="https://elysia.weaviate.io/"&gt;&lt;img src="https://img.shields.io/badge/Check%20out%20the%20demo!-yellow?&amp;amp;style=flat-square&amp;amp;logo=react&amp;amp;logoColor=white" alt="Demo" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Elysia is an agentic platform designed to use tools in a decision tree. A decision agent decides which tools to use dynamically based on its environment and context. You can use custom tools or use the pre-built tools designed to retrieve your data in a Weaviate cluster.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://weaviate.github.io/elysia/"&gt;Read the docs!&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Installation is as simple as:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install elysia-ai
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Get started (App)&lt;/h2&gt; 
&lt;p&gt;Run the app via&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;elysia start
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then navigate to the settings page, add your required API keys, Weaviate cloud cluster details and specify your models.&lt;/p&gt; 
&lt;p&gt;Don't forget to check out &lt;a href="https://github.com/weaviate/elysia-frontend"&gt;the Github Repository for the Frontend&lt;/a&gt;!&lt;/p&gt; 
&lt;p&gt;Alternatively, we have created a demo version of Elysia (rate-limited, fixed datasets) to experiment with. Find it at: &lt;a href="https://elysia.weaviate.io/"&gt;https://elysia.weaviate.io/&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Get Started (Python)&lt;/h2&gt; 
&lt;p&gt;To use Elysia, you need to either set up your models and API keys in your &lt;code&gt;.env&lt;/code&gt; file, or specify them in the config. &lt;a href="https://weaviate.github.io/elysia/setting_up/"&gt;See the setup page to get started.&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Elysia can be used very simply:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from elysia import tool, Tree

tree = Tree()

@tool(tree=tree)
async def add(x: int, y: int) -&amp;gt; int:
    return x + y

tree("What is the sum of 9009 and 6006?")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Elysia is pre-configured to be capable of connecting to and interacting with your &lt;a href="https://weaviate.io/deployment/serverless"&gt;Weaviate&lt;/a&gt; clusters!&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import elysia
tree = elysia.Tree()
response, objects = tree(
    "What are the 10 most expensive items in the Ecommerce collection?",
    collection_names = ["Ecommerce"]
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will use the built-in open source &lt;em&gt;query&lt;/em&gt; tool or &lt;em&gt;aggregate&lt;/em&gt; tool to interact with your Weaviate collections. To get started connecting to Weaviate, &lt;a href="https://weaviate.github.io/elysia/setting_up/"&gt;see the setting up page in the docs&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Installation (bash) (Linux/MacOS)&lt;/h2&gt; 
&lt;h3&gt;PyPi (Recommended)&lt;/h3&gt; 
&lt;p&gt;Elysia requires Python 3.12:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://formulae.brew.sh/formula/python@3.12"&gt;Installation via brew (macOS)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.python.org/downloads/release/python-3120/"&gt;Installation via installer (Windows)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ubuntuhandbook.org/index.php/2023/05/install-python-3-12-ubuntu/"&gt;Installation (Ubuntu)&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Optionally create a virtual environment via&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python3.12 -m venv .venv
source .venv/bin/activate
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then simply run&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install elysia-ai
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;to install straight away!&lt;/p&gt; 
&lt;h3&gt;GitHub&lt;/h3&gt; 
&lt;p&gt;To get the latest development version, you can clone the github repo by running&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/weaviate/elysia
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;move to the working directory via&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd elysia
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Create a virtual environment with Python (version 3.10 - 3.12)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python3.12 -m venv .venv
source .venv/bin/activate
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;and then install Elysia via pip&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Done! You can now use the Elysia python package&lt;/p&gt; 
&lt;h3&gt;Configuring Settings&lt;/h3&gt; 
&lt;p&gt;To use Elysia with Weaviate, i.e. for agentic searching and retrieval, you need to either have a &lt;em&gt;locally running&lt;/em&gt; instance of Weaviate, or access to a &lt;em&gt;Weaviate cloud cluster&lt;/em&gt; via an api key and URL. This can be specific in the app directly, or by creating a &lt;code&gt;.env&lt;/code&gt; file with&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;WCD_URL=...
WCD_API_KEY=...
WEAVIATE_IS_LOCAL=... # True or False
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Elysia will automatically detect these when running locally, and this will be the default Weaviate cluster for all users logging into the Elysia app. But these can be configured on a user-by-user basis through the config.&lt;/p&gt; 
&lt;p&gt;Whichever vectoriser you use for your Weaviate collection you will need to specify your corresponding API key, e.g.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;OPENAI_API_KEY=...
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;These will automatically be added to the headers for the Weaviate client.&lt;/p&gt; 
&lt;p&gt;Same for whichever model you choose for the LLM in Elysia, so if you are using GPT-4o, for example, specify an &lt;code&gt;OPENAI_API_KEY&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Elysia's recommended config is to use &lt;a href="https://openrouter.ai/"&gt;OpenRouter&lt;/a&gt; to give easy access to a variety of models. So this requires&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;OPENROUTER_API_KEY=...
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;FAQ&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;How do I use Elysia with my own data?&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;You can connect to your own Weaviate cloud cluster, which will automatically identify any collections that exist in the cluster.&lt;/p&gt; 
 &lt;p&gt;Collections require being &lt;em&gt;preprocessed&lt;/em&gt; for Elysia. In the app, you just click the 'analyze' button in the Data tab. In Python you can do:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from elysia.preprocessing.collection import preprocess

preprocess(collection_names=["YourCollectionName"])
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Can I run Elysia completely locally? (Locally running Weaviate, local models)&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;Yes!&lt;/p&gt; 
 &lt;p&gt;You can connect to a locally running Weaviate instance in Docker, and connect to Ollama for locally running language models. &lt;a href="https://weaviate.github.io/elysia/setting_up/"&gt;See the setup page to get started.&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;How do I clear all my Elysia data?&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;Everything Elysia doesn't store locally will be a collection in your Weaviate cluster. You can delete any collections that start with &lt;code&gt;ELYSIA_&lt;/code&gt; to reset all your Elysia data.&lt;/p&gt; 
 &lt;p&gt;For example, in Python:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from elysia.util.client import ClientManager()
with ClientManager().connect_to_client() as client:
    for collection_name in client.collections.list_all():
        if collection_name.startswith("ELYSIA_"):
            client.collections.delete(collection_name)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Can I contribute to Elysia?&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;Elysia is &lt;strong&gt;fully open source&lt;/strong&gt;, so yes of course you can! Clone and create a new branch of Elysia via&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/weaviate/elysia
git checkout -b &amp;lt;branch_name&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Make your changes, push them to your branch, go to GitHub and submit a pull request.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Where is the best place I can start contributing?&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;There are no 'huge' new features we are planning for Elysia (for the moment). You could start with creating a new tool, or multiple new tools to create a custom workflow for something specific. Look for pain points you experience from your user journey and find what exactly is causing these. Then try to fix them or create an alternative way of doing things!&lt;/p&gt; 
&lt;/details&gt;</description>
    </item>
    
    <item>
      <title>numpy/numpy</title>
      <link>https://github.com/numpy/numpy</link>
      <description>&lt;p&gt;The fundamental package for scientific computing with Python.&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt; &lt;img src="https://raw.githubusercontent.com/numpy/numpy/main/branding/logo/primary/numpylogo.svg?sanitize=true" width="300" /&gt; &lt;/h1&gt;
&lt;br /&gt; 
&lt;p&gt;&lt;a href="https://numfocus.org"&gt;&lt;img src="https://img.shields.io/badge/powered%20by-NumFOCUS-orange.svg?style=flat&amp;amp;colorA=E1523D&amp;amp;colorB=007D8A" alt="Powered by NumFOCUS" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/numpy/"&gt;&lt;img src="https://img.shields.io/pypi/dm/numpy.svg?label=PyPI%20downloads" alt="PyPI Downloads" /&gt;&lt;/a&gt; &lt;a href="https://anaconda.org/conda-forge/numpy"&gt;&lt;img src="https://img.shields.io/conda/dn/conda-forge/numpy.svg?label=Conda%20downloads" alt="Conda Downloads" /&gt;&lt;/a&gt; &lt;a href="https://stackoverflow.com/questions/tagged/numpy"&gt;&lt;img src="https://img.shields.io/badge/stackoverflow-Ask%20questions-blue.svg?sanitize=true" alt="Stack Overflow" /&gt;&lt;/a&gt; &lt;a href="https://doi.org/10.1038/s41586-020-2649-2"&gt;&lt;img src="https://img.shields.io/badge/DOI-10.1038%2Fs41586--020--2649--2-blue" alt="Nature Paper" /&gt;&lt;/a&gt; &lt;a href="https://insights.linuxfoundation.org/project/numpy"&gt;&lt;img src="https://insights.linuxfoundation.org/api/badge/health-score?project=numpy" alt="LFX Health Score" /&gt;&lt;/a&gt; &lt;a href="https://securityscorecards.dev/viewer/?uri=github.com/numpy/numpy"&gt;&lt;img src="https://api.securityscorecards.dev/projects/github.com/numpy/numpy/badge" alt="OpenSSF Scorecard" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/numpy/"&gt;&lt;img src="https://img.shields.io/pypi/types/numpy" alt="Typing" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;NumPy is the fundamental package for scientific computing with Python.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Website:&lt;/strong&gt; &lt;a href="https://numpy.org"&gt;https://numpy.org&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Documentation:&lt;/strong&gt; &lt;a href="https://numpy.org/doc"&gt;https://numpy.org/doc&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Mailing list:&lt;/strong&gt; &lt;a href="https://mail.python.org/mailman/listinfo/numpy-discussion"&gt;https://mail.python.org/mailman/listinfo/numpy-discussion&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Source code:&lt;/strong&gt; &lt;a href="https://github.com/numpy/numpy"&gt;https://github.com/numpy/numpy&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Contributing:&lt;/strong&gt; &lt;a href="https://numpy.org/devdocs/dev/index.html"&gt;https://numpy.org/devdocs/dev/index.html&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Bug reports:&lt;/strong&gt; &lt;a href="https://github.com/numpy/numpy/issues"&gt;https://github.com/numpy/numpy/issues&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Report a security vulnerability:&lt;/strong&gt; &lt;a href="https://tidelift.com/docs/security"&gt;https://tidelift.com/docs/security&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;It provides:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;a powerful N-dimensional array object&lt;/li&gt; 
 &lt;li&gt;sophisticated (broadcasting) functions&lt;/li&gt; 
 &lt;li&gt;tools for integrating C/C++ and Fortran code&lt;/li&gt; 
 &lt;li&gt;useful linear algebra, Fourier transform, and random number capabilities&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Testing:&lt;/p&gt; 
&lt;p&gt;NumPy requires &lt;code&gt;pytest&lt;/code&gt; and &lt;code&gt;hypothesis&lt;/code&gt;. Tests can then be run after installation with:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;python -c "import numpy, sys; sys.exit(numpy.test() is False)"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Code of Conduct&lt;/h2&gt; 
&lt;p&gt;NumPy is a community-driven open source project developed by a diverse group of &lt;a href="https://numpy.org/teams/"&gt;contributors&lt;/a&gt;. The NumPy leadership has made a strong commitment to creating an open, inclusive, and positive community. Please read the &lt;a href="https://numpy.org/code-of-conduct/"&gt;NumPy Code of Conduct&lt;/a&gt; for guidance on how to interact with others in a way that makes our community thrive.&lt;/p&gt; 
&lt;h2&gt;Call for Contributions&lt;/h2&gt; 
&lt;p&gt;The NumPy project welcomes your expertise and enthusiasm!&lt;/p&gt; 
&lt;p&gt;Small improvements or fixes are always appreciated. If you are considering larger contributions to the source code, please contact us through the &lt;a href="https://mail.python.org/mailman/listinfo/numpy-discussion"&gt;mailing list&lt;/a&gt; first.&lt;/p&gt; 
&lt;p&gt;Writing code isn‚Äôt the only way to contribute to NumPy. You can also:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;review pull requests&lt;/li&gt; 
 &lt;li&gt;help us stay on top of new and old issues&lt;/li&gt; 
 &lt;li&gt;develop tutorials, presentations, and other educational materials&lt;/li&gt; 
 &lt;li&gt;maintain and improve &lt;a href="https://github.com/numpy/numpy.org"&gt;our website&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;develop graphic design for our brand assets and promotional materials&lt;/li&gt; 
 &lt;li&gt;translate website content&lt;/li&gt; 
 &lt;li&gt;help with outreach and onboard new contributors&lt;/li&gt; 
 &lt;li&gt;write grant proposals and help with other fundraising efforts&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For more information about the ways you can contribute to NumPy, visit &lt;a href="https://numpy.org/contribute/"&gt;our website&lt;/a&gt;. If you‚Äôre unsure where to start or how your skills fit in, reach out! You can ask on the mailing list or here, on GitHub, by opening a new issue or leaving a comment on a relevant issue that is already open.&lt;/p&gt; 
&lt;p&gt;Our preferred channels of communication are all public, but if you‚Äôd like to speak to us in private first, contact our community coordinators at &lt;a href="mailto:numpy-team@googlegroups.com"&gt;numpy-team@googlegroups.com&lt;/a&gt; or on Slack (write &lt;a href="mailto:numpy-team@googlegroups.com"&gt;numpy-team@googlegroups.com&lt;/a&gt; for an invitation).&lt;/p&gt; 
&lt;p&gt;We also have a biweekly community call, details of which are announced on the mailing list. You are very welcome to join.&lt;/p&gt; 
&lt;p&gt;If you are new to contributing to open source, &lt;a href="https://opensource.guide/how-to-contribute/"&gt;this guide&lt;/a&gt; helps explain why, what, and how to successfully get involved.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>langflow-ai/langflow</title>
      <link>https://github.com/langflow-ai/langflow</link>
      <description>&lt;p&gt;Langflow is a powerful tool for building and deploying AI-powered agents and workflows.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/langflow-ai/langflow/main/docs/static/img/langflow-logo-color-black-solid.svg?sanitize=true" alt="Langflow logo" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/langflow-ai/langflow/releases"&gt;&lt;img src="https://img.shields.io/github/release/langflow-ai/langflow?style=flat-square" alt="Release Notes" /&gt;&lt;/a&gt; &lt;a href="https://opensource.org/licenses/MIT"&gt;&lt;img src="https://img.shields.io/badge/license-MIT-orange" alt="PyPI - License" /&gt;&lt;/a&gt; &lt;a href="https://pypistats.org/packages/langflow"&gt;&lt;img src="https://img.shields.io/pypi/dm/langflow?style=flat-square" alt="PyPI - Downloads" /&gt;&lt;/a&gt; &lt;a href="https://star-history.com/#langflow-ai/langflow"&gt;&lt;img src="https://img.shields.io/github/stars/langflow-ai/langflow?style=flat-square" alt="GitHub star chart" /&gt;&lt;/a&gt; &lt;a href="https://github.com/langflow-ai/langflow/issues"&gt;&lt;img src="https://img.shields.io/github/issues-raw/langflow-ai/langflow?style=flat-square" alt="Open Issues" /&gt;&lt;/a&gt; &lt;a href="https://twitter.com/langflow_ai"&gt;&lt;img src="https://img.shields.io/twitter/url/https/twitter.com/langflow-ai.svg?style=social&amp;amp;label=Follow%20%40Langflow" alt="Twitter" /&gt;&lt;/a&gt; &lt;a href="https://www.youtube.com/@Langflow"&gt;&lt;img src="https://img.shields.io/youtube/channel/subscribers/UCn2bInQrjdDYKEEmbpwblLQ?label=Subscribe" alt="YouTube Channel" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/EqksyE2EX9"&gt;&lt;img src="https://img.shields.io/discord/1116803230643527710?logo=discord&amp;amp;style=social&amp;amp;label=Join" alt="Discord Server" /&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/langflow-ai/langflow"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!CAUTION]&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Users must update to Langflow &amp;gt;= 1.3 to protect against &lt;a href="https://nvd.nist.gov/vuln/detail/CVE-2025-3248"&gt;CVE-2025-3248&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Users must update to Langflow &amp;gt;= 1.5.1 to protect against &lt;a href="https://github.com/langflow-ai/langflow/security/advisories/GHSA-4gv9-mp8m-592r"&gt;CVE-2025-57760&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;For security information, see our &lt;a href="https://raw.githubusercontent.com/langflow-ai/langflow/main/SECURITY.md"&gt;Security Policy&lt;/a&gt; and &lt;a href="https://github.com/langflow-ai/langflow/security/advisories"&gt;Security Advisories&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;a href="https://langflow.org"&gt;Langflow&lt;/a&gt; is a powerful tool for building and deploying AI-powered agents and workflows. It provides developers with both a visual authoring experience and built-in API and MCP servers that turn every workflow into a tool that can be integrated into applications built on any framework or stack. Langflow comes with batteries included and supports all major LLMs, vector databases and a growing library of AI tools.&lt;/p&gt; 
&lt;h2&gt;‚ú® Highlight features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Visual builder interface&lt;/strong&gt; to quickly get started and iterate .&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Source code access&lt;/strong&gt; lets you customize any component using Python.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Interactive playground&lt;/strong&gt; to immediately test and refine your flows with step-by-step control.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-agent orchestration&lt;/strong&gt; with conversation management and retrieval.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Deploy as an API&lt;/strong&gt; or export as JSON for Python apps.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Deploy as an MCP server&lt;/strong&gt; and turn your flows into tools for MCP clients.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Observability&lt;/strong&gt; with LangSmith, LangFuse and other integrations.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Enterprise-ready&lt;/strong&gt; security and scalability.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;‚ö°Ô∏è Quickstart&lt;/h2&gt; 
&lt;p&gt;Langflow requires &lt;a href="https://www.python.org/downloads/release/python-3100/"&gt;Python 3.10 to 3.13&lt;/a&gt; and &lt;a href="https://docs.astral.sh/uv/getting-started/installation/"&gt;uv&lt;/a&gt;.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;To install Langflow, run:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;uv pip install langflow -U
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;To run Langflow, run:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;uv run langflow run
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Go to the default Langflow URL at &lt;code&gt;http://127.0.0.1:7860&lt;/code&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;For more information about installing Langflow, including Docker and Desktop options, see &lt;a href="https://docs.langflow.org/get-started-installation"&gt;Install Langflow&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;üì¶ Deployment&lt;/h2&gt; 
&lt;p&gt;Langflow is completely open source and you can deploy it to all major deployment clouds. To learn how to use Docker to deploy Langflow, see the &lt;a href="https://docs.langflow.org/deployment-docker"&gt;Docker deployment guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;‚≠ê Stay up-to-date&lt;/h2&gt; 
&lt;p&gt;Star Langflow on GitHub to be instantly notified of new releases.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/03168b17-a11d-4b2a-b0f7-c1cce69e5a2c" alt="Star Langflow" /&gt;&lt;/p&gt; 
&lt;h2&gt;üëã Contribute&lt;/h2&gt; 
&lt;p&gt;We welcome contributions from developers of all levels. If you'd like to contribute, please check our &lt;a href="https://raw.githubusercontent.com/langflow-ai/langflow/main/CONTRIBUTING.md"&gt;contributing guidelines&lt;/a&gt; and help make Langflow more accessible.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#langflow-ai/langflow&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=langflow-ai/langflow&amp;amp;type=Timeline" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;‚ù§Ô∏è Contributors&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/langflow-ai/langflow/graphs/contributors"&gt;&lt;img src="https://contrib.rocks/image?repo=langflow-ai/langflow" alt="langflow contributors" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>kvcache-ai/ktransformers</title>
      <link>https://github.com/kvcache-ai/ktransformers</link>
      <description>&lt;p&gt;A Flexible Framework for Experiencing Cutting-edge LLM Inference Optimizations&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;!-- &lt;h1&gt;KTransformers&lt;/h1&gt; --&gt; 
 &lt;p align="center"&gt; 
  &lt;picture&gt; 
   &lt;img alt="KTransformers" src="https://github.com/user-attachments/assets/d5a2492f-a415-4456-af99-4ab102f13f8b" width="50%" /&gt; 
  &lt;/picture&gt; &lt;/p&gt; 
 &lt;h3&gt;A Flexible Framework for Experiencing Cutting-edge LLM Inference Optimizations&lt;/h3&gt; 
 &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/kvcache-ai/ktransformers/main/#show-cases"&gt;üåü Show Cases&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/kvcache-ai/ktransformers/main/#quick-start"&gt;üöÄ Quick Start&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/kvcache-ai/ktransformers/main/#tutorial"&gt;üìÉ Tutorial&lt;/a&gt; | &lt;a href="https://github.com/kvcache-ai/ktransformers/discussions"&gt;üí¨ Discussion &lt;/a&gt;|&lt;a href="https://raw.githubusercontent.com/kvcache-ai/ktransformers/main/#FAQ"&gt; üôã FAQ&lt;/a&gt; &lt;/strong&gt; 
&lt;/div&gt; 
&lt;h2 id="intro"&gt;üéâ Introduction&lt;/h2&gt; KTransformers, pronounced as Quick Transformers, is designed to enhance your ü§ó 
&lt;a href="https://github.com/huggingface/transformers"&gt;Transformers&lt;/a&gt; experience with advanced kernel optimizations and placement/parallelism strategies. 
&lt;br /&gt;
&lt;br /&gt; KTransformers is a flexible, Python-centric framework designed with extensibility at its core. By implementing and injecting an optimized module with a single line of code, users gain access to a Transformers-compatible interface, RESTful APIs compliant with OpenAI and Ollama, and even a simplified ChatGPT-like web UI. 
&lt;br /&gt;
&lt;br /&gt; Our vision for KTransformers is to serve as a flexible platform for experimenting with innovative LLM inference optimizations. Please let us know if you need any other features. 
&lt;h2 id="Updates"&gt;üî• Updates&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;July 11, 2025&lt;/strong&gt;: Support Kimi-K2-0905. (&lt;a href="https://raw.githubusercontent.com/kvcache-ai/ktransformers/main/doc/en/Kimi-K2.md"&gt;Tutorial&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;July 26, 2025&lt;/strong&gt;: Support SmallThinker and GLM4-MoE. (&lt;a href="https://raw.githubusercontent.com/kvcache-ai/ktransformers/main/doc/en/SmallThinker_and_Glm4moe.md"&gt;Tutorial&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;July 11, 2025&lt;/strong&gt;: Support Kimi-K2. (&lt;a href="https://raw.githubusercontent.com/kvcache-ai/ktransformers/main/doc/en/Kimi-K2.md"&gt;Tutorial&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;June 30, 2025&lt;/strong&gt;: Support 3-layer (GPU-CPU-Disk) &lt;a href="https://raw.githubusercontent.com/kvcache-ai/ktransformers/main/doc/en/prefix_cache.md"&gt;prefix cache&lt;/a&gt; reuse.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;May 14, 2025&lt;/strong&gt;: Support Intel Arc GPU (&lt;a href="https://raw.githubusercontent.com/kvcache-ai/ktransformers/main/doc/en/xpu.md"&gt;Tutorial&lt;/a&gt;).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Apr 29, 2025&lt;/strong&gt;: Support AMX-Int8„ÄÅ AMX-BF16 and Qwen3MoE (&lt;a href="https://raw.githubusercontent.com/kvcache-ai/ktransformers/main/doc/en/AMX.md"&gt;Tutorial&lt;/a&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/fafe8aec-4e22-49a8-8553-59fb5c6b00a2"&gt;https://github.com/user-attachments/assets/fafe8aec-4e22-49a8-8553-59fb5c6b00a2&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Apr 9, 2025&lt;/strong&gt;: Experimental support for LLaMA 4 models (&lt;a href="https://raw.githubusercontent.com/kvcache-ai/ktransformers/main/doc/en/llama4.md"&gt;Tutorial&lt;/a&gt;).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Apr 2, 2025&lt;/strong&gt;: Support Multi-concurrency. (&lt;a href="https://raw.githubusercontent.com/kvcache-ai/ktransformers/main/doc/en/balance-serve.md"&gt;Tutorial&lt;/a&gt;).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/faa3bda2-928b-45a7-b44f-21e12ec84b8a"&gt;https://github.com/user-attachments/assets/faa3bda2-928b-45a7-b44f-21e12ec84b8a&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Mar 15, 2025&lt;/strong&gt;: Support ROCm on AMD GPU (&lt;a href="https://raw.githubusercontent.com/kvcache-ai/ktransformers/main/doc/en/ROCm.md"&gt;Tutorial&lt;/a&gt;).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Mar 5, 2025&lt;/strong&gt;: Support unsloth 1.58/2.51 bits weights and &lt;a href="https://raw.githubusercontent.com/kvcache-ai/ktransformers/main/doc/en/fp8_kernel.md"&gt;IQ1_S/FP8 hybrid&lt;/a&gt; weights. Support 139K &lt;a href="https://raw.githubusercontent.com/kvcache-ai/ktransformers/main/doc/en/DeepseekR1_V3_tutorial.md#v022--v023-longer-context--fp8-kernel"&gt;Longer Context&lt;/a&gt; for DeepSeek-V3 and R1 in 24GB VRAM.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Feb 25, 2025&lt;/strong&gt;: Support &lt;a href="https://raw.githubusercontent.com/kvcache-ai/ktransformers/main/doc/en/fp8_kernel.md"&gt;FP8 GPU kernel&lt;/a&gt; for DeepSeek-V3 and R1; &lt;a href="https://raw.githubusercontent.com/kvcache-ai/ktransformers/main/doc/en/DeepseekR1_V3_tutorial.md#v022-longer-context"&gt;Longer Context&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Feb 15, 2025&lt;/strong&gt;: Longer Context (from 4K to 8K for 24GB VRAM) &amp;amp; Slightly Faster Speed Ôºà+15%, up to 16 Tokens/s), update &lt;a href="https://raw.githubusercontent.com/kvcache-ai/ktransformers/main/doc/en/DeepseekR1_V3_tutorial.md"&gt;docs&lt;/a&gt; and &lt;a href="https://kvcache-ai.github.io/ktransformers/"&gt;online books&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Feb 10, 2025&lt;/strong&gt;: Support Deepseek-R1 and V3 on single (24GB VRAM)/multi gpu and 382G DRAM, up to 3~28x speedup. For detailed show case and reproduction tutorial, see &lt;a href="https://raw.githubusercontent.com/kvcache-ai/ktransformers/main/doc/en/DeepseekR1_V3_tutorial.md"&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Aug 28, 2024&lt;/strong&gt;: Decrease DeepseekV2's required VRAM from 21G to 11G.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Aug 15, 2024&lt;/strong&gt;: Update detailed &lt;a href="https://raw.githubusercontent.com/kvcache-ai/ktransformers/main/doc/en/injection_tutorial.md"&gt;tutorial&lt;/a&gt; for injection and multi-GPU.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Aug 14, 2024&lt;/strong&gt;: Support llamfile as linear backend.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Aug 12, 2024&lt;/strong&gt;: Support multiple GPU; Support new model: mixtral 8*7B and 8*22B; Support q2k, q3k, q5k dequant on gpu.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Aug 9, 2024&lt;/strong&gt;: Support windows native.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- * **Aug 28, 2024**: Support 1M context under the InternLM2.5-7B-Chat-1M model, utilizing 24GB of VRAM and 150GB of DRAM. The detailed tutorial is [here](./doc/en/long_context_tutorial.md). --&gt; 
&lt;h2 id="show-cases"&gt;üåü Show Cases&lt;/h2&gt; 
&lt;div&gt; 
 &lt;h3&gt;GPT-4/o1-level Local VSCode Copilot on a Desktop with only 24GB VRAM&lt;/h3&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/ebd70bfa-b2c1-4abb-ae3b-296ed38aa285"&gt;https://github.com/user-attachments/assets/ebd70bfa-b2c1-4abb-ae3b-296ed38aa285&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;[NEW!!!] Local 671B DeepSeek-Coder-V3/R1:&lt;/strong&gt; Running its Q4_K_M version using only 14GB VRAM and 382GB DRAM(&lt;a href="https://raw.githubusercontent.com/kvcache-ai/ktransformers/main/doc/en/DeepseekR1_V3_tutorial.md"&gt;Tutorial&lt;/a&gt;).&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Prefill Speed (tokens/s): 
    &lt;ul&gt; 
     &lt;li&gt;KTransformers: 54.21 (32 cores) ‚Üí 74.362 (dual-socket, 2√ó32 cores) ‚Üí 255.26 (optimized AMX-based MoE kernel, V0.3 only) ‚Üí 286.55 (selectively using 6 experts, V0.3 only)&lt;/li&gt; 
     &lt;li&gt;Compared to 10.31 tokens/s in llama.cpp with 2√ó32 cores, achieving up to &lt;strong&gt;27.79√ó speedup&lt;/strong&gt;.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Decode Speed (tokens/s): 
    &lt;ul&gt; 
     &lt;li&gt;KTransformers: 8.73 (32 cores) ‚Üí 11.26 (dual-socket, 2√ó32 cores) ‚Üí 13.69 (selectively using 6 experts, V0.3 only)&lt;/li&gt; 
     &lt;li&gt;Compared to 4.51 tokens/s in llama.cpp with 2√ó32 cores, achieving up to &lt;strong&gt;3.03√ó speedup&lt;/strong&gt;.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Upcoming Open Source Release: 
    &lt;ul&gt; 
     &lt;li&gt;AMX optimizations and selective expert activation will be open-sourced in V0.3.&lt;/li&gt; 
     &lt;li&gt;Currently available only in preview binary distribution, which can be downloaded &lt;a href="https://raw.githubusercontent.com/kvcache-ai/ktransformers/main/doc/en/DeepseekR1_V3_tutorial.md"&gt;here&lt;/a&gt;.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Local 236B DeepSeek-Coder-V2:&lt;/strong&gt; Running its Q4_K_M version using only 21GB VRAM and 136GB DRAM, attainable on a local desktop machine, which scores even better than GPT4-0613 in &lt;a href="https://huggingface.co/blog/leaderboard-bigcodebench"&gt;BigCodeBench&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="center"&gt; 
 &lt;picture&gt; 
  &lt;img alt="DeepSeek-Coder-V2 Score" src="https://github.com/user-attachments/assets/d052924e-8631-44de-aad2-97c54b965693" width="100%" /&gt; 
 &lt;/picture&gt; &lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Faster Speed:&lt;/strong&gt; Achieving 126 tokens/s for 2K prompt prefill and 13.6 tokens/s for generation through MoE offloading and injecting advanced kernels from &lt;a href="https://github.com/Mozilla-Ocho/llamafile/tree/main"&gt;Llamafile&lt;/a&gt; and &lt;a href="https://github.com/IST-DASLab/marlin"&gt;Marlin&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;VSCode Integration:&lt;/strong&gt; Wrapped into an OpenAI and Ollama compatible API for seamless integration as a backend for &lt;a href="https://github.com/TabbyML/tabby"&gt;Tabby&lt;/a&gt; and various other frontends.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="center"&gt; &lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/4c6a8a38-05aa-497d-8eb1-3a5b3918429c"&gt;https://github.com/user-attachments/assets/4c6a8a38-05aa-497d-8eb1-3a5b3918429c&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;/p&gt; 
&lt;!-- &lt;h3&gt;1M Context Local Inference on a Desktop with Only 24GB VRAM&lt;/h3&gt;
&lt;p align="center"&gt;

https://github.com/user-attachments/assets/a865e5e4-bca3-401e-94b8-af3c080e6c12

* **1M Context InternLM 2.5 7B**: Operates at full bf16 precision, utilizing 24GB VRAM and 150GB DRAM, which is feasible on a local desktop setup. It achieves a 92.88% success rate on the 1M "Needle In a Haystack" test and 100% on the 128K NIAH test.

&lt;p align="center"&gt;
  &lt;picture&gt;
    &lt;img alt="Single Needle Retrieval 128K" src="./doc/assets/needle_128K.png" width=100%&gt;
  &lt;/picture&gt;
&lt;/p&gt;

&lt;p align="center"&gt;
  &lt;picture&gt;
    &lt;img alt="Single Needle Retrieval 1000K" src="./doc/assets/needle_1M.png" width=100%&gt;
  &lt;/picture&gt;
&lt;/p&gt;

* **Enhanced Speed**: Reaches 16.91 tokens/s for generation with a 1M context using sparse attention, powered by llamafile kernels. This method is over 10 times faster than full attention approach of llama.cpp.

* **Flexible Sparse Attention Framework**: Offers a flexible block sparse attention framework for CPU offloaded decoding. Compatible with SnapKV, Quest, and InfLLm. Further information is available [here](./doc/en/long_context_introduction.md).
 --&gt; 
&lt;p&gt;&lt;strong&gt;More advanced features will coming soon, so stay tuned!&lt;/strong&gt;&lt;/p&gt; 
&lt;h2 id="quick-start"&gt;üöÄ Quick Start&lt;/h2&gt; 
&lt;p&gt;Getting started with KTransformers is simple! Follow the steps below to set up and start using it.&lt;/p&gt; 
&lt;p&gt;we have already supported vendors:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Metax&lt;/li&gt; 
 &lt;li&gt;Sanechips (ZhuFeng V1.0)&lt;/li&gt; 
 &lt;li&gt;Intel&lt;/li&gt; 
 &lt;li&gt;Ascend&lt;/li&gt; 
 &lt;li&gt;Kunpeng&lt;/li&gt; 
 &lt;li&gt;AMD&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üì• Installation&lt;/h3&gt; 
&lt;p&gt;To install KTransformers, follow the official &lt;a href="https://kvcache-ai.github.io/ktransformers/en/install.html"&gt;Installation Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h2 id="tutorial"&gt;üìÉ Brief Injection Tutorial&lt;/h2&gt; At the heart of KTransformers is a user-friendly, template-based injection framework. This allows researchers to easily replace original torch modules with optimized variants. It also simplifies the process of combining multiple optimizations, allowing the exploration of their synergistic effects. 
&lt;br /&gt; 
&lt;p align="center"&gt; 
 &lt;picture&gt; 
  &lt;img alt="Inject-Struction" src="https://github.com/user-attachments/assets/6b4c1e54-9f6d-45c5-a3fc-8fa45e7d257e" width="65%" /&gt; 
 &lt;/picture&gt; &lt;/p&gt; 
&lt;p&gt;Given that vLLM already serves as a great framework for large-scale deployment optimizations, KTransformers is particularly focused on local deployments that are constrained by limited resources. We pay special attention to heterogeneous computing opportunities, such as GPU/CPU offloading of quantized models. For example, we support the efficient &lt;a herf="https://github.com/Mozilla-Ocho/llamafile/tree/main"&gt;Llamafile&lt;/a&gt; and &lt;a herf="https://github.com/IST-DASLab/marlin"&gt;Marlin&lt;/a&gt; kernels for CPU and GPU, respectively. More details can be found &lt;a herf="doc/en/operators/llamafile.md"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Example Usage&lt;/h3&gt; To utilize the provided kernels, users only need to create a YAML-based injection template and add the call to `optimize_and_load_gguf` before using the Transformers model. 
&lt;pre&gt;&lt;code class="language-python"&gt;with torch.device("meta"):
    model = AutoModelForCausalLM.from_config(config, trust_remote_code=True)
optimize_and_load_gguf(model, optimize_config_path, gguf_path, config)
...
generated = prefill_and_generate(model, tokenizer, input_tensor.cuda(), max_new_tokens=1000)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;In this example, the AutoModel is first initialized on the meta device to avoid occupying any memory resources. Then, &lt;code&gt;optimize_and_load_gguf&lt;/code&gt; iterates through all sub-modules of the model, matches rules specified in your YAML rule file, and replaces them with advanced modules as specified.&lt;/p&gt; 
&lt;p&gt;After injection, the original &lt;code&gt;generate&lt;/code&gt; interface is available, but we also provide a compatible &lt;code&gt;prefill_and_generate&lt;/code&gt; method, which enables further optimizations like CUDAGraph to improve generation speed.&lt;/p&gt; 
&lt;h3&gt;How to custom your model&lt;/h3&gt; 
&lt;p&gt;A detailed tutorial of the injection and multi-GPU using DeepSeek-V2 as an example is given &lt;a href="https://raw.githubusercontent.com/kvcache-ai/ktransformers/main/doc/en/injection_tutorial.md"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Below is an example of a YAML template for replacing all original Linear modules with Marlin, an advanced 4-bit quantization kernel.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;- match:
    name: "^model\\.layers\\..*$"  # regular expression 
    class: torch.nn.Linear  # only match modules matching name and class simultaneously
  replace:
    class: ktransformers.operators.linear.KTransformerLinear  # optimized Kernel on quantized data types
    device: "cpu"   # which devices to load this module when initializing
    kwargs:
      generate_device: "cuda"
      generate_linear_type: "QuantizedLinearMarlin"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Each rule in the YAML file has two parts: &lt;code&gt;match&lt;/code&gt; and &lt;code&gt;replace&lt;/code&gt;. The &lt;code&gt;match&lt;/code&gt; part specifies which module should be replaced, and the &lt;code&gt;replace&lt;/code&gt; part specifies the module to be injected into the model along with the initialization keywords.&lt;/p&gt; 
&lt;p&gt;You can find example rule templates for optimizing DeepSeek-V2 and Qwen2-57B-A14, two SOTA MoE models, in the &lt;a href="https://raw.githubusercontent.com/kvcache-ai/ktransformers/main/ktransformers/optimize/optimize_rules"&gt;ktransformers/optimize/optimize_rules&lt;/a&gt; directory. These templates are used to power the &lt;code&gt;local_chat.py&lt;/code&gt; demo.&lt;/p&gt; 
&lt;p&gt;If you are interested in our design principles and the implementation of the injection framework, please refer to the &lt;a href="https://raw.githubusercontent.com/kvcache-ai/ktransformers/main/doc/en/deepseek-v2-injection.md"&gt;design document&lt;/a&gt;.&lt;/p&gt; 
&lt;h2 id="ack"&gt;Acknowledgment and Contributors&lt;/h2&gt; 
&lt;p&gt;The development of KTransformers is based on the flexible and versatile framework provided by Transformers. We also benefit from advanced kernels such as GGUF/GGML, Llamafile, Marlin, sglang and flashinfer. We are planning to contribute back to the community by upstreaming our modifications.&lt;/p&gt; 
&lt;p&gt;KTransformers is actively maintained and developed by contributors from the &lt;a href="https://madsys.cs.tsinghua.edu.cn/"&gt;MADSys group&lt;/a&gt; at Tsinghua University and members from &lt;a href="http://approaching.ai/"&gt;Approaching.AI&lt;/a&gt;. We welcome new contributors to join us in making KTransformers faster and easier to use.&lt;/p&gt; 
&lt;h2 id="ack"&gt;Discussion&lt;/h2&gt; 
&lt;p&gt;If you have any questions, feel free to open an issue. Alternatively, you can join our WeChat group for further discussion. QR Code: &lt;a href="https://raw.githubusercontent.com/kvcache-ai/ktransformers/main/WeChatGroup.png"&gt;WeChat Group&lt;/a&gt;&lt;/p&gt; 
&lt;h2 id="FAQ"&gt;üôã FAQ&lt;/h2&gt; 
&lt;p&gt;Some common questions are answered in the &lt;a href="https://raw.githubusercontent.com/kvcache-ai/ktransformers/main/doc/en/FAQ.md"&gt;FAQ&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>subframe7536/maple-font</title>
      <link>https://github.com/subframe7536/maple-font</link>
      <description>&lt;p&gt;Maple Mono: Open source monospace font with round corner, ligatures and Nerd-Font icons for IDE and terminal, fine-grained customization options. Â∏¶ËøûÂ≠óÂíåÊéßÂà∂Âè∞ÂõæÊ†áÁöÑÂúÜËßíÁ≠âÂÆΩÂ≠ó‰ΩìÔºå‰∏≠Ëã±ÊñáÂÆΩÂ∫¶ÂÆåÁæé2:1ÔºåÁªÜÁ≤íÂ∫¶ÁöÑËá™ÂÆö‰πâÈÄâÈ°π&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/subframe7536/maple-font/variable/resources/header.png" alt="Cover" /&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/13165" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/13165" alt="subframe7536%2Fmaple-font | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; &lt;a href="https://hellogithub.com/repository/0601f355bd824d88b58f1af3066c486a" target="_blank"&gt;&lt;img src="https://api.hellogithub.com/v1/widgets/recommend.svg?rid=0601f355bd824d88b58f1af3066c486a&amp;amp;claim_uid=AO0yWRQ48ITGNqK" alt="FeaturedÔΩúHelloGitHub" style="width: 250px; height: 54px;" width="250" height="54" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;img alt="GitHub Repo Stars" src="https://img.shields.io/github/stars/subframe7536/maple-font" /&gt; &lt;img alt="GitHub Repo Forks" src="https://img.shields.io/github/forks/subframe7536/maple-font" /&gt; &lt;img alt="X (formerly Twitter) Follow" src="https://img.shields.io/twitter/follow/subframe7536" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;img alt="GitHub Release" src="https://img.shields.io/github/v/release/subframe7536/maple-font" /&gt; &lt;img alt="GitHub Downloads (all assets, all releases)" src="https://img.shields.io/github/downloads/subframe7536/maple-font/total" /&gt; &lt;img alt="GitHub Repo License" src="https://img.shields.io/github/license/subframe7536/maple-font" /&gt; &lt;img alt="GitHub Repo Issues" src="https://img.shields.io/github/issues/subframe7536/maple-font" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://raw.githubusercontent.com/subframe7536/maple-font/variable/#download"&gt;Download&lt;/a&gt; | &lt;a href="https://font.subf.dev"&gt;Website&lt;/a&gt; | English | &lt;a href="https://raw.githubusercontent.com/subframe7536/maple-font/variable/README_CN.md"&gt;‰∏≠Êñá&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/subframe7536/maple-font/variable/README_JA.md"&gt;Êó•Êú¨Ë™û&lt;/a&gt; &lt;/p&gt; 
&lt;h1&gt;Maple Mono&lt;/h1&gt; 
&lt;p&gt;Maple Mono is an open source monospace font focused on smoothing your coding flow.&lt;/p&gt; 
&lt;p&gt;I create it to enhance my working experience, and hope that it can be useful to others.&lt;/p&gt; 
&lt;p&gt;V7 is a completely remade version, providing variable font format and source files of font project, redesigning more than half of the glyphs and offering smarter ligatures. You can checkout V6 &lt;a href="https://github.com/subframe7536/maple-font/tree/main"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;‚ú® Variable - Infinity font weights with fine-grained italic glyphs.&lt;/li&gt; 
 &lt;li&gt;‚òÅÔ∏è Smooth - Round corner, brand-new glyph of &lt;code&gt;@ $ % &amp;amp; Q -&amp;gt;&lt;/code&gt; and cursive &lt;code&gt;f i j k l x y&lt;/code&gt; in italic style.&lt;/li&gt; 
 &lt;li&gt;üí™ Useful - Large amount of smart ligatures, see in &lt;a href="https://raw.githubusercontent.com/subframe7536/maple-font/variable/source/features/README.md"&gt;&lt;code&gt;features/&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üé® Icon - First-Class &lt;a href="https://github.com/ryanoasis/nerd-fonts"&gt;Nerd-Font&lt;/a&gt; support, make your terminal more vivid.&lt;/li&gt; 
 &lt;li&gt;üî® Customize - Enable or disable font features as you want, just make your own font.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Simpified Chinese, Traditional Chinese and Japanese&lt;/h3&gt; 
&lt;p&gt;CN version based on &lt;a href="https://github.com/CyanoHao/Resource-Han-Rounded"&gt;Resource Han Rounded&lt;/a&gt; provides complete character set support for Chinese development environments, including Simplified Chinese, Traditional Chinese, and Japanese. Meanwhile, the characteristic of perfect 2:1 alignment between Chinese and English allows this font to achieve a neat, uniform, beautiful, and comfortable appearance in scenarios such as multilingual display and Markdown tables. However, the spacing of Chinese characters is larger compared to other popular Chinese fonts. See details in &lt;a href="https://github.com/subframe7536/maple-font/releases/tag/cn-base"&gt;release notes&lt;/a&gt; and &lt;a href="https://github.com/subframe7536/maple-font/issues/211"&gt;this issue&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/subframe7536/maple-font/variable/resources/2-1.png" alt="2-1.png" /&gt;&lt;/p&gt; 
&lt;h2&gt;ScreenShots&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/subframe7536/maple-font/variable/resources/showcase.png" alt="showcase.png" /&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Pictured by &lt;a href="https://github.com/subframe7536/vscode-codeimg"&gt;CodeImg&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Theme: &lt;a href="https://github.com/subframe7536/vscode-theme-maple"&gt;Maple&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Config: font size 16px, line height 1.8, default letter spacing&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Download&lt;/h2&gt; 
&lt;p&gt;You can download all the font archives from &lt;a href="https://github.com/subframe7536/maple-font/releases"&gt;Releases&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Scoop (Windows)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Add bucket
scoop bucket add nerd-fonts
# Maple Mono (ttf format)
scoop install Maple-Mono
# Maple Mono NF
scoop install Maple-Mono-NF
# Maple Mono NF CN
scoop install Maple-Mono-NF-CN
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;All packages (Click to expand)&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-sh"&gt;# Add bucket
scoop bucket add nerd-fonts
# Maple Mono (ttf format)
scoop install Maple-Mono
# Maple Mono (hinted ttf format)
scoop install Maple-Mono-autohint
# Maple Mono (otf format)
scoop install Maple-Mono-otf
# Maple Mono NF
scoop install Maple-Mono-NF
# Maple Mono NF CN
scoop install Maple-Mono-NF-CN
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;Homebrew (MacOS, Linux)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Maple Mono
brew install --cask font-maple-mono
# Maple Mono NF
brew install --cask font-maple-mono-nf
# Maple Mono NF CN
brew install --cask font-maple-mono-nf-cn
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;All packages (Click to expand)&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-sh"&gt;# Maple Mono
brew install --cask font-maple-mono
# Maple Mono NF
brew install --cask font-maple-mono-nf
# Maple Mono CN
brew install --cask font-maple-mono-cn
# Maple Mono NF CN
brew install --cask font-maple-mono-nf-cn

# Maple Mono Normal
brew install --cask font-maple-mono-normal
# Maple Mono Normal NF
brew install --cask font-maple-mono-normal-nf
# Maple Mono Normal CN
brew install --cask font-maple-mono-normal-cn
# Maple Mono Normal NF CN
brew install --cask font-maple-mono-normal-nf-cn
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;Arch Linux&lt;/h3&gt; 
&lt;p&gt;ArchLinuxCN repository allows downloading a single package zip file without downloading all the package zip files in pkgbase, but AUR does not. (If you have a good solution, please contact Cyberczy(&lt;a href="mailto:czysheep@gmail.com"&gt;czysheep@gmail.com&lt;/a&gt;))&lt;/p&gt; 
&lt;h4&gt;ArchLinuxCN (Recommended)&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Maple Mono (Ligature TTF unhinted)
paru -S ttf-maplemono
# Maple Mono NF (Ligature unhinted)
paru -S ttf-maplemono-nf-unhinted
# Maple Mono NF CN (Ligature unhinted)
paru -S ttf-maplemono-nf-cn-unhinted
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;All packages (Click to expand)&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-sh"&gt;# Maple Mono (Ligature Variable)
paru -S ttf-maplemono-variable
# Maple Mono (Ligature TTF hinted)
paru -S ttf-maplemono-autohint
# Maple Mono (Ligature TTF unhinted)
paru -S ttf-maplemono
# Maple Mono (Ligature OTF)
paru -S otf-maplemono
# Maple Mono (Ligature WOFF2)
paru -S woff2-maplemono
# Maple Mono NF (Ligature hinted)
paru -S ttf-maplemono-nf
# Maple Mono NF (Ligature unhinted)
paru -S ttf-maplemono-nf-unhinted
# Maple Mono CN (Ligature hinted)
paru -S ttf-maplemono-cn
# Maple Mono CN (Ligature unhinted)
paru -S ttf-maplemono-cn-unhinted
# Maple Mono NF CN (Ligature hinted)
paru -S ttf-maplemono-nf-cn
# Maple Mono NF CN (Ligature unhinted)
paru -S ttf-maplemono-nf-cn-unhinted

# Maple Mono (No-Ligature Variable)
paru -S ttf-maplemononl-variable
# Maple Mono (No-Ligature TTF hinted)
paru -S ttf-maplemononl-autohint
# Maple Mono (No-Ligature TTF unhinted)
paru -S ttf-maplemononl
# Maple Mono (No-Ligature OTF)
paru -S otf-maplemononl
# Maple Mono (No-Ligature WOFF2)
paru -S woff2-maplemononl
# Maple Mono NF (No-Ligature hinted)
paru -S ttf-maplemononl-nf
# Maple Mono NF (No-Ligature unhinted)
paru -S ttf-maplemononl-nf-unhinted
# Maple Mono CN (No-Ligature hinted)
paru -S ttf-maplemononl-cn
# Maple Mono CN (No-Ligature unhinted)
paru -S ttf-maplemononl-cn-unhinted
# Maple Mono NF CN (No-Ligature hinted)
paru -S ttf-maplemononl-nf-cn
# Maple Mono NF CN (No-Ligature unhinted)
paru -S ttf-maplemononl-nf-cn-unhinted

# Maple Mono Normal (Ligature Variable)
paru -S ttf-maplemononormal-variable
# Maple Mono Normal (Ligature TTF hinted)
paru -S ttf-maplemononormal-autohint
# Maple Mono Normal (Ligature TTF unhinted)
paru -S ttf-maplemononormal
# Maple Mono Normal (Ligature OTF)
paru -S otf-maplemononormal
# Maple Mono Normal (Ligature WOFF2)
paru -S woff2-maplemononormal
# Maple Mono Normal NF (Ligature hinted)
paru -S ttf-maplemononormal-nf
# Maple Mono Normal NF (Ligature unhinted)
paru -S ttf-maplemononormal-nf-unhinted
# Maple Mono Normal CN (Ligature hinted)
paru -S ttf-maplemononormal-cn
# Maple Mono Normal CN (Ligature unhinted)
paru -S ttf-maplemononormal-cn-unhinted
# Maple Mono Normal NF CN (Ligature hinted)
paru -S ttf-maplemononormal-nf-cn
# Maple Mono Normal NF CN (Ligature unhinted)
paru -S ttf-maplemononormal-nf-cn-unhinted

# Maple Mono Normal (No-Ligature Variable)
paru -S ttf-maplemononormalnl-variable
# Maple Mono Normal (No-Ligature TTF hinted)
paru -S ttf-maplemononormalnl-autohint
# Maple Mono Normal (No-Ligature TTF unhinted)
paru -S ttf-maplemononormalnl
# Maple Mono Normal (No-Ligature OTF)
paru -S otf-maplemononormalnl
# Maple Mono Normal (No-Ligature WOFF2)
paru -S woff2-maplemononormalnl
# Maple Mono Normal NF (No-Ligature hinted)
paru -S ttf-maplemononormalnl-nf
# Maple Mono Normal NF (No-Ligature unhinted)
paru -S ttf-maplemononormalnl-nf-unhinted
# Maple Mono Normal CN (No-Ligature hinted)
paru -S ttf-maplemononormalnl-cn
# Maple Mono Normal CN (No-Ligature unhinted)
paru -S ttf-maplemononormalnl-cn-unhinted
# Maple Mono Normal NF CN (No-Ligature hinted)
paru -S ttf-maplemononormalnl-nf-cn
# Maple Mono Normal NF CN (No-Ligature unhinted)
paru -S ttf-maplemononormalnl-nf-cn-unhinted
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h4&gt;AUR (Not Recommended)&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Maple Mono (Ligature TTF unhinted)
paru -S maplemono-ttf
# Maple Mono NF (Ligature unhinted)
paru -S maplemono-nf-unhinted
# Maple Mono NF CN (Ligature unhinted)
paru -S maplemono-nf-cn-unhinted
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;All packages (Click to expand)&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-sh"&gt;# Maple Mono (Ligature Variable)
paru -S maplemono-variable
# Maple Mono (Ligature TTF hinted)
paru -S maplemono-ttf-autohint
# Maple Mono (Ligature TTF unhinted)
paru -S maplemono-ttf
# Maple Mono (Ligature OTF)
paru -S maplemono-otf
# Maple Mono (Ligature WOFF2)
paru -S maplemono-woff2
# Maple Mono NF (Ligature hinted)
paru -S maplemono-nf
# Maple Mono NF (Ligature unhinted)
paru -S maplemono-nf-unhinted
# Maple Mono CN (Ligature hinted)
paru -S maplemono-cn
# Maple Mono CN (Ligature unhinted)
paru -S maplemono-cn-unhinted
# Maple Mono NF CN (Ligature hinted)
paru -S maplemono-nf-cn
# Maple Mono NF CN (Ligature unhinted)
paru -S maplemono-nf-cn-unhinted

# Maple Mono (No-Ligature Variable)
paru -S maplemononl-variable
# Maple Mono (No-Ligature TTF hinted)
paru -S maplemononl-ttf-autohint
# Maple Mono (No-Ligature TTF unhinted)
paru -S maplemononl-ttf
# Maple Mono (No-Ligature OTF)
paru -S maplemononl-otf
# Maple Mono (No-Ligature WOFF2)
paru -S maplemononl-woff2
# Maple Mono NF (No-Ligature hinted)
paru -S maplemononl-nf
# Maple Mono NF (No-Ligature unhinted)
paru -S maplemononl-nf-unhinted
# Maple Mono CN (No-Ligature hinted)
paru -S maplemononl-cn
# Maple Mono CN (No-Ligature unhinted)
paru -S maplemononl-cn-unhinted
# Maple Mono NF CN (No-Ligature hinted)
paru -S maplemononl-nf-cn
# Maple Mono NF CN (No-Ligature unhinted)
paru -S maplemononl-nf-cn-unhinted

# Maple Mono Normal (Ligature Variable)
paru -S maplemononormal-variable
# Maple Mono Normal (Ligature TTF hinted)
paru -S maplemononormal-ttf-autohint
# Maple Mono Normal (Ligature TTF unhinted)
paru -S maplemononormal-ttf
# Maple Mono Normal (Ligature OTF)
paru -S maplemononormal-otf
# Maple Mono Normal (Ligature WOFF2)
paru -S maplemononormal-woff2
# Maple Mono Normal NF (Ligature hinted)
paru -S maplemononormal-nf
# Maple Mono Normal NF (Ligature unhinted)
paru -S maplemononormal-nf-unhinted
# Maple Mono Normal CN (Ligature hinted)
paru -S maplemononormal-cn
# Maple Mono Normal CN (Ligature unhinted)
paru -S maplemononormal-cn-unhinted
# Maple Mono Normal NF CN (Ligature hinted)
paru -S maplemononormal-nf-cn
# Maple Mono Normal NF CN (Ligature unhinted)
paru -S maplemononormal-nf-cn-unhinted

# Maple Mono Normal (No-Ligature Variable)
paru -S maplemononormalnl-variable
# Maple Mono Normal (No-Ligature TTF hinted)
paru -S maplemononormalnl-ttf-autohint
# Maple Mono Normal (No-Ligature TTF unhinted)
paru -S maplemononormalnl-ttf
# Maple Mono Normal (No-Ligature OTF)
paru -S maplemononormalnl-otf
# Maple Mono Normal (No-Ligature WOFF2)
paru -S maplemononormalnl-woff2
# Maple Mono Normal NF (No-Ligature hinted)
paru -S maplemononormalnl-nf
# Maple Mono Normal NF (No-Ligature unhinted)
paru -S maplemononormalnl-nf-unhinted
# Maple Mono Normal CN (No-Ligature hinted)
paru -S maplemononormalnl-cn
# Maple Mono Normal CN (No-Ligature unhinted)
paru -S maplemononormalnl-cn-unhinted
# Maple Mono Normal NF CN (No-Ligature hinted)
paru -S maplemononormalnl-nf-cn
# Maple Mono Normal NF CN (No-Ligature unhinted)
paru -S maplemononormalnl-nf-cn-unhinted
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;Nixpkgs (NixOS, Linux, MacOS)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-nix"&gt;fonts.packages = with pkgs; [
  # Maple Mono (Ligature TTF unhinted)
  maple-mono.truetype
  # Maple Mono NF (Ligature unhinted)
  maple-mono.NF-unhinted
  # Maple Mono NF CN (Ligature unhinted)
  maple-mono.NF-CN-unhinted
];
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;All packages (Click to expand)&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-nix"&gt;fonts.packages = with pkgs; [
  # Maple Mono (Ligature Variable)
  maple-mono.variable
  # Maple Mono (Ligature TTF hinted)
  maple-mono.truetype-autohint
  # Maple Mono (Ligature TTF unhinted)
  maple-mono.truetype
  # Maple Mono (Ligature OTF)
  maple-mono.opentype
  # Maple Mono (Ligature WOFF2)
  maple-mono.woff2
  # Maple Mono NF (Ligature hinted)
  maple-mono.NF
  # Maple Mono NF (Ligature unhinted)
  maple-mono.NF-unhinted
  # Maple Mono CN (Ligature hinted)
  maple-mono.CN
  # Maple Mono CN (Ligature unhinted)
  maple-mono.CN-unhinted
  # Maple Mono NF CN (Ligature hinted)
  maple-mono.NF-CN
  # Maple Mono NF CN (Ligature unhinted)
  maple-mono.NF-CN-unhinted

  # Maple Mono (No-Ligature Variable)
  maple-mono.NL-Variable
  # Maple Mono (No-Ligature TTF hinted)
  maple-mono.NL-TTF-AutoHint
  # Maple Mono (No-Ligature TTF unhinted)
  maple-mono.NL-TTF
  # Maple Mono (No-Ligature OTF)
  maple-mono.NL-OTF
  # Maple Mono (No-Ligature WOFF2)
  maple-mono.NL-Woff2
  # Maple Mono NF (No-Ligature hinted)
  maple-mono.NL-NF
  # Maple Mono NF (No-Ligature unhinted)
  maple-mono.NL-NF-unhinted
  # Maple Mono CN (No-Ligature hinted)
  maple-mono.NL-CN
  # Maple Mono CN (No-Ligature unhinted)
  maple-mono.NL-CN-unhinted
  # Maple Mono NF CN (No-Ligature hinted)
  maple-mono.NL-NF-CN
  # Maple Mono NF CN (No-Ligature unhinted)
  maple-mono.NL-NF-CN-unhinted

  # Maple Mono Normal (Ligature Variable)
  maple-mono.Normal-Variable
  # Maple Mono Normal (Ligature TTF hinted)
  maple-mono.Normal-TTF-AutoHint
  # Maple Mono Normal (Ligature TTF unhinted)
  maple-mono.Normal-TTF
  # Maple Mono Normal (Ligature OTF)
  maple-mono.Normal-OTF
  # Maple Mono Normal (Ligature WOFF2)
  maple-mono.Normal-Woff2
  # Maple Mono Normal NF (Ligature hinted)
  maple-mono.Normal-NF
  # Maple Mono Normal NF (Ligature unhinted)
  maple-mono.Normal-NF-unhinted
  # Maple Mono Normal CN (Ligature hinted)
  maple-mono.Normal-CN
  # Maple Mono Normal CN (Ligature unhinted)
  maple-mono.Normal-CN-unhinted
  # Maple Mono Normal NF CN (Ligature hinted)
  maple-mono.Normal-NF-CN
  # Maple Mono Normal NF CN (Ligature unhinted)
  maple-mono.Normal-NF-CN-unhinted

  # Maple Mono Normal (No-Ligature Variable)
  maple-mono.NormalNL-Variable
  # Maple Mono Normal (No-Ligature TTF hinted)
  maple-mono.NormalNL-TTF-AutoHint
  # Maple Mono Normal (No-Ligature TTF unhinted)
  maple-mono.NormalNL-TTF
  # Maple Mono Normal (No-Ligature OTF)
  maple-mono.NormalNL-OTF
  # Maple Mono Normal (No-Ligature WOFF2)
  maple-mono.NormalNL-Woff2
  # Maple Mono Normal NF (No-Ligature hinted)
  maple-mono.NormalNL-NF
  # Maple Mono Normal NF (No-Ligature unhinted)
  maple-mono.NormalNL-NF-unhinted
  # Maple Mono Normal CN (No-Ligature hinted)
  maple-mono.NormalNL-CN
  # Maple Mono Normal CN (No-Ligature unhinted)
  maple-mono.NormalNL-CN-unhinted
  # Maple Mono Normal NF CN (No-Ligature hinted)
  maple-mono.NormalNL-NF-CN
  # Maple Mono Normal NF CN (No-Ligature unhinted)
  maple-mono.NormalNL-NF-CN-unhinted
];
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;CDN&lt;/h2&gt; 
&lt;h3&gt;Maple Mono&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://fontsource.org/fonts/maple-mono"&gt;fontsource&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://fonts.zeoseven.com/items/443/"&gt;ZeoSeven Fonts&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Maple Mono CN&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://chinese-font.netlify.app/zh-cn/fonts/maple-mono-cn/MapleMono-CN-Regular"&gt;The Chinese Web Fonts Plan (‰∏≠ÊñáÁΩëÂ≠óËÆ°Âàí)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://fonts.zeoseven.com/items/442/"&gt;ZeoSeven Fonts&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Usage &amp;amp; Feature Configurations&lt;/h2&gt; 
&lt;p&gt;See in &lt;a href="https://raw.githubusercontent.com/subframe7536/maple-font/variable/source/features/README.md"&gt;document&lt;/a&gt; or try it in &lt;a href="https://font.subf.dev/en/playground"&gt;Playground&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Naming FAQ&lt;/h2&gt; 
&lt;h3&gt;Features&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Ligature&lt;/strong&gt;: Default version with ligatures (&lt;code&gt;Maple Mono&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;No-Ligature&lt;/strong&gt;: Default version without ligatures (&lt;code&gt;Maple Mono NL&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Normal-Ligature&lt;/strong&gt;: &lt;a href="https://raw.githubusercontent.com/subframe7536/maple-font/variable/#preset"&gt;&lt;code&gt;--normal&lt;/code&gt; preset&lt;/a&gt; with ligatures (&lt;code&gt;Maple Mono Normal&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Normal-No-Ligature&lt;/strong&gt;: &lt;a href="https://raw.githubusercontent.com/subframe7536/maple-font/variable/#preset"&gt;&lt;code&gt;--normal&lt;/code&gt; preset&lt;/a&gt; without ligatures (&lt;code&gt;Maple Mono Normal NL&lt;/code&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Format and Glyph Set&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Variable&lt;/strong&gt;: Minimal version, smoothly change font weight by variable&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;TTF&lt;/strong&gt;: Minimal version, ttf format [Recommend!]&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;OTF&lt;/strong&gt;: Minimal version, otf format&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;WOFF2&lt;/strong&gt;: Minimal version, woff2 format, for small size on web pages&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;NF&lt;/strong&gt;: Nerd-Font patched version, add icons for terminal (With &lt;code&gt;-NF&lt;/code&gt; suffix)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;CN&lt;/strong&gt;: Chinese version, embed with Chinese and Japanese glyphs (With &lt;code&gt;-CN&lt;/code&gt; suffix)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;NF-CN&lt;/strong&gt;: Full version, embed with icons, Chinese and Japanese glyphs (With &lt;code&gt;-NF-CN&lt;/code&gt; suffix)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Font Hint&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Hinted font&lt;/strong&gt; is used for low resolution screen to have better render effect. From my experience, if your screen resolution is lower or equal than 1080P, it is recommended to use "hinted font". Using "unhinted font" will lead to misalignment or uneven thickness on your text. 
  &lt;ul&gt; 
   &lt;li&gt;In this case, you can choose &lt;code&gt;MapleMono-TTF-AutoHint&lt;/code&gt; / &lt;code&gt;MapleMono-NF&lt;/code&gt; / &lt;code&gt;MapleMono-NF-CN&lt;/code&gt;, etc.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Unhinted font&lt;/strong&gt; is used for high resolution screen (e.g. for MacBook). Using "hinted font" will blur your text or make it looks weird. 
  &lt;ul&gt; 
   &lt;li&gt;In this case, you can choose &lt;code&gt;MapleMono-OTF&lt;/code&gt; / &lt;code&gt;MapleMono-TTF&lt;/code&gt; / &lt;code&gt;MapleMono-NF-unhinted&lt;/code&gt; / &lt;code&gt;MapleMono-NF-CN-unhinted&lt;/code&gt;, etc.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Why there exists &lt;code&gt;-AutoHint&lt;/code&gt; and &lt;code&gt;-unhinted&lt;/code&gt; suffix? 
  &lt;ul&gt; 
   &lt;li&gt;for backward compatibility, I keep the original naming scheme. &lt;code&gt;-AutoHint&lt;/code&gt; is only used for &lt;code&gt;TTF&lt;/code&gt; format.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Custom Build&lt;/h2&gt; 
&lt;p&gt;The &lt;a href="https://raw.githubusercontent.com/subframe7536/maple-font/variable/config.json"&gt;&lt;code&gt;config.json&lt;/code&gt;&lt;/a&gt; file is used to configure the build process. Checkout the &lt;a href="https://raw.githubusercontent.com/subframe7536/maple-font/variable/source/schema.json"&gt;schema&lt;/a&gt; or &lt;a href="https://raw.githubusercontent.com/subframe7536/maple-font/variable/source/features/README.md"&gt;document&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;p&gt;There also have some &lt;a href="https://raw.githubusercontent.com/subframe7536/maple-font/variable/#build-script-usage"&gt;command line options&lt;/a&gt; for customizing the build process. Cli options have higher priority than options in &lt;code&gt;config.json&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Build In Browser&lt;/h3&gt; 
&lt;p&gt;Go to &lt;a href="https://font.subf.dev/en/playground"&gt;Playground&lt;/a&gt;, and click "Custom Build" button in the bottom left corner&lt;/p&gt; 
&lt;h3&gt;Use Github Actions&lt;/h3&gt; 
&lt;p&gt;You can use &lt;a href="https://github.com/subframe7536/maple-font/actions/workflows/custom.yml"&gt;Github Actions&lt;/a&gt; to build the font.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Fork the repo&lt;/li&gt; 
 &lt;li&gt;(Optional) Change the content in &lt;code&gt;config.json&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Go to Actions tab&lt;/li&gt; 
 &lt;li&gt;Click on &lt;code&gt;Custom Build&lt;/code&gt; menu item on the left&lt;/li&gt; 
 &lt;li&gt;Click on &lt;code&gt;Run workflow&lt;/code&gt; button with options setup&lt;/li&gt; 
 &lt;li&gt;Wait for the build to finish&lt;/li&gt; 
 &lt;li&gt;Download the font archives from Releases&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Use Docker&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;git clone https://github.com/subframe7536/maple-font --depth 1 -b variable
docker build -t maple-font .
docker run -v "$(pwd)/fonts:/app/fonts" -e BUILD_ARGS="--normal" maple-font
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Local Build&lt;/h3&gt; 
&lt;p&gt;Clone the repo and run on your local machine. Make sure you have &lt;code&gt;python3&lt;/code&gt; and &lt;code&gt;pip&lt;/code&gt; installed&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;git clone https://github.com/subframe7536/maple-font --depth 1 -b variable
pip install -r requirements.txt
python build.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;For &lt;code&gt;Ubuntu&lt;/code&gt; or &lt;code&gt;Debian&lt;/code&gt;, maybe &lt;code&gt;python-is-python3&lt;/code&gt; is needed as well&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If you have trouble installing the dependencies, just create a new GitHub Codespace and run the commands there&lt;/p&gt; 
&lt;h4&gt;Custom Nerd-Font&lt;/h4&gt; 
&lt;p&gt;If you want to get fixed width icons, setup &lt;code&gt;"nerd_font.mono": true&lt;/code&gt; in &lt;code&gt;config.json&lt;/code&gt; or add &lt;code&gt;--nf-mono&lt;/code&gt; flag to build script args.&lt;/p&gt; 
&lt;p&gt;If you want to get variable width icons, setup &lt;code&gt;"nerd_font.propo": true&lt;/code&gt; in &lt;code&gt;config.json&lt;/code&gt; or add &lt;code&gt;--nf-propo&lt;/code&gt; flag to build script args.&lt;/p&gt; 
&lt;p&gt;For custom &lt;code&gt;font-patcher&lt;/code&gt; args, &lt;code&gt;font-forge&lt;/code&gt; (and maybe &lt;code&gt;python3-fontforge&lt;/code&gt; as well) is required.&lt;/p&gt; 
&lt;p&gt;Maybe you should also change &lt;code&gt;"nerd_font.extra_args"&lt;/code&gt; in &lt;a href="https://raw.githubusercontent.com/subframe7536/maple-font/variable/config.json"&gt;config.json&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Default args: &lt;code&gt;-l --careful --outputdir dir&lt;/code&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;if &lt;code&gt;"nerd_font.propo"&lt;/code&gt; is &lt;code&gt;true&lt;/code&gt;, then add &lt;code&gt;--variable-width-glyphs&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;else if &lt;code&gt;"nerd_font.mono"&lt;/code&gt; is &lt;code&gt;true&lt;/code&gt;, then add &lt;code&gt;--mono&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Preset&lt;/h4&gt; 
&lt;p&gt;Run &lt;code&gt;build.py&lt;/code&gt; with &lt;code&gt;--normal&lt;/code&gt; flag, make the font looks not such "Opinioned" , just like &lt;code&gt;JetBrains Mono&lt;/code&gt; (with slashed zero).&lt;/p&gt; 
&lt;p&gt;If you are using variable font (NOT recommended), please enable &lt;code&gt;calt&lt;/code&gt; to make all features work.&lt;/p&gt; 
&lt;p&gt;Enabled features:&lt;/p&gt; 
&lt;!-- NORMAL --&gt; 
&lt;pre&gt;&lt;code&gt;cv01, cv02, cv33, cv34, cv35, cv36, cv61, cv62, ss05, ss06, ss07, ss08
&lt;/code&gt;&lt;/pre&gt; 
&lt;!-- NORMAL --&gt; 
&lt;p&gt;&lt;a href="https://font.subf.dev/en/playground?normal"&gt;Online Preview&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;Font Feature Freeze&lt;/h4&gt; 
&lt;p&gt;There are three kinds of options for feature freeze (&lt;a href="https://github.com/subframe7536/maple-font/issues/233#issuecomment-2410170270"&gt;Why&lt;/a&gt;):&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;code&gt;enable&lt;/code&gt;: Forcely enable the features without setting up &lt;code&gt;cvXX&lt;/code&gt; / &lt;code&gt;ssXX&lt;/code&gt; / &lt;code&gt;zero&lt;/code&gt; in font features config, just as default glyphs / ligatures&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;disable&lt;/code&gt;: Remove the features in &lt;code&gt;cvXX&lt;/code&gt; / &lt;code&gt;ssXX&lt;/code&gt; / &lt;code&gt;zero&lt;/code&gt;, which will no longer effect, even if you enable it manually&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ignore&lt;/code&gt;: Do nothing&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Custom OpenType Feature&lt;/h4&gt; 
&lt;p&gt;OpenType Feature is used to control the font's built-in variants and ligatures. You can remove some ligatures or features you don't want to, change feature's trigger rule or add some new rules by modifying OpenType Feature.&lt;/p&gt; 
&lt;p&gt;By default, the Python module in &lt;a href="https://raw.githubusercontent.com/subframe7536/maple-font/variable/source/py/feature"&gt;&lt;code&gt;source/py/feature/&lt;/code&gt;&lt;/a&gt; will generate feature rule string and load it at build time. You can modify the features or customize tags there.&lt;/p&gt; 
&lt;p&gt;If you would like to modify the feature file instead, run &lt;code&gt;build.py&lt;/code&gt; with &lt;code&gt;--apply-fea-file&lt;/code&gt; flag, the feature file from &lt;a href="https://raw.githubusercontent.com/subframe7536/maple-font/variable/source/features"&gt;&lt;code&gt;source/features/{regular,italic}.fea&lt;/code&gt;&lt;/a&gt; will be loaded.&lt;/p&gt; 
&lt;h4&gt;Infinite Arrow Ligatures&lt;/h4&gt; 
&lt;p&gt;Inspired by Fira Code, the font enables infinite arrow ligatures by default from v7.3. For some reason, the ligatures are misaligned when using hinted font, so they are removed in hinted version by default from v7.4. You can setup &lt;code&gt;"keep_infinite_arrow": true&lt;/code&gt; in &lt;code&gt;config.json&lt;/code&gt; or add &lt;code&gt;--keep-infinite-arrow&lt;/code&gt; in cli flag. See more details in &lt;a href="https://github.com/subframe7536/maple-font/issues/508"&gt;#508&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;Custom Font Weight Mapping&lt;/h4&gt; 
&lt;p&gt;You can modify the static font weight through &lt;code&gt;"weight_mapping"&lt;/code&gt; item in &lt;code&gt;config.json&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;For example, if you want to make regular font weight a little bit lighter, just decrease the number of &lt;code&gt;"weight_mapping.regular"&lt;/code&gt; (from 400 to 350 in this example) :&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "weight_mapping": {
    "thin": 100,
    "extralight": 200,
    "light": 300,
    "regular": 350,
    "semibold": 500,
    "medium": 600,
    "bold": 700,
    "extrabold": 800
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Chinese version&lt;/h3&gt; 
&lt;p&gt;CN version is disabled by default. Run &lt;code&gt;python build.py&lt;/code&gt; with &lt;code&gt;--cn&lt;/code&gt; flag, the CN base fonts (about 111 MB) will download from GitHub.&lt;/p&gt; 
&lt;p&gt;If you want to build CN base fonts from variable (about 27 MB), setup &lt;code&gt;"cn.use_static_base_font": false&lt;/code&gt; in &lt;a href="https://raw.githubusercontent.com/subframe7536/maple-font/variable/config.json"&gt;config.json&lt;/a&gt; and &lt;strong&gt;BE PATIENT&lt;/strong&gt;, instantiation will take about 10-30 minutes.&lt;/p&gt; 
&lt;h4&gt;Narrow spacing in CN glyphs&lt;/h4&gt; 
&lt;p&gt;If you think that &lt;strong&gt;CN glyphs spacing is TOOOOOO large&lt;/strong&gt;, there is a build option &lt;code&gt;cn.narrow&lt;/code&gt; or cli flag &lt;code&gt;--cn-narrow&lt;/code&gt; to narrow spacing in CN glyphs, but this will make the font cannot be recogized as monospaced font.&lt;/p&gt; 
&lt;p&gt;You can see effect in &lt;a href="https://github.com/subframe7536/maple-font/issues/249#issuecomment-2871260476"&gt;#249&lt;/a&gt;.&lt;/p&gt; 
&lt;h4&gt;GitHub Mirror&lt;/h4&gt; 
&lt;p&gt;The build script will auto download required assets from GitHub. If you have trouble downloading, please setup &lt;code&gt;github_mirror&lt;/code&gt; in &lt;a href="https://raw.githubusercontent.com/subframe7536/maple-font/variable/config.json"&gt;config.json&lt;/a&gt; or &lt;code&gt;$GITHUB&lt;/code&gt; to your environment variable. (Target URL will be &lt;code&gt;https://&amp;lt;github_mirror&amp;gt;/&amp;lt;user&amp;gt;/&amp;lt;repo&amp;gt;/releases/download/&amp;lt;tag&amp;gt;/&amp;lt;file&amp;gt;&lt;/code&gt;), or just download the target &lt;code&gt;.zip&lt;/code&gt; file and put it in the same directory as &lt;code&gt;build.py&lt;/code&gt;.&lt;/p&gt; 
&lt;h4&gt;Traditional Chinese Punctuation Support&lt;/h4&gt; 
&lt;p&gt;By enabling &lt;code&gt;cv99&lt;/code&gt;, all Chinese punctuation marks will be centred. See more details in &lt;a href="https://github.com/subframe7536/maple-font/issues/150"&gt;#150&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Build Script Usage&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;usage: build.py [-h] [-v] [-d] [--debug] [-n] [--feat FEAT] [--apply-fea-file]
                [--hinted | --no-hinted] [--liga | --no-liga] [--keep-infinite-arrow]
                [--infinite-arrow] [--remove-tag-liga] [--line-height LINE_HEIGHT]
                [--nf-mono] [--nf-propo] [--cn-narrow]
                [--cn-scale-factor CN_SCALE_FACTOR] [--nf | --no-nf] [--cn | --no-cn]
                [--cn-both] [--ttf-only] [--least-styles] [--font-patcher] [--cache]
                [--cn-rebuild] [--archive]

‚ú® Builder and optimizer for Maple Mono

options:
  -h, --help            show this help message and exit
  -v, --version         show program's version number and exit
  -d, --dry             Output config and exit
  --debug               Add `Debug` suffix to family name and faster build

Feature Options:
  -n, --normal          Use normal preset, just like `JetBrains Mono` with slashed
                        zero
  --feat FEAT           Freeze font features, splited by `,` (e.g. `--feat
                        zero,cv01,ss07,ss08`). No effect on variable format
  --apply-fea-file      Load feature file from `source/features/{regular,italic}.fea`
                        to variable font
  --hinted              Use hinted font as base font in NF / CN / NF-CN (default)
  --no-hinted           Use unhinted font as base font in NF / CN / NF-CN
  --liga                Preserve all the ligatures (default)
  --no-liga             Remove all the ligatures
  --keep-infinite-arrow
                        (Deprecated) Keep infinite arrow ligatures in hinted font
                        (Removed by default)
  --infinite-arrow      Enable infinite arrow ligatures (Disabled in hinted font by
                        default)
  --remove-tag-liga     Remove plain text tag ligatures like `[TODO]`
  --line-height LINE_HEIGHT
                        Scale factor for line height (e.g. 1.1)
  --nf-mono             Make Nerd Font icons' width fixed
  --nf-propo            Make Nerd Font icons' width variable, override `--nf-mono`
  --cn-narrow           Make CN / JP characters narrow (And the font cannot be
                        recogized as monospaced font)
  --cn-scale-factor CN_SCALE_FACTOR
                        Scale factor for CN / JP glyphs. Format: &amp;lt;factor&amp;gt; or
                        &amp;lt;width_factor&amp;gt;,&amp;lt;height_factor&amp;gt; (e.g. 1.1 or 1.2,1.1)

Build Options:
  --nf, --nerd-font     Build Nerd-Font version (default)
  --no-nf, --no-nerd-font
                        Do not build Nerd-Font version
  --cn                  Build Chinese version
  --no-cn               Do not build Chinese version (default)
  --cn-both             Build both `Maple Mono CN` and `Maple Mono NF CN`. Nerd-Font
                        version must be enabled
  --ttf-only            Only build TTF format
  --least-styles        Only build Regular / Bold / Italic / BoldItalic style
  --font-patcher        Force the use of Nerd Font Patcher to build NF format
  --cache               Reuse font cache of TTF, OTF and Woff2 formats
  --cn-rebuild          Reinstantiate variable CN base font
  --archive             Build font archives with config and license. If has `--cache`
                        flag, only archive NF and CN formats
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Development&lt;/h2&gt; 
&lt;h3&gt;Design&lt;/h3&gt; 
&lt;p&gt;Using &lt;a href="https://www.fontlab.com/"&gt;FontLab&lt;/a&gt; or &lt;a href="https://glyphs.app"&gt;Glyphs&lt;/a&gt;, generate variable TTF into &lt;code&gt;source/&lt;/code&gt; folder.&lt;/p&gt; 
&lt;h3&gt;Build&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Init project
uv sync
# Dev
uv run build.py --ttf-only --cn --debug
# Update nerd font
uv run task.py nerd-font
# Update fea file
uv run task.py fea
# Update landing page info
uv run task.py page
# Release
uv run task.py release 7.0
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Credit&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/JetBrains/JetBrainsMono"&gt;JetBrains Mono&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/googlefonts/RobotoMono"&gt;Roboto Mono&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/tonsky/FiraCode"&gt;Fira Code&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/rubjo/victor-mono"&gt;Victor Mono&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/eigilnikolajsen/commit-mono"&gt;Commit Mono&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/TheRenegadeCoder/sample-programs-website"&gt;Code Sample&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ryanoasis/nerd-fonts"&gt;Nerd Font&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/MuTsunTsai/fontfreeze/"&gt;Font Freeze&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://tophix.com/font-tools/font-viewer"&gt;Font Viewer&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.monolisa.dev/"&gt;Monolisa&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.recursive.design/"&gt;Recursive&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Sponser&lt;/h2&gt; 
&lt;p&gt;If this font is helpful to you, please feel free to buy me a coffee&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.buymeacoffee.com/subframe753"&gt;&lt;img src="https://img.buymeacoffee.com/button-api/?text=Buy me a coffee&amp;amp;emoji=&amp;amp;slug=subframe753&amp;amp;button_colour=5F7FFF&amp;amp;font_colour=ffffff&amp;amp;font_family=Lato&amp;amp;outline_colour=000000&amp;amp;coffee_colour=FFDD00" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;or sponser me through &lt;a href="https://afdian.com/a/subframe7536"&gt;Afdian&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.star-history.com/#subframe7536/maple-font&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=subframe7536/maple-font&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;SIL Open Font License 1.1&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>