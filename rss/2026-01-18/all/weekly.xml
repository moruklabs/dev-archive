<rss version="2.0">
  <channel>
    <title>GitHub All Languages Weekly Trending</title>
    <description>Weekly Trending of All Languages in GitHub</description>
    <pubDate>Sat, 17 Jan 2026 01:39:36 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>home-assistant/home-assistant.io</title>
      <link>https://github.com/home-assistant/home-assistant.io</link>
      <description>&lt;p&gt;üìò Home Assistant User documentation&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href="https://www.home-assistant.io/join-chat/"&gt;&lt;img src="https://img.shields.io/discord/330944238910963714.svg?sanitize=true" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://creativecommons.org/licenses/by-nc-sa/4.0/"&gt;&lt;img src="https://img.shields.io/badge/License-CC%20BY--NC--SA%204.0-lightgrey.svg?sanitize=true" alt="License: CC BY-NC-SA 4.0" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.netlify.com"&gt;&lt;img src="https://www.netlify.com/img/global/badges/netlify-color-bg.svg?sanitize=true" alt="Deploys by netlify" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Home Assistant website&lt;/h1&gt; 
&lt;p&gt;This is the source for the &lt;a href="https://home-assistant.io"&gt;Home-Assistant.io website&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Access&lt;/h2&gt; 
&lt;p&gt;You can access the site at the following URLs, depending on the target branch:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Production&lt;/strong&gt; (&lt;code&gt;current&lt;/code&gt; branch): &lt;a href="https://www.home-assistant.io"&gt;https://www.home-assistant.io&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Beta&lt;/strong&gt; (&lt;code&gt;rc&lt;/code&gt; branch): &lt;a href="https://rc.home-assistant.io"&gt;https://rc.home-assistant.io&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Development&lt;/strong&gt; (&lt;code&gt;next&lt;/code&gt; branch): &lt;a href="https://next.home-assistant.io"&gt;https://next.home-assistant.io&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Additionally, Netlify provides a preview deployment for every pull request, linked in the first PR comment.&lt;/p&gt; 
&lt;h2&gt;Setup&lt;/h2&gt; 
&lt;p&gt;Setting up to contribute to documentation and the process for submitting pull requests is explained in the &lt;a href="https://developers.home-assistant.io/docs/documenting/"&gt;developer documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Site preview&lt;/h2&gt; 
&lt;p&gt;In order to make the preview available on &lt;code&gt;http://127.0.0.1:4000&lt;/code&gt;, use the following &lt;a href="https://bundler.io/"&gt;bundler&lt;/a&gt; command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;bundle exec rake preview
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If the preview is not running on your local machine, pass the IP of the target machine from where it should be served as a parameter, i.e. to access on &lt;code&gt;http://192.168.0.123:4000&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;bundle exec rake preview[192.168.0.123]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Speeding up site generation&lt;/h2&gt; 
&lt;p&gt;Every release we post long changelogs to the website. This slows down generation of the website significantly! We include some tools to temporarily exclude the blog posts that you're not working on out of the way.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;bundle exec rake isolate[filename-of-blogpost]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;When you're done working on the site, run the following command to move the posts back again:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;bundle exec rake integrate
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href="https://www.openhomefoundation.org/"&gt;&lt;img src="https://www.openhomefoundation.org/badges/home-assistant.png" alt="Home Assistant - A project from the Open Home Foundation" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>anthropics/claude-code</title>
      <link>https://github.com/anthropics/claude-code</link>
      <description>&lt;p&gt;Claude Code is an agentic coding tool that lives in your terminal, understands your codebase, and helps you code faster by executing routine tasks, explaining complex code, and handling git workflows - all through natural language commands.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Claude Code&lt;/h1&gt; 
&lt;p&gt;&lt;img src="https://img.shields.io/badge/Node.js-18%2B-brightgreen?style=flat-square" alt="" /&gt; &lt;a href="https://www.npmjs.com/package/@anthropic-ai/claude-code"&gt;&lt;img src="https://img.shields.io/npm/v/@anthropic-ai/claude-code.svg?style=flat-square" alt="npm" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Claude Code is an agentic coding tool that lives in your terminal, understands your codebase, and helps you code faster by executing routine tasks, explaining complex code, and handling git workflows -- all through natural language commands. Use it in your terminal, IDE, or tag @claude on Github.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Learn more in the &lt;a href="https://code.claude.com/docs/en/overview"&gt;official documentation&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/anthropics/claude-code/main/demo.gif" /&gt; 
&lt;h2&gt;Get started&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Installation via npm is deprecated. Use one of the recommended methods below.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;For more installation options, uninstall steps, and troubleshooting, see the &lt;a href="https://code.claude.com/docs/en/setup"&gt;setup documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Install Claude Code:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;MacOS/Linux (Recommended):&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;curl -fsSL https://claude.ai/install.sh | bash
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Homebrew (MacOS/Linux):&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;brew install --cask claude-code
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Windows (Recommended):&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-powershell"&gt;irm https://claude.ai/install.ps1 | iex
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;WinGet (Windows):&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-powershell"&gt;winget install Anthropic.ClaudeCode
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;NPM (Deprecated):&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;npm install -g @anthropic-ai/claude-code
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Navigate to your project directory and run &lt;code&gt;claude&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Plugins&lt;/h2&gt; 
&lt;p&gt;This repository includes several Claude Code plugins that extend functionality with custom commands and agents. See the &lt;a href="https://raw.githubusercontent.com/anthropics/claude-code/main/plugins/README.md"&gt;plugins directory&lt;/a&gt; for detailed documentation on available plugins.&lt;/p&gt; 
&lt;h2&gt;Reporting Bugs&lt;/h2&gt; 
&lt;p&gt;We welcome your feedback. Use the &lt;code&gt;/bug&lt;/code&gt; command to report issues directly within Claude Code, or file a &lt;a href="https://github.com/anthropics/claude-code/issues"&gt;GitHub issue&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Connect on Discord&lt;/h2&gt; 
&lt;p&gt;Join the &lt;a href="https://anthropic.com/discord"&gt;Claude Developers Discord&lt;/a&gt; to connect with other developers using Claude Code. Get help, share feedback, and discuss your projects with the community.&lt;/p&gt; 
&lt;h2&gt;Data collection, usage, and retention&lt;/h2&gt; 
&lt;p&gt;When you use Claude Code, we collect feedback, which includes usage data (such as code acceptance or rejections), associated conversation data, and user feedback submitted via the &lt;code&gt;/bug&lt;/code&gt; command.&lt;/p&gt; 
&lt;h3&gt;How we use your data&lt;/h3&gt; 
&lt;p&gt;See our &lt;a href="https://code.claude.com/docs/en/data-usage"&gt;data usage policies&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Privacy safeguards&lt;/h3&gt; 
&lt;p&gt;We have implemented several safeguards to protect your data, including limited retention periods for sensitive information, restricted access to user session data, and clear policies against using feedback for model training.&lt;/p&gt; 
&lt;p&gt;For full details, please review our &lt;a href="https://www.anthropic.com/legal/commercial-terms"&gt;Commercial Terms of Service&lt;/a&gt; and &lt;a href="https://www.anthropic.com/legal/privacy"&gt;Privacy Policy&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>anomalyco/opencode</title>
      <link>https://github.com/anomalyco/opencode</link>
      <description>&lt;p&gt;The open source coding agent.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="https://opencode.ai"&gt; 
  &lt;picture&gt; 
   &lt;source srcset="packages/console/app/src/asset/logo-ornate-dark.svg" media="(prefers-color-scheme: dark)" /&gt; 
   &lt;source srcset="packages/console/app/src/asset/logo-ornate-light.svg" media="(prefers-color-scheme: light)" /&gt; 
   &lt;img src="https://raw.githubusercontent.com/anomalyco/opencode/dev/packages/console/app/src/asset/logo-ornate-light.svg?sanitize=true" alt="OpenCode logo" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt;The open source AI coding agent.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://opencode.ai/discord"&gt;&lt;img alt="Discord" src="https://img.shields.io/discord/1391832426048651334?style=flat-square&amp;amp;label=discord" /&gt;&lt;/a&gt; &lt;a href="https://www.npmjs.com/package/opencode-ai"&gt;&lt;img alt="npm" src="https://img.shields.io/npm/v/opencode-ai?style=flat-square" /&gt;&lt;/a&gt; &lt;a href="https://github.com/anomalyco/opencode/actions/workflows/publish.yml"&gt;&lt;img alt="Build status" src="https://img.shields.io/github/actions/workflow/status/anomalyco/opencode/publish.yml?style=flat-square&amp;amp;branch=dev" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;&lt;a href="https://opencode.ai"&gt;&lt;img src="https://raw.githubusercontent.com/anomalyco/opencode/dev/packages/web/src/assets/lander/screenshot.png" alt="OpenCode Terminal UI" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# YOLO
curl -fsSL https://opencode.ai/install | bash

# Package managers
npm i -g opencode-ai@latest        # or bun/pnpm/yarn
scoop bucket add extras; scoop install extras/opencode  # Windows
choco install opencode             # Windows
brew install anomalyco/tap/opencode # macOS and Linux (recommended, always up to date)
brew install opencode              # macOS and Linux (official brew formula, updated less)
paru -S opencode-bin               # Arch Linux
mise use -g opencode               # Any OS
nix run nixpkgs#opencode           # or github:anomalyco/opencode for latest dev branch
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] Remove versions older than 0.1.x before installing.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Desktop App (BETA)&lt;/h3&gt; 
&lt;p&gt;OpenCode is also available as a desktop application. Download directly from the &lt;a href="https://github.com/anomalyco/opencode/releases"&gt;releases page&lt;/a&gt; or &lt;a href="https://opencode.ai/download"&gt;opencode.ai/download&lt;/a&gt;.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Platform&lt;/th&gt; 
   &lt;th&gt;Download&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;macOS (Apple Silicon)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;opencode-desktop-darwin-aarch64.dmg&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;macOS (Intel)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;opencode-desktop-darwin-x64.dmg&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Windows&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;opencode-desktop-windows-x64.exe&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Linux&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;.deb&lt;/code&gt;, &lt;code&gt;.rpm&lt;/code&gt;, or AppImage&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# macOS (Homebrew)
brew install --cask opencode-desktop
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Installation Directory&lt;/h4&gt; 
&lt;p&gt;The install script respects the following priority order for the installation path:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;code&gt;$OPENCODE_INSTALL_DIR&lt;/code&gt; - Custom installation directory&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;$XDG_BIN_DIR&lt;/code&gt; - XDG Base Directory Specification compliant path&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;$HOME/bin&lt;/code&gt; - Standard user binary directory (if exists or can be created)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;$HOME/.opencode/bin&lt;/code&gt; - Default fallback&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Examples
OPENCODE_INSTALL_DIR=/usr/local/bin curl -fsSL https://opencode.ai/install | bash
XDG_BIN_DIR=$HOME/.local/bin curl -fsSL https://opencode.ai/install | bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Agents&lt;/h3&gt; 
&lt;p&gt;OpenCode includes two built-in agents you can switch between with the &lt;code&gt;Tab&lt;/code&gt; key.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;build&lt;/strong&gt; - Default, full access agent for development work&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;plan&lt;/strong&gt; - Read-only agent for analysis and code exploration 
  &lt;ul&gt; 
   &lt;li&gt;Denies file edits by default&lt;/li&gt; 
   &lt;li&gt;Asks permission before running bash commands&lt;/li&gt; 
   &lt;li&gt;Ideal for exploring unfamiliar codebases or planning changes&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Also, included is a &lt;strong&gt;general&lt;/strong&gt; subagent for complex searches and multistep tasks. This is used internally and can be invoked using &lt;code&gt;@general&lt;/code&gt; in messages.&lt;/p&gt; 
&lt;p&gt;Learn more about &lt;a href="https://opencode.ai/docs/agents"&gt;agents&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Documentation&lt;/h3&gt; 
&lt;p&gt;For more info on how to configure OpenCode &lt;a href="https://opencode.ai/docs"&gt;&lt;strong&gt;head over to our docs&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Contributing&lt;/h3&gt; 
&lt;p&gt;If you're interested in contributing to OpenCode, please read our &lt;a href="https://raw.githubusercontent.com/anomalyco/opencode/dev/CONTRIBUTING.md"&gt;contributing docs&lt;/a&gt; before submitting a pull request.&lt;/p&gt; 
&lt;h3&gt;Building on OpenCode&lt;/h3&gt; 
&lt;p&gt;If you are working on a project that's related to OpenCode and is using "opencode" as a part of its name; for example, "opencode-dashboard" or "opencode-mobile", please add a note to your README to clarify that it is not built by the OpenCode team and is not affiliated with us in any way.&lt;/p&gt; 
&lt;h3&gt;FAQ&lt;/h3&gt; 
&lt;h4&gt;How is this different from Claude Code?&lt;/h4&gt; 
&lt;p&gt;It's very similar to Claude Code in terms of capability. Here are the key differences:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;100% open source&lt;/li&gt; 
 &lt;li&gt;Not coupled to any provider. Although we recommend the models we provide through &lt;a href="https://opencode.ai/zen"&gt;OpenCode Zen&lt;/a&gt;; OpenCode can be used with Claude, OpenAI, Google or even local models. As models evolve the gaps between them will close and pricing will drop so being provider-agnostic is important.&lt;/li&gt; 
 &lt;li&gt;Out of the box LSP support&lt;/li&gt; 
 &lt;li&gt;A focus on TUI. OpenCode is built by neovim users and the creators of &lt;a href="https://terminal.shop"&gt;terminal.shop&lt;/a&gt;; we are going to push the limits of what's possible in the terminal.&lt;/li&gt; 
 &lt;li&gt;A client/server architecture. This for example can allow OpenCode to run on your computer, while you can drive it remotely from a mobile app. Meaning that the TUI frontend is just one of the possible clients.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;strong&gt;Join our community&lt;/strong&gt; &lt;a href="https://discord.gg/opencode"&gt;Discord&lt;/a&gt; | &lt;a href="https://x.com/opencode"&gt;X.com&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>bytedance/UI-TARS-desktop</title>
      <link>https://github.com/bytedance/UI-TARS-desktop</link>
      <description>&lt;p&gt;The Open-Source Multimodal AI Agent Stack: Connecting Cutting-Edge AI Models and Agent Infra&lt;/p&gt;&lt;hr&gt;&lt;picture&gt; 
 &lt;img alt="Agent TARS Banner" src="https://raw.githubusercontent.com/bytedance/UI-TARS-desktop/main/images/tars.png" /&gt; 
&lt;/picture&gt; 
&lt;br /&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;English | &lt;a href="https://raw.githubusercontent.com/bytedance/UI-TARS-desktop/main/README.zh-CN.md"&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://trendshift.io/repositories/13584"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/13584" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;b&gt;TARS&lt;sup&gt;*&lt;/sup&gt;&lt;/b&gt; is a Multimodal AI Agent stack, currently shipping two projects: &lt;a href="https://raw.githubusercontent.com/bytedance/UI-TARS-desktop/main/#agent-tars"&gt;Agent TARS&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/bytedance/UI-TARS-desktop/main/#ui-tars-desktop"&gt;UI-TARS-desktop&lt;/a&gt;:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th width="50%" align="center"&gt;&lt;a href="https://raw.githubusercontent.com/bytedance/UI-TARS-desktop/main/#agent-tars"&gt;Agent TARS&lt;/a&gt;&lt;/th&gt; 
   &lt;th width="50%" align="center"&gt;&lt;a href="https://raw.githubusercontent.com/bytedance/UI-TARS-desktop/main/#ui-tars-desktop"&gt;UI-TARS-desktop&lt;/a&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt; 
    &lt;video src="https://github.com/user-attachments/assets/c9489936-afdc-4d12-adda-d4b90d2a869d" width="50%"&gt;&lt;/video&gt; &lt;/td&gt; 
   &lt;td align="center"&gt; 
    &lt;video src="https://github.com/user-attachments/assets/e0914ce9-ad33-494b-bdec-0c25c1b01a27" width="50%"&gt;&lt;/video&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt; &lt;b&gt;Agent TARS&lt;/b&gt; is a general multimodal AI Agent stack, it brings the power of GUI Agent and Vision into your terminal, computer, browser and product. &lt;br /&gt; &lt;br /&gt; It primarily ships with a &lt;a href="https://agent-tars.com/guide/basic/cli.html" target="_blank"&gt;CLI&lt;/a&gt; and &lt;a href="https://agent-tars.com/guide/basic/web-ui.html" target="_blank"&gt;Web UI&lt;/a&gt; for usage. It aims to provide a workflow that is closer to human-like task completion through cutting-edge multimodal LLMs and seamless integration with various real-world &lt;a href="https://agent-tars.com/guide/basic/mcp.html" target="_blank"&gt;MCP&lt;/a&gt; tools. &lt;/td&gt; 
   &lt;td align="left"&gt; &lt;b&gt;UI-TARS Desktop&lt;/b&gt; is a desktop application that provides a native GUI Agent based on the &lt;a href="https://github.com/bytedance/UI-TARS" target="_blank"&gt;UI-TARS&lt;/a&gt; model. &lt;br /&gt; &lt;br /&gt; It primarily ships a &lt;a href="https://github.com/bytedance/UI-TARS-desktop/raw/main/docs/quick-start.md#get-model-and-run-local-operator" target="_blank"&gt;local&lt;/a&gt; and &lt;a href="https://github.com/bytedance/UI-TARS-desktop/raw/main/docs/quick-start.md#run-remote-operator" target="_blank"&gt;remote&lt;/a&gt; computer as well as browser operators. &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;!-- START doctoc generated TOC please keep comment here to allow auto update --&gt; 
&lt;!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE --&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/bytedance/UI-TARS-desktop/main/#news"&gt;News&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/bytedance/UI-TARS-desktop/main/#agent-tars"&gt;Agent TARS&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/bytedance/UI-TARS-desktop/main/#showcase"&gt;Showcase&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/bytedance/UI-TARS-desktop/main/#core-features"&gt;Core Features&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/bytedance/UI-TARS-desktop/main/#quick-start"&gt;Quick Start&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/bytedance/UI-TARS-desktop/main/#documentation"&gt;Documentation&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/bytedance/UI-TARS-desktop/main/#ui-tars-desktop"&gt;UI-TARS Desktop&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/bytedance/UI-TARS-desktop/main/#showcase-1"&gt;Showcase&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/bytedance/UI-TARS-desktop/main/#features"&gt;Features&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/bytedance/UI-TARS-desktop/main/#quick-start-1"&gt;Quick Start&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/bytedance/UI-TARS-desktop/main/#contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/bytedance/UI-TARS-desktop/main/#license"&gt;License&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/bytedance/UI-TARS-desktop/main/#citation"&gt;Citation&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- END doctoc generated TOC please keep comment here to allow auto update --&gt; 
&lt;h2&gt;News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;[2025-11-05]&lt;/strong&gt; üéâ We're excited to announce the release of &lt;a href="https://github.com/bytedance/UI-TARS-desktop/releases/tag/v0.3.0"&gt;Agent TARS CLI v0.3.0&lt;/a&gt;! This version brings streaming support for multiple tools (shell commands, multi-file structured display), runtime settings with timing statistics for tool calls and deep thinking, Event Stream Viewer for data flow tracking and debugging. Additionally, it features exclusive support for &lt;a href="https://github.com/agent-infra/sandbox"&gt;AIO agent Sandbox&lt;/a&gt; as isolated all-in-one tools execution environment.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025-06-25]&lt;/strong&gt; We released a Agent TARS Beta and Agent TARS CLI - &lt;a href="https://agent-tars.com/blog/2025-06-25-introducing-agent-tars-beta.html"&gt;Introducing Agent TARS Beta&lt;/a&gt;, a multimodal AI agent that aims to explore a work form that is closer to human-like task completion through rich multimodal capabilities (such as GUI Agent, Vision) and seamless integration with various real-world tools.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025-06-12]&lt;/strong&gt; - üéÅ We are thrilled to announce the release of UI-TARS Desktop v0.2.0! This update introduces two powerful new features: &lt;strong&gt;Remote Computer Operator&lt;/strong&gt; and &lt;strong&gt;Remote Browser Operator&lt;/strong&gt;‚Äîboth completely free. No configuration required: simply click to remotely control any computer or browser, and experience a new level of convenience and intelligence.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025-04-17]&lt;/strong&gt; - üéâ We're thrilled to announce the release of new UI-TARS Desktop application v0.1.0, featuring a redesigned Agent UI. The application enhances the computer using experience, introduces new browser operation features, and supports &lt;a href="https://seed-tars.com/1.5"&gt;the advanced UI-TARS-1.5 model&lt;/a&gt; for improved performance and precise control.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025-02-20]&lt;/strong&gt; - üì¶ Introduced &lt;a href="https://raw.githubusercontent.com/bytedance/UI-TARS-desktop/main/docs/sdk.md"&gt;UI TARS SDK&lt;/a&gt;, is a powerful cross-platform toolkit for building GUI automation agents.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025-01-23]&lt;/strong&gt; - üöÄ We updated the &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/bytedance/UI-TARS-desktop/main/docs/deployment.md#cloud-deployment"&gt;Cloud Deployment&lt;/a&gt;&lt;/strong&gt; section in the ‰∏≠ÊñáÁâà: &lt;a href="https://bytedance.sg.larkoffice.com/docx/TCcudYwyIox5vyxiSDLlgIsTgWf#U94rdCxzBoJMLex38NPlHL21gNb"&gt;GUIÊ®°ÂûãÈÉ®ÁΩ≤ÊïôÁ®ã&lt;/a&gt; with new information related to the ModelScope platform. You can now use the ModelScope platform for deployment.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;h2&gt;Agent TARS&lt;/h2&gt; 
&lt;p&gt; &lt;a href="https://npmjs.com/package/@agent-tars/cli?activeTab=readme"&gt;&lt;img src="https://img.shields.io/npm/v/@agent-tars/cli?style=for-the-badge&amp;amp;colorA=1a1a2e&amp;amp;colorB=3B82F6&amp;amp;logo=npm&amp;amp;logoColor=white" alt="npm version" /&gt;&lt;/a&gt; &lt;a href="https://npmcharts.com/compare/@agent-tars/cli?minimal=true"&gt;&lt;img src="https://img.shields.io/npm/dm/@agent-tars/cli.svg?style=for-the-badge&amp;amp;colorA=1a1a2e&amp;amp;colorB=0EA5E9&amp;amp;logo=npm&amp;amp;logoColor=white" alt="downloads" /&gt;&lt;/a&gt; &lt;a href="https://nodejs.org/en/about/previous-releases"&gt;&lt;img src="https://img.shields.io/node/v/@agent-tars/cli.svg?style=for-the-badge&amp;amp;colorA=1a1a2e&amp;amp;colorB=06B6D4&amp;amp;logo=node.js&amp;amp;logoColor=white" alt="node version" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/HnKcSBgTVx"&gt;&lt;img src="https://img.shields.io/badge/Discord-Join%20Community-5865F2?style=for-the-badge&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Discord Community" /&gt;&lt;/a&gt; &lt;a href="https://twitter.com/agent_tars"&gt;&lt;img src="https://img.shields.io/badge/Twitter-Follow%20%40agent__tars-1DA1F2?style=for-the-badge&amp;amp;logo=twitter&amp;amp;logoColor=white" alt="Official Twitter" /&gt;&lt;/a&gt; &lt;a href="https://applink.larkoffice.com/client/chat/chatter/add_by_link?link_token=deen76f4-ea3c-4964-93a3-78f126f39651"&gt;&lt;img src="https://img.shields.io/badge/È£û‰π¶Áæ§-Âä†ÂÖ•‰∫§ÊµÅÁæ§-00D4AA?style=for-the-badge&amp;amp;logo=lark&amp;amp;logoColor=white" alt="È£û‰π¶‰∫§ÊµÅÁæ§" /&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/bytedance/UI-TARS-desktop"&gt;&lt;img src="https://img.shields.io/badge/DeepWiki-Ask%20AI-8B5CF6?style=for-the-badge&amp;amp;logo=gitbook&amp;amp;logoColor=white" alt="Ask DeepWiki" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;&lt;b&gt;Agent TARS&lt;/b&gt; is a general multimodal AI Agent stack, it brings the power of GUI Agent and Vision into your terminal, computer, browser and product. &lt;br /&gt; &lt;br /&gt; It primarily ships with a &lt;a href="https://agent-tars.com/guide/basic/cli.html" target="_blank"&gt;CLI&lt;/a&gt; and &lt;a href="https://agent-tars.com/guide/basic/web-ui.html" target="_blank"&gt;Web UI&lt;/a&gt; for usage. It aims to provide a workflow that is closer to human-like task completion through cutting-edge multimodal LLMs and seamless integration with various real-world &lt;a href="https://agent-tars.com/guide/basic/mcp.html" target="_blank"&gt;MCP&lt;/a&gt; tools.&lt;/p&gt; 
&lt;h3&gt;Showcase&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;Please help me book the earliest flight from San Jose to New York on September 1st and the last return flight on September 6th on Priceline
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/772b0eef-aef7-4ab9-8cb0-9611820539d8"&gt;https://github.com/user-attachments/assets/772b0eef-aef7-4ab9-8cb0-9611820539d8&lt;/a&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th width="50%" align="center"&gt;Booking Hotel&lt;/th&gt; 
   &lt;th width="50%" align="center"&gt;Generate Chart with extra MCP Servers&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt; 
    &lt;video src="https://github.com/user-attachments/assets/c9489936-afdc-4d12-adda-d4b90d2a869d" width="50%"&gt;&lt;/video&gt; &lt;/td&gt; 
   &lt;td align="center"&gt; 
    &lt;video src="https://github.com/user-attachments/assets/a9fd72d0-01bb-4233-aa27-ca95194bbce9" width="50%"&gt;&lt;/video&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt; &lt;b&gt;Instruction:&lt;/b&gt; &lt;i&gt;I am in Los Angeles from September 1st to September 6th, with a budget of $5,000. Please help me book a Ritz-Carlton hotel closest to the airport on booking.com and compile a transportation guide for me&lt;/i&gt; &lt;/td&gt; 
   &lt;td align="left"&gt; &lt;b&gt;Instruction:&lt;/b&gt; &lt;i&gt;Draw me a chart of Hangzhou's weather for one month&lt;/i&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;For more use cases, please check out &lt;a href="https://github.com/bytedance/UI-TARS-desktop/issues/842"&gt;#842&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Core Features&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;üñ±Ô∏è &lt;strong&gt;One-Click Out-of-the-box CLI&lt;/strong&gt; - Supports both &lt;strong&gt;headful&lt;/strong&gt; &lt;a href="https://agent-tars.com/guide/basic/web-ui.html"&gt;Web UI&lt;/a&gt; and &lt;strong&gt;headless&lt;/strong&gt; &lt;a href="https://agent-tars.com/guide/advanced/server.html"&gt;server&lt;/a&gt;) &lt;a href="https://agent-tars.com/guide/basic/cli.html"&gt;execution&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;üåê &lt;strong&gt;Hybrid Browser Agent&lt;/strong&gt; - Control browsers using &lt;a href="https://agent-tars.com/guide/basic/browser.html#visual-grounding"&gt;GUI Agent&lt;/a&gt;, &lt;a href="https://agent-tars.com/guide/basic/browser.html#dom"&gt;DOM&lt;/a&gt;, or a hybrid strategy.&lt;/li&gt; 
 &lt;li&gt;üîÑ &lt;strong&gt;Event Stream&lt;/strong&gt; - Protocol-driven Event Stream drives &lt;a href="https://agent-tars.com/beta#context-engineering"&gt;Context Engineering&lt;/a&gt; and &lt;a href="https://agent-tars.com/blog/2025-06-25-introducing-agent-tars-beta.html#easy-to-build-applications"&gt;Agent UI&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;üß∞ &lt;strong&gt;MCP Integration&lt;/strong&gt; - The kernel is built on MCP and also supports mounting &lt;a href="https://agent-tars.com/guide/basic/mcp.html"&gt;MCP Servers&lt;/a&gt; to connect to real-world tools.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Quick Start&lt;/h3&gt; 
&lt;img alt="Agent TARS CLI" src="https://agent-tars.com/agent-tars-cli.png" /&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Launch with `npx`.
npx @agent-tars/cli@latest

# Install globally, required Node.js &amp;gt;= 22
npm install @agent-tars/cli@latest -g

# Run with your preferred model provider
agent-tars --provider volcengine --model doubao-1-5-thinking-vision-pro-250428 --apiKey your-api-key
agent-tars --provider anthropic --model claude-3-7-sonnet-latest --apiKey your-api-key
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Visit the comprehensive &lt;a href="https://agent-tars.com/guide/get-started/quick-start.html"&gt;Quick Start&lt;/a&gt; guide for detailed setup instructions.&lt;/p&gt; 
&lt;h3&gt;Documentation&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;üåü &lt;strong&gt;Explore Agent TARS Universe&lt;/strong&gt; üåü&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th width="20%" align="center"&gt;Category&lt;/th&gt; 
   &lt;th width="30%" align="center"&gt;Resource Link&lt;/th&gt; 
   &lt;th width="50%" align="left"&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;üè† &lt;strong&gt;Central Hub&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt; &lt;a href="https://agent-tars.com"&gt; &lt;img src="https://img.shields.io/badge/Visit-Website-4F46E5?style=for-the-badge&amp;amp;logo=globe&amp;amp;logoColor=white" alt="Website" /&gt; &lt;/a&gt; &lt;/td&gt; 
   &lt;td align="left"&gt;Your gateway to Agent TARS ecosystem&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;üìö &lt;strong&gt;Quick Start&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt; &lt;a href="https://agent-tars.com/guide/get-started/quick-start.html"&gt; &lt;img src="https://img.shields.io/badge/Get-Started-06B6D4?style=for-the-badge&amp;amp;logo=rocket&amp;amp;logoColor=white" alt="Quick Start" /&gt; &lt;/a&gt; &lt;/td&gt; 
   &lt;td align="left"&gt;Zero to hero in 5 minutes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;üöÄ &lt;strong&gt;What's New&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt; &lt;a href="https://agent-tars.com/beta"&gt; &lt;img src="https://img.shields.io/badge/Read-Blog-F59E0B?style=for-the-badge&amp;amp;logo=rss&amp;amp;logoColor=white" alt="Blog" /&gt; &lt;/a&gt; &lt;/td&gt; 
   &lt;td align="left"&gt;Discover cutting-edge features &amp;amp; vision&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;üõ†Ô∏è &lt;strong&gt;Developer Zone&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt; &lt;a href="https://agent-tars.com/guide/get-started/introduction.html"&gt; &lt;img src="https://img.shields.io/badge/View-Docs-10B981?style=for-the-badge&amp;amp;logo=gitbook&amp;amp;logoColor=white" alt="Docs" /&gt; &lt;/a&gt; &lt;/td&gt; 
   &lt;td align="left"&gt;Master every command &amp;amp; features&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;üéØ &lt;strong&gt;Showcase&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt; &lt;a href="https://github.com/bytedance/UI-TARS-desktop/issues/842"&gt; &lt;img src="https://img.shields.io/badge/View-Examples-8B5CF6?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white" alt="Examples" /&gt; &lt;/a&gt; &lt;/td&gt; 
   &lt;td align="left"&gt;View use cases built by the official and community&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;üîß &lt;strong&gt;Reference&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt; &lt;a href="https://agent-tars.com/api/"&gt; &lt;img src="https://img.shields.io/badge/API-Reference-EF4444?style=for-the-badge&amp;amp;logo=book&amp;amp;logoColor=white" alt="API" /&gt; &lt;/a&gt; &lt;/td&gt; 
   &lt;td align="left"&gt;Complete technical reference&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;h2&gt;UI-TARS Desktop&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;img alt="UI-TARS" width="260" src="https://raw.githubusercontent.com/bytedance/UI-TARS-desktop/main/apps/ui-tars/resources/icon.png" /&gt; &lt;/p&gt; 
&lt;p&gt;UI-TARS Desktop is a native GUI agent for your local computer, driven by &lt;a href="https://github.com/bytedance/UI-TARS"&gt;UI-TARS&lt;/a&gt; and Seed-1.5-VL/1.6 series models.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &amp;nbsp;&amp;nbsp; üìë &lt;a href="https://arxiv.org/abs/2501.12326"&gt;Paper&lt;/a&gt; &amp;nbsp;&amp;nbsp; | ü§ó &lt;a href="https://huggingface.co/ByteDance-Seed/UI-TARS-1.5-7B"&gt;Hugging Face Models&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ü´® &lt;a href="https://discord.gg/pTXwYVjfcs"&gt;Discord&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ü§ñ &lt;a href="https://www.modelscope.cn/collections/UI-TARS-bccb56fa1ef640"&gt;ModelScope&lt;/a&gt;&amp;nbsp;&amp;nbsp; &lt;br /&gt; üñ•Ô∏è Desktop Application &amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp; üëì &lt;a href="https://github.com/web-infra-dev/midscene"&gt;Midscene (use in browser)&lt;/a&gt; &amp;nbsp;&amp;nbsp; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h3&gt;Showcase&lt;/h3&gt; 
&lt;!-- // FIXME: Choose only two demo, one local computer and one remote computer showcase. --&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;Instruction&lt;/th&gt; 
   &lt;th align="center"&gt;Local Operator&lt;/th&gt; 
   &lt;th align="center"&gt;Remote Operator&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Please help me open the autosave feature of VS Code and delay AutoSave operations for 500 milliseconds in the VS Code setting.&lt;/td&gt; 
   &lt;td align="center"&gt;
    &lt;video src="https://github.com/user-attachments/assets/e0914ce9-ad33-494b-bdec-0c25c1b01a27" height="300"&gt;&lt;/video&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;
    &lt;video src="https://github.com/user-attachments/assets/01e49b69-7070-46c8-b3e3-2aaaaec71800" height="300"&gt;&lt;/video&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Could you help me check the latest open issue of the UI-TARS-Desktop project on GitHub?&lt;/td&gt; 
   &lt;td align="center"&gt;
    &lt;video src="https://github.com/user-attachments/assets/3d159f54-d24a-4268-96c0-e149607e9199" height="300"&gt;&lt;/video&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;
    &lt;video src="https://github.com/user-attachments/assets/072fb72d-7394-4bfa-95f5-4736e29f7e58" height="300"&gt;&lt;/video&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Features&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ü§ñ Natural language control powered by Vision-Language Model&lt;/li&gt; 
 &lt;li&gt;üñ•Ô∏è Screenshot and visual recognition support&lt;/li&gt; 
 &lt;li&gt;üéØ Precise mouse and keyboard control&lt;/li&gt; 
 &lt;li&gt;üíª Cross-platform support (Windows/MacOS/Browser)&lt;/li&gt; 
 &lt;li&gt;üîÑ Real-time feedback and status display&lt;/li&gt; 
 &lt;li&gt;üîê Private and secure - fully local processing&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Quick Start&lt;/h3&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/bytedance/UI-TARS-desktop/main/docs/quick-start.md"&gt;Quick Start&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/bytedance/UI-TARS-desktop/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the Apache License 2.0.&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you find our paper and code useful in your research, please consider giving a star &lt;span&gt;‚≠ê&lt;/span&gt; and citation &lt;span&gt;üìù&lt;/span&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-BibTeX"&gt;@article{qin2025ui,
  title={UI-TARS: Pioneering Automated GUI Interaction with Native Agents},
  author={Qin, Yujia and Ye, Yining and Fang, Junjie and Wang, Haoming and Liang, Shihao and Tian, Shizuo and Zhang, Junda and Li, Jiahao and Li, Yunxin and Huang, Shijue and others},
  journal={arXiv preprint arXiv:2501.12326},
  year={2025}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>frankbria/ralph-claude-code</title>
      <link>https://github.com/frankbria/ralph-claude-code</link>
      <description>&lt;p&gt;Autonomous AI development loop for Claude Code with intelligent exit detection&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Ralph for Claude Code&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/frankbria/ralph-claude-code/actions/workflows/test.yml"&gt;&lt;img src="https://github.com/frankbria/ralph-claude-code/actions/workflows/test.yml/badge.svg?sanitize=true" alt="CI" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/frankbria/ralph-claude-code/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/License-MIT-blue.svg?sanitize=true" alt="License: MIT" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/badge/version-0.9.9-blue" alt="Version" /&gt; &lt;img src="https://img.shields.io/badge/tests-308%20passing-green" alt="Tests" /&gt; &lt;a href="https://github.com/frankbria/ralph-claude-code/issues"&gt;&lt;img src="https://img.shields.io/github/issues/frankbria/ralph-claude-code" alt="GitHub Issues" /&gt;&lt;/a&gt; &lt;a href="https://github.com/hesreallyhim/awesome-claude-code"&gt;&lt;img src="https://awesome.re/mentioned-badge.svg?sanitize=true" alt="Mentioned in Awesome Claude Code" /&gt;&lt;/a&gt; &lt;a href="https://x.com/FrankBria18044"&gt;&lt;img src="https://img.shields.io/twitter/follow/FrankBria18044?style=social" alt="Follow on X" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Autonomous AI development loop with intelligent exit detection and rate limiting&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Ralph is an implementation of the Geoffrey Huntley's technique for Claude Code that enables continuous autonomous development cycles he named after &lt;a href="https://ghuntley.com/ralph/"&gt;Ralph Wiggum&lt;/a&gt;. It enables continuous autonomous development cycles where Claude Code iteratively improves your project until completion, with built-in safeguards to prevent infinite loops and API overuse.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Install once, use everywhere&lt;/strong&gt; - Ralph becomes a global command available in any directory.&lt;/p&gt; 
&lt;h2&gt;Project Status&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Version&lt;/strong&gt;: v0.9.9 - Active Development &lt;strong&gt;Core Features&lt;/strong&gt;: Working and tested &lt;strong&gt;Test Coverage&lt;/strong&gt;: 308 tests, 100% pass rate&lt;/p&gt; 
&lt;h3&gt;What's Working Now&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Autonomous development loops with intelligent exit detection&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Dual-condition exit gate&lt;/strong&gt;: Requires BOTH completion indicators AND explicit EXIT_SIGNAL&lt;/li&gt; 
 &lt;li&gt;Rate limiting with hourly reset (100 calls/hour, configurable)&lt;/li&gt; 
 &lt;li&gt;Circuit breaker with advanced error detection (prevents runaway loops)&lt;/li&gt; 
 &lt;li&gt;Response analyzer with semantic understanding and two-stage error filtering&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;JSON output format support with automatic fallback to text parsing&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Session continuity with &lt;code&gt;--continue&lt;/code&gt; flag for context preservation&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Session expiration with configurable timeout (default: 24 hours)&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Modern CLI flags: &lt;code&gt;--output-format&lt;/code&gt;, &lt;code&gt;--allowed-tools&lt;/code&gt;, &lt;code&gt;--no-continue&lt;/code&gt;&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Multi-line error matching for accurate stuck loop detection&lt;/li&gt; 
 &lt;li&gt;5-hour API limit handling with user prompts&lt;/li&gt; 
 &lt;li&gt;tmux integration for live monitoring&lt;/li&gt; 
 &lt;li&gt;PRD import functionality&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;CI/CD pipeline with GitHub Actions&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Dedicated uninstall script for clean removal&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;308 passing tests across 11 test files&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Recent Improvements&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;v0.9.9 - EXIT_SIGNAL Gate &amp;amp; Uninstall Script&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Fixed premature exit bug: completion indicators now require Claude's explicit &lt;code&gt;EXIT_SIGNAL: true&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Added dual-condition check preventing exits when Claude reports work in progress&lt;/li&gt; 
 &lt;li&gt;Added &lt;code&gt;response_analyzer.sh&lt;/code&gt; fix to respect explicit EXIT_SIGNAL over heuristics&lt;/li&gt; 
 &lt;li&gt;Added dedicated &lt;code&gt;uninstall.sh&lt;/code&gt; script for clean Ralph removal&lt;/li&gt; 
 &lt;li&gt;Session expiration with configurable timeout (default: 24 hours)&lt;/li&gt; 
 &lt;li&gt;Added 32 new tests for EXIT_SIGNAL behavior and session expiration&lt;/li&gt; 
 &lt;li&gt;Test count: 308 (up from 276)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;v0.9.8 - Modern CLI for PRD Import&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Modernized &lt;code&gt;ralph_import.sh&lt;/code&gt; to use Claude Code CLI JSON output format&lt;/li&gt; 
 &lt;li&gt;JSON output format support with &lt;code&gt;--output-format json&lt;/code&gt; for structured responses&lt;/li&gt; 
 &lt;li&gt;Enhanced error handling with structured JSON error messages&lt;/li&gt; 
 &lt;li&gt;Improved file verification with JSON-derived status information&lt;/li&gt; 
 &lt;li&gt;Backward compatibility with older CLI versions (automatic text fallback)&lt;/li&gt; 
 &lt;li&gt;Added 11 new tests for modern CLI features&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;v0.9.7 - Session Lifecycle Management&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Complete session lifecycle management with automatic reset triggers&lt;/li&gt; 
 &lt;li&gt;Session auto-reset on: circuit breaker open, manual interrupt, project completion&lt;/li&gt; 
 &lt;li&gt;Added &lt;code&gt;--reset-session&lt;/code&gt; CLI flag for manual session reset&lt;/li&gt; 
 &lt;li&gt;Session history tracking (last 50 transitions) for debugging&lt;/li&gt; 
 &lt;li&gt;Added 26 new tests for session continuity features&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;v0.9.6 - JSON Output &amp;amp; Session Management&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Extended &lt;code&gt;parse_json_response()&lt;/code&gt; to support Claude Code CLI JSON format&lt;/li&gt; 
 &lt;li&gt;Added session management functions: &lt;code&gt;store_session_id()&lt;/code&gt;, &lt;code&gt;get_last_session_id()&lt;/code&gt;, &lt;code&gt;should_resume_session()&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Cross-platform epoch time utilities in date_utils.sh&lt;/li&gt; 
 &lt;li&gt;Added 16 new tests covering Claude CLI format and session management&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;v0.9.5 - PRD Import Tests&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Added 22 comprehensive tests for &lt;code&gt;ralph_import.sh&lt;/code&gt; PRD conversion script&lt;/li&gt; 
 &lt;li&gt;Tests cover: file format support, output file creation, project naming, error handling&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;v0.9.4 - Project Setup Tests&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Added 36 comprehensive tests for &lt;code&gt;setup.sh&lt;/code&gt; project initialization script&lt;/li&gt; 
 &lt;li&gt;Tests cover: directory creation, template copying, git initialization&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;v0.9.3 - Installation Tests&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Added 14 comprehensive tests for &lt;code&gt;install.sh&lt;/code&gt; global installation script&lt;/li&gt; 
 &lt;li&gt;Tests cover: directory creation, command installation, dependency detection&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;v0.9.2 - Prompt File Fix&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Fixed critical bug: replaced non-existent &lt;code&gt;--prompt-file&lt;/code&gt; CLI flag with &lt;code&gt;-p&lt;/code&gt; flag&lt;/li&gt; 
 &lt;li&gt;Modern CLI mode now correctly passes prompt content via &lt;code&gt;-p "$(cat file)"&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Added error handling for missing prompt files in &lt;code&gt;build_claude_command()&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;v0.9.1 - Modern CLI Commands (Phase 1.1)&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;JSON output format support with &lt;code&gt;--output-format json&lt;/code&gt; (default)&lt;/li&gt; 
 &lt;li&gt;Session continuity using &lt;code&gt;--continue&lt;/code&gt; flag for cross-loop context&lt;/li&gt; 
 &lt;li&gt;Tool permissions via &lt;code&gt;--allowed-tools&lt;/code&gt; flag&lt;/li&gt; 
 &lt;li&gt;CI/CD pipeline with kcov coverage reporting&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;v0.9.0 - Circuit Breaker Enhancements&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Fixed multi-line error matching in stuck loop detection&lt;/li&gt; 
 &lt;li&gt;Eliminated JSON field false positives (e.g., &lt;code&gt;"is_error": false&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Added two-stage error filtering for accurate detection&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;In Progress&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Expanding test coverage&lt;/li&gt; 
 &lt;li&gt;Log rotation functionality&lt;/li&gt; 
 &lt;li&gt;Dry-run mode&lt;/li&gt; 
 &lt;li&gt;Configuration file support (.ralphrc)&lt;/li&gt; 
 &lt;li&gt;Metrics and analytics tracking&lt;/li&gt; 
 &lt;li&gt;Desktop notifications&lt;/li&gt; 
 &lt;li&gt;Git backup and rollback system&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Timeline to v1.0&lt;/strong&gt;: ~4 weeks | &lt;a href="https://raw.githubusercontent.com/frankbria/ralph-claude-code/main/IMPLEMENTATION_PLAN.md"&gt;Full roadmap&lt;/a&gt; | &lt;strong&gt;Contributions welcome!&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Autonomous Development Loop&lt;/strong&gt; - Continuously executes Claude Code with your project requirements&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Intelligent Exit Detection&lt;/strong&gt; - Dual-condition check requiring BOTH completion indicators AND explicit EXIT_SIGNAL&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Session Continuity&lt;/strong&gt; - Preserves context across loop iterations with automatic session management&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Session Expiration&lt;/strong&gt; - Configurable timeout (default: 24 hours) with automatic session reset&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Rate Limiting&lt;/strong&gt; - Built-in API call management with hourly limits and countdown timers&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;5-Hour API Limit Handling&lt;/strong&gt; - Detects Claude's 5-hour usage limit and offers wait/exit options&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Live Monitoring&lt;/strong&gt; - Real-time dashboard showing loop status, progress, and logs&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Task Management&lt;/strong&gt; - Structured approach with prioritized task lists and progress tracking&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Project Templates&lt;/strong&gt; - Quick setup for new projects with best-practice structure&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Comprehensive Logging&lt;/strong&gt; - Detailed execution logs with timestamps and status tracking&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Configurable Timeouts&lt;/strong&gt; - Set execution timeout for Claude Code operations (1-120 minutes)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Verbose Progress Mode&lt;/strong&gt; - Optional detailed progress updates during execution&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Response Analyzer&lt;/strong&gt; - AI-powered analysis of Claude Code responses with semantic understanding&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Circuit Breaker&lt;/strong&gt; - Advanced error detection with two-stage filtering, multi-line error matching, and automatic recovery&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;CI/CD Integration&lt;/strong&gt; - GitHub Actions workflow with automated testing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Clean Uninstall&lt;/strong&gt; - Dedicated uninstall script for complete removal&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;p&gt;Ralph has two phases: &lt;strong&gt;one-time installation&lt;/strong&gt; and &lt;strong&gt;per-project setup&lt;/strong&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;INSTALL ONCE              USE MANY TIMES
+-----------------+          +----------------------+
| ./install.sh    |    -&amp;gt;    | ralph-setup project1 |
|                 |          | ralph-setup project2 |
| Adds global     |          | ralph-setup project3 |
| commands        |          | ...                  |
+-----------------+          +----------------------+
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Phase 1: Install Ralph (One Time Only)&lt;/h3&gt; 
&lt;p&gt;Install Ralph globally on your system:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/frankbria/ralph-claude-code.git
cd ralph-claude-code
./install.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This adds &lt;code&gt;ralph&lt;/code&gt;, &lt;code&gt;ralph-monitor&lt;/code&gt;, and &lt;code&gt;ralph-setup&lt;/code&gt; commands to your PATH.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: You only need to do this once per system. After installation, you can delete the cloned repository if desired.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Phase 2: Initialize New Projects (Per Project)&lt;/h3&gt; 
&lt;p&gt;For each new project you want Ralph to work on:&lt;/p&gt; 
&lt;h4&gt;Option A: Import Existing PRD/Specifications&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Convert existing PRD/specs to Ralph format (recommended)
ralph-import my-requirements.md my-project
cd my-project

# Review and adjust the generated files:
# - PROMPT.md (Ralph instructions)
# - @fix_plan.md (task priorities)
# - specs/requirements.md (technical specs)

# Start autonomous development
ralph --monitor
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Option B: Manual Project Setup&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Create blank Ralph project
ralph-setup my-awesome-project
cd my-awesome-project

# Configure your project requirements manually
# Edit PROMPT.md with your project goals
# Edit specs/ with detailed specifications
# Edit @fix_plan.md with initial priorities

# Start autonomous development
ralph --monitor
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Ongoing Usage (After Setup)&lt;/h3&gt; 
&lt;p&gt;Once Ralph is installed and your project is initialized:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Navigate to any Ralph project and run:
ralph --monitor              # Integrated tmux monitoring (recommended)

# Or use separate terminals:
ralph                        # Terminal 1: Ralph loop
ralph-monitor               # Terminal 2: Live monitor dashboard
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Uninstalling Ralph&lt;/h3&gt; 
&lt;p&gt;To completely remove Ralph from your system:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Run the uninstall script
./uninstall.sh

# Or if you deleted the repo, download and run:
curl -sL https://raw.githubusercontent.com/frankbria/ralph-claude-code/main/uninstall.sh | bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;How It Works&lt;/h2&gt; 
&lt;p&gt;Ralph operates on a simple but powerful cycle:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Read Instructions&lt;/strong&gt; - Loads &lt;code&gt;PROMPT.md&lt;/code&gt; with your project requirements&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Execute Claude Code&lt;/strong&gt; - Runs Claude Code with current context and priorities&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Track Progress&lt;/strong&gt; - Updates task lists and logs execution results&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Evaluate Completion&lt;/strong&gt; - Checks for exit conditions and project completion signals&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Repeat&lt;/strong&gt; - Continues until project is complete or limits are reached&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Intelligent Exit Detection&lt;/h3&gt; 
&lt;p&gt;Ralph uses a &lt;strong&gt;dual-condition check&lt;/strong&gt; to prevent premature exits during productive iterations:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Exit requires BOTH conditions:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;code&gt;completion_indicators &amp;gt;= 2&lt;/code&gt; (heuristic detection from natural language patterns)&lt;/li&gt; 
 &lt;li&gt;Claude's explicit &lt;code&gt;EXIT_SIGNAL: true&lt;/code&gt; in the RALPH_STATUS block&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Example behavior:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Loop 5: Claude outputs "Phase complete, moving to next feature"
        ‚Üí completion_indicators: 3 (high confidence from patterns)
        ‚Üí EXIT_SIGNAL: false (Claude says more work needed)
        ‚Üí Result: CONTINUE (respects Claude's explicit intent)

Loop 8: Claude outputs "All tasks complete, project ready"
        ‚Üí completion_indicators: 4
        ‚Üí EXIT_SIGNAL: true (Claude confirms done)
        ‚Üí Result: EXIT with "project_complete"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Other exit conditions:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;All tasks in &lt;code&gt;@fix_plan.md&lt;/code&gt; marked complete&lt;/li&gt; 
 &lt;li&gt;Multiple consecutive "done" signals from Claude Code&lt;/li&gt; 
 &lt;li&gt;Too many test-focused loops (indicating feature completeness)&lt;/li&gt; 
 &lt;li&gt;Claude API 5-hour usage limit reached (with user prompt to wait or exit)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Importing Existing Requirements&lt;/h2&gt; 
&lt;p&gt;Ralph can convert existing PRDs, specifications, or requirement documents into the proper Ralph format using Claude Code.&lt;/p&gt; 
&lt;h3&gt;Supported Formats&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Markdown&lt;/strong&gt; (.md) - Product requirements, technical specs&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Text files&lt;/strong&gt; (.txt) - Plain text requirements&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;JSON&lt;/strong&gt; (.json) - Structured requirement data&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Word documents&lt;/strong&gt; (.docx) - Business requirements&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;PDFs&lt;/strong&gt; (.pdf) - Design documents, specifications&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Any text-based format&lt;/strong&gt; - Ralph will intelligently parse the content&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Usage Examples&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Convert a markdown PRD
ralph-import product-requirements.md my-app

# Convert a text specification
ralph-import requirements.txt webapp

# Convert a JSON API spec
ralph-import api-spec.json backend-service

# Let Ralph auto-name the project from filename
ralph-import design-doc.pdf
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;What Gets Generated&lt;/h3&gt; 
&lt;p&gt;Ralph-import creates a complete project with:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;PROMPT.md&lt;/strong&gt; - Converted into Ralph development instructions&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;@fix_plan.md&lt;/strong&gt; - Requirements broken down into prioritized tasks&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;specs/requirements.md&lt;/strong&gt; - Technical specifications extracted from your document&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Standard Ralph structure&lt;/strong&gt; - All necessary directories and template files&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The conversion is intelligent and preserves your original requirements while making them actionable for autonomous development.&lt;/p&gt; 
&lt;h3&gt;Modern CLI Features (v0.9.8)&lt;/h3&gt; 
&lt;p&gt;Ralph-import uses modern Claude Code CLI features for improved reliability:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;JSON Output Format&lt;/strong&gt;: Structured responses enable precise parsing of conversion results&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Automatic Fallback&lt;/strong&gt;: Gracefully handles older CLI versions with text-based parsing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Enhanced Error Reporting&lt;/strong&gt;: Extracts specific error messages and codes from JSON responses&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Session Tracking&lt;/strong&gt;: Captures session IDs for potential continuation of interrupted conversions&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: These features require Claude Code CLI version 2.0.76 or later. Older versions will work with standard text output.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Configuration&lt;/h2&gt; 
&lt;h3&gt;Rate Limiting &amp;amp; Circuit Breaker&lt;/h3&gt; 
&lt;p&gt;Ralph includes intelligent rate limiting and circuit breaker functionality:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Default: 100 calls per hour
ralph --calls 50

# With integrated monitoring
ralph --monitor --calls 50

# Check current usage
ralph --status
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The circuit breaker automatically:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Detects API errors and rate limit issues with advanced two-stage filtering&lt;/li&gt; 
 &lt;li&gt;Opens circuit after 3 loops with no progress or 5 loops with same errors&lt;/li&gt; 
 &lt;li&gt;Eliminates false positives from JSON fields containing "error"&lt;/li&gt; 
 &lt;li&gt;Accurately detects stuck loops with multi-line error matching&lt;/li&gt; 
 &lt;li&gt;Gradually recovers with half-open monitoring state&lt;/li&gt; 
 &lt;li&gt;Provides detailed error tracking and logging with state history&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Claude API 5-Hour Limit&lt;/h3&gt; 
&lt;p&gt;When Claude's 5-hour usage limit is reached, Ralph:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Detects the limit error automatically&lt;/li&gt; 
 &lt;li&gt;Prompts you to choose: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Option 1&lt;/strong&gt;: Wait 60 minutes for the limit to reset (with countdown timer)&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Option 2&lt;/strong&gt;: Exit gracefully (or auto-exits after 30-second timeout)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Prevents endless retry loops that waste time&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Custom Prompts&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Use custom prompt file
ralph --prompt my_custom_instructions.md

# With integrated monitoring
ralph --monitor --prompt my_custom_instructions.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Execution Timeouts&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Set Claude Code execution timeout (default: 15 minutes)
ralph --timeout 30  # 30-minute timeout for complex tasks

# With monitoring and custom timeout
ralph --monitor --timeout 60  # 60-minute timeout

# Short timeout for quick iterations
ralph --verbose --timeout 5  # 5-minute timeout with progress
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Verbose Mode&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Enable detailed progress updates during execution
ralph --verbose

# Combine with other options
ralph --monitor --verbose --timeout 30
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Session Continuity&lt;/h3&gt; 
&lt;p&gt;Ralph maintains session context across loop iterations for improved coherence:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Sessions are enabled by default with --continue flag
ralph --monitor                 # Uses session continuity

# Start fresh without session context
ralph --no-continue             # Isolated iterations

# Reset session manually (clears context)
ralph --reset-session           # Clears current session

# Check session status
cat .ralph_session              # View current session file
cat .ralph_session_history      # View session transition history
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Session Auto-Reset Triggers:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Circuit breaker opens (stagnation detected)&lt;/li&gt; 
 &lt;li&gt;Manual interrupt (Ctrl+C / SIGINT)&lt;/li&gt; 
 &lt;li&gt;Project completion (graceful exit)&lt;/li&gt; 
 &lt;li&gt;Manual circuit breaker reset (&lt;code&gt;--reset-circuit&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Session expiration (default: 24 hours)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Sessions are persisted to &lt;code&gt;.ralph_session&lt;/code&gt; with a configurable expiration (default: 24 hours). The last 50 session transitions are logged to &lt;code&gt;.ralph_session_history&lt;/code&gt; for debugging.&lt;/p&gt; 
&lt;h3&gt;Exit Thresholds&lt;/h3&gt; 
&lt;p&gt;Modify these variables in &lt;code&gt;~/.ralph/ralph_loop.sh&lt;/code&gt;:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Exit Detection Thresholds:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;MAX_CONSECUTIVE_TEST_LOOPS=3     # Exit after 3 test-only loops
MAX_CONSECUTIVE_DONE_SIGNALS=2   # Exit after 2 "done" signals
TEST_PERCENTAGE_THRESHOLD=30     # Flag if 30%+ loops are test-only
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Circuit Breaker Thresholds:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;CB_NO_PROGRESS_THRESHOLD=3       # Open circuit after 3 loops with no file changes
CB_SAME_ERROR_THRESHOLD=5        # Open circuit after 5 loops with repeated errors
CB_OUTPUT_DECLINE_THRESHOLD=70   # Open circuit if output declines by &amp;gt;70%
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Completion Indicators with EXIT_SIGNAL Gate:&lt;/strong&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;completion_indicators&lt;/th&gt; 
   &lt;th&gt;EXIT_SIGNAL&lt;/th&gt; 
   &lt;th&gt;Result&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&amp;gt;= 2&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Exit&lt;/strong&gt; ("project_complete")&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&amp;gt;= 2&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Continue&lt;/strong&gt; (Claude still working)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&amp;gt;= 2&lt;/td&gt; 
   &lt;td&gt;missing&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Continue&lt;/strong&gt; (defaults to false)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&amp;lt; 2&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Continue&lt;/strong&gt; (threshold not met)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Project Structure&lt;/h2&gt; 
&lt;p&gt;Ralph creates a standardized structure for each project:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;my-project/
‚îú‚îÄ‚îÄ PROMPT.md           # Main development instructions for Ralph
‚îú‚îÄ‚îÄ @fix_plan.md        # Prioritized task list (@ prefix = Ralph control file)
‚îú‚îÄ‚îÄ @AGENT.md           # Build and run instructions
‚îú‚îÄ‚îÄ specs/              # Project specifications and requirements
‚îÇ   ‚îî‚îÄ‚îÄ stdlib/         # Standard library specifications
‚îú‚îÄ‚îÄ src/                # Source code implementation
‚îú‚îÄ‚îÄ examples/           # Usage examples and test cases
‚îú‚îÄ‚îÄ logs/               # Ralph execution logs
‚îî‚îÄ‚îÄ docs/generated/     # Auto-generated documentation
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Best Practices&lt;/h2&gt; 
&lt;h3&gt;Writing Effective Prompts&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Be Specific&lt;/strong&gt; - Clear requirements lead to better results&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Prioritize&lt;/strong&gt; - Use &lt;code&gt;@fix_plan.md&lt;/code&gt; to guide Ralph's focus&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Set Boundaries&lt;/strong&gt; - Define what's in/out of scope&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Include Examples&lt;/strong&gt; - Show expected inputs/outputs&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Project Specifications&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Place detailed requirements in &lt;code&gt;specs/&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Use &lt;code&gt;@fix_plan.md&lt;/code&gt; for prioritized task tracking&lt;/li&gt; 
 &lt;li&gt;Keep &lt;code&gt;@AGENT.md&lt;/code&gt; updated with build instructions&lt;/li&gt; 
 &lt;li&gt;Document key decisions and architecture&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Monitoring Progress&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Use &lt;code&gt;ralph-monitor&lt;/code&gt; for live status updates&lt;/li&gt; 
 &lt;li&gt;Check logs in &lt;code&gt;logs/&lt;/code&gt; for detailed execution history&lt;/li&gt; 
 &lt;li&gt;Monitor &lt;code&gt;status.json&lt;/code&gt; for programmatic access&lt;/li&gt; 
 &lt;li&gt;Watch for exit condition signals&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;System Requirements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Bash 4.0+&lt;/strong&gt; - For script execution&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Claude Code CLI&lt;/strong&gt; - &lt;code&gt;npm install -g @anthropic-ai/claude-code&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;tmux&lt;/strong&gt; - Terminal multiplexer for integrated monitoring (recommended)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;jq&lt;/strong&gt; - JSON processing for status tracking&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Git&lt;/strong&gt; - Version control (projects are initialized as git repos)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Standard Unix tools&lt;/strong&gt; - grep, date, etc.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Testing Requirements (Development)&lt;/h3&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/frankbria/ralph-claude-code/main/TESTING.md"&gt;TESTING.md&lt;/a&gt; for the comprehensive testing guide.&lt;/p&gt; 
&lt;p&gt;If you want to run the test suite:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install BATS testing framework
npm install -g bats bats-support bats-assert

# Run all tests (308 tests)
npm test

# Run specific test suites
bats tests/unit/test_rate_limiting.bats
bats tests/unit/test_exit_detection.bats
bats tests/unit/test_json_parsing.bats
bats tests/unit/test_cli_modern.bats
bats tests/unit/test_cli_parsing.bats
bats tests/unit/test_session_continuity.bats
bats tests/integration/test_loop_execution.bats
bats tests/integration/test_prd_import.bats
bats tests/integration/test_project_setup.bats
bats tests/integration/test_installation.bats

# Run error detection and circuit breaker tests
./tests/test_error_detection.sh
./tests/test_stuck_loop_detection.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Current test status:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;308 tests&lt;/strong&gt; across 11 test files&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;100% pass rate&lt;/strong&gt; (308/308 passing)&lt;/li&gt; 
 &lt;li&gt;Comprehensive unit and integration tests&lt;/li&gt; 
 &lt;li&gt;Specialized tests for JSON parsing, CLI flags, circuit breaker, EXIT_SIGNAL behavior, and installation workflows&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note on Coverage&lt;/strong&gt;: Bash code coverage measurement with kcov has fundamental limitations when tracing subprocess executions. Test pass rate (100%) is the quality gate. See &lt;a href="https://github.com/bats-core/bats-core/issues/15"&gt;bats-core#15&lt;/a&gt; for details.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Installing tmux&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Ubuntu/Debian
sudo apt-get install tmux

# macOS
brew install tmux

# CentOS/RHEL
sudo yum install tmux
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Monitoring and Debugging&lt;/h2&gt; 
&lt;h3&gt;Live Dashboard&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Integrated tmux monitoring (recommended)
ralph --monitor

# Manual monitoring in separate terminal
ralph-monitor
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Shows real-time:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Current loop count and status&lt;/li&gt; 
 &lt;li&gt;API calls used vs. limit&lt;/li&gt; 
 &lt;li&gt;Recent log entries&lt;/li&gt; 
 &lt;li&gt;Rate limit countdown&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;tmux Controls:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;Ctrl+B&lt;/code&gt; then &lt;code&gt;D&lt;/code&gt; - Detach from session (keeps Ralph running)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Ctrl+B&lt;/code&gt; then &lt;code&gt;‚Üê/‚Üí&lt;/code&gt; - Switch between panes&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;tmux list-sessions&lt;/code&gt; - View active sessions&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;tmux attach -t &amp;lt;session-name&amp;gt;&lt;/code&gt; - Reattach to session&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Status Checking&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# JSON status output
ralph --status

# Manual log inspection
tail -f logs/ralph.log
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Common Issues&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Rate Limits&lt;/strong&gt; - Ralph automatically waits and displays countdown&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;5-Hour API Limit&lt;/strong&gt; - Ralph detects and prompts for user action (wait or exit)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Stuck Loops&lt;/strong&gt; - Check &lt;code&gt;@fix_plan.md&lt;/code&gt; for unclear or conflicting tasks&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Early Exit&lt;/strong&gt; - Review exit thresholds if Ralph stops too soon&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Premature Exit&lt;/strong&gt; - Check if Claude is setting &lt;code&gt;EXIT_SIGNAL: false&lt;/code&gt; (Ralph now respects this)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Execution Timeouts&lt;/strong&gt; - Increase &lt;code&gt;--timeout&lt;/code&gt; value for complex operations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Missing Dependencies&lt;/strong&gt; - Ensure Claude Code CLI and tmux are installed&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;tmux Session Lost&lt;/strong&gt; - Use &lt;code&gt;tmux list-sessions&lt;/code&gt; and &lt;code&gt;tmux attach&lt;/code&gt; to reconnect&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Session Expired&lt;/strong&gt; - Sessions expire after 24 hours by default; use &lt;code&gt;--reset-session&lt;/code&gt; to start fresh&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Ralph is actively seeking contributors! We're working toward v1.0.0 with clear priorities and a detailed roadmap.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;See &lt;a href="https://raw.githubusercontent.com/frankbria/ralph-claude-code/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for the complete contributor guide&lt;/strong&gt; including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Getting started and setup instructions&lt;/li&gt; 
 &lt;li&gt;Development workflow and commit conventions&lt;/li&gt; 
 &lt;li&gt;Code style guidelines&lt;/li&gt; 
 &lt;li&gt;Testing requirements (100% pass rate mandatory)&lt;/li&gt; 
 &lt;li&gt;Pull request process and code review guidelines&lt;/li&gt; 
 &lt;li&gt;Quality standards and checklists&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Quick Start&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Fork and clone
git clone https://github.com/YOUR_USERNAME/ralph-claude-code.git
cd ralph-claude-code

# Install dependencies and run tests
npm install
npm test  # All 308 tests must pass
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Priority Contribution Areas&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Test Implementation&lt;/strong&gt; - Help expand test coverage&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Feature Development&lt;/strong&gt; - Log rotation, dry-run mode, config files, metrics&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Documentation&lt;/strong&gt; - Tutorials, troubleshooting guides, examples&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Real-World Testing&lt;/strong&gt; - Use Ralph, report bugs, share feedback&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Every contribution matters&lt;/strong&gt; - from fixing typos to implementing major features!&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the MIT License - see the &lt;a href="https://raw.githubusercontent.com/frankbria/ralph-claude-code/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;h2&gt;Acknowledgments&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Inspired by the &lt;a href="https://ghuntley.com/ralph/"&gt;Ralph technique&lt;/a&gt; created by Geoffrey Huntley&lt;/li&gt; 
 &lt;li&gt;Built for &lt;a href="https://claude.ai/code"&gt;Claude Code&lt;/a&gt; by Anthropic&lt;/li&gt; 
 &lt;li&gt;Community feedback and contributions&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Related Projects&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://claude.ai/code"&gt;Claude Code&lt;/a&gt; - The AI coding assistant that powers Ralph&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/paul-gauthier/aider"&gt;Aider&lt;/a&gt; - Original Ralph technique implementation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Command Reference&lt;/h2&gt; 
&lt;h3&gt;Installation Commands (Run Once)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;./install.sh              # Install Ralph globally
./uninstall.sh            # Remove Ralph from system (dedicated script)
./install.sh uninstall    # Alternative: Remove Ralph from system
./install.sh --help       # Show installation help
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Ralph Loop Options&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;ralph [OPTIONS]
  -h, --help              Show help message
  -c, --calls NUM         Set max calls per hour (default: 100)
  -p, --prompt FILE       Set prompt file (default: PROMPT.md)
  -s, --status            Show current status and exit
  -m, --monitor           Start with tmux session and live monitor
  -v, --verbose           Show detailed progress updates during execution
  -t, --timeout MIN       Set Claude Code execution timeout in minutes (1-120, default: 15)
  --output-format FORMAT  Set output format: json (default) or text
  --allowed-tools TOOLS   Set allowed Claude tools (default: Write,Bash(git *),Read)
  --no-continue           Disable session continuity (start fresh each loop)
  --reset-circuit         Reset the circuit breaker
  --circuit-status        Show circuit breaker status
  --reset-session         Reset session state manually
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Project Commands (Per Project)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;ralph-setup project-name     # Create new Ralph project
ralph-import prd.md project  # Convert PRD/specs to Ralph project
ralph --monitor              # Start with integrated monitoring
ralph --status               # Check current loop status
ralph --verbose              # Enable detailed progress updates
ralph --timeout 30           # Set 30-minute execution timeout
ralph --calls 50             # Limit to 50 API calls per hour
ralph --reset-session        # Reset session state manually
ralph-monitor                # Manual monitoring dashboard
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;tmux Session Management&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;tmux list-sessions        # View active Ralph sessions
tmux attach -t &amp;lt;name&amp;gt;     # Reattach to detached session
# Ctrl+B then D           # Detach from session (keeps running)
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Development Roadmap&lt;/h2&gt; 
&lt;p&gt;Ralph is under active development with a clear path to v1.0.0. See &lt;a href="https://raw.githubusercontent.com/frankbria/ralph-claude-code/main/IMPLEMENTATION_PLAN.md"&gt;IMPLEMENTATION_PLAN.md&lt;/a&gt; for the complete roadmap.&lt;/p&gt; 
&lt;h3&gt;Current Status: v0.9.9&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;What's Delivered:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Core loop functionality with intelligent exit detection&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Dual-condition exit gate&lt;/strong&gt; (completion indicators + EXIT_SIGNAL)&lt;/li&gt; 
 &lt;li&gt;Rate limiting (100 calls/hour) and circuit breaker pattern&lt;/li&gt; 
 &lt;li&gt;Response analyzer with semantic understanding&lt;/li&gt; 
 &lt;li&gt;308 comprehensive tests (100% pass rate)&lt;/li&gt; 
 &lt;li&gt;tmux integration and live monitoring&lt;/li&gt; 
 &lt;li&gt;PRD import functionality with modern CLI JSON parsing&lt;/li&gt; 
 &lt;li&gt;Installation system and project templates&lt;/li&gt; 
 &lt;li&gt;Modern CLI commands with JSON output support&lt;/li&gt; 
 &lt;li&gt;CI/CD pipeline with GitHub Actions&lt;/li&gt; 
 &lt;li&gt;Comprehensive installation test suite&lt;/li&gt; 
 &lt;li&gt;Session lifecycle management with auto-reset triggers&lt;/li&gt; 
 &lt;li&gt;Session expiration with configurable timeout&lt;/li&gt; 
 &lt;li&gt;Dedicated uninstall script&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Test Coverage Breakdown:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Unit Tests: 164 (CLI parsing, JSON, exit detection, rate limiting, session continuity)&lt;/li&gt; 
 &lt;li&gt;Integration Tests: 144 (loop execution, edge cases, installation, project setup, PRD import)&lt;/li&gt; 
 &lt;li&gt;Test Files: 11&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Path to v1.0.0 (~4 weeks)&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Enhanced Testing&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Installation and setup workflow tests&lt;/li&gt; 
 &lt;li&gt;tmux integration tests&lt;/li&gt; 
 &lt;li&gt;Monitor dashboard tests&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Core Features&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Log rotation functionality&lt;/li&gt; 
 &lt;li&gt;Dry-run mode&lt;/li&gt; 
 &lt;li&gt;Configuration file support - .ralphrc&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Advanced Features &amp;amp; Polish&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Metrics and analytics tracking&lt;/li&gt; 
 &lt;li&gt;Desktop notifications&lt;/li&gt; 
 &lt;li&gt;Git backup and rollback system&lt;/li&gt; 
 &lt;li&gt;End-to-end tests&lt;/li&gt; 
 &lt;li&gt;Final documentation and release prep&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/frankbria/ralph-claude-code/main/IMPLEMENTATION_STATUS.md"&gt;IMPLEMENTATION_STATUS.md&lt;/a&gt; for detailed progress tracking.&lt;/p&gt; 
&lt;h3&gt;How to Contribute&lt;/h3&gt; 
&lt;p&gt;Ralph is seeking contributors! See &lt;a href="https://raw.githubusercontent.com/frankbria/ralph-claude-code/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for the complete guide. Priority areas:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Test Implementation&lt;/strong&gt; - Help expand test coverage (&lt;a href="https://raw.githubusercontent.com/frankbria/ralph-claude-code/main/IMPLEMENTATION_PLAN.md"&gt;see plan&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Feature Development&lt;/strong&gt; - Log rotation, dry-run mode, config files&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Documentation&lt;/strong&gt; - Usage examples, tutorials, troubleshooting guides&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Bug Reports&lt;/strong&gt; - Real-world usage feedback and edge cases&lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;strong&gt;Ready to let AI build your project?&lt;/strong&gt; Start with &lt;code&gt;./install.sh&lt;/code&gt; and let Ralph take it from there!&lt;/p&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.star-history.com/#frankbria/ralph-claude-code&amp;amp;type=date&amp;amp;legend=top-left"&gt;&lt;img src="https://api.star-history.com/svg?repos=frankbria/ralph-claude-code&amp;amp;type=date&amp;amp;legend=top-left" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>obra/superpowers</title>
      <link>https://github.com/obra/superpowers</link>
      <description>&lt;p&gt;An agentic skills framework &amp; software development methodology that works.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Superpowers&lt;/h1&gt; 
&lt;p&gt;Superpowers is a complete software development workflow for your coding agents, built on top of a set of composable "skills" and some initial instructions that make sure your agent uses them.&lt;/p&gt; 
&lt;h2&gt;How it works&lt;/h2&gt; 
&lt;p&gt;It starts from the moment you fire up your coding agent. As soon as it sees that you're building something, it &lt;em&gt;doesn't&lt;/em&gt; just jump into trying to write code. Instead, it steps back and asks you what you're really trying to do.&lt;/p&gt; 
&lt;p&gt;Once it's teased a spec out of the conversation, it shows it to you in chunks short enough to actually read and digest.&lt;/p&gt; 
&lt;p&gt;After you've signed off on the design, your agent puts together an implementation plan that's clear enough for an enthusiastic junior engineer with poor taste, no judgement, no project context, and an aversion to testing to follow. It emphasizes true red/green TDD, YAGNI (You Aren't Gonna Need It), and DRY.&lt;/p&gt; 
&lt;p&gt;Next up, once you say "go", it launches a &lt;em&gt;subagent-driven-development&lt;/em&gt; process, having agents work through each engineering task, inspecting and reviewing their work, and continuing forward. It's not uncommon for Claude to be able to work autonomously for a couple hours at a time without deviating from the plan you put together.&lt;/p&gt; 
&lt;p&gt;There's a bunch more to it, but that's the core of the system. And because the skills trigger automatically, you don't need to do anything special. Your coding agent just has Superpowers.&lt;/p&gt; 
&lt;h2&gt;Sponsorship&lt;/h2&gt; 
&lt;p&gt;If Superpowers has helped you do stuff that makes money and you are so inclined, I'd greatly appreciate it if you'd consider &lt;a href="https://github.com/sponsors/obra"&gt;sponsoring my opensource work&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Thanks!&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Jesse&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Installation differs by platform. Claude Code has a built-in plugin system. Codex and OpenCode require manual setup.&lt;/p&gt; 
&lt;h3&gt;Claude Code (via Plugin Marketplace)&lt;/h3&gt; 
&lt;p&gt;In Claude Code, register the marketplace first:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;/plugin marketplace add obra/superpowers-marketplace
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then install the plugin from this marketplace:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;/plugin install superpowers@superpowers-marketplace
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Verify Installation&lt;/h3&gt; 
&lt;p&gt;Check that commands appear:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;/help
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code&gt;# Should see:
# /superpowers:brainstorm - Interactive design refinement
# /superpowers:write-plan - Create implementation plan
# /superpowers:execute-plan - Execute plan in batches
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Codex&lt;/h3&gt; 
&lt;p&gt;Tell Codex:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Fetch and follow instructions from https://raw.githubusercontent.com/obra/superpowers/refs/heads/main/.codex/INSTALL.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Detailed docs:&lt;/strong&gt; &lt;a href="https://raw.githubusercontent.com/obra/superpowers/main/docs/README.codex.md"&gt;docs/README.codex.md&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;OpenCode&lt;/h3&gt; 
&lt;p&gt;Tell OpenCode:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Fetch and follow instructions from https://raw.githubusercontent.com/obra/superpowers/refs/heads/main/.opencode/INSTALL.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Detailed docs:&lt;/strong&gt; &lt;a href="https://raw.githubusercontent.com/obra/superpowers/main/docs/README.opencode.md"&gt;docs/README.opencode.md&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;The Basic Workflow&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;brainstorming&lt;/strong&gt; - Activates before writing code. Refines rough ideas through questions, explores alternatives, presents design in sections for validation. Saves design document.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;using-git-worktrees&lt;/strong&gt; - Activates after design approval. Creates isolated workspace on new branch, runs project setup, verifies clean test baseline.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;writing-plans&lt;/strong&gt; - Activates with approved design. Breaks work into bite-sized tasks (2-5 minutes each). Every task has exact file paths, complete code, verification steps.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;subagent-driven-development&lt;/strong&gt; or &lt;strong&gt;executing-plans&lt;/strong&gt; - Activates with plan. Dispatches fresh subagent per task with two-stage review (spec compliance, then code quality), or executes in batches with human checkpoints.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;test-driven-development&lt;/strong&gt; - Activates during implementation. Enforces RED-GREEN-REFACTOR: write failing test, watch it fail, write minimal code, watch it pass, commit. Deletes code written before tests.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;requesting-code-review&lt;/strong&gt; - Activates between tasks. Reviews against plan, reports issues by severity. Critical issues block progress.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;finishing-a-development-branch&lt;/strong&gt; - Activates when tasks complete. Verifies tests, presents options (merge/PR/keep/discard), cleans up worktree.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;The agent checks for relevant skills before any task.&lt;/strong&gt; Mandatory workflows, not suggestions.&lt;/p&gt; 
&lt;h2&gt;What's Inside&lt;/h2&gt; 
&lt;h3&gt;Skills Library&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Testing&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;test-driven-development&lt;/strong&gt; - RED-GREEN-REFACTOR cycle (includes testing anti-patterns reference)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Debugging&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;systematic-debugging&lt;/strong&gt; - 4-phase root cause process (includes root-cause-tracing, defense-in-depth, condition-based-waiting techniques)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;verification-before-completion&lt;/strong&gt; - Ensure it's actually fixed&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Collaboration&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;brainstorming&lt;/strong&gt; - Socratic design refinement&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;writing-plans&lt;/strong&gt; - Detailed implementation plans&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;executing-plans&lt;/strong&gt; - Batch execution with checkpoints&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;dispatching-parallel-agents&lt;/strong&gt; - Concurrent subagent workflows&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;requesting-code-review&lt;/strong&gt; - Pre-review checklist&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;receiving-code-review&lt;/strong&gt; - Responding to feedback&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;using-git-worktrees&lt;/strong&gt; - Parallel development branches&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;finishing-a-development-branch&lt;/strong&gt; - Merge/PR decision workflow&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;subagent-driven-development&lt;/strong&gt; - Fast iteration with two-stage review (spec compliance, then code quality)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Meta&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;writing-skills&lt;/strong&gt; - Create new skills following best practices (includes testing methodology)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;using-superpowers&lt;/strong&gt; - Introduction to the skills system&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Philosophy&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Test-Driven Development&lt;/strong&gt; - Write tests first, always&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Systematic over ad-hoc&lt;/strong&gt; - Process over guessing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Complexity reduction&lt;/strong&gt; - Simplicity as primary goal&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Evidence over claims&lt;/strong&gt; - Verify before declaring success&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Read more: &lt;a href="https://blog.fsck.com/2025/10/09/superpowers/"&gt;Superpowers for Claude Code&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Skills live directly in this repository. To contribute:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Fork the repository&lt;/li&gt; 
 &lt;li&gt;Create a branch for your skill&lt;/li&gt; 
 &lt;li&gt;Follow the &lt;code&gt;writing-skills&lt;/code&gt; skill for creating and testing new skills&lt;/li&gt; 
 &lt;li&gt;Submit a PR&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;See &lt;code&gt;skills/writing-skills/SKILL.md&lt;/code&gt; for the complete guide.&lt;/p&gt; 
&lt;h2&gt;Updating&lt;/h2&gt; 
&lt;p&gt;Skills update automatically when you update the plugin:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;/plugin update superpowers
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;MIT License - see LICENSE file for details&lt;/p&gt; 
&lt;h2&gt;Support&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Issues&lt;/strong&gt;: &lt;a href="https://github.com/obra/superpowers/issues"&gt;https://github.com/obra/superpowers/issues&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Marketplace&lt;/strong&gt;: &lt;a href="https://github.com/obra/superpowers-marketplace"&gt;https://github.com/obra/superpowers-marketplace&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>MiroMindAI/MiroThinker</title>
      <link>https://github.com/MiroMindAI/MiroThinker</link>
      <description>&lt;p&gt;MiroThinker is an open source deep research agent optimized for research and prediction. It achieves a 60.2% Avg@8 score on the challenging GAIA benchmark.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/assets/miro_thinker.png" width="55%" alt="MiroThinker" /&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://dr.miromind.ai/"&gt;&lt;img src="https://img.shields.io/badge/Demo-FFB300?style=for-the-badge&amp;amp;logo=airplayvideo&amp;amp;logoColor=white" alt="DEMO" /&gt;&lt;/a&gt; &lt;a href="https://miromind.ai/#blog"&gt;&lt;img src="https://img.shields.io/badge/Blog-4285F4?style=for-the-badge&amp;amp;logo=google-chrome&amp;amp;logoColor=white" alt="Blog" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/datasets/miromind-ai/MiroVerse-v0.1"&gt;&lt;img src="https://img.shields.io/badge/Data-0040A1?style=for-the-badge&amp;amp;logo=huggingface&amp;amp;logoColor=ffffff&amp;amp;labelColor" alt="DATA" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/MiroMindAI"&gt;&lt;img src="https://img.shields.io/badge/Github-24292F?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white" alt="GITHUB" /&gt;&lt;/a&gt; &lt;a href="https://miromind.ai/"&gt;&lt;img src="https://img.shields.io/badge/Website-4285F4?style=for-the-badge&amp;amp;logo=google-chrome&amp;amp;logoColor=white" alt="WEBSITE" /&gt;&lt;/a&gt; &lt;a href="https://discord.com/invite/GPqEnkzQZd"&gt;&lt;img src="https://img.shields.io/badge/Discord-5865F2?style=for-the-badge&amp;amp;logo=discord&amp;amp;logoColor=white" alt="DISCORD" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;h3&gt;üöÄ &lt;a href="https://dr.miromind.ai/"&gt;Try our Demo!&lt;/a&gt;&lt;/h3&gt; 
&lt;/div&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;MiroThinker&lt;/strong&gt; is an open source deep research agent optimized for research and prediction. It achieves a 60.2% Avg@8 score on the challenging GAIA benchmark.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;The project currently comprises four key components:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üí° &lt;strong&gt;MiroThinker&lt;/strong&gt;: An open source deep research agent optimized for research and prediction. It achieves a 60.2% Avg@8 score on the challenging GAIA benchmark. See &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/#-quick-start"&gt;Quick Start&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;ü§ñ &lt;strong&gt;MiroFlow&lt;/strong&gt;: An agent framework that enables tool-use agent tasks, featuring a reproducible GAIA score of 82.4%. See &lt;a href="https://github.com/MiroMindAI/MiroFlow"&gt;MiroFlow&lt;/a&gt; for details.&lt;/li&gt; 
 &lt;li&gt;üìö &lt;strong&gt;MiroVerse&lt;/strong&gt;: A premium open-source training dataset with 147k samples supporting research agent training. See &lt;a href="https://huggingface.co/datasets/miromind-ai/MiroVerse-v0.1"&gt;MiroVerse&lt;/a&gt; on HuggingFace.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üìã Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;üì∞ &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/#-news--updates"&gt;News &amp;amp; Updates&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/#-introduction"&gt;Introduction&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;‚ú® &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/#-key-features"&gt;Key Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üìà &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/#-performance-on-benchmarks"&gt;Performance on Benchmarks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üöÄ &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/#-quick-start"&gt;Quick Start&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üìä &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/#-benchmark-evaluation"&gt;Benchmark Evaluation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üî¨ &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/#-trace-collection"&gt;Trace Collection&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;‚ùì &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/#-faq--troubleshooting"&gt;FAQ &amp;amp; Troubleshooting&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üìÑ &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/#-license"&gt;License&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üôè &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/#-acknowledgments"&gt;Acknowledgments&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üì∞ News &amp;amp; Updates&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;[2026-01-05]&lt;/strong&gt; üéâüéâ We release &lt;a href="https://huggingface.co/collections/miromind-ai/mirothinker-v15"&gt;MiroThinker-v1.5&lt;/a&gt;, a series of open source deep research agents optimized for financial prediction. &lt;a href="https://huggingface.co/miromind-ai/MiroThinker-v1.5-30B"&gt;MiroThinker-v1.5-30B&lt;/a&gt; surpasses Kimi-K2-Thinking on BrowseComp-ZH at much lower cost, using only 1/30 of the parameters. &lt;a href="https://huggingface.co/miromind-ai/MiroThinker-v1.5-235B"&gt;MiroThinker-v1.5-235B&lt;/a&gt; scores 39.2% on HLE-Text, 69.8% on BrowseComp, 71.5% on BrowseComp-ZH, and 80.8% on GAIA-Val-165, setting a new state-of-the-art among search agents.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025-11-13]&lt;/strong&gt; üéâ &lt;a href="https://huggingface.co/collections/miromind-ai/mirothinker-v10"&gt;MiroThinker-v1.0&lt;/a&gt; is now released! Introducing &lt;strong&gt;interactive scaling&lt;/strong&gt; as a third dimension of performance improvement, MiroThinker v1.0 supports 256K context window and up to 600 tool calls per task. Available in 8B, 30B, and 72B parameter scales, achieving 37.7%, 47.1%, 55.6%, and 81.9% on HLE-Text, BrowseComp, BrowseComp-ZH, and GAIA-Text-103, respectively. See &lt;a href="https://arxiv.org/abs/2511.11793"&gt;Technical Report&lt;/a&gt; for more details.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025-09-11]&lt;/strong&gt; MiroThinker-72B-Preview ranked 4th in this week's FutureX benchmark. See &lt;a href="https://futurex-ai.github.io/"&gt;FutureX&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;üìú Click to expand older updates&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;[2025-09-08]&lt;/strong&gt; &lt;a href="https://huggingface.co/collections/miromind-ai/mirothinker-v02"&gt;MiroThinker-v0.2&lt;/a&gt; is now released, achieving open-source SOTA performance across multiple benchmarks, including HLE (17.8%), HLE-Text-Only (19.1%), BrowseComp-EN (17.2%), BrowseComp-ZH (29.4%), XBench-DeepSearch (56.0%), and Frames (74.8%).&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;[2025-09-07]&lt;/strong&gt; We supported more benchmarks, including &lt;a href="https://arxiv.org/abs/2504.19314"&gt;BrowseComp-ZH&lt;/a&gt;, &lt;a href="https://xbench.org/agi/aisearch"&gt;XBench-DeepSearch&lt;/a&gt;, and &lt;a href="https://futurex-ai.github.io/"&gt;FutureX&lt;/a&gt;. We plan to add more benchmarks in the future.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;[2025-08-22]&lt;/strong&gt; Introducing streamlined deployment options for MiroThinker with optimized resource usage and faster startup times. Experience the interactive demo: &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/apps/gradio-demo"&gt;üöÄ Try Gradio Demo&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;[2025-08-08]&lt;/strong&gt; &lt;a href="https://huggingface.co/collections/miromind-ai/mirothinker-v01-689301b6d0563321862d44a1"&gt;MiroThinker-v0.1&lt;/a&gt; released.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;üìù Introduction&lt;/h2&gt; 
&lt;h3&gt;MiroThinker-v1.5&lt;/h3&gt; 
&lt;p&gt;MiroThinker v1.5 is the world-leading open-source search agent that advances tool-augmented reasoning through &lt;strong&gt;interactive scaling&lt;/strong&gt; ‚Äî training the agent to handle deeper and more frequent agent-environment interactions as a third dimension of performance improvement, beyond model size and context length.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://huggingface.co/datasets/miromind-ai/MiroFlow-Benchmarks/resolve/main/assets/mirothinker_v1.5_framework.png" alt="image" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Key Features&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üöÄ MiroThinker v1.5 supports a 256K context window, long-horizon reasoning, and deep multi-step analysis.&lt;/li&gt; 
 &lt;li&gt;üîß Handles up to 400 tool calls per task ‚Äî a substantial improvement over previous open-source research agents.&lt;/li&gt; 
 &lt;li&gt;üì¶ Released in 30B and 235B parameter scales, accompanied by a comprehensive suite of tools and workflows to flexibly support diverse research settings and compute budgets.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;Agent Name&lt;/th&gt; 
    &lt;th align="center"&gt;Base Agent&lt;/th&gt; 
    &lt;th align="center"&gt;Max Context&lt;/th&gt; 
    &lt;th align="center"&gt;Max Tool Calls&lt;/th&gt; 
    &lt;th align="center"&gt;HF Link&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;MiroThinker-v1.5-30B&lt;/td&gt; 
    &lt;td align="center"&gt;Qwen3-30B-A3B-Thinking-2507&lt;/td&gt; 
    &lt;td align="center"&gt;256K&lt;/td&gt; 
    &lt;td align="center"&gt;400&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-v1.5-30B"&gt;ü§ó link&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;MiroThinker-v1.5-235B&lt;/td&gt; 
    &lt;td align="center"&gt;Qwen3-235B-A22B-Thinking-2507&lt;/td&gt; 
    &lt;td align="center"&gt;256K&lt;/td&gt; 
    &lt;td align="center"&gt;400&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-v1.5-235B"&gt;ü§ó link&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;p&gt;MiroThinker v1.5 demonstrates strong general-research performance across a broad range of benchmarks, achieving&amp;nbsp;39.2%,&amp;nbsp;69.8%, 71.5%, and&amp;nbsp;80.8%&amp;nbsp;on HLE-Text, BrowseComp, BrowseComp-ZH, and GAIA-Val-165, respectively. These results surpass previous open-source agents and set the new world-leading BrowseComp performance.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://huggingface.co/datasets/miromind-ai/MiroFlow-Benchmarks/resolve/main/assets/mirothinker_v1.5_browsecomp.png" alt="image" /&gt;&lt;/p&gt; 
&lt;h3&gt;MiroThinker-v1.0&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;üì¶ Click to expand MiroThinker-v1.0 details&lt;/summary&gt; 
 &lt;p&gt;Unlike previous agents that scale only model size or context length, MiroThinker v1.0 introduces &lt;strong&gt;interactive scaling&lt;/strong&gt; at the agent level, systematically training the agent to handle deeper and more frequent agent‚Äìenvironment interactions as a third dimension of performance improvement. Interactive scaling leverages environment feedback and external information acquisition to correct errors and refine trajectories.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://huggingface.co/datasets/miromind-ai/MiroFlow-Benchmarks/resolve/main/assets/MiroThinker_v1.0_Overall.png" alt="image" /&gt;&lt;/p&gt; 
 &lt;h3&gt;‚ú® Key Features&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;üöÄ &lt;strong&gt;256K Context Window&lt;/strong&gt;: Supports long-horizon reasoning and deep multi-step analysis&lt;/li&gt; 
  &lt;li&gt;üîß &lt;strong&gt;600 Tool Calls&lt;/strong&gt;: Handles up to 600 tool calls per task ‚Äî a substantial improvement over previous open-source research agents&lt;/li&gt; 
  &lt;li&gt;üì¶ &lt;strong&gt;Multiple Scales&lt;/strong&gt;: Released in 8B, 30B, and 72B parameter scales, accompanied by a comprehensive suite of tools and workflows to flexibly support diverse research settings and compute budgets&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;div align="center"&gt; 
  &lt;table&gt; 
   &lt;thead&gt; 
    &lt;tr&gt; 
     &lt;th align="center"&gt;Agent Name&lt;/th&gt; 
     &lt;th align="center"&gt;Base Agent&lt;/th&gt; 
     &lt;th align="center"&gt;Max Context&lt;/th&gt; 
     &lt;th align="center"&gt;Max Tool Calls&lt;/th&gt; 
     &lt;th align="center"&gt;HF Link&lt;/th&gt; 
    &lt;/tr&gt; 
   &lt;/thead&gt; 
   &lt;tbody&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-v1.0-8B&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen3-8B&lt;/td&gt; 
     &lt;td align="center"&gt;256K&lt;/td&gt; 
     &lt;td align="center"&gt;600&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-v1.0-8B"&gt;ü§ó link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-v1.0-30B&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen3-30B-A3B-Thinking-2507&lt;/td&gt; 
     &lt;td align="center"&gt;256K&lt;/td&gt; 
     &lt;td align="center"&gt;600&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-v1.0-30B"&gt;ü§ó link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-v1.0-72B&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen2.5-72B-Instruct&lt;/td&gt; 
     &lt;td align="center"&gt;256K&lt;/td&gt; 
     &lt;td align="center"&gt;600&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-v1.0-72B"&gt;ü§ó link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
   &lt;/tbody&gt; 
  &lt;/table&gt; 
 &lt;/div&gt; 
 &lt;p&gt;MiroThinker v1.0 demonstrates strong general-research performance across a broad range of benchmarks, achieving &lt;strong&gt;37.7%&lt;/strong&gt;, &lt;strong&gt;47.1%&lt;/strong&gt;, &lt;strong&gt;55.6%&lt;/strong&gt;, and &lt;strong&gt;81.9%&lt;/strong&gt; on HLE-Text, BrowseComp, BrowseComp-ZH, and GAIA-Text-103, respectively. These results surpass previous open-source agents and narrow the gap with commercial counterparts such as &lt;strong&gt;GPT-5-high&lt;/strong&gt;.&lt;/p&gt; 
 &lt;div align="center"&gt; 
  &lt;img src="https://huggingface.co/datasets/miromind-ai/MiroFlow-Benchmarks/resolve/main/assets/MiroThinker_v1.0_Performance_1.png" width="100%" alt="MiroThinker" /&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;h3&gt;MiroThinker-v0.2&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;üì¶ Click to expand MiroThinker-v0.2 details&lt;/summary&gt; 
 &lt;p&gt;In this new version, we introduced three key improvements:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;üìö &lt;strong&gt;Richer training data&lt;/strong&gt; from both English and Chinese sources, yielding significant gains in benchmark performance and generalization&lt;/li&gt; 
  &lt;li&gt;üéØ &lt;strong&gt;Unified DPO training&lt;/strong&gt; with a single preference dataset across all agents&lt;/li&gt; 
  &lt;li&gt;üìè &lt;strong&gt;Extended context length&lt;/strong&gt; from 40k to 64k for more challenging multi-turn tool-use tasks&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;Compared to v0.1, MiroThinker v0.2 delivers consistent gains across benchmarks. For example, scores improved from &lt;strong&gt;57.3 ‚Üí 64.1&lt;/strong&gt; on &lt;strong&gt;GAIA-Text-103&lt;/strong&gt; and from &lt;strong&gt;17.0 ‚Üí 29.4&lt;/strong&gt; on &lt;strong&gt;BrowseComp-ZH&lt;/strong&gt;, reflecting substantial advancements in the model‚Äôs general research agent capabilities.&lt;/p&gt; 
 &lt;div align="center"&gt; 
  &lt;table&gt; 
   &lt;thead&gt; 
    &lt;tr&gt; 
     &lt;th align="center"&gt;Agent Name&lt;/th&gt; 
     &lt;th align="center"&gt;Base Agent&lt;/th&gt; 
     &lt;th align="center"&gt;Max Context&lt;/th&gt; 
     &lt;th align="center"&gt;HF Link&lt;/th&gt; 
    &lt;/tr&gt; 
   &lt;/thead&gt; 
   &lt;tbody&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-4B-SFT-v0.2&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen3-4B&lt;/td&gt; 
     &lt;td align="center"&gt;64K&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-4B-SFT-v0.2"&gt;ü§ó link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-4B-DPO-v0.2&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen3-4B&lt;/td&gt; 
     &lt;td align="center"&gt;64K&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-4B-DPO-v0.2"&gt;ü§ó link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-8B-SFT-v0.2&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen3-8B&lt;/td&gt; 
     &lt;td align="center"&gt;64K&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-8B-SFT-v0.2"&gt;ü§ó link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-8B-DPO-v0.2&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen3-8B&lt;/td&gt; 
     &lt;td align="center"&gt;64K&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-8B-DPO-v0.2"&gt;ü§ó link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-14B-SFT-v0.2&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen3-14B&lt;/td&gt; 
     &lt;td align="center"&gt;64K&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-14B-SFT-v0.2"&gt;ü§ó link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-14B-DPO-v0.2&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen3-14B&lt;/td&gt; 
     &lt;td align="center"&gt;64K&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-14B-DPO-v0.2"&gt;ü§ó link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-32B-SFT-v0.2&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen3-32B&lt;/td&gt; 
     &lt;td align="center"&gt;64K&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-32B-SFT-v0.2"&gt;ü§ó link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-32B-DPO-v0.2&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen3-32B&lt;/td&gt; 
     &lt;td align="center"&gt;64K&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-32B-DPO-v0.2"&gt;ü§ó link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
   &lt;/tbody&gt; 
  &lt;/table&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;h3&gt;MiroThinker-v0.1&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;üì¶ Click to expand MiroThinker-v0.1 details&lt;/summary&gt; 
 &lt;div align="center"&gt; 
  &lt;img src="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/assets/gaia_text_103.png" width="98%" alt="MiroFlow Performance on GAIA-Validation" /&gt; 
  &lt;p&gt;&lt;strong&gt;Performance of Open-Source Agents on GAIA-Validation Benchmark.&lt;/strong&gt;&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;p&gt;We have released the &lt;strong&gt;MiroThinker v0.1&lt;/strong&gt; series, including both SFT and DPO variants at parameter scales of &lt;strong&gt;8B&lt;/strong&gt;, &lt;strong&gt;14B&lt;/strong&gt;, and &lt;strong&gt;32B&lt;/strong&gt;. Notably, MiroThinker v0.1 achieves &lt;strong&gt;state-of-the-art performance&lt;/strong&gt; among open-source models on the &lt;a href="https://huggingface.co/datasets/gaia-benchmark/GAIA"&gt;GAIA benchmark&lt;/a&gt;, a rigorous evaluation suite for advanced agentic capabilities, demonstrating its strength in long-context, decision-intensive, and real-world task scenarios.&lt;/p&gt; 
 &lt;div align="center"&gt; 
  &lt;table&gt; 
   &lt;thead&gt; 
    &lt;tr&gt; 
     &lt;th align="center"&gt;Agent Name&lt;/th&gt; 
     &lt;th align="center"&gt;Base Agent&lt;/th&gt; 
     &lt;th align="center"&gt;Max Context&lt;/th&gt; 
     &lt;th align="center"&gt;HF Link&lt;/th&gt; 
    &lt;/tr&gt; 
   &lt;/thead&gt; 
   &lt;tbody&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-8B-SFT-v0.1&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen3-8B&lt;/td&gt; 
     &lt;td align="center"&gt;40K&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-8B-SFT-v0.1"&gt;ü§ó link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-8B-DPO-v0.1&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen3-8B&lt;/td&gt; 
     &lt;td align="center"&gt;40K&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-8B-DPO-v0.1"&gt;ü§ó link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-14B-SFT-v0.1&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen3-14B&lt;/td&gt; 
     &lt;td align="center"&gt;40K&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-14B-SFT-v0.1"&gt;ü§ó link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-14B-DPO-v0.1&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen3-14B&lt;/td&gt; 
     &lt;td align="center"&gt;40K&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-14B-DPO-v0.1"&gt;ü§ó link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-32B-SFT-v0.1&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen3-32B&lt;/td&gt; 
     &lt;td align="center"&gt;40K&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-32B-SFT-v0.1"&gt;ü§ó link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-32B-DPO-v0.1&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen3-32B&lt;/td&gt; 
     &lt;td align="center"&gt;40K&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-32B-DPO-v0.1"&gt;ü§ó link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
   &lt;/tbody&gt; 
  &lt;/table&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;h2&gt;‚ú® Key Features&lt;/h2&gt; 
&lt;h3&gt;ü§ñ &lt;strong&gt;MiroThinker-Optimized Framework&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;üîì &lt;strong&gt;Fully Open-Source Agent Framework&lt;/strong&gt;: Complete transparency with open framework and open agents&lt;/li&gt; 
 &lt;li&gt;üîó &lt;strong&gt;Tool Integration&lt;/strong&gt;: Seamless integration with external tools and APIs&lt;/li&gt; 
 &lt;li&gt;üìù &lt;strong&gt;Trace Collection&lt;/strong&gt;: Comprehensive logging and analysis of agent interactions with elapsed time and estimated completion time displayed in minutes. Ready for SFT and DPO&lt;/li&gt; 
 &lt;li&gt;üìä &lt;strong&gt;Benchmark Evaluation&lt;/strong&gt;: Extensive testing across multiple benchmark datasets&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üìä &lt;strong&gt;Comprehensive Benchmark Suite&lt;/strong&gt;&lt;/h3&gt; 
&lt;details open&gt; 
 &lt;summary&gt;üìã Click to expand benchmark list&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;GAIA Validation&lt;/strong&gt;: A benchmark for General AI Assistants. (&lt;a href="https://arxiv.org/abs/2311.12983"&gt;paper&lt;/a&gt;)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;GAIA-Text-103&lt;/strong&gt;: A subset of GAIA Validation for text-only tasks. (&lt;a href="https://arxiv.org/abs/2505.22648"&gt;paper&lt;/a&gt;)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;HLE&lt;/strong&gt;: Humanity's Last Exam. (&lt;a href="https://arxiv.org/abs/2501.14249"&gt;paper&lt;/a&gt;)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;HLE-Text-2158&lt;/strong&gt;: A subset of HLE for text-only tasks. (&lt;a href="https://arxiv.org/abs/2501.14249"&gt;paper&lt;/a&gt;)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;HLE-Text-500&lt;/strong&gt;: A subset of HLE for text-only tasks, created by &lt;a href="https://arxiv.org/pdf/2504.21776"&gt;WebThinker&lt;/a&gt;. (&lt;a href="https://arxiv.org/pdf/2504.21776"&gt;paper&lt;/a&gt;)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;BrowseComp-EN&lt;/strong&gt;: Web browsing and comprehension tasks. (&lt;a href="https://arxiv.org/abs/2504.12516"&gt;paper&lt;/a&gt;)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;BrowseComp-ZH&lt;/strong&gt;: A Chinese version of BrowseComp. (&lt;a href="https://arxiv.org/abs/2504.19314"&gt;paper&lt;/a&gt;)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;WebWalkerQA&lt;/strong&gt;: Web navigation and question answering. (&lt;a href="https://arxiv.org/abs/2501.07572"&gt;paper&lt;/a&gt;)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Frames&lt;/strong&gt;: Factuality, Retrieval, And reasoning MEasurement Set. (&lt;a href="https://arxiv.org/abs/2409.12941"&gt;paper&lt;/a&gt;)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;XBench-DeepSearch&lt;/strong&gt;: A benchmark for deep research agents. (&lt;a href="https://xbench.org/agi/aisearch"&gt;website&lt;/a&gt;)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;FutureX&lt;/strong&gt;: A live benchmark designed for predicting unknown future. (&lt;a href="https://futurex-ai.github.io/"&gt;website&lt;/a&gt;)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;SEAL-0&lt;/strong&gt;: A benchmark for evaluating LLMs on conflicting-evidence web questions. (&lt;a href="https://arxiv.org/abs/2506.01062"&gt;paper&lt;/a&gt;)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;AIME2025&lt;/strong&gt;: American Invitational Mathematics Examination 2025. (&lt;a href="https://artificialanalysis.ai/evaluations/aime-2025"&gt;website&lt;/a&gt;)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;DeepSearchQA&lt;/strong&gt;: Google's Deep Search Question Answering benchmark. (&lt;a href="https://arxiv.org/abs/2505.20827"&gt;paper&lt;/a&gt;)&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;üìà Performance on Benchmarks&lt;/h2&gt; 
&lt;h3&gt;MiroThinker-v1.5&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;To prevent potential information leakage (e.g., searching benchmark answers from HuggingFace), access to HuggingFace has been explicitly disabled in these tools.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;We further perform canary string testing on the tool outputs of all trajectories and disregard any trajectory found to be contaminated, treating it as an incorrect answer.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;div&gt; 
 &lt;img src="https://huggingface.co/datasets/miromind-ai/MiroFlow-Benchmarks/resolve/main/assets/mirothinker_v1.5_performance.png" width="100%" alt="MiroThinker" /&gt; 
&lt;/div&gt; 
&lt;h3&gt;MiroThinker-v1.0&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;üì¶ Click to expand MiroThinker-v1.0 details&lt;/summary&gt; 
 &lt;div align="center"&gt; 
  &lt;img src="https://github.com/user-attachments/assets/108a2105-4e1d-499e-a001-4713a03fd8ac" width="100%" alt="MiroThinker" /&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;h3&gt;MiroThinker-v0.2&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;üì¶ Click to expand MiroThinker-v0.2 details&lt;/summary&gt; 
 &lt;h4&gt;Comparison with SOTA Research Agents&lt;/h4&gt; 
 &lt;div align="center"&gt; 
  &lt;img src="https://huggingface.co/datasets/miromind-ai/MiroFlow-Benchmarks/resolve/main/assets/MiroThinker_v0.2_Performance_2.png" width="90%" alt="MiroThinker" /&gt; 
 &lt;/div&gt; 
 &lt;h4&gt;GAIA Benchmark&lt;/h4&gt; 
 &lt;div align="center"&gt; 
  &lt;img src="https://huggingface.co/datasets/miromind-ai/MiroFlow-Benchmarks/resolve/main/assets/MiroThinker_v0.2_Performance_1.png" width="80%" alt="MiroThinker" /&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;h3&gt;MiroThinker-v0.1&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;üì¶ Click to expand MiroThinker-v0.1 details&lt;/summary&gt; 
 &lt;h4&gt;GAIA Benchmark&lt;/h4&gt; 
 &lt;div align="center"&gt; 
  &lt;table&gt; 
   &lt;thead&gt; 
    &lt;tr&gt; 
     &lt;th&gt;&lt;strong&gt;Method&lt;/strong&gt;&lt;/th&gt; 
     &lt;th align="center"&gt;Text-103&lt;br /&gt;Best Pass@1&lt;/th&gt; 
     &lt;th align="center"&gt;Text-103&lt;br /&gt;Pass@1 (Avg@8)&lt;/th&gt; 
     &lt;th align="center"&gt;Val-165&lt;br /&gt;Best Pass@1&lt;/th&gt; 
     &lt;th align="center"&gt;Val-165&lt;br /&gt;Pass@1 (Avg@8)&lt;/th&gt; 
    &lt;/tr&gt; 
   &lt;/thead&gt; 
   &lt;tbody&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;üîπ‚Äî‚Äî 7B/8B Agents ‚Äî‚Äî&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;Search-o1-7B&lt;/td&gt; 
     &lt;td align="center"&gt;17.5&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;R1-Searcher-7B&lt;/td&gt; 
     &lt;td align="center"&gt;20.4&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;WebDancer-7B&lt;/td&gt; 
     &lt;td align="center"&gt;31.0&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;WebSailor-7B&lt;/td&gt; 
     &lt;td align="center"&gt;37.9&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;CK-Pro-8B&lt;/td&gt; 
     &lt;td align="center"&gt;40.3&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;32.7&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;MiroThinker-8B-SFT-v0.1&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;44.7&lt;/td&gt; 
     &lt;td align="center"&gt;40.1&lt;/td&gt; 
     &lt;td align="center"&gt;34.6&lt;/td&gt; 
     &lt;td align="center"&gt;31.8&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;+ Commercial Tools&lt;/td&gt; 
     &lt;td align="center"&gt;46.6&lt;/td&gt; 
     &lt;td align="center"&gt;42.1&lt;/td&gt; 
     &lt;td align="center"&gt;37.6&lt;/td&gt; 
     &lt;td align="center"&gt;33.9&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;MiroThinker-8B-DPO-v0.1&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;46.6&lt;/td&gt; 
     &lt;td align="center"&gt;44.8&lt;/td&gt; 
     &lt;td align="center"&gt;37.0&lt;/td&gt; 
     &lt;td align="center"&gt;35.4&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;+ Commercial Tools&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;strong&gt;50.5&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;strong&gt;46.7&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;strong&gt;38.2&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;strong&gt;35.9&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;üîπ‚Äî‚Äî 14B Agents ‚Äî‚Äî&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;MiroThinker-14B-SFT-v0.1&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;47.6&lt;/td&gt; 
     &lt;td align="center"&gt;44.4&lt;/td&gt; 
     &lt;td align="center"&gt;37.0&lt;/td&gt; 
     &lt;td align="center"&gt;34.4&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;+ Commercial Tools&lt;/td&gt; 
     &lt;td align="center"&gt;49.5&lt;/td&gt; 
     &lt;td align="center"&gt;47.5&lt;/td&gt; 
     &lt;td align="center"&gt;41.8&lt;/td&gt; 
     &lt;td align="center"&gt;39.8&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;MiroThinker-14B-DPO-v0.1&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;48.5&lt;/td&gt; 
     &lt;td align="center"&gt;46.6&lt;/td&gt; 
     &lt;td align="center"&gt;42.4&lt;/td&gt; 
     &lt;td align="center"&gt;39.2&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;+ Commercial Tools&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;strong&gt;52.4&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;strong&gt;48.5&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;strong&gt;45.5&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;strong&gt;42.0&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;üîπ‚Äî‚Äî 32B Agents ‚Äî‚Äî&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;Qwen3-32B&lt;/td&gt; 
     &lt;td align="center"&gt;31.1&lt;/td&gt; 
     &lt;td align="center"&gt;26.7&lt;/td&gt; 
     &lt;td align="center"&gt;29.7&lt;/td&gt; 
     &lt;td align="center"&gt;26.4&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;Search-o1-32B&lt;/td&gt; 
     &lt;td align="center"&gt;28.2&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;WebThinker-32B-RL&lt;/td&gt; 
     &lt;td align="center"&gt;48.5&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;WebDancer-QwQ-32B&lt;/td&gt; 
     &lt;td align="center"&gt;51.5&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;WebSailor-32B&lt;/td&gt; 
     &lt;td align="center"&gt;53.2&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;WebShaper-QwQ-32B&lt;/td&gt; 
     &lt;td align="center"&gt;53.3&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;MiroThinker-32B-SFT-v0.1&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;55.3&lt;/td&gt; 
     &lt;td align="center"&gt;51.3&lt;/td&gt; 
     &lt;td align="center"&gt;44.9&lt;/td&gt; 
     &lt;td align="center"&gt;42.7&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;+ Commercial Tools&lt;/td&gt; 
     &lt;td align="center"&gt;58.3&lt;/td&gt; 
     &lt;td align="center"&gt;54.2&lt;/td&gt; 
     &lt;td align="center"&gt;48.5&lt;/td&gt; 
     &lt;td align="center"&gt;45.8&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;MiroThinker-32B-DPO-v0.1&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;57.3&lt;/td&gt; 
     &lt;td align="center"&gt;54.1&lt;/td&gt; 
     &lt;td align="center"&gt;48.5&lt;/td&gt; 
     &lt;td align="center"&gt;45.9&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;+ Commercial Tools&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;strong&gt;60.2&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;strong&gt;57.9&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;strong&gt;50.9&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;strong&gt;48.9&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
   &lt;/tbody&gt; 
  &lt;/table&gt; 
 &lt;/div&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;Following the practices of WebThinker, WebAgents, and CognitiveKernel, we report the Best Pass@1, the highest score across three runs, which often reflects stronger performance, though it may exhibit some variability. To provide a more stable measure, we additionally report Pass@1 (Avg@8), which offers greater consistency at the cost of slightly lower scores.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;For consistency with prior open-source works, we evaluate GAIA-Text-103 using the WebAgents LLM-as-a-Judge template, and report results on GAIA-Val-165 using the official GAIA scorer script.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;By default, we use open-source tools wherever possible, except for the code tool &lt;a href="https://github.com/e2b-dev/E2B"&gt;E2B&lt;/a&gt; and the Google search tool &lt;a href="https://serper.dev/"&gt;Serper&lt;/a&gt;. We use &lt;a href="https://huggingface.co/openai/whisper-large-v3-turbo"&gt;Whisper&lt;/a&gt;, &lt;a href="https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct"&gt;Qwen2.5-VL-72B-Instruct&lt;/a&gt;, and &lt;a href="https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507"&gt;Qwen3-235B-A22B-Thinking-2507&lt;/a&gt; in our implementation. The framework can be easily extended to other open-source tools of your choice.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Replacing these open-source tools with commercial alternatives can yield performance gains. Commercial tools were mainly used for multimodal capabilities and certain complex reasoning subtasks. The majority of tasks, including planning, browsing, refinement, navigation, and more, were handled by our agents.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;h4&gt;More Benchmarks&lt;/h4&gt; 
 &lt;div align="center"&gt; 
  &lt;table&gt; 
   &lt;thead&gt; 
    &lt;tr&gt; 
     &lt;th&gt;Method&lt;/th&gt; 
     &lt;th align="center"&gt;HLE&lt;br /&gt;Pass@1&lt;/th&gt; 
     &lt;th align="center"&gt;Frames&lt;br /&gt;Pass@1&lt;/th&gt; 
     &lt;th align="center"&gt;BrowseComp&lt;br /&gt;Pass@1&lt;/th&gt; 
     &lt;th align="center"&gt;BrowseComp-ZH&lt;br /&gt;Pass@1&lt;/th&gt; 
     &lt;th align="center"&gt;WebWalkerQA&lt;br /&gt;Pass@1&lt;/th&gt; 
    &lt;/tr&gt; 
   &lt;/thead&gt; 
   &lt;tbody&gt; 
    &lt;tr&gt; 
     &lt;td&gt;OpenAI Deep Research&lt;/td&gt; 
     &lt;td align="center"&gt;26.6&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;51.5&lt;/td&gt; 
     &lt;td align="center"&gt;42.9&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;Gemini Deep Research&lt;/td&gt; 
     &lt;td align="center"&gt;26.9&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;Kimi-Researcher&lt;/td&gt; 
     &lt;td align="center"&gt;26.9&lt;/td&gt; 
     &lt;td align="center"&gt;78.8&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;WebDancer-7B&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;36.0&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;WebSailor-7B&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;6.7&lt;/td&gt; 
     &lt;td align="center"&gt;14.2&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;MiroThinker-8B-SFT-v0.1&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;58.0&lt;/td&gt; 
     &lt;td align="center"&gt;5.5&lt;/td&gt; 
     &lt;td align="center"&gt;9.3&lt;/td&gt; 
     &lt;td align="center"&gt;41.3&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;MiroThinker-8B-DPO-v0.1&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;64.4&lt;/td&gt; 
     &lt;td align="center"&gt;8.7&lt;/td&gt; 
     &lt;td align="center"&gt;13.6&lt;/td&gt; 
     &lt;td align="center"&gt;45.7&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;WebThinker-32B-RL&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;46.5&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;WebDancer-QwQ-32B&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;3.8&lt;/td&gt; 
     &lt;td align="center"&gt;18.0&lt;/td&gt; 
     &lt;td align="center"&gt;47.9&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;WebSailor-32B&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;10.5&lt;/td&gt; 
     &lt;td align="center"&gt;25.5&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;WebShaper-32B&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;51.4&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;MiroThinker-32B-SFT-v0.1&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;10.2&lt;/td&gt; 
     &lt;td align="center"&gt;70.4&lt;/td&gt; 
     &lt;td align="center"&gt;10.6&lt;/td&gt; 
     &lt;td align="center"&gt;13.8&lt;/td&gt; 
     &lt;td align="center"&gt;45.7&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;MiroThinker-32B-DPO-v0.1&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;11.8&lt;/td&gt; 
     &lt;td align="center"&gt;71.7&lt;/td&gt; 
     &lt;td align="center"&gt;13.0&lt;/td&gt; 
     &lt;td align="center"&gt;17.0&lt;/td&gt; 
     &lt;td align="center"&gt;49.3&lt;/td&gt; 
    &lt;/tr&gt; 
   &lt;/tbody&gt; 
  &lt;/table&gt; 
 &lt;/div&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;MiroThinker‚Äôs performance was tested with this repository and open-source tools; other agents‚Äô results are from their papers and official sites.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;As &lt;a href="https://huggingface.co/datasets/miromind-ai/MiroVerse-v0.1"&gt;MiroVerse-v0.1&lt;/a&gt; mainly contains English data, the agent‚Äôs Chinese capability is limited. We plan to add more Chinese data to improve performance in the next version.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/details&gt; 
&lt;h2&gt;üöÄ Quick Start&lt;/h2&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;üêç &lt;strong&gt;Python 3.10+&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;üì¶ &lt;strong&gt;uv package manager&lt;/strong&gt; (&lt;a href="https://github.com/astral-sh/uv"&gt;Installation guide&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;üîë &lt;strong&gt;Required API keys&lt;/strong&gt; (see configuration section below)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Clone the repository
git clone https://github.com/MiroMindAI/MiroThinker
cd MiroThinker

# Setup environment
cd apps/miroflow-agent
uv sync

# Configure API keys
cp .env.example .env
# Edit .env with your API keys (SERPER_API_KEY, JINA_API_KEY, E2B_API_KEY, etc.)
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;üìù Environment Variables&lt;/strong&gt;: See &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/#tool-configuration"&gt;Tool Configuration&lt;/a&gt; section for required API keys.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Tool Configuration&lt;/h3&gt; 
&lt;h4&gt;Minimal Configuration for MiroThinker v1.5 and v1.0&lt;/h4&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Server&lt;/th&gt; 
   &lt;th align="left"&gt;Description&lt;/th&gt; 
   &lt;th align="left"&gt;Tools Provided&lt;/th&gt; 
   &lt;th align="left"&gt;Required Environment Variables&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;&lt;code&gt;tool-python&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Execution environment and file management (E2B sandbox)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;create_sandbox&lt;/code&gt;, &lt;code&gt;run_command&lt;/code&gt;, &lt;code&gt;run_python_code&lt;/code&gt;, &lt;code&gt;upload_file_from_local_to_sandbox&lt;/code&gt;, &lt;code&gt;download_file_from_sandbox_to_local&lt;/code&gt;, &lt;code&gt;download_file_from_internet_to_sandbox&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;E2B_API_KEY&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;&lt;code&gt;search_and_scrape_webpage&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Google search via Serper API&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;google_search&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;SERPER_API_KEY&lt;/code&gt;, &lt;code&gt;SERPER_BASE_URL&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;&lt;code&gt;jina_scrape_llm_summary&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Web scraping with LLM-based information extraction&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;scrape_and_extract_info&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;JINA_API_KEY&lt;/code&gt;, &lt;code&gt;JINA_BASE_URL&lt;/code&gt;, &lt;code&gt;SUMMARY_LLM_BASE_URL&lt;/code&gt;, &lt;code&gt;SUMMARY_LLM_MODEL_NAME&lt;/code&gt;, &lt;code&gt;SUMMARY_LLM_API_KEY&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Minimal &lt;code&gt;.env&lt;/code&gt; configuration example:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Required for MiroThinker v1.5 and v1.0 (minimal setup)
SERPER_API_KEY=your_serper_key
SERPER_BASE_URL="https://google.serper.dev"
JINA_API_KEY=your_jina_key
JINA_BASE_URL="https://r.jina.ai"
E2B_API_KEY=your_e2b_key

# Required for jina_scrape_llm_summary
# Note: Summary LLM can be a small model (e.g., Qwen3-14B or GPT-5-Nano)
# The choice has minimal impact on performance, use what's most convenient
SUMMARY_LLM_BASE_URL="https://your_summary_llm_base_url/v1/chat/completions"
SUMMARY_LLM_MODEL_NAME=your_llm_model_name  # e.g., "Qwen/Qwen3-14B" or "gpt-5-nano"
SUMMARY_LLM_API_KEY=your_llm_api_key  # Optional, depends on LLM provider

# Required for benchmark evaluation (LLM-as-a-Judge)
OPENAI_API_KEY=your_openai_key  # Required for running benchmark evaluations
OPENAI_BASE_URL="https://api.openai.com/v1"  # Optional, defaults to OpenAI's API
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;üí° Why this is minimal&lt;/strong&gt;: These 3 MCP servers cover the core capabilities needed for research tasks: web search, content extraction, and code execution. All other servers are optional enhancements.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;ü§ñ Summary LLM&lt;/strong&gt;: The &lt;code&gt;SUMMARY_LLM&lt;/code&gt; can be a small model like Qwen3-14B or GPT-5-Nano. The choice has minimal impact on overall performance, use whichever is most convenient for your setup.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;üìä For Benchmark Evaluation&lt;/strong&gt;: If you plan to run benchmark evaluations, you also need &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; (and optionally &lt;code&gt;OPENAI_BASE_URL&lt;/code&gt;) for LLM-as-a-Judge functionality used in evaluation scripts.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;üñºÔ∏è For GAIA Multimodal Tasks&lt;/strong&gt;: GAIA-Val-165 includes tasks with image/audio/video files. Since MiroThinker is a text-only LLM, GPT-4o is used to pre-process these files into text descriptions. The same &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; is used for both this preprocessing and LLM-as-a-Judge.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;üìñ For more details&lt;/strong&gt;: See &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/libs/miroflow-tools/README.md"&gt;MiroFlow Tools README&lt;/a&gt; for complete documentation of all available tools.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;details&gt; 
 &lt;summary&gt;üîß Click to expand additional available tools&lt;/summary&gt; 
 &lt;p&gt;The following optional tools are available but were not used in MiroThinker v1.5 and v1.0 evaluation:&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="left"&gt;Server Name&lt;/th&gt; 
    &lt;th align="left"&gt;Type&lt;/th&gt; 
    &lt;th align="left"&gt;Description&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;&lt;code&gt;tool-vqa&lt;/code&gt;&lt;/td&gt; 
    &lt;td align="left"&gt;Commercial&lt;/td&gt; 
    &lt;td align="left"&gt;Vision processing using Claude&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;&lt;code&gt;tool-vqa-os&lt;/code&gt;&lt;/td&gt; 
    &lt;td align="left"&gt;Open-Source&lt;/td&gt; 
    &lt;td align="left"&gt;Vision processing (open-source alternative)&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;&lt;code&gt;tool-transcribe&lt;/code&gt;&lt;/td&gt; 
    &lt;td align="left"&gt;Commercial&lt;/td&gt; 
    &lt;td align="left"&gt;Audio transcription using OpenAI&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;&lt;code&gt;tool-transcribe-os&lt;/code&gt;&lt;/td&gt; 
    &lt;td align="left"&gt;Open-Source&lt;/td&gt; 
    &lt;td align="left"&gt;Audio transcription using Whisper&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;&lt;code&gt;tool-reasoning&lt;/code&gt;&lt;/td&gt; 
    &lt;td align="left"&gt;Commercial&lt;/td&gt; 
    &lt;td align="left"&gt;Reasoning engine using Claude&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;&lt;code&gt;tool-reasoning-os&lt;/code&gt;&lt;/td&gt; 
    &lt;td align="left"&gt;Open-Source&lt;/td&gt; 
    &lt;td align="left"&gt;Reasoning engine (open-source alternative)&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;&lt;code&gt;tool-reading&lt;/code&gt;&lt;/td&gt; 
    &lt;td align="left"&gt;Open-Source&lt;/td&gt; 
    &lt;td align="left"&gt;Document reading using MarkItDown&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;&lt;code&gt;tool-google-search&lt;/code&gt;&lt;/td&gt; 
    &lt;td align="left"&gt;Commercial&lt;/td&gt; 
    &lt;td align="left"&gt;Web search using Google + scraping&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;&lt;code&gt;tool-sogou-search&lt;/code&gt;&lt;/td&gt; 
    &lt;td align="left"&gt;Commercial&lt;/td&gt; 
    &lt;td align="left"&gt;Web search using Sogou (Chinese)&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;&lt;strong&gt;üìñ Local Deployment&lt;/strong&gt;: For instructions on deploying open-source tools (&lt;code&gt;tool-vqa-os&lt;/code&gt;, &lt;code&gt;tool-transcribe-os&lt;/code&gt;, &lt;code&gt;tool-reasoning-os&lt;/code&gt;) locally, see &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/assets/LOCAL-TOOL-DEPLOYMENT.md"&gt;Local Tool Deployment Guide&lt;/a&gt;.&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/libs/miroflow-tools/README.md"&gt;MiroFlow Tools README&lt;/a&gt; for complete documentation of all available tools.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h4&gt;Pre-configured Agent Settings&lt;/h4&gt; 
&lt;p&gt;The &lt;code&gt;apps/miroflow-agent/conf/agent/&lt;/code&gt; directory contains several pre-configured agent settings. Each configuration uses different tools and requires corresponding environment variables in your &lt;code&gt;.env&lt;/code&gt; file.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;üí° Recommended&lt;/strong&gt;: For MiroThinker v1.5, use &lt;code&gt;mirothinker_v1.5_keep5_max200&lt;/code&gt; (with context management, recommended for most tasks) or &lt;code&gt;mirothinker_v1.5_keep5_max400&lt;/code&gt; (only used for BrowseComp and BrowseComp-ZH). For v1.0, use &lt;code&gt;mirothinker_v1.0_keep5&lt;/code&gt; (with context management). All use minimal configuration with only 3 MCP servers.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Configuration&lt;/th&gt; 
   &lt;th align="left"&gt;Description&lt;/th&gt; 
   &lt;th align="left"&gt;Max Turns&lt;/th&gt; 
   &lt;th align="left"&gt;Context Retention&lt;/th&gt; 
   &lt;th align="left"&gt;Required Environment Variables&lt;/th&gt; 
   &lt;th align="left"&gt;Recommended For&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;&lt;code&gt;mirothinker_v1.5_keep5_max200&lt;/code&gt;&lt;/strong&gt; ‚≠ê&lt;/td&gt; 
   &lt;td align="left"&gt;Single-agent with context management&lt;/td&gt; 
   &lt;td align="left"&gt;200&lt;/td&gt; 
   &lt;td align="left"&gt;Keep 5 most recent&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;SERPER_API_KEY&lt;/code&gt;, &lt;code&gt;SERPER_BASE_URL&lt;/code&gt;, &lt;code&gt;JINA_API_KEY&lt;/code&gt;, &lt;code&gt;JINA_BASE_URL&lt;/code&gt;, &lt;code&gt;E2B_API_KEY&lt;/code&gt;, &lt;code&gt;SUMMARY_LLM_BASE_URL&lt;/code&gt;, &lt;code&gt;SUMMARY_LLM_MODEL_NAME&lt;/code&gt;, &lt;code&gt;SUMMARY_LLM_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;v1.5 (recommended for most tasks)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;&lt;code&gt;mirothinker_v1.5_keep5_max400&lt;/code&gt;&lt;/strong&gt; ‚≠ê&lt;/td&gt; 
   &lt;td align="left"&gt;Single-agent with context management&lt;/td&gt; 
   &lt;td align="left"&gt;400&lt;/td&gt; 
   &lt;td align="left"&gt;Keep 5 most recent&lt;/td&gt; 
   &lt;td align="left"&gt;Same as above&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;v1.5 (for BrowseComp &amp;amp; BrowseComp-ZH)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;&lt;code&gt;mirothinker_v1.5&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Single-agent for MiroThinker v1.5&lt;/td&gt; 
   &lt;td align="left"&gt;600&lt;/td&gt; 
   &lt;td align="left"&gt;Keep all results&lt;/td&gt; 
   &lt;td align="left"&gt;Same as above&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;v1.5&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;&lt;code&gt;mirothinker_v1.0_keep5&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Single-agent with context management&lt;/td&gt; 
   &lt;td align="left"&gt;600&lt;/td&gt; 
   &lt;td align="left"&gt;Keep 5 most recent&lt;/td&gt; 
   &lt;td align="left"&gt;Same as above&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;v1.0&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;&lt;code&gt;mirothinker_v1.0&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Single-agent for MiroThinker v1.0&lt;/td&gt; 
   &lt;td align="left"&gt;600&lt;/td&gt; 
   &lt;td align="left"&gt;Keep all results&lt;/td&gt; 
   &lt;td align="left"&gt;Same as above&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;v1.0&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;details&gt; 
 &lt;summary&gt;üì¶ Click to expand legacy configurations (v0.1/v0.2)&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="left"&gt;Configuration&lt;/th&gt; 
    &lt;th align="left"&gt;Description&lt;/th&gt; 
    &lt;th align="left"&gt;Max Turns&lt;/th&gt; 
    &lt;th align="left"&gt;Context Retention&lt;/th&gt; 
    &lt;th align="left"&gt;Required Environment Variables&lt;/th&gt; 
    &lt;th align="left"&gt;Recommended For&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;&lt;strong&gt;&lt;code&gt;multi_agent&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; 
    &lt;td align="left"&gt;Multi-agent with commercial tools (v0.1/v0.2)&lt;/td&gt; 
    &lt;td align="left"&gt;50&lt;/td&gt; 
    &lt;td align="left"&gt;Keep all results&lt;/td&gt; 
    &lt;td align="left"&gt;&lt;code&gt;E2B_API_KEY&lt;/code&gt;, &lt;code&gt;ANTHROPIC_API_KEY&lt;/code&gt;, &lt;code&gt;ANTHROPIC_BASE_URL&lt;/code&gt;, &lt;code&gt;OPENAI_API_KEY&lt;/code&gt;, &lt;code&gt;OPENAI_BASE_URL&lt;/code&gt;, &lt;code&gt;SERPER_API_KEY&lt;/code&gt;, &lt;code&gt;SERPER_BASE_URL&lt;/code&gt;, &lt;code&gt;JINA_API_KEY&lt;/code&gt;, &lt;code&gt;JINA_BASE_URL&lt;/code&gt;&lt;/td&gt; 
    &lt;td align="left"&gt;v0.1/v0.2&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;&lt;strong&gt;&lt;code&gt;multi_agent_os&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; 
    &lt;td align="left"&gt;Multi-agent with open-source tools (v0.1/v0.2)&lt;/td&gt; 
    &lt;td align="left"&gt;50&lt;/td&gt; 
    &lt;td align="left"&gt;Keep all results&lt;/td&gt; 
    &lt;td align="left"&gt;&lt;code&gt;E2B_API_KEY&lt;/code&gt;, &lt;code&gt;VISION_API_KEY&lt;/code&gt;, &lt;code&gt;VISION_BASE_URL&lt;/code&gt;, &lt;code&gt;VISION_MODEL_NAME&lt;/code&gt;, &lt;code&gt;WHISPER_API_KEY&lt;/code&gt;, &lt;code&gt;WHISPER_BASE_URL&lt;/code&gt;, &lt;code&gt;WHISPER_MODEL_NAME&lt;/code&gt;, &lt;code&gt;REASONING_API_KEY&lt;/code&gt;, &lt;code&gt;REASONING_BASE_URL&lt;/code&gt;, &lt;code&gt;REASONING_MODEL_NAME&lt;/code&gt;, &lt;code&gt;SERPER_API_KEY&lt;/code&gt;, &lt;code&gt;SERPER_BASE_URL&lt;/code&gt;, &lt;code&gt;JINA_API_KEY&lt;/code&gt;, &lt;code&gt;JINA_BASE_URL&lt;/code&gt;&lt;/td&gt; 
    &lt;td align="left"&gt;v0.1/v0.2&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;üí° Note&lt;/strong&gt;: All environment variables are listed in &lt;code&gt;apps/miroflow-agent/.env.example&lt;/code&gt;. Copy it to &lt;code&gt;.env&lt;/code&gt; and fill in the values for the tools you plan to use.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Creating Custom Tool Configurations&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;üîß Click to expand custom tool configuration guide&lt;/summary&gt; 
 &lt;p&gt;You can create your own YAML configuration file to freely combine MCP servers. Here's how:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;strong&gt;Create a new YAML file&lt;/strong&gt; in &lt;code&gt;apps/miroflow-agent/conf/agent/&lt;/code&gt;:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-yaml"&gt;# conf/agent/my_custom_config.yaml
defaults:
  - default
  - _self_

main_agent:
  tools:
    - tool-python                    # Execution environment
    - search_and_scrape_webpage      # Google search
    - jina_scrape_llm_summary        # Web scraping with LLM
    - tool-vqa                       # Vision processing (optional)
    - tool-transcribe                # Audio processing (optional)
    - tool-reasoning                 # Reasoning engine (optional)
    - tool-reading                   # Document reading (optional)
  max_turns: 400  # Maximum number of turns

sub_agents:
  agent-browsing:  # Optional sub-agent
    tools:
      - tool-google-search
      - tool-vqa
      - tool-reading
      - tool-python
    max_turns: 50

keep_tool_result: -1  # Context retention budget: -1 keeps all tool results, or specify K to keep only the K most recent tool responses
&lt;/code&gt;&lt;/pre&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;&lt;strong&gt;üí° Context Retention Strategy&lt;/strong&gt;: The &lt;code&gt;keep_tool_result&lt;/code&gt; parameter implements a &lt;strong&gt;recency-based context retention&lt;/strong&gt; strategy. In the standard ReAct paradigm, all tool outputs are retained in the message history, which can lead to inefficient context utilization. Empirically, we observe that the agent's subsequent actions depend primarily on recent observations rather than distant ones. This strategy retains only the most recent K tool responses (where K is the &lt;code&gt;keep_tool_result&lt;/code&gt; value) while preserving the complete sequence of thoughts and actions.&lt;/p&gt; 
  &lt;p&gt;&lt;strong&gt;Benefits:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;‚úÖ Preserves the reasoning and action trace&lt;/li&gt; 
   &lt;li&gt;‚úÖ Focuses the agent's attention on the most contextually relevant observations&lt;/li&gt; 
   &lt;li&gt;‚úÖ Frees additional context space for extended reasoning and deeper tool-use trajectories&lt;/li&gt; 
   &lt;li&gt;‚úÖ Does not lead to performance degradation while allowing more context space for interactive scaling&lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;p&gt;&lt;strong&gt;Usage:&lt;/strong&gt; Set &lt;code&gt;keep_tool_result: -1&lt;/code&gt; to keep all tool results, or specify a positive integer K (e.g., &lt;code&gt;keep_tool_result: 5&lt;/code&gt;) to keep only the K most recent tool responses.&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;ol start="2"&gt; 
  &lt;li&gt;&lt;strong&gt;Use your custom configuration&lt;/strong&gt; when running evaluations:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;cd apps/miroflow-agent
uv run main.py llm=qwen-3 agent=my_custom_config llm.base_url=https://your_base_url/v1
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="3"&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Configure environment variables&lt;/strong&gt; in &lt;code&gt;.env&lt;/code&gt; based on the tools you use.&lt;/p&gt; &lt;p&gt;All available environment variables are listed in &lt;code&gt;apps/miroflow-agent/.env.example&lt;/code&gt;. Copy it to &lt;code&gt;.env&lt;/code&gt; and configure the variables according to your chosen configuration:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cd apps/miroflow-agent
cp .env.example .env
# Edit .env with your actual API keys
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;For MiroThinker v1.5&lt;/strong&gt; (&lt;code&gt;mirothinker_v1.5_keep5_max200.yaml&lt;/code&gt;, &lt;code&gt;mirothinker_v1.5_keep5_max400.yaml&lt;/code&gt;, or &lt;code&gt;mirothinker_v1.5.yaml&lt;/code&gt;) and &lt;strong&gt;v1.0&lt;/strong&gt; (&lt;code&gt;mirothinker_v1.0_keep5.yaml&lt;/code&gt; or &lt;code&gt;mirothinker_v1.0.yaml&lt;/code&gt;), see the &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/#minimal-configuration-for-mirothinker-v15-and-v10"&gt;Minimal Configuration&lt;/a&gt; section above for the complete configuration example.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;For other configurations&lt;/strong&gt;, refer to the &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/#pre-configured-agent-settings"&gt;Pre-configured Agent Settings&lt;/a&gt; table above to see which environment variables are required.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;üîë Click to expand optional API keys&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# API for LLM-as-a-Judge (for benchmark testing, required for benchmark evaluation)
OPENAI_API_KEY=your_openai_key
OPENAI_BASE_URL="https://api.openai.com/v1"  # Optional, defaults to OpenAI's API

# API for Open-Source Audio Transcription Tool (for benchmark testing, optional)
WHISPER_MODEL_NAME="openai/whisper-large-v3-turbo"
WHISPER_API_KEY=your_whisper_key
WHISPER_BASE_URL="https://your_whisper_base_url/v1"

# API for Open-Source VQA Tool (for benchmark testing, optional)
VISION_MODEL_NAME="Qwen/Qwen2.5-VL-72B-Instruct"
VISION_API_KEY=your_vision_key
VISION_BASE_URL="https://your_vision_base_url/v1/chat/completions"

# API for Open-Source Reasoning Tool (for benchmark testing, optional)
REASONING_MODEL_NAME="Qwen/Qwen3-235B-A22B-Thinking-2507"
REASONING_API_KEY=your_reasoning_key
REASONING_BASE_URL="https://your_reasoning_base_url/v1/chat/completions"

# API for Claude Sonnet 3.7 as Commercial Tools (optional)
ANTHROPIC_API_KEY=your_anthropic_key

# API for Sogou Search (optional)
TENCENTCLOUD_SECRET_ID=your_tencent_cloud_secret_id
TENCENTCLOUD_SECRET_KEY=your_tencent_cloud_secret_key

# API for Summary LLM (can use small models like Qwen3-14B or GPT-5-Nano)
SUMMARY_LLM_BASE_URL="https://your_summary_llm_base_url/v1/chat/completions"
SUMMARY_LLM_MODEL_NAME=your_summary_llm_model_name  # e.g., "Qwen/Qwen3-14B" or "gpt-5-nano"
SUMMARY_LLM_API_KEY=your_summary_llm_api_key
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;Serve the MiroThinker Agent&lt;/h3&gt; 
&lt;h4&gt;Option 1 (Recommended): Serve with SGLang or vLLM&lt;/h4&gt; 
&lt;p&gt;Use SGLang to serve MiroThinker agents at port 61002:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;NUM_GPUS=4
PORT=61002

# Downloading agent from HF (v1.5 recommended)
AGENT_PATH=miromind-ai/MiroThinker-v1.5-30B

# Or use v1.0
# AGENT_PATH=miromind-ai/MiroThinker-v1.0-30B

python3 -m sglang.launch_server \
    --model-path $AGENT_PATH \
    --tp $NUM_GPUS \
    --dp 1 \
    --host 0.0.0.0 \
    --port $PORT \
    --trust-remote-code
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;üìç Server URL&lt;/strong&gt;: This will start a server at &lt;code&gt;http://0.0.0.0:$PORT&lt;/code&gt;. Use this as your server base URL (e.g., &lt;code&gt;http://0.0.0.0:61002/v1&lt;/code&gt;).&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Option 2: Quantized Light-Weight Options&lt;/h4&gt; 
&lt;p&gt;We also provide comprehensive guidance for serving MiroThinker agents using CPU-optimized and GPU-accelerated quantization techniques, along with detailed analysis and guidelines for deployment with llama.cpp, Ollama, SGLang, and other inference frameworks.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;üìñ Complete Guide&lt;/strong&gt;: See &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/apps/gradio-demo/"&gt;Deployment Documentation&lt;/a&gt; for detailed deployment instructions.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Run Your First Task&lt;/h3&gt; 
&lt;p&gt;After setting up the environment and starting your server, run &lt;code&gt;main.py&lt;/code&gt; to test with a default question: &lt;em&gt;"What is the title of today's arxiv paper in computer science?"&lt;/em&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd apps/miroflow-agent

# Using MiroThinker agents (requires your own server)
uv run python main.py llm=qwen-3 agent=mirothinker_v1.5_keep5_max200 llm.base_url=http://localhost:61002/v1

# Or using Claude (requires ANTHROPIC_API_KEY in .env)
uv run python main.py llm=claude-3-7 agent=single_agent_keep5

# Or using GPT-5 (requires OPENAI_API_KEY in .env)
uv run python main.py llm=gpt-5 agent=single_agent_keep5
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;To customize your question&lt;/strong&gt;, edit &lt;code&gt;main.py&lt;/code&gt; line 32:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;task_description = "Your custom question here"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The agent will search the web, execute code if needed, and provide an answer with sources.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;üìñ More details&lt;/strong&gt;: See &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/apps/miroflow-agent/README.md"&gt;apps/miroflow-agent/README.md&lt;/a&gt; for available configurations and troubleshooting.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;üìä Benchmark Evaluation&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;For researchers who want to reproduce our benchmark results or evaluate on standard benchmarks.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Download Benchmark Data&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd MiroThinker  # Back to project root
wget https://huggingface.co/datasets/miromind-ai/MiroFlow-Benchmarks/resolve/main/data_20251115_password_protected.zip
unzip data_20251115_password_protected.zip
# Password: pf4*
rm data_20251115_password_protected.zip
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Run Benchmark Evaluation&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; For MiroThinker v1.5, use &lt;code&gt;mirothinker_v1.5_keep5_max200&lt;/code&gt; (with context management), &lt;code&gt;mirothinker_v1.5_keep5_max400&lt;/code&gt; (with context management), or &lt;code&gt;mirothinker_v1.5&lt;/code&gt; configurations. For v1.0, use &lt;code&gt;mirothinker_v1.0_keep5&lt;/code&gt; (with context management) or &lt;code&gt;mirothinker_v1.0&lt;/code&gt; configurations.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;Available Parameters:&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;You can customize the evaluation by setting the following environment variables before running the script:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Parameter&lt;/th&gt; 
   &lt;th align="left"&gt;Default&lt;/th&gt; 
   &lt;th align="left"&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;LLM_AGENT&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;"MiroThinker-Agents"&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Agent name identifier&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;BASE_URL&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;"https://your-api.com/v1"&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Base URL of your server&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;NUM_RUNS&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Varies by benchmark&lt;/td&gt; 
   &lt;td align="left"&gt;Number of evaluation runs (3 for most benchmarks, 8 for GAIA/XBench/FutureX/SEAL-0, 32 for AIME2025)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;LLM_PROVIDER&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;"qwen"&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;LLM provider (e.g., &lt;code&gt;qwen&lt;/code&gt;, &lt;code&gt;openai&lt;/code&gt;, &lt;code&gt;anthropic&lt;/code&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;AGENT_SET&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;"mirothinker_v1.5_keep5_max200"&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Agent configuration (e.g., &lt;code&gt;mirothinker_v1.5_keep5_max200&lt;/code&gt;, &lt;code&gt;mirothinker_v1.5_keep5_max400&lt;/code&gt;, &lt;code&gt;mirothinker_v1.0_keep5&lt;/code&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;MAX_CONTEXT_LENGTH&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;262144&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Maximum context length (256K)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;MAX_CONCURRENT&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;10&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Maximum concurrent tasks&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;PASS_AT_K&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;1&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Pass@K evaluation metric&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;TEMPERATURE&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;1.0&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Sampling temperature&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;"xxx"&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;API key for the server&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Example Usage:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Navigate to the miroflow-agent directory first
cd apps/miroflow-agent

# Basic usage with v1.5 (recommended)
NUM_RUNS=8 LLM_AGENT="MiroThinker-v1.5-30B" BASE_URL="https://your-api.com/v1" bash scripts/run_evaluate_multiple_runs_gaia-validation-text-103.sh

# Or with v1.0
# NUM_RUNS=8 LLM_AGENT="MiroThinker-v1.0-30B" BASE_URL="https://your-api.com/v1" bash scripts/run_evaluate_multiple_runs_gaia-validation-text-103.sh

# Customize number of runs and agent configuration (v1.5 with context management)
LLM_AGENT="MiroThinker-v1.5-30B" \
BASE_URL="https://your-api.com/v1" \
NUM_RUNS=8 \
AGENT_SET="mirothinker_v1.5_keep5_max200" \
bash scripts/run_evaluate_multiple_runs_gaia-validation-text-103.sh

# Or with v1.0 configuration (with context management)
# LLM_AGENT="MiroThinker-v1.0-30B" \
# BASE_URL="https://your-api.com/v1" \
# NUM_RUNS=8 \
# AGENT_SET="mirothinker_v1.0_keep5" \
# bash scripts/run_evaluate_multiple_runs_gaia-validation-text-103.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;details open&gt; 
 &lt;summary&gt;üìã Click to expand all benchmark commands&lt;/summary&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;&lt;strong&gt;‚ö†Ô∏è Important for MiroThinker v1.5&lt;/strong&gt;: To reproduce our reported results, you must set the correct &lt;code&gt;AGENT_SET&lt;/code&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;BrowseComp &amp;amp; BrowseComp-ZH&lt;/strong&gt;: Use &lt;code&gt;AGENT_SET="mirothinker_v1.5_keep5_max400"&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;All other benchmarks&lt;/strong&gt;: Use &lt;code&gt;AGENT_SET="mirothinker_v1.5_keep5_max200"&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/blockquote&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Navigate to the miroflow-agent directory first
cd apps/miroflow-agent

# HLE
NUM_RUNS=3 LLM_AGENT="xxx" BASE_URL="xxx" AGENT_SET="mirothinker_v1.5_keep5_max200" bash scripts/run_evaluate_multiple_runs_hle.sh

# HLE-Text-2158
NUM_RUNS=3 LLM_AGENT="xxx" BASE_URL="xxx" AGENT_SET="mirothinker_v1.5_keep5_max200" bash scripts/run_evaluate_multiple_runs_hle-text-2158.sh

# HLE-Text-500
NUM_RUNS=3 LLM_AGENT="xxx" BASE_URL="xxx" AGENT_SET="mirothinker_v1.5_keep5_max200" bash scripts/run_evaluate_multiple_runs_hle-text-500.sh

# GAIA-Text-103
NUM_RUNS=8 LLM_AGENT="xxx" BASE_URL="xxx" AGENT_SET="mirothinker_v1.5_keep5_max200" bash scripts/run_evaluate_multiple_runs_gaia-validation-text-103.sh

# GAIA-Validation (GAIA-Val-165)
NUM_RUNS=8 LLM_AGENT="xxx" BASE_URL="xxx" AGENT_SET="mirothinker_v1.5_keep5_max200" bash scripts/run_evaluate_multiple_runs_gaia-validation.sh

# BrowseComp-EN (‚ö†Ô∏è use max400)
NUM_RUNS=3 LLM_AGENT="xxx" BASE_URL="xxx" AGENT_SET="mirothinker_v1.5_keep5_max400" bash scripts/run_evaluate_multiple_runs_browsecomp.sh

# BrowseComp-ZH (‚ö†Ô∏è use max400)
NUM_RUNS=3 LLM_AGENT="xxx" BASE_URL="xxx" AGENT_SET="mirothinker_v1.5_keep5_max400" bash scripts/run_evaluate_multiple_runs_browsecomp_zh.sh

# WebWalkerQA
NUM_RUNS=3 LLM_AGENT="xxx" BASE_URL="xxx" AGENT_SET="mirothinker_v1.5_keep5_max200" bash scripts/run_evaluate_multiple_runs_webwalkerqa.sh

# XBench-DeepSearch
NUM_RUNS=8 LLM_AGENT="xxx" BASE_URL="xxx" AGENT_SET="mirothinker_v1.5_keep5_max200" bash scripts/run_evaluate_multiple_runs_xbench_deepsearch.sh

# FRAMES
NUM_RUNS=3 LLM_AGENT="xxx" BASE_URL="xxx" AGENT_SET="mirothinker_v1.5_keep5_max200" bash scripts/run_evaluate_multiple_runs_frames.sh

# SEAL-0
NUM_RUNS=8 LLM_AGENT="xxx" BASE_URL="xxx" AGENT_SET="mirothinker_v1.5_keep5_max200" bash scripts/run_evaluate_multiple_runs_seal-0.sh

# FutureX
NUM_RUNS=8 LLM_AGENT="xxx" BASE_URL="xxx" AGENT_SET="mirothinker_v1.5_keep5_max200" bash scripts/run_evaluate_multiple_runs_futurex.sh

# AIME2025
NUM_RUNS=32 LLM_AGENT="xxx" BASE_URL="xxx" AGENT_SET="mirothinker_v1.5_keep5_max200" bash scripts/run_evaluate_multiple_runs_aime2025.sh

# DeepSearchQA
NUM_RUNS=3 LLM_AGENT="xxx" BASE_URL="xxx" AGENT_SET="mirothinker_v1.5_keep5_max200" bash scripts/run_evaluate_multiple_runs_deepsearchqa.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h4&gt;3. &lt;strong&gt;Monitor evaluation progress&lt;/strong&gt;&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;üìä Click to expand progress monitoring commands&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Navigate to the miroflow-agent directory first
cd apps/miroflow-agent

# For HLE
python benchmarks/check_progress/check_progress_hle.py /path/to/evaluation/logs

# For HLE-Text-2158
python benchmarks/check_progress/check_progress_hle-text-2158.py /path/to/evaluation/logs

# For HLE-Text-500
python benchmarks/check_progress/check_progress_hle-text-500.py /path/to/evaluation/logs

# For BrowseComp-EN
python benchmarks/check_progress/check_progress_browsecomp.py /path/to/evaluation/logs

# For BrowseComp-ZH
python benchmarks/check_progress/check_progress_browsecomp_zh.py /path/to/evaluation/logs

# For GAIA-Validation
python benchmarks/check_progress/check_progress_gaia-validation.py /path/to/evaluation/logs

# For GAIA-Text-103
python benchmarks/check_progress/check_progress_gaia-validation-text-103.py /path/to/evaluation/logs

# For WebWalkerQA
python benchmarks/check_progress/check_progress_webwalkerqa.py /path/to/evaluation/logs

# For Frames
python benchmarks/check_progress/check_progress_frames.py /path/to/evaluation/logs

# For XBench-DeepSearch
python benchmarks/check_progress/check_progress_xbench_deepsearch.py /path/to/evaluation/logs

# For SEAL-0
python benchmarks/check_progress/check_progress_seal-0.py /path/to/evaluation/logs

# For AIME2025
python benchmarks/check_progress/check_progress_aime2025.py /path/to/evaluation/logs

# For DeepSearchQA
python benchmarks/check_progress/check_progress_deepsearchqa.py /path/to/evaluation/logs
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;üî¨ Trace Collection&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;üìã Click to expand trace collection commands&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;cd apps/collect-trace

# Collect Traces for SFT
bash scripts/collect_trace_claude37.sh
bash scripts/collect_trace_gpt5.sh

# Collect Traces for DPO
bash scripts/collect_trace_qwen3.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;‚ùì FAQ &amp;amp; Troubleshooting&lt;/h2&gt; 
&lt;h3&gt;Common Issues&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;üîß Click to expand troubleshooting guide&lt;/summary&gt; 
 &lt;h4&gt;&lt;strong&gt;Q: Which version should I use?&lt;/strong&gt;&lt;/h4&gt; 
 &lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; We recommend &lt;strong&gt;MiroThinker v1.5&lt;/strong&gt; ‚≠ê with the minimal configuration:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;v1.5&lt;/strong&gt; ‚≠ê: Latest version with 256K context, world-leading performance. Use config (with context management): 
   &lt;ul&gt; 
    &lt;li&gt;&lt;code&gt;mirothinker_v1.5_keep5_max200&lt;/code&gt; (up to 200 turns, recommended for most tasks)&lt;/li&gt; 
    &lt;li&gt;&lt;code&gt;mirothinker_v1.5_keep5_max400&lt;/code&gt; (up to 400 turns, only used for BrowseComp and BrowseComp-ZH)&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h4&gt;&lt;strong&gt;Q: How do I get API keys?&lt;/strong&gt;&lt;/h4&gt; 
 &lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; You need these keys for minimal setup:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;SERPER_API_KEY&lt;/strong&gt;: Get from &lt;a href="https://serper.dev/"&gt;Serper.dev&lt;/a&gt; (Google search API)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;JINA_API_KEY&lt;/strong&gt;: Get from &lt;a href="https://jina.ai/"&gt;Jina.ai&lt;/a&gt; (Web scraping)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;E2B_API_KEY&lt;/strong&gt;: Get from &lt;a href="https://e2b.dev/"&gt;E2B.dev&lt;/a&gt; (Code execution sandbox)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;SUMMARY_LLM_API_KEY&lt;/strong&gt;: Your LLM API credentials (for content summarization). Can be a small model like Qwen3-14B or GPT-5-Nano‚Äîthe choice has minimal impact on performance.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;OPENAI_API_KEY&lt;/strong&gt;: Get from &lt;a href="https://platform.openai.com/"&gt;OpenAI&lt;/a&gt; (Required for benchmark evaluation, used for LLM-as-a-Judge)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;OPENAI_BASE_URL&lt;/strong&gt;: Optional, defaults to &lt;code&gt;https://api.openai.com/v1&lt;/code&gt;. Can be changed to use OpenAI-compatible APIs.&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h4&gt;&lt;strong&gt;Q: Agent server connection errors&lt;/strong&gt;&lt;/h4&gt; 
 &lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Common issues:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Check base URL format&lt;/strong&gt;: Should end with &lt;code&gt;/v1&lt;/code&gt; (e.g., &lt;code&gt;https://your-api.com/v1&lt;/code&gt;)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Verify API key&lt;/strong&gt;: Ensure &lt;code&gt;API_KEY&lt;/code&gt; is set correctly in environment or script&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Check server status&lt;/strong&gt;: Make sure your server is running and accessible&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Network issues&lt;/strong&gt;: Verify firewall/network settings allow connections&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h4&gt;&lt;strong&gt;Q: Evaluation script fails to run&lt;/strong&gt;&lt;/h4&gt; 
 &lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Troubleshooting steps:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;strong&gt;Check working directory&lt;/strong&gt;: Make sure you're in &lt;code&gt;apps/miroflow-agent&lt;/code&gt; directory&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Verify environment&lt;/strong&gt;: Run &lt;code&gt;uv sync&lt;/code&gt; to ensure dependencies are installed&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Check .env file&lt;/strong&gt;: Ensure all required environment variables are set&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Review logs&lt;/strong&gt;: Check &lt;code&gt;logs/&lt;/code&gt; directory for detailed error messages&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Verify data path&lt;/strong&gt;: Ensure benchmark data is downloaded and in correct location&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;h4&gt;&lt;strong&gt;Q: Out of memory errors&lt;/strong&gt;&lt;/h4&gt; 
 &lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Solutions:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Reduce context length&lt;/strong&gt;: Set &lt;code&gt;MAX_CONTEXT_LENGTH&lt;/code&gt; to a smaller value (e.g., 131072 for 128K)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Use context management with fewer turns&lt;/strong&gt;: 
   &lt;ul&gt; 
    &lt;li&gt;For v1.5: Use &lt;code&gt;mirothinker_v1.5_keep5_max200&lt;/code&gt; or &lt;code&gt;mirothinker_v1.5_keep5_max400&lt;/code&gt; (with context management)&lt;/li&gt; 
    &lt;li&gt;For v1.0: Use &lt;code&gt;mirothinker_v1.0_keep5&lt;/code&gt; (with context management)&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Reduce concurrent tasks&lt;/strong&gt;: Set &lt;code&gt;MAX_CONCURRENT&lt;/code&gt; to a smaller number (e.g., 5)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Use smaller agents&lt;/strong&gt;: 
   &lt;ul&gt; 
    &lt;li&gt;For v1.5: Try 30B instead of 235B&lt;/li&gt; 
    &lt;li&gt;For v1.0: Try 8B or 30B instead of 72B&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h4&gt;&lt;strong&gt;Q: Tool execution errors&lt;/strong&gt;&lt;/h4&gt; 
 &lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Common fixes:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;E2B errors&lt;/strong&gt;: Verify &lt;code&gt;E2B_API_KEY&lt;/code&gt; is valid and account has credits&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Serper errors&lt;/strong&gt;: Check &lt;code&gt;SERPER_API_KEY&lt;/code&gt; and rate limits&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Jina errors&lt;/strong&gt;: Verify &lt;code&gt;JINA_API_KEY&lt;/code&gt; and &lt;code&gt;JINA_BASE_URL&lt;/code&gt; are correct&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;LLM summarization errors&lt;/strong&gt;: Check &lt;code&gt;SUMMARY_LLM_*&lt;/code&gt; variables and agent availability&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h4&gt;&lt;strong&gt;Q: How to monitor long-running evaluations?&lt;/strong&gt;&lt;/h4&gt; 
 &lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Use the progress monitoring scripts:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;cd apps/miroflow-agent
python benchmarks/check_progress/check_progress_&amp;lt;benchmark_name&amp;gt;.py /path/to/logs
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;The scripts show completion status, elapsed time, and estimated remaining time.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h3&gt;Getting Help&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìñ &lt;strong&gt;Documentation&lt;/strong&gt;: Check &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/libs/miroflow-tools/README.md"&gt;MiroFlow Tools README&lt;/a&gt; for tool details&lt;/li&gt; 
 &lt;li&gt;üí¨ &lt;strong&gt;Discord&lt;/strong&gt;: Join our &lt;a href="https://discord.com/invite/GPqEnkzQZd"&gt;Discord community&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üêõ &lt;strong&gt;Issues&lt;/strong&gt;: Report bugs on &lt;a href="https://github.com/MiroMindAI/MiroThinker/issues"&gt;GitHub Issues&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üìß &lt;strong&gt;Contact&lt;/strong&gt;: Visit &lt;a href="https://miromind.ai/"&gt;our website&lt;/a&gt; for more information&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üìÑ License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the MIT License - see the &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;h2&gt;üôè Acknowledgments&lt;/h2&gt; 
&lt;p&gt;We extend our sincere gratitude to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üèÜ &lt;strong&gt;Benchmark Contributors&lt;/strong&gt; for the comprehensive evaluation datasets&lt;/li&gt; 
 &lt;li&gt;üåç &lt;strong&gt;Open Source Community&lt;/strong&gt; for the tools and libraries that make this possible&lt;/li&gt; 
 &lt;li&gt;üë• &lt;strong&gt;All Contributors&lt;/strong&gt; who have helped make MiroThinker better&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/MiroMindAI/MiroThinker/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=MiroMindAI/MiroThinker" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;p&gt;Join our community and help us build the future of AI agents!&lt;/p&gt; 
&lt;h3&gt;References&lt;/h3&gt; 
&lt;p&gt;If you find this project useful in your research, please consider citing:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@article{miromind2025mirothinker,
  title={MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling},
  author={MiroMind Team and Bai, Song and Bing, Lidong and Chen, Carson and Chen, Guanzheng and Chen, Yuntao and Chen, Zhe and Chen, Ziyi and Dong, Xuan and others},
  journal={arXiv preprint arXiv:2511.11793},
  year={2025}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#MiroMindAI/MiroThinker&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=MiroMindAI/MiroThinker&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>hacksider/Deep-Live-Cam</title>
      <link>https://github.com/hacksider/Deep-Live-Cam</link>
      <description>&lt;p&gt;real time face swap and one-click video deepfake with only a single image&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt;Deep-Live-Cam 2.0.1c&lt;/h1&gt; 
&lt;p align="center"&gt; Real-time face swap and video deepfake with a single click and only a single image. &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/11395" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/11395" alt="hacksider%2FDeep-Live-Cam | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/media/demo.gif" alt="Demo GIF" width="800" /&gt; &lt;/p&gt; 
&lt;h2&gt;Disclaimer&lt;/h2&gt; 
&lt;p&gt;This deepfake software is designed to be a productive tool for the AI-generated media industry. It can assist artists in animating custom characters, creating engaging content, and even using models for clothing design.&lt;/p&gt; 
&lt;p&gt;We are aware of the potential for unethical applications and are committed to preventative measures. A built-in check prevents the program from processing inappropriate media (nudity, graphic content, sensitive material like war footage, etc.). We will continue to develop this project responsibly, adhering to the law and ethics. We may shut down the project or add watermarks if legally required.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Ethical Use: Users are expected to use this software responsibly and legally. If using a real person's face, obtain their consent and clearly label any output as a deepfake when sharing online.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Content Restrictions: The software includes built-in checks to prevent processing inappropriate media, such as nudity, graphic content, or sensitive material.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Legal Compliance: We adhere to all relevant laws and ethical guidelines. If legally required, we may shut down the project or add watermarks to the output.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;User Responsibility: We are not responsible for end-user actions. Users must ensure their use of the software aligns with ethical standards and legal requirements.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;By using this software, you agree to these terms and commit to using it in a manner that respects the rights and dignity of others.&lt;/p&gt; 
&lt;p&gt;Users are expected to use this software responsibly and legally. If using a real person's face, obtain their consent and clearly label any output as a deepfake when sharing online. We are not responsible for end-user actions.&lt;/p&gt; 
&lt;h2&gt;Exclusive v2.4 Quick Start - Pre-built (Windows/Mac Silicon)&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://deeplivecam.net/index.php/quickstart"&gt; &lt;img src="https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/media/Download.png" width="285" height="77" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;a href="https://deeplivecam.net/index.php/quickstart"&gt; &lt;h5&gt;This is the fastest build you can get if you have a discrete NVIDIA or AMD GPU or Mac Silicon, And you'll receive special priority support.&lt;/h5&gt; &lt;h6&gt;These Pre-builts are perfect for non-technical users or those who don't have time to, or can't manually install all the requirements. Just a heads-up: this is an open-source project, so you can also install it manually.&lt;/h6&gt; &lt;h2&gt;TLDR; Live Deepfake in just 3 Clicks&lt;/h2&gt; &lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/af825228-852c-411b-b787-ffd9aac72fc6" alt="easysteps" /&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Select a face&lt;/li&gt; 
  &lt;li&gt;Select which camera to use&lt;/li&gt; 
  &lt;li&gt;Press live!&lt;/li&gt; 
 &lt;/ol&gt; &lt;h2&gt;Features &amp;amp; Uses - Everything is in real-time&lt;/h2&gt; &lt;h3&gt;Mouth Mask&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Retain your original mouth for accurate movement using Mouth Mask&lt;/strong&gt;&lt;/p&gt; &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/media/ludwig.gif" alt="resizable-gif" /&gt; &lt;/p&gt; &lt;h3&gt;Face Mapping&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Use different faces on multiple subjects simultaneously&lt;/strong&gt;&lt;/p&gt; &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/media/streamers.gif" alt="face_mapping_source" /&gt; &lt;/p&gt; &lt;h3&gt;Your Movie, Your Face&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Watch movies with any face in real-time&lt;/strong&gt;&lt;/p&gt; &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/media/movie.gif" alt="movie" /&gt; &lt;/p&gt; &lt;h3&gt;Live Show&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Run Live shows and performances&lt;/strong&gt;&lt;/p&gt; &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/media/live_show.gif" alt="show" /&gt; &lt;/p&gt; &lt;h3&gt;Memes&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Create Your Most Viral Meme Yet&lt;/strong&gt;&lt;/p&gt; &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/media/meme.gif" alt="show" width="450" /&gt; &lt;br /&gt; &lt;sub&gt;Created using Many Faces feature in Deep-Live-Cam&lt;/sub&gt; &lt;/p&gt; &lt;h3&gt;Omegle&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Surprise people on Omegle&lt;/strong&gt;&lt;/p&gt; &lt;p align="center"&gt; 
  &lt;video src="https://github.com/user-attachments/assets/2e9b9b82-fa04-4b70-9f56-b1f68e7672d0" width="450" controls&gt;&lt;/video&gt; &lt;/p&gt; &lt;h2&gt;Installation (Manual)&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;Please be aware that the installation requires technical skills and is not for beginners. Consider downloading the quickstart version.&lt;/strong&gt;&lt;/p&gt; &lt;/a&gt;
&lt;details&gt;
 &lt;a href="https://deeplivecam.net/index.php/quickstart"&gt; &lt;summary&gt;Click to see the process&lt;/summary&gt; &lt;h3&gt;Installation&lt;/h3&gt; &lt;p&gt;This is more likely to work on your computer but will be slower as it utilizes the CPU.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. Set up Your Platform&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Python (3.11 recommended)&lt;/li&gt; 
   &lt;li&gt;pip&lt;/li&gt; 
   &lt;li&gt;git&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=OlNWCpFdVMA"&gt;ffmpeg&lt;/a&gt; - &lt;code&gt;iex (irm ffmpeg.tc.ht)&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://visualstudio.microsoft.com/visual-cpp-build-tools/"&gt;Visual Studio 2022 Runtimes (Windows)&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt;&lt;/a&gt; 
 &lt;p&gt;&lt;strong&gt;2. Clone the Repository&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/hacksider/Deep-Live-Cam.git
cd Deep-Live-Cam
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;3. Download the Models&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/hacksider/deep-live-cam/resolve/main/GFPGANv1.4.pth"&gt;GFPGANv1.4&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/hacksider/deep-live-cam/resolve/main/inswapper_128_fp16.onnx"&gt;inswapper_128_fp16.onnx&lt;/a&gt;&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;Place these files in the "&lt;strong&gt;models&lt;/strong&gt;" folder.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;4. Install Dependencies&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;We highly recommend using a &lt;code&gt;venv&lt;/code&gt; to avoid issues.&lt;/p&gt; 
 &lt;p&gt;For Windows:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;python -m venv venv
venv\Scripts\activate
pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;For Linux:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Ensure you use the installed Python 3.10
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;For macOS:&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;Apple Silicon (M1/M2/M3) requires specific setup:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Install Python 3.11 (specific version is important)
brew install python@3.11

# Install tkinter package (required for the GUI)
brew install python-tk@3.10

# Create and activate virtual environment with Python 3.11
python3.11 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;** In case something goes wrong and you need to reinstall the virtual environment **&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Deactivate the virtual environment
rm -rf venv

# Reinstall the virtual environment
python -m venv venv
source venv/bin/activate

# install the dependencies again
pip install -r requirements.txt

# gfpgan and basicsrs issue fix
pip install git+https://github.com/xinntao/BasicSR.git@master
pip uninstall gfpgan -y
pip install git+https://github.com/TencentARC/GFPGAN.git@master
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Run:&lt;/strong&gt; If you don't have a GPU, you can run Deep-Live-Cam using &lt;code&gt;python run.py&lt;/code&gt;. Note that initial execution will download models (~300MB).&lt;/p&gt; 
 &lt;h3&gt;GPU Acceleration&lt;/h3&gt; 
 &lt;p&gt;&lt;strong&gt;CUDA Execution Provider (Nvidia)&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Install &lt;a href="https://developer.nvidia.com/cuda-12-8-0-download-archive"&gt;CUDA Toolkit 12.8.0&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Install &lt;a href="https://developer.nvidia.com/rdp/cudnn-archive"&gt;cuDNN v8.9.7 for CUDA 12.x&lt;/a&gt; (required for onnxruntime-gpu): 
   &lt;ul&gt; 
    &lt;li&gt;Download cuDNN v8.9.7 for CUDA 12.x&lt;/li&gt; 
    &lt;li&gt;Make sure the cuDNN bin directory is in your system PATH&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;Install dependencies:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;pip install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128
pip uninstall onnxruntime onnxruntime-gpu
pip install onnxruntime-gpu==1.21.0
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="3"&gt; 
  &lt;li&gt;Usage:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;python run.py --execution-provider cuda
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;CoreML Execution Provider (Apple Silicon)&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;Apple Silicon (M1/M2/M3) specific installation:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Make sure you've completed the macOS setup above using Python 3.10.&lt;/li&gt; 
  &lt;li&gt;Install dependencies:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;pip uninstall onnxruntime onnxruntime-silicon
pip install onnxruntime-silicon==1.13.1
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="3"&gt; 
  &lt;li&gt;Usage (important: specify Python 3.10):&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;python3.10 run.py --execution-provider coreml
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Important Notes for macOS:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;You &lt;strong&gt;must&lt;/strong&gt; use Python 3.10, not newer versions like 3.11 or 3.13&lt;/li&gt; 
  &lt;li&gt;Always run with &lt;code&gt;python3.10&lt;/code&gt; command not just &lt;code&gt;python&lt;/code&gt; if you have multiple Python versions installed&lt;/li&gt; 
  &lt;li&gt;If you get error about &lt;code&gt;_tkinter&lt;/code&gt; missing, reinstall the tkinter package: &lt;code&gt;brew reinstall python-tk@3.10&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;If you get model loading errors, check that your models are in the correct folder&lt;/li&gt; 
  &lt;li&gt;If you encounter conflicts with other Python versions, consider uninstalling them: &lt;pre&gt;&lt;code class="language-bash"&gt;# List all installed Python versions
brew list | grep python

# Uninstall conflicting versions if needed
brew uninstall --ignore-dependencies python@3.11 python@3.13

# Keep only Python 3.11
brew cleanup
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;CoreML Execution Provider (Apple Legacy)&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Install dependencies:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;pip uninstall onnxruntime onnxruntime-coreml
pip install onnxruntime-coreml==1.21.0
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="2"&gt; 
  &lt;li&gt;Usage:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;python run.py --execution-provider coreml
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;DirectML Execution Provider (Windows)&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Install dependencies:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;pip uninstall onnxruntime onnxruntime-directml
pip install onnxruntime-directml==1.21.0
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="2"&gt; 
  &lt;li&gt;Usage:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;python run.py --execution-provider directml
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;OpenVINO‚Ñ¢ Execution Provider (Intel)&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Install dependencies:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;pip uninstall onnxruntime onnxruntime-openvino
pip install onnxruntime-openvino==1.21.0
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="2"&gt; 
  &lt;li&gt;Usage:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;python run.py --execution-provider openvino
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;1. Image/Video Mode&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Execute &lt;code&gt;python run.py&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Choose a source face image and a target image/video.&lt;/li&gt; 
 &lt;li&gt;Click "Start".&lt;/li&gt; 
 &lt;li&gt;The output will be saved in a directory named after the target video.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;2. Webcam Mode&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Execute &lt;code&gt;python run.py&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Select a source face image.&lt;/li&gt; 
 &lt;li&gt;Click "Live".&lt;/li&gt; 
 &lt;li&gt;Wait for the preview to appear (10-30 seconds).&lt;/li&gt; 
 &lt;li&gt;Use a screen capture tool like OBS to stream.&lt;/li&gt; 
 &lt;li&gt;To change the face, select a new source image.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Command Line Arguments (Unmaintained)&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;options:
  -h, --help                                               show this help message and exit
  -s SOURCE_PATH, --source SOURCE_PATH                     select a source image
  -t TARGET_PATH, --target TARGET_PATH                     select a target image or video
  -o OUTPUT_PATH, --output OUTPUT_PATH                     select output file or directory
  --frame-processor FRAME_PROCESSOR [FRAME_PROCESSOR ...]  frame processors (choices: face_swapper, face_enhancer, ...)
  --keep-fps                                               keep original fps
  --keep-audio                                             keep original audio
  --keep-frames                                            keep temporary frames
  --many-faces                                             process every face
  --map-faces                                              map source target faces
  --mouth-mask                                             mask the mouth region
  --video-encoder {libx264,libx265,libvpx-vp9}             adjust output video encoder
  --video-quality [0-51]                                   adjust output video quality
  --live-mirror                                            the live camera display as you see it in the front-facing camera frame
  --live-resizable                                         the live camera frame is resizable
  --max-memory MAX_MEMORY                                  maximum amount of RAM in GB
  --execution-provider {cpu} [{cpu} ...]                   available execution provider (choices: cpu, ...)
  --execution-threads EXECUTION_THREADS                    number of execution threads
  -v, --version                                            show program's version number and exit
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Looking for a CLI mode? Using the -s/--source argument will make the run program in cli mode.&lt;/p&gt; 
&lt;h2&gt;Press&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;We are always open to criticism and are ready to improve, that's why we didn't cherry-pick anything.&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://arstechnica.com/information-technology/2024/08/new-ai-tool-enables-real-time-face-swapping-on-webcams-raising-fraud-concerns/"&gt;&lt;em&gt;"Deep-Live-Cam goes viral, allowing anyone to become a digital doppelganger"&lt;/em&gt;&lt;/a&gt; - Ars Technica&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://dataconomy.com/2024/08/15/what-is-deep-live-cam-github-deepfake/"&gt;&lt;em&gt;"Thanks Deep Live Cam, shapeshifters are among us now"&lt;/em&gt;&lt;/a&gt; - Dataconomy&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.newsbytesapp.com/news/science/deep-live-cam-ai-impersonation-tool-goes-viral/story"&gt;&lt;em&gt;"This free AI tool lets you become anyone during video-calls"&lt;/em&gt;&lt;/a&gt; - NewsBytes&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.creativebloq.com/ai/ok-this-viral-ai-live-stream-software-is-truly-terrifying"&gt;&lt;em&gt;"OK, this viral AI live stream software is truly terrifying"&lt;/em&gt;&lt;/a&gt; - Creative Bloq&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://petapixel.com/2024/08/14/deep-live-cam-deepfake-ai-tool-lets-you-become-anyone-in-a-video-call-with-single-photo-mark-zuckerberg-jd-vance-elon-musk/"&gt;&lt;em&gt;"Deepfake AI Tool Lets You Become Anyone in a Video Call With Single Photo"&lt;/em&gt;&lt;/a&gt; - PetaPixel&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.techeblog.com/deep-live-cam-ai-transform-face/"&gt;&lt;em&gt;"Deep-Live-Cam Uses AI to Transform Your Face in Real-Time, Celebrities Included"&lt;/em&gt;&lt;/a&gt; - TechEBlog&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://telegrafi.com/en/a-tool-that-makes-you-look-like-anyone-during-a-video-call-is-going-viral-on-the-Internet/"&gt;&lt;em&gt;"An AI tool that "makes you look like anyone" during a video call is going viral online"&lt;/em&gt;&lt;/a&gt; - Telegrafi&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://decrypt.co/244565/this-deepfake-tool-turning-images-into-livestreams-is-topping-the-github-charts"&gt;&lt;em&gt;"This Deepfake Tool Turning Images Into Livestreams is Topping the GitHub Charts"&lt;/em&gt;&lt;/a&gt; - Emerge&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.digitalmusicnews.com/2024/08/15/face-swapping-ai-real-time-mimic/"&gt;&lt;em&gt;"New Real-Time Face-Swapping AI Allows Anyone to Mimic Famous Faces"&lt;/em&gt;&lt;/a&gt; - Digital Music News&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.diyphotography.net/this-real-time-webcam-deepfake-tool-raises-alarms-about-the-future-of-identity-theft/"&gt;&lt;em&gt;"This real-time webcam deepfake tool raises alarms about the future of identity theft"&lt;/em&gt;&lt;/a&gt; - DIYPhotography&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/watch?time_continue=1074&amp;amp;v=py4Tc-Y8BcY"&gt;&lt;em&gt;"That's Crazy, Oh God. That's Fucking Freaky Dude... That's So Wild Dude"&lt;/em&gt;&lt;/a&gt; - SomeOrdinaryGamers&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/live/mFsCe7AIxq8?feature=shared&amp;amp;t=2686"&gt;&lt;em&gt;"Alright look look look, now look chat, we can do any face we want to look like chat"&lt;/em&gt;&lt;/a&gt; - IShowSpeed&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=wnCghLjqv3s&amp;amp;t=551s"&gt;&lt;em&gt;"They do a pretty good job matching poses, expression and even the lighting"&lt;/em&gt;&lt;/a&gt; - TechLinked (LTT)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.golem.de/news/deepfakes-als-sean-connery-an-der-redaktionskonferenz-teilnahm-2408-188172.html"&gt;&lt;em&gt;"Als Sean Connery an der Redaktionskonferenz teilnahm"&lt;/em&gt;&lt;/a&gt; - Golem.de (German)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://youtu.be/JbUPRmXRUtE?t=3964"&gt;&lt;em&gt;"What the F&lt;/em&gt;**! Why do I look like Vinny Jr? I look exactly like Vinny Jr!? No, this shit is crazy! Bro This is F*** Crazy! "*&lt;/a&gt; - IShowSpeed&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Credits&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://ffmpeg.org/"&gt;ffmpeg&lt;/a&gt;: for making video-related operations easy&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/henryruhs"&gt;Henry&lt;/a&gt;: One of the major contributor in this repo&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/deepinsight"&gt;deepinsight&lt;/a&gt;: for their &lt;a href="https://github.com/deepinsight/insightface"&gt;insightface&lt;/a&gt; project which provided a well-made library and models. Please be reminded that the &lt;a href="https://github.com/deepinsight/insightface?tab=readme-ov-file#license"&gt;use of the model is for non-commercial research purposes only&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/havok2-htwo"&gt;havok2-htwo&lt;/a&gt;: for sharing the code for webcam&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/GosuDRM"&gt;GosuDRM&lt;/a&gt;: for the open version of roop&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pereiraroland26"&gt;pereiraroland26&lt;/a&gt;: Multiple faces support&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/vic4key"&gt;vic4key&lt;/a&gt;: For supporting/contributing to this project&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/kier007"&gt;kier007&lt;/a&gt;: for improving the user experience&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/qitianai"&gt;qitianai&lt;/a&gt;: for multi-lingual support&lt;/li&gt; 
 &lt;li&gt;and &lt;a href="https://github.com/hacksider/Deep-Live-Cam/graphs/contributors"&gt;all developers&lt;/a&gt; behind libraries used in this project.&lt;/li&gt; 
 &lt;li&gt;Footnote: Please be informed that the base author of the code is &lt;a href="https://github.com/s0md3v/roop"&gt;s0md3v&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;All the wonderful users who helped make this project go viral by starring the repo ‚ù§Ô∏è&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://github.com/hacksider/Deep-Live-Cam/stargazers"&gt;&lt;img src="https://reporoster.com/stars/hacksider/Deep-Live-Cam" alt="Stargazers" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Contributions&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://repobeats.axiom.co/api/embed/fec8e29c45dfdb9c5916f3a7830e1249308d20e1.svg?sanitize=true" alt="Alt" title="Repobeats analytics image" /&gt;&lt;/p&gt; 
&lt;h2&gt;Stars to the Moon üöÄ&lt;/h2&gt; 
&lt;a href="https://star-history.com/#hacksider/deep-live-cam&amp;amp;Date"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=hacksider/deep-live-cam&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=hacksider/deep-live-cam&amp;amp;type=Date" /&gt; 
  &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=hacksider/deep-live-cam&amp;amp;type=Date" /&gt; 
 &lt;/picture&gt; &lt;/a&gt;</description>
    </item>
    
    <item>
      <title>NevaMind-AI/memU</title>
      <link>https://github.com/NevaMind-AI/memU</link>
      <description>&lt;p&gt;Memory infrastructure for LLMs and AI agents&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/banner.png" alt="MemU Banner" /&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;h1&gt;MemU&lt;/h1&gt; 
 &lt;h3&gt;A Future-Oriented Agentic Memory System&lt;/h3&gt; 
 &lt;p&gt;&lt;a href="https://badge.fury.io/py/memu-py"&gt;&lt;img src="https://badge.fury.io/py/memu-py.svg?sanitize=true" alt="PyPI version" /&gt;&lt;/a&gt; &lt;a href="https://opensource.org/licenses/Apache-2.0"&gt;&lt;img src="https://img.shields.io/badge/License-Apache%202.0-blue.svg?sanitize=true" alt="License: Apache 2.0" /&gt;&lt;/a&gt; &lt;a href="https://www.python.org/downloads/"&gt;&lt;img src="https://img.shields.io/badge/python-3.13+-blue.svg?sanitize=true" alt="Python 3.13+" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/memu"&gt;&lt;img src="https://img.shields.io/badge/Discord-Join%20Chat-5865F2?logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://x.com/memU_ai"&gt;&lt;img src="https://img.shields.io/badge/Twitter-Follow-1DA1F2?logo=x&amp;amp;logoColor=white" alt="Twitter" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/17374" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/17374" alt="NevaMind-AI%2FmemU | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;p&gt;MemU is an agentic memory framework for LLM and AI agent backends. It receives &lt;strong&gt;multimodal inputs&lt;/strong&gt; (conversations, documents, images), extracts them into structured memory, and organizes them into a &lt;strong&gt;hierarchical file system&lt;/strong&gt; that supports both &lt;strong&gt;embedding-based (RAG)&lt;/strong&gt; and &lt;strong&gt;non-embedding (LLM)&lt;/strong&gt; retrieval.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;‚≠êÔ∏è Star the repository&lt;/h2&gt; 
&lt;img width="100%" src="https://github.com/NevaMind-AI/memU/raw/main/assets/star.gif" /&gt; If you find memU useful or interesting, a GitHub Star ‚≠êÔ∏è would be greatly appreciated. 
&lt;hr /&gt; 
&lt;p&gt;MemU is collaborating with four open-source projects to launch the 2026 New Year Challenge. üéâBetween January 8‚Äì18, contributors can submit PRs to memU and earn cash rewards, community recognition, and platform credits. üéÅ&lt;a href="https://discord.gg/KaWy6SBAsx"&gt;Learn more &amp;amp; get involved&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;‚ú® Core Features&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Feature&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üóÇÔ∏è &lt;strong&gt;Hierarchical File System&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Three-layer architecture: Resource ‚Üí Item ‚Üí Category with full traceability&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üîç &lt;strong&gt;Dual Retrieval Methods&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;RAG (embedding-based) for speed, LLM (non-embedding) for deep semantic understanding&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üé® &lt;strong&gt;Multimodal Support&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Process conversations, documents, images, audio, and video&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üîÑ &lt;strong&gt;Self-Evolving Memory&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Memory structure adapts and improves based on usage patterns&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üóÇÔ∏è Hierarchical File System&lt;/h2&gt; 
&lt;p&gt;MemU organizes memory using a &lt;strong&gt;three-layer architecture&lt;/strong&gt; inspired by hierarchical storage systems:&lt;/p&gt; 
&lt;img width="100%" alt="structure" src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/structure.png" /&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Layer&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Examples&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Resource&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Raw multimodal data warehouse&lt;/td&gt; 
   &lt;td&gt;JSON conversations, text documents, images, videos&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Item&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Discrete extracted memory units&lt;/td&gt; 
   &lt;td&gt;Individual preferences, skills, opinions, habits&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Category&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Aggregated textual memory with summaries&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;preferences.md&lt;/code&gt;, &lt;code&gt;work_life.md&lt;/code&gt;, &lt;code&gt;relationships.md&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Key Benefits:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Full Traceability&lt;/strong&gt;: Track from raw data ‚Üí items ‚Üí categories and back&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Progressive Summarization&lt;/strong&gt;: Each layer provides increasingly abstracted views&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Flexible Organization&lt;/strong&gt;: Categories evolve based on content patterns&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üé® Multimodal Support&lt;/h2&gt; 
&lt;p&gt;MemU processes diverse content types into unified memory:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Modality&lt;/th&gt; 
   &lt;th&gt;Input&lt;/th&gt; 
   &lt;th&gt;Processing&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;conversation&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;JSON chat logs&lt;/td&gt; 
   &lt;td&gt;Extract preferences, opinions, habits, relationships&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;document&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Text files (.txt, .md)&lt;/td&gt; 
   &lt;td&gt;Extract knowledge, skills, facts&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;image&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;PNG, JPG, etc.&lt;/td&gt; 
   &lt;td&gt;Vision model extracts visual concepts and descriptions&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;video&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Video files&lt;/td&gt; 
   &lt;td&gt;Frame extraction + vision analysis&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;audio&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Audio files&lt;/td&gt; 
   &lt;td&gt;Transcription + text processing&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;All modalities are unified into the same three-layer hierarchy, enabling cross-modal retrieval.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üöÄ Quick Start&lt;/h2&gt; 
&lt;h3&gt;Option 1: Cloud Version&lt;/h3&gt; 
&lt;p&gt;Try MemU instantly without any setup:&lt;/p&gt; 
&lt;p&gt;üëâ &lt;strong&gt;&lt;a href="https://memu.so"&gt;memu.so&lt;/a&gt;&lt;/strong&gt; - Hosted cloud service with full API access&lt;/p&gt; 
&lt;p&gt;For enterprise deployment and custom solutions, contact &lt;strong&gt;&lt;a href="mailto:info@nevamind.ai"&gt;info@nevamind.ai&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h4&gt;Cloud API (v3)&lt;/h4&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Base URL&lt;/th&gt; 
   &lt;th&gt;&lt;code&gt;https://api.memu.so&lt;/code&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Auth&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;Authorization: Bearer YOUR_API_KEY&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Method&lt;/th&gt; 
   &lt;th&gt;Endpoint&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;POST&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;/api/v3/memory/memorize&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Register a memorization task&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;GET&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;/api/v3/memory/memorize/status/{task_id}&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Get task status&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;POST&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;/api/v3/memory/categories&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;List memory categories&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;POST&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;/api/v3/memory/retrieve&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Retrieve memories (semantic search)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;üìö &lt;strong&gt;&lt;a href="https://memu.pro/docs#cloud-version"&gt;Full API Documentation&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Option 2: Self-Hosted&lt;/h3&gt; 
&lt;h4&gt;Installation&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Basic Example&lt;/h4&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Requirements&lt;/strong&gt;: Python 3.13+ and an OpenAI API key&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;Test with In-Memory Storage&lt;/strong&gt; (no database required):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export OPENAI_API_KEY=your_api_key
cd tests
python test_inmemory.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Test with PostgreSQL Storage&lt;/strong&gt; (requires pgvector):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Start PostgreSQL with pgvector
docker run -d \
  --name memu-postgres \
  -e POSTGRES_USER=postgres \
  -e POSTGRES_PASSWORD=postgres \
  -e POSTGRES_DB=memu \
  -p 5432:5432 \
  pgvector/pgvector:pg16

# Run the test
export OPENAI_API_KEY=your_api_key
cd tests
python test_postgres.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Both examples demonstrate the complete workflow:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Memorize&lt;/strong&gt;: Process a conversation file and extract structured memory&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Retrieve (RAG)&lt;/strong&gt;: Fast embedding-based search&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Retrieve (LLM)&lt;/strong&gt;: Deep semantic understanding search&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/NevaMind-AI/memU/main/tests/test_inmemory.py"&gt;&lt;code&gt;tests/test_inmemory.py&lt;/code&gt;&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/NevaMind-AI/memU/main/tests/test_postgres.py"&gt;&lt;code&gt;tests/test_postgres.py&lt;/code&gt;&lt;/a&gt; for the full source code.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Custom LLM and Embedding Providers&lt;/h3&gt; 
&lt;p&gt;MemU supports custom LLM and embedding providers beyond OpenAI. Configure them via &lt;code&gt;llm_profiles&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from memu import MemUService

service = MemUService(
    llm_profiles={
        # Default profile for LLM operations
        "default": {
            "base_url": "https://dashscope.aliyuncs.com/compatible-mode/v1",
            "api_key": "your_api_key",
            "chat_model": "qwen3-max",
            "client_backend": "sdk"  # "sdk" or "http"
        },
        # Separate profile for embeddings
        "embedding": {
            "base_url": "https://api.voyageai.com/v1",
            "api_key": "your_voyage_api_key",
            "embed_model": "voyage-3.5-lite"
        }
    },
    # ... other configuration
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h3&gt;OpenRouter Integration&lt;/h3&gt; 
&lt;p&gt;MemU supports &lt;a href="https://openrouter.ai"&gt;OpenRouter&lt;/a&gt; as a model provider, giving you access to multiple LLM providers through a single API.&lt;/p&gt; 
&lt;h4&gt;Configuration&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from memu import MemoryService

service = MemoryService(
    llm_profiles={
        "default": {
            "provider": "openrouter",
            "client_backend": "httpx",
            "base_url": "https://openrouter.ai",
            "api_key": "your_openrouter_api_key",
            "chat_model": "anthropic/claude-3.5-sonnet",  # Any OpenRouter model
            "embed_model": "openai/text-embedding-3-small",  # Embedding model
        },
    },
    database_config={
        "metadata_store": {"provider": "inmemory"},
    },
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Environment Variables&lt;/h4&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENROUTER_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Your OpenRouter API key from &lt;a href="https://openrouter.ai/keys"&gt;openrouter.ai/keys&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h4&gt;Supported Features&lt;/h4&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Feature&lt;/th&gt; 
   &lt;th&gt;Status&lt;/th&gt; 
   &lt;th&gt;Notes&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Chat Completions&lt;/td&gt; 
   &lt;td&gt;Supported&lt;/td&gt; 
   &lt;td&gt;Works with any OpenRouter chat model&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Embeddings&lt;/td&gt; 
   &lt;td&gt;Supported&lt;/td&gt; 
   &lt;td&gt;Use OpenAI embedding models via OpenRouter&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Vision&lt;/td&gt; 
   &lt;td&gt;Supported&lt;/td&gt; 
   &lt;td&gt;Use vision-capable models (e.g., &lt;code&gt;openai/gpt-4o&lt;/code&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h4&gt;Running OpenRouter Tests&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export OPENROUTER_API_KEY=your_api_key

# Full workflow test (memorize + retrieve)
python tests/test_openrouter.py

# Embedding-specific tests
python tests/test_openrouter_embedding.py

# Vision-specific tests
python tests/test_openrouter_vision.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/NevaMind-AI/memU/main/examples/example_4_openrouter_memory.py"&gt;&lt;code&gt;examples/example_4_openrouter_memory.py&lt;/code&gt;&lt;/a&gt; for a complete working example.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üìñ Core APIs&lt;/h2&gt; 
&lt;h3&gt;&lt;code&gt;memorize()&lt;/code&gt; - Extract and Store Memory&lt;/h3&gt; 
&lt;p&gt;Processes input resources and extracts structured memory:&lt;/p&gt; 
&lt;img width="100%" alt="memorize" src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/memorize.png" /&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;result = await service.memorize(
    resource_url="path/to/file.json",  # File path or URL
    modality="conversation",            # conversation | document | image | video | audio
    user={"user_id": "123"}             # Optional: scope to a user
)

# Returns:
{
    "resource": {...},      # Stored resource metadata
    "items": [...],         # Extracted memory items
    "categories": [...]     # Updated category summaries
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;&lt;code&gt;retrieve()&lt;/code&gt; - Query Memory&lt;/h3&gt; 
&lt;p&gt;Retrieves relevant memory based on queries. MemU supports &lt;strong&gt;two retrieval strategies&lt;/strong&gt;:&lt;/p&gt; 
&lt;img width="100%" alt="retrieve" src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/retrieve.png" /&gt; 
&lt;h4&gt;RAG-based Retrieval (&lt;code&gt;method="rag"&lt;/code&gt;)&lt;/h4&gt; 
&lt;p&gt;Fast &lt;strong&gt;embedding vector search&lt;/strong&gt; using cosine similarity:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;‚úÖ &lt;strong&gt;Fast&lt;/strong&gt;: Pure vector computation&lt;/li&gt; 
 &lt;li&gt;‚úÖ &lt;strong&gt;Scalable&lt;/strong&gt;: Efficient for large memory stores&lt;/li&gt; 
 &lt;li&gt;‚úÖ &lt;strong&gt;Returns scores&lt;/strong&gt;: Each result includes similarity score&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;LLM-based Retrieval (&lt;code&gt;method="llm"&lt;/code&gt;)&lt;/h4&gt; 
&lt;p&gt;Deep &lt;strong&gt;semantic understanding&lt;/strong&gt; through direct LLM reasoning:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;‚úÖ &lt;strong&gt;Deep understanding&lt;/strong&gt;: LLM comprehends context and nuance&lt;/li&gt; 
 &lt;li&gt;‚úÖ &lt;strong&gt;Query rewriting&lt;/strong&gt;: Automatically refines query at each tier&lt;/li&gt; 
 &lt;li&gt;‚úÖ &lt;strong&gt;Adaptive&lt;/strong&gt;: Stops early when sufficient information is found&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Comparison&lt;/h4&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Aspect&lt;/th&gt; 
   &lt;th&gt;RAG&lt;/th&gt; 
   &lt;th&gt;LLM&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Speed&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;‚ö° Fast&lt;/td&gt; 
   &lt;td&gt;üê¢ Slower&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Cost&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;üí∞ Low&lt;/td&gt; 
   &lt;td&gt;üí∞üí∞ Higher&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Semantic depth&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Medium&lt;/td&gt; 
   &lt;td&gt;Deep&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Tier 2 scope&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;All items&lt;/td&gt; 
   &lt;td&gt;Only items in relevant categories&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Output&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;With similarity scores&lt;/td&gt; 
   &lt;td&gt;Ranked by LLM reasoning&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Both methods support:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Context-aware rewriting&lt;/strong&gt;: Resolves pronouns using conversation history&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Progressive search&lt;/strong&gt;: Categories ‚Üí Items ‚Üí Resources&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Sufficiency checking&lt;/strong&gt;: Stops when enough information is retrieved&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Usage&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;result = await service.retrieve(
    queries=[
        {"role": "user", "content": {"text": "What are their preferences?"}},
        {"role": "user", "content": {"text": "Tell me about work habits"}}
    ],
    where={"user_id": "123"}  # Optional: scope filter
)

# Returns:
{
    "categories": [...],     # Relevant categories (with scores for RAG)
    "items": [...],          # Relevant memory items
    "resources": [...],      # Related raw resources
    "next_step_query": "..." # Rewritten query for follow-up (if applicable)
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Scope Filtering&lt;/strong&gt;: Use &lt;code&gt;where&lt;/code&gt; to filter by user model fields:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;where={"user_id": "123"}&lt;/code&gt; - exact match&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;where={"agent_id__in": ["1", "2"]}&lt;/code&gt; - match any in list&lt;/li&gt; 
 &lt;li&gt;Omit &lt;code&gt;where&lt;/code&gt; to retrieve across all scopes&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;üìö &lt;strong&gt;For complete API documentation&lt;/strong&gt;, see &lt;a href="https://raw.githubusercontent.com/NevaMind-AI/memU/main/docs/SERVICE_API.md"&gt;SERVICE_API.md&lt;/a&gt; - includes all methods, CRUD operations, pipeline configuration, and configuration types.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üí° Use Cases&lt;/h2&gt; 
&lt;h3&gt;Example 1: Conversation Memory&lt;/h3&gt; 
&lt;p&gt;Extract and organize memory from multi-turn conversations:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export OPENAI_API_KEY=your_api_key
python examples/example_1_conversation_memory.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;What it does:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Processes multiple conversation JSON files&lt;/li&gt; 
 &lt;li&gt;Extracts memory items (preferences, habits, opinions, relationships)&lt;/li&gt; 
 &lt;li&gt;Generates category markdown files (&lt;code&gt;preferences.md&lt;/code&gt;, &lt;code&gt;work_life.md&lt;/code&gt;, etc.)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Best for:&lt;/strong&gt; Personal AI assistants, customer support bots, social chatbots&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Example 2: Skill Extraction from Logs&lt;/h3&gt; 
&lt;p&gt;Extract skills and lessons learned from agent execution logs:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export OPENAI_API_KEY=your_api_key
python examples/example_2_skill_extraction.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;What it does:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Processes agent logs sequentially&lt;/li&gt; 
 &lt;li&gt;Extracts actions, outcomes, and lessons learned&lt;/li&gt; 
 &lt;li&gt;Demonstrates &lt;strong&gt;incremental learning&lt;/strong&gt; - memory evolves with each file&lt;/li&gt; 
 &lt;li&gt;Generates evolving skill guides (&lt;code&gt;log_1.md&lt;/code&gt; ‚Üí &lt;code&gt;log_2.md&lt;/code&gt; ‚Üí &lt;code&gt;skill.md&lt;/code&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Best for:&lt;/strong&gt; DevOps teams, agent self-improvement, knowledge management&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Example 3: Multimodal Memory&lt;/h3&gt; 
&lt;p&gt;Process diverse content types into unified memory:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export OPENAI_API_KEY=your_api_key
python examples/example_3_multimodal_memory.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;What it does:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Processes documents and images together&lt;/li&gt; 
 &lt;li&gt;Extracts memory from different content types&lt;/li&gt; 
 &lt;li&gt;Unifies into cross-modal categories (&lt;code&gt;technical_documentation&lt;/code&gt;, &lt;code&gt;visual_diagrams&lt;/code&gt;, etc.)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Best for:&lt;/strong&gt; Documentation systems, learning platforms, research tools&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üìä Performance&lt;/h2&gt; 
&lt;p&gt;MemU achieves &lt;strong&gt;92.09% average accuracy&lt;/strong&gt; on the Locomo benchmark across all reasoning tasks.&lt;/p&gt; 
&lt;img width="100%" alt="benchmark" src="https://github.com/user-attachments/assets/6fec4884-94e5-4058-ad5c-baac3d7e76d9" /&gt; 
&lt;p&gt;View detailed experimental data: &lt;a href="https://github.com/NevaMind-AI/memU-experiment"&gt;memU-experiment&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üß© Ecosystem&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Repository&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Use Case&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;&lt;a href="https://github.com/NevaMind-AI/memU"&gt;memU&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Core algorithm engine&lt;/td&gt; 
   &lt;td&gt;Embed AI memory into your product&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;&lt;a href="https://github.com/NevaMind-AI/memU-server"&gt;memU-server&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Backend service with CRUD, user system, RBAC&lt;/td&gt; 
   &lt;td&gt;Self-host a memory backend&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;&lt;a href="https://github.com/NevaMind-AI/memU-ui"&gt;memU-ui&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Visual dashboard&lt;/td&gt; 
   &lt;td&gt;Ready-to-use memory console&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Quick Links:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üöÄ &lt;a href="https://app.memu.so/quick-start"&gt;Try MemU Cloud&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üìö &lt;a href="https://memu.pro/docs"&gt;API Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üí¨ &lt;a href="https://discord.gg/memu"&gt;Discord Community&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ü§ù Partners&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/TEN-framework/ten-framework"&gt;&lt;img src="https://avatars.githubusercontent.com/u/113095513?s=200&amp;amp;v=4" alt="Ten" height="40" style="margin: 10px;" /&gt;&lt;/a&gt; &lt;a href="https://openagents.org"&gt;&lt;img src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/partners/openagents.png" alt="OpenAgents" height="40" style="margin: 10px;" /&gt;&lt;/a&gt; &lt;a href="https://github.com/milvus-io/milvus"&gt;&lt;img src="https://miro.medium.com/v2/resize:fit:2400/1*-VEGyAgcIBD62XtZWavy8w.png" alt="Milvus" height="40" style="margin: 10px;" /&gt;&lt;/a&gt; &lt;a href="https://xroute.ai/"&gt;&lt;img src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/partners/xroute.png" alt="xRoute" height="40" style="margin: 10px;" /&gt;&lt;/a&gt; &lt;a href="https://jaaz.app/"&gt;&lt;img src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/partners/jazz.png" alt="Jazz" height="40" style="margin: 10px;" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Buddie-AI/Buddie"&gt;&lt;img src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/partners/buddie.png" alt="Buddie" height="40" style="margin: 10px;" /&gt;&lt;/a&gt; &lt;a href="https://github.com/bytebase/bytebase"&gt;&lt;img src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/partners/bytebase.png" alt="Bytebase" height="40" style="margin: 10px;" /&gt;&lt;/a&gt; &lt;a href="https://github.com/LazyAGI/LazyLLM"&gt;&lt;img src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/partners/LazyLLM.png" alt="LazyLLM" height="40" style="margin: 10px;" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ü§ù How to Contribute&lt;/h2&gt; 
&lt;p&gt;We welcome contributions from the community! Whether you're fixing bugs, adding features, or improving documentation, your help is appreciated.&lt;/p&gt; 
&lt;h3&gt;Getting Started&lt;/h3&gt; 
&lt;p&gt;To start contributing to MemU, you'll need to set up your development environment:&lt;/p&gt; 
&lt;h4&gt;Prerequisites&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.13+&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/astral-sh/uv"&gt;uv&lt;/a&gt; (Python package manager)&lt;/li&gt; 
 &lt;li&gt;Git&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Setup Development Environment&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 1. Fork and clone the repository
git clone https://github.com/YOUR_USERNAME/memU.git
cd memU

# 2. Install development dependencies
make install
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The &lt;code&gt;make install&lt;/code&gt; command will:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Create a virtual environment using &lt;code&gt;uv&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Install all project dependencies&lt;/li&gt; 
 &lt;li&gt;Set up pre-commit hooks for code quality checks&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Running Quality Checks&lt;/h4&gt; 
&lt;p&gt;Before submitting your contribution, ensure your code passes all quality checks:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;make check
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The &lt;code&gt;make check&lt;/code&gt; command runs:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Lock file verification&lt;/strong&gt;: Ensures &lt;code&gt;pyproject.toml&lt;/code&gt; consistency&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Pre-commit hooks&lt;/strong&gt;: Lints code with Ruff, formats with Black&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Type checking&lt;/strong&gt;: Runs &lt;code&gt;mypy&lt;/code&gt; for static type analysis&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Dependency analysis&lt;/strong&gt;: Uses &lt;code&gt;deptry&lt;/code&gt; to find obsolete dependencies&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Contributing Guidelines&lt;/h3&gt; 
&lt;p&gt;For detailed contribution guidelines, code standards, and development practices, please see &lt;a href="https://raw.githubusercontent.com/NevaMind-AI/memU/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Quick tips:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Create a new branch for each feature or bug fix&lt;/li&gt; 
 &lt;li&gt;Write clear commit messages&lt;/li&gt; 
 &lt;li&gt;Add tests for new functionality&lt;/li&gt; 
 &lt;li&gt;Update documentation as needed&lt;/li&gt; 
 &lt;li&gt;Run &lt;code&gt;make check&lt;/code&gt; before pushing&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üìÑ License&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/NevaMind-AI/memU/main/LICENSE.txt"&gt;Apache License 2.0&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üåç Community&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;GitHub Issues&lt;/strong&gt;: &lt;a href="https://github.com/NevaMind-AI/memU/issues"&gt;Report bugs &amp;amp; request features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Discord&lt;/strong&gt;: &lt;a href="https://discord.com/invite/hQZntfGsbJ"&gt;Join the community&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;X (Twitter)&lt;/strong&gt;: &lt;a href="https://x.com/memU_ai"&gt;Follow @memU_ai&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Contact&lt;/strong&gt;: &lt;a href="mailto:info@nevamind.ai"&gt;info@nevamind.ai&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;‚≠ê &lt;strong&gt;Star us on GitHub&lt;/strong&gt; to get notified about new releases!&lt;/p&gt; 
&lt;/div&gt;</description>
    </item>
    
  </channel>
</rss>