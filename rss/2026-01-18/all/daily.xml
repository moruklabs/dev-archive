<rss version="2.0">
  <channel>
    <title>GitHub All Languages Daily Trending</title>
    <description>Daily Trending of All Languages in GitHub</description>
    <pubDate>Sat, 17 Jan 2026 01:30:50 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>eigent-ai/eigent</title>
      <link>https://github.com/eigent-ai/eigent</link>
      <description>&lt;p&gt;Eigent: The Open Source Cowork Desktop to Unlock Your Exceptional Productivity.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt;
 &lt;a name="readme-top"&gt;&lt;/a&gt; 
 &lt;p&gt;&lt;a href="https://www.eigent.ai"&gt;&lt;img src="https://eigent-ai.github.io/.github/assets/head.png" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://www.eigent.ai"&gt;&lt;img src="https://eigent-ai.github.io/.github/assets/seperator.png" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;h3&gt;Eigent: The Open Source Cowork Desktop to Unlock Your Exceptional Productivity&lt;/h3&gt; 
 &lt;!-- SHIELD GROUP --&gt; 
 &lt;p&gt;&lt;a href="https://www.eigent.ai/download"&gt;&lt;img src="https://img.shields.io/badge/Download%20Eigent-363AF5?style=plastic" alt="" /&gt;&lt;/a&gt; &lt;a href="https://github.com/eigent-ai/eigent"&gt;&lt;img src="https://img.shields.io/github/stars/eigent-ai?color=F5F4F0&amp;amp;labelColor=gray&amp;amp;style=plastic&amp;amp;logo=github" alt="" /&gt;&lt;/a&gt; &lt;a href="https://x.com/Eigent_AI"&gt;&lt;img src="https://img.shields.io/badge/-%40Eigent_AI-white?labelColor=gray&amp;amp;logo=x&amp;amp;logoColor=white&amp;amp;style=plastic" alt="" /&gt;&lt;/a&gt; &lt;a href="https://discord.camel-ai.org/"&gt;&lt;img src="https://img.shields.io/discord/1082486657678311454?logo=discord&amp;amp;labelColor=%20%235462eb&amp;amp;logoColor=%20%23f5f5f5&amp;amp;color=%20%235462eb" alt="" /&gt;&lt;/a&gt;&lt;br /&gt; &lt;a href="https://www.reddit.com/r/CamelAI/"&gt;&lt;img src="https://img.shields.io/reddit/subreddit-subscribers/CamelAI?style=plastic&amp;amp;logo=reddit&amp;amp;label=r%2FCAMEL&amp;amp;labelColor=white" alt="Reddit" /&gt;&lt;/a&gt; &lt;a href="https://ghli.org/camel/wechat.png"&gt;&lt;img src="https://img.shields.io/badge/WeChat-CamelAIOrg-brightgreen?logo=wechat&amp;amp;logoColor=white" alt="Wechat" /&gt;&lt;/a&gt; &lt;a href="https://github.com/sponsors/camel-ai"&gt;&lt;img src="https://img.shields.io/badge/-Sponsor%20CAMEL--AI-1d1d1d?logo=github&amp;amp;logoColor=white&amp;amp;style=plastic" alt="" /&gt;&lt;/a&gt; &lt;a href="https://github.com/camel-ai/camel"&gt;&lt;img src="https://img.shields.io/badge/-Built--with--CAMEL-4C19E8.svg?logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQ4IiBoZWlnaHQ9IjI3MiIgdmlld0JveD0iMCAwIDI0OCAyNzIiIGZpbGw9Im5vbmUiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyI+CjxwYXRoIGQ9Ik04LjgzMTE3IDE4LjU4NjVMMCAzMC44MjY3QzUuNDY2OTIgMzUuMDQzMiAxNS4xMzkxIDM4LjgyNTggMjQuODExNCAzNi4yOTU5QzMwLjY5ODggNDAuOTM0MSAzOS42NzAyIDQwLjIzMTMgNDQuMTU1OSA0MC4wOTA4QzQzLjQ1NSA0Ny4zOTk0IDQyLjQ3MzcgNzAuOTU1OCA0NC4xNTU5IDEwNi43MTJDNDUuODM4IDE0Mi40NjggNzEuNzcwOCAxNjYuODY4IDg0LjUyNjkgMTc0LjU5OEw3Ni4wMDAyIDIyMEw4NC41MjY5IDI3MkgxMDguOTE4TDk4LjAwMDIgMjIwTDEwOC45MTggMTc0LjU5OEwxMjkuOTQ0IDI3MkgxNTQuNzU2TDEzNC4xNSAxNzQuNTk4SDE4Ny4xMzdMMTY2LjUzMSAyNzJIMTkxLjc2M0wyMTIuMzY5IDE3NC41OThMMjI2IDIyMEwyMTIuMzY5IDI3MkgyMzcuNjAxTDI0OC4wMDEgMjIwTDIzNy4xOCAxNzQuNTk4QzIzOS4yODMgMTY5LjExNyAyNDAuNDAxIDE2Ni45NzYgMjQxLjgwNiAxNjEuMTA1QzI0OS4zNzUgMTI5LjQ4MSAyMzUuMDc3IDEwMy45MDEgMjI2LjY2NyA5NC40ODRMMjA2LjQ4MSA3My44MjNDMTk3LjY1IDY0Ljk2ODMgMTgyLjUxMSA2NC41NDY3IDE3Mi44MzkgNzIuNTU4MUMxNjUuNzI4IDc4LjQ0NzcgMTYxLjcwMSA3OC43NzI3IDE1NC43NTYgNzIuNTU4MUMxNTEuODEyIDcwLjAyODEgMTQ0LjUzNSA2MS40ODg5IDEzNC45OTEgNTMuNTgzN0MxMjUuMzE5IDQ1LjU3MjMgMTA4LjQ5NyA0OC45NDU1IDEwMi4xODkgNTUuNjkxOUw3My41OTMxIDg0LjM2NDRWNy42MjM0OUw3OS4xMjczIDBDNjAuOTA0MiAzLjY1NDMzIDIzLjgwMjEgOS41NjMwOSAxOS43NjUgMTAuNTc1MUMxNS43Mjc5IDExLjU4NyAxMC43OTM3IDE2LjMzNzcgOC44MzExNyAxOC41ODY1WiIgZmlsbD0id2hpdGUiLz4KPHBhdGggZD0iTTQzLjIwMzggMTguNzE4N0w0OS4wOTEyIDEzLjA0OTNMNTQuOTc4NyAxOC43MTg3TDQ5LjA5MTIgMjQuODI0Mkw0My4yMDM4IDE4LjcxODdaIiBmaWxsPSIjNEMxOUU4Ii8+Cjwvc3ZnPgo=" alt="" /&gt;&lt;/a&gt; &lt;a href="https://eigent-ai.notion.site/eigent-ai-careers"&gt;&lt;img src="https://img.shields.io/badge/Join%20Us-yellow?style=plastic" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;strong&gt;English&lt;/strong&gt; ¬∑ &lt;a href="https://raw.githubusercontent.com/eigent-ai/eigent/main/README_CN.md"&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt; ¬∑ &lt;a href="https://raw.githubusercontent.com/eigent-ai/eigent/main/README_JA.md"&gt;Êó•Êú¨Ë™û&lt;/a&gt; ¬∑ &lt;a href="https://www.eigent.ai"&gt;Official Site&lt;/a&gt; ¬∑ &lt;a href="https://docs.eigent.ai"&gt;Documents&lt;/a&gt; ¬∑ &lt;a href="https://github.com/eigent-ai/eigent/issues"&gt;Feedback&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;strong&gt;Eigent&lt;/strong&gt;&amp;nbsp;is the open source cowork desktop application, empowering you to build, manage, and deploy a custom AI workforce that can turn your most complex workflows into automated tasks.&lt;/p&gt; 
&lt;p&gt;Built on &lt;a href="https://www.camel-ai.org"&gt;CAMEL-AI&lt;/a&gt;'s acclaimed open-source project, our system introduces a &lt;strong&gt;Multi-Agent Workforce&lt;/strong&gt; that &lt;strong&gt;boosts productivity&lt;/strong&gt; through parallel execution, customization, and privacy protection.&lt;/p&gt; 
&lt;h3&gt;‚≠ê 100% Open Source - ü•á Local Deployment - üèÜ MCP Integration&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;‚úÖ &lt;strong&gt;Zero Setup&lt;/strong&gt; - No technical configuration required&lt;/li&gt; 
 &lt;li&gt;‚úÖ &lt;strong&gt;Multi-Agent Coordination&lt;/strong&gt; - Handle complex multi-agent workflows&lt;/li&gt; 
 &lt;li&gt;‚úÖ &lt;strong&gt;Enterprise Feature&lt;/strong&gt; - SSO/Access control&lt;/li&gt; 
 &lt;li&gt;‚úÖ &lt;strong&gt;Local Deploymen&lt;/strong&gt;t&lt;/li&gt; 
 &lt;li&gt;‚úÖ &lt;strong&gt;Open Source&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;‚úÖ &lt;strong&gt;Custom Model Support&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;‚úÖ &lt;strong&gt;MCP Integration&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;a href="https://eigent-ai.notion.site/eigent-ai-careers"&gt;&lt;img src="https://camel-ai.github.io/camel_asset/graphics/join_us.png" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;kbd&gt;Table of contents&lt;/kbd&gt;&lt;/summary&gt; 
 &lt;h4&gt;TOC&lt;/h4&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eigent-ai/eigent/main/#-getting-started"&gt;üöÄ Getting Started&lt;/a&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eigent-ai/eigent/main/#-local-deployment-recommended"&gt;üè† Local Deployment (Recommended)&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eigent-ai/eigent/main/#-quick-start-cloud-connected"&gt;‚ö° Quick Start (Cloud-Connected)&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eigent-ai/eigent/main/#-enterprise"&gt;üè¢ Enterprise&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eigent-ai/eigent/main/#%EF%B8%8F-cloud-version"&gt;‚òÅÔ∏è Cloud Version&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eigent-ai/eigent/main/#-key-features"&gt;‚ú® Key features&lt;/a&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eigent-ai/eigent/main/#-workforce"&gt;üè≠ Workforce&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eigent-ai/eigent/main/#-comprehensive-model-support"&gt;üß† Comprehensive Model Support&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eigent-ai/eigent/main/#-mcp-tools-integration-mcp"&gt;üîå MCP Tools Integration (MCP)&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eigent-ai/eigent/main/#-human-in-the-loop"&gt;‚úã Human-in-the-Loop&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eigent-ai/eigent/main/#-100-open-source"&gt;üëê 100% Open Source&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eigent-ai/eigent/main/#-use-cases"&gt;üß© Use Cases&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eigent-ai/eigent/main/#-tech-stack"&gt;üõ†Ô∏è Tech Stack&lt;/a&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eigent-ai/eigent/main/#backend"&gt;Backend&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eigent-ai/eigent/main/#frontend"&gt;Frontend&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eigent-ai/eigent/main/#staying-ahead"&gt;üåü&amp;nbsp;Staying ahead&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eigent-ai/eigent/main/#-roadmap"&gt;üó∫Ô∏è Roadmap&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eigent-ai/eigent/main/#-contributing"&gt;üìñ&amp;nbsp;Contributing&lt;/a&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eigent-ai/eigent/main/#main-contributors"&gt;Main Contributors&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eigent-ai/eigent/main/#distinguished-amabssador"&gt;Distinguished amabssador&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eigent-ai/eigent/main/#ecosystem"&gt;Ecosystem&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eigent-ai/eigent/main/#-open-source-license"&gt;üìÑ&amp;nbsp;Open Source License&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/eigent-ai/eigent/main/#-community--contact"&gt;üåê&amp;nbsp;Community &amp;amp; contact&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h4&gt;&lt;/h4&gt; 
 &lt;br /&gt; 
&lt;/details&gt; 
&lt;h2&gt;&lt;strong&gt;üöÄ Getting Started&lt;/strong&gt;&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;üîì Build in Public&lt;/strong&gt; ‚Äî Eigent is &lt;strong&gt;100% open source&lt;/strong&gt; from day one. Every feature, every commit, every decision is transparent. We believe the best AI tools should be built openly with the community, not behind closed doors.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;üè† Local Deployment (Recommended)&lt;/h3&gt; 
&lt;p&gt;The recommended way to run Eigent ‚Äî fully standalone with complete control over your data, no cloud account required.&lt;/p&gt; 
&lt;p&gt;üëâ &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/eigent-ai/eigent/main/server/README_EN.md"&gt;Full Local Deployment Guide&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;This setup includes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Local backend server with full API&lt;/li&gt; 
 &lt;li&gt;Local model integration (vLLM, Ollama, LM Studio, etc.)&lt;/li&gt; 
 &lt;li&gt;Complete isolation from cloud services&lt;/li&gt; 
 &lt;li&gt;Zero external dependencies&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;‚ö° Quick Start (Cloud-Connected)&lt;/h3&gt; 
&lt;p&gt;For a quick preview using our cloud backend ‚Äî get started in seconds:&lt;/p&gt; 
&lt;h4&gt;Prerequisites&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Node.js (version 18-22) and npm&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Steps&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/eigent-ai/eigent.git
cd eigent
npm install
npm run dev
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Note: This mode connects to Eigent cloud services and requires account registration. For a fully standalone experience, use &lt;a href="https://raw.githubusercontent.com/eigent-ai/eigent/main/#-local-deployment-recommended"&gt;Local Deployment&lt;/a&gt; instead.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;üè¢ Enterprise&lt;/h3&gt; 
&lt;p&gt;For organizations requiring maximum security, customization, and control:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Exclusive Features&lt;/strong&gt; (like SSO &amp;amp; custom development)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Scalable Enterprise Deployment&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Negotiated SLAs&lt;/strong&gt; &amp;amp; implementation services&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;üìß For further details, please contact us at &lt;a href="mailto:info@eigent.ai"&gt;info@eigent.ai&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;‚òÅÔ∏è Cloud Version&lt;/h3&gt; 
&lt;p&gt;For teams who prefer managed infrastructure, we also offer a cloud platform. The fastest way to experience Eigent's multi-agent AI capabilities without setup complexity. We'll host the models, APIs, and cloud storage, ensuring Eigent runs flawlessly.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Instant Access&lt;/strong&gt; - Start building multi-agent workflows in minutes.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Managed Infrastructure&lt;/strong&gt; - We handle scaling, updates, and maintenance.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Premium Support&lt;/strong&gt; - Subscribe and get priority assistance from our engineering team.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;a href="https://www.eigent.ai/download"&gt;&lt;img src="https://eigent-ai.github.io/.github/assets/banner.png" alt="image-public-beta" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;div align="right"&gt; 
 &lt;a href="https://www.eigent.ai/download"&gt;Get started at Eigent.ai ‚Üí&lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;&lt;strong&gt;‚ú® Key features&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;Unlock the full potential of exceptional productivity with Eigent‚Äôs powerful features‚Äîbuilt for seamless integration, smarter task execution, and boundless automation.&lt;/p&gt; 
&lt;h3&gt;üè≠ Workforce&lt;/h3&gt; 
&lt;p&gt;Employs a team of specialized AI agents that collaborate to solve complex tasks. Eigent dynamically breaks down tasks and activates multiple agents to work&amp;nbsp;&lt;strong&gt;in parallel.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Eigent pre-defined the following agent workers:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Developer Agent:&lt;/strong&gt;&amp;nbsp;Writes and executes code, runs terminal commands.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Browser Agent:&lt;/strong&gt;&amp;nbsp;Searches the web and extracts content.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Document Agent:&lt;/strong&gt;&amp;nbsp;Creates and manages documents.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-Modal Agent:&lt;/strong&gt;&amp;nbsp;Processes images and audio.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://eigent-ai.github.io/.github/assets/gif/feature_dynamic_workforce.gif" alt="Workforce" /&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;h3&gt;üß† Comprehensive Model Support&lt;/h3&gt; 
&lt;p&gt;Deploy Eigent locally with your preferred models.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://eigent-ai.github.io/.github/assets/gif/feature_local_model.gif" alt="Model" /&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;h3&gt;üîå MCP Tools Integration (MCP)&lt;/h3&gt; 
&lt;p&gt;Eigent comes with massive built-in&amp;nbsp;&lt;strong&gt;Model Context Protocol (MCP)&lt;/strong&gt;&amp;nbsp;tools (for web browsing, code execution, Notion, Google suite, Slack etc.), and also lets you&amp;nbsp;&lt;strong&gt;install your own tools&lt;/strong&gt;. Equip agents with exactly the right tools for your scenarios ‚Äì even integrate internal APIs or custom functions ‚Äì to enhance their capabilities.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://eigent-ai.github.io/.github/assets/gif/feature_add_mcps.gif" alt="MCP" /&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;h3&gt;‚úã Human-in-the-Loop&lt;/h3&gt; 
&lt;p&gt;If a task gets stuck or encounters uncertainty, Eigent will automatically request human input.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://eigent-ai.github.io/.github/assets/gif/feature_human_in_the_loop.gif" alt="Human-in-the-loop" /&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;h3&gt;üëê 100% Open Source&lt;/h3&gt; 
&lt;p&gt;Eigent is completely open-sourced. You can download, inspect, and modify the code, ensuring transparency and fostering a community-driven ecosystem for multi-agent innovation.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://eigent-ai.github.io/.github/assets/opensource.png" alt="Opensource" /&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;h2&gt;üß© Use Cases&lt;/h2&gt; 
&lt;h3&gt;1. Palm Springs Tennis Trip Itinerary with Slack Summary &lt;a href="https://www.eigent.ai/download?share_token=IjE3NTM0MzUxNTEzMzctNzExMyI.aIeysw.MUeG6ZcBxI1GqvPDvn4dcv-CDWw__1753435151337-7113"&gt;Replay ‚ñ∂Ô∏è&lt;/a&gt;&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Prompt:&lt;/strong&gt; &lt;kbd&gt;We are two tennis fans and want to go see the tennis tournament ... &lt;kbd&gt;&lt;/kbd&gt;&lt;/kbd&gt;&lt;/summary&gt; 
 &lt;br /&gt; We are two tennis fans and want to go see the tennis tournament in Palm Springs 2026. I live in SF - please prepare a detailed itinerary with flights, hotels, things to do for 3 days - around the time semifinal/finals are happening. We like hiking, vegan food and spas. Our budget is $5K. The itinerary should be a detailed timeline of time, activity, cost, other details and if applicable a link to buy tickets/make reservations etc. for the item. Some preferences .Spa access would be nice but not necessary. When you finish this task, please generate a html report about this trip; write a summary of this plan and send text summary and report html link to slack #tennis-trip-sf channel. 
&lt;/details&gt; 
&lt;br /&gt; 
&lt;h3&gt;2. Generate Q2 Report from CSV Bank Data &lt;a href="https://www.eigent.ai/download?share_token=IjE3NTM1MjY4OTE4MDgtODczOSI.aIjJmQ.WTdoX9mATwrcBr_w53BmGEHPo8U__1753526891808-8739"&gt;Replay ‚ñ∂Ô∏è&lt;/a&gt;&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Prompt:&lt;/strong&gt; &lt;kbd&gt;Please help me prepare a Q2 financial statement based on my bank ... &lt;kbd&gt;&lt;/kbd&gt;&lt;/kbd&gt;&lt;/summary&gt; 
 &lt;br /&gt; Please help me prepare a Q2 financial statement based on my bank transfer record file bank_transacation.csv in my desktop to a html report with chart to investors how much we have spent. 
&lt;/details&gt; 
&lt;br /&gt; 
&lt;h3&gt;3. UK Healthcare Market Research Report Automation &lt;a href="https://www.eigent.ai/download?share_token=IjE3NTMzOTM1NTg3OTctODcwNyI.aIey-Q.Jh9QXzYrRYarY0kz_qsgoj3ewX0__1753393558797-8707"&gt;Replay ‚ñ∂Ô∏è&lt;/a&gt;&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Prompt:&lt;/strong&gt; &lt;kbd&gt;Analyze the UK healthcare industry to support the planning ... &lt;kbd&gt;&lt;/kbd&gt;&lt;/kbd&gt;&lt;/summary&gt; 
 &lt;br /&gt; Analyze the UK healthcare industry to support the planning of my next company. Provide a comprehensive market overview, including current trends, growth projections, and relevant regulations. Identify the top 5‚Äì10 major opportunities, gaps, or underserved segments within the market. Present all findings in a well-structured, professional HTML report. Then send a message to slack #eigentr-product-test channel when this task is done to align the report content with my teammates. 
&lt;/details&gt; 
&lt;br /&gt; 
&lt;h3&gt;4. German Electric Skateboard Market Feasibility &lt;a href="https://www.eigent.ai/download?share_token=IjE3NTM2NTI4MjY3ODctNjk2Ig.aIjGiA.t-qIXxk_BZ4ENqa-yVIm0wMVyXU__1753652826787-696"&gt;Replay ‚ñ∂Ô∏è&lt;/a&gt;&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Prompt:&lt;/strong&gt; &lt;kbd&gt;We are a company that produces high-end electric skateboards ... &lt;kbd&gt;&lt;/kbd&gt;&lt;/kbd&gt;&lt;/summary&gt; 
 &lt;br /&gt; We are a company that produces high-end electric skateboards, and we are considering entering the German market. Please prepare a detailed market entry feasibility report for me. The report needs to cover the following aspects: 1. Market Size &amp;amp; Regulations: Research the market size, annual growth rate, key players, and market share for Personal Light Electric Vehicles (PLEVs) in Germany. Simultaneously, provide a detailed breakdown and summary of German laws and regulations concerning the use of electric skateboards on public roads, including certification requirements (such as ABE certification) and insurance policies. 2. Consumer Profile: Analyze the profile of potential German consumers, including their age, income level, primary usage scenarios (commuting, recreation), key purchasing decision drivers (price, performance, brand, design), and the channels they typically use to gather information (forums, social media, offline retail stores). 3. Channels &amp;amp; Distribution: Investigate Germany‚Äôs mainstream online electronics sales platforms (e.g., Amazon.de, MediaMarkt.de) and high-end sporting goods offline retail chains. List the top 5 potential online and offline distribution partners and find the contact information for their purchasing departments, if possible. 4. Costing &amp;amp; Pricing: Based on the product cost structure in my Product_Cost.csv file on my desktop, and taking into account German customs duties, Value Added Tax (VAT), logistics and warehousing costs, and potential marketing expenses, estimate a Manufacturer‚Äôs Suggested Retail Price (MSRP) and analyze its competitiveness in the market. 5. Comprehensive Report &amp;amp; Presentation: Summarize all research findings into an HTML report file. The content should include data charts, key findings, and a final market entry strategy recommendation (Recommended / Not Recommended / Recommended with Conditions). 
&lt;/details&gt; 
&lt;br /&gt; 
&lt;h3&gt;5. SEO Audit for Workforce Multiagent Launch &lt;a href="https://www.eigent.ai/download?share_token=IjE3NTM2OTk5NzExNDQtNTY5NiI.aIex0w.jc_NIPmfIf9e3zGt-oG9fbMi3K4__1753699971144-5696"&gt;Replay ‚ñ∂Ô∏è&lt;/a&gt;&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Prompt:&lt;/strong&gt; &lt;kbd&gt;To support the launch of our new Workforce Multiagent product ... &lt;kbd&gt;&lt;/kbd&gt;&lt;/kbd&gt;&lt;/summary&gt; 
 &lt;br /&gt; To support the launch of our new Workforce Multiagent product, please run a thorough SEO audit on our official website (https://www.camel-ai.org/) and deliver a detailed optimization report with actionable recommendations. 
&lt;/details&gt; 
&lt;br /&gt; 
&lt;h3&gt;6. Identify Duplicate Files in Downloads &lt;a href="https://www.eigent.ai/download?share_token=IjE3NTM3NjAzODgxNzEtMjQ4Ig.aIhKLQ.epOG--0Nj0o4Bqjtdqm9OZdaqRQ__1753760388171-248"&gt;Replay ‚ñ∂Ô∏è&lt;/a&gt;&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Prompt:&lt;/strong&gt; &lt;kbd&gt;I have a folder named mydocs inside my Documents directory ... &lt;kbd&gt;&lt;/kbd&gt;&lt;/kbd&gt;&lt;/summary&gt; 
 &lt;br /&gt; I have a folder named mydocs inside my Documents directory. Please scan it and identify all files that are exact or near duplicates ‚Äî including those with identical content, file size, or format (even if file names or extensions differ). List them clearly, grouped by similarity. 
&lt;/details&gt; 
&lt;br /&gt; 
&lt;h3&gt;7. Add Signature to PDF &lt;a href="https://www.eigent.ai/download?share_token=IjE3NTQwOTU0ODM0NTItNTY2MSI.aJCHrA.Mg5yPOFqj86H_GQvvRNditzepXc__1754095483452-5661"&gt;Replay ‚ñ∂Ô∏è&lt;/a&gt;&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Prompt:&lt;/strong&gt; &lt;kbd&gt;Please add this signature image to the Signature Areas in the PDF ... &lt;kbd&gt;&lt;/kbd&gt;&lt;/kbd&gt;&lt;/summary&gt; 
 &lt;br /&gt; Please add this signature image to the Signature Areas in the PDF. You could install the CLI tool ‚Äòtesseract‚Äô (needed for reliable location of ‚ÄòSignature Areas‚Äô via OCR) to help finish this task. 
&lt;/details&gt; 
&lt;br /&gt; 
&lt;h2&gt;üõ†Ô∏è Tech Stack&lt;/h2&gt; 
&lt;h3&gt;Backend&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Framework:&lt;/strong&gt;&amp;nbsp;FastAPI&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Package Manager:&lt;/strong&gt;&amp;nbsp;uv&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Async Server:&lt;/strong&gt;&amp;nbsp;Uvicorn&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Authentication:&lt;/strong&gt;&amp;nbsp;OAuth 2.0, Passlib.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-agent framework:&lt;/strong&gt; CAMEL&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Frontend&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Framework:&lt;/strong&gt;&amp;nbsp;React&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Desktop App Framework:&lt;/strong&gt;&amp;nbsp;Electron&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Language:&lt;/strong&gt;&amp;nbsp;TypeScript&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;UI:&lt;/strong&gt;&amp;nbsp;Tailwind CSS, Radix UI, Lucide React, Framer Motion&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;State Management:&lt;/strong&gt;&amp;nbsp;Zustand&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Flow Editor:&lt;/strong&gt;&amp;nbsp;React Flow&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üåü&amp;nbsp;Staying ahead&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT]&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Star Eigent&lt;/strong&gt;, You will receive all release notifications from GitHub without any delay ~ ‚≠êÔ∏è&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://eigent-ai.github.io/.github/assets/star-us.gif" alt="" /&gt;&lt;/p&gt; 
&lt;h2&gt;üó∫Ô∏è Roadmap&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Topics&lt;/th&gt; 
   &lt;th&gt;Issues&lt;/th&gt; 
   &lt;th&gt;Discord Channel&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Context Engineering&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;- Prompt caching&lt;br /&gt; - System prompt optimize&lt;br /&gt; - Toolkit docstring optimize&lt;br /&gt; - Context compression&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://discord.gg/D2e3rBWD"&gt;&lt;strong&gt;Join Discord ‚Üí&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Multi-modal Enhancement&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;- More accurate image understanding when using browser&lt;br /&gt; - Advanced video generation&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://discord.gg/kyapNCeJ"&gt;&lt;strong&gt;Join Discord ‚Üí&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Multi-agent system&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;- Workforce support fixed workflow&lt;br /&gt; - Workforce support multi-round conversion&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://discord.gg/bFRmPuDB"&gt;&lt;strong&gt;Join Discord ‚Üí&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Browser Toolkit&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;- BrowseCamp integration&lt;br /&gt; - Benchmark improvement&lt;br /&gt; - Forbid repeated page visiting&lt;br /&gt; - Automatic cache button clicking&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://discord.gg/NF73ze5v"&gt;&lt;strong&gt;Join Discord ‚Üí&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Document Toolkit&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;- Support dynamic file editing&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://discord.gg/4yAWJxYr"&gt;&lt;strong&gt;Join Discord ‚Üí&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Terminal Toolkit&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;- Benchmark improvement&lt;br /&gt; - Terminal-Bench integration&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://discord.gg/FjQfnsrV"&gt;&lt;strong&gt;Join Discord ‚Üí&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Environment &amp;amp; RL&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;- Environment design&lt;br /&gt; - Data-generation&lt;br /&gt; - RL framework integration (VERL, TRL, OpenRLHF)&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://discord.gg/MaVZXEn8"&gt;&lt;strong&gt;Join Discord ‚Üí&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;&lt;a href="https://github.com/eigent-ai/eigent/raw/main/CONTRIBUTING.md"&gt;ü§ù Contributing&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;We believe in building trust and embracing all forms of open-source collaborations. Your creative contributions help drive the innovation of &lt;code&gt;Eigent&lt;/code&gt;. Explore our GitHub issues and projects to dive in and show us what you‚Äôve got ü§ù‚ù§Ô∏è &lt;a href="https://github.com/eigent-ai/eigent/raw/main/CONTRIBUTING.md"&gt;Contribution Guideline&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Contributors&lt;/h2&gt; 
&lt;a href="https://github.com/eigent-ai/eigent/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=eigent-ai/eigent" /&gt; &lt;/a&gt; 
&lt;p&gt;Made with &lt;a href="https://contrib.rocks"&gt;contrib.rocks&lt;/a&gt;.&lt;/p&gt; 
&lt;br /&gt; 
&lt;h2&gt;&lt;a href="https://github.com/sponsors/camel-ai"&gt;‚ù§Ô∏è Sponsor&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;Eigent is built on top of &lt;a href="https://github.com/camel-ai"&gt;CAMEL-AI.org&lt;/a&gt;'s research and infrastructures. &lt;a href="https://github.com/sponsors/camel-ai"&gt;Sponsoring CAMEL-AI.org&lt;/a&gt; will make &lt;code&gt;Eigent&lt;/code&gt; better.&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;üìÑ&amp;nbsp;Open Source License&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;This repository is licensed under the &lt;a href="https://raw.githubusercontent.com/eigent-ai/eigent/main/LICENSE"&gt;Apache License 2.0&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;üåê Community &amp;amp; Contact&lt;/h2&gt; 
&lt;p&gt;For more information please contact &lt;a href="mailto:info@eigent.ai"&gt;info@eigent.ai&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;GitHub Issues:&lt;/strong&gt; Report bugs, request features, and track development. &lt;a href="https://github.com/eigent-ai/eigent/issues"&gt;Submit an issue&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Discord:&lt;/strong&gt; Get real-time support, chat with the community, and stay updated. &lt;a href="https://discord.camel-ai.org/"&gt;Join us&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;X (Twitter):&lt;/strong&gt; Follow for updates, AI insights, and key announcements. &lt;a href="https://x.com/Eigent_AI"&gt;Follow us&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;WeChat Community:&lt;/strong&gt; Scan the QR code below to add our WeChat assistant, and join our WeChat community group.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/eigent-ai/eigent/main/src/assets/wechat_qr.jpg" width="200" style="display: inline-block; margin: 10px;" /&gt; 
&lt;/div&gt; 
&lt;!-- LINK GROUP --&gt; 
&lt;!-- Social --&gt; 
&lt;!-- camel &amp; eigent --&gt; 
&lt;!-- marketing --&gt; 
&lt;!-- feature --&gt;</description>
    </item>
    
    <item>
      <title>obra/superpowers</title>
      <link>https://github.com/obra/superpowers</link>
      <description>&lt;p&gt;An agentic skills framework &amp; software development methodology that works.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Superpowers&lt;/h1&gt; 
&lt;p&gt;Superpowers is a complete software development workflow for your coding agents, built on top of a set of composable "skills" and some initial instructions that make sure your agent uses them.&lt;/p&gt; 
&lt;h2&gt;How it works&lt;/h2&gt; 
&lt;p&gt;It starts from the moment you fire up your coding agent. As soon as it sees that you're building something, it &lt;em&gt;doesn't&lt;/em&gt; just jump into trying to write code. Instead, it steps back and asks you what you're really trying to do.&lt;/p&gt; 
&lt;p&gt;Once it's teased a spec out of the conversation, it shows it to you in chunks short enough to actually read and digest.&lt;/p&gt; 
&lt;p&gt;After you've signed off on the design, your agent puts together an implementation plan that's clear enough for an enthusiastic junior engineer with poor taste, no judgement, no project context, and an aversion to testing to follow. It emphasizes true red/green TDD, YAGNI (You Aren't Gonna Need It), and DRY.&lt;/p&gt; 
&lt;p&gt;Next up, once you say "go", it launches a &lt;em&gt;subagent-driven-development&lt;/em&gt; process, having agents work through each engineering task, inspecting and reviewing their work, and continuing forward. It's not uncommon for Claude to be able to work autonomously for a couple hours at a time without deviating from the plan you put together.&lt;/p&gt; 
&lt;p&gt;There's a bunch more to it, but that's the core of the system. And because the skills trigger automatically, you don't need to do anything special. Your coding agent just has Superpowers.&lt;/p&gt; 
&lt;h2&gt;Sponsorship&lt;/h2&gt; 
&lt;p&gt;If Superpowers has helped you do stuff that makes money and you are so inclined, I'd greatly appreciate it if you'd consider &lt;a href="https://github.com/sponsors/obra"&gt;sponsoring my opensource work&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Thanks!&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Jesse&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Installation differs by platform. Claude Code has a built-in plugin system. Codex and OpenCode require manual setup.&lt;/p&gt; 
&lt;h3&gt;Claude Code (via Plugin Marketplace)&lt;/h3&gt; 
&lt;p&gt;In Claude Code, register the marketplace first:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;/plugin marketplace add obra/superpowers-marketplace
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then install the plugin from this marketplace:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;/plugin install superpowers@superpowers-marketplace
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Verify Installation&lt;/h3&gt; 
&lt;p&gt;Check that commands appear:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;/help
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code&gt;# Should see:
# /superpowers:brainstorm - Interactive design refinement
# /superpowers:write-plan - Create implementation plan
# /superpowers:execute-plan - Execute plan in batches
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Codex&lt;/h3&gt; 
&lt;p&gt;Tell Codex:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Fetch and follow instructions from https://raw.githubusercontent.com/obra/superpowers/refs/heads/main/.codex/INSTALL.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Detailed docs:&lt;/strong&gt; &lt;a href="https://raw.githubusercontent.com/obra/superpowers/main/docs/README.codex.md"&gt;docs/README.codex.md&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;OpenCode&lt;/h3&gt; 
&lt;p&gt;Tell OpenCode:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Fetch and follow instructions from https://raw.githubusercontent.com/obra/superpowers/refs/heads/main/.opencode/INSTALL.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Detailed docs:&lt;/strong&gt; &lt;a href="https://raw.githubusercontent.com/obra/superpowers/main/docs/README.opencode.md"&gt;docs/README.opencode.md&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;The Basic Workflow&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;brainstorming&lt;/strong&gt; - Activates before writing code. Refines rough ideas through questions, explores alternatives, presents design in sections for validation. Saves design document.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;using-git-worktrees&lt;/strong&gt; - Activates after design approval. Creates isolated workspace on new branch, runs project setup, verifies clean test baseline.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;writing-plans&lt;/strong&gt; - Activates with approved design. Breaks work into bite-sized tasks (2-5 minutes each). Every task has exact file paths, complete code, verification steps.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;subagent-driven-development&lt;/strong&gt; or &lt;strong&gt;executing-plans&lt;/strong&gt; - Activates with plan. Dispatches fresh subagent per task with two-stage review (spec compliance, then code quality), or executes in batches with human checkpoints.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;test-driven-development&lt;/strong&gt; - Activates during implementation. Enforces RED-GREEN-REFACTOR: write failing test, watch it fail, write minimal code, watch it pass, commit. Deletes code written before tests.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;requesting-code-review&lt;/strong&gt; - Activates between tasks. Reviews against plan, reports issues by severity. Critical issues block progress.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;finishing-a-development-branch&lt;/strong&gt; - Activates when tasks complete. Verifies tests, presents options (merge/PR/keep/discard), cleans up worktree.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;The agent checks for relevant skills before any task.&lt;/strong&gt; Mandatory workflows, not suggestions.&lt;/p&gt; 
&lt;h2&gt;What's Inside&lt;/h2&gt; 
&lt;h3&gt;Skills Library&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Testing&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;test-driven-development&lt;/strong&gt; - RED-GREEN-REFACTOR cycle (includes testing anti-patterns reference)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Debugging&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;systematic-debugging&lt;/strong&gt; - 4-phase root cause process (includes root-cause-tracing, defense-in-depth, condition-based-waiting techniques)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;verification-before-completion&lt;/strong&gt; - Ensure it's actually fixed&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Collaboration&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;brainstorming&lt;/strong&gt; - Socratic design refinement&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;writing-plans&lt;/strong&gt; - Detailed implementation plans&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;executing-plans&lt;/strong&gt; - Batch execution with checkpoints&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;dispatching-parallel-agents&lt;/strong&gt; - Concurrent subagent workflows&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;requesting-code-review&lt;/strong&gt; - Pre-review checklist&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;receiving-code-review&lt;/strong&gt; - Responding to feedback&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;using-git-worktrees&lt;/strong&gt; - Parallel development branches&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;finishing-a-development-branch&lt;/strong&gt; - Merge/PR decision workflow&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;subagent-driven-development&lt;/strong&gt; - Fast iteration with two-stage review (spec compliance, then code quality)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Meta&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;writing-skills&lt;/strong&gt; - Create new skills following best practices (includes testing methodology)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;using-superpowers&lt;/strong&gt; - Introduction to the skills system&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Philosophy&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Test-Driven Development&lt;/strong&gt; - Write tests first, always&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Systematic over ad-hoc&lt;/strong&gt; - Process over guessing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Complexity reduction&lt;/strong&gt; - Simplicity as primary goal&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Evidence over claims&lt;/strong&gt; - Verify before declaring success&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Read more: &lt;a href="https://blog.fsck.com/2025/10/09/superpowers/"&gt;Superpowers for Claude Code&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Skills live directly in this repository. To contribute:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Fork the repository&lt;/li&gt; 
 &lt;li&gt;Create a branch for your skill&lt;/li&gt; 
 &lt;li&gt;Follow the &lt;code&gt;writing-skills&lt;/code&gt; skill for creating and testing new skills&lt;/li&gt; 
 &lt;li&gt;Submit a PR&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;See &lt;code&gt;skills/writing-skills/SKILL.md&lt;/code&gt; for the complete guide.&lt;/p&gt; 
&lt;h2&gt;Updating&lt;/h2&gt; 
&lt;p&gt;Skills update automatically when you update the plugin:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;/plugin update superpowers
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;MIT License - see LICENSE file for details&lt;/p&gt; 
&lt;h2&gt;Support&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Issues&lt;/strong&gt;: &lt;a href="https://github.com/obra/superpowers/issues"&gt;https://github.com/obra/superpowers/issues&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Marketplace&lt;/strong&gt;: &lt;a href="https://github.com/obra/superpowers-marketplace"&gt;https://github.com/obra/superpowers-marketplace&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>cjpais/Handy</title>
      <link>https://github.com/cjpais/Handy</link>
      <description>&lt;p&gt;A free, open source, and extensible speech-to-text application that works completely offline.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Handy&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://discord.com/invite/WVBeWsNXK4"&gt;&lt;img src="https://img.shields.io/badge/Discord-%235865F2.svg?style=for-the-badge&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;A free, open source, and extensible speech-to-text application that works completely offline.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Handy is a cross-platform desktop application built with Tauri (Rust + React/TypeScript) that provides simple, privacy-focused speech transcription. Press a shortcut, speak, and have your words appear in any text field‚Äîall without sending your voice to the cloud.&lt;/p&gt; 
&lt;h2&gt;Why Handy?&lt;/h2&gt; 
&lt;p&gt;Handy was created to fill the gap for a truly open source, extensible speech-to-text tool. As stated on &lt;a href="https://handy.computer"&gt;handy.computer&lt;/a&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Free&lt;/strong&gt;: Accessibility tooling belongs in everyone's hands, not behind a paywall&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Open Source&lt;/strong&gt;: Together we can build further. Extend Handy for yourself and contribute to something bigger&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Private&lt;/strong&gt;: Your voice stays on your computer. Get transcriptions without sending audio to the cloud&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Simple&lt;/strong&gt;: One tool, one job. Transcribe what you say and put it into a text box&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Handy isn't trying to be the best speech-to-text app‚Äîit's trying to be the most forkable one.&lt;/p&gt; 
&lt;h2&gt;How It Works&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Press&lt;/strong&gt; a configurable keyboard shortcut to start/stop recording (or use push-to-talk mode)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Speak&lt;/strong&gt; your words while the shortcut is active&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Release&lt;/strong&gt; and Handy processes your speech using Whisper&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Get&lt;/strong&gt; your transcribed text pasted directly into whatever app you're using&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;The process is entirely local:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Silence is filtered using VAD (Voice Activity Detection) with Silero&lt;/li&gt; 
 &lt;li&gt;Transcription uses your choice of models: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Whisper models&lt;/strong&gt; (Small/Medium/Turbo/Large) with GPU acceleration when available&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Parakeet V3&lt;/strong&gt; - CPU-optimized model with excellent performance and automatic language detection&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Works on Windows, macOS, and Linux&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Download the latest release from the &lt;a href="https://github.com/cjpais/Handy/releases"&gt;releases page&lt;/a&gt; or the &lt;a href="https://handy.computer"&gt;website&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Install the application following platform-specific instructions&lt;/li&gt; 
 &lt;li&gt;Launch Handy and grant necessary system permissions (microphone, accessibility)&lt;/li&gt; 
 &lt;li&gt;Configure your preferred keyboard shortcuts in Settings&lt;/li&gt; 
 &lt;li&gt;Start transcribing!&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Development Setup&lt;/h3&gt; 
&lt;p&gt;For detailed build instructions including platform-specific requirements, see &lt;a href="https://raw.githubusercontent.com/cjpais/Handy/main/BUILD.md"&gt;BUILD.md&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Architecture&lt;/h2&gt; 
&lt;p&gt;Handy is built as a Tauri application combining:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Frontend&lt;/strong&gt;: React + TypeScript with Tailwind CSS for the settings UI&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Backend&lt;/strong&gt;: Rust for system integration, audio processing, and ML inference&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Core Libraries&lt;/strong&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;whisper-rs&lt;/code&gt;: Local speech recognition with Whisper models&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;transcription-rs&lt;/code&gt;: CPU-optimized speech recognition with Parakeet models&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;cpal&lt;/code&gt;: Cross-platform audio I/O&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;vad-rs&lt;/code&gt;: Voice Activity Detection&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;rdev&lt;/code&gt;: Global keyboard shortcuts and system events&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;rubato&lt;/code&gt;: Audio resampling&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Debug Mode&lt;/h3&gt; 
&lt;p&gt;Handy includes an advanced debug mode for development and troubleshooting. Access it by pressing:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;macOS&lt;/strong&gt;: &lt;code&gt;Cmd+Shift+D&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Windows/Linux&lt;/strong&gt;: &lt;code&gt;Ctrl+Shift+D&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Known Issues &amp;amp; Current Limitations&lt;/h2&gt; 
&lt;p&gt;This project is actively being developed and has some &lt;a href="https://github.com/cjpais/Handy/issues"&gt;known issues&lt;/a&gt;. We believe in transparency about the current state:&lt;/p&gt; 
&lt;h3&gt;Major Issues (Help Wanted)&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Whisper Model Crashes:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Whisper models crash on certain system configurations (Windows and Linux)&lt;/li&gt; 
 &lt;li&gt;Does not affect all systems - issue is configuration-dependent 
  &lt;ul&gt; 
   &lt;li&gt;If you experience crashes and are a developer, please help to fix and provide debug logs!&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Wayland Support (Linux):&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Limited support for Wayland display server&lt;/li&gt; 
 &lt;li&gt;Requires &lt;a href="https://github.com/atx/wtype"&gt;&lt;code&gt;wtype&lt;/code&gt;&lt;/a&gt; or &lt;a href="https://sr.ht/~geb/dotool/"&gt;&lt;code&gt;dotool&lt;/code&gt;&lt;/a&gt; for text input to work correctly (see &lt;a href="https://raw.githubusercontent.com/cjpais/Handy/main/#linux-notes"&gt;Linux Notes&lt;/a&gt; below for installation)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Linux Notes&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Text Input Tools:&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;For reliable text input on Linux, install the appropriate tool for your display server:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Display Server&lt;/th&gt; 
   &lt;th&gt;Recommended Tool&lt;/th&gt; 
   &lt;th&gt;Install Command&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;X11&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;xdotool&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;sudo apt install xdotool&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Wayland&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;wtype&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;sudo apt install wtype&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Both&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;dotool&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;sudo apt install dotool&lt;/code&gt; (requires &lt;code&gt;input&lt;/code&gt; group)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;X11&lt;/strong&gt;: Install &lt;code&gt;xdotool&lt;/code&gt; for both direct typing and clipboard paste shortcuts&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Wayland&lt;/strong&gt;: Install &lt;code&gt;wtype&lt;/code&gt; (preferred) or &lt;code&gt;dotool&lt;/code&gt; for text input to work correctly&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;dotool setup&lt;/strong&gt;: Requires adding your user to the &lt;code&gt;input&lt;/code&gt; group: &lt;code&gt;sudo usermod -aG input $USER&lt;/code&gt; (then log out and back in)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Without these tools, Handy falls back to enigo which may have limited compatibility, especially on Wayland.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Other Notes:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;The recording overlay is disabled by default on Linux (&lt;code&gt;Overlay Position: None&lt;/code&gt;) because certain compositors treat it as the active window. When the overlay is visible it can steal focus, which prevents Handy from pasting back into the application that triggered transcription. If you enable the overlay anyway, be aware that clipboard-based pasting might fail or end up in the wrong window.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;If you are having trouble with the app, running with the environment variable &lt;code&gt;WEBKIT_DISABLE_DMABUF_RENDERER=1&lt;/code&gt; may help&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;You can manage global shortcuts outside of Handy and still control the app via signals. Sending &lt;code&gt;SIGUSR2&lt;/code&gt; to the Handy process toggles recording on/off, which lets Wayland window managers or other hotkey daemons keep ownership of keybindings. Example (Sway):&lt;/p&gt; &lt;pre&gt;&lt;code class="language-ini"&gt;bindsym $mod+o exec pkill -USR2 -n handy
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;code&gt;pkill&lt;/code&gt; here simply delivers the signal‚Äîit does not terminate the process.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Platform Support&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;macOS (both Intel and Apple Silicon)&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;x64 Windows&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;x64 Linux&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;System Requirements/Recommendations&lt;/h3&gt; 
&lt;p&gt;The following are recommendations for running Handy on your own machine. If you don't meet the system requirements, the performance of the application may be degraded. We are working on improving the performance across all kinds of computers and hardware.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;For Whisper Models:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;macOS&lt;/strong&gt;: M series Mac, Intel Mac&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt;: Intel, AMD, or NVIDIA GPU&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Linux&lt;/strong&gt;: Intel, AMD, or NVIDIA GPU 
  &lt;ul&gt; 
   &lt;li&gt;Ubuntu 22.04, 24.04&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;For Parakeet V3 Model:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;CPU-only operation&lt;/strong&gt; - runs on a wide variety of hardware&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Minimum&lt;/strong&gt;: Intel Skylake (6th gen) or equivalent AMD processors&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Performance&lt;/strong&gt;: ~5x real-time speed on mid-range hardware (tested on i5)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Automatic language detection&lt;/strong&gt; - no manual language selection required&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Roadmap &amp;amp; Active Development&lt;/h2&gt; 
&lt;p&gt;We're actively working on several features and improvements. Contributions and feedback are welcome!&lt;/p&gt; 
&lt;h3&gt;In Progress&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Debug Logging:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Adding debug logging to a file to help diagnose issues&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;macOS Keyboard Improvements:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Support for Globe key as transcription trigger&lt;/li&gt; 
 &lt;li&gt;A rewrite of global shortcut handling for MacOS, and potentially other OS's too.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Opt-in Analytics:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Collect anonymous usage data to help improve Handy&lt;/li&gt; 
 &lt;li&gt;Privacy-first approach with clear opt-in&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Settings Refactoring:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Cleanup and refactor settings system which is becoming bloated and messy&lt;/li&gt; 
 &lt;li&gt;Implement better abstractions for settings management&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Tauri Commands Cleanup:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Abstract and organize Tauri command patterns&lt;/li&gt; 
 &lt;li&gt;Investigate tauri-specta for improved type safety and organization&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Troubleshooting&lt;/h2&gt; 
&lt;h3&gt;Manual Model Installation (For Proxy Users or Network Restrictions)&lt;/h3&gt; 
&lt;p&gt;If you're behind a proxy, firewall, or in a restricted network environment where Handy cannot download models automatically, you can manually download and install them. The URLs are publicly accessible from any browser.&lt;/p&gt; 
&lt;h4&gt;Step 1: Find Your App Data Directory&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;Open Handy settings&lt;/li&gt; 
 &lt;li&gt;Navigate to the &lt;strong&gt;About&lt;/strong&gt; section&lt;/li&gt; 
 &lt;li&gt;Copy the "App Data Directory" path shown there, or use the shortcuts: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;macOS&lt;/strong&gt;: &lt;code&gt;Cmd+Shift+D&lt;/code&gt; to open debug menu&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Windows/Linux&lt;/strong&gt;: &lt;code&gt;Ctrl+Shift+D&lt;/code&gt; to open debug menu&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;The typical paths are:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;macOS&lt;/strong&gt;: &lt;code&gt;~/Library/Application Support/com.pais.handy/&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt;: &lt;code&gt;C:\Users\{username}\AppData\Roaming\com.pais.handy\&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Linux&lt;/strong&gt;: &lt;code&gt;~/.config/com.pais.handy/&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Step 2: Create Models Directory&lt;/h4&gt; 
&lt;p&gt;Inside your app data directory, create a &lt;code&gt;models&lt;/code&gt; folder if it doesn't already exist:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# macOS/Linux
mkdir -p ~/Library/Application\ Support/com.pais.handy/models

# Windows (PowerShell)
New-Item -ItemType Directory -Force -Path "$env:APPDATA\com.pais.handy\models"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Step 3: Download Model Files&lt;/h4&gt; 
&lt;p&gt;Download the models you want from below&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Whisper Models (single .bin files):&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Small (487 MB): &lt;code&gt;https://blob.handy.computer/ggml-small.bin&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Medium (492 MB): &lt;code&gt;https://blob.handy.computer/whisper-medium-q4_1.bin&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Turbo (1600 MB): &lt;code&gt;https://blob.handy.computer/ggml-large-v3-turbo.bin&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Large (1100 MB): &lt;code&gt;https://blob.handy.computer/ggml-large-v3-q5_0.bin&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Parakeet Models (compressed archives):&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;V2 (473 MB): &lt;code&gt;https://blob.handy.computer/parakeet-v2-int8.tar.gz&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;V3 (478 MB): &lt;code&gt;https://blob.handy.computer/parakeet-v3-int8.tar.gz&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Step 4: Install Models&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;For Whisper Models (.bin files):&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Simply place the &lt;code&gt;.bin&lt;/code&gt; file directly into the &lt;code&gt;models&lt;/code&gt; directory:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;{app_data_dir}/models/
‚îú‚îÄ‚îÄ ggml-small.bin
‚îú‚îÄ‚îÄ whisper-medium-q4_1.bin
‚îú‚îÄ‚îÄ ggml-large-v3-turbo.bin
‚îî‚îÄ‚îÄ ggml-large-v3-q5_0.bin
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;For Parakeet Models (.tar.gz archives):&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Extract the &lt;code&gt;.tar.gz&lt;/code&gt; file&lt;/li&gt; 
 &lt;li&gt;Place the &lt;strong&gt;extracted directory&lt;/strong&gt; into the &lt;code&gt;models&lt;/code&gt; folder&lt;/li&gt; 
 &lt;li&gt;The directory must be named exactly as follows: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Parakeet V2&lt;/strong&gt;: &lt;code&gt;parakeet-tdt-0.6b-v2-int8&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Parakeet V3&lt;/strong&gt;: &lt;code&gt;parakeet-tdt-0.6b-v3-int8&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Final structure should look like:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;{app_data_dir}/models/
‚îú‚îÄ‚îÄ parakeet-tdt-0.6b-v2-int8/     (directory with model files inside)
‚îÇ   ‚îú‚îÄ‚îÄ (model files)
‚îÇ   ‚îî‚îÄ‚îÄ (config files)
‚îî‚îÄ‚îÄ parakeet-tdt-0.6b-v3-int8/     (directory with model files inside)
    ‚îú‚îÄ‚îÄ (model files)
    ‚îî‚îÄ‚îÄ (config files)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Important Notes:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;For Parakeet models, the extracted directory name &lt;strong&gt;must&lt;/strong&gt; match exactly as shown above&lt;/li&gt; 
 &lt;li&gt;Do not rename the &lt;code&gt;.bin&lt;/code&gt; files for Whisper models‚Äîuse the exact filenames from the download URLs&lt;/li&gt; 
 &lt;li&gt;After placing the files, restart Handy to detect the new models&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Step 5: Verify Installation&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;Restart Handy&lt;/li&gt; 
 &lt;li&gt;Open Settings ‚Üí Models&lt;/li&gt; 
 &lt;li&gt;Your manually installed models should now appear as "Downloaded"&lt;/li&gt; 
 &lt;li&gt;Select the model you want to use and test transcription&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;How to Contribute&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Check existing issues&lt;/strong&gt; at &lt;a href="https://github.com/cjpais/Handy/issues"&gt;github.com/cjpais/Handy/issues&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Fork the repository&lt;/strong&gt; and create a feature branch&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Test thoroughly&lt;/strong&gt; on your target platform&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Submit a pull request&lt;/strong&gt; with clear description of changes&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Join the discussion&lt;/strong&gt; - reach out at &lt;a href="mailto:contact@handy.computer"&gt;contact@handy.computer&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;The goal is to create both a useful tool and a foundation for others to build upon‚Äîa well-patterned, simple codebase that serves the community.&lt;/p&gt; 
&lt;h2&gt;Sponsors&lt;/h2&gt; 
&lt;div align="center"&gt;
  We're grateful for the support of our sponsors who help make Handy possible: 
 &lt;br /&gt;
 &lt;br /&gt; 
 &lt;a href="https://wordcab.com"&gt; &lt;img src="https://raw.githubusercontent.com/cjpais/Handy/main/sponsor-images/wordcab.png" alt="Wordcab" width="120" height="120" /&gt; &lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 
 &lt;a href="https://github.com/epicenter-so/epicenter"&gt; &lt;img src="https://raw.githubusercontent.com/cjpais/Handy/main/sponsor-images/epicenter.png" alt="Epicenter" width="120" height="120" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;Related Projects&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/cjpais/handy-cli"&gt;Handy CLI&lt;/a&gt;&lt;/strong&gt; - The original Python command-line version&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://handy.computer"&gt;handy.computer&lt;/a&gt;&lt;/strong&gt; - Project website with demos and documentation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;MIT License - see &lt;a href="https://raw.githubusercontent.com/cjpais/Handy/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;h2&gt;Acknowledgments&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Whisper&lt;/strong&gt; by OpenAI for the speech recognition model&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;whisper.cpp and ggml&lt;/strong&gt; for amazing cross-platform whisper inference/acceleration&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Silero&lt;/strong&gt; for great lightweight VAD&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Tauri&lt;/strong&gt; team for the excellent Rust-based app framework&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Community contributors&lt;/strong&gt; helping make Handy better&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;em&gt;"Your search for the right speech-to-text tool can end here‚Äînot because Handy is perfect, but because you can make it perfect for you."&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ultralytics/ultralytics</title>
      <link>https://github.com/ultralytics/ultralytics</link>
      <description>&lt;p&gt;Ultralytics YOLO üöÄ&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p&gt; &lt;a href="https://platform.ultralytics.com/ultralytics/yolo26" target="_blank"&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/ultralytics/assets/main/yolov8/banner-yolov8.png" alt="Ultralytics YOLO banner" /&gt;&lt;/a&gt; &lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://docs.ultralytics.com/zh/"&gt;‰∏≠Êñá&lt;/a&gt; | &lt;a href="https://docs.ultralytics.com/ko/"&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt; | &lt;a href="https://docs.ultralytics.com/ja/"&gt;Êó•Êú¨Ë™û&lt;/a&gt; | &lt;a href="https://docs.ultralytics.com/ru/"&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt; | &lt;a href="https://docs.ultralytics.com/de/"&gt;Deutsch&lt;/a&gt; | &lt;a href="https://docs.ultralytics.com/fr/"&gt;Fran√ßais&lt;/a&gt; | &lt;a href="https://docs.ultralytics.com/es"&gt;Espa√±ol&lt;/a&gt; | &lt;a href="https://docs.ultralytics.com/pt/"&gt;Portugu√™s&lt;/a&gt; | &lt;a href="https://docs.ultralytics.com/tr/"&gt;T√ºrk√ße&lt;/a&gt; | &lt;a href="https://docs.ultralytics.com/vi/"&gt;Ti·∫øng Vi·ªát&lt;/a&gt; | &lt;a href="https://docs.ultralytics.com/ar/"&gt;ÿßŸÑÿπÿ±ÿ®Ÿäÿ©&lt;/a&gt; &lt;br /&gt;&lt;/p&gt; 
 &lt;div&gt; 
  &lt;a href="https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml"&gt;&lt;img src="https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg?sanitize=true" alt="Ultralytics CI" /&gt;&lt;/a&gt; 
  &lt;a href="https://clickpy.clickhouse.com/dashboard/ultralytics"&gt;&lt;img src="https://static.pepy.tech/badge/ultralytics" alt="Ultralytics Downloads" /&gt;&lt;/a&gt; 
  &lt;a href="https://discord.com/invite/ultralytics"&gt;&lt;img alt="Ultralytics Discord" src="https://img.shields.io/discord/1089800235347353640?logo=discord&amp;amp;logoColor=white&amp;amp;label=Discord&amp;amp;color=blue" /&gt;&lt;/a&gt; 
  &lt;a href="https://community.ultralytics.com/"&gt;&lt;img alt="Ultralytics Forums" src="https://img.shields.io/discourse/users?server=https%3A%2F%2Fcommunity.ultralytics.com&amp;amp;logo=discourse&amp;amp;label=Forums&amp;amp;color=blue" /&gt;&lt;/a&gt; 
  &lt;a href="https://www.reddit.com/r/ultralytics/"&gt;&lt;img alt="Ultralytics Reddit" src="https://img.shields.io/reddit/subreddit-subscribers/ultralytics?style=flat&amp;amp;logo=reddit&amp;amp;logoColor=white&amp;amp;label=Reddit&amp;amp;color=blue" /&gt;&lt;/a&gt; 
  &lt;br /&gt; 
  &lt;a href="https://console.paperspace.com/github/ultralytics/ultralytics"&gt;&lt;img src="https://assets.paperspace.io/img/gradient-badge.svg?sanitize=true" alt="Run Ultralytics on Gradient" /&gt;&lt;/a&gt; 
  &lt;a href="https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open Ultralytics In Colab" /&gt;&lt;/a&gt; 
  &lt;a href="https://www.kaggle.com/models/ultralytics/yolo26"&gt;&lt;img src="https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true" alt="Open Ultralytics In Kaggle" /&gt;&lt;/a&gt; 
  &lt;a href="https://mybinder.org/v2/gh/ultralytics/ultralytics/HEAD?labpath=examples%2Ftutorial.ipynb"&gt;&lt;img src="https://mybinder.org/badge_logo.svg?sanitize=true" alt="Open Ultralytics In Binder" /&gt;&lt;/a&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;a href="https://www.ultralytics.com/"&gt;Ultralytics&lt;/a&gt; creates cutting-edge, state-of-the-art (SOTA) &lt;a href="https://www.ultralytics.com/yolo"&gt;YOLO models&lt;/a&gt; built on years of foundational research in computer vision and AI. Constantly updated for performance and flexibility, our models are &lt;strong&gt;fast&lt;/strong&gt;, &lt;strong&gt;accurate&lt;/strong&gt;, and &lt;strong&gt;easy to use&lt;/strong&gt;. They excel at &lt;a href="https://docs.ultralytics.com/tasks/detect/"&gt;object detection&lt;/a&gt;, &lt;a href="https://docs.ultralytics.com/modes/track/"&gt;tracking&lt;/a&gt;, &lt;a href="https://docs.ultralytics.com/tasks/segment/"&gt;instance segmentation&lt;/a&gt;, &lt;a href="https://docs.ultralytics.com/tasks/classify/"&gt;image classification&lt;/a&gt;, and &lt;a href="https://docs.ultralytics.com/tasks/pose/"&gt;pose estimation&lt;/a&gt; tasks.&lt;/p&gt; 
&lt;p&gt;Find detailed documentation in the &lt;a href="https://docs.ultralytics.com/"&gt;Ultralytics Docs&lt;/a&gt;. Get support via &lt;a href="https://github.com/ultralytics/ultralytics/issues/new/choose"&gt;GitHub Issues&lt;/a&gt;. Join discussions on &lt;a href="https://discord.com/invite/ultralytics"&gt;Discord&lt;/a&gt;, &lt;a href="https://www.reddit.com/r/ultralytics/"&gt;Reddit&lt;/a&gt;, and the &lt;a href="https://community.ultralytics.com/"&gt;Ultralytics Community Forums&lt;/a&gt;!&lt;/p&gt; 
&lt;p&gt;Request an Enterprise License for commercial use at &lt;a href="https://www.ultralytics.com/license"&gt;Ultralytics Licensing&lt;/a&gt;.&lt;/p&gt; 
&lt;a href="https://platform.ultralytics.com/ultralytics/yolo26" target="_blank"&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/ultralytics/assets/refs/heads/main/yolo/performance-comparison.png" alt="YOLO26 performance plots" /&gt; &lt;/a&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/ultralytics"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-github.png" width="2%" alt="Ultralytics GitHub" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="2%" alt="space" /&gt; 
 &lt;a href="https://www.linkedin.com/company/ultralytics/"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-linkedin.png" width="2%" alt="Ultralytics LinkedIn" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="2%" alt="space" /&gt; 
 &lt;a href="https://twitter.com/ultralytics"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-twitter.png" width="2%" alt="Ultralytics Twitter" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="2%" alt="space" /&gt; 
 &lt;a href="https://www.youtube.com/ultralytics?sub_confirmation=1"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-youtube.png" width="2%" alt="Ultralytics YouTube" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="2%" alt="space" /&gt; 
 &lt;a href="https://www.tiktok.com/@ultralytics"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-tiktok.png" width="2%" alt="Ultralytics TikTok" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="2%" alt="space" /&gt; 
 &lt;a href="https://ultralytics.com/bilibili"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-bilibili.png" width="2%" alt="Ultralytics BiliBili" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="2%" alt="space" /&gt; 
 &lt;a href="https://discord.com/invite/ultralytics"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-discord.png" width="2%" alt="Ultralytics Discord" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;üìÑ Documentation&lt;/h2&gt; 
&lt;p&gt;See below for quickstart installation and usage examples. For comprehensive guidance on training, validation, prediction, and deployment, refer to our full &lt;a href="https://docs.ultralytics.com/"&gt;Ultralytics Docs&lt;/a&gt;.&lt;/p&gt; 
&lt;details open&gt; 
 &lt;summary&gt;Install&lt;/summary&gt; 
 &lt;p&gt;Install the &lt;code&gt;ultralytics&lt;/code&gt; package, including all &lt;a href="https://github.com/ultralytics/ultralytics/raw/main/pyproject.toml"&gt;requirements&lt;/a&gt;, in a &lt;a href="https://www.python.org/"&gt;&lt;strong&gt;Python&amp;gt;=3.8&lt;/strong&gt;&lt;/a&gt; environment with &lt;a href="https://pytorch.org/get-started/locally/"&gt;&lt;strong&gt;PyTorch&amp;gt;=1.8&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://pypi.org/project/ultralytics/"&gt;&lt;img src="https://img.shields.io/pypi/v/ultralytics?logo=pypi&amp;amp;logoColor=white" alt="PyPI - Version" /&gt;&lt;/a&gt; &lt;a href="https://clickpy.clickhouse.com/dashboard/ultralytics"&gt;&lt;img src="https://static.pepy.tech/badge/ultralytics" alt="Ultralytics Downloads" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/ultralytics/"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/ultralytics?logo=python&amp;amp;logoColor=gold" alt="PyPI - Python Version" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;pip install ultralytics
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;For alternative installation methods, including &lt;a href="https://anaconda.org/conda-forge/ultralytics"&gt;Conda&lt;/a&gt;, &lt;a href="https://hub.docker.com/r/ultralytics/ultralytics"&gt;Docker&lt;/a&gt;, and building from source via Git, please consult the &lt;a href="https://docs.ultralytics.com/quickstart/"&gt;Quickstart Guide&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://anaconda.org/conda-forge/ultralytics"&gt;&lt;img src="https://img.shields.io/conda/vn/conda-forge/ultralytics?logo=condaforge" alt="Conda Version" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/ultralytics/ultralytics"&gt;&lt;img src="https://img.shields.io/docker/v/ultralytics/ultralytics?sort=semver&amp;amp;logo=docker" alt="Docker Image Version" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/ultralytics/ultralytics"&gt;&lt;img src="https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker" alt="Ultralytics Docker Pulls" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details open&gt; 
 &lt;summary&gt;Usage&lt;/summary&gt; 
 &lt;h3&gt;CLI&lt;/h3&gt; 
 &lt;p&gt;You can use Ultralytics YOLO directly from the Command Line Interface (CLI) with the &lt;code&gt;yolo&lt;/code&gt; command:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Predict using a pretrained YOLO model (e.g., YOLO26n) on an image
yolo predict model=yolo26n.pt source='https://ultralytics.com/images/bus.jpg'
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;The &lt;code&gt;yolo&lt;/code&gt; command supports various tasks and modes, accepting additional arguments like &lt;code&gt;imgsz=640&lt;/code&gt;. Explore the YOLO &lt;a href="https://docs.ultralytics.com/usage/cli/"&gt;CLI Docs&lt;/a&gt; for more examples.&lt;/p&gt; 
 &lt;h3&gt;Python&lt;/h3&gt; 
 &lt;p&gt;Ultralytics YOLO can also be integrated directly into your Python projects. It accepts the same &lt;a href="https://docs.ultralytics.com/usage/cfg/"&gt;configuration arguments&lt;/a&gt; as the CLI:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from ultralytics import YOLO

# Load a pretrained YOLO26n model
model = YOLO("yolo26n.pt")

# Train the model on the COCO8 dataset for 100 epochs
train_results = model.train(
    data="coco8.yaml",  # Path to dataset configuration file
    epochs=100,  # Number of training epochs
    imgsz=640,  # Image size for training
    device="cpu",  # Device to run on (e.g., 'cpu', 0, [0,1,2,3])
)

# Evaluate the model's performance on the validation set
metrics = model.val()

# Perform object detection on an image
results = model("path/to/image.jpg")  # Predict on an image
results[0].show()  # Display results

# Export the model to ONNX format for deployment
path = model.export(format="onnx")  # Returns the path to the exported model
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Discover more examples in the YOLO &lt;a href="https://docs.ultralytics.com/usage/python/"&gt;Python Docs&lt;/a&gt;.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;‚ú® Models&lt;/h2&gt; 
&lt;p&gt;Ultralytics supports a wide range of YOLO models, from early versions like &lt;a href="https://docs.ultralytics.com/models/yolov3/"&gt;YOLOv3&lt;/a&gt; to the latest &lt;a href="https://docs.ultralytics.com/models/yolo26/"&gt;YOLO26&lt;/a&gt;. The tables below showcase YOLO26 models pretrained on the &lt;a href="https://docs.ultralytics.com/datasets/detect/coco/"&gt;COCO&lt;/a&gt; dataset for &lt;a href="https://docs.ultralytics.com/tasks/detect/"&gt;Detection&lt;/a&gt;, &lt;a href="https://docs.ultralytics.com/tasks/segment/"&gt;Segmentation&lt;/a&gt;, and &lt;a href="https://docs.ultralytics.com/tasks/pose/"&gt;Pose Estimation&lt;/a&gt;. Additionally, &lt;a href="https://docs.ultralytics.com/tasks/classify/"&gt;Classification&lt;/a&gt; models pretrained on the &lt;a href="https://docs.ultralytics.com/datasets/classify/imagenet/"&gt;ImageNet&lt;/a&gt; dataset are available. &lt;a href="https://docs.ultralytics.com/modes/track/"&gt;Tracking&lt;/a&gt; mode is compatible with all Detection, Segmentation, and Pose models. All &lt;a href="https://docs.ultralytics.com/models/"&gt;Models&lt;/a&gt; are automatically downloaded from the latest Ultralytics &lt;a href="https://github.com/ultralytics/assets/releases"&gt;release&lt;/a&gt; upon first use.&lt;/p&gt; 
&lt;a href="https://docs.ultralytics.com/tasks/" target="_blank"&gt; &lt;img width="100%" src="https://github.com/ultralytics/docs/releases/download/0/ultralytics-yolov8-tasks-banner.avif" alt="Ultralytics YOLO supported tasks" /&gt; &lt;/a&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;details open&gt;
 &lt;summary&gt;Detection (COCO)&lt;/summary&gt; 
 &lt;p&gt;Explore the &lt;a href="https://docs.ultralytics.com/tasks/detect/"&gt;Detection Docs&lt;/a&gt; for usage examples. These models are trained on the &lt;a href="https://cocodataset.org/"&gt;COCO dataset&lt;/a&gt;, featuring 80 object classes.&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model&lt;/th&gt; 
    &lt;th&gt;size&lt;br /&gt;&lt;sup&gt;(pixels)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;mAP&lt;sup&gt;val&lt;br /&gt;50-95&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;mAP&lt;sup&gt;val&lt;br /&gt;50-95(e2e)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;Speed&lt;br /&gt;&lt;sup&gt;CPU ONNX&lt;br /&gt;(ms)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;Speed&lt;br /&gt;&lt;sup&gt;T4 TensorRT10&lt;br /&gt;(ms)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;params&lt;br /&gt;&lt;sup&gt;(M)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;FLOPs&lt;br /&gt;&lt;sup&gt;(B)&lt;/sup&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26n.pt"&gt;YOLO26n&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;40.9&lt;/td&gt; 
    &lt;td&gt;40.1&lt;/td&gt; 
    &lt;td&gt;38.9 ¬± 0.7&lt;/td&gt; 
    &lt;td&gt;1.7 ¬± 0.0&lt;/td&gt; 
    &lt;td&gt;2.4&lt;/td&gt; 
    &lt;td&gt;5.4&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26s.pt"&gt;YOLO26s&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;48.6&lt;/td&gt; 
    &lt;td&gt;47.8&lt;/td&gt; 
    &lt;td&gt;87.2 ¬± 0.9&lt;/td&gt; 
    &lt;td&gt;2.5 ¬± 0.0&lt;/td&gt; 
    &lt;td&gt;9.5&lt;/td&gt; 
    &lt;td&gt;20.7&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26m.pt"&gt;YOLO26m&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;53.1&lt;/td&gt; 
    &lt;td&gt;52.5&lt;/td&gt; 
    &lt;td&gt;220.0 ¬± 1.4&lt;/td&gt; 
    &lt;td&gt;4.7 ¬± 0.1&lt;/td&gt; 
    &lt;td&gt;20.4&lt;/td&gt; 
    &lt;td&gt;68.2&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26l.pt"&gt;YOLO26l&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;55.0&lt;/td&gt; 
    &lt;td&gt;54.4&lt;/td&gt; 
    &lt;td&gt;286.2 ¬± 2.0&lt;/td&gt; 
    &lt;td&gt;6.2 ¬± 0.2&lt;/td&gt; 
    &lt;td&gt;24.8&lt;/td&gt; 
    &lt;td&gt;86.4&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26x.pt"&gt;YOLO26x&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;57.5&lt;/td&gt; 
    &lt;td&gt;56.9&lt;/td&gt; 
    &lt;td&gt;525.8 ¬± 4.0&lt;/td&gt; 
    &lt;td&gt;11.8 ¬± 0.2&lt;/td&gt; 
    &lt;td&gt;55.7&lt;/td&gt; 
    &lt;td&gt;193.9&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;mAP&lt;sup&gt;val&lt;/sup&gt;&lt;/strong&gt; values refer to single-model single-scale performance on the &lt;a href="https://cocodataset.org/"&gt;COCO val2017&lt;/a&gt; dataset. See &lt;a href="https://docs.ultralytics.com/guides/yolo-performance-metrics/"&gt;YOLO Performance Metrics&lt;/a&gt; for details. &lt;br /&gt;Reproduce with &lt;code&gt;yolo val detect data=coco.yaml device=0&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Speed&lt;/strong&gt; metrics are averaged over COCO val images using an &lt;a href="https://aws.amazon.com/ec2/instance-types/p4/"&gt;Amazon EC2 P4d&lt;/a&gt; instance. CPU speeds measured with &lt;a href="https://onnx.ai/"&gt;ONNX&lt;/a&gt; export. GPU speeds measured with &lt;a href="https://developer.nvidia.com/tensorrt"&gt;TensorRT&lt;/a&gt; export. &lt;br /&gt;Reproduce with &lt;code&gt;yolo val detect data=coco.yaml batch=1 device=0|cpu&lt;/code&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;Segmentation (COCO)&lt;/summary&gt; 
 &lt;p&gt;Refer to the &lt;a href="https://docs.ultralytics.com/tasks/segment/"&gt;Segmentation Docs&lt;/a&gt; for usage examples. These models are trained on &lt;a href="https://docs.ultralytics.com/datasets/segment/coco/"&gt;COCO-Seg&lt;/a&gt;, including 80 classes.&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model&lt;/th&gt; 
    &lt;th&gt;size&lt;br /&gt;&lt;sup&gt;(pixels)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;mAP&lt;sup&gt;box&lt;br /&gt;50-95(e2e)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;mAP&lt;sup&gt;mask&lt;br /&gt;50-95(e2e)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;Speed&lt;br /&gt;&lt;sup&gt;CPU ONNX&lt;br /&gt;(ms)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;Speed&lt;br /&gt;&lt;sup&gt;T4 TensorRT10&lt;br /&gt;(ms)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;params&lt;br /&gt;&lt;sup&gt;(M)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;FLOPs&lt;br /&gt;&lt;sup&gt;(B)&lt;/sup&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26n-seg.pt"&gt;YOLO26n-seg&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;39.6&lt;/td&gt; 
    &lt;td&gt;33.9&lt;/td&gt; 
    &lt;td&gt;53.3 ¬± 0.5&lt;/td&gt; 
    &lt;td&gt;2.1 ¬± 0.0&lt;/td&gt; 
    &lt;td&gt;2.7&lt;/td&gt; 
    &lt;td&gt;9.1&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26s-seg.pt"&gt;YOLO26s-seg&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;47.3&lt;/td&gt; 
    &lt;td&gt;40.0&lt;/td&gt; 
    &lt;td&gt;118.4 ¬± 0.9&lt;/td&gt; 
    &lt;td&gt;3.3 ¬± 0.0&lt;/td&gt; 
    &lt;td&gt;10.4&lt;/td&gt; 
    &lt;td&gt;34.2&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26m-seg.pt"&gt;YOLO26m-seg&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;52.5&lt;/td&gt; 
    &lt;td&gt;44.1&lt;/td&gt; 
    &lt;td&gt;328.2 ¬± 2.4&lt;/td&gt; 
    &lt;td&gt;6.7 ¬± 0.1&lt;/td&gt; 
    &lt;td&gt;23.6&lt;/td&gt; 
    &lt;td&gt;121.5&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26l-seg.pt"&gt;YOLO26l-seg&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;54.4&lt;/td&gt; 
    &lt;td&gt;45.5&lt;/td&gt; 
    &lt;td&gt;387.0 ¬± 3.7&lt;/td&gt; 
    &lt;td&gt;8.0 ¬± 0.1&lt;/td&gt; 
    &lt;td&gt;28.0&lt;/td&gt; 
    &lt;td&gt;139.8&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26x-seg.pt"&gt;YOLO26x-seg&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;56.5&lt;/td&gt; 
    &lt;td&gt;47.0&lt;/td&gt; 
    &lt;td&gt;787.0 ¬± 6.8&lt;/td&gt; 
    &lt;td&gt;16.4 ¬± 0.1&lt;/td&gt; 
    &lt;td&gt;62.8&lt;/td&gt; 
    &lt;td&gt;313.5&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;mAP&lt;sup&gt;val&lt;/sup&gt;&lt;/strong&gt; values are for single-model single-scale on the &lt;a href="https://cocodataset.org/"&gt;COCO val2017&lt;/a&gt; dataset. See &lt;a href="https://docs.ultralytics.com/guides/yolo-performance-metrics/"&gt;YOLO Performance Metrics&lt;/a&gt; for details. &lt;br /&gt;Reproduce with &lt;code&gt;yolo val segment data=coco.yaml device=0&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Speed&lt;/strong&gt; metrics are averaged over COCO val images using an &lt;a href="https://aws.amazon.com/ec2/instance-types/p4/"&gt;Amazon EC2 P4d&lt;/a&gt; instance. CPU speeds measured with &lt;a href="https://onnx.ai/"&gt;ONNX&lt;/a&gt; export. GPU speeds measured with &lt;a href="https://developer.nvidia.com/tensorrt"&gt;TensorRT&lt;/a&gt; export. &lt;br /&gt;Reproduce with &lt;code&gt;yolo val segment data=coco.yaml batch=1 device=0|cpu&lt;/code&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;Classification (ImageNet)&lt;/summary&gt; 
 &lt;p&gt;Consult the &lt;a href="https://docs.ultralytics.com/tasks/classify/"&gt;Classification Docs&lt;/a&gt; for usage examples. These models are trained on &lt;a href="https://docs.ultralytics.com/datasets/classify/imagenet/"&gt;ImageNet&lt;/a&gt;, covering 1000 classes.&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model&lt;/th&gt; 
    &lt;th&gt;size&lt;br /&gt;&lt;sup&gt;(pixels)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;acc&lt;br /&gt;&lt;sup&gt;top1&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;acc&lt;br /&gt;&lt;sup&gt;top5&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;Speed&lt;br /&gt;&lt;sup&gt;CPU ONNX&lt;br /&gt;(ms)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;Speed&lt;br /&gt;&lt;sup&gt;T4 TensorRT10&lt;br /&gt;(ms)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;params&lt;br /&gt;&lt;sup&gt;(M)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;FLOPs&lt;br /&gt;&lt;sup&gt;(B) at 224&lt;/sup&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26n-cls.pt"&gt;YOLO26n-cls&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;224&lt;/td&gt; 
    &lt;td&gt;71.4&lt;/td&gt; 
    &lt;td&gt;90.1&lt;/td&gt; 
    &lt;td&gt;5.0 ¬± 0.3&lt;/td&gt; 
    &lt;td&gt;1.1 ¬± 0.0&lt;/td&gt; 
    &lt;td&gt;2.8&lt;/td&gt; 
    &lt;td&gt;0.5&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26s-cls.pt"&gt;YOLO26s-cls&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;224&lt;/td&gt; 
    &lt;td&gt;76.0&lt;/td&gt; 
    &lt;td&gt;92.9&lt;/td&gt; 
    &lt;td&gt;7.9 ¬± 0.2&lt;/td&gt; 
    &lt;td&gt;1.3 ¬± 0.0&lt;/td&gt; 
    &lt;td&gt;6.7&lt;/td&gt; 
    &lt;td&gt;1.6&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26m-cls.pt"&gt;YOLO26m-cls&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;224&lt;/td&gt; 
    &lt;td&gt;78.1&lt;/td&gt; 
    &lt;td&gt;94.2&lt;/td&gt; 
    &lt;td&gt;17.2 ¬± 0.4&lt;/td&gt; 
    &lt;td&gt;2.0 ¬± 0.0&lt;/td&gt; 
    &lt;td&gt;11.6&lt;/td&gt; 
    &lt;td&gt;4.9&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26l-cls.pt"&gt;YOLO26l-cls&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;224&lt;/td&gt; 
    &lt;td&gt;79.0&lt;/td&gt; 
    &lt;td&gt;94.6&lt;/td&gt; 
    &lt;td&gt;23.2 ¬± 0.3&lt;/td&gt; 
    &lt;td&gt;2.8 ¬± 0.0&lt;/td&gt; 
    &lt;td&gt;14.1&lt;/td&gt; 
    &lt;td&gt;6.2&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26x-cls.pt"&gt;YOLO26x-cls&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;224&lt;/td&gt; 
    &lt;td&gt;79.9&lt;/td&gt; 
    &lt;td&gt;95.0&lt;/td&gt; 
    &lt;td&gt;41.4 ¬± 0.9&lt;/td&gt; 
    &lt;td&gt;3.8 ¬± 0.0&lt;/td&gt; 
    &lt;td&gt;29.6&lt;/td&gt; 
    &lt;td&gt;13.6&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;acc&lt;/strong&gt; values represent model accuracy on the &lt;a href="https://www.image-net.org/"&gt;ImageNet&lt;/a&gt; dataset validation set. &lt;br /&gt;Reproduce with &lt;code&gt;yolo val classify data=path/to/ImageNet device=0&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Speed&lt;/strong&gt; metrics are averaged over ImageNet val images using an &lt;a href="https://aws.amazon.com/ec2/instance-types/p4/"&gt;Amazon EC2 P4d&lt;/a&gt; instance. CPU speeds measured with &lt;a href="https://onnx.ai/"&gt;ONNX&lt;/a&gt; export. GPU speeds measured with &lt;a href="https://developer.nvidia.com/tensorrt"&gt;TensorRT&lt;/a&gt; export. &lt;br /&gt;Reproduce with &lt;code&gt;yolo val classify data=path/to/ImageNet batch=1 device=0|cpu&lt;/code&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;Pose (COCO)&lt;/summary&gt; 
 &lt;p&gt;See the &lt;a href="https://docs.ultralytics.com/tasks/pose/"&gt;Pose Estimation Docs&lt;/a&gt; for usage examples. These models are trained on &lt;a href="https://docs.ultralytics.com/datasets/pose/coco/"&gt;COCO-Pose&lt;/a&gt;, focusing on the 'person' class.&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model&lt;/th&gt; 
    &lt;th&gt;size&lt;br /&gt;&lt;sup&gt;(pixels)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;mAP&lt;sup&gt;pose&lt;br /&gt;50-95(e2e)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;mAP&lt;sup&gt;pose&lt;br /&gt;50(e2e)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;Speed&lt;br /&gt;&lt;sup&gt;CPU ONNX&lt;br /&gt;(ms)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;Speed&lt;br /&gt;&lt;sup&gt;T4 TensorRT10&lt;br /&gt;(ms)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;params&lt;br /&gt;&lt;sup&gt;(M)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;FLOPs&lt;br /&gt;&lt;sup&gt;(B)&lt;/sup&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26n-pose.pt"&gt;YOLO26n-pose&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;57.2&lt;/td&gt; 
    &lt;td&gt;83.3&lt;/td&gt; 
    &lt;td&gt;40.3 ¬± 0.5&lt;/td&gt; 
    &lt;td&gt;1.8 ¬± 0.0&lt;/td&gt; 
    &lt;td&gt;2.9&lt;/td&gt; 
    &lt;td&gt;7.5&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26s-pose.pt"&gt;YOLO26s-pose&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;63.0&lt;/td&gt; 
    &lt;td&gt;86.6&lt;/td&gt; 
    &lt;td&gt;85.3 ¬± 0.9&lt;/td&gt; 
    &lt;td&gt;2.7 ¬± 0.0&lt;/td&gt; 
    &lt;td&gt;10.4&lt;/td&gt; 
    &lt;td&gt;23.9&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26m-pose.pt"&gt;YOLO26m-pose&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;68.8&lt;/td&gt; 
    &lt;td&gt;89.6&lt;/td&gt; 
    &lt;td&gt;218.0 ¬± 1.5&lt;/td&gt; 
    &lt;td&gt;5.0 ¬± 0.1&lt;/td&gt; 
    &lt;td&gt;21.5&lt;/td&gt; 
    &lt;td&gt;73.1&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26l-pose.pt"&gt;YOLO26l-pose&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;70.4&lt;/td&gt; 
    &lt;td&gt;90.5&lt;/td&gt; 
    &lt;td&gt;275.4 ¬± 2.4&lt;/td&gt; 
    &lt;td&gt;6.5 ¬± 0.1&lt;/td&gt; 
    &lt;td&gt;25.9&lt;/td&gt; 
    &lt;td&gt;91.3&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26x-pose.pt"&gt;YOLO26x-pose&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;71.6&lt;/td&gt; 
    &lt;td&gt;91.6&lt;/td&gt; 
    &lt;td&gt;565.4 ¬± 3.0&lt;/td&gt; 
    &lt;td&gt;12.2 ¬± 0.2&lt;/td&gt; 
    &lt;td&gt;57.6&lt;/td&gt; 
    &lt;td&gt;201.7&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;mAP&lt;sup&gt;val&lt;/sup&gt;&lt;/strong&gt; values are for single-model single-scale on the &lt;a href="https://docs.ultralytics.com/datasets/pose/coco/"&gt;COCO Keypoints val2017&lt;/a&gt; dataset. See &lt;a href="https://docs.ultralytics.com/guides/yolo-performance-metrics/"&gt;YOLO Performance Metrics&lt;/a&gt; for details. &lt;br /&gt;Reproduce with &lt;code&gt;yolo val pose data=coco-pose.yaml device=0&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Speed&lt;/strong&gt; metrics are averaged over COCO val images using an &lt;a href="https://aws.amazon.com/ec2/instance-types/p4/"&gt;Amazon EC2 P4d&lt;/a&gt; instance. CPU speeds measured with &lt;a href="https://onnx.ai/"&gt;ONNX&lt;/a&gt; export. GPU speeds measured with &lt;a href="https://developer.nvidia.com/tensorrt"&gt;TensorRT&lt;/a&gt; export. &lt;br /&gt;Reproduce with &lt;code&gt;yolo val pose data=coco-pose.yaml batch=1 device=0|cpu&lt;/code&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;Oriented Bounding Boxes (DOTAv1)&lt;/summary&gt; 
 &lt;p&gt;Check the &lt;a href="https://docs.ultralytics.com/tasks/obb/"&gt;OBB Docs&lt;/a&gt; for usage examples. These models are trained on &lt;a href="https://docs.ultralytics.com/datasets/obb/dota-v2/#dota-v10/"&gt;DOTAv1&lt;/a&gt;, including 15 classes.&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model&lt;/th&gt; 
    &lt;th&gt;size&lt;br /&gt;&lt;sup&gt;(pixels)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;mAP&lt;sup&gt;test&lt;br /&gt;50-95(e2e)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;mAP&lt;sup&gt;test&lt;br /&gt;50(e2e)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;Speed&lt;br /&gt;&lt;sup&gt;CPU ONNX&lt;br /&gt;(ms)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;Speed&lt;br /&gt;&lt;sup&gt;T4 TensorRT10&lt;br /&gt;(ms)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;params&lt;br /&gt;&lt;sup&gt;(M)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;FLOPs&lt;br /&gt;&lt;sup&gt;(B)&lt;/sup&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26n-obb.pt"&gt;YOLO26n-obb&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;1024&lt;/td&gt; 
    &lt;td&gt;52.4&lt;/td&gt; 
    &lt;td&gt;78.9&lt;/td&gt; 
    &lt;td&gt;97.7 ¬± 0.9&lt;/td&gt; 
    &lt;td&gt;2.8 ¬± 0.0&lt;/td&gt; 
    &lt;td&gt;2.5&lt;/td&gt; 
    &lt;td&gt;14.0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26s-obb.pt"&gt;YOLO26s-obb&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;1024&lt;/td&gt; 
    &lt;td&gt;54.8&lt;/td&gt; 
    &lt;td&gt;80.9&lt;/td&gt; 
    &lt;td&gt;218.0 ¬± 1.4&lt;/td&gt; 
    &lt;td&gt;4.9 ¬± 0.1&lt;/td&gt; 
    &lt;td&gt;9.8&lt;/td&gt; 
    &lt;td&gt;55.1&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26m-obb.pt"&gt;YOLO26m-obb&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;1024&lt;/td&gt; 
    &lt;td&gt;55.3&lt;/td&gt; 
    &lt;td&gt;81.0&lt;/td&gt; 
    &lt;td&gt;579.2 ¬± 3.8&lt;/td&gt; 
    &lt;td&gt;10.2 ¬± 0.3&lt;/td&gt; 
    &lt;td&gt;21.2&lt;/td&gt; 
    &lt;td&gt;183.3&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26l-obb.pt"&gt;YOLO26l-obb&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;1024&lt;/td&gt; 
    &lt;td&gt;56.2&lt;/td&gt; 
    &lt;td&gt;81.6&lt;/td&gt; 
    &lt;td&gt;735.6 ¬± 3.1&lt;/td&gt; 
    &lt;td&gt;13.0 ¬± 0.2&lt;/td&gt; 
    &lt;td&gt;25.6&lt;/td&gt; 
    &lt;td&gt;230.0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26x-obb.pt"&gt;YOLO26x-obb&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;1024&lt;/td&gt; 
    &lt;td&gt;56.7&lt;/td&gt; 
    &lt;td&gt;81.7&lt;/td&gt; 
    &lt;td&gt;1485.7 ¬± 11.5&lt;/td&gt; 
    &lt;td&gt;30.5 ¬± 0.9&lt;/td&gt; 
    &lt;td&gt;57.6&lt;/td&gt; 
    &lt;td&gt;516.5&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;mAP&lt;sup&gt;test&lt;/sup&gt;&lt;/strong&gt; values are for single-model multiscale performance on the &lt;a href="https://captain-whu.github.io/DOTA/dataset.html"&gt;DOTAv1 test set&lt;/a&gt;. &lt;br /&gt;Reproduce by &lt;code&gt;yolo val obb data=DOTAv1.yaml device=0 split=test&lt;/code&gt; and submit merged results to the &lt;a href="https://captain-whu.github.io/DOTA/evaluation.html"&gt;DOTA evaluation server&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Speed&lt;/strong&gt; metrics are averaged over &lt;a href="https://docs.ultralytics.com/datasets/obb/dota-v2/#dota-v10"&gt;DOTAv1 val images&lt;/a&gt; using an &lt;a href="https://aws.amazon.com/ec2/instance-types/p4/"&gt;Amazon EC2 P4d&lt;/a&gt; instance. CPU speeds measured with &lt;a href="https://onnx.ai/"&gt;ONNX&lt;/a&gt; export. GPU speeds measured with &lt;a href="https://developer.nvidia.com/tensorrt"&gt;TensorRT&lt;/a&gt; export. &lt;br /&gt;Reproduce by &lt;code&gt;yolo val obb data=DOTAv1.yaml batch=1 device=0|cpu&lt;/code&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;üß© Integrations&lt;/h2&gt; 
&lt;p&gt;Our key integrations with leading AI platforms extend the functionality of Ultralytics' offerings, enhancing tasks like dataset labeling, training, visualization, and model management. Discover how Ultralytics, in collaboration with partners like &lt;a href="https://docs.ultralytics.com/integrations/weights-biases/"&gt;Weights &amp;amp; Biases&lt;/a&gt;, &lt;a href="https://docs.ultralytics.com/integrations/comet/"&gt;Comet ML&lt;/a&gt;, &lt;a href="https://docs.ultralytics.com/integrations/roboflow/"&gt;Roboflow&lt;/a&gt;, and &lt;a href="https://docs.ultralytics.com/integrations/openvino/"&gt;Intel OpenVINO&lt;/a&gt;, can optimize your AI workflow. Explore more at &lt;a href="https://docs.ultralytics.com/integrations/"&gt;Ultralytics Integrations&lt;/a&gt;.&lt;/p&gt; 
&lt;a href="https://docs.ultralytics.com/integrations/" target="_blank"&gt; &lt;img width="100%" src="https://github.com/ultralytics/assets/raw/main/yolov8/banner-integrations.png" alt="Ultralytics active learning integrations" /&gt; &lt;/a&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://platform.ultralytics.com/ultralytics/yolo26"&gt; &lt;img src="https://github.com/ultralytics/assets/raw/main/partners/logo-ultralytics-hub.png" width="10%" alt="Ultralytics Platform logo" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="15%" height="0" alt="space" /&gt; 
 &lt;a href="https://docs.ultralytics.com/integrations/weights-biases/"&gt; &lt;img src="https://github.com/ultralytics/assets/raw/main/partners/logo-wb.png" width="10%" alt="Weights &amp;amp; Biases logo" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="15%" height="0" alt="space" /&gt; 
 &lt;a href="https://docs.ultralytics.com/integrations/comet/"&gt; &lt;img src="https://github.com/ultralytics/assets/raw/main/partners/logo-comet.png" width="10%" alt="Comet ML logo" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="15%" height="0" alt="space" /&gt; 
 &lt;a href="https://docs.ultralytics.com/integrations/neural-magic/"&gt; &lt;img src="https://github.com/ultralytics/assets/raw/main/partners/logo-neuralmagic.png" width="10%" alt="Neural Magic logo" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;Ultralytics Platform üåü&lt;/th&gt; 
   &lt;th align="center"&gt;Weights &amp;amp; Biases&lt;/th&gt; 
   &lt;th align="center"&gt;Comet&lt;/th&gt; 
   &lt;th align="center"&gt;Neural Magic&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Streamline YOLO workflows: Label, train, and deploy effortlessly with &lt;a href="https://platform.ultralytics.com/ultralytics/yolo26"&gt;Ultralytics Platform&lt;/a&gt;. Try now!&lt;/td&gt; 
   &lt;td align="center"&gt;Track experiments, hyperparameters, and results with &lt;a href="https://docs.ultralytics.com/integrations/weights-biases/"&gt;Weights &amp;amp; Biases&lt;/a&gt;.&lt;/td&gt; 
   &lt;td align="center"&gt;Free forever, &lt;a href="https://docs.ultralytics.com/integrations/comet/"&gt;Comet ML&lt;/a&gt; lets you save YOLO models, resume training, and interactively visualize predictions.&lt;/td&gt; 
   &lt;td align="center"&gt;Run YOLO inference up to 6x faster with &lt;a href="https://docs.ultralytics.com/integrations/neural-magic/"&gt;Neural Magic DeepSparse&lt;/a&gt;.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;ü§ù Contribute&lt;/h2&gt; 
&lt;p&gt;We thrive on community collaboration! Ultralytics YOLO wouldn't be the SOTA framework it is without contributions from developers like you. Please see our &lt;a href="https://docs.ultralytics.com/help/contributing/"&gt;Contributing Guide&lt;/a&gt; to get started. We also welcome your feedback‚Äîshare your experience by completing our &lt;a href="https://www.ultralytics.com/survey?utm_source=github&amp;amp;utm_medium=social&amp;amp;utm_campaign=Survey"&gt;Survey&lt;/a&gt;. A huge &lt;strong&gt;Thank You&lt;/strong&gt; üôè to everyone who contributes!&lt;/p&gt; 
&lt;!-- SVG image from https://opencollective.com/ultralytics/contributors.svg?width=1280 --&gt; 
&lt;p&gt;&lt;a href="https://github.com/ultralytics/ultralytics/graphs/contributors"&gt;&lt;img src="https://raw.githubusercontent.com/ultralytics/assets/main/im/image-contributors.png" alt="Ultralytics open-source contributors" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;We look forward to your contributions to help make the Ultralytics ecosystem even better!&lt;/p&gt; 
&lt;h2&gt;üìú License&lt;/h2&gt; 
&lt;p&gt;Ultralytics offers two licensing options to suit different needs:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;AGPL-3.0 License&lt;/strong&gt;: This &lt;a href="https://opensource.org/license/agpl-v3"&gt;OSI-approved&lt;/a&gt; open-source license is perfect for students, researchers, and enthusiasts. It encourages open collaboration and knowledge sharing. See the &lt;a href="https://github.com/ultralytics/ultralytics/raw/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for full details.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Ultralytics Enterprise License&lt;/strong&gt;: Designed for commercial use, this license allows for the seamless integration of Ultralytics software and AI models into commercial products and services, bypassing the open-source requirements of AGPL-3.0. If your use case involves commercial deployment, please contact us via &lt;a href="https://www.ultralytics.com/license"&gt;Ultralytics Licensing&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üìû Contact&lt;/h2&gt; 
&lt;p&gt;For bug reports and feature requests related to Ultralytics software, please visit &lt;a href="https://github.com/ultralytics/ultralytics/issues"&gt;GitHub Issues&lt;/a&gt;. For questions, discussions, and community support, join our active communities on &lt;a href="https://discord.com/invite/ultralytics"&gt;Discord&lt;/a&gt;, &lt;a href="https://www.reddit.com/r/ultralytics/"&gt;Reddit&lt;/a&gt;, and the &lt;a href="https://community.ultralytics.com/"&gt;Ultralytics Community Forums&lt;/a&gt;. We're here to help with all things Ultralytics!&lt;/p&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/ultralytics"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-github.png" width="3%" alt="Ultralytics GitHub" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="3%" alt="space" /&gt; 
 &lt;a href="https://www.linkedin.com/company/ultralytics/"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-linkedin.png" width="3%" alt="Ultralytics LinkedIn" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="3%" alt="space" /&gt; 
 &lt;a href="https://twitter.com/ultralytics"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-twitter.png" width="3%" alt="Ultralytics Twitter" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="3%" alt="space" /&gt; 
 &lt;a href="https://www.youtube.com/ultralytics?sub_confirmation=1"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-youtube.png" width="3%" alt="Ultralytics YouTube" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="3%" alt="space" /&gt; 
 &lt;a href="https://www.tiktok.com/@ultralytics"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-tiktok.png" width="3%" alt="Ultralytics TikTok" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="3%" alt="space" /&gt; 
 &lt;a href="https://ultralytics.com/bilibili"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-bilibili.png" width="3%" alt="Ultralytics BiliBili" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="3%" alt="space" /&gt; 
 &lt;a href="https://discord.com/invite/ultralytics"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-discord.png" width="3%" alt="Ultralytics Discord" /&gt;&lt;/a&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Gentleman-Programming/Gentleman.Dots</title>
      <link>https://github.com/Gentleman-Programming/Gentleman.Dots</link>
      <description>&lt;p&gt;My personal configuration for LazyVim !&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Gentleman.Dots&lt;/h1&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;‚ÑπÔ∏è &lt;strong&gt;Update (January 2026)&lt;/strong&gt;: OpenCode now supports Claude Max/Pro subscriptions via the &lt;code&gt;opencode-anthropic-auth&lt;/code&gt; plugin (included in this config). Both &lt;strong&gt;Claude Code&lt;/strong&gt; and &lt;strong&gt;OpenCode&lt;/strong&gt; work with your Claude subscription. &lt;em&gt;Note: This workaround is stable for now, but Anthropic could block it in the future.&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Gentleman-Programming/Gentleman.Dots/main/#what-is-this"&gt;What is this?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Gentleman-Programming/Gentleman.Dots/main/#quick-start"&gt;Quick Start&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Gentleman-Programming/Gentleman.Dots/main/#supported-platforms"&gt;Supported Platforms&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Gentleman-Programming/Gentleman.Dots/main/#-vim-mastery-trainer"&gt;Vim Mastery Trainer&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Gentleman-Programming/Gentleman.Dots/main/#documentation"&gt;Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Gentleman-Programming/Gentleman.Dots/main/#tools-overview"&gt;Tools Overview&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Gentleman-Programming/Gentleman.Dots/main/#bleeding-edge"&gt;Bleeding Edge&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Gentleman-Programming/Gentleman.Dots/main/#project-structure"&gt;Project Structure&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Gentleman-Programming/Gentleman.Dots/main/#support"&gt;Support&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Preview&lt;/h2&gt; 
&lt;h3&gt;TUI Installer&lt;/h3&gt; 
&lt;img width="1424" height="1536" alt="TUI Installer" src="https://github.com/user-attachments/assets/1db56d3b-a8c0-4885-82aa-c5ec04af4ac0" /&gt; 
&lt;h3&gt;Showcase&lt;/h3&gt; 
&lt;img width="3840" height="2160" alt="Development Environment Showcase" src="https://github.com/user-attachments/assets/fff14c05-9676-4e04-b05e-dab5e3cf300a" /&gt; 
&lt;hr /&gt; 
&lt;h2&gt;What is this?&lt;/h2&gt; 
&lt;p&gt;A complete development environment configuration including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Neovim&lt;/strong&gt; with LSP, autocompletion, and AI assistants (Claude Code, Gemini, OpenCode)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Shells&lt;/strong&gt;: Fish, Zsh, Nushell&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Terminal Multiplexers&lt;/strong&gt;: Tmux, Zellij&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Terminal Emulators&lt;/strong&gt;: Alacritty, WezTerm, Kitty, Ghostty&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;h3&gt;Option 1: Homebrew (Recommended)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;brew install Gentleman-Programming/tap/gentleman-dots
gentleman.dots
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Option 2: Direct Download&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# macOS Apple Silicon
curl -fsSL https://github.com/Gentleman-Programming/Gentleman.Dots/releases/latest/download/gentleman-installer-darwin-arm64 -o gentleman.dots

# macOS Intel
curl -fsSL https://github.com/Gentleman-Programming/Gentleman.Dots/releases/latest/download/gentleman-installer-darwin-amd64 -o gentleman.dots

# Linux x86_64
curl -fsSL https://github.com/Gentleman-Programming/Gentleman.Dots/releases/latest/download/gentleman-installer-linux-amd64 -o gentleman.dots

# Linux ARM64 (Raspberry Pi, etc.)
curl -fsSL https://github.com/Gentleman-Programming/Gentleman.Dots/releases/latest/download/gentleman-installer-linux-arm64 -o gentleman.dots

# Then run
chmod +x gentleman.dots
./gentleman.dots
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Option 3: Termux (Android)&lt;/h3&gt; 
&lt;p&gt;Termux requires building the installer locally (Go cross-compilation to Android has limitations).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 1. Install dependencies
pkg update &amp;amp;&amp;amp; pkg upgrade
pkg install git golang

# 2. Clone the repository
git clone https://github.com/Gentleman-Programming/Gentleman.Dots.git
cd Gentleman.Dots/installer

# 3. Build and run
go build -o ~/gentleman-installer ./cmd/gentleman-installer
cd ~
./gentleman-installer
&lt;/code&gt;&lt;/pre&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Termux Support&lt;/th&gt; 
   &lt;th&gt;Status&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Shells (Fish, Zsh, Nushell)&lt;/td&gt; 
   &lt;td&gt;‚úÖ Available&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Multiplexers (Tmux, Zellij)&lt;/td&gt; 
   &lt;td&gt;‚úÖ Available&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Neovim with full config&lt;/td&gt; 
   &lt;td&gt;‚úÖ Available&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Nerd Fonts&lt;/td&gt; 
   &lt;td&gt;‚úÖ Auto-installed to &lt;code&gt;~/.termux/font.ttf&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Terminal emulators&lt;/td&gt; 
   &lt;td&gt;‚ùå Not applicable&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Homebrew&lt;/td&gt; 
   &lt;td&gt;‚ùå Uses &lt;code&gt;pkg&lt;/code&gt; instead&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Tip:&lt;/strong&gt; After installation, restart Termux to apply the font, then run &lt;code&gt;tmux&lt;/code&gt; or &lt;code&gt;zellij&lt;/code&gt; to start your configured environment.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;The TUI guides you through selecting your preferred tools and handles all the configuration automatically.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Windows users:&lt;/strong&gt; You must set up WSL first. See the &lt;a href="https://raw.githubusercontent.com/Gentleman-Programming/Gentleman.Dots/main/docs/manual-installation.md#windows-wsl"&gt;Manual Installation Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Supported Platforms&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Platform&lt;/th&gt; 
   &lt;th&gt;Architecture&lt;/th&gt; 
   &lt;th&gt;Install Method&lt;/th&gt; 
   &lt;th&gt;Package Manager&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;macOS&lt;/td&gt; 
   &lt;td&gt;Apple Silicon (ARM64)&lt;/td&gt; 
   &lt;td&gt;Homebrew, Direct Download&lt;/td&gt; 
   &lt;td&gt;Homebrew&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;macOS&lt;/td&gt; 
   &lt;td&gt;Intel (x86_64)&lt;/td&gt; 
   &lt;td&gt;Homebrew, Direct Download&lt;/td&gt; 
   &lt;td&gt;Homebrew&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Linux (Ubuntu/Debian)&lt;/td&gt; 
   &lt;td&gt;x86_64, ARM64&lt;/td&gt; 
   &lt;td&gt;Homebrew, Direct Download&lt;/td&gt; 
   &lt;td&gt;Homebrew&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Linux (Fedora/RHEL)&lt;/td&gt; 
   &lt;td&gt;x86_64, ARM64&lt;/td&gt; 
   &lt;td&gt;Direct Download&lt;/td&gt; 
   &lt;td&gt;dnf&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Linux (Arch)&lt;/td&gt; 
   &lt;td&gt;x86_64&lt;/td&gt; 
   &lt;td&gt;Homebrew, Direct Download&lt;/td&gt; 
   &lt;td&gt;Homebrew&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Windows&lt;/td&gt; 
   &lt;td&gt;WSL&lt;/td&gt; 
   &lt;td&gt;Direct Download (see docs)&lt;/td&gt; 
   &lt;td&gt;Homebrew&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Android&lt;/td&gt; 
   &lt;td&gt;Termux (ARM64)&lt;/td&gt; 
   &lt;td&gt;Build locally (see above)&lt;/td&gt; 
   &lt;td&gt;pkg&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üéÆ Vim Mastery Trainer&lt;/h2&gt; 
&lt;p&gt;Learn Vim the fun way! The installer includes an interactive RPG-style trainer with:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Module&lt;/th&gt; 
   &lt;th&gt;Keys Covered&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üî§ Horizontal Movement&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;w&lt;/code&gt;, &lt;code&gt;e&lt;/code&gt;, &lt;code&gt;b&lt;/code&gt;, &lt;code&gt;f&lt;/code&gt;, &lt;code&gt;t&lt;/code&gt;, &lt;code&gt;0&lt;/code&gt;, &lt;code&gt;$&lt;/code&gt;, &lt;code&gt;^&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;‚ÜïÔ∏è Vertical Movement&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;j&lt;/code&gt;, &lt;code&gt;k&lt;/code&gt;, &lt;code&gt;G&lt;/code&gt;, &lt;code&gt;gg&lt;/code&gt;, &lt;code&gt;{&lt;/code&gt;, &lt;code&gt;}&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üì¶ Text Objects&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;iw&lt;/code&gt;, &lt;code&gt;aw&lt;/code&gt;, &lt;code&gt;i"&lt;/code&gt;, &lt;code&gt;a(&lt;/code&gt;, &lt;code&gt;it&lt;/code&gt;, &lt;code&gt;at&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;‚úÇÔ∏è Change &amp;amp; Repeat&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;d&lt;/code&gt;, &lt;code&gt;c&lt;/code&gt;, &lt;code&gt;dd&lt;/code&gt;, &lt;code&gt;cc&lt;/code&gt;, &lt;code&gt;D&lt;/code&gt;, &lt;code&gt;C&lt;/code&gt;, &lt;code&gt;x&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üîÑ Substitution&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;r&lt;/code&gt;, &lt;code&gt;R&lt;/code&gt;, &lt;code&gt;s&lt;/code&gt;, &lt;code&gt;S&lt;/code&gt;, &lt;code&gt;~&lt;/code&gt;, &lt;code&gt;gu&lt;/code&gt;, &lt;code&gt;gU&lt;/code&gt;, &lt;code&gt;J&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üé¨ Macros &amp;amp; Registers&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;qa&lt;/code&gt;, &lt;code&gt;@a&lt;/code&gt;, &lt;code&gt;@@&lt;/code&gt;, &lt;code&gt;"ay&lt;/code&gt;, &lt;code&gt;"+p&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üîç Regex/Search&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;/&lt;/code&gt;, &lt;code&gt;?&lt;/code&gt;, &lt;code&gt;n&lt;/code&gt;, &lt;code&gt;N&lt;/code&gt;, &lt;code&gt;*&lt;/code&gt;, &lt;code&gt;#&lt;/code&gt;, &lt;code&gt;\v&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Each module includes 15 progressive lessons, practice mode with intelligent exercise selection, boss fights, and XP tracking.&lt;/p&gt; 
&lt;p&gt;Launch it from the main menu: &lt;strong&gt;Vim Mastery Trainer&lt;/strong&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Document&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/Gentleman-Programming/Gentleman.Dots/main/docs/tui-installer.md"&gt;TUI Installer Guide&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Interactive installer features, navigation, backup/restore&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/Gentleman-Programming/Gentleman.Dots/main/docs/manual-installation.md"&gt;Manual Installation&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Step-by-step manual setup for all platforms&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/Gentleman-Programming/Gentleman.Dots/main/docs/neovim-keymaps.md"&gt;Neovim Keymaps&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Complete reference of all keybindings&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/Gentleman-Programming/Gentleman.Dots/main/docs/ai-configuration.md"&gt;AI Configuration&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Claude Code, OpenCode, Copilot, and other AI assistants&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/Gentleman-Programming/Gentleman.Dots/main/docs/vim-trainer-spec.md"&gt;Vim Trainer Spec&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Technical specification for the Vim Mastery Trainer&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/Gentleman-Programming/Gentleman.Dots/main/docs/docker-testing.md"&gt;Docker Testing&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;E2E testing with Docker containers&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/Gentleman-Programming/Gentleman.Dots/main/docs/contributing.md"&gt;Contributing&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Development setup, skills system, E2E tests, release process&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Tools Overview&lt;/h2&gt; 
&lt;h3&gt;Terminal Emulators&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Tool&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Ghostty&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;GPU-accelerated, native, blazing fast&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Kitty&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Feature-rich, GPU-based rendering&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;WezTerm&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Lua-configurable, cross-platform&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Alacritty&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Minimal, Rust-based, lightweight&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Shells&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Tool&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Nushell&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Structured data, modern syntax, pipelines&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Fish&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;User-friendly, great defaults, no config needed&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Zsh&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Highly customizable, POSIX-compatible, Powerlevel10k&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Multiplexers&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Tool&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Tmux&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Battle-tested, widely used, lots of plugins&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Zellij&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Modern, WebAssembly plugins, floating panes&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Editor&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Tool&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Neovim&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;LazyVim config with LSP, completions, AI&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Prompts&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Tool&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Starship&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Cross-shell prompt with Git integration&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Bleeding Edge&lt;/h2&gt; 
&lt;p&gt;Want the latest experimental features from my daily workflow (macOS only)?&lt;/p&gt; 
&lt;p&gt;Check out the &lt;a href="https://github.com/Gentleman-Programming/Gentleman.Dots/tree/nix-migration"&gt;&lt;code&gt;nix-migration&lt;/code&gt; branch&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;This branch contains cutting-edge configurations that eventually make their way to &lt;code&gt;main&lt;/code&gt; once stable.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Project Structure&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;Gentleman.Dots/
‚îú‚îÄ‚îÄ installer/               # Go TUI installer source
‚îÇ   ‚îú‚îÄ‚îÄ cmd/                 # Entry point
‚îÇ   ‚îú‚îÄ‚îÄ internal/            # TUI, system, and trainer packages
‚îÇ   ‚îî‚îÄ‚îÄ e2e/                 # Docker-based E2E tests
‚îú‚îÄ‚îÄ docs/                    # Documentation
‚îú‚îÄ‚îÄ skills/                  # AI agent skills (repo-specific)
‚îÇ
‚îú‚îÄ‚îÄ GentlemanNvim/           # Neovim configuration (LazyVim)
‚îú‚îÄ‚îÄ GentlemanClaude/         # Claude Code config + user skills
‚îÇ   ‚îî‚îÄ‚îÄ skills/              # Installable skills (React, Next.js, etc.)
‚îú‚îÄ‚îÄ GentlemanOpenCode/       # OpenCode AI config
‚îÇ
‚îú‚îÄ‚îÄ GentlemanFish/           # Fish shell config
‚îú‚îÄ‚îÄ GentlemanZsh/            # Zsh + Oh-My-Zsh + Powerlevel10k
‚îú‚îÄ‚îÄ GentlemanNushell/        # Nushell config
‚îú‚îÄ‚îÄ GentlemanTmux/           # Tmux config
‚îú‚îÄ‚îÄ GentlemanZellij/         # Zellij config
‚îÇ
‚îú‚îÄ‚îÄ GentlemanGhostty/        # Ghostty terminal config
‚îú‚îÄ‚îÄ GentlemanKitty/          # Kitty terminal config
‚îú‚îÄ‚îÄ alacritty.toml           # Alacritty config
‚îú‚îÄ‚îÄ .wezterm.lua             # WezTerm config
‚îÇ
‚îî‚îÄ‚îÄ starship.toml            # Starship prompt config
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Support&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Issues&lt;/strong&gt;: &lt;a href="https://github.com/Gentleman-Programming/Gentleman.Dots/issues"&gt;GitHub Issues&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Discord&lt;/strong&gt;: &lt;a href="https://discord.gg/gentleman-programming"&gt;Gentleman Programming Community&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;YouTube&lt;/strong&gt;: &lt;a href="https://youtube.com/@GentlemanProgramming"&gt;@GentlemanProgramming&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Twitch&lt;/strong&gt;: &lt;a href="https://twitch.tv/GentlemanProgramming"&gt;GentlemanProgramming&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;MIT License - feel free to use, modify, and share.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Happy coding!&lt;/strong&gt; üé©&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>puckeditor/puck</title>
      <link>https://github.com/puckeditor/puck</link>
      <description>&lt;p&gt;The visual editor for React&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://puckeditor.com?utm_source=readme&amp;amp;utm_medium=code&amp;amp;utm_campaign=repo&amp;amp;utm_contents=logo"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="https://res.cloudinary.com/die3nptcg/image/upload/Puck_Logo_White_RGB_j2rwgg.svg" height="100px" aria-label="Puck logo" /&gt; 
   &lt;img src="https://res.cloudinary.com/die3nptcg/image/upload/Puck_Logo_Black_RGB_dqsjag.svg?sanitize=true" height="100px" aria-label="Puck logo" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; 
 &lt;p&gt;&lt;em&gt;The visual editor for React&lt;/em&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://puckeditor.com/docs?utm_source=readme&amp;amp;utm_medium=code&amp;amp;utm_campaign=repo&amp;amp;utm_contents=docs_link"&gt;Documentation&lt;/a&gt; ‚Ä¢ &lt;a href="https://demo.puckeditor.com/edit?utm_source=readme&amp;amp;utm_medium=code&amp;amp;utm_campaign=repo&amp;amp;utm_contents=demo_link"&gt;Demo&lt;/a&gt; ‚Ä¢ &lt;a href="https://discord.gg/V9mDAhuxyZ"&gt;Discord&lt;/a&gt; ‚Ä¢ &lt;a href="https://github.com/puckeditor/puck/raw/main/CONTRIBUTING.md"&gt;Contributing&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;‚≠êÔ∏è Enjoying Puck? Please &lt;a href="https://github.com/puckeditor/puck"&gt;leave a star&lt;/a&gt;!&lt;/p&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://demo.puckeditor.com/edit"&gt;&lt;img src="https://github.com/user-attachments/assets/25e1ae25-ca5e-450f-afa0-01816830b731" alt="GIF showing a page being created in the Puck Editor, with components being added, arranged, and customized in real time" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;What is Puck?&lt;/h2&gt; 
&lt;p&gt;Puck is a modular, open-source visual editor for React.js. You can use Puck to build custom drag-and-drop experiences with your own application and React components.&lt;/p&gt; 
&lt;p&gt;Because Puck is just a React component, it plays well with all React.js environments, including Next.js. You own your data and there‚Äôs no vendor lock-in.&lt;/p&gt; 
&lt;p&gt;Puck is also &lt;a href="https://github.com/puckeditor/puck?tab=MIT-1-ov-file#readme"&gt;licensed under MIT&lt;/a&gt;, making it suitable for both internal systems and commercial applications.&lt;/p&gt; 
&lt;h2&gt;Quick start&lt;/h2&gt; 
&lt;p&gt;Install the package:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;npm i @puckeditor/core --save # or npx create-puck-app my-app
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Render the editor:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;// Editor.jsx
import { Puck } from "@puckeditor/core";
import "@puckeditor/core/puck.css";

// Create Puck component config
const config = {
  components: {
    HeadingBlock: {
      fields: {
        children: {
          type: "text",
        },
      },
      render: ({ children }) =&amp;gt; {
        return &amp;lt;h1&amp;gt;{children}&amp;lt;/h1&amp;gt;;
      },
    },
  },
};

// Describe the initial data
const initialData = {};

// Save the data to your database
const save = (data) =&amp;gt; {};

// Render Puck editor
export function Editor() {
  return &amp;lt;Puck config={config} data={initialData} onPublish={save} /&amp;gt;;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Render the page:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsx"&gt;// Page.jsx
import { Render } from "@puckeditor/core";
import "@puckeditor/core/puck.css";

export function Page() {
  return &amp;lt;Render config={config} data={data} /&amp;gt;;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Recipes&lt;/h2&gt; 
&lt;p&gt;Use &lt;code&gt;create-puck-app&lt;/code&gt; to quickly spin up a a pre-configured app based on our provided &lt;a href="https://github.com/puckeditor/puck/tree/main/recipes"&gt;recipes&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;npx create-puck-app my-app
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Available recipes include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/puckeditor/puck/tree/main/recipes/next"&gt;&lt;strong&gt;next&lt;/strong&gt;&lt;/a&gt;: Next.js example, using App Router and static page generation&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/puckeditor/puck/tree/main/recipes/remix"&gt;&lt;strong&gt;remix&lt;/strong&gt;&lt;/a&gt;: Remix Run v2 example, using dynamic routes at root-level&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/puckeditor/puck/tree/main/recipes/react-router"&gt;&lt;strong&gt;react-router&lt;/strong&gt;&lt;/a&gt;: React Router v7 app example, using dynamic routes to create pages at any level&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Community&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://discord.gg/D9e4E3MQVZ"&gt;Discord server&lt;/a&gt; for discussions&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/puckeditor/awesome-puck"&gt;awesome-puck&lt;/a&gt; community repo for plugins, custom fields &amp;amp; more&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Get support&lt;/h2&gt; 
&lt;p&gt;If you have any questions about Puck, please open a &lt;a href="https://github.com/puckeditor/puck/issues"&gt;GitHub issue&lt;/a&gt; or join us on &lt;a href="https://discord.gg/D9e4E3MQVZ"&gt;Discord&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Or &lt;a href="https://app.cal.com/chrisvxd/puck-enquiry/"&gt;book a discovery call&lt;/a&gt; for hands-on support and consultancy.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;MIT ¬© &lt;a href="https://github.com/puckeditor/puck/graphs/contributors"&gt;The Puck Contributors&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>google/langextract</title>
      <link>https://github.com/google/langextract</link>
      <description>&lt;p&gt;A Python library for extracting structured information from unstructured text using LLMs with precise source grounding and interactive visualization.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="https://github.com/google/langextract"&gt; &lt;img src="https://raw.githubusercontent.com/google/langextract/main/docs/_static/logo.svg?sanitize=true" alt="LangExtract Logo" width="128" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h1&gt;LangExtract&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://pypi.org/project/langextract/"&gt;&lt;img src="https://img.shields.io/pypi/v/langextract.svg?sanitize=true" alt="PyPI version" /&gt;&lt;/a&gt; &lt;a href="https://github.com/google/langextract"&gt;&lt;img src="https://img.shields.io/github/stars/google/langextract.svg?style=social&amp;amp;label=Star" alt="GitHub stars" /&gt;&lt;/a&gt; &lt;img src="https://github.com/google/langextract/actions/workflows/ci.yaml/badge.svg?sanitize=true" alt="Tests" /&gt; &lt;a href="https://doi.org/10.5281/zenodo.17015089"&gt;&lt;img src="https://zenodo.org/badge/DOI/10.5281/zenodo.17015089.svg?sanitize=true" alt="DOI" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#introduction"&gt;Introduction&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#why-langextract"&gt;Why LangExtract?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#quick-start"&gt;Quick Start&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#api-key-setup-for-cloud-models"&gt;API Key Setup for Cloud Models&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#adding-custom-model-providers"&gt;Adding Custom Model Providers&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#using-openai-models"&gt;Using OpenAI Models&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#using-local-llms-with-ollama"&gt;Using Local LLMs with Ollama&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#more-examples"&gt;More Examples&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#romeo-and-juliet-full-text-extraction"&gt;&lt;em&gt;Romeo and Juliet&lt;/em&gt; Full Text Extraction&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#medication-extraction"&gt;Medication Extraction&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#radiology-report-structuring-radextract"&gt;Radiology Report Structuring: RadExtract&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#community-providers"&gt;Community Providers&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#testing"&gt;Testing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#disclaimer"&gt;Disclaimer&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;LangExtract is a Python library that uses LLMs to extract structured information from unstructured text documents based on user-defined instructions. It processes materials such as clinical notes or reports, identifying and organizing key details while ensuring the extracted data corresponds to the source text.&lt;/p&gt; 
&lt;h2&gt;Why LangExtract?&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Precise Source Grounding:&lt;/strong&gt; Maps every extraction to its exact location in the source text, enabling visual highlighting for easy traceability and verification.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Reliable Structured Outputs:&lt;/strong&gt; Enforces a consistent output schema based on your few-shot examples, leveraging controlled generation in supported models like Gemini to guarantee robust, structured results.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Optimized for Long Documents:&lt;/strong&gt; Overcomes the "needle-in-a-haystack" challenge of large document extraction by using an optimized strategy of text chunking, parallel processing, and multiple passes for higher recall.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Interactive Visualization:&lt;/strong&gt; Instantly generates a self-contained, interactive HTML file to visualize and review thousands of extracted entities in their original context.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Flexible LLM Support:&lt;/strong&gt; Supports your preferred models, from cloud-based LLMs like the Google Gemini family to local open-source models via the built-in Ollama interface.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Adaptable to Any Domain:&lt;/strong&gt; Define extraction tasks for any domain using just a few examples. LangExtract adapts to your needs without requiring any model fine-tuning.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Leverages LLM World Knowledge:&lt;/strong&gt; Utilize precise prompt wording and few-shot examples to influence how the extraction task may utilize LLM knowledge. The accuracy of any inferred information and its adherence to the task specification are contingent upon the selected LLM, the complexity of the task, the clarity of the prompt instructions, and the nature of the prompt examples.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Using cloud-hosted models like Gemini requires an API key. See the &lt;a href="https://raw.githubusercontent.com/google/langextract/main/#api-key-setup-for-cloud-models"&gt;API Key Setup&lt;/a&gt; section for instructions on how to get and configure your key.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Extract structured information with just a few lines of code.&lt;/p&gt; 
&lt;h3&gt;1. Define Your Extraction Task&lt;/h3&gt; 
&lt;p&gt;First, create a prompt that clearly describes what you want to extract. Then, provide a high-quality example to guide the model.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import langextract as lx
import textwrap

# 1. Define the prompt and extraction rules
prompt = textwrap.dedent("""\
    Extract characters, emotions, and relationships in order of appearance.
    Use exact text for extractions. Do not paraphrase or overlap entities.
    Provide meaningful attributes for each entity to add context.""")

# 2. Provide a high-quality example to guide the model
examples = [
    lx.data.ExampleData(
        text="ROMEO. But soft! What light through yonder window breaks? It is the east, and Juliet is the sun.",
        extractions=[
            lx.data.Extraction(
                extraction_class="character",
                extraction_text="ROMEO",
                attributes={"emotional_state": "wonder"}
            ),
            lx.data.Extraction(
                extraction_class="emotion",
                extraction_text="But soft!",
                attributes={"feeling": "gentle awe"}
            ),
            lx.data.Extraction(
                extraction_class="relationship",
                extraction_text="Juliet is the sun",
                attributes={"type": "metaphor"}
            ),
        ]
    )
]
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Examples drive model behavior. Each &lt;code&gt;extraction_text&lt;/code&gt; should ideally be verbatim from the example's &lt;code&gt;text&lt;/code&gt; (no paraphrasing), listed in order of appearance. LangExtract raises &lt;code&gt;Prompt alignment&lt;/code&gt; warnings by default if examples don't follow this pattern‚Äîresolve these for best results.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;2. Run the Extraction&lt;/h3&gt; 
&lt;p&gt;Provide your input text and the prompt materials to the &lt;code&gt;lx.extract&lt;/code&gt; function.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# The input text to be processed
input_text = "Lady Juliet gazed longingly at the stars, her heart aching for Romeo"

# Run the extraction
result = lx.extract(
    text_or_documents=input_text,
    prompt_description=prompt,
    examples=examples,
    model_id="gemini-2.5-flash",
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Model Selection&lt;/strong&gt;: &lt;code&gt;gemini-2.5-flash&lt;/code&gt; is the recommended default, offering an excellent balance of speed, cost, and quality. For highly complex tasks requiring deeper reasoning, &lt;code&gt;gemini-2.5-pro&lt;/code&gt; may provide superior results. For large-scale or production use, a Tier 2 Gemini quota is suggested to increase throughput and avoid rate limits. See the &lt;a href="https://ai.google.dev/gemini-api/docs/rate-limits#tier-2"&gt;rate-limit documentation&lt;/a&gt; for details.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Model Lifecycle&lt;/strong&gt;: Note that Gemini models have a lifecycle with defined retirement dates. Users should consult the &lt;a href="https://cloud.google.com/vertex-ai/generative-ai/docs/learn/model-versions"&gt;official model version documentation&lt;/a&gt; to stay informed about the latest stable and legacy versions.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;3. Visualize the Results&lt;/h3&gt; 
&lt;p&gt;The extractions can be saved to a &lt;code&gt;.jsonl&lt;/code&gt; file, a popular format for working with language model data. LangExtract can then generate an interactive HTML visualization from this file to review the entities in context.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Save the results to a JSONL file
lx.io.save_annotated_documents([result], output_name="extraction_results.jsonl", output_dir=".")

# Generate the visualization from the file
html_content = lx.visualize("extraction_results.jsonl")
with open("visualization.html", "w") as f:
    if hasattr(html_content, 'data'):
        f.write(html_content.data)  # For Jupyter/Colab
    else:
        f.write(html_content)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This creates an animated and interactive HTML file:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/google/langextract/main/docs/_static/romeo_juliet_basic.gif" alt="Romeo and Juliet Basic Visualization " /&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note on LLM Knowledge Utilization:&lt;/strong&gt; This example demonstrates extractions that stay close to the text evidence - extracting "longing" for Lady Juliet's emotional state and identifying "yearning" from "gazed longingly at the stars." The task could be modified to generate attributes that draw more heavily from the LLM's world knowledge (e.g., adding &lt;code&gt;"identity": "Capulet family daughter"&lt;/code&gt; or &lt;code&gt;"literary_context": "tragic heroine"&lt;/code&gt;). The balance between text-evidence and knowledge-inference is controlled by your prompt instructions and example attributes.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Scaling to Longer Documents&lt;/h3&gt; 
&lt;p&gt;For larger texts, you can process entire documents directly from URLs with parallel processing and enhanced sensitivity:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Process Romeo &amp;amp; Juliet directly from Project Gutenberg
result = lx.extract(
    text_or_documents="https://www.gutenberg.org/files/1513/1513-0.txt",
    prompt_description=prompt,
    examples=examples,
    model_id="gemini-2.5-flash",
    extraction_passes=3,    # Improves recall through multiple passes
    max_workers=20,         # Parallel processing for speed
    max_char_buffer=1000    # Smaller contexts for better accuracy
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This approach can extract hundreds of entities from full novels while maintaining high accuracy. The interactive visualization seamlessly handles large result sets, making it easy to explore hundreds of entities from the output JSONL file. &lt;strong&gt;&lt;a href="https://github.com/google/langextract/raw/main/docs/examples/longer_text_example.md"&gt;See the full &lt;em&gt;Romeo and Juliet&lt;/em&gt; extraction example ‚Üí&lt;/a&gt;&lt;/strong&gt; for detailed results and performance insights.&lt;/p&gt; 
&lt;h3&gt;Vertex AI Batch Processing&lt;/h3&gt; 
&lt;p&gt;Save costs on large-scale tasks by enabling Vertex AI Batch API: &lt;code&gt;language_model_params={"vertexai": True, "batch": {"enabled": True}}&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;See an example of the Vertex AI Batch API usage in &lt;a href="https://raw.githubusercontent.com/google/langextract/main/docs/examples/batch_api_example.md"&gt;this example&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;From PyPI&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install langextract
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Recommended for most users. For isolated environments, consider using a virtual environment:&lt;/em&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m venv langextract_env
source langextract_env/bin/activate  # On Windows: langextract_env\Scripts\activate
pip install langextract
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;From Source&lt;/h3&gt; 
&lt;p&gt;LangExtract uses modern Python packaging with &lt;code&gt;pyproject.toml&lt;/code&gt; for dependency management:&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Installing with &lt;code&gt;-e&lt;/code&gt; puts the package in development mode, allowing you to modify the code without reinstalling.&lt;/em&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/google/langextract.git
cd langextract

# For basic installation:
pip install -e .

# For development (includes linting tools):
pip install -e ".[dev]"

# For testing (includes pytest):
pip install -e ".[test]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Docker&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker build -t langextract .
docker run --rm -e LANGEXTRACT_API_KEY="your-api-key" langextract python your_script.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;API Key Setup for Cloud Models&lt;/h2&gt; 
&lt;p&gt;When using LangExtract with cloud-hosted models (like Gemini or OpenAI), you'll need to set up an API key. On-device models don't require an API key. For developers using local LLMs, LangExtract offers built-in support for Ollama and can be extended to other third-party APIs by updating the inference endpoints.&lt;/p&gt; 
&lt;h3&gt;API Key Sources&lt;/h3&gt; 
&lt;p&gt;Get API keys from:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://aistudio.google.com/app/apikey"&gt;AI Studio&lt;/a&gt; for Gemini models&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://cloud.google.com/vertex-ai/generative-ai/docs/sdks/overview"&gt;Vertex AI&lt;/a&gt; for enterprise use&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://platform.openai.com/api-keys"&gt;OpenAI Platform&lt;/a&gt; for OpenAI models&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Setting up API key in your environment&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Option 1: Environment Variable&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export LANGEXTRACT_API_KEY="your-api-key-here"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Option 2: .env File (Recommended)&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Add your API key to a &lt;code&gt;.env&lt;/code&gt; file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Add API key to .env file
cat &amp;gt;&amp;gt; .env &amp;lt;&amp;lt; 'EOF'
LANGEXTRACT_API_KEY=your-api-key-here
EOF

# Keep your API key secure
echo '.env' &amp;gt;&amp;gt; .gitignore
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;In your Python code:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import langextract as lx

result = lx.extract(
    text_or_documents=input_text,
    prompt_description="Extract information...",
    examples=[...],
    model_id="gemini-2.5-flash"
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Option 3: Direct API Key (Not Recommended for Production)&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;You can also provide the API key directly in your code, though this is not recommended for production use:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;result = lx.extract(
    text_or_documents=input_text,
    prompt_description="Extract information...",
    examples=[...],
    model_id="gemini-2.5-flash",
    api_key="your-api-key-here"  # Only use this for testing/development
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Option 4: Vertex AI (Service Accounts)&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Use &lt;a href="https://cloud.google.com/vertex-ai/docs/start/introduction-unified-platform"&gt;Vertex AI&lt;/a&gt; for authentication with service accounts:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;result = lx.extract(
    text_or_documents=input_text,
    prompt_description="Extract information...",
    examples=[...],
    model_id="gemini-2.5-flash",
    language_model_params={
        "vertexai": True,
        "project": "your-project-id",
        "location": "global"  # or regional endpoint
    }
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Adding Custom Model Providers&lt;/h2&gt; 
&lt;p&gt;LangExtract supports custom LLM providers via a lightweight plugin system. You can add support for new models without changing core code.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Add new model support independently of the core library&lt;/li&gt; 
 &lt;li&gt;Distribute your provider as a separate Python package&lt;/li&gt; 
 &lt;li&gt;Keep custom dependencies isolated&lt;/li&gt; 
 &lt;li&gt;Override or extend built-in providers via priority-based resolution&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See the detailed guide in &lt;a href="https://raw.githubusercontent.com/google/langextract/main/langextract/providers/README.md"&gt;Provider System Documentation&lt;/a&gt; to learn how to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Register a provider with &lt;code&gt;@registry.register(...)&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Publish an entry point for discovery&lt;/li&gt; 
 &lt;li&gt;Optionally provide a schema with &lt;code&gt;get_schema_class()&lt;/code&gt; for structured output&lt;/li&gt; 
 &lt;li&gt;Integrate with the factory via &lt;code&gt;create_model(...)&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Using OpenAI Models&lt;/h2&gt; 
&lt;p&gt;LangExtract supports OpenAI models (requires optional dependency: &lt;code&gt;pip install langextract[openai]&lt;/code&gt;):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import langextract as lx

result = lx.extract(
    text_or_documents=input_text,
    prompt_description=prompt,
    examples=examples,
    model_id="gpt-4o",  # Automatically selects OpenAI provider
    api_key=os.environ.get('OPENAI_API_KEY'),
    fence_output=True,
    use_schema_constraints=False
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note: OpenAI models require &lt;code&gt;fence_output=True&lt;/code&gt; and &lt;code&gt;use_schema_constraints=False&lt;/code&gt; because LangExtract doesn't implement schema constraints for OpenAI yet.&lt;/p&gt; 
&lt;h2&gt;Using Local LLMs with Ollama&lt;/h2&gt; 
&lt;p&gt;LangExtract supports local inference using Ollama, allowing you to run models without API keys:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import langextract as lx

result = lx.extract(
    text_or_documents=input_text,
    prompt_description=prompt,
    examples=examples,
    model_id="gemma2:2b",  # Automatically selects Ollama provider
    model_url="http://localhost:11434",
    fence_output=False,
    use_schema_constraints=False
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Quick setup:&lt;/strong&gt; Install Ollama from &lt;a href="https://ollama.com/"&gt;ollama.com&lt;/a&gt;, run &lt;code&gt;ollama pull gemma2:2b&lt;/code&gt;, then &lt;code&gt;ollama serve&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;For detailed installation, Docker setup, and examples, see &lt;a href="https://raw.githubusercontent.com/google/langextract/main/examples/ollama/"&gt;&lt;code&gt;examples/ollama/&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;More Examples&lt;/h2&gt; 
&lt;p&gt;Additional examples of LangExtract in action:&lt;/p&gt; 
&lt;h3&gt;&lt;em&gt;Romeo and Juliet&lt;/em&gt; Full Text Extraction&lt;/h3&gt; 
&lt;p&gt;LangExtract can process complete documents directly from URLs. This example demonstrates extraction from the full text of &lt;em&gt;Romeo and Juliet&lt;/em&gt; from Project Gutenberg (147,843 characters), showing parallel processing, sequential extraction passes, and performance optimization for long document processing.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/google/langextract/raw/main/docs/examples/longer_text_example.md"&gt;View &lt;em&gt;Romeo and Juliet&lt;/em&gt; Full Text Example ‚Üí&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h3&gt;Medication Extraction&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Disclaimer:&lt;/strong&gt; This demonstration is for illustrative purposes of LangExtract's baseline capability only. It does not represent a finished or approved product, is not intended to diagnose or suggest treatment of any disease or condition, and should not be used for medical advice.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;LangExtract excels at extracting structured medical information from clinical text. These examples demonstrate both basic entity recognition (medication names, dosages, routes) and relationship extraction (connecting medications to their attributes), showing LangExtract's effectiveness for healthcare applications.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/google/langextract/raw/main/docs/examples/medication_examples.md"&gt;View Medication Examples ‚Üí&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h3&gt;Radiology Report Structuring: RadExtract&lt;/h3&gt; 
&lt;p&gt;Explore RadExtract, a live interactive demo on HuggingFace Spaces that shows how LangExtract can automatically structure radiology reports. Try it directly in your browser with no setup required.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://huggingface.co/spaces/google/radextract"&gt;View RadExtract Demo ‚Üí&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;Community Providers&lt;/h2&gt; 
&lt;p&gt;Extend LangExtract with custom model providers! Check out our &lt;a href="https://raw.githubusercontent.com/google/langextract/main/COMMUNITY_PROVIDERS.md"&gt;Community Provider Plugins&lt;/a&gt; registry to discover providers created by the community or add your own.&lt;/p&gt; 
&lt;p&gt;For detailed instructions on creating a provider plugin, see the &lt;a href="https://raw.githubusercontent.com/google/langextract/main/examples/custom_provider_plugin/"&gt;Custom Provider Plugin Example&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Contributions are welcome! See &lt;a href="https://github.com/google/langextract/raw/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; to get started with development, testing, and pull requests. You must sign a &lt;a href="https://cla.developers.google.com/about"&gt;Contributor License Agreement&lt;/a&gt; before submitting patches.&lt;/p&gt; 
&lt;h2&gt;Testing&lt;/h2&gt; 
&lt;p&gt;To run tests locally from the source:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Clone the repository
git clone https://github.com/google/langextract.git
cd langextract

# Install with test dependencies
pip install -e ".[test]"

# Run all tests
pytest tests
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or reproduce the full CI matrix locally with tox:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;tox  # runs pylint + pytest on Python 3.10 and 3.11
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Ollama Integration Testing&lt;/h3&gt; 
&lt;p&gt;If you have Ollama installed locally, you can run integration tests:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Test Ollama integration (requires Ollama running with gemma2:2b model)
tox -e ollama-integration
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This test will automatically detect if Ollama is available and run real inference tests.&lt;/p&gt; 
&lt;h2&gt;Development&lt;/h2&gt; 
&lt;h3&gt;Code Formatting&lt;/h3&gt; 
&lt;p&gt;This project uses automated formatting tools to maintain consistent code style:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Auto-format all code
./autoformat.sh

# Or run formatters separately
isort langextract tests --profile google --line-length 80
pyink langextract tests --config pyproject.toml
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Pre-commit Hooks&lt;/h3&gt; 
&lt;p&gt;For automatic formatting checks:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pre-commit install  # One-time setup
pre-commit run --all-files  # Manual run
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Linting&lt;/h3&gt; 
&lt;p&gt;Run linting before submitting PRs:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pylint --rcfile=.pylintrc langextract tests
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/google/langextract/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for full development guidelines.&lt;/p&gt; 
&lt;h2&gt;Disclaimer&lt;/h2&gt; 
&lt;p&gt;This is not an officially supported Google product. If you use LangExtract in production or publications, please cite accordingly and acknowledge usage. Use is subject to the &lt;a href="https://github.com/google/langextract/raw/main/LICENSE"&gt;Apache 2.0 License&lt;/a&gt;. For health-related applications, use of LangExtract is also subject to the &lt;a href="https://developers.google.com/health-ai-developer-foundations/terms"&gt;Health AI Developer Foundations Terms of Use&lt;/a&gt;.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;strong&gt;Happy Extracting!&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>iOfficeAI/AionUi</title>
      <link>https://github.com/iOfficeAI/AionUi</link>
      <description>&lt;p&gt;Free, local, open-source Cowork for Gemini CLI, Claude Code, Codex, Opencode, Qwen Code, Goose Cli, Auggie, and more | üåü Star if you like it!&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="./resources/aionui-banner-1 copy.png" alt="AionUi - Cowork with Your CLI AI Agent" width="100%" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://img.shields.io/github/v/release/iOfficeAI/AionUi?style=flat-square&amp;amp;color=32CD32" alt="Version" /&gt; &amp;nbsp; &lt;img src="https://img.shields.io/badge/license-Apache--2.0-32CD32?style=flat-square&amp;amp;logo=apache&amp;amp;logoColor=white" alt="License" /&gt; &amp;nbsp; &lt;img src="https://img.shields.io/badge/platform-macOS%20%7C%20Windows%20%7C%20Linux-6C757D?style=flat-square&amp;amp;logo=linux&amp;amp;logoColor=white" alt="Platform" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/15423" target="_blank"&gt; &lt;img src="https://trendshift.io/api/badge/repositories/15423" alt="GitHub Trending" height="80" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;p align="center"&gt; &lt;strong&gt;üöÄ Cowork with Your AI, Gemini CLI, Claude Code, Codex, Qwen Code, Goose CLI, Auggie, and more&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;User-friendly | Visual graphical interface | Multi-model support | Local data security&lt;/em&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/iOfficeAI/AionUi/releases"&gt; &lt;img src="https://img.shields.io/badge/‚¨áÔ∏è%20Download%20Now-Latest%20Release-32CD32?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white" alt="Download Latest Release" height="50" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;strong&gt;English&lt;/strong&gt; | &lt;a href="https://raw.githubusercontent.com/iOfficeAI/AionUi/main/readme_ch.md"&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/iOfficeAI/AionUi/main/readme_jp.md"&gt;Êó•Êú¨Ë™û&lt;/a&gt; | &lt;a href="https://www.aionui.com" target="_blank"&gt;Official Website&lt;/a&gt; | &lt;a href="https://twitter.com/AionUI" target="_blank"&gt;Twitter&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;strong&gt;üí¨ Community:&lt;/strong&gt; &lt;a href="https://discord.gg/g6u66vV9" target="_blank"&gt;Discord (English)&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/iOfficeAI/AionUi/main/resources/wechat.jpg" target="_blank"&gt;ÂæÆ‰ø° (‰∏≠ÊñáÁæ§)&lt;/a&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üìã Quick Navigation&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;/p&gt;
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/iOfficeAI/AionUi/main/#%E2%9C%A8-what-can-aionui-do"&gt;‚ú® What Can AionUi Do?&lt;/a&gt; ¬∑ &lt;a href="https://raw.githubusercontent.com/iOfficeAI/AionUi/main/#%F0%9F%A4%94-why-choose-aionui"&gt;ü§î Why Choose AionUi?&lt;/a&gt; ¬∑ &lt;a href="https://raw.githubusercontent.com/iOfficeAI/AionUi/main/#%E2%9C%A8-core-features"&gt;‚ú® Core Features&lt;/a&gt; ¬∑ &lt;a href="https://raw.githubusercontent.com/iOfficeAI/AionUi/main/#%F0%9F%9A%80-quick-start"&gt;üöÄ Quick Start&lt;/a&gt; ¬∑ &lt;a href="https://raw.githubusercontent.com/iOfficeAI/AionUi/main/#%F0%9F%93%96-detailed-usage-guide"&gt;üìñ Detailed Usage Guide&lt;/a&gt; ¬∑ &lt;a href="https://raw.githubusercontent.com/iOfficeAI/AionUi/main/#%F0%9F%A4%9D-community--support"&gt;üí¨ Community&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;‚ú® What Can AionUi Do?&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;img src="./resources/offica-ai BANNER-function copy.png" alt="AionUi - Cowork with Your CLI AI Agent" width="800" /&gt; &lt;/p&gt; 
&lt;h3&gt;ü§ñ &lt;strong&gt;Multi-Agent Mode - Cowork for Your Command-Line AI Tools, Unified Graphical Interface&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;em&gt;If you have installed command-line tools like Gemini CLI, Claude Code, CodeX, Qwen Code, Goose AI, Augment Code, AionUi can automatically detect them and provide a unified graphical interface&lt;/em&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;‚úÖ &lt;strong&gt;Auto Detection + Unified Interface&lt;/strong&gt; - Automatically recognizes local CLI tools, provides a unified graphical interface, say goodbye to command line&lt;/li&gt; 
 &lt;li&gt;‚úÖ &lt;strong&gt;Local Storage + Multi-Session&lt;/strong&gt; - Conversations saved locally, supports multiple parallel sessions, each session with independent context&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="center"&gt; &lt;img src="./resources/acp home page.gif" alt="Multi-Agent Mode Demo" width="800" /&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;üìÅ &lt;strong&gt;Smart File Management (AI Cowork)&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;em&gt;Batch renaming, automatic organization, smart classification, file merging&lt;/em&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Auto Organize&lt;/strong&gt;: Intelligently identify content and auto-classify, keeping folders tidy.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Efficient Batch&lt;/strong&gt;: One-click rename, merge files, say goodbye to tedious manual tasks.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="center"&gt; &lt;img src="./resources/aionui sort file.gif" alt="Smart File Management Demo" width="800" /&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;üìÑ &lt;strong&gt;Preview Panel - Quickly View AI-Generated Results&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;em&gt;Supports 9+ formats of visual preview (PDF, Word, Excel, PPT, code, Markdown, images, HTML, Diff, etc.)&lt;/em&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;‚úÖ &lt;strong&gt;View Results Instantly&lt;/strong&gt; - After AI generates files, view preview immediately without switching apps&lt;/li&gt; 
 &lt;li&gt;‚úÖ &lt;strong&gt;Real-time Tracking + Editable&lt;/strong&gt; - Automatically tracks file changes, editor and preview sync intelligently; supports real-time editing of Markdown, code, HTML, WYSIWYG&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/iOfficeAI/AionUi/main/resources/preview.gif" alt="Preview Panel Demo" width="800" /&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;üé® &lt;strong&gt;AI Image Generation &amp;amp; Editing&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;em&gt;Intelligent image generation, editing, and recognition, powered by Gemini&lt;/em&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/iOfficeAI/AionUi/main/resources/Image_Generation.gif" alt="AI Image Generation Demo" width="800" /&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;üí¨ &lt;strong&gt;Multi-Task Parallel Processing&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;em&gt;Open multiple conversations, tasks don't get mixed up, independent memory, double efficiency&lt;/em&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/iOfficeAI/AionUi/main/resources/multichat-side-by-side.gif" alt="Conversation Management Demo" width="800" /&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;üåê &lt;strong&gt;Access Anywhere - WebUI Mode&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;em&gt;Remotely control your AI tools - Access AionUi from any device on the network! Securely control local Gemini CLI, Claude Code, Codex, and other tools, data never leaves your device&lt;/em&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Basic startup
AionUi --webui

# Remote access (accessible from other devices on the local network)
AionUi --webui --remote
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;üí° &lt;strong&gt;Need detailed configuration guide?&lt;/strong&gt; Check out the &lt;a href="https://github.com/iOfficeAI/AionUi/wiki/WebUI-Configuration-Guide"&gt;WebUI Configuration Tutorial&lt;/a&gt; - includes complete startup commands for all platforms&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p align="center"&gt; &lt;img src="./resources/webui banner.png" alt="WebUI Remote Access Demo" width="800" /&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ü§î Why Choose AionUi?&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Just like Claude Cowork makes Claude Code easier to use, AionUi is the Cowork platform for all your command-line AI tools&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Gemini CLI, Claude Code, Codex, Qwen Code are powerful, but share common pain points: conversations can't be saved, single-session limitations, cumbersome file operations, and only support a single model.&lt;/p&gt; 
&lt;p&gt;AionUi provides unified &lt;strong&gt;Cowork capabilities&lt;/strong&gt; for these command-line tools:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üéØ &lt;strong&gt;Unified Platform&lt;/strong&gt; - One interface to manage all command-line AI tools, no switching needed&lt;/li&gt; 
 &lt;li&gt;üöÄ &lt;strong&gt;Multi-Tool Support&lt;/strong&gt; - Not only supports Claude Code, but also Gemini CLI, Codex, Qwen Code, and more&lt;/li&gt; 
 &lt;li&gt;üåê &lt;strong&gt;Cross-Platform&lt;/strong&gt; - Full platform support for macOS, Windows, Linux (Claude Cowork currently only macOS)&lt;/li&gt; 
 &lt;li&gt;üîÑ &lt;strong&gt;Multi-Model Switching&lt;/strong&gt; - Flexibly switch between different models in the same interface, meeting different task requirements&lt;/li&gt; 
 &lt;li&gt;üìÑ &lt;strong&gt;Real-time Preview&lt;/strong&gt; - Visual preview for 9+ formats, immediately view the effects of AI-generated files&lt;/li&gt; 
 &lt;li&gt;üíæ &lt;strong&gt;Local Data Security&lt;/strong&gt; - All conversations and files saved locally, data never leaves your device&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h3&gt;‚ùì Quick Q&amp;amp;A&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Q: Why is AionUi a great replacement for Claude Cowork?&lt;/strong&gt;&lt;/summary&gt; A: AionUi is a **free and open-source** **Multi-AI Agent Desktop**. Compared to the official Cowork which only runs on macOS and is locked to Claude, AionUi is its **full-model, cross-platform enhanced version**, deeply covering **AI Office Automation** scenarios. 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="left"&gt;Dimension&lt;/th&gt; 
    &lt;th align="left"&gt;Claude Cowork&lt;/th&gt; 
    &lt;th align="left"&gt;AionUi (This Project)&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;OS&lt;/td&gt; 
    &lt;td align="left"&gt;macOS Only&lt;/td&gt; 
    &lt;td align="left"&gt;üçè macOS / ü™ü Windows / üêß Linux&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;Model Support&lt;/td&gt; 
    &lt;td align="left"&gt;Claude Only&lt;/td&gt; 
    &lt;td align="left"&gt;ü§ñ Gemini, Claude, DeepSeek, OpenAI, Ollama&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;Interaction&lt;/td&gt; 
    &lt;td align="left"&gt;GUI&lt;/td&gt; 
    &lt;td align="left"&gt;üñ•Ô∏è Full GUI + WebUI Remote Access&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;Cost&lt;/td&gt; 
    &lt;td align="left"&gt;Subscription $100/mo&lt;/td&gt; 
    &lt;td align="left"&gt;üÜì Completely Free &amp;amp; Open Source&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;p&gt;&lt;strong&gt;Deep AI Office Scenario Support:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;File Management&lt;/strong&gt;: Intelligently organize messy local folders and batch rename with one click.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Data Processing&lt;/strong&gt;: Deeply analyze and automatically beautify Excel reports.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Document Generation&lt;/strong&gt;: Automatically write and format PPT, Word, and Markdown documents.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Instant Preview&lt;/strong&gt;: Built-in 9+ format preview panels, making AI office collaboration results instantly visible.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Q: What can I do with AionUi?&lt;/strong&gt;&lt;/summary&gt; A: It can be your **private Cowork workspace**. You can let it help you batch organize folders, deeply beautify Excel, and preview web code in real-time. It's your best graphical choice for exploring office automation workflows and enhancing your experience with Claude Code or Gemini CLI. 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Q: Is AionUi ready to use out of the box?&lt;/strong&gt;&lt;/summary&gt; A: Yes! After installation, you can directly use Google account login, AionUi will automatically associate with Gemini CLI, no additional configuration needed to start using. 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Q: Is it free?&lt;/strong&gt;&lt;/summary&gt; A: AionUi is completely free and open source, but using AI models requires corresponding API Keys. 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Q: Which AI models are supported?&lt;/strong&gt;&lt;/summary&gt; A: Supports mainstream models like Gemini, OpenAI, Claude, Qwen, as well as local models like Ollama, LM Studio. 
 &lt;p&gt;You can also run multiple AI Agents simultaneously (such as Gemini CLI, Claude Code, Qwen Code, etc.), see the configuration guide for details.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Q: Is my data secure?&lt;/strong&gt;&lt;/summary&gt; A: All conversation data is stored in a local SQLite database and will not be uploaded to any server. 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h2&gt;‚ú® Core Features&lt;/h2&gt; 
&lt;h3&gt;üí¨ &lt;strong&gt;Multi-Session Chat&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-Session + Independent Context&lt;/strong&gt; - Open multiple chats simultaneously, each session has independent context memory, no confusion&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Local Storage&lt;/strong&gt; - All conversations are saved locally and will not be lost&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ü§ñ &lt;strong&gt;Multi-Model Support&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-Platform Support&lt;/strong&gt; - Supports mainstream models like Gemini, OpenAI, Claude, Qwen, flexible switching&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Local Model Support&lt;/strong&gt; - Supports local model deployment like Ollama, LM Studio, select Custom platform and set local API address (e.g., &lt;code&gt;http://localhost:11434/v1&lt;/code&gt;) to connect&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Gemini 3 Subscription Optimization&lt;/strong&gt; - Automatically identifies subscribed users, recommends advanced models&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üóÇÔ∏è &lt;strong&gt;File Management&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;File Tree Browsing + Drag &amp;amp; Drop Upload&lt;/strong&gt; - Browse files like folders, support drag and drop files or folders for one-click import&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Smart Organization&lt;/strong&gt; - You can let AI help organize folders, automatic classification&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üìÑ &lt;strong&gt;Preview Panel - Give AI Agent a Display&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;9+ Format Preview&lt;/strong&gt; - Supports PDF, Word, Excel, PPT, code, Markdown, images, etc., view results immediately after AI generation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Real-time Tracking + Editable&lt;/strong&gt; - Automatically tracks file changes, supports real-time editing and debugging of Markdown, code, HTML&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üé® &lt;strong&gt;AI Image Generation &amp;amp; Editing&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Intelligent Image Generation&lt;/strong&gt; - Supports multiple image generation models like Gemini 2.5 Flash Image Preview, Nano, Banana&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Image Recognition &amp;amp; Editing&lt;/strong&gt; - AI-driven image analysis and editing features&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üåê &lt;strong&gt;WebUI Remote Access&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Cross-Device Access&lt;/strong&gt; - Access from any device on the network via browser, supports mobile devices&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Local Data Security&lt;/strong&gt; - All data stored locally in SQLite database, suitable for server deployment&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üé® &lt;strong&gt;Personalized Interface Customization&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;em&gt;Customize with your own CSS code, make your interface match your preferences&lt;/em&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="./resources/css with skin.gif" alt="CSS Custom Interface Demo" width="800" /&gt; &lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Fully Customizable&lt;/strong&gt; - Freely customize interface colors, styles, layout through CSS code, create your exclusive experience&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üìñ Detailed Usage Guide&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìñ Expand to View Complete Usage Guide&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h3&gt;üöÄ Quick Start&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://github.com/iOfficeAI/AionUi/wiki/Getting-Started"&gt;üìñ Complete Installation Guide&lt;/a&gt; - Detailed steps from download to configuration&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/iOfficeAI/AionUi/wiki/LLM-Configuration"&gt;‚öôÔ∏è LLM Configuration Guide&lt;/a&gt; - Multi-platform AI model configuration&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/iOfficeAI/AionUi/wiki/ACP-Setup"&gt;ü§ñ Multi-Agent Mode Setup&lt;/a&gt; - Integrate terminal AI agents&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/iOfficeAI/AionUi/wiki/MCP-Configuration-Guide"&gt;üîå MCP Tool Configuration&lt;/a&gt; - Model Context Protocol server setup&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/iOfficeAI/AionUi/wiki/AionUi-Image-Generation-Tool-Model-Configuration-Guide"&gt;üé® Image Generation Configuration&lt;/a&gt; - AI image generation setup tutorial&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/iOfficeAI/AionUi/wiki/WebUI-Configuration-Guide"&gt;üåê WebUI Configuration Guide&lt;/a&gt; - Complete WebUI setup and configuration tutorial&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;üéØ Use Cases&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://github.com/iOfficeAI/AionUi/wiki/file-management"&gt;üìÅ File Management&lt;/a&gt; - Smart file organization&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/iOfficeAI/AionUi/wiki/excel-processing"&gt;üìä Excel Processing&lt;/a&gt; - AI-driven data processing&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/iOfficeAI/AionUi/wiki/AionUi-Image-Generation-Tool-Model-Configuration-Guide"&gt;üé® Image Generation&lt;/a&gt; - AI image creation&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/iOfficeAI/AionUi/wiki/Use-Cases-Overview"&gt;üìö More Use Cases&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;‚ùì Support &amp;amp; Help&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://github.com/iOfficeAI/AionUi/wiki/FAQ"&gt;‚ùì FAQ&lt;/a&gt; - Questions and troubleshooting&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/iOfficeAI/AionUi/wiki/Configuration-Guides"&gt;üîß Configuration &amp;amp; Usage Tutorials&lt;/a&gt; - Complete configuration documentation&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üöÄ Quick Start&lt;/h2&gt; 
&lt;h3&gt;üíª System Requirements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;macOS&lt;/strong&gt;: 10.15 or higher&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt;: Windows 10 or higher&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Linux&lt;/strong&gt;: Ubuntu 18.04+ / Debian 10+ / Fedora 32+&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Memory&lt;/strong&gt;: Recommended 4GB or more&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Storage&lt;/strong&gt;: At least 500MB available space&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üì• Download&lt;/h3&gt; 
&lt;p&gt; &lt;a href="https://github.com/iOfficeAI/AionUi/releases"&gt; &lt;img src="https://img.shields.io/badge/Download-Latest%20Release-32CD32?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white" alt="Download Latest Release" height="50" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h3&gt;üîß Simple Installation&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Download and install&lt;/strong&gt; AionUi application&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Configure AI service&lt;/strong&gt; - Support Google account login or API Key authentication&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Start using&lt;/strong&gt; - Immediately experience modern AI chat interface&lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;üí° &lt;strong&gt;Need detailed configuration guide?&lt;/strong&gt; Check out our &lt;a href="https://github.com/iOfficeAI/AionUi/wiki/Getting-Started"&gt;Complete Installation Tutorial&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ü§ù Community &amp;amp; Support&lt;/h2&gt; 
&lt;h3&gt;üí¨ Community&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;üí° Your ideas matter!&lt;/strong&gt; We highly value every user's suggestions and feedback. Whether it's feature ideas, user experience, or issues you encounter, feel free to contact us anytime!&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://x.com/AionUi" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/iOfficeAI/AionUi/main/resources/contactus-x.png" alt="Contact Us on X" width="600" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/iOfficeAI/AionUi/discussions"&gt;üí¨ GitHub Discussions&lt;/a&gt; - &lt;strong&gt;Share ideas, make suggestions, exchange usage tips&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/iOfficeAI/AionUi/issues"&gt;üêõ Report Issues&lt;/a&gt; - Report bugs or feature requests&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/iOfficeAI/AionUi/releases"&gt;üì¶ Release Updates&lt;/a&gt; - Get the latest version&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://discord.gg/g6u66vV9"&gt;üí¨ Discord Community&lt;/a&gt; - &lt;strong&gt;Join our English community on Discord&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/iOfficeAI/AionUi/main/resources/wechat.jpg"&gt;üí¨ ÂæÆ‰ø° (‰∏≠ÊñáÁæ§)&lt;/a&gt; - &lt;strong&gt;Click to view QR code&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ü§ù Contributing&lt;/h3&gt; 
&lt;p&gt;Welcome to submit Issues and Pull Requests!&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Fork this project&lt;/li&gt; 
 &lt;li&gt;Create a feature branch (&lt;code&gt;git checkout -b feature/AmazingFeature&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Commit your changes (&lt;code&gt;git commit -m 'Add some AmazingFeature'&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Push to the branch (&lt;code&gt;git push origin feature/AmazingFeature&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Open a Pull Request&lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üìÑ License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under &lt;a href="https://raw.githubusercontent.com/iOfficeAI/AionUi/main/LICENSE"&gt;Apache-2.0&lt;/a&gt;.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üë• Contributors&lt;/h2&gt; 
&lt;p&gt;Thanks to all developers who have contributed to AionUi!&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/iOfficeAI/AionUi/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=iOfficeAI/AionUi&amp;amp;max=20" alt="Contributors" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;üìä Star History&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;a href="https://www.star-history.com/#iOfficeAI/aionui&amp;amp;Date" target="_blank"&gt; &lt;img src="https://api.star-history.com/svg?repos=iOfficeAI/aionui&amp;amp;type=Date" alt="GitHub Star Trends" width="600" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;strong&gt;‚≠ê If you like it, give us a star&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/iOfficeAI/AionUi/issues"&gt;Report Bug&lt;/a&gt; ¬∑ &lt;a href="https://github.com/iOfficeAI/AionUi/issues"&gt;Request Feature&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>agentsmd/agents.md</title>
      <link>https://github.com/agentsmd/agents.md</link>
      <description>&lt;p&gt;AGENTS.md ‚Äî a simple, open format for guiding coding agents&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AGENTS.md&lt;/h1&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/agentsmd/agents.md/main/public/og.png" alt="AGENTS.md logo" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://agents.md"&gt;AGENTS.md&lt;/a&gt; is a simple, open format for guiding coding agents.&lt;/p&gt; 
&lt;p&gt;Think of AGENTS.md as a README for agents: a dedicated, predictable place to provide context and instructions to help AI coding agents work on your project.&lt;/p&gt; 
&lt;p&gt;Below is a minimal example of an AGENTS.md file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-markdown"&gt;# Sample AGENTS.md file

## Dev environment tips
- Use `pnpm dlx turbo run where &amp;lt;project_name&amp;gt;` to jump to a package instead of scanning with `ls`.
- Run `pnpm install --filter &amp;lt;project_name&amp;gt;` to add the package to your workspace so Vite, ESLint, and TypeScript can see it.
- Use `pnpm create vite@latest &amp;lt;project_name&amp;gt; -- --template react-ts` to spin up a new React + Vite package with TypeScript checks ready.
- Check the name field inside each package's package.json to confirm the right name‚Äîskip the top-level one.

## Testing instructions
- Find the CI plan in the .github/workflows folder.
- Run `pnpm turbo run test --filter &amp;lt;project_name&amp;gt;` to run every check defined for that package.
- From the package root you can just call `pnpm test`. The commit should pass all tests before you merge.
- To focus on one step, add the Vitest pattern: `pnpm vitest run -t "&amp;lt;test name&amp;gt;"`.
- Fix any test or type errors until the whole suite is green.
- After moving files or changing imports, run `pnpm lint --filter &amp;lt;project_name&amp;gt;` to be sure ESLint and TypeScript rules still pass.
- Add or update tests for the code you change, even if nobody asked.

## PR instructions
- Title format: [&amp;lt;project_name&amp;gt;] &amp;lt;Title&amp;gt;
- Always run `pnpm lint` and `pnpm test` before committing.
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Website&lt;/h2&gt; 
&lt;p&gt;This repository also includes a basic Next.js website hosted at &lt;a href="https://agents.md/"&gt;https://agents.md/&lt;/a&gt; that explains the project‚Äôs goals in a simple way, and featuring some examples.&lt;/p&gt; 
&lt;h3&gt;Running the app locally&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install dependencies: &lt;pre&gt;&lt;code class="language-bash"&gt;pnpm install
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Start the development server: &lt;pre&gt;&lt;code class="language-bash"&gt;pnpm run dev
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Open your browser and go to &lt;a href="http://localhost:3000"&gt;http://localhost:3000&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt;</description>
    </item>
    
  </channel>
</rss>