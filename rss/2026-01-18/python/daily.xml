<rss version="2.0">
  <channel>
    <title>GitHub Python Daily Trending</title>
    <description>Daily Trending of Python in GitHub</description>
    <pubDate>Sat, 17 Jan 2026 01:37:01 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>davila7/claude-code-templates</title>
      <link>https://github.com/davila7/claude-code-templates</link>
      <description>&lt;p&gt;CLI tool for configuring and monitoring Claude Code&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href="https://www.npmjs.com/package/claude-code-templates"&gt;&lt;img src="https://img.shields.io/npm/v/claude-code-templates.svg?sanitize=true" alt="npm version" /&gt;&lt;/a&gt; &lt;a href="https://www.npmjs.com/package/claude-code-templates"&gt;&lt;img src="https://img.shields.io/npm/dt/claude-code-templates.svg?sanitize=true" alt="npm downloads" /&gt;&lt;/a&gt; &lt;a href="https://opensource.org/licenses/MIT"&gt;&lt;img src="https://img.shields.io/badge/License-MIT-yellow.svg?sanitize=true" alt="License: MIT" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/davila7/claude-code-templates/main/CONTRIBUTING.md"&gt;&lt;img src="https://img.shields.io/badge/PRs-welcome-brightgreen.svg?sanitize=true" alt="PRs Welcome" /&gt;&lt;/a&gt; &lt;a href="https://z.ai/subscribe?ic=8JVLJQFSKB&amp;amp;utm_source=github&amp;amp;utm_medium=badge&amp;amp;utm_campaign=readme"&gt;&lt;img src="https://img.shields.io/badge/Sponsored%20by-Z.AI-2563eb?style=flat&amp;amp;logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggZD0iTTEyIDJMMiAyMkgyMkwxMiAyWiIgZmlsbD0id2hpdGUiLz4KPC9zdmc+" alt="Sponsored by Z.AI" /&gt;&lt;/a&gt; &lt;a href="https://buymeacoffee.com/daniavila"&gt;&lt;img src="https://img.shields.io/badge/Buy%20Me%20A%20Coffee-support-yellow?style=flat&amp;amp;logo=buy-me-a-coffee" alt="Buy Me A Coffee" /&gt;&lt;/a&gt; &lt;a href="https://github.com/davila7/claude-code-templates"&gt;&lt;img src="https://img.shields.io/github/stars/davila7/claude-code-templates.svg?style=social&amp;amp;label=Star" alt="GitHub stars" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/15113" target="_blank"&gt; &lt;img src="https://trendshift.io/api/badge/repositories/15113" alt="davila7%2Fclaude-code-templates | Trendshift" style="width: 200px; height: 40px;" width="125" height="40" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;br /&gt; &lt;a href="https://vercel.com/oss"&gt; &lt;img alt="Vercel OSS Program" src="https://vercel.com/oss/program-badge.svg?sanitize=true" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;h3&gt;ü§ù Partnership&lt;/h3&gt; 
 &lt;p&gt; &lt;strong&gt;This project is sponsored by &lt;a href="https://z.ai" target="_blank"&gt;Z.AI&lt;/a&gt;&lt;/strong&gt;&lt;br /&gt; Supporting Claude Code Templates with the &lt;strong&gt;GLM CODING PLAN&lt;/strong&gt; &lt;/p&gt; 
 &lt;p&gt; &lt;a href="https://z.ai/subscribe?ic=8JVLJQFSKB&amp;amp;utm_source=github&amp;amp;utm_medium=readme&amp;amp;utm_campaign=partnership" target="_blank"&gt; &lt;img src="https://img.shields.io/badge/Get%2010%25%20OFF-GLM%20Coding%20Plan-2563eb?style=for-the-badge" alt="GLM Coding Plan" /&gt; &lt;/a&gt; &lt;/p&gt; 
 &lt;p&gt; &lt;em&gt;Top-tier coding performance powered by GLM-4.6 ‚Ä¢ Starting at $3/month&lt;/em&gt;&lt;br /&gt; &lt;em&gt;Seamlessly integrates with Claude Code, Cursor, Cline &amp;amp; 10+ AI coding tools&lt;/em&gt; &lt;/p&gt; 
 &lt;p&gt; &lt;code&gt;npx claude-code-templates@latest --setting partnerships/glm-coding-plan --yes&lt;/code&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h1&gt;Claude Code Templates (&lt;a href="https://aitmpl.com"&gt;aitmpl.com&lt;/a&gt;)&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;Ready-to-use configurations for Anthropic's Claude Code.&lt;/strong&gt; A comprehensive collection of AI agents, custom commands, settings, hooks, external integrations (MCPs), and project templates to enhance your development workflow.&lt;/p&gt; 
&lt;h2&gt;Browse &amp;amp; Install Components and Templates&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://aitmpl.com"&gt;Browse All Templates&lt;/a&gt;&lt;/strong&gt; - Interactive web interface to explore and install 100+ agents, commands, settings, hooks, and MCPs.&lt;/p&gt; 
&lt;img width="1049" height="855" alt="Screenshot 2025-08-19 at 08 09 24" src="https://github.com/user-attachments/assets/e3617410-9b1c-4731-87b7-a3858800b737" /&gt; 
&lt;h2&gt;üöÄ Quick Installation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install a complete development stack
npx claude-code-templates@latest --agent development-team/frontend-developer --command testing/generate-tests --mcp development/github-integration --yes

# Browse and install interactively
npx claude-code-templates@latest

# Install specific components
npx claude-code-templates@latest --agent development-tools/code-reviewer --yes
npx claude-code-templates@latest --command performance/optimize-bundle --yes
npx claude-code-templates@latest --setting performance/mcp-timeouts --yes
npx claude-code-templates@latest --hook git/pre-commit-validation --yes
npx claude-code-templates@latest --mcp database/postgresql-integration --yes
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;What You Get&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Component&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Examples&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ü§ñ Agents&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;AI specialists for specific domains&lt;/td&gt; 
   &lt;td&gt;Security auditor, React performance optimizer, database architect&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;‚ö° Commands&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Custom slash commands&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;/generate-tests&lt;/code&gt;, &lt;code&gt;/optimize-bundle&lt;/code&gt;, &lt;code&gt;/check-security&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üîå MCPs&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;External service integrations&lt;/td&gt; 
   &lt;td&gt;GitHub, PostgreSQL, Stripe, AWS, OpenAI&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;‚öôÔ∏è Settings&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Claude Code configurations&lt;/td&gt; 
   &lt;td&gt;Timeouts, memory settings, output styles&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ü™ù Hooks&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Automation triggers&lt;/td&gt; 
   &lt;td&gt;Pre-commit validation, post-completion actions&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üé® Skills&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Reusable capabilities with progressive disclosure&lt;/td&gt; 
   &lt;td&gt;PDF processing, Excel automation, custom workflows&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;üõ†Ô∏è Additional Tools&lt;/h2&gt; 
&lt;p&gt;Beyond the template catalog, Claude Code Templates includes powerful development tools:&lt;/p&gt; 
&lt;h3&gt;üìä Claude Code Analytics&lt;/h3&gt; 
&lt;p&gt;Monitor your AI-powered development sessions in real-time with live state detection and performance metrics.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npx claude-code-templates@latest --analytics
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;üí¨ Conversation Monitor&lt;/h3&gt; 
&lt;p&gt;Mobile-optimized interface to view Claude responses in real-time with secure remote access.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Local access
npx claude-code-templates@latest --chats

# Secure remote access via Cloudflare Tunnel
npx claude-code-templates@latest --chats --tunnel
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;üîç Health Check&lt;/h3&gt; 
&lt;p&gt;Comprehensive diagnostics to ensure your Claude Code installation is optimized.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npx claude-code-templates@latest --health-check
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;üîå Plugin Dashboard&lt;/h3&gt; 
&lt;p&gt;View marketplaces, installed plugins, and manage permissions from a unified interface.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npx claude-code-templates@latest --plugins
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;üìñ Documentation&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://docs.aitmpl.com/"&gt;üìö docs.aitmpl.com&lt;/a&gt;&lt;/strong&gt; - Complete guides, examples, and API reference for all components and tools.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions! &lt;strong&gt;&lt;a href="https://aitmpl.com"&gt;Browse existing templates&lt;/a&gt;&lt;/strong&gt; to see what's available, then check our &lt;a href="https://raw.githubusercontent.com/davila7/claude-code-templates/main/CONTRIBUTING.md"&gt;contributing guidelines&lt;/a&gt; to add your own agents, commands, MCPs, settings, or hooks.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Please read our &lt;a href="https://raw.githubusercontent.com/davila7/claude-code-templates/main/CODE_OF_CONDUCT.md"&gt;Code of Conduct&lt;/a&gt; before contributing.&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;Attribution&lt;/h2&gt; 
&lt;p&gt;This collection includes components from multiple sources:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Scientific Skills:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/K-Dense-AI/claude-scientific-skills"&gt;K-Dense-AI/claude-scientific-skills&lt;/a&gt;&lt;/strong&gt; by K-Dense Inc. - MIT License (139 scientific skills for biology, chemistry, medicine, and computational research)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Official Anthropic:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/anthropics/skills"&gt;anthropics/skills&lt;/a&gt;&lt;/strong&gt; - Official Anthropic skills (21 skills)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/anthropics/claude-code"&gt;anthropics/claude-code&lt;/a&gt;&lt;/strong&gt; - Development guides and examples (10 skills)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Community Skills &amp;amp; Agents:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/obra/superpowers"&gt;obra/superpowers&lt;/a&gt;&lt;/strong&gt; by Jesse Obra - MIT License (14 workflow skills)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/alirezarezvani/claude-skills"&gt;alirezarezvani/claude-skills&lt;/a&gt;&lt;/strong&gt; by Alireza Rezvani - MIT License (36 professional role skills)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/wshobson/agents"&gt;wshobson/agents&lt;/a&gt;&lt;/strong&gt; by wshobson - MIT License (48 agents)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;NerdyChefsAI Skills&lt;/strong&gt; - Community contribution - MIT License (specialized enterprise skills)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Commands &amp;amp; Tools:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/hesreallyhim/awesome-claude-code"&gt;awesome-claude-code&lt;/a&gt;&lt;/strong&gt; by hesreallyhim - CC0 1.0 Universal (21 commands)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/mehdi-lamrani/awesome-claude-skills"&gt;awesome-claude-skills&lt;/a&gt;&lt;/strong&gt; - Apache 2.0 (community skills)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;move-code-quality-skill&lt;/strong&gt; - MIT License&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;cocoindex-claude&lt;/strong&gt; - Apache 2.0&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Each of these resources retains its &lt;strong&gt;original license and attribution&lt;/strong&gt;, as defined by their respective authors. We respect and credit all original creators for their work and contributions to the Claude ecosystem.&lt;/p&gt; 
&lt;h2&gt;üìÑ License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the MIT License - see the &lt;a href="https://raw.githubusercontent.com/davila7/claude-code-templates/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;h2&gt;üîó Links&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;üåê Browse Templates&lt;/strong&gt;: &lt;a href="https://aitmpl.com"&gt;aitmpl.com&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üìö Documentation&lt;/strong&gt;: &lt;a href="https://docs.aitmpl.com"&gt;docs.aitmpl.com&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üí¨ Community&lt;/strong&gt;: &lt;a href="https://github.com/davila7/claude-code-templates/discussions"&gt;GitHub Discussions&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üêõ Issues&lt;/strong&gt;: &lt;a href="https://github.com/davila7/claude-code-templates/issues"&gt;GitHub Issues&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Stargazers over time&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://starchart.cc/davila7/claude-code-templates"&gt;&lt;img src="https://starchart.cc/davila7/claude-code-templates.svg?variant=adaptive" alt="Stargazers over time" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;strong&gt;‚≠ê Found this useful? Give us a star to support the project!&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://buymeacoffee.com/daniavila"&gt;&lt;img src="https://img.buymeacoffee.com/button-api/?text=Buy%20me%20a%20coffee&amp;amp;slug=daniavila&amp;amp;button_colour=FFDD00&amp;amp;font_colour=000000&amp;amp;font_family=Cookie&amp;amp;outline_colour=000000&amp;amp;coffee_colour=ffffff" alt="Buy Me A Coffee" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>google/langextract</title>
      <link>https://github.com/google/langextract</link>
      <description>&lt;p&gt;A Python library for extracting structured information from unstructured text using LLMs with precise source grounding and interactive visualization.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="https://github.com/google/langextract"&gt; &lt;img src="https://raw.githubusercontent.com/google/langextract/main/docs/_static/logo.svg?sanitize=true" alt="LangExtract Logo" width="128" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h1&gt;LangExtract&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://pypi.org/project/langextract/"&gt;&lt;img src="https://img.shields.io/pypi/v/langextract.svg?sanitize=true" alt="PyPI version" /&gt;&lt;/a&gt; &lt;a href="https://github.com/google/langextract"&gt;&lt;img src="https://img.shields.io/github/stars/google/langextract.svg?style=social&amp;amp;label=Star" alt="GitHub stars" /&gt;&lt;/a&gt; &lt;img src="https://github.com/google/langextract/actions/workflows/ci.yaml/badge.svg?sanitize=true" alt="Tests" /&gt; &lt;a href="https://doi.org/10.5281/zenodo.17015089"&gt;&lt;img src="https://zenodo.org/badge/DOI/10.5281/zenodo.17015089.svg?sanitize=true" alt="DOI" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#introduction"&gt;Introduction&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#why-langextract"&gt;Why LangExtract?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#quick-start"&gt;Quick Start&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#api-key-setup-for-cloud-models"&gt;API Key Setup for Cloud Models&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#adding-custom-model-providers"&gt;Adding Custom Model Providers&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#using-openai-models"&gt;Using OpenAI Models&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#using-local-llms-with-ollama"&gt;Using Local LLMs with Ollama&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#more-examples"&gt;More Examples&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#romeo-and-juliet-full-text-extraction"&gt;&lt;em&gt;Romeo and Juliet&lt;/em&gt; Full Text Extraction&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#medication-extraction"&gt;Medication Extraction&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#radiology-report-structuring-radextract"&gt;Radiology Report Structuring: RadExtract&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#community-providers"&gt;Community Providers&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#testing"&gt;Testing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#disclaimer"&gt;Disclaimer&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;LangExtract is a Python library that uses LLMs to extract structured information from unstructured text documents based on user-defined instructions. It processes materials such as clinical notes or reports, identifying and organizing key details while ensuring the extracted data corresponds to the source text.&lt;/p&gt; 
&lt;h2&gt;Why LangExtract?&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Precise Source Grounding:&lt;/strong&gt; Maps every extraction to its exact location in the source text, enabling visual highlighting for easy traceability and verification.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Reliable Structured Outputs:&lt;/strong&gt; Enforces a consistent output schema based on your few-shot examples, leveraging controlled generation in supported models like Gemini to guarantee robust, structured results.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Optimized for Long Documents:&lt;/strong&gt; Overcomes the "needle-in-a-haystack" challenge of large document extraction by using an optimized strategy of text chunking, parallel processing, and multiple passes for higher recall.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Interactive Visualization:&lt;/strong&gt; Instantly generates a self-contained, interactive HTML file to visualize and review thousands of extracted entities in their original context.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Flexible LLM Support:&lt;/strong&gt; Supports your preferred models, from cloud-based LLMs like the Google Gemini family to local open-source models via the built-in Ollama interface.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Adaptable to Any Domain:&lt;/strong&gt; Define extraction tasks for any domain using just a few examples. LangExtract adapts to your needs without requiring any model fine-tuning.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Leverages LLM World Knowledge:&lt;/strong&gt; Utilize precise prompt wording and few-shot examples to influence how the extraction task may utilize LLM knowledge. The accuracy of any inferred information and its adherence to the task specification are contingent upon the selected LLM, the complexity of the task, the clarity of the prompt instructions, and the nature of the prompt examples.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Using cloud-hosted models like Gemini requires an API key. See the &lt;a href="https://raw.githubusercontent.com/google/langextract/main/#api-key-setup-for-cloud-models"&gt;API Key Setup&lt;/a&gt; section for instructions on how to get and configure your key.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Extract structured information with just a few lines of code.&lt;/p&gt; 
&lt;h3&gt;1. Define Your Extraction Task&lt;/h3&gt; 
&lt;p&gt;First, create a prompt that clearly describes what you want to extract. Then, provide a high-quality example to guide the model.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import langextract as lx
import textwrap

# 1. Define the prompt and extraction rules
prompt = textwrap.dedent("""\
    Extract characters, emotions, and relationships in order of appearance.
    Use exact text for extractions. Do not paraphrase or overlap entities.
    Provide meaningful attributes for each entity to add context.""")

# 2. Provide a high-quality example to guide the model
examples = [
    lx.data.ExampleData(
        text="ROMEO. But soft! What light through yonder window breaks? It is the east, and Juliet is the sun.",
        extractions=[
            lx.data.Extraction(
                extraction_class="character",
                extraction_text="ROMEO",
                attributes={"emotional_state": "wonder"}
            ),
            lx.data.Extraction(
                extraction_class="emotion",
                extraction_text="But soft!",
                attributes={"feeling": "gentle awe"}
            ),
            lx.data.Extraction(
                extraction_class="relationship",
                extraction_text="Juliet is the sun",
                attributes={"type": "metaphor"}
            ),
        ]
    )
]
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Examples drive model behavior. Each &lt;code&gt;extraction_text&lt;/code&gt; should ideally be verbatim from the example's &lt;code&gt;text&lt;/code&gt; (no paraphrasing), listed in order of appearance. LangExtract raises &lt;code&gt;Prompt alignment&lt;/code&gt; warnings by default if examples don't follow this pattern‚Äîresolve these for best results.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;2. Run the Extraction&lt;/h3&gt; 
&lt;p&gt;Provide your input text and the prompt materials to the &lt;code&gt;lx.extract&lt;/code&gt; function.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# The input text to be processed
input_text = "Lady Juliet gazed longingly at the stars, her heart aching for Romeo"

# Run the extraction
result = lx.extract(
    text_or_documents=input_text,
    prompt_description=prompt,
    examples=examples,
    model_id="gemini-2.5-flash",
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Model Selection&lt;/strong&gt;: &lt;code&gt;gemini-2.5-flash&lt;/code&gt; is the recommended default, offering an excellent balance of speed, cost, and quality. For highly complex tasks requiring deeper reasoning, &lt;code&gt;gemini-2.5-pro&lt;/code&gt; may provide superior results. For large-scale or production use, a Tier 2 Gemini quota is suggested to increase throughput and avoid rate limits. See the &lt;a href="https://ai.google.dev/gemini-api/docs/rate-limits#tier-2"&gt;rate-limit documentation&lt;/a&gt; for details.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Model Lifecycle&lt;/strong&gt;: Note that Gemini models have a lifecycle with defined retirement dates. Users should consult the &lt;a href="https://cloud.google.com/vertex-ai/generative-ai/docs/learn/model-versions"&gt;official model version documentation&lt;/a&gt; to stay informed about the latest stable and legacy versions.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;3. Visualize the Results&lt;/h3&gt; 
&lt;p&gt;The extractions can be saved to a &lt;code&gt;.jsonl&lt;/code&gt; file, a popular format for working with language model data. LangExtract can then generate an interactive HTML visualization from this file to review the entities in context.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Save the results to a JSONL file
lx.io.save_annotated_documents([result], output_name="extraction_results.jsonl", output_dir=".")

# Generate the visualization from the file
html_content = lx.visualize("extraction_results.jsonl")
with open("visualization.html", "w") as f:
    if hasattr(html_content, 'data'):
        f.write(html_content.data)  # For Jupyter/Colab
    else:
        f.write(html_content)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This creates an animated and interactive HTML file:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/google/langextract/main/docs/_static/romeo_juliet_basic.gif" alt="Romeo and Juliet Basic Visualization " /&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note on LLM Knowledge Utilization:&lt;/strong&gt; This example demonstrates extractions that stay close to the text evidence - extracting "longing" for Lady Juliet's emotional state and identifying "yearning" from "gazed longingly at the stars." The task could be modified to generate attributes that draw more heavily from the LLM's world knowledge (e.g., adding &lt;code&gt;"identity": "Capulet family daughter"&lt;/code&gt; or &lt;code&gt;"literary_context": "tragic heroine"&lt;/code&gt;). The balance between text-evidence and knowledge-inference is controlled by your prompt instructions and example attributes.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Scaling to Longer Documents&lt;/h3&gt; 
&lt;p&gt;For larger texts, you can process entire documents directly from URLs with parallel processing and enhanced sensitivity:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Process Romeo &amp;amp; Juliet directly from Project Gutenberg
result = lx.extract(
    text_or_documents="https://www.gutenberg.org/files/1513/1513-0.txt",
    prompt_description=prompt,
    examples=examples,
    model_id="gemini-2.5-flash",
    extraction_passes=3,    # Improves recall through multiple passes
    max_workers=20,         # Parallel processing for speed
    max_char_buffer=1000    # Smaller contexts for better accuracy
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This approach can extract hundreds of entities from full novels while maintaining high accuracy. The interactive visualization seamlessly handles large result sets, making it easy to explore hundreds of entities from the output JSONL file. &lt;strong&gt;&lt;a href="https://github.com/google/langextract/raw/main/docs/examples/longer_text_example.md"&gt;See the full &lt;em&gt;Romeo and Juliet&lt;/em&gt; extraction example ‚Üí&lt;/a&gt;&lt;/strong&gt; for detailed results and performance insights.&lt;/p&gt; 
&lt;h3&gt;Vertex AI Batch Processing&lt;/h3&gt; 
&lt;p&gt;Save costs on large-scale tasks by enabling Vertex AI Batch API: &lt;code&gt;language_model_params={"vertexai": True, "batch": {"enabled": True}}&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;See an example of the Vertex AI Batch API usage in &lt;a href="https://raw.githubusercontent.com/google/langextract/main/docs/examples/batch_api_example.md"&gt;this example&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;From PyPI&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install langextract
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Recommended for most users. For isolated environments, consider using a virtual environment:&lt;/em&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m venv langextract_env
source langextract_env/bin/activate  # On Windows: langextract_env\Scripts\activate
pip install langextract
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;From Source&lt;/h3&gt; 
&lt;p&gt;LangExtract uses modern Python packaging with &lt;code&gt;pyproject.toml&lt;/code&gt; for dependency management:&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Installing with &lt;code&gt;-e&lt;/code&gt; puts the package in development mode, allowing you to modify the code without reinstalling.&lt;/em&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/google/langextract.git
cd langextract

# For basic installation:
pip install -e .

# For development (includes linting tools):
pip install -e ".[dev]"

# For testing (includes pytest):
pip install -e ".[test]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Docker&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker build -t langextract .
docker run --rm -e LANGEXTRACT_API_KEY="your-api-key" langextract python your_script.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;API Key Setup for Cloud Models&lt;/h2&gt; 
&lt;p&gt;When using LangExtract with cloud-hosted models (like Gemini or OpenAI), you'll need to set up an API key. On-device models don't require an API key. For developers using local LLMs, LangExtract offers built-in support for Ollama and can be extended to other third-party APIs by updating the inference endpoints.&lt;/p&gt; 
&lt;h3&gt;API Key Sources&lt;/h3&gt; 
&lt;p&gt;Get API keys from:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://aistudio.google.com/app/apikey"&gt;AI Studio&lt;/a&gt; for Gemini models&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://cloud.google.com/vertex-ai/generative-ai/docs/sdks/overview"&gt;Vertex AI&lt;/a&gt; for enterprise use&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://platform.openai.com/api-keys"&gt;OpenAI Platform&lt;/a&gt; for OpenAI models&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Setting up API key in your environment&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Option 1: Environment Variable&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export LANGEXTRACT_API_KEY="your-api-key-here"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Option 2: .env File (Recommended)&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Add your API key to a &lt;code&gt;.env&lt;/code&gt; file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Add API key to .env file
cat &amp;gt;&amp;gt; .env &amp;lt;&amp;lt; 'EOF'
LANGEXTRACT_API_KEY=your-api-key-here
EOF

# Keep your API key secure
echo '.env' &amp;gt;&amp;gt; .gitignore
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;In your Python code:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import langextract as lx

result = lx.extract(
    text_or_documents=input_text,
    prompt_description="Extract information...",
    examples=[...],
    model_id="gemini-2.5-flash"
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Option 3: Direct API Key (Not Recommended for Production)&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;You can also provide the API key directly in your code, though this is not recommended for production use:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;result = lx.extract(
    text_or_documents=input_text,
    prompt_description="Extract information...",
    examples=[...],
    model_id="gemini-2.5-flash",
    api_key="your-api-key-here"  # Only use this for testing/development
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Option 4: Vertex AI (Service Accounts)&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Use &lt;a href="https://cloud.google.com/vertex-ai/docs/start/introduction-unified-platform"&gt;Vertex AI&lt;/a&gt; for authentication with service accounts:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;result = lx.extract(
    text_or_documents=input_text,
    prompt_description="Extract information...",
    examples=[...],
    model_id="gemini-2.5-flash",
    language_model_params={
        "vertexai": True,
        "project": "your-project-id",
        "location": "global"  # or regional endpoint
    }
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Adding Custom Model Providers&lt;/h2&gt; 
&lt;p&gt;LangExtract supports custom LLM providers via a lightweight plugin system. You can add support for new models without changing core code.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Add new model support independently of the core library&lt;/li&gt; 
 &lt;li&gt;Distribute your provider as a separate Python package&lt;/li&gt; 
 &lt;li&gt;Keep custom dependencies isolated&lt;/li&gt; 
 &lt;li&gt;Override or extend built-in providers via priority-based resolution&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See the detailed guide in &lt;a href="https://raw.githubusercontent.com/google/langextract/main/langextract/providers/README.md"&gt;Provider System Documentation&lt;/a&gt; to learn how to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Register a provider with &lt;code&gt;@registry.register(...)&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Publish an entry point for discovery&lt;/li&gt; 
 &lt;li&gt;Optionally provide a schema with &lt;code&gt;get_schema_class()&lt;/code&gt; for structured output&lt;/li&gt; 
 &lt;li&gt;Integrate with the factory via &lt;code&gt;create_model(...)&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Using OpenAI Models&lt;/h2&gt; 
&lt;p&gt;LangExtract supports OpenAI models (requires optional dependency: &lt;code&gt;pip install langextract[openai]&lt;/code&gt;):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import langextract as lx

result = lx.extract(
    text_or_documents=input_text,
    prompt_description=prompt,
    examples=examples,
    model_id="gpt-4o",  # Automatically selects OpenAI provider
    api_key=os.environ.get('OPENAI_API_KEY'),
    fence_output=True,
    use_schema_constraints=False
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note: OpenAI models require &lt;code&gt;fence_output=True&lt;/code&gt; and &lt;code&gt;use_schema_constraints=False&lt;/code&gt; because LangExtract doesn't implement schema constraints for OpenAI yet.&lt;/p&gt; 
&lt;h2&gt;Using Local LLMs with Ollama&lt;/h2&gt; 
&lt;p&gt;LangExtract supports local inference using Ollama, allowing you to run models without API keys:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import langextract as lx

result = lx.extract(
    text_or_documents=input_text,
    prompt_description=prompt,
    examples=examples,
    model_id="gemma2:2b",  # Automatically selects Ollama provider
    model_url="http://localhost:11434",
    fence_output=False,
    use_schema_constraints=False
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Quick setup:&lt;/strong&gt; Install Ollama from &lt;a href="https://ollama.com/"&gt;ollama.com&lt;/a&gt;, run &lt;code&gt;ollama pull gemma2:2b&lt;/code&gt;, then &lt;code&gt;ollama serve&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;For detailed installation, Docker setup, and examples, see &lt;a href="https://raw.githubusercontent.com/google/langextract/main/examples/ollama/"&gt;&lt;code&gt;examples/ollama/&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;More Examples&lt;/h2&gt; 
&lt;p&gt;Additional examples of LangExtract in action:&lt;/p&gt; 
&lt;h3&gt;&lt;em&gt;Romeo and Juliet&lt;/em&gt; Full Text Extraction&lt;/h3&gt; 
&lt;p&gt;LangExtract can process complete documents directly from URLs. This example demonstrates extraction from the full text of &lt;em&gt;Romeo and Juliet&lt;/em&gt; from Project Gutenberg (147,843 characters), showing parallel processing, sequential extraction passes, and performance optimization for long document processing.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/google/langextract/raw/main/docs/examples/longer_text_example.md"&gt;View &lt;em&gt;Romeo and Juliet&lt;/em&gt; Full Text Example ‚Üí&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h3&gt;Medication Extraction&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Disclaimer:&lt;/strong&gt; This demonstration is for illustrative purposes of LangExtract's baseline capability only. It does not represent a finished or approved product, is not intended to diagnose or suggest treatment of any disease or condition, and should not be used for medical advice.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;LangExtract excels at extracting structured medical information from clinical text. These examples demonstrate both basic entity recognition (medication names, dosages, routes) and relationship extraction (connecting medications to their attributes), showing LangExtract's effectiveness for healthcare applications.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/google/langextract/raw/main/docs/examples/medication_examples.md"&gt;View Medication Examples ‚Üí&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h3&gt;Radiology Report Structuring: RadExtract&lt;/h3&gt; 
&lt;p&gt;Explore RadExtract, a live interactive demo on HuggingFace Spaces that shows how LangExtract can automatically structure radiology reports. Try it directly in your browser with no setup required.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://huggingface.co/spaces/google/radextract"&gt;View RadExtract Demo ‚Üí&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;Community Providers&lt;/h2&gt; 
&lt;p&gt;Extend LangExtract with custom model providers! Check out our &lt;a href="https://raw.githubusercontent.com/google/langextract/main/COMMUNITY_PROVIDERS.md"&gt;Community Provider Plugins&lt;/a&gt; registry to discover providers created by the community or add your own.&lt;/p&gt; 
&lt;p&gt;For detailed instructions on creating a provider plugin, see the &lt;a href="https://raw.githubusercontent.com/google/langextract/main/examples/custom_provider_plugin/"&gt;Custom Provider Plugin Example&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Contributions are welcome! See &lt;a href="https://github.com/google/langextract/raw/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; to get started with development, testing, and pull requests. You must sign a &lt;a href="https://cla.developers.google.com/about"&gt;Contributor License Agreement&lt;/a&gt; before submitting patches.&lt;/p&gt; 
&lt;h2&gt;Testing&lt;/h2&gt; 
&lt;p&gt;To run tests locally from the source:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Clone the repository
git clone https://github.com/google/langextract.git
cd langextract

# Install with test dependencies
pip install -e ".[test]"

# Run all tests
pytest tests
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or reproduce the full CI matrix locally with tox:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;tox  # runs pylint + pytest on Python 3.10 and 3.11
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Ollama Integration Testing&lt;/h3&gt; 
&lt;p&gt;If you have Ollama installed locally, you can run integration tests:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Test Ollama integration (requires Ollama running with gemma2:2b model)
tox -e ollama-integration
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This test will automatically detect if Ollama is available and run real inference tests.&lt;/p&gt; 
&lt;h2&gt;Development&lt;/h2&gt; 
&lt;h3&gt;Code Formatting&lt;/h3&gt; 
&lt;p&gt;This project uses automated formatting tools to maintain consistent code style:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Auto-format all code
./autoformat.sh

# Or run formatters separately
isort langextract tests --profile google --line-length 80
pyink langextract tests --config pyproject.toml
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Pre-commit Hooks&lt;/h3&gt; 
&lt;p&gt;For automatic formatting checks:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pre-commit install  # One-time setup
pre-commit run --all-files  # Manual run
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Linting&lt;/h3&gt; 
&lt;p&gt;Run linting before submitting PRs:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pylint --rcfile=.pylintrc langextract tests
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/google/langextract/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for full development guidelines.&lt;/p&gt; 
&lt;h2&gt;Disclaimer&lt;/h2&gt; 
&lt;p&gt;This is not an officially supported Google product. If you use LangExtract in production or publications, please cite accordingly and acknowledge usage. Use is subject to the &lt;a href="https://github.com/google/langextract/raw/main/LICENSE"&gt;Apache 2.0 License&lt;/a&gt;. For health-related applications, use of LangExtract is also subject to the &lt;a href="https://developers.google.com/health-ai-developer-foundations/terms"&gt;Health AI Developer Foundations Terms of Use&lt;/a&gt;.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;strong&gt;Happy Extracting!&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ultralytics/ultralytics</title>
      <link>https://github.com/ultralytics/ultralytics</link>
      <description>&lt;p&gt;Ultralytics YOLO üöÄ&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p&gt; &lt;a href="https://platform.ultralytics.com/ultralytics/yolo26" target="_blank"&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/ultralytics/assets/main/yolov8/banner-yolov8.png" alt="Ultralytics YOLO banner" /&gt;&lt;/a&gt; &lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://docs.ultralytics.com/zh/"&gt;‰∏≠Êñá&lt;/a&gt; | &lt;a href="https://docs.ultralytics.com/ko/"&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt; | &lt;a href="https://docs.ultralytics.com/ja/"&gt;Êó•Êú¨Ë™û&lt;/a&gt; | &lt;a href="https://docs.ultralytics.com/ru/"&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt; | &lt;a href="https://docs.ultralytics.com/de/"&gt;Deutsch&lt;/a&gt; | &lt;a href="https://docs.ultralytics.com/fr/"&gt;Fran√ßais&lt;/a&gt; | &lt;a href="https://docs.ultralytics.com/es"&gt;Espa√±ol&lt;/a&gt; | &lt;a href="https://docs.ultralytics.com/pt/"&gt;Portugu√™s&lt;/a&gt; | &lt;a href="https://docs.ultralytics.com/tr/"&gt;T√ºrk√ße&lt;/a&gt; | &lt;a href="https://docs.ultralytics.com/vi/"&gt;Ti·∫øng Vi·ªát&lt;/a&gt; | &lt;a href="https://docs.ultralytics.com/ar/"&gt;ÿßŸÑÿπÿ±ÿ®Ÿäÿ©&lt;/a&gt; &lt;br /&gt;&lt;/p&gt; 
 &lt;div&gt; 
  &lt;a href="https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml"&gt;&lt;img src="https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg?sanitize=true" alt="Ultralytics CI" /&gt;&lt;/a&gt; 
  &lt;a href="https://clickpy.clickhouse.com/dashboard/ultralytics"&gt;&lt;img src="https://static.pepy.tech/badge/ultralytics" alt="Ultralytics Downloads" /&gt;&lt;/a&gt; 
  &lt;a href="https://discord.com/invite/ultralytics"&gt;&lt;img alt="Ultralytics Discord" src="https://img.shields.io/discord/1089800235347353640?logo=discord&amp;amp;logoColor=white&amp;amp;label=Discord&amp;amp;color=blue" /&gt;&lt;/a&gt; 
  &lt;a href="https://community.ultralytics.com/"&gt;&lt;img alt="Ultralytics Forums" src="https://img.shields.io/discourse/users?server=https%3A%2F%2Fcommunity.ultralytics.com&amp;amp;logo=discourse&amp;amp;label=Forums&amp;amp;color=blue" /&gt;&lt;/a&gt; 
  &lt;a href="https://www.reddit.com/r/ultralytics/"&gt;&lt;img alt="Ultralytics Reddit" src="https://img.shields.io/reddit/subreddit-subscribers/ultralytics?style=flat&amp;amp;logo=reddit&amp;amp;logoColor=white&amp;amp;label=Reddit&amp;amp;color=blue" /&gt;&lt;/a&gt; 
  &lt;br /&gt; 
  &lt;a href="https://console.paperspace.com/github/ultralytics/ultralytics"&gt;&lt;img src="https://assets.paperspace.io/img/gradient-badge.svg?sanitize=true" alt="Run Ultralytics on Gradient" /&gt;&lt;/a&gt; 
  &lt;a href="https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open Ultralytics In Colab" /&gt;&lt;/a&gt; 
  &lt;a href="https://www.kaggle.com/models/ultralytics/yolo26"&gt;&lt;img src="https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true" alt="Open Ultralytics In Kaggle" /&gt;&lt;/a&gt; 
  &lt;a href="https://mybinder.org/v2/gh/ultralytics/ultralytics/HEAD?labpath=examples%2Ftutorial.ipynb"&gt;&lt;img src="https://mybinder.org/badge_logo.svg?sanitize=true" alt="Open Ultralytics In Binder" /&gt;&lt;/a&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;a href="https://www.ultralytics.com/"&gt;Ultralytics&lt;/a&gt; creates cutting-edge, state-of-the-art (SOTA) &lt;a href="https://www.ultralytics.com/yolo"&gt;YOLO models&lt;/a&gt; built on years of foundational research in computer vision and AI. Constantly updated for performance and flexibility, our models are &lt;strong&gt;fast&lt;/strong&gt;, &lt;strong&gt;accurate&lt;/strong&gt;, and &lt;strong&gt;easy to use&lt;/strong&gt;. They excel at &lt;a href="https://docs.ultralytics.com/tasks/detect/"&gt;object detection&lt;/a&gt;, &lt;a href="https://docs.ultralytics.com/modes/track/"&gt;tracking&lt;/a&gt;, &lt;a href="https://docs.ultralytics.com/tasks/segment/"&gt;instance segmentation&lt;/a&gt;, &lt;a href="https://docs.ultralytics.com/tasks/classify/"&gt;image classification&lt;/a&gt;, and &lt;a href="https://docs.ultralytics.com/tasks/pose/"&gt;pose estimation&lt;/a&gt; tasks.&lt;/p&gt; 
&lt;p&gt;Find detailed documentation in the &lt;a href="https://docs.ultralytics.com/"&gt;Ultralytics Docs&lt;/a&gt;. Get support via &lt;a href="https://github.com/ultralytics/ultralytics/issues/new/choose"&gt;GitHub Issues&lt;/a&gt;. Join discussions on &lt;a href="https://discord.com/invite/ultralytics"&gt;Discord&lt;/a&gt;, &lt;a href="https://www.reddit.com/r/ultralytics/"&gt;Reddit&lt;/a&gt;, and the &lt;a href="https://community.ultralytics.com/"&gt;Ultralytics Community Forums&lt;/a&gt;!&lt;/p&gt; 
&lt;p&gt;Request an Enterprise License for commercial use at &lt;a href="https://www.ultralytics.com/license"&gt;Ultralytics Licensing&lt;/a&gt;.&lt;/p&gt; 
&lt;a href="https://platform.ultralytics.com/ultralytics/yolo26" target="_blank"&gt; &lt;img width="100%" src="https://raw.githubusercontent.com/ultralytics/assets/refs/heads/main/yolo/performance-comparison.png" alt="YOLO26 performance plots" /&gt; &lt;/a&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/ultralytics"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-github.png" width="2%" alt="Ultralytics GitHub" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="2%" alt="space" /&gt; 
 &lt;a href="https://www.linkedin.com/company/ultralytics/"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-linkedin.png" width="2%" alt="Ultralytics LinkedIn" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="2%" alt="space" /&gt; 
 &lt;a href="https://twitter.com/ultralytics"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-twitter.png" width="2%" alt="Ultralytics Twitter" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="2%" alt="space" /&gt; 
 &lt;a href="https://www.youtube.com/ultralytics?sub_confirmation=1"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-youtube.png" width="2%" alt="Ultralytics YouTube" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="2%" alt="space" /&gt; 
 &lt;a href="https://www.tiktok.com/@ultralytics"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-tiktok.png" width="2%" alt="Ultralytics TikTok" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="2%" alt="space" /&gt; 
 &lt;a href="https://ultralytics.com/bilibili"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-bilibili.png" width="2%" alt="Ultralytics BiliBili" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="2%" alt="space" /&gt; 
 &lt;a href="https://discord.com/invite/ultralytics"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-discord.png" width="2%" alt="Ultralytics Discord" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;üìÑ Documentation&lt;/h2&gt; 
&lt;p&gt;See below for quickstart installation and usage examples. For comprehensive guidance on training, validation, prediction, and deployment, refer to our full &lt;a href="https://docs.ultralytics.com/"&gt;Ultralytics Docs&lt;/a&gt;.&lt;/p&gt; 
&lt;details open&gt; 
 &lt;summary&gt;Install&lt;/summary&gt; 
 &lt;p&gt;Install the &lt;code&gt;ultralytics&lt;/code&gt; package, including all &lt;a href="https://github.com/ultralytics/ultralytics/raw/main/pyproject.toml"&gt;requirements&lt;/a&gt;, in a &lt;a href="https://www.python.org/"&gt;&lt;strong&gt;Python&amp;gt;=3.8&lt;/strong&gt;&lt;/a&gt; environment with &lt;a href="https://pytorch.org/get-started/locally/"&gt;&lt;strong&gt;PyTorch&amp;gt;=1.8&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://pypi.org/project/ultralytics/"&gt;&lt;img src="https://img.shields.io/pypi/v/ultralytics?logo=pypi&amp;amp;logoColor=white" alt="PyPI - Version" /&gt;&lt;/a&gt; &lt;a href="https://clickpy.clickhouse.com/dashboard/ultralytics"&gt;&lt;img src="https://static.pepy.tech/badge/ultralytics" alt="Ultralytics Downloads" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/ultralytics/"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/ultralytics?logo=python&amp;amp;logoColor=gold" alt="PyPI - Python Version" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;pip install ultralytics
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;For alternative installation methods, including &lt;a href="https://anaconda.org/conda-forge/ultralytics"&gt;Conda&lt;/a&gt;, &lt;a href="https://hub.docker.com/r/ultralytics/ultralytics"&gt;Docker&lt;/a&gt;, and building from source via Git, please consult the &lt;a href="https://docs.ultralytics.com/quickstart/"&gt;Quickstart Guide&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://anaconda.org/conda-forge/ultralytics"&gt;&lt;img src="https://img.shields.io/conda/vn/conda-forge/ultralytics?logo=condaforge" alt="Conda Version" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/ultralytics/ultralytics"&gt;&lt;img src="https://img.shields.io/docker/v/ultralytics/ultralytics?sort=semver&amp;amp;logo=docker" alt="Docker Image Version" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/ultralytics/ultralytics"&gt;&lt;img src="https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker" alt="Ultralytics Docker Pulls" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details open&gt; 
 &lt;summary&gt;Usage&lt;/summary&gt; 
 &lt;h3&gt;CLI&lt;/h3&gt; 
 &lt;p&gt;You can use Ultralytics YOLO directly from the Command Line Interface (CLI) with the &lt;code&gt;yolo&lt;/code&gt; command:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Predict using a pretrained YOLO model (e.g., YOLO26n) on an image
yolo predict model=yolo26n.pt source='https://ultralytics.com/images/bus.jpg'
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;The &lt;code&gt;yolo&lt;/code&gt; command supports various tasks and modes, accepting additional arguments like &lt;code&gt;imgsz=640&lt;/code&gt;. Explore the YOLO &lt;a href="https://docs.ultralytics.com/usage/cli/"&gt;CLI Docs&lt;/a&gt; for more examples.&lt;/p&gt; 
 &lt;h3&gt;Python&lt;/h3&gt; 
 &lt;p&gt;Ultralytics YOLO can also be integrated directly into your Python projects. It accepts the same &lt;a href="https://docs.ultralytics.com/usage/cfg/"&gt;configuration arguments&lt;/a&gt; as the CLI:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from ultralytics import YOLO

# Load a pretrained YOLO26n model
model = YOLO("yolo26n.pt")

# Train the model on the COCO8 dataset for 100 epochs
train_results = model.train(
    data="coco8.yaml",  # Path to dataset configuration file
    epochs=100,  # Number of training epochs
    imgsz=640,  # Image size for training
    device="cpu",  # Device to run on (e.g., 'cpu', 0, [0,1,2,3])
)

# Evaluate the model's performance on the validation set
metrics = model.val()

# Perform object detection on an image
results = model("path/to/image.jpg")  # Predict on an image
results[0].show()  # Display results

# Export the model to ONNX format for deployment
path = model.export(format="onnx")  # Returns the path to the exported model
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Discover more examples in the YOLO &lt;a href="https://docs.ultralytics.com/usage/python/"&gt;Python Docs&lt;/a&gt;.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;‚ú® Models&lt;/h2&gt; 
&lt;p&gt;Ultralytics supports a wide range of YOLO models, from early versions like &lt;a href="https://docs.ultralytics.com/models/yolov3/"&gt;YOLOv3&lt;/a&gt; to the latest &lt;a href="https://docs.ultralytics.com/models/yolo26/"&gt;YOLO26&lt;/a&gt;. The tables below showcase YOLO26 models pretrained on the &lt;a href="https://docs.ultralytics.com/datasets/detect/coco/"&gt;COCO&lt;/a&gt; dataset for &lt;a href="https://docs.ultralytics.com/tasks/detect/"&gt;Detection&lt;/a&gt;, &lt;a href="https://docs.ultralytics.com/tasks/segment/"&gt;Segmentation&lt;/a&gt;, and &lt;a href="https://docs.ultralytics.com/tasks/pose/"&gt;Pose Estimation&lt;/a&gt;. Additionally, &lt;a href="https://docs.ultralytics.com/tasks/classify/"&gt;Classification&lt;/a&gt; models pretrained on the &lt;a href="https://docs.ultralytics.com/datasets/classify/imagenet/"&gt;ImageNet&lt;/a&gt; dataset are available. &lt;a href="https://docs.ultralytics.com/modes/track/"&gt;Tracking&lt;/a&gt; mode is compatible with all Detection, Segmentation, and Pose models. All &lt;a href="https://docs.ultralytics.com/models/"&gt;Models&lt;/a&gt; are automatically downloaded from the latest Ultralytics &lt;a href="https://github.com/ultralytics/assets/releases"&gt;release&lt;/a&gt; upon first use.&lt;/p&gt; 
&lt;a href="https://docs.ultralytics.com/tasks/" target="_blank"&gt; &lt;img width="100%" src="https://github.com/ultralytics/docs/releases/download/0/ultralytics-yolov8-tasks-banner.avif" alt="Ultralytics YOLO supported tasks" /&gt; &lt;/a&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;details open&gt;
 &lt;summary&gt;Detection (COCO)&lt;/summary&gt; 
 &lt;p&gt;Explore the &lt;a href="https://docs.ultralytics.com/tasks/detect/"&gt;Detection Docs&lt;/a&gt; for usage examples. These models are trained on the &lt;a href="https://cocodataset.org/"&gt;COCO dataset&lt;/a&gt;, featuring 80 object classes.&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model&lt;/th&gt; 
    &lt;th&gt;size&lt;br /&gt;&lt;sup&gt;(pixels)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;mAP&lt;sup&gt;val&lt;br /&gt;50-95&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;mAP&lt;sup&gt;val&lt;br /&gt;50-95(e2e)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;Speed&lt;br /&gt;&lt;sup&gt;CPU ONNX&lt;br /&gt;(ms)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;Speed&lt;br /&gt;&lt;sup&gt;T4 TensorRT10&lt;br /&gt;(ms)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;params&lt;br /&gt;&lt;sup&gt;(M)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;FLOPs&lt;br /&gt;&lt;sup&gt;(B)&lt;/sup&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26n.pt"&gt;YOLO26n&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;40.9&lt;/td&gt; 
    &lt;td&gt;40.1&lt;/td&gt; 
    &lt;td&gt;38.9 ¬± 0.7&lt;/td&gt; 
    &lt;td&gt;1.7 ¬± 0.0&lt;/td&gt; 
    &lt;td&gt;2.4&lt;/td&gt; 
    &lt;td&gt;5.4&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26s.pt"&gt;YOLO26s&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;48.6&lt;/td&gt; 
    &lt;td&gt;47.8&lt;/td&gt; 
    &lt;td&gt;87.2 ¬± 0.9&lt;/td&gt; 
    &lt;td&gt;2.5 ¬± 0.0&lt;/td&gt; 
    &lt;td&gt;9.5&lt;/td&gt; 
    &lt;td&gt;20.7&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26m.pt"&gt;YOLO26m&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;53.1&lt;/td&gt; 
    &lt;td&gt;52.5&lt;/td&gt; 
    &lt;td&gt;220.0 ¬± 1.4&lt;/td&gt; 
    &lt;td&gt;4.7 ¬± 0.1&lt;/td&gt; 
    &lt;td&gt;20.4&lt;/td&gt; 
    &lt;td&gt;68.2&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26l.pt"&gt;YOLO26l&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;55.0&lt;/td&gt; 
    &lt;td&gt;54.4&lt;/td&gt; 
    &lt;td&gt;286.2 ¬± 2.0&lt;/td&gt; 
    &lt;td&gt;6.2 ¬± 0.2&lt;/td&gt; 
    &lt;td&gt;24.8&lt;/td&gt; 
    &lt;td&gt;86.4&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26x.pt"&gt;YOLO26x&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;57.5&lt;/td&gt; 
    &lt;td&gt;56.9&lt;/td&gt; 
    &lt;td&gt;525.8 ¬± 4.0&lt;/td&gt; 
    &lt;td&gt;11.8 ¬± 0.2&lt;/td&gt; 
    &lt;td&gt;55.7&lt;/td&gt; 
    &lt;td&gt;193.9&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;mAP&lt;sup&gt;val&lt;/sup&gt;&lt;/strong&gt; values refer to single-model single-scale performance on the &lt;a href="https://cocodataset.org/"&gt;COCO val2017&lt;/a&gt; dataset. See &lt;a href="https://docs.ultralytics.com/guides/yolo-performance-metrics/"&gt;YOLO Performance Metrics&lt;/a&gt; for details. &lt;br /&gt;Reproduce with &lt;code&gt;yolo val detect data=coco.yaml device=0&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Speed&lt;/strong&gt; metrics are averaged over COCO val images using an &lt;a href="https://aws.amazon.com/ec2/instance-types/p4/"&gt;Amazon EC2 P4d&lt;/a&gt; instance. CPU speeds measured with &lt;a href="https://onnx.ai/"&gt;ONNX&lt;/a&gt; export. GPU speeds measured with &lt;a href="https://developer.nvidia.com/tensorrt"&gt;TensorRT&lt;/a&gt; export. &lt;br /&gt;Reproduce with &lt;code&gt;yolo val detect data=coco.yaml batch=1 device=0|cpu&lt;/code&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;Segmentation (COCO)&lt;/summary&gt; 
 &lt;p&gt;Refer to the &lt;a href="https://docs.ultralytics.com/tasks/segment/"&gt;Segmentation Docs&lt;/a&gt; for usage examples. These models are trained on &lt;a href="https://docs.ultralytics.com/datasets/segment/coco/"&gt;COCO-Seg&lt;/a&gt;, including 80 classes.&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model&lt;/th&gt; 
    &lt;th&gt;size&lt;br /&gt;&lt;sup&gt;(pixels)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;mAP&lt;sup&gt;box&lt;br /&gt;50-95(e2e)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;mAP&lt;sup&gt;mask&lt;br /&gt;50-95(e2e)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;Speed&lt;br /&gt;&lt;sup&gt;CPU ONNX&lt;br /&gt;(ms)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;Speed&lt;br /&gt;&lt;sup&gt;T4 TensorRT10&lt;br /&gt;(ms)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;params&lt;br /&gt;&lt;sup&gt;(M)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;FLOPs&lt;br /&gt;&lt;sup&gt;(B)&lt;/sup&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26n-seg.pt"&gt;YOLO26n-seg&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;39.6&lt;/td&gt; 
    &lt;td&gt;33.9&lt;/td&gt; 
    &lt;td&gt;53.3 ¬± 0.5&lt;/td&gt; 
    &lt;td&gt;2.1 ¬± 0.0&lt;/td&gt; 
    &lt;td&gt;2.7&lt;/td&gt; 
    &lt;td&gt;9.1&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26s-seg.pt"&gt;YOLO26s-seg&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;47.3&lt;/td&gt; 
    &lt;td&gt;40.0&lt;/td&gt; 
    &lt;td&gt;118.4 ¬± 0.9&lt;/td&gt; 
    &lt;td&gt;3.3 ¬± 0.0&lt;/td&gt; 
    &lt;td&gt;10.4&lt;/td&gt; 
    &lt;td&gt;34.2&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26m-seg.pt"&gt;YOLO26m-seg&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;52.5&lt;/td&gt; 
    &lt;td&gt;44.1&lt;/td&gt; 
    &lt;td&gt;328.2 ¬± 2.4&lt;/td&gt; 
    &lt;td&gt;6.7 ¬± 0.1&lt;/td&gt; 
    &lt;td&gt;23.6&lt;/td&gt; 
    &lt;td&gt;121.5&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26l-seg.pt"&gt;YOLO26l-seg&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;54.4&lt;/td&gt; 
    &lt;td&gt;45.5&lt;/td&gt; 
    &lt;td&gt;387.0 ¬± 3.7&lt;/td&gt; 
    &lt;td&gt;8.0 ¬± 0.1&lt;/td&gt; 
    &lt;td&gt;28.0&lt;/td&gt; 
    &lt;td&gt;139.8&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26x-seg.pt"&gt;YOLO26x-seg&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;56.5&lt;/td&gt; 
    &lt;td&gt;47.0&lt;/td&gt; 
    &lt;td&gt;787.0 ¬± 6.8&lt;/td&gt; 
    &lt;td&gt;16.4 ¬± 0.1&lt;/td&gt; 
    &lt;td&gt;62.8&lt;/td&gt; 
    &lt;td&gt;313.5&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;mAP&lt;sup&gt;val&lt;/sup&gt;&lt;/strong&gt; values are for single-model single-scale on the &lt;a href="https://cocodataset.org/"&gt;COCO val2017&lt;/a&gt; dataset. See &lt;a href="https://docs.ultralytics.com/guides/yolo-performance-metrics/"&gt;YOLO Performance Metrics&lt;/a&gt; for details. &lt;br /&gt;Reproduce with &lt;code&gt;yolo val segment data=coco.yaml device=0&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Speed&lt;/strong&gt; metrics are averaged over COCO val images using an &lt;a href="https://aws.amazon.com/ec2/instance-types/p4/"&gt;Amazon EC2 P4d&lt;/a&gt; instance. CPU speeds measured with &lt;a href="https://onnx.ai/"&gt;ONNX&lt;/a&gt; export. GPU speeds measured with &lt;a href="https://developer.nvidia.com/tensorrt"&gt;TensorRT&lt;/a&gt; export. &lt;br /&gt;Reproduce with &lt;code&gt;yolo val segment data=coco.yaml batch=1 device=0|cpu&lt;/code&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;Classification (ImageNet)&lt;/summary&gt; 
 &lt;p&gt;Consult the &lt;a href="https://docs.ultralytics.com/tasks/classify/"&gt;Classification Docs&lt;/a&gt; for usage examples. These models are trained on &lt;a href="https://docs.ultralytics.com/datasets/classify/imagenet/"&gt;ImageNet&lt;/a&gt;, covering 1000 classes.&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model&lt;/th&gt; 
    &lt;th&gt;size&lt;br /&gt;&lt;sup&gt;(pixels)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;acc&lt;br /&gt;&lt;sup&gt;top1&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;acc&lt;br /&gt;&lt;sup&gt;top5&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;Speed&lt;br /&gt;&lt;sup&gt;CPU ONNX&lt;br /&gt;(ms)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;Speed&lt;br /&gt;&lt;sup&gt;T4 TensorRT10&lt;br /&gt;(ms)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;params&lt;br /&gt;&lt;sup&gt;(M)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;FLOPs&lt;br /&gt;&lt;sup&gt;(B) at 224&lt;/sup&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26n-cls.pt"&gt;YOLO26n-cls&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;224&lt;/td&gt; 
    &lt;td&gt;71.4&lt;/td&gt; 
    &lt;td&gt;90.1&lt;/td&gt; 
    &lt;td&gt;5.0 ¬± 0.3&lt;/td&gt; 
    &lt;td&gt;1.1 ¬± 0.0&lt;/td&gt; 
    &lt;td&gt;2.8&lt;/td&gt; 
    &lt;td&gt;0.5&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26s-cls.pt"&gt;YOLO26s-cls&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;224&lt;/td&gt; 
    &lt;td&gt;76.0&lt;/td&gt; 
    &lt;td&gt;92.9&lt;/td&gt; 
    &lt;td&gt;7.9 ¬± 0.2&lt;/td&gt; 
    &lt;td&gt;1.3 ¬± 0.0&lt;/td&gt; 
    &lt;td&gt;6.7&lt;/td&gt; 
    &lt;td&gt;1.6&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26m-cls.pt"&gt;YOLO26m-cls&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;224&lt;/td&gt; 
    &lt;td&gt;78.1&lt;/td&gt; 
    &lt;td&gt;94.2&lt;/td&gt; 
    &lt;td&gt;17.2 ¬± 0.4&lt;/td&gt; 
    &lt;td&gt;2.0 ¬± 0.0&lt;/td&gt; 
    &lt;td&gt;11.6&lt;/td&gt; 
    &lt;td&gt;4.9&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26l-cls.pt"&gt;YOLO26l-cls&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;224&lt;/td&gt; 
    &lt;td&gt;79.0&lt;/td&gt; 
    &lt;td&gt;94.6&lt;/td&gt; 
    &lt;td&gt;23.2 ¬± 0.3&lt;/td&gt; 
    &lt;td&gt;2.8 ¬± 0.0&lt;/td&gt; 
    &lt;td&gt;14.1&lt;/td&gt; 
    &lt;td&gt;6.2&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26x-cls.pt"&gt;YOLO26x-cls&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;224&lt;/td&gt; 
    &lt;td&gt;79.9&lt;/td&gt; 
    &lt;td&gt;95.0&lt;/td&gt; 
    &lt;td&gt;41.4 ¬± 0.9&lt;/td&gt; 
    &lt;td&gt;3.8 ¬± 0.0&lt;/td&gt; 
    &lt;td&gt;29.6&lt;/td&gt; 
    &lt;td&gt;13.6&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;acc&lt;/strong&gt; values represent model accuracy on the &lt;a href="https://www.image-net.org/"&gt;ImageNet&lt;/a&gt; dataset validation set. &lt;br /&gt;Reproduce with &lt;code&gt;yolo val classify data=path/to/ImageNet device=0&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Speed&lt;/strong&gt; metrics are averaged over ImageNet val images using an &lt;a href="https://aws.amazon.com/ec2/instance-types/p4/"&gt;Amazon EC2 P4d&lt;/a&gt; instance. CPU speeds measured with &lt;a href="https://onnx.ai/"&gt;ONNX&lt;/a&gt; export. GPU speeds measured with &lt;a href="https://developer.nvidia.com/tensorrt"&gt;TensorRT&lt;/a&gt; export. &lt;br /&gt;Reproduce with &lt;code&gt;yolo val classify data=path/to/ImageNet batch=1 device=0|cpu&lt;/code&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;Pose (COCO)&lt;/summary&gt; 
 &lt;p&gt;See the &lt;a href="https://docs.ultralytics.com/tasks/pose/"&gt;Pose Estimation Docs&lt;/a&gt; for usage examples. These models are trained on &lt;a href="https://docs.ultralytics.com/datasets/pose/coco/"&gt;COCO-Pose&lt;/a&gt;, focusing on the 'person' class.&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model&lt;/th&gt; 
    &lt;th&gt;size&lt;br /&gt;&lt;sup&gt;(pixels)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;mAP&lt;sup&gt;pose&lt;br /&gt;50-95(e2e)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;mAP&lt;sup&gt;pose&lt;br /&gt;50(e2e)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;Speed&lt;br /&gt;&lt;sup&gt;CPU ONNX&lt;br /&gt;(ms)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;Speed&lt;br /&gt;&lt;sup&gt;T4 TensorRT10&lt;br /&gt;(ms)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;params&lt;br /&gt;&lt;sup&gt;(M)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;FLOPs&lt;br /&gt;&lt;sup&gt;(B)&lt;/sup&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26n-pose.pt"&gt;YOLO26n-pose&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;57.2&lt;/td&gt; 
    &lt;td&gt;83.3&lt;/td&gt; 
    &lt;td&gt;40.3 ¬± 0.5&lt;/td&gt; 
    &lt;td&gt;1.8 ¬± 0.0&lt;/td&gt; 
    &lt;td&gt;2.9&lt;/td&gt; 
    &lt;td&gt;7.5&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26s-pose.pt"&gt;YOLO26s-pose&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;63.0&lt;/td&gt; 
    &lt;td&gt;86.6&lt;/td&gt; 
    &lt;td&gt;85.3 ¬± 0.9&lt;/td&gt; 
    &lt;td&gt;2.7 ¬± 0.0&lt;/td&gt; 
    &lt;td&gt;10.4&lt;/td&gt; 
    &lt;td&gt;23.9&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26m-pose.pt"&gt;YOLO26m-pose&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;68.8&lt;/td&gt; 
    &lt;td&gt;89.6&lt;/td&gt; 
    &lt;td&gt;218.0 ¬± 1.5&lt;/td&gt; 
    &lt;td&gt;5.0 ¬± 0.1&lt;/td&gt; 
    &lt;td&gt;21.5&lt;/td&gt; 
    &lt;td&gt;73.1&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26l-pose.pt"&gt;YOLO26l-pose&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;70.4&lt;/td&gt; 
    &lt;td&gt;90.5&lt;/td&gt; 
    &lt;td&gt;275.4 ¬± 2.4&lt;/td&gt; 
    &lt;td&gt;6.5 ¬± 0.1&lt;/td&gt; 
    &lt;td&gt;25.9&lt;/td&gt; 
    &lt;td&gt;91.3&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26x-pose.pt"&gt;YOLO26x-pose&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;640&lt;/td&gt; 
    &lt;td&gt;71.6&lt;/td&gt; 
    &lt;td&gt;91.6&lt;/td&gt; 
    &lt;td&gt;565.4 ¬± 3.0&lt;/td&gt; 
    &lt;td&gt;12.2 ¬± 0.2&lt;/td&gt; 
    &lt;td&gt;57.6&lt;/td&gt; 
    &lt;td&gt;201.7&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;mAP&lt;sup&gt;val&lt;/sup&gt;&lt;/strong&gt; values are for single-model single-scale on the &lt;a href="https://docs.ultralytics.com/datasets/pose/coco/"&gt;COCO Keypoints val2017&lt;/a&gt; dataset. See &lt;a href="https://docs.ultralytics.com/guides/yolo-performance-metrics/"&gt;YOLO Performance Metrics&lt;/a&gt; for details. &lt;br /&gt;Reproduce with &lt;code&gt;yolo val pose data=coco-pose.yaml device=0&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Speed&lt;/strong&gt; metrics are averaged over COCO val images using an &lt;a href="https://aws.amazon.com/ec2/instance-types/p4/"&gt;Amazon EC2 P4d&lt;/a&gt; instance. CPU speeds measured with &lt;a href="https://onnx.ai/"&gt;ONNX&lt;/a&gt; export. GPU speeds measured with &lt;a href="https://developer.nvidia.com/tensorrt"&gt;TensorRT&lt;/a&gt; export. &lt;br /&gt;Reproduce with &lt;code&gt;yolo val pose data=coco-pose.yaml batch=1 device=0|cpu&lt;/code&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;Oriented Bounding Boxes (DOTAv1)&lt;/summary&gt; 
 &lt;p&gt;Check the &lt;a href="https://docs.ultralytics.com/tasks/obb/"&gt;OBB Docs&lt;/a&gt; for usage examples. These models are trained on &lt;a href="https://docs.ultralytics.com/datasets/obb/dota-v2/#dota-v10/"&gt;DOTAv1&lt;/a&gt;, including 15 classes.&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model&lt;/th&gt; 
    &lt;th&gt;size&lt;br /&gt;&lt;sup&gt;(pixels)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;mAP&lt;sup&gt;test&lt;br /&gt;50-95(e2e)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;mAP&lt;sup&gt;test&lt;br /&gt;50(e2e)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;Speed&lt;br /&gt;&lt;sup&gt;CPU ONNX&lt;br /&gt;(ms)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;Speed&lt;br /&gt;&lt;sup&gt;T4 TensorRT10&lt;br /&gt;(ms)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;params&lt;br /&gt;&lt;sup&gt;(M)&lt;/sup&gt;&lt;/th&gt; 
    &lt;th&gt;FLOPs&lt;br /&gt;&lt;sup&gt;(B)&lt;/sup&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26n-obb.pt"&gt;YOLO26n-obb&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;1024&lt;/td&gt; 
    &lt;td&gt;52.4&lt;/td&gt; 
    &lt;td&gt;78.9&lt;/td&gt; 
    &lt;td&gt;97.7 ¬± 0.9&lt;/td&gt; 
    &lt;td&gt;2.8 ¬± 0.0&lt;/td&gt; 
    &lt;td&gt;2.5&lt;/td&gt; 
    &lt;td&gt;14.0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26s-obb.pt"&gt;YOLO26s-obb&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;1024&lt;/td&gt; 
    &lt;td&gt;54.8&lt;/td&gt; 
    &lt;td&gt;80.9&lt;/td&gt; 
    &lt;td&gt;218.0 ¬± 1.4&lt;/td&gt; 
    &lt;td&gt;4.9 ¬± 0.1&lt;/td&gt; 
    &lt;td&gt;9.8&lt;/td&gt; 
    &lt;td&gt;55.1&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26m-obb.pt"&gt;YOLO26m-obb&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;1024&lt;/td&gt; 
    &lt;td&gt;55.3&lt;/td&gt; 
    &lt;td&gt;81.0&lt;/td&gt; 
    &lt;td&gt;579.2 ¬± 3.8&lt;/td&gt; 
    &lt;td&gt;10.2 ¬± 0.3&lt;/td&gt; 
    &lt;td&gt;21.2&lt;/td&gt; 
    &lt;td&gt;183.3&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26l-obb.pt"&gt;YOLO26l-obb&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;1024&lt;/td&gt; 
    &lt;td&gt;56.2&lt;/td&gt; 
    &lt;td&gt;81.6&lt;/td&gt; 
    &lt;td&gt;735.6 ¬± 3.1&lt;/td&gt; 
    &lt;td&gt;13.0 ¬± 0.2&lt;/td&gt; 
    &lt;td&gt;25.6&lt;/td&gt; 
    &lt;td&gt;230.0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26x-obb.pt"&gt;YOLO26x-obb&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;1024&lt;/td&gt; 
    &lt;td&gt;56.7&lt;/td&gt; 
    &lt;td&gt;81.7&lt;/td&gt; 
    &lt;td&gt;1485.7 ¬± 11.5&lt;/td&gt; 
    &lt;td&gt;30.5 ¬± 0.9&lt;/td&gt; 
    &lt;td&gt;57.6&lt;/td&gt; 
    &lt;td&gt;516.5&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;mAP&lt;sup&gt;test&lt;/sup&gt;&lt;/strong&gt; values are for single-model multiscale performance on the &lt;a href="https://captain-whu.github.io/DOTA/dataset.html"&gt;DOTAv1 test set&lt;/a&gt;. &lt;br /&gt;Reproduce by &lt;code&gt;yolo val obb data=DOTAv1.yaml device=0 split=test&lt;/code&gt; and submit merged results to the &lt;a href="https://captain-whu.github.io/DOTA/evaluation.html"&gt;DOTA evaluation server&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Speed&lt;/strong&gt; metrics are averaged over &lt;a href="https://docs.ultralytics.com/datasets/obb/dota-v2/#dota-v10"&gt;DOTAv1 val images&lt;/a&gt; using an &lt;a href="https://aws.amazon.com/ec2/instance-types/p4/"&gt;Amazon EC2 P4d&lt;/a&gt; instance. CPU speeds measured with &lt;a href="https://onnx.ai/"&gt;ONNX&lt;/a&gt; export. GPU speeds measured with &lt;a href="https://developer.nvidia.com/tensorrt"&gt;TensorRT&lt;/a&gt; export. &lt;br /&gt;Reproduce by &lt;code&gt;yolo val obb data=DOTAv1.yaml batch=1 device=0|cpu&lt;/code&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;üß© Integrations&lt;/h2&gt; 
&lt;p&gt;Our key integrations with leading AI platforms extend the functionality of Ultralytics' offerings, enhancing tasks like dataset labeling, training, visualization, and model management. Discover how Ultralytics, in collaboration with partners like &lt;a href="https://docs.ultralytics.com/integrations/weights-biases/"&gt;Weights &amp;amp; Biases&lt;/a&gt;, &lt;a href="https://docs.ultralytics.com/integrations/comet/"&gt;Comet ML&lt;/a&gt;, &lt;a href="https://docs.ultralytics.com/integrations/roboflow/"&gt;Roboflow&lt;/a&gt;, and &lt;a href="https://docs.ultralytics.com/integrations/openvino/"&gt;Intel OpenVINO&lt;/a&gt;, can optimize your AI workflow. Explore more at &lt;a href="https://docs.ultralytics.com/integrations/"&gt;Ultralytics Integrations&lt;/a&gt;.&lt;/p&gt; 
&lt;a href="https://docs.ultralytics.com/integrations/" target="_blank"&gt; &lt;img width="100%" src="https://github.com/ultralytics/assets/raw/main/yolov8/banner-integrations.png" alt="Ultralytics active learning integrations" /&gt; &lt;/a&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://platform.ultralytics.com/ultralytics/yolo26"&gt; &lt;img src="https://github.com/ultralytics/assets/raw/main/partners/logo-ultralytics-hub.png" width="10%" alt="Ultralytics Platform logo" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="15%" height="0" alt="space" /&gt; 
 &lt;a href="https://docs.ultralytics.com/integrations/weights-biases/"&gt; &lt;img src="https://github.com/ultralytics/assets/raw/main/partners/logo-wb.png" width="10%" alt="Weights &amp;amp; Biases logo" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="15%" height="0" alt="space" /&gt; 
 &lt;a href="https://docs.ultralytics.com/integrations/comet/"&gt; &lt;img src="https://github.com/ultralytics/assets/raw/main/partners/logo-comet.png" width="10%" alt="Comet ML logo" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="15%" height="0" alt="space" /&gt; 
 &lt;a href="https://docs.ultralytics.com/integrations/neural-magic/"&gt; &lt;img src="https://github.com/ultralytics/assets/raw/main/partners/logo-neuralmagic.png" width="10%" alt="Neural Magic logo" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;Ultralytics Platform üåü&lt;/th&gt; 
   &lt;th align="center"&gt;Weights &amp;amp; Biases&lt;/th&gt; 
   &lt;th align="center"&gt;Comet&lt;/th&gt; 
   &lt;th align="center"&gt;Neural Magic&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Streamline YOLO workflows: Label, train, and deploy effortlessly with &lt;a href="https://platform.ultralytics.com/ultralytics/yolo26"&gt;Ultralytics Platform&lt;/a&gt;. Try now!&lt;/td&gt; 
   &lt;td align="center"&gt;Track experiments, hyperparameters, and results with &lt;a href="https://docs.ultralytics.com/integrations/weights-biases/"&gt;Weights &amp;amp; Biases&lt;/a&gt;.&lt;/td&gt; 
   &lt;td align="center"&gt;Free forever, &lt;a href="https://docs.ultralytics.com/integrations/comet/"&gt;Comet ML&lt;/a&gt; lets you save YOLO models, resume training, and interactively visualize predictions.&lt;/td&gt; 
   &lt;td align="center"&gt;Run YOLO inference up to 6x faster with &lt;a href="https://docs.ultralytics.com/integrations/neural-magic/"&gt;Neural Magic DeepSparse&lt;/a&gt;.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;ü§ù Contribute&lt;/h2&gt; 
&lt;p&gt;We thrive on community collaboration! Ultralytics YOLO wouldn't be the SOTA framework it is without contributions from developers like you. Please see our &lt;a href="https://docs.ultralytics.com/help/contributing/"&gt;Contributing Guide&lt;/a&gt; to get started. We also welcome your feedback‚Äîshare your experience by completing our &lt;a href="https://www.ultralytics.com/survey?utm_source=github&amp;amp;utm_medium=social&amp;amp;utm_campaign=Survey"&gt;Survey&lt;/a&gt;. A huge &lt;strong&gt;Thank You&lt;/strong&gt; üôè to everyone who contributes!&lt;/p&gt; 
&lt;!-- SVG image from https://opencollective.com/ultralytics/contributors.svg?width=1280 --&gt; 
&lt;p&gt;&lt;a href="https://github.com/ultralytics/ultralytics/graphs/contributors"&gt;&lt;img src="https://raw.githubusercontent.com/ultralytics/assets/main/im/image-contributors.png" alt="Ultralytics open-source contributors" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;We look forward to your contributions to help make the Ultralytics ecosystem even better!&lt;/p&gt; 
&lt;h2&gt;üìú License&lt;/h2&gt; 
&lt;p&gt;Ultralytics offers two licensing options to suit different needs:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;AGPL-3.0 License&lt;/strong&gt;: This &lt;a href="https://opensource.org/license/agpl-v3"&gt;OSI-approved&lt;/a&gt; open-source license is perfect for students, researchers, and enthusiasts. It encourages open collaboration and knowledge sharing. See the &lt;a href="https://github.com/ultralytics/ultralytics/raw/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for full details.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Ultralytics Enterprise License&lt;/strong&gt;: Designed for commercial use, this license allows for the seamless integration of Ultralytics software and AI models into commercial products and services, bypassing the open-source requirements of AGPL-3.0. If your use case involves commercial deployment, please contact us via &lt;a href="https://www.ultralytics.com/license"&gt;Ultralytics Licensing&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üìû Contact&lt;/h2&gt; 
&lt;p&gt;For bug reports and feature requests related to Ultralytics software, please visit &lt;a href="https://github.com/ultralytics/ultralytics/issues"&gt;GitHub Issues&lt;/a&gt;. For questions, discussions, and community support, join our active communities on &lt;a href="https://discord.com/invite/ultralytics"&gt;Discord&lt;/a&gt;, &lt;a href="https://www.reddit.com/r/ultralytics/"&gt;Reddit&lt;/a&gt;, and the &lt;a href="https://community.ultralytics.com/"&gt;Ultralytics Community Forums&lt;/a&gt;. We're here to help with all things Ultralytics!&lt;/p&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/ultralytics"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-github.png" width="3%" alt="Ultralytics GitHub" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="3%" alt="space" /&gt; 
 &lt;a href="https://www.linkedin.com/company/ultralytics/"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-linkedin.png" width="3%" alt="Ultralytics LinkedIn" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="3%" alt="space" /&gt; 
 &lt;a href="https://twitter.com/ultralytics"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-twitter.png" width="3%" alt="Ultralytics Twitter" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="3%" alt="space" /&gt; 
 &lt;a href="https://www.youtube.com/ultralytics?sub_confirmation=1"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-youtube.png" width="3%" alt="Ultralytics YouTube" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="3%" alt="space" /&gt; 
 &lt;a href="https://www.tiktok.com/@ultralytics"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-tiktok.png" width="3%" alt="Ultralytics TikTok" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="3%" alt="space" /&gt; 
 &lt;a href="https://ultralytics.com/bilibili"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-bilibili.png" width="3%" alt="Ultralytics BiliBili" /&gt;&lt;/a&gt; 
 &lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="3%" alt="space" /&gt; 
 &lt;a href="https://discord.com/invite/ultralytics"&gt;&lt;img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-discord.png" width="3%" alt="Ultralytics Discord" /&gt;&lt;/a&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>anthropics/skills</title>
      <link>https://github.com/anthropics/skills</link>
      <description>&lt;p&gt;Public repository for Agent Skills&lt;/p&gt;&lt;hr&gt;&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; This repository contains Anthropic's implementation of skills for Claude. For information about the Agent Skills standard, see &lt;a href="http://agentskills.io"&gt;agentskills.io&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h1&gt;Skills&lt;/h1&gt; 
&lt;p&gt;Skills are folders of instructions, scripts, and resources that Claude loads dynamically to improve performance on specialized tasks. Skills teach Claude how to complete specific tasks in a repeatable way, whether that's creating documents with your company's brand guidelines, analyzing data using your organization's specific workflows, or automating personal tasks.&lt;/p&gt; 
&lt;p&gt;For more information, check out:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://support.claude.com/en/articles/12512176-what-are-skills"&gt;What are skills?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://support.claude.com/en/articles/12512180-using-skills-in-claude"&gt;Using skills in Claude&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://support.claude.com/en/articles/12512198-creating-custom-skills"&gt;How to create custom skills&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://anthropic.com/engineering/equipping-agents-for-the-real-world-with-agent-skills"&gt;Equipping agents for the real world with Agent Skills&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;About This Repository&lt;/h1&gt; 
&lt;p&gt;This repository contains skills that demonstrate what's possible with Claude's skills system. These skills range from creative applications (art, music, design) to technical tasks (testing web apps, MCP server generation) to enterprise workflows (communications, branding, etc.).&lt;/p&gt; 
&lt;p&gt;Each skill is self-contained in its own folder with a &lt;code&gt;SKILL.md&lt;/code&gt; file containing the instructions and metadata that Claude uses. Browse through these skills to get inspiration for your own skills or to understand different patterns and approaches.&lt;/p&gt; 
&lt;p&gt;Many skills in this repo are open source (Apache 2.0). We've also included the document creation &amp;amp; editing skills that power &lt;a href="https://www.anthropic.com/news/create-files"&gt;Claude's document capabilities&lt;/a&gt; under the hood in the &lt;a href="https://raw.githubusercontent.com/anthropics/skills/main/skills/docx"&gt;&lt;code&gt;skills/docx&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/anthropics/skills/main/skills/pdf"&gt;&lt;code&gt;skills/pdf&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/anthropics/skills/main/skills/pptx"&gt;&lt;code&gt;skills/pptx&lt;/code&gt;&lt;/a&gt;, and &lt;a href="https://raw.githubusercontent.com/anthropics/skills/main/skills/xlsx"&gt;&lt;code&gt;skills/xlsx&lt;/code&gt;&lt;/a&gt; subfolders. These are source-available, not open source, but we wanted to share these with developers as a reference for more complex skills that are actively used in a production AI application.&lt;/p&gt; 
&lt;h2&gt;Disclaimer&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;These skills are provided for demonstration and educational purposes only.&lt;/strong&gt; While some of these capabilities may be available in Claude, the implementations and behaviors you receive from Claude may differ from what is shown in these skills. These skills are meant to illustrate patterns and possibilities. Always test skills thoroughly in your own environment before relying on them for critical tasks.&lt;/p&gt; 
&lt;h1&gt;Skill Sets&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/anthropics/skills/main/skills"&gt;./skills&lt;/a&gt;: Skill examples for Creative &amp;amp; Design, Development &amp;amp; Technical, Enterprise &amp;amp; Communication, and Document Skills&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/anthropics/skills/main/spec"&gt;./spec&lt;/a&gt;: The Agent Skills specification&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/anthropics/skills/main/template"&gt;./template&lt;/a&gt;: Skill template&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Try in Claude Code, Claude.ai, and the API&lt;/h1&gt; 
&lt;h2&gt;Claude Code&lt;/h2&gt; 
&lt;p&gt;You can register this repository as a Claude Code Plugin marketplace by running the following command in Claude Code:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;/plugin marketplace add anthropics/skills
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then, to install a specific set of skills:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Select &lt;code&gt;Browse and install plugins&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Select &lt;code&gt;anthropic-agent-skills&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Select &lt;code&gt;document-skills&lt;/code&gt; or &lt;code&gt;example-skills&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Select &lt;code&gt;Install now&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Alternatively, directly install either Plugin via:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;/plugin install document-skills@anthropic-agent-skills
/plugin install example-skills@anthropic-agent-skills
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After installing the plugin, you can use the skill by just mentioning it. For instance, if you install the &lt;code&gt;document-skills&lt;/code&gt; plugin from the marketplace, you can ask Claude Code to do something like: "Use the PDF skill to extract the form fields from &lt;code&gt;path/to/some-file.pdf&lt;/code&gt;"&lt;/p&gt; 
&lt;h2&gt;Claude.ai&lt;/h2&gt; 
&lt;p&gt;These example skills are all already available to paid plans in Claude.ai.&lt;/p&gt; 
&lt;p&gt;To use any skill from this repository or upload custom skills, follow the instructions in &lt;a href="https://support.claude.com/en/articles/12512180-using-skills-in-claude#h_a4222fa77b"&gt;Using skills in Claude&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Claude API&lt;/h2&gt; 
&lt;p&gt;You can use Anthropic's pre-built skills, and upload custom skills, via the Claude API. See the &lt;a href="https://docs.claude.com/en/api/skills-guide#creating-a-skill"&gt;Skills API Quickstart&lt;/a&gt; for more.&lt;/p&gt; 
&lt;h1&gt;Creating a Basic Skill&lt;/h1&gt; 
&lt;p&gt;Skills are simple to create - just a folder with a &lt;code&gt;SKILL.md&lt;/code&gt; file containing YAML frontmatter and instructions. You can use the &lt;strong&gt;template-skill&lt;/strong&gt; in this repository as a starting point:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-markdown"&gt;---
name: my-skill-name
description: A clear description of what this skill does and when to use it
---

# My Skill Name

[Add your instructions here that Claude will follow when this skill is active]

## Examples
- Example usage 1
- Example usage 2

## Guidelines
- Guideline 1
- Guideline 2
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The frontmatter requires only two fields:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;name&lt;/code&gt; - A unique identifier for your skill (lowercase, hyphens for spaces)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;description&lt;/code&gt; - A complete description of what the skill does and when to use it&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The markdown content below contains the instructions, examples, and guidelines that Claude will follow. For more details, see &lt;a href="https://support.claude.com/en/articles/12512198-creating-custom-skills"&gt;How to create custom skills&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Partner Skills&lt;/h1&gt; 
&lt;p&gt;Skills are a great way to teach Claude how to get better at using specific pieces of software. As we see awesome example skills from partners, we may highlight some of them here:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Notion&lt;/strong&gt; - &lt;a href="https://www.notion.so/notiondevs/Notion-Skills-for-Claude-28da4445d27180c7af1df7d8615723d0"&gt;Notion Skills for Claude&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>facebookresearch/fairseq</title>
      <link>https://github.com/facebookresearch/fairseq</link>
      <description>&lt;p&gt;Facebook AI Research Sequence-to-Sequence Toolkit written in Python.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/facebookresearch/fairseq/main/docs/fairseq_logo.png" width="150" /&gt; &lt;br /&gt; &lt;br /&gt; &lt;a href="https://opensource.fb.com/support-ukraine"&gt;&lt;img alt="Support Ukraine" src="https://img.shields.io/badge/Support-Ukraine-FFD500?style=flat&amp;amp;labelColor=005BBB" /&gt;&lt;/a&gt; &lt;a href="https://github.com/pytorch/fairseq/raw/main/LICENSE"&gt;&lt;img alt="MIT License" src="https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://github.com/pytorch/fairseq/releases"&gt;&lt;img alt="Latest Release" src="https://img.shields.io/github/release/pytorch/fairseq.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://github.com/pytorch/fairseq/actions?query=workflow:build"&gt;&lt;img alt="Build Status" src="https://github.com/pytorch/fairseq/workflows/build/badge.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://fairseq.readthedocs.io/en/latest/?badge=latest"&gt;&lt;img alt="Documentation Status" src="https://readthedocs.org/projects/fairseq/badge/?version=latest" /&gt;&lt;/a&gt; &lt;a href="https://app.circleci.com/pipelines/github/facebookresearch/fairseq/"&gt;&lt;img alt="CicleCI Status" src="https://circleci.com/gh/facebookresearch/fairseq.svg?style=shield" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;Fairseq(-py) is a sequence modeling toolkit that allows researchers and developers to train custom models for translation, summarization, language modeling and other text generation tasks.&lt;/p&gt; 
&lt;p&gt;We provide reference implementations of various sequence modeling papers:&lt;/p&gt; 
&lt;details&gt;
 &lt;summary&gt;List of implemented papers&lt;/summary&gt;
 &lt;p&gt; &lt;/p&gt;
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Convolutional Neural Networks (CNN)&lt;/strong&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/language_model/conv_lm/README.md"&gt;Language Modeling with Gated Convolutional Networks (Dauphin et al., 2017)&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/conv_seq2seq/README.md"&gt;Convolutional Sequence to Sequence Learning (Gehring et al., 2017)&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://github.com/pytorch/fairseq/tree/classic_seqlevel"&gt;Classical Structured Prediction Losses for Sequence to Sequence Learning (Edunov et al., 2018)&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/stories/README.md"&gt;Hierarchical Neural Story Generation (Fan et al., 2018)&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/wav2vec/README.md"&gt;wav2vec: Unsupervised Pre-training for Speech Recognition (Schneider et al., 2019)&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;LightConv and DynamicConv models&lt;/strong&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/pay_less_attention_paper/README.md"&gt;Pay Less Attention with Lightweight and Dynamic Convolutions (Wu et al., 2019)&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Long Short-Term Memory (LSTM) networks&lt;/strong&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Effective Approaches to Attention-based Neural Machine Translation (Luong et al., 2015)&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Transformer (self-attention) networks&lt;/strong&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Attention Is All You Need (Vaswani et al., 2017)&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/scaling_nmt/README.md"&gt;Scaling Neural Machine Translation (Ott et al., 2018)&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/backtranslation/README.md"&gt;Understanding Back-Translation at Scale (Edunov et al., 2018)&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/language_model/README.adaptive_inputs.md"&gt;Adaptive Input Representations for Neural Language Modeling (Baevski and Auli, 2018)&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/constrained_decoding/README.md"&gt;Lexically constrained decoding with dynamic beam allocation (Post &amp;amp; Vilar, 2018)&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/truncated_bptt/README.md"&gt;Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context (Dai et al., 2019)&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/adaptive_span/README.md"&gt;Adaptive Attention Span in Transformers (Sukhbaatar et al., 2019)&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/translation_moe/README.md"&gt;Mixture Models for Diverse Machine Translation: Tricks of the Trade (Shen et al., 2019)&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/roberta/README.md"&gt;RoBERTa: A Robustly Optimized BERT Pretraining Approach (Liu et al., 2019)&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/wmt19/README.md"&gt;Facebook FAIR's WMT19 News Translation Task Submission (Ng et al., 2019)&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/joint_alignment_translation/README.md"&gt;Jointly Learning to Align and Translate with Transformer Models (Garg et al., 2019)&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/mbart/README.md"&gt;Multilingual Denoising Pre-training for Neural Machine Translation (Liu et at., 2020)&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/byte_level_bpe/README.md"&gt;Neural Machine Translation with Byte-Level Subwords (Wang et al., 2020)&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/unsupervised_quality_estimation/README.md"&gt;Unsupervised Quality Estimation for Neural Machine Translation (Fomicheva et al., 2020)&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/wav2vec/README.md"&gt;wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations (Baevski et al., 2020)&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/pointer_generator/README.md"&gt;Generating Medical Reports from Patient-Doctor Conversations Using Sequence-to-Sequence Models (Enarvi et al., 2020)&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/linformer/README.md"&gt;Linformer: Self-Attention with Linear Complexity (Wang et al., 2020)&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/criss/README.md"&gt;Cross-lingual Retrieval for Iterative Self-Supervised Training (Tran et al., 2020)&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/latent_depth/README.md"&gt;Deep Transformers with Latent Depth (Li et al., 2020)&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://arxiv.org/abs/2006.13979"&gt;Unsupervised Cross-lingual Representation Learning for Speech Recognition (Conneau et al., 2020)&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://arxiv.org/abs/2010.11430"&gt;Self-training and Pre-training are Complementary for Speech Recognition (Xu et al., 2020)&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://arxiv.org/abs/2104.01027"&gt;Robust wav2vec 2.0: Analyzing Domain Shift in Self-Supervised Pre-Training (Hsu, et al., 2021)&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://arxiv.org/abs/2105.11084"&gt;Unsupervised Speech Recognition (Baevski, et al., 2021)&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://arxiv.org/abs/2109.11680"&gt;Simple and Effective Zero-shot Cross-lingual Phoneme Recognition (Xu et al., 2021)&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://arxiv.org/pdf/2109.14084.pdf"&gt;VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding (Xu et. al., 2021)&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://aclanthology.org/2021.findings-acl.370.pdf"&gt;VLM: Task-agnostic Video-Language Model Pre-training for Video Understanding (Xu et. al., 2021)&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/normformer/README.md"&gt;NormFormer: Improved Transformer Pretraining with Extra Normalization (Shleifer et. al, 2021)&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Non-autoregressive Transformers&lt;/strong&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Non-Autoregressive Neural Machine Translation (Gu et al., 2017)&lt;/li&gt; 
    &lt;li&gt;Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement (Lee et al. 2018)&lt;/li&gt; 
    &lt;li&gt;Insertion Transformer: Flexible Sequence Generation via Insertion Operations (Stern et al. 2019)&lt;/li&gt; 
    &lt;li&gt;Mask-Predict: Parallel Decoding of Conditional Masked Language Models (Ghazvininejad et al., 2019)&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/nonautoregressive_translation/README.md"&gt;Levenshtein Transformer (Gu et al., 2019)&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Finetuning&lt;/strong&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/rxf/README.md"&gt;Better Fine-Tuning by Reducing Representational Collapse (Aghajanyan et al. 2020)&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;/p&gt;
&lt;/details&gt; 
&lt;h3&gt;What's New:&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;May 2023 &lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/mms/README.md"&gt;Released models for Scaling Speech Technology to 1,000+ Languages (Pratap, et al., 2023)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;June 2022 &lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/wav2vec/unsupervised/README.md"&gt;Released code for wav2vec-U 2.0 from Towards End-to-end Unsupervised Speech Recognition (Liu, et al., 2022)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;May 2022 &lt;a href="https://github.com/facebookresearch/xformers"&gt;Integration with xFormers&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;December 2021 &lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/speech_to_speech/README.md"&gt;Released Direct speech-to-speech translation code&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;October 2021 &lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/MMPT/README.md"&gt;Released VideoCLIP and VLM models&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;October 2021 &lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/wav2vec/README.md"&gt;Released multilingual finetuned XLSR-53 model&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;September 2021 &lt;a href="https://github.com/github/renaming"&gt;&lt;code&gt;master&lt;/code&gt; branch renamed to &lt;code&gt;main&lt;/code&gt;&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;July 2021 &lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/discriminative_reranking_nmt/README.md"&gt;Released DrNMT code&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;July 2021 &lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/wav2vec/README.md"&gt;Released Robust wav2vec 2.0 model&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;June 2021 &lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/xlmr/README.md"&gt;Released XLMR-XL and XLMR-XXL models&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;May 2021 &lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/wav2vec/unsupervised/README.md"&gt;Released Unsupervised Speech Recognition code&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;March 2021 &lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/fully_sharded_data_parallel/README.md"&gt;Added full parameter and optimizer state sharding + CPU offloading&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;February 2021 &lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/laser/README.md"&gt;Added LASER training code&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;December 2020: &lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/adaptive_span/README.md"&gt;Added Adaptive Attention Span code&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;December 2020: &lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/gottbert/README.md"&gt;GottBERT model and code released&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;November 2020: Adopted the &lt;a href="https://github.com/facebookresearch/hydra"&gt;Hydra&lt;/a&gt; configuration framework 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/docs/hydra_integration.md"&gt;see documentation explaining how to use it for new and existing projects&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;November 2020: &lt;a href="https://github.com/pytorch/fairseq/releases/tag/v0.10.0"&gt;fairseq 0.10.0 released&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;October 2020: &lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/rxf/README.md"&gt;Added R3F/R4F (Better Fine-Tuning) code&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;October 2020: &lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/latent_depth/README.md"&gt;Deep Transformer with Latent Depth code released&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;October 2020: &lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/criss/README.md"&gt;Added CRISS models and code&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt;
 &lt;summary&gt;Previous updates&lt;/summary&gt;
 &lt;p&gt; &lt;/p&gt;
 &lt;ul&gt; 
  &lt;li&gt;September 2020: &lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/linformer/README.md"&gt;Added Linformer code&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;September 2020: &lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/pointer_generator/README.md"&gt;Added pointer-generator networks&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;August 2020: &lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/constrained_decoding/README.md"&gt;Added lexically constrained decoding&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;August 2020: &lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/wav2vec/README.md"&gt;wav2vec2 models and code released&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;July 2020: &lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/unsupervised_quality_estimation/README.md"&gt;Unsupervised Quality Estimation code released&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;May 2020: &lt;a href="https://twitter.com/fairseq"&gt;Follow fairseq on Twitter&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;April 2020: &lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/simultaneous_translation/README.md"&gt;Monotonic Multihead Attention code released&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;April 2020: &lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/quant_noise/README.md"&gt;Quant-Noise code released&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;April 2020: &lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/megatron_11b/README.md"&gt;Initial model parallel support and 11B parameters unidirectional LM released&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;March 2020: &lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/byte_level_bpe/README.md"&gt;Byte-level BPE code released&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;February 2020: &lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/mbart/README.md"&gt;mBART model and code released&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;February 2020: &lt;a href="https://github.com/pytorch/fairseq/tree/main/examples/backtranslation#training-your-own-model-wmt18-english-german"&gt;Added tutorial for back-translation&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;December 2019: &lt;a href="https://github.com/pytorch/fairseq/releases/tag/v0.9.0"&gt;fairseq 0.9.0 released&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;November 2019: &lt;a href="https://facebookresearch.github.io/vizseq/docs/getting_started/fairseq_example"&gt;VizSeq released (a visual analysis toolkit for evaluating fairseq models)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;November 2019: &lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/camembert/README.md"&gt;CamemBERT model and code released&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;November 2019: &lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/bart/README.md"&gt;BART model and code released&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;November 2019: &lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/xlmr/README.md"&gt;XLM-R models and code released&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;September 2019: &lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/nonautoregressive_translation/README.md"&gt;Nonautoregressive translation code released&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;August 2019: &lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/wmt19/README.md"&gt;WMT'19 models released&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;July 2019: fairseq relicensed under MIT license&lt;/li&gt; 
  &lt;li&gt;July 2019: &lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/roberta/README.md"&gt;RoBERTa models and code released&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;June 2019: &lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/wav2vec/README.md"&gt;wav2vec models and code released&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;/p&gt;
&lt;/details&gt; 
&lt;h3&gt;Features:&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;multi-GPU training on one machine or across multiple machines (data and model parallel)&lt;/li&gt; 
 &lt;li&gt;fast generation on both CPU and GPU with multiple search algorithms implemented: 
  &lt;ul&gt; 
   &lt;li&gt;beam search&lt;/li&gt; 
   &lt;li&gt;Diverse Beam Search (&lt;a href="https://arxiv.org/abs/1610.02424"&gt;Vijayakumar et al., 2016&lt;/a&gt;)&lt;/li&gt; 
   &lt;li&gt;sampling (unconstrained, top-k and top-p/nucleus)&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/constrained_decoding/README.md"&gt;lexically constrained decoding&lt;/a&gt; (Post &amp;amp; Vilar, 2018)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://fairseq.readthedocs.io/en/latest/getting_started.html#large-mini-batch-training-with-delayed-updates"&gt;gradient accumulation&lt;/a&gt; enables training with large mini-batches even on a single GPU&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://fairseq.readthedocs.io/en/latest/getting_started.html#training-with-half-precision-floating-point-fp16"&gt;mixed precision training&lt;/a&gt; (trains faster with less GPU memory on &lt;a href="https://developer.nvidia.com/tensor-cores"&gt;NVIDIA tensor cores&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://fairseq.readthedocs.io/en/latest/overview.html"&gt;extensible&lt;/a&gt;: easily register new models, criterions, tasks, optimizers and learning rate schedulers&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/docs/hydra_integration.md"&gt;flexible configuration&lt;/a&gt; based on &lt;a href="https://github.com/facebookresearch/hydra"&gt;Hydra&lt;/a&gt; allowing a combination of code, command-line and file based configuration&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/fully_sharded_data_parallel/README.md"&gt;full parameter and optimizer state sharding&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/fully_sharded_data_parallel/README.md"&gt;offloading parameters to CPU&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We also provide &lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/#pre-trained-models-and-examples"&gt;pre-trained models for translation and language modeling&lt;/a&gt; with a convenient &lt;code&gt;torch.hub&lt;/code&gt; interface:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;en2de = torch.hub.load('pytorch/fairseq', 'transformer.wmt19.en-de.single_model')
en2de.translate('Hello world', beam=5)
# 'Hallo Welt'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See the PyTorch Hub tutorials for &lt;a href="https://pytorch.org/hub/pytorch_fairseq_translation/"&gt;translation&lt;/a&gt; and &lt;a href="https://pytorch.org/hub/pytorch_fairseq_roberta/"&gt;RoBERTa&lt;/a&gt; for more examples.&lt;/p&gt; 
&lt;h1&gt;Requirements and Installation&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="http://pytorch.org/"&gt;PyTorch&lt;/a&gt; version &amp;gt;= 1.10.0&lt;/li&gt; 
 &lt;li&gt;Python version &amp;gt;= 3.8&lt;/li&gt; 
 &lt;li&gt;For training new models, you'll also need an NVIDIA GPU and &lt;a href="https://github.com/NVIDIA/nccl"&gt;NCCL&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;To install fairseq&lt;/strong&gt; and develop locally:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/pytorch/fairseq
cd fairseq
pip install --editable ./

# on MacOS:
# CFLAGS="-stdlib=libc++" pip install --editable ./

# to install the latest stable release (0.10.x)
# pip install fairseq
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;For faster training&lt;/strong&gt; install NVIDIA's &lt;a href="https://github.com/NVIDIA/apex"&gt;apex&lt;/a&gt; library:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/NVIDIA/apex
cd apex
pip install -v --no-cache-dir --global-option="--cpp_ext" --global-option="--cuda_ext" \
  --global-option="--deprecated_fused_adam" --global-option="--xentropy" \
  --global-option="--fast_multihead_attn" ./
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;For large datasets&lt;/strong&gt; install &lt;a href="https://arrow.apache.org/docs/python/install.html#using-pip"&gt;PyArrow&lt;/a&gt;: &lt;code&gt;pip install pyarrow&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;If you use Docker make sure to increase the shared memory size either with &lt;code&gt;--ipc=host&lt;/code&gt; or &lt;code&gt;--shm-size&lt;/code&gt; as command line options to &lt;code&gt;nvidia-docker run&lt;/code&gt; .&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Getting Started&lt;/h1&gt; 
&lt;p&gt;The &lt;a href="https://fairseq.readthedocs.io/"&gt;full documentation&lt;/a&gt; contains instructions for getting started, training new models and extending fairseq with new model types and tasks.&lt;/p&gt; 
&lt;h1&gt;Pre-trained models and examples&lt;/h1&gt; 
&lt;p&gt;We provide pre-trained models and pre-processed, binarized test sets for several tasks listed below, as well as example training and evaluation commands.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/translation/README.md"&gt;Translation&lt;/a&gt;: convolutional and transformer models are available&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/language_model/README.md"&gt;Language Modeling&lt;/a&gt;: convolutional and transformer models are available&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We also have more detailed READMEs to reproduce results from specific papers:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/wav2vec/xlsr/README.md"&gt;XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale (Babu et al., 2021)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/criss/README.md"&gt;Cross-lingual Retrieval for Iterative Self-Supervised Training (Tran et al., 2020)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/wav2vec/README.md"&gt;wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations (Baevski et al., 2020)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/unsupervised_quality_estimation/README.md"&gt;Unsupervised Quality Estimation for Neural Machine Translation (Fomicheva et al., 2020)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/quant_noise/README.md"&gt;Training with Quantization Noise for Extreme Model Compression ({Fan*, Stock*} et al., 2020)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/byte_level_bpe/README.md"&gt;Neural Machine Translation with Byte-Level Subwords (Wang et al., 2020)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/mbart/README.md"&gt;Multilingual Denoising Pre-training for Neural Machine Translation (Liu et at., 2020)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/layerdrop/README.md"&gt;Reducing Transformer Depth on Demand with Structured Dropout (Fan et al., 2019)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/joint_alignment_translation/README.md"&gt;Jointly Learning to Align and Translate with Transformer Models (Garg et al., 2019)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/nonautoregressive_translation/README.md"&gt;Levenshtein Transformer (Gu et al., 2019)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/wmt19/README.md"&gt;Facebook FAIR's WMT19 News Translation Task Submission (Ng et al., 2019)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/roberta/README.md"&gt;RoBERTa: A Robustly Optimized BERT Pretraining Approach (Liu et al., 2019)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/wav2vec/README.md"&gt;wav2vec: Unsupervised Pre-training for Speech Recognition (Schneider et al., 2019)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/translation_moe/README.md"&gt;Mixture Models for Diverse Machine Translation: Tricks of the Trade (Shen et al., 2019)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/pay_less_attention_paper/README.md"&gt;Pay Less Attention with Lightweight and Dynamic Convolutions (Wu et al., 2019)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/backtranslation/README.md"&gt;Understanding Back-Translation at Scale (Edunov et al., 2018)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pytorch/fairseq/tree/classic_seqlevel"&gt;Classical Structured Prediction Losses for Sequence to Sequence Learning (Edunov et al., 2018)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/stories/README.md"&gt;Hierarchical Neural Story Generation (Fan et al., 2018)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/scaling_nmt/README.md"&gt;Scaling Neural Machine Translation (Ott et al., 2018)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/conv_seq2seq/README.md"&gt;Convolutional Sequence to Sequence Learning (Gehring et al., 2017)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/facebookresearch/fairseq/main/examples/language_model/README.conv.md"&gt;Language Modeling with Gated Convolutional Networks (Dauphin et al., 2017)&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Join the fairseq community&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;Twitter: &lt;a href="https://twitter.com/fairseq"&gt;https://twitter.com/fairseq&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Facebook page: &lt;a href="https://www.facebook.com/groups/fairseq.users"&gt;https://www.facebook.com/groups/fairseq.users&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Google group: &lt;a href="https://groups.google.com/forum/#!forum/fairseq-users"&gt;https://groups.google.com/forum/#!forum/fairseq-users&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;License&lt;/h1&gt; 
&lt;p&gt;fairseq(-py) is MIT-licensed. The license applies to the pre-trained models as well.&lt;/p&gt; 
&lt;h1&gt;Citation&lt;/h1&gt; 
&lt;p&gt;Please cite as:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@inproceedings{ott2019fairseq,
  title = {fairseq: A Fast, Extensible Toolkit for Sequence Modeling},
  author = {Myle Ott and Sergey Edunov and Alexei Baevski and Angela Fan and Sam Gross and Nathan Ng and David Grangier and Michael Auli},
  booktitle = {Proceedings of NAACL-HLT 2019: Demonstrations},
  year = {2019},
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>prowler-cloud/prowler</title>
      <link>https://github.com/prowler-cloud/prowler</link>
      <description>&lt;p&gt;Prowler is the world‚Äôs most widely used open-source cloud security platform that automates security and compliance across any cloud environment.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img align="center" src="https://github.com/prowler-cloud/prowler/raw/master/docs/img/prowler-logo-black.png#gh-light-mode-only" width="50%" height="50%" /&gt; &lt;img align="center" src="https://github.com/prowler-cloud/prowler/raw/master/docs/img/prowler-logo-white.png#gh-dark-mode-only" width="50%" height="50%" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;b&gt;&lt;i&gt;Prowler&lt;/i&gt;&lt;/b&gt;&lt;i&gt; is the Open Cloud Security platform trusted by thousands to automate security and compliance in any cloud environment. With hundreds of ready-to-use checks and compliance frameworks, Prowler delivers real-time, customizable monitoring and seamless integrations, making cloud security simple, scalable, and cost-effective for organizations of any size. &lt;/i&gt;&lt;/p&gt;
&lt;i&gt; &lt;/i&gt;
&lt;p align="center"&gt;&lt;i&gt; &lt;b&gt;Secure ANY cloud at AI Speed at &lt;a href="https://prowler.com"&gt;prowler.com&lt;/a&gt;&lt;/b&gt;&lt;/i&gt;&lt;a href="https://prowler.com"&gt; &lt;/a&gt;&lt;/p&gt;
&lt;a href="https://prowler.com"&gt; &lt;/a&gt;
&lt;p align="center"&gt;&lt;a href="https://prowler.com"&gt; &lt;/a&gt;&lt;a href="https://goto.prowler.com/slack"&gt;&lt;img width="30" height="30" alt="Prowler community on Slack" src="https://github.com/prowler-cloud/prowler/assets/38561120/3c8b4ec5-6849-41a5-b5e1-52bbb94af73a" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;a href="https://goto.prowler.com/slack"&gt;Join our Prowler community!&lt;/a&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;p align="center"&gt; &lt;a href="https://goto.prowler.com/slack"&gt;&lt;img alt="Slack Shield" src="https://img.shields.io/badge/slack-prowler-brightgreen.svg?logo=slack" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/prowler/"&gt;&lt;img alt="Python Version" src="https://img.shields.io/pypi/v/prowler.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://pypi.python.org/pypi/prowler/"&gt;&lt;img alt="Python Version" src="https://img.shields.io/pypi/pyversions/prowler.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://pypistats.org/packages/prowler"&gt;&lt;img alt="PyPI Downloads" src="https://img.shields.io/pypi/dw/prowler.svg?label=downloads" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/toniblyx/prowler"&gt;&lt;img alt="Docker Pulls" src="https://img.shields.io/docker/pulls/toniblyx/prowler" /&gt;&lt;/a&gt; &lt;a href="https://gallery.ecr.aws/prowler-cloud/prowler"&gt;&lt;img width="120" height="19&amp;quot;" alt="AWS ECR Gallery" src="https://user-images.githubusercontent.com/3985464/151531396-b6535a68-c907-44eb-95a1-a09508178616.png" /&gt;&lt;/a&gt; &lt;a href="https://codecov.io/gh/prowler-cloud/prowler"&gt;&lt;img src="https://codecov.io/gh/prowler-cloud/prowler/graph/badge.svg?token=OflBGsdpDl" /&gt;&lt;/a&gt; &lt;a href="https://insights.linuxfoundation.org/project/prowler-cloud-prowler"&gt;&lt;img src="https://insights.linuxfoundation.org/api/badge/health-score?project=prowler-cloud-prowler" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/prowler-cloud/prowler/releases"&gt;&lt;img alt="Version" src="https://img.shields.io/github/v/release/prowler-cloud/prowler" /&gt;&lt;/a&gt; &lt;a href="https://github.com/prowler-cloud/prowler/releases"&gt;&lt;img alt="Version" src="https://img.shields.io/github/release-date/prowler-cloud/prowler" /&gt;&lt;/a&gt; &lt;a href="https://github.com/prowler-cloud/prowler"&gt;&lt;img alt="Contributors" src="https://img.shields.io/github/contributors-anon/prowler-cloud/prowler" /&gt;&lt;/a&gt; &lt;a href="https://github.com/prowler-cloud/prowler/issues"&gt;&lt;img alt="Issues" src="https://img.shields.io/github/issues/prowler-cloud/prowler" /&gt;&lt;/a&gt; &lt;a href="https://github.com/prowler-cloud/prowler"&gt;&lt;img alt="License" src="https://img.shields.io/github/license/prowler-cloud/prowler" /&gt;&lt;/a&gt; &lt;a href="https://twitter.com/ToniBlyx"&gt;&lt;img alt="Twitter" src="https://img.shields.io/twitter/follow/toniblyx?style=social" /&gt;&lt;/a&gt; &lt;a href="https://twitter.com/prowlercloud"&gt;&lt;img alt="Twitter" src="https://img.shields.io/twitter/follow/prowlercloud?style=social" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;p align="center"&gt; &lt;img align="center" src="https://raw.githubusercontent.com/prowler-cloud/prowler/master/docs/img/prowler-cloud.gif" width="100%" height="100%" /&gt; &lt;/p&gt; 
&lt;h1&gt;Description&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;Prowler&lt;/strong&gt; is the world‚Äôs most widely used &lt;em&gt;open-source cloud security platform&lt;/em&gt; that automates security and compliance across &lt;strong&gt;any cloud environment&lt;/strong&gt;. With hundreds of ready-to-use security checks, remediation guidance, and compliance frameworks, Prowler is built to &lt;em&gt;‚ÄúSecure ANY cloud at AI Speed‚Äù&lt;/em&gt;. Prowler delivers &lt;strong&gt;AI-driven&lt;/strong&gt;, &lt;strong&gt;customizable&lt;/strong&gt;, and &lt;strong&gt;easy-to-use&lt;/strong&gt; assessments, dashboards, reports, and integrations, making cloud security &lt;strong&gt;simple&lt;/strong&gt;, &lt;strong&gt;scalable&lt;/strong&gt;, and &lt;strong&gt;cost-effective&lt;/strong&gt; for organizations of any size.&lt;/p&gt; 
&lt;p&gt;Prowler includes hundreds of built-in controls to ensure compliance with standards and frameworks, including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Prowler ThreatScore:&lt;/strong&gt; Weighted risk prioritization scoring that helps you focus on the most critical security findings first&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Industry Standards:&lt;/strong&gt; CIS, NIST 800, NIST CSF, CISA, and MITRE ATT&amp;amp;CK&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Regulatory Compliance and Governance:&lt;/strong&gt; RBI, FedRAMP, PCI-DSS, and NIS2&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Frameworks for Sensitive Data and Privacy:&lt;/strong&gt; GDPR, HIPAA, and FFIEC&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Frameworks for Organizational Governance and Quality Control:&lt;/strong&gt; SOC2, GXP, and ISO 27001&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Cloud-Specific Frameworks:&lt;/strong&gt; AWS Foundational Technical Review (FTR), AWS Well-Architected Framework, and BSI C5&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;National Security Standards:&lt;/strong&gt; ENS (Spanish National Security Scheme) and KISA ISMS-P (Korean)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Custom Security Frameworks:&lt;/strong&gt; Tailored to your needs&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Prowler App / Prowler Cloud&lt;/h2&gt; 
&lt;p&gt;Prowler App / &lt;a href="https://cloud.prowler.com/"&gt;Prowler Cloud&lt;/a&gt; is a web-based application that simplifies running Prowler across your cloud provider accounts. It provides a user-friendly interface to visualize the results and streamline your security assessments.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/prowler-cloud/prowler/master/docs/images/products/overview.png" alt="Prowler App" /&gt; &lt;img src="https://raw.githubusercontent.com/prowler-cloud/prowler/master/docs/images/products/risk-pipeline.png" alt="Risk Pipeline" /&gt; &lt;img src="https://raw.githubusercontent.com/prowler-cloud/prowler/master/docs/images/products/threat-map.png" alt="Threat Map" /&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;For more details, refer to the &lt;a href="https://docs.prowler.com/projects/prowler-open-source/en/latest/#prowler-app-installation"&gt;Prowler App Documentation&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Prowler CLI&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;prowler &amp;lt;provider&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/prowler-cloud/prowler/master/docs/img/short-display.png" alt="Prowler CLI Execution" /&gt;&lt;/p&gt; 
&lt;h2&gt;Prowler Dashboard&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;prowler dashboard
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/prowler-cloud/prowler/master/docs/images/products/dashboard.png" alt="Prowler Dashboard" /&gt;&lt;/p&gt; 
&lt;h2&gt;Attack Paths&lt;/h2&gt; 
&lt;p&gt;Attack Paths automatically extends every completed AWS scan with a Neo4j graph that combines Cartography's cloud inventory with Prowler findings. The feature runs in the API worker after each scan and therefore requires:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;An accessible Neo4j instance (the Docker Compose files already ships a &lt;code&gt;neo4j&lt;/code&gt; service).&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;The following environment variables so Django and Celery can connect:&lt;/p&gt; 
  &lt;table&gt; 
   &lt;thead&gt; 
    &lt;tr&gt; 
     &lt;th&gt;Variable&lt;/th&gt; 
     &lt;th&gt;Description&lt;/th&gt; 
     &lt;th&gt;Default&lt;/th&gt; 
    &lt;/tr&gt; 
   &lt;/thead&gt; 
   &lt;tbody&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;code&gt;NEO4J_HOST&lt;/code&gt;&lt;/td&gt; 
     &lt;td&gt;Hostname used by the API containers.&lt;/td&gt; 
     &lt;td&gt;&lt;code&gt;neo4j&lt;/code&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;code&gt;NEO4J_PORT&lt;/code&gt;&lt;/td&gt; 
     &lt;td&gt;Bolt port exposed by Neo4j.&lt;/td&gt; 
     &lt;td&gt;&lt;code&gt;7687&lt;/code&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;code&gt;NEO4J_USER&lt;/code&gt; / &lt;code&gt;NEO4J_PASSWORD&lt;/code&gt;&lt;/td&gt; 
     &lt;td&gt;Credentials with rights to create per-tenant databases.&lt;/td&gt; 
     &lt;td&gt;&lt;code&gt;neo4j&lt;/code&gt; / &lt;code&gt;neo4j_password&lt;/code&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
   &lt;/tbody&gt; 
  &lt;/table&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Every AWS provider scan will enqueue an Attack Paths ingestion job automatically. Other cloud providers will be added in future iterations.&lt;/p&gt; 
&lt;h1&gt;Prowler at a Glance&lt;/h1&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Tip] For the most accurate and up-to-date information about checks, services, frameworks, and categories, visit &lt;a href="https://hub.prowler.com"&gt;&lt;strong&gt;Prowler Hub&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Provider&lt;/th&gt; 
   &lt;th&gt;Checks&lt;/th&gt; 
   &lt;th&gt;Services&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://docs.prowler.com/projects/prowler-open-source/en/latest/tutorials/compliance/"&gt;Compliance Frameworks&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://docs.prowler.com/projects/prowler-open-source/en/latest/tutorials/misc/#categories"&gt;Categories&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;Support&lt;/th&gt; 
   &lt;th&gt;Interface&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;AWS&lt;/td&gt; 
   &lt;td&gt;584&lt;/td&gt; 
   &lt;td&gt;85&lt;/td&gt; 
   &lt;td&gt;40&lt;/td&gt; 
   &lt;td&gt;17&lt;/td&gt; 
   &lt;td&gt;Official&lt;/td&gt; 
   &lt;td&gt;UI, API, CLI&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;GCP&lt;/td&gt; 
   &lt;td&gt;89&lt;/td&gt; 
   &lt;td&gt;17&lt;/td&gt; 
   &lt;td&gt;14&lt;/td&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;Official&lt;/td&gt; 
   &lt;td&gt;UI, API, CLI&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Azure&lt;/td&gt; 
   &lt;td&gt;169&lt;/td&gt; 
   &lt;td&gt;22&lt;/td&gt; 
   &lt;td&gt;15&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;Official&lt;/td&gt; 
   &lt;td&gt;UI, API, CLI&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Kubernetes&lt;/td&gt; 
   &lt;td&gt;84&lt;/td&gt; 
   &lt;td&gt;7&lt;/td&gt; 
   &lt;td&gt;6&lt;/td&gt; 
   &lt;td&gt;9&lt;/td&gt; 
   &lt;td&gt;Official&lt;/td&gt; 
   &lt;td&gt;UI, API, CLI&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;GitHub&lt;/td&gt; 
   &lt;td&gt;20&lt;/td&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;1&lt;/td&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;Official&lt;/td&gt; 
   &lt;td&gt;UI, API, CLI&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;M365&lt;/td&gt; 
   &lt;td&gt;70&lt;/td&gt; 
   &lt;td&gt;7&lt;/td&gt; 
   &lt;td&gt;3&lt;/td&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;Official&lt;/td&gt; 
   &lt;td&gt;UI, API, CLI&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OCI&lt;/td&gt; 
   &lt;td&gt;52&lt;/td&gt; 
   &lt;td&gt;15&lt;/td&gt; 
   &lt;td&gt;1&lt;/td&gt; 
   &lt;td&gt;12&lt;/td&gt; 
   &lt;td&gt;Official&lt;/td&gt; 
   &lt;td&gt;UI, API, CLI&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Alibaba Cloud&lt;/td&gt; 
   &lt;td&gt;63&lt;/td&gt; 
   &lt;td&gt;10&lt;/td&gt; 
   &lt;td&gt;1&lt;/td&gt; 
   &lt;td&gt;9&lt;/td&gt; 
   &lt;td&gt;Official&lt;/td&gt; 
   &lt;td&gt;CLI&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;IaC&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://trivy.dev/latest/docs/coverage/iac/"&gt;See &lt;code&gt;trivy&lt;/code&gt; docs.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
   &lt;td&gt;Official&lt;/td&gt; 
   &lt;td&gt;UI, API, CLI&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MongoDB Atlas&lt;/td&gt; 
   &lt;td&gt;10&lt;/td&gt; 
   &lt;td&gt;4&lt;/td&gt; 
   &lt;td&gt;0&lt;/td&gt; 
   &lt;td&gt;3&lt;/td&gt; 
   &lt;td&gt;Official&lt;/td&gt; 
   &lt;td&gt;UI, API, CLI&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;LLM&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.promptfoo.dev/docs/red-team/plugins/"&gt;See &lt;code&gt;promptfoo&lt;/code&gt; docs.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
   &lt;td&gt;Official&lt;/td&gt; 
   &lt;td&gt;CLI&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;NHN&lt;/td&gt; 
   &lt;td&gt;6&lt;/td&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;1&lt;/td&gt; 
   &lt;td&gt;0&lt;/td&gt; 
   &lt;td&gt;Unofficial&lt;/td&gt; 
   &lt;td&gt;CLI&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Note] The numbers in the table are updated periodically.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Note] Use the following commands to list Prowler's available checks, services, compliance frameworks, and categories:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;prowler &amp;lt;provider&amp;gt; --list-checks&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;prowler &amp;lt;provider&amp;gt; --list-services&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;prowler &amp;lt;provider&amp;gt; --list-compliance&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;prowler &amp;lt;provider&amp;gt; --list-categories&lt;/code&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;h1&gt;üíª Installation&lt;/h1&gt; 
&lt;h2&gt;Prowler App&lt;/h2&gt; 
&lt;p&gt;Prowler App offers flexible installation methods tailored to various environments:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;For detailed instructions on using Prowler App, refer to the &lt;a href="https://docs.prowler.com/projects/prowler-open-source/en/latest/tutorials/prowler-app/"&gt;Prowler App Usage Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Docker Compose&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Requirements&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;Docker Compose&lt;/code&gt; installed: &lt;a href="https://docs.docker.com/compose/install/"&gt;https://docs.docker.com/compose/install/&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Commands&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;curl -LO https://raw.githubusercontent.com/prowler-cloud/prowler/refs/heads/master/docker-compose.yml
curl -LO https://raw.githubusercontent.com/prowler-cloud/prowler/refs/heads/master/.env
docker compose up -d
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Containers are built for &lt;code&gt;linux/amd64&lt;/code&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Configuring Your Workstation for Prowler App&lt;/h3&gt; 
&lt;p&gt;If your workstation's architecture is incompatible, you can resolve this by:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Setting the environment variable&lt;/strong&gt;: &lt;code&gt;DOCKER_DEFAULT_PLATFORM=linux/amd64&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Using the following flag in your Docker command&lt;/strong&gt;: &lt;code&gt;--platform linux/amd64&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Once configured, access the Prowler App at &lt;a href="http://localhost:3000"&gt;http://localhost:3000&lt;/a&gt;. Sign up using your email and password to get started.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Common Issues with Docker Pull Installation&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Note] If you want to use AWS role assumption (e.g., with the "Connect assuming IAM Role" option), you may need to mount your local &lt;code&gt;.aws&lt;/code&gt; directory into the container as a volume (e.g., &lt;code&gt;- "${HOME}/.aws:/home/prowler/.aws:ro"&lt;/code&gt;). There are several ways to configure credentials for Docker containers. See the &lt;a href="https://raw.githubusercontent.com/prowler-cloud/prowler/master/docs/troubleshooting.mdx"&gt;Troubleshooting&lt;/a&gt; section for more details and examples.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;You can find more information in the &lt;a href="https://raw.githubusercontent.com/prowler-cloud/prowler/master/docs/troubleshooting.mdx"&gt;Troubleshooting&lt;/a&gt; section.&lt;/p&gt; 
&lt;h3&gt;From GitHub&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Requirements&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;git&lt;/code&gt; installed.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;poetry&lt;/code&gt; v2 installed: &lt;a href="https://python-poetry.org/docs/#installation"&gt;poetry installation&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;pnpm&lt;/code&gt; installed: &lt;a href="https://pnpm.io/installation"&gt;pnpm installation&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Docker Compose&lt;/code&gt; installed: &lt;a href="https://docs.docker.com/compose/install/"&gt;https://docs.docker.com/compose/install/&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Commands to run the API&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;git clone https://github.com/prowler-cloud/prowler
cd prowler/api
poetry install
eval $(poetry env activate)
set -a
source .env
docker compose up postgres valkey -d
cd src/backend
python manage.py migrate --database admin
gunicorn -c config/guniconf.py config.wsgi:application
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] As of Poetry v2.0.0, the &lt;code&gt;poetry shell&lt;/code&gt; command has been deprecated. Use &lt;code&gt;poetry env activate&lt;/code&gt; instead for environment activation.&lt;/p&gt; 
 &lt;p&gt;If your Poetry version is below v2.0.0, continue using &lt;code&gt;poetry shell&lt;/code&gt; to activate your environment. For further guidance, refer to the Poetry Environment Activation Guide &lt;a href="https://python-poetry.org/docs/managing-environments/#activating-the-environment"&gt;https://python-poetry.org/docs/managing-environments/#activating-the-environment&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;After completing the setup, access the API documentation at &lt;a href="http://localhost:8080/api/v1/docs"&gt;http://localhost:8080/api/v1/docs&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;Commands to run the API Worker&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;git clone https://github.com/prowler-cloud/prowler
cd prowler/api
poetry install
eval $(poetry env activate)
set -a
source .env
cd src/backend
python -m celery -A config.celery worker -l info -E
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Commands to run the API Scheduler&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;git clone https://github.com/prowler-cloud/prowler
cd prowler/api
poetry install
eval $(poetry env activate)
set -a
source .env
cd src/backend
python -m celery -A config.celery beat -l info --scheduler django_celery_beat.schedulers:DatabaseScheduler
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Commands to run the UI&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;git clone https://github.com/prowler-cloud/prowler
cd prowler/ui
pnpm install
pnpm run build
pnpm start
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Once configured, access the Prowler App at &lt;a href="http://localhost:3000"&gt;http://localhost:3000&lt;/a&gt;. Sign up using your email and password to get started.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Prowler CLI&lt;/h2&gt; 
&lt;h3&gt;Pip package&lt;/h3&gt; 
&lt;p&gt;Prowler CLI is available as a project in &lt;a href="https://pypi.org/project/prowler-cloud/"&gt;PyPI&lt;/a&gt;. Consequently, it can be installed using pip with Python &amp;gt;3.9.1, &amp;lt;3.13:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;pip install prowler
prowler -v
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;For further guidance, refer to &lt;a href="https://docs.prowler.com/projects/prowler-open-source/en/latest/#prowler-cli-installation"&gt;https://docs.prowler.com&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Containers&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Available Versions of Prowler CLI&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;The following versions of Prowler CLI are available, depending on your requirements:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;latest&lt;/code&gt;: Synchronizes with the &lt;code&gt;master&lt;/code&gt; branch. Note that this version is not stable.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;v4-latest&lt;/code&gt;: Synchronizes with the &lt;code&gt;v4&lt;/code&gt; branch. Note that this version is not stable.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;v3-latest&lt;/code&gt;: Synchronizes with the &lt;code&gt;v3&lt;/code&gt; branch. Note that this version is not stable.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;&amp;lt;x.y.z&amp;gt;&lt;/code&gt; (release): Stable releases corresponding to specific versions. You can find the complete list of releases &lt;a href="https://github.com/prowler-cloud/prowler/releases"&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;stable&lt;/code&gt;: Always points to the latest release.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;v4-stable&lt;/code&gt;: Always points to the latest release for v4.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;v3-stable&lt;/code&gt;: Always points to the latest release for v3.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The container images are available here:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Prowler CLI: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://hub.docker.com/r/prowlercloud/prowler/tags"&gt;DockerHub&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://gallery.ecr.aws/prowler-cloud/prowler"&gt;AWS Public ECR&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Prowler App: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://hub.docker.com/r/prowlercloud/prowler-ui/tags"&gt;DockerHub - Prowler UI&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://hub.docker.com/r/prowlercloud/prowler-api/tags"&gt;DockerHub - Prowler API&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;From GitHub&lt;/h3&gt; 
&lt;p&gt;Python &amp;gt;3.9.1, &amp;lt;3.13 is required with pip and Poetry:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;git clone https://github.com/prowler-cloud/prowler
cd prowler
eval $(poetry env activate)
poetry install
python prowler-cli.py -v
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] To clone Prowler on Windows, configure Git to support long file paths by running the following command: &lt;code&gt;git config core.longpaths true&lt;/code&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] As of Poetry v2.0.0, the &lt;code&gt;poetry shell&lt;/code&gt; command has been deprecated. Use &lt;code&gt;poetry env activate&lt;/code&gt; instead for environment activation.&lt;/p&gt; 
 &lt;p&gt;If your Poetry version is below v2.0.0, continue using &lt;code&gt;poetry shell&lt;/code&gt; to activate your environment. For further guidance, refer to the Poetry Environment Activation Guide &lt;a href="https://python-poetry.org/docs/managing-environments/#activating-the-environment"&gt;https://python-poetry.org/docs/managing-environments/#activating-the-environment&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h1&gt;‚úèÔ∏è High level architecture&lt;/h1&gt; 
&lt;h2&gt;Prowler App&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Prowler App&lt;/strong&gt; is composed of four key components:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Prowler UI&lt;/strong&gt;: A web-based interface, built with Next.js, providing a user-friendly experience for executing Prowler scans and visualizing results.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Prowler API&lt;/strong&gt;: A backend service, developed with Django REST Framework, responsible for running Prowler scans and storing the generated results.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Prowler SDK&lt;/strong&gt;: A Python SDK designed to extend the functionality of the Prowler CLI for advanced capabilities.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Prowler MCP Server&lt;/strong&gt;: A Model Context Protocol server that provides AI tools for Lighthouse, the AI-powered security assistant. This is a critical dependency for Lighthouse functionality.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/prowler-cloud/prowler/master/docs/products/img/prowler-app-architecture.png" alt="Prowler App Architecture" /&gt;&lt;/p&gt; 
&lt;h2&gt;Prowler CLI&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Running Prowler&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Prowler can be executed across various environments, offering flexibility to meet your needs. It can be run from:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Your own workstation&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;A Kubernetes Job&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Google Compute Engine&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Azure Virtual Machines (VMs)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Amazon EC2 instances&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;AWS Fargate or other container platforms&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;CloudShell&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;And many more environments.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/prowler-cloud/prowler/master/docs/img/architecture.png" alt="Architecture" /&gt;&lt;/p&gt; 
&lt;h1&gt;ü§ñ AI Skills for Development&lt;/h1&gt; 
&lt;p&gt;Prowler includes a comprehensive set of &lt;strong&gt;AI Skills&lt;/strong&gt; that help AI coding assistants understand Prowler's codebase patterns and conventions.&lt;/p&gt; 
&lt;h2&gt;What are AI Skills?&lt;/h2&gt; 
&lt;p&gt;Skills are structured instructions that give AI assistants the context they need to write code that follows Prowler's standards. They include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Coding patterns&lt;/strong&gt; for each component (SDK, API, UI, MCP Server)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Testing conventions&lt;/strong&gt; (pytest, Playwright)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Architecture guidelines&lt;/strong&gt; (Clean Architecture, RLS patterns)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Framework-specific rules&lt;/strong&gt; (React 19, Next.js 15, Django DRF, Tailwind 4)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Available Skills&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Category&lt;/th&gt; 
   &lt;th&gt;Skills&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Generic&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;typescript&lt;/code&gt;, &lt;code&gt;react-19&lt;/code&gt;, &lt;code&gt;nextjs-15&lt;/code&gt;, &lt;code&gt;tailwind-4&lt;/code&gt;, &lt;code&gt;playwright&lt;/code&gt;, &lt;code&gt;pytest&lt;/code&gt;, &lt;code&gt;django-drf&lt;/code&gt;, &lt;code&gt;zod-4&lt;/code&gt;, &lt;code&gt;zustand-5&lt;/code&gt;, &lt;code&gt;ai-sdk-5&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Prowler&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;prowler&lt;/code&gt;, &lt;code&gt;prowler-api&lt;/code&gt;, &lt;code&gt;prowler-ui&lt;/code&gt;, &lt;code&gt;prowler-mcp&lt;/code&gt;, &lt;code&gt;prowler-sdk-check&lt;/code&gt;, &lt;code&gt;prowler-test-ui&lt;/code&gt;, &lt;code&gt;prowler-test-api&lt;/code&gt;, &lt;code&gt;prowler-test-sdk&lt;/code&gt;, &lt;code&gt;prowler-compliance&lt;/code&gt;, &lt;code&gt;prowler-provider&lt;/code&gt;, &lt;code&gt;prowler-pr&lt;/code&gt;, &lt;code&gt;prowler-docs&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Setup&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;./skills/setup.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This configures skills for AI coding assistants that follow the &lt;a href="https://agentskills.io"&gt;agentskills.io&lt;/a&gt; standard:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Tool&lt;/th&gt; 
   &lt;th&gt;Configuration&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Claude Code&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;.claude/skills/&lt;/code&gt; (symlink)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;OpenCode&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;.claude/skills/&lt;/code&gt; (symlink)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Codex (OpenAI)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;.codex/skills/&lt;/code&gt; (symlink)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;GitHub Copilot&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;.github/skills/&lt;/code&gt; (symlink)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Gemini CLI&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;.gemini/skills/&lt;/code&gt; (symlink)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Restart your AI coding assistant after running setup to load the skills. Gemini CLI requires &lt;code&gt;experimental.skills&lt;/code&gt; enabled in settings.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h1&gt;üìñ Documentation&lt;/h1&gt; 
&lt;p&gt;For installation instructions, usage details, tutorials, and the Developer Guide, visit &lt;a href="https://docs.prowler.com/"&gt;https://docs.prowler.com/&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;üìÉ License&lt;/h1&gt; 
&lt;p&gt;Prowler is licensed under the Apache License 2.0.&lt;/p&gt; 
&lt;p&gt;A copy of the License is available at &lt;a href="http://www.apache.org/licenses/LICENSE-2.0"&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>camel-ai/owl</title>
      <link>https://github.com/camel-ai/owl</link>
      <description>&lt;p&gt;ü¶â OWL: Optimized Workforce Learning for General Multi-Agent Assistance in Real-World Task Automation&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
&lt;/div&gt; 
&lt;h1 align="center"&gt; ü¶â OWL: Optimized Workforce Learning for General Multi-Agent Assistance in Real-World Task Automation &lt;/h1&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://camel-ai.github.io/camel/index.html"&gt;&lt;img src="https://img.shields.io/badge/Documentation-EB3ECC" alt="Documentation" /&gt;&lt;/a&gt; &lt;a href="https://discord.camel-ai.org/"&gt;&lt;img src="https://img.shields.io/discord/1082486657678311454?logo=discord&amp;amp;labelColor=%20%235462eb&amp;amp;logoColor=%20%23f5f5f5&amp;amp;color=%20%235462eb" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://x.com/CamelAIOrg"&gt;&lt;img src="https://img.shields.io/twitter/follow/CamelAIOrg?style=social" alt="X" /&gt;&lt;/a&gt; &lt;a href="https://www.reddit.com/r/CamelAI/"&gt;&lt;img src="https://img.shields.io/reddit/subreddit-subscribers/CamelAI?style=plastic&amp;amp;logo=reddit&amp;amp;label=r%2FCAMEL&amp;amp;labelColor=white" alt="Reddit" /&gt;&lt;/a&gt; &lt;a href="https://ghli.org/camel/wechat.png"&gt;&lt;img src="https://img.shields.io/badge/WeChat-CamelAIOrg-brightgreen?logo=wechat&amp;amp;logoColor=white" alt="Wechat" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/assets/qr_code.jpg"&gt;&lt;img src="https://img.shields.io/badge/WeChat-OWLProject-brightgreen?logo=wechat&amp;amp;logoColor=white" alt="Wechat" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/camel-ai"&gt;&lt;img src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-CAMEL--AI-ffc107?color=ffc107&amp;amp;logoColor=white" alt="Hugging Face" /&gt;&lt;/a&gt; &lt;a href="https://github.com/camel-ai/owl/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/camel-ai/owl?label=stars&amp;amp;logo=github&amp;amp;color=brightgreen" alt="Star" /&gt;&lt;/a&gt; &lt;a href="https://github.com/camel-ai/owl/raw/main/licenses/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/License-Apache_2.0-blue.svg?sanitize=true" alt="Package License" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;h4 align="center"&gt; &lt;p&gt;&lt;a href="https://github.com/camel-ai/owl/tree/main/README_zh.md"&gt;‰∏≠ÊñáÈòÖËØª&lt;/a&gt; | &lt;a href="https://github.com/camel-ai/owl#community"&gt;Community&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#%EF%B8%8F-installation"&gt;Installation&lt;/a&gt; | &lt;a href="https://github.com/camel-ai/owl/tree/main/owl"&gt;Examples&lt;/a&gt; | &lt;a href="https://arxiv.org/abs/2505.23885"&gt;Paper&lt;/a&gt; | &lt;a href="https://github.com/camel-ai/owl#citation"&gt;Citation&lt;/a&gt; | &lt;a href="https://github.com/camel-ai/owl/graphs/contributors"&gt;Contributing&lt;/a&gt; | &lt;a href="https://www.camel-ai.org/"&gt;CAMEL-AI&lt;/a&gt; |&lt;/p&gt; &lt;/h4&gt; 
 &lt;div align="center" style="background-color: #f0f7ff; padding: 10px; border-radius: 5px; margin: 15px 0;"&gt; 
  &lt;h3 style="color: #1e88e5; margin: 0;"&gt; üèÜ OWL achieves &lt;span style="color: #d81b60; font-weight: bold; font-size: 1.2em;"&gt;69.09&lt;/span&gt; average score on GAIA benchmark and ranks &lt;span style="color: #d81b60; font-weight: bold; font-size: 1.2em;"&gt;üèÖÔ∏è #1&lt;/span&gt; among open-source frameworks! üèÜ &lt;/h3&gt; 
 &lt;/div&gt; 
 &lt;div align="center"&gt; 
  &lt;p&gt;ü¶â OWL is a cutting-edge framework for multi-agent collaboration that pushes the boundaries of task automation, built on top of the &lt;a href="https://github.com/camel-ai/camel"&gt;CAMEL-AI Framework&lt;/a&gt;.&lt;/p&gt; 
  &lt;p&gt;Our vision is to revolutionize how AI agents collaborate to solve real-world tasks. By leveraging dynamic agent interactions, OWL enables more natural, efficient, and robust task automation across diverse domains.&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;div align="center" style="background-color: #fff3cd; padding: 15px; border-radius: 10px; border: 3px solid #ff6b6b; margin: 20px 0;"&gt; 
  &lt;h3 style="color: #d63031; margin: 0;"&gt; ‚ÄºÔ∏è &lt;b&gt;IMPORTANT NOTICE&lt;/b&gt; ‚ÄºÔ∏è &lt;/h3&gt; 
  &lt;p style="font-size: 1.1em; margin: 10px 0; color: #2d3436;"&gt; &lt;b&gt;The current version of OWL does not use the up-to-date version of &lt;a href="https://github.com/camel-ai/camel"&gt;CAMEL&lt;/a&gt;.&lt;/b&gt;&lt;br /&gt; If you want to build the best performing agents powered by workforce, please check out &lt;a href="https://github.com/camel-ai/camel/raw/master/examples/workforce/eigent.py"&gt;&lt;b&gt;eigent.py&lt;/b&gt;&lt;/a&gt;.&lt;br /&gt; We are also working on updating CAMEL to the latest version. &lt;/p&gt; 
 &lt;/div&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/camel-ai/owl/main/assets/owl_architecture.png" alt="" /&gt;&lt;/p&gt; 
 &lt;br /&gt; 
&lt;/div&gt; 
&lt;!-- # Key Features --&gt; 
&lt;h1&gt;üìã Table of Contents&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#-table-of-contents"&gt;üìã Table of Contents&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#-news"&gt;üî• News&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#-demo-video"&gt;üé¨ Demo Video&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#%EF%B8%8F-core-features"&gt;‚ú®Ô∏è Core Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#%EF%B8%8F-installation"&gt;üõ†Ô∏è Installation&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#prerequisites"&gt;&lt;strong&gt;Prerequisites&lt;/strong&gt;&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#install-python"&gt;Install Python&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#installation-options"&gt;&lt;strong&gt;Installation Options&lt;/strong&gt;&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#option-1-using-uv-recommended"&gt;Option 1: Using uv (Recommended)&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#option-2-using-venv-and-pip"&gt;Option 2: Using venv and pip&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#option-3-using-conda"&gt;Option 3: Using conda&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#option-4-using-docker"&gt;Option 4: Using Docker&lt;/a&gt; 
      &lt;ul&gt; 
       &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#using-pre-built-image-recommended"&gt;&lt;strong&gt;Using Pre-built Image (Recommended)&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
       &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#building-image-locally"&gt;&lt;strong&gt;Building Image Locally&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
       &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#using-convenience-scripts"&gt;&lt;strong&gt;Using Convenience Scripts&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#setup-environment-variables"&gt;&lt;strong&gt;Setup Environment Variables&lt;/strong&gt;&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#setting-environment-variables-directly"&gt;Setting Environment Variables Directly&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#alternative-using-a-env-file"&gt;Alternative: Using a &lt;code&gt;.env&lt;/code&gt; File&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#mcp-desktop-commander-setup"&gt;&lt;strong&gt;MCP Desktop Commander Setup&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#-quick-start"&gt;üöÄ Quick Start&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#basic-usage"&gt;Basic Usage&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#running-with-different-models"&gt;Running with Different Models&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#model-requirements"&gt;Model Requirements&lt;/a&gt; 
      &lt;ul&gt; 
       &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#supported-models"&gt;Supported Models&lt;/a&gt;&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#example-tasks"&gt;Example Tasks&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#-toolkits-and-capabilities"&gt;üß∞ Toolkits and Capabilities&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#model-context-protocol-mcp"&gt;Model Context Protocol (MCP)&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#install-nodejs"&gt;&lt;strong&gt;Install Node.js&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#windows"&gt;Windows&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#linux"&gt;Linux&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#mac"&gt;Mac&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#install-playwright-mcp-service"&gt;&lt;strong&gt;Install Playwright MCP Service&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#available-toolkits"&gt;Available Toolkits&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#available-toolkits-1"&gt;Available Toolkits&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#multimodal-toolkits-require-multimodal-model-capabilities"&gt;Multimodal Toolkits (Require multimodal model capabilities)&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#text-based-toolkits"&gt;Text-Based Toolkits&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#customizing-your-configuration"&gt;Customizing Your Configuration&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#-web-interface"&gt;üåê Web Interface&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#starting-the-web-ui"&gt;Starting the Web UI&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#features"&gt;Features&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#-experiments"&gt;üß™ Experiments&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#%EF%B8%8F-future-plans"&gt;‚è±Ô∏è Future Plans&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#-license"&gt;üìÑ License&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#-contributing"&gt;ü§ù Contributing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#-community"&gt;üî• Community&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#-faq"&gt;‚ùì FAQ&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#general-questions"&gt;General Questions&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#experiment-questions"&gt;Experiment Questions&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#-exploring-camel-dependency"&gt;üìö Exploring CAMEL Dependency&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#accessing-camel-source-code"&gt;Accessing CAMEL Source Code&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#%EF%B8%8F-cite"&gt;üñäÔ∏è Cite&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#-star-history"&gt;‚≠ê Star History&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;üî• News&lt;/h1&gt; 
&lt;div align="center" style="background-color: #e8f5e9; padding: 15px; border-radius: 10px; border: 2px solid #4caf50; margin: 20px 0;"&gt; 
 &lt;h3 style="color: #2e7d32; margin: 0; font-size: 1.3em;"&gt; üß© &lt;b&gt;NEW: COMMUNITY AGENT CHALLENGES!&lt;/b&gt; üß© &lt;/h3&gt; 
 &lt;p style="font-size: 1.1em; margin: 10px 0;"&gt; Showcase your creativity by designing unique challenges for AI agents! &lt;br /&gt; Join our community and see your innovative ideas tackled by cutting-edge AI. &lt;/p&gt; 
 &lt;p&gt; &lt;a href="https://github.com/camel-ai/owl/raw/main/community_challenges.md" style="background-color: #2e7d32; color: white; padding: 8px 15px; text-decoration: none; border-radius: 5px; font-weight: bold;"&gt;View &amp;amp; Submit Challenges&lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;!-- &lt;div style="background-color: #e3f2fd; padding: 12px; border-radius: 8px; border-left: 4px solid #1e88e5; margin: 10px 0;"&gt;
  &lt;h4 style="color: #1e88e5; margin: 0 0 8px 0;"&gt;
    üéâ Latest Major Update - March 15, 2025
  &lt;/h4&gt;
  &lt;p style="margin: 0;"&gt;
    &lt;b&gt;Significant Improvements:&lt;/b&gt;
    &lt;ul style="margin: 5px 0 0 0; padding-left: 20px;"&gt;
      &lt;li&gt;Restructured web-based UI architecture for enhanced stability üèóÔ∏è&lt;/li&gt;
      &lt;li&gt;Optimized OWL Agent execution mechanisms for better performance üöÄ&lt;/li&gt;
    &lt;/ul&gt;
    &lt;i&gt;Try it now and experience the improved performance in your automation tasks!&lt;/i&gt;
  &lt;/p&gt;
&lt;/div&gt; --&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.09.22]&lt;/strong&gt;: Exicited to announce that OWL has been accepted by NeurIPS 2025!üöÄ Check the latest paper &lt;a href="https://arxiv.org/abs/2505.23885"&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.07.21]&lt;/strong&gt;: We open-sourced the training dataset and model checkpoints of OWL project. Training code coming soon. &lt;a href="https://huggingface.co/collections/camel-ai/optimized-workforce-learning-682ef4ab498befb9426e6e27"&gt;huggingface link&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.05.27]&lt;/strong&gt;: We released the technical report of OWL, including more details on the workforce (framework) and optimized workforce learning (training methodology). &lt;a href="https://arxiv.org/abs/2505.23885"&gt;paper&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.05.18]&lt;/strong&gt;: We open-sourced an initial version for replicating workforce experiment on GAIA &lt;a href="https://github.com/camel-ai/owl/tree/gaia69"&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.04.18]&lt;/strong&gt;: We uploaded OWL's new GAIA benchmark score of &lt;strong&gt;69.09%&lt;/strong&gt;, ranking #1 among open-source frameworks. Check the technical report &lt;a href="https://hypnotic-mind-6bd.notion.site/OWL-Optimized-Workforce-Learning-for-General-Multi-Agent-Assistance-in-Real-World-Task-Automation-1d4004aeb21380158749c7f84b20643f"&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.03.27]&lt;/strong&gt;: Integrate SearxNGToolkit performing web searches using SearxNG search engine.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.03.26]&lt;/strong&gt;: Enhanced Browser Toolkit with multi-browser support for "chrome", "msedge", and "chromium" channels.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.03.25]&lt;/strong&gt;: Supported Gemini 2.5 Pro, added example run code&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.03.21]&lt;/strong&gt;: Integrated OpenRouter model platform, fix bug with Gemini tool calling.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.03.20]&lt;/strong&gt;: Accept header in MCP Toolkit, support automatic playwright installation.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.03.16]&lt;/strong&gt;: Support Bing search, Baidu search.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.03.12]&lt;/strong&gt;: Added Bocha search in SearchToolkit, integrated Volcano Engine model platform, and enhanced Azure and OpenAI Compatible models with structured output and tool calling.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.03.11]&lt;/strong&gt;: We added MCPToolkit, FileWriteToolkit, and TerminalToolkit to enhance OWL agents with MCP tool calling, file writing capabilities, and terminal command execution.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.03.09]&lt;/strong&gt;: We added a web-based user interface that makes it easier to interact with the system.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.03.07]&lt;/strong&gt;: We open-sourced the codebase of the ü¶â OWL project.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025.03.03]&lt;/strong&gt;: OWL achieved the #1 position among open-source frameworks on the GAIA benchmark with a score of 58.18.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;üé¨ Demo Video&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/2a2a825d-39ea-45c5-9ba1-f9d58efbc372"&gt;https://github.com/user-attachments/assets/2a2a825d-39ea-45c5-9ba1-f9d58efbc372&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://private-user-images.githubusercontent.com/55657767/420212194-e813fc05-136a-485f-8df3-f10d9b4e63ec.mp4"&gt;https://private-user-images.githubusercontent.com/55657767/420212194-e813fc05-136a-485f-8df3-f10d9b4e63ec.mp4&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;This video demonstrates how to install OWL locally and showcases its capabilities as a cutting-edge framework for multi-agent collaboration: &lt;a href="https://www.youtube.com/watch?v=8XlqVyAZOr8"&gt;https://www.youtube.com/watch?v=8XlqVyAZOr8&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;‚ú®Ô∏è Core Features&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Online Search&lt;/strong&gt;: Support for multiple search engines (including Wikipedia, Google, DuckDuckGo, Baidu, Bocha, etc.) for real-time information retrieval and knowledge acquisition.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multimodal Processing&lt;/strong&gt;: Support for handling internet or local videos, images, and audio data.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Browser Automation&lt;/strong&gt;: Utilize the Playwright framework for simulating browser interactions, including scrolling, clicking, input handling, downloading, navigation, and more.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Document Parsing&lt;/strong&gt;: Extract content from Word, Excel, PDF, and PowerPoint files, converting them into text or Markdown format.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Code Execution&lt;/strong&gt;: Write and execute Python code using interpreter.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Built-in Toolkits&lt;/strong&gt;: Access to a comprehensive set of built-in toolkits including: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Model Context Protocol (MCP)&lt;/strong&gt;: A universal protocol layer that standardizes AI model interactions with various tools and data sources&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Core Toolkits&lt;/strong&gt;: ArxivToolkit, AudioAnalysisToolkit, CodeExecutionToolkit, DalleToolkit, DataCommonsToolkit, ExcelToolkit, GitHubToolkit, GoogleMapsToolkit, GoogleScholarToolkit, ImageAnalysisToolkit, MathToolkit, NetworkXToolkit, NotionToolkit, OpenAPIToolkit, RedditToolkit, SearchToolkit, SemanticScholarToolkit, SymPyToolkit, VideoAnalysisToolkit, WeatherToolkit, BrowserToolkit, and many more for specialized tasks&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;üõ†Ô∏è Installation&lt;/h1&gt; 
&lt;h2&gt;&lt;strong&gt;Prerequisites&lt;/strong&gt;&lt;/h2&gt; 
&lt;h3&gt;Install Python&lt;/h3&gt; 
&lt;p&gt;Before installing OWL, ensure you have Python installed (version 3.10, 3.11, or 3.12 is supported):&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note for GAIA Benchmark Users&lt;/strong&gt;: When running the GAIA benchmark evaluation, please use the &lt;code&gt;gaia58.18&lt;/code&gt; branch which includes a customized version of the CAMEL framework in the &lt;code&gt;owl/camel&lt;/code&gt; directory. This version contains enhanced toolkits with improved stability specifically optimized for the GAIA benchmark compared to the standard CAMEL installation.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Check if Python is installed
python --version

# If not installed, download and install from https://www.python.org/downloads/
# For macOS users with Homebrew:
brew install python@3.10

# For Ubuntu/Debian:
sudo apt update
sudo apt install python3.10 python3.10-venv python3-pip
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;&lt;strong&gt;Installation Options&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;OWL supports multiple installation methods to fit your workflow preferences.&lt;/p&gt; 
&lt;h3&gt;Option 1: Using uv (Recommended)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Clone github repo
git clone https://github.com/camel-ai/owl.git

# Change directory into project directory
cd owl

# Install uv if you don't have it already
pip install uv

# Create a virtual environment and install dependencies
uv venv .venv --python=3.10

# Activate the virtual environment
# For macOS/Linux
source .venv/bin/activate
# For Windows
.venv\Scripts\activate

# Install CAMEL with all dependencies
uv pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Option 2: Using venv and pip&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Clone github repo
git clone https://github.com/camel-ai/owl.git

# Change directory into project directory
cd owl

# Create a virtual environment
# For Python 3.10 (also works with 3.11, 3.12)
python3.10 -m venv .venv

# Activate the virtual environment
# For macOS/Linux
source .venv/bin/activate
# For Windows
.venv\Scripts\activate

# Install from requirements.txt
pip install -r requirements.txt --use-pep517
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Option 3: Using conda&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Clone github repo
git clone https://github.com/camel-ai/owl.git

# Change directory into project directory
cd owl

# Create a conda environment
conda create -n owl python=3.10

# Activate the conda environment
conda activate owl

# Option 1: Install as a package (recommended)
pip install -e .

# Option 2: Install from requirements.txt
pip install -r requirements.txt --use-pep517
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Option 4: Using Docker&lt;/h3&gt; 
&lt;h4&gt;&lt;strong&gt;Using Pre-built Image (Recommended)&lt;/strong&gt;&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# This option downloads a ready-to-use image from Docker Hub
# Fastest and recommended for most users
docker compose up -d

# Run OWL inside the container
docker compose exec owl bash
cd .. &amp;amp;&amp;amp; source .venv/bin/activate
playwright install-deps
xvfb-python examples/run.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;&lt;strong&gt;Building Image Locally&lt;/strong&gt;&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# For users who need to customize the Docker image or cannot access Docker Hub:
# 1. Open docker-compose.yml
# 2. Comment out the "image: mugglejinx/owl:latest" line
# 3. Uncomment the "build:" section and its nested properties
# 4. Then run:
docker compose up -d --build

# Run OWL inside the container
docker compose exec owl bash
cd .. &amp;amp;&amp;amp; source .venv/bin/activate
playwright install-deps
xvfb-python examples/run.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;&lt;strong&gt;Using Convenience Scripts&lt;/strong&gt;&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Navigate to container directory
cd .container

# Make the script executable and build the Docker image
chmod +x build_docker.sh
./build_docker.sh

# Run OWL with your question
./run_in_docker.sh "your question"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;&lt;strong&gt;Setup Environment Variables&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;OWL requires various API keys to interact with different services.&lt;/p&gt; 
&lt;h3&gt;Setting Environment Variables Directly&lt;/h3&gt; 
&lt;p&gt;You can set environment variables directly in your terminal:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;macOS/Linux (Bash/Zsh)&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;export OPENAI_API_KEY="your-openai-api-key-here"
# Add other required API keys as needed
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Windows (Command Prompt)&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-batch"&gt;set OPENAI_API_KEY=your-openai-api-key-here
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Windows (PowerShell)&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-powershell"&gt;$env:OPENAI_API_KEY = "your-openai-api-key-here"
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Environment variables set directly in the terminal will only persist for the current session.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Alternative: Using a &lt;code&gt;.env&lt;/code&gt; File&lt;/h3&gt; 
&lt;p&gt;If you prefer using a &lt;code&gt;.env&lt;/code&gt; file instead, you can:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Copy and Rename the Template&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# For macOS/Linux
cd owl
cp .env_template .env

# For Windows
cd owl
copy .env_template .env
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Alternatively, you can manually create a new file named &lt;code&gt;.env&lt;/code&gt; in the owl directory and copy the contents from &lt;code&gt;.env_template&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Configure Your API Keys&lt;/strong&gt;: Open the &lt;code&gt;.env&lt;/code&gt; file in your preferred text editor and insert your API keys in the corresponding fields.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: For the minimal example (&lt;code&gt;examples/run_mini.py&lt;/code&gt;), you only need to configure the LLM API key (e.g., &lt;code&gt;OPENAI_API_KEY&lt;/code&gt;).&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;&lt;strong&gt;MCP Desktop Commander Setup&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;If using MCP Desktop Commander within Docker, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npx -y @wonderwhy-er/desktop-commander setup --force-file-protocol
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more detailed Docker usage instructions, including cross-platform support, optimized configurations, and troubleshooting, please refer to &lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/.container/DOCKER_README_en.md"&gt;DOCKER_README.md&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;üöÄ Quick Start&lt;/h1&gt; 
&lt;h2&gt;Basic Usage&lt;/h2&gt; 
&lt;p&gt;After installation and setting up your environment variables, you can start using OWL right away:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python examples/run.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Running with Different Models&lt;/h2&gt; 
&lt;h3&gt;Model Requirements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Tool Calling&lt;/strong&gt;: OWL requires models with robust tool calling capabilities to interact with various toolkits. Models must be able to understand tool descriptions, generate appropriate tool calls, and process tool outputs.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Multimodal Understanding&lt;/strong&gt;: For tasks involving web interaction, image analysis, or video processing, models with multimodal capabilities are required to interpret visual content and context.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Supported Models&lt;/h4&gt; 
&lt;p&gt;For information on configuring AI models, please refer to our &lt;a href="https://docs.camel-ai.org/key_modules/models.html#supported-model-platforms-in-camel"&gt;CAMEL models documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: For optimal performance, we strongly recommend using OpenAI models (GPT-4 or later versions). Our experiments show that other models may result in significantly lower performance on complex tasks and benchmarks, especially those requiring advanced multi-modal understanding and tool use.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;OWL supports various LLM backends, though capabilities may vary depending on the model's tool calling and multimodal abilities. You can use the following scripts to run with different models:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Run with Claude model
python examples/run_claude.py

# Run with Qwen model
python examples/run_qwen_zh.py

# Run with Deepseek model
python examples/run_deepseek_zh.py

# Run with other OpenAI-compatible models
python examples/run_openai_compatible_model.py

# Run with Gemini model
python examples/run_gemini.py

# Run with Azure OpenAI
python examples/run_azure_openai.py

# Run with Ollama
python examples/run_ollama.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For a simpler version that only requires an LLM API key, you can try our minimal example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python examples/run_mini.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can run OWL agent with your own task by modifying the &lt;code&gt;examples/run.py&lt;/code&gt; script:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Define your own task
task = "Task description here."

society = construct_society(question)
answer, chat_history, token_count = run_society(society)

print(f"\033[94mAnswer: {answer}\033[0m")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For uploading files, simply provide the file path along with your question:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Task with a local file (e.g., file path: `tmp/example.docx`)
task = "What is in the given DOCX file? Here is the file path: tmp/example.docx"

society = construct_society(question)
answer, chat_history, token_count = run_society(society)
print(f"\033[94mAnswer: {answer}\033[0m")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;OWL will then automatically invoke document-related tools to process the file and extract the answer.&lt;/p&gt; 
&lt;h3&gt;Example Tasks&lt;/h3&gt; 
&lt;p&gt;Here are some tasks you can try with OWL:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;"Find the latest stock price for Apple Inc."&lt;/li&gt; 
 &lt;li&gt;"Analyze the sentiment of recent tweets about climate change"&lt;/li&gt; 
 &lt;li&gt;"Help me debug this Python code: [your code here]"&lt;/li&gt; 
 &lt;li&gt;"Summarize the main points from this research paper: [paper URL]"&lt;/li&gt; 
 &lt;li&gt;"Create a data visualization for this dataset: [dataset path]"&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;üß∞ Toolkits and Capabilities&lt;/h1&gt; 
&lt;h2&gt;Model Context Protocol (MCP)&lt;/h2&gt; 
&lt;p&gt;OWL's MCP integration provides a standardized way for AI models to interact with various tools and data sources:&lt;/p&gt; 
&lt;p&gt;Before using MCP, you need to install Node.js first.&lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;Install Node.js&lt;/strong&gt;&lt;/h3&gt; 
&lt;h3&gt;Windows&lt;/h3&gt; 
&lt;p&gt;Download the official installer: &lt;a href="https://nodejs.org/en"&gt;Node.js&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Check "Add to PATH" option during installation.&lt;/p&gt; 
&lt;h3&gt;Linux&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo apt update
sudo apt install nodejs npm -y
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Mac&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;brew install node
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;&lt;strong&gt;Install Playwright MCP Service&lt;/strong&gt;&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npm install -g @executeautomation/playwright-mcp-server
npx playwright install-deps
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Try our comprehensive MCP examples:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;examples/run_mcp.py&lt;/code&gt; - Basic MCP functionality demonstration (local call, requires dependencies)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;examples/run_mcp_sse.py&lt;/code&gt; - Example using the SSE protocol (Use remote services, no dependencies)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Available Toolkits&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: Effective use of toolkits requires models with strong tool calling capabilities. For multimodal toolkits (Web, Image, Video), models must also have multimodal understanding abilities.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;OWL supports various toolkits that can be customized by modifying the &lt;code&gt;tools&lt;/code&gt; list in your script:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Configure toolkits
tools = [
    *BrowserToolkit(headless=False).get_tools(),  # Browser automation
    *VideoAnalysisToolkit(model=models["video"]).get_tools(),
    *AudioAnalysisToolkit().get_tools(),  # Requires OpenAI Key
    *CodeExecutionToolkit(sandbox="subprocess").get_tools(),
    *ImageAnalysisToolkit(model=models["image"]).get_tools(),
    SearchToolkit().search_duckduckgo,
    SearchToolkit().search_google,  # Comment out if unavailable
    SearchToolkit().search_wiki,
    SearchToolkit().search_bocha,
    SearchToolkit().search_baidu,
    *ExcelToolkit().get_tools(),
    *DocumentProcessingToolkit(model=models["document"]).get_tools(),
    *FileWriteToolkit(output_dir="./").get_tools(),
]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Available Toolkits&lt;/h2&gt; 
&lt;p&gt;Key toolkits include:&lt;/p&gt; 
&lt;h3&gt;Multimodal Toolkits (Require multimodal model capabilities)&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;BrowserToolkit&lt;/strong&gt;: Browser automation for web interaction and navigation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;VideoAnalysisToolkit&lt;/strong&gt;: Video processing and content analysis&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ImageAnalysisToolkit&lt;/strong&gt;: Image analysis and interpretation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Text-Based Toolkits&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;AudioAnalysisToolkit&lt;/strong&gt;: Audio processing (requires OpenAI API)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;CodeExecutionToolkit&lt;/strong&gt;: Python code execution and evaluation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;SearchToolkit&lt;/strong&gt;: Web searches (Google, DuckDuckGo, Wikipedia)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;DocumentProcessingToolkit&lt;/strong&gt;: Document parsing (PDF, DOCX, etc.)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Additional specialized toolkits: ArxivToolkit, GitHubToolkit, GoogleMapsToolkit, MathToolkit, NetworkXToolkit, NotionToolkit, RedditToolkit, WeatherToolkit, and more. For a complete list, see the &lt;a href="https://docs.camel-ai.org/key_modules/tools.html#built-in-toolkits"&gt;CAMEL toolkits documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Customizing Your Configuration&lt;/h2&gt; 
&lt;p&gt;To customize available tools:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# 1. Import toolkits
from camel.toolkits import BrowserToolkit, SearchToolkit, CodeExecutionToolkit

# 2. Configure tools list
tools = [
    *BrowserToolkit(headless=True).get_tools(),
    SearchToolkit().search_wiki,
    *CodeExecutionToolkit(sandbox="subprocess").get_tools(),
]

# 3. Pass to assistant agent
assistant_agent_kwargs = {"model": models["assistant"], "tools": tools}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Selecting only necessary toolkits optimizes performance and reduces resource usage.&lt;/p&gt; 
&lt;h1&gt;üåê Web Interface&lt;/h1&gt; 
&lt;div align="center" style="background-color: #f0f7ff; padding: 15px; border-radius: 10px; border: 2px solid #1e88e5; margin: 20px 0;"&gt; 
 &lt;h3 style="color: #1e88e5; margin: 0;"&gt; üöÄ Enhanced Web Interface Now Available! &lt;/h3&gt; 
 &lt;p style="margin: 10px 0;"&gt; Experience improved system stability and optimized performance with our latest update. Start exploring the power of OWL through our user-friendly interface! &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;Starting the Web UI&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Start the Chinese version
python owl/webapp_zh.py

# Start the English version
python owl/webapp.py

# Start the Japanese version
python owl/webapp_jp.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Easy Model Selection&lt;/strong&gt;: Choose between different models (OpenAI, Qwen, DeepSeek, etc.)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Environment Variable Management&lt;/strong&gt;: Configure your API keys and other settings directly from the UI&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Interactive Chat Interface&lt;/strong&gt;: Communicate with OWL agents through a user-friendly interface&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Task History&lt;/strong&gt;: View the history and results of your interactions&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The web interface is built using Gradio and runs locally on your machine. No data is sent to external servers beyond what's required for the model API calls you configure.&lt;/p&gt; 
&lt;h1&gt;üß™ Experiments&lt;/h1&gt; 
&lt;p&gt;To reproduce OWL's GAIA benchmark score: Furthermore, to ensure optimal performance on the GAIA benchmark, please note that our &lt;code&gt;gaia69&lt;/code&gt; branch includes a customized version of the CAMEL framework in the &lt;code&gt;owl/camel&lt;/code&gt; directory. This version contains enhanced toolkits with improved stability for gaia benchmark compared to the standard CAMEL installation.&lt;/p&gt; 
&lt;p&gt;When running the benchmark evaluation:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Switch to the &lt;code&gt;gaia69&lt;/code&gt; branch:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git checkout gaia69
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run the evaluation script:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;python run_gaia_workforce_claude.py
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;This will execute the same configuration that achieved our top-ranking performance on the GAIA benchmark.&lt;/p&gt; 
&lt;h1&gt;‚è±Ô∏è Future Plans&lt;/h1&gt; 
&lt;p&gt;We're continuously working to improve OWL. Here's what's on our roadmap:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Write a technical blog post detailing our exploration and insights in multi-agent collaboration in real-world tasks&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Enhance the toolkit ecosystem with more specialized tools for domain-specific tasks&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Develop more sophisticated agent interaction patterns and communication protocols&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Improve performance on complex multi-step reasoning tasks&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;üìÑ License&lt;/h1&gt; 
&lt;p&gt;The source code is licensed under Apache 2.0.&lt;/p&gt; 
&lt;h1&gt;ü§ù Contributing&lt;/h1&gt; 
&lt;p&gt;We welcome contributions from the community! Here's how you can help:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Read our &lt;a href="https://github.com/camel-ai/camel/raw/master/CONTRIBUTING.md"&gt;Contribution Guidelines&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Check &lt;a href="https://github.com/camel-ai/camel/issues"&gt;open issues&lt;/a&gt; or create new ones&lt;/li&gt; 
 &lt;li&gt;Submit pull requests with your improvements&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Current Issues Open for Contribution:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/camel-ai/camel/issues/1915"&gt;#1915&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/camel-ai/camel/issues/2190"&gt;#2190&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/camel-ai/camel/issues/2165"&gt;#2165&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/camel-ai/camel/issues/2121"&gt;#2121&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/camel-ai/camel/issues/1908"&gt;#1908&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/camel-ai/camel/issues/1538"&gt;#1538&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/camel-ai/camel/issues/1481"&gt;#1481&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To take on an issue, simply leave a comment stating your interest.&lt;/p&gt; 
&lt;h1&gt;üî• Community&lt;/h1&gt; 
&lt;p&gt;Join us (&lt;a href="https://discord.camel-ai.org/"&gt;&lt;em&gt;Discord&lt;/em&gt;&lt;/a&gt; or &lt;a href="https://ghli.org/camel/wechat.png"&gt;&lt;em&gt;WeChat&lt;/em&gt;&lt;/a&gt;) in pushing the boundaries of finding the scaling laws of agents.&lt;/p&gt; 
&lt;p&gt;Join us for further discussions!&lt;/p&gt; 
&lt;!-- ![](./assets/community.png) --&gt; 
&lt;img src="https://raw.githubusercontent.com/camel-ai/owl/main/assets/community_code.jpeg" width="50%" /&gt; 
&lt;h1&gt;‚ùì FAQ&lt;/h1&gt; 
&lt;h2&gt;General Questions&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Q: Why don't I see Chrome running locally after starting the example script?&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;A: If OWL determines that a task can be completed using non-browser tools (such as search or code execution), the browser will not be launched. The browser window will only appear when OWL determines that browser-based interaction is necessary.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Q: Which Python version should I use?&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;A: OWL supports Python 3.10, 3.11, and 3.12.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Q: How can I contribute to the project?&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;A: See our &lt;a href="https://raw.githubusercontent.com/camel-ai/owl/main/#-contributing"&gt;Contributing&lt;/a&gt; section for details on how to get involved. We welcome contributions of all kinds, from code improvements to documentation updates.&lt;/p&gt; 
&lt;h2&gt;Experiment Questions&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Q: Which CAMEL version should I use for replicate the role playing result?&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;A: We provide a modified version of CAMEL (owl/camel) in the gaia58.18 branch. Please make sure you use this CAMEL version for your experiments.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Q: Why are my experiment results lower than the reported numbers?&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;A: Since the GAIA benchmark evaluates LLM agents in a realistic world, it introduces a significant amount of randomness. Based on user feedback, one of the most common issues for replication is, for example, agents being blocked on certain webpages due to network reasons. We have uploaded a keywords matching script to help quickly filter out these errors &lt;a href="https://github.com/camel-ai/owl/raw/gaia58.18/owl/filter_failed_cases.py"&gt;here&lt;/a&gt;. You can also check this &lt;a href="https://hypnotic-mind-6bd.notion.site/OWL-Optimized-Workforce-Learning-for-General-Multi-Agent-Assistance-in-Real-World-Task-Automation-1d4004aeb21380158749c7f84b20643f?pvs=74"&gt;technical report&lt;/a&gt; for more details when evaluating LLM agents in realistic open-world environments.&lt;/p&gt; 
&lt;h1&gt;üìö Exploring CAMEL Dependency&lt;/h1&gt; 
&lt;p&gt;OWL is built on top of the &lt;a href="https://github.com/camel-ai/camel"&gt;CAMEL&lt;/a&gt; Framework, here's how you can explore the CAMEL source code and understand how it works with OWL:&lt;/p&gt; 
&lt;h2&gt;Accessing CAMEL Source Code&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Clone the CAMEL repository
git clone https://github.com/camel-ai/camel.git
cd camel
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;üñäÔ∏è Cite&lt;/h1&gt; 
&lt;p&gt;If you find this repo useful, please cite:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@misc{hu2025owl,
      title={OWL: Optimized Workforce Learning for General Multi-Agent Assistance in Real-World Task Automation}, 
      author={Mengkang Hu and Yuhang Zhou and Wendong Fan and Yuzhou Nie and Bowei Xia and Tao Sun and Ziyu Ye and Zhaoxuan Jin and Yingru Li and Qiguang Chen and Zeyu Zhang and Yifeng Wang and Qianshuo Ye and Bernard Ghanem and Ping Luo and Guohao Li},
      year={2025},
      eprint={2505.23885},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2505.23885}, 
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;‚≠ê Star History&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#camel-ai/owl&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=camel-ai/owl&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ml-explore/mlx-examples</title>
      <link>https://github.com/ml-explore/mlx-examples</link>
      <description>&lt;p&gt;Examples in the MLX framework&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MLX Examples&lt;/h1&gt; 
&lt;p&gt;This repo contains a variety of standalone examples using the &lt;a href="https://github.com/ml-explore/mlx"&gt;MLX framework&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The &lt;a href="https://raw.githubusercontent.com/ml-explore/mlx-examples/main/mnist"&gt;MNIST&lt;/a&gt; example is a good starting point to learn how to use MLX. Some more useful examples are listed below. Check-out &lt;a href="https://github.com/ml-explore/mlx-lm"&gt;MLX LM&lt;/a&gt; for a more fully featured Python package for LLMs with MLX.&lt;/p&gt; 
&lt;h3&gt;Text Models&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ml-explore/mlx-examples/main/transformer_lm"&gt;Transformer language model&lt;/a&gt; training.&lt;/li&gt; 
 &lt;li&gt;Minimal examples of large scale text generation with &lt;a href="https://raw.githubusercontent.com/ml-explore/mlx-examples/main/llms/llama"&gt;LLaMA&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/ml-explore/mlx-examples/main/llms/mistral"&gt;Mistral&lt;/a&gt;, and more in the &lt;a href="https://raw.githubusercontent.com/ml-explore/mlx-examples/main/llms"&gt;LLMs&lt;/a&gt; directory.&lt;/li&gt; 
 &lt;li&gt;A mixture-of-experts (MoE) language model with &lt;a href="https://raw.githubusercontent.com/ml-explore/mlx-examples/main/llms/mixtral"&gt;Mixtral 8x7B&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Parameter efficient fine-tuning with &lt;a href="https://raw.githubusercontent.com/ml-explore/mlx-examples/main/lora"&gt;LoRA or QLoRA&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Text-to-text multi-task Transformers with &lt;a href="https://raw.githubusercontent.com/ml-explore/mlx-examples/main/t5"&gt;T5&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Bidirectional language understanding with &lt;a href="https://raw.githubusercontent.com/ml-explore/mlx-examples/main/bert"&gt;BERT&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Image Models&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Generating images 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ml-explore/mlx-examples/main/flux"&gt;FLUX&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ml-explore/mlx-examples/main/stable_diffusion"&gt;Stable Diffusion or SDXL&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Image classification using &lt;a href="https://raw.githubusercontent.com/ml-explore/mlx-examples/main/cifar"&gt;ResNets on CIFAR-10&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Convolutional variational autoencoder &lt;a href="https://raw.githubusercontent.com/ml-explore/mlx-examples/main/cvae"&gt;(CVAE) on MNIST&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Audio Models&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Speech recognition with &lt;a href="https://raw.githubusercontent.com/ml-explore/mlx-examples/main/whisper"&gt;OpenAI's Whisper&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Audio compression and generation with &lt;a href="https://raw.githubusercontent.com/ml-explore/mlx-examples/main/encodec"&gt;Meta's EnCodec&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Music generation with &lt;a href="https://raw.githubusercontent.com/ml-explore/mlx-examples/main/musicgen"&gt;Meta's MusicGen&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Multimodal models&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Joint text and image embeddings with &lt;a href="https://raw.githubusercontent.com/ml-explore/mlx-examples/main/clip"&gt;CLIP&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Text generation from image and text inputs with &lt;a href="https://raw.githubusercontent.com/ml-explore/mlx-examples/main/llava"&gt;LLaVA&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Image segmentation with &lt;a href="https://raw.githubusercontent.com/ml-explore/mlx-examples/main/segment_anything"&gt;Segment Anything (SAM)&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Other Models&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Semi-supervised learning on graph-structured data with &lt;a href="https://raw.githubusercontent.com/ml-explore/mlx-examples/main/gcn"&gt;GCN&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Real NVP &lt;a href="https://raw.githubusercontent.com/ml-explore/mlx-examples/main/normalizing_flow"&gt;normalizing flow&lt;/a&gt; for density estimation and sampling.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Hugging Face&lt;/h3&gt; 
&lt;p&gt;You can directly use or download converted checkpoints from the &lt;a href="https://huggingface.co/mlx-community"&gt;MLX Community&lt;/a&gt; organization on Hugging Face. We encourage you to join the community and &lt;a href="https://github.com/ml-explore/mlx-examples/issues/155"&gt;contribute new models&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We are grateful for all of &lt;a href="https://raw.githubusercontent.com/ml-explore/mlx-examples/main/ACKNOWLEDGMENTS.md#Individual-Contributors"&gt;our contributors&lt;/a&gt;. If you contribute to MLX Examples and wish to be acknowledged, please add your name to the list in your pull request.&lt;/p&gt; 
&lt;h2&gt;Citing MLX Examples&lt;/h2&gt; 
&lt;p&gt;The MLX software suite was initially developed with equal contribution by Awni Hannun, Jagrit Digani, Angelos Katharopoulos, and Ronan Collobert. If you find MLX Examples useful in your research and wish to cite it, please use the following BibTex entry:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@software{mlx2023,
  author = {Awni Hannun and Jagrit Digani and Angelos Katharopoulos and Ronan Collobert},
  title = {{MLX}: Efficient and flexible machine learning on Apple silicon},
  url = {https://github.com/ml-explore},
  version = {0.0},
  year = {2023},
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>neuphonic/neutts</title>
      <link>https://github.com/neuphonic/neutts</link>
      <description>&lt;p&gt;On-device TTS model by Neuphonic&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;NeuTTS&lt;/h1&gt; 
&lt;p&gt;HuggingFace ü§ó:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;NeuTTS-Air: &lt;a href="https://huggingface.co/neuphonic/neutts-air"&gt;Model&lt;/a&gt;, &lt;a href="https://huggingface.co/neuphonic/neutts-air-q8-gguf"&gt;Q8 GGUF&lt;/a&gt;, &lt;a href="https://huggingface.co/neuphonic/neutts-air-q4-gguf"&gt;Q4 GGUF&lt;/a&gt;, &lt;a href="https://huggingface.co/spaces/neuphonic/neutts-air"&gt;Spaces&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;NeuTTS-Nano: &lt;a href="https://huggingface.co/neuphonic/neutts-nano"&gt;Model&lt;/a&gt;, &lt;a href="https://huggingface.co/neuphonic/neutts-nano-q8-gguf"&gt;Q8 GGUF&lt;/a&gt;, &lt;a href="https://huggingface.co/neuphonic/neutts-nano-q4-gguf"&gt;Q4 GGUF&lt;/a&gt;, &lt;a href="https://huggingface.co/spaces/neuphonic/neutts-nano"&gt;Spaces&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/629ec5b2-4818-4fa6-987a-99fcbadc56bc"&gt;NeuTTS-Nano Demo Video&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Created by &lt;a href="http://neuphonic.com/"&gt;Neuphonic&lt;/a&gt; - building faster, smaller, on-device voice AI&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;State-of-the-art Voice AI has been locked behind web APIs for too long. NeuTTS is a collection of open source, on-device, TTS speech language models with instant voice cloning. Built off of LLM backbones, NeuTTS brings natural-sounding speech, real-time performance, built-in security and speaker cloning to your local device - unlocking a new category of embedded voice agents, assistants, toys, and compliance-safe apps.&lt;/p&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;üó£Best-in-class realism for their size - produce natural, ultra-realistic voices that sound human, at the sweet spot between speed, size, and quality for real-world applications&lt;/li&gt; 
 &lt;li&gt;üì±Optimised for on-device deployment - provided in GGML format, ready to run on phones, laptops, or even Raspberry Pis&lt;/li&gt; 
 &lt;li&gt;üë´Instant voice cloning - create your own speaker with as little as 3 seconds of audio&lt;/li&gt; 
 &lt;li&gt;üöÑSimple LM + codec architecture - making development and deployment simple&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!CAUTION] Websites like neutts.com are popping up and they're not affliated with Neuphonic, our github or this repo.&lt;/p&gt; 
 &lt;p&gt;We are on neuphonic.com only. Please be careful out there! üôè&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Model Details&lt;/h2&gt; 
&lt;p&gt;NeuTTS models are built from small LLM backbones - lightweight yet capable language models optimised for text understanding and generation - as well as a powerful combination of technologies designed for efficiency and quality:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Supported Languages&lt;/strong&gt;: English&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Audio Codec&lt;/strong&gt;: &lt;a href="https://huggingface.co/neuphonic/neucodec"&gt;NeuCodec&lt;/a&gt; - our 50hz neural audio codec that achieves exceptional audio quality at low bitrates using a single codebook&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Context Window&lt;/strong&gt;: 2048 tokens, enough for processing ~30 seconds of audio (including prompt duration)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Format&lt;/strong&gt;: Available in GGML format for efficient on-device inference&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Responsibility&lt;/strong&gt;: Watermarked outputs&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Inference Speed&lt;/strong&gt;: Real-time generation on mid-range devices&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Power Consumption&lt;/strong&gt;: Optimised for mobile and embedded devices&lt;/li&gt; 
&lt;/ul&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th align="right"&gt;NeuTTSAir&lt;/th&gt; 
   &lt;th align="right"&gt;NeuTTSNano&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;# Params (Active)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="right"&gt;~360m&lt;/td&gt; 
   &lt;td align="right"&gt;~120m&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;# Params (Emb + Active)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="right"&gt;~552m&lt;/td&gt; 
   &lt;td align="right"&gt;~229m&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Cloning&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="right"&gt;Yes&lt;/td&gt; 
   &lt;td align="right"&gt;Yes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;License&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="right"&gt;Apache 2.0&lt;/td&gt; 
   &lt;td align="right"&gt;NeuTTS Open License 1.0&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Throughput Benchmarking&lt;/h2&gt; 
&lt;p&gt;The two models were benchmarked using the Q4 quantisations &lt;a href="https://huggingface.co/neuphonic/neutts-air-q4-gguf"&gt;neutts-air-Q4-0&lt;/a&gt; and &lt;a href="https://huggingface.co/neuphonic/neutts-nano-q4-gguf"&gt;neutts-nano-Q4-0&lt;/a&gt;. Benchmarks on CPU were run through llama-bench (llama.cpp) to measure prefill and decode throughput at multiple context sizes.&lt;/p&gt; 
&lt;p&gt;For GPU's (specifically RTX 4090), we leverage vLLM to maximise throughput. We run benchmarks using the &lt;a href="https://docs.vllm.ai/en/stable/cli/bench/throughput/"&gt;vLLM benchmark&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;We include benchmarks on four devices: Galaxy A25 5G, AMD Ryzen 9HX 370, iMac M4 16GB, NVIDIA GeForce RTX 4090.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th align="right"&gt;NeuTTSAir&lt;/th&gt; 
   &lt;th align="right"&gt;NeuTTSNano&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Galaxy A25 5G (CPU only)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="right"&gt;20 tokens/s&lt;/td&gt; 
   &lt;td align="right"&gt;45 tokens/s&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;AMD Ryzen 9 HX 370 (CPU only)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="right"&gt;119 tokens/s&lt;/td&gt; 
   &lt;td align="right"&gt;221 tokens/s&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;iMAc M4 16 GB (CPU only)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="right"&gt;111 tokens/s&lt;/td&gt; 
   &lt;td align="right"&gt;195 tokens/s&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;RTX 4090&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="right"&gt;16194 tokens/s&lt;/td&gt; 
   &lt;td align="right"&gt;19268 tokens/s&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] llama-bench used 14 threads for prefill and 16 threads for decode (as configured in the benchmark run) on AMD Ryzen 9HX 370 and iMac M4 16GB, and 6 threads for each on the Galaxy A25 5G. The tokens/s reported are when having 500 prefill tokens and generating 250 output tokens.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Please note that these benchmarks only include the Speech Language Model and do not include the Codec which is needed for a full audio generation pipeline.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Get Started with NeuTTS&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] We have added a &lt;a href="https://raw.githubusercontent.com/neuphonic/neutts/main/examples/basic_streaming_example.py"&gt;streaming example&lt;/a&gt; using the &lt;code&gt;llama-cpp-python&lt;/code&gt; library as well as a &lt;a href="https://raw.githubusercontent.com/neuphonic/neutts/main/examples/finetune.py"&gt;finetuning script&lt;/a&gt;. For finetuning, please refer to the &lt;a href="https://raw.githubusercontent.com/neuphonic/neutts/main/TRAINING.md"&gt;finetune guide&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Clone Git Repo&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/neuphonic/neutts.git
cd neutts
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install &lt;code&gt;espeak&lt;/code&gt; (required dependency)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Please refer to the following link for instructions on how to install &lt;code&gt;espeak&lt;/code&gt;:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/espeak-ng/espeak-ng/raw/master/docs/guide.md"&gt;https://github.com/espeak-ng/espeak-ng/blob/master/docs/guide.md&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# Mac OS
brew install espeak-ng

# Ubuntu/Debian
sudo apt install espeak-ng

# Windows install
# via chocolatey (https://community.chocolatey.org/packages?page=1&amp;amp;prerelease=False&amp;amp;moderatorQueue=False&amp;amp;tags=espeak)
choco install espeak-ng
# via wingit
winget install -e --id eSpeak-NG.eSpeak-NG
# via msi (need to add to path or folow the "Windows users who installed via msi" below)
# find the msi at https://github.com/espeak-ng/espeak-ng/releases
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Windows users who installed via msi / do not have their install on path need to run the following (see &lt;a href="https://github.com/bootphon/phonemizer/issues/163"&gt;https://github.com/bootphon/phonemizer/issues/163&lt;/a&gt;)&lt;/p&gt; &lt;pre&gt;&lt;code class="language-pwsh"&gt;$env:PHONEMIZER_ESPEAK_LIBRARY = "c:\Program Files\eSpeak NG\libespeak-ng.dll"
$env:PHONEMIZER_ESPEAK_PATH = "c:\Program Files\eSpeak NG"
setx PHONEMIZER_ESPEAK_LIBRARY "c:\Program Files\eSpeak NG\libespeak-ng.dll"
setx PHONEMIZER_ESPEAK_PATH "c:\Program Files\eSpeak NG"
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install Python dependencies&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The requirements file includes the dependencies needed to run the model with PyTorch. When using an ONNX decoder or a GGML model, some dependencies (such as PyTorch) are no longer required.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;[!CAUTION] The inference is compatible and tested on &lt;code&gt;python"&amp;gt;=3.11, &amp;lt;=3.13"&lt;/code&gt;. This is restricted due to pytorch compatibility. &lt;a href="https://github.com/pytorch/pytorch/raw/main/RELEASE.md#release-compatibility-matrix"&gt;PyTorch Compatibility Matrix&lt;/a&gt;&lt;/p&gt; 
  &lt;/blockquote&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;(Optional) Install Llama-cpp-python to use the &lt;code&gt;GGUF&lt;/code&gt; models.&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pip install llama-cpp-python
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To run llama-cpp with GPU suport (CUDA, MPS) support please refer to: &lt;a href="https://pypi.org/project/llama-cpp-python/"&gt;https://pypi.org/project/llama-cpp-python/&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;(Optional) Install onnxruntime to use the &lt;code&gt;.onnx&lt;/code&gt; decoder.&lt;/strong&gt; If you want to run the onnxdecoder&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pip install onnxruntime
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Running the Model&lt;/h2&gt; 
&lt;p&gt;Run the basic example script to synthesize speech:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m examples.basic_example \
  --input_text "My name is Andy. I'm 25 and I just moved to London. The underground is pretty confusing, but it gets me around in no time at all." \
  --ref_audio samples/jo.wav \
  --ref_text samples/jo.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To specify a particular model repo for the backbone or codec, add the &lt;code&gt;--backbone&lt;/code&gt; argument. Available backbones are listed in &lt;a href="https://huggingface.co/collections/neuphonic/neutts-air"&gt;NeuTTS-Air&lt;/a&gt; and &lt;a href="https://huggingface.co/collections/neuphonic/neutts-nano"&gt;NeuTTS-Nano&lt;/a&gt; huggingface collections.&lt;/p&gt; 
&lt;p&gt;Several examples are available, including a Jupyter notebook in the &lt;code&gt;examples&lt;/code&gt; folder.&lt;/p&gt; 
&lt;h3&gt;One-Code Block Usage&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from neutts import NeuTTS
import soundfile as sf

tts = NeuTTS(
   backbone_repo="neuphonic/neutts-nano", # or 'neutts-nano-q4-gguf' with llama-cpp-python installed
   backbone_device="cpu",
   codec_repo="neuphonic/neucodec",
   codec_device="cpu"
)
input_text = "My name is Andy. I'm 25 and I just moved to London. The underground is pretty confusing, but it gets me around in no time at all."

ref_text = "samples/jo.txt"
ref_audio_path = "samples/jo.wav"

ref_text = open(ref_text, "r").read().strip()
ref_codes = tts.encode_reference(ref_audio_path)

wav = tts.infer(input_text, ref_codes, ref_text)
sf.write("test.wav", wav, 24000)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Streaming&lt;/h3&gt; 
&lt;p&gt;Speech can also be synthesised in &lt;em&gt;streaming mode&lt;/em&gt;, where audio is generated in chunks and plays as generated. Note that this requires pyaudio to be installed. To do this, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m examples.basic_streaming_example \
  --input_text "My name is Andy. I'm 25 and I just moved to London. The underground is pretty confusing, but it gets me around in no time at all." \
  --ref_codes samples/jo.pt \
  --ref_text samples/jo.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Again, a particular model repo can be specified with the &lt;code&gt;--backbone&lt;/code&gt; argument - note that for streaming the model must be in GGUF format.&lt;/p&gt; 
&lt;h2&gt;Preparing References for Cloning&lt;/h2&gt; 
&lt;p&gt;NeuTTS requires two inputs:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;A reference audio sample (&lt;code&gt;.wav&lt;/code&gt; file)&lt;/li&gt; 
 &lt;li&gt;A text string&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;The model then synthesises the text as speech in the style of the reference audio. This is what enables NeuTTS models instant voice cloning capability.&lt;/p&gt; 
&lt;h3&gt;Example Reference Files&lt;/h3&gt; 
&lt;p&gt;You can find some ready-to-use samples in the &lt;code&gt;examples&lt;/code&gt; folder:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;samples/dave.wav&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;samples/jo.wav&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Guidelines for Best Results&lt;/h3&gt; 
&lt;p&gt;For optimal performance, reference audio samples should be:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Mono channel&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;16-44 kHz sample rate&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;3‚Äì15 seconds in length&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Saved as a &lt;code&gt;.wav&lt;/code&gt; file&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Clean&lt;/strong&gt; ‚Äî minimal to no background noise&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Natural, continuous speech&lt;/strong&gt; ‚Äî like a monologue or conversation, with few pauses, so the model can capture tone effectively&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Guidelines for minimizing Latency&lt;/h2&gt; 
&lt;p&gt;For optimal performance on-device:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Use the GGUF model backbones&lt;/li&gt; 
 &lt;li&gt;Pre-encode references&lt;/li&gt; 
 &lt;li&gt;Use the &lt;a href="https://huggingface.co/neuphonic/neucodec-onnx-decoder"&gt;onnx codec decoder&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Take a look at this example &lt;a href="examples/README.md###minimal-latency-example"&gt;examples README&lt;/a&gt; to get started.&lt;/p&gt; 
&lt;h2&gt;Responsibility&lt;/h2&gt; 
&lt;p&gt;Every audio file generated by NeuTTS includes &lt;a href="https://github.com/resemble-ai/perth"&gt;Perth (Perceptual Threshold) Watermarker&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Disclaimer&lt;/h2&gt; 
&lt;p&gt;Don't use this model to do bad things‚Ä¶ please.&lt;/p&gt; 
&lt;h2&gt;Developer Requirements&lt;/h2&gt; 
&lt;p&gt;To run the pre commit hooks to contribute to this project run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install pre-commit
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pre-commit install
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Running Tests&lt;/h2&gt; 
&lt;p&gt;First, install the dev requirements:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install -r requirements-dev.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To run the tests:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pytest tests/
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>ml-explore/mlx-lm</title>
      <link>https://github.com/ml-explore/mlx-lm</link>
      <description>&lt;p&gt;Run LLMs with MLX&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;MLX LM&lt;/h2&gt; 
&lt;p&gt;MLX LM is a Python package for generating text and fine-tuning large language models on Apple silicon with MLX.&lt;/p&gt; 
&lt;p&gt;Some key features include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Integration with the Hugging Face Hub to easily use thousands of LLMs with a single command.&lt;/li&gt; 
 &lt;li&gt;Support for quantizing and uploading models to the Hugging Face Hub.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ml-explore/mlx-lm/raw/main/mlx_lm/LORA.md"&gt;Low-rank and full model fine-tuning&lt;/a&gt; with support for quantized models.&lt;/li&gt; 
 &lt;li&gt;Distributed inference and fine-tuning with &lt;code&gt;mx.distributed&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The easiest way to get started is to install the &lt;code&gt;mlx-lm&lt;/code&gt; package:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;With &lt;code&gt;pip&lt;/code&gt;&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;pip install mlx-lm
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;With &lt;code&gt;conda&lt;/code&gt;&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;conda install -c conda-forge mlx-lm
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Quick Start&lt;/h3&gt; 
&lt;p&gt;To generate text with an LLM use:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;mlx_lm.generate --prompt "How tall is Mt Everest?"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To chat with an LLM use:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;mlx_lm.chat
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will give you a chat REPL that you can use to interact with the LLM. The chat context is preserved during the lifetime of the REPL.&lt;/p&gt; 
&lt;p&gt;Commands in &lt;code&gt;mlx-lm&lt;/code&gt; typically take command line options which let you specify the model, sampling parameters, and more. Use &lt;code&gt;-h&lt;/code&gt; to see a list of available options for a command, e.g.:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;mlx_lm.generate -h
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The default model for generation and chat is &lt;code&gt;mlx-community/Llama-3.2-3B-Instruct-4bit&lt;/code&gt;. You can specify any MLX-compatible model with the &lt;code&gt;--model&lt;/code&gt; flag. Thousands are available in the &lt;a href="https://huggingface.co/mlx-community"&gt;MLX Community&lt;/a&gt; Hugging Face organization.&lt;/p&gt; 
&lt;h3&gt;Python API&lt;/h3&gt; 
&lt;p&gt;You can use &lt;code&gt;mlx-lm&lt;/code&gt; as a module:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mlx_lm import load, generate

model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

prompt = "Write a story about Einstein"

messages = [{"role": "user", "content": prompt}]
prompt = tokenizer.apply_chat_template(
    messages, add_generation_prompt=True,
)

text = generate(model, tokenizer, prompt=prompt, verbose=True)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To see a description of all the arguments you can do:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; help(generate)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Check out the &lt;a href="https://github.com/ml-explore/mlx-lm/tree/main/mlx_lm/examples/generate_response.py"&gt;generation example&lt;/a&gt; to see how to use the API in more detail. Check out the &lt;a href="https://github.com/ml-explore/mlx-lm/tree/main/mlx_lm/examples/batch_generate_response.py"&gt;batch generation example&lt;/a&gt; to see how to efficiently generate continuations for a batch of prompts.&lt;/p&gt; 
&lt;p&gt;The &lt;code&gt;mlx-lm&lt;/code&gt; package also comes with functionality to quantize and optionally upload models to the Hugging Face Hub.&lt;/p&gt; 
&lt;p&gt;You can convert models using the Python API:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mlx_lm import convert

repo = "mistralai/Mistral-7B-Instruct-v0.3"
upload_repo = "mlx-community/My-Mistral-7B-Instruct-v0.3-4bit"

convert(repo, quantize=True, upload_repo=upload_repo)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will generate a 4-bit quantized Mistral 7B and upload it to the repo &lt;code&gt;mlx-community/My-Mistral-7B-Instruct-v0.3-4bit&lt;/code&gt;. It will also save the converted model in the path &lt;code&gt;mlx_model&lt;/code&gt; by default.&lt;/p&gt; 
&lt;p&gt;To see a description of all the arguments you can do:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; help(convert)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Streaming&lt;/h4&gt; 
&lt;p&gt;For streaming generation, use the &lt;code&gt;stream_generate&lt;/code&gt; function. This yields a generation response object.&lt;/p&gt; 
&lt;p&gt;For example,&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mlx_lm import load, stream_generate

repo = "mlx-community/Mistral-7B-Instruct-v0.3-4bit"
model, tokenizer = load(repo)

prompt = "Write a story about Einstein"

messages = [{"role": "user", "content": prompt}]
prompt = tokenizer.apply_chat_template(
    messages, add_generation_prompt=True,
)

for response in stream_generate(model, tokenizer, prompt, max_tokens=512):
    print(response.text, end="", flush=True)
print()
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Sampling&lt;/h4&gt; 
&lt;p&gt;The &lt;code&gt;generate&lt;/code&gt; and &lt;code&gt;stream_generate&lt;/code&gt; functions accept &lt;code&gt;sampler&lt;/code&gt; and &lt;code&gt;logits_processors&lt;/code&gt; keyword arguments. A sampler is any callable which accepts a possibly batched logits array and returns an array of sampled tokens. The &lt;code&gt;logits_processors&lt;/code&gt; must be a list of callables which take the token history and current logits as input and return the processed logits. The logits processors are applied in order.&lt;/p&gt; 
&lt;p&gt;Some standard sampling functions and logits processors are provided in &lt;code&gt;mlx_lm.sample_utils&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Command Line&lt;/h3&gt; 
&lt;p&gt;You can also use &lt;code&gt;mlx-lm&lt;/code&gt; from the command line with:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;mlx_lm.generate --model mistralai/Mistral-7B-Instruct-v0.3 --prompt "hello"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will download a Mistral 7B model from the Hugging Face Hub and generate text using the given prompt.&lt;/p&gt; 
&lt;p&gt;For a full list of options run:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;mlx_lm.generate --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To quantize a model from the command line run:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;mlx_lm.convert --model mistralai/Mistral-7B-Instruct-v0.3 -q
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more options run:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;mlx_lm.convert --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can upload new models to Hugging Face by specifying &lt;code&gt;--upload-repo&lt;/code&gt; to &lt;code&gt;convert&lt;/code&gt;. For example, to upload a quantized Mistral-7B model to the &lt;a href="https://huggingface.co/mlx-community"&gt;MLX Hugging Face community&lt;/a&gt; you can do:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;mlx_lm.convert \
    --model mistralai/Mistral-7B-Instruct-v0.3 \
    -q \
    --upload-repo mlx-community/my-4bit-mistral
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Models can also be converted and quantized directly in the &lt;a href="https://huggingface.co/spaces/mlx-community/mlx-my-repo"&gt;mlx-my-repo&lt;/a&gt; Hugging Face Space.&lt;/p&gt; 
&lt;h3&gt;Long Prompts and Generations&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;mlx-lm&lt;/code&gt; has some tools to scale efficiently to long prompts and generations:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;A rotating fixed-size key-value cache.&lt;/li&gt; 
 &lt;li&gt;Prompt caching&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To use the rotating key-value cache pass the argument &lt;code&gt;--max-kv-size n&lt;/code&gt; where &lt;code&gt;n&lt;/code&gt; can be any integer. Smaller values like &lt;code&gt;512&lt;/code&gt; will use very little RAM but result in worse quality. Larger values like &lt;code&gt;4096&lt;/code&gt; or higher will use more RAM but have better quality.&lt;/p&gt; 
&lt;p&gt;Caching prompts can substantially speedup reusing the same long context with different queries. To cache a prompt use &lt;code&gt;mlx_lm.cache_prompt&lt;/code&gt;. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cat prompt.txt | mlx_lm.cache_prompt \
  --model mistralai/Mistral-7B-Instruct-v0.3 \
  --prompt - \
  --prompt-cache-file mistral_prompt.safetensors
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then use the cached prompt with &lt;code&gt;mlx_lm.generate&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;mlx_lm.generate \
    --prompt-cache-file mistral_prompt.safetensors \
    --prompt "\nSummarize the above text."
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The cached prompt is treated as a prefix to the supplied prompt. Also notice when using a cached prompt, the model to use is read from the cache and need not be supplied explicitly.&lt;/p&gt; 
&lt;p&gt;Prompt caching can also be used in the Python API in order to avoid recomputing the prompt. This is useful in multi-turn dialogues or across requests that use the same context. See the &lt;a href="https://github.com/ml-explore/mlx-lm/raw/main/mlx_lm/examples/chat.py"&gt;example&lt;/a&gt; for more usage details.&lt;/p&gt; 
&lt;h3&gt;Supported Models&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;mlx-lm&lt;/code&gt; supports thousands of LLMs available on the Hugging Face Hub. If the model you want to run is not supported, file an &lt;a href="https://github.com/ml-explore/mlx-lm/issues/new"&gt;issue&lt;/a&gt; or better yet, submit a pull request. Many supported models are available in various quantization formats in the &lt;a href="https://huggingface.co/mlx-community"&gt;MLX Community&lt;/a&gt; Hugging Face organization.&lt;/p&gt; 
&lt;p&gt;For some models the tokenizer may require you to enable the &lt;code&gt;trust_remote_code&lt;/code&gt; option. You can do this by passing &lt;code&gt;--trust-remote-code&lt;/code&gt; in the command line. If you don't specify the flag explicitly, you will be prompted to trust remote code in the terminal when running the model.&lt;/p&gt; 
&lt;p&gt;Tokenizer options can also be set in the Python API. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;model, tokenizer = load(
    "qwen/Qwen-7B",
    tokenizer_config={"eos_token": "&amp;lt;|endoftext|&amp;gt;", "trust_remote_code": True},
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Large Models&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] This requires macOS 15.0 or higher to work.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Models which are large relative to the total RAM available on the machine can be slow. &lt;code&gt;mlx-lm&lt;/code&gt; will attempt to make them faster by wiring the memory occupied by the model and cache. This requires macOS 15 or higher to work.&lt;/p&gt; 
&lt;p&gt;If you see the following warning message:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[WARNING] Generating with a model that requires ...&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;then the model will likely be slow on the given machine. If the model fits in RAM then it can often be sped up by increasing the system wired memory limit. To increase the limit, set the following &lt;code&gt;sysctl&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo sysctl iogpu.wired_limit_mb=N
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The value &lt;code&gt;N&lt;/code&gt; should be larger than the size of the model in megabytes but smaller than the memory size of the machine.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>modelscope/evalscope</title>
      <link>https://github.com/modelscope/evalscope</link>
      <description>&lt;p&gt;A streamlined and customizable framework for efficient large model (LLM, VLM, AIGC) evaluation and performance benchmarking.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;br /&gt; &lt;img src="https://raw.githubusercontent.com/modelscope/evalscope/main/docs/en/_static/images/evalscope_logo.png" /&gt; &lt;br /&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p align="center"&gt; &lt;a href="https://raw.githubusercontent.com/modelscope/evalscope/main/README_zh.md"&gt;‰∏≠Êñá&lt;/a&gt; &amp;nbsp; ÔΩú &amp;nbsp; English &amp;nbsp; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://img.shields.io/badge/python-%E2%89%A53.10-5be.svg?sanitize=true" /&gt; &lt;a href="https://badge.fury.io/py/evalscope"&gt;&lt;img src="https://badge.fury.io/py/evalscope.svg?sanitize=true" alt="PyPI version" height="18" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/evalscope"&gt;&lt;img alt="PyPI - Downloads" src="https://static.pepy.tech/badge/evalscope" /&gt;&lt;/a&gt; &lt;a href="https://github.com/modelscope/evalscope/pulls"&gt;&lt;img src="https://img.shields.io/badge/PR-welcome-55EB99.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://evalscope.readthedocs.io/en/latest/?badge=latest"&gt;&lt;img src="https://readthedocs.org/projects/evalscope/badge/?version=latest" alt="Documentation Status" /&gt;&lt;/a&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p align="center"&gt; &lt;a href="https://evalscope.readthedocs.io/zh-cn/latest/"&gt; üìñ ‰∏≠ÊñáÊñáÊ°£&lt;/a&gt; &amp;nbsp; ÔΩú &amp;nbsp; &lt;a href="https://evalscope.readthedocs.io/en/latest/"&gt; üìñ English Documentation&lt;/a&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;blockquote&gt; 
 &lt;p&gt;‚≠ê If you like this project, please click the "Star" button in the upper right corner to support us. Your support is our motivation to move forward!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;üìù Introduction&lt;/h2&gt; 
&lt;p&gt;EvalScope is a powerful and easily extensible model evaluation framework created by the &lt;a href="https://modelscope.cn/"&gt;ModelScope Community&lt;/a&gt;, aiming to provide a one-stop evaluation solution for large model developers.&lt;/p&gt; 
&lt;p&gt;Whether you want to evaluate the general capabilities of models, conduct multi-model performance comparisons, or need to stress test models, EvalScope can meet your needs.&lt;/p&gt; 
&lt;h2&gt;‚ú® Key Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;üìö Comprehensive Evaluation Benchmarks&lt;/strong&gt;: Built-in multiple industry-recognized evaluation benchmarks including MMLU, C-Eval, GSM8K, and more.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üß© Multi-modal and Multi-domain Support&lt;/strong&gt;: Supports evaluation of various model types including Large Language Models (LLM), Vision Language Models (VLM), Embedding, Reranker, AIGC, and more.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üöÄ Multi-backend Integration&lt;/strong&gt;: Seamlessly integrates multiple evaluation backends including OpenCompass, VLMEvalKit, RAGEval to meet different evaluation needs.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;‚ö° Inference Performance Testing&lt;/strong&gt;: Provides powerful model service stress testing tools, supporting multiple performance metrics such as TTFT, TPOT.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üìä Interactive Reports&lt;/strong&gt;: Provides WebUI visualization interface, supporting multi-dimensional model comparison, report overview and detailed inspection.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;‚öîÔ∏è Arena Mode&lt;/strong&gt;: Supports multi-model battles (Pairwise Battle), intuitively ranking and evaluating models.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üîß Highly Extensible&lt;/strong&gt;: Developers can easily add custom datasets, models and evaluation metrics.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt;
 &lt;summary&gt;üèõÔ∏è Overall Architecture&lt;/summary&gt; 
 &lt;p align="center"&gt; &lt;img src="https://sail-moe.oss-cn-hangzhou.aliyuncs.com/yunlin/images/evalscope/doc/EvalScope%E6%9E%B6%E6%9E%84%E5%9B%BE.png" style="width: 70%;" /&gt; &lt;br /&gt;EvalScope Overall Architecture. &lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Input Layer&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Model Sources&lt;/strong&gt;: API models (OpenAI API), Local models (ModelScope)&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Datasets&lt;/strong&gt;: Standard evaluation benchmarks (MMLU/GSM8k etc.), Custom data (MCQ/QA)&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Core Functions&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Multi-backend Evaluation&lt;/strong&gt;: Native backend, OpenCompass, MTEB, VLMEvalKit, RAGAS&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Performance Monitoring&lt;/strong&gt;: Supports multiple model service APIs and data formats, tracking TTFT/TPOP and other metrics&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Tool Extensions&lt;/strong&gt;: Integrates Tool-Bench, Needle-in-a-Haystack, etc.&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Output Layer&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Structured Reports&lt;/strong&gt;: Supports JSON, Table, Logs&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Visualization Platform&lt;/strong&gt;: Supports Gradio, Wandb, SwanLab&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/details&gt; 
&lt;h2&gt;üéâ What's New&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] &lt;strong&gt;Version 1.0 Refactoring&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;Version 1.0 introduces a major overhaul of the evaluation framework, establishing a new, more modular and extensible API layer under &lt;code&gt;evalscope/api&lt;/code&gt;. Key improvements include standardized data models for benchmarks, samples, and results; a registry-based design for components such as benchmarks and metrics; and a rewritten core evaluator that orchestrates the new architecture. Existing benchmark adapters have been migrated to this API, resulting in cleaner, more consistent, and easier-to-maintain implementations.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;üî• &lt;strong&gt;[2026.01.13]&lt;/strong&gt; Added support for Embedding and Rerank model service stress testing. Refer to the &lt;a href="https://evalscope.readthedocs.io/en/latest/user_guides/stress_test/examples.html#embedding"&gt;usage documentation&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;üî• &lt;strong&gt;[2025.12.26]&lt;/strong&gt; Added support for Terminal-Bench-2.0, which evaluates AI Agent performance on 89 real-world multi-step terminal tasks. Refer to the &lt;a href="https://evalscope.readthedocs.io/en/latest/third_party/terminal_bench.html"&gt;usage documentation&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;üî• &lt;strong&gt;[2025.12.18]&lt;/strong&gt; Added support for SLA auto-tuning model API services, automatically testing the maximum concurrency of model services under specific latency, TTFT, and throughput conditions. Refer to the &lt;a href="https://evalscope.readthedocs.io/en/latest/user_guides/stress_test/sla_auto_tune.html"&gt;usage documentation&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;üî• &lt;strong&gt;[2025.12.16]&lt;/strong&gt; Added support for audio evaluation benchmarks such as Fleurs, LibriSpeech; added support for multilingual code evaluation benchmarks such as MultiplE, MBPP.&lt;/li&gt; 
 &lt;li&gt;üî• &lt;strong&gt;[2025.12.02]&lt;/strong&gt; Added support for custom multimodal VQA evaluation; refer to the &lt;a href="https://evalscope.readthedocs.io/en/latest/advanced_guides/custom_dataset/vlm.html"&gt;usage documentation&lt;/a&gt;. Added support for visualizing model service stress testing in ClearML; refer to the &lt;a href="https://evalscope.readthedocs.io/en/latest/user_guides/stress_test/examples.html#clearml"&gt;usage documentation&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;üî• &lt;strong&gt;[2025.11.26]&lt;/strong&gt; Added support for OpenAI-MRCR, GSM8K-V, MGSM, MicroVQA, IFBench, SciCode benchmarks.&lt;/li&gt; 
 &lt;li&gt;üî• &lt;strong&gt;[2025.11.18]&lt;/strong&gt; Added support for custom Function-Call (tool invocation) datasets to test whether models can timely and correctly call tools. Refer to the &lt;a href="https://evalscope.readthedocs.io/en/latest/advanced_guides/custom_dataset/llm.html#function-calling-format-fc"&gt;usage documentation&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;üî• &lt;strong&gt;[2025.11.14]&lt;/strong&gt; Added support for SWE-bench_Verified, SWE-bench_Lite, SWE-bench_Verified_mini code evaluation benchmarks. Refer to the &lt;a href="https://evalscope.readthedocs.io/en/latest/third_party/swe_bench.html"&gt;usage documentation&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;üî• &lt;strong&gt;[2025.11.12]&lt;/strong&gt; Added &lt;code&gt;pass@k&lt;/code&gt;, &lt;code&gt;vote@k&lt;/code&gt;, &lt;code&gt;pass^k&lt;/code&gt; and other metric aggregation methods; added support for multimodal evaluation benchmarks such as A_OKVQA, CMMU, ScienceQA, V*Bench.&lt;/li&gt; 
 &lt;li&gt;üî• &lt;strong&gt;[2025.11.07]&lt;/strong&gt; Added support for œÑ¬≤-bench, an extended and enhanced version of œÑ-bench that includes a series of code fixes and adds telecom domain troubleshooting scenarios. Refer to the &lt;a href="https://evalscope.readthedocs.io/en/latest/third_party/tau2_bench.html"&gt;usage documentation&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;üî• &lt;strong&gt;[2025.10.30]&lt;/strong&gt; Added support for BFCL-v4, enabling evaluation of agent capabilities including web search and long-term memory. See the &lt;a href="https://evalscope.readthedocs.io/en/latest/third_party/bfcl_v4.html"&gt;usage documentation&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;üî• &lt;strong&gt;[2025.10.27]&lt;/strong&gt; Added support for LogiQA, HaluEval, MathQA, MRI-QA, PIQA, QASC, CommonsenseQA and other evaluation benchmarks. Thanks to @&lt;a href="https://github.com/penguinwang96825"&gt;penguinwang96825&lt;/a&gt; for the code implementation.&lt;/li&gt; 
 &lt;li&gt;üî• &lt;strong&gt;[2025.10.26]&lt;/strong&gt; Added support for Conll-2003, CrossNER, Copious, GeniaNER, HarveyNER, MIT-Movie-Trivia, MIT-Restaurant, OntoNotes5, WNUT2017 and other Named Entity Recognition evaluation benchmarks. Thanks to @&lt;a href="https://github.com/penguinwang96825"&gt;penguinwang96825&lt;/a&gt; for the code implementation.&lt;/li&gt; 
 &lt;li&gt;üî• &lt;strong&gt;[2025.10.21]&lt;/strong&gt; Optimized sandbox environment usage in code evaluation, supporting both local and remote operation modes. For details, refer to the &lt;a href="https://evalscope.readthedocs.io/en/latest/user_guides/sandbox.html"&gt;documentation&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;üî• &lt;strong&gt;[2025.10.20]&lt;/strong&gt; Added support for evaluation benchmarks including PolyMath, SimpleVQA, MathVerse, MathVision, AA-LCR; optimized evalscope perf performance to align with vLLM Bench. For details, refer to the &lt;a href="https://evalscope.readthedocs.io/en/latest/user_guides/stress_test/vs_vllm_bench.html"&gt;documentation&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;üî• &lt;strong&gt;[2025.10.14]&lt;/strong&gt; Added support for OCRBench, OCRBench-v2, DocVQA, InfoVQA, ChartQA, and BLINK multimodal image-text evaluation benchmarks.&lt;/li&gt; 
 &lt;li&gt;üî• &lt;strong&gt;[2025.09.22]&lt;/strong&gt; Code evaluation benchmarks (HumanEval, LiveCodeBench) now support running in a sandbox environment. To use this feature, please install &lt;a href="https://github.com/modelscope/ms-enclave"&gt;ms-enclave&lt;/a&gt; first.&lt;/li&gt; 
 &lt;li&gt;üî• &lt;strong&gt;[2025.09.19]&lt;/strong&gt; Added support for multimodal image-text evaluation benchmarks including RealWorldQA, AI2D, MMStar, MMBench, and OmniBench, as well as pure text evaluation benchmarks such as Multi-IF, HealthBench, and AMC.&lt;/li&gt; 
 &lt;li&gt;üî• &lt;strong&gt;[2025.09.05]&lt;/strong&gt; Added support for vision-language multimodal model evaluation tasks, such as MathVista and MMMU. For more supported datasets, please &lt;a href="https://evalscope.readthedocs.io/en/latest/get_started/supported_dataset/vlm.html"&gt;refer to the documentation&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;üî• &lt;strong&gt;[2025.09.04]&lt;/strong&gt; Added support for image editing task evaluation, including the &lt;a href="https://modelscope.cn/datasets/stepfun-ai/GEdit-Bench"&gt;GEdit-Bench&lt;/a&gt; benchmark. For usage instructions, refer to the &lt;a href="https://evalscope.readthedocs.io/en/latest/user_guides/aigc/image_edit.html"&gt;documentation&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;üî• &lt;strong&gt;[2025.08.22]&lt;/strong&gt; Version 1.0 Refactoring. Break changes, please &lt;a href="https://evalscope.readthedocs.io/en/latest/get_started/basic_usage.html#switching-to-version-v1-0"&gt;refer to&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt;
 &lt;summary&gt;More&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;üî• &lt;strong&gt;[2025.07.18]&lt;/strong&gt; The model stress testing now supports randomly generating image-text data for multimodal model evaluation. For usage instructions, refer to the &lt;a href="https://evalscope.readthedocs.io/en/latest/user_guides/stress_test/examples.html#id4"&gt;documentation&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;üî• &lt;strong&gt;[2025.07.16]&lt;/strong&gt; Support for &lt;a href="https://github.com/sierra-research/tau-bench"&gt;œÑ-bench&lt;/a&gt; has been added, enabling the evaluation of AI Agent performance and reliability in real-world scenarios involving dynamic user and tool interactions. For usage instructions, please refer to the &lt;a href="https://evalscope.readthedocs.io/en/latest/get_started/supported_dataset/llm.html#bench"&gt;documentation&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;üî• &lt;strong&gt;[2025.07.14]&lt;/strong&gt; Support for "Humanity's Last Exam" (&lt;a href="https://modelscope.cn/datasets/cais/hle"&gt;Humanity's-Last-Exam&lt;/a&gt;), a highly challenging evaluation benchmark. For usage instructions, refer to the &lt;a href="https://evalscope.readthedocs.io/en/latest/get_started/supported_dataset/llm.html#humanity-s-last-exam"&gt;documentation&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;üî• &lt;strong&gt;[2025.07.03]&lt;/strong&gt; Refactored Arena Mode: now supports custom model battles, outputs a model leaderboard, and provides battle result visualization. See &lt;a href="https://evalscope.readthedocs.io/en/latest/user_guides/arena.html"&gt;reference&lt;/a&gt; for details.&lt;/li&gt; 
  &lt;li&gt;üî• &lt;strong&gt;[2025.06.28]&lt;/strong&gt; Optimized custom dataset evaluation: now supports evaluation without reference answers. Enhanced LLM judge usage, with built-in modes for "scoring directly without reference answers" and "checking answer consistency with reference answers". See &lt;a href="https://evalscope.readthedocs.io/en/latest/advanced_guides/custom_dataset/llm.html#qa"&gt;reference&lt;/a&gt; for details.&lt;/li&gt; 
  &lt;li&gt;üî• &lt;strong&gt;[2025.06.19]&lt;/strong&gt; Added support for the &lt;a href="https://modelscope.cn/datasets/AI-ModelScope/bfcl_v3"&gt;BFCL-v3&lt;/a&gt; benchmark, designed to evaluate model function-calling capabilities across various scenarios. For more information, refer to the &lt;a href="https://evalscope.readthedocs.io/en/latest/third_party/bfcl_v3.html"&gt;documentation&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;üî• &lt;strong&gt;[2025.06.02]&lt;/strong&gt; Added support for the Needle-in-a-Haystack test. Simply specify &lt;code&gt;needle_haystack&lt;/code&gt; to conduct the test, and a corresponding heatmap will be generated in the &lt;code&gt;outputs/reports&lt;/code&gt; folder, providing a visual representation of the model's performance. Refer to the &lt;a href="https://evalscope.readthedocs.io/en/latest/third_party/needle_haystack.html"&gt;documentation&lt;/a&gt; for more details.&lt;/li&gt; 
  &lt;li&gt;üî• &lt;strong&gt;[2025.05.29]&lt;/strong&gt; Added support for two long document evaluation benchmarks: &lt;a href="https://modelscope.cn/datasets/yale-nlp/DocMath-Eval/summary"&gt;DocMath&lt;/a&gt; and &lt;a href="https://modelscope.cn/datasets/iic/frames/summary"&gt;FRAMES&lt;/a&gt;. For usage guidelines, please refer to the &lt;a href="https://evalscope.readthedocs.io/en/latest/get_started/supported_dataset/index.html"&gt;documentation&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;üî• &lt;strong&gt;[2025.05.16]&lt;/strong&gt; Model service performance stress testing now supports setting various levels of concurrency and outputs a performance test report. &lt;a href="https://evalscope.readthedocs.io/en/latest/user_guides/stress_test/quick_start.html#id3"&gt;Reference example&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;üî• &lt;strong&gt;[2025.05.13]&lt;/strong&gt; Added support for the &lt;a href="https://modelscope.cn/datasets/AI-ModelScope/ToolBench-Static"&gt;ToolBench-Static&lt;/a&gt; dataset to evaluate model's tool-calling capabilities. Refer to the &lt;a href="https://evalscope.readthedocs.io/en/latest/third_party/toolbench.html"&gt;documentation&lt;/a&gt; for usage instructions. Also added support for the &lt;a href="https://modelscope.cn/datasets/AI-ModelScope/DROP/dataPeview"&gt;DROP&lt;/a&gt; and &lt;a href="https://modelscope.cn/datasets/AI-ModelScope/winogrande_val"&gt;Winogrande&lt;/a&gt; benchmarks to assess the reasoning capabilities of models.&lt;/li&gt; 
  &lt;li&gt;üî• &lt;strong&gt;[2025.04.29]&lt;/strong&gt; Added Qwen3 Evaluation Best Practices, &lt;a href="https://evalscope.readthedocs.io/en/latest/best_practice/qwen3.html"&gt;welcome to read üìñ&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;üî• &lt;strong&gt;[2025.04.27]&lt;/strong&gt; Support for text-to-image evaluation: Supports 8 metrics including MPS, HPSv2.1Score, etc., and evaluation benchmarks such as EvalMuse, GenAI-Bench. Refer to the &lt;a href="https://evalscope.readthedocs.io/en/latest/user_guides/aigc/t2i.html"&gt;user documentation&lt;/a&gt; for more details.&lt;/li&gt; 
  &lt;li&gt;üî• &lt;strong&gt;[2025.04.10]&lt;/strong&gt; Model service stress testing tool now supports the &lt;code&gt;/v1/completions&lt;/code&gt; endpoint (the default endpoint for vLLM benchmarking)&lt;/li&gt; 
  &lt;li&gt;üî• &lt;strong&gt;[2025.04.08]&lt;/strong&gt; Support for evaluating embedding model services compatible with the OpenAI API has been added. For more details, check the &lt;a href="https://evalscope.readthedocs.io/en/latest/user_guides/backend/rageval_backend/mteb.html#configure-evaluation-parameters"&gt;user guide&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;üî• &lt;strong&gt;[2025.03.27]&lt;/strong&gt; Added support for &lt;a href="https://www.modelscope.cn/datasets/AI-ModelScope/alpaca_eval/dataPeview"&gt;AlpacaEval&lt;/a&gt; and &lt;a href="https://modelscope.cn/datasets/AI-ModelScope/arena-hard-auto-v0.1/summary"&gt;ArenaHard&lt;/a&gt; evaluation benchmarks. For usage notes, please refer to the &lt;a href="https://evalscope.readthedocs.io/en/latest/get_started/supported_dataset/index.html"&gt;documentation&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;üî• &lt;strong&gt;[2025.03.20]&lt;/strong&gt; The model inference service stress testing now supports generating prompts of specified length using random values. Refer to the &lt;a href="https://evalscope.readthedocs.io/en/latest/user_guides/stress_test/examples.html#using-the-random-dataset"&gt;user guide&lt;/a&gt; for more details.&lt;/li&gt; 
  &lt;li&gt;üî• &lt;strong&gt;[2025.03.13]&lt;/strong&gt; Added support for the &lt;a href="https://www.modelscope.cn/datasets/AI-ModelScope/code_generation_lite/summary"&gt;LiveCodeBench&lt;/a&gt; code evaluation benchmark, which can be used by specifying &lt;code&gt;live_code_bench&lt;/code&gt;. Supports evaluating QwQ-32B on LiveCodeBench, refer to the &lt;a href="https://evalscope.readthedocs.io/en/latest/best_practice/eval_qwq.html"&gt;best practices&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;üî• &lt;strong&gt;[2025.03.11]&lt;/strong&gt; Added support for the &lt;a href="https://modelscope.cn/datasets/AI-ModelScope/SimpleQA/summary"&gt;SimpleQA&lt;/a&gt; and &lt;a href="https://modelscope.cn/datasets/AI-ModelScope/Chinese-SimpleQA/summary"&gt;Chinese SimpleQA&lt;/a&gt; evaluation benchmarks. These are used to assess the factual accuracy of models, and you can specify &lt;code&gt;simple_qa&lt;/code&gt; and &lt;code&gt;chinese_simpleqa&lt;/code&gt; for use. Support for specifying a judge model is also available. For more details, refer to the &lt;a href="https://evalscope.readthedocs.io/en/latest/get_started/parameters.html"&gt;relevant parameter documentation&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;üî• &lt;strong&gt;[2025.03.07]&lt;/strong&gt; Added support for the &lt;a href="https://modelscope.cn/models/Qwen/QwQ-32B/summary"&gt;QwQ-32B&lt;/a&gt; model, evaluate the model's reasoning ability and reasoning efficiency, refer to &lt;a href="https://evalscope.readthedocs.io/en/latest/best_practice/eval_qwq.html"&gt;üìñ Best Practices for QwQ-32B Evaluation&lt;/a&gt; for more details.&lt;/li&gt; 
  &lt;li&gt;üî• &lt;strong&gt;[2025.03.04]&lt;/strong&gt; Added support for the &lt;a href="https://modelscope.cn/datasets/m-a-p/SuperGPQA/summary"&gt;SuperGPQA&lt;/a&gt; dataset, which covers 13 categories, 72 first-level disciplines, and 285 second-level disciplines, totaling 26,529 questions. You can use it by specifying &lt;code&gt;super_gpqa&lt;/code&gt;.&lt;/li&gt; 
  &lt;li&gt;üî• &lt;strong&gt;[2025.03.03]&lt;/strong&gt; Added support for evaluating the IQ and EQ of models. Refer to &lt;a href="https://evalscope.readthedocs.io/en/latest/best_practice/iquiz.html"&gt;üìñ Best Practices for IQ and EQ Evaluation&lt;/a&gt; to find out how smart your AI is!&lt;/li&gt; 
  &lt;li&gt;üî• &lt;strong&gt;[2025.02.27]&lt;/strong&gt; Added support for evaluating the reasoning efficiency of models. Refer to &lt;a href="https://evalscope.readthedocs.io/en/latest/best_practice/think_eval.html"&gt;üìñ Best Practices for Evaluating Thinking Efficiency&lt;/a&gt;. This implementation is inspired by the works &lt;a href="https://doi.org/10.48550/arXiv.2412.21187"&gt;Overthinking&lt;/a&gt; and &lt;a href="https://doi.org/10.48550/arXiv.2501.18585"&gt;Underthinking&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;üî• &lt;strong&gt;[2025.02.25]&lt;/strong&gt; Added support for two model inference-related evaluation benchmarks: &lt;a href="https://modelscope.cn/datasets/AI-ModelScope/MuSR"&gt;MuSR&lt;/a&gt; and &lt;a href="https://www.modelscope.cn/datasets/Qwen/ProcessBench/summary"&gt;ProcessBench&lt;/a&gt;. To use them, simply specify &lt;code&gt;musr&lt;/code&gt; and &lt;code&gt;process_bench&lt;/code&gt; respectively in the datasets parameter.&lt;/li&gt; 
  &lt;li&gt;üî• &lt;strong&gt;[2025.02.18]&lt;/strong&gt; Supports the AIME25 dataset, which contains 15 questions (Grok3 scored 93 on this dataset).&lt;/li&gt; 
  &lt;li&gt;üî• &lt;strong&gt;[2025.02.13]&lt;/strong&gt; Added support for evaluating DeepSeek distilled models, including AIME24, MATH-500, and GPQA-Diamond datasetsÔºårefer to &lt;a href="https://evalscope.readthedocs.io/en/latest/best_practice/deepseek_r1_distill.html"&gt;best practice&lt;/a&gt;; Added support for specifying the &lt;code&gt;eval_batch_size&lt;/code&gt; parameter to accelerate model evaluation.&lt;/li&gt; 
  &lt;li&gt;üî• &lt;strong&gt;[2025.01.20]&lt;/strong&gt; Support for visualizing evaluation results, including single model evaluation results and multi-model comparison, refer to the &lt;a href="https://evalscope.readthedocs.io/en/latest/get_started/visualization.html"&gt;üìñ Visualizing Evaluation Results&lt;/a&gt; for more details; Added &lt;a href="https://modelscope.cn/datasets/AI-ModelScope/IQuiz/summary"&gt;&lt;code&gt;iquiz&lt;/code&gt;&lt;/a&gt; evaluation example, evaluating the IQ and EQ of the model.&lt;/li&gt; 
  &lt;li&gt;üî• &lt;strong&gt;[2025.01.07]&lt;/strong&gt; Native backend: Support for model API evaluation is now available. Refer to the &lt;a href="https://evalscope.readthedocs.io/en/latest/get_started/basic_usage.html#api"&gt;üìñ Model API Evaluation Guide&lt;/a&gt; for more details. Additionally, support for the &lt;code&gt;ifeval&lt;/code&gt; evaluation benchmark has been added.&lt;/li&gt; 
  &lt;li&gt;üî•üî• &lt;strong&gt;[2024.12.31]&lt;/strong&gt; Support for adding benchmark evaluations, refer to the &lt;a href="https://evalscope.readthedocs.io/en/latest/advanced_guides/add_benchmark.html"&gt;üìñ Benchmark Evaluation Addition Guide&lt;/a&gt;; support for custom mixed dataset evaluations, allowing for more comprehensive model evaluations with less data, refer to the &lt;a href="https://evalscope.readthedocs.io/en/latest/advanced_guides/collection/index.html"&gt;üìñ Mixed Dataset Evaluation Guide&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;üî• &lt;strong&gt;[2024.12.13]&lt;/strong&gt; Model evaluation optimization: no need to pass the &lt;code&gt;--template-type&lt;/code&gt; parameter anymore; supports starting evaluation with &lt;code&gt;evalscope eval --args&lt;/code&gt;. Refer to the &lt;a href="https://evalscope.readthedocs.io/en/latest/get_started/basic_usage.html"&gt;üìñ User Guide&lt;/a&gt; for more details.&lt;/li&gt; 
  &lt;li&gt;üî• &lt;strong&gt;[2024.11.26]&lt;/strong&gt; The model inference service performance evaluator has been completely refactored: it now supports local inference service startup and Speed Benchmark; asynchronous call error handling has been optimized. For more details, refer to the &lt;a href="https://evalscope.readthedocs.io/en/latest/user_guides/stress_test/index.html"&gt;üìñ User Guide&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;üî• &lt;strong&gt;[2024.10.31]&lt;/strong&gt; The best practice for evaluating Multimodal-RAG has been updated, please check the &lt;a href="https://evalscope.readthedocs.io/zh-cn/latest/blog/RAG/multimodal_RAG.html#multimodal-rag"&gt;üìñ Blog&lt;/a&gt; for more details.&lt;/li&gt; 
  &lt;li&gt;üî• &lt;strong&gt;[2024.10.23]&lt;/strong&gt; Supports multimodal RAG evaluation, including the assessment of image-text retrieval using &lt;a href="https://evalscope.readthedocs.io/en/latest/user_guides/backend/rageval_backend/clip_benchmark.html"&gt;CLIP_Benchmark&lt;/a&gt;, and extends &lt;a href="https://evalscope.readthedocs.io/en/latest/user_guides/backend/rageval_backend/ragas.html"&gt;RAGAS&lt;/a&gt; to support end-to-end multimodal metrics evaluation.&lt;/li&gt; 
  &lt;li&gt;üî• &lt;strong&gt;[2024.10.8]&lt;/strong&gt; Support for RAG evaluation, including independent evaluation of embedding models and rerankers using &lt;a href="https://evalscope.readthedocs.io/en/latest/user_guides/backend/rageval_backend/mteb.html"&gt;MTEB/CMTEB&lt;/a&gt;, as well as end-to-end evaluation using &lt;a href="https://evalscope.readthedocs.io/en/latest/user_guides/backend/rageval_backend/ragas.html"&gt;RAGAS&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;üî• &lt;strong&gt;[2024.09.18]&lt;/strong&gt; Our documentation has been updated to include a blog module, featuring some technical research and discussions related to evaluations. We invite you to &lt;a href="https://evalscope.readthedocs.io/en/refact_readme/blog/index.html"&gt;üìñ read it&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;üî• &lt;strong&gt;[2024.09.12]&lt;/strong&gt; Support for LongWriter evaluation, which supports 10,000+ word generation. You can use the benchmark &lt;a href="https://raw.githubusercontent.com/modelscope/evalscope/main/evalscope/third_party/longbench_write/README.md"&gt;LongBench-Write&lt;/a&gt; to measure the long output quality as well as the output length.&lt;/li&gt; 
  &lt;li&gt;üî• &lt;strong&gt;[2024.08.30]&lt;/strong&gt; Support for custom dataset evaluations, including text datasets and multimodal image-text datasets.&lt;/li&gt; 
  &lt;li&gt;üî• &lt;strong&gt;[2024.08.20]&lt;/strong&gt; Updated the official documentation, including getting started guides, best practices, and FAQs. Feel free to &lt;a href="https://evalscope.readthedocs.io/en/latest/"&gt;üìñread it here&lt;/a&gt;!&lt;/li&gt; 
  &lt;li&gt;üî• &lt;strong&gt;[2024.08.09]&lt;/strong&gt; Simplified the installation process, allowing for pypi installation of vlmeval dependencies; optimized the multimodal model evaluation experience, achieving up to 10x acceleration based on the OpenAI API evaluation chain.&lt;/li&gt; 
  &lt;li&gt;üî• &lt;strong&gt;[2024.07.31]&lt;/strong&gt; Important change: The package name &lt;code&gt;llmuses&lt;/code&gt; has been changed to &lt;code&gt;evalscope&lt;/code&gt;. Please update your code accordingly.&lt;/li&gt; 
  &lt;li&gt;üî• &lt;strong&gt;[2024.07.26]&lt;/strong&gt; Support for &lt;strong&gt;VLMEvalKit&lt;/strong&gt; as a third-party evaluation framework to initiate multimodal model evaluation tasks.&lt;/li&gt; 
  &lt;li&gt;üî• &lt;strong&gt;[2024.06.29]&lt;/strong&gt; Support for &lt;strong&gt;OpenCompass&lt;/strong&gt; as a third-party evaluation framework, which we have encapsulated at a higher level, supporting pip installation and simplifying evaluation task configuration.&lt;/li&gt; 
  &lt;li&gt;üî• &lt;strong&gt;[2024.06.13]&lt;/strong&gt; EvalScope seamlessly integrates with the fine-tuning framework SWIFT, providing full-chain support from LLM training to evaluation.&lt;/li&gt; 
  &lt;li&gt;üî• &lt;strong&gt;[2024.06.13]&lt;/strong&gt; Integrated the Agent evaluation dataset ToolBench.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;‚ù§Ô∏è Community &amp;amp; Support&lt;/h2&gt; 
&lt;p&gt;Welcome to join our community to communicate with other developers and get help.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;&lt;a href="https://discord.com/invite/D27yfEFVz5"&gt;Discord Group&lt;/a&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;WeChat Group&lt;/th&gt; 
   &lt;th align="center"&gt;DingTalk Group&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/modelscope/evalscope/main/docs/asset/discord_qr.jpg" width="160" height="160" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/modelscope/evalscope/main/docs/asset/wechat.png" width="160" height="160" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/modelscope/evalscope/main/docs/asset/dingding.png" width="160" height="160" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;üõ†Ô∏è Environment Setup&lt;/h2&gt; 
&lt;p&gt;We recommend using &lt;code&gt;conda&lt;/code&gt; to create a virtual environment and install with &lt;code&gt;pip&lt;/code&gt;.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Create and Activate Conda Environment&lt;/strong&gt; (Python 3.10 recommended)&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;conda create -n evalscope python=3.10
conda activate evalscope
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install EvalScope&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Method 1: Install via PyPI (Recommended)&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;pip install evalscope
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Method 2: Install from Source (For Development)&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;git clone https://github.com/modelscope/evalscope.git
cd evalscope
pip install -e .
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install Additional Dependencies&lt;/strong&gt; (Optional) Install corresponding feature extensions according to your needs:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;# Performance testing
pip install 'evalscope[perf]'

# Visualization App
pip install 'evalscope[app]'

# Other evaluation backends
pip install 'evalscope[opencompass]'
pip install 'evalscope[vlmeval]'
pip install 'evalscope[rag]'

# Install all dependencies
pip install 'evalscope[all]'
&lt;/code&gt;&lt;/pre&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;If you installed from source, please replace &lt;code&gt;evalscope&lt;/code&gt; with &lt;code&gt;.&lt;/code&gt;, for example &lt;code&gt;pip install '.[perf]'&lt;/code&gt;.&lt;/p&gt; 
  &lt;/blockquote&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] This project was formerly known as &lt;code&gt;llmuses&lt;/code&gt;. If you need to use &lt;code&gt;v0.4.3&lt;/code&gt; or earlier versions, please run &lt;code&gt;pip install llmuses&amp;lt;=0.4.3&lt;/code&gt; and use &lt;code&gt;from llmuses import ...&lt;/code&gt; for imports.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;üöÄ Quick Start&lt;/h2&gt; 
&lt;p&gt;You can start evaluation tasks in two ways: &lt;strong&gt;command line&lt;/strong&gt; or &lt;strong&gt;Python code&lt;/strong&gt;.&lt;/p&gt; 
&lt;h3&gt;Method 1. Using Command Line&lt;/h3&gt; 
&lt;p&gt;Execute the &lt;code&gt;evalscope eval&lt;/code&gt; command in any path to start evaluation. The following command will evaluate the &lt;code&gt;Qwen/Qwen2.5-0.5B-Instruct&lt;/code&gt; model on &lt;code&gt;gsm8k&lt;/code&gt; and &lt;code&gt;arc&lt;/code&gt; datasets, taking only 5 samples from each dataset.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;evalscope eval \
 --model Qwen/Qwen2.5-0.5B-Instruct \
 --datasets gsm8k arc \
 --limit 5
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Method 2. Using Python Code&lt;/h3&gt; 
&lt;p&gt;Use the &lt;code&gt;run_task&lt;/code&gt; function and &lt;code&gt;TaskConfig&lt;/code&gt; object to configure and start evaluation tasks.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from evalscope import run_task, TaskConfig

# Configure evaluation task
task_cfg = TaskConfig(
    model='Qwen/Qwen2.5-0.5B-Instruct',
    datasets=['gsm8k', 'arc'],
    limit=5
)

# Start evaluation
run_task(task_cfg)
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt;
 &lt;summary&gt;&lt;b&gt;üí° Tip:&lt;/b&gt; `run_task` also supports dictionaries, YAML or JSON files as configuration.&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;Using Python Dictionary&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from evalscope.run import run_task

task_cfg = {
    'model': 'Qwen/Qwen2.5-0.5B-Instruct',
    'datasets': ['gsm8k', 'arc'],
    'limit': 5
}
run_task(task_cfg=task_cfg)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Using YAML File&lt;/strong&gt; (&lt;code&gt;config.yaml&lt;/code&gt;)&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-yaml"&gt;model: Qwen/Qwen2.5-0.5B-Instruct
datasets:
  - gsm8k
  - arc
limit: 5
&lt;/code&gt;&lt;/pre&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from evalscope.run import run_task

run_task(task_cfg="config.yaml")
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;Output Results&lt;/h3&gt; 
&lt;p&gt;After evaluation completion, you will see a report in the terminal in the following format:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;+-----------------------+----------------+-----------------+-----------------+---------------+-------+---------+
| Model Name            | Dataset Name   | Metric Name     | Category Name   | Subset Name   |   Num |   Score |
+=======================+================+=================+=================+===============+=======+=========+
| Qwen2.5-0.5B-Instruct | gsm8k          | AverageAccuracy | default         | main          |     5 |     0.4 |
+-----------------------+----------------+-----------------+-----------------+---------------+-------+---------+
| Qwen2.5-0.5B-Instruct | ai2_arc        | AverageAccuracy | default         | ARC-Easy      |     5 |     0.8 |
+-----------------------+----------------+-----------------+-----------------+---------------+-------+---------+
| Qwen2.5-0.5B-Instruct | ai2_arc        | AverageAccuracy | default         | ARC-Challenge |     5 |     0.4 |
+-----------------------+----------------+-----------------+-----------------+---------------+-------+---------+
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;üìà Advanced Usage&lt;/h2&gt; 
&lt;h3&gt;Custom Evaluation Parameters&lt;/h3&gt; 
&lt;p&gt;You can fine-tune model loading, inference, and dataset configuration through command line parameters.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;evalscope eval \
 --model Qwen/Qwen3-0.6B \
 --model-args '{"revision": "master", "precision": "torch.float16", "device_map": "auto"}' \
 --generation-config '{"do_sample":true,"temperature":0.6,"max_tokens":512}' \
 --dataset-args '{"gsm8k": {"few_shot_num": 0, "few_shot_random": false}}' \
 --datasets gsm8k \
 --limit 10
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--model-args&lt;/code&gt;: Model loading parameters such as &lt;code&gt;revision&lt;/code&gt;, &lt;code&gt;precision&lt;/code&gt;, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--generation-config&lt;/code&gt;: Model generation parameters such as &lt;code&gt;temperature&lt;/code&gt;, &lt;code&gt;max_tokens&lt;/code&gt;, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--dataset-args&lt;/code&gt;: Dataset configuration parameters such as &lt;code&gt;few_shot_num&lt;/code&gt;, etc.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For details, please refer to &lt;a href="https://evalscope.readthedocs.io/en/latest/get_started/parameters.html"&gt;üìñ Complete Parameter Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Evaluating Online Model APIs&lt;/h3&gt; 
&lt;p&gt;EvalScope supports evaluating model services deployed via APIs (such as services deployed with vLLM). Simply specify the service address and API Key.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Start Model Service&lt;/strong&gt; (using vLLM as example)&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;export VLLM_USE_MODELSCOPE=True
python -m vllm.entrypoints.openai.api_server \
  --model Qwen/Qwen2.5-0.5B-Instruct \
  --served-model-name qwen2.5 \
  --port 8801
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Run Evaluation&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;evalscope eval \
 --model qwen2.5 \
 --eval-type openai_api \
 --api-url http://127.0.0.1:8801/v1 \
 --api-key EMPTY \
 --datasets gsm8k \
 --limit 10
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;‚öîÔ∏è Arena Mode&lt;/h3&gt; 
&lt;p&gt;Arena mode evaluates model performance through pairwise battles between models, providing win rates and rankings, perfect for horizontal comparison of multiple models.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;# Example evaluation results
Model           WinRate (%)  CI (%)
------------  -------------  ---------------
qwen2.5-72b            69.3  (-13.3 / +12.2)
qwen2.5-7b             50    (+0.0 / +0.0)
qwen2.5-0.5b            4.7  (-2.5 / +4.4)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For details, please refer to &lt;a href="https://evalscope.readthedocs.io/en/latest/user_guides/arena.html"&gt;üìñ Arena Mode Usage Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;üñäÔ∏è Custom Dataset Evaluation&lt;/h3&gt; 
&lt;p&gt;EvalScope allows you to easily add and evaluate your own datasets. For details, please refer to &lt;a href="https://evalscope.readthedocs.io/en/latest/advanced_guides/custom_dataset/index.html"&gt;üìñ Custom Dataset Evaluation Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;üß™ Other Evaluation Backends&lt;/h2&gt; 
&lt;p&gt;EvalScope supports launching evaluation tasks through third-party evaluation frameworks (we call them "backends") to meet diverse evaluation needs.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Native&lt;/strong&gt;: EvalScope's default evaluation framework with comprehensive functionality.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;OpenCompass&lt;/strong&gt;: Focuses on text-only evaluation. &lt;a href="https://evalscope.readthedocs.io/en/latest/user_guides/backend/opencompass_backend.html"&gt;üìñ Usage Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;VLMEvalKit&lt;/strong&gt;: Focuses on multi-modal evaluation. &lt;a href="https://evalscope.readthedocs.io/en/latest/user_guides/backend/vlmevalkit_backend.html"&gt;üìñ Usage Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;RAGEval&lt;/strong&gt;: Focuses on RAG evaluation, supporting Embedding and Reranker models. &lt;a href="https://evalscope.readthedocs.io/en/latest/user_guides/backend/rageval_backend/index.html"&gt;üìñ Usage Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Third-party Evaluation Tools&lt;/strong&gt;: Supports evaluation tasks like &lt;a href="https://evalscope.readthedocs.io/en/latest/third_party/toolbench.html"&gt;ToolBench&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;‚ö° Inference Performance Evaluation Tool&lt;/h2&gt; 
&lt;p&gt;EvalScope provides a powerful stress testing tool for evaluating the performance of large language model services.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Key Metrics&lt;/strong&gt;: Supports throughput (Tokens/s), first token latency (TTFT), token generation latency (TPOT), etc.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Result Recording&lt;/strong&gt;: Supports recording results to &lt;code&gt;wandb&lt;/code&gt; and &lt;code&gt;swanlab&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Speed Benchmarks&lt;/strong&gt;: Can generate speed benchmark results similar to official reports.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For details, please refer to &lt;a href="https://evalscope.readthedocs.io/en/latest/user_guides/stress_test/index.html"&gt;üìñ Performance Testing Usage Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Example output is shown below:&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/modelscope/evalscope/main/docs/en/user_guides/stress_test/images/multi_perf.png" style="width: 80%;" /&gt; &lt;/p&gt; 
&lt;h2&gt;üìä Visualizing Evaluation Results&lt;/h2&gt; 
&lt;p&gt;EvalScope provides a Gradio-based WebUI for interactive analysis and comparison of evaluation results.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install Dependencies&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pip install 'evalscope[app]'
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Start Service&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;evalscope app
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Visit &lt;code&gt;http://127.0.0.1:7861&lt;/code&gt; to open the visualization interface.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td style="text-align: center;"&gt; &lt;img src="https://raw.githubusercontent.com/modelscope/evalscope/main/docs/en/get_started/images/setting.png" alt="Setting" style="width: 85%;" /&gt; &lt;p&gt;Settings Interface&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="text-align: center;"&gt; &lt;img src="https://raw.githubusercontent.com/modelscope/evalscope/main/docs/en/get_started/images/model_compare.png" alt="Model Compare" style="width: 100%;" /&gt; &lt;p&gt;Model Comparison&lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td style="text-align: center;"&gt; &lt;img src="https://raw.githubusercontent.com/modelscope/evalscope/main/docs/en/get_started/images/report_overview.png" alt="Report Overview" style="width: 100%;" /&gt; &lt;p&gt;Report Overview&lt;/p&gt; &lt;/td&gt; 
   &lt;td style="text-align: center;"&gt; &lt;img src="https://raw.githubusercontent.com/modelscope/evalscope/main/docs/en/get_started/images/report_details.png" alt="Report Details" style="width: 85%;" /&gt; &lt;p&gt;Report Details&lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;p&gt;For details, please refer to &lt;a href="https://evalscope.readthedocs.io/en/latest/get_started/visualization.html"&gt;üìñ Visualizing Evaluation Results&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;üë∑‚Äç‚ôÇÔ∏è Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome any contributions from the community! If you want to add new evaluation benchmarks, models, or features, please refer to our &lt;a href="https://evalscope.readthedocs.io/en/latest/advanced_guides/add_benchmark.html"&gt;Contributing Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Thanks to all developers who have contributed to EvalScope!&lt;/p&gt; 
&lt;a href="https://github.com/modelscope/evalscope/graphs/contributors" target="_blank"&gt; 
 &lt;table&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;th colspan="2"&gt; &lt;br /&gt;&lt;img src="https://contrib.rocks/image?repo=modelscope/evalscope" /&gt;&lt;br /&gt;&lt;br /&gt; &lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; &lt;/a&gt; 
&lt;h2&gt;üìö Citation&lt;/h2&gt; 
&lt;p&gt;If you use EvalScope in your research, please cite our work:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{evalscope_2024,
    title={{EvalScope}: Evaluation Framework for Large Models},
    author={ModelScope Team},
    year={2024},
    url={https://github.com/modelscope/evalscope}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;‚≠ê Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#modelscope/evalscope&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=modelscope/evalscope&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>NVIDIA/warp</title>
      <link>https://github.com/NVIDIA/warp</link>
      <description>&lt;p&gt;A Python framework for accelerated simulation, data generation and spatial computing.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href="https://badge.fury.io/py/warp-lang"&gt;&lt;img src="https://badge.fury.io/py/warp-lang.svg?sanitize=true" alt="PyPI version" /&gt;&lt;/a&gt; &lt;a href="https://opensource.org/licenses/Apache-2.0"&gt;&lt;img src="https://img.shields.io/badge/License-Apache_2.0-blue.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/github/commit-activity/m/NVIDIA/warp?link=https%3A%2F%2Fgithub.com%2FNVIDIA%2Fwarp%2Fcommits%2Fmain" alt="GitHub commit activity" /&gt; &lt;a href="https://pepy.tech/project/warp-lang"&gt;&lt;img src="https://static.pepy.tech/badge/warp-lang/month" alt="Downloads" /&gt;&lt;/a&gt; &lt;a href="https://codecov.io/github/NVIDIA/warp"&gt;&lt;img src="https://codecov.io/github/NVIDIA/warp/graph/badge.svg?token=7O1KSM79FG" alt="codecov" /&gt;&lt;/a&gt; &lt;img src="https://github.com/NVIDIA/warp/actions/workflows/ci.yml/badge.svg?sanitize=true" alt="GitHub - CI" /&gt;&lt;/p&gt; 
&lt;h1&gt;NVIDIA Warp&lt;/h1&gt; 
&lt;p&gt;Warp is a Python framework for writing high-performance simulation and graphics code. Warp takes regular Python functions and JIT compiles them to efficient kernel code that can run on the CPU or GPU.&lt;/p&gt; 
&lt;p&gt;Warp is designed for &lt;a href="https://en.wikipedia.org/wiki/Spatial_computing"&gt;spatial computing&lt;/a&gt; and comes with a rich set of primitives that make it easy to write programs for physics simulation, perception, robotics, and geometry processing. In addition, Warp kernels are differentiable and can be used as part of machine-learning pipelines with frameworks such as PyTorch, JAX and Paddle.&lt;/p&gt; 
&lt;p&gt;Please refer to the project &lt;a href="https://nvidia.github.io/warp/"&gt;Documentation&lt;/a&gt; for API and language reference and &lt;a href="https://github.com/NVIDIA/warp/raw/main/CHANGELOG.md"&gt;CHANGELOG.md&lt;/a&gt; for release history.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://github.com/NVIDIA/warp/raw/main/docs/img/header.jpg" /&gt; 
 &lt;p&gt;&lt;i&gt;A selection of physical simulations computed with Warp&lt;/i&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;Installing&lt;/h2&gt; 
&lt;p&gt;Python version 3.9 or newer is recommended. Warp can run on x86-64 and ARMv8 CPUs on Windows, Linux, and macOS. GPU support requires a CUDA-capable NVIDIA GPU and driver (minimum GeForce GTX 9xx).&lt;/p&gt; 
&lt;p&gt;The easiest way to install Warp is from &lt;a href="https://pypi.org/project/warp-lang/"&gt;PyPI&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;pip install warp-lang
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also use &lt;code&gt;pip install warp-lang[examples]&lt;/code&gt; to install additional dependencies for running examples and USD-related features.&lt;/p&gt; 
&lt;p&gt;The binaries hosted on PyPI are currently built with the CUDA 12 runtime. We also provide binaries built with the CUDA 13.0 runtime on the &lt;a href="https://github.com/NVIDIA/warp/releases"&gt;GitHub Releases&lt;/a&gt; page. Copy the URL of the appropriate wheel file (&lt;code&gt;warp-lang-{ver}+cu13-py3-none-{platform}.whl&lt;/code&gt;) and pass it to the &lt;code&gt;pip install&lt;/code&gt; command, e.g.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Platform&lt;/th&gt; 
   &lt;th&gt;Install Command&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Linux aarch64&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;pip install https://github.com/NVIDIA/warp/releases/download/v1.11.0/warp_lang-1.11.0+cu13-py3-none-manylinux_2_34_aarch64.whl&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Linux x86-64&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;pip install https://github.com/NVIDIA/warp/releases/download/v1.11.0/warp_lang-1.11.0+cu13-py3-none-manylinux_2_28_x86_64.whl&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Windows x86-64&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;pip install https://github.com/NVIDIA/warp/releases/download/v1.11.0/warp_lang-1.11.0+cu13-py3-none-win_amd64.whl&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;The &lt;code&gt;--force-reinstall&lt;/code&gt; option may need to be used to overwrite a previous installation.&lt;/p&gt; 
&lt;h3&gt;Nightly Builds&lt;/h3&gt; 
&lt;p&gt;Nightly builds of Warp from the &lt;code&gt;main&lt;/code&gt; branch are available on the &lt;a href="https://pypi.nvidia.com/warp-lang/"&gt;NVIDIA Package Index&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;To install the latest nightly build, use the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;pip install -U --pre warp-lang --extra-index-url=https://pypi.nvidia.com/
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note that the nightly builds are built with the CUDA 12 runtime and are not published for macOS.&lt;/p&gt; 
&lt;p&gt;If you plan to install nightly builds regularly, you can simplify future installations by adding NVIDIA's package repository as an extra index via the &lt;code&gt;PIP_EXTRA_INDEX_URL&lt;/code&gt; environment variable. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;export PIP_EXTRA_INDEX_URL="https://pypi.nvidia.com"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This ensures the index is automatically used for &lt;code&gt;pip&lt;/code&gt; commands, avoiding the need to specify it explicitly.&lt;/p&gt; 
&lt;h3&gt;CUDA Requirements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Warp packages built with CUDA Toolkit 12.x require NVIDIA driver 525 or newer.&lt;/li&gt; 
 &lt;li&gt;Warp packages built with CUDA Toolkit 13.x require NVIDIA driver 580 or newer.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This applies to pre-built packages distributed on PyPI and GitHub and also when building Warp from source.&lt;/p&gt; 
&lt;p&gt;Note that building Warp with the &lt;code&gt;--quick&lt;/code&gt; flag changes the driver requirements. The quick build skips CUDA backward compatibility, so the minimum required driver is determined by the CUDA Toolkit version. Refer to the &lt;a href="https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html"&gt;latest CUDA Toolkit release notes&lt;/a&gt; to find the minimum required driver for different CUDA Toolkit versions (e.g., &lt;a href="https://docs.nvidia.com/cuda/archive/12.6.0/cuda-toolkit-release-notes/index.html#id5"&gt;this table from CUDA Toolkit 12.6&lt;/a&gt;).&lt;/p&gt; 
&lt;p&gt;Warp checks the installed driver during initialization and will report a warning if the driver is not suitable, e.g.:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;Warp UserWarning:
   Insufficient CUDA driver version.
   The minimum required CUDA driver version is 12.0, but the installed CUDA driver version is 11.8.
   Visit https://github.com/NVIDIA/warp/blob/main/README.md#installing for guidance.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will make CUDA devices unavailable, but the CPU can still be used.&lt;/p&gt; 
&lt;p&gt;To remedy the situation there are a few options:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Update the driver.&lt;/li&gt; 
 &lt;li&gt;Install a compatible pre-built Warp package.&lt;/li&gt; 
 &lt;li&gt;Build Warp from source using a CUDA Toolkit that's compatible with the installed driver.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Tutorial Notebooks&lt;/h2&gt; 
&lt;p&gt;The &lt;a href="https://github.com/NVIDIA/accelerated-computing-hub"&gt;NVIDIA Accelerated Computing Hub&lt;/a&gt; contains the current, actively maintained set of Warp tutorials:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Notebook&lt;/th&gt; 
   &lt;th&gt;Colab Link&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/NVIDIA/accelerated-computing-hub/raw/9c334fcfcbbaf8d0cff91d012cdb2c11bf0f3dba/Accelerated_Python_User_Guide/notebooks/Chapter_12_Intro_to_NVIDIA_Warp.ipynb"&gt;Introduction to NVIDIA Warp&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/NVIDIA/accelerated-computing-hub/blob/9c334fcfcbbaf8d0cff91d012cdb2c11bf0f3dba/Accelerated_Python_User_Guide/notebooks/Chapter_12_Intro_to_NVIDIA_Warp.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/NVIDIA/accelerated-computing-hub/raw/9c334fcfcbbaf8d0cff91d012cdb2c11bf0f3dba/Accelerated_Python_User_Guide/notebooks/Chapter_12.1_IsingModel_In_Warp.ipynb"&gt;GPU-Accelerated Ising Model Simulation in NVIDIA Warp&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/NVIDIA/accelerated-computing-hub/blob/9c334fcfcbbaf8d0cff91d012cdb2c11bf0f3dba/Accelerated_Python_User_Guide/notebooks/Chapter_12.1_IsingModel_In_Warp.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Additionally, several notebooks in the &lt;a href="https://github.com/NVIDIA/warp/tree/main/notebooks"&gt;notebooks&lt;/a&gt; directory provide additional examples and cover key Warp features:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Notebook&lt;/th&gt; 
   &lt;th&gt;Colab Link&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/NVIDIA/warp/raw/main/notebooks/core_01_basics.ipynb"&gt;Warp Core Tutorial: Basics&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/NVIDIA/warp/blob/main/notebooks/core_01_basics.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/NVIDIA/warp/raw/main/notebooks/core_02_generics.ipynb"&gt;Warp Core Tutorial: Generics&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/NVIDIA/warp/blob/main/notebooks/core_02_generics.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/NVIDIA/warp/raw/main/notebooks/core_03_points.ipynb"&gt;Warp Core Tutorial: Points&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/NVIDIA/warp/blob/main/notebooks/core_03_points.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/NVIDIA/warp/raw/main/notebooks/core_04_meshes.ipynb"&gt;Warp Core Tutorial: Meshes&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/NVIDIA/warp/blob/main/notebooks/core_04_meshes.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/NVIDIA/warp/raw/main/notebooks/core_05_volumes.ipynb"&gt;Warp Core Tutorial: Volumes&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/NVIDIA/warp/blob/main/notebooks/core_05_volumes.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/NVIDIA/warp/raw/main/notebooks/pytorch_01_basics.ipynb"&gt;Warp PyTorch Tutorial: Basics&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/NVIDIA/warp/blob/main/notebooks/pytorch_01_basics.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/NVIDIA/warp/raw/main/notebooks/pytorch_02_custom_operators.ipynb"&gt;Warp PyTorch Tutorial: Custom Operators&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/NVIDIA/warp/blob/main/notebooks/pytorch_02_custom_operators.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Running Examples&lt;/h2&gt; 
&lt;p&gt;The &lt;a href="https://github.com/NVIDIA/warp/tree/main/warp/examples"&gt;warp/examples&lt;/a&gt; directory contains a number of scripts categorized under subdirectories that show how to implement various simulation methods using the Warp API. Most examples will generate USD files containing time-sampled animations in the current working directory. Before running examples, users should ensure that the &lt;code&gt;usd-core&lt;/code&gt;, &lt;code&gt;matplotlib&lt;/code&gt;, and &lt;code&gt;pyglet&lt;/code&gt; packages are installed using:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;pip install warp-lang[extras]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;These dependencies can also be manually installed using:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;pip install usd-core matplotlib pyglet
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Examples can be run from the command-line as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;python -m warp.examples.&amp;lt;example_subdir&amp;gt;.&amp;lt;example&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To browse the example source code, you can open the directory where the files are located like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;python -m warp.examples.browse
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Most examples can be run on either the CPU or a CUDA-capable device, but a handful require a CUDA-capable device. These are marked at the top of the example script.&lt;/p&gt; 
&lt;p&gt;USD files can be viewed or rendered inside &lt;a href="https://developer.nvidia.com/omniverse"&gt;NVIDIA Omniverse&lt;/a&gt;, Pixar's UsdView, and Blender. Note that Preview in macOS is not recommended as it has limited support for time-sampled animations.&lt;/p&gt; 
&lt;p&gt;Built-in unit tests can be run from the command-line as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;python -m warp.tests
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;warp/examples/core&lt;/h3&gt; 
&lt;table&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/core/example_dem.py"&gt;&lt;img src="https://media.githubusercontent.com/media/NVIDIA/warp/refs/heads/main/docs/img/examples/core_dem.png" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/core/example_fluid.py"&gt;&lt;img src="https://media.githubusercontent.com/media/NVIDIA/warp/refs/heads/main/docs/img/examples/core_fluid.png" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/core/example_graph_capture.py"&gt;&lt;img src="https://media.githubusercontent.com/media/NVIDIA/warp/refs/heads/main/docs/img/examples/core_graph_capture.png" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/core/example_marching_cubes.py"&gt;&lt;img src="https://media.githubusercontent.com/media/NVIDIA/warp/refs/heads/main/docs/img/examples/core_marching_cubes.png" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;dem&lt;/td&gt; 
   &lt;td align="center"&gt;fluid&lt;/td&gt; 
   &lt;td align="center"&gt;graph capture&lt;/td&gt; 
   &lt;td align="center"&gt;marching cubes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/core/example_mesh.py"&gt;&lt;img src="https://media.githubusercontent.com/media/NVIDIA/warp/refs/heads/main/docs/img/examples/core_mesh.png" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/core/example_nvdb.py"&gt;&lt;img src="https://media.githubusercontent.com/media/NVIDIA/warp/refs/heads/main/docs/img/examples/core_nvdb.png" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/core/example_raycast.py"&gt;&lt;img src="https://media.githubusercontent.com/media/NVIDIA/warp/refs/heads/main/docs/img/examples/core_raycast.png" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/core/example_raymarch.py"&gt;&lt;img src="https://media.githubusercontent.com/media/NVIDIA/warp/refs/heads/main/docs/img/examples/core_raymarch.png" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;mesh&lt;/td&gt; 
   &lt;td align="center"&gt;nvdb&lt;/td&gt; 
   &lt;td align="center"&gt;raycast&lt;/td&gt; 
   &lt;td align="center"&gt;raymarch&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/core/example_sample_mesh.py"&gt;&lt;img src="https://media.githubusercontent.com/media/NVIDIA/warp/refs/heads/main/docs/img/examples/core_sample_mesh.png" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/core/example_sph.py"&gt;&lt;img src="https://media.githubusercontent.com/media/NVIDIA/warp/refs/heads/main/docs/img/examples/core_sph.png" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/core/example_torch.py"&gt;&lt;img src="https://media.githubusercontent.com/media/NVIDIA/warp/refs/heads/main/docs/img/examples/core_torch.png" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/core/example_wave.py"&gt;&lt;img src="https://media.githubusercontent.com/media/NVIDIA/warp/refs/heads/main/docs/img/examples/core_wave.png" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;sample mesh&lt;/td&gt; 
   &lt;td align="center"&gt;sph&lt;/td&gt; 
   &lt;td align="center"&gt;torch&lt;/td&gt; 
   &lt;td align="center"&gt;wave&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;warp/examples/fem&lt;/h3&gt; 
&lt;table&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/fem/example_diffusion_3d.py"&gt;&lt;img src="https://media.githubusercontent.com/media/NVIDIA/warp/refs/heads/main/docs/img/examples/fem_diffusion_3d.png" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/fem/example_mixed_elasticity.py"&gt;&lt;img src="https://media.githubusercontent.com/media/NVIDIA/warp/refs/heads/main/docs/img/examples/fem_mixed_elasticity.png" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/fem/example_apic_fluid.py"&gt;&lt;img src="https://media.githubusercontent.com/media/NVIDIA/warp/refs/heads/main/docs/img/examples/fem_apic_fluid.png" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/fem/example_streamlines.py"&gt;&lt;img src="https://media.githubusercontent.com/media/NVIDIA/warp/refs/heads/main/docs/img/examples/fem_streamlines.png" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;diffusion 3d&lt;/td&gt; 
   &lt;td align="center"&gt;mixed elasticity&lt;/td&gt; 
   &lt;td align="center"&gt;apic fluid&lt;/td&gt; 
   &lt;td align="center"&gt;streamlines&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/fem/example_distortion_energy.py"&gt;&lt;img src="https://media.githubusercontent.com/media/NVIDIA/warp/refs/heads/main/docs/img/examples/fem_distortion_energy.png" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/fem/example_navier_stokes.py"&gt;&lt;img src="https://media.githubusercontent.com/media/NVIDIA/warp/refs/heads/main/docs/img/examples/fem_navier_stokes.png" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/fem/example_burgers.py"&gt;&lt;img src="https://media.githubusercontent.com/media/NVIDIA/warp/refs/heads/main/docs/img/examples/fem_burgers.png" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/fem/example_magnetostatics.py"&gt;&lt;img src="https://media.githubusercontent.com/media/NVIDIA/warp/refs/heads/main/docs/img/examples/fem_magnetostatics.png" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;distortion energy&lt;/td&gt; 
   &lt;td align="center"&gt;navier stokes&lt;/td&gt; 
   &lt;td align="center"&gt;burgers&lt;/td&gt; 
   &lt;td align="center"&gt;magnetostatics&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/fem/example_adaptive_grid.py"&gt;&lt;img src="https://media.githubusercontent.com/media/NVIDIA/warp/refs/heads/main/docs/img/examples/fem_adaptive_grid.png" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/fem/example_nonconforming_contact.py"&gt;&lt;img src="https://media.githubusercontent.com/media/NVIDIA/warp/refs/heads/main/docs/img/examples/fem_nonconforming_contact.png" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/fem/example_darcy_ls_optimization.py"&gt;&lt;img src="https://media.githubusercontent.com/media/NVIDIA/warp/refs/heads/main/docs/img/examples/fem_darcy_ls_optimization.png" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/fem/example_elastic_shape_optimization.py"&gt;&lt;img src="https://media.githubusercontent.com/media/NVIDIA/warp/refs/heads/main/docs/img/examples/fem_elastic_shape_optimization.png" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;adaptive grid&lt;/td&gt; 
   &lt;td align="center"&gt;nonconforming contact&lt;/td&gt; 
   &lt;td align="center"&gt;darcy level-set optimization&lt;/td&gt; 
   &lt;td align="center"&gt;elastic shape optimization&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;warp/examples/optim&lt;/h3&gt; 
&lt;table&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/optim/example_diffray.py"&gt;&lt;img src="https://media.githubusercontent.com/media/NVIDIA/warp/refs/heads/main/docs/img/examples/optim_diffray.png" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/optim/example_fluid_checkpoint.py"&gt;&lt;img src="https://media.githubusercontent.com/media/NVIDIA/warp/refs/heads/main/docs/img/examples/optim_fluid_checkpoint.png" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/optim/example_particle_repulsion.py"&gt;&lt;img src="https://media.githubusercontent.com/media/NVIDIA/warp/refs/heads/main/docs/img/examples/optim_particle_repulsion.png" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;diffray&lt;/td&gt; 
   &lt;td align="center"&gt;fluid checkpoint&lt;/td&gt; 
   &lt;td align="center"&gt;particle repulsion&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;warp/examples/tile&lt;/h3&gt; 
&lt;table&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/tile/example_tile_mlp.py"&gt;&lt;img src="https://media.githubusercontent.com/media/NVIDIA/warp/refs/heads/main/docs/img/examples/tile_mlp.png" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/tile/example_tile_nbody.py"&gt;&lt;img src="https://media.githubusercontent.com/media/NVIDIA/warp/refs/heads/main/docs/img/examples/tile_nbody.png" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/NVIDIA/warp/tree/main/warp/examples/tile/example_tile_mcgp.py"&gt;&lt;img src="https://media.githubusercontent.com/media/NVIDIA/warp/refs/heads/main/docs/img/examples/tile_mcgp.png" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;mlp&lt;/td&gt; 
   &lt;td align="center"&gt;nbody&lt;/td&gt; 
   &lt;td align="center"&gt;mcgp&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Building&lt;/h2&gt; 
&lt;p&gt;For developers who want to build the library themselves, the following tools are required:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Microsoft Visual Studio 2019 upwards (Windows)&lt;/li&gt; 
 &lt;li&gt;GCC 9.4 upwards (Linux)&lt;/li&gt; 
 &lt;li&gt;CUDA Toolkit 12.0 or higher&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://git-lfs.github.com/"&gt;Git LFS&lt;/a&gt; installed&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;After cloning the repository, users should run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;python build_lib.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Upon success, the script will output platform-specific binary files in &lt;code&gt;warp/bin/&lt;/code&gt;. The build script will look for the CUDA Toolkit in its default installation path. This path can be overridden by setting the &lt;code&gt;CUDA_PATH&lt;/code&gt; environment variable. Alternatively, the path to the CUDA Toolkit can be passed to the build command as &lt;code&gt;--cuda_path="..."&lt;/code&gt;. After building, the Warp package should be installed using:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This ensures that subsequent modifications to the library will be reflected in the Python package.&lt;/p&gt; 
&lt;h2&gt;Learn More&lt;/h2&gt; 
&lt;p&gt;Please see the following resources for additional background on Warp:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://developer.nvidia.com/warp-python"&gt;Product Page&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://dl.acm.org/doi/10.1145/3664475.3664543"&gt;SIGGRAPH 2024 Course Slides&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.nvidia.com/en-us/on-demand/session/gtc24-s63345/"&gt;GTC 2024 Presentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.nvidia.com/en-us/on-demand/session/gtcspring22-s41599"&gt;GTC 2022 Presentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.nvidia.com/en-us/on-demand/session/gtcspring21-s31838"&gt;GTC 2021 Presentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://dl.acm.org/doi/abs/10.1145/3476117.3483433"&gt;SIGGRAPH Asia 2021 Differentiable Simulation Course&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The underlying technology in Warp has been used in a number of research projects at NVIDIA including the following publications:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Accelerated Policy Learning with Parallel Differentiable Simulation - Xu, J., Makoviychuk, V., Narang, Y., Ramos, F., Matusik, W., Garg, A., &amp;amp; Macklin, M. &lt;a href="https://short-horizon-actor-critic.github.io"&gt;(2022)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;DiSECt: Differentiable Simulator for Robotic Cutting - Heiden, E., Macklin, M., Narang, Y., Fox, D., Garg, A., &amp;amp; Ramos, F &lt;a href="https://github.com/NVlabs/DiSECt"&gt;(2021)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;gradSim: Differentiable Simulation for System Identification and Visuomotor Control - Murthy, J. Krishna, Miles Macklin, Florian Golemo, Vikram Voleti, Linda Petrini, Martin Weiss, Breandan Considine et al. &lt;a href="https://gradsim.github.io"&gt;(2021)&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Frequently Asked Questions&lt;/h2&gt; 
&lt;p&gt;See the &lt;a href="https://nvidia.github.io/warp/faq.html"&gt;FAQ&lt;/a&gt; in the Warp documentation.&lt;/p&gt; 
&lt;h2&gt;Support&lt;/h2&gt; 
&lt;p&gt;Problems, questions, and feature requests can be opened on &lt;a href="https://github.com/NVIDIA/warp/issues"&gt;GitHub Issues&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For inquiries not suited for GitHub Issues, please email &lt;a href="mailto:warp-python@nvidia.com"&gt;warp-python@nvidia.com&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Versioning&lt;/h2&gt; 
&lt;p&gt;Versions take the format X.Y.Z, similar to &lt;a href="https://devguide.python.org/developer-workflow/development-cycle/#devcycle"&gt;Python itself&lt;/a&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Increments in X are reserved for major reworks of the project causing disruptive incompatibility (or reaching the 1.0 milestone).&lt;/li&gt; 
 &lt;li&gt;Increments in Y are for regular releases with a new set of features.&lt;/li&gt; 
 &lt;li&gt;Increments in Z are for bug fixes. In principle, there are no new features. Can be omitted if 0 or not relevant.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This is similar to &lt;a href="https://semver.org/"&gt;Semantic Versioning&lt;/a&gt; but is less strict regarding backward compatibility. Like with Python, some breaking changes can be present between minor versions if well-documented and gradually introduced.&lt;/p&gt; 
&lt;p&gt;Note that prior to 0.11.0, this schema was not strictly adhered to.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Warp is provided under the Apache License, Version 2.0. Please see &lt;a href="https://github.com/NVIDIA/warp/raw/main/LICENSE.md"&gt;LICENSE.md&lt;/a&gt; for full license text.&lt;/p&gt; 
&lt;p&gt;This project will download and install additional third-party open source software projects. Review the license terms of these open source projects before use.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Contributions and pull requests from the community are welcome. Please see the &lt;a href="https://nvidia.github.io/warp/user_guide/contribution_guide.html"&gt;Contribution Guide&lt;/a&gt; for more information on contributing to the development of Warp.&lt;/p&gt; 
&lt;h2&gt;Publications &amp;amp; Citation&lt;/h2&gt; 
&lt;h3&gt;Research Using Warp&lt;/h3&gt; 
&lt;p&gt;Our &lt;a href="https://github.com/NVIDIA/warp/raw/main/PUBLICATIONS.md"&gt;PUBLICATIONS.md&lt;/a&gt; file lists academic and research publications that leverage the capabilities of Warp. We encourage you to add your own published work using Warp to this list.&lt;/p&gt; 
&lt;h3&gt;Citing Warp&lt;/h3&gt; 
&lt;p&gt;To cite Warp itself in your own publications, please use the following BibTeX entry:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{warp2022,
  title        = {Warp: A High-performance Python Framework for GPU Simulation and Graphics},
  author       = {Miles Macklin},
  month        = {March},
  year         = {2022},
  note         = {NVIDIA GPU Technology Conference (GTC)},
  howpublished = {\url{https://github.com/nvidia/warp}}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
  </channel>
</rss>