<rss version="2.0">
  <channel>
    <title>GitHub Python Weekly Trending</title>
    <description>Weekly Trending of Python in GitHub</description>
    <pubDate>Sat, 17 Jan 2026 01:45:31 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>gyoridavid/ai_agents_az</title>
      <link>https://github.com/gyoridavid/ai_agents_az</link>
      <description>&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AI Agents A-Z&lt;/h1&gt; 
&lt;p&gt;In this repo, you can find the n8n templates we created for the episodes of &lt;a href="https://www.youtube.com/channel/UCloXqLhp_KGhHBe1kwaL2Tg"&gt;AI Agents A-Z&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Season 1&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_1"&gt;Episode 1: Creating a prescription agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_2"&gt;Episode 2: Making a daily digest agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_3"&gt;Episode 3: Making LinkedIn posts using Human in the Loop approval process&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_4"&gt;Episode 4: Deep Research Agent using Google&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_5"&gt;Episode 5: Creating a blog writing system using deep research&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_6"&gt;Episode 6: Lead generation with X-Ray search and LinkedIn&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_7"&gt;Episode 7: Creating Youtube short videos using our custom MCP server&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_8"&gt;Episode 8: Creating an AI influencer on Instagram using n8n&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_9"&gt;Episode 9: Create revenge story videos for YouTube&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_10"&gt;Episode 10: n8n best practices&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_11"&gt;Episode 11: Create short (motivational) stories for YouTube and TikTok&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_12"&gt;Episode 12: Scheduling social media posts with Postiz and n8n&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_13"&gt;Episode 13: Create AI videos with MiniMax Hailuo 2 and n8n&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_14"&gt;Episode 14: Create AI videos with Seedance and n8n&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_15"&gt;Episode 15: Generate AI startup ideas from Reddit&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_16"&gt;Episode 16: Create AI poem videos with n8n for TikTok&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_17"&gt;Episode 17: Create Shopify product videos with Seedance, ElevenLabs, Latentsync, Flux Kontext and n8n&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_18"&gt;Episode 18: Scary story TikTok videos workflow&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_19"&gt;Episode 19: Run FLUX.1 Kontext [dev] with modal.com&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_20"&gt;Episode 20: Use Wan 2.2, ComfyUI and n8n to generate videos for free&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_21"&gt;Episode 21: 10 EASY faceless niches that pay well - monetize in a MONTH (2025)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_22"&gt;Episode 22: Sleep long-form videos with GPT-5, ElevenMusic, Imagen4, Seendance and n8n&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_23"&gt;Episode 23: UGC videos with nanobanana and n8n&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_24"&gt;Episode 24: generate images with Qwen Image, Flux.1 [dev] and Flux.1 Schnell with modal.com and Cloudflare Workers AI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_25"&gt;Episode 25: Fal.ai n8n subworkflows for Qwen Image Edit Plus and Wan 2.2 animate&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_31"&gt;Episode 31: Veo 3.1 is now in n8n - how to use it for FREE&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_35"&gt;Episode 35: Instagram influencer machine&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_36"&gt;Episode 36: Viral bodycam footage creator with Sora 2&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_38"&gt;Episode 38: Create AI reaction videos with Veo 3.1 and n8n&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_39"&gt;Episode 39: Create infographics with Nano Banana Pro in n8n&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_40"&gt;Episode 40: Flux.2[dev] with n8n&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_41"&gt;Episode 41: FREE z-image-turbo with n8n&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_42"&gt;Episode 42: 100% FREE explainer videos with n8n and Z-Image&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;servers&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://hub.docker.com/r/gyoridavid/ai-agents-no-code-tools"&gt;AI Agents No-Code Tools&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/gyoridavid/short-video-maker"&gt;Short video maker MCP/REST server&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://hub.docker.com/r/gyoridavid/narrated-story-creator"&gt;Narrated story creator REST/MCP server&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>MiroMindAI/MiroThinker</title>
      <link>https://github.com/MiroMindAI/MiroThinker</link>
      <description>&lt;p&gt;MiroThinker is an open source deep research agent optimized for research and prediction. It achieves a 60.2% Avg@8 score on the challenging GAIA benchmark.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/assets/miro_thinker.png" width="55%" alt="MiroThinker" /&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://dr.miromind.ai/"&gt;&lt;img src="https://img.shields.io/badge/Demo-FFB300?style=for-the-badge&amp;amp;logo=airplayvideo&amp;amp;logoColor=white" alt="DEMO" /&gt;&lt;/a&gt; &lt;a href="https://miromind.ai/#blog"&gt;&lt;img src="https://img.shields.io/badge/Blog-4285F4?style=for-the-badge&amp;amp;logo=google-chrome&amp;amp;logoColor=white" alt="Blog" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/datasets/miromind-ai/MiroVerse-v0.1"&gt;&lt;img src="https://img.shields.io/badge/Data-0040A1?style=for-the-badge&amp;amp;logo=huggingface&amp;amp;logoColor=ffffff&amp;amp;labelColor" alt="DATA" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/MiroMindAI"&gt;&lt;img src="https://img.shields.io/badge/Github-24292F?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white" alt="GITHUB" /&gt;&lt;/a&gt; &lt;a href="https://miromind.ai/"&gt;&lt;img src="https://img.shields.io/badge/Website-4285F4?style=for-the-badge&amp;amp;logo=google-chrome&amp;amp;logoColor=white" alt="WEBSITE" /&gt;&lt;/a&gt; &lt;a href="https://discord.com/invite/GPqEnkzQZd"&gt;&lt;img src="https://img.shields.io/badge/Discord-5865F2?style=for-the-badge&amp;amp;logo=discord&amp;amp;logoColor=white" alt="DISCORD" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;h3&gt;ğŸš€ &lt;a href="https://dr.miromind.ai/"&gt;Try our Demo!&lt;/a&gt;&lt;/h3&gt; 
&lt;/div&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;MiroThinker&lt;/strong&gt; is an open source deep research agent optimized for research and prediction. It achieves a 60.2% Avg@8 score on the challenging GAIA benchmark.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;The project currently comprises four key components:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ’¡ &lt;strong&gt;MiroThinker&lt;/strong&gt;: An open source deep research agent optimized for research and prediction. It achieves a 60.2% Avg@8 score on the challenging GAIA benchmark. See &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/#-quick-start"&gt;Quick Start&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;ğŸ¤– &lt;strong&gt;MiroFlow&lt;/strong&gt;: An agent framework that enables tool-use agent tasks, featuring a reproducible GAIA score of 82.4%. See &lt;a href="https://github.com/MiroMindAI/MiroFlow"&gt;MiroFlow&lt;/a&gt; for details.&lt;/li&gt; 
 &lt;li&gt;ğŸ“š &lt;strong&gt;MiroVerse&lt;/strong&gt;: A premium open-source training dataset with 147k samples supporting research agent training. See &lt;a href="https://huggingface.co/datasets/miromind-ai/MiroVerse-v0.1"&gt;MiroVerse&lt;/a&gt; on HuggingFace.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ“‹ Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“° &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/#-news--updates"&gt;News &amp;amp; Updates&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/#-introduction"&gt;Introduction&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;âœ¨ &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/#-key-features"&gt;Key Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸ“ˆ &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/#-performance-on-benchmarks"&gt;Performance on Benchmarks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸš€ &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/#-quick-start"&gt;Quick Start&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸ“Š &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/#-benchmark-evaluation"&gt;Benchmark Evaluation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸ”¬ &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/#-trace-collection"&gt;Trace Collection&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;â“ &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/#-faq--troubleshooting"&gt;FAQ &amp;amp; Troubleshooting&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸ“„ &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/#-license"&gt;License&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸ™ &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/#-acknowledgments"&gt;Acknowledgments&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ“° News &amp;amp; Updates&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;[2026-01-05]&lt;/strong&gt; ğŸ‰ğŸ‰ We release &lt;a href="https://huggingface.co/collections/miromind-ai/mirothinker-v15"&gt;MiroThinker-v1.5&lt;/a&gt;, a series of open source deep research agents optimized for financial prediction. &lt;a href="https://huggingface.co/miromind-ai/MiroThinker-v1.5-30B"&gt;MiroThinker-v1.5-30B&lt;/a&gt; surpasses Kimi-K2-Thinking on BrowseComp-ZH at much lower cost, using only 1/30 of the parameters. &lt;a href="https://huggingface.co/miromind-ai/MiroThinker-v1.5-235B"&gt;MiroThinker-v1.5-235B&lt;/a&gt; scores 39.2% on HLE-Text, 69.8% on BrowseComp, 71.5% on BrowseComp-ZH, and 80.8% on GAIA-Val-165, setting a new state-of-the-art among search agents.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025-11-13]&lt;/strong&gt; ğŸ‰ &lt;a href="https://huggingface.co/collections/miromind-ai/mirothinker-v10"&gt;MiroThinker-v1.0&lt;/a&gt; is now released! Introducing &lt;strong&gt;interactive scaling&lt;/strong&gt; as a third dimension of performance improvement, MiroThinker v1.0 supports 256K context window and up to 600 tool calls per task. Available in 8B, 30B, and 72B parameter scales, achieving 37.7%, 47.1%, 55.6%, and 81.9% on HLE-Text, BrowseComp, BrowseComp-ZH, and GAIA-Text-103, respectively. See &lt;a href="https://arxiv.org/abs/2511.11793"&gt;Technical Report&lt;/a&gt; for more details.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025-09-11]&lt;/strong&gt; MiroThinker-72B-Preview ranked 4th in this week's FutureX benchmark. See &lt;a href="https://futurex-ai.github.io/"&gt;FutureX&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;ğŸ“œ Click to expand older updates&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;[2025-09-08]&lt;/strong&gt; &lt;a href="https://huggingface.co/collections/miromind-ai/mirothinker-v02"&gt;MiroThinker-v0.2&lt;/a&gt; is now released, achieving open-source SOTA performance across multiple benchmarks, including HLE (17.8%), HLE-Text-Only (19.1%), BrowseComp-EN (17.2%), BrowseComp-ZH (29.4%), XBench-DeepSearch (56.0%), and Frames (74.8%).&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;[2025-09-07]&lt;/strong&gt; We supported more benchmarks, including &lt;a href="https://arxiv.org/abs/2504.19314"&gt;BrowseComp-ZH&lt;/a&gt;, &lt;a href="https://xbench.org/agi/aisearch"&gt;XBench-DeepSearch&lt;/a&gt;, and &lt;a href="https://futurex-ai.github.io/"&gt;FutureX&lt;/a&gt;. We plan to add more benchmarks in the future.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;[2025-08-22]&lt;/strong&gt; Introducing streamlined deployment options for MiroThinker with optimized resource usage and faster startup times. Experience the interactive demo: &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/apps/gradio-demo"&gt;ğŸš€ Try Gradio Demo&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;[2025-08-08]&lt;/strong&gt; &lt;a href="https://huggingface.co/collections/miromind-ai/mirothinker-v01-689301b6d0563321862d44a1"&gt;MiroThinker-v0.1&lt;/a&gt; released.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;ğŸ“ Introduction&lt;/h2&gt; 
&lt;h3&gt;MiroThinker-v1.5&lt;/h3&gt; 
&lt;p&gt;MiroThinker v1.5 is the world-leading open-source search agent that advances tool-augmented reasoning through &lt;strong&gt;interactive scaling&lt;/strong&gt; â€” training the agent to handle deeper and more frequent agent-environment interactions as a third dimension of performance improvement, beyond model size and context length.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://huggingface.co/datasets/miromind-ai/MiroFlow-Benchmarks/resolve/main/assets/mirothinker_v1.5_framework.png" alt="image" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Key Features&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸš€ MiroThinker v1.5 supports a 256K context window, long-horizon reasoning, and deep multi-step analysis.&lt;/li&gt; 
 &lt;li&gt;ğŸ”§ Handles up to 400 tool calls per task â€” a substantial improvement over previous open-source research agents.&lt;/li&gt; 
 &lt;li&gt;ğŸ“¦ Released in 30B and 235B parameter scales, accompanied by a comprehensive suite of tools and workflows to flexibly support diverse research settings and compute budgets.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;Agent Name&lt;/th&gt; 
    &lt;th align="center"&gt;Base Agent&lt;/th&gt; 
    &lt;th align="center"&gt;Max Context&lt;/th&gt; 
    &lt;th align="center"&gt;Max Tool Calls&lt;/th&gt; 
    &lt;th align="center"&gt;HF Link&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;MiroThinker-v1.5-30B&lt;/td&gt; 
    &lt;td align="center"&gt;Qwen3-30B-A3B-Thinking-2507&lt;/td&gt; 
    &lt;td align="center"&gt;256K&lt;/td&gt; 
    &lt;td align="center"&gt;400&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-v1.5-30B"&gt;ğŸ¤— link&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;MiroThinker-v1.5-235B&lt;/td&gt; 
    &lt;td align="center"&gt;Qwen3-235B-A22B-Thinking-2507&lt;/td&gt; 
    &lt;td align="center"&gt;256K&lt;/td&gt; 
    &lt;td align="center"&gt;400&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-v1.5-235B"&gt;ğŸ¤— link&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;p&gt;MiroThinker v1.5 demonstrates strong general-research performance across a broad range of benchmarks, achieving&amp;nbsp;39.2%,&amp;nbsp;69.8%, 71.5%, and&amp;nbsp;80.8%&amp;nbsp;on HLE-Text, BrowseComp, BrowseComp-ZH, and GAIA-Val-165, respectively. These results surpass previous open-source agents and set the new world-leading BrowseComp performance.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://huggingface.co/datasets/miromind-ai/MiroFlow-Benchmarks/resolve/main/assets/mirothinker_v1.5_browsecomp.png" alt="image" /&gt;&lt;/p&gt; 
&lt;h3&gt;MiroThinker-v1.0&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;ğŸ“¦ Click to expand MiroThinker-v1.0 details&lt;/summary&gt; 
 &lt;p&gt;Unlike previous agents that scale only model size or context length, MiroThinker v1.0 introduces &lt;strong&gt;interactive scaling&lt;/strong&gt; at the agent level, systematically training the agent to handle deeper and more frequent agentâ€“environment interactions as a third dimension of performance improvement. Interactive scaling leverages environment feedback and external information acquisition to correct errors and refine trajectories.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://huggingface.co/datasets/miromind-ai/MiroFlow-Benchmarks/resolve/main/assets/MiroThinker_v1.0_Overall.png" alt="image" /&gt;&lt;/p&gt; 
 &lt;h3&gt;âœ¨ Key Features&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;ğŸš€ &lt;strong&gt;256K Context Window&lt;/strong&gt;: Supports long-horizon reasoning and deep multi-step analysis&lt;/li&gt; 
  &lt;li&gt;ğŸ”§ &lt;strong&gt;600 Tool Calls&lt;/strong&gt;: Handles up to 600 tool calls per task â€” a substantial improvement over previous open-source research agents&lt;/li&gt; 
  &lt;li&gt;ğŸ“¦ &lt;strong&gt;Multiple Scales&lt;/strong&gt;: Released in 8B, 30B, and 72B parameter scales, accompanied by a comprehensive suite of tools and workflows to flexibly support diverse research settings and compute budgets&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;div align="center"&gt; 
  &lt;table&gt; 
   &lt;thead&gt; 
    &lt;tr&gt; 
     &lt;th align="center"&gt;Agent Name&lt;/th&gt; 
     &lt;th align="center"&gt;Base Agent&lt;/th&gt; 
     &lt;th align="center"&gt;Max Context&lt;/th&gt; 
     &lt;th align="center"&gt;Max Tool Calls&lt;/th&gt; 
     &lt;th align="center"&gt;HF Link&lt;/th&gt; 
    &lt;/tr&gt; 
   &lt;/thead&gt; 
   &lt;tbody&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-v1.0-8B&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen3-8B&lt;/td&gt; 
     &lt;td align="center"&gt;256K&lt;/td&gt; 
     &lt;td align="center"&gt;600&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-v1.0-8B"&gt;ğŸ¤— link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-v1.0-30B&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen3-30B-A3B-Thinking-2507&lt;/td&gt; 
     &lt;td align="center"&gt;256K&lt;/td&gt; 
     &lt;td align="center"&gt;600&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-v1.0-30B"&gt;ğŸ¤— link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-v1.0-72B&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen2.5-72B-Instruct&lt;/td&gt; 
     &lt;td align="center"&gt;256K&lt;/td&gt; 
     &lt;td align="center"&gt;600&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-v1.0-72B"&gt;ğŸ¤— link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
   &lt;/tbody&gt; 
  &lt;/table&gt; 
 &lt;/div&gt; 
 &lt;p&gt;MiroThinker v1.0 demonstrates strong general-research performance across a broad range of benchmarks, achieving &lt;strong&gt;37.7%&lt;/strong&gt;, &lt;strong&gt;47.1%&lt;/strong&gt;, &lt;strong&gt;55.6%&lt;/strong&gt;, and &lt;strong&gt;81.9%&lt;/strong&gt; on HLE-Text, BrowseComp, BrowseComp-ZH, and GAIA-Text-103, respectively. These results surpass previous open-source agents and narrow the gap with commercial counterparts such as &lt;strong&gt;GPT-5-high&lt;/strong&gt;.&lt;/p&gt; 
 &lt;div align="center"&gt; 
  &lt;img src="https://huggingface.co/datasets/miromind-ai/MiroFlow-Benchmarks/resolve/main/assets/MiroThinker_v1.0_Performance_1.png" width="100%" alt="MiroThinker" /&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;h3&gt;MiroThinker-v0.2&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;ğŸ“¦ Click to expand MiroThinker-v0.2 details&lt;/summary&gt; 
 &lt;p&gt;In this new version, we introduced three key improvements:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;ğŸ“š &lt;strong&gt;Richer training data&lt;/strong&gt; from both English and Chinese sources, yielding significant gains in benchmark performance and generalization&lt;/li&gt; 
  &lt;li&gt;ğŸ¯ &lt;strong&gt;Unified DPO training&lt;/strong&gt; with a single preference dataset across all agents&lt;/li&gt; 
  &lt;li&gt;ğŸ“ &lt;strong&gt;Extended context length&lt;/strong&gt; from 40k to 64k for more challenging multi-turn tool-use tasks&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;Compared to v0.1, MiroThinker v0.2 delivers consistent gains across benchmarks. For example, scores improved from &lt;strong&gt;57.3 â†’ 64.1&lt;/strong&gt; on &lt;strong&gt;GAIA-Text-103&lt;/strong&gt; and from &lt;strong&gt;17.0 â†’ 29.4&lt;/strong&gt; on &lt;strong&gt;BrowseComp-ZH&lt;/strong&gt;, reflecting substantial advancements in the modelâ€™s general research agent capabilities.&lt;/p&gt; 
 &lt;div align="center"&gt; 
  &lt;table&gt; 
   &lt;thead&gt; 
    &lt;tr&gt; 
     &lt;th align="center"&gt;Agent Name&lt;/th&gt; 
     &lt;th align="center"&gt;Base Agent&lt;/th&gt; 
     &lt;th align="center"&gt;Max Context&lt;/th&gt; 
     &lt;th align="center"&gt;HF Link&lt;/th&gt; 
    &lt;/tr&gt; 
   &lt;/thead&gt; 
   &lt;tbody&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-4B-SFT-v0.2&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen3-4B&lt;/td&gt; 
     &lt;td align="center"&gt;64K&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-4B-SFT-v0.2"&gt;ğŸ¤— link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-4B-DPO-v0.2&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen3-4B&lt;/td&gt; 
     &lt;td align="center"&gt;64K&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-4B-DPO-v0.2"&gt;ğŸ¤— link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-8B-SFT-v0.2&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen3-8B&lt;/td&gt; 
     &lt;td align="center"&gt;64K&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-8B-SFT-v0.2"&gt;ğŸ¤— link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-8B-DPO-v0.2&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen3-8B&lt;/td&gt; 
     &lt;td align="center"&gt;64K&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-8B-DPO-v0.2"&gt;ğŸ¤— link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-14B-SFT-v0.2&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen3-14B&lt;/td&gt; 
     &lt;td align="center"&gt;64K&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-14B-SFT-v0.2"&gt;ğŸ¤— link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-14B-DPO-v0.2&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen3-14B&lt;/td&gt; 
     &lt;td align="center"&gt;64K&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-14B-DPO-v0.2"&gt;ğŸ¤— link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-32B-SFT-v0.2&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen3-32B&lt;/td&gt; 
     &lt;td align="center"&gt;64K&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-32B-SFT-v0.2"&gt;ğŸ¤— link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-32B-DPO-v0.2&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen3-32B&lt;/td&gt; 
     &lt;td align="center"&gt;64K&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-32B-DPO-v0.2"&gt;ğŸ¤— link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
   &lt;/tbody&gt; 
  &lt;/table&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;h3&gt;MiroThinker-v0.1&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;ğŸ“¦ Click to expand MiroThinker-v0.1 details&lt;/summary&gt; 
 &lt;div align="center"&gt; 
  &lt;img src="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/assets/gaia_text_103.png" width="98%" alt="MiroFlow Performance on GAIA-Validation" /&gt; 
  &lt;p&gt;&lt;strong&gt;Performance of Open-Source Agents on GAIA-Validation Benchmark.&lt;/strong&gt;&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;p&gt;We have released the &lt;strong&gt;MiroThinker v0.1&lt;/strong&gt; series, including both SFT and DPO variants at parameter scales of &lt;strong&gt;8B&lt;/strong&gt;, &lt;strong&gt;14B&lt;/strong&gt;, and &lt;strong&gt;32B&lt;/strong&gt;. Notably, MiroThinker v0.1 achieves &lt;strong&gt;state-of-the-art performance&lt;/strong&gt; among open-source models on the &lt;a href="https://huggingface.co/datasets/gaia-benchmark/GAIA"&gt;GAIA benchmark&lt;/a&gt;, a rigorous evaluation suite for advanced agentic capabilities, demonstrating its strength in long-context, decision-intensive, and real-world task scenarios.&lt;/p&gt; 
 &lt;div align="center"&gt; 
  &lt;table&gt; 
   &lt;thead&gt; 
    &lt;tr&gt; 
     &lt;th align="center"&gt;Agent Name&lt;/th&gt; 
     &lt;th align="center"&gt;Base Agent&lt;/th&gt; 
     &lt;th align="center"&gt;Max Context&lt;/th&gt; 
     &lt;th align="center"&gt;HF Link&lt;/th&gt; 
    &lt;/tr&gt; 
   &lt;/thead&gt; 
   &lt;tbody&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-8B-SFT-v0.1&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen3-8B&lt;/td&gt; 
     &lt;td align="center"&gt;40K&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-8B-SFT-v0.1"&gt;ğŸ¤— link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-8B-DPO-v0.1&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen3-8B&lt;/td&gt; 
     &lt;td align="center"&gt;40K&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-8B-DPO-v0.1"&gt;ğŸ¤— link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-14B-SFT-v0.1&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen3-14B&lt;/td&gt; 
     &lt;td align="center"&gt;40K&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-14B-SFT-v0.1"&gt;ğŸ¤— link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-14B-DPO-v0.1&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen3-14B&lt;/td&gt; 
     &lt;td align="center"&gt;40K&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-14B-DPO-v0.1"&gt;ğŸ¤— link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-32B-SFT-v0.1&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen3-32B&lt;/td&gt; 
     &lt;td align="center"&gt;40K&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-32B-SFT-v0.1"&gt;ğŸ¤— link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td align="center"&gt;MiroThinker-32B-DPO-v0.1&lt;/td&gt; 
     &lt;td align="center"&gt;Qwen3-32B&lt;/td&gt; 
     &lt;td align="center"&gt;40K&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;a href="https://huggingface.co/miromind-ai/MiroThinker-32B-DPO-v0.1"&gt;ğŸ¤— link&lt;/a&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
   &lt;/tbody&gt; 
  &lt;/table&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;h2&gt;âœ¨ Key Features&lt;/h2&gt; 
&lt;h3&gt;ğŸ¤– &lt;strong&gt;MiroThinker-Optimized Framework&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ”“ &lt;strong&gt;Fully Open-Source Agent Framework&lt;/strong&gt;: Complete transparency with open framework and open agents&lt;/li&gt; 
 &lt;li&gt;ğŸ”— &lt;strong&gt;Tool Integration&lt;/strong&gt;: Seamless integration with external tools and APIs&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;strong&gt;Trace Collection&lt;/strong&gt;: Comprehensive logging and analysis of agent interactions with elapsed time and estimated completion time displayed in minutes. Ready for SFT and DPO&lt;/li&gt; 
 &lt;li&gt;ğŸ“Š &lt;strong&gt;Benchmark Evaluation&lt;/strong&gt;: Extensive testing across multiple benchmark datasets&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ“Š &lt;strong&gt;Comprehensive Benchmark Suite&lt;/strong&gt;&lt;/h3&gt; 
&lt;details open&gt; 
 &lt;summary&gt;ğŸ“‹ Click to expand benchmark list&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;GAIA Validation&lt;/strong&gt;: A benchmark for General AI Assistants. (&lt;a href="https://arxiv.org/abs/2311.12983"&gt;paper&lt;/a&gt;)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;GAIA-Text-103&lt;/strong&gt;: A subset of GAIA Validation for text-only tasks. (&lt;a href="https://arxiv.org/abs/2505.22648"&gt;paper&lt;/a&gt;)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;HLE&lt;/strong&gt;: Humanity's Last Exam. (&lt;a href="https://arxiv.org/abs/2501.14249"&gt;paper&lt;/a&gt;)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;HLE-Text-2158&lt;/strong&gt;: A subset of HLE for text-only tasks. (&lt;a href="https://arxiv.org/abs/2501.14249"&gt;paper&lt;/a&gt;)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;HLE-Text-500&lt;/strong&gt;: A subset of HLE for text-only tasks, created by &lt;a href="https://arxiv.org/pdf/2504.21776"&gt;WebThinker&lt;/a&gt;. (&lt;a href="https://arxiv.org/pdf/2504.21776"&gt;paper&lt;/a&gt;)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;BrowseComp-EN&lt;/strong&gt;: Web browsing and comprehension tasks. (&lt;a href="https://arxiv.org/abs/2504.12516"&gt;paper&lt;/a&gt;)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;BrowseComp-ZH&lt;/strong&gt;: A Chinese version of BrowseComp. (&lt;a href="https://arxiv.org/abs/2504.19314"&gt;paper&lt;/a&gt;)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;WebWalkerQA&lt;/strong&gt;: Web navigation and question answering. (&lt;a href="https://arxiv.org/abs/2501.07572"&gt;paper&lt;/a&gt;)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Frames&lt;/strong&gt;: Factuality, Retrieval, And reasoning MEasurement Set. (&lt;a href="https://arxiv.org/abs/2409.12941"&gt;paper&lt;/a&gt;)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;XBench-DeepSearch&lt;/strong&gt;: A benchmark for deep research agents. (&lt;a href="https://xbench.org/agi/aisearch"&gt;website&lt;/a&gt;)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;FutureX&lt;/strong&gt;: A live benchmark designed for predicting unknown future. (&lt;a href="https://futurex-ai.github.io/"&gt;website&lt;/a&gt;)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;SEAL-0&lt;/strong&gt;: A benchmark for evaluating LLMs on conflicting-evidence web questions. (&lt;a href="https://arxiv.org/abs/2506.01062"&gt;paper&lt;/a&gt;)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;AIME2025&lt;/strong&gt;: American Invitational Mathematics Examination 2025. (&lt;a href="https://artificialanalysis.ai/evaluations/aime-2025"&gt;website&lt;/a&gt;)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;DeepSearchQA&lt;/strong&gt;: Google's Deep Search Question Answering benchmark. (&lt;a href="https://arxiv.org/abs/2505.20827"&gt;paper&lt;/a&gt;)&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;ğŸ“ˆ Performance on Benchmarks&lt;/h2&gt; 
&lt;h3&gt;MiroThinker-v1.5&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;To prevent potential information leakage (e.g., searching benchmark answers from HuggingFace), access to HuggingFace has been explicitly disabled in these tools.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;We further perform canary string testing on the tool outputs of all trajectories and disregard any trajectory found to be contaminated, treating it as an incorrect answer.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;div&gt; 
 &lt;img src="https://huggingface.co/datasets/miromind-ai/MiroFlow-Benchmarks/resolve/main/assets/mirothinker_v1.5_performance.png" width="100%" alt="MiroThinker" /&gt; 
&lt;/div&gt; 
&lt;h3&gt;MiroThinker-v1.0&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;ğŸ“¦ Click to expand MiroThinker-v1.0 details&lt;/summary&gt; 
 &lt;div align="center"&gt; 
  &lt;img src="https://github.com/user-attachments/assets/108a2105-4e1d-499e-a001-4713a03fd8ac" width="100%" alt="MiroThinker" /&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;h3&gt;MiroThinker-v0.2&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;ğŸ“¦ Click to expand MiroThinker-v0.2 details&lt;/summary&gt; 
 &lt;h4&gt;Comparison with SOTA Research Agents&lt;/h4&gt; 
 &lt;div align="center"&gt; 
  &lt;img src="https://huggingface.co/datasets/miromind-ai/MiroFlow-Benchmarks/resolve/main/assets/MiroThinker_v0.2_Performance_2.png" width="90%" alt="MiroThinker" /&gt; 
 &lt;/div&gt; 
 &lt;h4&gt;GAIA Benchmark&lt;/h4&gt; 
 &lt;div align="center"&gt; 
  &lt;img src="https://huggingface.co/datasets/miromind-ai/MiroFlow-Benchmarks/resolve/main/assets/MiroThinker_v0.2_Performance_1.png" width="80%" alt="MiroThinker" /&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;h3&gt;MiroThinker-v0.1&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;ğŸ“¦ Click to expand MiroThinker-v0.1 details&lt;/summary&gt; 
 &lt;h4&gt;GAIA Benchmark&lt;/h4&gt; 
 &lt;div align="center"&gt; 
  &lt;table&gt; 
   &lt;thead&gt; 
    &lt;tr&gt; 
     &lt;th&gt;&lt;strong&gt;Method&lt;/strong&gt;&lt;/th&gt; 
     &lt;th align="center"&gt;Text-103&lt;br /&gt;Best Pass@1&lt;/th&gt; 
     &lt;th align="center"&gt;Text-103&lt;br /&gt;Pass@1 (Avg@8)&lt;/th&gt; 
     &lt;th align="center"&gt;Val-165&lt;br /&gt;Best Pass@1&lt;/th&gt; 
     &lt;th align="center"&gt;Val-165&lt;br /&gt;Pass@1 (Avg@8)&lt;/th&gt; 
    &lt;/tr&gt; 
   &lt;/thead&gt; 
   &lt;tbody&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;ğŸ”¹â€”â€” 7B/8B Agents â€”â€”&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;Search-o1-7B&lt;/td&gt; 
     &lt;td align="center"&gt;17.5&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;R1-Searcher-7B&lt;/td&gt; 
     &lt;td align="center"&gt;20.4&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;WebDancer-7B&lt;/td&gt; 
     &lt;td align="center"&gt;31.0&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;WebSailor-7B&lt;/td&gt; 
     &lt;td align="center"&gt;37.9&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;CK-Pro-8B&lt;/td&gt; 
     &lt;td align="center"&gt;40.3&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;32.7&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;MiroThinker-8B-SFT-v0.1&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;44.7&lt;/td&gt; 
     &lt;td align="center"&gt;40.1&lt;/td&gt; 
     &lt;td align="center"&gt;34.6&lt;/td&gt; 
     &lt;td align="center"&gt;31.8&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;+ Commercial Tools&lt;/td&gt; 
     &lt;td align="center"&gt;46.6&lt;/td&gt; 
     &lt;td align="center"&gt;42.1&lt;/td&gt; 
     &lt;td align="center"&gt;37.6&lt;/td&gt; 
     &lt;td align="center"&gt;33.9&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;MiroThinker-8B-DPO-v0.1&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;46.6&lt;/td&gt; 
     &lt;td align="center"&gt;44.8&lt;/td&gt; 
     &lt;td align="center"&gt;37.0&lt;/td&gt; 
     &lt;td align="center"&gt;35.4&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;+ Commercial Tools&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;strong&gt;50.5&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;strong&gt;46.7&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;strong&gt;38.2&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;strong&gt;35.9&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;ğŸ”¹â€”â€” 14B Agents â€”â€”&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;MiroThinker-14B-SFT-v0.1&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;47.6&lt;/td&gt; 
     &lt;td align="center"&gt;44.4&lt;/td&gt; 
     &lt;td align="center"&gt;37.0&lt;/td&gt; 
     &lt;td align="center"&gt;34.4&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;+ Commercial Tools&lt;/td&gt; 
     &lt;td align="center"&gt;49.5&lt;/td&gt; 
     &lt;td align="center"&gt;47.5&lt;/td&gt; 
     &lt;td align="center"&gt;41.8&lt;/td&gt; 
     &lt;td align="center"&gt;39.8&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;MiroThinker-14B-DPO-v0.1&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;48.5&lt;/td&gt; 
     &lt;td align="center"&gt;46.6&lt;/td&gt; 
     &lt;td align="center"&gt;42.4&lt;/td&gt; 
     &lt;td align="center"&gt;39.2&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;+ Commercial Tools&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;strong&gt;52.4&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;strong&gt;48.5&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;strong&gt;45.5&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;strong&gt;42.0&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;ğŸ”¹â€”â€” 32B Agents â€”â€”&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;Qwen3-32B&lt;/td&gt; 
     &lt;td align="center"&gt;31.1&lt;/td&gt; 
     &lt;td align="center"&gt;26.7&lt;/td&gt; 
     &lt;td align="center"&gt;29.7&lt;/td&gt; 
     &lt;td align="center"&gt;26.4&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;Search-o1-32B&lt;/td&gt; 
     &lt;td align="center"&gt;28.2&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;WebThinker-32B-RL&lt;/td&gt; 
     &lt;td align="center"&gt;48.5&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;WebDancer-QwQ-32B&lt;/td&gt; 
     &lt;td align="center"&gt;51.5&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;WebSailor-32B&lt;/td&gt; 
     &lt;td align="center"&gt;53.2&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;WebShaper-QwQ-32B&lt;/td&gt; 
     &lt;td align="center"&gt;53.3&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;MiroThinker-32B-SFT-v0.1&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;55.3&lt;/td&gt; 
     &lt;td align="center"&gt;51.3&lt;/td&gt; 
     &lt;td align="center"&gt;44.9&lt;/td&gt; 
     &lt;td align="center"&gt;42.7&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;+ Commercial Tools&lt;/td&gt; 
     &lt;td align="center"&gt;58.3&lt;/td&gt; 
     &lt;td align="center"&gt;54.2&lt;/td&gt; 
     &lt;td align="center"&gt;48.5&lt;/td&gt; 
     &lt;td align="center"&gt;45.8&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;MiroThinker-32B-DPO-v0.1&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;57.3&lt;/td&gt; 
     &lt;td align="center"&gt;54.1&lt;/td&gt; 
     &lt;td align="center"&gt;48.5&lt;/td&gt; 
     &lt;td align="center"&gt;45.9&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;+ Commercial Tools&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;strong&gt;60.2&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;strong&gt;57.9&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;strong&gt;50.9&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;strong&gt;48.9&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
   &lt;/tbody&gt; 
  &lt;/table&gt; 
 &lt;/div&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;Following the practices of WebThinker, WebAgents, and CognitiveKernel, we report the Best Pass@1, the highest score across three runs, which often reflects stronger performance, though it may exhibit some variability. To provide a more stable measure, we additionally report Pass@1 (Avg@8), which offers greater consistency at the cost of slightly lower scores.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;For consistency with prior open-source works, we evaluate GAIA-Text-103 using the WebAgents LLM-as-a-Judge template, and report results on GAIA-Val-165 using the official GAIA scorer script.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;By default, we use open-source tools wherever possible, except for the code tool &lt;a href="https://github.com/e2b-dev/E2B"&gt;E2B&lt;/a&gt; and the Google search tool &lt;a href="https://serper.dev/"&gt;Serper&lt;/a&gt;. We use &lt;a href="https://huggingface.co/openai/whisper-large-v3-turbo"&gt;Whisper&lt;/a&gt;, &lt;a href="https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct"&gt;Qwen2.5-VL-72B-Instruct&lt;/a&gt;, and &lt;a href="https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507"&gt;Qwen3-235B-A22B-Thinking-2507&lt;/a&gt; in our implementation. The framework can be easily extended to other open-source tools of your choice.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Replacing these open-source tools with commercial alternatives can yield performance gains. Commercial tools were mainly used for multimodal capabilities and certain complex reasoning subtasks. The majority of tasks, including planning, browsing, refinement, navigation, and more, were handled by our agents.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;h4&gt;More Benchmarks&lt;/h4&gt; 
 &lt;div align="center"&gt; 
  &lt;table&gt; 
   &lt;thead&gt; 
    &lt;tr&gt; 
     &lt;th&gt;Method&lt;/th&gt; 
     &lt;th align="center"&gt;HLE&lt;br /&gt;Pass@1&lt;/th&gt; 
     &lt;th align="center"&gt;Frames&lt;br /&gt;Pass@1&lt;/th&gt; 
     &lt;th align="center"&gt;BrowseComp&lt;br /&gt;Pass@1&lt;/th&gt; 
     &lt;th align="center"&gt;BrowseComp-ZH&lt;br /&gt;Pass@1&lt;/th&gt; 
     &lt;th align="center"&gt;WebWalkerQA&lt;br /&gt;Pass@1&lt;/th&gt; 
    &lt;/tr&gt; 
   &lt;/thead&gt; 
   &lt;tbody&gt; 
    &lt;tr&gt; 
     &lt;td&gt;OpenAI Deep Research&lt;/td&gt; 
     &lt;td align="center"&gt;26.6&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;51.5&lt;/td&gt; 
     &lt;td align="center"&gt;42.9&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;Gemini Deep Research&lt;/td&gt; 
     &lt;td align="center"&gt;26.9&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;Kimi-Researcher&lt;/td&gt; 
     &lt;td align="center"&gt;26.9&lt;/td&gt; 
     &lt;td align="center"&gt;78.8&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;WebDancer-7B&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;36.0&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;WebSailor-7B&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;6.7&lt;/td&gt; 
     &lt;td align="center"&gt;14.2&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;MiroThinker-8B-SFT-v0.1&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;58.0&lt;/td&gt; 
     &lt;td align="center"&gt;5.5&lt;/td&gt; 
     &lt;td align="center"&gt;9.3&lt;/td&gt; 
     &lt;td align="center"&gt;41.3&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;MiroThinker-8B-DPO-v0.1&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;64.4&lt;/td&gt; 
     &lt;td align="center"&gt;8.7&lt;/td&gt; 
     &lt;td align="center"&gt;13.6&lt;/td&gt; 
     &lt;td align="center"&gt;45.7&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;WebThinker-32B-RL&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;46.5&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;WebDancer-QwQ-32B&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;3.8&lt;/td&gt; 
     &lt;td align="center"&gt;18.0&lt;/td&gt; 
     &lt;td align="center"&gt;47.9&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;WebSailor-32B&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;10.5&lt;/td&gt; 
     &lt;td align="center"&gt;25.5&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;WebShaper-32B&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;-&lt;/td&gt; 
     &lt;td align="center"&gt;51.4&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;MiroThinker-32B-SFT-v0.1&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;10.2&lt;/td&gt; 
     &lt;td align="center"&gt;70.4&lt;/td&gt; 
     &lt;td align="center"&gt;10.6&lt;/td&gt; 
     &lt;td align="center"&gt;13.8&lt;/td&gt; 
     &lt;td align="center"&gt;45.7&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td&gt;&lt;strong&gt;MiroThinker-32B-DPO-v0.1&lt;/strong&gt;&lt;/td&gt; 
     &lt;td align="center"&gt;11.8&lt;/td&gt; 
     &lt;td align="center"&gt;71.7&lt;/td&gt; 
     &lt;td align="center"&gt;13.0&lt;/td&gt; 
     &lt;td align="center"&gt;17.0&lt;/td&gt; 
     &lt;td align="center"&gt;49.3&lt;/td&gt; 
    &lt;/tr&gt; 
   &lt;/tbody&gt; 
  &lt;/table&gt; 
 &lt;/div&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;MiroThinkerâ€™s performance was tested with this repository and open-source tools; other agentsâ€™ results are from their papers and official sites.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;As &lt;a href="https://huggingface.co/datasets/miromind-ai/MiroVerse-v0.1"&gt;MiroVerse-v0.1&lt;/a&gt; mainly contains English data, the agentâ€™s Chinese capability is limited. We plan to add more Chinese data to improve performance in the next version.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/details&gt; 
&lt;h2&gt;ğŸš€ Quick Start&lt;/h2&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ &lt;strong&gt;Python 3.10+&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸ“¦ &lt;strong&gt;uv package manager&lt;/strong&gt; (&lt;a href="https://github.com/astral-sh/uv"&gt;Installation guide&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;ğŸ”‘ &lt;strong&gt;Required API keys&lt;/strong&gt; (see configuration section below)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Clone the repository
git clone https://github.com/MiroMindAI/MiroThinker
cd MiroThinker

# Setup environment
cd apps/miroflow-agent
uv sync

# Configure API keys
cp .env.example .env
# Edit .env with your API keys (SERPER_API_KEY, JINA_API_KEY, E2B_API_KEY, etc.)
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;ğŸ“ Environment Variables&lt;/strong&gt;: See &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/#tool-configuration"&gt;Tool Configuration&lt;/a&gt; section for required API keys.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Tool Configuration&lt;/h3&gt; 
&lt;h4&gt;Minimal Configuration for MiroThinker v1.5 and v1.0&lt;/h4&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Server&lt;/th&gt; 
   &lt;th align="left"&gt;Description&lt;/th&gt; 
   &lt;th align="left"&gt;Tools Provided&lt;/th&gt; 
   &lt;th align="left"&gt;Required Environment Variables&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;&lt;code&gt;tool-python&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Execution environment and file management (E2B sandbox)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;create_sandbox&lt;/code&gt;, &lt;code&gt;run_command&lt;/code&gt;, &lt;code&gt;run_python_code&lt;/code&gt;, &lt;code&gt;upload_file_from_local_to_sandbox&lt;/code&gt;, &lt;code&gt;download_file_from_sandbox_to_local&lt;/code&gt;, &lt;code&gt;download_file_from_internet_to_sandbox&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;E2B_API_KEY&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;&lt;code&gt;search_and_scrape_webpage&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Google search via Serper API&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;google_search&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;SERPER_API_KEY&lt;/code&gt;, &lt;code&gt;SERPER_BASE_URL&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;&lt;code&gt;jina_scrape_llm_summary&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Web scraping with LLM-based information extraction&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;scrape_and_extract_info&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;JINA_API_KEY&lt;/code&gt;, &lt;code&gt;JINA_BASE_URL&lt;/code&gt;, &lt;code&gt;SUMMARY_LLM_BASE_URL&lt;/code&gt;, &lt;code&gt;SUMMARY_LLM_MODEL_NAME&lt;/code&gt;, &lt;code&gt;SUMMARY_LLM_API_KEY&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Minimal &lt;code&gt;.env&lt;/code&gt; configuration example:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Required for MiroThinker v1.5 and v1.0 (minimal setup)
SERPER_API_KEY=your_serper_key
SERPER_BASE_URL="https://google.serper.dev"
JINA_API_KEY=your_jina_key
JINA_BASE_URL="https://r.jina.ai"
E2B_API_KEY=your_e2b_key

# Required for jina_scrape_llm_summary
# Note: Summary LLM can be a small model (e.g., Qwen3-14B or GPT-5-Nano)
# The choice has minimal impact on performance, use what's most convenient
SUMMARY_LLM_BASE_URL="https://your_summary_llm_base_url/v1/chat/completions"
SUMMARY_LLM_MODEL_NAME=your_llm_model_name  # e.g., "Qwen/Qwen3-14B" or "gpt-5-nano"
SUMMARY_LLM_API_KEY=your_llm_api_key  # Optional, depends on LLM provider

# Required for benchmark evaluation (LLM-as-a-Judge)
OPENAI_API_KEY=your_openai_key  # Required for running benchmark evaluations
OPENAI_BASE_URL="https://api.openai.com/v1"  # Optional, defaults to OpenAI's API
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;ğŸ’¡ Why this is minimal&lt;/strong&gt;: These 3 MCP servers cover the core capabilities needed for research tasks: web search, content extraction, and code execution. All other servers are optional enhancements.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;ğŸ¤– Summary LLM&lt;/strong&gt;: The &lt;code&gt;SUMMARY_LLM&lt;/code&gt; can be a small model like Qwen3-14B or GPT-5-Nano. The choice has minimal impact on overall performance, use whichever is most convenient for your setup.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;ğŸ“Š For Benchmark Evaluation&lt;/strong&gt;: If you plan to run benchmark evaluations, you also need &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; (and optionally &lt;code&gt;OPENAI_BASE_URL&lt;/code&gt;) for LLM-as-a-Judge functionality used in evaluation scripts.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;ğŸ–¼ï¸ For GAIA Multimodal Tasks&lt;/strong&gt;: GAIA-Val-165 includes tasks with image/audio/video files. Since MiroThinker is a text-only LLM, GPT-4o is used to pre-process these files into text descriptions. The same &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; is used for both this preprocessing and LLM-as-a-Judge.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;ğŸ“– For more details&lt;/strong&gt;: See &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/libs/miroflow-tools/README.md"&gt;MiroFlow Tools README&lt;/a&gt; for complete documentation of all available tools.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;details&gt; 
 &lt;summary&gt;ğŸ”§ Click to expand additional available tools&lt;/summary&gt; 
 &lt;p&gt;The following optional tools are available but were not used in MiroThinker v1.5 and v1.0 evaluation:&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="left"&gt;Server Name&lt;/th&gt; 
    &lt;th align="left"&gt;Type&lt;/th&gt; 
    &lt;th align="left"&gt;Description&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;&lt;code&gt;tool-vqa&lt;/code&gt;&lt;/td&gt; 
    &lt;td align="left"&gt;Commercial&lt;/td&gt; 
    &lt;td align="left"&gt;Vision processing using Claude&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;&lt;code&gt;tool-vqa-os&lt;/code&gt;&lt;/td&gt; 
    &lt;td align="left"&gt;Open-Source&lt;/td&gt; 
    &lt;td align="left"&gt;Vision processing (open-source alternative)&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;&lt;code&gt;tool-transcribe&lt;/code&gt;&lt;/td&gt; 
    &lt;td align="left"&gt;Commercial&lt;/td&gt; 
    &lt;td align="left"&gt;Audio transcription using OpenAI&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;&lt;code&gt;tool-transcribe-os&lt;/code&gt;&lt;/td&gt; 
    &lt;td align="left"&gt;Open-Source&lt;/td&gt; 
    &lt;td align="left"&gt;Audio transcription using Whisper&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;&lt;code&gt;tool-reasoning&lt;/code&gt;&lt;/td&gt; 
    &lt;td align="left"&gt;Commercial&lt;/td&gt; 
    &lt;td align="left"&gt;Reasoning engine using Claude&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;&lt;code&gt;tool-reasoning-os&lt;/code&gt;&lt;/td&gt; 
    &lt;td align="left"&gt;Open-Source&lt;/td&gt; 
    &lt;td align="left"&gt;Reasoning engine (open-source alternative)&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;&lt;code&gt;tool-reading&lt;/code&gt;&lt;/td&gt; 
    &lt;td align="left"&gt;Open-Source&lt;/td&gt; 
    &lt;td align="left"&gt;Document reading using MarkItDown&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;&lt;code&gt;tool-google-search&lt;/code&gt;&lt;/td&gt; 
    &lt;td align="left"&gt;Commercial&lt;/td&gt; 
    &lt;td align="left"&gt;Web search using Google + scraping&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;&lt;code&gt;tool-sogou-search&lt;/code&gt;&lt;/td&gt; 
    &lt;td align="left"&gt;Commercial&lt;/td&gt; 
    &lt;td align="left"&gt;Web search using Sogou (Chinese)&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;&lt;strong&gt;ğŸ“– Local Deployment&lt;/strong&gt;: For instructions on deploying open-source tools (&lt;code&gt;tool-vqa-os&lt;/code&gt;, &lt;code&gt;tool-transcribe-os&lt;/code&gt;, &lt;code&gt;tool-reasoning-os&lt;/code&gt;) locally, see &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/assets/LOCAL-TOOL-DEPLOYMENT.md"&gt;Local Tool Deployment Guide&lt;/a&gt;.&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/libs/miroflow-tools/README.md"&gt;MiroFlow Tools README&lt;/a&gt; for complete documentation of all available tools.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h4&gt;Pre-configured Agent Settings&lt;/h4&gt; 
&lt;p&gt;The &lt;code&gt;apps/miroflow-agent/conf/agent/&lt;/code&gt; directory contains several pre-configured agent settings. Each configuration uses different tools and requires corresponding environment variables in your &lt;code&gt;.env&lt;/code&gt; file.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;ğŸ’¡ Recommended&lt;/strong&gt;: For MiroThinker v1.5, use &lt;code&gt;mirothinker_v1.5_keep5_max200&lt;/code&gt; (with context management, recommended for most tasks) or &lt;code&gt;mirothinker_v1.5_keep5_max400&lt;/code&gt; (only used for BrowseComp and BrowseComp-ZH). For v1.0, use &lt;code&gt;mirothinker_v1.0_keep5&lt;/code&gt; (with context management). All use minimal configuration with only 3 MCP servers.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Configuration&lt;/th&gt; 
   &lt;th align="left"&gt;Description&lt;/th&gt; 
   &lt;th align="left"&gt;Max Turns&lt;/th&gt; 
   &lt;th align="left"&gt;Context Retention&lt;/th&gt; 
   &lt;th align="left"&gt;Required Environment Variables&lt;/th&gt; 
   &lt;th align="left"&gt;Recommended For&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;&lt;code&gt;mirothinker_v1.5_keep5_max200&lt;/code&gt;&lt;/strong&gt; â­&lt;/td&gt; 
   &lt;td align="left"&gt;Single-agent with context management&lt;/td&gt; 
   &lt;td align="left"&gt;200&lt;/td&gt; 
   &lt;td align="left"&gt;Keep 5 most recent&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;SERPER_API_KEY&lt;/code&gt;, &lt;code&gt;SERPER_BASE_URL&lt;/code&gt;, &lt;code&gt;JINA_API_KEY&lt;/code&gt;, &lt;code&gt;JINA_BASE_URL&lt;/code&gt;, &lt;code&gt;E2B_API_KEY&lt;/code&gt;, &lt;code&gt;SUMMARY_LLM_BASE_URL&lt;/code&gt;, &lt;code&gt;SUMMARY_LLM_MODEL_NAME&lt;/code&gt;, &lt;code&gt;SUMMARY_LLM_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;v1.5 (recommended for most tasks)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;&lt;code&gt;mirothinker_v1.5_keep5_max400&lt;/code&gt;&lt;/strong&gt; â­&lt;/td&gt; 
   &lt;td align="left"&gt;Single-agent with context management&lt;/td&gt; 
   &lt;td align="left"&gt;400&lt;/td&gt; 
   &lt;td align="left"&gt;Keep 5 most recent&lt;/td&gt; 
   &lt;td align="left"&gt;Same as above&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;v1.5 (for BrowseComp &amp;amp; BrowseComp-ZH)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;&lt;code&gt;mirothinker_v1.5&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Single-agent for MiroThinker v1.5&lt;/td&gt; 
   &lt;td align="left"&gt;600&lt;/td&gt; 
   &lt;td align="left"&gt;Keep all results&lt;/td&gt; 
   &lt;td align="left"&gt;Same as above&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;v1.5&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;&lt;code&gt;mirothinker_v1.0_keep5&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Single-agent with context management&lt;/td&gt; 
   &lt;td align="left"&gt;600&lt;/td&gt; 
   &lt;td align="left"&gt;Keep 5 most recent&lt;/td&gt; 
   &lt;td align="left"&gt;Same as above&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;v1.0&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;&lt;code&gt;mirothinker_v1.0&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Single-agent for MiroThinker v1.0&lt;/td&gt; 
   &lt;td align="left"&gt;600&lt;/td&gt; 
   &lt;td align="left"&gt;Keep all results&lt;/td&gt; 
   &lt;td align="left"&gt;Same as above&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;v1.0&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;details&gt; 
 &lt;summary&gt;ğŸ“¦ Click to expand legacy configurations (v0.1/v0.2)&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="left"&gt;Configuration&lt;/th&gt; 
    &lt;th align="left"&gt;Description&lt;/th&gt; 
    &lt;th align="left"&gt;Max Turns&lt;/th&gt; 
    &lt;th align="left"&gt;Context Retention&lt;/th&gt; 
    &lt;th align="left"&gt;Required Environment Variables&lt;/th&gt; 
    &lt;th align="left"&gt;Recommended For&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;&lt;strong&gt;&lt;code&gt;multi_agent&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; 
    &lt;td align="left"&gt;Multi-agent with commercial tools (v0.1/v0.2)&lt;/td&gt; 
    &lt;td align="left"&gt;50&lt;/td&gt; 
    &lt;td align="left"&gt;Keep all results&lt;/td&gt; 
    &lt;td align="left"&gt;&lt;code&gt;E2B_API_KEY&lt;/code&gt;, &lt;code&gt;ANTHROPIC_API_KEY&lt;/code&gt;, &lt;code&gt;ANTHROPIC_BASE_URL&lt;/code&gt;, &lt;code&gt;OPENAI_API_KEY&lt;/code&gt;, &lt;code&gt;OPENAI_BASE_URL&lt;/code&gt;, &lt;code&gt;SERPER_API_KEY&lt;/code&gt;, &lt;code&gt;SERPER_BASE_URL&lt;/code&gt;, &lt;code&gt;JINA_API_KEY&lt;/code&gt;, &lt;code&gt;JINA_BASE_URL&lt;/code&gt;&lt;/td&gt; 
    &lt;td align="left"&gt;v0.1/v0.2&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;&lt;strong&gt;&lt;code&gt;multi_agent_os&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; 
    &lt;td align="left"&gt;Multi-agent with open-source tools (v0.1/v0.2)&lt;/td&gt; 
    &lt;td align="left"&gt;50&lt;/td&gt; 
    &lt;td align="left"&gt;Keep all results&lt;/td&gt; 
    &lt;td align="left"&gt;&lt;code&gt;E2B_API_KEY&lt;/code&gt;, &lt;code&gt;VISION_API_KEY&lt;/code&gt;, &lt;code&gt;VISION_BASE_URL&lt;/code&gt;, &lt;code&gt;VISION_MODEL_NAME&lt;/code&gt;, &lt;code&gt;WHISPER_API_KEY&lt;/code&gt;, &lt;code&gt;WHISPER_BASE_URL&lt;/code&gt;, &lt;code&gt;WHISPER_MODEL_NAME&lt;/code&gt;, &lt;code&gt;REASONING_API_KEY&lt;/code&gt;, &lt;code&gt;REASONING_BASE_URL&lt;/code&gt;, &lt;code&gt;REASONING_MODEL_NAME&lt;/code&gt;, &lt;code&gt;SERPER_API_KEY&lt;/code&gt;, &lt;code&gt;SERPER_BASE_URL&lt;/code&gt;, &lt;code&gt;JINA_API_KEY&lt;/code&gt;, &lt;code&gt;JINA_BASE_URL&lt;/code&gt;&lt;/td&gt; 
    &lt;td align="left"&gt;v0.1/v0.2&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;ğŸ’¡ Note&lt;/strong&gt;: All environment variables are listed in &lt;code&gt;apps/miroflow-agent/.env.example&lt;/code&gt;. Copy it to &lt;code&gt;.env&lt;/code&gt; and fill in the values for the tools you plan to use.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Creating Custom Tool Configurations&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;ğŸ”§ Click to expand custom tool configuration guide&lt;/summary&gt; 
 &lt;p&gt;You can create your own YAML configuration file to freely combine MCP servers. Here's how:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;strong&gt;Create a new YAML file&lt;/strong&gt; in &lt;code&gt;apps/miroflow-agent/conf/agent/&lt;/code&gt;:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-yaml"&gt;# conf/agent/my_custom_config.yaml
defaults:
  - default
  - _self_

main_agent:
  tools:
    - tool-python                    # Execution environment
    - search_and_scrape_webpage      # Google search
    - jina_scrape_llm_summary        # Web scraping with LLM
    - tool-vqa                       # Vision processing (optional)
    - tool-transcribe                # Audio processing (optional)
    - tool-reasoning                 # Reasoning engine (optional)
    - tool-reading                   # Document reading (optional)
  max_turns: 400  # Maximum number of turns

sub_agents:
  agent-browsing:  # Optional sub-agent
    tools:
      - tool-google-search
      - tool-vqa
      - tool-reading
      - tool-python
    max_turns: 50

keep_tool_result: -1  # Context retention budget: -1 keeps all tool results, or specify K to keep only the K most recent tool responses
&lt;/code&gt;&lt;/pre&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;&lt;strong&gt;ğŸ’¡ Context Retention Strategy&lt;/strong&gt;: The &lt;code&gt;keep_tool_result&lt;/code&gt; parameter implements a &lt;strong&gt;recency-based context retention&lt;/strong&gt; strategy. In the standard ReAct paradigm, all tool outputs are retained in the message history, which can lead to inefficient context utilization. Empirically, we observe that the agent's subsequent actions depend primarily on recent observations rather than distant ones. This strategy retains only the most recent K tool responses (where K is the &lt;code&gt;keep_tool_result&lt;/code&gt; value) while preserving the complete sequence of thoughts and actions.&lt;/p&gt; 
  &lt;p&gt;&lt;strong&gt;Benefits:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;âœ… Preserves the reasoning and action trace&lt;/li&gt; 
   &lt;li&gt;âœ… Focuses the agent's attention on the most contextually relevant observations&lt;/li&gt; 
   &lt;li&gt;âœ… Frees additional context space for extended reasoning and deeper tool-use trajectories&lt;/li&gt; 
   &lt;li&gt;âœ… Does not lead to performance degradation while allowing more context space for interactive scaling&lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;p&gt;&lt;strong&gt;Usage:&lt;/strong&gt; Set &lt;code&gt;keep_tool_result: -1&lt;/code&gt; to keep all tool results, or specify a positive integer K (e.g., &lt;code&gt;keep_tool_result: 5&lt;/code&gt;) to keep only the K most recent tool responses.&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;ol start="2"&gt; 
  &lt;li&gt;&lt;strong&gt;Use your custom configuration&lt;/strong&gt; when running evaluations:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;cd apps/miroflow-agent
uv run main.py llm=qwen-3 agent=my_custom_config llm.base_url=https://your_base_url/v1
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="3"&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Configure environment variables&lt;/strong&gt; in &lt;code&gt;.env&lt;/code&gt; based on the tools you use.&lt;/p&gt; &lt;p&gt;All available environment variables are listed in &lt;code&gt;apps/miroflow-agent/.env.example&lt;/code&gt;. Copy it to &lt;code&gt;.env&lt;/code&gt; and configure the variables according to your chosen configuration:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cd apps/miroflow-agent
cp .env.example .env
# Edit .env with your actual API keys
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;For MiroThinker v1.5&lt;/strong&gt; (&lt;code&gt;mirothinker_v1.5_keep5_max200.yaml&lt;/code&gt;, &lt;code&gt;mirothinker_v1.5_keep5_max400.yaml&lt;/code&gt;, or &lt;code&gt;mirothinker_v1.5.yaml&lt;/code&gt;) and &lt;strong&gt;v1.0&lt;/strong&gt; (&lt;code&gt;mirothinker_v1.0_keep5.yaml&lt;/code&gt; or &lt;code&gt;mirothinker_v1.0.yaml&lt;/code&gt;), see the &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/#minimal-configuration-for-mirothinker-v15-and-v10"&gt;Minimal Configuration&lt;/a&gt; section above for the complete configuration example.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;For other configurations&lt;/strong&gt;, refer to the &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/#pre-configured-agent-settings"&gt;Pre-configured Agent Settings&lt;/a&gt; table above to see which environment variables are required.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;ğŸ”‘ Click to expand optional API keys&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# API for LLM-as-a-Judge (for benchmark testing, required for benchmark evaluation)
OPENAI_API_KEY=your_openai_key
OPENAI_BASE_URL="https://api.openai.com/v1"  # Optional, defaults to OpenAI's API

# API for Open-Source Audio Transcription Tool (for benchmark testing, optional)
WHISPER_MODEL_NAME="openai/whisper-large-v3-turbo"
WHISPER_API_KEY=your_whisper_key
WHISPER_BASE_URL="https://your_whisper_base_url/v1"

# API for Open-Source VQA Tool (for benchmark testing, optional)
VISION_MODEL_NAME="Qwen/Qwen2.5-VL-72B-Instruct"
VISION_API_KEY=your_vision_key
VISION_BASE_URL="https://your_vision_base_url/v1/chat/completions"

# API for Open-Source Reasoning Tool (for benchmark testing, optional)
REASONING_MODEL_NAME="Qwen/Qwen3-235B-A22B-Thinking-2507"
REASONING_API_KEY=your_reasoning_key
REASONING_BASE_URL="https://your_reasoning_base_url/v1/chat/completions"

# API for Claude Sonnet 3.7 as Commercial Tools (optional)
ANTHROPIC_API_KEY=your_anthropic_key

# API for Sogou Search (optional)
TENCENTCLOUD_SECRET_ID=your_tencent_cloud_secret_id
TENCENTCLOUD_SECRET_KEY=your_tencent_cloud_secret_key

# API for Summary LLM (can use small models like Qwen3-14B or GPT-5-Nano)
SUMMARY_LLM_BASE_URL="https://your_summary_llm_base_url/v1/chat/completions"
SUMMARY_LLM_MODEL_NAME=your_summary_llm_model_name  # e.g., "Qwen/Qwen3-14B" or "gpt-5-nano"
SUMMARY_LLM_API_KEY=your_summary_llm_api_key
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;Serve the MiroThinker Agent&lt;/h3&gt; 
&lt;h4&gt;Option 1 (Recommended): Serve with SGLang or vLLM&lt;/h4&gt; 
&lt;p&gt;Use SGLang to serve MiroThinker agents at port 61002:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;NUM_GPUS=4
PORT=61002

# Downloading agent from HF (v1.5 recommended)
AGENT_PATH=miromind-ai/MiroThinker-v1.5-30B

# Or use v1.0
# AGENT_PATH=miromind-ai/MiroThinker-v1.0-30B

python3 -m sglang.launch_server \
    --model-path $AGENT_PATH \
    --tp $NUM_GPUS \
    --dp 1 \
    --host 0.0.0.0 \
    --port $PORT \
    --trust-remote-code
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;ğŸ“ Server URL&lt;/strong&gt;: This will start a server at &lt;code&gt;http://0.0.0.0:$PORT&lt;/code&gt;. Use this as your server base URL (e.g., &lt;code&gt;http://0.0.0.0:61002/v1&lt;/code&gt;).&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Option 2: Quantized Light-Weight Options&lt;/h4&gt; 
&lt;p&gt;We also provide comprehensive guidance for serving MiroThinker agents using CPU-optimized and GPU-accelerated quantization techniques, along with detailed analysis and guidelines for deployment with llama.cpp, Ollama, SGLang, and other inference frameworks.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;ğŸ“– Complete Guide&lt;/strong&gt;: See &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/apps/gradio-demo/"&gt;Deployment Documentation&lt;/a&gt; for detailed deployment instructions.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Run Your First Task&lt;/h3&gt; 
&lt;p&gt;After setting up the environment and starting your server, run &lt;code&gt;main.py&lt;/code&gt; to test with a default question: &lt;em&gt;"What is the title of today's arxiv paper in computer science?"&lt;/em&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd apps/miroflow-agent

# Using MiroThinker agents (requires your own server)
uv run python main.py llm=qwen-3 agent=mirothinker_v1.5_keep5_max200 llm.base_url=http://localhost:61002/v1

# Or using Claude (requires ANTHROPIC_API_KEY in .env)
uv run python main.py llm=claude-3-7 agent=single_agent_keep5

# Or using GPT-5 (requires OPENAI_API_KEY in .env)
uv run python main.py llm=gpt-5 agent=single_agent_keep5
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;To customize your question&lt;/strong&gt;, edit &lt;code&gt;main.py&lt;/code&gt; line 32:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;task_description = "Your custom question here"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The agent will search the web, execute code if needed, and provide an answer with sources.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;ğŸ“– More details&lt;/strong&gt;: See &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/apps/miroflow-agent/README.md"&gt;apps/miroflow-agent/README.md&lt;/a&gt; for available configurations and troubleshooting.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;ğŸ“Š Benchmark Evaluation&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;For researchers who want to reproduce our benchmark results or evaluate on standard benchmarks.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Download Benchmark Data&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd MiroThinker  # Back to project root
wget https://huggingface.co/datasets/miromind-ai/MiroFlow-Benchmarks/resolve/main/data_20251115_password_protected.zip
unzip data_20251115_password_protected.zip
# Password: pf4*
rm data_20251115_password_protected.zip
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Run Benchmark Evaluation&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; For MiroThinker v1.5, use &lt;code&gt;mirothinker_v1.5_keep5_max200&lt;/code&gt; (with context management), &lt;code&gt;mirothinker_v1.5_keep5_max400&lt;/code&gt; (with context management), or &lt;code&gt;mirothinker_v1.5&lt;/code&gt; configurations. For v1.0, use &lt;code&gt;mirothinker_v1.0_keep5&lt;/code&gt; (with context management) or &lt;code&gt;mirothinker_v1.0&lt;/code&gt; configurations.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;Available Parameters:&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;You can customize the evaluation by setting the following environment variables before running the script:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Parameter&lt;/th&gt; 
   &lt;th align="left"&gt;Default&lt;/th&gt; 
   &lt;th align="left"&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;LLM_AGENT&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;"MiroThinker-Agents"&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Agent name identifier&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;BASE_URL&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;"https://your-api.com/v1"&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Base URL of your server&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;NUM_RUNS&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Varies by benchmark&lt;/td&gt; 
   &lt;td align="left"&gt;Number of evaluation runs (3 for most benchmarks, 8 for GAIA/XBench/FutureX/SEAL-0, 32 for AIME2025)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;LLM_PROVIDER&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;"qwen"&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;LLM provider (e.g., &lt;code&gt;qwen&lt;/code&gt;, &lt;code&gt;openai&lt;/code&gt;, &lt;code&gt;anthropic&lt;/code&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;AGENT_SET&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;"mirothinker_v1.5_keep5_max200"&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Agent configuration (e.g., &lt;code&gt;mirothinker_v1.5_keep5_max200&lt;/code&gt;, &lt;code&gt;mirothinker_v1.5_keep5_max400&lt;/code&gt;, &lt;code&gt;mirothinker_v1.0_keep5&lt;/code&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;MAX_CONTEXT_LENGTH&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;262144&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Maximum context length (256K)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;MAX_CONCURRENT&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;10&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Maximum concurrent tasks&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;PASS_AT_K&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;1&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Pass@K evaluation metric&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;TEMPERATURE&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;1.0&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Sampling temperature&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;"xxx"&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;API key for the server&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Example Usage:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Navigate to the miroflow-agent directory first
cd apps/miroflow-agent

# Basic usage with v1.5 (recommended)
NUM_RUNS=8 LLM_AGENT="MiroThinker-v1.5-30B" BASE_URL="https://your-api.com/v1" bash scripts/run_evaluate_multiple_runs_gaia-validation-text-103.sh

# Or with v1.0
# NUM_RUNS=8 LLM_AGENT="MiroThinker-v1.0-30B" BASE_URL="https://your-api.com/v1" bash scripts/run_evaluate_multiple_runs_gaia-validation-text-103.sh

# Customize number of runs and agent configuration (v1.5 with context management)
LLM_AGENT="MiroThinker-v1.5-30B" \
BASE_URL="https://your-api.com/v1" \
NUM_RUNS=8 \
AGENT_SET="mirothinker_v1.5_keep5_max200" \
bash scripts/run_evaluate_multiple_runs_gaia-validation-text-103.sh

# Or with v1.0 configuration (with context management)
# LLM_AGENT="MiroThinker-v1.0-30B" \
# BASE_URL="https://your-api.com/v1" \
# NUM_RUNS=8 \
# AGENT_SET="mirothinker_v1.0_keep5" \
# bash scripts/run_evaluate_multiple_runs_gaia-validation-text-103.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;details open&gt; 
 &lt;summary&gt;ğŸ“‹ Click to expand all benchmark commands&lt;/summary&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;&lt;strong&gt;âš ï¸ Important for MiroThinker v1.5&lt;/strong&gt;: To reproduce our reported results, you must set the correct &lt;code&gt;AGENT_SET&lt;/code&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;BrowseComp &amp;amp; BrowseComp-ZH&lt;/strong&gt;: Use &lt;code&gt;AGENT_SET="mirothinker_v1.5_keep5_max400"&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;All other benchmarks&lt;/strong&gt;: Use &lt;code&gt;AGENT_SET="mirothinker_v1.5_keep5_max200"&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/blockquote&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Navigate to the miroflow-agent directory first
cd apps/miroflow-agent

# HLE
NUM_RUNS=3 LLM_AGENT="xxx" BASE_URL="xxx" AGENT_SET="mirothinker_v1.5_keep5_max200" bash scripts/run_evaluate_multiple_runs_hle.sh

# HLE-Text-2158
NUM_RUNS=3 LLM_AGENT="xxx" BASE_URL="xxx" AGENT_SET="mirothinker_v1.5_keep5_max200" bash scripts/run_evaluate_multiple_runs_hle-text-2158.sh

# HLE-Text-500
NUM_RUNS=3 LLM_AGENT="xxx" BASE_URL="xxx" AGENT_SET="mirothinker_v1.5_keep5_max200" bash scripts/run_evaluate_multiple_runs_hle-text-500.sh

# GAIA-Text-103
NUM_RUNS=8 LLM_AGENT="xxx" BASE_URL="xxx" AGENT_SET="mirothinker_v1.5_keep5_max200" bash scripts/run_evaluate_multiple_runs_gaia-validation-text-103.sh

# GAIA-Validation (GAIA-Val-165)
NUM_RUNS=8 LLM_AGENT="xxx" BASE_URL="xxx" AGENT_SET="mirothinker_v1.5_keep5_max200" bash scripts/run_evaluate_multiple_runs_gaia-validation.sh

# BrowseComp-EN (âš ï¸ use max400)
NUM_RUNS=3 LLM_AGENT="xxx" BASE_URL="xxx" AGENT_SET="mirothinker_v1.5_keep5_max400" bash scripts/run_evaluate_multiple_runs_browsecomp.sh

# BrowseComp-ZH (âš ï¸ use max400)
NUM_RUNS=3 LLM_AGENT="xxx" BASE_URL="xxx" AGENT_SET="mirothinker_v1.5_keep5_max400" bash scripts/run_evaluate_multiple_runs_browsecomp_zh.sh

# WebWalkerQA
NUM_RUNS=3 LLM_AGENT="xxx" BASE_URL="xxx" AGENT_SET="mirothinker_v1.5_keep5_max200" bash scripts/run_evaluate_multiple_runs_webwalkerqa.sh

# XBench-DeepSearch
NUM_RUNS=8 LLM_AGENT="xxx" BASE_URL="xxx" AGENT_SET="mirothinker_v1.5_keep5_max200" bash scripts/run_evaluate_multiple_runs_xbench_deepsearch.sh

# FRAMES
NUM_RUNS=3 LLM_AGENT="xxx" BASE_URL="xxx" AGENT_SET="mirothinker_v1.5_keep5_max200" bash scripts/run_evaluate_multiple_runs_frames.sh

# SEAL-0
NUM_RUNS=8 LLM_AGENT="xxx" BASE_URL="xxx" AGENT_SET="mirothinker_v1.5_keep5_max200" bash scripts/run_evaluate_multiple_runs_seal-0.sh

# FutureX
NUM_RUNS=8 LLM_AGENT="xxx" BASE_URL="xxx" AGENT_SET="mirothinker_v1.5_keep5_max200" bash scripts/run_evaluate_multiple_runs_futurex.sh

# AIME2025
NUM_RUNS=32 LLM_AGENT="xxx" BASE_URL="xxx" AGENT_SET="mirothinker_v1.5_keep5_max200" bash scripts/run_evaluate_multiple_runs_aime2025.sh

# DeepSearchQA
NUM_RUNS=3 LLM_AGENT="xxx" BASE_URL="xxx" AGENT_SET="mirothinker_v1.5_keep5_max200" bash scripts/run_evaluate_multiple_runs_deepsearchqa.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h4&gt;3. &lt;strong&gt;Monitor evaluation progress&lt;/strong&gt;&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;ğŸ“Š Click to expand progress monitoring commands&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Navigate to the miroflow-agent directory first
cd apps/miroflow-agent

# For HLE
python benchmarks/check_progress/check_progress_hle.py /path/to/evaluation/logs

# For HLE-Text-2158
python benchmarks/check_progress/check_progress_hle-text-2158.py /path/to/evaluation/logs

# For HLE-Text-500
python benchmarks/check_progress/check_progress_hle-text-500.py /path/to/evaluation/logs

# For BrowseComp-EN
python benchmarks/check_progress/check_progress_browsecomp.py /path/to/evaluation/logs

# For BrowseComp-ZH
python benchmarks/check_progress/check_progress_browsecomp_zh.py /path/to/evaluation/logs

# For GAIA-Validation
python benchmarks/check_progress/check_progress_gaia-validation.py /path/to/evaluation/logs

# For GAIA-Text-103
python benchmarks/check_progress/check_progress_gaia-validation-text-103.py /path/to/evaluation/logs

# For WebWalkerQA
python benchmarks/check_progress/check_progress_webwalkerqa.py /path/to/evaluation/logs

# For Frames
python benchmarks/check_progress/check_progress_frames.py /path/to/evaluation/logs

# For XBench-DeepSearch
python benchmarks/check_progress/check_progress_xbench_deepsearch.py /path/to/evaluation/logs

# For SEAL-0
python benchmarks/check_progress/check_progress_seal-0.py /path/to/evaluation/logs

# For AIME2025
python benchmarks/check_progress/check_progress_aime2025.py /path/to/evaluation/logs

# For DeepSearchQA
python benchmarks/check_progress/check_progress_deepsearchqa.py /path/to/evaluation/logs
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;ğŸ”¬ Trace Collection&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;ğŸ“‹ Click to expand trace collection commands&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;cd apps/collect-trace

# Collect Traces for SFT
bash scripts/collect_trace_claude37.sh
bash scripts/collect_trace_gpt5.sh

# Collect Traces for DPO
bash scripts/collect_trace_qwen3.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;â“ FAQ &amp;amp; Troubleshooting&lt;/h2&gt; 
&lt;h3&gt;Common Issues&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;ğŸ”§ Click to expand troubleshooting guide&lt;/summary&gt; 
 &lt;h4&gt;&lt;strong&gt;Q: Which version should I use?&lt;/strong&gt;&lt;/h4&gt; 
 &lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; We recommend &lt;strong&gt;MiroThinker v1.5&lt;/strong&gt; â­ with the minimal configuration:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;v1.5&lt;/strong&gt; â­: Latest version with 256K context, world-leading performance. Use config (with context management): 
   &lt;ul&gt; 
    &lt;li&gt;&lt;code&gt;mirothinker_v1.5_keep5_max200&lt;/code&gt; (up to 200 turns, recommended for most tasks)&lt;/li&gt; 
    &lt;li&gt;&lt;code&gt;mirothinker_v1.5_keep5_max400&lt;/code&gt; (up to 400 turns, only used for BrowseComp and BrowseComp-ZH)&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h4&gt;&lt;strong&gt;Q: How do I get API keys?&lt;/strong&gt;&lt;/h4&gt; 
 &lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; You need these keys for minimal setup:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;SERPER_API_KEY&lt;/strong&gt;: Get from &lt;a href="https://serper.dev/"&gt;Serper.dev&lt;/a&gt; (Google search API)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;JINA_API_KEY&lt;/strong&gt;: Get from &lt;a href="https://jina.ai/"&gt;Jina.ai&lt;/a&gt; (Web scraping)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;E2B_API_KEY&lt;/strong&gt;: Get from &lt;a href="https://e2b.dev/"&gt;E2B.dev&lt;/a&gt; (Code execution sandbox)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;SUMMARY_LLM_API_KEY&lt;/strong&gt;: Your LLM API credentials (for content summarization). Can be a small model like Qwen3-14B or GPT-5-Nanoâ€”the choice has minimal impact on performance.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;OPENAI_API_KEY&lt;/strong&gt;: Get from &lt;a href="https://platform.openai.com/"&gt;OpenAI&lt;/a&gt; (Required for benchmark evaluation, used for LLM-as-a-Judge)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;OPENAI_BASE_URL&lt;/strong&gt;: Optional, defaults to &lt;code&gt;https://api.openai.com/v1&lt;/code&gt;. Can be changed to use OpenAI-compatible APIs.&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h4&gt;&lt;strong&gt;Q: Agent server connection errors&lt;/strong&gt;&lt;/h4&gt; 
 &lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Common issues:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Check base URL format&lt;/strong&gt;: Should end with &lt;code&gt;/v1&lt;/code&gt; (e.g., &lt;code&gt;https://your-api.com/v1&lt;/code&gt;)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Verify API key&lt;/strong&gt;: Ensure &lt;code&gt;API_KEY&lt;/code&gt; is set correctly in environment or script&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Check server status&lt;/strong&gt;: Make sure your server is running and accessible&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Network issues&lt;/strong&gt;: Verify firewall/network settings allow connections&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h4&gt;&lt;strong&gt;Q: Evaluation script fails to run&lt;/strong&gt;&lt;/h4&gt; 
 &lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Troubleshooting steps:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;strong&gt;Check working directory&lt;/strong&gt;: Make sure you're in &lt;code&gt;apps/miroflow-agent&lt;/code&gt; directory&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Verify environment&lt;/strong&gt;: Run &lt;code&gt;uv sync&lt;/code&gt; to ensure dependencies are installed&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Check .env file&lt;/strong&gt;: Ensure all required environment variables are set&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Review logs&lt;/strong&gt;: Check &lt;code&gt;logs/&lt;/code&gt; directory for detailed error messages&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Verify data path&lt;/strong&gt;: Ensure benchmark data is downloaded and in correct location&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;h4&gt;&lt;strong&gt;Q: Out of memory errors&lt;/strong&gt;&lt;/h4&gt; 
 &lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Solutions:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Reduce context length&lt;/strong&gt;: Set &lt;code&gt;MAX_CONTEXT_LENGTH&lt;/code&gt; to a smaller value (e.g., 131072 for 128K)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Use context management with fewer turns&lt;/strong&gt;: 
   &lt;ul&gt; 
    &lt;li&gt;For v1.5: Use &lt;code&gt;mirothinker_v1.5_keep5_max200&lt;/code&gt; or &lt;code&gt;mirothinker_v1.5_keep5_max400&lt;/code&gt; (with context management)&lt;/li&gt; 
    &lt;li&gt;For v1.0: Use &lt;code&gt;mirothinker_v1.0_keep5&lt;/code&gt; (with context management)&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Reduce concurrent tasks&lt;/strong&gt;: Set &lt;code&gt;MAX_CONCURRENT&lt;/code&gt; to a smaller number (e.g., 5)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Use smaller agents&lt;/strong&gt;: 
   &lt;ul&gt; 
    &lt;li&gt;For v1.5: Try 30B instead of 235B&lt;/li&gt; 
    &lt;li&gt;For v1.0: Try 8B or 30B instead of 72B&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h4&gt;&lt;strong&gt;Q: Tool execution errors&lt;/strong&gt;&lt;/h4&gt; 
 &lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Common fixes:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;E2B errors&lt;/strong&gt;: Verify &lt;code&gt;E2B_API_KEY&lt;/code&gt; is valid and account has credits&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Serper errors&lt;/strong&gt;: Check &lt;code&gt;SERPER_API_KEY&lt;/code&gt; and rate limits&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Jina errors&lt;/strong&gt;: Verify &lt;code&gt;JINA_API_KEY&lt;/code&gt; and &lt;code&gt;JINA_BASE_URL&lt;/code&gt; are correct&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;LLM summarization errors&lt;/strong&gt;: Check &lt;code&gt;SUMMARY_LLM_*&lt;/code&gt; variables and agent availability&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h4&gt;&lt;strong&gt;Q: How to monitor long-running evaluations?&lt;/strong&gt;&lt;/h4&gt; 
 &lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Use the progress monitoring scripts:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;cd apps/miroflow-agent
python benchmarks/check_progress/check_progress_&amp;lt;benchmark_name&amp;gt;.py /path/to/logs
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;The scripts show completion status, elapsed time, and estimated remaining time.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h3&gt;Getting Help&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“– &lt;strong&gt;Documentation&lt;/strong&gt;: Check &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/libs/miroflow-tools/README.md"&gt;MiroFlow Tools README&lt;/a&gt; for tool details&lt;/li&gt; 
 &lt;li&gt;ğŸ’¬ &lt;strong&gt;Discord&lt;/strong&gt;: Join our &lt;a href="https://discord.com/invite/GPqEnkzQZd"&gt;Discord community&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸ› &lt;strong&gt;Issues&lt;/strong&gt;: Report bugs on &lt;a href="https://github.com/MiroMindAI/MiroThinker/issues"&gt;GitHub Issues&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸ“§ &lt;strong&gt;Contact&lt;/strong&gt;: Visit &lt;a href="https://miromind.ai/"&gt;our website&lt;/a&gt; for more information&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ“„ License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the MIT License - see the &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroThinker/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;h2&gt;ğŸ™ Acknowledgments&lt;/h2&gt; 
&lt;p&gt;We extend our sincere gratitude to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ† &lt;strong&gt;Benchmark Contributors&lt;/strong&gt; for the comprehensive evaluation datasets&lt;/li&gt; 
 &lt;li&gt;ğŸŒ &lt;strong&gt;Open Source Community&lt;/strong&gt; for the tools and libraries that make this possible&lt;/li&gt; 
 &lt;li&gt;ğŸ‘¥ &lt;strong&gt;All Contributors&lt;/strong&gt; who have helped make MiroThinker better&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/MiroMindAI/MiroThinker/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=MiroMindAI/MiroThinker" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;p&gt;Join our community and help us build the future of AI agents!&lt;/p&gt; 
&lt;h3&gt;References&lt;/h3&gt; 
&lt;p&gt;If you find this project useful in your research, please consider citing:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@article{miromind2025mirothinker,
  title={MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling},
  author={MiroMind Team and Bai, Song and Bing, Lidong and Chen, Carson and Chen, Guanzheng and Chen, Yuntao and Chen, Zhe and Chen, Ziyi and Dong, Xuan and others},
  journal={arXiv preprint arXiv:2511.11793},
  year={2025}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#MiroMindAI/MiroThinker&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=MiroMindAI/MiroThinker&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>OpenBB-finance/OpenBB</title>
      <link>https://github.com/OpenBB-finance/OpenBB</link>
      <description>&lt;p&gt;Financial data platform for analysts, quants and AI agents.&lt;/p&gt;&lt;hr&gt;&lt;br /&gt; 
&lt;img src="https://github.com/OpenBB-finance/OpenBB/raw/develop/images/odp-light.svg?raw=true#gh-light-mode-only" alt="Open Data Platform by OpenBB logo" width="600" /&gt; 
&lt;img src="https://github.com/OpenBB-finance/OpenBB/raw/develop/images/odp-dark.svg?raw=true#gh-dark-mode-only" alt="Open Data Platform by OpenBB logo" width="600" /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;a href="https://x.com/openbb_finance"&gt;&lt;img src="https://img.shields.io/twitter/url/https/twitter.com/openbb_finance.svg?style=social&amp;amp;label=Follow%20%40openbb_finance" alt="Twitter" /&gt;&lt;/a&gt; &lt;a href="https://discord.com/invite/xPHTuHCmuV"&gt;&lt;img src="https://img.shields.io/discord/831165782750789672" alt="Discord Shield" /&gt;&lt;/a&gt; &lt;a href="https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/OpenBB-finance/OpenBB"&gt;&lt;img src="https://img.shields.io/static/v1?label=Dev%20Containers&amp;amp;message=Open&amp;amp;color=blue&amp;amp;logo=visualstudiocode" alt="Open in Dev Containers" /&gt;&lt;/a&gt; &lt;a href="https://codespaces.new/OpenBB-finance/OpenBB"&gt; &lt;img src="https://github.com/codespaces/badge.svg?sanitize=true" height="20" /&gt; &lt;/a&gt; &lt;a target="_blank" href="https://colab.research.google.com/github/OpenBB-finance/OpenBB/blob/develop/examples/googleColab.ipynb"&gt; &lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt; &lt;/a&gt; &lt;a href="https://pypi.org/project/openbb/"&gt;&lt;img src="https://img.shields.io/pypi/v/openbb?color=blue&amp;amp;label=PyPI%20Package" alt="PyPI" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Open Data Platform by OpenBB (ODP) is the open-source toolset that helps data engineers integrate proprietary, licensed, and public data sources into downstream applications like AI copilots and research dashboards.&lt;/p&gt; 
&lt;p&gt;ODP operates as the "connect once, consume everywhere" infrastructure layer that consolidates and exposes data to multiple surfaces at once: Python environments for quants, OpenBB Workspace and Excel for analysts, MCP servers for AI agents, and REST APIs for other applications.&lt;/p&gt; 
&lt;a href="https://pro.openbb.co"&gt; 
 &lt;div align="center"&gt; 
  &lt;img src="https://openbb-cms.directus.app/assets/70b971ef-7a7e-486e-b5ae-1cc602f2162c.png" alt="Logo" width="1000" /&gt; 
 &lt;/div&gt; &lt;/a&gt; 
&lt;p&gt;Get started with: &lt;code&gt;pip install openbb&lt;/code&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from openbb import obb
output = obb.equity.price.historical("AAPL")
df = output.to_dataframe()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Data integrations available can be found here: &lt;a href="https://docs.openbb.co/python/reference"&gt;https://docs.openbb.co/python/reference&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;OpenBB Workspace&lt;/h2&gt; 
&lt;p&gt;While the Open Data Platform provides the open-source data integration foundation, &lt;strong&gt;OpenBB Workspace&lt;/strong&gt; offers the enterprise UI for analysts to visualize datasets and leverage AI agents. The platform's "connect once, consume everywhere" architecture enables seamless integration between the two.&lt;/p&gt; 
&lt;p&gt;You can find OpenBB Workspace at &lt;a href="https://pro.openbb.co"&gt;https://pro.openbb.co&lt;/a&gt;. &lt;a href="https://pro.openbb.co"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;a href="https://pro.openbb.co"&gt; 
 &lt;div align="center"&gt; 
  &lt;img src="https://openbb-cms.directus.app/assets/f69b6aaf-0821-4bc8-a43c-715e03a924ef.png" alt="Logo" width="1000" /&gt; 
 &lt;/div&gt; &lt;/a&gt; 
&lt;p&gt;Data integration:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;You can learn more about adding data to the OpenBB workspace from the &lt;a href="https://docs.openbb.co/workspace"&gt;docs&lt;/a&gt; or &lt;a href="https://github.com/OpenBB-finance/backends-for-openbb"&gt;this open source repository&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;AI Agents integration:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;You can learn more about adding AI agents to the OpenBB workspace from &lt;a href="https://github.com/OpenBB-finance/agents-for-openbb"&gt;this open source repository&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Integrating Open Data Platform to the OpenBB Workspace&lt;/h3&gt; 
&lt;p&gt;Connect this library to the OpenBB Workspace with a few simple commands, in a Python (3.9.21 - 3.12) environment.&lt;/p&gt; 
&lt;h4&gt;Run an ODP backend&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Install the packages.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;pip install "openbb[all]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Start the API server over localhost.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;openbb-api
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will launch a FastAPI server, via Uvicorn, at &lt;code&gt;127.0.0.1:6900&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;You can check that it works by going to &lt;a href="http://127.0.0.1:6900"&gt;http://127.0.0.1:6900&lt;/a&gt;.&lt;/p&gt; 
&lt;h4&gt;Integrate the ODP Backend to OpenBB Workspace&lt;/h4&gt; 
&lt;p&gt;Sign-in to the &lt;a href="https://pro.openbb.co/"&gt;OpenBB Workspace&lt;/a&gt;, and follow the following steps:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/75cffb4a-5e95-470a-b9d0-6ffd4067e069" alt="CleanShot 2025-05-17 at 09 51 56@2x" /&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Go to the "Apps" tab&lt;/li&gt; 
 &lt;li&gt;Click on "Connect backend"&lt;/li&gt; 
 &lt;li&gt;Fill in the form with: Name: Open Data Platform URL: &lt;a href="http://127.0.0.1:6900"&gt;http://127.0.0.1:6900&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Click on "Test". You should get a "Test successful" with the number of apps found.&lt;/li&gt; 
 &lt;li&gt;Click on "Add".&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;That's it.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;!-- TABLE OF CONTENTS --&gt; 
&lt;details closed="closed"&gt; 
 &lt;summary&gt;&lt;h2 style="display: inline-block"&gt;Table of Contents&lt;/h2&gt;&lt;/summary&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#1-installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#2-contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#3-license"&gt;License&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#4-disclaimer"&gt;Disclaimer&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#5-contacts"&gt;Contacts&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#6-star-history"&gt;Star History&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#7-contributors"&gt;Contributors&lt;/a&gt;&lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/details&gt; 
&lt;h2&gt;1. Installation&lt;/h2&gt; 
&lt;p&gt;The ODP Python Package can be installed from &lt;a href="https://pypi.org/project/openbb/"&gt;PyPI package&lt;/a&gt; by running &lt;code&gt;pip install openbb&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;or by cloning the repository directly with &lt;code&gt;git clone https://github.com/OpenBB-finance/OpenBB.git&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Please find more about the installation process, in the &lt;a href="https://docs.openbb.co/python/installation"&gt;OpenBB Documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;ODP CLI installation&lt;/h3&gt; 
&lt;p&gt;The ODP CLI is a command-line interface that allows you to access the ODP directly from your command line.&lt;/p&gt; 
&lt;p&gt;It can be installed by running &lt;code&gt;pip install openbb-cli&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;or by cloning the repository directly with &lt;code&gt;git clone https://github.com/OpenBB-finance/OpenBB.git&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Please find more about the installation process in the &lt;a href="https://docs.openbb.co/cli/installation"&gt;OpenBB Documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;2. Contributing&lt;/h2&gt; 
&lt;p&gt;There are three main ways of contributing to this project. (Hopefully you have starred the project by now â­ï¸)&lt;/p&gt; 
&lt;h3&gt;Become a Contributor&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;More information on our &lt;a href="https://docs.openbb.co/python/developer"&gt;Developer Documentation&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Create a GitHub ticket&lt;/h3&gt; 
&lt;p&gt;Before creating a ticket make sure the one you are creating doesn't exist already &lt;a href="https://github.com/OpenBB-finance/OpenBB/issues"&gt;among the existing issues&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&amp;amp;labels=bug&amp;amp;template=bug_report.md&amp;amp;title=%5BBug%5D"&gt;Report bug&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&amp;amp;labels=enhancement&amp;amp;template=enhancement.md&amp;amp;title=%5BIMPROVE%5D"&gt;Suggest improvement&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&amp;amp;labels=new+feature&amp;amp;template=feature_request.md&amp;amp;title=%5BFR%5D"&gt;Request a feature&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Provide feedback&lt;/h3&gt; 
&lt;p&gt;We are most active on &lt;a href="https://openbb.co/discord"&gt;our Discord&lt;/a&gt;, but feel free to reach out to us in any of &lt;a href="https://openbb.co/links"&gt;our social media&lt;/a&gt; for feedback.&lt;/p&gt; 
&lt;h2&gt;3. License&lt;/h2&gt; 
&lt;p&gt;Distributed under the AGPLv3 License. See &lt;a href="https://github.com/OpenBB-finance/OpenBB/raw/main/LICENSE"&gt;LICENSE&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h2&gt;4. Disclaimer&lt;/h2&gt; 
&lt;p&gt;Trading in financial instruments involves high risks including the risk of losing some, or all, of your investment amount, and may not be suitable for all investors.&lt;/p&gt; 
&lt;p&gt;Before deciding to trade in a financial instrument you should be fully informed of the risks and costs associated with trading the financial markets, carefully consider your investment objectives, level of experience, and risk appetite, and seek professional advice where needed.&lt;/p&gt; 
&lt;p&gt;The data contained in the Open Data Platform is not necessarily accurate.&lt;/p&gt; 
&lt;p&gt;OpenBB and any provider of the data contained in this website will not accept liability for any loss or damage as a result of your trading, or your reliance on the information displayed.&lt;/p&gt; 
&lt;p&gt;All names, logos, and brands of third parties that may be referenced in our sites, products or documentation are trademarks of their respective owners. Unless otherwise specified, OpenBB and its products and services are not endorsed by, sponsored by, or affiliated with these third parties.&lt;/p&gt; 
&lt;p&gt;Our use of these names, logos, and brands is for identification purposes only, and does not imply any such endorsement, sponsorship, or affiliation.&lt;/p&gt; 
&lt;h2&gt;5. Contacts&lt;/h2&gt; 
&lt;p&gt;If you have any questions about the platform or anything OpenBB, feel free to email us at &lt;code&gt;support@openbb.co&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;If you want to say hi, or are interested in partnering with us, feel free to reach us at &lt;code&gt;hello@openbb.co&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Any of our social media platforms: &lt;a href="https://openbb.co/links"&gt;openbb.co/links&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;6. Star History&lt;/h2&gt; 
&lt;p&gt;This is a proxy of our growth and that we are just getting started.&lt;/p&gt; 
&lt;p&gt;But for more metrics important to us check &lt;a href="https://openbb.co/open"&gt;openbb.co/open&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://api.star-history.com/svg?repos=openbb-finance/OpenBB&amp;amp;type=Date&amp;amp;theme=dark"&gt;&lt;img src="https://api.star-history.com/svg?repos=openbb-finance/OpenBB&amp;amp;type=Date&amp;amp;theme=dark" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;7. Contributors&lt;/h2&gt; 
&lt;p&gt;OpenBB wouldn't be OpenBB without you. If we are going to disrupt financial industry, every contribution counts. Thank you for being part of this journey.&lt;/p&gt; 
&lt;a href="https://github.com/OpenBB-finance/OpenBB/graphs/contributors"&gt; &lt;img src="https://contributors-img.web.app/image?repo=OpenBB-finance/OpenBB" width="800" /&gt; &lt;/a&gt; 
&lt;!-- MARKDOWN LINKS &amp; IMAGES --&gt; 
&lt;!-- https://www.markdownguide.org/basic-syntax/#reference-style-links --&gt;</description>
    </item>
    
    <item>
      <title>Lightricks/ComfyUI-LTXVideo</title>
      <link>https://github.com/Lightricks/ComfyUI-LTXVideo</link>
      <description>&lt;p&gt;LTX-Video Support for ComfyUI&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ComfyUI-LTXVideo&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/Lightricks/LTX-2"&gt;&lt;img src="https://img.shields.io/badge/LTX-Repo-blue?logo=github" alt="GitHub" /&gt;&lt;/a&gt; &lt;a href="https://ltx.io/model"&gt;&lt;img src="https://img.shields.io/badge/Website-LTX-181717?logo=google-chrome" alt="Website" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/Lightricks/LTX-2"&gt;&lt;img src="https://img.shields.io/badge/HuggingFace-Model-orange?logo=huggingface" alt="Model" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Lightricks/LTX-2/tree/main/packages/ltx-trainer"&gt;&lt;img src="https://img.shields.io/badge/LTX-Trainer%20Repo-9146FF" alt="LTXV Trainer" /&gt;&lt;/a&gt; &lt;a href="https://app.ltx.studio/ltx-2-playground/i2v"&gt;&lt;img src="https://img.shields.io/badge/Demo-Try%20Now-brightgreen?logo=vercel" alt="Demo" /&gt;&lt;/a&gt; &lt;a href="https://videos.ltx.io/LTX-2/grants/LTX_2_Technical_Report_compressed.pdf"&gt;&lt;img src="https://img.shields.io/badge/Paper-arXiv-B31B1B?logo=arxiv" alt="Paper" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/ltxplatform"&gt;&lt;img src="https://img.shields.io/badge/Join-Discord-5865F2?logo=discord" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;A collection of powerful custom nodes that extend ComfyUI's capabilities for the LTX-2 video generation model.&lt;/p&gt; 
&lt;p&gt;LTX-2 is built into ComfyUI core (&lt;a href="https://github.com/comfyanonymous/ComfyUI/tree/master/comfy/ldm/lightricks"&gt;see it here&lt;/a&gt;), making it readily accessible to all ComfyUI users. This repository hosts additional nodes and workflows to help you get the most out of LTX-2's advanced features.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;To learn more about LTX-2&lt;/strong&gt; See the &lt;a href="https://github.com/Lightricks/LTX-2"&gt;main LTX-2 repository&lt;/a&gt; for model details and additional resources.&lt;/p&gt; 
&lt;h2&gt;Prerequisites&lt;/h2&gt; 
&lt;p&gt;Before you begin using an LTX-2 workflow in ComfyUI, make sure you have:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ComfyUI installed (Download here](&lt;a href="https://www.comfy.org/download"&gt;https://www.comfy.org/download&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;CUDA-compatible GPU with 32GB+ VRAM&lt;/li&gt; 
 &lt;li&gt;100GB+ free disk space for models and cache&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Quick Start ğŸš€&lt;/h2&gt; 
&lt;p&gt;We recommend using the LTX-2 workflows available in Comfy Manager.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Open ComfyUI&lt;/li&gt; 
 &lt;li&gt;Click the Manager button (or press Ctrl+M)&lt;/li&gt; 
 &lt;li&gt;Select Install Custom Nodes&lt;/li&gt; 
 &lt;li&gt;Search for â€œLTXVideoâ€&lt;/li&gt; 
 &lt;li&gt;Click Install&lt;/li&gt; 
 &lt;li&gt;Wait for installation to complete&lt;/li&gt; 
 &lt;li&gt;Restart ComfyUI&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;The nodes will appear in your node menu under the â€œLTXVideoâ€ category. Required models will be downloaded on first use.&lt;/p&gt; 
&lt;h2&gt;Example Workflows&lt;/h2&gt; 
&lt;p&gt;The ComfyUI-LTXVideo installation includes several example workflows. You can see them all at: ''' ComfyUI/custom_nodes/ComfyUI-LTXVideo/example_workflows/ '''&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/ComfyUI-LTXVideo/master/example_workflows/LTX-2_T2V_Full_wLora.json"&gt;&lt;code&gt;Text to video full model&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/ComfyUI-LTXVideo/master/example_workflows/LTX-2_T2V_Distilled_wLora.json"&gt;&lt;code&gt;Text to video distilled model (Fast)&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/ComfyUI-LTXVideo/master/example_workflows/LTX-2_I2V_Full_wLora.json"&gt;&lt;code&gt;Image to video full model&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/ComfyUI-LTXVideo/master/example_workflows/LTX-2_I2V_Distilled_wLora.json"&gt;&lt;code&gt;Image to video distilled model (Fast)&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/ComfyUI-LTXVideo/master/example_workflows/LTX-2_V2V_Detailer.json"&gt;&lt;code&gt;Video to video detailer&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/ComfyUI-LTXVideo/master/example_workflows/LTX-2_ICLoRA_All_Distilled.json"&gt;&lt;code&gt;IC-LoRA distilled model (depth + human pose + edges)&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Required Models&lt;/h2&gt; 
&lt;p&gt;Download the following models:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;LTX-2 Model Checkpoint&lt;/strong&gt; - Choose and download one of the models to &lt;code&gt;COMFYUI_ROOT_FOLDER/models/checkpoints&lt;/code&gt; folder.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-2/blob/main/ltx-2-19b-dev-fp8.safetensors"&gt;&lt;code&gt;ltx-2-19b-dev-fp8.safetensors&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-2/blob/main/ltx-2-19b-distilled-fp8.safetensors"&gt;&lt;code&gt;ltx-2-19b-distilled-fp8.safetensors&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-2/blob/main/ltx-2-19b-dev.safetensors"&gt;&lt;code&gt;ltx-2-19b-dev.safetensors&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-2/blob/main/ltx-2-19b-distilled.safetensors"&gt;&lt;code&gt;ltx-2-19b-distilled.safetensors&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Spatial Upscaler&lt;/strong&gt; - Required for current two-stage pipeline implementations in this repository. Download to &lt;code&gt;COMFYUI_ROOT_FOLDER/models/latent_upscale_models&lt;/code&gt; folder.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-2/blob/main/ltx-2-spatial-upscaler-x2-1.0.safetensors"&gt;&lt;code&gt;ltx-2-spatial-upscaler-x2-1.0.safetensors&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Temporal Upscaler&lt;/strong&gt; - Required for current two-stage pipeline implementations in this repository. Download to &lt;code&gt;COMFYUI_ROOT_FOLDER/models/latent_upscale_models&lt;/code&gt; folder.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-2/blob/main/ltx-2-temporal-upscaler-x2-1.0.safetensors"&gt;&lt;code&gt;ltx-2-temporal-upscaler-x2-1.0.safetensors&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Distilled LoRA&lt;/strong&gt; - Required for current two-stage pipeline implementations in this repository (except DistilledPipeline and ICLoraPipeline). Download to &lt;code&gt;COMFYUI_ROOT_FOLDER/models/loras&lt;/code&gt; folder.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-2/blob/main/ltx-2-19b-distilled-lora-384.safetensors"&gt;&lt;code&gt;ltx-2-19b-distilled-lora-384.safetensors&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Gemma Text Encoder&lt;/strong&gt; Download all files from the repository to &lt;code&gt;COMFYUI_ROOT_FOLDER/models/text_encoders/gemma-3-12b-it-qat-q4_0-unquantized&lt;/code&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/google/gemma-3-12b-it-qat-q4_0-unquantized"&gt;&lt;code&gt;Gemma 3&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;LoRAs&lt;/strong&gt; Choose and download to &lt;code&gt;COMFYUI_ROOT_FOLDER/models/loras&lt;/code&gt; folder.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-2-19b-IC-LoRA-Canny-Control/blob/main/ltx-2-19b-ic-lora-canny-control.safetensors"&gt;&lt;code&gt;ltx-2-19b-ic-lora-canny-control.safetensors&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-2-19b-IC-LoRA-Depth-Control/blob/main/ltx-2-19b-ic-lora-depth-control.safetensors"&gt;&lt;code&gt;ltx-2-19b-ic-lora-depth-control.safetensors&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-2-19b-IC-LoRA-Detailer/blob/main/ltx-2-19b-ic-lora-detailer.safetensors"&gt;&lt;code&gt;ltx-2-19b-ic-lora-detailer.safetensors&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-2-19b-IC-LoRA-Pose-Control/blob/main/ltx-2-19b-ic-lora-pose-control.safetensors"&gt;&lt;code&gt;ltx-2-19b-ic-lora-pose-control.safetensors&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-2-19b-LoRA-Camera-Control-Dolly-In/blob/main/ltx-2-19b-lora-camera-control-dolly-in.safetensors"&gt;&lt;code&gt;ltx-2-19b-lora-camera-control-dolly-in.safetensors&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-2-19b-LoRA-Camera-Control-Dolly-Left/blob/main/ltx-2-19b-lora-camera-control-dolly-left.safetensors"&gt;&lt;code&gt;ltx-2-19b-lora-camera-control-dolly-left.safetensors&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-2-19b-LoRA-Camera-Control-Dolly-Out/blob/main/ltx-2-19b-lora-camera-control-dolly-out.safetensors"&gt;&lt;code&gt;ltx-2-19b-lora-camera-control-dolly-out.safetensors&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-2-19b-LoRA-Camera-Control-Dolly-Right/blob/main/ltx-2-19b-lora-camera-control-dolly-right.safetensors"&gt;&lt;code&gt;ltx-2-19b-lora-camera-control-dolly-right.safetensors&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-2-19b-LoRA-Camera-Control-Jib-Down/blob/main/ltx-2-19b-lora-camera-control-jib-down.safetensors"&gt;&lt;code&gt;ltx-2-19b-lora-camera-control-jib-down.safetensors&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-2-19b-LoRA-Camera-Control-Jib-Up/blob/main/ltx-2-19b-lora-camera-control-jib-up.safetensors"&gt;&lt;code&gt;ltx-2-19b-lora-camera-control-jib-up.safetensors&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-2-19b-LoRA-Camera-Control-Static/blob/main/ltx-2-19b-lora-camera-control-static.safetensors"&gt;&lt;code&gt;ltx-2-19b-lora-camera-control-static.safetensors&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Advanced Techniques&lt;/h2&gt; 
&lt;h3&gt;Low VRAM&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;For systems with low VRAM you can use the model loader nodes from &lt;a href="https://raw.githubusercontent.com/Lightricks/ComfyUI-LTXVideo/master/low_vram_loaders.py"&gt;low_vram_loaders.py&lt;/a&gt;. Those nodes ensure the correct order of execution and perform the model offloading such that generation fits in 32 GB VRAM.&lt;/li&gt; 
 &lt;li&gt;Use --reserve-vram ComfyUI parameter: &lt;code&gt;python -m main --reserve-vram 5&lt;/code&gt; (or other number in GB).&lt;/li&gt; 
 &lt;li&gt;For complete information about using LTX-2 models, workflows, and nodes in ComfyUI, please visit our &lt;a href="https://docs.ltx.video/open-source-model/integration-tools/comfy-ui"&gt;Open Source documentation&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>VectifyAI/PageIndex</title>
      <link>https://github.com/VectifyAI/PageIndex</link>
      <description>&lt;p&gt;ğŸ“‘ PageIndex: Document Index for Vectorless, Reasoning-based RAG&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;a href="https://vectify.ai/pageindex" target="_blank"&gt; &lt;img src="https://github.com/user-attachments/assets/46201e72-675b-43bc-bfbd-081cc6b65a1d" alt="PageIndex Banner" /&gt; &lt;/a&gt; 
 &lt;br /&gt; 
 &lt;br /&gt; 
 &lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/14736" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/14736" alt="VectifyAI%2FPageIndex | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; &lt;/p&gt; 
 &lt;p align="center"&gt;&lt;b&gt;Reasoning-based RAG&amp;nbsp; â—¦ &amp;nbsp;No Vector DB&amp;nbsp; â—¦ &amp;nbsp;No Chunking&amp;nbsp; â—¦ &amp;nbsp;Human-like Retrieval&lt;/b&gt;&lt;/p&gt; 
 &lt;h4 align="center"&gt; &lt;a href="https://vectify.ai"&gt;ğŸ  Homepage&lt;/a&gt;&amp;nbsp; â€¢ &amp;nbsp; &lt;a href="https://chat.pageindex.ai"&gt;ğŸ–¥ï¸ Chat Platform&lt;/a&gt;&amp;nbsp; â€¢ &amp;nbsp; &lt;a href="https://pageindex.ai/mcp"&gt;ğŸ”Œ MCP&lt;/a&gt;&amp;nbsp; â€¢ &amp;nbsp; &lt;a href="https://docs.pageindex.ai"&gt;ğŸ“š Docs&lt;/a&gt;&amp;nbsp; â€¢ &amp;nbsp; &lt;a href="https://discord.com/invite/VuXuf29EUj"&gt;ğŸ’¬ Discord&lt;/a&gt;&amp;nbsp; â€¢ &amp;nbsp; &lt;a href="https://ii2abc2jejf.typeform.com/to/tK3AXl8T"&gt;âœ‰ï¸ Contact&lt;/a&gt;&amp;nbsp; &lt;/h4&gt; 
&lt;/div&gt; 
&lt;details open&gt; 
 &lt;summary&gt;&lt;h2&gt;ğŸ“¢ Latest Updates&lt;/h2&gt;&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;ğŸ”¥ Releases:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://chat.pageindex.ai"&gt;&lt;strong&gt;PageIndex Chat&lt;/strong&gt;&lt;/a&gt;: The first human-like document-analysis agent &lt;a href="https://chat.pageindex.ai"&gt;platform&lt;/a&gt; built for professional long documents. Can also be integrated via &lt;a href="https://pageindex.ai/mcp"&gt;MCP&lt;/a&gt; or &lt;a href="https://docs.pageindex.ai/quickstart"&gt;API&lt;/a&gt; (beta).&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;!-- - [**PageIndex Chat API**](https://docs.pageindex.ai/quickstart): An API that brings PageIndex's advanced long-document intelligence directly into your applications and workflows. --&gt; 
 &lt;!-- - [PageIndex MCP](https://pageindex.ai/mcp): Bring PageIndex into Claude, Cursor, or any MCP-enabled agent. Chat with long PDFs in a reasoning-based, human-like way. --&gt; 
 &lt;p&gt;&lt;strong&gt;ğŸ“ Articles:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://pageindex.ai/blog/pageindex-intro"&gt;&lt;strong&gt;PageIndex Framework&lt;/strong&gt;&lt;/a&gt;: Introduces the PageIndex framework â€” an &lt;em&gt;agentic, in-context&lt;/em&gt; &lt;em&gt;tree index&lt;/em&gt; that enables LLMs to perform &lt;em&gt;reasoning-based&lt;/em&gt;, &lt;em&gt;human-like retrieval&lt;/em&gt; over long documents, without vector DB or chunking.&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;!-- - [Do We Still Need OCR?](https://pageindex.ai/blog/do-we-need-ocr): Explores how vision-based, reasoning-native RAG challenges the traditional OCR pipeline, and why the future of document AI might be *vectorless* and *vision-based*. --&gt; 
 &lt;p&gt;&lt;strong&gt;ğŸ§ª Cookbooks:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://docs.pageindex.ai/cookbook/vectorless-rag-pageindex"&gt;Vectorless RAG&lt;/a&gt;: A minimal, hands-on example of reasoning-based RAG using PageIndex. No vectors, no chunking, and human-like retrieval.&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://docs.pageindex.ai/cookbook/vision-rag-pageindex"&gt;Vision-based Vectorless RAG&lt;/a&gt;: OCR-free, vision-only RAG with PageIndex's reasoning-native retrieval workflow that works directly over PDF page images.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h1&gt;ğŸ“‘ Introduction to PageIndex&lt;/h1&gt; 
&lt;p&gt;Are you frustrated with vector database retrieval accuracy for long professional documents? Traditional vector-based RAG relies on semantic &lt;em&gt;similarity&lt;/em&gt; rather than true &lt;em&gt;relevance&lt;/em&gt;. But &lt;strong&gt;similarity â‰  relevance&lt;/strong&gt; â€” what we truly need in retrieval is &lt;strong&gt;relevance&lt;/strong&gt;, and that requires &lt;strong&gt;reasoning&lt;/strong&gt;. When working with professional documents that demand domain expertise and multi-step reasoning, similarity search often falls short.&lt;/p&gt; 
&lt;p&gt;Inspired by AlphaGo, we propose &lt;strong&gt;&lt;a href="https://vectify.ai/pageindex"&gt;PageIndex&lt;/a&gt;&lt;/strong&gt; â€” a &lt;strong&gt;vectorless&lt;/strong&gt;, &lt;strong&gt;reasoning-based RAG&lt;/strong&gt; system that builds a &lt;strong&gt;hierarchical tree index&lt;/strong&gt; from long documents and uses LLMs to &lt;strong&gt;reason&lt;/strong&gt; &lt;em&gt;over that index&lt;/em&gt; for &lt;strong&gt;agentic, context-aware retrieval&lt;/strong&gt;. It simulates how &lt;em&gt;human experts&lt;/em&gt; navigate and extract knowledge from complex documents through &lt;em&gt;tree search&lt;/em&gt;, enabling LLMs to &lt;em&gt;think&lt;/em&gt; and &lt;em&gt;reason&lt;/em&gt; their way to the most relevant document sections. PageIndex performs retrieval in two steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Generate a â€œTable-of-Contentsâ€ &lt;strong&gt;tree structure index&lt;/strong&gt; of documents&lt;/li&gt; 
 &lt;li&gt;Perform reasoning-based retrieval through &lt;strong&gt;tree search&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://pageindex.ai/blog/pageindex-intro" target="_blank" title="The PageIndex Framework"&gt; &lt;img src="https://docs.pageindex.ai/images/cookbook/vectorless-rag.png" width="70%" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h3&gt;ğŸ¯ Features&lt;/h3&gt; 
&lt;p&gt;Compared to traditional vector-based RAG, &lt;strong&gt;PageIndex&lt;/strong&gt; features:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;No Vector DB&lt;/strong&gt;: Uses document structure and LLM reasoning for retrieval, instead of vector similarity search.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;No Chunking&lt;/strong&gt;: Documents are organized into natural sections, not artificial chunks.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Human-like Retrieval&lt;/strong&gt;: Simulates how human experts navigate and extract knowledge from complex documents.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Better Explainability and Traceability&lt;/strong&gt;: Retrieval is based on reasoning â€” traceable and interpretable, with page and section references. No more opaque, approximate vector search (â€œvibe retrievalâ€).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;PageIndex powers a reasoning-based RAG system that achieved &lt;strong&gt;state-of-the-art&lt;/strong&gt; &lt;a href="https://github.com/VectifyAI/Mafin2.5-FinanceBench"&gt;98.7% accuracy&lt;/a&gt; on FinanceBench, demonstrating superior performance over vector-based RAG solutions in professional document analysis (see our &lt;a href="https://vectify.ai/blog/Mafin2.5"&gt;blog post&lt;/a&gt; for details).&lt;/p&gt; 
&lt;h3&gt;ğŸ“ Explore PageIndex&lt;/h3&gt; 
&lt;p&gt;To learn more, please see a detailed introduction of the &lt;a href="https://pageindex.ai/blog/pageindex-intro"&gt;PageIndex framework&lt;/a&gt;. Check out this GitHub repo for open-source code, and the &lt;a href="https://docs.pageindex.ai/cookbook"&gt;cookbooks&lt;/a&gt;, &lt;a href="https://docs.pageindex.ai/tutorials"&gt;tutorials&lt;/a&gt;, and &lt;a href="https://pageindex.ai/blog"&gt;blog&lt;/a&gt; for additional usage guides and examples.&lt;/p&gt; 
&lt;p&gt;The PageIndex service is available as a ChatGPT-style &lt;a href="https://chat.pageindex.ai"&gt;chat platform&lt;/a&gt;, or can be integrated via &lt;a href="https://pageindex.ai/mcp"&gt;MCP&lt;/a&gt; or &lt;a href="https://docs.pageindex.ai/quickstart"&gt;API&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;ğŸ› ï¸ Deployment Options&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Self-host â€” run locally with this open-source repo.&lt;/li&gt; 
 &lt;li&gt;Cloud Service â€” try instantly with our &lt;a href="https://chat.pageindex.ai/"&gt;Chat Platform&lt;/a&gt;, or integrate with &lt;a href="https://pageindex.ai/mcp"&gt;MCP&lt;/a&gt; or &lt;a href="https://docs.pageindex.ai/quickstart"&gt;API&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;Enterprise&lt;/em&gt; â€” private or on-prem deployment. &lt;a href="https://ii2abc2jejf.typeform.com/to/tK3AXl8T"&gt;Contact us&lt;/a&gt; or &lt;a href="https://calendly.com/pageindex/meet"&gt;book a demo&lt;/a&gt; for more details.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ§ª Quick Hands-on&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Try the &lt;a href="https://github.com/VectifyAI/PageIndex/raw/main/cookbook/pageindex_RAG_simple.ipynb"&gt;&lt;strong&gt;Vectorless RAG&lt;/strong&gt;&lt;/a&gt; notebook â€” a &lt;em&gt;minimal&lt;/em&gt;, hands-on example of reasoning-based RAG using PageIndex.&lt;/li&gt; 
 &lt;li&gt;Experiment with &lt;a href="https://github.com/VectifyAI/PageIndex/raw/main/cookbook/vision_RAG_pageindex.ipynb"&gt;&lt;em&gt;Vision-based Vectorless RAG&lt;/em&gt;&lt;/a&gt; â€” no OCR; a minimal, reasoning-native RAG pipeline that works directly over page images.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://colab.research.google.com/github/VectifyAI/PageIndex/blob/main/cookbook/pageindex_RAG_simple.ipynb" target="_blank" rel="noopener"&gt; &lt;img src="https://img.shields.io/badge/Open_In_Colab-Vectorless_RAG-orange?style=for-the-badge&amp;amp;logo=googlecolab" alt="Open in Colab: Vectorless RAG" /&gt; &lt;/a&gt; &amp;nbsp;&amp;nbsp; 
 &lt;a href="https://colab.research.google.com/github/VectifyAI/PageIndex/blob/main/cookbook/vision_RAG_pageindex.ipynb" target="_blank" rel="noopener"&gt; &lt;img src="https://img.shields.io/badge/Open_In_Colab-Vision_RAG-orange?style=for-the-badge&amp;amp;logo=googlecolab" alt="Open in Colab: Vision RAG" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h1&gt;ğŸŒ² PageIndex Tree Structure&lt;/h1&gt; 
&lt;p&gt;PageIndex can transform lengthy PDF documents into a semantic &lt;strong&gt;tree structure&lt;/strong&gt;, similar to a &lt;em&gt;"table of contents"&lt;/em&gt; but optimized for use with Large Language Models (LLMs). It's ideal for: financial reports, regulatory filings, academic textbooks, legal or technical manuals, and any document that exceeds LLM context limits.&lt;/p&gt; 
&lt;p&gt;Below is an example PageIndex tree structure. Also see more example &lt;a href="https://github.com/VectifyAI/PageIndex/tree/main/tests/pdfs"&gt;documents&lt;/a&gt; and generated &lt;a href="https://github.com/VectifyAI/PageIndex/tree/main/tests/results"&gt;tree structures&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsonc"&gt;...
{
  "title": "Financial Stability",
  "node_id": "0006",
  "start_index": 21,
  "end_index": 22,
  "summary": "The Federal Reserve ...",
  "nodes": [
    {
      "title": "Monitoring Financial Vulnerabilities",
      "node_id": "0007",
      "start_index": 22,
      "end_index": 28,
      "summary": "The Federal Reserve's monitoring ..."
    },
    {
      "title": "Domestic and International Cooperation and Coordination",
      "node_id": "0008",
      "start_index": 28,
      "end_index": 31,
      "summary": "In 2023, the Federal Reserve collaborated ..."
    }
  ]
}
...
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can generate the PageIndex tree structure with this open-source repo, or use our &lt;a href="https://docs.pageindex.ai/quickstart"&gt;API&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h1&gt;âš™ï¸ Package Usage&lt;/h1&gt; 
&lt;p&gt;You can follow these steps to generate a PageIndex tree from a PDF document.&lt;/p&gt; 
&lt;h3&gt;1. Install dependencies&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip3 install --upgrade -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. Set your OpenAI API key&lt;/h3&gt; 
&lt;p&gt;Create a &lt;code&gt;.env&lt;/code&gt; file in the root directory and add your API key:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;CHATGPT_API_KEY=your_openai_key_here
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;3. Run PageIndex on your PDF&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python3 run_pageindex.py --pdf_path /path/to/your/document.pdf
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Optional parameters&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; You can customize the processing with additional optional arguments: 
 &lt;pre&gt;&lt;code&gt;--model                 OpenAI model to use (default: gpt-4o-2024-11-20)
--toc-check-pages       Pages to check for table of contents (default: 20)
--max-pages-per-node    Max pages per node (default: 10)
--max-tokens-per-node   Max tokens per node (default: 20000)
--if-add-node-id        Add node ID (yes/no, default: yes)
--if-add-node-summary   Add node summary (yes/no, default: yes)
--if-add-doc-description Add doc description (yes/no, default: yes)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Markdown support&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; We also provide markdown support for PageIndex. You can use the `-md_path` flag to generate a tree structure for a markdown file. 
 &lt;pre&gt;&lt;code class="language-bash"&gt;python3 run_pageindex.py --md_path /path/to/your/document.md
&lt;/code&gt;&lt;/pre&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;Note: in this function, we use "#" to determine node heading and their levels. For example, "##" is level 2, "###" is level 3, etc. Make sure your markdown file is formatted correctly. If your Markdown file was converted from a PDF or HTML, we don't recommend using this function, since most existing conversion tools cannot preserve the original hierarchy. Instead, use our &lt;a href="https://pageindex.ai/blog/ocr"&gt;PageIndex OCR&lt;/a&gt;, which is designed to preserve the original hierarchy, to convert the PDF to a markdown file and then use this function.&lt;/p&gt; 
 &lt;/blockquote&gt; 
&lt;/details&gt; 
&lt;!-- 
# â˜ï¸ Improved Tree Generation with PageIndex OCR

This repo is designed for generating PageIndex tree structure for simple PDFs, but many real-world use cases involve complex PDFs that are hard to parse by classic Python tools. However, extracting high-quality text from PDF documents remains a non-trivial challenge. Most OCR tools only extract page-level content, losing the broader document context and hierarchy.

To address this, we introduced PageIndex OCR â€” the first long-context OCR model designed to preserve the global structure of documents. PageIndex OCR significantly outperforms other leading OCR tools, such as those from Mistral and Contextual AI, in recognizing true hierarchy and semantic relationships across document pages.

- Experience next-level OCR quality with PageIndex OCR at our [Dashboard](https://dash.pageindex.ai/).
- Integrate PageIndex OCR seamlessly into your stack via our [API](https://docs.pageindex.ai/quickstart).

&lt;p align="center"&gt;
  &lt;img src="https://github.com/user-attachments/assets/eb35d8ae-865c-4e60-a33b-ebbd00c41732" width="80%"&gt;
&lt;/p&gt;
--&gt; 
&lt;hr /&gt; 
&lt;h1&gt;ğŸ“ˆ Case Study: PageIndex Leads Finance QA Benchmark&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://vectify.ai/mafin"&gt;Mafin 2.5&lt;/a&gt; is a reasoning-based RAG system for financial document analysis, powered by &lt;strong&gt;PageIndex&lt;/strong&gt;. It achieved a state-of-the-art &lt;a href="https://vectify.ai/blog/Mafin2.5"&gt;&lt;strong&gt;98.7% accuracy&lt;/strong&gt;&lt;/a&gt; on the &lt;a href="https://arxiv.org/abs/2311.11944"&gt;FinanceBench&lt;/a&gt; benchmark, significantly outperforming traditional vector-based RAG systems.&lt;/p&gt; 
&lt;p&gt;PageIndex's hierarchical indexing and reasoning-driven retrieval enable precise navigation and extraction of relevant context from complex financial reports, such as SEC filings and earnings disclosures.&lt;/p&gt; 
&lt;p&gt;Explore the full &lt;a href="https://github.com/VectifyAI/Mafin2.5-FinanceBench"&gt;benchmark results&lt;/a&gt; and our &lt;a href="https://vectify.ai/blog/Mafin2.5"&gt;blog post&lt;/a&gt; for detailed comparisons and performance metrics.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/VectifyAI/Mafin2.5-FinanceBench"&gt; &lt;img src="https://github.com/user-attachments/assets/571aa074-d803-43c7-80c4-a04254b782a3" width="70%" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h1&gt;ğŸ§­ Resources&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ§ª &lt;a href="https://docs.pageindex.ai/cookbook/vectorless-rag-pageindex"&gt;Cookbooks&lt;/a&gt;: hands-on, runnable examples and advanced use cases.&lt;/li&gt; 
 &lt;li&gt;ğŸ“– &lt;a href="https://docs.pageindex.ai/doc-search"&gt;Tutorials&lt;/a&gt;: practical guides and strategies, including &lt;em&gt;Document Search&lt;/em&gt; and &lt;em&gt;Tree Search&lt;/em&gt;.&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://pageindex.ai/blog"&gt;Blog&lt;/a&gt;: technical articles, research insights, and product updates.&lt;/li&gt; 
 &lt;li&gt;ğŸ”Œ &lt;a href="https://pageindex.ai/mcp#quick-setup"&gt;MCP setup&lt;/a&gt; &amp;amp; &lt;a href="https://docs.pageindex.ai/quickstart"&gt;API docs&lt;/a&gt;: integration details and configuration options.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h1&gt;â­ Support Us&lt;/h1&gt; 
&lt;p&gt;Leave us a star ğŸŒŸ if you like our project. Thank you!&lt;/p&gt; 
&lt;p&gt; &lt;img src="https://github.com/user-attachments/assets/eae4ff38-48ae-4a7c-b19f-eab81201d794" width="80%" /&gt; &lt;/p&gt; 
&lt;h3&gt;Connect with Us&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://x.com/VectifyAI"&gt;&lt;img src="https://img.shields.io/badge/Twitter-000000?style=for-the-badge&amp;amp;logo=x&amp;amp;logoColor=white" alt="Twitter" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://www.linkedin.com/company/vectify-ai/"&gt;&lt;img src="https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&amp;amp;logo=linkedin&amp;amp;logoColor=white" alt="LinkedIn" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://discord.com/invite/VuXuf29EUj"&gt;&lt;img src="https://img.shields.io/badge/Discord-5865F2?style=for-the-badge&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://ii2abc2jejf.typeform.com/to/tK3AXl8T"&gt;&lt;img src="https://img.shields.io/badge/Contact_Us-3B82F6?style=for-the-badge&amp;amp;logo=envelope&amp;amp;logoColor=white" alt="Contact Us" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;Â© 2025 &lt;a href="https://vectify.ai"&gt;Vectify AI&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>hacksider/Deep-Live-Cam</title>
      <link>https://github.com/hacksider/Deep-Live-Cam</link>
      <description>&lt;p&gt;real time face swap and one-click video deepfake with only a single image&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt;Deep-Live-Cam 2.0.1c&lt;/h1&gt; 
&lt;p align="center"&gt; Real-time face swap and video deepfake with a single click and only a single image. &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/11395" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/11395" alt="hacksider%2FDeep-Live-Cam | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/media/demo.gif" alt="Demo GIF" width="800" /&gt; &lt;/p&gt; 
&lt;h2&gt;Disclaimer&lt;/h2&gt; 
&lt;p&gt;This deepfake software is designed to be a productive tool for the AI-generated media industry. It can assist artists in animating custom characters, creating engaging content, and even using models for clothing design.&lt;/p&gt; 
&lt;p&gt;We are aware of the potential for unethical applications and are committed to preventative measures. A built-in check prevents the program from processing inappropriate media (nudity, graphic content, sensitive material like war footage, etc.). We will continue to develop this project responsibly, adhering to the law and ethics. We may shut down the project or add watermarks if legally required.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Ethical Use: Users are expected to use this software responsibly and legally. If using a real person's face, obtain their consent and clearly label any output as a deepfake when sharing online.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Content Restrictions: The software includes built-in checks to prevent processing inappropriate media, such as nudity, graphic content, or sensitive material.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Legal Compliance: We adhere to all relevant laws and ethical guidelines. If legally required, we may shut down the project or add watermarks to the output.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;User Responsibility: We are not responsible for end-user actions. Users must ensure their use of the software aligns with ethical standards and legal requirements.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;By using this software, you agree to these terms and commit to using it in a manner that respects the rights and dignity of others.&lt;/p&gt; 
&lt;p&gt;Users are expected to use this software responsibly and legally. If using a real person's face, obtain their consent and clearly label any output as a deepfake when sharing online. We are not responsible for end-user actions.&lt;/p&gt; 
&lt;h2&gt;Exclusive v2.4 Quick Start - Pre-built (Windows/Mac Silicon)&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://deeplivecam.net/index.php/quickstart"&gt; &lt;img src="https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/media/Download.png" width="285" height="77" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;a href="https://deeplivecam.net/index.php/quickstart"&gt; &lt;h5&gt;This is the fastest build you can get if you have a discrete NVIDIA or AMD GPU or Mac Silicon, And you'll receive special priority support.&lt;/h5&gt; &lt;h6&gt;These Pre-builts are perfect for non-technical users or those who don't have time to, or can't manually install all the requirements. Just a heads-up: this is an open-source project, so you can also install it manually.&lt;/h6&gt; &lt;h2&gt;TLDR; Live Deepfake in just 3 Clicks&lt;/h2&gt; &lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/af825228-852c-411b-b787-ffd9aac72fc6" alt="easysteps" /&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Select a face&lt;/li&gt; 
  &lt;li&gt;Select which camera to use&lt;/li&gt; 
  &lt;li&gt;Press live!&lt;/li&gt; 
 &lt;/ol&gt; &lt;h2&gt;Features &amp;amp; Uses - Everything is in real-time&lt;/h2&gt; &lt;h3&gt;Mouth Mask&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Retain your original mouth for accurate movement using Mouth Mask&lt;/strong&gt;&lt;/p&gt; &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/media/ludwig.gif" alt="resizable-gif" /&gt; &lt;/p&gt; &lt;h3&gt;Face Mapping&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Use different faces on multiple subjects simultaneously&lt;/strong&gt;&lt;/p&gt; &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/media/streamers.gif" alt="face_mapping_source" /&gt; &lt;/p&gt; &lt;h3&gt;Your Movie, Your Face&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Watch movies with any face in real-time&lt;/strong&gt;&lt;/p&gt; &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/media/movie.gif" alt="movie" /&gt; &lt;/p&gt; &lt;h3&gt;Live Show&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Run Live shows and performances&lt;/strong&gt;&lt;/p&gt; &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/media/live_show.gif" alt="show" /&gt; &lt;/p&gt; &lt;h3&gt;Memes&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Create Your Most Viral Meme Yet&lt;/strong&gt;&lt;/p&gt; &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/media/meme.gif" alt="show" width="450" /&gt; &lt;br /&gt; &lt;sub&gt;Created using Many Faces feature in Deep-Live-Cam&lt;/sub&gt; &lt;/p&gt; &lt;h3&gt;Omegle&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Surprise people on Omegle&lt;/strong&gt;&lt;/p&gt; &lt;p align="center"&gt; 
  &lt;video src="https://github.com/user-attachments/assets/2e9b9b82-fa04-4b70-9f56-b1f68e7672d0" width="450" controls&gt;&lt;/video&gt; &lt;/p&gt; &lt;h2&gt;Installation (Manual)&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;Please be aware that the installation requires technical skills and is not for beginners. Consider downloading the quickstart version.&lt;/strong&gt;&lt;/p&gt; &lt;/a&gt;
&lt;details&gt;
 &lt;a href="https://deeplivecam.net/index.php/quickstart"&gt; &lt;summary&gt;Click to see the process&lt;/summary&gt; &lt;h3&gt;Installation&lt;/h3&gt; &lt;p&gt;This is more likely to work on your computer but will be slower as it utilizes the CPU.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. Set up Your Platform&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Python (3.11 recommended)&lt;/li&gt; 
   &lt;li&gt;pip&lt;/li&gt; 
   &lt;li&gt;git&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=OlNWCpFdVMA"&gt;ffmpeg&lt;/a&gt; - &lt;code&gt;iex (irm ffmpeg.tc.ht)&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://visualstudio.microsoft.com/visual-cpp-build-tools/"&gt;Visual Studio 2022 Runtimes (Windows)&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt;&lt;/a&gt; 
 &lt;p&gt;&lt;strong&gt;2. Clone the Repository&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/hacksider/Deep-Live-Cam.git
cd Deep-Live-Cam
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;3. Download the Models&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/hacksider/deep-live-cam/resolve/main/GFPGANv1.4.pth"&gt;GFPGANv1.4&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/hacksider/deep-live-cam/resolve/main/inswapper_128_fp16.onnx"&gt;inswapper_128_fp16.onnx&lt;/a&gt;&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;Place these files in the "&lt;strong&gt;models&lt;/strong&gt;" folder.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;4. Install Dependencies&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;We highly recommend using a &lt;code&gt;venv&lt;/code&gt; to avoid issues.&lt;/p&gt; 
 &lt;p&gt;For Windows:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;python -m venv venv
venv\Scripts\activate
pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;For Linux:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Ensure you use the installed Python 3.10
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;For macOS:&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;Apple Silicon (M1/M2/M3) requires specific setup:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Install Python 3.11 (specific version is important)
brew install python@3.11

# Install tkinter package (required for the GUI)
brew install python-tk@3.10

# Create and activate virtual environment with Python 3.11
python3.11 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;** In case something goes wrong and you need to reinstall the virtual environment **&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Deactivate the virtual environment
rm -rf venv

# Reinstall the virtual environment
python -m venv venv
source venv/bin/activate

# install the dependencies again
pip install -r requirements.txt

# gfpgan and basicsrs issue fix
pip install git+https://github.com/xinntao/BasicSR.git@master
pip uninstall gfpgan -y
pip install git+https://github.com/TencentARC/GFPGAN.git@master
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Run:&lt;/strong&gt; If you don't have a GPU, you can run Deep-Live-Cam using &lt;code&gt;python run.py&lt;/code&gt;. Note that initial execution will download models (~300MB).&lt;/p&gt; 
 &lt;h3&gt;GPU Acceleration&lt;/h3&gt; 
 &lt;p&gt;&lt;strong&gt;CUDA Execution Provider (Nvidia)&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Install &lt;a href="https://developer.nvidia.com/cuda-12-8-0-download-archive"&gt;CUDA Toolkit 12.8.0&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Install &lt;a href="https://developer.nvidia.com/rdp/cudnn-archive"&gt;cuDNN v8.9.7 for CUDA 12.x&lt;/a&gt; (required for onnxruntime-gpu): 
   &lt;ul&gt; 
    &lt;li&gt;Download cuDNN v8.9.7 for CUDA 12.x&lt;/li&gt; 
    &lt;li&gt;Make sure the cuDNN bin directory is in your system PATH&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;Install dependencies:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;pip install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128
pip uninstall onnxruntime onnxruntime-gpu
pip install onnxruntime-gpu==1.21.0
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="3"&gt; 
  &lt;li&gt;Usage:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;python run.py --execution-provider cuda
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;CoreML Execution Provider (Apple Silicon)&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;Apple Silicon (M1/M2/M3) specific installation:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Make sure you've completed the macOS setup above using Python 3.10.&lt;/li&gt; 
  &lt;li&gt;Install dependencies:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;pip uninstall onnxruntime onnxruntime-silicon
pip install onnxruntime-silicon==1.13.1
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="3"&gt; 
  &lt;li&gt;Usage (important: specify Python 3.10):&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;python3.10 run.py --execution-provider coreml
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Important Notes for macOS:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;You &lt;strong&gt;must&lt;/strong&gt; use Python 3.10, not newer versions like 3.11 or 3.13&lt;/li&gt; 
  &lt;li&gt;Always run with &lt;code&gt;python3.10&lt;/code&gt; command not just &lt;code&gt;python&lt;/code&gt; if you have multiple Python versions installed&lt;/li&gt; 
  &lt;li&gt;If you get error about &lt;code&gt;_tkinter&lt;/code&gt; missing, reinstall the tkinter package: &lt;code&gt;brew reinstall python-tk@3.10&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;If you get model loading errors, check that your models are in the correct folder&lt;/li&gt; 
  &lt;li&gt;If you encounter conflicts with other Python versions, consider uninstalling them: &lt;pre&gt;&lt;code class="language-bash"&gt;# List all installed Python versions
brew list | grep python

# Uninstall conflicting versions if needed
brew uninstall --ignore-dependencies python@3.11 python@3.13

# Keep only Python 3.11
brew cleanup
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;CoreML Execution Provider (Apple Legacy)&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Install dependencies:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;pip uninstall onnxruntime onnxruntime-coreml
pip install onnxruntime-coreml==1.21.0
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="2"&gt; 
  &lt;li&gt;Usage:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;python run.py --execution-provider coreml
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;DirectML Execution Provider (Windows)&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Install dependencies:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;pip uninstall onnxruntime onnxruntime-directml
pip install onnxruntime-directml==1.21.0
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="2"&gt; 
  &lt;li&gt;Usage:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;python run.py --execution-provider directml
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;OpenVINOâ„¢ Execution Provider (Intel)&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Install dependencies:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;pip uninstall onnxruntime onnxruntime-openvino
pip install onnxruntime-openvino==1.21.0
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="2"&gt; 
  &lt;li&gt;Usage:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;python run.py --execution-provider openvino
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;1. Image/Video Mode&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Execute &lt;code&gt;python run.py&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Choose a source face image and a target image/video.&lt;/li&gt; 
 &lt;li&gt;Click "Start".&lt;/li&gt; 
 &lt;li&gt;The output will be saved in a directory named after the target video.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;2. Webcam Mode&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Execute &lt;code&gt;python run.py&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Select a source face image.&lt;/li&gt; 
 &lt;li&gt;Click "Live".&lt;/li&gt; 
 &lt;li&gt;Wait for the preview to appear (10-30 seconds).&lt;/li&gt; 
 &lt;li&gt;Use a screen capture tool like OBS to stream.&lt;/li&gt; 
 &lt;li&gt;To change the face, select a new source image.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Command Line Arguments (Unmaintained)&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;options:
  -h, --help                                               show this help message and exit
  -s SOURCE_PATH, --source SOURCE_PATH                     select a source image
  -t TARGET_PATH, --target TARGET_PATH                     select a target image or video
  -o OUTPUT_PATH, --output OUTPUT_PATH                     select output file or directory
  --frame-processor FRAME_PROCESSOR [FRAME_PROCESSOR ...]  frame processors (choices: face_swapper, face_enhancer, ...)
  --keep-fps                                               keep original fps
  --keep-audio                                             keep original audio
  --keep-frames                                            keep temporary frames
  --many-faces                                             process every face
  --map-faces                                              map source target faces
  --mouth-mask                                             mask the mouth region
  --video-encoder {libx264,libx265,libvpx-vp9}             adjust output video encoder
  --video-quality [0-51]                                   adjust output video quality
  --live-mirror                                            the live camera display as you see it in the front-facing camera frame
  --live-resizable                                         the live camera frame is resizable
  --max-memory MAX_MEMORY                                  maximum amount of RAM in GB
  --execution-provider {cpu} [{cpu} ...]                   available execution provider (choices: cpu, ...)
  --execution-threads EXECUTION_THREADS                    number of execution threads
  -v, --version                                            show program's version number and exit
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Looking for a CLI mode? Using the -s/--source argument will make the run program in cli mode.&lt;/p&gt; 
&lt;h2&gt;Press&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;We are always open to criticism and are ready to improve, that's why we didn't cherry-pick anything.&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://arstechnica.com/information-technology/2024/08/new-ai-tool-enables-real-time-face-swapping-on-webcams-raising-fraud-concerns/"&gt;&lt;em&gt;"Deep-Live-Cam goes viral, allowing anyone to become a digital doppelganger"&lt;/em&gt;&lt;/a&gt; - Ars Technica&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://dataconomy.com/2024/08/15/what-is-deep-live-cam-github-deepfake/"&gt;&lt;em&gt;"Thanks Deep Live Cam, shapeshifters are among us now"&lt;/em&gt;&lt;/a&gt; - Dataconomy&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.newsbytesapp.com/news/science/deep-live-cam-ai-impersonation-tool-goes-viral/story"&gt;&lt;em&gt;"This free AI tool lets you become anyone during video-calls"&lt;/em&gt;&lt;/a&gt; - NewsBytes&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.creativebloq.com/ai/ok-this-viral-ai-live-stream-software-is-truly-terrifying"&gt;&lt;em&gt;"OK, this viral AI live stream software is truly terrifying"&lt;/em&gt;&lt;/a&gt; - Creative Bloq&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://petapixel.com/2024/08/14/deep-live-cam-deepfake-ai-tool-lets-you-become-anyone-in-a-video-call-with-single-photo-mark-zuckerberg-jd-vance-elon-musk/"&gt;&lt;em&gt;"Deepfake AI Tool Lets You Become Anyone in a Video Call With Single Photo"&lt;/em&gt;&lt;/a&gt; - PetaPixel&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.techeblog.com/deep-live-cam-ai-transform-face/"&gt;&lt;em&gt;"Deep-Live-Cam Uses AI to Transform Your Face in Real-Time, Celebrities Included"&lt;/em&gt;&lt;/a&gt; - TechEBlog&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://telegrafi.com/en/a-tool-that-makes-you-look-like-anyone-during-a-video-call-is-going-viral-on-the-Internet/"&gt;&lt;em&gt;"An AI tool that "makes you look like anyone" during a video call is going viral online"&lt;/em&gt;&lt;/a&gt; - Telegrafi&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://decrypt.co/244565/this-deepfake-tool-turning-images-into-livestreams-is-topping-the-github-charts"&gt;&lt;em&gt;"This Deepfake Tool Turning Images Into Livestreams is Topping the GitHub Charts"&lt;/em&gt;&lt;/a&gt; - Emerge&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.digitalmusicnews.com/2024/08/15/face-swapping-ai-real-time-mimic/"&gt;&lt;em&gt;"New Real-Time Face-Swapping AI Allows Anyone to Mimic Famous Faces"&lt;/em&gt;&lt;/a&gt; - Digital Music News&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.diyphotography.net/this-real-time-webcam-deepfake-tool-raises-alarms-about-the-future-of-identity-theft/"&gt;&lt;em&gt;"This real-time webcam deepfake tool raises alarms about the future of identity theft"&lt;/em&gt;&lt;/a&gt; - DIYPhotography&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/watch?time_continue=1074&amp;amp;v=py4Tc-Y8BcY"&gt;&lt;em&gt;"That's Crazy, Oh God. That's Fucking Freaky Dude... That's So Wild Dude"&lt;/em&gt;&lt;/a&gt; - SomeOrdinaryGamers&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/live/mFsCe7AIxq8?feature=shared&amp;amp;t=2686"&gt;&lt;em&gt;"Alright look look look, now look chat, we can do any face we want to look like chat"&lt;/em&gt;&lt;/a&gt; - IShowSpeed&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=wnCghLjqv3s&amp;amp;t=551s"&gt;&lt;em&gt;"They do a pretty good job matching poses, expression and even the lighting"&lt;/em&gt;&lt;/a&gt; - TechLinked (LTT)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.golem.de/news/deepfakes-als-sean-connery-an-der-redaktionskonferenz-teilnahm-2408-188172.html"&gt;&lt;em&gt;"Als Sean Connery an der Redaktionskonferenz teilnahm"&lt;/em&gt;&lt;/a&gt; - Golem.de (German)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://youtu.be/JbUPRmXRUtE?t=3964"&gt;&lt;em&gt;"What the F&lt;/em&gt;**! Why do I look like Vinny Jr? I look exactly like Vinny Jr!? No, this shit is crazy! Bro This is F*** Crazy! "*&lt;/a&gt; - IShowSpeed&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Credits&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://ffmpeg.org/"&gt;ffmpeg&lt;/a&gt;: for making video-related operations easy&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/henryruhs"&gt;Henry&lt;/a&gt;: One of the major contributor in this repo&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/deepinsight"&gt;deepinsight&lt;/a&gt;: for their &lt;a href="https://github.com/deepinsight/insightface"&gt;insightface&lt;/a&gt; project which provided a well-made library and models. Please be reminded that the &lt;a href="https://github.com/deepinsight/insightface?tab=readme-ov-file#license"&gt;use of the model is for non-commercial research purposes only&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/havok2-htwo"&gt;havok2-htwo&lt;/a&gt;: for sharing the code for webcam&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/GosuDRM"&gt;GosuDRM&lt;/a&gt;: for the open version of roop&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pereiraroland26"&gt;pereiraroland26&lt;/a&gt;: Multiple faces support&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/vic4key"&gt;vic4key&lt;/a&gt;: For supporting/contributing to this project&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/kier007"&gt;kier007&lt;/a&gt;: for improving the user experience&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/qitianai"&gt;qitianai&lt;/a&gt;: for multi-lingual support&lt;/li&gt; 
 &lt;li&gt;and &lt;a href="https://github.com/hacksider/Deep-Live-Cam/graphs/contributors"&gt;all developers&lt;/a&gt; behind libraries used in this project.&lt;/li&gt; 
 &lt;li&gt;Footnote: Please be informed that the base author of the code is &lt;a href="https://github.com/s0md3v/roop"&gt;s0md3v&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;All the wonderful users who helped make this project go viral by starring the repo â¤ï¸&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://github.com/hacksider/Deep-Live-Cam/stargazers"&gt;&lt;img src="https://reporoster.com/stars/hacksider/Deep-Live-Cam" alt="Stargazers" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Contributions&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://repobeats.axiom.co/api/embed/fec8e29c45dfdb9c5916f3a7830e1249308d20e1.svg?sanitize=true" alt="Alt" title="Repobeats analytics image" /&gt;&lt;/p&gt; 
&lt;h2&gt;Stars to the Moon ğŸš€&lt;/h2&gt; 
&lt;a href="https://star-history.com/#hacksider/deep-live-cam&amp;amp;Date"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=hacksider/deep-live-cam&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=hacksider/deep-live-cam&amp;amp;type=Date" /&gt; 
  &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=hacksider/deep-live-cam&amp;amp;type=Date" /&gt; 
 &lt;/picture&gt; &lt;/a&gt;</description>
    </item>
    
    <item>
      <title>NevaMind-AI/memU</title>
      <link>https://github.com/NevaMind-AI/memU</link>
      <description>&lt;p&gt;Memory infrastructure for LLMs and AI agents&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/banner.png" alt="MemU Banner" /&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;h1&gt;MemU&lt;/h1&gt; 
 &lt;h3&gt;A Future-Oriented Agentic Memory System&lt;/h3&gt; 
 &lt;p&gt;&lt;a href="https://badge.fury.io/py/memu-py"&gt;&lt;img src="https://badge.fury.io/py/memu-py.svg?sanitize=true" alt="PyPI version" /&gt;&lt;/a&gt; &lt;a href="https://opensource.org/licenses/Apache-2.0"&gt;&lt;img src="https://img.shields.io/badge/License-Apache%202.0-blue.svg?sanitize=true" alt="License: Apache 2.0" /&gt;&lt;/a&gt; &lt;a href="https://www.python.org/downloads/"&gt;&lt;img src="https://img.shields.io/badge/python-3.13+-blue.svg?sanitize=true" alt="Python 3.13+" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/memu"&gt;&lt;img src="https://img.shields.io/badge/Discord-Join%20Chat-5865F2?logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://x.com/memU_ai"&gt;&lt;img src="https://img.shields.io/badge/Twitter-Follow-1DA1F2?logo=x&amp;amp;logoColor=white" alt="Twitter" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/17374" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/17374" alt="NevaMind-AI%2FmemU | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;p&gt;MemU is an agentic memory framework for LLM and AI agent backends. It receives &lt;strong&gt;multimodal inputs&lt;/strong&gt; (conversations, documents, images), extracts them into structured memory, and organizes them into a &lt;strong&gt;hierarchical file system&lt;/strong&gt; that supports both &lt;strong&gt;embedding-based (RAG)&lt;/strong&gt; and &lt;strong&gt;non-embedding (LLM)&lt;/strong&gt; retrieval.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;â­ï¸ Star the repository&lt;/h2&gt; 
&lt;img width="100%" src="https://github.com/NevaMind-AI/memU/raw/main/assets/star.gif" /&gt; If you find memU useful or interesting, a GitHub Star â­ï¸ would be greatly appreciated. 
&lt;hr /&gt; 
&lt;p&gt;MemU is collaborating with four open-source projects to launch the 2026 New Year Challenge. ğŸ‰Between January 8â€“18, contributors can submit PRs to memU and earn cash rewards, community recognition, and platform credits. ğŸ&lt;a href="https://discord.gg/KaWy6SBAsx"&gt;Learn more &amp;amp; get involved&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;âœ¨ Core Features&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Feature&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ğŸ—‚ï¸ &lt;strong&gt;Hierarchical File System&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Three-layer architecture: Resource â†’ Item â†’ Category with full traceability&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ğŸ” &lt;strong&gt;Dual Retrieval Methods&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;RAG (embedding-based) for speed, LLM (non-embedding) for deep semantic understanding&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ğŸ¨ &lt;strong&gt;Multimodal Support&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Process conversations, documents, images, audio, and video&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ğŸ”„ &lt;strong&gt;Self-Evolving Memory&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Memory structure adapts and improves based on usage patterns&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ—‚ï¸ Hierarchical File System&lt;/h2&gt; 
&lt;p&gt;MemU organizes memory using a &lt;strong&gt;three-layer architecture&lt;/strong&gt; inspired by hierarchical storage systems:&lt;/p&gt; 
&lt;img width="100%" alt="structure" src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/structure.png" /&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Layer&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Examples&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Resource&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Raw multimodal data warehouse&lt;/td&gt; 
   &lt;td&gt;JSON conversations, text documents, images, videos&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Item&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Discrete extracted memory units&lt;/td&gt; 
   &lt;td&gt;Individual preferences, skills, opinions, habits&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Category&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Aggregated textual memory with summaries&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;preferences.md&lt;/code&gt;, &lt;code&gt;work_life.md&lt;/code&gt;, &lt;code&gt;relationships.md&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Key Benefits:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Full Traceability&lt;/strong&gt;: Track from raw data â†’ items â†’ categories and back&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Progressive Summarization&lt;/strong&gt;: Each layer provides increasingly abstracted views&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Flexible Organization&lt;/strong&gt;: Categories evolve based on content patterns&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ¨ Multimodal Support&lt;/h2&gt; 
&lt;p&gt;MemU processes diverse content types into unified memory:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Modality&lt;/th&gt; 
   &lt;th&gt;Input&lt;/th&gt; 
   &lt;th&gt;Processing&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;conversation&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;JSON chat logs&lt;/td&gt; 
   &lt;td&gt;Extract preferences, opinions, habits, relationships&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;document&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Text files (.txt, .md)&lt;/td&gt; 
   &lt;td&gt;Extract knowledge, skills, facts&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;image&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;PNG, JPG, etc.&lt;/td&gt; 
   &lt;td&gt;Vision model extracts visual concepts and descriptions&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;video&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Video files&lt;/td&gt; 
   &lt;td&gt;Frame extraction + vision analysis&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;audio&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Audio files&lt;/td&gt; 
   &lt;td&gt;Transcription + text processing&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;All modalities are unified into the same three-layer hierarchy, enabling cross-modal retrieval.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸš€ Quick Start&lt;/h2&gt; 
&lt;h3&gt;Option 1: Cloud Version&lt;/h3&gt; 
&lt;p&gt;Try MemU instantly without any setup:&lt;/p&gt; 
&lt;p&gt;ğŸ‘‰ &lt;strong&gt;&lt;a href="https://memu.so"&gt;memu.so&lt;/a&gt;&lt;/strong&gt; - Hosted cloud service with full API access&lt;/p&gt; 
&lt;p&gt;For enterprise deployment and custom solutions, contact &lt;strong&gt;&lt;a href="mailto:info@nevamind.ai"&gt;info@nevamind.ai&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h4&gt;Cloud API (v3)&lt;/h4&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Base URL&lt;/th&gt; 
   &lt;th&gt;&lt;code&gt;https://api.memu.so&lt;/code&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Auth&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;Authorization: Bearer YOUR_API_KEY&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Method&lt;/th&gt; 
   &lt;th&gt;Endpoint&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;POST&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;/api/v3/memory/memorize&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Register a memorization task&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;GET&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;/api/v3/memory/memorize/status/{task_id}&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Get task status&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;POST&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;/api/v3/memory/categories&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;List memory categories&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;POST&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;/api/v3/memory/retrieve&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Retrieve memories (semantic search)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;ğŸ“š &lt;strong&gt;&lt;a href="https://memu.pro/docs#cloud-version"&gt;Full API Documentation&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Option 2: Self-Hosted&lt;/h3&gt; 
&lt;h4&gt;Installation&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Basic Example&lt;/h4&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Requirements&lt;/strong&gt;: Python 3.13+ and an OpenAI API key&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;Test with In-Memory Storage&lt;/strong&gt; (no database required):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export OPENAI_API_KEY=your_api_key
cd tests
python test_inmemory.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Test with PostgreSQL Storage&lt;/strong&gt; (requires pgvector):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Start PostgreSQL with pgvector
docker run -d \
  --name memu-postgres \
  -e POSTGRES_USER=postgres \
  -e POSTGRES_PASSWORD=postgres \
  -e POSTGRES_DB=memu \
  -p 5432:5432 \
  pgvector/pgvector:pg16

# Run the test
export OPENAI_API_KEY=your_api_key
cd tests
python test_postgres.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Both examples demonstrate the complete workflow:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Memorize&lt;/strong&gt;: Process a conversation file and extract structured memory&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Retrieve (RAG)&lt;/strong&gt;: Fast embedding-based search&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Retrieve (LLM)&lt;/strong&gt;: Deep semantic understanding search&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/NevaMind-AI/memU/main/tests/test_inmemory.py"&gt;&lt;code&gt;tests/test_inmemory.py&lt;/code&gt;&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/NevaMind-AI/memU/main/tests/test_postgres.py"&gt;&lt;code&gt;tests/test_postgres.py&lt;/code&gt;&lt;/a&gt; for the full source code.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Custom LLM and Embedding Providers&lt;/h3&gt; 
&lt;p&gt;MemU supports custom LLM and embedding providers beyond OpenAI. Configure them via &lt;code&gt;llm_profiles&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from memu import MemUService

service = MemUService(
    llm_profiles={
        # Default profile for LLM operations
        "default": {
            "base_url": "https://dashscope.aliyuncs.com/compatible-mode/v1",
            "api_key": "your_api_key",
            "chat_model": "qwen3-max",
            "client_backend": "sdk"  # "sdk" or "http"
        },
        # Separate profile for embeddings
        "embedding": {
            "base_url": "https://api.voyageai.com/v1",
            "api_key": "your_voyage_api_key",
            "embed_model": "voyage-3.5-lite"
        }
    },
    # ... other configuration
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h3&gt;OpenRouter Integration&lt;/h3&gt; 
&lt;p&gt;MemU supports &lt;a href="https://openrouter.ai"&gt;OpenRouter&lt;/a&gt; as a model provider, giving you access to multiple LLM providers through a single API.&lt;/p&gt; 
&lt;h4&gt;Configuration&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from memu import MemoryService

service = MemoryService(
    llm_profiles={
        "default": {
            "provider": "openrouter",
            "client_backend": "httpx",
            "base_url": "https://openrouter.ai",
            "api_key": "your_openrouter_api_key",
            "chat_model": "anthropic/claude-3.5-sonnet",  # Any OpenRouter model
            "embed_model": "openai/text-embedding-3-small",  # Embedding model
        },
    },
    database_config={
        "metadata_store": {"provider": "inmemory"},
    },
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Environment Variables&lt;/h4&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENROUTER_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Your OpenRouter API key from &lt;a href="https://openrouter.ai/keys"&gt;openrouter.ai/keys&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h4&gt;Supported Features&lt;/h4&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Feature&lt;/th&gt; 
   &lt;th&gt;Status&lt;/th&gt; 
   &lt;th&gt;Notes&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Chat Completions&lt;/td&gt; 
   &lt;td&gt;Supported&lt;/td&gt; 
   &lt;td&gt;Works with any OpenRouter chat model&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Embeddings&lt;/td&gt; 
   &lt;td&gt;Supported&lt;/td&gt; 
   &lt;td&gt;Use OpenAI embedding models via OpenRouter&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Vision&lt;/td&gt; 
   &lt;td&gt;Supported&lt;/td&gt; 
   &lt;td&gt;Use vision-capable models (e.g., &lt;code&gt;openai/gpt-4o&lt;/code&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h4&gt;Running OpenRouter Tests&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export OPENROUTER_API_KEY=your_api_key

# Full workflow test (memorize + retrieve)
python tests/test_openrouter.py

# Embedding-specific tests
python tests/test_openrouter_embedding.py

# Vision-specific tests
python tests/test_openrouter_vision.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/NevaMind-AI/memU/main/examples/example_4_openrouter_memory.py"&gt;&lt;code&gt;examples/example_4_openrouter_memory.py&lt;/code&gt;&lt;/a&gt; for a complete working example.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ“– Core APIs&lt;/h2&gt; 
&lt;h3&gt;&lt;code&gt;memorize()&lt;/code&gt; - Extract and Store Memory&lt;/h3&gt; 
&lt;p&gt;Processes input resources and extracts structured memory:&lt;/p&gt; 
&lt;img width="100%" alt="memorize" src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/memorize.png" /&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;result = await service.memorize(
    resource_url="path/to/file.json",  # File path or URL
    modality="conversation",            # conversation | document | image | video | audio
    user={"user_id": "123"}             # Optional: scope to a user
)

# Returns:
{
    "resource": {...},      # Stored resource metadata
    "items": [...],         # Extracted memory items
    "categories": [...]     # Updated category summaries
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;&lt;code&gt;retrieve()&lt;/code&gt; - Query Memory&lt;/h3&gt; 
&lt;p&gt;Retrieves relevant memory based on queries. MemU supports &lt;strong&gt;two retrieval strategies&lt;/strong&gt;:&lt;/p&gt; 
&lt;img width="100%" alt="retrieve" src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/retrieve.png" /&gt; 
&lt;h4&gt;RAG-based Retrieval (&lt;code&gt;method="rag"&lt;/code&gt;)&lt;/h4&gt; 
&lt;p&gt;Fast &lt;strong&gt;embedding vector search&lt;/strong&gt; using cosine similarity:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;âœ… &lt;strong&gt;Fast&lt;/strong&gt;: Pure vector computation&lt;/li&gt; 
 &lt;li&gt;âœ… &lt;strong&gt;Scalable&lt;/strong&gt;: Efficient for large memory stores&lt;/li&gt; 
 &lt;li&gt;âœ… &lt;strong&gt;Returns scores&lt;/strong&gt;: Each result includes similarity score&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;LLM-based Retrieval (&lt;code&gt;method="llm"&lt;/code&gt;)&lt;/h4&gt; 
&lt;p&gt;Deep &lt;strong&gt;semantic understanding&lt;/strong&gt; through direct LLM reasoning:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;âœ… &lt;strong&gt;Deep understanding&lt;/strong&gt;: LLM comprehends context and nuance&lt;/li&gt; 
 &lt;li&gt;âœ… &lt;strong&gt;Query rewriting&lt;/strong&gt;: Automatically refines query at each tier&lt;/li&gt; 
 &lt;li&gt;âœ… &lt;strong&gt;Adaptive&lt;/strong&gt;: Stops early when sufficient information is found&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Comparison&lt;/h4&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Aspect&lt;/th&gt; 
   &lt;th&gt;RAG&lt;/th&gt; 
   &lt;th&gt;LLM&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Speed&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;âš¡ Fast&lt;/td&gt; 
   &lt;td&gt;ğŸ¢ Slower&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Cost&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;ğŸ’° Low&lt;/td&gt; 
   &lt;td&gt;ğŸ’°ğŸ’° Higher&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Semantic depth&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Medium&lt;/td&gt; 
   &lt;td&gt;Deep&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Tier 2 scope&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;All items&lt;/td&gt; 
   &lt;td&gt;Only items in relevant categories&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Output&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;With similarity scores&lt;/td&gt; 
   &lt;td&gt;Ranked by LLM reasoning&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Both methods support:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Context-aware rewriting&lt;/strong&gt;: Resolves pronouns using conversation history&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Progressive search&lt;/strong&gt;: Categories â†’ Items â†’ Resources&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Sufficiency checking&lt;/strong&gt;: Stops when enough information is retrieved&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Usage&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;result = await service.retrieve(
    queries=[
        {"role": "user", "content": {"text": "What are their preferences?"}},
        {"role": "user", "content": {"text": "Tell me about work habits"}}
    ],
    where={"user_id": "123"}  # Optional: scope filter
)

# Returns:
{
    "categories": [...],     # Relevant categories (with scores for RAG)
    "items": [...],          # Relevant memory items
    "resources": [...],      # Related raw resources
    "next_step_query": "..." # Rewritten query for follow-up (if applicable)
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Scope Filtering&lt;/strong&gt;: Use &lt;code&gt;where&lt;/code&gt; to filter by user model fields:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;where={"user_id": "123"}&lt;/code&gt; - exact match&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;where={"agent_id__in": ["1", "2"]}&lt;/code&gt; - match any in list&lt;/li&gt; 
 &lt;li&gt;Omit &lt;code&gt;where&lt;/code&gt; to retrieve across all scopes&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ğŸ“š &lt;strong&gt;For complete API documentation&lt;/strong&gt;, see &lt;a href="https://raw.githubusercontent.com/NevaMind-AI/memU/main/docs/SERVICE_API.md"&gt;SERVICE_API.md&lt;/a&gt; - includes all methods, CRUD operations, pipeline configuration, and configuration types.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ’¡ Use Cases&lt;/h2&gt; 
&lt;h3&gt;Example 1: Conversation Memory&lt;/h3&gt; 
&lt;p&gt;Extract and organize memory from multi-turn conversations:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export OPENAI_API_KEY=your_api_key
python examples/example_1_conversation_memory.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;What it does:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Processes multiple conversation JSON files&lt;/li&gt; 
 &lt;li&gt;Extracts memory items (preferences, habits, opinions, relationships)&lt;/li&gt; 
 &lt;li&gt;Generates category markdown files (&lt;code&gt;preferences.md&lt;/code&gt;, &lt;code&gt;work_life.md&lt;/code&gt;, etc.)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Best for:&lt;/strong&gt; Personal AI assistants, customer support bots, social chatbots&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Example 2: Skill Extraction from Logs&lt;/h3&gt; 
&lt;p&gt;Extract skills and lessons learned from agent execution logs:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export OPENAI_API_KEY=your_api_key
python examples/example_2_skill_extraction.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;What it does:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Processes agent logs sequentially&lt;/li&gt; 
 &lt;li&gt;Extracts actions, outcomes, and lessons learned&lt;/li&gt; 
 &lt;li&gt;Demonstrates &lt;strong&gt;incremental learning&lt;/strong&gt; - memory evolves with each file&lt;/li&gt; 
 &lt;li&gt;Generates evolving skill guides (&lt;code&gt;log_1.md&lt;/code&gt; â†’ &lt;code&gt;log_2.md&lt;/code&gt; â†’ &lt;code&gt;skill.md&lt;/code&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Best for:&lt;/strong&gt; DevOps teams, agent self-improvement, knowledge management&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Example 3: Multimodal Memory&lt;/h3&gt; 
&lt;p&gt;Process diverse content types into unified memory:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export OPENAI_API_KEY=your_api_key
python examples/example_3_multimodal_memory.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;What it does:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Processes documents and images together&lt;/li&gt; 
 &lt;li&gt;Extracts memory from different content types&lt;/li&gt; 
 &lt;li&gt;Unifies into cross-modal categories (&lt;code&gt;technical_documentation&lt;/code&gt;, &lt;code&gt;visual_diagrams&lt;/code&gt;, etc.)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Best for:&lt;/strong&gt; Documentation systems, learning platforms, research tools&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ“Š Performance&lt;/h2&gt; 
&lt;p&gt;MemU achieves &lt;strong&gt;92.09% average accuracy&lt;/strong&gt; on the Locomo benchmark across all reasoning tasks.&lt;/p&gt; 
&lt;img width="100%" alt="benchmark" src="https://github.com/user-attachments/assets/6fec4884-94e5-4058-ad5c-baac3d7e76d9" /&gt; 
&lt;p&gt;View detailed experimental data: &lt;a href="https://github.com/NevaMind-AI/memU-experiment"&gt;memU-experiment&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ§© Ecosystem&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Repository&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Use Case&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;&lt;a href="https://github.com/NevaMind-AI/memU"&gt;memU&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Core algorithm engine&lt;/td&gt; 
   &lt;td&gt;Embed AI memory into your product&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;&lt;a href="https://github.com/NevaMind-AI/memU-server"&gt;memU-server&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Backend service with CRUD, user system, RBAC&lt;/td&gt; 
   &lt;td&gt;Self-host a memory backend&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;&lt;a href="https://github.com/NevaMind-AI/memU-ui"&gt;memU-ui&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Visual dashboard&lt;/td&gt; 
   &lt;td&gt;Ready-to-use memory console&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Quick Links:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸš€ &lt;a href="https://app.memu.so/quick-start"&gt;Try MemU Cloud&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸ“š &lt;a href="https://memu.pro/docs"&gt;API Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸ’¬ &lt;a href="https://discord.gg/memu"&gt;Discord Community&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ¤ Partners&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/TEN-framework/ten-framework"&gt;&lt;img src="https://avatars.githubusercontent.com/u/113095513?s=200&amp;amp;v=4" alt="Ten" height="40" style="margin: 10px;" /&gt;&lt;/a&gt; &lt;a href="https://openagents.org"&gt;&lt;img src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/partners/openagents.png" alt="OpenAgents" height="40" style="margin: 10px;" /&gt;&lt;/a&gt; &lt;a href="https://github.com/milvus-io/milvus"&gt;&lt;img src="https://miro.medium.com/v2/resize:fit:2400/1*-VEGyAgcIBD62XtZWavy8w.png" alt="Milvus" height="40" style="margin: 10px;" /&gt;&lt;/a&gt; &lt;a href="https://xroute.ai/"&gt;&lt;img src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/partners/xroute.png" alt="xRoute" height="40" style="margin: 10px;" /&gt;&lt;/a&gt; &lt;a href="https://jaaz.app/"&gt;&lt;img src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/partners/jazz.png" alt="Jazz" height="40" style="margin: 10px;" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Buddie-AI/Buddie"&gt;&lt;img src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/partners/buddie.png" alt="Buddie" height="40" style="margin: 10px;" /&gt;&lt;/a&gt; &lt;a href="https://github.com/bytebase/bytebase"&gt;&lt;img src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/partners/bytebase.png" alt="Bytebase" height="40" style="margin: 10px;" /&gt;&lt;/a&gt; &lt;a href="https://github.com/LazyAGI/LazyLLM"&gt;&lt;img src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/partners/LazyLLM.png" alt="LazyLLM" height="40" style="margin: 10px;" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ¤ How to Contribute&lt;/h2&gt; 
&lt;p&gt;We welcome contributions from the community! Whether you're fixing bugs, adding features, or improving documentation, your help is appreciated.&lt;/p&gt; 
&lt;h3&gt;Getting Started&lt;/h3&gt; 
&lt;p&gt;To start contributing to MemU, you'll need to set up your development environment:&lt;/p&gt; 
&lt;h4&gt;Prerequisites&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.13+&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/astral-sh/uv"&gt;uv&lt;/a&gt; (Python package manager)&lt;/li&gt; 
 &lt;li&gt;Git&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Setup Development Environment&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 1. Fork and clone the repository
git clone https://github.com/YOUR_USERNAME/memU.git
cd memU

# 2. Install development dependencies
make install
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The &lt;code&gt;make install&lt;/code&gt; command will:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Create a virtual environment using &lt;code&gt;uv&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Install all project dependencies&lt;/li&gt; 
 &lt;li&gt;Set up pre-commit hooks for code quality checks&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Running Quality Checks&lt;/h4&gt; 
&lt;p&gt;Before submitting your contribution, ensure your code passes all quality checks:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;make check
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The &lt;code&gt;make check&lt;/code&gt; command runs:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Lock file verification&lt;/strong&gt;: Ensures &lt;code&gt;pyproject.toml&lt;/code&gt; consistency&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Pre-commit hooks&lt;/strong&gt;: Lints code with Ruff, formats with Black&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Type checking&lt;/strong&gt;: Runs &lt;code&gt;mypy&lt;/code&gt; for static type analysis&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Dependency analysis&lt;/strong&gt;: Uses &lt;code&gt;deptry&lt;/code&gt; to find obsolete dependencies&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Contributing Guidelines&lt;/h3&gt; 
&lt;p&gt;For detailed contribution guidelines, code standards, and development practices, please see &lt;a href="https://raw.githubusercontent.com/NevaMind-AI/memU/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Quick tips:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Create a new branch for each feature or bug fix&lt;/li&gt; 
 &lt;li&gt;Write clear commit messages&lt;/li&gt; 
 &lt;li&gt;Add tests for new functionality&lt;/li&gt; 
 &lt;li&gt;Update documentation as needed&lt;/li&gt; 
 &lt;li&gt;Run &lt;code&gt;make check&lt;/code&gt; before pushing&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ“„ License&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/NevaMind-AI/memU/main/LICENSE.txt"&gt;Apache License 2.0&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸŒ Community&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;GitHub Issues&lt;/strong&gt;: &lt;a href="https://github.com/NevaMind-AI/memU/issues"&gt;Report bugs &amp;amp; request features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Discord&lt;/strong&gt;: &lt;a href="https://discord.com/invite/hQZntfGsbJ"&gt;Join the community&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;X (Twitter)&lt;/strong&gt;: &lt;a href="https://x.com/memU_ai"&gt;Follow @memU_ai&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Contact&lt;/strong&gt;: &lt;a href="mailto:info@nevamind.ai"&gt;info@nevamind.ai&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;â­ &lt;strong&gt;Star us on GitHub&lt;/strong&gt; to get notified about new releases!&lt;/p&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>icloud-photos-downloader/icloud_photos_downloader</title>
      <link>https://github.com/icloud-photos-downloader/icloud_photos_downloader</link>
      <description>&lt;p&gt;A command-line tool to download photos from iCloud&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;!!!! &lt;a href="https://github.com/icloud-photos-downloader/icloud_photos_downloader/issues/1305"&gt;Looking for MAINTAINER for this project&lt;/a&gt; !!!!&lt;/h1&gt; 
&lt;h1&gt;iCloud Photos Downloader &lt;a href="https://github.com/icloud-photos-downloader/icloud_photos_downloader/actions/workflows/quality-checks.yml"&gt;&lt;img src="https://github.com/icloud-photos-downloader/icloud_photos_downloader/workflows/Quality%20Checks/badge.svg?sanitize=true" alt="Quality Checks" /&gt;&lt;/a&gt; &lt;a href="https://github.com/icloud-photos-downloader/icloud_photos_downloader/actions/workflows/produce-artifacts.yml"&gt;&lt;img src="https://github.com/icloud-photos-downloader/icloud_photos_downloader/workflows/Produce%20Artifacts/badge.svg?sanitize=true" alt="Build and Package" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/icloud-photos-downloader/icloud_photos_downloader/master/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true" alt="MIT License" /&gt;&lt;/a&gt;&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;A command-line tool to download all your iCloud photos.&lt;/li&gt; 
 &lt;li&gt;Works on Linux, Windows, and macOS; laptop, desktop, and NAS&lt;/li&gt; 
 &lt;li&gt;Available as an executable for direct downloading and through package managers/ecosystems (&lt;a href="https://icloud-photos-downloader.github.io/icloud_photos_downloader/install.html#docker"&gt;Docker&lt;/a&gt;, &lt;a href="https://icloud-photos-downloader.github.io/icloud_photos_downloader/install.html#pypi"&gt;PyPI&lt;/a&gt;, &lt;a href="https://icloud-photos-downloader.github.io/icloud_photos_downloader/install.html#aur"&gt;AUR&lt;/a&gt;, &lt;a href="https://icloud-photos-downloader.github.io/icloud_photos_downloader/install.html#npm"&gt;npm&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;Developed and maintained by volunteers (we are always looking for &lt;a href="https://raw.githubusercontent.com/icloud-photos-downloader/icloud_photos_downloader/master/CONTRIBUTING.md"&gt;help&lt;/a&gt;).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See &lt;a href="https://icloud-photos-downloader.github.io/icloud_photos_downloader/"&gt;Documentation&lt;/a&gt; for more details. Also, check &lt;a href="https://github.com/icloud-photos-downloader/icloud_photos_downloader/issues"&gt;Issues&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;We aim to release new versions once a week (Friday), if there is something worth delivering.&lt;/p&gt; 
&lt;h2&gt;iCloud Prerequisites&lt;/h2&gt; 
&lt;p&gt;To make iCloud Photo Downloader work, ensure the iCloud account is configured with the following settings, otherwise Apple Servers will return an ACCESS_DENIED error:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Enable Access iCloud Data on the Web:&lt;/strong&gt; On your iPhone / iPad, enable &lt;code&gt;Settings &amp;gt; Apple ID &amp;gt; iCloud &amp;gt; Access iCloud Data on the Web&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Disable Advanced Data Protection:&lt;/strong&gt; On your iPhone /iPad disable &lt;code&gt;Settings &amp;gt; Apple ID &amp;gt; iCloud &amp;gt; Advanced Data Protection&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Install and Run&lt;/h2&gt; 
&lt;p&gt;There are three ways to run &lt;code&gt;icloudpd&lt;/code&gt;:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Download executable for your platform from the GitHub &lt;a href="https://github.com/icloud-photos-downloader/icloud_photos_downloader/releases/tag/v1.32.2"&gt;Release&lt;/a&gt; and run it&lt;/li&gt; 
 &lt;li&gt;Use package manager to install, update, and, in some cases, run (&lt;a href="https://icloud-photos-downloader.github.io/icloud_photos_downloader/install.html#docker"&gt;Docker&lt;/a&gt;, &lt;a href="https://icloud-photos-downloader.github.io/icloud_photos_downloader/install.html#pypi"&gt;PyPI&lt;/a&gt;, &lt;a href="https://icloud-photos-downloader.github.io/icloud_photos_downloader/install.html#aur"&gt;AUR&lt;/a&gt;, &lt;a href="https://icloud-photos-downloader.github.io/icloud_photos_downloader/install.html#npm"&gt;npm&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;Build and run from the source&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;See &lt;a href="https://icloud-photos-downloader.github.io/icloud_photos_downloader/install.html"&gt;Documentation&lt;/a&gt; for more details&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;!-- start features --&gt; 
&lt;ul&gt; 
 &lt;li&gt;Three modes of operation: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Copy&lt;/strong&gt; - download new photos from iCloud (default mode)&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Sync&lt;/strong&gt; - download new photos from iCloud and delete local files that were removed in iCloud (&lt;code&gt;--auto-delete&lt;/code&gt; option)&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Move&lt;/strong&gt; - download new photos from iCloud and delete photos in iCloud (&lt;code&gt;--keep-icloud-recent-days&lt;/code&gt; option)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Support for Live Photos (image and video as separate files) and RAW images (including RAW+JPEG)&lt;/li&gt; 
 &lt;li&gt;Automatic de-duplication of photos with the same name&lt;/li&gt; 
 &lt;li&gt;One time download and an option to monitor for iCloud changes continuously (&lt;code&gt;--watch-with-interval&lt;/code&gt; option)&lt;/li&gt; 
 &lt;li&gt;Optimizations for incremental runs (&lt;code&gt;--until-found&lt;/code&gt; and &lt;code&gt;--recent&lt;/code&gt; options)&lt;/li&gt; 
 &lt;li&gt;Photo metadata (EXIF) updates (&lt;code&gt;--set-exif-datetime&lt;/code&gt; option)&lt;/li&gt; 
 &lt;li&gt;... and many more (use &lt;code&gt;--help&lt;/code&gt; option to get full list)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- end features --&gt; 
&lt;h2&gt;Experimental Mode&lt;/h2&gt; 
&lt;p&gt;Some changes are added to the experimental mode before they graduate into the main package. &lt;a href="https://raw.githubusercontent.com/icloud-photos-downloader/icloud_photos_downloader/master/EXPERIMENTAL.md"&gt;Details&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;p&gt;To keep your iCloud photo collection synchronized to your local system:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;icloudpd --directory /data --username my@email.address --watch-with-interval 3600
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] It is &lt;code&gt;icloudpd&lt;/code&gt;, not &lt;code&gt;icloud&lt;/code&gt; executable&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] Synchronization logic can be adjusted with command-line parameters. Run &lt;code&gt;icloudpd --help&lt;/code&gt; to get full list.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;To independently create and authorize a session (and complete 2SA/2FA validation if needed) on your local system:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;icloudpd --username my@email.address --password my_password --auth-only
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] This feature can also be used to check and verify that the session is still authenticated.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Want to contribute to iCloud Photos Downloader? Awesome! Check out the &lt;a href="https://raw.githubusercontent.com/icloud-photos-downloader/icloud_photos_downloader/master/CONTRIBUTING.md"&gt;contributing guidelines&lt;/a&gt; to get involved.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>OpenBMB/ChatDev</title>
      <link>https://github.com/OpenBMB/ChatDev</link>
      <description>&lt;p&gt;ChatDev 2.0: Dev All through LLM-powered Multi-Agent Collaboration&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ChatDev 2.0 - DevAll&lt;/h1&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/frontend/public/media/logo.png" alt="DevAll Logo" width="500" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;strong&gt;A Zero-Code Multi-Agent Platform for Developing Everything&lt;/strong&gt; &lt;/p&gt; 
&lt;p align="center"&gt; ã€&lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/README.md"&gt;English&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/README-zh.md"&gt;ç®€ä½“ä¸­æ–‡&lt;/a&gt;ã€‘ &lt;/p&gt; 
&lt;p align="center"&gt; ã€ğŸ“š &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/#developers"&gt;Developers&lt;/a&gt; | ğŸ‘¥ &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/#primary-contributors"&gt;Contributors&lt;/a&gt;ï½œâ­ï¸ &lt;a href="https://github.com/OpenBMB/ChatDev/tree/chatdev1.0"&gt;ChatDev 1.0 (Legacy)&lt;/a&gt;ã€‘ &lt;/p&gt; 
&lt;h2&gt;ğŸ“– Overview&lt;/h2&gt; 
&lt;p&gt;ChatDev has evolved from a specialized software development multi-agent system into a comprehensive multi-agent orchestration platform.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenBMB/ChatDev/tree/main"&gt;&lt;strong&gt;ChatDev 2.0 (DevAll)&lt;/strong&gt;&lt;/a&gt; is a &lt;strong&gt;Zero-Code Multi-Agent Platform&lt;/strong&gt; for "Developing Everything". It empowers users to rapidly build and execute customized multi-agent systems through simple configuration. No coding is requiredâ€”users can define agents, workflows, and tasks to orchestrate complex scenarios such as data visualization, 3D generation, and deep research.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenBMB/ChatDev/tree/chatdev1.0"&gt;&lt;strong&gt;ChatDev 1.0 (Legacy)&lt;/strong&gt;&lt;/a&gt; operates as a &lt;strong&gt;Virtual Software Company&lt;/strong&gt;. It utilizes various intelligent agents (e.g., CEO, CTO, Programmer) participating in specialized functional seminars to automate the entire software development life cycleâ€”including designing, coding, testing, and documenting. It serves as the foundational paradigm for communicative agent collaboration.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ‰ News&lt;/h2&gt; 
&lt;p&gt;â€¢ &lt;strong&gt;Jan 07, 2026: ğŸš€ We are excited to announce the official release of ChatDev 2.0 (DevAll)!&lt;/strong&gt; This version introduces a zero-code multi-agent orchestration platform. The classic ChatDev (v1.x) has been moved to the &lt;a href="https://github.com/OpenBMB/ChatDev/tree/chatdev1.0"&gt;&lt;code&gt;chatdev1.0&lt;/code&gt;&lt;/a&gt; branch for maintenance. More details about ChatDev 2.0 can be found on &lt;a href="https://x.com/OpenBMB/status/2008916790399701335"&gt;our official post&lt;/a&gt;.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Old News&lt;/summary&gt; 
 &lt;p&gt;â€¢Sep 24, 2025: ğŸ‰ Our paper &lt;a href="https://arxiv.org/abs/2505.19591"&gt;Multi-Agent Collaboration via Evolving Orchestration&lt;/a&gt; has been accepted to NeurIPS 2025. The implementation is available in the &lt;code&gt;puppeteer&lt;/code&gt; branch of this repository.&lt;/p&gt; 
 &lt;p&gt;â€¢May 26, 2025: ğŸ‰ We propose a novel puppeteer-style paradigm for multi-agent collaboration among large language model based agents. By leveraging a learnable central orchestrator optimized with reinforcement learning, our method dynamically activates and sequences agents to construct efficient, context-aware reasoning paths. This approach not only improves reasoning quality but also reduces computational costs, enabling scalable and adaptable multi-agent cooperation in complex tasks. See our paper in &lt;a href="https://arxiv.org/abs/2505.19591"&gt;Multi-Agent Collaboration via Evolving Orchestration&lt;/a&gt;.&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/puppeteer.png" width="800" /&gt; &lt;/p&gt; 
 &lt;p&gt;â€¢June 25, 2024: ğŸ‰To foster development in LLM-powered multi-agent collaborationğŸ¤–ğŸ¤– and related fields, the ChatDev team has curated a collection of seminal papersğŸ“„ presented in a &lt;a href="https://github.com/OpenBMB/ChatDev/tree/main/MultiAgentEbook"&gt;open-source&lt;/a&gt; interactive e-bookğŸ“š format. Now you can explore the latest advancements on the &lt;a href="https://thinkwee.top/multiagent_ebook"&gt;Ebook Website&lt;/a&gt; and download the &lt;a href="https://github.com/OpenBMB/ChatDev/raw/main/MultiAgentEbook/papers.csv"&gt;paper list&lt;/a&gt;.&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/ebook.png" width="800" /&gt; &lt;/p&gt; 
 &lt;p&gt;â€¢June 12, 2024: We introduced Multi-Agent Collaboration Networks (MacNet) ğŸ‰, which utilize directed acyclic graphs to facilitate effective task-oriented collaboration among agents through linguistic interactions ğŸ¤–ğŸ¤–. MacNet supports co-operation across various topologies and among more than a thousand agents without exceeding context limits. More versatile and scalable, MacNet can be considered as a more advanced version of ChatDev's chain-shaped topology. Our preprint paper is available at &lt;a href="https://arxiv.org/abs/2406.07155"&gt;https://arxiv.org/abs/2406.07155&lt;/a&gt;. This technique has been incorporated into the &lt;a href="https://github.com/OpenBMB/ChatDev/tree/macnet"&gt;macnet&lt;/a&gt; branch, enhancing support for diverse organizational structures and offering richer solutions beyond software development (e.g., logical reasoning, data analysis, story generation, and more).&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/macnet.png" width="500" /&gt; &lt;/p&gt; 
 &lt;p&gt;â€¢ May 07, 2024, we introduced "Iterative Experience Refinement" (IER), a novel method where instructor and assistant agents enhance shortcut-oriented experiences to efficiently adapt to new tasks. This approach encompasses experience acquisition, utilization, propagation and elimination across a series of tasks and making the pricess shorter and efficient. Our preprint paper is available at &lt;a href="https://arxiv.org/abs/2405.04219"&gt;https://arxiv.org/abs/2405.04219&lt;/a&gt;, and this technique will soon be incorporated into ChatDev.&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/ier.png" width="220" /&gt; &lt;/p&gt; 
 &lt;p&gt;â€¢ January 25, 2024: We have integrated Experiential Co-Learning Module into ChatDev. Please see the &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/wiki.md#co-tracking"&gt;Experiential Co-Learning Guide&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;â€¢ December 28, 2023: We present Experiential Co-Learning, an innovative approach where instructor and assistant agents accumulate shortcut-oriented experiences to effectively solve new tasks, reducing repetitive errors and enhancing efficiency. Check out our preprint paper at &lt;a href="https://arxiv.org/abs/2312.17025"&gt;https://arxiv.org/abs/2312.17025&lt;/a&gt; and this technique will soon be integrated into ChatDev.&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/ecl.png" width="860" /&gt; &lt;/p&gt; â€¢ November 15, 2023: We launched ChatDev as a SaaS platform that enables software developers and innovative entrepreneurs to build software efficiently at a very low cost and remove the barrier to entry. Try it out at https://chatdev.modelbest.cn/. 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/saas.png" width="560" /&gt; &lt;/p&gt; 
 &lt;p&gt;â€¢ November 2, 2023: ChatDev is now supported with a new feature: incremental development, which allows agents to develop upon existing codes. Try &lt;code&gt;--config "incremental" --path "[source_code_directory_path]"&lt;/code&gt; to start it.&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/increment.png" width="700" /&gt; &lt;/p&gt; 
 &lt;p&gt;â€¢ October 26, 2023: ChatDev is now supported with Docker for safe execution (thanks to contribution from &lt;a href="https://github.com/ManindraDeMel"&gt;ManindraDeMel&lt;/a&gt;). Please see &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/wiki.md#docker-start"&gt;Docker Start Guide&lt;/a&gt;.&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/docker.png" width="400" /&gt; &lt;/p&gt; 
 &lt;p&gt;â€¢ September 25, 2023: The &lt;strong&gt;Git&lt;/strong&gt; mode is now available, enabling the programmer &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/visualizer/static/figures/programmer.png" height="20" /&gt; to utilize Git for version control. To enable this feature, simply set &lt;code&gt;"git_management"&lt;/code&gt; to &lt;code&gt;"True"&lt;/code&gt; in &lt;code&gt;ChatChainConfig.json&lt;/code&gt;. See &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/wiki.md#git-mode"&gt;guide&lt;/a&gt;.&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/github.png" width="600" /&gt; &lt;/p&gt; 
 &lt;p&gt;â€¢ September 20, 2023: The &lt;strong&gt;Human-Agent-Interaction&lt;/strong&gt; mode is now available! You can get involved with the ChatDev team by playing the role of reviewer &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/visualizer/static/figures/reviewer.png" height="20" /&gt; and making suggestions to the programmer &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/visualizer/static/figures/programmer.png" height="20" /&gt;; try &lt;code&gt;python3 run.py --task [description_of_your_idea] --config "Human"&lt;/code&gt;. See &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/wiki.md#human-agent-interaction"&gt;guide&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/WareHouse/Gomoku_HumanAgentInteraction_20230920135038"&gt;example&lt;/a&gt;.&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/Human_intro.png" width="600" /&gt; &lt;/p&gt; 
 &lt;p&gt;â€¢ September 1, 2023: The &lt;strong&gt;Art&lt;/strong&gt; mode is available now! You can activate the designer agent &lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/visualizer/static/figures/designer.png" height="20" /&gt; to generate images used in the software; try &lt;code&gt;python3 run.py --task [description_of_your_idea] --config "Art"&lt;/code&gt;. See &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/wiki.md#art"&gt;guide&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/WareHouse/gomokugameArtExample_THUNLP_20230831122822"&gt;example&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;â€¢ August 28, 2023: The system is publicly available.&lt;/p&gt; 
 &lt;p&gt;â€¢ August 17, 2023: The v1.0.0 version was ready for release.&lt;/p&gt; 
 &lt;p&gt;â€¢ July 30, 2023: Users can customize ChatChain, Phasea and Role settings. Additionally, both online Log mode and replay mode are now supported.&lt;/p&gt; 
 &lt;p&gt;â€¢ July 16, 2023: The &lt;a href="https://arxiv.org/abs/2307.07924"&gt;preprint paper&lt;/a&gt; associated with this project was published.&lt;/p&gt; 
 &lt;p&gt;â€¢ June 30, 2023: The initial version of the ChatDev repository was released.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;ğŸš€ Quick Start&lt;/h2&gt; 
&lt;h3&gt;ğŸ“‹ Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;OS&lt;/strong&gt;: macOS / Linux / WSL / Windows&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Python&lt;/strong&gt;: 3.12+&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Node.js&lt;/strong&gt;: 18+&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Package Manager&lt;/strong&gt;: &lt;a href="https://docs.astral.sh/uv/"&gt;uv&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ“¦ Installation&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Backend Dependencies&lt;/strong&gt; (Python managed by &lt;code&gt;uv&lt;/code&gt;):&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;uv sync
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Frontend Dependencies&lt;/strong&gt; (Vite + Vue 3):&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cd frontend &amp;amp;&amp;amp; npm install
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;âš¡ï¸ Run the Application&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Start Backend&lt;/strong&gt; :&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# Run from the project root
uv run python server_main.py --port 6400 --reload
&lt;/code&gt;&lt;/pre&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;Remove &lt;code&gt;--reload&lt;/code&gt; if output files (e.g., GameDev) trigger restarts, which interrupts tasks and loses progress.&lt;/p&gt; 
  &lt;/blockquote&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Start Frontend&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cd frontend
VITE_API_BASE_URL=http://localhost:6400 npm run dev
&lt;/code&gt;&lt;/pre&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;Then access the Web Console at &lt;strong&gt;&lt;a href="http://localhost:5173"&gt;http://localhost:5173&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; 
  &lt;/blockquote&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;&lt;strong&gt;ğŸ’¡ Tip&lt;/strong&gt;: If the frontend fails to connect to the backend, the default port &lt;code&gt;6400&lt;/code&gt; may already be occupied. Please switch both services to an available port, for example:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;Backend&lt;/strong&gt;: start with &lt;code&gt;--port 6401&lt;/code&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Frontend&lt;/strong&gt;: set &lt;code&gt;VITE_API_BASE_URL=http://localhost:6401&lt;/code&gt;&lt;/li&gt; 
   &lt;/ul&gt; 
  &lt;/blockquote&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;ğŸ”‘ Configuration&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Environment Variables&lt;/strong&gt;: Create a &lt;code&gt;.env&lt;/code&gt; file in the project root.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Model Keys&lt;/strong&gt;: Set &lt;code&gt;API_KEY&lt;/code&gt; and &lt;code&gt;BASE_URL&lt;/code&gt; in &lt;code&gt;.env&lt;/code&gt; for your LLM provider.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;YAML placeholders&lt;/strong&gt;: Use &lt;code&gt;${VAR}&lt;/code&gt;ï¼ˆe.g., &lt;code&gt;${API_KEY}&lt;/code&gt;ï¼‰in configuration files to reference these variables.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ’¡ How to Use&lt;/h2&gt; 
&lt;h3&gt;ğŸ–¥ï¸ Web Console&lt;/h3&gt; 
&lt;p&gt;The DevAll interface provides a seamless experience for both construction and execution&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Tutorial&lt;/strong&gt;: Comprehensive step-by-step guides and documentation integrated directly into the platform to help you get started quickly.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/tutorial-en.png" /&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Workflow&lt;/strong&gt;: A visual canvas to design your multi-agent systems. Configure node parameters, define context flows, and orchestrate complex agent interactions with drag-and-drop ease.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/workflow.gif" /&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Launch&lt;/strong&gt;: Initiate workflows, monitor real-time logs, inspect intermediate artifacts, and provide human-in-the-loop feedback.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/launch.gif" /&gt; 
&lt;h3&gt;ğŸ§° Python SDK&lt;/h3&gt; 
&lt;p&gt;For automation and batch processing, use our lightweight Python SDK to execute workflows programmatically and retrieve results directly.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from runtime.sdk import run_workflow

# Execute a workflow and get the final node message
result = run_workflow(
    yaml_file="yaml_instance/demo.yaml",
    task_prompt="Summarize the attached document in one sentence.",
    attachments=["/path/to/document.pdf"],
    variables={"API_KEY": "sk-xxxx"} # Override .env variables if needed
)

if result.final_message:
    print(f"Output: {result.final_message.text_content()}")
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;a id="developers"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;âš™ï¸ For Developers&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;For secondary development and extensions, please proceed with this section.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Extend DevAll with new nodes, providers, and tools. The project is organized into a modular structure:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Core Systems&lt;/strong&gt;: &lt;code&gt;server/&lt;/code&gt; hosts the FastAPI backend, while &lt;code&gt;runtime/&lt;/code&gt; manages agent abstraction and tool execution.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Orchestration&lt;/strong&gt;: &lt;code&gt;workflow/&lt;/code&gt; handles the multi-agent logic, driven by configurations in &lt;code&gt;entity/&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Frontend&lt;/strong&gt;: &lt;code&gt;frontend/&lt;/code&gt; contains the Vue 3 Web Console.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Extensibility&lt;/strong&gt;: &lt;code&gt;functions/&lt;/code&gt; is the place for custom Python tools.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Relevant reference documentation:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Getting Started&lt;/strong&gt;: &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/docs/user_guide/en/index.md"&gt;Start Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Core Modules&lt;/strong&gt;: &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/docs/user_guide/en/workflow_authoring.md"&gt;Workflow Authoring&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/docs/user_guide/en/modules/memory.md"&gt;Memory&lt;/a&gt;, and &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/docs/user_guide/en/modules/tooling/index.md"&gt;Tooling&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸŒŸ Featured Workflows&lt;/h2&gt; 
&lt;p&gt;We provide robust, out-of-the-box templates for common scenarios. All runnable workflow configs are located in &lt;code&gt;yaml_instance/&lt;/code&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Demos&lt;/strong&gt;: Files named &lt;code&gt;demo_*.yaml&lt;/code&gt; showcase specific features or modules.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Implementations&lt;/strong&gt;: Files named directly (e.g., &lt;code&gt;ChatDev_v1.yaml&lt;/code&gt;) are full in-house or recreated workflows. As follows:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ“‹ Workflow Collection&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Category&lt;/th&gt; 
   &lt;th align="left"&gt;Workflow&lt;/th&gt; 
   &lt;th align="left"&gt;Case&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;ğŸ“ˆ Data Visualization&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;data_visualization_basic.yaml&lt;/code&gt;&lt;br /&gt;&lt;code&gt;data_visualization_enhanced.yaml&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/cases/data_analysis/data_analysis.gif" width="100%" /&gt;&lt;br /&gt;Prompt: &lt;em&gt;"Create 4â€“6 high-quality PNG charts for my large real-estate transactions dataset."&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;ğŸ› ï¸ 3D Generation&lt;/strong&gt;&lt;br /&gt;&lt;em&gt;(Requires &lt;a href="https://www.blender.org/"&gt;Blender&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/ahujasid/blender-mcp"&gt;blender-mcp&lt;/a&gt;)&lt;/em&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;blender_3d_builder_simple.yaml&lt;/code&gt;&lt;br /&gt;&lt;code&gt;blender_3d_builder_hub.yaml&lt;/code&gt;&lt;br /&gt;&lt;code&gt;blender_scientific_illustration.yaml&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/cases/3d_generation/3d.gif" width="100%" /&gt;&lt;br /&gt;Prompt: &lt;em&gt;"Please build a Christmas tree."&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;ğŸ® Game Dev&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;GameDev_v1.yaml&lt;/code&gt;&lt;br /&gt;&lt;code&gt;ChatDev_v1.yaml&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/cases/game_development/game.gif" width="100%" /&gt;&lt;br /&gt;Prompt: &lt;em&gt;"Please help me design and develop a Tank Battle game."&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;ğŸ“š Deep Research&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;deep_research_v1.yaml&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/cases/deep_research/deep_research.gif" width="85%" /&gt;&lt;br /&gt;Prompt: &lt;em&gt;"Research about recent advances in the field of LLM-based agent RL"&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;ğŸ“ Teach Video&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;teach_video.yaml&lt;/code&gt; (Please run command &lt;code&gt;uv add manim&lt;/code&gt; before running this workflow)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/cases/video_generation/video.gif" width="140%" /&gt;&lt;br /&gt;Prompt: &lt;em&gt;"è®²ä¸€ä¸‹ä»€ä¹ˆæ˜¯å‡¸ä¼˜åŒ–"&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h3&gt;ğŸ’¡ Usage Guide&lt;/h3&gt; 
&lt;p&gt;For those implementations, you can use the &lt;strong&gt;Launch&lt;/strong&gt; tab to execute them.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Select&lt;/strong&gt;: Choose a workflow in the &lt;strong&gt;Launch&lt;/strong&gt; tab.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Upload&lt;/strong&gt;: Upload necessary files (e.g., &lt;code&gt;.csv&lt;/code&gt; for data analysis) if required.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Prompt&lt;/strong&gt;: Enter your request (e.g., &lt;em&gt;"Visualize the sales trends"&lt;/em&gt; or &lt;em&gt;"Design a snake game"&lt;/em&gt;).&lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ¤ Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions from the community! Whether you're fixing bugs, adding new workflow templates, or sharing high-quality cases/artifacts produced by DevAll, your help is much appreciated. Feel free to contribute by submitting &lt;strong&gt;Issues&lt;/strong&gt; or &lt;strong&gt;Pull Requests&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;By contributing to DevAll, you'll be recognized in our &lt;strong&gt;Contributors&lt;/strong&gt; list below. Check out our &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/#developers"&gt;Developer Guide&lt;/a&gt; to get started!&lt;/p&gt; 
&lt;h3&gt;ğŸ‘¥ Contributors&lt;/h3&gt; 
&lt;h4&gt;Primary Contributors&lt;/h4&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/NA-Wen"&gt;&lt;img src="https://github.com/NA-Wen.png?size=100" width="64px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;NA-Wen&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/zxrys"&gt;&lt;img src="https://github.com/zxrys.png?size=100" width="64px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;zxrys&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/swugi"&gt;&lt;img src="https://github.com/swugi.png?size=100" width="64px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;swugi&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/huatl98"&gt;&lt;img src="https://github.com/huatl98.png?size=100" width="64px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;huatl98&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h4&gt;Contributors&lt;/h4&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/shiowen"&gt;&lt;img src="https://github.com/shiowen.png?size=100" width="64px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;shiowen&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/kilo2127"&gt;&lt;img src="https://github.com/kilo2127.png?size=100" width="64px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;kilo2127&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/AckerlyLau"&gt;&lt;img src="https://github.com/AckerlyLau.png?size=100" width="64px;" alt="" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;AckerlyLau&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;ğŸ¤ Acknowledgments&lt;/h2&gt; 
&lt;p&gt;&lt;a href="http://nlp.csai.tsinghua.edu.cn/"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/thunlp.png" height="50pt" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp; &lt;a href="https://modelbest.cn/"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/modelbest.png" height="50pt" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp; &lt;a href="https://github.com/OpenBMB/AgentVerse/"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/agentverse.png" height="50pt" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp; &lt;a href="https://github.com/OpenBMB/RepoAgent"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/repoagent.png" height="50pt" /&gt;&lt;/a&gt; &lt;a href="https://app.commanddash.io/agent?github=https://github.com/OpenBMB/ChatDev"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/CommandDash.png" height="50pt" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/www.teachmaster.cn"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/teachmaster.png" height="50pt" /&gt;&lt;/a&gt; &lt;a href="https://github.com/OpenBMB/AppCopilot"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/ChatDev/main/assets/appcopilot.png" height="50pt" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;ğŸ” Citation&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;@article{chatdev,
    title = {ChatDev: Communicative Agents for Software Development},
    author = {Chen Qian and Wei Liu and Hongzhang Liu and Nuo Chen and Yufan Dang and Jiahao Li and Cheng Yang and Weize Chen and Yusheng Su and Xin Cong and Juyuan Xu and Dahai Li and Zhiyuan Liu and Maosong Sun},
    journal = {arXiv preprint arXiv:2307.07924},
    url = {https://arxiv.org/abs/2307.07924},
    year = {2023}
}

@article{colearning,
    title = {Experiential Co-Learning of Software-Developing Agents},
    author = {Chen Qian and Yufan Dang and Jiahao Li and Wei Liu and Zihao Xie and Yifei Wang and Weize Chen and Cheng Yang and Xin Cong and Xiaoyin Che and Zhiyuan Liu and Maosong Sun},
    journal = {arXiv preprint arXiv:2312.17025},
    url = {https://arxiv.org/abs/2312.17025},
    year = {2023}
}

@article{macnet,
    title={Scaling Large-Language-Model-based Multi-Agent Collaboration},
    author={Chen Qian and Zihao Xie and Yifei Wang and Wei Liu and Yufan Dang and Zhuoyun Du and Weize Chen and Cheng Yang and Zhiyuan Liu and Maosong Sun}
    journal={arXiv preprint arXiv:2406.07155},
    url = {https://arxiv.org/abs/2406.07155},
    year={2024}
}

@article{iagents,
    title={Autonomous Agents for Collaborative Task under Information Asymmetry},
    author={Wei Liu and Chenxi Wang and Yifei Wang and Zihao Xie and Rennai Qiu and Yufan Dnag and Zhuoyun Du and Weize Chen and Cheng Yang and Chen Qian},
    journal={arXiv preprint arXiv:2406.14928},
    url = {https://arxiv.org/abs/2406.14928},
    year={2024}
}

@article{puppeteer,
      title={Multi-Agent Collaboration via Evolving Orchestration}, 
      author={Yufan Dang and Chen Qian and Xueheng Luo and Jingru Fan and Zihao Xie and Ruijie Shi and Weize Chen and Cheng Yang and Xiaoyin Che and Ye Tian and Xuantang Xiong and Lei Han and Zhiyuan Liu and Maosong Sun},
      journal={arXiv preprint arXiv:2505.19591},
      url={https://arxiv.org/abs/2505.19591},
      year={2025}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ğŸ“¬ Contact&lt;/h2&gt; 
&lt;p&gt;If you have any questions, feedback, or would like to get in touch, please feel free to reach out to us via email at &lt;a href="mailto:qianc62@gmail.com"&gt;qianc62@gmail.com&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>anthropics/skills</title>
      <link>https://github.com/anthropics/skills</link>
      <description>&lt;p&gt;Public repository for Agent Skills&lt;/p&gt;&lt;hr&gt;&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; This repository contains Anthropic's implementation of skills for Claude. For information about the Agent Skills standard, see &lt;a href="http://agentskills.io"&gt;agentskills.io&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h1&gt;Skills&lt;/h1&gt; 
&lt;p&gt;Skills are folders of instructions, scripts, and resources that Claude loads dynamically to improve performance on specialized tasks. Skills teach Claude how to complete specific tasks in a repeatable way, whether that's creating documents with your company's brand guidelines, analyzing data using your organization's specific workflows, or automating personal tasks.&lt;/p&gt; 
&lt;p&gt;For more information, check out:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://support.claude.com/en/articles/12512176-what-are-skills"&gt;What are skills?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://support.claude.com/en/articles/12512180-using-skills-in-claude"&gt;Using skills in Claude&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://support.claude.com/en/articles/12512198-creating-custom-skills"&gt;How to create custom skills&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://anthropic.com/engineering/equipping-agents-for-the-real-world-with-agent-skills"&gt;Equipping agents for the real world with Agent Skills&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;About This Repository&lt;/h1&gt; 
&lt;p&gt;This repository contains skills that demonstrate what's possible with Claude's skills system. These skills range from creative applications (art, music, design) to technical tasks (testing web apps, MCP server generation) to enterprise workflows (communications, branding, etc.).&lt;/p&gt; 
&lt;p&gt;Each skill is self-contained in its own folder with a &lt;code&gt;SKILL.md&lt;/code&gt; file containing the instructions and metadata that Claude uses. Browse through these skills to get inspiration for your own skills or to understand different patterns and approaches.&lt;/p&gt; 
&lt;p&gt;Many skills in this repo are open source (Apache 2.0). We've also included the document creation &amp;amp; editing skills that power &lt;a href="https://www.anthropic.com/news/create-files"&gt;Claude's document capabilities&lt;/a&gt; under the hood in the &lt;a href="https://raw.githubusercontent.com/anthropics/skills/main/skills/docx"&gt;&lt;code&gt;skills/docx&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/anthropics/skills/main/skills/pdf"&gt;&lt;code&gt;skills/pdf&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/anthropics/skills/main/skills/pptx"&gt;&lt;code&gt;skills/pptx&lt;/code&gt;&lt;/a&gt;, and &lt;a href="https://raw.githubusercontent.com/anthropics/skills/main/skills/xlsx"&gt;&lt;code&gt;skills/xlsx&lt;/code&gt;&lt;/a&gt; subfolders. These are source-available, not open source, but we wanted to share these with developers as a reference for more complex skills that are actively used in a production AI application.&lt;/p&gt; 
&lt;h2&gt;Disclaimer&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;These skills are provided for demonstration and educational purposes only.&lt;/strong&gt; While some of these capabilities may be available in Claude, the implementations and behaviors you receive from Claude may differ from what is shown in these skills. These skills are meant to illustrate patterns and possibilities. Always test skills thoroughly in your own environment before relying on them for critical tasks.&lt;/p&gt; 
&lt;h1&gt;Skill Sets&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/anthropics/skills/main/skills"&gt;./skills&lt;/a&gt;: Skill examples for Creative &amp;amp; Design, Development &amp;amp; Technical, Enterprise &amp;amp; Communication, and Document Skills&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/anthropics/skills/main/spec"&gt;./spec&lt;/a&gt;: The Agent Skills specification&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/anthropics/skills/main/template"&gt;./template&lt;/a&gt;: Skill template&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Try in Claude Code, Claude.ai, and the API&lt;/h1&gt; 
&lt;h2&gt;Claude Code&lt;/h2&gt; 
&lt;p&gt;You can register this repository as a Claude Code Plugin marketplace by running the following command in Claude Code:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;/plugin marketplace add anthropics/skills
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then, to install a specific set of skills:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Select &lt;code&gt;Browse and install plugins&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Select &lt;code&gt;anthropic-agent-skills&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Select &lt;code&gt;document-skills&lt;/code&gt; or &lt;code&gt;example-skills&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Select &lt;code&gt;Install now&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Alternatively, directly install either Plugin via:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;/plugin install document-skills@anthropic-agent-skills
/plugin install example-skills@anthropic-agent-skills
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After installing the plugin, you can use the skill by just mentioning it. For instance, if you install the &lt;code&gt;document-skills&lt;/code&gt; plugin from the marketplace, you can ask Claude Code to do something like: "Use the PDF skill to extract the form fields from &lt;code&gt;path/to/some-file.pdf&lt;/code&gt;"&lt;/p&gt; 
&lt;h2&gt;Claude.ai&lt;/h2&gt; 
&lt;p&gt;These example skills are all already available to paid plans in Claude.ai.&lt;/p&gt; 
&lt;p&gt;To use any skill from this repository or upload custom skills, follow the instructions in &lt;a href="https://support.claude.com/en/articles/12512180-using-skills-in-claude#h_a4222fa77b"&gt;Using skills in Claude&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Claude API&lt;/h2&gt; 
&lt;p&gt;You can use Anthropic's pre-built skills, and upload custom skills, via the Claude API. See the &lt;a href="https://docs.claude.com/en/api/skills-guide#creating-a-skill"&gt;Skills API Quickstart&lt;/a&gt; for more.&lt;/p&gt; 
&lt;h1&gt;Creating a Basic Skill&lt;/h1&gt; 
&lt;p&gt;Skills are simple to create - just a folder with a &lt;code&gt;SKILL.md&lt;/code&gt; file containing YAML frontmatter and instructions. You can use the &lt;strong&gt;template-skill&lt;/strong&gt; in this repository as a starting point:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-markdown"&gt;---
name: my-skill-name
description: A clear description of what this skill does and when to use it
---

# My Skill Name

[Add your instructions here that Claude will follow when this skill is active]

## Examples
- Example usage 1
- Example usage 2

## Guidelines
- Guideline 1
- Guideline 2
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The frontmatter requires only two fields:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;name&lt;/code&gt; - A unique identifier for your skill (lowercase, hyphens for spaces)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;description&lt;/code&gt; - A complete description of what the skill does and when to use it&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The markdown content below contains the instructions, examples, and guidelines that Claude will follow. For more details, see &lt;a href="https://support.claude.com/en/articles/12512198-creating-custom-skills"&gt;How to create custom skills&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Partner Skills&lt;/h1&gt; 
&lt;p&gt;Skills are a great way to teach Claude how to get better at using specific pieces of software. As we see awesome example skills from partners, we may highlight some of them here:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Notion&lt;/strong&gt; - &lt;a href="https://www.notion.so/notiondevs/Notion-Skills-for-Claude-28da4445d27180c7af1df7d8615723d0"&gt;Notion Skills for Claude&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>hesreallyhim/awesome-claude-code</title>
      <link>https://github.com/hesreallyhim/awesome-claude-code</link>
      <description>&lt;p&gt;A curated list of awesome skills, hooks, slash-commands, agent orchestrators, applications, and plugins for Claude Code by Anthropic&lt;/p&gt;&lt;hr&gt;&lt;h3 align="center"&gt;Pick Your Style:&lt;/h3&gt; 
&lt;p align="center"&gt; &lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/"&gt;&lt;img src="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/assets/badge-style-awesome.svg?sanitize=true" alt="Awesome" height="28" style="border: 2px solid #cc3366; border-radius: 4px;" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/README_ALTERNATIVES/README_EXTRA.md"&gt;&lt;img src="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/assets/badge-style-extra.svg?sanitize=true" alt="Extra" height="28" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/README_ALTERNATIVES/README_CLASSIC.md"&gt;&lt;img src="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/assets/badge-style-classic.svg?sanitize=true" alt="Classic" height="28" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/README_ALTERNATIVES/README_FLAT_ALL_AZ.md"&gt;&lt;img src="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/assets/badge-style-flat.svg?sanitize=true" alt="Flat" height="28" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; 
 &lt;picture&gt; 
  &lt;img src="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/assets/awesome-claude-code-social-clawd-2.png" alt="Awesome Claude Code" width="600" /&gt; 
 &lt;/picture&gt; &lt;/p&gt; 
&lt;h1&gt;Awesome Claude Code&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://awesome.re"&gt;&lt;img src="https://awesome.re/badge.svg?sanitize=true" alt="Awesome" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;A curated list of slash-commands, CLAUDE.md files, CLI tools, and other resources for enhancing your &lt;a href="https://docs.anthropic.com/en/docs/claude-code"&gt;Claude Code&lt;/a&gt; workflow.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Claude Code is a CLI-based coding assistant from &lt;a href="https://www.anthropic.com/"&gt;Anthropic&lt;/a&gt; that you can access in your terminal or IDE. This list helps the community share knowledge and best practices.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/assets/repo-ticker-awesome.svg?sanitize=true" alt="Featured Claude Code Projects" width="100%" /&gt; 
&lt;/div&gt; 
&lt;h2&gt;Latest Additions&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/muratcankoylan/ralph-wiggum-marketer"&gt;Ralph Wiggum Marketer&lt;/a&gt; by &lt;a href="https://github.com/muratcankoylan"&gt;Muratcan Koylan&lt;/a&gt; - A Claude Code plugin that provides an autonomous AI copywriter, integrating the Ralph loop with customized knowledge bases for market research agents. The agents do the research, Ralph writes the copy, you stay in bed. Whether or not you practice Ralph-Driven Development (RDD), I think these projects are interesting and creative explorations of general agentic patterns.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ClaytonFarr/ralph-playbook"&gt;The Ralph Playbook&lt;/a&gt; by &lt;a href="https://github.com/ClaytonFarr"&gt;Clayton Farr&lt;/a&gt; - A remarkably detailed and comprehensive guide to the Ralph Wiggum technique, featuring well-written theoretical commentary paired with practical guidelines and advice.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/mikeyobrien/ralph-orchestrator"&gt;ralph-orchestrator&lt;/a&gt; by &lt;a href="https://github.com/mikeyobrien"&gt;mikeyobrien&lt;/a&gt; - Ralph Orchestrator implements the simple but effective "Ralph Wiggum" technique for autonomous task completion, continuously running an AI agent against a prompt file until the task is marked as complete or limits are reached. This implementation provides a robust, well-tested, and feature-complete orchestration system for AI-driven development. Also cited in the Anthropic Ralph plugin documentation.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/frankbria/ralph-claude-code"&gt;Ralph for Claude Code&lt;/a&gt; by &lt;a href="https://github.com/frankbria"&gt;Frank Bria&lt;/a&gt; - An autonomous AI development framework that enables Claude Code to work iteratively on projects until completion. Features intelligent exit detection, rate limiting, circuit breaker patterns, and comprehensive safety guardrails to prevent infinite loops and API overuse. Built with Bash, integrated with tmux for live monitoring, and includes 75+ comprehensive tests.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#agent-skills-"&gt;Agent Skills ğŸ¤–&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#general"&gt;General&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#workflows--knowledge-guides-"&gt;Workflows &amp;amp; Knowledge Guides ğŸ§ &lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#general-1"&gt;General&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#ralph-wiggum"&gt;Ralph Wiggum&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#tooling-"&gt;Tooling ğŸ§°&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#general-2"&gt;General&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#ide-integrations"&gt;IDE Integrations&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#usage-monitors"&gt;Usage Monitors&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#orchestrators"&gt;Orchestrators&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#status-lines-"&gt;Status Lines ğŸ“Š&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#general-3"&gt;General&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#hooks-"&gt;Hooks ğŸª&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#general-4"&gt;General&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#slash-commands-"&gt;Slash-Commands ğŸ”ª&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#general-5"&gt;General&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#version-control--git"&gt;Version Control &amp;amp; Git&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#code-analysis--testing"&gt;Code Analysis &amp;amp; Testing&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#context-loading--priming"&gt;Context Loading &amp;amp; Priming&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#documentation--changelogs"&gt;Documentation &amp;amp; Changelogs&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#ci--deployment"&gt;CI / Deployment&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#project--task-management"&gt;Project &amp;amp; Task Management&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#miscellaneous"&gt;Miscellaneous&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#claudemd-files-"&gt;CLAUDE.md Files ğŸ“‚&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#language-specific"&gt;Language-Specific&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#domain-specific"&gt;Domain-Specific&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#project-scaffolding--mcp"&gt;Project Scaffolding &amp;amp; MCP&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#alternative-clients-"&gt;Alternative Clients ğŸ“±&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#general-6"&gt;General&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#official-documentation-%EF%B8%8F"&gt;Official Documentation ğŸ›ï¸&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#general-7"&gt;General&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Agent Skills ğŸ¤–&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Agent skills are model-controlled configurations (files, scripts, resources, etc.) that enable Claude Code to perform specialized tasks requiring specific knowledge or capabilities.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;General&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/fcakyon/claude-codex-settings"&gt;Claude Codex Settings&lt;/a&gt; by &lt;a href="https://github.com/fcakyon"&gt;fatih akyon&lt;/a&gt; - A well-organized, well-written set of plugins covering core developer activities, such as working with common cloud platforms like GitHub, Azure, MongoDB, and popular services such as Tavily, Playwright, and more. Clear, not overly-opinionated, and compatible with a few other providers.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/dreamiurg/claude-mountaineering-skills"&gt;Claude Mountaineering Skills&lt;/a&gt; by &lt;a href="https://github.com/dreamiurg"&gt;Dmytro Gaivoronsky&lt;/a&gt; - Claude Code skill that automates mountain route research for North American peaks. Aggregates data from 10+ mountaineering sources like Mountaineers.org, PeakBagger.com and SummitPost.com to generate detailed route beta reports with weather, avalanche conditions, and trip reports.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/skills-directory/skill-codex"&gt;Codex Skill&lt;/a&gt; by &lt;a href="https://github.com/klaudworks"&gt;klaudworks&lt;/a&gt; - Enables users to prompt codex from claude code. Unlike the raw codex mcp server, this skill infers parameters such as model, reasoning effort, sandboxing from your prompt or asks you to specify them. It also simplifies continuing prior codex sessions so that codex can continue with the prior context.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/NeoLabHQ/context-engineering-kit"&gt;Context Engineering Kit&lt;/a&gt; by &lt;a href="https://github.com/LeoVS09"&gt;Vlad Goncharov&lt;/a&gt; - Hand-crafted collection of advanced context engineering techniques and patterns with minimal token footprint focused on improving agent result quality.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/obra/superpowers"&gt;Superpowers&lt;/a&gt; by &lt;a href="https://github.com/obra"&gt;Jesse Vincent&lt;/a&gt; - A strong bundle of core competencies for software engineering, with good coverage of a large portion of the SDLC - from planning, reviewing, testing, debugging... Well written, well organized, and adaptable. The author refers to them as "superpowers", but many of them are just consolidating engineering best practices - which sometimes does feel like a superpower when working with Claude Code.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/glittercowboy/taches-cc-resources"&gt;TÃ‚CHES Claude Code Resources&lt;/a&gt; by &lt;a href="https://github.com/glittercowboy"&gt;TÃ‚CHES&lt;/a&gt; - A well-balanced, "down-to-Earth" set of sub agents, skills, and commands, that are well-organized, easy to read, and a healthy focus on "meta"-skills/agents, like "skill-auditor", hook creation, etc. - the kind of things you can adapt to your workflow, and not the other way around.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/alonw0/web-asset-generator"&gt;Web Assets Generator Skill&lt;/a&gt; by &lt;a href="https://github.com/alonw0"&gt;Alon Wolenitz&lt;/a&gt; - Easily generate web assets from Claude Code including favicons, app icons (PWA), and social media meta images (Open Graph) for Facebook, Twitter, WhatsApp, and LinkedIn. Handles image resizing, text-to-image generation, emojis, and provides proper HTML meta tags.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;h2&gt;Workflows &amp;amp; Knowledge Guides ğŸ§ &lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;A workflow is a tightly coupled set of Claude Code-native resources that facilitate specific projects&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;General&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ayoubben18/ab-method"&gt;AB Method&lt;/a&gt; by &lt;a href="https://github.com/ayoubben18"&gt;Ayoub Bensalah&lt;/a&gt; - A principled, spec-driven workflow that transforms large problems into focused, incremental missions using Claude Code's specialized sub agents. Includes slash-commands, sub agents, and specialized workflows designed for specific parts of the SDLC.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ThibautMelen/agentic-workflow-patterns"&gt;Agentic Workflow Patterns&lt;/a&gt; by &lt;a href="https://github.com/ThibautMelen"&gt;ThibautMelen&lt;/a&gt; - A comprehensive and well-documented collection of agentic patterns from Anthropic docs, with colorful Mermaid diagrams and code examples for each pattern. Covers Subagent Orchestration, Progressive Skills, Parallel Tool Calling, Master-Clone Architecture, Wizard Workflows, and more. Also compatible with other providers.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/cloudartisan/cloudartisan.github.io/tree/main/.claude/commands"&gt;Blogging Platform Instructions&lt;/a&gt; by &lt;a href="https://github.com/cloudartisan"&gt;cloudartisan&lt;/a&gt; - Provides a well-structured set of commands for publishing and maintaining a blogging platform, including commands for creating posts, managing categories, and handling media files.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ericbuess/claude-code-docs"&gt;Claude Code Documentation Mirror&lt;/a&gt; by &lt;a href="https://github.com/ericbuess"&gt;Eric Buess&lt;/a&gt; - A mirror of the Anthropic Â© PBC documentation pages for Claude Code, updated every few hours. Can come in handy when trying to stay on top of the ever-expanding feature-set of Dr. Claw D. Code, Ph.D.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nikiforovall.blog/claude-code-rules/"&gt;Claude Code Handbook&lt;/a&gt; by &lt;a href="https://github.com/nikiforovall"&gt;nikiforovall&lt;/a&gt; - Collection of best practices, tips, and techniques for Claude Code development workflows, enhanced with distributable plugins.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/diet103/claude-code-infrastructure-showcase"&gt;Claude Code Infrastructure Showcase&lt;/a&gt; by &lt;a href="https://github.com/diet103"&gt;diet103&lt;/a&gt; - A remarkably innovative approach to working with Skills, the centerpiece of which being a technique that leverages hooks to ensure that Claude intelligently selects and activates the appropriate Skill given the current context. Well-documented and adaptable to different projects and workflows.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/automazeio/ccpm"&gt;Claude Code PM&lt;/a&gt; by &lt;a href="https://github.com/ranaroussi"&gt;Ran Aroussi&lt;/a&gt; - Really comprehensive and feature-packed project-management workflow for Claude Code. Numerous specialized agents, slash-commands, and strong documentation.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/danielrosehill/Claude-Code-Repos-Index"&gt;Claude Code Repos Index&lt;/a&gt; by &lt;a href="https://github.com/danielrosehill"&gt;Daniel Rosehill&lt;/a&gt; - This is either the work of a prolific genius, or a very clever bot (or both), although it hardly matters because the quality is so good - an index of 75+ Claude Code repositories published by the author - and I'm not talking about slop. CMS, system design, deep research, IoT, agentic workflows, server management, personal health... If you spot the lie, let me know, otherwise please check these out.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Piebald-AI/claude-code-system-prompts"&gt;Claude Code System Prompts&lt;/a&gt; by &lt;a href="https://github.com/Piebald-AI"&gt;Piebald AI&lt;/a&gt; - All parts of Claude Code's system prompt, including builtin tool descriptions, sub agent prompts (Plan/Explore/Task), utility prompts (CLAUDE.md, compact, Bash cmd, security review, agent creation, etc.). Updated for each Claude Code version.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ykdojo/claude-code-tips"&gt;Claude Code Tips&lt;/a&gt; by &lt;a href="https://github.com/ykdojo"&gt;ykdojo&lt;/a&gt; - A nice variety of 35+ brief but information-dense Claude Code tips covering voice input, system prompt patching, container workflows for risky tasks, conversation cloning(!), multi-model orchestration with Gemini CLI, and plenty more. Nice demos, working scripts, a plugin, I'd say this probably has a little something for everyone.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/maxritter/claude-codepro"&gt;Claude CodePro&lt;/a&gt; by &lt;a href="https://www.maxritter.net"&gt;Max Ritter&lt;/a&gt; - Professional development environment for Claude Code with spec-driven workflow, TDD enforcement, cross-session memory, semantic search, quality hooks, and modular rules integration. A bit "heavyweight" but feature-packed and has wide coverage.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/costiash/claude-code-docs"&gt;claude-code-docs&lt;/a&gt; by &lt;a href="https://github.com/costiash"&gt;Constantin Shafranski&lt;/a&gt; - A mirror of the AnthropicÂ© PBC documentation site for Claude/Code, but with bonus features like full-text search and query-time updates - a nice companion to &lt;code&gt;claude-code-docs&lt;/code&gt; for up-to-the-minute, fully-indexed information so that Claude Code can read about itself.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/JSONbored/claudepro-directory"&gt;ClaudoPro Directory&lt;/a&gt; by &lt;a href="https://github.com/JSONbored"&gt;ghost&lt;/a&gt; - Well-crafted, wide selection of Claude Code hooks, slash commands, subagent files, and more, covering a range of specialized tasks and workflows. Better resources than your average "Claude-template-for-everything" site.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/disler/just-prompt/tree/main/.claude/commands"&gt;Context Priming&lt;/a&gt; by &lt;a href="https://github.com/disler"&gt;disler&lt;/a&gt; - Provides a systematic approach to priming Claude Code with comprehensive project context through specialized commands for different project scenarios and development contexts.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OneRedOak/claude-code-workflows/tree/main/design-review"&gt;Design Review Workflow&lt;/a&gt; by &lt;a href="https://github.com/OneRedOak"&gt;Patrick Ellis&lt;/a&gt; - A tailored workflow for enabling automated UI/UX design review, including specialized sub agents, slash commands, &lt;code&gt;CLAUDE.md&lt;/code&gt; excerpts, and more. Covers a broad range of criteria from responsive design to accessibility.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/tott/laravel-tall-claude-ai-configs"&gt;Laravel TALL Stack AI Development Starter Kit&lt;/a&gt; by &lt;a href="https://github.com/tott"&gt;tott&lt;/a&gt; - Transform your Laravel TALL (Tailwind, AlpineJS, Laravel, Livewire) stack development with comprehensive Claude Code configurations that provide intelligent assistance, systematic workflows, and domain expert consultation.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/cheukyin175/learn-faster-kit"&gt;learn-faster-kit&lt;/a&gt; by &lt;a href="https://github.com/cheukyin175"&gt;Hugo Lau&lt;/a&gt; - A creative educational framework for Claude Code, inspired by the "FASTER" approach to self-teaching. Ships with a variety of agents, slash commands, and tools that enable Claude Code to help you progress at your own pace, employing well-established pedagogical techniques like active learning and spaced repetition.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/kingler/n8n_agent/tree/main/.claude/commands"&gt;n8n_agent&lt;/a&gt; by &lt;a href="https://github.com/kingler"&gt;kingler&lt;/a&gt; - Amazing comprehensive set of comments for code analysis, QA, design, documentation, project structure, project management, optimization, and many more.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/steadycursor/steadystart/tree/main/.claude/commands"&gt;Project Bootstrapping and Task Management&lt;/a&gt; by &lt;a href="https://github.com/steadycursor"&gt;steadycursor&lt;/a&gt; - Provides a structured set of commands for bootstrapping and managing a new project, including meta-commands for creating and editing custom slash-commands.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/scopecraft/command/tree/main/.claude/commands"&gt;Project Management, Implementation, Planning, and Release&lt;/a&gt; by &lt;a href="https://github.com/scopecraft"&gt;scopecraft&lt;/a&gt; - Really comprehensive set of commands for all aspects of SDLC.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/harperreed/dotfiles/tree/master/.claude/commands"&gt;Project Workflow System&lt;/a&gt; by &lt;a href="https://github.com/harperreed"&gt;harperreed&lt;/a&gt; - A set of commands that provide a comprehensive workflow system for managing projects, including task management, code review, and deployment processes.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/tony/claude-code-riper-5"&gt;RIPER Workflow&lt;/a&gt; by &lt;a href="https://tony.sh"&gt;Tony Narlock&lt;/a&gt; - Structured development workflow enforcing separation between Research, Innovate, Plan, Execute, and Review phases. Features consolidated subagents for context-efficiency, branch-aware memory bank, and strict mode enforcement for guided development.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://diwank.space/field-notes-from-shipping-real-code-with-claude"&gt;Shipping Real Code w/ Claude&lt;/a&gt; by &lt;a href="https://github.com/creatorrr"&gt;Diwank&lt;/a&gt; - A detailed blog post explaining the author's process for shipping a product with Claude Code, including CLAUDE.md files and other interesting resources.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Helmi/claude-simone"&gt;Simone&lt;/a&gt; by &lt;a href="https://github.com/Helmi"&gt;Helmi&lt;/a&gt; - A broader project management workflow for Claude Code that encompasses not just a set of commands, but a system of documents, guidelines, and processes to facilitate project planning and execution.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Ralph Wiggum&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/frankbria/ralph-claude-code"&gt;Ralph for Claude Code&lt;/a&gt; by &lt;a href="https://github.com/frankbria"&gt;Frank Bria&lt;/a&gt; - An autonomous AI development framework that enables Claude Code to work iteratively on projects until completion. Features intelligent exit detection, rate limiting, circuit breaker patterns, and comprehensive safety guardrails to prevent infinite loops and API overuse. Built with Bash, integrated with tmux for live monitoring, and includes 75+ comprehensive tests.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/muratcankoylan/ralph-wiggum-marketer"&gt;Ralph Wiggum Marketer&lt;/a&gt; by &lt;a href="https://github.com/muratcankoylan"&gt;Muratcan Koylan&lt;/a&gt; - A Claude Code plugin that provides an autonomous AI copywriter, integrating the Ralph loop with customized knowledge bases for market research agents. The agents do the research, Ralph writes the copy, you stay in bed. Whether or not you practice Ralph-Driven Development (RDD), I think these projects are interesting and creative explorations of general agentic patterns.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/mikeyobrien/ralph-orchestrator"&gt;ralph-orchestrator&lt;/a&gt; by &lt;a href="https://github.com/mikeyobrien"&gt;mikeyobrien&lt;/a&gt; - Ralph Orchestrator implements the simple but effective "Ralph Wiggum" technique for autonomous task completion, continuously running an AI agent against a prompt file until the task is marked as complete or limits are reached. This implementation provides a robust, well-tested, and feature-complete orchestration system for AI-driven development. Also cited in the Anthropic Ralph plugin documentation.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ClaytonFarr/ralph-playbook"&gt;The Ralph Playbook&lt;/a&gt; by &lt;a href="https://github.com/ClaytonFarr"&gt;Clayton Farr&lt;/a&gt; - A remarkably detailed and comprehensive guide to the Ralph Wiggum technique, featuring well-written theoretical commentary paired with practical guidelines and advice.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;h2&gt;Tooling ğŸ§°&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Tooling denotes applications that are built on top of Claude Code and consist of more components than slash-commands and &lt;code&gt;CLAUDE.md&lt;/code&gt; files&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;General&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/GWUDCAP/cc-sessions"&gt;cc-sessions&lt;/a&gt; by &lt;a href="https://github.com/satoastshi"&gt;toastdev&lt;/a&gt; - An opinionated approach to productive development with Claude Code.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Veraticus/cc-tools"&gt;cc-tools&lt;/a&gt; by &lt;a href="https://github.com/Veraticus"&gt;Josh Symonds&lt;/a&gt; - High-performance Go implementation of Claude Code hooks and utilities. Provides smart linting, testing, and statusline generation with minimal overhead.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/nyatinte/ccexp"&gt;ccexp&lt;/a&gt; by &lt;a href="https://github.com/nyatinte"&gt;nyatinte&lt;/a&gt; - Interactive CLI tool for discovering and managing Claude Code configuration files and slash commands with a beautiful terminal UI.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/eckardt/cchistory"&gt;cchistory&lt;/a&gt; by &lt;a href="https://github.com/eckardt"&gt;eckardt&lt;/a&gt; - Like the shell history command but for your Claude Code sessions. Easily list all Bash or "Bash-mode" (&lt;code&gt;!&lt;/code&gt;) commands Claude Code ran in a session for reference.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Brads3290/cclogviewer"&gt;cclogviewer&lt;/a&gt; by &lt;a href="https://github.com/Brads3290"&gt;Brad S.&lt;/a&gt; - A humble but handy utility for viewing Claude Code &lt;code&gt;.jsonl&lt;/code&gt; conversation files in a pretty HTML UI.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/davila7/claude-code-templates"&gt;Claude Code Templates&lt;/a&gt; by &lt;a href="https://github.com/davila7"&gt;Daniel Avila&lt;/a&gt; - Incredibly awesome collection of resources from every category in this list, presented with a neatly polished UI, great features like usage dashboard, analytics, and everything from slash commands to hooks to agents. An awesome companion for this awesome list.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/possibilities/claude-composer"&gt;Claude Composer&lt;/a&gt; by &lt;a href="https://github.com/possibilities"&gt;Mike Bannister&lt;/a&gt; - A tool that adds small enhancements to Claude Code.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/claude-did-this/claude-hub"&gt;Claude Hub&lt;/a&gt; by &lt;a href="https://github.com/claude-did-this"&gt;Claude Did This&lt;/a&gt; - A webhook service that connects Claude Code to GitHub repositories, enabling AI-powered code assistance directly through pull requests and issues. This integration allows Claude to analyze repositories, answer technical questions, and help developers understand and improve their codebase through simple @mentions.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pchalasani/claude-code-tools"&gt;claude-code-tools&lt;/a&gt; by &lt;a href="https://github.com/pchalasani"&gt;Prasad Chalasani&lt;/a&gt; - Well-crafted toolset for session continuity, featuring skills/commands to avoid compaction and recover context across sessions with cross-agent handoff between Claude Code and Codex CLI. Includes a fast Rust/Tantivy-powered full-text session search (TUI for humans, skill/CLI for agents), tmux-cli skill + command for interacting with scripts and CLI agents, and safety hooks to block dangerous commands.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/serpro69/claude-starter-kit"&gt;claude-starter-kit&lt;/a&gt; by &lt;a href="https://github.com/serpro69"&gt;serpro69&lt;/a&gt; - This is a starter template repository designed to provide a complete development environment for Claude-Code with pre-configured MCP servers and tools for AI-powered development workflows. The repository is intentionally minimal, containing only configuration templates for three primary systems: Claude Code, Serena, and Task Master.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/carlrannaberg/claudekit"&gt;claudekit&lt;/a&gt; by &lt;a href="https://github.com/carlrannaberg"&gt;Carl Rannaberg&lt;/a&gt; - Impressive CLI toolkit providing auto-save checkpointing, code quality hooks, specification generation and execution, and 20+ specialized subagents including oracle (gpt-5), code-reviewer (6-aspect deep analysis), ai-sdk-expert (Vercel AI SDK), typescript-expert and many more for Claude Code workflows.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/dagger/container-use"&gt;Container Use&lt;/a&gt; by &lt;a href="https://github.com/dagger"&gt;dagger&lt;/a&gt; - Development environments for coding agents. Enable multiple agents to work safely and independently with your preferred stack.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/FlineDev/ContextKit"&gt;ContextKit&lt;/a&gt; by &lt;a href="https://github.com/Jeehut"&gt;Cihat GÃ¼ndÃ¼z&lt;/a&gt; - A systematic development framework that transforms Claude Code into a proactive development partner. Features 4-phase planning methodology, specialized quality agents, and structured workflows that help AI produce production-ready code on first try.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/zippoxer/recall"&gt;recall&lt;/a&gt; by &lt;a href="https://github.com/zippoxer"&gt;zippoxer&lt;/a&gt; - Full-text search your Claude Code sessions. Run &lt;code&gt;recall&lt;/code&gt; in terminal, type to search, Enter to resume. Alternative to &lt;code&gt;claude --resume&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/dyoshikawa/rulesync"&gt;Rulesync&lt;/a&gt; by &lt;a href="https://github.com/dyoshikawa"&gt;dyoshikawa&lt;/a&gt; - A Node.js CLI tool that automatically generates configs (rules, ignore files, MCP servers, commands, and subagents) for various AI coding agents. Rulesync can convert configs between Claude Code and other AI agents in both directions.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/icanhasjonas/run-claude-docker"&gt;run-claude-docker&lt;/a&gt; by &lt;a href="https://github.com/icanhasjonas/"&gt;Jonas&lt;/a&gt; - A self-contained Docker runner that forwards your current workspace into a safe(r) isolated docker container, where you still have access to your Claude Code settings, authentication, ssh agent, pgp, optionally aws keys etc.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/marcindulak/stt-mcp-server-linux"&gt;stt-mcp-server-linux&lt;/a&gt; by &lt;a href="https://github.com/marcindulak"&gt;marcindulak&lt;/a&gt; - A push-to-talk speech transcription setup for Linux using a Python MCP server. Runs locally in Docker with no external API calls. Your speech is recorded, transcribed into text, and then sent to Claude running in a Tmux session.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/SuperClaude-Org/SuperClaude_Framework"&gt;SuperClaude&lt;/a&gt; by &lt;a href="https://github.com/SuperClaude-Org"&gt;SuperClaude-Org&lt;/a&gt; - A versatile configuration framework that enhances Claude Code with specialized commands, cognitive personas, and development methodologies, such as "Introspection" and "Orchestration".&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Piebald-AI/tweakcc"&gt;tweakcc&lt;/a&gt; by &lt;a href="https://github.com/Piebald-AI"&gt;Piebald-AI&lt;/a&gt; - Command-line tool to customize your Claude Code styling.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/vibe-log/vibe-log-cli"&gt;Vibe-Log&lt;/a&gt; by &lt;a href="https://github.com/vibe-log"&gt;Vibe-Log&lt;/a&gt; - Analyzes your Claude Code prompts locally (using CC), provides intelligent session analysis and actionable strategic guidance - works in the statusline and produces very pretty HTML reports as well. Easy to install and remove.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OverseedAI/viwo"&gt;viwo-cli&lt;/a&gt; by &lt;a href="https://github.com/hal-shin"&gt;Hal Shin&lt;/a&gt; - Run Claude Code in a Docker container with git worktrees as volume mounts to enable safer usage of &lt;code&gt;--dangerously-skip-permissions&lt;/code&gt; for frictionless one-shotting prompts. Allows users to spin up multiple instances of Claude Code in the background easily with reduced permission fatigue.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/mbailey/voicemode"&gt;VoiceMode MCP&lt;/a&gt; by &lt;a href="https://github.com/mbailey"&gt;Mike Bailey&lt;/a&gt; - VoiceMode MCP brings natural conversations to Claude Code. It supports any OpenAI API compatible voice services and installs free and open source voice services (Whisper.cpp and Kokoro-FastAPI).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;IDE Integrations&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://marketplace.visualstudio.com/items?itemName=AndrePimenta.claude-code-chat"&gt;Claude Code Chat&lt;/a&gt; by &lt;a href="https://github.com/andrepimenta"&gt;andrepimenta&lt;/a&gt; - An elegant and user-friendly Claude Code chat interface for VS Code.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/manzaltu/claude-code-ide.el"&gt;claude-code-ide.el&lt;/a&gt; by &lt;a href="https://github.com/manzaltu"&gt;manzaltu&lt;/a&gt; - claude-code-ide.el integrates Claude Code with Emacs, like Anthropicâ€™s VS Code/IntelliJ extensions. It shows ediff-based code suggestions, pulls LSP/flymake/flycheck diagnostics, and tracks buffer context. It adds an extensible MCP tool support for symbol refs/defs, project metadata, and tree-sitter AST queries.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/stevemolitor/claude-code.el"&gt;claude-code.el&lt;/a&gt; by &lt;a href="https://github.com/stevemolitor"&gt;stevemolitor&lt;/a&gt; - An Emacs interface for Claude Code CLI.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/greggh/claude-code.nvim"&gt;claude-code.nvim&lt;/a&gt; by &lt;a href="https://github.com/greggh"&gt;greggh&lt;/a&gt; - A seamless integration between Claude Code AI assistant and Neovim.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Haleclipse/Claudix"&gt;Claudix - Claude Code for VSCode&lt;/a&gt; by &lt;a href="https://github.com/Haleclipse"&gt;Haleclipse&lt;/a&gt; - A VSCode extension that brings Claude Code directly into your editor with interactive chat interface, session management, intelligent file operations, terminal execution, and real-time streaming responses. Built with Vue 3, TypeScript.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/stravu/crystal"&gt;crystal&lt;/a&gt; by &lt;a href="https://github.com/stravu"&gt;stravu&lt;/a&gt; - A full-fledged desktop application for orchestrating, monitoring, and interacting with Claude Code agents.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Usage Monitors&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ryoppippi/ccusage"&gt;CC Usage&lt;/a&gt; by &lt;a href="https://github.com/ryoppippi"&gt;ryoppippi&lt;/a&gt; - Handy CLI tool for managing and analyzing Claude Code usage, based on analyzing local Claude Code logs. Presents a nice dashboard regarding cost information, token consumption, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/snipeship/ccflare"&gt;ccflare&lt;/a&gt; by &lt;a href="https://github.com/snipeship"&gt;snipeship&lt;/a&gt; - Claude Code usage dashboard with a web-UI that would put Tableau to shame. Thoroughly comprehensive metrics, frictionless setup, detailed logging, really really nice UI.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/tombii/better-ccflare/"&gt;ccflare -&amp;gt; &lt;strong&gt;better-ccflare&lt;/strong&gt;&lt;/a&gt; by &lt;a href="https://github.com/tombii"&gt;tombii&lt;/a&gt; - A well-maintained and feature-enhanced fork of the glorious &lt;code&gt;ccflare&lt;/code&gt; usage dashboard by @snipeship (which at the time of writing has not had an update in a few months). &lt;code&gt;better-ccflare&lt;/code&gt; builds on this foundation with some performance enhancements, extended provider support, bug fixes, Docker deployment, and more.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Maciek-roboblog/Claude-Code-Usage-Monitor"&gt;Claude Code Usage Monitor&lt;/a&gt; by &lt;a href="https://github.com/Maciek-roboblog"&gt;Maciek-roboblog&lt;/a&gt; - A real-time terminal-based tool for monitoring Claude Code token usage. It shows live token consumption, burn rate, and predictions for token depletion. Features include visual progress bars, session-aware analytics, and support for multiple subscription plans.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/kunwar-shah/claudex"&gt;Claudex&lt;/a&gt; by &lt;a href="https://github.com/kunwar-shah"&gt;Kunwar Shah&lt;/a&gt; - Claudex - A web-based browser for exploring your Claude Code conversation history across projects. Indexes your codebase for full-text search. Nice, easy-to-navigate UI. Simple dashboard interface for high-level analytics, and multiple export options as well. (And completely local w/ no telemetry!).&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/sculptdotfun/viberank"&gt;viberank&lt;/a&gt; by &lt;a href="https://github.com/nikshepsvn"&gt;nikshepsvn&lt;/a&gt; - A community-driven leaderboard tool that enables developers to visualize, track, and compete based on their Claude Code usage statistics. It features robust data analytics, GitHub OAuth, data validation, and user-friendly CLI/web submission methods.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Orchestrators&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ruvnet/claude-code-flow"&gt;Claude Code Flow&lt;/a&gt; by &lt;a href="https://github.com/ruvnet"&gt;ruvnet&lt;/a&gt; - This mode serves as a code-first orchestration layer, enabling Claude to write, edit, test, and optimize code autonomously across recursive agent cycles.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/smtg-ai/claude-squad"&gt;Claude Squad&lt;/a&gt; by &lt;a href="https://github.com/smtg-ai"&gt;smtg-ai&lt;/a&gt; - Claude Squad is a terminal app that manages multiple Claude Code, Codex (and other local agents including Aider) in separate workspaces, allowing you to work on multiple tasks simultaneously.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/parruda/claude-swarm"&gt;Claude Swarm&lt;/a&gt; by &lt;a href="https://github.com/parruda"&gt;parruda&lt;/a&gt; - Launch Claude Code session that is connected to a swarm of Claude Code Agents.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/eyaltoledano/claude-task-master"&gt;Claude Task Master&lt;/a&gt; by &lt;a href="https://github.com/eyaltoledano"&gt;eyaltoledano&lt;/a&gt; - A task management system for AI-driven development with Claude, designed to work seamlessly with Cursor AI.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/grahama1970/claude-task-runner"&gt;Claude Task Runner&lt;/a&gt; by &lt;a href="https://github.com/grahama1970"&gt;grahama1970&lt;/a&gt; - A specialized tool to manage context isolation and focused task execution with Claude Code, solving the critical challenge of context length limitations and task focus when working with Claude on complex, multi-step projects.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/slopus/happy"&gt;Happy Coder&lt;/a&gt; by &lt;a href="https://peoplesgrocers.com/en/projects"&gt;GrocerPublishAgent&lt;/a&gt; - Spawn and control multiple Claude Codes in parallel from your phone or desktop. Happy Coder runs Claude Code on your hardware, sends push notifications when Claude needs more input or permission, and costs nothing.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/rsmdt/the-startup"&gt;The Agentic Startup&lt;/a&gt; by &lt;a href="https://github.com/rsmdt"&gt;Rudolf Schmidt&lt;/a&gt; - Yet Another Claude Orchestrator - a collection of agents, commands, etc., for shipping production code - but I like this because it's comprehensive, well-written, and one of the few resources that actually uses Output Styles! +10 points!&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/dtormoen/tsk"&gt;TSK - AI Agent Task Manager and Sandbox&lt;/a&gt; by &lt;a href="https://github.com/dtormoen"&gt;dtormoen&lt;/a&gt; - A Rust CLI tool that lets you delegate development tasks to AI agents running in sandboxed Docker environments. Multiple agents work in parallel, returning git branches for human review.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;h2&gt;Status Lines ğŸ“Š&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Status lines - Configurations and customizations for Claude Code's status bar functionality&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;General&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Haleclipse/CCometixLine"&gt;CCometixLine - Claude Code Statusline&lt;/a&gt; by &lt;a href="https://github.com/Haleclipse"&gt;Haleclipse&lt;/a&gt; - A high-performance Claude Code statusline tool written in Rust with Git integration, usage tracking, interactive TUI configuration, and Claude Code enhancement utilities.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/sirmalloc/ccstatusline"&gt;ccstatusline&lt;/a&gt; by &lt;a href="https://github.com/sirmalloc"&gt;sirmalloc&lt;/a&gt; - A highly customizable status line formatter for Claude Code CLI that displays model info, git branch, token usage, and other metrics in your terminal.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/rz1989s/claude-code-statusline"&gt;claude-code-statusline&lt;/a&gt; by &lt;a href="https://github.com/rz1989s"&gt;rz1989s&lt;/a&gt; - Enhanced 4-line statusline for Claude Code with themes, cost tracking, and MCP server monitoring.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Owloops/claude-powerline"&gt;claude-powerline&lt;/a&gt; by &lt;a href="https://github.com/Owloops"&gt;Owloops&lt;/a&gt; - A vim-style powerline statusline for Claude Code with real-time usage tracking, git integration, custom themes, and more.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/hagan/claudia-statusline"&gt;claudia-statusline&lt;/a&gt; by &lt;a href="https://github.com/hagan"&gt;Hagan Franks&lt;/a&gt; - High-performance Rust-based statusline for Claude Code with persistent stats tracking, progress bars, and optional cloud sync. Features SQLite-first persistence, git integration, context progress bars, burn rate calculation, XDG-compliant with theme support (dark/light, NO_COLOR).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;h2&gt;Hooks ğŸª&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Hooks are a powerful API for Claude Code that allows users to activate commands and run scripts at different points in Claude's agentic lifecycle.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;General&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Talieisin/britfix"&gt;Britfix&lt;/a&gt; by &lt;a href="https://github.com/Talieisin"&gt;Talieisin&lt;/a&gt; - Claude outputs American spellings by default, which can have an impact on: professional credibility, compliance, documentation, and more. Britfix converts to British English, with a Claude Code hook for automatic conversion as files are written. Context-aware: handles code files intelligently by only converting comments and docstrings, never identifiers or string literals.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/dazuiba/CCNotify"&gt;CC Notify&lt;/a&gt; by &lt;a href="https://github.com/dazuiba"&gt;dazuiba&lt;/a&gt; - CCNotify provides desktop notifications for Claude Code, alerting you to input needs or task completion, with one-click jumps back to VS Code and task duration display.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/GowayLee/cchooks"&gt;cchooks&lt;/a&gt; by &lt;a href="https://github.com/GowayLee"&gt;GowayLee&lt;/a&gt; - A lightweight Python SDK with a clean API and good documentation; simplifies the process of writing hooks and integrating them into your codebase, providing a nice abstraction over the JSON configuration files.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aannoo/claude-hook-comms"&gt;Claude Code Hook Comms (HCOM)&lt;/a&gt; by &lt;a href="https://github.com/aannoo"&gt;aannoo&lt;/a&gt; - Lightweight CLI tool for real-time communication between Claude Code sub agents using hooks. Enables multi-agent collaboration with @-mention targeting, live dashboard monitoring, and zero-dependency implementation. [NOTE: At the time of posting, this resource is a little unstable - I'm sharing it anyway, because I think it's incredibly promising and creative. I hope by the time you read this, it is production-ready.].&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/beyondcode/claude-hooks-sdk"&gt;claude-code-hooks-sdk&lt;/a&gt; by &lt;a href="https://github.com/beyondcode"&gt;beyondcode&lt;/a&gt; - A Laravel-inspired PHP SDK for building Claude Code hook responses with a clean, fluent API. This SDK makes it easy to create structured JSON responses for Claude Code hooks using an expressive, chainable interface.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/johnlindquist/claude-hooks"&gt;claude-hooks&lt;/a&gt; by &lt;a href="https://github.com/johnlindquist"&gt;John Lindquist&lt;/a&gt; - A TypeScript-based system for configuring and customizing Claude Code hooks with a powerful and flexible interface.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ctoth/claudio"&gt;Claudio&lt;/a&gt; by &lt;a href="https://github.com/ctoth"&gt;Christopher Toth&lt;/a&gt; - A no-frills little library that adds delightful OS-native sounds to Claude Code via simple hooks. It really sparks joy.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/nizos/tdd-guard"&gt;TDD Guard&lt;/a&gt; by &lt;a href="https://github.com/nizos"&gt;Nizar Selander&lt;/a&gt; - A hooks-driven system that monitors file operations in real-time and blocks changes that violate TDD principles.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/bartolli/claude-code-typescript-hooks"&gt;TypeScript Quality Hooks&lt;/a&gt; by &lt;a href="https://github.com/bartolli"&gt;bartolli&lt;/a&gt; - Quality check hook for Node.js TypeScript projects with TypeScript compilation. ESLint auto-fixing, and Prettier formatting. Uses SHA256 config caching for &amp;lt; 5ms validation performance during real-time editing.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;h2&gt;Slash-Commands ğŸ”ª&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;"Slash Commands are customized, carefully refined prompts that control Claude's behavior in order to perform a specific task"&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;General&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/omril321/automated-notebooklm/raw/main/.claude/commands/create-hook.md"&gt;/create-hook&lt;/a&gt; by &lt;a href="https://github.com/omril321"&gt;Omri Lavi&lt;/a&gt; - Slash command for hook creation - intelligently prompts you through the creation process with smart suggestions based on your project setup (TS, Prettier, ESLint...).&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/danielrosehill/Claude-Code-Linux-Desktop-Slash-Commands"&gt;/linux-desktop-slash-commands&lt;/a&gt; by &lt;a href="https://github.com/danielrosehill"&gt;Daniel Rosehill&lt;/a&gt; - A library of slash commands intended specifically to facilitate common and advanced operations on Linux desktop environments (although many would also be useful on Linux servers). Command groups include hardware benchmarking, filesystem organisation, and security posture validation.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Version Control &amp;amp; Git&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/jerseycheese/Narraitor/raw/feature/issue-227-ai-suggestions/.claude/commands/analyze-issue.md"&gt;/analyze-issue&lt;/a&gt; by &lt;a href="https://github.com/jerseycheese"&gt;jerseycheese&lt;/a&gt; - Fetches GitHub issue details to create comprehensive implementation specifications, analyzing requirements and planning structured approach with clear implementation steps.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/evmts/tevm-monorepo/raw/main/.claude/commands/commit.md"&gt;/commit&lt;/a&gt; by &lt;a href="https://github.com/evmts"&gt;evmts&lt;/a&gt; - Creates git commits using conventional commit format with appropriate emojis, following project standards and creating descriptive messages that explain the purpose of changes.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/steadycursor/steadystart/raw/main/.claude/commands/2-commit-fast.md"&gt;/commit-fast&lt;/a&gt; by &lt;a href="https://github.com/steadycursor"&gt;steadycursor&lt;/a&gt; - Automates git commit process by selecting the first suggested message, generating structured commits with consistent formatting while skipping manual confirmation and removing Claude co-Contributorship footer.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/toyamarinyon/giselle/raw/main/.claude/commands/create-pr.md"&gt;/create-pr&lt;/a&gt; by &lt;a href="https://github.com/toyamarinyon"&gt;toyamarinyon&lt;/a&gt; - Streamlines pull request creation by handling the entire workflow: creating a new branch, committing changes, formatting modified files with Biome, and submitting the PR.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/liam-hq/liam/raw/main/.claude/commands/create-pull-request.md"&gt;/create-pull-request&lt;/a&gt; by &lt;a href="https://github.com/liam-hq"&gt;liam-hq&lt;/a&gt; - Provides comprehensive PR creation guidance with GitHub CLI, enforcing title conventions, following template structure, and offering concrete command examples with best practices.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/evmts/tevm-monorepo/raw/main/.claude/commands/create-worktrees.md"&gt;/create-worktrees&lt;/a&gt; by &lt;a href="https://github.com/evmts"&gt;evmts&lt;/a&gt; - Creates git worktrees for all open PRs or specific branches, handling branches with slashes, cleaning up stale worktrees, and supporting custom branch creation for development.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/jeremymailen/kotlinter-gradle/raw/master/.claude/commands/fix-github-issue.md"&gt;/fix-github-issue&lt;/a&gt; by &lt;a href="https://github.com/jeremymailen"&gt;jeremymailen&lt;/a&gt; - Analyzes and fixes GitHub issues using a structured approach with GitHub CLI for issue details, implementing necessary code changes, running tests, and creating proper commit messages.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/metabase/metabase/raw/master/.claude/commands/fix-issue.md"&gt;/fix-issue&lt;/a&gt; by &lt;a href="https://github.com/metabase"&gt;metabase&lt;/a&gt; - Addresses GitHub issues by taking issue number as parameter, analyzing context, implementing solution, and testing/validating the fix for proper integration.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/metabase/metabase/raw/master/.claude/commands/fix-pr.md"&gt;/fix-pr&lt;/a&gt; by &lt;a href="https://github.com/metabase"&gt;metabase&lt;/a&gt; - Fetches and fixes unresolved PR comments by automatically retrieving feedback, addressing reviewer concerns, making targeted code improvements, and streamlining the review process.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/evmts/tevm-monorepo/raw/main/.claude/commands/husky.md"&gt;/husky&lt;/a&gt; by &lt;a href="https://github.com/evmts"&gt;evmts&lt;/a&gt; - Sets up and manages Husky Git hooks by configuring pre-commit hooks, establishing commit message standards, integrating with linting tools, and ensuring code quality on commits.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/giselles-ai/giselle/raw/main/.claude/commands/update-branch-name.md"&gt;/update-branch-name&lt;/a&gt; by &lt;a href="https://github.com/giselles-ai"&gt;giselles-ai&lt;/a&gt; - Updates branch names with proper prefixes and formats, enforcing naming conventions, supporting semantic prefixes, and managing remote branch updates.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Code Analysis &amp;amp; Testing&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/rygwdn/slack-tools/raw/main/.claude/commands/check.md"&gt;/check&lt;/a&gt; by &lt;a href="https://github.com/rygwdn"&gt;rygwdn&lt;/a&gt; - Performs comprehensive code quality and security checks, featuring static analysis integration, security vulnerability scanning, code style enforcement, and detailed reporting.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/kingler/n8n_agent/raw/main/.claude/commands/code_analysis.md"&gt;/code_analysis&lt;/a&gt; by &lt;a href="https://github.com/kingler"&gt;kingler&lt;/a&gt; - Provides a menu of advanced code analysis commands for deep inspection, including knowledge graph generation, optimization suggestions, and quality evaluation.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/to4iki/ai-project-rules/raw/main/.claude/commands/optimize.md"&gt;/optimize&lt;/a&gt; by &lt;a href="https://github.com/to4iki"&gt;to4iki&lt;/a&gt; - Analyzes code performance to identify bottlenecks, proposing concrete optimizations with implementation guidance for improved application performance.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/rzykov/metabase/raw/master/.claude/commands/repro-issue.md"&gt;/repro-issue&lt;/a&gt; by &lt;a href="https://github.com/rzykov"&gt;rzykov&lt;/a&gt; - Creates reproducible test cases for GitHub issues, ensuring tests fail reliably and documenting clear reproduction steps for developers.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/zscott/pane/raw/main/.claude/commands/tdd.md"&gt;/tdd&lt;/a&gt; by &lt;a href="https://github.com/zscott"&gt;zscott&lt;/a&gt; - Guides development using Test-Driven Development principles, enforcing Red-Green-Refactor discipline, integrating with git workflow, and managing PR creation.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/jerseycheese/Narraitor/raw/feature/issue-227-ai-suggestions/.claude/commands/tdd-implement.md"&gt;/tdd-implement&lt;/a&gt; by &lt;a href="https://github.com/jerseycheese"&gt;jerseycheese&lt;/a&gt; - Implements Test-Driven Development by analyzing feature requirements, creating tests first (red), implementing minimal passing code (green), and refactoring while maintaining tests.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Context Loading &amp;amp; Priming&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/elizaOS/elizaos.github.io/raw/main/.claude/commands/context-prime.md"&gt;/context-prime&lt;/a&gt; by &lt;a href="https://github.com/elizaOS"&gt;elizaOS&lt;/a&gt; - Primes Claude with comprehensive project understanding by loading repository structure, setting development context, establishing project goals, and defining collaboration parameters.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/okuvshynov/cubestat/raw/main/.claude/commands/initref.md"&gt;/initref&lt;/a&gt; by &lt;a href="https://github.com/okuvshynov"&gt;okuvshynov&lt;/a&gt; - Initializes reference documentation structure with standard doc templates, API reference setup, documentation conventions, and placeholder content generation.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ethpandaops/xatu-data/raw/master/.claude/commands/load-llms-txt.md"&gt;/load-llms-txt&lt;/a&gt; by &lt;a href="https://github.com/ethpandaops"&gt;ethpandaops&lt;/a&gt; - Loads LLM configuration files to context, importing specific terminology, model configurations, and establishing baseline terminology for AI discussions.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Mjvolk3/torchcell/raw/main/.claude/commands/load_coo_context.md"&gt;/load_coo_context&lt;/a&gt; by &lt;a href="https://github.com/Mjvolk3"&gt;Mjvolk3&lt;/a&gt; - References specific files for sparse matrix operations, explains transform usage, compares with previous approaches, and sets data formatting context for development.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Mjvolk3/torchcell/raw/main/.claude/commands/load_dango_pipeline.md"&gt;/load_dango_pipeline&lt;/a&gt; by &lt;a href="https://github.com/Mjvolk3"&gt;Mjvolk3&lt;/a&gt; - Sets context for model training by referencing pipeline files, establishing working context, and preparing for pipeline work with relevant documentation.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/yzyydev/AI-Engineering-Structure/raw/main/.claude/commands/prime.md"&gt;/prime&lt;/a&gt; by &lt;a href="https://github.com/yzyydev"&gt;yzyydev&lt;/a&gt; - Sets up initial project context by viewing directory structure and reading key files, creating standardized context with directory visualization and key documentation focus.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ddisisto/si/raw/main/.claude/commands/rsi.md"&gt;/rsi&lt;/a&gt; by &lt;a href="https://github.com/ddisisto"&gt;ddisisto&lt;/a&gt; - Reads all commands and key project files to optimize AI-assisted development by streamlining the process, loading command context, and setting up for better development workflow.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Documentation &amp;amp; Changelogs&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/berrydev-ai/blockdoc-python/raw/main/.claude/commands/add-to-changelog.md"&gt;/add-to-changelog&lt;/a&gt; by &lt;a href="https://github.com/berrydev-ai"&gt;berrydev-ai&lt;/a&gt; - Adds new entries to changelog files while maintaining format consistency, properly documenting changes, and following established project standards for version tracking.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/jerseycheese/Narraitor/raw/feature/issue-227-ai-suggestions/.claude/commands/create-docs.md"&gt;/create-docs&lt;/a&gt; by &lt;a href="https://github.com/jerseycheese"&gt;jerseycheese&lt;/a&gt; - Analyzes code structure and purpose to create comprehensive documentation detailing inputs/outputs, behavior, user interaction flows, and edge cases with error handling.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/slunsford/coffee-analytics/raw/main/.claude/commands/docs.md"&gt;/docs&lt;/a&gt; by &lt;a href="https://github.com/slunsford"&gt;slunsford&lt;/a&gt; - Generates comprehensive documentation that follows project structure, documenting APIs and usage patterns with consistent formatting for better user understanding.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/hackdays-io/toban-contribution-viewer/raw/main/.claude/commands/explain-issue-fix.md"&gt;/explain-issue-fix&lt;/a&gt; by &lt;a href="https://github.com/hackdays-io"&gt;hackdays-io&lt;/a&gt; - Documents solution approaches for GitHub issues, explaining technical decisions, detailing challenges overcome, and providing implementation context for better understanding.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Consiliency/Flutter-Structurizr/raw/main/.claude/commands/update-docs.md"&gt;/update-docs&lt;/a&gt; by &lt;a href="https://github.com/Consiliency"&gt;Consiliency&lt;/a&gt; - Reviews current documentation status, updates implementation progress, reviews phase documents, and maintains documentation consistency across the project.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;CI / Deployment&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/kelp/webdown/raw/main/.claude/commands/release.md"&gt;/release&lt;/a&gt; by &lt;a href="https://github.com/kelp"&gt;kelp&lt;/a&gt; - Manages software releases by updating changelogs, reviewing README changes, evaluating version increments, and documenting release changes for better version tracking.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/hackdays-io/toban-contribution-viewer/raw/main/.claude/commands/run-ci.md"&gt;/run-ci&lt;/a&gt; by &lt;a href="https://github.com/hackdays-io"&gt;hackdays-io&lt;/a&gt; - Activates virtual environments, runs CI-compatible check scripts, iteratively fixes errors, and ensures all tests pass before completion.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Project &amp;amp; Task Management&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/scopecraft/command/raw/main/.claude/commands/create-command.md"&gt;/create-command&lt;/a&gt; by &lt;a href="https://github.com/scopecraft"&gt;scopecraft&lt;/a&gt; - Guides Claude through creating new custom commands with proper structure by analyzing requirements, templating commands by category, enforcing command standards, and creating supporting documentation.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/taddyorg/inkverse/raw/main/.claude/commands/create-jtbd.md"&gt;/create-jtbd&lt;/a&gt; by &lt;a href="https://github.com/taddyorg"&gt;taddyorg&lt;/a&gt; - Creates Jobs-to-be-Done frameworks that outline user needs with structured format, focusing on specific user problems and organizing by job categories for product development.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/taddyorg/inkverse/raw/main/.claude/commands/create-prd.md"&gt;/create-prd&lt;/a&gt; by &lt;a href="https://github.com/taddyorg"&gt;taddyorg&lt;/a&gt; - Generates comprehensive product requirement documents outlining detailed specifications, requirements, and features following standardized document structure and format.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Wirasm/claudecode-utils/raw/main/.claude/commands/create-prp.md"&gt;/create-prp&lt;/a&gt; by &lt;a href="https://github.com/Wirasm"&gt;Wirasm&lt;/a&gt; - Creates product requirement plans by reading PRP methodology, following template structure, creating comprehensive requirements, and structuring product definitions for development.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/jerseycheese/Narraitor/raw/feature/issue-227-ai-suggestions/.claude/commands/do-issue.md"&gt;/do-issue&lt;/a&gt; by &lt;a href="https://github.com/jerseycheese"&gt;jerseycheese&lt;/a&gt; - Implements GitHub issues with manual review points, following a structured approach with issue number parameter and offering alternative automated mode for efficiency.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/disler/just-prompt/raw/main/.claude/commands/project_hello_w_name.md"&gt;/project_hello_w_name&lt;/a&gt; by &lt;a href="https://github.com/disler"&gt;disler&lt;/a&gt; - Creates customizable greeting components with name input, demonstrating argument passing, component reusability, state management, and user input handling.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/chrisleyva/todo-slash-command/raw/main/todo.md"&gt;/todo&lt;/a&gt; by &lt;a href="https://github.com/chrisleyva"&gt;chrisleyva&lt;/a&gt; - A convenient command to quickly manage project todo items without leaving the Claude Code interface, featuring due dates, sorting, task prioritization, and comprehensive todo list management.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Miscellaneous&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Mjvolk3/torchcell/raw/main/.claude/commands/fixing_go_in_graph.md"&gt;/fixing_go_in_graph&lt;/a&gt; by &lt;a href="https://github.com/Mjvolk3"&gt;Mjvolk3&lt;/a&gt; - Focuses on Gene Ontology annotation integration in graph databases, handling multiple data sources, addressing graph representation issues, and ensuring correct data incorporation.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/GaloyMoney/lana-bank/raw/main/.claude/commands/mermaid.md"&gt;/mermaid&lt;/a&gt; by &lt;a href="https://github.com/GaloyMoney"&gt;GaloyMoney&lt;/a&gt; - Generates Mermaid diagrams from SQL schema files, creating entity relationship diagrams with table properties, validating diagram compilation, and ensuring complete entity coverage.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Mjvolk3/torchcell/raw/main/.claude/commands/review_dcell_model.md"&gt;/review_dcell_model&lt;/a&gt; by &lt;a href="https://github.com/Mjvolk3"&gt;Mjvolk3&lt;/a&gt; - Reviews old Dcell implementation files, comparing with newer Dango model, noting changes over time, and analyzing refactoring approaches for better code organization.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/zuplo/docs/raw/main/.claude/commands/use-stepper.md"&gt;/use-stepper&lt;/a&gt; by &lt;a href="https://github.com/zuplo"&gt;zuplo&lt;/a&gt; - Reformats documentation to use React Stepper component, transforming heading formats, applying proper indentation, and maintaining markdown compatibility with admonition formatting.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;h2&gt;CLAUDE.md Files ğŸ“‚&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;code&gt;CLAUDE.md&lt;/code&gt; files are files that contain important guidelines and context-specific information or instructions that help Claude Code to better understand your project and your coding standards&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Language-Specific&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/didalgolab/ai-intellij-plugin/raw/main/CLAUDE.md"&gt;AI IntelliJ Plugin&lt;/a&gt; by &lt;a href="https://github.com/didalgolab"&gt;didalgolab&lt;/a&gt; - Provides comprehensive Gradle commands for IntelliJ plugin development with platform-specific coding patterns, detailed package structure guidelines, and clear internationalization standards.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/alexei-led/aws-mcp-server/raw/main/CLAUDE.md"&gt;AWS MCP Server&lt;/a&gt; by &lt;a href="https://github.com/alexei-led"&gt;alexei-led&lt;/a&gt; - Features multiple Python environment setup options with detailed code style guidelines, comprehensive error handling recommendations, and security considerations for AWS CLI interactions.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/touchlab/DroidconKotlin/raw/main/CLAUDE.md"&gt;DroidconKotlin&lt;/a&gt; by &lt;a href="https://github.com/touchlab"&gt;touchlab&lt;/a&gt; - Delivers comprehensive Gradle commands for cross-platform Kotlin Multiplatform development with clear module structure and practical guidance for dependency injection.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/hesreallyhim/awesome-claude-code/raw/main/resources/claude.md-files/EDSL/CLAUDE.md"&gt;EDSL&lt;/a&gt; by &lt;a href="https://github.com/expectedparrot"&gt;expectedparrot&lt;/a&gt; - Offers detailed build and test commands with strict code style enforcement, comprehensive testing requirements, and standardized development workflow using Black and mypy. &lt;em&gt;(Removed from origin)&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/giselles-ai/giselle/raw/main/CLAUDE.md"&gt;Giselle&lt;/a&gt; by &lt;a href="https://github.com/giselles-ai"&gt;giselles-ai&lt;/a&gt; - Provides detailed build and test commands using pnpm and Vitest with strict code formatting requirements and comprehensive naming conventions for code consistency.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/hashintel/hash/raw/main/CLAUDE.md"&gt;HASH&lt;/a&gt; by &lt;a href="https://github.com/hashintel"&gt;hashintel&lt;/a&gt; - Features comprehensive repository structure breakdown with strong emphasis on coding standards, detailed Rust documentation guidelines, and systematic PR review process.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/inkline/inkline/raw/main/CLAUDE.md"&gt;Inkline&lt;/a&gt; by &lt;a href="https://github.com/inkline"&gt;inkline&lt;/a&gt; - Structures development workflow using pnpm with emphasis on TypeScript and Vue 3 Composition API, detailed component creation process, and comprehensive testing recommendations.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/mattgodbolt/jsbeeb/raw/main/CLAUDE.md"&gt;JSBeeb&lt;/a&gt; by &lt;a href="https://github.com/mattgodbolt"&gt;mattgodbolt&lt;/a&gt; - Provides development guide for JavaScript BBC Micro emulator with build and testing instructions, architecture documentation, and debugging workflows.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/LamoomAI/lamoom-python/raw/main/CLAUDE.md"&gt;Lamoom Python&lt;/a&gt; by &lt;a href="https://github.com/LamoomAI"&gt;LamoomAI&lt;/a&gt; - Serves as reference for production prompt engineering library with load balancing of AI Models, API documentation, and usage patterns with examples.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/langchain-ai/langgraphjs/raw/main/CLAUDE.md"&gt;LangGraphJS&lt;/a&gt; by &lt;a href="https://github.com/langchain-ai"&gt;langchain-ai&lt;/a&gt; - Offers comprehensive build and test commands with detailed TypeScript style guidelines, layered library architecture, and monorepo structure using yarn workspaces.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/metabase/metabase/raw/master/CLAUDE.md"&gt;Metabase&lt;/a&gt; by &lt;a href="https://github.com/metabase"&gt;metabase&lt;/a&gt; - Details workflow for REPL-driven development in Clojure/ClojureScript with emphasis on incremental development, testing, and step-by-step approach for feature implementation.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/sgcarstrends/backend/raw/main/CLAUDE.md"&gt;SG Cars Trends Backend&lt;/a&gt; by &lt;a href="https://github.com/sgcarstrends"&gt;sgcarstrends&lt;/a&gt; - Provides comprehensive structure for TypeScript monorepo projects with detailed commands for development, testing, deployment, and AWS/Cloudflare integration.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/spylang/spy/raw/main/CLAUDE.md"&gt;SPy&lt;/a&gt; by &lt;a href="https://github.com/spylang"&gt;spylang&lt;/a&gt; - Enforces strict coding conventions with comprehensive testing guidelines, multiple code compilation options, and backend-specific test decorators for targeted filtering.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/KarpelesLab/tpl/raw/master/CLAUDE.md"&gt;TPL&lt;/a&gt; by &lt;a href="https://github.com/KarpelesLab"&gt;KarpelesLab&lt;/a&gt; - Details Go project conventions with comprehensive error handling recommendations, table-driven testing approach guidelines, and modernization suggestions for latest Go features.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Domain-Specific&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Layr-Labs/avs-vibe-developer-guide/raw/master/CLAUDE.md"&gt;AVS Vibe Developer Guide&lt;/a&gt; by &lt;a href="https://github.com/Layr-Labs"&gt;Layr-Labs&lt;/a&gt; - Structures AI-assisted EigenLayer AVS development workflow with consistent naming conventions for prompt files and established terminology standards for blockchain concepts.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/CommE2E/comm/raw/master/CLAUDE.md"&gt;Comm&lt;/a&gt; by &lt;a href="https://github.com/CommE2E"&gt;CommE2E&lt;/a&gt; - Serves as a development reference for E2E-encrypted messaging applications with code organization architecture, security implementation details, and testing procedures.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/badass-courses/course-builder/raw/main/CLAUDE.md"&gt;Course Builder&lt;/a&gt; by &lt;a href="https://github.com/badass-courses"&gt;badass-courses&lt;/a&gt; - Enables real-time multiplayer capabilities for collaborative course creation with diverse tech stack integration and monorepo architecture using Turborepo.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/eastlondoner/cursor-tools/raw/main/CLAUDE.md"&gt;Cursor Tools&lt;/a&gt; by &lt;a href="https://github.com/eastlondoner"&gt;eastlondoner&lt;/a&gt; - Creates a versatile AI command interface supporting multiple providers and models with flexible command options and browser automation through "Stagehand" feature.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/soramimi/Guitar/raw/master/CLAUDE.md"&gt;Guitar&lt;/a&gt; by &lt;a href="https://github.com/soramimi"&gt;soramimi&lt;/a&gt; - Serves as development guide for Guitar Git GUI Client with build commands for various platforms, code style guidelines for contributing, and project structure explanation.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Fimeg/NetworkChronicles/raw/legacy-v1/CLAUDE.md"&gt;Network Chronicles&lt;/a&gt; by &lt;a href="https://github.com/Fimeg"&gt;Fimeg&lt;/a&gt; - Presents detailed implementation plan for AI-driven game characters with technical specifications for LLM integration, character guidelines, and service discovery mechanics.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ParetoSecurity/pareto-mac/raw/main/CLAUDE.md"&gt;Pareto Mac&lt;/a&gt; by &lt;a href="https://github.com/ParetoSecurity"&gt;ParetoSecurity&lt;/a&gt; - Serves as development guide for Mac security audit tool with build instructions, contribution guidelines, testing procedures, and workflow documentation.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/steadycursor/steadystart/raw/main/CLAUDE.md"&gt;SteadyStart&lt;/a&gt; by &lt;a href="https://github.com/steadycursor"&gt;steadycursor&lt;/a&gt; - Clear and direct instructives about style, permissions, Claude's "role", communications, and documentation of Claude Code sessions for other team members to stay abreast.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Project Scaffolding &amp;amp; MCP&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/basicmachines-co/basic-memory/raw/main/CLAUDE.md"&gt;Basic Memory&lt;/a&gt; by &lt;a href="https://github.com/basicmachines-co"&gt;basicmachines-co&lt;/a&gt; - Presents an innovative AI-human collaboration framework with Model Context Protocol for bidirectional LLM-markdown communication and flexible knowledge structure for complex projects.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/grahama1970/claude-code-mcp-enhanced/raw/main/CLAUDE.md"&gt;claude-code-mcp-enhanced&lt;/a&gt; by &lt;a href="https://github.com/grahama1970"&gt;grahama1970&lt;/a&gt; - Provides detailed and emphatic instructions for Claude to follow as a coding agent, with testing guidance, code examples, and compliance checks.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;h2&gt;Alternative Clients ğŸ“±&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Alternative Clients are alternative UIs and front-ends for interacting with Claude Code, either on mobile or on the desktop.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;General&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/opactorai/Claudable"&gt;Claudable&lt;/a&gt; by &lt;a href="https://www.linkedin.com/in/seongil-park/"&gt;Ethan Park&lt;/a&gt; - Claudable is an open-source web builder that leverages local CLI agents, such as Claude Code and Cursor Agent, to build and deploy products effortlessly.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/omnara-ai/omnara"&gt;Omnara&lt;/a&gt; by &lt;a href="https://github.com/ishaansehgal99"&gt;Ishaan Sehgal&lt;/a&gt; - A command center for AI agents that syncs Claude Code sessions across terminal, web, and mobile. Allows for remote monitoring, human-in-the-loop interaction, and team collaboration.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;h2&gt;Official Documentation ğŸ›ï¸&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Links to some of Anthropic's terrific documentation and resources regarding Claude Code&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;General&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.claude.com/en/home"&gt;Anthropic Documentation&lt;/a&gt; by &lt;a href="https://github.com/anthropics"&gt;Anthropic&lt;/a&gt; - The official documentation for Claude Code, including installation instructions, usage guidelines, API references, tutorials, examples, loads of information that I won't list individually. Like Claude Code, the documentation is frequently updated.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/anthropics/claude-quickstarts"&gt;Anthropic Quickstarts&lt;/a&gt; by &lt;a href="https://github.com/anthropics"&gt;Anthropic&lt;/a&gt; - Offers comprehensive development guides for three distinct AI-powered demo projects with standardized workflows, strict code style guidelines, and containerization instructions.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/anthropics/claude-code-action/tree/main/examples"&gt;Claude Code GitHub Actions&lt;/a&gt; by &lt;a href="https://github.com/anthropics"&gt;Anthropic&lt;/a&gt; - Official GitHub Actions integration for Claude Code with examples and documentation for automating AI-powered workflows in CI/CD pipelines.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing &lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#awesome-claude-code"&gt;ğŸ”&lt;/a&gt;&lt;/h2&gt; 
&lt;h3&gt;&lt;strong&gt;&lt;a href="https://github.com/hesreallyhim/awesome-claude-code/issues/new?template=recommend-resource.yml"&gt;Recommend a new resource here!&lt;/a&gt;&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;Recommending a resource for the list is very simple, and the automated system handles everything for you. Please do not open a PR to submit a recommendation - the only person who is allowed to submit PRs to this repo is Claude.&lt;/p&gt; 
&lt;p&gt;Make sure that you have read the CONTRIBUTING.md document and CODE_OF_CONDUCT.md before you submit a recommendation.&lt;/p&gt; 
&lt;p&gt;For suggestions about the repository itself, please &lt;a href="https://github.com/hesreallyhim/awesome-claude-code/issues/new?template=repository-enhancement.yml"&gt;open a repository enhancement issue&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;This project is released with a Code of Conduct. By participating, you agree to abide by its terms. And although I take strong measures to uphold the quality and safety of this list, I take no responsibility or liability for anything that might happen as a result of these third-party resources.&lt;/p&gt; 
&lt;h2&gt;Growing thanks to you&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://starchart.cc/hesreallyhim/awesome-claude-code"&gt;&lt;img src="https://starchart.cc/hesreallyhim/awesome-claude-code.svg?variant=adaptive" alt="Stargazers over time" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This list is licensed under &lt;a href="https://creativecommons.org/licenses/by-nc-nd/4.0/"&gt;Creative Commons CC BY-NC-ND 4.0&lt;/a&gt; - this means you are welcome to fork, clone, copy and redistribute the list, provided you include appropriate attribution; however you are not permitted to distribute any modified versions or to use it for any commercial purposes. This is to prevent disregard for the licenses of the authors whose resources are listed here. Please note that all resources included in this list have their own license terms.&lt;/p&gt; 
&lt;!-- OBLIGATORY GUARD AGAINST SILLY END-OF-FILE PROBLEM --&gt;</description>
    </item>
    
    <item>
      <title>MiroMindAI/MiroFlow</title>
      <link>https://github.com/MiroMindAI/MiroFlow</link>
      <description>&lt;p&gt;MiroFlow is an agent framework that enables tool-use agent tasks, featuring a reproducible GAIA score of 82.4%.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/MiroMindAI/MiroFlow/main/docs/mkdocs/docs/assets/miroflow_logo.png" width="45%" alt="MiroFlow" /&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://dr.miromind.ai/"&gt;&lt;img src="https://img.shields.io/badge/Demo-FFB300?style=for-the-badge&amp;amp;logo=airplayvideo&amp;amp;logoColor=white" alt="DEMO" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/datasets/miromind-ai/MiroVerse-v0.1"&gt;&lt;img src="https://img.shields.io/badge/Data-0040A1?style=for-the-badge&amp;amp;logo=huggingface&amp;amp;logoColor=ffffff&amp;amp;labelColor" alt="DATA" /&gt;&lt;/a&gt; &lt;a href="https://miromind.ai/blog/miroflow"&gt;&lt;img src="https://img.shields.io/badge/Blog-4285F4?style=for-the-badge&amp;amp;logo=google-chrome&amp;amp;logoColor=white" alt="BLOG" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/MiroMindAI"&gt;&lt;img src="https://img.shields.io/badge/Github-24292F?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white" alt="GITHUB" /&gt;&lt;/a&gt; &lt;a href="https://discord.com/invite/GPqEnkzQZd"&gt;&lt;img src="https://img.shields.io/badge/Discord-5865F2?style=for-the-badge&amp;amp;logo=discord&amp;amp;logoColor=white" alt="DISCORD" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;h3&gt;ğŸš€ &lt;a href="https://dr.miromind.ai/"&gt;Try Demo&lt;/a&gt; ï½œ &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroFlow/main/README_zh.md"&gt;ä¸­æ–‡&lt;/a&gt; ï½œ &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroFlow/main/README_ja.md"&gt;æ—¥æœ¬èª&lt;/a&gt;&lt;/h3&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;img width="100%" alt="image" src="https://raw.githubusercontent.com/MiroMindAI/MiroFlow/main/docs/mkdocs/docs/assets/futurex-09-12.png" /&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;p&gt;This repo is the official implementation of the &lt;strong&gt;MiroMind Research Agent Project&lt;/strong&gt;. It is a leading-performance, fully open-source system designed to perform multi-step internet research for addressing complex challenges such as future event prediction. The project currently comprises four key components:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ¤– &lt;strong&gt;MiroFlow&lt;/strong&gt;: an open-source research agent framework that offers reproducible state-of-the-art performance on representative benchmarks (e.g., FutureX, GAIA, HLE, xBench-DeepSearch, and BrowserComp benchmarks), included in this repo. See &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroFlow/main/#-get-started-in-under-5-minutes"&gt;[Get Started in Under 5 Minutes]&lt;/a&gt; for a quick start.&lt;/li&gt; 
 &lt;li&gt;ğŸ¤” &lt;strong&gt;MiroThinker&lt;/strong&gt;: an open-source agent that natively supports tool-assisted reasoning. See &lt;a href="https://github.com/MiroMindAI/mirothinker"&gt;MiroThinker&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;ğŸ“Š &lt;strong&gt;MiroVerse&lt;/strong&gt;: 147k premium open-source training data supporting research agent training. See &lt;a href="https://huggingface.co/datasets/miromind-ai/MiroVerse-v0.1"&gt;MiroVerse&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ“‹ Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“° &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroFlow/main/#-news--updates"&gt;News &amp;amp; Updates&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸš€ &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroFlow/main/#-get-started-in-under-5-minutes"&gt;Get Started in Under 5 Minutes&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸ¤– &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroFlow/main/#-what-is-miroflow"&gt;What is MiroFlow?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸŒŸ &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroFlow/main/#-Highlights"&gt;Highlights&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸ“ˆ &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroFlow/main/#-performance-on-benchmarks"&gt;Performance on Benchmarks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸ”§ &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroFlow/main/#-supported-models--tools"&gt;Supported Models &amp;amp; Tools&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;â“ &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroFlow/main/#-faq"&gt;FAQ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸ¤ &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroFlow/main/#-contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸ“„ &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroFlow/main/#-license"&gt;License&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸ™ &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroFlow/main/#-acknowledgments-and-contributors"&gt;Acknowledgments&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ“° News &amp;amp; Updates&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;[2025-09-15]&lt;/strong&gt;: ğŸ‰ğŸ‰ &lt;strong&gt;MiroFlow v0.3&lt;/strong&gt;: Enhanced codebase architecture and significantly improved benchmark performance, boosting GPT-5's prediction accuracy for future events by 11%. MiroFlow now ranks #1 in the future prediction benchmark. See &lt;a href="https://futurex-ai.github.io/"&gt;FutureX&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025-08-27]&lt;/strong&gt;: &lt;strong&gt;MiroFlow v0.2&lt;/strong&gt;: Achieves state-of-the-art performance across &lt;a href="https://miromind.ai/blog/miroflow"&gt;multiple agentic benchmarks&lt;/a&gt;, including HLE (27.2%), HLE-Text-Only (29.5%), BrowserComp-EN (33.2%), BrowserComp-ZH (47.1%), and xBench-DeepSearch (72.0%).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025-08-26]&lt;/strong&gt;: Released &lt;a href="https://raw.githubusercontent.com/MiroMindAI/MiroFlow/main/docs/public_trace.md"&gt;GAIA Validation Trace&lt;/a&gt; (73.94% pass@1) and &lt;a href="https://github.com/MiroMindAI/MiroThinker/tree/main/apps/gradio-demo"&gt;Gradio Demo&lt;/a&gt; for local deployment.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[2025-08-08]&lt;/strong&gt;: &lt;strong&gt;MiroFlow v0.1&lt;/strong&gt;: Complete open-source release of the research agent framework.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸš€ Get Started in Under 5 Minutes&lt;/h2&gt; 
&lt;h3&gt;ğŸ“‹ Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Python&lt;/strong&gt;: 3.12 or higher&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Package Manager&lt;/strong&gt;: &lt;a href="https://docs.astral.sh/uv/"&gt;&lt;code&gt;uv&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Operating System&lt;/strong&gt;: Linux, macOS&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;âš¡ Quick Setup&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;: Intelligent document analysis with file processing capabilities.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 1. Clone and setup
git clone https://github.com/MiroMindAI/MiroFlow &amp;amp;&amp;amp; cd MiroFlow
uv sync

# 2. Configure API key
cp .env.template .env
# Edit .env and add your OPENROUTER_API_KEY

# 3. Run your first agent
uv run main.py trace --config_file_name=agent_quickstart_reading --task="What is the first country listed in the XLSX file that have names starting with Co?" --task_file_name="data/FSI-2023-DOWNLOAD.xlsx"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;ğŸ‰ &lt;strong&gt;Expected Output:&lt;/strong&gt; Your agent should return &lt;strong&gt;\boxed{Congo Democratic Republic}&lt;/strong&gt; ğŸ˜Š&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;ğŸ’¡ Tip:&lt;/strong&gt; If you encounter issues, check that your API key is correctly set in the &lt;code&gt;.env&lt;/code&gt; file and that all dependencies are installed.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ¤– What is MiroFlow?&lt;/h2&gt; 
&lt;p&gt;MiroFlow is a high-performance, modular framework for building intelligent AI agents that deliver state-of-the-art results on complex reasoning tasks like future event prediction. The framework features advanced multi-turn conversation capabilities, extensive tool ecosystem integration, and hierarchical sub-agent orchestration for optimal task completion. Learn more about our &lt;a href="https://miromindai.github.io/MiroFlow/core_concepts/"&gt;agent framework&lt;/a&gt;.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/MiroMindAI/MiroFlow/main/docs/mkdocs/docs/assets/miroflow_architecture.png" width="100%" alt="MiroFlow Architecture" /&gt; 
&lt;/div&gt; 
&lt;table align="center" style="border: 1px solid #ccc; border-radius: 8px; padding: 12px; background-color: #f9f9f9; width: 60%;"&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td style="text-align: center; padding: 10px;"&gt; &lt;strong&gt;Research Assistant Demo&lt;/strong&gt; - &lt;span style="font-size: 0.9em; color: #555;"&gt;Read CVPR 2025 Best Paper and Provide Research Advice&lt;/span&gt; &lt;br /&gt; 
    &lt;video src="https://github.com/user-attachments/assets/99ed3172-6e9a-467a-9ccb-be45957fe2e4" controls muted preload="metadata" width="50%" height="50%" &lt; video&gt; 
    &lt;/video&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸŒŸ Highlights&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Reproducible State-of-the-Art Performance&lt;/strong&gt;: #1 ranking across &lt;a href="https://miromindai.github.io/MiroFlow/evaluation_overview/"&gt;multiple representative agentic benchmarks&lt;/a&gt;, including FutureX, GAIA, HLE, xBench-DeepSearch, and BrowserComp benchmarks)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;High Concurrency &amp;amp; Reliability&lt;/strong&gt;: Built with robust concurrency management and fault-tolerant design, MiroFlow efficiently handles rate-limited APIs and unstable networks, ensuring seamless trajectory collection and reliable execution of complex tasks.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Cost-Effective Deployment&lt;/strong&gt;: Powered by the open-source MiroThinker model, MiroFlow can run a research agent service on a single RTX 4090. The entire stack relies on free, open-source tools, making it simple to deploy, scale, and reproduce. See &lt;a href="https://github.com/MiroMindAI/mirothinker"&gt;MiroThinker&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ”§ Supported Models &amp;amp; Tools&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Models&lt;/strong&gt;: GPT, Claude, Gemini, Qwen, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Tools&lt;/strong&gt;: &lt;a href="https://github.com/MiroMindAI/MiroFlow/raw/miroflow-v0.3/src/tool/mcp_servers/audio_mcp_server.py"&gt;Audio Transcription&lt;/a&gt;, &lt;a href="https://github.com/MiroMindAI/MiroFlow/raw/miroflow-v0.3/src/tool/mcp_servers/python_server.py"&gt;Python&lt;/a&gt;, &lt;a href="https://github.com/MiroMindAI/MiroFlow/raw/miroflow-v0.3/src/tool/mcp_servers/reading_mcp_server.py"&gt;File Reading&lt;/a&gt;, &lt;a href="https://github.com/MiroMindAI/MiroFlow/raw/miroflow-v0.3/src/tool/mcp_servers/reasoning_mcp_server.py"&gt;Reasoning&lt;/a&gt;, &lt;a href="https://github.com/MiroMindAI/MiroFlow/raw/miroflow-v0.3/src/tool/mcp_servers/searching_mcp_server.py"&gt;Google Search&lt;/a&gt;, &lt;a href="https://github.com/MiroMindAI/MiroFlow/raw/miroflow-v0.3/src/tool/mcp_servers/vision_mcp_server.py"&gt;VQA&lt;/a&gt;, E2B, etc.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ“ˆ Performance on Benchmarks&lt;/h2&gt; 
&lt;p&gt;We achieved the #1 ranking on the FutureX Benchmark Leaderboard as of September 10, 2025, boosting GPT-5's prediction accuracy for future events by 11%.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img width="100%" alt="image" src="https://raw.githubusercontent.com/MiroMindAI/MiroFlow/main/docs/mkdocs/docs/assets/futurex-09-12.png" /&gt; 
&lt;/div&gt; 
&lt;p&gt;We benchmark MiroFlow on a series of benchmarks, including &lt;strong&gt;GAIA&lt;/strong&gt;, &lt;strong&gt;HLE&lt;/strong&gt;, &lt;strong&gt;BrowseComp&lt;/strong&gt;, and &lt;strong&gt;xBench-DeepSearch&lt;/strong&gt;, and achieved SOTA results.&lt;/p&gt; 
&lt;img width="100%" alt="image" src="https://raw.githubusercontent.com/MiroMindAI/MiroFlow/main/docs/mkdocs/docs/assets/benchmark_results.png" /&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model/Framework&lt;/th&gt; 
   &lt;th&gt;GAIA Val&lt;/th&gt; 
   &lt;th&gt;HLE&lt;/th&gt; 
   &lt;th&gt;HLE-Text&lt;/th&gt; 
   &lt;th&gt;BrowserComp-EN&lt;/th&gt; 
   &lt;th&gt;BrowserComp-ZH&lt;/th&gt; 
   &lt;th&gt;xBench-DeepSearch&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;MiroFlow&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;82.4%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;27.2%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;29.5%&lt;/td&gt; 
   &lt;td&gt;33.2%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;47.1%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;72.0%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenAI Deep Research&lt;/td&gt; 
   &lt;td&gt;67.4%&lt;/td&gt; 
   &lt;td&gt;26.6%&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;51.5%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;42.9%&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Gemini Deep Research&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;26.9%&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;50+%&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Kimi Researcher&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;26.9%&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;69.0%&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;WebSailor-72B&lt;/td&gt; 
   &lt;td&gt;55.4%&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;30.1%&lt;/td&gt; 
   &lt;td&gt;55.0%&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Manus&lt;/td&gt; 
   &lt;td&gt;73.3%&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;DeepSeek v3.1&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;29.8%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;71.2%&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Follow our detailed guides to reproduce benchmark results in our &lt;a href="https://miromindai.github.io/MiroFlow/evaluation_overview/"&gt;Benchmarks Documentation&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;â“ FAQ&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;What API keys do I need?&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; You only need an OpenRouter API key to get started. OpenRouter provides access to multiple language models through a single API. 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Can I use other language models besides OpenRouter?&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; Yes, MiroFlow supports various language models. Check our documentation for configuration details. 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;How do I reproduce the benchmark results?&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; Follow our detailed 
 &lt;a href="https://miromindai.github.io/MiroFlow/evaluation_overview/"&gt;Benchmarks Documentation&lt;/a&gt; for step-by-step reproduction guides. 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Is there commercial support available?&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; For commercial inquiries and enterprise support, please contact us through our 
 &lt;a href="https://miromind.ai/"&gt;website&lt;/a&gt;. 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ¤ Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions from the community! Whether you're fixing bugs, adding features, or improving documentation, your help is appreciated.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“‹ &lt;strong&gt;Issues&lt;/strong&gt;: Report bugs or request features via &lt;a href="https://github.com/MiroMindAI/MiroFlow/issues"&gt;GitHub Issues&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;ğŸ”€ &lt;strong&gt;Pull Requests&lt;/strong&gt;: Submit improvements via pull requests.&lt;/li&gt; 
 &lt;li&gt;ğŸ’¬ &lt;strong&gt;Discussions&lt;/strong&gt;: Join our &lt;a href="https://discord.com/invite/GPqEnkzQZd"&gt;Discord community&lt;/a&gt; for questions and discussions.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ“„ License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the Apache License 2.0.&lt;/p&gt; 
&lt;h2&gt;ğŸ™ Acknowledgments&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Benchmark Contributors&lt;/strong&gt; for the comprehensive evaluation datasets.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Open Source Community&lt;/strong&gt; for the tools and libraries that make this possible.&lt;/p&gt; 
&lt;p&gt;We thank all contributors who have helped make MiroFlow better:&lt;/p&gt; 
&lt;a href="https://github.com/MiroMindAI/MiroFlow/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=MiroMindAI/MiroFlow" /&gt; &lt;/a&gt; 
&lt;p&gt;Join our community and help us build the future of AI agents!&lt;/p&gt; 
&lt;h2&gt;References&lt;/h2&gt; 
&lt;p&gt;The technical report is coming soon!&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@misc{2025mirothinker,
    title={MiroFlow: A High-Performance Open-Source Research Agent Framework},
    author={MiroMind AI Team},
    howpublished={\url{https://github.com/MiroMindAI/MiroFlow}},
    year={2025}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href="https://www.star-history.com/#MiroMindAI/MiroFlow&amp;amp;Timeline"&gt;&lt;img src="https://api.star-history.com/svg?repos=MiroMindAI/MiroFlow&amp;amp;type=Timeline" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>mikeyobrien/ralph-orchestrator</title>
      <link>https://github.com/mikeyobrien/ralph-orchestrator</link>
      <description>&lt;p&gt;An improved implementation of the Ralph Wiggum technique for autonomous AI agent orchestration&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Ralph Orchestrator&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/mikeyobrien/ralph-orchestrator/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/license-MIT-blue" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://www.rust-lang.org/"&gt;&lt;img src="https://img.shields.io/badge/rust-1.75+-orange" alt="Rust" /&gt;&lt;/a&gt; &lt;a href="https://github.com/mikeyobrien/ralph-orchestrator/actions"&gt;&lt;img src="https://img.shields.io/github/actions/workflow/status/mikeyobrien/ralph-orchestrator/release.yml?branch=main&amp;amp;label=CI" alt="Build" /&gt;&lt;/a&gt; &lt;a href="https://github.com/hesreallyhim/awesome-claude-code"&gt;&lt;img src="https://awesome.re/mentioned-badge.svg?sanitize=true" alt="Mentioned in Awesome Claude Code" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;A hat-based multi-agent orchestration framework that keeps AI agents in a loop until the task is done.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;"Me fail English? That's unpossible!" - Ralph Wiggum&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;Alpha Notice:&lt;/strong&gt; Ralph Orchestrator is under active development. It works today, but expect rough edges and breaking changes between releases.&lt;/p&gt; 
&lt;p&gt;v1.0.0 was ralphed into existence with little oversight and guidance. v2.0.0 is a simpler, more-structured implementation. Looking for the old version? See &lt;a href="https://github.com/mikeyobrien/ralph-orchestrator/tree/v1.2.3"&gt;v1.2.3&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/mikeyobrien/ralph-orchestrator/main/#what-is-ralph"&gt;What is Ralph?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/mikeyobrien/ralph-orchestrator/main/#features"&gt;Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/mikeyobrien/ralph-orchestrator/main/#installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/mikeyobrien/ralph-orchestrator/main/#quick-start"&gt;Quick Start&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/mikeyobrien/ralph-orchestrator/main/#configuration"&gt;Configuration&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/mikeyobrien/ralph-orchestrator/main/#presets"&gt;Presets&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/mikeyobrien/ralph-orchestrator/main/#key-concepts"&gt;Key Concepts&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/mikeyobrien/ralph-orchestrator/main/#cli-reference"&gt;CLI Reference&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/mikeyobrien/ralph-orchestrator/main/#architecture"&gt;Architecture&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/mikeyobrien/ralph-orchestrator/main/#building--testing"&gt;Building &amp;amp; Testing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/mikeyobrien/ralph-orchestrator/main/#contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/mikeyobrien/ralph-orchestrator/main/#license"&gt;License&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/mikeyobrien/ralph-orchestrator/main/#acknowledgments"&gt;Acknowledgments&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;What is Ralph?&lt;/h2&gt; 
&lt;p&gt;Ralph implements the &lt;a href="https://ghuntley.com/ralph/"&gt;Ralph Wiggum technique&lt;/a&gt; â€” autonomous task completion through continuous AI agent iteration. Unlike simple loops, Ralph v2 introduces &lt;strong&gt;hat-based orchestration&lt;/strong&gt;: specialized agent roles that coordinate through events.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;"The orchestrator is a thin coordination layer, not a platform. Agents are smart; let them do the work."&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;The Ralph Tenets&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Fresh Context Is Reliability&lt;/strong&gt; â€” Each iteration clears context. Re-read specs, plan, code every cycle.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Backpressure Over Prescription&lt;/strong&gt; â€” Don't prescribe how; create gates that reject bad work.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;The Plan Is Disposable&lt;/strong&gt; â€” Regeneration costs one planning loop. Cheap.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Disk Is State, Git Is Memory&lt;/strong&gt; â€” Files are the handoff mechanism.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Steer With Signals, Not Scripts&lt;/strong&gt; â€” Add signs, not scripts.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Let Ralph Ralph&lt;/strong&gt; â€” Sit &lt;em&gt;on&lt;/em&gt; the loop, not &lt;em&gt;in&lt;/em&gt; it.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/mikeyobrien/ralph-orchestrator/main/AGENTS.md"&gt;AGENTS.md&lt;/a&gt; for the full philosophy.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-Backend Support&lt;/strong&gt; â€” Works with Claude Code, Kiro, Gemini CLI, Codex, and Amp&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Hat System&lt;/strong&gt; â€” Specialized agent personas with distinct behaviors&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Event-Driven Coordination&lt;/strong&gt; â€” Hats communicate through typed events with glob pattern matching&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Backpressure Enforcement&lt;/strong&gt; â€” Gates that reject incomplete work (tests, lint, typecheck)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Presets Library&lt;/strong&gt; â€” 20+ pre-configured workflows for common development patterns&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Interactive TUI&lt;/strong&gt; â€” Real-time terminal UI for monitoring agent activity (experimental)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Session Recording&lt;/strong&gt; â€” Record and replay sessions for debugging and testing (experimental)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://rustup.rs/"&gt;Rust&lt;/a&gt; 1.75+&lt;/li&gt; 
 &lt;li&gt;At least one AI CLI: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://github.com/anthropics/claude-code"&gt;Claude Code&lt;/a&gt; (recommended)&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://kiro.dev/"&gt;Kiro&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/google-gemini/gemini-cli"&gt;Gemini CLI&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/openai/codex"&gt;Codex&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/sourcegraph/amp"&gt;Amp&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Via npm (Recommended)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install globally
npm install -g @ralph-orchestrator/ralph-cli

# Or run directly with npx
npx @ralph-orchestrator/ralph-cli --version
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Via Cargo&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cargo install ralph-cli
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;From Source&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/mikeyobrien/ralph-orchestrator.git
cd ralph-orchestrator
cargo build --release

# Add to PATH
export PATH="$PATH:$(pwd)/target/release"

# Or create symlink
sudo ln -s $(pwd)/target/release/ralph /usr/local/bin/ralph
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Verify Installation&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;ralph --version
ralph --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Migrating from v1 (Python)&lt;/h3&gt; 
&lt;p&gt;If you have the old Python-based Ralph v1 installed, uninstall it first to avoid conflicts:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# If installed via pip
pip uninstall ralph-orchestrator

# If installed via pipx
pipx uninstall ralph-orchestrator

# If installed via uv
uv tool uninstall ralph-orchestrator

# Verify removal
which ralph  # Should return nothing or point to new Rust version
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The v1 Python version is no longer maintained. See &lt;a href="https://github.com/mikeyobrien/ralph-orchestrator/tree/v1.2.3"&gt;v1.2.3&lt;/a&gt; for historical reference.&lt;/p&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;h3&gt;1. Initialize a Project&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Minimal config for Claude (recommended)
ralph init --backend claude

# Use a preset workflow
ralph init --preset tdd-red-green

# Combine preset with different backend
ralph init --preset spec-driven --backend kiro

# See all available presets
ralph init --list-presets
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This creates &lt;code&gt;ralph.yml&lt;/code&gt; in your current directory.&lt;/p&gt; 
&lt;h3&gt;2. Define Your Task&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Option A:&lt;/strong&gt; Create a &lt;code&gt;PROMPT.md&lt;/code&gt; file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cat &amp;gt; PROMPT.md &amp;lt;&amp;lt; 'EOF'
Build a REST API with the following endpoints:
- POST /users - Create a new user
- GET /users/:id - Get user by ID
- PUT /users/:id - Update user
- DELETE /users/:id - Delete user

Use Express.js with TypeScript. Include input validation
and proper error handling.
EOF
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Option B:&lt;/strong&gt; Pass inline prompt when running:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;ralph run -p "Add input validation to the user API endpoints"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;3. Run Ralph&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Autonomous mode (headless, default)
ralph run

# With inline prompt
ralph run -p "Implement the login endpoint with JWT authentication"

# Interactive TUI mode (experimental)
ralph run -i

# Resume interrupted session
ralph resume

# Dry run (show what would execute)
ralph run --dry-run
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Alternative: SOP-Driven Sessions&lt;/h3&gt; 
&lt;p&gt;For standalone planning and task generation (without Ralph's event loop), use these commands:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Start an interactive PDD planning session
ralph plan                           # SOP prompts for input
ralph plan "build a REST API"        # Provide idea inline
ralph plan --backend kiro "my idea"  # Use specific backend

# Generate code task files from descriptions
ralph task                           # SOP prompts for input
ralph task "add authentication"      # From description
ralph task specs/feature/plan.md     # From PDD plan file
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;These commands spawn an interactive AI session with bundled SOPs â€” perfect for one-off planning without configuring a full workflow.&lt;/p&gt; 
&lt;h2&gt;Configuration&lt;/h2&gt; 
&lt;p&gt;Ralph uses a YAML configuration file (&lt;code&gt;ralph.yml&lt;/code&gt; by default).&lt;/p&gt; 
&lt;h3&gt;Minimal Configuration&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;# ralph.yml
cli:
  backend: "claude"

event_loop:
  completion_promise: "LOOP_COMPLETE"
  max_iterations: 100
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Full Configuration Reference&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;# Event loop settings
event_loop:
  completion_promise: "LOOP_COMPLETE"  # Output that signals completion
  max_iterations: 100                   # Maximum orchestration loops
  max_runtime_seconds: 14400            # 4 hours max runtime
  idle_timeout_secs: 1800               # 30 min idle timeout
  starting_event: "task.start"          # First event published

# CLI backend settings
cli:
  backend: "claude"                     # claude, kiro, gemini, codex, amp, custom
  prompt_mode: "arg"                    # arg (CLI argument) or stdin
  experimental_tui: false               # Enable TUI mode support

# Core behaviors (always injected into prompts)
core:
  scratchpad: ".agent/scratchpad.md"    # Shared memory across iterations
  specs_dir: "./specs/"                 # Directory for specifications
  guardrails:                           # Rules injected into every prompt
    - "Fresh context each iteration - scratchpad is memory"
    - "Don't assume 'not implemented' - search first"
    - "Backpressure is law - tests/typecheck/lint must pass"

# Custom hats (omit to use default planner/builder)
hats:
  my_hat:
    name: "My Hat Name"                 # Display name
    triggers: ["some.event"]            # Events that activate this hat
    publishes: ["other.event"]          # Events this hat can emit
    instructions: |                     # Prompt instructions
      What this hat should do...
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Presets&lt;/h2&gt; 
&lt;p&gt;Presets are pre-configured workflows for common development patterns.&lt;/p&gt; 
&lt;h3&gt;Development Workflows&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Preset&lt;/th&gt; 
   &lt;th&gt;Pattern&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;feature&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Planner-Builder&lt;/td&gt; 
   &lt;td&gt;Standard feature development&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;feature-minimal&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Single hat&lt;/td&gt; 
   &lt;td&gt;Minimal feature development&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;tdd-red-green&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Test-Implement-Refactor&lt;/td&gt; 
   &lt;td&gt;TDD with red-green-refactor cycle&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;spec-driven&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Spec-Build-Verify&lt;/td&gt; 
   &lt;td&gt;Specification-first development&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;refactor&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Analyze-Plan-Execute&lt;/td&gt; 
   &lt;td&gt;Code refactoring workflow&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Debugging &amp;amp; Investigation&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Preset&lt;/th&gt; 
   &lt;th&gt;Pattern&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;debug&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Investigate-Fix-Verify&lt;/td&gt; 
   &lt;td&gt;Bug investigation and fixing&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;incident-response&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Triage-Fix-Postmortem&lt;/td&gt; 
   &lt;td&gt;Production incident handling&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;code-archaeology&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Explore-Document-Present&lt;/td&gt; 
   &lt;td&gt;Legacy code understanding&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Review &amp;amp; Quality&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Preset&lt;/th&gt; 
   &lt;th&gt;Pattern&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;review&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Analyze-Critique-Suggest&lt;/td&gt; 
   &lt;td&gt;Code review workflow&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;pr-review&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Multi-Perspective&lt;/td&gt; 
   &lt;td&gt;PR review with specialized reviewers&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;adversarial-review&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Critic-Defender&lt;/td&gt; 
   &lt;td&gt;Devil's advocate review style&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Documentation&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Preset&lt;/th&gt; 
   &lt;th&gt;Pattern&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;docs&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Write-Review-Publish&lt;/td&gt; 
   &lt;td&gt;Documentation writing&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;documentation-first&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Doc-Implement-Sync&lt;/td&gt; 
   &lt;td&gt;Doc-first development&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Specialized&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Preset&lt;/th&gt; 
   &lt;th&gt;Pattern&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;api-design&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Design-Implement-Document&lt;/td&gt; 
   &lt;td&gt;API-first development&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;migration-safety&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Analyze-Migrate-Verify&lt;/td&gt; 
   &lt;td&gt;Safe code migrations&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;performance-optimization&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Profile-Optimize-Benchmark&lt;/td&gt; 
   &lt;td&gt;Performance tuning&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;scientific-method&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Hypothesis-Experiment-Conclude&lt;/td&gt; 
   &lt;td&gt;Experimental approach&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;mob-programming&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Rotate roles&lt;/td&gt; 
   &lt;td&gt;Simulated mob programming&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;socratic-learning&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Question-Answer-Synthesize&lt;/td&gt; 
   &lt;td&gt;Learning through dialogue&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;research&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Gather-Analyze-Synthesize&lt;/td&gt; 
   &lt;td&gt;Research and analysis&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;gap-analysis&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Current-Target-Plan&lt;/td&gt; 
   &lt;td&gt;Gap identification&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Using Presets&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# List all available presets
ralph init --list-presets

# Initialize with a preset
ralph init --preset tdd-red-green

# Use preset with different backend
ralph init --preset spec-driven --backend gemini

# Override existing config
ralph init --preset debug --force
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Key Concepts&lt;/h2&gt; 
&lt;h3&gt;Hats&lt;/h3&gt; 
&lt;p&gt;Hats are specialized agent personas. Each hat has:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Triggers&lt;/strong&gt;: Events that activate this hat&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Publishes&lt;/strong&gt;: Events this hat can emit&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Instructions&lt;/strong&gt;: Prompt injected when hat is active&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;View event history:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;ralph events
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Scratchpad&lt;/h3&gt; 
&lt;p&gt;All hats share &lt;code&gt;.agent/scratchpad.md&lt;/code&gt; â€” persistent memory across iterations. This enables hats to build on previous work rather than starting fresh.&lt;/p&gt; 
&lt;p&gt;The scratchpad is the primary mechanism for:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Task tracking (with &lt;code&gt;[ ]&lt;/code&gt;, &lt;code&gt;[x]&lt;/code&gt;, &lt;code&gt;[~]&lt;/code&gt; markers)&lt;/li&gt; 
 &lt;li&gt;Context preservation between iterations&lt;/li&gt; 
 &lt;li&gt;Handoff between hats&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Backpressure&lt;/h3&gt; 
&lt;p&gt;Ralph enforces quality gates through backpressure. When a builder publishes &lt;code&gt;build.done&lt;/code&gt;, it must include evidence:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;tests: pass, lint: pass, typecheck: pass
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;CLI Reference&lt;/h2&gt; 
&lt;h3&gt;Commands&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Command&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ralph run&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Run the orchestration loop (default)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ralph resume&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Resume from existing scratchpad&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ralph plan&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Start an interactive PDD planning session&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ralph task&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Start an interactive code-task-generator session&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ralph events&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;View event history&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ralph init&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Initialize configuration file&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ralph clean&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Clean up &lt;code&gt;.agent/&lt;/code&gt; directory&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ralph emit&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Emit an event to the event log&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Global Options&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Option&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-c, --config &amp;lt;FILE&amp;gt;&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Config file path (default: &lt;code&gt;ralph.yml&lt;/code&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-v, --verbose&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Verbose output&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--color &amp;lt;MODE&amp;gt;&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Color output: &lt;code&gt;auto&lt;/code&gt;, &lt;code&gt;always&lt;/code&gt;, &lt;code&gt;never&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;&lt;code&gt;ralph run&lt;/code&gt; Options&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Option&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-p, --prompt &amp;lt;TEXT&amp;gt;&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Inline prompt text&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-P, --prompt-file &amp;lt;FILE&amp;gt;&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Prompt file path&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--max-iterations &amp;lt;N&amp;gt;&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Override max iterations&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--completion-promise &amp;lt;TEXT&amp;gt;&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Override completion trigger&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--dry-run&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Show what would execute&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-i, --interactive&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Enable TUI mode (experimental)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-a, --autonomous&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Force headless mode&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--idle-timeout &amp;lt;SECS&amp;gt;&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;TUI idle timeout (default: 30)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--record-session &amp;lt;FILE&amp;gt;&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Record session to JSONL&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-q, --quiet&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Suppress output (for CI)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;&lt;code&gt;ralph init&lt;/code&gt; Options&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Option&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--backend &amp;lt;NAME&amp;gt;&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Backend: &lt;code&gt;claude&lt;/code&gt;, &lt;code&gt;kiro&lt;/code&gt;, &lt;code&gt;gemini&lt;/code&gt;, &lt;code&gt;codex&lt;/code&gt;, &lt;code&gt;amp&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--preset &amp;lt;NAME&amp;gt;&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Use preset configuration&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--list-presets&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;List available presets&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--force&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Overwrite existing config&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;&lt;code&gt;ralph plan&lt;/code&gt; Options&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Option&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;&amp;lt;IDEA&amp;gt;&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Optional rough idea to develop (SOP prompts if not provided)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-b, --backend &amp;lt;BACKEND&amp;gt;&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Backend to use (overrides config and auto-detection)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;&lt;code&gt;ralph task&lt;/code&gt; Options&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Option&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;&amp;lt;INPUT&amp;gt;&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Optional description text or path to PDD plan file&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-b, --backend &amp;lt;BACKEND&amp;gt;&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Backend to use (overrides config and auto-detection)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Architecture&lt;/h2&gt; 
&lt;p&gt;Ralph is organized as a Cargo workspace with six crates:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Crate&lt;/th&gt; 
   &lt;th&gt;Purpose&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ralph-proto&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Protocol types: Event, Hat, Topic, Error&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ralph-core&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Business logic: EventLoop, HatRegistry, Config&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ralph-adapters&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;CLI backend integrations (Claude, Kiro, Gemini, etc.)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ralph-tui&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Terminal UI with ratatui&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ralph-cli&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Binary entry point and CLI parsing&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ralph-bench&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Benchmarking harness (dev-only)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Building &amp;amp; Testing&lt;/h2&gt; 
&lt;h3&gt;Build&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cargo build           # Debug build
cargo build --release # Release build
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Test&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Run all tests (includes smoke tests with JSONL replay)
cargo test

# Run smoke tests specifically
cargo test -p ralph-core smoke_runner

# Run Kiro-specific smoke tests
cargo test -p ralph-core kiro
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Smoke Tests&lt;/h3&gt; 
&lt;p&gt;Smoke tests use recorded JSONL fixtures instead of live API calls â€” fast, free, and deterministic.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Fixture locations:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;crates/ralph-core/tests/fixtures/basic_session.jsonl&lt;/code&gt; â€” Claude CLI session&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;crates/ralph-core/tests/fixtures/kiro/&lt;/code&gt; â€” Kiro CLI sessions&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Recording new fixtures:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Record a session
ralph run -c ralph.yml --record-session session.jsonl -p "your prompt"

# Or capture raw CLI output
claude -p "your prompt" 2&amp;gt;&amp;amp;1 | tee output.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Linting&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cargo clippy --all-targets --all-features
cargo fmt --check
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Contributions are welcome! Please:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Fork the repository&lt;/li&gt; 
 &lt;li&gt;Create a feature branch (&lt;code&gt;git checkout -b feature/amazing-feature&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Write tests for new functionality&lt;/li&gt; 
 &lt;li&gt;Ensure &lt;code&gt;cargo test&lt;/code&gt; passes&lt;/li&gt; 
 &lt;li&gt;Run &lt;code&gt;cargo clippy&lt;/code&gt; and &lt;code&gt;cargo fmt&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Commit your changes (&lt;code&gt;git commit -m 'Add amazing feature'&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Push to the branch (&lt;code&gt;git push origin feature/amazing-feature&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Open a Pull Request&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/mikeyobrien/ralph-orchestrator/main/AGENTS.md"&gt;AGENTS.md&lt;/a&gt; for development philosophy and conventions.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;MIT License â€” See &lt;a href="https://raw.githubusercontent.com/mikeyobrien/ralph-orchestrator/main/LICENSE"&gt;LICENSE&lt;/a&gt; for details.&lt;/p&gt; 
&lt;h2&gt;Acknowledgments&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://ghuntley.com/ralph/"&gt;Geoffrey Huntley&lt;/a&gt;&lt;/strong&gt; â€” Creator of the Ralph Wiggum technique&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://harper.blog/"&gt;Harper Reed&lt;/a&gt;&lt;/strong&gt; â€” Spec-driven development methodology&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/strands-agents/agent-sop"&gt;Strands Agent SOPs&lt;/a&gt;&lt;/strong&gt; â€” Natural language workflows that enable AI agents to perform complex, multi-step tasks with consistency and reliability.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://ratatui.rs/"&gt;ratatui&lt;/a&gt;&lt;/strong&gt; â€” Terminal UI framework&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://crates.io/crates/portable-pty"&gt;portable-pty&lt;/a&gt;&lt;/strong&gt; â€” Cross-platform PTY support&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;em&gt;"I'm learnding!" - Ralph Wiggum&lt;/em&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;a href="https://www.star-history.com/#mikeyobrien/ralph-orchestrator&amp;amp;type=date&amp;amp;legend=top-left"&gt;&lt;img src="https://api.star-history.com/svg?repos=mikeyobrien/ralph-orchestrator&amp;amp;type=date&amp;amp;legend=top-left" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>