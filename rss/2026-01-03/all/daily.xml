<rss version="2.0">
  <channel>
    <title>GitHub All Languages Daily Trending</title>
    <description>Daily Trending of All Languages in GitHub</description>
    <pubDate>Fri, 02 Jan 2026 01:31:52 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>organicmaps/organicmaps</title>
      <link>https://github.com/organicmaps/organicmaps</link>
      <description>&lt;p&gt;üçÉ Organic Maps is a free Android &amp; iOS offline maps app for travelers, tourists, hikers, and cyclists. It uses crowd-sourced OpenStreetMap data and is developed with love by the community. No ads, no tracking, no data collection, no crapware. Please donate to support the development!&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/organicmaps/organicmaps/master/qt/res/logo.png" height="100" /&gt; 
&lt;/div&gt; 
&lt;h1 align="center"&gt;Organic Maps&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;Organic Maps&lt;/strong&gt; is a privacy-first offline maps &amp;amp; GPS app for hiking, cycling, biking, and driving. Absolutely free. No ads. No tracking. Developed with love by the open-source community. Powered by &lt;a href="https://www.openstreetmap.org"&gt;OpenStreetMap&lt;/a&gt; data.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://apps.apple.com/app/organic-maps/id1567437057"&gt;&lt;img src="https://raw.githubusercontent.com/organicmaps/organicmaps/master/docs/badges/apple-appstore.png" alt="App Store" width="160" /&gt;&lt;/a&gt; &lt;a href="https://play.google.com/store/apps/details?id=app.organicmaps"&gt;&lt;img src="https://raw.githubusercontent.com/organicmaps/organicmaps/master/docs/badges/google-play.png" alt="Google Play" width="160" /&gt;&lt;/a&gt; &lt;a href="https://appgallery.huawei.com/#/app/C104325611"&gt;&lt;img src="https://raw.githubusercontent.com/organicmaps/organicmaps/master/docs/badges/huawei-appgallery.png" alt="AppGallery" width="160" /&gt;&lt;/a&gt; &lt;a href="https://github.com/organicmaps/organicmaps/wiki/Installing-Organic-Maps-from-GitHub-using-Obtainium"&gt;&lt;img src="https://raw.githubusercontent.com/organicmaps/organicmaps/master/docs/badges/obtainium.png" alt="Obtainium" width="160" /&gt;&lt;/a&gt; &lt;a href="https://f-droid.org/en/packages/app.organicmaps/"&gt;&lt;img src="https://raw.githubusercontent.com/organicmaps/organicmaps/master/docs/badges/fdroid.png" alt="F-Droid" width="160" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p float="left"&gt; &lt;img src="https://raw.githubusercontent.com/organicmaps/organicmaps/master/android/app/src/fdroid/play/listings/en-US/graphics/phone-screenshots/1.jpg" width="400" /&gt; &lt;img src="https://raw.githubusercontent.com/organicmaps/organicmaps/master/android/app/src/fdroid/play/listings/en-US/graphics/phone-screenshots/2.jpg" width="400" /&gt; &lt;img src="https://raw.githubusercontent.com/organicmaps/organicmaps/master/android/app/src/fdroid/play/listings/en-US/graphics/phone-screenshots/3.jpg" width="400" /&gt; &lt;img src="https://raw.githubusercontent.com/organicmaps/organicmaps/master/android/app/src/fdroid/play/listings/en-US/graphics/phone-screenshots/4.jpg" width="400" /&gt; &lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;p&gt;Organic Maps is the ultimate companion app for travellers, tourists, hikers, and cyclists:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Detailed offline maps with places that don't exist on other maps, thanks to &lt;a href="https://openstreetmap.org"&gt;OpenStreetMap&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Cycling routes, hiking trails, and walking paths&lt;/li&gt; 
 &lt;li&gt;Contour lines, elevation profiles, peaks, and slopes&lt;/li&gt; 
 &lt;li&gt;Turn-by-turn walking, cycling, and car navigation with voice guidance&lt;/li&gt; 
 &lt;li&gt;Fast offline search on the map&lt;/li&gt; 
 &lt;li&gt;Bookmarks and tracks import and export in KML, KMZ &amp;amp; GPX formats&lt;/li&gt; 
 &lt;li&gt;Dark Mode to protect your eyes&lt;/li&gt; 
 &lt;li&gt;Countries and regions don't take a lot of space&lt;/li&gt; 
 &lt;li&gt;Free and open-source&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Why Organic?&lt;/h2&gt; 
&lt;p&gt;Organic Maps is pure and organic, made with love:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Respects your privacy&lt;/li&gt; 
 &lt;li&gt;Saves your battery&lt;/li&gt; 
 &lt;li&gt;No unexpected mobile data charges&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Organic Maps is free from trackers and other bad stuff:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;No ads&lt;/li&gt; 
 &lt;li&gt;No tracking&lt;/li&gt; 
 &lt;li&gt;No data collection&lt;/li&gt; 
 &lt;li&gt;No phoning home&lt;/li&gt; 
 &lt;li&gt;No annoying registration&lt;/li&gt; 
 &lt;li&gt;No mandatory tutorials&lt;/li&gt; 
 &lt;li&gt;No noisy email spam&lt;/li&gt; 
 &lt;li&gt;No push notifications&lt;/li&gt; 
 &lt;li&gt;No crapware&lt;/li&gt; 
 &lt;li&gt;&lt;del&gt;No pesticides&lt;/del&gt; Purely organic!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The Android application is verified by the &lt;a href="https://reports.exodus-privacy.eu.org/en/reports/app.organicmaps/latest/"&gt;Exodus Privacy Project:&lt;/a&gt;&lt;/p&gt;
&lt;a href="https://reports.exodus-privacy.eu.org/en/reports/app.organicmaps/latest/"&gt; &lt;img src="https://raw.githubusercontent.com/organicmaps/organicmaps/master/docs/privacy/exodus.png" width="400" /&gt; &lt;/a&gt; 
&lt;p&gt;The iOS application is verified by &lt;a href="https://ios.trackercontrol.org/analysis/app.organicmaps"&gt;TrackerControl for iOS:&lt;/a&gt;&lt;/p&gt;
&lt;a href="https://ios.trackercontrol.org/analysis/app.organicmaps"&gt; &lt;img src="https://raw.githubusercontent.com/organicmaps/organicmaps/master/docs/privacy/trackercontrol-ios.png" width="400" /&gt; &lt;/a&gt; 
&lt;br /&gt; 
&lt;p&gt;Organic Maps doesn't request excessive permissions to spy on you:&lt;/p&gt; 
&lt;p float="left"&gt; &lt;img src="https://raw.githubusercontent.com/organicmaps/organicmaps/master/docs/privacy/om.jpg" width="400" /&gt; &lt;img src="https://raw.githubusercontent.com/organicmaps/organicmaps/master/docs/privacy/mm.jpg" width="400" /&gt; &lt;/p&gt; 
&lt;p&gt;At Organic Maps, we believe that privacy is a fundamental human right:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Organic Maps is an indie community-driven open-source project&lt;/li&gt; 
 &lt;li&gt;We protect your privacy from Big Tech's prying eyes&lt;/li&gt; 
 &lt;li&gt;Stay safe no matter where you are&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Reject surveillance - embrace your freedom.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/organicmaps/organicmaps/master/#install"&gt;&lt;strong&gt;Give Organic Maps a try!&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Who is paying for the development?&lt;/h2&gt; 
&lt;p&gt;The app is free for everyone, so we rely on donations. Please donate at &lt;a href="https://organicmaps.app/donate"&gt;organicmaps.app/donate&lt;/a&gt; to support us!&lt;/p&gt; 
&lt;p&gt;Beloved institutional sponsors below have provided targeted grants to cover some infrastructure costs and fund development of new selected features:&lt;/p&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td&gt; &lt;a href="https://nlnet.nl/"&gt;&lt;img src="https://raw.githubusercontent.com/organicmaps/organicmaps/master/docs/sponsors/nlnet.svg?sanitize=true" alt="The NLnet Foundation" width="200px" /&gt;&lt;/a&gt; &lt;/td&gt; 
   &lt;td&gt; &lt;a href="https://github.com/organicmaps/organicmaps/milestone/7"&gt;The Search &amp;amp; Fonts improvement project&lt;/a&gt; has been &lt;a href="https://nlnet.nl/project/OrganicMaps/"&gt;funded&lt;/a&gt; through NGI0 Entrust Fund. &lt;a href="https://nlnet.nl/entrust/"&gt;NGI0 Entrust Fund&lt;/a&gt; is established by the &lt;a href="https://nlnet.nl/"&gt;NLnet Foundation&lt;/a&gt; with financial support from the European Commission's &lt;a href="https://www.ngi.eu/"&gt;Next Generation Internet programme&lt;/a&gt;, under the aegis of DG Communications Networks, Content and Technology under grant agreement No 101069594. &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt; &lt;a href="https://summerofcode.withgoogle.com/"&gt;&lt;img src="https://raw.githubusercontent.com/organicmaps/organicmaps/master/docs/sponsors/gsoc.svg?sanitize=true" alt="Google Summer of Code" width="200px" /&gt;&lt;/a&gt; &lt;/td&gt; 
   &lt;td&gt; &lt;a href="https://summerofcode.withgoogle.com/"&gt;Google&lt;/a&gt; backed 5 student's projects in the Google Summer of Code program during &lt;a href="https://summerofcode.withgoogle.com/programs/2022/organizations/organic-maps"&gt;2022&lt;/a&gt; and &lt;a href="https://summerofcode.withgoogle.com/programs/2023/organizations/organic-maps"&gt;2023&lt;/a&gt; programs. Noteworthy projects included Android Auto and Wikipedia Dump Extractor. &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt; &lt;a href="https://www.mythic-beasts.com/"&gt;&lt;img src="https://raw.githubusercontent.com/organicmaps/organicmaps/master/docs/sponsors/mythic-beasts.png" alt="Mythic Beasts" width="200px" /&gt;&lt;/a&gt; &lt;/td&gt; 
   &lt;td&gt; &lt;a href="https://www.mythic-beasts.com/"&gt;Mythic Beasts&lt;/a&gt; ISP &lt;a href="https://www.mythic-beasts.com/blog/2021/10/06/improving-the-world-bit-by-expensive-bit/"&gt;provides us&lt;/a&gt; two virtual servers with 400 TB/month of free bandwidth to host and serve maps downloads and updates. &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt; &lt;a href="https://44plus.vn"&gt;&lt;img src="https://raw.githubusercontent.com/organicmaps/organicmaps/master/docs/sponsors/44plus.svg?sanitize=true" alt="44+ Technologies" width="200px" /&gt;&lt;/a&gt; &lt;/td&gt; 
   &lt;td&gt; &lt;a href="https://44plus.vn"&gt;44+ Technologies&lt;/a&gt; is &lt;a href="https://44plus.vn/organicmaps"&gt;providing us &lt;/a&gt;with a free dedicated server worth around $12,000/year to serve maps across Vietnam &amp;amp; Southeast Asia. &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt; &lt;a href="https://futo.org"&gt;&lt;img src="https://raw.githubusercontent.com/organicmaps/organicmaps/master/docs/sponsors/futo.svg?sanitize=true" alt="FUTO" width="200px" /&gt;&lt;/a&gt; &lt;/td&gt; 
   &lt;td&gt; &lt;a href="https://futo.org"&gt;FUTO&lt;/a&gt; has &lt;a href="https://www.youtube.com/watch?v=fJJclgBHrEw"&gt;awarded $1000 micro-grant&lt;/a&gt; to Organic Maps in February 2023. &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;p&gt;The majority of all expenses have been funded by founders of the project since its inception. The project is far from achieving any sort of financial sustainability. The current level of voluntary donations falls significantly short of covering efforts needed to sustain the app. Any new developments of features are beyond the scope of possibility due to the absence of the necessary financial resources.&lt;/p&gt; 
&lt;p&gt;Please consider &lt;a href="https://organicmaps.app/donate"&gt;donating&lt;/a&gt; if you want to see this open-source project thriving, not dying. There are &lt;a href="https://raw.githubusercontent.com/organicmaps/organicmaps/master/#contributing"&gt;other ways how to support the project&lt;/a&gt;. No coding skills required.&lt;/p&gt; 
&lt;h2&gt;Copyrights&lt;/h2&gt; 
&lt;p&gt;Licensed under the Apache License, Version 2.0. See &lt;a href="https://github.com/organicmaps/organicmaps/raw/master/LICENSE"&gt;LICENSE&lt;/a&gt;, &lt;a href="https://github.com/organicmaps/organicmaps/raw/master/NOTICE"&gt;NOTICE&lt;/a&gt;, &lt;a href="http://htmlpreview.github.io/?https://github.com/organicmaps/organicmaps/raw/master/data/copyright.html"&gt;data/copyright.html&lt;/a&gt; and &lt;a href="https://github.com/organicmaps/organicmaps/raw/master/.reuse/dep5"&gt;.reuse/dep5&lt;/a&gt;, for more information.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://api.reuse.software/info/github.com/organicmaps/organicmaps"&gt;&lt;img src="https://api.reuse.software/badge/github.com/organicmaps/organicmaps" alt="REUSE status" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Governance&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/organicmaps/organicmaps/master/docs/GOVERNANCE.md"&gt;docs/GOVERNANCE.md&lt;/a&gt;.&lt;/p&gt; 
&lt;a name="contributing"&gt; &lt;h2&gt;Contributing&lt;/h2&gt; &lt;/a&gt;
&lt;p&gt;&lt;a name="contributing"&gt;If you want to build the project, check &lt;/a&gt;&lt;a href="https://raw.githubusercontent.com/organicmaps/organicmaps/master/docs/INSTALL.md"&gt;docs/INSTALL.md&lt;/a&gt;. If you want to help the project, see &lt;a href="https://raw.githubusercontent.com/organicmaps/organicmaps/master/docs/CONTRIBUTING.md"&gt;docs/CONTRIBUTING.md&lt;/a&gt;. You can &lt;a href="https://organicmaps.app/support-us/"&gt;help in many ways&lt;/a&gt;, the ability to code is not necessary.&lt;/p&gt; 
&lt;h2&gt;Beta&lt;/h2&gt; 
&lt;p&gt;Please join our beta program, suggest your features, and report bugs:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://testflight.apple.com/join/lrKCl08I"&gt;iOS Beta (TestFlight)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://appdistribution.firebase.dev/i/f3e918f9abc40c9c"&gt;Android Beta (Firebase)&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Feedback&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Rate us on the &lt;a href="https://apps.apple.com/app/organic-maps/id1567437057"&gt;App Store&lt;/a&gt; and &lt;a href="https://play.google.com/store/apps/details?id=app.organicmaps"&gt;Google Play&lt;/a&gt;&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Star us on Github&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;Report bugs or issues to &lt;a href="https://github.com/organicmaps/organicmaps/issues"&gt;the issue tracker&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Subscribe to our &lt;a href="https://t.me/OrganicMapsApp"&gt;Telegram Channel&lt;/a&gt; or to the &lt;a href="https://matrix.to/#/#organicmaps:matrix.org"&gt;[matrix] space&lt;/a&gt; for updates.&lt;/li&gt; 
 &lt;li&gt;Join our &lt;a href="https://t.me/OrganicMaps"&gt;Telegram Group&lt;/a&gt; to discuss with other users. 
  &lt;ul&gt; 
   &lt;li&gt;–ü—Ä–∏—Å–æ–µ–¥–∏–Ω—è–π—Ç–µ—Å—å –∫ –Ω–∞—à–µ–π &lt;a href="https://t.me/OrganicMapsRu"&gt;—Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω–æ–π –≥—Ä—É–ø–ø–µ –≤ Telegram&lt;/a&gt; –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –∏ –ø–æ–º–æ—â–∏.&lt;/li&gt; 
   &lt;li&gt;Diƒüer kullanƒ±cƒ±larla tartƒ±≈ümak i√ßin &lt;a href="https://t.me/OrganicMapsTR"&gt;Telegram Grubumuza&lt;/a&gt; katƒ±lƒ±n.&lt;/li&gt; 
   &lt;li&gt;Rejoignez notre groupe &lt;a href="https://t.me/OrganicMapsFR"&gt;Telegram&lt;/a&gt; pour obtenir de l'aide.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Contact us by &lt;a href="mailto:hello@organicmaps.app"&gt;email&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Follow our updates in &lt;a href="https://fosstodon.org/@organicmaps"&gt;Mastodon&lt;/a&gt;, &lt;a href="https://facebook.com/OrganicMaps"&gt;Facebook&lt;/a&gt;, &lt;a href="https://x.com/OrganicMapsApp"&gt;X (Twitter)&lt;/a&gt;, &lt;a href="https://instagram.com/organicmaps.app/"&gt;Instagram&lt;/a&gt;. 
  &lt;ul&gt; 
   &lt;li&gt;G√ºncellemelerimizi &lt;a href="https://instagram.com/organicmapstr/"&gt;Instagram&lt;/a&gt; √ºzerinden takip edin.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The Organic Maps community abides by the CNCF &lt;a href="https://github.com/organicmaps/organicmaps/raw/master/docs/CODE_OF_CONDUCT.md"&gt;code of conduct&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Organic Maps is licensed under the &lt;a href="https://raw.githubusercontent.com/organicmaps/organicmaps/master/LICENSE"&gt;Apache License 2.0&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Binary map data files (&lt;code&gt;.mwm&lt;/code&gt;) are provided under a separate license. See &lt;code&gt;DATA_LICENSE.txt&lt;/code&gt; for details.&lt;/p&gt; 
&lt;h3&gt;Attribution for forks and derivative apps based on Organic Maps&lt;/h3&gt; 
&lt;p&gt;If you use Organic Maps source code or its user interface in your project, please include a visible, human-readable mention of the ‚ÄúOrganic Maps Project‚Äù and a clickable link to &lt;a href="https://organicmaps.app"&gt;https://organicmaps.app&lt;/a&gt;. To respect the work of all project contributors and to comply with license attribution terms, this notice should appear in user-visible locations, such as the product‚Äôs ‚ÄúAbout‚Äù and ‚ÄúMain Menu‚Äù screens.&lt;/p&gt; 
&lt;h3&gt;ü§ù White-label&lt;/h3&gt; 
&lt;p&gt;For inquiries about white-labeling or using our servers for your products, please contact us in advance at:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="mailto:legal@organicmaps.app"&gt;legal@organicmaps.app&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Thank you!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>HandsOnLLM/Hands-On-Large-Language-Models</title>
      <link>https://github.com/HandsOnLLM/Hands-On-Large-Language-Models</link>
      <description>&lt;p&gt;Official code repo for the O'Reilly Book - "Hands-On Large Language Models"&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Hands-On Large Language Models&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://www.linkedin.com/in/jalammar/"&gt;&lt;img src="https://img.shields.io/badge/Follow%20Jay-blue.svg?logo=linkedin" /&gt;&lt;/a&gt; &lt;a href="https://www.linkedin.com/in/mgrootendorst/"&gt;&lt;img src="https://img.shields.io/badge/Follow%20Maarten-blue.svg?logo=linkedin" /&gt;&lt;/a&gt; &lt;a href="https://www.deeplearning.ai/short-courses/how-transformer-llms-work/?utm_campaign=handsonllm-launch&amp;amp;utm_medium=partner"&gt;&lt;img src="https://img.shields.io/badge/DeepLearning.AI%20Course-NEW!-&amp;amp;labelColor=black&amp;amp;color=red.svg?logo=data:image/svg%2bxml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAuMDAwMzY1MjgxIC0wLjAwMDE0MDE0MiAzMy4yOSAzMy4xNSI+Cgk8cGF0aCBkPSJNMTYuNjQzIDMzLjE0NWMtMy4yOTIgMC02LjUxLS45NzItOS4yNDYtMi43OTNhMTYuNTg4IDE2LjU4OCAwIDAxLTYuMTMtNy40MzhBMTYuNTA3IDE2LjUwNyAwIDAxLjMyIDEzLjM0YTE2LjU1IDE2LjU1IDAgMDE0LjU1NS04LjQ4NUExNi42NjUgMTYuNjY1IDAgMDExMy4zOTYuMzE4YTE2LjcxIDE2LjcxIDAgMDE5LjYxNi45NDQgMTYuNjI4IDE2LjYyOCAwIDAxNy40NyA2LjEwMyAxNi41MjIgMTYuNTIyIDAgMDEyLjgwNCA5LjIwN2MwIDQuMzk2LTEuNzUzIDguNjEtNC44NzQgMTEuNzE5YTE2LjY4IDE2LjY4IDAgMDEtMTEuNzY5IDQuODU0em0uMTI1LTYuNjI4YzYuOTA2IDAgMTIuNTE3LTUuNjk4IDEyLjUxNy0xMi43MyAwLTcuMDMtNS42MS0xMi43MjUtMTIuNTE3LTEyLjcyNS02LjkwNiAwLTEyLjUxNyA1LjY5OC0xMi41MTcgMTIuNzI1IDAgNy4wMjcgNS42MTEgMTIuNzMgMTIuNTE3IDEyLjczem0tLjEyNS0yLjkxOGMtNi4yODkgMC0xMS4zODYtNC45MjUtMTEuMzg2LTExLjAwMkM1LjI1NyA2LjUyIDEwLjM2IDEuNTkgMTYuNjQzIDEuNTljNi4yODQgMCAxMS4zODYgNC45MyAxMS4zODYgMTEuMDA3cy01LjA5NyAxMS4wMDItMTEuMzg2IDExLjAwMnptLS4yNDItNC41MDhjNC43NyAwIDguNjMzLTMuNjc5IDguNjMzLTguMjE4IDAtNC41MzgtMy44ODUtOC4yMjEtOC42MzMtOC4yMjEtNC43NDcgMC04LjYzMiAzLjY3OS04LjYzMiA4LjIyMSAwIDQuNTQzIDMuODg1IDguMjE4IDguNjMyIDguMjE4eiIgZmlsbD0iI0ZENEE2MSIvPgo8L3N2Zz4=" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Welcome! In this repository you will find the code for all examples throughout the book &lt;a href="https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961"&gt;Hands-On Large Language Models&lt;/a&gt; written by &lt;a href="https://www.linkedin.com/in/jalammar/"&gt;Jay Alammar&lt;/a&gt; and &lt;a href="https://www.linkedin.com/in/mgrootendorst/"&gt;Maarten Grootendorst&lt;/a&gt; which we playfully dubbed: &lt;br /&gt;&lt;/p&gt; 
&lt;p align="center"&gt;&lt;b&gt;&lt;i&gt;"The Illustrated LLM Book"&lt;/i&gt;&lt;/b&gt;&lt;/p&gt; 
&lt;p&gt;Through the visually educational nature of this book and with &lt;strong&gt;almost 300 custom made figures&lt;/strong&gt;, learn the practical tools and concepts you need to use Large Language Models today!&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961"&gt;&lt;img src="https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/images/book_cover.png" width="50%" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;p&gt;The book is available on:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961"&gt;Amazon&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.shroffpublishers.com/books/computer-science/large-language-models/9789355425522/"&gt;Shroff Publishers (India)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.oreilly.com/library/view/hands-on-large-language/9781098150952/"&gt;O'Reilly&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.amazon.com/Hands-Large-Language-Models-Alammar-ebook/dp/B0DGZ46G88/ref=tmm_kin_swatch_0?_encoding=UTF8&amp;amp;qid=&amp;amp;sr="&gt;Kindle&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.barnesandnoble.com/w/hands-on-large-language-models-jay-alammar/1145185960"&gt;Barnes and Noble&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.goodreads.com/book/show/210408850-hands-on-large-language-models"&gt;Goodreads&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;p&gt;We advise to run all examples through Google Colab for the easiest setup. Google Colab allows you to use a T4 GPU with 16GB of VRAM for free. All examples were mainly built and tested using Google Colab, so it should be the most stable platform. However, any other cloud provider should work.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Chapter&lt;/th&gt; 
   &lt;th&gt;Notebook&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Chapter 1: Introduction to Language Models&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/chapter01/Chapter%201%20-%20Introduction%20to%20Language%20Models.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Chapter 2: Tokens and Embeddings&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/chapter02/Chapter%202%20-%20Tokens%20and%20Token%20Embeddings.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Chapter 3: Looking Inside Transformer LLMs&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/chapter03/Chapter%203%20-%20Looking%20Inside%20LLMs.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Chapter 4: Text Classification&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/chapter04/Chapter%204%20-%20Text%20Classification.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Chapter 5: Text Clustering and Topic Modeling&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/chapter05/Chapter%205%20-%20Text%20Clustering%20and%20Topic%20Modeling.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Chapter 6: Prompt Engineering&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/chapter06/Chapter%206%20-%20Prompt%20Engineering.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Chapter 7: Advanced Text Generation Techniques and Tools&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/chapter07/Chapter%207%20-%20Advanced%20Text%20Generation%20Techniques%20and%20Tools.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Chapter 8: Semantic Search and Retrieval-Augmented Generation&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/chapter08/Chapter%208%20-%20Semantic%20Search.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Chapter 9: Multimodal Large Language Models&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/chapter09/Chapter%209%20-%20Multimodal%20Large%20Language%20Models.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Chapter 10: Creating Text Embedding Models&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/chapter10/Chapter%2010%20-%20Creating%20Text%20Embedding%20Models.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Chapter 11: Fine-tuning Representation Models for Classification&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/chapter11/Chapter%2011%20-%20Fine-Tuning%20BERT.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Chapter 12: Fine-tuning Generation Models&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/chapter12/Chapter%2012%20-%20Fine-tuning%20Generation%20Models.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] You can check the &lt;a href="https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/.setup/"&gt;setup&lt;/a&gt; folder for a quick-start guide to install all packages locally and you can check the &lt;a href="https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/.setup/conda/"&gt;conda&lt;/a&gt; folder for a complete guide on how to setup your environment, including conda and PyTorch installation. Note that the depending on your OS, Python version, and dependencies your results might be slightly differ. However, they should this be similar to the examples in the book.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Reviews&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;"&lt;em&gt;Jay and Maarten have continued their tradition of providing beautifully illustrated and insightful descriptions of complex topics in their new book. Bolstered with working code, timelines, and references to key papers, their book is a valuable resource for anyone looking to understand the main techniques behind how Large Language Models are built.&lt;/em&gt;"&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Andrew Ng&lt;/strong&gt; - founder of &lt;a href="https://www.deeplearning.ai/"&gt;DeepLearning.AI&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr /&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;"&lt;em&gt;This is an exceptional guide to the world of language models and their practical applications in industry. Its highly-visual coverage of generative, representational, and retrieval applications of language models empowers readers to quickly understand, use, and refine LLMs. Highly recommended!&lt;/em&gt;"&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Nils Reimers&lt;/strong&gt; - Director of Machine Learning at Cohere | creator of &lt;a href="https://github.com/UKPLab/sentence-transformers"&gt;sentence-transformers&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr /&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;"&lt;em&gt;I can‚Äôt think of another book that is more important to read right now. On every single page, I learned something that is critical to success in this era of language models.&lt;/em&gt;"&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Josh Starmer&lt;/strong&gt; - &lt;a href="https://www.youtube.com/channel/UCtYLUTtgS3k1Fg4y5tAhLbw"&gt;StatQuest&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr /&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;"&lt;em&gt;If you‚Äôre looking to get up to speed in everything regarding LLMs, look no further! In this wonderful book, Jay and Maarten will take you from zero to expert in the history and latest advances in large language models. With very intuitive explanations, great real-life examples, clear illustrations, and comprehensive code labs, this book lifts the curtain on the complexities of transformer models, tokenizers, semantic search, RAG, and many other cutting-edge technologies. A must read for anyone interested in the latest AI technology!&lt;/em&gt;"&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Luis Serrano, PhD&lt;/strong&gt; - Founder and CEO of &lt;a href="https://www.youtube.com/@SerranoAcademy"&gt;Serrano Academy&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr /&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;"&lt;em&gt;Hands-On Large Language Models brings clarity and practical examples to cut through the hype of AI. It provides a wealth of great diagrams and visual aids to supplement the clear explanations. The worked examples and code make concrete what other books leave abstract. The book starts with simple introductory beginnings, and steadily builds in scope. By the final chapters, you will be fine-tuning and building your own large language models with confidence.&lt;/em&gt;"&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Leland McInnes&lt;/strong&gt; - Researcher at the Tutte Institute for Mathematics and Computing | creator of &lt;a href="https://github.com/lmcinnes/umap"&gt;UMAP&lt;/a&gt; and &lt;a href="https://github.com/scikit-learn-contrib/hdbscan"&gt;HDBSCAN&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr /&gt; 
&lt;h2&gt;&lt;a href="https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/bonus/"&gt;Bonus content!&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;We attempted to put as much information into the book without it being overwhelming. However, even with a 400-page book there is still much to discover!&lt;/p&gt; 
&lt;p&gt;We continue to create more guides that compliment the book and go more in-depth into new and &lt;a href="https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/(bonus/)"&gt;exciting topics&lt;/a&gt;:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;&lt;a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mamba-and-state"&gt;A Visual Guide to Mamba&lt;/a&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization"&gt;A Visual Guide to Quantization&lt;/a&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;a href="https://jalammar.github.io/illustrated-stable-diffusion/"&gt;The Illustrated Stable Diffusion&lt;/a&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/images/mamba.png" alt="" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/images/quant.png" alt="" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/images/diffusion.png" alt="" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;&lt;a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mixture-of-experts"&gt;A Visual Guide to Mixture of Experts&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;&lt;a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-reasoning-llms"&gt;A Visual Guide to Reasoning LLMs&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;&lt;a href="https://newsletter.languagemodels.co/p/the-illustrated-deepseek-r1"&gt;The Illustrated DeepSeek-R1&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/images/moe.png" alt="" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/images/reasoning.png" alt="" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/images/deepseek.png" alt="" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;Please consider citing the book if you consider it useful for your research:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@book{hands-on-llms-book,
  author       = {Jay Alammar and Maarten Grootendorst},
  title        = {Hands-On Large Language Models},
  publisher    = {O'Reilly},
  year         = {2024},
  isbn         = {978-1098150969},
  url          = {https://www.oreilly.com/library/view/hands-on-large-language/9781098150952/},
  github       = {https://github.com/HandsOnLLM/Hands-On-Large-Language-Models}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>DayuanJiang/next-ai-draw-io</title>
      <link>https://github.com/DayuanJiang/next-ai-draw-io</link>
      <description>&lt;p&gt;A next.js web application that integrates AI capabilities with draw.io diagrams. This app allows you to create, modify, and enhance diagrams through natural language commands and AI-assisted visualization.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Next AI Draw.io&lt;/h1&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;strong&gt;AI-Powered Diagram Creation Tool - Chat, Draw, Visualize&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;English | &lt;a href="https://raw.githubusercontent.com/DayuanJiang/next-ai-draw-io/main/docs/cn/README_CN.md"&gt;‰∏≠Êñá&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/DayuanJiang/next-ai-draw-io/main/docs/ja/README_JA.md"&gt;Êó•Êú¨Ë™û&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://next-ai-drawio.jiang.jp/"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/15449" alt="TrendShift" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://opensource.org/licenses/Apache-2.0"&gt;&lt;img src="https://img.shields.io/badge/License-Apache%202.0-blue.svg?sanitize=true" alt="License: Apache 2.0" /&gt;&lt;/a&gt; &lt;a href="https://nextjs.org/"&gt;&lt;img src="https://img.shields.io/badge/Next.js-16.x-black" alt="Next.js" /&gt;&lt;/a&gt; &lt;a href="https://react.dev/"&gt;&lt;img src="https://img.shields.io/badge/React-19.x-61dafb" alt="React" /&gt;&lt;/a&gt; &lt;a href="https://github.com/sponsors/DayuanJiang"&gt;&lt;img src="https://img.shields.io/badge/Sponsor-%E2%9D%A4-ea4aaa" alt="Sponsor" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://next-ai-drawio.jiang.jp/"&gt;&lt;img src="https://raw.githubusercontent.com/DayuanJiang/next-ai-draw-io/main/public/live-demo-button.svg?sanitize=true" alt="Live Demo" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;A Next.js web application that integrates AI capabilities with draw.io diagrams. Create, modify, and enhance diagrams through natural language commands and AI-assisted visualization.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Note: Thanks to &lt;img src="https://raw.githubusercontent.com/DayuanJiang/next-ai-draw-io/main/public/doubao-color.png" alt="" height="20" /&gt; &lt;a href="https://console.volcengine.com/ark/region:ark+cn-beijing/overview?briefPage=0&amp;amp;briefType=introduce&amp;amp;type=new&amp;amp;utm_campaign=doubao&amp;amp;utm_content=aidrawio&amp;amp;utm_medium=github&amp;amp;utm_source=coopensrc&amp;amp;utm_term=project"&gt;ByteDance Doubao&lt;/a&gt; sponsorship, the demo site now uses the powerful K2-thinking model!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/9d60a3e8-4a1c-4b5e-acbb-26af2d3eabd1"&gt;https://github.com/user-attachments/assets/9d60a3e8-4a1c-4b5e-acbb-26af2d3eabd1&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DayuanJiang/next-ai-draw-io/main/#next-ai-drawio"&gt;Next AI Draw.io&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DayuanJiang/next-ai-draw-io/main/#table-of-contents"&gt;Table of Contents&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DayuanJiang/next-ai-draw-io/main/#examples"&gt;Examples&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DayuanJiang/next-ai-draw-io/main/#features"&gt;Features&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DayuanJiang/next-ai-draw-io/main/#mcp-server-preview"&gt;MCP Server (Preview)&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DayuanJiang/next-ai-draw-io/main/#claude-code-cli"&gt;Claude Code CLI&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DayuanJiang/next-ai-draw-io/main/#getting-started"&gt;Getting Started&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DayuanJiang/next-ai-draw-io/main/#try-it-online"&gt;Try it Online&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DayuanJiang/next-ai-draw-io/main/#desktop-application"&gt;Desktop Application&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DayuanJiang/next-ai-draw-io/main/#run-with-docker"&gt;Run with Docker&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DayuanJiang/next-ai-draw-io/main/#installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DayuanJiang/next-ai-draw-io/main/#deployment"&gt;Deployment&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DayuanJiang/next-ai-draw-io/main/#deploy-to-edgeone-pages"&gt;Deploy to EdgeOne Pages&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DayuanJiang/next-ai-draw-io/main/#deploy-on-vercel-recommended"&gt;Deploy on Vercel (Recommended)&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DayuanJiang/next-ai-draw-io/main/#deploy-on-cloudflare-workers"&gt;Deploy on Cloudflare Workers&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DayuanJiang/next-ai-draw-io/main/#multi-provider-support"&gt;Multi-Provider Support&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DayuanJiang/next-ai-draw-io/main/#how-it-works"&gt;How It Works&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DayuanJiang/next-ai-draw-io/main/#support--contact"&gt;Support &amp;amp; Contact&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DayuanJiang/next-ai-draw-io/main/#star-history"&gt;Star History&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;p&gt;Here are some example prompts and their generated diagrams:&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;table width="100%"&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td colspan="2" valign="top" align="center"&gt; &lt;strong&gt;Animated transformer connectors&lt;/strong&gt;&lt;br /&gt; &lt;p&gt;&lt;strong&gt;Prompt:&lt;/strong&gt; Give me a **animated connector** diagram of transformer's architecture.&lt;/p&gt; &lt;img src="https://raw.githubusercontent.com/DayuanJiang/next-ai-draw-io/main/public/animated_connectors.svg?sanitize=true" alt="Transformer Architecture with Animated Connectors" width="480" /&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td width="50%" valign="top"&gt; &lt;strong&gt;GCP architecture diagram&lt;/strong&gt;&lt;br /&gt; &lt;p&gt;&lt;strong&gt;Prompt:&lt;/strong&gt; Generate a GCP architecture diagram with **GCP icons**. In this diagram, users connect to a frontend hosted on an instance.&lt;/p&gt; &lt;img src="https://raw.githubusercontent.com/DayuanJiang/next-ai-draw-io/main/public/gcp_demo.svg?sanitize=true" alt="GCP Architecture Diagram" width="480" /&gt; &lt;/td&gt; 
    &lt;td width="50%" valign="top"&gt; &lt;strong&gt;AWS architecture diagram&lt;/strong&gt;&lt;br /&gt; &lt;p&gt;&lt;strong&gt;Prompt:&lt;/strong&gt; Generate a AWS architecture diagram with **AWS icons**. In this diagram, users connect to a frontend hosted on an instance.&lt;/p&gt; &lt;img src="https://raw.githubusercontent.com/DayuanJiang/next-ai-draw-io/main/public/aws_demo.svg?sanitize=true" alt="AWS Architecture Diagram" width="480" /&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td width="50%" valign="top"&gt; &lt;strong&gt;Azure architecture diagram&lt;/strong&gt;&lt;br /&gt; &lt;p&gt;&lt;strong&gt;Prompt:&lt;/strong&gt; Generate a Azure architecture diagram with **Azure icons**. In this diagram, users connect to a frontend hosted on an instance.&lt;/p&gt; &lt;img src="https://raw.githubusercontent.com/DayuanJiang/next-ai-draw-io/main/public/azure_demo.svg?sanitize=true" alt="Azure Architecture Diagram" width="480" /&gt; &lt;/td&gt; 
    &lt;td width="50%" valign="top"&gt; &lt;strong&gt;Cat sketch prompt&lt;/strong&gt;&lt;br /&gt; &lt;p&gt;&lt;strong&gt;Prompt:&lt;/strong&gt; Draw a cute cat for me.&lt;/p&gt; &lt;img src="https://raw.githubusercontent.com/DayuanJiang/next-ai-draw-io/main/public/cat_demo.svg?sanitize=true" alt="Cat Drawing" width="240" /&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;LLM-Powered Diagram Creation&lt;/strong&gt;: Leverage Large Language Models to create and manipulate draw.io diagrams directly through natural language commands&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Image-Based Diagram Replication&lt;/strong&gt;: Upload existing diagrams or images and have the AI replicate and enhance them automatically&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;PDF &amp;amp; Text File Upload&lt;/strong&gt;: Upload PDF documents and text files to extract content and generate diagrams from existing documents&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;AI Reasoning Display&lt;/strong&gt;: View the AI's thinking process for supported models (OpenAI o1/o3, Gemini, Claude, etc.)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Diagram History&lt;/strong&gt;: Comprehensive version control that tracks all changes, allowing you to view and restore previous versions of your diagrams before the AI editing.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Interactive Chat Interface&lt;/strong&gt;: Communicate with AI to refine your diagrams in real-time&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Cloud Architecture Diagram Support&lt;/strong&gt;: Specialized support for generating cloud architecture diagrams (AWS, GCP, Azure)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Animated Connectors&lt;/strong&gt;: Create dynamic and animated connectors between diagram elements for better visualization&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;MCP Server (Preview)&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Preview Feature&lt;/strong&gt;: This feature is experimental and may not be stable.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Use Next AI Draw.io with AI agents like Claude Desktop, Cursor, and VS Code via MCP (Model Context Protocol).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "mcpServers": {
    "drawio": {
      "command": "npx",
      "args": ["@next-ai-drawio/mcp-server@latest"]
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Claude Code CLI&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;claude mcp add drawio -- npx @next-ai-drawio/mcp-server@latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then ask Claude to create diagrams:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;"Create a flowchart showing user authentication with login, MFA, and session management"&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;The diagram appears in your browser in real-time!&lt;/p&gt; 
&lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/DayuanJiang/next-ai-draw-io/main/packages/mcp-server/README.md"&gt;MCP Server README&lt;/a&gt; for VS Code, Cursor, and other client configurations.&lt;/p&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;h3&gt;Try it Online&lt;/h3&gt; 
&lt;p&gt;No installation needed! Try the app directly on our demo site:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://next-ai-drawio.jiang.jp/"&gt;&lt;img src="https://raw.githubusercontent.com/DayuanJiang/next-ai-draw-io/main/public/live-demo-button.svg?sanitize=true" alt="Live Demo" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Bring Your Own API Key&lt;/strong&gt;: You can use your own API key to bypass usage limits on the demo site. Click the Settings icon in the chat panel to configure your provider and API key. Your key is stored locally in your browser and is never stored on the server.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Desktop Application&lt;/h3&gt; 
&lt;p&gt;Download the native desktop app for your platform from the &lt;a href="https://github.com/DayuanJiang/next-ai-draw-io/releases"&gt;Releases page&lt;/a&gt;:&lt;/p&gt; 
&lt;p&gt;Supported platforms: Windows, macOS, Linux.&lt;/p&gt; 
&lt;h3&gt;Run with Docker&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/DayuanJiang/next-ai-draw-io/main/docs/en/docker.md"&gt;Go to Docker Guide&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Clone the repository:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/DayuanJiang/next-ai-draw-io
cd next-ai-draw-io
npm install
cp env.example .env.local
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/DayuanJiang/next-ai-draw-io/main/docs/en/ai-providers.md"&gt;Provider Configuration Guide&lt;/a&gt; for detailed setup instructions for each provider.&lt;/p&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Run the development server:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npm run dev
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Open &lt;a href="http://localhost:6002"&gt;http://localhost:6002&lt;/a&gt; in your browser to see the application.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Deployment&lt;/h2&gt; 
&lt;h3&gt;Deploy to EdgeOne Pages&lt;/h3&gt; 
&lt;p&gt;You can deploy with one click using &lt;a href="https://pages.edgeone.ai/"&gt;Tencent EdgeOne Pages&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Deploy by this button:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://edgeone.ai/pages/new?repository-url=https%3A%2F%2Fgithub.com%2FDayuanJiang%2Fnext-ai-draw-io"&gt;&lt;img src="https://cdnstatic.tencentcs.com/edgeone/pages/deploy.svg?sanitize=true" alt="Deploy to EdgeOne Pages" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Check out the &lt;a href="https://pages.edgeone.ai/document/deployment-overview"&gt;Tencent EdgeOne Pages documentation&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;p&gt;Additionally, deploying through Tencent EdgeOne Pages will also grant you a &lt;a href="https://pages.edgeone.ai/document/edge-ai"&gt;daily free quota for DeepSeek models&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Deploy on Vercel (Recommended)&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://vercel.com/new/clone?repository-url=https%3A%2F%2Fgithub.com%2FDayuanJiang%2Fnext-ai-draw-io"&gt;&lt;img src="https://vercel.com/button" alt="Deploy with Vercel" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;The easiest way to deploy is using &lt;a href="https://vercel.com/new"&gt;Vercel&lt;/a&gt;, the creators of Next.js. Be sure to &lt;strong&gt;set the environment variables&lt;/strong&gt; in the Vercel dashboard as you did in your local &lt;code&gt;.env.local&lt;/code&gt; file.&lt;/p&gt; 
&lt;p&gt;See the &lt;a href="https://nextjs.org/docs/app/building-your-application/deploying"&gt;Next.js deployment documentation&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h3&gt;Deploy on Cloudflare Workers&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/DayuanJiang/next-ai-draw-io/main/docs/en/cloudflare-deploy.md"&gt;Go to Cloudflare Deploy Guide&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Multi-Provider Support&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://console.volcengine.com/ark/region:ark+cn-beijing/overview?briefPage=0&amp;amp;briefType=introduce&amp;amp;type=new&amp;amp;utm_campaign=doubao&amp;amp;utm_content=aidrawio&amp;amp;utm_medium=github&amp;amp;utm_source=coopensrc&amp;amp;utm_term=project"&gt;ByteDance Doubao&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;AWS Bedrock (default)&lt;/li&gt; 
 &lt;li&gt;OpenAI&lt;/li&gt; 
 &lt;li&gt;Anthropic&lt;/li&gt; 
 &lt;li&gt;Google AI&lt;/li&gt; 
 &lt;li&gt;Azure OpenAI&lt;/li&gt; 
 &lt;li&gt;Ollama&lt;/li&gt; 
 &lt;li&gt;OpenRouter&lt;/li&gt; 
 &lt;li&gt;DeepSeek&lt;/li&gt; 
 &lt;li&gt;SiliconFlow&lt;/li&gt; 
 &lt;li&gt;SGLang&lt;/li&gt; 
 &lt;li&gt;Vercel AI Gateway&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;All providers except AWS Bedrock and OpenRouter support custom endpoints.&lt;/p&gt; 
&lt;p&gt;üìñ &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/DayuanJiang/next-ai-draw-io/main/docs/en/ai-providers.md"&gt;Detailed Provider Configuration Guide&lt;/a&gt;&lt;/strong&gt; - See setup instructions for each provider.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Model Requirements&lt;/strong&gt;: This task requires strong model capabilities for generating long-form text with strict formatting constraints (draw.io XML). Recommended models include Claude Sonnet 4.5, GPT-5.1, Gemini 3 Pro, and DeepSeek V3.2/R1.&lt;/p&gt; 
&lt;p&gt;Note that the &lt;code&gt;claude&lt;/code&gt; series has been trained on draw.io diagrams with cloud architecture logos like AWS, Azure, GCP. So if you want to create cloud architecture diagrams, this is the best choice.&lt;/p&gt; 
&lt;h2&gt;How It Works&lt;/h2&gt; 
&lt;p&gt;The application uses the following technologies:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Next.js&lt;/strong&gt;: For the frontend framework and routing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Vercel AI SDK&lt;/strong&gt; (&lt;code&gt;ai&lt;/code&gt; + &lt;code&gt;@ai-sdk/*&lt;/code&gt;): For streaming AI responses and multi-provider support&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;react-drawio&lt;/strong&gt;: For diagram representation and manipulation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Diagrams are represented as XML that can be rendered in draw.io. The AI processes your commands and generates or modifies this XML accordingly.&lt;/p&gt; 
&lt;h2&gt;Support &amp;amp; Contact&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Special thanks to &lt;a href="https://console.volcengine.com/ark/region:ark+cn-beijing/overview?briefPage=0&amp;amp;briefType=introduce&amp;amp;type=new&amp;amp;utm_campaign=doubao&amp;amp;utm_content=aidrawio&amp;amp;utm_medium=github&amp;amp;utm_source=coopensrc&amp;amp;utm_term=project"&gt;ByteDance Doubao&lt;/a&gt; for sponsoring the API token usage of the demo site!&lt;/strong&gt; Register on the ARK platform to get 500K free tokens for all models!&lt;/p&gt; 
&lt;p&gt;If you find this project useful, please consider &lt;a href="https://github.com/sponsors/DayuanJiang"&gt;sponsoring&lt;/a&gt; to help me host the live demo site!&lt;/p&gt; 
&lt;p&gt;For support or inquiries, please open an issue on the GitHub repository or contact the maintainer at:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Email: me[at]jiang.jp&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.star-history.com/#DayuanJiang/next-ai-draw-io&amp;amp;type=date&amp;amp;legend=top-left"&gt;&lt;img src="https://api.star-history.com/svg?repos=DayuanJiang/next-ai-draw-io&amp;amp;type=date&amp;amp;legend=top-left" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt;</description>
    </item>
    
    <item>
      <title>HQarroum/docker-android</title>
      <link>https://github.com/HQarroum/docker-android</link>
      <description>&lt;p&gt;ü§ñ A minimal and customizable Docker image running the Android emulator as a service.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img width="400" src="https://raw.githubusercontent.com/HQarroum/docker-android/main/assets/icon.png" /&gt; &lt;/p&gt;
&lt;br /&gt;
&lt;br /&gt; 
&lt;h1&gt;docker-android&lt;/h1&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;A minimal and customizable Docker image running the Android emulator as a service.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;a href="https://github.com/HQarroum/docker-android/actions/workflows/docker-image.yml"&gt;&lt;img src="https://github.com/HQarroum/docker-android/actions/workflows/docker-image.yml/badge.svg?sanitize=true" alt="Docker Image CI" /&gt;&lt;/a&gt; &lt;a href="https://deepsource.io/gh/HQarroum/docker-android/?ref=repository-badge"&gt;&lt;img src="https://deepsource.io/gh/HQarroum/docker-android.svg/?label=active+issues&amp;amp;show_trend=true&amp;amp;token=JTfGSHolIiMj0WNfv2ES0I6X" alt="DeepSource" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/docker/pulls/halimqarroum/docker-android" alt="Docker Pulls" /&gt;&lt;/p&gt; 
&lt;p&gt;Current version: &lt;strong&gt;1.1.0&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;üìã Table of contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HQarroum/docker-android/main/#-features"&gt;Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HQarroum/docker-android/main/#-description"&gt;Description&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HQarroum/docker-android/main/#-usage"&gt;Usage&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HQarroum/docker-android/main/#-see-also"&gt;See also&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üîñ Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Minimal Alpine based image bundled with the Android emulator and KVM support.&lt;/li&gt; 
 &lt;li&gt;Bundles the Java Runtime Environment 11 in the image.&lt;/li&gt; 
 &lt;li&gt;Customizable Android version, device type and image types.&lt;/li&gt; 
 &lt;li&gt;Port-forwarding of emulator and ADB on the container network interface built-in.&lt;/li&gt; 
 &lt;li&gt;Emulator images are wiped each time the emulator re-starts.&lt;/li&gt; 
 &lt;li&gt;Runs headless, suitable for CI farms. Compatible with &lt;a href="https://github.com/Genymobile/scrcpy"&gt;&lt;code&gt;scrcpy&lt;/code&gt;&lt;/a&gt; to remotely control the Android screen.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üî∞ Description&lt;/h2&gt; 
&lt;p&gt;The focus of this project is to provide a size-optimized Docker image bundled with the minimal amount of software required to expose a fully functionning Android emulator that's remotely controllable over the network. This image only contains the Android emulator itself, an ADB server used to remotely connect into the emulator from outside the container, and QEMU with &lt;code&gt;libvirt&lt;/code&gt; support.&lt;/p&gt; 
&lt;p&gt;You can build this image without the Android SDK and without the Android emulator to make the image smaller. Below is a size comparison between some of the possible build variants.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variant&lt;/th&gt; 
   &lt;th&gt;Uncompressed&lt;/th&gt; 
   &lt;th&gt;Compressed&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;API 33 + Emulator&lt;/td&gt; 
   &lt;td&gt;5.84 GB&lt;/td&gt; 
   &lt;td&gt;1.97 GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;API 32 + Emulator&lt;/td&gt; 
   &lt;td&gt;5.89 GB&lt;/td&gt; 
   &lt;td&gt;1.93 GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;API 28 + Emulator&lt;/td&gt; 
   &lt;td&gt;4.29 GB&lt;/td&gt; 
   &lt;td&gt;1.46 GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Without SDK and emulator&lt;/td&gt; 
   &lt;td&gt;414 MB&lt;/td&gt; 
   &lt;td&gt;138 MB&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;üìò Usage&lt;/h2&gt; 
&lt;p&gt;By default, a build will bundle the Android SDK, platform tools and emulator with the image.&lt;/p&gt; 
&lt;p&gt;with docker-compose:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker compose up android-emulator
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or with GPU acceleration&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker compose up android-emulator-cuda
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or for example with GPU acceleration and google playstore&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker compose up android-emulator-cuda-store
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;with only docker&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker build -t android-emulator .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Keys&lt;/h2&gt; 
&lt;p&gt;To run google_apis_playstore image, you need to have same adbkey between emulator and client.&lt;/p&gt; 
&lt;p&gt;You can generate one by running &lt;code&gt;adb keygen adbkey&lt;/code&gt;, that generates 2 files - adbkey and adbkey.pub.&lt;/p&gt; 
&lt;p&gt;override them inside ./keys directory.&lt;/p&gt; 
&lt;h3&gt;Running the container&lt;/h3&gt; 
&lt;p&gt;Once the image is built, you can mount your KVM driver on the container and expose its ADB port.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Ensure 4GB of memory and at least 8GB of disk space for API 33.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run -it --rm --device /dev/kvm -p 5555:5555 android-emulator
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Save data/storage after restart (wipe)&lt;/h3&gt; 
&lt;p&gt;All avd save in docker dir &lt;code&gt;/data&lt;/code&gt;, name for avd is &lt;code&gt;android&lt;/code&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run -it --rm --device /dev/kvm -p 5555:5555 -v ~/android_avd:/data android-emulator
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Connect ADB to the container&lt;/h3&gt; 
&lt;p&gt;The ADB server in the container will be spawned automatically and listen on all interfaces in the container. After a few seconds, once the kernel has booted, you will be able to connect ADB to the container.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;adb connect 127.0.0.1:5555
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Additionally, you can use &lt;a href="https://github.com/Genymobile/scrcpy"&gt;&lt;code&gt;scrcpy&lt;/code&gt;&lt;/a&gt; to control the screen of the emulator remotely. To do so, you simply have to connect ADB and run it locally.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;By default, the emulator runs with a Pixel preset (1080x1920).&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;scrcpy
&lt;/code&gt;&lt;/pre&gt; 
&lt;br /&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td&gt; &lt;img width="260" src="https://raw.githubusercontent.com/HQarroum/docker-android/main/assets/screenshot.png" /&gt; &lt;/td&gt; 
   &lt;td&gt; &lt;img width="260" src="https://raw.githubusercontent.com/HQarroum/docker-android/main/assets/screenshot-2.png" /&gt; &lt;/td&gt; 
   &lt;td&gt; &lt;img width="260" src="https://raw.githubusercontent.com/HQarroum/docker-android/main/assets/screenshot-3.png" /&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;br /&gt; 
&lt;h3&gt;Customize the image&lt;/h3&gt; 
&lt;p&gt;It is possible to customize the API level (Android version) and the image type (Google APIs vs PlayStore) when building the image.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;By default, the image will build with API 33 with support for Google APIs for an x86_64 architecture.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;This can come in handy when integrating multiple images as part of a CI pipeline where an application or a set of applications need to be tested against different Android versions. There are 2 variables that can be specified at build time to change the Android image.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;API_LEVEL&lt;/code&gt; - Specifies the &lt;a href="https://apilevels.com/"&gt;API level&lt;/a&gt; associated with the image. Use this parameter to change the Android version.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;IMG_TYPE&lt;/code&gt; - Specifies the type of image to install.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ARCHITECTURE&lt;/code&gt; Specifies the CPU architecture of the Android image. Note that only &lt;code&gt;x86_64&lt;/code&gt; and &lt;code&gt;x86&lt;/code&gt; are actively supported by this image.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The below example will install Android Pie with support for the Google Play Store.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker build \
  --build-arg API_LEVEL=28 \
  --build-arg IMG_TYPE=google_apis_playstore \
  --build-arg ARCHITECTURE=x86 \
  --tag android-emulator .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Variables&lt;/h3&gt; 
&lt;h4&gt;Default variables&lt;/h4&gt; 
&lt;h4&gt;Disable animation&lt;/h4&gt; 
&lt;p&gt;DISABLE_ANIMATION=false&lt;/p&gt; 
&lt;h4&gt;Disable hidden policy&lt;/h4&gt; 
&lt;p&gt;DISABLE_HIDDEN_POLICY=false&lt;/p&gt; 
&lt;h4&gt;Skip adb authentication&lt;/h4&gt; 
&lt;p&gt;SKIP_AUTH=true&lt;/p&gt; 
&lt;h4&gt;Memory for emulator&lt;/h4&gt; 
&lt;p&gt;MEMORY=8192&lt;/p&gt; 
&lt;h4&gt;Cores for emulator&lt;/h4&gt; 
&lt;p&gt;CORES=4&lt;/p&gt; 
&lt;h3&gt;Mount an external drive in the container&lt;/h3&gt; 
&lt;p&gt;It might be sometimes useful to have the entire Android SDK folder outside of the container (stored on a shared distributed filesystem such as NFS for example), to significantly reduce the size and the build time of the image.&lt;/p&gt; 
&lt;p&gt;To do so, you can specify a specific argument at build time to disable the download and installation of the SDK in the image.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker build -t android-emulator --build-arg INSTALL_ANDROID_SDK=0 .
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;You will need mount the SDK in the container at &lt;code&gt;/opt/android&lt;/code&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run -it --rm --device /dev/kvm -p 5555:5555 -v /shared/android/sdk:/opt/android/ android-emulator
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Pull from Docker Hub&lt;/h3&gt; 
&lt;p&gt;Different pre-built images of &lt;code&gt;docker-android&lt;/code&gt; exist on &lt;a href="https://hub.docker.com/r/halimqarroum/docker-android"&gt;Docker Hub&lt;/a&gt;. Each image variant is tagged using its the api level and image type. For example, to pull an API 33 image, you can run the following.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker pull halimqarroum/docker-android:api-33
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;üëÄ See also&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;The &lt;a href="https://github.com/alvr/alpine-android"&gt;alpine-android&lt;/a&gt; project which is based on a different Alpine image.&lt;/li&gt; 
 &lt;li&gt;The &lt;a href="https://github.com/budtmo/docker-android"&gt;docker-android&lt;/a&gt; project which offers a WebRTC interface to an Android emulator.&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>yichuan-w/LEANN</title>
      <link>https://github.com/yichuan-w/LEANN</link>
      <description>&lt;p&gt;RAG on Everything with LEANN. Enjoy 97% storage savings while running a fast, accurate, and 100% private RAG application on your personal device.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/yichuan-w/LEANN/main/assets/logo-text.png" alt="LEANN Logo" width="400" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://img.shields.io/badge/Python-3.9%20%7C%203.10%20%7C%203.11%20%7C%203.12%20%7C%203.13-blue.svg?sanitize=true" alt="Python Versions" /&gt; &lt;img src="https://github.com/yichuan-w/LEANN/actions/workflows/build-and-publish.yml/badge.svg?sanitize=true" alt="CI Status" /&gt; &lt;img src="https://img.shields.io/badge/Platform-Ubuntu%20%26%20Arch%20%26%20WSL%20%7C%20macOS%20(ARM64%2FIntel)-lightgrey" alt="Platform" /&gt; &lt;img src="https://img.shields.io/badge/License-MIT-green.svg?sanitize=true" alt="MIT License" /&gt; &lt;img src="https://img.shields.io/badge/MCP-Native%20Integration-blue" alt="MCP Integration" /&gt; &lt;a href="https://join.slack.com/t/leann-e2u9779/shared_invite/zt-3ckd2f6w1-OX08~NN4gkWhh10PRVBj1Q"&gt; &lt;img src="https://img.shields.io/badge/Slack-Join-4A154B?logo=slack&amp;amp;logoColor=white" alt="Join Slack" /&gt; &lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/assets/wechat_user_group.JPG" title="Join WeChat group"&gt; &lt;img src="https://img.shields.io/badge/WeChat-Join-2DC100?logo=wechat&amp;amp;logoColor=white" alt="Join WeChat group" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://forms.gle/rDbZf864gMNxhpTq8"&gt; &lt;img src="https://img.shields.io/badge/üì£_Community_Survey-Help_Shape_v0.4-007ec6?style=for-the-badge&amp;amp;logo=google-forms&amp;amp;logoColor=white" alt="Take Survey" /&gt; &lt;/a&gt; 
 &lt;p&gt; We track &lt;b&gt;zero telemetry&lt;/b&gt;. This survey is the ONLY way to tell us if you want &lt;br /&gt; &lt;b&gt;GPU Acceleration&lt;/b&gt; or &lt;b&gt;More Integrations&lt;/b&gt; next.&lt;br /&gt; üëâ &lt;a href="https://forms.gle/rDbZf864gMNxhpTq8"&gt;&lt;b&gt;Click here to cast your vote (2 mins)&lt;/b&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2 align="center" tabindex="-1" class="heading-element" dir="auto"&gt; The smallest vector index in the world. RAG Everything with LEANN! &lt;/h2&gt; 
&lt;p&gt;LEANN is an innovative vector database that democratizes personal AI. Transform your laptop into a powerful RAG system that can index and search through millions of documents while using &lt;strong&gt;97% less storage&lt;/strong&gt; than traditional solutions &lt;strong&gt;without accuracy loss&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;LEANN achieves this through &lt;em&gt;graph-based selective recomputation&lt;/em&gt; with &lt;em&gt;high-degree preserving pruning&lt;/em&gt;, computing embeddings on-demand instead of storing them all. &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#%EF%B8%8F-architecture--how-it-works"&gt;Illustration Fig ‚Üí&lt;/a&gt; | &lt;a href="https://arxiv.org/abs/2506.08276"&gt;Paper ‚Üí&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Ready to RAG Everything?&lt;/strong&gt; Transform your laptop into a personal AI assistant that can semantic search your &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#-personal-data-manager-process-any-documents-pdf-txt-md"&gt;file system&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#-your-personal-email-secretary-rag-on-apple-mail"&gt;emails&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#-time-machine-for-the-web-rag-your-entire-browser-history"&gt;browser history&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#-wechat-detective-unlock-your-golden-memories"&gt;chat history&lt;/a&gt;&lt;/strong&gt; (&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#-wechat-detective-unlock-your-golden-memories"&gt;WeChat&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#-imessage-history-your-personal-conversation-archive"&gt;iMessage&lt;/a&gt;), &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#-chatgpt-chat-history-your-personal-ai-conversation-archive"&gt;agent memory&lt;/a&gt;&lt;/strong&gt; (&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#-chatgpt-chat-history-your-personal-ai-conversation-archive"&gt;ChatGPT&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#-claude-chat-history-your-personal-ai-conversation-archive"&gt;Claude&lt;/a&gt;), &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#mcp-integration-rag-on-live-data-from-any-platform"&gt;live data&lt;/a&gt;&lt;/strong&gt; (&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#slack-messages-search-your-team-conversations"&gt;Slack&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#-twitter-bookmarks-your-personal-tweet-library"&gt;Twitter&lt;/a&gt;), &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#-claude-code-integration-transform-your-development-workflow"&gt;codebase&lt;/a&gt;&lt;/strong&gt;* , or external knowledge bases (i.e., 60M documents) - all on your laptop, with zero cloud costs and complete privacy.&lt;/p&gt; 
&lt;p&gt;* Claude Code only supports basic &lt;code&gt;grep&lt;/code&gt;-style keyword search. &lt;strong&gt;LEANN&lt;/strong&gt; is a drop-in &lt;strong&gt;semantic search MCP service fully compatible with Claude Code&lt;/strong&gt;, unlocking intelligent retrieval without changing your workflow. üî• Check out &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/packages/leann-mcp/README.md"&gt;the easy setup ‚Üí&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Why LEANN?&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/yichuan-w/LEANN/main/assets/effects.png" alt="LEANN vs Traditional Vector DB Storage Comparison" width="70%" /&gt; &lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;The numbers speak for themselves:&lt;/strong&gt; Index 60 million text chunks in just 6GB instead of 201GB. From emails to browser history, everything fits on your laptop. &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#-storage-comparison"&gt;See detailed benchmarks for different applications below ‚Üì&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;üîí &lt;strong&gt;Privacy:&lt;/strong&gt; Your data never leaves your laptop. No OpenAI, no cloud, no "terms of service".&lt;/p&gt; 
&lt;p&gt;ü™∂ &lt;strong&gt;Lightweight:&lt;/strong&gt; Graph-based recomputation eliminates heavy embedding storage, while smart graph pruning and CSR format minimize graph storage overhead. Always less storage, less memory usage!&lt;/p&gt; 
&lt;p&gt;üì¶ &lt;strong&gt;Portable:&lt;/strong&gt; Transfer your entire knowledge base between devices (even with others) with minimal cost - your personal AI memory travels with you.&lt;/p&gt; 
&lt;p&gt;üìà &lt;strong&gt;Scalability:&lt;/strong&gt; Handle messy personal data that would crash traditional vector DBs, easily managing your growing personalized data and agent generated memory!&lt;/p&gt; 
&lt;p&gt;‚ú® &lt;strong&gt;No Accuracy Loss:&lt;/strong&gt; Maintain the same search quality as heavyweight solutions while using 97% less storage.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;üì¶ Prerequisites: Install uv&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://docs.astral.sh/uv/getting-started/installation/#installation-methods"&gt;Install uv&lt;/a&gt; first if you don't have it. Typically, you can install it with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -LsSf https://astral.sh/uv/install.sh | sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;üöÄ Quick Install&lt;/h3&gt; 
&lt;p&gt;Clone the repository to access all examples and try amazing applications,&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/yichuan-w/LEANN.git leann
cd leann
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;and install LEANN from &lt;a href="https://pypi.org/project/leann/"&gt;PyPI&lt;/a&gt; to run them immediately:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv venv
source .venv/bin/activate
uv pip install leann
&lt;/code&gt;&lt;/pre&gt; 
&lt;!--
&gt; Low-resource? See "Low-resource setups" in the [Configuration Guide](docs/configuration-guide.md#low-resource-setups). --&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;strong&gt;üîß Build from Source (Recommended for development)&lt;/strong&gt; &lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/yichuan-w/LEANN.git leann
cd leann
git submodule update --init --recursive
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;macOS:&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;Note: DiskANN requires MacOS 13.3 or later.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;brew install libomp boost protobuf zeromq pkgconf
uv sync --extra diskann
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Linux (Ubuntu/Debian):&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;Note: On Ubuntu 20.04, you may need to build a newer Abseil and pin Protobuf (e.g., v3.20.x) for building DiskANN. See &lt;a href="https://github.com/yichuan-w/LEANN/issues/30"&gt;Issue #30&lt;/a&gt; for a step-by-step note.&lt;/p&gt; 
 &lt;p&gt;You can manually install &lt;a href="https://www.intel.com/content/www/us/en/developer/tools/oneapi/onemkl.html"&gt;Intel oneAPI MKL&lt;/a&gt; instead of &lt;code&gt;libmkl-full-dev&lt;/code&gt; for DiskANN. You can also use &lt;code&gt;libopenblas-dev&lt;/code&gt; for building HNSW only, by removing &lt;code&gt;--extra diskann&lt;/code&gt; in the command below.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;sudo apt-get update &amp;amp;&amp;amp; sudo apt-get install -y \
  libomp-dev libboost-all-dev protobuf-compiler libzmq3-dev \
  pkg-config libabsl-dev libaio-dev libprotobuf-dev \
  libmkl-full-dev

uv sync --extra diskann
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Linux (Arch Linux):&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;sudo pacman -Syu &amp;amp;&amp;amp; sudo pacman -S --needed base-devel cmake pkgconf git gcc \
  boost boost-libs protobuf abseil-cpp libaio zeromq

# For MKL in DiskANN
sudo pacman -S --needed base-devel git
git clone https://aur.archlinux.org/paru-bin.git
cd paru-bin &amp;amp;&amp;amp; makepkg -si
paru -S intel-oneapi-mkl intel-oneapi-compiler
source /opt/intel/oneapi/setvars.sh

uv sync --extra diskann
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Linux (RHEL / CentOS Stream / Oracle / Rocky / AlmaLinux):&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;See &lt;a href="https://github.com/yichuan-w/LEANN/issues/50"&gt;Issue #50&lt;/a&gt; for more details.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;sudo dnf groupinstall -y "Development Tools"
sudo dnf install -y libomp-devel boost-devel protobuf-compiler protobuf-devel \
  abseil-cpp-devel libaio-devel zeromq-devel pkgconf-pkg-config

# For MKL in DiskANN
sudo dnf install -y intel-oneapi-mkl intel-oneapi-mkl-devel \
  intel-oneapi-openmp || sudo dnf install -y intel-oneapi-compiler
source /opt/intel/oneapi/setvars.sh

uv sync --extra diskann
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;p&gt;Our declarative API makes RAG as easy as writing a config file.&lt;/p&gt; 
&lt;p&gt;Check out &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/demo.ipynb"&gt;demo.ipynb&lt;/a&gt; or &lt;a href="https://colab.research.google.com/github/yichuan-w/LEANN/blob/main/demo.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from leann import LeannBuilder, LeannSearcher, LeannChat
from pathlib import Path
INDEX_PATH = str(Path("./").resolve() / "demo.leann")

# Build an index
builder = LeannBuilder(backend_name="hnsw")
builder.add_text("LEANN saves 97% storage compared to traditional vector databases.")
builder.add_text("Tung Tung Tung Sahur called‚Äîthey need their banana‚Äëcrocodile hybrid back")
builder.build_index(INDEX_PATH)

# Search
searcher = LeannSearcher(INDEX_PATH)
results = searcher.search("fantastical AI-generated creatures", top_k=1)

# Chat with your data
chat = LeannChat(INDEX_PATH, llm_config={"type": "hf", "model": "Qwen/Qwen3-0.6B"})
response = chat.ask("How much storage does LEANN save?", top_k=1)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Performance Optimization: Task-Specific Prompt Templates&lt;/h2&gt; 
&lt;p&gt;LEANN now supports prompt templates for task-specific embedding models like Google's EmbeddingGemma. This feature enables &lt;strong&gt;significant performance gains&lt;/strong&gt; by using smaller, faster models without sacrificing search quality.&lt;/p&gt; 
&lt;h3&gt;Real-World Performance&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Benchmark (MacBook M1 Pro, LM Studio):&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;EmbeddingGemma 300M (QAT)&lt;/strong&gt; with templates: &lt;strong&gt;4-5x faster&lt;/strong&gt; than Qwen 600M&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Search quality:&lt;/strong&gt; Identical ranking to larger models&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Use case:&lt;/strong&gt; Ideal for real-time workflows (e.g., pre-commit hooks in Claude Code; ~7min for whole LEANN's code + doc files on MacBook M1 Pro)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Quick Example&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Build index with task-specific templates
leann build my-index ./docs \
  --embedding-mode ollama \
  --embedding-model embeddinggemma \
  --embedding-prompt-template "title: none | text: " \
  --query-prompt-template "task: search result | query: "

# Search automatically applies query template
leann search my-index "How does LEANN optimize vector search?"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Templates are automatically persisted and applied during searches (CLI, MCP, API). No manual configuration needed after indexing.&lt;/p&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/docs/configuration-guide.md#task-specific-prompt-templates"&gt;Configuration Guide&lt;/a&gt; for detailed usage and model recommendations.&lt;/p&gt; 
&lt;h2&gt;RAG on Everything!&lt;/h2&gt; 
&lt;p&gt;LEANN supports RAG on various data sources including documents (&lt;code&gt;.pdf&lt;/code&gt;, &lt;code&gt;.txt&lt;/code&gt;, &lt;code&gt;.md&lt;/code&gt;), Apple Mail, Google Search History, WeChat, ChatGPT conversations, Claude conversations, iMessage conversations, and &lt;strong&gt;live data from any platform through MCP (Model Context Protocol) servers&lt;/strong&gt; - including Slack, Twitter, and more.&lt;/p&gt; 
&lt;h3&gt;Generation Model Setup&lt;/h3&gt; 
&lt;h4&gt;LLM Backend&lt;/h4&gt; 
&lt;p&gt;LEANN supports many LLM providers for text generation (HuggingFace, Ollama, Anthropic, and Any OpenAI compatible API).&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üîë OpenAI API Setup (Default)&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;Set your OpenAI API key as an environment variable:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;export OPENAI_API_KEY="your-api-key-here"
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Make sure to use &lt;code&gt;--llm openai&lt;/code&gt; flag when using the CLI. You can also specify the model name with &lt;code&gt;--llm-model &amp;lt;model-name&amp;gt;&lt;/code&gt; flag.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üõ†Ô∏è Supported LLM &amp;amp; Embedding Providers (via OpenAI Compatibility)&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;Thanks to the widespread adoption of the OpenAI API format, LEANN is compatible out-of-the-box with a vast array of LLM and embedding providers. Simply set the &lt;code&gt;OPENAI_BASE_URL&lt;/code&gt; and &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; environment variables to connect to your preferred service.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-sh"&gt;export OPENAI_API_KEY="xxx"
export OPENAI_BASE_URL="http://localhost:1234/v1" # base url of the provider
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;To use OpenAI compatible endpoint with the CLI interface:&lt;/p&gt; 
 &lt;p&gt;If you are using it for text generation, make sure to use &lt;code&gt;--llm openai&lt;/code&gt; flag and specify the model name with &lt;code&gt;--llm-model &amp;lt;model-name&amp;gt;&lt;/code&gt; flag.&lt;/p&gt; 
 &lt;p&gt;If you are using it for embedding, set the &lt;code&gt;--embedding-mode openai&lt;/code&gt; flag and specify the model name with &lt;code&gt;--embedding-model &amp;lt;MODEL&amp;gt;&lt;/code&gt;.&lt;/p&gt; 
 &lt;hr /&gt; 
 &lt;p&gt;Below is a list of base URLs for common providers to get you started.&lt;/p&gt; 
 &lt;h3&gt;üñ•Ô∏è Local Inference Engines (Recommended for full privacy)&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Provider&lt;/th&gt; 
    &lt;th&gt;Sample Base URL&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Ollama&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;http://localhost:11434/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;LM Studio&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;http://localhost:1234/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;vLLM&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;http://localhost:8000/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;llama.cpp&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;http://localhost:8080/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;SGLang&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;http://localhost:30000/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;LiteLLM&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;http://localhost:4000&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;hr /&gt; 
 &lt;h3&gt;‚òÅÔ∏è Cloud Providers&lt;/h3&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;&lt;strong&gt;üö® A Note on Privacy:&lt;/strong&gt; Before choosing a cloud provider, carefully review their privacy and data retention policies. Depending on their terms, your data may be used for their own purposes, including but not limited to human reviews and model training, which can lead to serious consequences if not handled properly.&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Provider&lt;/th&gt; 
    &lt;th&gt;Base URL&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;OpenAI&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;https://api.openai.com/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;OpenRouter&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;https://openrouter.ai/api/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Gemini&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;https://generativelanguage.googleapis.com/v1beta/openai/&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;x.AI (Grok)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;https://api.x.ai/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Groq AI&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;https://api.groq.com/openai/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;DeepSeek&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;https://api.deepseek.com/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;SiliconFlow&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;https://api.siliconflow.cn/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Zhipu (BigModel)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;https://open.bigmodel.cn/api/paas/v4/&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Mistral AI&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;https://api.mistral.ai/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Anthropic&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;https://api.anthropic.com/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;p&gt;If your provider isn't on this list, don't worry! Check their documentation for an OpenAI-compatible endpoint‚Äîchances are, it's OpenAI Compatible too!&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üîß Ollama Setup (Recommended for full privacy)&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;macOS:&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;First, &lt;a href="https://ollama.com/download/mac"&gt;download Ollama for macOS&lt;/a&gt;.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Pull a lightweight model (recommended for consumer hardware)
ollama pull llama3.2:1b
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Linux:&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Install Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Start Ollama service manually
ollama serve &amp;amp;

# Pull a lightweight model (recommended for consumer hardware)
ollama pull llama3.2:1b
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;‚≠ê Flexible Configuration&lt;/h2&gt; 
&lt;p&gt;LEANN provides flexible parameters for embedding models, search strategies, and data processing to fit your specific needs.&lt;/p&gt; 
&lt;p&gt;üìö &lt;strong&gt;Need configuration best practices?&lt;/strong&gt; Check our &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/docs/configuration-guide.md"&gt;Configuration Guide&lt;/a&gt; for detailed optimization tips, model selection advice, and solutions to common issues like slow embeddings or poor search quality.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìã Click to expand: Common Parameters (Available in All Examples)&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;All RAG examples share these common parameters. &lt;strong&gt;Interactive mode&lt;/strong&gt; is available in all examples - simply run without &lt;code&gt;--query&lt;/code&gt; to start a continuous Q&amp;amp;A session where you can ask multiple questions. Type 'quit' to exit.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Core Parameters (General preprocessing for all examples)
--index-dir DIR              # Directory to store the index (default: current directory)
--query "YOUR QUESTION"      # Single query mode. Omit for interactive chat (type 'quit' to exit), and now you can play with your index interactively
--max-items N                # Limit data preprocessing (default: -1, process all data)
--force-rebuild              # Force rebuild index even if it exists

# Embedding Parameters
--embedding-model MODEL      # e.g., facebook/contriever, text-embedding-3-small, mlx-community/Qwen3-Embedding-0.6B-8bit or nomic-embed-text
--embedding-mode MODE        # sentence-transformers, openai, mlx, or ollama

# LLM Parameters (Text generation models)
--llm TYPE                   # LLM backend: openai, ollama, hf, or anthropic (default: openai)
--llm-model MODEL            # Model name (default: gpt-4o) e.g., gpt-4o-mini, llama3.2:1b, Qwen/Qwen2.5-1.5B-Instruct
--thinking-budget LEVEL      # Thinking budget for reasoning models: low/medium/high (supported by o3, o3-mini, GPT-Oss:20b, and other reasoning models)

# Search Parameters
--top-k N                    # Number of results to retrieve (default: 20)
--search-complexity N        # Search complexity for graph traversal (default: 32)

# Chunking Parameters
--chunk-size N               # Size of text chunks (default varies by source: 256 for most, 192 for WeChat)
--chunk-overlap N            # Overlap between chunks (default varies: 25-128 depending on source)

# Index Building Parameters
--backend-name NAME          # Backend to use: hnsw or diskann (default: hnsw)
--graph-degree N             # Graph degree for index construction (default: 32)
--build-complexity N         # Build complexity for index construction (default: 64)
--compact / --no-compact     # Use compact storage (default: true). Must be `no-compact` for `no-recompute` build.
--recompute / --no-recompute # Enable/disable embedding recomputation (default: enabled). Should not do a `no-recompute` search in a `recompute` build.
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;üìÑ Personal Data Manager: Process Any Documents (&lt;code&gt;.pdf&lt;/code&gt;, &lt;code&gt;.txt&lt;/code&gt;, &lt;code&gt;.md&lt;/code&gt;)!&lt;/h3&gt; 
&lt;p&gt;Ask questions directly about your personal PDFs, documents, and any directory containing your files!&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/yichuan-w/LEANN/main/videos/paper_clear.gif" alt="LEANN Document Search Demo" width="600" /&gt; &lt;/p&gt; 
&lt;p&gt;The example below asks a question about summarizing our paper (uses default data in &lt;code&gt;data/&lt;/code&gt;, which is a directory with diverse data sources: two papers, Pride and Prejudice, and a Technical report about LLM in Huawei in Chinese), and this is the &lt;strong&gt;easiest example&lt;/strong&gt; to run here:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;source .venv/bin/activate # Don't forget to activate the virtual environment
python -m apps.document_rag --query "What are the main techniques LEANN explores?"
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìã Click to expand: Document-Specific Arguments&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h4&gt;Parameters&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;--data-dir DIR           # Directory containing documents to process (default: data)
--file-types .ext .ext   # Filter by specific file types (optional - all LlamaIndex supported types if omitted)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Example Commands&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Process all documents with larger chunks for academic papers
python -m apps.document_rag --data-dir "~/Documents/Papers" --chunk-size 1024

# Filter only markdown and Python files with smaller chunks
python -m apps.document_rag --data-dir "./docs" --chunk-size 256 --file-types .md .py

# Enable AST-aware chunking for code files
python -m apps.document_rag --enable-code-chunking --data-dir "./my_project"

# Or use the specialized code RAG for better code understanding
python -m apps.code_rag --repo-dir "./my_codebase" --query "How does authentication work?"
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;üé® ColQwen: Multimodal PDF Retrieval with Vision-Language Models&lt;/h3&gt; 
&lt;p&gt;Search through PDFs using both text and visual understanding with ColQwen2/ColPali models. Perfect for research papers, technical documents, and any PDFs with complex layouts, figures, or diagrams.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;üçé Mac Users&lt;/strong&gt;: ColQwen is optimized for Apple Silicon with MPS acceleration for faster inference!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Build index from PDFs
python -m apps.colqwen_rag build --pdfs ./my_papers/ --index research_papers

# Search with text queries
python -m apps.colqwen_rag search research_papers "How does attention mechanism work?"

# Interactive Q&amp;amp;A
python -m apps.colqwen_rag ask research_papers --interactive
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìã Click to expand: ColQwen Setup &amp;amp; Usage&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h4&gt;Prerequisites&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Install dependencies
uv pip install colpali_engine pdf2image pillow matplotlib qwen_vl_utils einops seaborn
brew install poppler  # macOS only, for PDF processing
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Build Index&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;python -m apps.colqwen_rag build \
  --pdfs ./pdf_directory/ \
  --index my_index \
  --model colqwen2  # or colpali
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Search&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;python -m apps.colqwen_rag search my_index "your question here" --top-k 5
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Models&lt;/h4&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;ColQwen2&lt;/strong&gt; (&lt;code&gt;colqwen2&lt;/code&gt;): Latest vision-language model with improved performance&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;ColPali&lt;/strong&gt; (&lt;code&gt;colpali&lt;/code&gt;): Proven multimodal retriever&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;For detailed usage, see the &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/docs/COLQWEN_GUIDE.md"&gt;ColQwen Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h3&gt;üìß Your Personal Email Secretary: RAG on Apple Mail!&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; The examples below currently support macOS only. Windows support coming soon.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/yichuan-w/LEANN/main/videos/mail_clear.gif" alt="LEANN Email Search Demo" width="600" /&gt; &lt;/p&gt; 
&lt;p&gt;Before running the example below, you need to grant full disk access to your terminal/VS Code in System Preferences ‚Üí Privacy &amp;amp; Security ‚Üí Full Disk Access.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m apps.email_rag --query "What's the food I ordered by DoorDash or Uber Eats mostly?"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;780K email chunks ‚Üí 78MB storage.&lt;/strong&gt; Finally, search your email like you search Google.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìã Click to expand: Email-Specific Arguments&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h4&gt;Parameters&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;--mail-path PATH         # Path to specific mail directory (auto-detects if omitted)
--include-html          # Include HTML content in processing (useful for newsletters)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Example Commands&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Search work emails from a specific account
python -m apps.email_rag --mail-path "~/Library/Mail/V10/WORK_ACCOUNT"

# Find all receipts and order confirmations (includes HTML)
python -m apps.email_rag --query "receipt order confirmation invoice" --include-html
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìã Click to expand: Example queries you can try&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;Once the index is built, you can ask questions like:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;"Find emails from my boss about deadlines"&lt;/li&gt; 
  &lt;li&gt;"What did John say about the project timeline?"&lt;/li&gt; 
  &lt;li&gt;"Show me emails about travel expenses"&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;üîç Time Machine for the Web: RAG Your Entire Chrome Browser History!&lt;/h3&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/yichuan-w/LEANN/main/videos/google_clear.gif" alt="LEANN Browser History Search Demo" width="600" /&gt; &lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m apps.browser_rag --query "Tell me my browser history about machine learning?"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;38K browser entries ‚Üí 6MB storage.&lt;/strong&gt; Your browser history becomes your personal search engine.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìã Click to expand: Browser-Specific Arguments&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h4&gt;Parameters&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;--chrome-profile PATH    # Path to Chrome profile directory (auto-detects if omitted)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Example Commands&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Search academic research from your browsing history
python -m apps.browser_rag --query "arxiv papers machine learning transformer architecture"

# Track competitor analysis across work profile
python -m apps.browser_rag --chrome-profile "~/Library/Application Support/Google/Chrome/Work Profile" --max-items 5000
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìã Click to expand: How to find your Chrome profile&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;The default Chrome profile path is configured for a typical macOS setup. If you need to find your specific Chrome profile:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Open Terminal&lt;/li&gt; 
  &lt;li&gt;Run: &lt;code&gt;ls ~/Library/Application\ Support/Google/Chrome/&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;Look for folders like "Default", "Profile 1", "Profile 2", etc.&lt;/li&gt; 
  &lt;li&gt;Use the full path as your &lt;code&gt;--chrome-profile&lt;/code&gt; argument&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;&lt;strong&gt;Common Chrome profile locations:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;macOS: &lt;code&gt;~/Library/Application Support/Google/Chrome/Default&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;Linux: &lt;code&gt;~/.config/google-chrome/Default&lt;/code&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üí¨ Click to expand: Example queries you can try&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;Once the index is built, you can ask questions like:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;"What websites did I visit about machine learning?"&lt;/li&gt; 
  &lt;li&gt;"Find my search history about programming"&lt;/li&gt; 
  &lt;li&gt;"What YouTube videos did I watch recently?"&lt;/li&gt; 
  &lt;li&gt;"Show me websites I visited about travel planning"&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;üí¨ WeChat Detective: Unlock Your Golden Memories!&lt;/h3&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/yichuan-w/LEANN/main/videos/wechat_clear.gif" alt="LEANN WeChat Search Demo" width="600" /&gt; &lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m apps.wechat_rag --query "Show me all group chats about weekend plans"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;400K messages ‚Üí 64MB storage&lt;/strong&gt; Search years of chat history in any language.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üîß Click to expand: Installation Requirements&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;First, you need to install the &lt;a href="https://github.com/sunnyyoung/WeChatTweak-CLI"&gt;WeChat exporter&lt;/a&gt;,&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;brew install sunnyyoung/repo/wechattweak-cli
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;or install it manually (if you have issues with Homebrew):&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;sudo packages/wechat-exporter/wechattweak-cli install
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Troubleshooting:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Installation issues&lt;/strong&gt;: Check the &lt;a href="https://github.com/sunnyyoung/WeChatTweak-CLI/issues/41"&gt;WeChatTweak-CLI issues page&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Export errors&lt;/strong&gt;: If you encounter the error below, try restarting WeChat &lt;pre&gt;&lt;code class="language-bash"&gt;Failed to export WeChat data. Please ensure WeChat is running and WeChatTweak is installed.
Failed to find or export WeChat data. Exiting.
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìã Click to expand: WeChat-Specific Arguments&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h4&gt;Parameters&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;--export-dir DIR         # Directory to store exported WeChat data (default: wechat_export_direct)
--force-export          # Force re-export even if data exists
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Example Commands&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Search for travel plans discussed in group chats
python -m apps.wechat_rag --query "travel plans" --max-items 10000

# Re-export and search recent chats (useful after new messages)
python -m apps.wechat_rag --force-export --query "work schedule"
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üí¨ Click to expand: Example queries you can try&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;Once the index is built, you can ask questions like:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;"ÊàëÊÉ≥‰π∞È≠îÊúØÂ∏àÁ∫¶Áø∞ÈÄäÁöÑÁêÉË°£ÔºåÁªôÊàë‰∏Ä‰∫õÂØπÂ∫îËÅäÂ§©ËÆ∞ÂΩï?" (Chinese: Show me chat records about buying Magic Johnson's jersey)&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;ü§ñ ChatGPT Chat History: Your Personal AI Conversation Archive!&lt;/h3&gt; 
&lt;p&gt;Transform your ChatGPT conversations into a searchable knowledge base! Search through all your ChatGPT discussions about coding, research, brainstorming, and more.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m apps.chatgpt_rag --export-path chatgpt_export.html --query "How do I create a list in Python?"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Unlock your AI conversation history.&lt;/strong&gt; Never lose track of valuable insights from your ChatGPT discussions again.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìã Click to expand: How to Export ChatGPT Data&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;Step-by-step export process:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;strong&gt;Sign in to ChatGPT&lt;/strong&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Click your profile icon&lt;/strong&gt; in the top right corner&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Navigate to Settings&lt;/strong&gt; ‚Üí &lt;strong&gt;Data Controls&lt;/strong&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Click "Export"&lt;/strong&gt; under Export Data&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Confirm the export&lt;/strong&gt; request&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Download the ZIP file&lt;/strong&gt; from the email link (expires in 24 hours)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Extract or use directly&lt;/strong&gt; with LEANN&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;&lt;strong&gt;Supported formats:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;.html&lt;/code&gt; files from ChatGPT exports&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;.zip&lt;/code&gt; archives from ChatGPT&lt;/li&gt; 
  &lt;li&gt;Directories with multiple export files&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìã Click to expand: ChatGPT-Specific Arguments&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h4&gt;Parameters&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;--export-path PATH           # Path to ChatGPT export file (.html/.zip) or directory (default: ./chatgpt_export)
--separate-messages         # Process each message separately instead of concatenated conversations
--chunk-size N              # Text chunk size (default: 512)
--chunk-overlap N           # Overlap between chunks (default: 128)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Example Commands&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Basic usage with HTML export
python -m apps.chatgpt_rag --export-path conversations.html

# Process ZIP archive from ChatGPT
python -m apps.chatgpt_rag --export-path chatgpt_export.zip

# Search with specific query
python -m apps.chatgpt_rag --export-path chatgpt_data.html --query "Python programming help"

# Process individual messages for fine-grained search
python -m apps.chatgpt_rag --separate-messages --export-path chatgpt_export.html

# Process directory containing multiple exports
python -m apps.chatgpt_rag --export-path ./chatgpt_exports/ --max-items 1000
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üí° Click to expand: Example queries you can try&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;Once your ChatGPT conversations are indexed, you can search with queries like:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;"What did I ask ChatGPT about Python programming?"&lt;/li&gt; 
  &lt;li&gt;"Show me conversations about machine learning algorithms"&lt;/li&gt; 
  &lt;li&gt;"Find discussions about web development frameworks"&lt;/li&gt; 
  &lt;li&gt;"What coding advice did ChatGPT give me?"&lt;/li&gt; 
  &lt;li&gt;"Search for conversations about debugging techniques"&lt;/li&gt; 
  &lt;li&gt;"Find ChatGPT's recommendations for learning resources"&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;ü§ñ Claude Chat History: Your Personal AI Conversation Archive!&lt;/h3&gt; 
&lt;p&gt;Transform your Claude conversations into a searchable knowledge base! Search through all your Claude discussions about coding, research, brainstorming, and more.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m apps.claude_rag --export-path claude_export.json --query "What did I ask about Python dictionaries?"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Unlock your AI conversation history.&lt;/strong&gt; Never lose track of valuable insights from your Claude discussions again.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìã Click to expand: How to Export Claude Data&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;Step-by-step export process:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;strong&gt;Open Claude&lt;/strong&gt; in your browser&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Navigate to Settings&lt;/strong&gt; (look for gear icon or settings menu)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Find Export/Download&lt;/strong&gt; options in your account settings&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Download conversation data&lt;/strong&gt; (usually in JSON format)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Place the file&lt;/strong&gt; in your project directory&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;&lt;em&gt;Note: Claude export methods may vary depending on the interface you're using. Check Claude's help documentation for the most current export instructions.&lt;/em&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Supported formats:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;.json&lt;/code&gt; files (recommended)&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;.zip&lt;/code&gt; archives containing JSON data&lt;/li&gt; 
  &lt;li&gt;Directories with multiple export files&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìã Click to expand: Claude-Specific Arguments&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h4&gt;Parameters&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;--export-path PATH           # Path to Claude export file (.json/.zip) or directory (default: ./claude_export)
--separate-messages         # Process each message separately instead of concatenated conversations
--chunk-size N              # Text chunk size (default: 512)
--chunk-overlap N           # Overlap between chunks (default: 128)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Example Commands&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Basic usage with JSON export
python -m apps.claude_rag --export-path my_claude_conversations.json

# Process ZIP archive from Claude
python -m apps.claude_rag --export-path claude_export.zip

# Search with specific query
python -m apps.claude_rag --export-path claude_data.json --query "machine learning advice"

# Process individual messages for fine-grained search
python -m apps.claude_rag --separate-messages --export-path claude_export.json

# Process directory containing multiple exports
python -m apps.claude_rag --export-path ./claude_exports/ --max-items 1000
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üí° Click to expand: Example queries you can try&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;Once your Claude conversations are indexed, you can search with queries like:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;"What did I ask Claude about Python programming?"&lt;/li&gt; 
  &lt;li&gt;"Show me conversations about machine learning algorithms"&lt;/li&gt; 
  &lt;li&gt;"Find discussions about software architecture patterns"&lt;/li&gt; 
  &lt;li&gt;"What debugging advice did Claude give me?"&lt;/li&gt; 
  &lt;li&gt;"Search for conversations about data structures"&lt;/li&gt; 
  &lt;li&gt;"Find Claude's recommendations for learning resources"&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;üí¨ iMessage History: Your Personal Conversation Archive!&lt;/h3&gt; 
&lt;p&gt;Transform your iMessage conversations into a searchable knowledge base! Search through all your text messages, group chats, and conversations with friends, family, and colleagues.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m apps.imessage_rag --query "What did we discuss about the weekend plans?"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Unlock your message history.&lt;/strong&gt; Never lose track of important conversations, shared links, or memorable moments from your iMessage history.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìã Click to expand: How to Access iMessage Data&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;iMessage data location:&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;iMessage conversations are stored in a SQLite database on your Mac at:&lt;/p&gt; 
 &lt;pre&gt;&lt;code&gt;~/Library/Messages/chat.db
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Important setup requirements:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Grant Full Disk Access&lt;/strong&gt; to your terminal or IDE:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Open &lt;strong&gt;System Preferences&lt;/strong&gt; ‚Üí &lt;strong&gt;Security &amp;amp; Privacy&lt;/strong&gt; ‚Üí &lt;strong&gt;Privacy&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;Select &lt;strong&gt;Full Disk Access&lt;/strong&gt; from the left sidebar&lt;/li&gt; 
    &lt;li&gt;Click the &lt;strong&gt;+&lt;/strong&gt; button and add your terminal app (Terminal, iTerm2) or IDE (VS Code, etc.)&lt;/li&gt; 
    &lt;li&gt;Restart your terminal/IDE after granting access&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Alternative: Use a backup database&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;If you have Time Machine backups or manual copies of the database&lt;/li&gt; 
    &lt;li&gt;Use &lt;code&gt;--db-path&lt;/code&gt; to specify a custom location&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;&lt;strong&gt;Supported formats:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Direct access to &lt;code&gt;~/Library/Messages/chat.db&lt;/code&gt; (default)&lt;/li&gt; 
  &lt;li&gt;Custom database path with &lt;code&gt;--db-path&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;Works with backup copies of the database&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìã Click to expand: iMessage-Specific Arguments&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h4&gt;Parameters&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;--db-path PATH                    # Path to chat.db file (default: ~/Library/Messages/chat.db)
--concatenate-conversations       # Group messages by conversation (default: True)
--no-concatenate-conversations    # Process each message individually
--chunk-size N                    # Text chunk size (default: 1000)
--chunk-overlap N                 # Overlap between chunks (default: 200)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Example Commands&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Basic usage (requires Full Disk Access)
python -m apps.imessage_rag

# Search with specific query
python -m apps.imessage_rag --query "family dinner plans"

# Use custom database path
python -m apps.imessage_rag --db-path /path/to/backup/chat.db

# Process individual messages instead of conversations
python -m apps.imessage_rag --no-concatenate-conversations

# Limit processing for testing
python -m apps.imessage_rag --max-items 100 --query "weekend"
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üí° Click to expand: Example queries you can try&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;Once your iMessage conversations are indexed, you can search with queries like:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;"What did we discuss about vacation plans?"&lt;/li&gt; 
  &lt;li&gt;"Find messages about restaurant recommendations"&lt;/li&gt; 
  &lt;li&gt;"Show me conversations with John about the project"&lt;/li&gt; 
  &lt;li&gt;"Search for shared links about technology"&lt;/li&gt; 
  &lt;li&gt;"Find group chat discussions about weekend events"&lt;/li&gt; 
  &lt;li&gt;"What did mom say about the family gathering?"&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;MCP Integration: RAG on Live Data from Any Platform&lt;/h3&gt; 
&lt;p&gt;Connect to live data sources through the Model Context Protocol (MCP). LEANN now supports real-time RAG on platforms like Slack, Twitter, and more through standardized MCP servers.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Key Benefits:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Live Data Access&lt;/strong&gt;: Fetch real-time data without manual exports&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Standardized Protocol&lt;/strong&gt;: Use any MCP-compatible server&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Easy Extension&lt;/strong&gt;: Add new platforms with minimal code&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Secure Access&lt;/strong&gt;: MCP servers handle authentication&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;üí¨ Slack Messages: Search Your Team Conversations&lt;/h4&gt; 
&lt;p&gt;Transform your Slack workspace into a searchable knowledge base! Find discussions, decisions, and shared knowledge across all your channels.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Test MCP server connection
python -m apps.slack_rag --mcp-server "slack-mcp-server" --test-connection

# Index and search Slack messages
python -m apps.slack_rag \
  --mcp-server "slack-mcp-server" \
  --workspace-name "my-team" \
  --channels general dev-team random \
  --query "What did we decide about the product launch?"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;üìñ Comprehensive Setup Guide&lt;/strong&gt;: For detailed setup instructions, troubleshooting common issues (like "users cache is not ready yet"), and advanced configuration options, see our &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/docs/slack-setup-guide.md"&gt;&lt;strong&gt;Slack Setup Guide&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Quick Setup:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install a Slack MCP server (e.g., &lt;code&gt;npm install -g slack-mcp-server&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Create a Slack App and get API credentials (see detailed guide above)&lt;/li&gt; 
 &lt;li&gt;Set environment variables: &lt;pre&gt;&lt;code class="language-bash"&gt;export SLACK_BOT_TOKEN="xoxb-your-bot-token"
export SLACK_APP_TOKEN="xapp-your-app-token"  # Optional
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Test connection with &lt;code&gt;--test-connection&lt;/code&gt; flag&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Arguments:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--mcp-server&lt;/code&gt;: Command to start the Slack MCP server&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--workspace-name&lt;/code&gt;: Slack workspace name for organization&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--channels&lt;/code&gt;: Specific channels to index (optional)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--concatenate-conversations&lt;/code&gt;: Group messages by channel (default: true)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--max-messages-per-channel&lt;/code&gt;: Limit messages per channel (default: 100)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--max-retries&lt;/code&gt;: Maximum retries for cache sync issues (default: 5)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--retry-delay&lt;/code&gt;: Initial delay between retries in seconds (default: 2.0)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;üê¶ Twitter Bookmarks: Your Personal Tweet Library&lt;/h4&gt; 
&lt;p&gt;Search through your Twitter bookmarks! Find that perfect article, thread, or insight you saved for later.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Test MCP server connection
python -m apps.twitter_rag --mcp-server "twitter-mcp-server" --test-connection

# Index and search Twitter bookmarks
python -m apps.twitter_rag \
  --mcp-server "twitter-mcp-server" \
  --max-bookmarks 1000 \
  --query "What AI articles did I bookmark about machine learning?"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Setup Requirements:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install a Twitter MCP server (e.g., &lt;code&gt;npm install -g twitter-mcp-server&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Get Twitter API credentials: 
  &lt;ul&gt; 
   &lt;li&gt;Apply for a Twitter Developer Account at &lt;a href="https://developer.twitter.com"&gt;developer.twitter.com&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Create a new app in the Twitter Developer Portal&lt;/li&gt; 
   &lt;li&gt;Generate API keys and access tokens with "Read" permissions&lt;/li&gt; 
   &lt;li&gt;For bookmarks access, you may need Twitter API v2 with appropriate scopes&lt;/li&gt; 
  &lt;/ul&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;export TWITTER_API_KEY="your-api-key"
export TWITTER_API_SECRET="your-api-secret"
export TWITTER_ACCESS_TOKEN="your-access-token"
export TWITTER_ACCESS_TOKEN_SECRET="your-access-token-secret"
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Test connection with &lt;code&gt;--test-connection&lt;/code&gt; flag&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Arguments:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--mcp-server&lt;/code&gt;: Command to start the Twitter MCP server&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--username&lt;/code&gt;: Filter bookmarks by username (optional)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--max-bookmarks&lt;/code&gt;: Maximum bookmarks to fetch (default: 1000)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--no-tweet-content&lt;/code&gt;: Exclude tweet content, only metadata&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--no-metadata&lt;/code&gt;: Exclude engagement metadata&lt;/li&gt; 
&lt;/ul&gt;  
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üí° Click to expand: Example queries you can try&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;Slack Queries:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;"What did the team discuss about the project deadline?"&lt;/li&gt; 
  &lt;li&gt;"Find messages about the new feature launch"&lt;/li&gt; 
  &lt;li&gt;"Show me conversations about budget planning"&lt;/li&gt; 
  &lt;li&gt;"What decisions were made in the dev-team channel?"&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;Twitter Queries:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;"What AI articles did I bookmark last month?"&lt;/li&gt; 
  &lt;li&gt;"Find tweets about machine learning techniques"&lt;/li&gt; 
  &lt;li&gt;"Show me bookmarked threads about startup advice"&lt;/li&gt; 
  &lt;li&gt;"What Python tutorials did I save?"&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;summary&gt;&lt;strong&gt;üîß Using MCP with CLI Commands&lt;/strong&gt;&lt;/summary&gt; 
&lt;p&gt;&lt;strong&gt;Want to use MCP data with regular LEANN CLI?&lt;/strong&gt; You can combine MCP apps with CLI commands:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Step 1: Use MCP app to fetch and index data
python -m apps.slack_rag --mcp-server "slack-mcp-server" --workspace-name "my-team"

# Step 2: The data is now indexed and available via CLI
leann search slack_messages "project deadline"
leann ask slack_messages "What decisions were made about the product launch?"

# Same for Twitter bookmarks
python -m apps.twitter_rag --mcp-server "twitter-mcp-server"
leann search twitter_bookmarks "machine learning articles"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;MCP vs Manual Export:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;MCP&lt;/strong&gt;: Live data, automatic updates, requires server setup&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Manual Export&lt;/strong&gt;: One-time setup, works offline, requires manual data export&lt;/li&gt; 
&lt;/ul&gt;  
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üîß Adding New MCP Platforms&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;Want to add support for other platforms? LEANN's MCP integration is designed for easy extension:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;strong&gt;Find or create an MCP server&lt;/strong&gt; for your platform&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Create a reader class&lt;/strong&gt; following the pattern in &lt;code&gt;apps/slack_data/slack_mcp_reader.py&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Create a RAG application&lt;/strong&gt; following the pattern in &lt;code&gt;apps/slack_rag.py&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Test and contribute&lt;/strong&gt; back to the community!&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;&lt;strong&gt;Popular MCP servers to explore:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;GitHub repositories and issues&lt;/li&gt; 
  &lt;li&gt;Discord messages&lt;/li&gt; 
  &lt;li&gt;Notion pages&lt;/li&gt; 
  &lt;li&gt;Google Drive documents&lt;/li&gt; 
  &lt;li&gt;And many more in the MCP ecosystem!&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;üöÄ Claude Code Integration: Transform Your Development Workflow!&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;AST‚ÄëAware Code Chunking&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;LEANN features intelligent code chunking that preserves semantic boundaries (functions, classes, methods) for Python, Java, C#, and TypeScript, improving code understanding compared to text-based chunking.&lt;/p&gt; 
 &lt;p&gt;üìñ Read the &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/docs/ast_chunking_guide.md"&gt;AST Chunking Guide ‚Üí&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;p&gt;&lt;strong&gt;The future of code assistance is here.&lt;/strong&gt; Transform your development workflow with LEANN's native MCP integration for Claude Code. Index your entire codebase and get intelligent code assistance directly in your IDE.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Key features:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üîç &lt;strong&gt;Semantic code search&lt;/strong&gt; across your entire project, fully local index and lightweight&lt;/li&gt; 
 &lt;li&gt;üß† &lt;strong&gt;AST-aware chunking&lt;/strong&gt; preserves code structure (functions, classes)&lt;/li&gt; 
 &lt;li&gt;üìö &lt;strong&gt;Context-aware assistance&lt;/strong&gt; for debugging and development&lt;/li&gt; 
 &lt;li&gt;üöÄ &lt;strong&gt;Zero-config setup&lt;/strong&gt; with automatic language detection&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install LEANN globally for MCP integration
uv tool install leann-core --with leann
claude mcp add --scope user leann-server -- leann_mcp
# Setup is automatic - just start using Claude Code!
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Try our fully agentic pipeline with auto query rewriting, semantic search planning, and more:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/yichuan-w/LEANN/main/assets/mcp_leann.png" alt="LEANN MCP Integration" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;üî• Ready to supercharge your coding?&lt;/strong&gt; &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/packages/leann-mcp/README.md"&gt;Complete Setup Guide ‚Üí&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Command Line Interface&lt;/h2&gt; 
&lt;p&gt;LEANN includes a powerful CLI for document processing and search. Perfect for quick document indexing and interactive chat.&lt;/p&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;p&gt;If you followed the Quick Start, &lt;code&gt;leann&lt;/code&gt; is already installed in your virtual environment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;source .venv/bin/activate
leann --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;To make it globally available:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install the LEANN CLI globally using uv tool
uv tool install leann-core --with leann


# Now you can use leann from anywhere without activating venv
leann --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Global installation is required for Claude Code integration. The &lt;code&gt;leann_mcp&lt;/code&gt; server depends on the globally available &lt;code&gt;leann&lt;/code&gt; command.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Usage Examples&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# build from a specific directory, and my_docs is the index name(Here you can also build from multiple dict or multiple files)
leann build my-docs --docs ./your_documents

# Search your documents
leann search my-docs "machine learning concepts"

# Interactive chat with your documents
leann ask my-docs --interactive

# Ask a single question (non-interactive)
leann ask my-docs "Where are prompts configured?"

# List all your indexes
leann list

# Remove an index
leann remove my-docs
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Key CLI features:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Auto-detects document formats (PDF, TXT, MD, DOCX, PPTX + code files)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üß† AST-aware chunking&lt;/strong&gt; for Python, Java, C#, TypeScript files&lt;/li&gt; 
 &lt;li&gt;Smart text chunking with overlap for all other content&lt;/li&gt; 
 &lt;li&gt;Multiple LLM providers (Ollama, OpenAI, HuggingFace)&lt;/li&gt; 
 &lt;li&gt;Organized index storage in &lt;code&gt;.leann/indexes/&lt;/code&gt; (project-local)&lt;/li&gt; 
 &lt;li&gt;Support for advanced search parameters&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìã Click to expand: Complete CLI Reference&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;You can use &lt;code&gt;leann --help&lt;/code&gt;, or &lt;code&gt;leann build --help&lt;/code&gt;, &lt;code&gt;leann search --help&lt;/code&gt;, &lt;code&gt;leann ask --help&lt;/code&gt;, &lt;code&gt;leann list --help&lt;/code&gt;, &lt;code&gt;leann remove --help&lt;/code&gt; to get the complete CLI reference.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Build Command:&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;leann build INDEX_NAME --docs DIRECTORY|FILE [DIRECTORY|FILE ...] [OPTIONS]

Options:
  --backend {hnsw,diskann}     Backend to use (default: hnsw)
  --embedding-model MODEL      Embedding model (default: facebook/contriever)
  --graph-degree N             Graph degree (default: 32)
  --complexity N               Build complexity (default: 64)
  --force                      Force rebuild existing index
  --compact / --no-compact     Use compact storage (default: true). Must be `no-compact` for `no-recompute` build.
  --recompute / --no-recompute Enable recomputation (default: true)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Search Command:&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;leann search INDEX_NAME QUERY [OPTIONS]

Options:
  --top-k N                     Number of results (default: 5)
  --complexity N                Search complexity (default: 64)
  --recompute / --no-recompute  Enable/disable embedding recomputation (default: enabled). Should not do a `no-recompute` search in a `recompute` build.
  --pruning-strategy {global,local,proportional}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Ask Command:&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;leann ask INDEX_NAME [OPTIONS]

Options:
  --llm {ollama,openai,hf,anthropic}    LLM provider (default: ollama)
  --model MODEL                         Model name (default: qwen3:8b)
  --interactive                         Interactive chat mode
  --top-k N                             Retrieval count (default: 20)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;List Command:&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;leann list

# Lists all indexes across all projects with status indicators:
# ‚úÖ - Index is complete and ready to use
# ‚ùå - Index is incomplete or corrupted
# üìÅ - CLI-created index (in .leann/indexes/)
# üìÑ - App-created index (*.leann.meta.json files)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Remove Command:&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;leann remove INDEX_NAME [OPTIONS]

Options:
  --force, -f    Force removal without confirmation

# Smart removal: automatically finds and safely removes indexes
# - Shows all matching indexes across projects
# - Requires confirmation for cross-project removal
# - Interactive selection when multiple matches found
# - Supports both CLI and app-created indexes
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;üöÄ Advanced Features&lt;/h2&gt; 
&lt;h3&gt;üéØ Metadata Filtering&lt;/h3&gt; 
&lt;p&gt;LEANN supports a simple metadata filtering system to enable sophisticated use cases like document filtering by date/type, code search by file extension, and content management based on custom criteria.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Add metadata during indexing
builder.add_text(
    "def authenticate_user(token): ...",
    metadata={"file_extension": ".py", "lines_of_code": 25}
)

# Search with filters
results = searcher.search(
    query="authentication function",
    metadata_filters={
        "file_extension": {"==": ".py"},
        "lines_of_code": {"&amp;lt;": 100}
    }
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Supported operators&lt;/strong&gt;: &lt;code&gt;==&lt;/code&gt;, &lt;code&gt;!=&lt;/code&gt;, &lt;code&gt;&amp;lt;&lt;/code&gt;, &lt;code&gt;&amp;lt;=&lt;/code&gt;, &lt;code&gt;&amp;gt;&lt;/code&gt;, &lt;code&gt;&amp;gt;=&lt;/code&gt;, &lt;code&gt;in&lt;/code&gt;, &lt;code&gt;not_in&lt;/code&gt;, &lt;code&gt;contains&lt;/code&gt;, &lt;code&gt;starts_with&lt;/code&gt;, &lt;code&gt;ends_with&lt;/code&gt;, &lt;code&gt;is_true&lt;/code&gt;, &lt;code&gt;is_false&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;üìñ &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/docs/metadata_filtering.md"&gt;Complete Metadata filtering guide ‚Üí&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h3&gt;üîç Grep Search&lt;/h3&gt; 
&lt;p&gt;For exact text matching instead of semantic search, use the &lt;code&gt;use_grep&lt;/code&gt; parameter:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Exact text search
results = searcher.search("banana‚Äëcrocodile", use_grep=True, top_k=1)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Use cases&lt;/strong&gt;: Finding specific code patterns, error messages, function names, or exact phrases where semantic similarity isn't needed.&lt;/p&gt; 
&lt;p&gt;üìñ &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/docs/grep_search.md"&gt;Complete grep search guide ‚Üí&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;üèóÔ∏è Architecture &amp;amp; How It Works&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/yichuan-w/LEANN/main/assets/arch.png" alt="LEANN Architecture" width="800" /&gt; &lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;The magic:&lt;/strong&gt; Most vector DBs store every single embedding (expensive). LEANN stores a pruned graph structure (cheap) and recomputes embeddings only when needed (fast).&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Core techniques:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Graph-based selective recomputation:&lt;/strong&gt; Only compute embeddings for nodes in the search path&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;High-degree preserving pruning:&lt;/strong&gt; Keep important "hub" nodes while removing redundant connections&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Dynamic batching:&lt;/strong&gt; Efficiently batch embedding computations for GPU utilization&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Two-level search:&lt;/strong&gt; Smart graph traversal that prioritizes promising nodes&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Backends:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;HNSW&lt;/strong&gt; (default): Ideal for most datasets with maximum storage savings through full recomputation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;DiskANN&lt;/strong&gt;: Advanced option with superior search performance, using PQ-based graph traversal with real-time reranking for the best speed-accuracy trade-off&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Benchmarks&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/benchmarks/diskann_vs_hnsw_speed_comparison.py"&gt;DiskANN vs HNSW Performance Comparison ‚Üí&lt;/a&gt;&lt;/strong&gt; - Compare search performance between both backends&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/benchmarks/compare_faiss_vs_leann.py"&gt;Simple Example: Compare LEANN vs FAISS ‚Üí&lt;/a&gt;&lt;/strong&gt; - See storage savings in action&lt;/p&gt; 
&lt;h3&gt;üìä Storage Comparison&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;System&lt;/th&gt; 
   &lt;th&gt;DPR (2.1M)&lt;/th&gt; 
   &lt;th&gt;Wiki (60M)&lt;/th&gt; 
   &lt;th&gt;Chat (400K)&lt;/th&gt; 
   &lt;th&gt;Email (780K)&lt;/th&gt; 
   &lt;th&gt;Browser (38K)&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Traditional vector database (e.g., FAISS)&lt;/td&gt; 
   &lt;td&gt;3.8 GB&lt;/td&gt; 
   &lt;td&gt;201 GB&lt;/td&gt; 
   &lt;td&gt;1.8 GB&lt;/td&gt; 
   &lt;td&gt;2.4 GB&lt;/td&gt; 
   &lt;td&gt;130 MB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;LEANN&lt;/td&gt; 
   &lt;td&gt;324 MB&lt;/td&gt; 
   &lt;td&gt;6 GB&lt;/td&gt; 
   &lt;td&gt;64 MB&lt;/td&gt; 
   &lt;td&gt;79 MB&lt;/td&gt; 
   &lt;td&gt;6.4 MB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Savings&lt;/td&gt; 
   &lt;td&gt;91%&lt;/td&gt; 
   &lt;td&gt;97%&lt;/td&gt; 
   &lt;td&gt;97%&lt;/td&gt; 
   &lt;td&gt;97%&lt;/td&gt; 
   &lt;td&gt;95%&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Reproduce Our Results&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run benchmarks/run_evaluation.py    # Will auto-download evaluation data and run benchmarks
uv run benchmarks/run_evaluation.py benchmarks/data/indices/rpj_wiki/rpj_wiki --num-queries 2000    # After downloading data, you can run the benchmark with our biggest index
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The evaluation script downloads data automatically on first run. The last three results were tested with partial personal data, and you can reproduce them with your own data!&lt;/p&gt; 
&lt;h2&gt;üî¨ Paper&lt;/h2&gt; 
&lt;p&gt;If you find Leann useful, please cite:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://arxiv.org/abs/2506.08276"&gt;LEANN: A Low-Storage Vector Index&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{wang2025leannlowstoragevectorindex,
      title={LEANN: A Low-Storage Vector Index},
      author={Yichuan Wang and Shu Liu and Zhifei Li and Yongji Wu and Ziming Mao and Yilong Zhao and Xiao Yan and Zhiying Xu and Yang Zhou and Ion Stoica and Sewon Min and Matei Zaharia and Joseph E. Gonzalez},
      year={2025},
      eprint={2506.08276},
      archivePrefix={arXiv},
      primaryClass={cs.DB},
      url={https://arxiv.org/abs/2506.08276},
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;‚ú® &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/docs/features.md"&gt;Detailed Features ‚Üí&lt;/a&gt;&lt;/h2&gt; 
&lt;h2&gt;ü§ù &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/docs/CONTRIBUTING.md"&gt;CONTRIBUTING ‚Üí&lt;/a&gt;&lt;/h2&gt; 
&lt;h2&gt;‚ùì &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/docs/faq.md"&gt;FAQ ‚Üí&lt;/a&gt;&lt;/h2&gt; 
&lt;h2&gt;üìà &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/docs/roadmap.md"&gt;Roadmap ‚Üí&lt;/a&gt;&lt;/h2&gt; 
&lt;h2&gt;üìÑ License&lt;/h2&gt; 
&lt;p&gt;MIT License - see &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/LICENSE"&gt;LICENSE&lt;/a&gt; for details.&lt;/p&gt; 
&lt;h2&gt;üôè Acknowledgments&lt;/h2&gt; 
&lt;p&gt;Core Contributors: &lt;a href="https://yichuan-w.github.io/"&gt;Yichuan Wang&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/andylizf"&gt;Zhifei Li&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Active Contributors: &lt;a href="https://github.com/gabriel-dehan"&gt;Gabriel Dehan&lt;/a&gt;, &lt;a href="https://github.com/ASuresh0524"&gt;Aakash Suresh&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;We welcome more contributors! Feel free to open issues or submit PRs.&lt;/p&gt; 
&lt;p&gt;This work is done at &lt;a href="https://sky.cs.berkeley.edu/"&gt;&lt;strong&gt;Berkeley Sky Computing Lab&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.star-history.com/#yichuan-w/LEANN&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=yichuan-w/LEANN&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;strong&gt;‚≠ê Star us on GitHub if Leann is useful for your research or applications!&lt;/strong&gt; &lt;/p&gt; 
&lt;p align="center"&gt; Made with ‚ù§Ô∏è by the Leann team &lt;/p&gt; 
&lt;h2&gt;ü§ñ Explore LEANN with AI&lt;/h2&gt; 
&lt;p&gt;LEANN is indexed on &lt;a href="https://deepwiki.com/yichuan-w/LEANN"&gt;DeepWiki&lt;/a&gt;, so you can ask questions to LLMs using Deep Research to explore the codebase and get help to add new features.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Polymarket/agents</title>
      <link>https://github.com/Polymarket/agents</link>
      <description>&lt;p&gt;Trade autonomously on Polymarket using AI Agents&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href="https://github.com/polymarket/agents/graphs/contributors"&gt;&lt;img src="https://img.shields.io/github/contributors/polymarket/agents?style=for-the-badge" alt="Contributors" /&gt;&lt;/a&gt; &lt;a href="https://github.com/polymarket/agents/network/members"&gt;&lt;img src="https://img.shields.io/github/forks/polymarket/agents?style=for-the-badge" alt="Forks" /&gt;&lt;/a&gt; &lt;a href="https://github.com/polymarket/agents/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/polymarket/agents?style=for-the-badge" alt="Stargazers" /&gt;&lt;/a&gt; &lt;a href="https://github.com/polymarket/agents/issues"&gt;&lt;img src="https://img.shields.io/github/issues/polymarket/agents?style=for-the-badge" alt="Issues" /&gt;&lt;/a&gt; &lt;a href="https://github.com/polymarket/agents/raw/master/LICENSE.md"&gt;&lt;img src="https://img.shields.io/github/license/polymarket/agents?style=for-the-badge" alt="MIT License" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;!-- PROJECT LOGO --&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/polymarket/agents"&gt; &lt;img src="https://raw.githubusercontent.com/Polymarket/agents/main/docs/images/cli.png" alt="Logo" width="466" height="262" /&gt; &lt;/a&gt; 
 &lt;h3 align="center"&gt;Polymarket Agents&lt;/h3&gt; 
 &lt;p align="center"&gt; Trade autonomously on Polymarket using AI Agents &lt;br /&gt; &lt;a href="https://github.com/polymarket/agents"&gt;&lt;strong&gt;Explore the docs ¬ª&lt;/strong&gt;&lt;/a&gt; &lt;br /&gt; &lt;br /&gt; &lt;a href="https://github.com/polymarket/agents"&gt;View Demo&lt;/a&gt; ¬∑ &lt;a href="https://github.com/polymarket/agents/issues/new?labels=bug&amp;amp;template=bug-report---.md"&gt;Report Bug&lt;/a&gt; ¬∑ &lt;a href="https://github.com/polymarket/agents/issues/new?labels=enhancement&amp;amp;template=feature-request---.md"&gt;Request Feature&lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;!-- CONTENT --&gt; 
&lt;h1&gt;Polymarket Agents&lt;/h1&gt; 
&lt;p&gt;Polymarket Agents is a developer framework and set of utilities for building AI agents for Polymarket.&lt;/p&gt; 
&lt;p&gt;This code is free and publicly available under MIT License open source license (&lt;a href="https://raw.githubusercontent.com/Polymarket/agents/main/#terms-of-service"&gt;terms of service&lt;/a&gt;)!&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Integration with Polymarket API&lt;/li&gt; 
 &lt;li&gt;AI agent utilities for prediction markets&lt;/li&gt; 
 &lt;li&gt;Local and remote RAG (Retrieval-Augmented Generation) support&lt;/li&gt; 
 &lt;li&gt;Data sourcing from betting services, news providers, and web search&lt;/li&gt; 
 &lt;li&gt;Comphrehensive LLM tools for prompt engineering&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Getting started&lt;/h1&gt; 
&lt;p&gt;This repo is inteded for use with Python 3.9&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Clone the repository&lt;/p&gt; &lt;pre&gt;&lt;code&gt;git clone https://github.com/{username}/polymarket-agents.git
cd polymarket-agents
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Create the virtual environment&lt;/p&gt; &lt;pre&gt;&lt;code&gt;virtualenv --python=python3.9 .venv
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Activate the virtual environment&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;On Windows:&lt;/li&gt; 
  &lt;/ul&gt; &lt;pre&gt;&lt;code&gt;.venv\Scripts\activate
&lt;/code&gt;&lt;/pre&gt; 
  &lt;ul&gt; 
   &lt;li&gt;On macOS and Linux:&lt;/li&gt; 
  &lt;/ul&gt; &lt;pre&gt;&lt;code&gt;source .venv/bin/activate
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install the required dependencies:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Set up your environment variables:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Create a &lt;code&gt;.env&lt;/code&gt; file in the project root directory&lt;/li&gt; 
  &lt;/ul&gt; &lt;pre&gt;&lt;code&gt;cp .env.example .env
&lt;/code&gt;&lt;/pre&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Add the following environment variables:&lt;/li&gt; 
  &lt;/ul&gt; &lt;pre&gt;&lt;code&gt;POLYGON_WALLET_PRIVATE_KEY=""
OPENAI_API_KEY=""
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Load your wallet with USDC.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Try the command line interface...&lt;/p&gt; &lt;pre&gt;&lt;code&gt;python scripts/python/cli.py
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Or just go trade!&lt;/p&gt; &lt;pre&gt;&lt;code&gt;python agents/application/trade.py
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Note: If running the command outside of docker, please set the following env var:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;export PYTHONPATH="."
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If running with docker is preferred, we provide the following scripts:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./scripts/bash/build-docker.sh
./scripts/bash/run-docker-dev.sh
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Architecture&lt;/h2&gt; 
&lt;p&gt;The Polymarket Agents architecture features modular components that can be maintained and extended by individual community members.&lt;/p&gt; 
&lt;h3&gt;APIs&lt;/h3&gt; 
&lt;p&gt;Polymarket Agents connectors standardize data sources and order types.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;Chroma.py&lt;/code&gt;: chroma DB for vectorizing news sources and other API data. Developers are able to add their own vector database implementations.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;Gamma.py&lt;/code&gt;: defines &lt;code&gt;GammaMarketClient&lt;/code&gt; class, which interfaces with the Polymarket Gamma API to fetch and parse market and event metadata. Methods to retrieve current and tradable markets, as well as defined information on specific markets and events.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;Polymarket.py&lt;/code&gt;: defines a Polymarket class that interacts with the Polymarket API to retrieve and manage market and event data, and to execute orders on the Polymarket DEX. It includes methods for API key initialization, market and event data retrieval, and trade execution. The file also provides utility functions for building and signing orders, as well as examples for testing API interactions.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;Objects.py&lt;/code&gt;: data models using Pydantic; representations for trades, markets, events, and related entities.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Scripts&lt;/h3&gt; 
&lt;p&gt;Files for managing your local environment, server set-up to run the application remotely, and cli for end-user commands.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;cli.py&lt;/code&gt; is the primary user interface for the repo. Users can run various commands to interact with the Polymarket API, retrieve relevant news articles, query local data, send data/prompts to LLMs, and execute trades in Polymarkets.&lt;/p&gt; 
&lt;p&gt;Commands should follow this format:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;python scripts/python/cli.py command_name [attribute value] [attribute value]&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Example:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;get-all-markets&lt;/code&gt; Retrieve and display a list of markets from Polymarket, sorted by volume.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;python scripts/python/cli.py get-all-markets --limit &amp;lt;LIMIT&amp;gt; --sort-by &amp;lt;SORT_BY&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;limit: The number of markets to retrieve (default: 5).&lt;/li&gt; 
 &lt;li&gt;sort_by: The sorting criterion, either volume (default) or another valid attribute.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Contributing&lt;/h1&gt; 
&lt;p&gt;If you would like to contribute to this project, please follow these steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Fork the repository.&lt;/li&gt; 
 &lt;li&gt;Create a new branch.&lt;/li&gt; 
 &lt;li&gt;Make your changes.&lt;/li&gt; 
 &lt;li&gt;Submit a pull request.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Please run pre-commit hooks before making contributions. To initialize them:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pre-commit install
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;Related Repos&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Polymarket/py-clob-client"&gt;py-clob-client&lt;/a&gt;: Python client for the Polymarket CLOB&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Polymarket/python-order-utils"&gt;python-order-utils&lt;/a&gt;: Python utilities to generate and sign orders from Polymarket's CLOB&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Polymarket/clob-client"&gt;Polymarket CLOB client&lt;/a&gt;: Typescript client for Polymarket CLOB&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/langchain-ai/langchain"&gt;Langchain&lt;/a&gt;: Utility for building context-aware reasoning applications&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.trychroma.com/getting-started"&gt;Chroma&lt;/a&gt;: Chroma is an AI-native open-source vector database&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Prediction markets reading&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;Prediction Markets: Bottlenecks and the Next Major Unlocks, Mikey 0x: &lt;a href="https://mirror.xyz/1kx.eth/jnQhA56Kx9p3RODKiGzqzHGGEODpbskivUUNdd7hwh0"&gt;https://mirror.xyz/1kx.eth/jnQhA56Kx9p3RODKiGzqzHGGEODpbskivUUNdd7hwh0&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;The promise and challenges of crypto + AI applications, Vitalik Buterin: &lt;a href="https://vitalik.eth.limo/general/2024/01/30/cryptoai.html"&gt;https://vitalik.eth.limo/general/2024/01/30/cryptoai.html&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Superforecasting: How to Upgrade Your Company's Judgement, Schoemaker and Tetlock: &lt;a href="https://hbr.org/2016/05/superforecasting-how-to-upgrade-your-companys-judgment"&gt;https://hbr.org/2016/05/superforecasting-how-to-upgrade-your-companys-judgment&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;License&lt;/h1&gt; 
&lt;p&gt;This project is licensed under the MIT License. See the &lt;a href="https://github.com/Polymarket/agents/raw/main/LICENSE.md"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;h1&gt;Contact&lt;/h1&gt; 
&lt;p&gt;For any questions or inquiries, please contact &lt;a href="mailto:liam@polymarket.com"&gt;liam@polymarket.com&lt;/a&gt; or reach out at &lt;a href="http://www.greenestreet.xyz"&gt;www.greenestreet.xyz&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Enjoy using the CLI application! If you encounter any issues, feel free to open an issue on the repository.&lt;/p&gt; 
&lt;h1&gt;Terms of Service&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://polymarket.com/tos"&gt;Terms of Service&lt;/a&gt; prohibit US persons and persons from certain other jurisdictions from trading on Polymarket (via UI &amp;amp; API and including agents developed by persons in restricted jurisdictions), although data and information is viewable globally.&lt;/p&gt; 
&lt;!-- LINKS --&gt;</description>
    </item>
    
    <item>
      <title>BloopAI/vibe-kanban</title>
      <link>https://github.com/BloopAI/vibe-kanban</link>
      <description>&lt;p&gt;Get 10X more out of Claude Code, Codex or any coding agent&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="https://vibekanban.com"&gt; 
  &lt;picture&gt; 
   &lt;source srcset="frontend/public/vibe-kanban-logo-dark.svg" media="(prefers-color-scheme: dark)" /&gt; 
   &lt;source srcset="frontend/public/vibe-kanban-logo.svg" media="(prefers-color-scheme: light)" /&gt; 
   &lt;img src="https://raw.githubusercontent.com/BloopAI/vibe-kanban/main/frontend/public/vibe-kanban-logo.svg?sanitize=true" alt="Vibe Kanban Logo" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt;Get 10X more out of Claude Code, Gemini CLI, Codex, Amp and other coding agents...&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://www.npmjs.com/package/vibe-kanban"&gt;&lt;img alt="npm" src="https://img.shields.io/npm/v/vibe-kanban?style=flat-square" /&gt;&lt;/a&gt; &lt;a href="https://github.com/BloopAI/vibe-kanban/raw/main/.github/workflows/publish.yml"&gt;&lt;img alt="Build status" src="https://img.shields.io/github/actions/workflow/status/BloopAI/vibe-kanban/.github%2Fworkflows%2Fpublish.yml" /&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/BloopAI/vibe-kanban"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;h1 align="center"&gt; &lt;a href="https://jobs.polymer.co/vibe-kanban?source=github"&gt;&lt;strong&gt;We're hiring!&lt;/strong&gt;&lt;/a&gt; &lt;/h1&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/BloopAI/vibe-kanban/main/frontend/public/vibe-kanban-screenshot-overview.png" alt="" /&gt;&lt;/p&gt; 
&lt;h2&gt;Overview&lt;/h2&gt; 
&lt;p&gt;AI coding agents are increasingly writing the world's code and human engineers now spend the majority of their time planning, reviewing, and orchestrating tasks. Vibe Kanban streamlines this process, enabling you to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Easily switch between different coding agents&lt;/li&gt; 
 &lt;li&gt;Orchestrate the execution of multiple coding agents in parallel or in sequence&lt;/li&gt; 
 &lt;li&gt;Quickly review work and start dev servers&lt;/li&gt; 
 &lt;li&gt;Track the status of tasks that your coding agents are working on&lt;/li&gt; 
 &lt;li&gt;Centralise configuration of coding agent MCP configs&lt;/li&gt; 
 &lt;li&gt;Open projects remotely via SSH when running Vibe Kanban on a remote server&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can watch a video overview &lt;a href="https://youtu.be/TFT3KnZOOAk"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;Make sure you have authenticated with your favourite coding agent. A full list of supported coding agents can be found in the &lt;a href="https://vibekanban.com/docs"&gt;docs&lt;/a&gt;. Then in your terminal run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npx vibe-kanban
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;Please head to the &lt;a href="https://vibekanban.com/docs"&gt;website&lt;/a&gt; for the latest documentation and user guides.&lt;/p&gt; 
&lt;h2&gt;Support&lt;/h2&gt; 
&lt;p&gt;We use &lt;a href="https://github.com/BloopAI/vibe-kanban/discussions"&gt;GitHub Discussions&lt;/a&gt; for feature requests. Please open a discussion to create a feature request. For bugs please open an issue on this repo.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We would prefer that ideas and changes are first raised with the core team via &lt;a href="https://github.com/BloopAI/vibe-kanban/discussions"&gt;GitHub Discussions&lt;/a&gt; or &lt;a href="https://discord.gg/AC4nwVtJM3"&gt;Discord&lt;/a&gt;, where we can discuss implementation details and alignment with the existing roadmap. Please do not open PRs without first discussing your proposal with the team.&lt;/p&gt; 
&lt;h2&gt;Development&lt;/h2&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://rustup.rs/"&gt;Rust&lt;/a&gt; (latest stable)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nodejs.org/"&gt;Node.js&lt;/a&gt; (&amp;gt;=18)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pnpm.io/"&gt;pnpm&lt;/a&gt; (&amp;gt;=8)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Additional development tools:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cargo install cargo-watch
cargo install sqlx-cli
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Install dependencies:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pnpm i
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Running the dev server&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pnpm run dev
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will start the backend. A blank DB will be copied from the &lt;code&gt;dev_assets_seed&lt;/code&gt; folder.&lt;/p&gt; 
&lt;h3&gt;Building the frontend&lt;/h3&gt; 
&lt;p&gt;To build just the frontend:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd frontend
pnpm build
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Build from source (macOS)&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Run &lt;code&gt;./local-build.sh&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Test with &lt;code&gt;cd npx-cli &amp;amp;&amp;amp; node bin/cli.js&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Environment Variables&lt;/h3&gt; 
&lt;p&gt;The following environment variables can be configured at build time or runtime:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Default&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;POSTHOG_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Build-time&lt;/td&gt; 
   &lt;td&gt;Empty&lt;/td&gt; 
   &lt;td&gt;PostHog analytics API key (disables analytics if empty)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;POSTHOG_API_ENDPOINT&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Build-time&lt;/td&gt; 
   &lt;td&gt;Empty&lt;/td&gt; 
   &lt;td&gt;PostHog analytics endpoint (disables analytics if empty)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;PORT&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Runtime&lt;/td&gt; 
   &lt;td&gt;Auto-assign&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Production&lt;/strong&gt;: Server port. &lt;strong&gt;Dev&lt;/strong&gt;: Frontend port (backend uses PORT+1)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;BACKEND_PORT&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Runtime&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;0&lt;/code&gt; (auto-assign)&lt;/td&gt; 
   &lt;td&gt;Backend server port (dev mode only, overrides PORT+1)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;FRONTEND_PORT&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Runtime&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;3000&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Frontend dev server port (dev mode only, overrides PORT)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;HOST&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Runtime&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;127.0.0.1&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Backend server host&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;DISABLE_WORKTREE_ORPHAN_CLEANUP&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Runtime&lt;/td&gt; 
   &lt;td&gt;Not set&lt;/td&gt; 
   &lt;td&gt;Disable git worktree cleanup (for debugging)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Build-time variables&lt;/strong&gt; must be set when running &lt;code&gt;pnpm run build&lt;/code&gt;. &lt;strong&gt;Runtime variables&lt;/strong&gt; are read when the application starts.&lt;/p&gt; 
&lt;h3&gt;Remote Deployment&lt;/h3&gt; 
&lt;p&gt;When running Vibe Kanban on a remote server (e.g., via systemctl, Docker, or cloud hosting), you can configure your editor to open projects via SSH:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Access via tunnel&lt;/strong&gt;: Use Cloudflare Tunnel, ngrok, or similar to expose the web UI&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Configure remote SSH&lt;/strong&gt; in Settings ‚Üí Editor Integration: 
  &lt;ul&gt; 
   &lt;li&gt;Set &lt;strong&gt;Remote SSH Host&lt;/strong&gt; to your server hostname or IP&lt;/li&gt; 
   &lt;li&gt;Set &lt;strong&gt;Remote SSH User&lt;/strong&gt; to your SSH username (optional)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Prerequisites&lt;/strong&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;SSH access from your local machine to the remote server&lt;/li&gt; 
   &lt;li&gt;SSH keys configured (passwordless authentication)&lt;/li&gt; 
   &lt;li&gt;VSCode Remote-SSH extension&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;When configured, the "Open in VSCode" buttons will generate URLs like &lt;code&gt;vscode://vscode-remote/ssh-remote+user@host/path&lt;/code&gt; that open your local editor and connect to the remote server.&lt;/p&gt; 
&lt;p&gt;See the &lt;a href="https://vibekanban.com/docs/configuration-customisation/global-settings#remote-ssh-configuration"&gt;documentation&lt;/a&gt; for detailed setup instructions.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>awslabs/amazon-bedrock-agentcore-samples</title>
      <link>https://github.com/awslabs/amazon-bedrock-agentcore-samples</link>
      <description>&lt;p&gt;Amazon Bedrock Agentcore accelerates AI agents into production with the scale, reliability, and security, critical to real-world deployment.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;div&gt; 
  &lt;a href="https://aws.amazon.com/bedrock/agentcore/"&gt; &lt;img width="150" height="150" alt="image" src="https://github.com/user-attachments/assets/b8b9456d-c9e2-45e1-ac5b-760f21f1ac18" /&gt; &lt;/a&gt; 
 &lt;/div&gt; 
 &lt;h1&gt; Amazon Bedrock AgentCore Samples &lt;/h1&gt; 
 &lt;h2&gt; Deploy and operate AI agents securely at scale - using any framework and model &lt;/h2&gt; 
 &lt;div align="center"&gt; 
  &lt;a href="https://github.com/awslabs/amazon-bedrock-agentcore-samples/graphs/commit-activity"&gt;&lt;img alt="GitHub commit activity" src="https://img.shields.io/github/commit-activity/m/awslabs/amazon-bedrock-agentcore-samples" /&gt;&lt;/a&gt; 
  &lt;a href="https://github.com/awslabs/amazon-bedrock-agentcore-samples/issues"&gt;&lt;img alt="GitHub open issues" src="https://img.shields.io/github/issues/awslabs/amazon-bedrock-agentcore-samples" /&gt;&lt;/a&gt; 
  &lt;a href="https://github.com/awslabs/amazon-bedrock-agentcore-samples/pulls"&gt;&lt;img alt="GitHub open pull requests" src="https://img.shields.io/github/issues-pr/awslabs/amazon-bedrock-agentcore-samples" /&gt;&lt;/a&gt; 
  &lt;a href="https://github.com/awslabs/amazon-bedrock-agentcore-samples/raw/main/LICENSE"&gt;&lt;img alt="License" src="https://img.shields.io/github/license/awslabs/amazon-bedrock-agentcore-samples" /&gt;&lt;/a&gt; 
 &lt;/div&gt; 
 &lt;p&gt; &lt;a href="https://docs.aws.amazon.com/bedrock-agentcore/"&gt;Documentation&lt;/a&gt; ‚óÜ &lt;a href="https://github.com/aws/bedrock-agentcore-sdk-python"&gt;Python SDK&lt;/a&gt; ‚óÜ &lt;a href="https://github.com/aws/bedrock-agentcore-starter-toolkit"&gt;Starter Toolkit &lt;/a&gt; ‚óÜ &lt;a href="https://discord.gg/bedrockagentcore-preview"&gt;Discord&lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;Welcome to the Amazon Bedrock AgentCore Samples repository!&lt;/p&gt; 
&lt;p&gt;Amazon Bedrock AgentCore is both framework-agnostic and model-agnostic, giving you the flexibility to deploy and operate advanced AI agents securely and at scale. Whether you‚Äôre building with &lt;a href="https://strandsagents.com/latest/"&gt;Strands Agents&lt;/a&gt;, &lt;a href="https://www.crewai.com/"&gt;CrewAI&lt;/a&gt;, &lt;a href="https://www.langchain.com/langgraph"&gt;LangGraph&lt;/a&gt;, &lt;a href="https://www.llamaindex.ai/"&gt;LlamaIndex&lt;/a&gt;, or any other framework‚Äîand running them on any Large Language Model (LLM)‚ÄîAmazon Bedrock AgentCore provides the infrastructure to support them. By eliminating the undifferentiated heavy lifting of building and managing specialized agent infrastructure, Amazon Bedrock AgentCore lets you bring your preferred framework and model, and deploy without rewriting code.&lt;/p&gt; 
&lt;p&gt;This collection provides examples and tutorials to help you understand, implement, and integrate Amazon Bedrock AgentCore capabilities into your applications.&lt;/p&gt; 
&lt;h2&gt;üé• Video&lt;/h2&gt; 
&lt;p&gt;Build your first production-ready AI agent with Amazon Bedrock AgentCore. We‚Äôll take you beyond prototyping and show you how to productionize your first agentic AI application using Amazon Bedrock AgentCore.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://www.youtube.com/watch?v=wzIQDPFQx30"&gt;&lt;img src="https://markdown-videos-api.jorgenkh.no/youtube/wzIQDPFQx30?width=640&amp;amp;height=360&amp;amp;filetype=jpeg" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;üìÅ Repository Structure&lt;/h2&gt; 
&lt;h3&gt;üìö &lt;a href="https://raw.githubusercontent.com/awslabs/amazon-bedrock-agentcore-samples/main/01-tutorials/"&gt;&lt;code&gt;01-tutorials/&lt;/code&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Interactive Learning &amp;amp; Foundation&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;This folder contains notebook-based tutorials that teach you the fundamentals of Amazon Bedrock AgentCore capabilities through hands-on examples.&lt;/p&gt; 
&lt;p&gt;The structure is divided by AgentCore component:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/awslabs/amazon-bedrock-agentcore-samples/main/01-tutorials/01-AgentCore-runtime"&gt;Runtime&lt;/a&gt;&lt;/strong&gt;: Amazon Bedrock AgentCore Runtime is a secure, serverless runtime capability that empowers organizations to deploy and scale both AI agents and tools, regardless of framework, protocol, or model choice‚Äîenabling rapid prototyping, seamless scaling, and accelerated time to market&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/awslabs/amazon-bedrock-agentcore-samples/main/01-tutorials/02-AgentCore-gateway"&gt;Gateway&lt;/a&gt;&lt;/strong&gt;: AI agents need tools to perform real-world tasks‚Äîfrom searching databases to sending messages. Amazon Bedrock AgentCore Gateway automatically converts APIs, Lambda functions, and existing services into MCP-compatible tools so developers can quickly make these essential capabilities available to agents without managing integrations.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/awslabs/amazon-bedrock-agentcore-samples/main/01-tutorials/04-AgentCore-memory"&gt;Memory&lt;/a&gt;&lt;/strong&gt;: Amazon Bedrock AgentCore Memory makes it easy for developer to build rich, personalized agent experiences with fully-manged memory infrastructure and the ability to customize memory for your needs.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/awslabs/amazon-bedrock-agentcore-samples/main/01-tutorials/03-AgentCore-identity"&gt;Identity&lt;/a&gt;&lt;/strong&gt;: Amazon Bedrock AgentCore Identity provides seamless agent identity and access management across AWS services and third-party applications such as Slack and Zoom while supporting any standard identity providers such as Okta, Entra, and Amazon Cognito.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/awslabs/amazon-bedrock-agentcore-samples/main/01-tutorials/05-AgentCore-tools"&gt;Tools&lt;/a&gt;&lt;/strong&gt;: Amazon Bedrock AgentCore provides two built-in tools to simplify your agentic AI application development: Amazon Bedrock AgentCore &lt;strong&gt;Code Interpreter&lt;/strong&gt; tool enables AI agents to write and execute code securely, enhancing their accuracy and expanding their ability to solve complex end-to-end tasks. Amazon Bedrock AgentCore &lt;strong&gt;Browser Tool&lt;/strong&gt; is an enterprise-grade capability that enables AI agents to navigate websites, complete multi-step forms, and perform complex web-based tasks with human-like precision within a fully managed, secure sandbox environment with low latency&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/awslabs/amazon-bedrock-agentcore-samples/main/01-tutorials/06-AgentCore-observability"&gt;Observability&lt;/a&gt;&lt;/strong&gt;: Amazon Bedrock AgentCore Observability helps developers trace, debug, and monitor agent performance through unified operational dashboards. With support for OpenTelemetry compatible telemetry and detailed visualizations of each step of the agent workflow, Amazon Bedrock AgentCore Observability enables developers to easily gain visibility into agent behavior and maintain quality standards at scale.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/awslabs/amazon-bedrock-agentcore-samples/main/01-tutorials/07-AgentCore-E2E"&gt;AgentCore end-to-end&lt;/a&gt;&lt;/strong&gt;: In this tutorial we will move a customer support agent from prototype to production using Amazon Bedrock AgentCore services.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The examples provided as perfect for beginners and those looking to understand the underlying concepts before building AI Agents applications.&lt;/p&gt; 
&lt;h3&gt;üí° &lt;a href="https://raw.githubusercontent.com/awslabs/amazon-bedrock-agentcore-samples/main/02-use-cases/"&gt;&lt;code&gt;02-use-cases/&lt;/code&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;End-to-end Applications&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Explore practical use case implementations that demonstrate how to apply Amazon Bedrock AgentCore capabilities to solve real business problems.&lt;/p&gt; 
&lt;p&gt;Each use case includes complete implementation focused on the AgentCore components with detailed explanations.&lt;/p&gt; 
&lt;h3&gt;üîå &lt;a href="https://raw.githubusercontent.com/awslabs/amazon-bedrock-agentcore-samples/main/03-integrations/"&gt;&lt;code&gt;03-integrations/&lt;/code&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Framework &amp;amp; Protocol Integration&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Learn how to integrate Amazon Bedrock AgentCore capabilities with popular Agentic frameworks such as Strands Agents, LangChain and CrewAI.&lt;/p&gt; 
&lt;p&gt;Set agent-to-agent communication with A2A and different multi-agent collaboration patterns. Integrate agentic interfaces and learn how to use Amazon Bedrock AgentCore with different entry points.&lt;/p&gt; 
&lt;h3&gt;üèóÔ∏è &lt;a href="https://raw.githubusercontent.com/awslabs/amazon-bedrock-agentcore-samples/main/04-infrastructure-as-code/"&gt;&lt;code&gt;04-infrastructure-as-code/&lt;/code&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Deployment Automation &amp;amp; Infrastructure&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Deploy Amazon Bedrock AgentCore resources Infrastructure as code. We are providing examples using CloudFormation, AWS CDK, or Terraform.&lt;/p&gt; 
&lt;p&gt;Automate infrastructure provisioning with production-ready templates for basic runtimes, MCP servers, multi-agent systems, and complete agent solutions with tools and memory.&lt;/p&gt; 
&lt;h3&gt;üöÄ &lt;a href="https://raw.githubusercontent.com/awslabs/amazon-bedrock-agentcore-samples/main/05-blueprints/"&gt;&lt;code&gt;05-blueprints/&lt;/code&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Full-Stack Reference Applications&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Jump-start your development with complete, deployment-ready agentic applications built on Amazon Bedrock AgentCore.&lt;/p&gt; 
&lt;p&gt;Each blueprint provides a comprehensive foundation with integrated services, authentication, and business logic that you can customize and deploy for your use case.&lt;/p&gt; 
&lt;h2&gt;Running a Notebook&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Create and activate a virtual environment&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m venv .venv
source .venv/bin/activate
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Install dependencies&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt; &lt;p&gt;Export/Activate required AWS Credentials for the notebook to run&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Register your virtual environment as a kernel for Jupyter notebook to use&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m ipykernel install --user --name=notebook-venv --display-name="Python (notebook-venv)"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can list your kernels using:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;jupyter kernelspec list
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="5"&gt; 
 &lt;li&gt;Run the notebook and ensure the correct kernel is selected&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;jupyter notebook path/to/your/notebook.ipynb
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Important:&lt;/strong&gt; After opening the notebook in Jupyter, make sure to select the correct kernel by going to &lt;code&gt;Kernel&lt;/code&gt; ‚Üí &lt;code&gt;Change kernel&lt;/code&gt; ‚Üí select "Python (notebook-venv)" to ensure your virtual environment packages are available.&lt;/p&gt; 
&lt;h2&gt;Quick Start - &lt;a href="https://github.com/aws/bedrock-agentcore-starter-toolkit/raw/main/documentation/docs/user-guide/runtime/quickstart.md"&gt;Amazon Bedrock AgentCore Runtime&lt;/a&gt;&lt;/h2&gt; 
&lt;h3&gt;Step 1: Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;An &lt;a href="https://signin.aws.amazon.com/signin?redirect_uri=https%3A%2F%2Fportal.aws.amazon.com%2Fbilling%2Fsignup%2Fresume&amp;amp;client_id=signup"&gt;AWS account&lt;/a&gt; with credentials configured (&lt;code&gt;aws configure&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.python.org/downloads/"&gt;Python 3.10&lt;/a&gt; or later&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.docker.com/"&gt;Docker&lt;/a&gt; or &lt;a href="https://runfinch.com/"&gt;Finch&lt;/a&gt; installed and running - only for local development&lt;/li&gt; 
 &lt;li&gt;Model Access: Anthropic Claude 4.0 enabled in &lt;a href="https://docs.aws.amazon.com/bedrock/latest/userguide/model-access-modify.html"&gt;Amazon Bedrock console&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;AWS Permissions: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;BedrockAgentCoreFullAccess&lt;/code&gt; managed policy&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;AmazonBedrockFullAccess&lt;/code&gt; managed policy&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;Caller permissions&lt;/code&gt;: See detailed policy &lt;a href="https://github.com/aws/bedrock-agentcore-starter-toolkit/raw/main/documentation/docs/user-guide/runtime/permissions.md#developercaller-permissions"&gt;here&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Step 2: Install and Create Your Agent&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install both packages
pip install bedrock-agentcore strands-agents bedrock-agentcore-starter-toolkit
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Create &lt;code&gt;my_agent.py&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from bedrock_agentcore import BedrockAgentCoreApp
from strands import Agent

app = BedrockAgentCoreApp()
agent = Agent()

@app.entrypoint
def invoke(payload):
    """Your AI agent function"""
    user_message = payload.get("prompt", "Hello! How can I help you today?")
    result = agent(user_message)
    return {"result": result.message}

if __name__ == "__main__":
    app.run()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Create &lt;code&gt;requirements.txt&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cat &amp;gt; requirements.txt &amp;lt;&amp;lt; EOF
bedrock-agentcore
strands-agents
EOF
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Step 3: Test Locally&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Start your agent
python my_agent.py

# Test it (in another terminal)
curl -X POST http://localhost:8080/invocations \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Hello!"}'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Success: You should see a response like {"result": "Hello! I'm here to help..."}&lt;/p&gt; 
&lt;h3&gt;Step 4: Deploy to AWS&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Configure and deploy (auto-creates all required resources)
agentcore configure -e my_agent.py
agentcore launch

# Test your deployed agent
agentcore invoke '{"prompt": "tell me a joke"}'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Congratulations! Your agent is now running on Amazon Bedrock AgentCore Runtime!&lt;/p&gt; 
&lt;p&gt;Follow quickstart guides for &lt;a href="https://github.com/aws/bedrock-agentcore-starter-toolkit/raw/main/documentation/docs/user-guide/gateway/quickstart.md"&gt;Gateway&lt;/a&gt;, &lt;a href="https://github.com/aws/bedrock-agentcore-starter-toolkit/raw/main/documentation/docs/user-guide/identity/quickstart.md"&gt;Identity&lt;/a&gt;, &lt;a href="https://github.com/aws/bedrock-agentcore-starter-toolkit/raw/main/documentation/docs/user-guide/memory/quickstart.md"&gt;Memory&lt;/a&gt;, &lt;a href="https://github.com/aws/bedrock-agentcore-starter-toolkit/raw/main/documentation/docs/user-guide/observability/quickstart.md"&gt;Observability&lt;/a&gt;, and &lt;a href="https://github.com/aws/bedrock-agentcore-starter-toolkit/tree/main/documentation/docs/user-guide/builtin-tools"&gt;builtin-tools&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;üîó Related Links:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://catalog.us-east-1.prod.workshops.aws/workshops/850fcd5c-fd1f-48d7-932c-ad9babede979/en-US"&gt;Getting started with Amazon Bedrock AgentCore - Workshop&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://catalog.workshops.aws/agentcore-deep-dive/en-US"&gt;Diving Deep into Bedrock AgentCore - Workshop&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aws.amazon.com/bedrock/agentcore/pricing/"&gt;Amazon Bedrock AgentCore pricing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aws.amazon.com/bedrock/agentcore/faqs/"&gt;Amazon Bedrock AgentCore FAQs&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ü§ù Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions! Please see our &lt;a href="https://raw.githubusercontent.com/awslabs/amazon-bedrock-agentcore-samples/main/CONTRIBUTING.md"&gt;Contributing Guidelines&lt;/a&gt; for details on:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Adding new samples&lt;/li&gt; 
 &lt;li&gt;Improving existing examples&lt;/li&gt; 
 &lt;li&gt;Reporting issues&lt;/li&gt; 
 &lt;li&gt;Suggesting enhancements&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üìÑ License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the Apache License 2.0 - see the &lt;a href="https://raw.githubusercontent.com/awslabs/amazon-bedrock-agentcore-samples/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;h2&gt;Contributors&lt;/h2&gt; 
&lt;a href="https://github.com/awslabs/amazon-bedrock-agentcore-samples/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=awslabs/amazon-bedrock-agentcore-samples" /&gt; &lt;/a&gt;</description>
    </item>
    
    <item>
      <title>usememos/memos</title>
      <link>https://github.com/usememos/memos</link>
      <description>&lt;p&gt;An open-source, self-hosted note-taking service. Your thoughts, your data, your control ‚Äî no tracking, no ads, no subscription fees.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Memos&lt;/h1&gt; 
&lt;img align="right" height="96px" src="https://raw.githubusercontent.com/usememos/.github/refs/heads/main/assets/logo-rounded.png" alt="Memos" /&gt; 
&lt;p&gt;An open-source, self-hosted note-taking service. Your thoughts, your data, your control ‚Äî no tracking, no ads, no subscription fees.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.usememos.com"&gt;&lt;img src="https://img.shields.io/badge/%F0%9F%8F%A0-usememos.com-blue?style=flat-square" alt="Home" /&gt;&lt;/a&gt; &lt;a href="https://demo.usememos.com/"&gt;&lt;img src="https://img.shields.io/badge/%E2%9C%A8-Try%20Demo-orange?style=flat-square" alt="Live Demo" /&gt;&lt;/a&gt; &lt;a href="https://www.usememos.com/docs"&gt;&lt;img src="https://img.shields.io/badge/%F0%9F%93%9A-Documentation-green?style=flat-square" alt="Docs" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/tfPJa4UmAv"&gt;&lt;img src="https://img.shields.io/badge/%F0%9F%92%AC-Discord-5865f2?style=flat-square&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/neosmemo/memos"&gt;&lt;img src="https://img.shields.io/docker/pulls/neosmemo/memos?style=flat-square&amp;amp;logo=docker" alt="Docker Pulls" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/usememos/.github/refs/heads/main/assets/demo.png" alt="Memos Demo Screenshot" height="512" /&gt; 
&lt;h3&gt;üíé Featured Sponsors&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://go.warp.dev/memos"&gt;&lt;strong&gt;Warp&lt;/strong&gt; ‚Äî The AI-powered terminal built for speed and collaboration&lt;/a&gt;&lt;/p&gt; 
&lt;a href="https://go.warp.dev/memos" target="_blank" rel="noopener"&gt; &lt;img src="https://raw.githubusercontent.com/warpdotdev/brand-assets/main/Github/Sponsor/Warp-Github-LG-02.png" alt="Warp - The AI-powered terminal built for speed and collaboration" width="512" /&gt; &lt;/a&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;a href="https://www.lambdatest.com/?utm_source=memos&amp;amp;utm_medium=sponsor"&gt;&lt;strong&gt;LambdaTest&lt;/strong&gt; - Cross-browser testing cloud&lt;/a&gt;&lt;/p&gt; 
&lt;a href="https://www.lambdatest.com/?utm_source=memos&amp;amp;utm_medium=sponsor" target="_blank" rel="noopener"&gt; &lt;img src="https://www.lambdatest.com/blue-logo.png" alt="LambdaTest - Cross-browser testing cloud" height="50" /&gt; &lt;/a&gt; 
&lt;h2&gt;Overview&lt;/h2&gt; 
&lt;p&gt;Memos is a privacy-first, self-hosted knowledge base that works seamlessly for personal notes, team wikis, and knowledge management. Built with Go and React, it offers lightning-fast performance without compromising on features or usability.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Why choose Memos over cloud services?&lt;/strong&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Feature&lt;/th&gt; 
   &lt;th&gt;Memos&lt;/th&gt; 
   &lt;th&gt;Cloud Services&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Privacy&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ Self-hosted, zero telemetry&lt;/td&gt; 
   &lt;td&gt;‚ùå Your data on their servers&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Cost&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ Free forever, MIT license&lt;/td&gt; 
   &lt;td&gt;‚ùå Subscription fees&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Performance&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ Instant load, no latency&lt;/td&gt; 
   &lt;td&gt;‚ö†Ô∏è Depends on internet&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Ownership&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ Full control &amp;amp; export&lt;/td&gt; 
   &lt;td&gt;‚ùå Vendor lock-in&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;API Access&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ Full REST + gRPC APIs&lt;/td&gt; 
   &lt;td&gt;‚ö†Ô∏è Limited or paid&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Customization&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ Open source, forkable&lt;/td&gt; 
   &lt;td&gt;‚ùå Closed ecosystem&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üîí Privacy-First Architecture&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Self-hosted on your infrastructure with zero telemetry&lt;/li&gt; 
   &lt;li&gt;Complete data ownership and export capabilities&lt;/li&gt; 
   &lt;li&gt;No tracking, no ads, no vendor lock-in&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üìù Markdown Native&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Full markdown support&lt;/li&gt; 
   &lt;li&gt;Plain text storage ‚Äî take your data anywhere&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;‚ö° Blazing Fast&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Built with Go backend and React frontend&lt;/li&gt; 
   &lt;li&gt;Optimized for performance at any scale&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üê≥ Simple Deployment&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;One-line Docker installation&lt;/li&gt; 
   &lt;li&gt;Supports SQLite, MySQL, and PostgreSQL&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üîó Developer-Friendly&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Full REST and gRPC APIs&lt;/li&gt; 
   &lt;li&gt;Easy integration with existing workflows&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üé® Beautiful Interface&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Clean, minimal design and dark mode support&lt;/li&gt; 
   &lt;li&gt;Mobile-responsive layout&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;h3&gt;Docker (Recommended)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run -d \
  --name memos \
  -p 5230:5230 \
  -v ~/.memos:/var/opt/memos \
  neosmemo/memos:stable
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Open &lt;code&gt;http://localhost:5230&lt;/code&gt; and start writing!&lt;/p&gt; 
&lt;h3&gt;Try the Live Demo&lt;/h3&gt; 
&lt;p&gt;Don't want to install yet? Try our &lt;a href="https://demo.usememos.com/"&gt;live demo&lt;/a&gt; first!&lt;/p&gt; 
&lt;h3&gt;Other Installation Methods&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Docker Compose&lt;/strong&gt; - Recommended for production deployments&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Pre-built Binaries&lt;/strong&gt; - Available for Linux, macOS, and Windows&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Kubernetes&lt;/strong&gt; - Helm charts and manifests available&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Build from Source&lt;/strong&gt; - For development and customization&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See our &lt;a href="https://www.usememos.com/docs/installation"&gt;installation guide&lt;/a&gt; for detailed instructions.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions of all kinds! Whether you're fixing bugs, adding features, improving documentation, or helping with translations ‚Äî every contribution matters.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Ways to contribute:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üêõ &lt;a href="https://github.com/usememos/memos/issues/new?template=bug_report.md"&gt;Report bugs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üí° &lt;a href="https://github.com/usememos/memos/issues/new?template=feature_request.md"&gt;Suggest features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üîß &lt;a href="https://github.com/usememos/memos/pulls"&gt;Submit pull requests&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üìñ &lt;a href="https://github.com/usememos/memos/tree/main/docs"&gt;Improve documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üåç &lt;a href="https://github.com/usememos/memos/tree/main/web/src/locales"&gt;Help with translations&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Sponsors&lt;/h2&gt; 
&lt;p&gt;Love Memos? &lt;a href="https://github.com/sponsors/usememos"&gt;Sponsor us on GitHub&lt;/a&gt; to help keep the project growing!&lt;/p&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#usememos/memos&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=usememos/memos&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Memos is open-source software licensed under the &lt;a href="https://raw.githubusercontent.com/usememos/memos/main/LICENSE"&gt;MIT License&lt;/a&gt;.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://www.usememos.com"&gt;Website&lt;/a&gt;&lt;/strong&gt; ‚Ä¢ &lt;strong&gt;&lt;a href="https://www.usememos.com/docs"&gt;Documentation&lt;/a&gt;&lt;/strong&gt; ‚Ä¢ &lt;strong&gt;&lt;a href="https://demo.usememos.com/"&gt;Demo&lt;/a&gt;&lt;/strong&gt; ‚Ä¢ &lt;strong&gt;&lt;a href="https://discord.gg/tfPJa4UmAv"&gt;Discord&lt;/a&gt;&lt;/strong&gt; ‚Ä¢ &lt;strong&gt;&lt;a href="https://x.com/usememos"&gt;X/Twitter&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;a href="https://vercel.com/oss"&gt; &lt;img alt="Vercel OSS Program" src="https://vercel.com/oss/program-badge.svg?sanitize=true" /&gt; &lt;/a&gt;</description>
    </item>
    
    <item>
      <title>afkarxyz/SpotiFLAC</title>
      <link>https://github.com/afkarxyz/SpotiFLAC</link>
      <description>&lt;p&gt;Get Spotify tracks in true FLAC from Tidal, Qobuz &amp; Amazon Music ‚Äî no account required.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href="https://github.com/afkarxyz/SpotiFLAC/releases"&gt;&lt;img src="https://img.shields.io/github/downloads/afkarxyz/SpotiFLAC/total?style=for-the-badge" alt="GitHub All Releases" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/a6e92fdd-2944-45c1-83e8-e23a26c827af" alt="Image" /&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;Get Spotify tracks in true FLAC from Tidal, Qobuz &amp;amp; Amazon Music ‚Äî no account required.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://img.shields.io/badge/Windows-10%2B-0078D6?style=for-the-badge&amp;amp;logo=data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI1MTIiIGhlaWdodD0iNTEyIiB2aWV3Qm94PSIwIDAgMjAgMjAiPjxwYXRoIGZpbGw9IiNmZmZmZmYiIGZpbGwtcnVsZT0iZXZlbm9kZCIgZD0iTTIwIDEwLjg3M1YyMEw4LjQ3OSAxOC41MzdsLjAwMS03LjY2NEgyMFptLTEzLjEyIDBsLS4wMDEgNy40NjFMMCAxNy40NjF2LTYuNTg4aDYuODhaTTIwIDkuMjczSDguNDhsLS4wMDEtNy44MUwyMCAwdjkuMjczWk02Ljg3OSAxLjY2NmwuMDAxIDcuNjA3SDBWMi41MzlsNi44NzktLjg3M1oiLz48L3N2Zz4=" alt="Windows" /&gt; &lt;img src="https://img.shields.io/badge/macOS-10.13%2B-000000?style=for-the-badge&amp;amp;logo=apple&amp;amp;logoColor=white" alt="macOS" /&gt; &lt;img src="https://img.shields.io/badge/Linux-Any-FCC624?style=for-the-badge&amp;amp;logo=linux&amp;amp;logoColor=white" alt="Linux" /&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h3&gt;&lt;a href="https://github.com/afkarxyz/SpotiFLAC/releases"&gt;Download&lt;/a&gt;&lt;/h3&gt; 
&lt;h2&gt;Screenshot&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/afe01529-bcf0-4486-8792-62af26adafee" alt="Image" /&gt;&lt;/p&gt; 
&lt;h2&gt;Other projects&lt;/h2&gt; 
&lt;h3&gt;&lt;a href="https://github.com/zarzet/SpotiFLAC-Mobile"&gt;SpotiFLAC Mobile&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Mobile port of SpotiFLAC for Android &amp;amp; iOS ‚Äî maintained by &lt;a href="https://github.com/zarzet"&gt;@zarzet&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;&lt;a href="https://github.com/afkarxyz/SpotiDownloader"&gt;SpotiDownloader&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Get Spotify tracks in MP3 and FLAC via the spotidownloader.com API&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://ko-fi.com/afkarxyz"&gt;&lt;img src="https://ko-fi.com/img/githubbutton_sm.svg?sanitize=true" alt="Ko-fi" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Disclaimer&lt;/h2&gt; 
&lt;p&gt;This project is for &lt;strong&gt;educational and private use only&lt;/strong&gt;. The developer does not condone or encourage copyright infringement.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;SpotiFLAC&lt;/strong&gt; is a third-party tool and is not affiliated with, endorsed by, or connected to Spotify, Tidal, Qobuz, Amazon Music, or any other streaming service.&lt;/p&gt; 
&lt;p&gt;You are solely responsible for:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Ensuring your use of this software complies with your local laws.&lt;/li&gt; 
 &lt;li&gt;Reading and adhering to the Terms of Service of the respective platforms.&lt;/li&gt; 
 &lt;li&gt;Any legal consequences resulting from the misuse of this tool.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;The software is provided "as is", without warranty of any kind. The author assumes no liability for any bans, damages, or legal issues arising from its use.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>harvard-edge/cs249r_book</title>
      <link>https://github.com/harvard-edge/cs249r_book</link>
      <description>&lt;p&gt;Introduction to Machine Learning Systems&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Machine Learning Systems&lt;/h1&gt; 
&lt;p&gt;&lt;em&gt;Principles and Practices of Engineering Artificially Intelligent Systems&lt;/em&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p align="center"&gt; &lt;/p&gt;
 &lt;p&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book/actions/workflows/book-validate-dev.yml"&gt;&lt;img src="https://img.shields.io/github/actions/workflow/status/harvard-edge/cs249r_book/book-validate-dev.yml?branch=dev&amp;amp;label=Book&amp;amp;logo=githubactions&amp;amp;cacheSeconds=300" alt="Book" /&gt;&lt;/a&gt; &lt;a href="https://github.com/harvard-edge/cs249r_book/actions/workflows/tinytorch-ci.yml"&gt;&lt;img src="https://img.shields.io/github/actions/workflow/status/harvard-edge/cs249r_book/tinytorch-ci.yml?branch=dev&amp;amp;label=TinyTorch&amp;amp;logo=python&amp;amp;cacheSeconds=300" alt="TinyTorch" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/github/last-commit/harvard-edge/cs249r_book/dev?label=Updated&amp;amp;logo=git&amp;amp;cacheSeconds=300" alt="Updated" /&gt; &lt;a href="https://github.com/harvard-edge/cs249r_book/raw/dev/LICENSE.md"&gt;&lt;img src="https://img.shields.io/badge/License-CC--BY--NC--ND%204.0-blue.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/harvard-edge/cs249r_book/dev/#-citation--license"&gt;&lt;img src="https://img.shields.io/badge/Cite-IEEE%202024-blue?logo=ieee" alt="Cite" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/mlsysbook"&gt;&lt;img src="https://img.shields.io/badge/Fund%20Us-Open%20Collective-blue.svg?logo=open-collective" alt="Fund Us" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;/p&gt; 
 &lt;p align="center"&gt; 
  &lt;!-- Reader Navigation --&gt; &lt;/p&gt;
 &lt;p&gt;&lt;strong&gt;&lt;a href="https://mlsysbook.ai"&gt;üìñ Read Online&lt;/a&gt;&lt;/strong&gt; ‚Ä¢ &lt;strong&gt;&lt;a href="https://mlsysbook.ai/tinytorch"&gt;Tinyüî•Torch&lt;/a&gt;&lt;/strong&gt; ‚Ä¢ &lt;strong&gt;&lt;a href="https://mlsysbook.ai/pdf"&gt;üìÑ Download PDF&lt;/a&gt;&lt;/strong&gt; ‚Ä¢ &lt;strong&gt;&lt;a href="https://mlsysbook.ai/epub"&gt;üìì Download EPUB&lt;/a&gt;&lt;/strong&gt; ‚Ä¢ &lt;strong&gt;&lt;a href="https://mlsysbook.org"&gt;üåê Explore Ecosystem&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;/p&gt; 
 &lt;p&gt;üìö &lt;strong&gt;Hardcopy edition coming 2026 with MIT Press.&lt;/strong&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Mission&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;The world is rushing to build AI systems. It is not engineering them.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;That gap is what we mean by AI engineering.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;AI engineering is the discipline of building efficient, reliable, safe, and robust intelligent systems that operate in the real world, not just models in isolation.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Our mission:&lt;/strong&gt; Establish AI engineering as a foundational discipline, alongside software engineering and computer engineering, by teaching how to design, build, and evaluate end to end intelligent systems. The long term impact of AI will be shaped by engineers who can turn ideas into working, dependable systems.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;What‚Äôs in this repo&lt;/h2&gt; 
&lt;p&gt;This repository is the open learning stack for AI systems engineering.&lt;/p&gt; 
&lt;p&gt;It includes the textbook source, TinyTorch, hardware kits, and upcoming co-labs that connect principles to runnable code and real devices.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Start Here&lt;/h2&gt; 
&lt;p&gt;Choose a path based on your goal.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;READ&lt;/strong&gt; Start with the &lt;a href="https://mlsysbook.ai"&gt;textbook&lt;/a&gt;. Try &lt;a href="https://www.mlsysbook.ai/contents/core/introduction/introduction.html"&gt;Chapter 1&lt;/a&gt; and the &lt;a href="https://mlsysbook.ai/contents/core/benchmarking/benchmarking.html"&gt;Benchmarking chapter&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;BUILD&lt;/strong&gt; Start TinyTorch with the &lt;a href="https://mlsysbook.ai/tinytorch/getting-started.html"&gt;getting started guide&lt;/a&gt;. Begin with Module 01 and work up from CNNs to transformers and the MLPerf benchmarks.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;DEPLOY&lt;/strong&gt; Pick a &lt;a href="https://mlsysbook.ai/kits"&gt;hardware kit&lt;/a&gt; and run the labs on Arduino, Raspberry Pi, and other edge devices.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;CONNECT&lt;/strong&gt; Say hello in &lt;a href="https://github.com/harvard-edge/cs249r_book/discussions"&gt;Discussions&lt;/a&gt;. We will do our best to reply.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;The Learning Stack&lt;/h2&gt; 
&lt;p&gt;The learning stack below shows how the textbook connects to hands on work and deployment. Read the textbook, then pick your path:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                                               ‚îÇ
‚îÇ                           MACHINE LEARNING SYSTEMS                            ‚îÇ
‚îÇ                              Read the Textbook                                ‚îÇ
‚îÇ                                                                               ‚îÇ
‚îÇ                    Theory ‚Ä¢ Concepts ‚Ä¢ Best Practices                         ‚îÇ
‚îÇ                                                                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                        ‚îÇ
                          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                          ‚îÇ             ‚îÇ             ‚îÇ
                          ‚ñº             ‚ñº             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                            HANDS-ON ACTIVITIES                                ‚îÇ
‚îÇ                           (pick one or all)                                   ‚îÇ
‚îÇ                                                                               ‚îÇ
‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ     ‚îÇ                 ‚îÇ      ‚îÇ                 ‚îÇ      ‚îÇ                 ‚îÇ     ‚îÇ
‚îÇ     ‚îÇ    SOFTWARE     ‚îÇ      ‚îÇ    TINYTORCH    ‚îÇ      ‚îÇ    HARDWARE     ‚îÇ     ‚îÇ
‚îÇ     ‚îÇ    CO-LABS      ‚îÇ      ‚îÇ    FRAMEWORK    ‚îÇ      ‚îÇ      LABS       ‚îÇ     ‚îÇ
‚îÇ     ‚îÇ                 ‚îÇ      ‚îÇ                 ‚îÇ      ‚îÇ                 ‚îÇ     ‚îÇ
‚îÇ     ‚îÇ EXPLORE         ‚îÇ      ‚îÇ BUILD           ‚îÇ      ‚îÇ DEPLOY          ‚îÇ     ‚îÇ
‚îÇ     ‚îÇ                 ‚îÇ      ‚îÇ                 ‚îÇ      ‚îÇ                 ‚îÇ     ‚îÇ
‚îÇ     ‚îÇ Run controlled  ‚îÇ      ‚îÇ Understand      ‚îÇ      ‚îÇ Engineer under  ‚îÇ     ‚îÇ
‚îÇ     ‚îÇ experiments on  ‚îÇ      ‚îÇ frameworks by   ‚îÇ      ‚îÇ real constraints‚îÇ     ‚îÇ
‚îÇ     ‚îÇ latency, memory,‚îÇ      ‚îÇ implementing    ‚îÇ      ‚îÇ memory, power,  ‚îÇ     ‚îÇ
‚îÇ     ‚îÇ energy, cost    ‚îÇ      ‚îÇ them            ‚îÇ      ‚îÇ timing, safety  ‚îÇ     ‚îÇ
‚îÇ     ‚îÇ                 ‚îÇ      ‚îÇ                 ‚îÇ      ‚îÇ                 ‚îÇ     ‚îÇ
‚îÇ     ‚îÇ (coming 2026)   ‚îÇ      ‚îÇ                 ‚îÇ      ‚îÇ Arduino, Pi     ‚îÇ     ‚îÇ
‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
‚îÇ                                                                               ‚îÇ
‚îÇ           EXPLORE                  BUILD                   DEPLOY             ‚îÇ
‚îÇ                                                                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                        ‚îÇ
                                        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                                               ‚îÇ
‚îÇ                                  AI OLYMPICS                                  ‚îÇ
‚îÇ                                 Prove Mastery                                 ‚îÇ
‚îÇ                                                                               ‚îÇ
‚îÇ       Compete across all tracks ‚Ä¢ University teams ‚Ä¢ Public leaderboards      ‚îÇ
‚îÇ                                                                               ‚îÇ
‚îÇ                                (coming 2026)                                  ‚îÇ
‚îÇ                                                                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
&lt;/code&gt;&lt;/pre&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th&gt;Component&lt;/th&gt; 
   &lt;th&gt;What You Do&lt;/th&gt; 
   &lt;th&gt;Link&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;READ&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://mlsysbook.ai"&gt;üìñ Textbook&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Understand ML systems concepts&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/harvard-edge/cs249r_book/dev/book/README.md"&gt;book/&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;EXPLORE&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;üîÆ Software Co-Labs&lt;/td&gt; 
   &lt;td&gt;Run controlled experiments on latency, memory, energy, cost&lt;/td&gt; 
   &lt;td&gt;&lt;em&gt;Coming 2026&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;BUILD&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://mlsysbook.ai/tinytorch"&gt;üî• TinyTorch&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Understand frameworks by implementing them&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/harvard-edge/cs249r_book/dev/tinytorch/README.md"&gt;tinytorch/&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;DEPLOY&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://mlsysbook.ai/kits"&gt;üîß Hardware Kits&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Engineer under real constraints: memory, power, timing, safety&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/harvard-edge/cs249r_book/dev/kits/README.md"&gt;kits/&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;PROVE&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;üèÜ AI Olympics&lt;/td&gt; 
   &lt;td&gt;Compete and benchmark across all tracks&lt;/td&gt; 
   &lt;td&gt;&lt;em&gt;Coming 2026&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;What each path teaches:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;EXPLORE&lt;/strong&gt; teaches &lt;em&gt;why&lt;/em&gt; ‚Äî Understand tradeoffs. Change batch sizes, precision, model architectures and see how latency, memory, and accuracy shift.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;BUILD&lt;/strong&gt; teaches &lt;em&gt;how&lt;/em&gt; ‚Äî Understand internals. Implement autograd, optimizers, and attention from scratch to see how TensorFlow and PyTorch actually work.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;DEPLOY&lt;/strong&gt; teaches &lt;em&gt;where&lt;/em&gt; ‚Äî Understand constraints. Face real memory limits, power budgets, and latency requirements on actual hardware.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;What You Will Learn&lt;/h2&gt; 
&lt;p&gt;This textbook teaches you to think at the intersection of machine learning and systems engineering. Each chapter bridges algorithmic concepts with the infrastructure that makes them work in practice.&lt;/p&gt; 
&lt;h3&gt;The ML ‚Üî Systems Bridge&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;ML Concept&lt;/th&gt; 
   &lt;th&gt;Systems Concept&lt;/th&gt; 
   &lt;th&gt;What You Learn&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Model parameters&lt;/td&gt; 
   &lt;td&gt;Memory constraints&lt;/td&gt; 
   &lt;td&gt;How to fit large models on resource-limited devices&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Inference latency&lt;/td&gt; 
   &lt;td&gt;Hardware acceleration&lt;/td&gt; 
   &lt;td&gt;How GPUs, TPUs, and accelerators execute neural networks&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Training convergence&lt;/td&gt; 
   &lt;td&gt;Compute efficiency&lt;/td&gt; 
   &lt;td&gt;How mixed-precision and optimization techniques reduce cost&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Model accuracy&lt;/td&gt; 
   &lt;td&gt;Quantization and pruning&lt;/td&gt; 
   &lt;td&gt;How to compress models while preserving performance&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Data requirements&lt;/td&gt; 
   &lt;td&gt;Pipeline infrastructure&lt;/td&gt; 
   &lt;td&gt;How to build efficient data loading and preprocessing&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Model deployment&lt;/td&gt; 
   &lt;td&gt;MLOps practices&lt;/td&gt; 
   &lt;td&gt;How to monitor, version, and update models in production&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Privacy constraints&lt;/td&gt; 
   &lt;td&gt;On-device learning&lt;/td&gt; 
   &lt;td&gt;How to train and adapt models without sending data to the cloud&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Book Structure&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Part&lt;/th&gt; 
   &lt;th&gt;Focus&lt;/th&gt; 
   &lt;th&gt;Chapters&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;I. Foundations&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Core concepts&lt;/td&gt; 
   &lt;td&gt;Introduction, ML Systems, DL Primer, Architectures&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;II. Design&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Building blocks&lt;/td&gt; 
   &lt;td&gt;Workflow, Data Engineering, Frameworks, Training&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;III. Performance&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Making it fast&lt;/td&gt; 
   &lt;td&gt;Efficient AI, Optimizations, HW Acceleration, Benchmarking&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;IV. Deployment&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Making it work&lt;/td&gt; 
   &lt;td&gt;MLOps, On-device Learning, Privacy, Robustness&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;V. Trust&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Making it right&lt;/td&gt; 
   &lt;td&gt;Responsible AI, Sustainable AI, AI for Good&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;VI. Frontiers&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;What's next&lt;/td&gt; 
   &lt;td&gt;Emerging trends and future directions&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h2&gt;What Makes This Different&lt;/h2&gt; 
&lt;p&gt;This is a living textbook. We keep it updated as the field grows, with community input along the way.&lt;/p&gt; 
&lt;p&gt;AI may feel like it is moving at lightning speed, but the engineering building blocks that make it work do not change as quickly as the headlines. This project is built around those stable foundations.&lt;/p&gt; 
&lt;p&gt;Think of it like LEGO. New sets arrive all the time, but the bricks themselves stay the same. Once you learn how the bricks fit together, you can build anything. Here, those "AI bricks" are the solid systems principles that make AI work.&lt;/p&gt; 
&lt;p&gt;Whether you are reading a chapter, running a lab, or sharing feedback, you are helping make these ideas more accessible to the next learner.&lt;/p&gt; 
&lt;h3&gt;Research to Teaching Loop&lt;/h3&gt; 
&lt;p&gt;We use the same loop for research and teaching: define the system problem, build a reference implementation, benchmark it, then turn it into curriculum and tooling so others can reproduce and extend it.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Loop Step&lt;/th&gt; 
   &lt;th&gt;Research Artifacts&lt;/th&gt; 
   &lt;th&gt;Teaching Artifacts&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Measure&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Benchmarks, suites, metrics&lt;/td&gt; 
   &lt;td&gt;Benchmarking chapter, assignments&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Build&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Reference systems, compilers, runtimes&lt;/td&gt; 
   &lt;td&gt;TinyTorch modules, co-labs&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Deploy&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Hardware targets, constraints, reliability&lt;/td&gt; 
   &lt;td&gt;Hardware labs, kits&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Support This Work&lt;/h2&gt; 
&lt;p&gt;We are working toward &lt;strong&gt;1 million learners by 2030&lt;/strong&gt; so that AI engineering becomes a shared, teachable discipline, not a collection of isolated practices. Every star, share, and contribution helps move this effort forward.&lt;/p&gt; 
&lt;h3&gt;Why GitHub Stars Matter&lt;/h3&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;em&gt;What gets measured gets improved.&lt;/em&gt;&lt;/p&gt; 
 &lt;p&gt;Each star is a learner, educator, or supporter who believes AI systems should be engineered with rigor and real world constraints in mind.&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/harvard-edge/cs249r_book?style=for-the-badge&amp;amp;logo=github&amp;amp;color=gold" alt="Stars" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://star-history.com/#harvard-edge/cs249r_book&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=harvard-edge/cs249r_book&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;1 learner ‚Üí 10 learners ‚Üí 100 learners ‚Üí 1,000 learners ‚Üí &lt;strong&gt;10,000 learners&lt;/strong&gt; ‚Üí 100,000 learners ‚Üí &lt;strong&gt;1M learners&lt;/strong&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;Stars are not the goal. They are a signal.&lt;/p&gt; 
&lt;p&gt;A visible, growing community makes it easier for universities, foundations, and industry partners to adopt this material, donate hardware, and fund workshops. That momentum lowers the barrier for the next institution, the next classroom, and the next cohort of learners.&lt;/p&gt; 
&lt;p&gt;Support raised through this signal flows into &lt;a href="https://opencollective.com/mlsysbook"&gt;Open Collective&lt;/a&gt; and funds concrete outcomes such as TinyML4D workshops, hardware kits for underserved classrooms, and the infrastructure required to keep this resource free and open.&lt;/p&gt; 
&lt;p&gt;One click can unlock the next classroom, the next contributor, and the next generation of AI engineers.&lt;/p&gt; 
&lt;h3&gt;Fund the Mission&lt;/h3&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;All contributions go to &lt;a href="https://opencollective.com/mlsysbook"&gt;Open Collective&lt;/a&gt;, a transparent fund that supports educational outreach.&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://opencollective.com/mlsysbook"&gt;&lt;img src="https://img.shields.io/badge/%F0%9F%92%9D%20Support%20AI%20Education-Open%20Collective-blue.svg?style=for-the-badge" alt="Open Collective" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Community and Resources&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Resource&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://mlsysbook.ai"&gt;üìñ &lt;strong&gt;Textbook&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Interactive online textbook&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://mlsysbook.ai/tinytorch"&gt;üî• &lt;strong&gt;TinyTorch&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Build ML frameworks from scratch&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://mlsysbook.ai/kits"&gt;üîß &lt;strong&gt;Hardware Kits&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Deploy to Arduino, Raspberry Pi, edge devices&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://mlsysbook.org"&gt;üåê &lt;strong&gt;Ecosystem&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Resources, workshops, and community&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book/discussions"&gt;üí¨ &lt;strong&gt;Discussions&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Questions and ideas&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions to the book, TinyTorch, and hardware kits!&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;I want to...&lt;/th&gt; 
   &lt;th&gt;Go here&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Fix a typo or improve a chapter&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/harvard-edge/cs249r_book/dev/book/docs/CONTRIBUTING.md"&gt;book/docs/CONTRIBUTING.md&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Add a TinyTorch module or fix a bug&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/harvard-edge/cs249r_book/dev/tinytorch/CONTRIBUTING.md"&gt;tinytorch/CONTRIBUTING.md&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Improve hardware labs&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/harvard-edge/cs249r_book/dev/kits/README.md"&gt;kits/README.md&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Report an issue&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book/issues"&gt;GitHub Issues&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ask a question&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book/discussions"&gt;GitHub Discussions&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Citation &amp;amp; License&lt;/h2&gt; 
&lt;h3&gt;Citation&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@inproceedings{reddi2024mlsysbook,
  title        = {MLSysBook.AI: Principles and Practices of Machine Learning Systems Engineering},
  author       = {Reddi, Vijay Janapa},
  booktitle    = {2024 International Conference on Hardware/Software Codesign and System Synthesis (CODES+ ISSS)},
  pages        = {41--42},
  year         = {2024},
  organization = {IEEE},
  url          = {https://mlsysbook.org}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;License&lt;/h3&gt; 
&lt;p&gt;This project uses a dual-license structure:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Component&lt;/th&gt; 
   &lt;th&gt;License&lt;/th&gt; 
   &lt;th&gt;What It Means&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Book content&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/harvard-edge/cs249r_book/dev/LICENSE.md"&gt;CC BY-NC-ND 4.0&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Share freely with attribution; no commercial use; no derivatives&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;TinyTorch code&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/harvard-edge/cs249r_book/dev/tinytorch/LICENSE"&gt;Apache 2.0&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Use, modify, and distribute freely; includes patent protection&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;The textbook content (chapters, figures, explanations) is educational material that should circulate with attribution and without commercial exploitation. The software framework is a tool designed to be easy for anyone to use, modify, or integrate into their own projects.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Contributors&lt;/h2&gt; 
&lt;p&gt;Thanks goes to these wonderful people who have contributed to making this resource better for everyone:&lt;/p&gt; 
&lt;!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section --&gt; 
&lt;!-- prettier-ignore-start --&gt; 
&lt;!-- markdownlint-disable --&gt; 
&lt;table&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/profvjreddi"&gt;&lt;img src="https://avatars.githubusercontent.com/profvjreddi?s=100" width="100px;" alt="Vijay Janapa Reddi" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Vijay Janapa Reddi&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/hzeljko"&gt;&lt;img src="https://avatars.githubusercontent.com/hzeljko?s=100" width="100px;" alt="Zeljko Hrcek" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Zeljko Hrcek&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/Mjrovai"&gt;&lt;img src="https://avatars.githubusercontent.com/Mjrovai?s=100" width="100px;" alt="Marcelo Rovai" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Marcelo Rovai&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/jasonjabbour"&gt;&lt;img src="https://avatars.githubusercontent.com/jasonjabbour?s=100" width="100px;" alt="Jason Jabbour" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Jason Jabbour&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/kai4avaya"&gt;&lt;img src="https://avatars.githubusercontent.com/kai4avaya?s=100" width="100px;" alt="Kai Kleinbard" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Kai Kleinbard&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/uchendui"&gt;&lt;img src="https://avatars.githubusercontent.com/uchendui?s=100" width="100px;" alt="Ikechukwu Uchendu" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Ikechukwu Uchendu&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/Naeemkh"&gt;&lt;img src="https://avatars.githubusercontent.com/Naeemkh?s=100" width="100px;" alt="Naeem Khoshnevis" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Naeem Khoshnevis&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/Sara-Khosravi"&gt;&lt;img src="https://avatars.githubusercontent.com/Sara-Khosravi?s=100" width="100px;" alt="Sara Khosravi" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Sara Khosravi&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/V0XNIHILI"&gt;&lt;img src="https://avatars.githubusercontent.com/V0XNIHILI?s=100" width="100px;" alt="Douwe den Blanken" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Douwe den Blanken&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/18jeffreyma"&gt;&lt;img src="https://avatars.githubusercontent.com/18jeffreyma?s=100" width="100px;" alt="Jeffrey Ma" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Jeffrey Ma&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/didier-durand"&gt;&lt;img src="https://avatars.githubusercontent.com/didier-durand?s=100" width="100px;" alt="Didier Durand" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Didier Durand&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/shanzehbatool"&gt;&lt;img src="https://avatars.githubusercontent.com/shanzehbatool?s=100" width="100px;" alt="shanzehbatool" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;shanzehbatool&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/eliasab16"&gt;&lt;img src="https://avatars.githubusercontent.com/eliasab16?s=100" width="100px;" alt="Elias" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Elias&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/JaredP94"&gt;&lt;img src="https://avatars.githubusercontent.com/JaredP94?s=100" width="100px;" alt="Jared Ping" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Jared Ping&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/ishapira1"&gt;&lt;img src="https://avatars.githubusercontent.com/ishapira1?s=100" width="100px;" alt="Itai Shapira" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Itai Shapira&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"&gt;&lt;img src="https://www.gravatar.com/avatar/8863743b4f26c1a20e730fcf7ebc3bc0?d=identicon&amp;amp;s=100?s=100" width="100px;" alt="Maximilian Lam" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Maximilian Lam&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/jaysonzlin"&gt;&lt;img src="https://avatars.githubusercontent.com/jaysonzlin?s=100" width="100px;" alt="Jayson Lin" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Jayson Lin&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/sophiacho1"&gt;&lt;img src="https://avatars.githubusercontent.com/sophiacho1?s=100" width="100px;" alt="Sophia Cho" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Sophia Cho&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/andreamurillomtz"&gt;&lt;img src="https://avatars.githubusercontent.com/andreamurillomtz?s=100" width="100px;" alt="Andrea" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Andrea&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/alxrod"&gt;&lt;img src="https://avatars.githubusercontent.com/alxrod?s=100" width="100px;" alt="Alex Rodriguez" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Alex Rodriguez&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/korneelf1"&gt;&lt;img src="https://avatars.githubusercontent.com/korneelf1?s=100" width="100px;" alt="Korneel Van den Berghe" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Korneel Van den Berghe&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/foundingnimo"&gt;&lt;img src="https://avatars.githubusercontent.com/foundingnimo?s=100" width="100px;" alt="Nimo" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Nimo&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/colbybanbury"&gt;&lt;img src="https://avatars.githubusercontent.com/colbybanbury?s=100" width="100px;" alt="Colby Banbury" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Colby Banbury&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/zishenwan"&gt;&lt;img src="https://avatars.githubusercontent.com/zishenwan?s=100" width="100px;" alt="Zishen Wan" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Zishen Wan&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/mmaz"&gt;&lt;img src="https://avatars.githubusercontent.com/mmaz?s=100" width="100px;" alt="Mark Mazumder" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Mark Mazumder&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/DivyaAmirtharaj"&gt;&lt;img src="https://avatars.githubusercontent.com/DivyaAmirtharaj?s=100" width="100px;" alt="Divya Amirtharaj" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Divya Amirtharaj&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/srivatsankrishnan"&gt;&lt;img src="https://avatars.githubusercontent.com/srivatsankrishnan?s=100" width="100px;" alt="Srivatsan Krishnan" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Srivatsan Krishnan&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/ma3mool"&gt;&lt;img src="https://avatars.githubusercontent.com/ma3mool?s=100" width="100px;" alt="Abdulrahman Mahmoud" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Abdulrahman Mahmoud&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/arnaumarin"&gt;&lt;img src="https://avatars.githubusercontent.com/arnaumarin?s=100" width="100px;" alt="marin-llobet" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;marin-llobet&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/James-QiuHaoran"&gt;&lt;img src="https://avatars.githubusercontent.com/James-QiuHaoran?s=100" width="100px;" alt="Haoran Qiu" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Haoran Qiu&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/aptl26"&gt;&lt;img src="https://avatars.githubusercontent.com/aptl26?s=100" width="100px;" alt="Aghyad Deeb" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Aghyad Deeb&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/MichaelSchnebly"&gt;&lt;img src="https://avatars.githubusercontent.com/MichaelSchnebly?s=100" width="100px;" alt="Michael Schnebly" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Michael Schnebly&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/Ekhao"&gt;&lt;img src="https://avatars.githubusercontent.com/Ekhao?s=100" width="100px;" alt="Emil Njor" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Emil Njor&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/GabrielAmazonas"&gt;&lt;img src="https://avatars.githubusercontent.com/GabrielAmazonas?s=100" width="100px;" alt="Gabriel Amazonas" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Gabriel Amazonas&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/AditiR-42"&gt;&lt;img src="https://avatars.githubusercontent.com/AditiR-42?s=100" width="100px;" alt="Aditi Raju" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Aditi Raju&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/VThuong99"&gt;&lt;img src="https://avatars.githubusercontent.com/VThuong99?s=100" width="100px;" alt="Thuong Duong" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Thuong Duong&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/jared-ni"&gt;&lt;img src="https://avatars.githubusercontent.com/jared-ni?s=100" width="100px;" alt="Jared Ni" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Jared Ni&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/kaiM0ves"&gt;&lt;img src="https://avatars.githubusercontent.com/kaiM0ves?s=100" width="100px;" alt="kaiM0ves" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;kaiM0ves&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/ELSuitorHarvard"&gt;&lt;img src="https://avatars.githubusercontent.com/ELSuitorHarvard?s=100" width="100px;" alt="ELSuitorHarvard" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;ELSuitorHarvard&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/oishib"&gt;&lt;img src="https://avatars.githubusercontent.com/oishib?s=100" width="100px;" alt="oishib" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;oishib&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/leo47007"&gt;&lt;img src="https://avatars.githubusercontent.com/leo47007?s=100" width="100px;" alt="Yu-Shun Hsiao" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Yu-Shun Hsiao&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/BaeHenryS"&gt;&lt;img src="https://avatars.githubusercontent.com/BaeHenryS?s=100" width="100px;" alt="Henry Bae" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Henry Bae&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/jaywonchung"&gt;&lt;img src="https://avatars.githubusercontent.com/jaywonchung?s=100" width="100px;" alt="Jae-Won Chung" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Jae-Won Chung&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/eimlav"&gt;&lt;img src="https://avatars.githubusercontent.com/eimlav?s=100" width="100px;" alt="Eimhin Laverty" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Eimhin Laverty&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"&gt;&lt;img src="https://www.gravatar.com/avatar/af39c27c6090c50a1921a9b6366e81cc?d=identicon&amp;amp;s=100?s=100" width="100px;" alt="Emeka Ezike" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Emeka Ezike&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"&gt;&lt;img src="https://www.gravatar.com/avatar/0c931fcfd03cd548d44c90602dd773ba?d=identicon&amp;amp;s=100?s=100" width="100px;" alt="Matthew Stewart" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Matthew Stewart&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/jianqingdu"&gt;&lt;img src="https://avatars.githubusercontent.com/jianqingdu?s=100" width="100px;" alt="jianqingdu" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;jianqingdu&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/jzhou1318"&gt;&lt;img src="https://avatars.githubusercontent.com/jzhou1318?s=100" width="100px;" alt="Jennifer Zhou" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Jennifer Zhou&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/marcozennaro"&gt;&lt;img src="https://avatars.githubusercontent.com/marcozennaro?s=100" width="100px;" alt="Marco Zennaro" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Marco Zennaro&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/pongtr"&gt;&lt;img src="https://avatars.githubusercontent.com/pongtr?s=100" width="100px;" alt="Pong Trairatvorakul" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Pong Trairatvorakul&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/aryatschand"&gt;&lt;img src="https://avatars.githubusercontent.com/aryatschand?s=100" width="100px;" alt="Arya Tschand" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Arya Tschand&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/arbass22"&gt;&lt;img src="https://avatars.githubusercontent.com/arbass22?s=100" width="100px;" alt="Andrew Bass" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Andrew Bass&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/ShvetankPrakash"&gt;&lt;img src="https://avatars.githubusercontent.com/ShvetankPrakash?s=100" width="100px;" alt="Shvetank Prakash" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Shvetank Prakash&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/euranofshin"&gt;&lt;img src="https://avatars.githubusercontent.com/euranofshin?s=100" width="100px;" alt="Eura Nofshin" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Eura Nofshin&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/vitasam"&gt;&lt;img src="https://avatars.githubusercontent.com/vitasam?s=100" width="100px;" alt="The Random DIY" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;The Random DIY&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/taunoe"&gt;&lt;img src="https://avatars.githubusercontent.com/taunoe?s=100" width="100px;" alt="Tauno Erik" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Tauno Erik&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"&gt;&lt;img src="https://www.gravatar.com/avatar/468ef35acc69f3266efd700992daa369?d=identicon&amp;amp;s=100?s=100" width="100px;" alt="Fatima Shah" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Fatima Shah&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"&gt;&lt;img src="https://www.gravatar.com/avatar/4ad8cdf19eb3b666ace97d3eedb19278?d=identicon&amp;amp;s=100?s=100" width="100px;" alt="Tess314" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Tess314&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/gnodipac886"&gt;&lt;img src="https://avatars.githubusercontent.com/gnodipac886?s=100" width="100px;" alt="gnodipac886" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;gnodipac886&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/serco425"&gt;&lt;img src="https://avatars.githubusercontent.com/serco425?s=100" width="100px;" alt="Sercan Ayg√ºn" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Sercan Ayg√ºn&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/alex-oesterling"&gt;&lt;img src="https://avatars.githubusercontent.com/alex-oesterling?s=100" width="100px;" alt="Alex Oesterling" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Alex Oesterling&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/Gjain234"&gt;&lt;img src="https://avatars.githubusercontent.com/Gjain234?s=100" width="100px;" alt="Gauri Jain" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Gauri Jain&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/BrunoScaglione"&gt;&lt;img src="https://avatars.githubusercontent.com/BrunoScaglione?s=100" width="100px;" alt="Bruno Scaglione" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Bruno Scaglione&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/TheHiddenLayer"&gt;&lt;img src="https://avatars.githubusercontent.com/TheHiddenLayer?s=100" width="100px;" alt="TheHiddenLayer" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;TheHiddenLayer&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/Allen-Kuang"&gt;&lt;img src="https://avatars.githubusercontent.com/Allen-Kuang?s=100" width="100px;" alt="Allen-Kuang" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Allen-Kuang&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/FinAminToastCrunch"&gt;&lt;img src="https://avatars.githubusercontent.com/FinAminToastCrunch?s=100" width="100px;" alt="Fin Amin" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Fin Amin&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/AbenezerKb"&gt;&lt;img src="https://avatars.githubusercontent.com/AbenezerKb?s=100" width="100px;" alt="Abenezer Angamo" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Abenezer Angamo&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/KarthikDani"&gt;&lt;img src="https://avatars.githubusercontent.com/KarthikDani?s=100" width="100px;" alt="Karthik Dani" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Karthik Dani&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/arighosh05"&gt;&lt;img src="https://avatars.githubusercontent.com/arighosh05?s=100" width="100px;" alt="Aritra Ghosh" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Aritra Ghosh&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/aethernavshulkraven-allain"&gt;&lt;img src="https://avatars.githubusercontent.com/aethernavshulkraven-allain?s=100" width="100px;" alt="‡§Ö‡§∞‡§®‡§µ ‡§∂‡•Å‡§ï‡•ç‡§≤‡§æ | Arnav Shukla" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;‡§Ö‡§∞‡§®‡§µ ‡§∂‡•Å‡§ï‡•ç‡§≤‡§æ | Arnav Shukla&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/YangZhou1997"&gt;&lt;img src="https://avatars.githubusercontent.com/YangZhou1997?s=100" width="100px;" alt="Yang Zhou" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Yang Zhou&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/abigailswallow"&gt;&lt;img src="https://avatars.githubusercontent.com/abigailswallow?s=100" width="100px;" alt="abigailswallow" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;abigailswallow&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/YLab-UChicago"&gt;&lt;img src="https://avatars.githubusercontent.com/YLab-UChicago?s=100" width="100px;" alt="yanjingl" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;yanjingl&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/BravoBaldo"&gt;&lt;img src="https://avatars.githubusercontent.com/BravoBaldo?s=100" width="100px;" alt="Baldassarre Cesarano" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Baldassarre Cesarano&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/Jahnic-kb"&gt;&lt;img src="https://avatars.githubusercontent.com/Jahnic-kb?s=100" width="100px;" alt="Jahnic Beck" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Jahnic Beck&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/happyappledog"&gt;&lt;img src="https://avatars.githubusercontent.com/happyappledog?s=100" width="100px;" alt="happyappledog" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;happyappledog&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/jasonlyik"&gt;&lt;img src="https://avatars.githubusercontent.com/jasonlyik?s=100" width="100px;" alt="Jason Yik" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Jason Yik&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/jessicaquaye"&gt;&lt;img src="https://avatars.githubusercontent.com/jessicaquaye?s=100" width="100px;" alt="Jessica Quaye" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Jessica Quaye&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/emmanuel2406"&gt;&lt;img src="https://avatars.githubusercontent.com/emmanuel2406?s=100" width="100px;" alt="Emmanuel Rassou" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Emmanuel Rassou&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/atcheng2"&gt;&lt;img src="https://avatars.githubusercontent.com/atcheng2?s=100" width="100px;" alt="Andy Cheng" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Andy Cheng&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/cursoragent"&gt;&lt;img src="https://avatars.githubusercontent.com/cursoragent?s=100" width="100px;" alt="Cursor Agent" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Cursor Agent&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/bilgeacun"&gt;&lt;img src="https://avatars.githubusercontent.com/bilgeacun?s=100" width="100px;" alt="Bilge Acun" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Bilge Acun&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/sjohri20"&gt;&lt;img src="https://avatars.githubusercontent.com/sjohri20?s=100" width="100px;" alt="Shreya Johri" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Shreya Johri&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/swilcock0"&gt;&lt;img src="https://avatars.githubusercontent.com/swilcock0?s=100" width="100px;" alt="Sam Wilcock" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Sam Wilcock&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/skmur"&gt;&lt;img src="https://avatars.githubusercontent.com/skmur?s=100" width="100px;" alt="Sonia Murthy" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Sonia Murthy&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"&gt;&lt;img src="https://www.gravatar.com/avatar/fc4f3460cdfb9365ab59bdeafb06413e?d=identicon&amp;amp;s=100?s=100" width="100px;" alt="Costin-Andrei Oncescu" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Costin-Andrei Oncescu&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"&gt;&lt;img src="https://www.gravatar.com/avatar/0d6b8616427d8b19d425c9808692e347?d=identicon&amp;amp;s=100?s=100" width="100px;" alt="formlsysbookissue" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;formlsysbookissue&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"&gt;&lt;img src="https://www.gravatar.com/avatar/7cd8d5dfd83071f23979019d97655dc5?d=identicon&amp;amp;s=100?s=100" width="100px;" alt="Annie Laurie Cook" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Annie Laurie Cook&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"&gt;&lt;img src="https://www.gravatar.com/avatar/b15b6e0e9adf58099905c1a0fd474cb9?d=identicon&amp;amp;s=100?s=100" width="100px;" alt="Vijay Edupuganti" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Vijay Edupuganti&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"&gt;&lt;img src="https://www.gravatar.com/avatar/f88052cca4f401d9b0f43aed0a53434a?d=identicon&amp;amp;s=100?s=100" width="100px;" alt="Jothi Ramaswamy" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Jothi Ramaswamy&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"&gt;&lt;img src="https://www.gravatar.com/avatar/35a8d9ffd03f05e79a2c6ce6206a56f2?d=identicon&amp;amp;s=100?s=100" width="100px;" alt="Batur Arslan" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Batur Arslan&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"&gt;&lt;img src="https://www.gravatar.com/avatar/bd53d146aa888548c8db4da02bf81e7a?d=identicon&amp;amp;s=100?s=100" width="100px;" alt="Curren Iyer" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Curren Iyer&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"&gt;&lt;img src="https://www.gravatar.com/avatar/468ef35acc69f3266efd700992daa369?d=identicon&amp;amp;s=100?s=100" width="100px;" alt="Fatima Shah" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Fatima Shah&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"&gt;&lt;img src="https://www.gravatar.com/avatar/8d8410338458e08bd5e4b96f58e1c217?d=identicon&amp;amp;s=100?s=100" width="100px;" alt="Edward Jin" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Edward Jin&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"&gt;&lt;img src="https://www.gravatar.com/avatar/28c6123d2c9f75578d3ccdedb0df3d11?d=identicon&amp;amp;s=100?s=100" width="100px;" alt="Tess Watt" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Tess Watt&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"&gt;&lt;img src="https://www.gravatar.com/avatar/ef139181fe00190f21730f6912532e9e?d=identicon&amp;amp;s=100?s=100" width="100px;" alt="bluebaer7" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;bluebaer7&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"&gt;&lt;img src="https://www.gravatar.com/avatar/a5a47df988ab1720dd706062e523ca32?d=identicon&amp;amp;s=100?s=100" width="100px;" alt="a-saraf" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;a-saraf&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"&gt;&lt;img src="https://www.gravatar.com/avatar/c2dc311aa8122d5f5f061e1db14682b1?d=identicon&amp;amp;s=100?s=100" width="100px;" alt="songhan" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;songhan&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"&gt;&lt;img src="https://www.gravatar.com/avatar/4814aad67982ab07a69006a1ce9d2a72?d=identicon&amp;amp;s=100?s=100" width="100px;" alt="jvijay" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;jvijay&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td align="center" valign="top" width="20%"&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book/graphs/contributors"&gt;&lt;img src="https://www.gravatar.com/avatar/43b1feff77c8a95fd581774fb8ec891f?d=identicon&amp;amp;s=100?s=100" width="100px;" alt="Zishen" /&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Zishen&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;!-- markdownlint-restore --&gt; 
&lt;!-- prettier-ignore-end --&gt; 
&lt;!-- ALL-CONTRIBUTORS-LIST:END --&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/harvard-edge/cs249r_book#support-this-work"&gt;‚≠ê Star us on GitHub&lt;/a&gt; ‚Ä¢ &lt;a href="https://buttondown.email/mlsysbook"&gt;‚úâÔ∏è Subscribe&lt;/a&gt; ‚Ä¢ &lt;a href="https://github.com/harvard-edge/cs249r_book/discussions"&gt;üí¨ Join discussions&lt;/a&gt; ‚Ä¢ &lt;a href="https://mlsysbook.ai"&gt;üåê Visit mlsysbook.ai&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;Built with dedication by the MLSysBook community.&lt;/em&gt;&lt;/p&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>livekit/agents</title>
      <link>https://github.com/livekit/agents</link>
      <description>&lt;p&gt;A powerful framework for building realtime voice AI agents ü§ñüéôÔ∏èüìπ&lt;/p&gt;&lt;hr&gt;&lt;picture&gt; 
 &lt;source media="(prefers-color-scheme: dark)" srcset="/.github/banner_dark.png" /&gt; 
 &lt;source media="(prefers-color-scheme: light)" srcset="/.github/banner_light.png" /&gt; 
 &lt;img style="width:100%;" alt="The LiveKit icon, the name of the repository and some sample code in the background." src="https://raw.githubusercontent.com/livekit/agents/main/.github/banner_light.png" /&gt; 
&lt;/picture&gt; 
&lt;!--END_BANNER_IMAGE--&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;img src="https://img.shields.io/pypi/v/livekit-agents" alt="PyPI - Version" /&gt; &lt;a href="https://pepy.tech/projects/livekit-agents"&gt;&lt;img src="https://static.pepy.tech/badge/livekit-agents/month" alt="PyPI Downloads" /&gt;&lt;/a&gt; &lt;a href="https://livekit.io/join-slack"&gt;&lt;img src="https://img.shields.io/endpoint?url=https%3A%2F%2Flivekit.io%2Fbadges%2Fslack" alt="Slack community" /&gt;&lt;/a&gt; &lt;a href="https://twitter.com/livekit"&gt;&lt;img src="https://img.shields.io/twitter/follow/livekit" alt="Twitter Follow" /&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/livekit/agents"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki for understanding the codebase" /&gt;&lt;/a&gt; &lt;a href="https://github.com/livekit/livekit/raw/master/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/livekit/livekit" alt="License" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;p&gt;Looking for the JS/TS library? Check out &lt;a href="https://github.com/livekit/agents-js"&gt;AgentsJS&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;What is Agents?&lt;/h2&gt; 
&lt;!--BEGIN_DESCRIPTION--&gt; 
&lt;p&gt;The Agent Framework is designed for building realtime, programmable participants that run on servers. Use it to create conversational, multi-modal voice agents that can see, hear, and understand.&lt;/p&gt; 
&lt;!--END_DESCRIPTION--&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Flexible integrations&lt;/strong&gt;: A comprehensive ecosystem to mix and match the right STT, LLM, TTS, and Realtime API to suit your use case.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Integrated job scheduling&lt;/strong&gt;: Built-in task scheduling and distribution with &lt;a href="https://docs.livekit.io/agents/build/dispatch/"&gt;dispatch APIs&lt;/a&gt; to connect end users to agents.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Extensive WebRTC clients&lt;/strong&gt;: Build client applications using LiveKit's open-source SDK ecosystem, supporting all major platforms.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Telephony integration&lt;/strong&gt;: Works seamlessly with LiveKit's &lt;a href="https://docs.livekit.io/sip/"&gt;telephony stack&lt;/a&gt;, allowing your agent to make calls to or receive calls from phones.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Exchange data with clients&lt;/strong&gt;: Use &lt;a href="https://docs.livekit.io/home/client/data/rpc/"&gt;RPCs&lt;/a&gt; and other &lt;a href="https://docs.livekit.io/home/client/data/"&gt;Data APIs&lt;/a&gt; to seamlessly exchange data with clients.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Semantic turn detection&lt;/strong&gt;: Uses a transformer model to detect when a user is done with their turn, helps to reduce interruptions.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MCP support&lt;/strong&gt;: Native support for MCP. Integrate tools provided by MCP servers with one loc.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Builtin test framework&lt;/strong&gt;: Write tests and use judges to ensure your agent is performing as expected.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Open-source&lt;/strong&gt;: Fully open-source, allowing you to run the entire stack on your own servers, including &lt;a href="https://github.com/livekit/livekit"&gt;LiveKit server&lt;/a&gt;, one of the most widely used WebRTC media servers.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;To install the core Agents library, along with plugins for popular model providers:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install "livekit-agents[openai,silero,deepgram,cartesia,turn-detector]~=1.0"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Docs and guides&lt;/h2&gt; 
&lt;p&gt;Documentation on the framework and how to use it can be found &lt;a href="https://docs.livekit.io/agents/"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Core concepts&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Agent: An LLM-based application with defined instructions.&lt;/li&gt; 
 &lt;li&gt;AgentSession: A container for agents that manages interactions with end users.&lt;/li&gt; 
 &lt;li&gt;entrypoint: The starting point for an interactive session, similar to a request handler in a web server.&lt;/li&gt; 
 &lt;li&gt;Worker: The main process that coordinates job scheduling and launches agents for user sessions.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;h3&gt;Simple voice agent&lt;/h3&gt; 
&lt;hr /&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from livekit.agents import (
    Agent,
    AgentSession,
    JobContext,
    RunContext,
    WorkerOptions,
    cli,
    function_tool,
)
from livekit.plugins import silero

@function_tool
async def lookup_weather(
    context: RunContext,
    location: str,
):
    """Used to look up weather information."""

    return {"weather": "sunny", "temperature": 70}


async def entrypoint(ctx: JobContext):
    await ctx.connect()

    agent = Agent(
        instructions="You are a friendly voice assistant built by LiveKit.",
        tools=[lookup_weather],
    )
    session = AgentSession(
        vad=silero.VAD.load(),
        # any combination of STT, LLM, TTS, or realtime API can be used
        stt="assemblyai/universal-streaming:en",
        llm="openai/gpt-4.1-mini",
        tts="cartesia/sonic-2:9626c31c-bec5-4cca-baa8-f8ba9e84c8bc",
    )

    await session.start(agent=agent, room=ctx.room)
    await session.generate_reply(instructions="greet the user and ask about their day")


if __name__ == "__main__":
    cli.run_app(WorkerOptions(entrypoint_fnc=entrypoint))
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You'll need the following environment variables for this example:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;DEEPGRAM_API_KEY&lt;/li&gt; 
 &lt;li&gt;OPENAI_API_KEY&lt;/li&gt; 
 &lt;li&gt;ELEVEN_API_KEY&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Multi-agent handoff&lt;/h3&gt; 
&lt;hr /&gt; 
&lt;p&gt;This code snippet is abbreviated. For the full example, see &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/voice_agents/multi_agent.py"&gt;multi_agent.py&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;...
class IntroAgent(Agent):
    def __init__(self) -&amp;gt; None:
        super().__init__(
            instructions=f"You are a story teller. Your goal is to gather a few pieces of information from the user to make the story personalized and engaging."
            "Ask the user for their name and where they are from"
        )

    async def on_enter(self):
        self.session.generate_reply(instructions="greet the user and gather information")

    @function_tool
    async def information_gathered(
        self,
        context: RunContext,
        name: str,
        location: str,
    ):
        """Called when the user has provided the information needed to make the story personalized and engaging.

        Args:
            name: The name of the user
            location: The location of the user
        """

        context.userdata.name = name
        context.userdata.location = location

        story_agent = StoryAgent(name, location)
        return story_agent, "Let's start the story!"


class StoryAgent(Agent):
    def __init__(self, name: str, location: str) -&amp;gt; None:
        super().__init__(
            instructions=f"You are a storyteller. Use the user's information in order to make the story personalized."
            f"The user's name is {name}, from {location}"
            # override the default model, switching to Realtime API from standard LLMs
            llm=openai.realtime.RealtimeModel(voice="echo"),
            chat_ctx=chat_ctx,
        )

    async def on_enter(self):
        self.session.generate_reply()


async def entrypoint(ctx: JobContext):
    await ctx.connect()

    userdata = StoryData()
    session = AgentSession[StoryData](
        vad=silero.VAD.load(),
        stt="deepgram/nova-3",
        llm="openai/gpt-4o",
        tts="cartesia/sonic-2:9626c31c-bec5-4cca-baa8-f8ba9e84c8bc",
        userdata=userdata,
    )

    await session.start(
        agent=IntroAgent(),
        room=ctx.room,
    )
...
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Testing&lt;/h3&gt; 
&lt;p&gt;Automated tests are essential for building reliable agents, especially with the non-deterministic behavior of LLMs. LiveKit Agents include native test integration to help you create dependable agents.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;@pytest.mark.asyncio
async def test_no_availability() -&amp;gt; None:
    llm = google.LLM()
    async AgentSession(llm=llm) as sess:
        await sess.start(MyAgent())
        result = await sess.run(
            user_input="Hello, I need to place an order."
        )
        result.expect.skip_next_event_if(type="message", role="assistant")
        result.expect.next_event().is_function_call(name="start_order")
        result.expect.next_event().is_function_call_output()
        await (
            result.expect.next_event()
            .is_message(role="assistant")
            .judge(llm, intent="assistant should be asking the user what they would like")
        )

&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;üéôÔ∏è Starter Agent&lt;/h3&gt; &lt;p&gt;A starter agent optimized for voice conversations.&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/voice_agents/basic_agent.py"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;üîÑ Multi-user push to talk&lt;/h3&gt; &lt;p&gt;Responds to multiple users in the room via push-to-talk.&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/voice_agents/push_to_talk.py"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;üéµ Background audio&lt;/h3&gt; &lt;p&gt;Background ambient and thinking audio to improve realism.&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/voice_agents/background_audio.py"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;üõ†Ô∏è Dynamic tool creation&lt;/h3&gt; &lt;p&gt;Creating function tools dynamically.&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/voice_agents/dynamic_tool_creation.py"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;‚òéÔ∏è Outbound caller&lt;/h3&gt; &lt;p&gt;Agent that makes outbound phone calls&lt;/p&gt; &lt;p&gt; &lt;a href="https://github.com/livekit-examples/outbound-caller-python"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;üìã Structured output&lt;/h3&gt; &lt;p&gt;Using structured output from LLM to guide TTS tone.&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/voice_agents/structured_output.py"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;üîå MCP support&lt;/h3&gt; &lt;p&gt;Use tools from MCP servers&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/voice_agents/mcp"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;üí¨ Text-only agent&lt;/h3&gt; &lt;p&gt;Skip voice altogether and use the same code for text-only integrations&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/other/text_only.py"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;üìù Multi-user transcriber&lt;/h3&gt; &lt;p&gt;Produce transcriptions from all users in the room&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/other/transcription/multi-user-transcriber.py"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;üé• Video avatars&lt;/h3&gt; &lt;p&gt;Add an AI avatar with Tavus, Beyond Presence, and Bithuman&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/avatar_agents/"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;üçΩÔ∏è Restaurant ordering and reservations&lt;/h3&gt; &lt;p&gt;Full example of an agent that handles calls for a restaurant.&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/voice_agents/restaurant_agent.py"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;üëÅÔ∏è Gemini Live vision&lt;/h3&gt; &lt;p&gt;Full example (including iOS app) of Gemini Live agent that can see.&lt;/p&gt; &lt;p&gt; &lt;a href="https://github.com/livekit-examples/vision-demo"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;Running your agent&lt;/h2&gt; 
&lt;h3&gt;Testing in terminal&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;python myagent.py console
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Runs your agent in terminal mode, enabling local audio input and output for testing. This mode doesn't require external servers or dependencies and is useful for quickly validating behavior.&lt;/p&gt; 
&lt;h3&gt;Developing with LiveKit clients&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;python myagent.py dev
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Starts the agent server and enables hot reloading when files change. This mode allows each process to host multiple concurrent agents efficiently.&lt;/p&gt; 
&lt;p&gt;The agent connects to LiveKit Cloud or your self-hosted server. Set the following environment variables:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;LIVEKIT_URL&lt;/li&gt; 
 &lt;li&gt;LIVEKIT_API_KEY&lt;/li&gt; 
 &lt;li&gt;LIVEKIT_API_SECRET&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can connect using any LiveKit client SDK or telephony integration. To get started quickly, try the &lt;a href="https://agents-playground.livekit.io/"&gt;Agents Playground&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Running for production&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;python myagent.py start
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Runs the agent with production-ready optimizations.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;The Agents framework is under active development in a rapidly evolving field. We welcome and appreciate contributions of any kind, be it feedback, bugfixes, features, new plugins and tools, or better documentation. You can file issues under this repo, open a PR, or chat with us in LiveKit's &lt;a href="https://livekit.io/join-slack"&gt;Slack community&lt;/a&gt;.&lt;/p&gt; 
&lt;!--BEGIN_REPO_NAV--&gt; 
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;table&gt; 
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th colspan="2"&gt;LiveKit Ecosystem&lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt;
   &lt;td&gt;LiveKit SDKs&lt;/td&gt;
   &lt;td&gt;&lt;a href="https://github.com/livekit/client-sdk-js"&gt;Browser&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/client-sdk-swift"&gt;iOS/macOS/visionOS&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/client-sdk-android"&gt;Android&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/client-sdk-flutter"&gt;Flutter&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/client-sdk-react-native"&gt;React Native&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/rust-sdks"&gt;Rust&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/node-sdks"&gt;Node.js&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/python-sdks"&gt;Python&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/client-sdk-unity"&gt;Unity&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/client-sdk-unity-web"&gt;Unity (WebGL)&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/client-sdk-esp32"&gt;ESP32&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;&lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;Server APIs&lt;/td&gt;
   &lt;td&gt;&lt;a href="https://github.com/livekit/node-sdks"&gt;Node.js&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/server-sdk-go"&gt;Golang&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/server-sdk-ruby"&gt;Ruby&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/server-sdk-kotlin"&gt;Java/Kotlin&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/python-sdks"&gt;Python&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/rust-sdks"&gt;Rust&lt;/a&gt; ¬∑ &lt;a href="https://github.com/agence104/livekit-server-sdk-php"&gt;PHP (community)&lt;/a&gt; ¬∑ &lt;a href="https://github.com/pabloFuente/livekit-server-sdk-dotnet"&gt;.NET (community)&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;&lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;UI Components&lt;/td&gt;
   &lt;td&gt;&lt;a href="https://github.com/livekit/components-js"&gt;React&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/components-android"&gt;Android Compose&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/components-swift"&gt;SwiftUI&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/components-flutter"&gt;Flutter&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;&lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;Agents Frameworks&lt;/td&gt;
   &lt;td&gt;&lt;b&gt;Python&lt;/b&gt; ¬∑ &lt;a href="https://github.com/livekit/agents-js"&gt;Node.js&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/agent-playground"&gt;Playground&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;&lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;Services&lt;/td&gt;
   &lt;td&gt;&lt;a href="https://github.com/livekit/livekit"&gt;LiveKit server&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/egress"&gt;Egress&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/ingress"&gt;Ingress&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/sip"&gt;SIP&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;&lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;Resources&lt;/td&gt;
   &lt;td&gt;&lt;a href="https://docs.livekit.io"&gt;Docs&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit-examples"&gt;Example apps&lt;/a&gt; ¬∑ &lt;a href="https://livekit.io/cloud"&gt;Cloud&lt;/a&gt; ¬∑ &lt;a href="https://docs.livekit.io/home/self-hosting/deployment"&gt;Self-hosting&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/livekit-cli"&gt;CLI&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;!--END_REPO_NAV--&gt;</description>
    </item>
    
  </channel>
</rss>