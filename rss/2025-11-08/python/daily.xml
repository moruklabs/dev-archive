<rss version="2.0">
  <channel>
    <title>GitHub Python Daily Trending</title>
    <description>Daily Trending of Python in GitHub</description>
    <pubDate>Fri, 07 Nov 2025 01:39:05 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>HKUDS/DeepCode</title>
      <link>https://github.com/HKUDS/DeepCode</link>
      <description>&lt;p&gt;"DeepCode: Open Agentic Coding (Paper2Code &amp; Text2Web &amp; Text2Backend)"&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;table style="border: none; margin: 0 auto; padding: 0; border-collapse: collapse;"&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td align="center" style="vertical-align: middle; padding: 10px; border: none; width: 250px;"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/DeepCode/main/assets/logo.png" alt="DeepCode Logo" width="200" style="margin: 0; padding: 0; display: block;" /&gt; &lt;/td&gt; 
    &lt;td align="left" style="vertical-align: middle; padding: 10px 0 10px 30px; border: none;"&gt; &lt;pre style="font-family: 'Courier New', monospace; font-size: 16px; color: #0EA5E9; margin: 0; padding: 0; text-shadow: 0 0 10px #0EA5E9, 0 0 20px rgba(14,165,233,0.5); line-height: 1.2; transform: skew(-1deg, 0deg); display: block;"&gt;    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—
    â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•
    â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—
    â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â•â• â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•
    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘     â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—
    â•šâ•â•â•â•â•â• â•šâ•â•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•      â•šâ•â•â•â•â•â• â•šâ•â•â•â•â•â• â•šâ•â•â•â•â•â• â•šâ•â•â•â•â•â•â•&lt;/pre&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
 &lt;div align="center"&gt; 
  &lt;a href="https://trendshift.io/repositories/14665" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/14665" alt="HKUDS%2FDeepCode | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; 
 &lt;/div&gt; 
 &lt;!-- &lt;img src="https://readme-typing-svg.herokuapp.com?font=Russo+One&amp;size=28&amp;duration=2000&amp;pause=800&amp;color=06B6D4&amp;background=00000000&amp;center=true&amp;vCenter=true&amp;width=800&amp;height=50&amp;lines=%E2%9A%A1+OPEN+AGENTIC+CODING+%E2%9A%A1" alt="DeepCode Tech Subtitle" style="margin-top: 5px; filter: drop-shadow(0 0 12px #06B6D4) drop-shadow(0 0 24px rgba(6,182,212,0.4));"/&gt; --&gt; 
 &lt;h1&gt;&lt;img src="https://github.com/Zongwei9888/Experiment_Images/raw/43c585dca3d21b8e4b6390d835cdd34dc4b4b23d/DeepCode_images/title_logo.svg?sanitize=true" alt="DeepCode Logo" width="32" height="32" style="vertical-align: middle; margin-right: 8px;" /&gt; DeepCode: Open Agentic Coding&lt;/h1&gt; 
 &lt;h3&gt;&lt;em&gt;Advancing Code Generation with Multi-Agent Systems&lt;/em&gt;&lt;/h3&gt; 
 &lt;!-- &lt;p align="center"&gt;
  &lt;img src="https://img.shields.io/badge/Version-1.0.0-00d4ff?style=for-the-badge&amp;logo=rocket&amp;logoColor=white" alt="Version"&gt;

  &lt;img src="https://img.shields.io/badge/License-MIT-4ecdc4?style=for-the-badge&amp;logo=opensourceinitiative&amp;logoColor=white" alt="License"&gt;
  &lt;img src="https://img.shields.io/badge/AI-Multi--Agent-9b59b6?style=for-the-badge&amp;logo=brain&amp;logoColor=white" alt="AI"&gt;
  &lt;img src="https://img.shields.io/badge/HKU-Data_Intelligence_Lab-f39c12?style=for-the-badge&amp;logo=university&amp;logoColor=white" alt="HKU"&gt;
&lt;/p&gt; --&gt; 
 &lt;p&gt; &lt;a href="https://github.com/HKUDS/DeepCode/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/HKUDS/DeepCode?color=00d9ff&amp;amp;style=for-the-badge&amp;amp;logo=star&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/badge/ğŸPython-3.13-4ecdc4?style=for-the-badge&amp;amp;logo=python&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt; &lt;a href="https://pypi.org/project/deepcode-hku/"&gt;&lt;img src="https://img.shields.io/pypi/v/deepcode-hku.svg?style=for-the-badge&amp;amp;logo=pypi&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e&amp;amp;color=ff6b6b" /&gt;&lt;/a&gt; &lt;/p&gt; 
 &lt;p&gt; &lt;a href="https://discord.gg/yF2MmDJyGJ"&gt;&lt;img src="https://img.shields.io/badge/ğŸ’¬Discord-Community-7289da?style=for-the-badge&amp;amp;logo=discord&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;a href="https://github.com/HKUDS/DeepCode/issues/11"&gt;&lt;img src="https://img.shields.io/badge/ğŸ’¬WeChat-Group-07c160?style=for-the-badge&amp;amp;logo=wechat&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;/p&gt; 
 &lt;div align="center"&gt; 
  &lt;div style="width: 100%; height: 2px; margin: 20px 0; background: linear-gradient(90deg, transparent, #00d9ff, transparent);"&gt;&lt;/div&gt; 
 &lt;/div&gt; 
 &lt;div align="center"&gt; 
  &lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-quick-start" style="text-decoration: none;"&gt; &lt;img src="https://img.shields.io/badge/Quick%20Start-Get%20Started%20Now-00d9ff?style=for-the-badge&amp;amp;logo=rocket&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt; &lt;/a&gt; 
 &lt;/div&gt; 
 &lt;div align="center" style="margin-top: 10px;"&gt; 
  &lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/README.md"&gt; &lt;img src="https://img.shields.io/badge/English-00d4ff?style=for-the-badge&amp;amp;logo=readme&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" alt="English" /&gt; &lt;/a&gt; 
  &lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/README_ZH.md"&gt; &lt;img src="https://img.shields.io/badge/ä¸­æ–‡-00d4ff?style=for-the-badge&amp;amp;logo=readme&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" alt="ä¸­æ–‡" /&gt; &lt;/a&gt; 
 &lt;/div&gt; 
 &lt;h3&gt;ğŸ–¥ï¸ &lt;strong&gt;Interface Showcase&lt;/strong&gt;&lt;/h3&gt; 
 &lt;table align="center" width="100%" style="border: none; border-collapse: collapse; margin: 30px 0;"&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td width="50%" align="center" style="vertical-align: top; padding: 20px;"&gt; &lt;h4&gt;ğŸ–¥ï¸ &lt;strong&gt;CLI Interface&lt;/strong&gt;&lt;/h4&gt; &lt;p&gt;&lt;strong&gt;Terminal-Based Development&lt;/strong&gt;&lt;/p&gt; 
     &lt;div align="center"&gt; 
      &lt;img src="https://github.com/Zongwei9888/Experiment_Images/raw/8882a7313c504ca97ead6e7b36c51aa761b6a4f3/DeepCode_images/CLI.gif" alt="CLI Interface Demo" width="100%" style="border-radius: 10px; box-shadow: 0 8px 20px rgba(45,55,72,0.3); margin: 15px 0;" /&gt; 
      &lt;div style="background: linear-gradient(135deg, #2D3748 0%, #4A5568 100%); border-radius: 12px; padding: 15px; margin: 15px 0; color: white;"&gt; 
       &lt;strong&gt;ğŸš€ Advanced Terminal Experience&lt;/strong&gt;
       &lt;br /&gt; 
       &lt;small&gt;âš¡ Fast command-line workflow&lt;br /&gt;ğŸ”§ Developer-friendly interface&lt;br /&gt;ğŸ“Š Real-time progress tracking&lt;/small&gt; 
      &lt;/div&gt; 
      &lt;p&gt;&lt;em&gt;Professional terminal interface for advanced users and CI/CD integration&lt;/em&gt;&lt;/p&gt; 
     &lt;/div&gt; &lt;/td&gt; 
    &lt;td width="50%" align="center" style="vertical-align: top; padding: 20px;"&gt; &lt;h4&gt;ğŸŒ &lt;strong&gt;Web Interface&lt;/strong&gt;&lt;/h4&gt; &lt;p&gt;&lt;strong&gt;Visual Interactive Experience&lt;/strong&gt;&lt;/p&gt; 
     &lt;div align="center"&gt; 
      &lt;img src="https://github.com/Zongwei9888/Experiment_Images/raw/8882a7313c504ca97ead6e7b36c51aa761b6a4f3/DeepCode_images/UI.gif" alt="Web Interface Demo" width="100%" style="border-radius: 10px; box-shadow: 0 8px 20px rgba(14,165,233,0.3); margin: 15px 0;" /&gt; 
      &lt;div style="background: linear-gradient(135deg, #0EA5E9 0%, #00D4FF 100%); border-radius: 12px; padding: 15px; margin: 15px 0; color: white;"&gt; 
       &lt;strong&gt;ğŸ¨ Modern Web Dashboard&lt;/strong&gt;
       &lt;br /&gt; 
       &lt;small&gt;ğŸ–±ï¸ Intuitive drag-and-drop&lt;br /&gt;ğŸ“± Responsive design&lt;br /&gt;ğŸ¯ Visual progress tracking&lt;/small&gt; 
      &lt;/div&gt; 
      &lt;p&gt;&lt;em&gt;Beautiful web interface with streamlined workflow for all skill levels&lt;/em&gt;&lt;/p&gt; 
     &lt;/div&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
 &lt;hr /&gt; 
 &lt;div align="center"&gt; 
  &lt;h3&gt;ğŸ¬ &lt;strong&gt;Introduction Video&lt;/strong&gt;&lt;/h3&gt; 
  &lt;div style="margin: 20px 0;"&gt; 
   &lt;a href="https://youtu.be/PRgmP8pOI08" target="_blank"&gt; &lt;img src="https://img.youtube.com/vi/PRgmP8pOI08/maxresdefault.jpg" alt="DeepCode Introduction Video" width="75%" style="border-radius: 12px; box-shadow: 0 8px 25px rgba(0,0,0,0.15); transition: transform 0.3s ease;" /&gt; &lt;/a&gt; 
  &lt;/div&gt; 
  &lt;p&gt;&lt;em&gt;ğŸ¯ &lt;strong&gt;Watch our complete introduction&lt;/strong&gt; - See how DeepCode transforms research papers and natural language into production-ready code&lt;/em&gt;&lt;/p&gt; 
  &lt;p&gt; &lt;a href="https://youtu.be/PRgmP8pOI08" target="_blank"&gt; &lt;img src="https://img.shields.io/badge/â–¶ï¸_Watch_Video-FF0000?style=for-the-badge&amp;amp;logo=youtube&amp;amp;logoColor=white" alt="Watch Video" /&gt; &lt;/a&gt; &lt;/p&gt; 
 &lt;/div&gt; 
 &lt;hr /&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;&lt;em&gt;"Where AI Agents Transform Ideas into Production-Ready Code"&lt;/em&gt;&lt;/p&gt; 
 &lt;/blockquote&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ“‘ Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-news"&gt;ğŸ“° News&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-key-features"&gt;ğŸš€ Key Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#%EF%B8%8F-architecture"&gt;ğŸ—ï¸ Architecture&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-experimental-results"&gt;ğŸ“Š Experimental Results&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-quick-start"&gt;ğŸš€ Quick Start&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-examples"&gt;ğŸ’¡ Examples&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-live-demonstrations"&gt;ğŸ¬ Live Demonstrations&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-star-history"&gt;â­ Star History&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-license"&gt;ğŸ“„ License&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ“° News&lt;/h2&gt; 
&lt;p&gt;ğŸ‰ &lt;strong&gt;[2025-10] ğŸ‰ [2025-10-28] DeepCode Achieves SOTA on PaperBench!&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;DeepCode sets new benchmarks on OpenAI's PaperBench Code-Dev across all categories:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ† &lt;strong&gt;Surpasses Human Experts&lt;/strong&gt;: &lt;strong&gt;75.9%&lt;/strong&gt; (DeepCode) vs Top Machine Learning PhDs 72.4% (+3.5%).&lt;/li&gt; 
 &lt;li&gt;ğŸ¥‡ &lt;strong&gt;Outperforms SOTA Commercial Code Agents&lt;/strong&gt;: &lt;strong&gt;84.8%&lt;/strong&gt; (DeepCode) vs Leading Commercial Code Agents (+26.1%) (Cursor, Claude Code, and Codex).&lt;/li&gt; 
 &lt;li&gt;ğŸ”¬ &lt;strong&gt;Advances Scientific Coding&lt;/strong&gt;: &lt;strong&gt;73.5%&lt;/strong&gt; (DeepCode) vs PaperCoder 51.1% (+22.4%).&lt;/li&gt; 
 &lt;li&gt;ğŸš€ &lt;strong&gt;Beats LLM Agents&lt;/strong&gt;: &lt;strong&gt;73.5%&lt;/strong&gt; (DeepCode) vs best LLM frameworks 43.3% (+30.2%).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸš€ Key Features&lt;/h2&gt; 
&lt;br /&gt; 
&lt;table align="center" width="100%" style="border: none; table-layout: fixed;"&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td width="30%" align="center" style="vertical-align: top; padding: 20px;"&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;h3 style="margin: 0; padding: 0;"&gt;ğŸš€ &lt;strong&gt;Paper2Code&lt;/strong&gt;&lt;/h3&gt; 
    &lt;/div&gt; 
    &lt;div align="center" style="margin: 15px 0;"&gt; 
     &lt;img src="https://img.shields.io/badge/ALGORITHM-IMPLEMENTATION-ff6b6b?style=for-the-badge&amp;amp;logo=algorithm&amp;amp;logoColor=white" alt="Algorithm Badge" /&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;&lt;strong&gt;Automated Implementation of Complex Algorithms&lt;/strong&gt;&lt;/p&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 60px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;Effortlessly converts complex algorithms from research papers into &lt;strong&gt;high-quality&lt;/strong&gt;, &lt;strong&gt;production-ready&lt;/strong&gt; code, accelerating algorithm reproduction.&lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
   &lt;td width="30%" align="center" style="vertical-align: top; padding: 20px;"&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;h3 style="margin: 0; padding: 0;"&gt;ğŸ¨ &lt;strong&gt;Text2Web&lt;/strong&gt;&lt;/h3&gt; 
    &lt;/div&gt; 
    &lt;div align="center" style="margin: 15px 0;"&gt; 
     &lt;img src="https://img.shields.io/badge/FRONTEND-DEVELOPMENT-4ecdc4?style=for-the-badge&amp;amp;logo=react&amp;amp;logoColor=white" alt="Frontend Badge" /&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;&lt;strong&gt;Automated Front-End Web Development&lt;/strong&gt;&lt;/p&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 60px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;Translates plain textual descriptions into &lt;strong&gt;fully functional&lt;/strong&gt;, &lt;strong&gt;visually appealing&lt;/strong&gt; front-end web code for rapid interface creation.&lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
   &lt;td width="30%" align="center" style="vertical-align: top; padding: 20px;"&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;h3 style="margin: 0; padding: 0;"&gt;âš™ï¸ &lt;strong&gt;Text2Backend&lt;/strong&gt;&lt;/h3&gt; 
    &lt;/div&gt; 
    &lt;div align="center" style="margin: 15px 0;"&gt; 
     &lt;img src="https://img.shields.io/badge/BACKEND-DEVELOPMENT-9b59b6?style=for-the-badge&amp;amp;logo=server&amp;amp;logoColor=white" alt="Backend Badge" /&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;&lt;strong&gt;Automated Back-End Development&lt;/strong&gt;&lt;/p&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 60px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;Generates &lt;strong&gt;efficient&lt;/strong&gt;, &lt;strong&gt;scalable&lt;/strong&gt;, and &lt;strong&gt;feature-rich&lt;/strong&gt; back-end code from simple text inputs, streamlining server-side development.&lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;br /&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ“Š Experimental Results&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/HKUDS/DeepCode/main/assets/result_main02.jpg" /&gt;
 &lt;br /&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;p&gt;We evaluate &lt;strong&gt;DeepCode&lt;/strong&gt; on the &lt;a href="https://openai.com/index/paperbench/"&gt;&lt;em&gt;PaperBench&lt;/em&gt;&lt;/a&gt; benchmark (released by OpenAI), a rigorous testbed requiring AI agents to independently reproduce 20 ICML 2024 papers from scratch. The benchmark comprises 8,316 gradable components assessed using SimpleJudge with hierarchical weighting.&lt;/p&gt; 
&lt;p&gt;Our experiments compare DeepCode against four baseline categories: &lt;strong&gt;(1) Human Experts&lt;/strong&gt;, &lt;strong&gt;(2) State-of-the-Art Commercial Code Agents&lt;/strong&gt;, &lt;strong&gt;(3) Scientific Code Agents&lt;/strong&gt;, and &lt;strong&gt;(4) LLM-Based Agents&lt;/strong&gt;.&lt;/p&gt; 
&lt;h3&gt;â‘  ğŸ§  Human Expert Performance (Top Machine Learning PhD)&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;DeepCode: 75.9% vs. Top Machine Learning PhD: 72.4% (+3.5%)&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;DeepCode achieves &lt;strong&gt;75.9%&lt;/strong&gt; on the 3-paper human evaluation subset, &lt;strong&gt;surpassing the best-of-3 human expert baseline (72.4%) by +3.5 percentage points&lt;/strong&gt;. This demonstrates that our framework not only matches but exceeds expert-level code reproduction capabilities, representing a significant milestone in autonomous scientific software engineering.&lt;/p&gt; 
&lt;h3&gt;â‘¡ ğŸ’¼ State-of-the-Art Commercial Code Agents&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;DeepCode: 84.8% vs. Best Commercial Agent: 58.7% (+26.1%)&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;On the 5-paper subset, DeepCode substantially outperforms leading commercial coding tools:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Cursor: 58.4%&lt;/li&gt; 
 &lt;li&gt;Claude Code: 58.7%&lt;/li&gt; 
 &lt;li&gt;Codex: 40.0%&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;DeepCode: 84.8%&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This represents a &lt;strong&gt;+26.1% improvement&lt;/strong&gt; over the leading commercial code agent. All commercial agents utilize Claude Sonnet 4.5 or GPT-5 Codex-high, highlighting that &lt;strong&gt;DeepCode's superior architecture&lt;/strong&gt;â€”rather than base model capabilityâ€”drives this performance gap.&lt;/p&gt; 
&lt;h3&gt;â‘¢ ğŸ”¬ Scientific Code Agents&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;DeepCode: 73.5% vs. PaperCoder: 51.1% (+22.4%)&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Compared to PaperCoder (&lt;strong&gt;51.1%&lt;/strong&gt;), the state-of-the-art scientific code reproduction framework, DeepCode achieves &lt;strong&gt;73.5%&lt;/strong&gt;, demonstrating a &lt;strong&gt;+22.4% relative improvement&lt;/strong&gt;. This substantial margin validates our multi-module architecture combining planning, hierarchical task decomposition, code generation, and iterative debugging over simpler pipeline-based approaches.&lt;/p&gt; 
&lt;h3&gt;â‘£ ğŸ¤– LLM-Based Agents&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;DeepCode: 73.5% vs. Best LLM Agent: 43.3% (+30.2%)&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;DeepCode significantly outperforms all tested LLM agents:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Claude 3.5 Sonnet + IterativeAgent: 27.5%&lt;/li&gt; 
 &lt;li&gt;o1 + IterativeAgent (36 hours): 42.4%&lt;/li&gt; 
 &lt;li&gt;o1 BasicAgent: 43.3%&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;DeepCode: 73.5%&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The &lt;strong&gt;+30.2% improvement&lt;/strong&gt; over the best-performing LLM agent demonstrates that sophisticated agent scaffolding, rather than extended inference time or larger models, is critical for complex code reproduction tasks.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;ğŸ¯ &lt;strong&gt;Autonomous Self-Orchestrating Multi-Agent Architecture&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;The Challenges&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;ğŸ“„ &lt;strong&gt;Implementation Complexity&lt;/strong&gt;: Converting academic papers and complex algorithms into working code requires significant technical effort and domain expertise&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;ğŸ”¬ &lt;strong&gt;Research Bottleneck&lt;/strong&gt;: Researchers spend valuable time implementing algorithms instead of focusing on their core research and discovery work&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;â±ï¸ &lt;strong&gt;Development Delays&lt;/strong&gt;: Product teams experience long wait times between concept and testable prototypes, slowing down innovation cycles&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;ğŸ”„ &lt;strong&gt;Repetitive Coding&lt;/strong&gt;: Developers repeatedly implement similar patterns and functionality instead of building on existing solutions&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;DeepCode&lt;/strong&gt; addresses these workflow inefficiencies by providing reliable automation for common development tasks, streamlining your development workflow from concept to code.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;pre&gt;&lt;code class="language-mermaid"&gt;flowchart LR
    A["ğŸ“„ Research Papers&amp;lt;br/&amp;gt;ğŸ’¬ Text Prompts&amp;lt;br/&amp;gt;ğŸŒ URLs &amp;amp; Document&amp;lt;br/&amp;gt;ğŸ“ Files: PDF, DOC, PPTX, TXT, HTML"] --&amp;gt; B["ğŸ§  DeepCode&amp;lt;br/&amp;gt;Multi-Agent Engine"]
    B --&amp;gt; C["ğŸš€ Algorithm Implementation &amp;lt;br/&amp;gt;ğŸ¨ Frontend Development &amp;lt;br/&amp;gt;âš™ï¸ Backend Development"]

    style A fill:#ff6b6b,stroke:#c0392b,stroke-width:2px,color:#000
    style B fill:#00d4ff,stroke:#0984e3,stroke-width:3px,color:#000
    style C fill:#00b894,stroke:#00a085,stroke-width:2px,color:#000
&lt;/code&gt;&lt;/pre&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ—ï¸ Architecture&lt;/h2&gt; 
&lt;h3&gt;ğŸ“Š &lt;strong&gt;System Overview&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;DeepCode&lt;/strong&gt; is an AI-powered development platform that automates code generation and implementation tasks. Our multi-agent system handles the complexity of translating requirements into functional, well-structured code, allowing you to focus on innovation rather than implementation details.&lt;/p&gt; 
&lt;p&gt;ğŸ¯ &lt;strong&gt;Technical Capabilities&lt;/strong&gt;:&lt;/p&gt; 
&lt;p&gt;ğŸ§¬ &lt;strong&gt;Research-to-Production Pipeline&lt;/strong&gt;&lt;br /&gt; Multi-modal document analysis engine that extracts algorithmic logic and mathematical models from academic papers. Generates optimized implementations with proper data structures while preserving computational complexity characteristics.&lt;/p&gt; 
&lt;p&gt;ğŸª„ &lt;strong&gt;Natural Language Code Synthesis&lt;/strong&gt;&lt;br /&gt; Context-aware code generation using fine-tuned language models trained on curated code repositories. Maintains architectural consistency across modules while supporting multiple programming languages and frameworks.&lt;/p&gt; 
&lt;p&gt;âš¡ &lt;strong&gt;Automated Prototyping Engine&lt;/strong&gt;&lt;br /&gt; Intelligent scaffolding system generating complete application structures including database schemas, API endpoints, and frontend components. Uses dependency analysis to ensure scalable architecture from initial generation.&lt;/p&gt; 
&lt;p&gt;ğŸ’ &lt;strong&gt;Quality Assurance Automation&lt;/strong&gt;&lt;br /&gt; Integrated static analysis with automated unit test generation and documentation synthesis. Employs AST analysis for code correctness and property-based testing for comprehensive coverage.&lt;/p&gt; 
&lt;p&gt;ğŸ”® &lt;strong&gt;CodeRAG Integration System&lt;/strong&gt;&lt;br /&gt; Advanced retrieval-augmented generation combining semantic vector embeddings with graph-based dependency analysis. Automatically discovers optimal libraries and implementation patterns from large-scale code corpus.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;ğŸ”§ &lt;strong&gt;Core Techniques&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;ğŸ§  &lt;strong&gt;Intelligent Orchestration Agent&lt;/strong&gt;: Central decision-making system that coordinates workflow phases and analyzes requirements. Employs dynamic planning algorithms to adapt execution strategies in real-time based on evolving project complexity. Dynamically selects optimal processing strategies for each implementation step. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;ğŸ’¾ &lt;strong&gt;Efficient Memory Mechanism&lt;/strong&gt;: Advanced context engineering system that manages large-scale code contexts efficiently. Implements hierarchical memory structures with intelligent compression for handling complex codebases. This component enables instant retrieval of implementation patterns and maintains semantic coherence across extended development sessions. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;ğŸ” &lt;strong&gt;Advanced CodeRAG System&lt;/strong&gt;: Global code comprehension engine that analyzes complex inter-dependencies across repositories. Performs cross-codebase relationship mapping to understand architectural patterns from a holistic perspective. This module leverages dependency graphs and semantic analysis to provide globally-aware code recommendations during implementation.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h3&gt;ğŸ¤– &lt;strong&gt;Multi-Agent Architecture of DeepCode&lt;/strong&gt;:&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ¯ Central Orchestrating Agent&lt;/strong&gt;: Orchestrates entire workflow execution and makes strategic decisions. Coordinates specialized agents based on input complexity analysis. Implements dynamic task planning and resource allocation algorithms. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ“ Intent Understanding Agent&lt;/strong&gt;: Performs deep semantic analysis of user requirements to decode complex intentions. Extracts functional specifications and technical constraints through advanced NLP processing. Transforms ambiguous human descriptions into precise, actionable development specifications with structured task decomposition. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ“„ Document Parsing Agent&lt;/strong&gt;: Processes complex technical documents and research papers with advanced parsing capabilities. Extracts algorithms and methodologies using document understanding models. Converts academic concepts into practical implementation specifications through intelligent content analysis. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ—ï¸ Code Planning Agent&lt;/strong&gt;: Performs architectural design and technology stack optimization. Dynamic planning for adaptive development roadmaps. Enforces coding standards and generates modular structures through automated design pattern selection.&lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ” Code Reference Mining Agent&lt;/strong&gt;: Discovers relevant repositories and frameworks through intelligent search algorithms. Analyzes codebases for compatibility and integration potential. Provides recommendations based on similarity metrics and automated dependency analysis. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ“š Code Indexing Agent&lt;/strong&gt;: Builds comprehensive knowledge graphs of discovered codebases. Maintains semantic relationships between code components. Enables intelligent retrieval and cross-reference capabilities. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ§¬ Code Generation Agent&lt;/strong&gt;: Synthesizes gathered information into executable code implementations. Creates functional interfaces and integrates discovered components. Generates comprehensive test suites and documentation for reproducibility.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h4&gt;ğŸ› ï¸ &lt;strong&gt;Implementation Tools Matrix&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;ğŸ”§ Powered by MCP (Model Context Protocol)&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;DeepCode leverages the &lt;strong&gt;Model Context Protocol (MCP)&lt;/strong&gt; standard to seamlessly integrate with various tools and services. This standardized approach ensures reliable communication between AI agents and external systems, enabling powerful automation capabilities.&lt;/p&gt; 
&lt;h5&gt;ğŸ“¡ &lt;strong&gt;MCP Servers &amp;amp; Tools&lt;/strong&gt;&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;ğŸ› ï¸ &lt;strong&gt;MCP Server&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;ğŸ”§ &lt;strong&gt;Primary Function&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;ğŸ’¡ &lt;strong&gt;Purpose &amp;amp; Capabilities&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ğŸ” brave&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Web Search Engine&lt;/td&gt; 
   &lt;td&gt;Real-time information retrieval via Brave Search API&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ğŸŒ bocha-mcp&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Alternative Search&lt;/td&gt; 
   &lt;td&gt;Secondary search option with independent API access&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ğŸ“‚ filesystem&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;File System Operations&lt;/td&gt; 
   &lt;td&gt;Local file and directory management, read/write operations&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ğŸŒ fetch&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Web Content Retrieval&lt;/td&gt; 
   &lt;td&gt;Fetch and extract content from URLs and web resources&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ğŸ“¥ github-downloader&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Repository Management&lt;/td&gt; 
   &lt;td&gt;Clone and download GitHub repositories for analysis&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ğŸ“‹ file-downloader&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Document Processing&lt;/td&gt; 
   &lt;td&gt;Download and convert files (PDF, DOCX, etc.) to Markdown&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;âš¡ command-executor&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;System Commands&lt;/td&gt; 
   &lt;td&gt;Execute bash/shell commands for environment management&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ğŸ§¬ code-implementation&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Code Generation Hub&lt;/td&gt; 
   &lt;td&gt;Comprehensive code reproduction with execution and testing&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ğŸ“š code-reference-indexer&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Smart Code Search&lt;/td&gt; 
   &lt;td&gt;Intelligent indexing and search of code repositories&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ğŸ“„ document-segmentation&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Smart Document Analysis&lt;/td&gt; 
   &lt;td&gt;Intelligent document segmentation for large papers and technical documents&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h5&gt;ğŸ”§ &lt;strong&gt;Legacy Tool Functions&lt;/strong&gt; &lt;em&gt;(for reference)&lt;/em&gt;&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;ğŸ› ï¸ &lt;strong&gt;Function&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;ğŸ¯ &lt;strong&gt;Usage Context&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ğŸ“„ read_code_mem&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Efficient code context retrieval from memory&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;âœï¸ write_file&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Direct file content generation and modification&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ğŸ execute_python&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Python code testing and validation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ğŸ“ get_file_structure&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Project structure analysis and organization&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;âš™ï¸ set_workspace&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Dynamic workspace and environment configuration&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ğŸ“Š get_operation_history&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Process monitoring and operation tracking&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;p&gt;ğŸ›ï¸ &lt;strong&gt;Multi-Interface Framework&lt;/strong&gt;&lt;br /&gt; RESTful API with CLI and web frontends featuring real-time code streaming, interactive debugging, and extensible plugin architecture for CI/CD integration.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;ğŸš€ Multi-Agent Intelligent Pipeline:&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;h3&gt;ğŸŒŸ &lt;strong&gt;Intelligence Processing Flow&lt;/strong&gt;&lt;/h3&gt; 
 &lt;table align="center" width="100%" style="border: none; border-collapse: collapse;"&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td colspan="3" align="center" style="padding: 20px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 15px; color: white; font-weight: bold;"&gt; ğŸ’¡ &lt;strong&gt;INPUT LAYER&lt;/strong&gt;&lt;br /&gt; ğŸ“„ Research Papers â€¢ ğŸ’¬ Natural Language â€¢ ğŸŒ URLs â€¢ ğŸ“‹ Requirements &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan="3" height="20"&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td colspan="3" align="center" style="padding: 15px; background: linear-gradient(135deg, #ff6b6b 0%, #ee5a24 100%); border-radius: 12px; color: white; font-weight: bold;"&gt; ğŸ¯ &lt;strong&gt;CENTRAL ORCHESTRATION&lt;/strong&gt;&lt;br /&gt; Strategic Decision Making â€¢ Workflow Coordination â€¢ Agent Management &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan="3" height="15"&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center" style="padding: 12px; background: linear-gradient(135deg, #3742fa 0%, #2f3542 100%); border-radius: 10px; color: white; width: 50%;"&gt; ğŸ“ &lt;strong&gt;TEXT ANALYSIS&lt;/strong&gt;&lt;br /&gt; &lt;small&gt;Requirement Processing&lt;/small&gt; &lt;/td&gt; 
    &lt;td width="10"&gt;&lt;/td&gt; 
    &lt;td align="center" style="padding: 12px; background: linear-gradient(135deg, #8c7ae6 0%, #9c88ff 100%); border-radius: 10px; color: white; width: 50%;"&gt; ğŸ“„ &lt;strong&gt;DOCUMENT ANALYSIS&lt;/strong&gt;&lt;br /&gt; &lt;small&gt;Paper &amp;amp; Spec Processing&lt;/small&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan="3" height="15"&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td colspan="3" align="center" style="padding: 15px; background: linear-gradient(135deg, #00d2d3 0%, #54a0ff 100%); border-radius: 12px; color: white; font-weight: bold;"&gt; ğŸ“‹ &lt;strong&gt;REPRODUCTION PLANNING&lt;/strong&gt;&lt;br /&gt; Deep Paper Analysis â€¢ Code Requirements Parsing â€¢ Reproduction Strategy Development &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan="3" height="15"&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center" style="padding: 12px; background: linear-gradient(135deg, #ffa726 0%, #ff7043 100%); border-radius: 10px; color: white; width: 50%;"&gt; ğŸ” &lt;strong&gt;REFERENCE ANALYSIS&lt;/strong&gt;&lt;br /&gt; &lt;small&gt;Repository Discovery&lt;/small&gt; &lt;/td&gt; 
    &lt;td width="10"&gt;&lt;/td&gt; 
    &lt;td align="center" style="padding: 12px; background: linear-gradient(135deg, #e056fd 0%, #f368e0 100%); border-radius: 10px; color: white; width: 50%;"&gt; ğŸ“š &lt;strong&gt;CODE INDEXING&lt;/strong&gt;&lt;br /&gt; &lt;small&gt;Knowledge Graph Building&lt;/small&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan="3" height="15"&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td colspan="3" align="center" style="padding: 15px; background: linear-gradient(135deg, #26de81 0%, #20bf6b 100%); border-radius: 12px; color: white; font-weight: bold;"&gt; ğŸ§¬ &lt;strong&gt;CODE IMPLEMENTATION&lt;/strong&gt;&lt;br /&gt; Implementation Generation â€¢ Testing â€¢ Documentation &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan="3" height="15"&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td colspan="3" align="center" style="padding: 20px; background: linear-gradient(135deg, #045de9 0%, #09c6f9 100%); border-radius: 15px; color: white; font-weight: bold;"&gt; âš¡ &lt;strong&gt;OUTPUT DELIVERY&lt;/strong&gt;&lt;br /&gt; ğŸ“¦ Complete Codebase â€¢ ğŸ§ª Test Suite â€¢ ğŸ“š Documentation â€¢ ğŸš€ Deployment Ready &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;br /&gt; 
 &lt;h3&gt;ğŸ”„ &lt;strong&gt;Process Intelligence Features&lt;/strong&gt;&lt;/h3&gt; 
 &lt;table align="center" style="border: none;"&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td align="center" width="25%" style="padding: 15px;"&gt; 
     &lt;div style="background: #f8f9fa; border-radius: 10px; padding: 15px; border-left: 4px solid #ff6b6b;"&gt; 
      &lt;h4&gt;ğŸ¯ Adaptive Flow&lt;/h4&gt; 
      &lt;p&gt;&lt;small&gt;Dynamic agent selection based on input complexity&lt;/small&gt;&lt;/p&gt; 
     &lt;/div&gt; &lt;/td&gt; 
    &lt;td align="center" width="25%" style="padding: 15px;"&gt; 
     &lt;div style="background: #f8f9fa; border-radius: 10px; padding: 15px; border-left: 4px solid #4ecdc4;"&gt; 
      &lt;h4&gt;ğŸ§  Smart Coordination&lt;/h4&gt; 
      &lt;p&gt;&lt;small&gt;Intelligent task distribution and parallel processing&lt;/small&gt;&lt;/p&gt; 
     &lt;/div&gt; &lt;/td&gt; 
    &lt;td align="center" width="25%" style="padding: 15px;"&gt; 
     &lt;div style="background: #f8f9fa; border-radius: 10px; padding: 15px; border-left: 4px solid #45b7d1;"&gt; 
      &lt;h4&gt;ğŸ” Context Awareness&lt;/h4&gt; 
      &lt;p&gt;&lt;small&gt;Deep understanding through CodeRAG integration&lt;/small&gt;&lt;/p&gt; 
     &lt;/div&gt; &lt;/td&gt; 
    &lt;td align="center" width="25%" style="padding: 15px;"&gt; 
     &lt;div style="background: #f8f9fa; border-radius: 10px; padding: 15px; border-left: 4px solid #96ceb4;"&gt; 
      &lt;h4&gt;âš¡ Quality Assurance&lt;/h4&gt; 
      &lt;p&gt;&lt;small&gt;Automated testing and validation throughout&lt;/small&gt;&lt;/p&gt; 
     &lt;/div&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸš€ Quick Start&lt;/h2&gt; 
&lt;h3&gt;ğŸ“¦ &lt;strong&gt;Step 1: Installation&lt;/strong&gt;&lt;/h3&gt; 
&lt;h4&gt;âš¡ &lt;strong&gt;Direct Installation (Recommended)&lt;/strong&gt;&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# ğŸš€ Install DeepCode package directly
pip install deepcode-hku

# ğŸ”‘ Download configuration files
curl -O https://raw.githubusercontent.com/HKUDS/DeepCode/main/mcp_agent.config.yaml
curl -O https://raw.githubusercontent.com/HKUDS/DeepCode/main/mcp_agent.secrets.yaml

# ğŸ”‘ Configure API keys (required)
# Edit mcp_agent.secrets.yaml with your API keys and base_url:
# - openai: api_key, base_url (for OpenAI/custom endpoints)
# - anthropic: api_key (for Claude models)

# ğŸ”‘ Configure search API keys for web search (optional)
# Edit mcp_agent.config.yaml to set your API keys:
# - For Brave Search: Set BRAVE_API_KEY: "your_key_here" in brave.env section (line ~28)
# - For Bocha-MCP: Set BOCHA_API_KEY: "your_key_here" in bocha-mcp.env section (line ~74)

# ğŸ“„ Configure document segmentation (optional)
# Edit mcp_agent.config.yaml to control document processing:
# - enabled: true/false (whether to use intelligent document segmentation)
# - size_threshold_chars: 50000 (document size threshold to trigger segmentation)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;ğŸ”§ &lt;strong&gt;Development Installation (From Source)&lt;/strong&gt;&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ğŸ“‚ Click to expand development installation options&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h5&gt;ğŸ”¥ &lt;strong&gt;Using UV (Recommended for Development)&lt;/strong&gt;&lt;/h5&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# ğŸ”½ Clone the repository
git clone https://github.com/HKUDS/DeepCode.git
cd DeepCode/

# ğŸ“¦ Install UV package manager
curl -LsSf https://astral.sh/uv/install.sh | sh

# ğŸ”§ Install dependencies with UV
uv venv --python=3.13
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
uv pip install -r requirements.txt

# ğŸ”‘ Configure API keys (required)
# Edit mcp_agent.secrets.yaml with your API keys and base_url:
# - openai: api_key, base_url (for OpenAI/custom endpoints)
# - anthropic: api_key (for Claude models)

# ğŸ”‘ Configure search API keys for web search (optional)
# Edit mcp_agent.config.yaml to set your API keys:
# - For Brave Search: Set BRAVE_API_KEY: "your_key_here" in brave.env section (line ~28)
# - For Bocha-MCP: Set BOCHA_API_KEY: "your_key_here" in bocha-mcp.env section (line ~74)

# ğŸ“„ Configure document segmentation (optional)
# Edit mcp_agent.config.yaml to control document processing:
# - enabled: true/false (whether to use intelligent document segmentation)
# - size_threshold_chars: 50000 (document size threshold to trigger segmentation)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h5&gt;ğŸ &lt;strong&gt;Using Traditional pip&lt;/strong&gt;&lt;/h5&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# ğŸ”½ Clone the repository
git clone https://github.com/HKUDS/DeepCode.git
cd DeepCode/

# ğŸ“¦ Install dependencies
pip install -r requirements.txt

# ğŸ”‘ Configure API keys (required)
# Edit mcp_agent.secrets.yaml with your API keys and base_url:
# - openai: api_key, base_url (for OpenAI/custom endpoints)
# - anthropic: api_key (for Claude models)

# ğŸ”‘ Configure search API keys for web search (optional)
# Edit mcp_agent.config.yaml to set your API keys:
# - For Brave Search: Set BRAVE_API_KEY: "your_key_here" in brave.env section (line ~28)
# - For Bocha-MCP: Set BOCHA_API_KEY: "your_key_here" in bocha-mcp.env section (line ~74)

# ğŸ“„ Configure document segmentation (optional)
# Edit mcp_agent.config.yaml to control document processing:
# - enabled: true/false (whether to use intelligent document segmentation)
# - size_threshold_chars: 50000 (document size threshold to trigger segmentation)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h4&gt;ğŸªŸ &lt;strong&gt;Windows Users: Additional MCP Server Configuration&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;If you're using Windows, you may need to configure MCP servers manually in &lt;code&gt;mcp_agent.config.yaml&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 1. Install MCP servers globally
npm i -g @modelcontextprotocol/server-brave-search
npm i -g @modelcontextprotocol/server-filesystem

# 2. Find your global node_modules path
npm -g root
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then update your &lt;code&gt;mcp_agent.config.yaml&lt;/code&gt; to use absolute paths:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;mcp:
  servers:
    brave:
      command: "node"
      args: ["C:/Program Files/nodejs/node_modules/@modelcontextprotocol/server-brave-search/dist/index.js"]
    filesystem:
      command: "node"
      args: ["C:/Program Files/nodejs/node_modules/@modelcontextprotocol/server-filesystem/dist/index.js", "."]
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Replace the path with your actual global node_modules path from step 2.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;ğŸ” &lt;strong&gt;Search Server Configuration (Optional)&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;DeepCode supports multiple search servers for web search functionality. You can configure your preferred option in &lt;code&gt;mcp_agent.config.yaml&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;# Default search server configuration
# Options: "brave" or "bocha-mcp"
default_search_server: "brave"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Available Options:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ” Brave Search&lt;/strong&gt; (&lt;code&gt;"brave"&lt;/code&gt;):&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Default option with high-quality search results&lt;/li&gt; 
   &lt;li&gt;Requires BRAVE_API_KEY configuration&lt;/li&gt; 
   &lt;li&gt;Recommended for most users&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸŒ Bocha-MCP&lt;/strong&gt; (&lt;code&gt;"bocha-mcp"&lt;/code&gt;):&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Alternative search server option&lt;/li&gt; 
   &lt;li&gt;Requires BOCHA_API_KEY configuration&lt;/li&gt; 
   &lt;li&gt;Uses local Python server implementation&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;API Key Configuration in mcp_agent.config.yaml:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;# For Brave Search (default) - around line 28
brave:
  command: "npx"
  args: ["-y", "@modelcontextprotocol/server-brave-search"]
  env:
    BRAVE_API_KEY: "your_brave_api_key_here"

# For Bocha-MCP (alternative) - around line 74
bocha-mcp:
  command: "python"
  args: ["tools/bocha_search_server.py"]
  env:
    PYTHONPATH: "."
    BOCHA_API_KEY: "your_bocha_api_key_here"
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;ğŸ’¡ Tip&lt;/strong&gt;: Both search servers require API key configuration. Choose the one that best fits your API access and requirements.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;âš¡ &lt;strong&gt;Step 2: Launch Application&lt;/strong&gt;&lt;/h3&gt; 
&lt;h4&gt;ğŸš€ &lt;strong&gt;Using Installed Package (Recommended)&lt;/strong&gt;&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# ğŸŒ Launch web interface directly
deepcode

# The application will automatically start at http://localhost:8501
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;ğŸ› ï¸ &lt;strong&gt;Using Source Code&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;Choose your preferred interface:&lt;/p&gt; 
&lt;h5&gt;ğŸŒ &lt;strong&gt;Web Interface&lt;/strong&gt; (Recommended)&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Using UV
uv run streamlit run ui/streamlit_app.py
# Or using traditional Python
streamlit run ui/streamlit_app.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://img.shields.io/badge/Access-localhost:8501-00d4ff?style=flat-square&amp;amp;logo=streamlit&amp;amp;logoColor=white" alt="Web Access" /&gt; 
&lt;/div&gt; 
&lt;h5&gt;ğŸ–¥ï¸ &lt;strong&gt;CLI Interface&lt;/strong&gt; (Advanced Users)&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Using UV
uv run python cli/main_cli.py
# Or using traditional Python
python cli/main_cli.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://img.shields.io/badge/Mode-Interactive_Terminal-9b59b6?style=flat-square&amp;amp;logo=terminal&amp;amp;logoColor=white" alt="CLI Mode" /&gt; 
&lt;/div&gt; 
&lt;h3&gt;ğŸ¯ &lt;strong&gt;Step 3: Generate Code&lt;/strong&gt;&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ“„ Input&lt;/strong&gt;: Upload your research paper, provide requirements, or paste a URL&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ¤– Processing&lt;/strong&gt;: Watch the multi-agent system analyze and plan&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;âš¡ Output&lt;/strong&gt;: Receive production-ready code with tests and documentation&lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ’¡ Examples&lt;/h2&gt; 
&lt;h3&gt;ğŸ¬ &lt;strong&gt;Live Demonstrations&lt;/strong&gt;&lt;/h3&gt; 
&lt;table align="center"&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td width="33%" align="center"&gt; &lt;h4&gt;ğŸ“„ &lt;strong&gt;Paper2Code Demo&lt;/strong&gt;&lt;/h4&gt; &lt;p&gt;&lt;strong&gt;Research to Implementation&lt;/strong&gt;&lt;/p&gt; 
    &lt;div align="center"&gt; 
     &lt;a href="https://www.youtube.com/watch?v=MQZYpLkzsbw"&gt; &lt;img src="https://img.youtube.com/vi/MQZYpLkzsbw/maxresdefault.jpg" alt="Paper2Code Demo" width="100%" style="border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);" /&gt; &lt;/a&gt; 
     &lt;p&gt;&lt;strong&gt;&lt;a href="https://www.youtube.com/watch?v=MQZYpLkzsbw"&gt;â–¶ï¸ Watch Demo&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
     &lt;p&gt;&lt;em&gt;Transform academic papers into production-ready code automatically&lt;/em&gt;&lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
   &lt;td width="33%" align="center"&gt; &lt;h4&gt;ğŸ–¼ï¸ &lt;strong&gt;Image Processing Demo&lt;/strong&gt;&lt;/h4&gt; &lt;p&gt;&lt;strong&gt;AI-Powered Image Tools&lt;/strong&gt;&lt;/p&gt; 
    &lt;div align="center"&gt; 
     &lt;a href="https://www.youtube.com/watch?v=nFt5mLaMEac"&gt; &lt;img src="https://img.youtube.com/vi/nFt5mLaMEac/maxresdefault.jpg" alt="Image Processing Demo" width="100%" style="border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);" /&gt; &lt;/a&gt; 
     &lt;p&gt;&lt;strong&gt;&lt;a href="https://www.youtube.com/watch?v=nFt5mLaMEac"&gt;â–¶ï¸ Watch Demo&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
     &lt;p&gt;&lt;em&gt;Intelligent image processing with background removal and enhancement&lt;/em&gt;&lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
   &lt;td width="33%" align="center"&gt; &lt;h4&gt;ğŸŒ &lt;strong&gt;Frontend Implementation&lt;/strong&gt;&lt;/h4&gt; &lt;p&gt;&lt;strong&gt;Complete Web Application&lt;/strong&gt;&lt;/p&gt; 
    &lt;div align="center"&gt; 
     &lt;a href="https://www.youtube.com/watch?v=78wx3dkTaAU"&gt; &lt;img src="https://img.youtube.com/vi/78wx3dkTaAU/maxresdefault.jpg" alt="Frontend Demo" width="100%" style="border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);" /&gt; &lt;/a&gt; 
     &lt;p&gt;&lt;strong&gt;&lt;a href="https://www.youtube.com/watch?v=78wx3dkTaAU"&gt;â–¶ï¸ Watch Demo&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
     &lt;p&gt;&lt;em&gt;Full-stack web development from concept to deployment&lt;/em&gt;&lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h3&gt;ğŸ†• &lt;strong&gt;Recent Updates&lt;/strong&gt;&lt;/h3&gt; 
&lt;h4&gt;ğŸ“„ &lt;strong&gt;Smart Document Segmentation (v1.2.0)&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Intelligent Processing&lt;/strong&gt;: Automatically handles large research papers and technical documents that exceed LLM token limits&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Configurable Control&lt;/strong&gt;: Toggle segmentation via configuration with size-based thresholds&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Semantic Analysis&lt;/strong&gt;: Advanced content understanding with algorithm, concept, and formula preservation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Backward Compatibility&lt;/strong&gt;: Seamlessly falls back to traditional processing for smaller documents&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸš€ &lt;strong&gt;Coming Soon&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;We're continuously enhancing DeepCode with exciting new features:&lt;/p&gt; 
&lt;h4&gt;ğŸ”§ &lt;strong&gt;Enhanced Code Reliability &amp;amp; Validation&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Automated Testing&lt;/strong&gt;: Comprehensive functionality testing with execution verification and error detection.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Code Quality Assurance&lt;/strong&gt;: Multi-level validation through static analysis, dynamic testing, and performance benchmarking.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Smart Debugging&lt;/strong&gt;: AI-powered error detection with automatic correction suggestions&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;ğŸ“Š &lt;strong&gt;PaperBench Performance Showcase&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Benchmark Dashboard&lt;/strong&gt;: Comprehensive performance metrics on the PaperBench evaluation suite.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Accuracy Metrics&lt;/strong&gt;: Detailed comparison with state-of-the-art paper reproduction systems.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Success Analytics&lt;/strong&gt;: Statistical analysis across paper categories and complexity levels.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;âš¡ &lt;strong&gt;System-wide Optimizations&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Performance Boost&lt;/strong&gt;: Multi-threaded processing and optimized agent coordination for faster generation.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Enhanced Reasoning&lt;/strong&gt;: Advanced reasoning capabilities with improved context understanding.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Expanded Support&lt;/strong&gt;: Extended compatibility with additional programming languages and frameworks.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;â­ Star History&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;em&gt;Community Growth Trajectory&lt;/em&gt;&lt;/p&gt; 
 &lt;a href="https://star-history.com/#HKUDS/DeepCode&amp;amp;Date"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=HKUDS/DeepCode&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
   &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=HKUDS/DeepCode&amp;amp;type=Date" /&gt; 
   &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=HKUDS/DeepCode&amp;amp;type=Date" style="border-radius: 15px; box-shadow: 0 0 30px rgba(0, 217, 255, 0.3);" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h3&gt;ğŸš€ &lt;strong&gt;Ready to Transform Development?&lt;/strong&gt;&lt;/h3&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-quick-start"&gt;&lt;img src="https://img.shields.io/badge/ğŸš€_Get_Started-00d4ff?style=for-the-badge&amp;amp;logo=rocket&amp;amp;logoColor=white" alt="Get Started" /&gt;&lt;/a&gt; &lt;a href="https://github.com/HKUDS"&gt;&lt;img src="https://img.shields.io/badge/ğŸ›ï¸_View_on_GitHub-00d4ff?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white" alt="View on GitHub" /&gt;&lt;/a&gt; &lt;a href="https://github.com/HKUDS/deepcode-agent"&gt;&lt;img src="https://img.shields.io/badge/â­_Star_Project-00d4ff?style=for-the-badge&amp;amp;logo=star&amp;amp;logoColor=white" alt="Star Project" /&gt;&lt;/a&gt; &lt;/p&gt; 
 &lt;hr /&gt; 
 &lt;h3&gt;ğŸ“„ &lt;strong&gt;License&lt;/strong&gt;&lt;/h3&gt; 
 &lt;img src="https://img.shields.io/badge/License-MIT-4ecdc4?style=for-the-badge&amp;amp;logo=opensourceinitiative&amp;amp;logoColor=white" alt="MIT License" /&gt; 
 &lt;p&gt;&lt;strong&gt;MIT License&lt;/strong&gt; - Copyright (c) 2025 Data Intelligence Lab, The University of Hong Kong&lt;/p&gt; 
 &lt;hr /&gt; 
 &lt;img src="https://visitor-badge.laobi.icu/badge?page_id=deepcode.readme&amp;amp;style=for-the-badge&amp;amp;color=00d4ff" alt="Visitors" /&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>coleam00/ottomator-agents</title>
      <link>https://github.com/coleam00/ottomator-agents</link>
      <description>&lt;p&gt;All the open source AI Agents hosted on the oTTomator Live Agent Studio platform!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;What is the Live Agent Studio?&lt;/h1&gt; 
&lt;p&gt;The &lt;a href="https://studio.ottomator.ai"&gt;Live Agent Studio&lt;/a&gt; is a community-driven platform developed by &lt;a href="https://ottomator.ai"&gt;oTTomator&lt;/a&gt; for you to explore cutting-edge AI agents and learn how to implement them for yourself or your business! All agents on this platform are open source and, over time, will cover a very large variety of use cases.&lt;/p&gt; 
&lt;p&gt;The goal with the studio is to build an educational platform for you to learn how to do incredible things with AI, while still providing practical value so that youâ€™ll want to use the agents just for the sake of what they can do for you!&lt;/p&gt; 
&lt;p&gt;This platform is still in beta â€“ expect longer response times under load, a rapidly growing agent library over the coming months, and a lot more content on this platform soon on Cole Medinâ€™s YouTube channel!&lt;/p&gt; 
&lt;h1&gt;What is this Repository for?&lt;/h1&gt; 
&lt;p&gt;This repository contains the source code/workflow JSON for all the agents on the Live Agent Studio! Every agent being added to the platform is currently be open sourced here so we can not only create a curated collection of cutting-edge agents together as a community, but also learn from one another!&lt;/p&gt; 
&lt;h2&gt;Tokens&lt;/h2&gt; 
&lt;p&gt;Most agents on the Live Agent Studio cost tokens to use, which are purchasable on the platform. However, when you first sign in you are given some tokens to start so you can use the agents free of charge! The biggest reason agents cost tokens is that we pay for the LLM usage since we host all the agents developed by you and the rest of the community!&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://studio.ottomator.ai/pricing"&gt;Purchase Tokens&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Future Plans&lt;/h2&gt; 
&lt;p&gt;As the Live Agent Studio develops, it will become the go-to place to stay on top of what is possible with AI agents! Anytime there is a new AI technology, groundbreaking agent research, or a new tool/library to build agents with, itâ€™ll be featured through agents on the platform. Itâ€™s a tall order, but we have big plans for the oTTomator community, and weâ€™re confident we can grow to accomplish this!&lt;/p&gt; 
&lt;h2&gt;FAQ&lt;/h2&gt; 
&lt;h3&gt;I want to build an agent to showcase in the Live Agent Studio! How do I do that?&lt;/h3&gt; 
&lt;p&gt;Head on over here to learn how to build an agent for the platform:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://studio.ottomator.ai/guide"&gt;Developer Guide&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Also check out &lt;a href="https://raw.githubusercontent.com/coleam00/ottomator-agents/main/~sample-n8n-agent~"&gt;the sample n8n agent&lt;/a&gt; for a starting point of building an n8n agent for the Live Agent Studio, and &lt;a href="https://raw.githubusercontent.com/coleam00/ottomator-agents/main/~sample-python-agent~"&gt;the sample Python agent&lt;/a&gt; for Python.&lt;/p&gt; 
&lt;h3&gt;How many tokens does it cost to use an agent?&lt;/h3&gt; 
&lt;p&gt;Each agent will charge tokens per prompt. The number of tokens depends on the agent, as some agents use larger LLMs, some call LLMs multiple times, and some use paid APIs.&lt;/p&gt; 
&lt;h3&gt;Where can I go to talk about all these agents and get help implementing them myself?&lt;/h3&gt; 
&lt;p&gt;Head on over to our Think Tank community and feel free to make a post!&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://thinktank.ottomator.ai"&gt;Think Tank Community&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;Â© 2024 Live Agent Studio. All rights reserved.&lt;br /&gt; Created by oTTomator&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>apache/airflow</title>
      <link>https://github.com/apache/airflow</link>
      <description>&lt;p&gt;Apache Airflow - A platform to programmatically author, schedule, and monitor workflows&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Apache Airflow&lt;/h1&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Badges&lt;/th&gt; 
   &lt;th&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;License&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.apache.org/licenses/LICENSE-2.0.txt"&gt;&lt;img src="https://img.shields.io/:license-Apache%202-blue.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;PyPI&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://badge.fury.io/py/apache-airflow"&gt;&lt;img src="https://badge.fury.io/py/apache-airflow.svg?sanitize=true" alt="PyPI version" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/apache-airflow/"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/apache-airflow.svg?sanitize=true" alt="PyPI - Python Version" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/apache-airflow/"&gt;&lt;img src="https://img.shields.io/pypi/dm/apache-airflow" alt="PyPI - Downloads" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Containers&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://hub.docker.com/r/apache/airflow"&gt;&lt;img src="https://img.shields.io/docker/pulls/apache/airflow.svg?sanitize=true" alt="Docker Pulls" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/apache/airflow"&gt;&lt;img src="https://img.shields.io/docker/stars/apache/airflow.svg?sanitize=true" alt="Docker Stars" /&gt;&lt;/a&gt; &lt;a href="https://artifacthub.io/packages/search?repo=apache-airflow"&gt;&lt;img src="https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/apache-airflow" alt="Artifact HUB" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Community&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/apache/airflow/graphs/contributors"&gt;&lt;img src="https://img.shields.io/github/contributors/apache/airflow" alt="Contributors" /&gt;&lt;/a&gt; &lt;a href="https://s.apache.org/airflow-slack"&gt;&lt;img src="https://img.shields.io/badge/slack-join_chat-white.svg?logo=slack&amp;amp;style=social" alt="Slack Status" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/github/commit-activity/m/apache/airflow" alt="Commit Activity" /&gt; &lt;a href="https://insights.linuxfoundation.org/project/apache-airflow"&gt;&lt;img src="https://insights.linuxfoundation.org/api/badge/health-score?project=apache-airflow" alt="LFX Health Score" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Version&lt;/th&gt; 
   &lt;th&gt;Build Status&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Main&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/apache/airflow/actions"&gt;&lt;img src="https://github.com/apache/airflow/actions/workflows/ci-amd-arm.yml/badge.svg?sanitize=true" alt="GitHub Build main" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;3.x&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/apache/airflow/actions"&gt;&lt;img src="https://github.com/apache/airflow/actions/workflows/ci-amd-arm.yml/badge.svg?branch=v3-1-test" alt="GitHub Build 3.1" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;2.x&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/apache/airflow/actions"&gt;&lt;img src="https://github.com/apache/airflow/actions/workflows/ci.yml/badge.svg?branch=v2-11-test" alt="GitHub Build 2.11" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;picture width="500"&gt; 
 &lt;img src="https://github.com/apache/airflow/raw/19ebcac2395ef9a6b6ded3a2faa29dc960c1e635/docs/apache-airflow/img/logos/wordmark_1.png?raw=true" alt="Apache Airflow logo" /&gt; 
&lt;/picture&gt; 
&lt;p&gt;&lt;a href="https://airflow.apache.org/docs/apache-airflow/stable/"&gt;Apache Airflow&lt;/a&gt; (or simply Airflow) is a platform to programmatically author, schedule, and monitor workflows.&lt;/p&gt; 
&lt;p&gt;When workflows are defined as code, they become more maintainable, versionable, testable, and collaborative.&lt;/p&gt; 
&lt;p&gt;Use Airflow to author workflows (Dags) that orchestrate tasks. The Airflow scheduler executes your tasks on an array of workers while following the specified dependencies. Rich command line utilities make performing complex surgeries on DAGs a snap. The rich user interface makes it easy to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed.&lt;/p&gt; 
&lt;!-- END Apache Airflow, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;!-- START doctoc generated TOC please keep comment here to allow auto update --&gt; 
&lt;!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE --&gt; 
&lt;p&gt;&lt;strong&gt;Table of contents&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#project-focus"&gt;Project Focus&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#principles"&gt;Principles&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#requirements"&gt;Requirements&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#getting-started"&gt;Getting started&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#installing-from-pypi"&gt;Installing from PyPI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#official-source-code"&gt;Official source code&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#convenience-packages"&gt;Convenience packages&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#user-interface"&gt;User Interface&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#semantic-versioning"&gt;Semantic versioning&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#version-life-cycle"&gt;Version Life Cycle&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#support-for-python-and-kubernetes-versions"&gt;Support for Python and Kubernetes versions&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#base-os-support-for-reference-airflow-images"&gt;Base OS support for reference Airflow images&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#approach-to-dependencies-of-airflow"&gt;Approach to dependencies of Airflow&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#voting-policy"&gt;Voting Policy&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#who-uses-apache-airflow"&gt;Who uses Apache Airflow?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#who-maintains-apache-airflow"&gt;Who maintains Apache Airflow?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#what-goes-into-the-next-release"&gt;What goes into the next release?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#can-i-use-the-apache-airflow-logo-in-my-presentation"&gt;Can I use the Apache Airflow logo in my presentation?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#links"&gt;Links&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#sponsors"&gt;Sponsors&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- END doctoc generated TOC please keep comment here to allow auto update --&gt; 
&lt;h2&gt;Project Focus&lt;/h2&gt; 
&lt;p&gt;Airflow works best with workflows that are mostly static and slowly changing. When the DAG structure is similar from one run to the next, it clarifies the unit of work and continuity. Other similar projects include &lt;a href="https://github.com/spotify/luigi"&gt;Luigi&lt;/a&gt;, &lt;a href="https://oozie.apache.org/"&gt;Oozie&lt;/a&gt; and &lt;a href="https://azkaban.github.io/"&gt;Azkaban&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Airflow is commonly used to process data, but has the opinion that tasks should ideally be idempotent (i.e., results of the task will be the same, and will not create duplicated data in a destination system), and should not pass large quantities of data from one task to the next (though tasks can pass metadata using Airflow's &lt;a href="https://airflow.apache.org/docs/apache-airflow/stable/concepts/xcoms.html"&gt;XCom feature&lt;/a&gt;). For high-volume, data-intensive tasks, a best practice is to delegate to external services specializing in that type of work.&lt;/p&gt; 
&lt;p&gt;Airflow is not a streaming solution, but it is often used to process real-time data, pulling data off streams in batches.&lt;/p&gt; 
&lt;h2&gt;Principles&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Dynamic&lt;/strong&gt;: Pipelines are defined in code, enabling dynamic dag generation and parameterization.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Extensible&lt;/strong&gt;: The Airflow framework includes a wide range of built-in operators and can be extended to fit your needs.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Flexible&lt;/strong&gt;: Airflow leverages the &lt;a href="https://jinja.palletsprojects.com"&gt;&lt;strong&gt;Jinja&lt;/strong&gt;&lt;/a&gt; templating engine, allowing rich customizations.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- START Requirements, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;p&gt;Apache Airflow is tested with:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th&gt;Main version (dev)&lt;/th&gt; 
   &lt;th&gt;Stable version (3.1.2)&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Python&lt;/td&gt; 
   &lt;td&gt;3.10, 3.11, 3.12, 3.13&lt;/td&gt; 
   &lt;td&gt;3.10, 3.11, 3.12, 3.13&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Platform&lt;/td&gt; 
   &lt;td&gt;AMD64/ARM64(*)&lt;/td&gt; 
   &lt;td&gt;AMD64/ARM64(*)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Kubernetes&lt;/td&gt; 
   &lt;td&gt;1.30, 1.31, 1.32, 1.33, 1.34&lt;/td&gt; 
   &lt;td&gt;1.30, 1.31, 1.32, 1.33&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;PostgreSQL&lt;/td&gt; 
   &lt;td&gt;14, 15, 16, 17, 18&lt;/td&gt; 
   &lt;td&gt;13, 14, 15, 16, 17&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MySQL&lt;/td&gt; 
   &lt;td&gt;8.0, 8.4, Innovation&lt;/td&gt; 
   &lt;td&gt;8.0, 8.4, Innovation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;SQLite&lt;/td&gt; 
   &lt;td&gt;3.15.0+&lt;/td&gt; 
   &lt;td&gt;3.15.0+&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;* Experimental&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: MariaDB is not tested/recommended.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: SQLite is used in Airflow tests. Do not use it in production. We recommend using the latest stable version of SQLite for local development.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Airflow currently can be run on POSIX-compliant Operating Systems. For development, it is regularly tested on fairly modern Linux Distros and recent versions of macOS. On Windows you can run it via WSL2 (Windows Subsystem for Linux 2) or via Linux Containers. The work to add Windows support is tracked via &lt;a href="https://github.com/apache/airflow/issues/10388"&gt;#10388&lt;/a&gt;, but it is not a high priority. You should only use Linux-based distros as "Production" execution environment as this is the only environment that is supported. The only distro that is used in our CI tests and that is used in the &lt;a href="https://hub.docker.com/p/apache/airflow"&gt;Community managed DockerHub image&lt;/a&gt; is &lt;code&gt;Debian Bookworm&lt;/code&gt;.&lt;/p&gt; 
&lt;!-- END Requirements, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;!-- START Getting started, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;h2&gt;Getting started&lt;/h2&gt; 
&lt;p&gt;Visit the official Airflow website documentation (latest &lt;strong&gt;stable&lt;/strong&gt; release) for help with &lt;a href="https://airflow.apache.org/docs/apache-airflow/stable/installation/"&gt;installing Airflow&lt;/a&gt;, &lt;a href="https://airflow.apache.org/docs/apache-airflow/stable/start.html"&gt;getting started&lt;/a&gt;, or walking through a more complete &lt;a href="https://airflow.apache.org/docs/apache-airflow/stable/tutorial/"&gt;tutorial&lt;/a&gt;.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Note: If you're looking for documentation for the main branch (latest development branch): you can find it on &lt;a href="https://s.apache.org/airflow-docs/"&gt;s.apache.org/airflow-docs&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;For more information on Airflow Improvement Proposals (AIPs), visit the &lt;a href="https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals"&gt;Airflow Wiki&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Documentation for dependent projects like provider distributions, Docker image, Helm Chart, you'll find it in &lt;a href="https://airflow.apache.org/docs/"&gt;the documentation index&lt;/a&gt;.&lt;/p&gt; 
&lt;!-- END Getting started, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;!-- START Installing from PyPI, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;h2&gt;Installing from PyPI&lt;/h2&gt; 
&lt;p&gt;We publish Apache Airflow as &lt;code&gt;apache-airflow&lt;/code&gt; package in PyPI. Installing it however might be sometimes tricky because Airflow is a bit of both a library and application. Libraries usually keep their dependencies open, and applications usually pin them, but we should do neither and both simultaneously. We decided to keep our dependencies as open as possible (in &lt;code&gt;pyproject.toml&lt;/code&gt;) so users can install different versions of libraries if needed. This means that &lt;code&gt;pip install apache-airflow&lt;/code&gt; will not work from time to time or will produce unusable Airflow installation.&lt;/p&gt; 
&lt;p&gt;To have repeatable installation, however, we keep a set of "known-to-be-working" constraint files in the orphan &lt;code&gt;constraints-main&lt;/code&gt; and &lt;code&gt;constraints-2-0&lt;/code&gt; branches. We keep those "known-to-be-working" constraints files separately per major/minor Python version. You can use them as constraint files when installing Airflow from PyPI. Note that you have to specify correct Airflow tag/version/branch and Python versions in the URL.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Installing just Airflow:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Note: Only &lt;code&gt;pip&lt;/code&gt; installation is currently officially supported.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;While it is possible to install Airflow with tools like &lt;a href="https://python-poetry.org"&gt;Poetry&lt;/a&gt; or &lt;a href="https://pypi.org/project/pip-tools"&gt;pip-tools&lt;/a&gt;, they do not share the same workflow as &lt;code&gt;pip&lt;/code&gt; - especially when it comes to constraint vs. requirements management. Installing via &lt;code&gt;Poetry&lt;/code&gt; or &lt;code&gt;pip-tools&lt;/code&gt; is not currently supported.&lt;/p&gt; 
&lt;p&gt;If you wish to install Airflow using those tools, you should use the constraint files and convert them to the appropriate format and workflow that your tool requires.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install 'apache-airflow==3.1.2' \
 --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-3.1.2/constraints-3.10.txt"
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Installing with extras (i.e., postgres, google)&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install 'apache-airflow[postgres,google]==3.1.2' \
 --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-3.1.2/constraints-3.10.txt"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For information on installing provider distributions, check &lt;a href="http://airflow.apache.org/docs/apache-airflow-providers/index.html"&gt;providers&lt;/a&gt;.&lt;/p&gt; 
&lt;!-- END Installing from PyPI, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;For comprehensive instructions on setting up your local development environment and installing Apache Airflow, please refer to the &lt;a href="https://raw.githubusercontent.com/apache/airflow/main/INSTALLING.md"&gt;INSTALLING.md&lt;/a&gt; file.&lt;/p&gt; 
&lt;!-- START Official source code, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;h2&gt;Official source code&lt;/h2&gt; 
&lt;p&gt;Apache Airflow is an &lt;a href="https://www.apache.org"&gt;Apache Software Foundation&lt;/a&gt; (ASF) project, and our official source code releases:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Follow the &lt;a href="https://www.apache.org/legal/release-policy.html"&gt;ASF Release Policy&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Can be downloaded from &lt;a href="https://downloads.apache.org/airflow"&gt;the ASF Distribution Directory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Are cryptographically signed by the release manager&lt;/li&gt; 
 &lt;li&gt;Are officially voted on by the PMC members during the &lt;a href="https://www.apache.org/legal/release-policy.html#release-approval"&gt;Release Approval Process&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Following the ASF rules, the source packages released must be sufficient for a user to build and test the release provided they have access to the appropriate platform and tools.&lt;/p&gt; 
&lt;!-- END Official source code, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;h2&gt;Convenience packages&lt;/h2&gt; 
&lt;p&gt;There are other ways of installing and using Airflow. Those are "convenience" methods - they are not "official releases" as stated by the &lt;code&gt;ASF Release Policy&lt;/code&gt;, but they can be used by the users who do not want to build the software themselves.&lt;/p&gt; 
&lt;p&gt;Those are - in the order of most common ways people install Airflow:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://pypi.org/project/apache-airflow/"&gt;PyPI releases&lt;/a&gt; to install Airflow using standard &lt;code&gt;pip&lt;/code&gt; tool&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://hub.docker.com/r/apache/airflow"&gt;Docker Images&lt;/a&gt; to install airflow via &lt;code&gt;docker&lt;/code&gt; tool, use them in Kubernetes, Helm Charts, &lt;code&gt;docker-compose&lt;/code&gt;, &lt;code&gt;docker swarm&lt;/code&gt;, etc. You can read more about using, customizing, and extending the images in the &lt;a href="https://airflow.apache.org/docs/docker-stack/index.html"&gt;Latest docs&lt;/a&gt;, and learn details on the internals in the &lt;a href="https://airflow.apache.org/docs/docker-stack/index.html"&gt;images&lt;/a&gt; document.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/apache/airflow/tags"&gt;Tags in GitHub&lt;/a&gt; to retrieve the git project sources that were used to generate official source packages via git&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;All those artifacts are not official releases, but they are prepared using officially released sources. Some of those artifacts are "development" or "pre-release" ones, and they are clearly marked as such following the ASF Policy.&lt;/p&gt; 
&lt;h2&gt;User Interface&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;DAGs&lt;/strong&gt;: Overview of all DAGs in your environment.&lt;/p&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/ui-dark/dags.png" alt="DAGs" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Assets&lt;/strong&gt;: Overview of Assets with dependencies.&lt;/p&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/ui-dark/assets_graph.png" alt="Asset Dependencies" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Grid&lt;/strong&gt;: Grid representation of a DAG that spans across time.&lt;/p&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/ui-dark/grid.png" alt="Grid" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Graph&lt;/strong&gt;: Visualization of a DAG's dependencies and their current status for a specific run.&lt;/p&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/ui-dark/graph.png" alt="Graph" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Home&lt;/strong&gt;: Summary statistics of your Airflow environment.&lt;/p&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/ui-dark/home.png" alt="Home" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Backfill&lt;/strong&gt;: Backfilling a DAG for a specific date range.&lt;/p&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/ui-dark/backfill.png" alt="Backfill" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Code&lt;/strong&gt;: Quick way to view source code of a DAG.&lt;/p&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/ui-dark/code.png" alt="Code" /&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Semantic versioning&lt;/h2&gt; 
&lt;p&gt;As of Airflow 2.0.0, we support a strict &lt;a href="https://semver.org/"&gt;SemVer&lt;/a&gt; approach for all packages released.&lt;/p&gt; 
&lt;p&gt;There are few specific rules that we agreed to that define details of versioning of the different packages:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Airflow&lt;/strong&gt;: SemVer rules apply to core airflow only (excludes any changes to providers). Changing limits for versions of Airflow dependencies is not a breaking change on its own.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Airflow Providers&lt;/strong&gt;: SemVer rules apply to changes in the particular provider's code only. SemVer MAJOR and MINOR versions for the packages are independent of the Airflow version. For example, &lt;code&gt;google 4.1.0&lt;/code&gt; and &lt;code&gt;amazon 3.1.1&lt;/code&gt; providers can happily be installed with &lt;code&gt;Airflow 2.1.2&lt;/code&gt;. If there are limits of cross-dependencies between providers and Airflow packages, they are present in providers as &lt;code&gt;install_requires&lt;/code&gt; limitations. We aim to keep backwards compatibility of providers with all previously released Airflow 2 versions but there will sometimes be breaking changes that might make some, or all providers, have minimum Airflow version specified.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Airflow Helm Chart&lt;/strong&gt;: SemVer rules apply to changes in the chart only. SemVer MAJOR and MINOR versions for the chart are independent of the Airflow version. We aim to keep backwards compatibility of the Helm Chart with all released Airflow 2 versions, but some new features might only work starting from specific Airflow releases. We might however limit the Helm Chart to depend on minimal Airflow version.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Airflow API clients&lt;/strong&gt;: Their versioning is independent from Airflow versions. They follow their own SemVer rules for breaking changes and new features - which for example allows to change the way we generate the clients.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Version Life Cycle&lt;/h2&gt; 
&lt;p&gt;Apache Airflow version life cycle:&lt;/p&gt; 
&lt;!-- This table is automatically updated by prek scripts/ci/prek/supported_versions.py --&gt; 
&lt;!-- Beginning of auto-generated table --&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Version&lt;/th&gt; 
   &lt;th&gt;Current Patch/Minor&lt;/th&gt; 
   &lt;th&gt;State&lt;/th&gt; 
   &lt;th&gt;First Release&lt;/th&gt; 
   &lt;th&gt;Limited Maintenance&lt;/th&gt; 
   &lt;th&gt;EOL/Terminated&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;3&lt;/td&gt; 
   &lt;td&gt;3.1.2&lt;/td&gt; 
   &lt;td&gt;Supported&lt;/td&gt; 
   &lt;td&gt;Apr 22, 2025&lt;/td&gt; 
   &lt;td&gt;TBD&lt;/td&gt; 
   &lt;td&gt;TBD&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;2.11.0&lt;/td&gt; 
   &lt;td&gt;Supported&lt;/td&gt; 
   &lt;td&gt;Dec 17, 2020&lt;/td&gt; 
   &lt;td&gt;Oct 22, 2025&lt;/td&gt; 
   &lt;td&gt;Apr 22, 2026&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;1.10&lt;/td&gt; 
   &lt;td&gt;1.10.15&lt;/td&gt; 
   &lt;td&gt;EOL&lt;/td&gt; 
   &lt;td&gt;Aug 27, 2018&lt;/td&gt; 
   &lt;td&gt;Dec 17, 2020&lt;/td&gt; 
   &lt;td&gt;June 17, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;1.9&lt;/td&gt; 
   &lt;td&gt;1.9.0&lt;/td&gt; 
   &lt;td&gt;EOL&lt;/td&gt; 
   &lt;td&gt;Jan 03, 2018&lt;/td&gt; 
   &lt;td&gt;Aug 27, 2018&lt;/td&gt; 
   &lt;td&gt;Aug 27, 2018&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;1.8&lt;/td&gt; 
   &lt;td&gt;1.8.2&lt;/td&gt; 
   &lt;td&gt;EOL&lt;/td&gt; 
   &lt;td&gt;Mar 19, 2017&lt;/td&gt; 
   &lt;td&gt;Jan 03, 2018&lt;/td&gt; 
   &lt;td&gt;Jan 03, 2018&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;1.7&lt;/td&gt; 
   &lt;td&gt;1.7.1.2&lt;/td&gt; 
   &lt;td&gt;EOL&lt;/td&gt; 
   &lt;td&gt;Mar 28, 2016&lt;/td&gt; 
   &lt;td&gt;Mar 19, 2017&lt;/td&gt; 
   &lt;td&gt;Mar 19, 2017&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;!-- End of auto-generated table --&gt; 
&lt;p&gt;Limited support versions will be supported with security and critical bug fix only. EOL versions will not get any fixes nor support. We always recommend that all users run the latest available minor release for whatever major version is in use. We &lt;strong&gt;highly&lt;/strong&gt; recommend upgrading to the latest Airflow major release at the earliest convenient time and before the EOL date.&lt;/p&gt; 
&lt;h2&gt;Support for Python and Kubernetes versions&lt;/h2&gt; 
&lt;p&gt;As of Airflow 2.0, we agreed to certain rules we follow for Python and Kubernetes support. They are based on the official release schedule of Python and Kubernetes, nicely summarized in the &lt;a href="https://devguide.python.org/#status-of-python-branches"&gt;Python Developer's Guide&lt;/a&gt; and &lt;a href="https://kubernetes.io/docs/setup/release/version-skew-policy/"&gt;Kubernetes version skew policy&lt;/a&gt;.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;We drop support for Python and Kubernetes versions when they reach EOL. Except for Kubernetes, a version stays supported by Airflow if two major cloud providers still provide support for it. We drop support for those EOL versions in main right after EOL date, and it is effectively removed when we release the first new MINOR (Or MAJOR if there is no new MINOR version) of Airflow. For example, for Python 3.10 it means that we will drop support in main right after 27.06.2023, and the first MAJOR or MINOR version of Airflow released after will not have it.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;We support a new version of Python/Kubernetes in main after they are officially released, as soon as we make them work in our CI pipeline (which might not be immediate due to dependencies catching up with new versions of Python mostly) we release new images/support in Airflow based on the working CI setup.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;This policy is best-effort which means there may be situations where we might terminate support earlier if circumstances require it.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Base OS support for reference Airflow images&lt;/h2&gt; 
&lt;p&gt;The Airflow Community provides conveniently packaged container images that are published whenever we publish an Apache Airflow release. Those images contain:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Base OS with necessary packages to install Airflow (stable Debian OS)&lt;/li&gt; 
 &lt;li&gt;Base Python installation in versions supported at the time of release for the MINOR version of Airflow released (so there could be different versions for 2.3 and 2.2 line for example)&lt;/li&gt; 
 &lt;li&gt;Libraries required to connect to supported Databases (again the set of databases supported depends on the MINOR version of Airflow)&lt;/li&gt; 
 &lt;li&gt;Predefined set of popular providers (for details see the &lt;a href="https://raw.githubusercontent.com/apache/airflow/main/Dockerfile"&gt;Dockerfile&lt;/a&gt;).&lt;/li&gt; 
 &lt;li&gt;Possibility of building your own, custom image where the user can choose their own set of providers and libraries (see &lt;a href="https://airflow.apache.org/docs/docker-stack/build.html"&gt;Building the image&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;In the future Airflow might also support a "slim" version without providers nor database clients installed&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The version of the base OS image is the stable version of Debian. Airflow supports using all currently active stable versions - as soon as all Airflow dependencies support building, and we set up the CI pipeline for building and testing the OS version. Approximately 6 months before the end-of-regular support of a previous stable version of the OS, Airflow switches the images released to use the latest supported version of the OS.&lt;/p&gt; 
&lt;p&gt;For example switch from &lt;code&gt;Debian Bullseye&lt;/code&gt; to &lt;code&gt;Debian Bookworm&lt;/code&gt; has been implemented before 2.8.0 release in October 2023 and &lt;code&gt;Debian Bookworm&lt;/code&gt; will be the only option supported as of Airflow 2.10.0.&lt;/p&gt; 
&lt;p&gt;Users will continue to be able to build their images using stable Debian releases until the end of regular support and building and verifying of the images happens in our CI but no unit tests were executed using this image in the &lt;code&gt;main&lt;/code&gt; branch.&lt;/p&gt; 
&lt;h2&gt;Approach to dependencies of Airflow&lt;/h2&gt; 
&lt;p&gt;Airflow has a lot of dependencies - direct and transitive, also Airflow is both - library and application, therefore our policies to dependencies has to include both - stability of installation of application, but also ability to install newer version of dependencies for those users who develop DAGs. We developed the approach where &lt;code&gt;constraints&lt;/code&gt; are used to make sure airflow can be installed in a repeatable way, while we do not limit our users to upgrade most of the dependencies. As a result we decided not to upper-bound version of Airflow dependencies by default, unless we have good reasons to believe upper-bounding them is needed because of importance of the dependency as well as risk it involves to upgrade specific dependency. We also upper-bound the dependencies that we know cause problems.&lt;/p&gt; 
&lt;p&gt;The constraint mechanism of ours takes care about finding and upgrading all the non-upper bound dependencies automatically (providing that all the tests pass). Our &lt;code&gt;main&lt;/code&gt; build failures will indicate in case there are versions of dependencies that break our tests - indicating that we should either upper-bind them or that we should fix our code/tests to account for the upstream changes from those dependencies.&lt;/p&gt; 
&lt;p&gt;Whenever we upper-bound such a dependency, we should always comment why we are doing it - i.e. we should have a good reason why dependency is upper-bound. And we should also mention what is the condition to remove the binding.&lt;/p&gt; 
&lt;h3&gt;Approach for dependencies for Airflow Core&lt;/h3&gt; 
&lt;p&gt;Those dependencies are maintained in &lt;code&gt;pyproject.toml&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;There are few dependencies that we decided are important enough to upper-bound them by default, as they are known to follow predictable versioning scheme, and we know that new versions of those are very likely to bring breaking changes. We commit to regularly review and attempt to upgrade to the newer versions of the dependencies as they are released, but this is manual process.&lt;/p&gt; 
&lt;p&gt;The important dependencies are:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;SQLAlchemy&lt;/code&gt;: upper-bound to specific MINOR version (SQLAlchemy is known to remove deprecations and introduce breaking changes especially that support for different Databases varies and changes at various speed)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Alembic&lt;/code&gt;: it is important to handle our migrations in predictable and performant way. It is developed together with SQLAlchemy. Our experience with Alembic is that it very stable in MINOR version&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Flask&lt;/code&gt;: We are using Flask as the back-bone of our web UI and API. We know major version of Flask are very likely to introduce breaking changes across those so limiting it to MAJOR version makes sense&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;werkzeug&lt;/code&gt;: the library is known to cause problems in new versions. It is tightly coupled with Flask libraries, and we should update them together&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;celery&lt;/code&gt;: Celery is a crucial component of Airflow as it used for CeleryExecutor (and similar). Celery &lt;a href="https://docs.celeryq.dev/en/stable/contributing.html?highlight=semver#versions"&gt;follows SemVer&lt;/a&gt;, so we should upper-bound it to the next MAJOR version. Also, when we bump the upper version of the library, we should make sure Celery Provider minimum Airflow version is updated.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;kubernetes&lt;/code&gt;: Kubernetes is a crucial component of Airflow as it is used for the KubernetesExecutor (and similar). Kubernetes Python library &lt;a href="https://github.com/kubernetes-client/python#compatibility"&gt;follows SemVer&lt;/a&gt;, so we should upper-bound it to the next MAJOR version. Also, when we bump the upper version of the library, we should make sure Kubernetes Provider minimum Airflow version is updated.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Approach for dependencies in Airflow Providers and extras&lt;/h3&gt; 
&lt;p&gt;The main part of the Airflow is the Airflow Core, but the power of Airflow also comes from a number of providers that extend the core functionality and are released separately, even if we keep them (for now) in the same monorepo for convenience. You can read more about the providers in the &lt;a href="https://airflow.apache.org/docs/apache-airflow-providers/index.html"&gt;Providers documentation&lt;/a&gt;. We also have set of policies implemented for maintaining and releasing community-managed providers as well as the approach for community vs. 3rd party providers in the &lt;a href="https://github.com/apache/airflow/raw/main/PROVIDERS.rst"&gt;providers&lt;/a&gt; document.&lt;/p&gt; 
&lt;p&gt;Those &lt;code&gt;extras&lt;/code&gt; and &lt;code&gt;providers&lt;/code&gt; dependencies are maintained in &lt;code&gt;provider.yaml&lt;/code&gt; of each provider.&lt;/p&gt; 
&lt;p&gt;By default, we should not upper-bound dependencies for providers, however each provider's maintainer might decide to add additional limits (and justify them with comment).&lt;/p&gt; 
&lt;!-- START Contributing, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Want to help build Apache Airflow? Check out our &lt;a href="https://github.com/apache/airflow/raw/main/contributing-docs/README.rst"&gt;contributors' guide&lt;/a&gt; for a comprehensive overview of how to contribute, including setup instructions, coding standards, and pull request guidelines.&lt;/p&gt; 
&lt;p&gt;If you can't wait to contribute, and want to get started asap, check out the &lt;a href="https://github.com/apache/airflow/raw/main/contributing-docs/03a_contributors_quick_start_beginners.rst"&gt;contribution quickstart&lt;/a&gt; here!&lt;/p&gt; 
&lt;p&gt;Official Docker (container) images for Apache Airflow are described in &lt;a href="https://github.com/apache/airflow/raw/main/dev/breeze/doc/ci/02_images.md"&gt;images&lt;/a&gt;.&lt;/p&gt; 
&lt;!-- END Contributing, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;!-- START Who uses Apache Airflow, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;h2&gt;Voting Policy&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Commits need a +1 vote from a committer who is not the author&lt;/li&gt; 
 &lt;li&gt;When we do AIP voting, both PMC member's and committer's &lt;code&gt;+1s&lt;/code&gt; are considered a binding vote.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Who uses Apache Airflow?&lt;/h2&gt; 
&lt;p&gt;We know about around 500 organizations that are using Apache Airflow (but there are likely many more) &lt;a href="https://github.com/apache/airflow/raw/main/INTHEWILD.md"&gt;in the wild&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you use Airflow - feel free to make a PR to add your organisation to the list.&lt;/p&gt; 
&lt;!-- END Who uses Apache Airflow, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;!-- START Who maintains Apache Airflow, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;h2&gt;Who maintains Apache Airflow?&lt;/h2&gt; 
&lt;p&gt;Airflow is the work of the &lt;a href="https://github.com/apache/airflow/graphs/contributors"&gt;community&lt;/a&gt;, but the &lt;a href="https://people.apache.org/committers-by-project.html#airflow"&gt;core committers/maintainers&lt;/a&gt; are responsible for reviewing and merging PRs as well as steering conversations around new feature requests. If you would like to become a maintainer, please review the Apache Airflow &lt;a href="https://github.com/apache/airflow/raw/main/COMMITTERS.rst#guidelines-to-become-an-airflow-committer"&gt;committer requirements&lt;/a&gt;.&lt;/p&gt; 
&lt;!-- END Who maintains Apache Airflow, please keep comment here to allow auto update of PyPI readme.md --&gt; 
&lt;h2&gt;What goes into the next release?&lt;/h2&gt; 
&lt;p&gt;Often you will see an issue that is assigned to specific milestone with Airflow version, or a PR that gets merged to the main branch and you might wonder which release the merged PR(s) will be released in or which release the fixed issues will be in. The answer to this is as usual - it depends on various scenarios. The answer is different for PRs and Issues.&lt;/p&gt; 
&lt;p&gt;To add a bit of context, we are following the &lt;a href="https://semver.org/"&gt;Semver&lt;/a&gt; versioning scheme as described in &lt;a href="https://airflow.apache.org/docs/apache-airflow/stable/release-process.html"&gt;Airflow release process&lt;/a&gt;. More details are explained in detail in this README under the &lt;a href="https://raw.githubusercontent.com/apache/airflow/main/#semantic-versioning"&gt;Semantic versioning&lt;/a&gt; chapter, but in short, we have &lt;code&gt;MAJOR.MINOR.PATCH&lt;/code&gt; versions of Airflow.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;MAJOR&lt;/code&gt; version is incremented in case of breaking changes&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;MINOR&lt;/code&gt; version is incremented when there are new features added&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;PATCH&lt;/code&gt; version is incremented when there are only bug-fixes and doc-only changes&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Generally we release &lt;code&gt;MINOR&lt;/code&gt; versions of Airflow from a branch that is named after the MINOR version. For example &lt;code&gt;2.7.*&lt;/code&gt; releases are released from &lt;code&gt;v2-7-stable&lt;/code&gt; branch, &lt;code&gt;2.8.*&lt;/code&gt; releases are released from &lt;code&gt;v2-8-stable&lt;/code&gt; branch, etc.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Most of the time in our release cycle, when the branch for next &lt;code&gt;MINOR&lt;/code&gt; branch is not yet created, all PRs merged to &lt;code&gt;main&lt;/code&gt; (unless they get reverted), will find their way to the next &lt;code&gt;MINOR&lt;/code&gt; release. For example if the last release is &lt;code&gt;2.7.3&lt;/code&gt; and &lt;code&gt;v2-8-stable&lt;/code&gt; branch is not created yet, the next &lt;code&gt;MINOR&lt;/code&gt; release is &lt;code&gt;2.8.0&lt;/code&gt; and all PRs merged to main will be released in &lt;code&gt;2.8.0&lt;/code&gt;. However, some PRs (bug-fixes and doc-only changes) when merged, can be cherry-picked to current &lt;code&gt;MINOR&lt;/code&gt; branch and released in the next &lt;code&gt;PATCHLEVEL&lt;/code&gt; release. For example, if &lt;code&gt;2.8.1&lt;/code&gt; is already released and we are working on &lt;code&gt;2.9.0dev&lt;/code&gt;, then marking a PR with &lt;code&gt;2.8.2&lt;/code&gt; milestone means that it will be cherry-picked to &lt;code&gt;v2-8-test&lt;/code&gt; branch and released in &lt;code&gt;2.8.2rc1&lt;/code&gt;, and eventually in &lt;code&gt;2.8.2&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;When we prepare for the next &lt;code&gt;MINOR&lt;/code&gt; release, we cut new &lt;code&gt;v2-*-test&lt;/code&gt; and &lt;code&gt;v2-*-stable&lt;/code&gt; branch and prepare &lt;code&gt;alpha&lt;/code&gt;, &lt;code&gt;beta&lt;/code&gt; releases for the next &lt;code&gt;MINOR&lt;/code&gt; version, the PRs merged to main will still be released in the next &lt;code&gt;MINOR&lt;/code&gt; release until &lt;code&gt;rc&lt;/code&gt; version is cut. This is happening because the &lt;code&gt;v2-*-test&lt;/code&gt; and &lt;code&gt;v2-*-stable&lt;/code&gt; branches are rebased on top of main when next &lt;code&gt;beta&lt;/code&gt; and &lt;code&gt;rc&lt;/code&gt; releases are prepared. For example, when we cut &lt;code&gt;2.10.0beta1&lt;/code&gt; version, anything merged to main before &lt;code&gt;2.10.0rc1&lt;/code&gt; is released, will find its way to 2.10.0rc1.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Then, once we prepare the first RC candidate for the MINOR release, we stop moving the &lt;code&gt;v2-*-test&lt;/code&gt; and &lt;code&gt;v2-*-stable&lt;/code&gt; branches and the PRs merged to main will be released in the next &lt;code&gt;MINOR&lt;/code&gt; release. However, some PRs (bug-fixes and doc-only changes) when merged, can be cherry-picked to current &lt;code&gt;MINOR&lt;/code&gt; branch and released in the next &lt;code&gt;PATCHLEVEL&lt;/code&gt; release - for example when the last released version from &lt;code&gt;v2-10-stable&lt;/code&gt; branch is &lt;code&gt;2.10.0rc1&lt;/code&gt;, some of the PRs from main can be marked as &lt;code&gt;2.10.0&lt;/code&gt; milestone by committers, the release manager will try to cherry-pick them into the release branch. If successful, they will be released in &lt;code&gt;2.10.0rc2&lt;/code&gt; and subsequently in &lt;code&gt;2.10.0&lt;/code&gt;. This also applies to subsequent &lt;code&gt;PATCHLEVEL&lt;/code&gt; versions. When for example &lt;code&gt;2.10.1&lt;/code&gt; is already released, marking a PR with &lt;code&gt;2.10.2&lt;/code&gt; milestone will mean that it will be cherry-picked to &lt;code&gt;v2-10-stable&lt;/code&gt; branch and released in &lt;code&gt;2.10.2rc1&lt;/code&gt; and eventually in &lt;code&gt;2.10.2&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;The final decision about cherry-picking is made by the release manager.&lt;/p&gt; 
&lt;p&gt;Marking issues with a milestone is a bit different. Maintainers do not mark issues with a milestone usually, normally they are only marked in PRs. If PR linked to the issue (and "fixing it") gets merged and released in a specific version following the process described above, the issue will be automatically closed, no milestone will be set for the issue, you need to check the PR that fixed the issue to see which version it was released in.&lt;/p&gt; 
&lt;p&gt;However, sometimes maintainers mark issues with specific milestone, which means that the issue is important to become a candidate to take a look when the release is being prepared. Since this is an Open-Source project, where basically all contributors volunteer their time, there is no guarantee that specific issue will be fixed in specific version. We do not want to hold the release because some issue is not fixed, so in such case release manager will reassign such unfixed issues to the next milestone in case they are not fixed in time for the current release. Therefore, the milestone for issue is more of an intent that it should be looked at, than promise it will be fixed in the version.&lt;/p&gt; 
&lt;p&gt;More context and &lt;strong&gt;FAQ&lt;/strong&gt; about the patchlevel release can be found in the &lt;a href="https://raw.githubusercontent.com/apache/airflow/main/dev/WHAT_GOES_INTO_THE_NEXT_RELEASE.md"&gt;What goes into the next release&lt;/a&gt; document in the &lt;code&gt;dev&lt;/code&gt; folder of the repository.&lt;/p&gt; 
&lt;h2&gt;Can I use the Apache Airflow logo in my presentation?&lt;/h2&gt; 
&lt;p&gt;Yes! Be sure to abide by the Apache Foundation &lt;a href="https://www.apache.org/foundation/marks/#books"&gt;trademark policies&lt;/a&gt; and the Apache Airflow &lt;a href="https://cwiki.apache.org/confluence/display/AIRFLOW/Brandbook"&gt;Brandbook&lt;/a&gt;. The most up-to-date logos are found in &lt;a href="https://github.com/apache/airflow/tree/main/airflow-core/docs/img/logos/"&gt;this repo&lt;/a&gt; and on the Apache Software Foundation &lt;a href="https://www.apache.org/logos/about.html"&gt;website&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Links&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://airflow.apache.org/docs/apache-airflow/stable/"&gt;Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://s.apache.org/airflow-slack"&gt;Chat&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://airflow.apache.org/community/"&gt;Community Information&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Sponsors&lt;/h2&gt; 
&lt;p&gt;The CI infrastructure for Apache Airflow has been sponsored by:&lt;/p&gt; 
&lt;!-- Ordered by most recently "funded" --&gt; 
&lt;p&gt;&lt;a href="https://astronomer.io"&gt;&lt;img src="https://assets2.astronomer.io/logos/logoForLIGHTbackground.png" alt="astronomer.io" width="250px" /&gt;&lt;/a&gt; &lt;a href="https://aws.amazon.com/opensource/"&gt;&lt;img src="https://github.com/apache/airflow/raw/main/providers/amazon/docs/integration-logos/AWS-Cloud-alt_light-bg@4x.png?raw=true" alt="AWS OpenSource" width="130px" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>qodo-ai/pr-agent</title>
      <link>https://github.com/qodo-ai/pr-agent</link>
      <description>&lt;p&gt;ğŸš€ PR-Agent: An AI-Powered ğŸ¤– Tool for Automated Pull Request Analysis, Feedback, Suggestions and More! ğŸ’»ğŸ”&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ğŸ§  PR Agent LEGACY STATUS (open source)&lt;/h1&gt; 
&lt;p&gt;Originally created and open-sourced by Qodo - the team behind next-generation AI Code Review.&lt;/p&gt; 
&lt;h2&gt;ğŸš€ About&lt;/h2&gt; 
&lt;p&gt;PR Agent was the first AI assistant for pull requests, built by Qodo, and contributed to the open-source community.&lt;br /&gt; It represents the first generation of intelligent code review - the project that started Qodoâ€™s journey toward fully AI-driven development, Code Review.&lt;br /&gt; If you enjoy this project, youâ€™ll love the next-level PR Agent - Qodo free tier version, which is faster, smarter, and built for todayâ€™s workflows.&lt;/p&gt; 
&lt;p&gt;ğŸš€ Qodo includes a free user trial, 250 tokens, bonus tokens for active contributors, and 50% more advanced features than this open-source version.&lt;/p&gt; 
&lt;p&gt;If you have an open-source project, you can get the Qodo paid version for free for your project, powered by Google Gemini 2.5 Pro â€“ &lt;a href="https://www.qodo.ai/solutions/open-source/"&gt;https://www.qodo.ai/solutions/open-source/&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;âœ¨ Advanced Features in Qodo&lt;/h2&gt; 
&lt;h3&gt;ğŸ§­ PR â†’ Ticket Automation&lt;/h3&gt; 
&lt;p&gt;Seamlessly links pull requests to your project tracking system for end-to-end visibility.&lt;/p&gt; 
&lt;h3&gt;âœ… Auto Best Practices&lt;/h3&gt; 
&lt;p&gt;Learns your teamâ€™s standards and automatically enforces them during code reviews.&lt;/p&gt; 
&lt;h3&gt;ğŸ§ª Code Validation&lt;/h3&gt; 
&lt;p&gt;Performs advanced static and semantic analysis to catch issues before merge.&lt;/p&gt; 
&lt;h3&gt;ğŸ’¬ PR Chat Interface&lt;/h3&gt; 
&lt;p&gt;Lets you converse with your PR to explain, summarize, or suggest improvements instantly.&lt;/p&gt; 
&lt;h3&gt;ğŸ” Impact Evaluation&lt;/h3&gt; 
&lt;p&gt;Analyzes the business and technical effect of each change before approval.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;â¤ï¸ Community&lt;/h2&gt; 
&lt;p&gt;This open-source release remains here as a community contribution from Qodo â€” the origin of modern AI-powered code collaboration.&lt;br /&gt; Weâ€™re proud to share it and inspire developers worldwide.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>kvcache-ai/ktransformers</title>
      <link>https://github.com/kvcache-ai/ktransformers</link>
      <description>&lt;p&gt;A Flexible Framework for Experiencing Cutting-edge LLM Inference Optimizations&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;!-- &lt;h1&gt;KTransformers&lt;/h1&gt; --&gt; 
 &lt;p align="center"&gt; 
  &lt;picture&gt; 
   &lt;img alt="KTransformers" src="https://github.com/user-attachments/assets/d5a2492f-a415-4456-af99-4ab102f13f8b" width="50%" /&gt; 
  &lt;/picture&gt; &lt;/p&gt; 
 &lt;h3&gt;A Flexible Framework for Experiencing Cutting-edge LLM Inference Optimizations&lt;/h3&gt; 
 &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/kvcache-ai/ktransformers/main/#show-cases"&gt;ğŸŒŸ Show Cases&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/kvcache-ai/ktransformers/main/#quick-start"&gt;ğŸš€ Quick Start&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/kvcache-ai/ktransformers/main/#tutorial"&gt;ğŸ“ƒ Tutorial&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/kvcache-ai/ktransformers/main/#Citation"&gt;ğŸ”¥ Citation &lt;/a&gt; | &lt;a href="https://github.com/kvcache-ai/ktransformers/discussions"&gt;ğŸ’¬ Discussion &lt;/a&gt;|&lt;a href="https://raw.githubusercontent.com/kvcache-ai/ktransformers/main/#FAQ"&gt; ğŸ™‹ FAQ&lt;/a&gt; &lt;/strong&gt; 
&lt;/div&gt; 
&lt;h2 id="intro"&gt;ğŸ‰ Introduction&lt;/h2&gt; KTransformers, pronounced as Quick Transformers, is designed to enhance your ğŸ¤— 
&lt;a href="https://github.com/huggingface/transformers"&gt;Transformers&lt;/a&gt; experience with advanced kernel optimizations and placement/parallelism strategies. 
&lt;br /&gt;
&lt;br /&gt; KTransformers is a flexible, Python-centric framework designed with extensibility at its core. By implementing and injecting an optimized module with a single line of code, users gain access to a Transformers-compatible interface, RESTful APIs compliant with OpenAI and Ollama, and even a simplified ChatGPT-like web UI. 
&lt;br /&gt;
&lt;br /&gt; Our vision for KTransformers is to serve as a flexible platform for experimenting with innovative LLM inference optimizations. Please let us know if you need any other features. 
&lt;h2 id="Updates"&gt;ğŸ”¥ Updates&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Nov 6, 2025&lt;/strong&gt;: Support Kimi-K2-Thinking inference (&lt;a href="https://raw.githubusercontent.com/kvcache-ai/ktransformers/main/doc/en/Kimi-K2-Thinking.md"&gt;Tutorial&lt;/a&gt;) and fine-tune (&lt;a href="https://raw.githubusercontent.com/kvcache-ai/ktransformers/main/doc/en/SFT_Installation_Guide_KimiK2.md"&gt;Tutorial&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Nov 4, 2025&lt;/strong&gt;: KTransformers Fine-Tuning Ã— LLaMA-Factory Integration. (&lt;a href="https://raw.githubusercontent.com/kvcache-ai/ktransformers/main/doc/en/KTransformers-Fine-Tuning_User-Guide.md"&gt;Tutorial&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Oct 27, 2025&lt;/strong&gt;: Support Ascend NPU. (&lt;a href="https://raw.githubusercontent.com/kvcache-ai/ktransformers/main/doc/zh/DeepseekR1_V3_tutorial_zh_for_Ascend_NPU.md"&gt;Tutorial&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Oct 10, 2025&lt;/strong&gt;: Integrating into SGLang. (&lt;a href="https://github.com/sgl-project/sglang/issues/11425"&gt;Roadmap&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Sept 11, 2025&lt;/strong&gt;: Support Qwen3-Next. (&lt;a href="https://raw.githubusercontent.com/kvcache-ai/ktransformers/main/doc/en/Qwen3-Next.md"&gt;Tutorial&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Sept 05, 2025&lt;/strong&gt;: Support Kimi-K2-0905. (&lt;a href="https://raw.githubusercontent.com/kvcache-ai/ktransformers/main/doc/en/Kimi-K2.md"&gt;Tutorial&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;July 26, 2025&lt;/strong&gt;: Support SmallThinker and GLM4-MoE. (&lt;a href="https://raw.githubusercontent.com/kvcache-ai/ktransformers/main/doc/en/SmallThinker_and_Glm4moe.md"&gt;Tutorial&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;July 11, 2025&lt;/strong&gt;: Support Kimi-K2. (&lt;a href="https://raw.githubusercontent.com/kvcache-ai/ktransformers/main/doc/en/Kimi-K2.md"&gt;Tutorial&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;June 30, 2025&lt;/strong&gt;: Support 3-layer (GPU-CPU-Disk) &lt;a href="https://raw.githubusercontent.com/kvcache-ai/ktransformers/main/doc/en/prefix_cache.md"&gt;prefix cache&lt;/a&gt; reuse.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;May 14, 2025&lt;/strong&gt;: Support Intel Arc GPU (&lt;a href="https://raw.githubusercontent.com/kvcache-ai/ktransformers/main/doc/en/xpu.md"&gt;Tutorial&lt;/a&gt;).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Apr 29, 2025&lt;/strong&gt;: Support AMX-Int8ã€ AMX-BF16 and Qwen3MoE (&lt;a href="https://raw.githubusercontent.com/kvcache-ai/ktransformers/main/doc/en/AMX.md"&gt;Tutorial&lt;/a&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/fafe8aec-4e22-49a8-8553-59fb5c6b00a2"&gt;https://github.com/user-attachments/assets/fafe8aec-4e22-49a8-8553-59fb5c6b00a2&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Apr 9, 2025&lt;/strong&gt;: Experimental support for LLaMA 4 models (&lt;a href="https://raw.githubusercontent.com/kvcache-ai/ktransformers/main/doc/en/llama4.md"&gt;Tutorial&lt;/a&gt;).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Apr 2, 2025&lt;/strong&gt;: Support Multi-concurrency. (&lt;a href="https://raw.githubusercontent.com/kvcache-ai/ktransformers/main/doc/en/balance-serve.md"&gt;Tutorial&lt;/a&gt;).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/faa3bda2-928b-45a7-b44f-21e12ec84b8a"&gt;https://github.com/user-attachments/assets/faa3bda2-928b-45a7-b44f-21e12ec84b8a&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Mar 15, 2025&lt;/strong&gt;: Support ROCm on AMD GPU (&lt;a href="https://raw.githubusercontent.com/kvcache-ai/ktransformers/main/doc/en/ROCm.md"&gt;Tutorial&lt;/a&gt;).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Mar 5, 2025&lt;/strong&gt;: Support unsloth 1.58/2.51 bits weights and &lt;a href="https://raw.githubusercontent.com/kvcache-ai/ktransformers/main/doc/en/fp8_kernel.md"&gt;IQ1_S/FP8 hybrid&lt;/a&gt; weights. Support 139K &lt;a href="https://raw.githubusercontent.com/kvcache-ai/ktransformers/main/doc/en/DeepseekR1_V3_tutorial.md#v022--v023-longer-context--fp8-kernel"&gt;Longer Context&lt;/a&gt; for DeepSeek-V3 and R1 in 24GB VRAM.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Feb 25, 2025&lt;/strong&gt;: Support &lt;a href="https://raw.githubusercontent.com/kvcache-ai/ktransformers/main/doc/en/fp8_kernel.md"&gt;FP8 GPU kernel&lt;/a&gt; for DeepSeek-V3 and R1; &lt;a href="https://raw.githubusercontent.com/kvcache-ai/ktransformers/main/doc/en/DeepseekR1_V3_tutorial.md#v022-longer-context"&gt;Longer Context&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Feb 15, 2025&lt;/strong&gt;: Longer Context (from 4K to 8K for 24GB VRAM) &amp;amp; Slightly Faster Speed ï¼ˆ+15%, up to 16 Tokens/s), update &lt;a href="https://raw.githubusercontent.com/kvcache-ai/ktransformers/main/doc/en/DeepseekR1_V3_tutorial.md"&gt;docs&lt;/a&gt; and &lt;a href="https://kvcache-ai.github.io/ktransformers/"&gt;online books&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Feb 10, 2025&lt;/strong&gt;: Support Deepseek-R1 and V3 on single (24GB VRAM)/multi gpu and 382G DRAM, up to 3~28x speedup. For detailed show case and reproduction tutorial, see &lt;a href="https://raw.githubusercontent.com/kvcache-ai/ktransformers/main/doc/en/DeepseekR1_V3_tutorial.md"&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Aug 28, 2024&lt;/strong&gt;: Decrease DeepseekV2's required VRAM from 21G to 11G.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Aug 15, 2024&lt;/strong&gt;: Update detailed &lt;a href="https://raw.githubusercontent.com/kvcache-ai/ktransformers/main/doc/en/injection_tutorial.md"&gt;tutorial&lt;/a&gt; for injection and multi-GPU.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Aug 14, 2024&lt;/strong&gt;: Support llamfile as linear backend.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Aug 12, 2024&lt;/strong&gt;: Support multiple GPU; Support new model: mixtral 8*7B and 8*22B; Support q2k, q3k, q5k dequant on gpu.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Aug 9, 2024&lt;/strong&gt;: Support windows native.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- * **Aug 28, 2024**: Support 1M context under the InternLM2.5-7B-Chat-1M model, utilizing 24GB of VRAM and 150GB of DRAM. The detailed tutorial is [here](./doc/en/long_context_tutorial.md). --&gt; 
&lt;h2 id="show-cases"&gt;ğŸŒŸ Show Cases&lt;/h2&gt; 
&lt;div&gt; 
 &lt;h3&gt;GPT-4/o1-level Local VSCode Copilot on a Desktop with only 24GB VRAM&lt;/h3&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/ebd70bfa-b2c1-4abb-ae3b-296ed38aa285"&gt;https://github.com/user-attachments/assets/ebd70bfa-b2c1-4abb-ae3b-296ed38aa285&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;[NEW!!!] Local 671B DeepSeek-Coder-V3/R1:&lt;/strong&gt; Running its Q4_K_M version using only 14GB VRAM and 382GB DRAM(&lt;a href="https://raw.githubusercontent.com/kvcache-ai/ktransformers/main/doc/en/DeepseekR1_V3_tutorial.md"&gt;Tutorial&lt;/a&gt;).&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Prefill Speed (tokens/s): 
    &lt;ul&gt; 
     &lt;li&gt;KTransformers: 54.21 (32 cores) â†’ 74.362 (dual-socket, 2Ã—32 cores) â†’ 255.26 (optimized AMX-based MoE kernel, V0.3 only) â†’ 286.55 (selectively using 6 experts, V0.3 only)&lt;/li&gt; 
     &lt;li&gt;Compared to 10.31 tokens/s in llama.cpp with 2Ã—32 cores, achieving up to &lt;strong&gt;27.79Ã— speedup&lt;/strong&gt;.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Decode Speed (tokens/s): 
    &lt;ul&gt; 
     &lt;li&gt;KTransformers: 8.73 (32 cores) â†’ 11.26 (dual-socket, 2Ã—32 cores) â†’ 13.69 (selectively using 6 experts, V0.3 only)&lt;/li&gt; 
     &lt;li&gt;Compared to 4.51 tokens/s in llama.cpp with 2Ã—32 cores, achieving up to &lt;strong&gt;3.03Ã— speedup&lt;/strong&gt;.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Upcoming Open Source Release: 
    &lt;ul&gt; 
     &lt;li&gt;AMX optimizations and selective expert activation will be open-sourced in V0.3.&lt;/li&gt; 
     &lt;li&gt;Currently available only in preview binary distribution, which can be downloaded &lt;a href="https://raw.githubusercontent.com/kvcache-ai/ktransformers/main/doc/en/DeepseekR1_V3_tutorial.md"&gt;here&lt;/a&gt;.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Local 236B DeepSeek-Coder-V2:&lt;/strong&gt; Running its Q4_K_M version using only 21GB VRAM and 136GB DRAM, attainable on a local desktop machine, which scores even better than GPT4-0613 in &lt;a href="https://huggingface.co/blog/leaderboard-bigcodebench"&gt;BigCodeBench&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="center"&gt; 
 &lt;picture&gt; 
  &lt;img alt="DeepSeek-Coder-V2 Score" src="https://github.com/user-attachments/assets/d052924e-8631-44de-aad2-97c54b965693" width="100%" /&gt; 
 &lt;/picture&gt; &lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Faster Speed:&lt;/strong&gt; Achieving 126 tokens/s for 2K prompt prefill and 13.6 tokens/s for generation through MoE offloading and injecting advanced kernels from &lt;a href="https://github.com/Mozilla-Ocho/llamafile/tree/main"&gt;Llamafile&lt;/a&gt; and &lt;a href="https://github.com/IST-DASLab/marlin"&gt;Marlin&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;VSCode Integration:&lt;/strong&gt; Wrapped into an OpenAI and Ollama compatible API for seamless integration as a backend for &lt;a href="https://github.com/TabbyML/tabby"&gt;Tabby&lt;/a&gt; and various other frontends.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="center"&gt; &lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/4c6a8a38-05aa-497d-8eb1-3a5b3918429c"&gt;https://github.com/user-attachments/assets/4c6a8a38-05aa-497d-8eb1-3a5b3918429c&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;/p&gt; 
&lt;!-- &lt;h3&gt;1M Context Local Inference on a Desktop with Only 24GB VRAM&lt;/h3&gt;
&lt;p align="center"&gt;

https://github.com/user-attachments/assets/a865e5e4-bca3-401e-94b8-af3c080e6c12

* **1M Context InternLM 2.5 7B**: Operates at full bf16 precision, utilizing 24GB VRAM and 150GB DRAM, which is feasible on a local desktop setup. It achieves a 92.88% success rate on the 1M "Needle In a Haystack" test and 100% on the 128K NIAH test.

&lt;p align="center"&gt;
  &lt;picture&gt;
    &lt;img alt="Single Needle Retrieval 128K" src="./doc/assets/needle_128K.png" width=100%&gt;
  &lt;/picture&gt;
&lt;/p&gt;

&lt;p align="center"&gt;
  &lt;picture&gt;
    &lt;img alt="Single Needle Retrieval 1000K" src="./doc/assets/needle_1M.png" width=100%&gt;
  &lt;/picture&gt;
&lt;/p&gt;

* **Enhanced Speed**: Reaches 16.91 tokens/s for generation with a 1M context using sparse attention, powered by llamafile kernels. This method is over 10 times faster than full attention approach of llama.cpp.

* **Flexible Sparse Attention Framework**: Offers a flexible block sparse attention framework for CPU offloaded decoding. Compatible with SnapKV, Quest, and InfLLm. Further information is available [here](./doc/en/long_context_introduction.md).
 --&gt; 
&lt;p&gt;&lt;strong&gt;More advanced features will coming soon, so stay tuned!&lt;/strong&gt;&lt;/p&gt; 
&lt;h2 id="quick-start"&gt;ğŸš€ Quick Start&lt;/h2&gt; 
&lt;p&gt;Getting started with KTransformers is simple! Follow the steps below to set up and start using it.&lt;/p&gt; 
&lt;p&gt;we have already supported vendors:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Metax&lt;/li&gt; 
 &lt;li&gt;Sanechips (ZhuFeng V1.0)&lt;/li&gt; 
 &lt;li&gt;Intel&lt;/li&gt; 
 &lt;li&gt;Ascend&lt;/li&gt; 
 &lt;li&gt;Kunpeng&lt;/li&gt; 
 &lt;li&gt;AMD&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ“¥ Installation&lt;/h3&gt; 
&lt;p&gt;To install KTransformers, follow the official &lt;a href="https://kvcache-ai.github.io/ktransformers/en/install.html"&gt;Installation Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h2 id="tutorial"&gt;ğŸ“ƒ Brief Injection Tutorial&lt;/h2&gt; At the heart of KTransformers is a user-friendly, template-based injection framework. This allows researchers to easily replace original torch modules with optimized variants. It also simplifies the process of combining multiple optimizations, allowing the exploration of their synergistic effects. 
&lt;br /&gt; 
&lt;p align="center"&gt; 
 &lt;picture&gt; 
  &lt;img alt="Inject-Struction" src="https://github.com/user-attachments/assets/6b4c1e54-9f6d-45c5-a3fc-8fa45e7d257e" width="65%" /&gt; 
 &lt;/picture&gt; &lt;/p&gt; 
&lt;p&gt;Given that vLLM already serves as a great framework for large-scale deployment optimizations, KTransformers is particularly focused on local deployments that are constrained by limited resources. We pay special attention to heterogeneous computing opportunities, such as GPU/CPU offloading of quantized models. For example, we support the efficient &lt;a herf="https://github.com/Mozilla-Ocho/llamafile/tree/main"&gt;Llamafile&lt;/a&gt; and &lt;a herf="https://github.com/IST-DASLab/marlin"&gt;Marlin&lt;/a&gt; kernels for CPU and GPU, respectively. More details can be found &lt;a herf="doc/en/operators/llamafile.md"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Example Usage&lt;/h3&gt; To utilize the provided kernels, users only need to create a YAML-based injection template and add the call to `optimize_and_load_gguf` before using the Transformers model. 
&lt;pre&gt;&lt;code class="language-python"&gt;with torch.device("meta"):
    model = AutoModelForCausalLM.from_config(config, trust_remote_code=True)
optimize_and_load_gguf(model, optimize_config_path, gguf_path, config)
...
generated = prefill_and_generate(model, tokenizer, input_tensor.cuda(), max_new_tokens=1000)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;In this example, the AutoModel is first initialized on the meta device to avoid occupying any memory resources. Then, &lt;code&gt;optimize_and_load_gguf&lt;/code&gt; iterates through all sub-modules of the model, matches rules specified in your YAML rule file, and replaces them with advanced modules as specified.&lt;/p&gt; 
&lt;p&gt;After injection, the original &lt;code&gt;generate&lt;/code&gt; interface is available, but we also provide a compatible &lt;code&gt;prefill_and_generate&lt;/code&gt; method, which enables further optimizations like CUDAGraph to improve generation speed.&lt;/p&gt; 
&lt;h3&gt;How to custom your model&lt;/h3&gt; 
&lt;p&gt;A detailed tutorial of the injection and multi-GPU using DeepSeek-V2 as an example is given &lt;a href="https://raw.githubusercontent.com/kvcache-ai/ktransformers/main/doc/en/injection_tutorial.md"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Below is an example of a YAML template for replacing all original Linear modules with Marlin, an advanced 4-bit quantization kernel.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;- match:
    name: "^model\\.layers\\..*$"  # regular expression 
    class: torch.nn.Linear  # only match modules matching name and class simultaneously
  replace:
    class: ktransformers.operators.linear.KTransformerLinear  # optimized Kernel on quantized data types
    device: "cpu"   # which devices to load this module when initializing
    kwargs:
      generate_device: "cuda"
      generate_linear_type: "QuantizedLinearMarlin"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Each rule in the YAML file has two parts: &lt;code&gt;match&lt;/code&gt; and &lt;code&gt;replace&lt;/code&gt;. The &lt;code&gt;match&lt;/code&gt; part specifies which module should be replaced, and the &lt;code&gt;replace&lt;/code&gt; part specifies the module to be injected into the model along with the initialization keywords.&lt;/p&gt; 
&lt;p&gt;You can find example rule templates for optimizing DeepSeek-V2 and Qwen2-57B-A14, two SOTA MoE models, in the &lt;a href="https://raw.githubusercontent.com/kvcache-ai/ktransformers/main/ktransformers/optimize/optimize_rules"&gt;ktransformers/optimize/optimize_rules&lt;/a&gt; directory. These templates are used to power the &lt;code&gt;local_chat.py&lt;/code&gt; demo.&lt;/p&gt; 
&lt;p&gt;If you are interested in our design principles and the implementation of the injection framework, please refer to the &lt;a href="https://raw.githubusercontent.com/kvcache-ai/ktransformers/main/doc/en/deepseek-v2-injection.md"&gt;design document&lt;/a&gt;.&lt;/p&gt; 
&lt;h2 id="Citation"&gt;ğŸ”¥ Citation&lt;/h2&gt; 
&lt;p&gt;If you use KTransformers for your research, please cite our &lt;a href="https://madsys.cs.tsinghua.edu.cn/publication/ktransformers-unleashing-the-full-potential-of-cpu/gpu-hybrid-inference-for-moe-models/"&gt;paper&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@inproceedings{10.1145/3731569.3764843,
title = {KTransformers: Unleashing the Full Potential of CPU/GPU Hybrid Inference for MoE Models},
author = {Chen, Hongtao and Xie, Weiyu and Zhang, Boxin and Tang, Jingqi and Wang, Jiahao and Dong, Jianwei and Chen, Shaoyuan and Yuan, Ziwei and Lin, Chen and Qiu, Chengyu and Zhu, Yuening and Ou, Qingliang and Liao, Jiaqi and Chen, Xianglin and Ai, Zhiyuan and Wu, Yongwei and Zhang, Mingxing},
booktitle = {Proceedings of the ACM SIGOPS 31st Symposium on Operating Systems Principles},
year = {2025}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2 id="ack"&gt;Acknowledgment and Contributors&lt;/h2&gt; 
&lt;p&gt;The development of KTransformers is based on the flexible and versatile framework provided by Transformers. We also benefit from advanced kernels such as GGUF/GGML, Llamafile, Marlin, sglang and flashinfer. We are planning to contribute back to the community by upstreaming our modifications.&lt;/p&gt; 
&lt;p&gt;KTransformers is actively maintained and developed by contributors from the &lt;a href="https://madsys.cs.tsinghua.edu.cn/"&gt;MADSys group&lt;/a&gt; at Tsinghua University and members from &lt;a href="http://approaching.ai/"&gt;Approaching.AI&lt;/a&gt;. We welcome new contributors to join us in making KTransformers faster and easier to use.&lt;/p&gt; 
&lt;h2 id="ack"&gt;Discussion&lt;/h2&gt; 
&lt;p&gt;If you have any questions, feel free to open an issue. Alternatively, you can join our WeChat group for further discussion. QR Code: &lt;a href="https://raw.githubusercontent.com/kvcache-ai/ktransformers/main/WeChatGroup.png"&gt;WeChat Group&lt;/a&gt;&lt;/p&gt; 
&lt;h2 id="FAQ"&gt;ğŸ™‹ FAQ&lt;/h2&gt; 
&lt;p&gt;Some common questions are answered in the &lt;a href="https://raw.githubusercontent.com/kvcache-ai/ktransformers/main/doc/en/FAQ.md"&gt;FAQ&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>mindsdb/mindsdb</title>
      <link>https://github.com/mindsdb/mindsdb</link>
      <description>&lt;p&gt;Federated query engine for AI - The only MCP Server you'll ever need&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a name="readme-top"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://pypi.org/project/MindsDB/" target="_blank"&gt;&lt;img src="https://badge.fury.io/py/MindsDB.svg?sanitize=true" alt="MindsDB Release" /&gt;&lt;/a&gt; 
 &lt;a href="https://www.python.org/downloads/" target="_blank"&gt;&lt;img src="https://img.shields.io/badge/python-3.10.x%7C%203.11.x%7C%203.12.x%7C%203.13.x-brightgreen.svg?sanitize=true" alt="Python supported" /&gt;&lt;/a&gt; 
 &lt;a href="https://hub.docker.com/u/mindsdb" target="_blank"&gt;&lt;img src="https://img.shields.io/docker/pulls/mindsdb/mindsdb" alt="Docker pulls" /&gt;&lt;/a&gt; 
 &lt;br /&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/3068" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/3068" alt="mindsdb%2Fmindsdb | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;a href="https://github.com/mindsdb/mindsdb"&gt; &lt;img src="https://raw.githubusercontent.com/mindsdb/mindsdb/main/docs/assets/mindsdb_logo.png" alt="MindsDB" width="300" /&gt; &lt;/a&gt; 
 &lt;p align="center"&gt; &lt;br /&gt; &lt;a href="https://www.mindsdb.com?utm_medium=community&amp;amp;utm_source=github&amp;amp;utm_campaign=mindsdb%20repo"&gt;Website&lt;/a&gt; Â· &lt;a href="https://docs.mindsdb.com?utm_medium=community&amp;amp;utm_source=github&amp;amp;utm_campaign=mindsdb%20repo"&gt;Docs&lt;/a&gt; Â· &lt;a href="https://mindsdb.com/contact"&gt;Contact us for a Demo&lt;/a&gt; Â· &lt;a href="https://mindsdb.com/joincommunity?utm_medium=community&amp;amp;utm_source=github&amp;amp;utm_campaign=mindsdb%20repo"&gt;Community Slack&lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;p&gt;MindsDB enables humans, AI, agents, and applications to get highly accurate answers across large scale data sources.&lt;/p&gt; 
&lt;a href="https://www.youtube.com/watch?v=MX3OKpnsoLM" target="_blank"&gt; &lt;img src="https://github.com/user-attachments/assets/119e7b82-f901-4214-a26f-ff7c5ad86064" alt="MindsDB Demo" /&gt; &lt;/a&gt; 
&lt;h2&gt;Install MindsDB Server&lt;/h2&gt; 
&lt;p&gt;MindsDB is an open-source server that can be deployed anywhere - from your laptop to the cloud, and everywhere in between. And yes, you can customize it to your heart's content.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.mindsdb.com/setup/self-hosted/docker-desktop"&gt;Using Docker Desktop&lt;/a&gt;. This is the fastest and recommended way to get started and have it all running.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.mindsdb.com/setup/self-hosted/docker"&gt;Using Docker&lt;/a&gt;. This is also simple, but gives you more flexibility on how to further customize your server.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://docs.mindsdb.com/mcp/overview"&gt;MindsDB has an MCP server built in&lt;/a&gt; that enables your MCP applications to connect, unify and respond to questions over large-scale federated dataâ€”spanning databases, data warehouses, and SaaS applications.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h1&gt;Core Philosophy: Connect, Unify, Respond&lt;/h1&gt; 
&lt;p&gt;MindsDB's architecture is built around three fundamental capabilities:&lt;/p&gt; 
&lt;h2&gt;&lt;a href="https://docs.mindsdb.com/integrations/data-overview"&gt;Connect&lt;/a&gt; Your Data&lt;/h2&gt; 
&lt;p&gt;You can connect to hundreds of enterprise &lt;a href="https://docs.mindsdb.com/integrations/data-overview"&gt;data sources (learn more)&lt;/a&gt;. These integrations allow MindsDB to access data wherever it resides, forming the foundation for all other capabilities.&lt;/p&gt; 
&lt;h2&gt;&lt;a href="https://docs.mindsdb.com/mindsdb_sql/overview"&gt;Unify&lt;/a&gt; Your Data&lt;/h2&gt; 
&lt;p&gt;In many situations, itâ€™s important to be able to prepare and unify data before generating responses from it. MindsDB SQL offers knowledge bases and views that allow indexing and organizing structured and unstructured data as if it were unified in a single system.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.mindsdb.com/mindsdb_sql/knowledge-bases"&gt;&lt;strong&gt;KNOWLEDGE BASES&lt;/strong&gt;&lt;/a&gt; â€“ Index and organize unstructured data for efficient Q&amp;amp;A.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.mindsdb.com/mindsdb_sql/sql/create/view"&gt;&lt;strong&gt;VIEWS&lt;/strong&gt;&lt;/a&gt; â€“ Simplify data access by creating unified views across different sources (no-ETL).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Unification of data can be automated using JOBs&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.mindsdb.com/mindsdb_sql/sql/create/jobs"&gt;&lt;strong&gt;JOBS&lt;/strong&gt;&lt;/a&gt; â€“ Schedule synchronization and transformation tasks for real-time processing.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;a href="https://docs.mindsdb.com/mindsdb_sql/agents/agent"&gt;Respond&lt;/a&gt; From Your Data&lt;/h2&gt; 
&lt;p&gt;Chat with Your Data&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.mindsdb.com/mindsdb_sql/agents/agent"&gt;&lt;strong&gt;AGENTS&lt;/strong&gt;&lt;/a&gt; â€“ Configure built-in agents specialized in answering questions over your connected and unified data.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.mindsdb.com/mcp/overview"&gt;&lt;strong&gt;MCP&lt;/strong&gt;&lt;/a&gt; â€“ Connect to MindsDB through the MCP (Model Context Protocol) for seamless interaction.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ¤ Contribute&lt;/h2&gt; 
&lt;p&gt;Interested in contributing to MindsDB? Follow our &lt;a href="https://docs.mindsdb.com/contribute/install?utm_medium=community&amp;amp;utm_source=github&amp;amp;utm_campaign=mindsdb%20repo"&gt;installation guide for development&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You can find our &lt;a href="https://docs.mindsdb.com/contribute/contribute?utm_medium=community&amp;amp;utm_source=github&amp;amp;utm_campaign=mindsdb%20repo"&gt;contribution guide here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;We welcome suggestions! Feel free to open new issues with your ideas, and weâ€™ll guide you.&lt;/p&gt; 
&lt;p&gt;This project adheres to a &lt;a href="https://github.com/mindsdb/mindsdb/raw/main/CODE_OF_CONDUCT.md"&gt;Contributor Code of Conduct&lt;/a&gt;. By participating, you agree to follow its terms.&lt;/p&gt; 
&lt;p&gt;Also, check out our &lt;a href="https://mindsdb.com/community?utm_medium=community&amp;amp;utm_source=github&amp;amp;utm_campaign=mindsdb%20repo"&gt;community rewards and programs&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;ğŸ¤ Support&lt;/h2&gt; 
&lt;p&gt;If you find a bug, please submit an &lt;a href="https://github.com/mindsdb/mindsdb/issues/new/choose"&gt;issue on GitHub&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Hereâ€™s how you can get community support:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Ask a question in our &lt;a href="https://mindsdb.com/joincommunity?utm_medium=community&amp;amp;utm_source=github&amp;amp;utm_campaign=mindsdb%20repo"&gt;Slack Community&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Join our &lt;a href="https://github.com/mindsdb/mindsdb/discussions"&gt;GitHub Discussions&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Post on &lt;a href="https://stackoverflow.com/questions/tagged/mindsdb"&gt;Stack Overflow&lt;/a&gt; with the MindsDB tag.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For commercial support, please &lt;a href="https://mindsdb.com/contact?utm_medium=community&amp;amp;utm_source=github&amp;amp;utm_campaign=mindsdb%20repo"&gt;contact the MindsDB team&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;ğŸ’š Current Contributors&lt;/h2&gt; 
&lt;a href="https://github.com/mindsdb/mindsdb/graphs/contributors"&gt; &lt;img src="https://contributors-img.web.app/image?repo=mindsdb/mindsdb" /&gt; &lt;/a&gt; 
&lt;p&gt;Generated with &lt;a href="https://contributors-img.web.app"&gt;contributors-img&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;ğŸ”” Subscribe for Updates&lt;/h2&gt; 
&lt;p&gt;Join our &lt;a href="https://mindsdb.com/joincommunity"&gt;Slack community&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Skyvern-AI/skyvern</title>
      <link>https://github.com/Skyvern-AI/skyvern</link>
      <description>&lt;p&gt;Automate browser based workflows with AI&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt; &lt;a href="https://www.skyvern.com"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="fern/images/skyvern_logo.png" /&gt; 
   &lt;img height="120" src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/skyvern_logo_blackbg.png" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; &lt;br /&gt; &lt;/h1&gt; 
&lt;p align="center"&gt; ğŸ‰ Automate Browser-based workflows using LLMs and Computer Vision ğŸ‰ &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://www.skyvern.com/"&gt;&lt;img src="https://img.shields.io/badge/Website-blue?logo=googlechrome&amp;amp;logoColor=black" /&gt;&lt;/a&gt; &lt;a href="https://www.skyvern.com/docs/"&gt;&lt;img src="https://img.shields.io/badge/Docs-yellow?logo=gitbook&amp;amp;logoColor=black" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/fG2XXEuQX3"&gt;&lt;img src="https://img.shields.io/discord/1212486326352617534?logo=discord&amp;amp;label=discord" /&gt;&lt;/a&gt; 
 &lt;!-- &lt;a href="https://pepy.tech/project/skyvern" target="_blank"&gt;&lt;img src="https://static.pepy.tech/badge/skyvern" alt="Total Downloads"/&gt;&lt;/a&gt; --&gt; &lt;a href="https://github.com/skyvern-ai/skyvern"&gt;&lt;img src="https://img.shields.io/github/stars/skyvern-ai/skyvern" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Skyvern-AI/skyvern/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/skyvern-ai/skyvern" /&gt;&lt;/a&gt; &lt;a href="https://twitter.com/skyvernai"&gt;&lt;img src="https://img.shields.io/twitter/follow/skyvernai?style=social" /&gt;&lt;/a&gt; &lt;a href="https://www.linkedin.com/company/95726232"&gt;&lt;img src="https://img.shields.io/badge/Follow%20 on%20LinkedIn-8A2BE2?logo=linkedin" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.skyvern.com"&gt;Skyvern&lt;/a&gt; automates browser-based workflows using LLMs and computer vision. It provides a simple API endpoint to fully automate manual workflows on a large number of websites, replacing brittle or unreliable automation solutions.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/geico_shu_recording_cropped.gif" /&gt; &lt;/p&gt; 
&lt;p&gt;Traditional approaches to browser automations required writing custom scripts for websites, often relying on DOM parsing and XPath-based interactions which would break whenever the website layouts changed.&lt;/p&gt; 
&lt;p&gt;Instead of only relying on code-defined XPath interactions, Skyvern relies on Vision LLMs to learn and interact with the websites.&lt;/p&gt; 
&lt;h1&gt;How it works&lt;/h1&gt; 
&lt;p&gt;Skyvern was inspired by the Task-Driven autonomous agent design popularized by &lt;a href="https://github.com/yoheinakajima/babyagi"&gt;BabyAGI&lt;/a&gt; and &lt;a href="https://github.com/Significant-Gravitas/AutoGPT"&gt;AutoGPT&lt;/a&gt; -- with one major bonus: we give Skyvern the ability to interact with websites using browser automation libraries like &lt;a href="https://playwright.dev/"&gt;Playwright&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Skyvern uses a swarm of agents to comprehend a website, and plan and execute its actions:&lt;/p&gt; 
&lt;picture&gt; 
 &lt;source media="(prefers-color-scheme: dark)" srcset="fern/images/skyvern_2_0_system_diagram.png" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/skyvern_2_0_system_diagram.png" /&gt; 
&lt;/picture&gt; 
&lt;p&gt;This approach has a few advantages:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Skyvern can operate on websites it's never seen before, as it's able to map visual elements to actions necessary to complete a workflow, without any customized code&lt;/li&gt; 
 &lt;li&gt;Skyvern is resistant to website layout changes, as there are no pre-determined XPaths or other selectors our system is looking for while trying to navigate&lt;/li&gt; 
 &lt;li&gt;Skyvern is able to take a single workflow and apply it to a large number of websites, as it's able to reason through the interactions necessary to complete the workflow&lt;/li&gt; 
 &lt;li&gt;Skyvern leverages LLMs to reason through interactions to ensure we can cover complex situations. Examples include: 
  &lt;ol&gt; 
   &lt;li&gt;If you wanted to get an auto insurance quote from Geico, the answer to a common question "Were you eligible to drive at 18?" could be inferred from the driver receiving their license at age 16&lt;/li&gt; 
   &lt;li&gt;If you were doing competitor analysis, it's understanding that an Arnold Palmer 22 oz can at 7/11 is almost definitely the same product as a 23 oz can at Gopuff (even though the sizes are slightly different, which could be a rounding error!)&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;A detailed technical report can be found &lt;a href="https://www.skyvern.com/blog/skyvern-2-0-state-of-the-art-web-navigation-with-85-8-on-webvoyager-eval/"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Demo&lt;/h1&gt; 
&lt;!-- Redo demo --&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/5cab4668-e8e2-4982-8551-aab05ff73a7f"&gt;https://github.com/user-attachments/assets/5cab4668-e8e2-4982-8551-aab05ff73a7f&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Performance &amp;amp; Evaluation&lt;/h1&gt; 
&lt;p&gt;Skyvern has SOTA performance on the &lt;a href="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/webbench.ai"&gt;WebBench benchmark&lt;/a&gt; with a 64.4% accuracy. The technical report + evaluation can be found &lt;a href="https://www.skyvern.com/blog/web-bench-a-new-way-to-compare-ai-browser-agents/"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/performance/webbench_overall.png" /&gt; &lt;/p&gt; 
&lt;h2&gt;Performance on WRITE tasks (eg filling out forms, logging in, downloading files, etc)&lt;/h2&gt; 
&lt;p&gt;Skyvern is the best performing agent on WRITE tasks (eg filling out forms, logging in, downloading files, etc), which is primarily used for RPA (Robotic Process Automation) adjacent tasks.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/performance/webbench_write.png" /&gt; &lt;/p&gt; 
&lt;h1&gt;Quickstart&lt;/h1&gt; 
&lt;h2&gt;Skyvern Cloud&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://app.skyvern.com"&gt;Skyvern Cloud&lt;/a&gt; is a managed cloud version of Skyvern that allows you to run Skyvern without worrying about the infrastructure. It allows you to run multiple Skyvern instances in parallel and comes bundled with anti-bot detection mechanisms, proxy network, and CAPTCHA solvers.&lt;/p&gt; 
&lt;p&gt;If you'd like to try it out, navigate to &lt;a href="https://app.skyvern.com"&gt;app.skyvern.com&lt;/a&gt; and create an account.&lt;/p&gt; 
&lt;h2&gt;Install &amp;amp; Run&lt;/h2&gt; 
&lt;p&gt;Dependencies needed:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.python.org/downloads/"&gt;Python 3.11.x&lt;/a&gt;, works with 3.12, not ready yet for 3.13&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nodejs.org/en/download/"&gt;NodeJS &amp;amp; NPM&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Additionally, for Windows:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://rustup.rs/"&gt;Rust&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;VS Code with C++ dev tools and Windows SDK&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;1. Install Skyvern&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install skyvern
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. Run Skyvern&lt;/h3&gt; 
&lt;p&gt;This is most helpful for first time run (db setup, db migrations etc).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;skyvern quickstart
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;3. Run task&lt;/h3&gt; 
&lt;h4&gt;UI (Recommended)&lt;/h4&gt; 
&lt;p&gt;Start the Skyvern service and UI (when DB is up and running)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;skyvern run all
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Go to &lt;a href="http://localhost:8080"&gt;http://localhost:8080&lt;/a&gt; and use the UI to run a task&lt;/p&gt; 
&lt;h4&gt;Code&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from skyvern import Skyvern

skyvern = Skyvern()
task = await skyvern.run_task(prompt="Find the top post on hackernews today")
print(task)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Skyvern starts running the task in a browser that pops up and closes it when the task is done. You will be able to view the task from &lt;a href="http://localhost:8080/history"&gt;http://localhost:8080/history&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;You can also run a task on different targets:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from skyvern import Skyvern

# Run on Skyvern Cloud
skyvern = Skyvern(api_key="SKYVERN API KEY")

# Local Skyvern service
skyvern = Skyvern(base_url="http://localhost:8000", api_key="LOCAL SKYVERN API KEY")

task = await skyvern.run_task(prompt="Find the top post on hackernews today")
print(task)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Advanced Usage&lt;/h2&gt; 
&lt;h3&gt;Control your own browser (Chrome)&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;âš ï¸ WARNING: Since &lt;a href="https://developer.chrome.com/blog/remote-debugging-port"&gt;Chrome 136&lt;/a&gt;, Chrome refuses any CDP connect to the browser using the default user_data_dir. In order to use your browser data, Skyvern copies your default user_data_dir to &lt;code&gt;./tmp/user_data_dir&lt;/code&gt; the first time connecting to your local browser. âš ï¸&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ol&gt; 
 &lt;li&gt;Just With Python Code&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from skyvern import Skyvern

# The path to your Chrome browser. This example path is for Mac.
browser_path = "/Applications/Google Chrome.app/Contents/MacOS/Google Chrome"
skyvern = Skyvern(
    base_url="http://localhost:8000",
    api_key="YOUR_API_KEY",
    browser_path=browser_path,
)
task = await skyvern.run_task(
    prompt="Find the top post on hackernews today",
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;With Skyvern Service&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Add two variables to your .env file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# The path to your Chrome browser. This example path is for Mac.
CHROME_EXECUTABLE_PATH="/Applications/Google Chrome.app/Contents/MacOS/Google Chrome"
BROWSER_TYPE=cdp-connect
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Restart Skyvern service &lt;code&gt;skyvern run all&lt;/code&gt; and run the task through UI or code&lt;/p&gt; 
&lt;h3&gt;Run Skyvern with any remote browser&lt;/h3&gt; 
&lt;p&gt;Grab the cdp connection url and pass it to Skyvern&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from skyvern import Skyvern

skyvern = Skyvern(cdp_url="your cdp connection url")
task = await skyvern.run_task(
    prompt="Find the top post on hackernews today",
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Get consistent output schema from your run&lt;/h3&gt; 
&lt;p&gt;You can do this by adding the &lt;code&gt;data_extraction_schema&lt;/code&gt; parameter:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from skyvern import Skyvern

skyvern = Skyvern()
task = await skyvern.run_task(
    prompt="Find the top post on hackernews today",
    data_extraction_schema={
        "type": "object",
        "properties": {
            "title": {
                "type": "string",
                "description": "The title of the top post"
            },
            "url": {
                "type": "string",
                "description": "The URL of the top post"
            },
            "points": {
                "type": "integer",
                "description": "Number of points the post has received"
            }
        }
    }
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Helpful commands to debug issues&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Launch the Skyvern Server Separately*
skyvern run server

# Launch the Skyvern UI
skyvern run ui

# Check status of the Skyvern service
skyvern status

# Stop the Skyvern service
skyvern stop all

# Stop the Skyvern UI
skyvern stop ui

# Stop the Skyvern Server Separately
skyvern stop server
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Docker Compose setup&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Make sure you have &lt;a href="https://www.docker.com/products/docker-desktop/"&gt;Docker Desktop&lt;/a&gt; installed and running on your machine&lt;/li&gt; 
 &lt;li&gt;Make sure you don't have postgres running locally (Run &lt;code&gt;docker ps&lt;/code&gt; to check)&lt;/li&gt; 
 &lt;li&gt;Clone the repository and navigate to the root directory&lt;/li&gt; 
 &lt;li&gt;Run &lt;code&gt;skyvern init llm&lt;/code&gt; to generate a &lt;code&gt;.env&lt;/code&gt; file. This will be copied into the Docker image.&lt;/li&gt; 
 &lt;li&gt;Fill in the LLM provider key on the &lt;a href="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/docker-compose.yml"&gt;docker-compose.yml&lt;/a&gt;. &lt;em&gt;If you want to run Skyvern on a remote server, make sure you set the correct server ip for the UI container in &lt;a href="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/docker-compose.yml"&gt;docker-compose.yml&lt;/a&gt;.&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;Run the following command via the commandline: &lt;pre&gt;&lt;code class="language-bash"&gt; docker compose up -d
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Navigate to &lt;code&gt;http://localhost:8080&lt;/code&gt; in your browser to start using the UI&lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Important:&lt;/strong&gt; Only one Postgres container can run on port 5432 at a time. If you switch from the CLI-managed Postgres to Docker Compose, you must first remove the original container:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;docker rm -f postgresql-container
&lt;/code&gt;&lt;/pre&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;If you encounter any database related errors while using Docker to run Skyvern, check which Postgres container is running with &lt;code&gt;docker ps&lt;/code&gt;.&lt;/p&gt; 
&lt;h1&gt;Skyvern Features&lt;/h1&gt; 
&lt;h2&gt;Skyvern Tasks&lt;/h2&gt; 
&lt;p&gt;Tasks are the fundamental building block inside Skyvern. Each task is a single request to Skyvern, instructing it to navigate through a website and accomplish a specific goal.&lt;/p&gt; 
&lt;p&gt;Tasks require you to specify a &lt;code&gt;url&lt;/code&gt;, &lt;code&gt;prompt&lt;/code&gt;, and can optionally include a &lt;code&gt;data schema&lt;/code&gt; (if you want the output to conform to a specific schema) and &lt;code&gt;error codes&lt;/code&gt; (if you want Skyvern to stop running in specific situations).&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/skyvern_2_0_screenshot.png" /&gt; &lt;/p&gt; 
&lt;h2&gt;Skyvern Workflows&lt;/h2&gt; 
&lt;p&gt;Workflows are a way to chain multiple tasks together to form a cohesive unit of work.&lt;/p&gt; 
&lt;p&gt;For example, if you wanted to download all invoices newer than January 1st, you could create a workflow that first navigated to the invoices page, then filtered down to only show invoices newer than January 1st, extracted a list of all eligible invoices, and iterated through each invoice to download it.&lt;/p&gt; 
&lt;p&gt;Another example is if you wanted to automate purchasing products from an e-commerce store, you could create a workflow that first navigated to the desired product, then added it to a cart. Second, it would navigate to the cart and validate the cart state. Finally, it would go through the checkout process to purchase the items.&lt;/p&gt; 
&lt;p&gt;Supported workflow features include:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Browser Task&lt;/li&gt; 
 &lt;li&gt;Browser Action&lt;/li&gt; 
 &lt;li&gt;Data Extraction&lt;/li&gt; 
 &lt;li&gt;Validation&lt;/li&gt; 
 &lt;li&gt;For Loops&lt;/li&gt; 
 &lt;li&gt;File parsing&lt;/li&gt; 
 &lt;li&gt;Sending emails&lt;/li&gt; 
 &lt;li&gt;Text Prompts&lt;/li&gt; 
 &lt;li&gt;HTTP Request Block&lt;/li&gt; 
 &lt;li&gt;Custom Code Block&lt;/li&gt; 
 &lt;li&gt;Uploading files to block storage&lt;/li&gt; 
 &lt;li&gt;(Coming soon) Conditionals&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/block_example_v2.png" /&gt; &lt;/p&gt; 
&lt;h2&gt;Livestreaming&lt;/h2&gt; 
&lt;p&gt;Skyvern allows you to livestream the viewport of the browser to your local machine so that you can see exactly what Skyvern is doing on the web. This is useful for debugging and understanding how Skyvern is interacting with a website, and intervening when necessary&lt;/p&gt; 
&lt;h2&gt;Form Filling&lt;/h2&gt; 
&lt;p&gt;Skyvern is natively capable of filling out form inputs on websites. Passing in information via the &lt;code&gt;navigation_goal&lt;/code&gt; will allow Skyvern to comprehend the information and fill out the form accordingly.&lt;/p&gt; 
&lt;h2&gt;Data Extraction&lt;/h2&gt; 
&lt;p&gt;Skyvern is also capable of extracting data from a website.&lt;/p&gt; 
&lt;p&gt;You can also specify a &lt;code&gt;data_extraction_schema&lt;/code&gt; directly within the main prompt to tell Skyvern exactly what data you'd like to extract from the website, in jsonc format. Skyvern's output will be structured in accordance to the supplied schema.&lt;/p&gt; 
&lt;h2&gt;File Downloading&lt;/h2&gt; 
&lt;p&gt;Skyvern is also capable of downloading files from a website. All downloaded files are automatically uploaded to block storage (if configured), and you can access them via the UI.&lt;/p&gt; 
&lt;h2&gt;Authentication&lt;/h2&gt; 
&lt;p&gt;Skyvern supports a number of different authentication methods to make it easier to automate tasks behind a login. If you'd like to try it out, please reach out to us &lt;a href="mailto:founders@skyvern.com"&gt;via email&lt;/a&gt; or &lt;a href="https://discord.gg/fG2XXEuQX3"&gt;discord&lt;/a&gt;.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/secure_password_task_example.png" /&gt; &lt;/p&gt; 
&lt;h3&gt;ğŸ” 2FA Support (TOTP)&lt;/h3&gt; 
&lt;p&gt;Skyvern supports a number of different 2FA methods to allow you to automate workflows that require 2FA.&lt;/p&gt; 
&lt;p&gt;Examples include:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;QR-based 2FA (e.g. Google Authenticator, Authy)&lt;/li&gt; 
 &lt;li&gt;Email based 2FA&lt;/li&gt; 
 &lt;li&gt;SMS based 2FA&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;ğŸ” Learn more about 2FA support &lt;a href="https://www.skyvern.com/docs/credentials/totp"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Password Manager Integrations&lt;/h3&gt; 
&lt;p&gt;Skyvern currently supports the following password manager integrations:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Bitwarden&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; 1Password&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; LastPass&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Model Context Protocol (MCP)&lt;/h2&gt; 
&lt;p&gt;Skyvern supports the Model Context Protocol (MCP) to allow you to use any LLM that supports MCP.&lt;/p&gt; 
&lt;p&gt;See the MCP documentation &lt;a href="https://github.com/Skyvern-AI/skyvern/raw/main/integrations/mcp/README.md"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Zapier / Make.com / N8N Integration&lt;/h2&gt; 
&lt;p&gt;Skyvern supports Zapier, Make.com, and N8N to allow you to connect your Skyvern workflows to other apps.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.skyvern.com/docs/integrations/zapier"&gt;Zapier&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.skyvern.com/docs/integrations/make.com"&gt;Make.com&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.skyvern.com/docs/integrations/n8n"&gt;N8N&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;ğŸ” Learn more about 2FA support &lt;a href="https://www.skyvern.com/docs/credentials/totp"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Real-world examples of Skyvern&lt;/h1&gt; 
&lt;p&gt;We love to see how Skyvern is being used in the wild. Here are some examples of how Skyvern is being used to automate workflows in the real world. Please open PRs to add your own examples!&lt;/p&gt; 
&lt;h2&gt;Invoice Downloading on many different websites&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://meetings.hubspot.com/skyvern/demo"&gt;Book a demo to see it live&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/invoice_downloading.gif" /&gt; &lt;/p&gt; 
&lt;h2&gt;Automate the job application process&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://app.skyvern.com/tasks/create/job_application"&gt;ğŸ’¡ See it in action&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/job_application_demo.gif" /&gt; &lt;/p&gt; 
&lt;h2&gt;Automate materials procurement for a manufacturing company&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://app.skyvern.com/tasks/create/finditparts"&gt;ğŸ’¡ See it in action&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/finditparts_recording_crop.gif" /&gt; &lt;/p&gt; 
&lt;h2&gt;Navigating to government websites to register accounts or fill out forms&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://app.skyvern.com/tasks/create/california_edd"&gt;ğŸ’¡ See it in action&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/edd_services.gif" /&gt; &lt;/p&gt; 
&lt;!-- Add example of delaware entity lookups x2 --&gt; 
&lt;h2&gt;Filling out random contact us forms&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://app.skyvern.com/tasks/create/contact_us_forms"&gt;ğŸ’¡ See it in action&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/contact_forms.gif" /&gt; &lt;/p&gt; 
&lt;h2&gt;Retrieving insurance quotes from insurance providers in any language&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://app.skyvern.com/tasks/create/bci_seguros"&gt;ğŸ’¡ See it in action&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/bci_seguros_recording.gif" /&gt; &lt;/p&gt; 
&lt;p&gt;&lt;a href="https://app.skyvern.com/tasks/create/geico"&gt;ğŸ’¡ See it in action&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/fern/images/geico_shu_recording_cropped.gif" /&gt; &lt;/p&gt; 
&lt;h1&gt;Contributor Setup&lt;/h1&gt; 
&lt;p&gt;Make sure to have &lt;a href="https://docs.astral.sh/uv/getting-started/installation/"&gt;uv&lt;/a&gt; installed.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Run this to create your virtual environment (&lt;code&gt;.venv&lt;/code&gt;) &lt;pre&gt;&lt;code class="language-bash"&gt;uv sync --group dev
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Perform initial server configuration &lt;pre&gt;&lt;code class="language-bash"&gt;uv run skyvern quickstart
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Navigate to &lt;code&gt;http://localhost:8080&lt;/code&gt; in your browser to start using the UI &lt;em&gt;The Skyvern CLI supports Windows, WSL, macOS, and Linux environments.&lt;/em&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h1&gt;Documentation&lt;/h1&gt; 
&lt;p&gt;More extensive documentation can be found on our &lt;a href="https://www.skyvern.com/docs"&gt;ğŸ“• docs page&lt;/a&gt;. Please let us know if something is unclear or missing by opening an issue or reaching out to us &lt;a href="mailto:founders@skyvern.com"&gt;via email&lt;/a&gt; or &lt;a href="https://discord.gg/fG2XXEuQX3"&gt;discord&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Supported LLMs&lt;/h1&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Provider&lt;/th&gt; 
   &lt;th&gt;Supported Models&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenAI&lt;/td&gt; 
   &lt;td&gt;gpt4-turbo, gpt-4o, gpt-4o-mini&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Anthropic&lt;/td&gt; 
   &lt;td&gt;Claude 3 (Haiku, Sonnet, Opus), Claude 3.5 (Sonnet)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Azure OpenAI&lt;/td&gt; 
   &lt;td&gt;Any GPT models. Better performance with a multimodal llm (azure/gpt4-o)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;AWS Bedrock&lt;/td&gt; 
   &lt;td&gt;Anthropic Claude 3 (Haiku, Sonnet, Opus), Claude 3.5 (Sonnet)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Gemini&lt;/td&gt; 
   &lt;td&gt;Gemini 2.5 Pro and flash, Gemini 2.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ollama&lt;/td&gt; 
   &lt;td&gt;Run any locally hosted model via &lt;a href="https://github.com/ollama/ollama"&gt;Ollama&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenRouter&lt;/td&gt; 
   &lt;td&gt;Access models through &lt;a href="https://openrouter.ai"&gt;OpenRouter&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenAI-compatible&lt;/td&gt; 
   &lt;td&gt;Any custom API endpoint that follows OpenAI's API format (via &lt;a href="https://docs.litellm.ai/docs/providers/openai_compatible"&gt;liteLLM&lt;/a&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h4&gt;Environment Variables&lt;/h4&gt; 
&lt;h5&gt;OpenAI&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Sample Value&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ENABLE_OPENAI&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Register OpenAI models&lt;/td&gt; 
   &lt;td&gt;Boolean&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;OpenAI API Key&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;sk-1234567890&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_API_BASE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;OpenAI API Base, optional&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;https://openai.api.base&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_ORGANIZATION&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;OpenAI Organization ID, optional&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;your-org-id&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Recommended &lt;code&gt;LLM_KEY&lt;/code&gt;: &lt;code&gt;OPENAI_GPT4O&lt;/code&gt;, &lt;code&gt;OPENAI_GPT4O_MINI&lt;/code&gt;, &lt;code&gt;OPENAI_GPT4_1&lt;/code&gt;, &lt;code&gt;OPENAI_O4_MINI&lt;/code&gt;, &lt;code&gt;OPENAI_O3&lt;/code&gt;&lt;/p&gt; 
&lt;h5&gt;Anthropic&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Sample Value&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ENABLE_ANTHROPIC&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Register Anthropic models&lt;/td&gt; 
   &lt;td&gt;Boolean&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ANTHROPIC_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Anthropic API key&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;sk-1234567890&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Recommended&lt;code&gt;LLM_KEY&lt;/code&gt;: &lt;code&gt;ANTHROPIC_CLAUDE3.5_SONNET&lt;/code&gt;, &lt;code&gt;ANTHROPIC_CLAUDE3.7_SONNET&lt;/code&gt;, &lt;code&gt;ANTHROPIC_CLAUDE4_OPUS&lt;/code&gt;, &lt;code&gt;ANTHROPIC_CLAUDE4_SONNET&lt;/code&gt;&lt;/p&gt; 
&lt;h5&gt;Azure OpenAI&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Sample Value&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ENABLE_AZURE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Register Azure OpenAI models&lt;/td&gt; 
   &lt;td&gt;Boolean&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;AZURE_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Azure deployment API key&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;sk-1234567890&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;AZURE_DEPLOYMENT&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Azure OpenAI Deployment Name&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;skyvern-deployment&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;AZURE_API_BASE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Azure deployment api base url&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;https://skyvern-deployment.openai.azure.com/&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;AZURE_API_VERSION&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Azure API Version&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;2024-02-01&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Recommended &lt;code&gt;LLM_KEY&lt;/code&gt;: &lt;code&gt;AZURE_OPENAI&lt;/code&gt;&lt;/p&gt; 
&lt;h5&gt;AWS Bedrock&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Sample Value&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ENABLE_BEDROCK&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Register AWS Bedrock models. To use AWS Bedrock, you need to make sure your &lt;a href="https://github.com/boto/boto3?tab=readme-ov-file#using-boto3"&gt;AWS configurations&lt;/a&gt; are set up correctly first.&lt;/td&gt; 
   &lt;td&gt;Boolean&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Recommended &lt;code&gt;LLM_KEY&lt;/code&gt;: &lt;code&gt;BEDROCK_ANTHROPIC_CLAUDE3.7_SONNET_INFERENCE_PROFILE&lt;/code&gt;, &lt;code&gt;BEDROCK_ANTHROPIC_CLAUDE4_OPUS_INFERENCE_PROFILE&lt;/code&gt;, &lt;code&gt;BEDROCK_ANTHROPIC_CLAUDE4_SONNET_INFERENCE_PROFILE&lt;/code&gt;&lt;/p&gt; 
&lt;h5&gt;Gemini&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Sample Value&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ENABLE_GEMINI&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Register Gemini models&lt;/td&gt; 
   &lt;td&gt;Boolean&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;GEMINI_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Gemini API Key&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;your_google_gemini_api_key&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Recommended &lt;code&gt;LLM_KEY&lt;/code&gt;: &lt;code&gt;GEMINI_2.5_PRO_PREVIEW&lt;/code&gt;, &lt;code&gt;GEMINI_2.5_FLASH_PREVIEW&lt;/code&gt;&lt;/p&gt; 
&lt;h5&gt;Ollama&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Sample Value&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ENABLE_OLLAMA&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Register local models via Ollama&lt;/td&gt; 
   &lt;td&gt;Boolean&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OLLAMA_SERVER_URL&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;URL for your Ollama server&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;http://host.docker.internal:11434&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OLLAMA_MODEL&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Ollama model name to load&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;qwen2.5:7b-instruct&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Recommended &lt;code&gt;LLM_KEY&lt;/code&gt;: &lt;code&gt;OLLAMA&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Note: Ollama does not support vision yet.&lt;/p&gt; 
&lt;h5&gt;OpenRouter&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Sample Value&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ENABLE_OPENROUTER&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Register OpenRouter models&lt;/td&gt; 
   &lt;td&gt;Boolean&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENROUTER_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;OpenRouter API key&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;sk-1234567890&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENROUTER_MODEL&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;OpenRouter model name&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;mistralai/mistral-small-3.1-24b-instruct&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENROUTER_API_BASE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;OpenRouter API base URL&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;https://api.openrouter.ai/v1&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Recommended &lt;code&gt;LLM_KEY&lt;/code&gt;: &lt;code&gt;OPENROUTER&lt;/code&gt;&lt;/p&gt; 
&lt;h5&gt;OpenAI-Compatible&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Sample Value&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ENABLE_OPENAI_COMPATIBLE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Register a custom OpenAI-compatible API endpoint&lt;/td&gt; 
   &lt;td&gt;Boolean&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_COMPATIBLE_MODEL_NAME&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Model name for OpenAI-compatible endpoint&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;yi-34b&lt;/code&gt;, &lt;code&gt;gpt-3.5-turbo&lt;/code&gt;, &lt;code&gt;mistral-large&lt;/code&gt;, etc.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_COMPATIBLE_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;API key for OpenAI-compatible endpoint&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;sk-1234567890&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_COMPATIBLE_API_BASE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Base URL for OpenAI-compatible endpoint&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;https://api.together.xyz/v1&lt;/code&gt;, &lt;code&gt;http://localhost:8000/v1&lt;/code&gt;, etc.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_COMPATIBLE_API_VERSION&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;API version for OpenAI-compatible endpoint, optional&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;2023-05-15&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_COMPATIBLE_MAX_TOKENS&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Maximum tokens for completion, optional&lt;/td&gt; 
   &lt;td&gt;Integer&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;4096&lt;/code&gt;, &lt;code&gt;8192&lt;/code&gt;, etc.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_COMPATIBLE_TEMPERATURE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Temperature setting, optional&lt;/td&gt; 
   &lt;td&gt;Float&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;0.0&lt;/code&gt;, &lt;code&gt;0.5&lt;/code&gt;, &lt;code&gt;0.7&lt;/code&gt;, etc.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_COMPATIBLE_SUPPORTS_VISION&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Whether model supports vision, optional&lt;/td&gt; 
   &lt;td&gt;Boolean&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Supported LLM Key: &lt;code&gt;OPENAI_COMPATIBLE&lt;/code&gt;&lt;/p&gt; 
&lt;h5&gt;General LLM Configuration&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Sample Value&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;LLM_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;The name of the model you want to use&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;See supported LLM keys above&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;SECONDARY_LLM_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;The name of the model for mini agents skyvern runs with&lt;/td&gt; 
   &lt;td&gt;String&lt;/td&gt; 
   &lt;td&gt;See supported LLM keys above&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;LLM_CONFIG_MAX_TOKENS&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Override the max tokens used by the LLM&lt;/td&gt; 
   &lt;td&gt;Integer&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;128000&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;Feature Roadmap&lt;/h1&gt; 
&lt;p&gt;This is our planned roadmap for the next few months. If you have any suggestions or would like to see a feature added, please don't hesitate to reach out to us &lt;a href="mailto:founders@skyvern.com"&gt;via email&lt;/a&gt; or &lt;a href="https://discord.gg/fG2XXEuQX3"&gt;discord&lt;/a&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Open Source&lt;/strong&gt; - Open Source Skyvern's core codebase&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Workflow support&lt;/strong&gt; - Allow support to chain multiple Skyvern calls together&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Improved context&lt;/strong&gt; - Improve Skyvern's ability to understand content around interactable elements by introducing feeding relevant label context through the text prompt&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Cost Savings&lt;/strong&gt; - Improve Skyvern's stability and reduce the cost of running Skyvern by optimizing the context tree passed into Skyvern&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Self-serve UI&lt;/strong&gt; - Deprecate the Streamlit UI in favour of a React-based UI component that allows users to kick off new jobs in Skyvern&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Workflow UI Builder&lt;/strong&gt; - Introduce a UI to allow users to build and analyze workflows visually&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Chrome Viewport streaming&lt;/strong&gt; - Introduce a way to live-stream the Chrome viewport to the user's browser (as a part of the self-serve UI)&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Past Runs UI&lt;/strong&gt; - Deprecate the Streamlit UI in favour of a React-based UI that allows you to visualize past runs and their results&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Auto workflow builder ("Observer") mode&lt;/strong&gt; - Allow Skyvern to auto-generate workflows as it's navigating the web to make it easier to build new workflows&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Prompt Caching&lt;/strong&gt; - Introduce a caching layer to the LLM calls to dramatically reduce the cost of running Skyvern (memorize past actions and repeat them!)&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Web Evaluation Dataset&lt;/strong&gt; - Integrate Skyvern with public benchmark tests to track the quality of our models over time&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;strong&gt;Improved Debug mode&lt;/strong&gt; - Allow Skyvern to plan its actions and get "approval" before running them, allowing you to debug what it's doing and more easily iterate on the prompt&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;strong&gt;Chrome Extension&lt;/strong&gt; - Allow users to interact with Skyvern through a Chrome extension (incl voice mode, saving tasks, etc.)&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;strong&gt;Skyvern Action Recorder&lt;/strong&gt; - Allow Skyvern to watch a user complete a task and then automatically generate a workflow for it&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;strong&gt;Interactable Livestream&lt;/strong&gt; - Allow users to interact with the livestream in real-time to intervene when necessary (such as manually submitting sensitive forms)&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;strong&gt;Integrate LLM Observability tools&lt;/strong&gt; - Integrate LLM Observability tools to allow back-testing prompt changes with specific data sets + visualize the performance of Skyvern over time&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Langchain Integration&lt;/strong&gt; - Create langchain integration in langchain_community to use Skyvern as a "tool".&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Contributing&lt;/h1&gt; 
&lt;p&gt;We welcome PRs and suggestions! Don't hesitate to open a PR/issue or to reach out to us &lt;a href="mailto:founders@skyvern.com"&gt;via email&lt;/a&gt; or &lt;a href="https://discord.gg/fG2XXEuQX3"&gt;discord&lt;/a&gt;. Please have a look at our &lt;a href="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/CONTRIBUTING.md"&gt;contribution guide&lt;/a&gt; and &lt;a href="https://github.com/skyvern-ai/skyvern/issues?q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22"&gt;"Help Wanted" issues&lt;/a&gt; to get started!&lt;/p&gt; 
&lt;p&gt;If you want to chat with the skyvern repository to get a high level overview of how it is structured, how to build off it, and how to resolve usage questions, check out &lt;a href="https://sage.storia.ai?utm_source=github&amp;amp;utm_medium=referral&amp;amp;utm_campaign=skyvern-readme"&gt;Code Sage&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Telemetry&lt;/h1&gt; 
&lt;p&gt;By Default, Skyvern collects basic usage statistics to help us understand how Skyvern is being used. If you would like to opt-out of telemetry, please set the &lt;code&gt;SKYVERN_TELEMETRY&lt;/code&gt; environment variable to &lt;code&gt;false&lt;/code&gt;.&lt;/p&gt; 
&lt;h1&gt;License&lt;/h1&gt; 
&lt;p&gt;Skyvern's open source repository is supported via a managed cloud. All of the core logic powering Skyvern is available in this open source repository licensed under the &lt;a href="https://raw.githubusercontent.com/Skyvern-AI/skyvern/main/LICENSE"&gt;AGPL-3.0 License&lt;/a&gt;, with the exception of anti-bot measures available in our managed cloud offering.&lt;/p&gt; 
&lt;p&gt;If you have any questions or concerns around licensing, please &lt;a href="mailto:support@skyvern.com"&gt;contact us&lt;/a&gt; and we would be happy to help.&lt;/p&gt; 
&lt;h1&gt;Star History&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#Skyvern-AI/skyvern&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=Skyvern-AI/skyvern&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>microsoft/TRELLIS</title>
      <link>https://github.com/microsoft/TRELLIS</link>
      <description>&lt;p&gt;Official repo for paper "Structured 3D Latents for Scalable and Versatile 3D Generation" (CVPR'25 Spotlight).&lt;/p&gt;&lt;hr&gt;&lt;img src="https://raw.githubusercontent.com/microsoft/TRELLIS/main/assets/logo.webp" width="100%" align="center" /&gt; 
&lt;h1 align="center"&gt;Structured 3D Latents&lt;br /&gt;for Scalable and Versatile 3D Generation&lt;/h1&gt; 
&lt;p align="center"&gt;&lt;a href="https://arxiv.org/abs/2412.01506"&gt;&lt;img src="https://img.shields.io/badge/arXiv-Paper-red?logo=arxiv&amp;amp;logoColor=white" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://microsoft.github.io/TRELLIS/"&gt;&lt;img src="https://img.shields.io/badge/Project_Page-Website-green?logo=googlechrome&amp;amp;logoColor=white" alt="Project Page" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/spaces/Microsoft/TRELLIS"&gt;&lt;img src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Live_Demo-blue" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt;&lt;img src="https://raw.githubusercontent.com/microsoft/TRELLIS/main/assets/teaser.png" width="100%" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span style="font-size: 16px; font-weight: 600;"&gt;T&lt;/span&gt;&lt;span style="font-size: 12px; font-weight: 700;"&gt;RELLIS&lt;/span&gt; is a large 3D asset generation model. It takes in text or image prompts and generates high-quality 3D assets in various formats, such as Radiance Fields, 3D Gaussians, and meshes. The cornerstone of &lt;span style="font-size: 16px; font-weight: 600;"&gt;T&lt;/span&gt;&lt;span style="font-size: 12px; font-weight: 700;"&gt;RELLIS&lt;/span&gt; is a unified Structured LATent (&lt;span style="font-size: 16px; font-weight: 600;"&gt;SL&lt;/span&gt;&lt;span style="font-size: 12px; font-weight: 700;"&gt;AT&lt;/span&gt;) representation that allows decoding to different output formats and Rectified Flow Transformers tailored for &lt;span style="font-size: 16px; font-weight: 600;"&gt;SL&lt;/span&gt;&lt;span style="font-size: 12px; font-weight: 700;"&gt;AT&lt;/span&gt; as the powerful backbones. We provide large-scale pre-trained models with up to 2 billion parameters on a large 3D asset dataset of 500K diverse objects. &lt;span style="font-size: 16px; font-weight: 600;"&gt;T&lt;/span&gt;&lt;span style="font-size: 12px; font-weight: 700;"&gt;RELLIS&lt;/span&gt; significantly surpasses existing methods, including recent ones at similar scales, and showcases flexible output format selection and local 3D editing capabilities which were not offered by previous models.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Check out our &lt;a href="https://microsoft.github.io/TRELLIS/"&gt;Project Page&lt;/a&gt; for more videos and interactive demos!&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;!-- Features --&gt; 
&lt;h2&gt;ğŸŒŸ Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;High Quality&lt;/strong&gt;: It produces diverse 3D assets at high quality with intricate shape and texture details.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Versatility&lt;/strong&gt;: It takes text or image prompts and can generate various final 3D representations including but not limited to &lt;em&gt;Radiance Fields&lt;/em&gt;, &lt;em&gt;3D Gaussians&lt;/em&gt;, and &lt;em&gt;meshes&lt;/em&gt;, accommodating diverse downstream requirements.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Flexible Editing&lt;/strong&gt;: It allows for easy editings of generated 3D assets, such as generating variants of the same object or local editing of the 3D asset.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- Updates --&gt; 
&lt;h2&gt;â© Updates&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;03/25/2025&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Release training code.&lt;/li&gt; 
 &lt;li&gt;Release &lt;strong&gt;TRELLIS-text&lt;/strong&gt; models and asset variants generation. 
  &lt;ul&gt; 
   &lt;li&gt;Examples are provided as &lt;a href="https://raw.githubusercontent.com/microsoft/TRELLIS/main/example_text.py"&gt;example_text.py&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/microsoft/TRELLIS/main/example_variant.py"&gt;example_variant.py&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;Gradio demo is provided as &lt;a href="https://raw.githubusercontent.com/microsoft/TRELLIS/main/app_text.py"&gt;app_text.py&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;&lt;em&gt;Note: It is always recommended to do text to 3D generation by first generating images using text-to-image models and then using TRELLIS-image models for 3D generation. Text-conditioned models are less creative and detailed due to data limitations.&lt;/em&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;12/26/2024&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Release &lt;a href="https://github.com/microsoft/TRELLIS#-dataset"&gt;&lt;strong&gt;TRELLIS-500K&lt;/strong&gt;&lt;/a&gt; dataset and toolkits for data preparation.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;12/18/2024&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Implementation of multi-image conditioning for &lt;strong&gt;TRELLIS-image&lt;/strong&gt; model. (&lt;a href="https://github.com/microsoft/TRELLIS/issues/7"&gt;#7&lt;/a&gt;). This is based on tuning-free algorithm without training a specialized model, so it may not give the best results for all input images.&lt;/li&gt; 
 &lt;li&gt;Add Gaussian export in &lt;code&gt;app.py&lt;/code&gt; and &lt;code&gt;example.py&lt;/code&gt;. (&lt;a href="https://github.com/microsoft/TRELLIS/issues/40"&gt;#40&lt;/a&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- Installation --&gt; 
&lt;h2&gt;ğŸ“¦ Installation&lt;/h2&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;System&lt;/strong&gt;: The code is currently tested only on &lt;strong&gt;Linux&lt;/strong&gt;. For windows setup, you may refer to &lt;a href="https://github.com/microsoft/TRELLIS/issues/3"&gt;#3&lt;/a&gt; (not fully tested).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Hardware&lt;/strong&gt;: An NVIDIA GPU with at least 16GB of memory is necessary. The code has been verified on NVIDIA A100 and A6000 GPUs.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Software&lt;/strong&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;The &lt;a href="https://developer.nvidia.com/cuda-toolkit-archive"&gt;CUDA Toolkit&lt;/a&gt; is needed to compile certain submodules. The code has been tested with CUDA versions 11.8 and 12.2.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://docs.anaconda.com/miniconda/install/#quick-command-line-install"&gt;Conda&lt;/a&gt; is recommended for managing dependencies.&lt;/li&gt; 
   &lt;li&gt;Python version 3.8 or higher is required.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Installation Steps&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Clone the repo:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;git clone --recurse-submodules https://github.com/microsoft/TRELLIS.git
cd TRELLIS
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install the dependencies:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Before running the following command there are somethings to note:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;By adding &lt;code&gt;--new-env&lt;/code&gt;, a new conda environment named &lt;code&gt;trellis&lt;/code&gt; will be created. If you want to use an existing conda environment, please remove this flag.&lt;/li&gt; 
   &lt;li&gt;By default the &lt;code&gt;trellis&lt;/code&gt; environment will use pytorch 2.4.0 with CUDA 11.8. If you want to use a different version of CUDA (e.g., if you have CUDA Toolkit 12.2 installed and do not want to install another 11.8 version for submodule compilation), you can remove the &lt;code&gt;--new-env&lt;/code&gt; flag and manually install the required dependencies. Refer to &lt;a href="https://pytorch.org/get-started/previous-versions/"&gt;PyTorch&lt;/a&gt; for the installation command.&lt;/li&gt; 
   &lt;li&gt;If you have multiple CUDA Toolkit versions installed, &lt;code&gt;PATH&lt;/code&gt; should be set to the correct version before running the command. For example, if you have CUDA Toolkit 11.8 and 12.2 installed, you should run &lt;code&gt;export PATH=/usr/local/cuda-11.8/bin:$PATH&lt;/code&gt; before running the command.&lt;/li&gt; 
   &lt;li&gt;By default, the code uses the &lt;code&gt;flash-attn&lt;/code&gt; backend for attention. For GPUs do not support &lt;code&gt;flash-attn&lt;/code&gt; (e.g., NVIDIA V100), you can remove the &lt;code&gt;--flash-attn&lt;/code&gt; flag to install &lt;code&gt;xformers&lt;/code&gt; only and set the &lt;code&gt;ATTN_BACKEND&lt;/code&gt; environment variable to &lt;code&gt;xformers&lt;/code&gt; before running the code. See the &lt;a href="https://raw.githubusercontent.com/microsoft/TRELLIS/main/#minimal-example"&gt;Minimal Example&lt;/a&gt; for more details.&lt;/li&gt; 
   &lt;li&gt;The installation may take a while due to the large number of dependencies. Please be patient. If you encounter any issues, you can try to install the dependencies one by one, specifying one flag at a time.&lt;/li&gt; 
   &lt;li&gt;If you encounter any issues during the installation, feel free to open an issue or contact us.&lt;/li&gt; 
  &lt;/ul&gt; &lt;p&gt;Create a new conda environment named &lt;code&gt;trellis&lt;/code&gt; and install the dependencies:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;. ./setup.sh --new-env --basic --xformers --flash-attn --diffoctreerast --spconv --mipgaussian --kaolin --nvdiffrast
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The detailed usage of &lt;code&gt;setup.sh&lt;/code&gt; can be found by running &lt;code&gt;. ./setup.sh --help&lt;/code&gt;.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;Usage: setup.sh [OPTIONS]
Options:
    -h, --help              Display this help message
    --new-env               Create a new conda environment
    --basic                 Install basic dependencies
    --train                 Install training dependencies
    --xformers              Install xformers
    --flash-attn            Install flash-attn
    --diffoctreerast        Install diffoctreerast
    --spconv                Install spconv
    --mipgaussian           Install mip-splatting
    --kaolin                Install kaolin
    --nvdiffrast            Install nvdiffrast
    --demo                  Install all dependencies for demo
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;!-- Pretrained Models --&gt; 
&lt;h2&gt;ğŸ¤– Pretrained Models&lt;/h2&gt; 
&lt;p&gt;We provide the following pretrained models:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;#Params&lt;/th&gt; 
   &lt;th&gt;Download&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;TRELLIS-image-large&lt;/td&gt; 
   &lt;td&gt;Large image-to-3D model&lt;/td&gt; 
   &lt;td&gt;1.2B&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/microsoft/TRELLIS-image-large"&gt;Download&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;TRELLIS-text-base&lt;/td&gt; 
   &lt;td&gt;Base text-to-3D model&lt;/td&gt; 
   &lt;td&gt;342M&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/microsoft/TRELLIS-text-base"&gt;Download&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;TRELLIS-text-large&lt;/td&gt; 
   &lt;td&gt;Large text-to-3D model&lt;/td&gt; 
   &lt;td&gt;1.1B&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/microsoft/TRELLIS-text-large"&gt;Download&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;TRELLIS-text-xlarge&lt;/td&gt; 
   &lt;td&gt;Extra-large text-to-3D model&lt;/td&gt; 
   &lt;td&gt;2.0B&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/microsoft/TRELLIS-text-xlarge"&gt;Download&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;em&gt;Note: It is always recommended to use the image conditioned version of the models for better performance.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Note: All VAEs are included in &lt;strong&gt;TRELLIS-image-large&lt;/strong&gt; model repo.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;The models are hosted on Hugging Face. You can directly load the models with their repository names in the code:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;TrellisImageTo3DPipeline.from_pretrained("microsoft/TRELLIS-image-large")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you prefer loading the model from local, you can download the model files from the links above and load the model with the folder path (folder structure should be maintained):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;TrellisImageTo3DPipeline.from_pretrained("/path/to/TRELLIS-image-large")
&lt;/code&gt;&lt;/pre&gt; 
&lt;!-- Usage --&gt; 
&lt;h2&gt;ğŸ’¡ Usage&lt;/h2&gt; 
&lt;h3&gt;Minimal Example&lt;/h3&gt; 
&lt;p&gt;Here is an &lt;a href="https://raw.githubusercontent.com/microsoft/TRELLIS/main/example.py"&gt;example&lt;/a&gt; of how to use the pretrained models for 3D asset generation.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import os
# os.environ['ATTN_BACKEND'] = 'xformers'   # Can be 'flash-attn' or 'xformers', default is 'flash-attn'
os.environ['SPCONV_ALGO'] = 'native'        # Can be 'native' or 'auto', default is 'auto'.
                                            # 'auto' is faster but will do benchmarking at the beginning.
                                            # Recommended to set to 'native' if run only once.

import imageio
from PIL import Image
from trellis.pipelines import TrellisImageTo3DPipeline
from trellis.utils import render_utils, postprocessing_utils

# Load a pipeline from a model folder or a Hugging Face model hub.
pipeline = TrellisImageTo3DPipeline.from_pretrained("microsoft/TRELLIS-image-large")
pipeline.cuda()

# Load an image
image = Image.open("assets/example_image/T.png")

# Run the pipeline
outputs = pipeline.run(
    image,
    seed=1,
    # Optional parameters
    # sparse_structure_sampler_params={
    #     "steps": 12,
    #     "cfg_strength": 7.5,
    # },
    # slat_sampler_params={
    #     "steps": 12,
    #     "cfg_strength": 3,
    # },
)
# outputs is a dictionary containing generated 3D assets in different formats:
# - outputs['gaussian']: a list of 3D Gaussians
# - outputs['radiance_field']: a list of radiance fields
# - outputs['mesh']: a list of meshes

# Render the outputs
video = render_utils.render_video(outputs['gaussian'][0])['color']
imageio.mimsave("sample_gs.mp4", video, fps=30)
video = render_utils.render_video(outputs['radiance_field'][0])['color']
imageio.mimsave("sample_rf.mp4", video, fps=30)
video = render_utils.render_video(outputs['mesh'][0])['normal']
imageio.mimsave("sample_mesh.mp4", video, fps=30)

# GLB files can be extracted from the outputs
glb = postprocessing_utils.to_glb(
    outputs['gaussian'][0],
    outputs['mesh'][0],
    # Optional parameters
    simplify=0.95,          # Ratio of triangles to remove in the simplification process
    texture_size=1024,      # Size of the texture used for the GLB
)
glb.export("sample.glb")

# Save Gaussians as PLY files
outputs['gaussian'][0].save_ply("sample.ply")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After running the code, you will get the following files:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;sample_gs.mp4&lt;/code&gt;: a video showing the 3D Gaussian representation&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;sample_rf.mp4&lt;/code&gt;: a video showing the Radiance Field representation&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;sample_mesh.mp4&lt;/code&gt;: a video showing the mesh representation&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;sample.glb&lt;/code&gt;: a GLB file containing the extracted textured mesh&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;sample.ply&lt;/code&gt;: a PLY file containing the 3D Gaussian representation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Web Demo&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/TRELLIS/main/app.py"&gt;app.py&lt;/a&gt; provides a simple web demo for 3D asset generation. Since this demo is based on &lt;a href="https://gradio.app/"&gt;Gradio&lt;/a&gt;, additional dependencies are required:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;. ./setup.sh --demo
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After installing the dependencies, you can run the demo with the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python app.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then, you can access the demo at the address shown in the terminal.&lt;/p&gt; 
&lt;!-- Dataset --&gt; 
&lt;h2&gt;ğŸ“š Dataset&lt;/h2&gt; 
&lt;p&gt;We provide &lt;strong&gt;TRELLIS-500K&lt;/strong&gt;, a large-scale dataset containing 500K 3D assets curated from &lt;a href="https://objaverse.allenai.org/"&gt;Objaverse(XL)&lt;/a&gt;, &lt;a href="https://amazon-berkeley-objects.s3.amazonaws.com/index.html"&gt;ABO&lt;/a&gt;, &lt;a href="https://tianchi.aliyun.com/specials/promotion/alibaba-3d-future"&gt;3D-FUTURE&lt;/a&gt;, &lt;a href="https://huggingface.co/datasets/hssd/hssd-models"&gt;HSSD&lt;/a&gt;, and &lt;a href="https://github.com/rehg-lab/lowshot-shapebias/tree/main/toys4k"&gt;Toys4k&lt;/a&gt;, filtered based on aesthetic scores. Please refer to the &lt;a href="https://raw.githubusercontent.com/microsoft/TRELLIS/main/DATASET.md"&gt;dataset README&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;!-- Training --&gt; 
&lt;h2&gt;ğŸ‹ï¸â€â™‚ï¸ Training&lt;/h2&gt; 
&lt;p&gt;TRELLISâ€™s training framework is organized to provide a flexible and modular approach to building and fine-tuning large-scale 3D generation models. The training code is centered around &lt;code&gt;train.py&lt;/code&gt; and is structured into several directories to clearly separate dataset handling, model components, training logic, and visualization utilities.&lt;/p&gt; 
&lt;h3&gt;Code Structure&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;train.py&lt;/strong&gt;: Main entry point for training.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;trellis/datasets&lt;/strong&gt;: Dataset loading and preprocessing.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;trellis/models&lt;/strong&gt;: Different models and their components.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;trellis/modules&lt;/strong&gt;: Custom modules for various models.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;trellis/pipelines&lt;/strong&gt;: Inference pipelines for different models.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;trellis/renderers&lt;/strong&gt;: Renderers for different 3D representations.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;trellis/representations&lt;/strong&gt;: Different 3D representations.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;trellis/trainers&lt;/strong&gt;: Training logic for different models.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;trellis/utils&lt;/strong&gt;: Utility functions for training and visualization.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Training Setup&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Prepare the Environment:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Ensure all training dependencies are installed.&lt;/li&gt; 
   &lt;li&gt;Use a Linux system with an NVIDIA GPU (The models are trained on NVIDIA A100 GPUs).&lt;/li&gt; 
   &lt;li&gt;For distributed training, verify that your nodes can communicate through the designated master address and port.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Dataset Preparation:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Organize your dataset similar to TRELLIS-500K. Specify your dataset path using the &lt;code&gt;--data_dir&lt;/code&gt; argument when launching training.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Configuration Files:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Training hyperparameters and model architectures are defined in configuration files under the &lt;code&gt;configs/&lt;/code&gt; directory.&lt;/li&gt; 
   &lt;li&gt;Example configuration files include:&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Config&lt;/th&gt; 
   &lt;th&gt;Pretained Model&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/TRELLIS/main/configs/vae/ss_vae_conv3d_16l8_fp16.json"&gt;&lt;code&gt;vae/ss_vae_conv3d_16l8_fp16.json&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/microsoft/TRELLIS-image-large/blob/main/ckpts/ss_enc_conv3d_16l8_fp16.safetensors"&gt;Encoder&lt;/a&gt; &lt;a href="https://huggingface.co/microsoft/TRELLIS-image-large/blob/main/ckpts/ss_dec_conv3d_16l8_fp16.safetensors"&gt;Decoder&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Sparse structure VAE&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/TRELLIS/main/configs/vae/slat_vae_enc_dec_gs_swin8_B_64l8_fp16.json"&gt;&lt;code&gt;vae/slat_vae_enc_dec_gs_swin8_B_64l8_fp16.json&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/microsoft/TRELLIS-image-large/blob/main/ckpts/slat_enc_swin8_B_64l8_fp16.safetensors"&gt;Encoder&lt;/a&gt; &lt;a href="https://huggingface.co/microsoft/TRELLIS-image-large/blob/main/ckpts/slat_dec_gs_swin8_B_64l8gs32_fp16.safetensors"&gt;Decoder&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;SLat VAE with Gaussian Decoder&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/TRELLIS/main/configs/vae/slat_vae_dec_rf_swin8_B_64l8_fp16.json"&gt;&lt;code&gt;vae/slat_vae_dec_rf_swin8_B_64l8_fp16.json&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/microsoft/TRELLIS-image-large/blob/main/ckpts/slat_dec_rf_swin8_B_64l8r16_fp16.safetensors"&gt;Decoder&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;SLat Radiance Field Decoder&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/TRELLIS/main/configs/vae/slat_vae_dec_mesh_swin8_B_64l8_fp16.json"&gt;&lt;code&gt;vae/slat_vae_dec_mesh_swin8_B_64l8_fp16.json&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/microsoft/TRELLIS-image-large/blob/main/ckpts/slat_dec_mesh_swin8_B_64l8m256c_fp16.safetensors"&gt;Decoder&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;SLat Mesh Decoder&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/TRELLIS/main/configs/generation/ss_flow_img_dit_L_16l8_fp16.json"&gt;&lt;code&gt;generation/ss_flow_img_dit_L_16l8_fp16.json&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/microsoft/TRELLIS-image-large/blob/main/ckpts/ss_flow_img_dit_L_16l8_fp16.safetensors"&gt;Denoiser&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Image conditioned sparse structure Flow Model&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/TRELLIS/main/configs/generation/slat_flow_img_dit_L_64l8p2_fp16.json"&gt;&lt;code&gt;generation/slat_flow_img_dit_L_64l8p2_fp16.json&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/microsoft/TRELLIS-image-large/blob/main/ckpts/slat_flow_img_dit_L_64l8p2_fp16.safetensors"&gt;Denoiser&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Image conditioned SLat Flow Model&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/TRELLIS/main/configs/generation/ss_flow_txt_dit_B_16l8_fp16.json"&gt;&lt;code&gt;generation/ss_flow_txt_dit_B_16l8_fp16.json&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/microsoft/TRELLIS-text-base/blob/main/ckpts/ss_flow_txt_dit_B_16l8_fp16.safetensors"&gt;Denoiser&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Base text-conditioned sparse structure Flow Model&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/TRELLIS/main/configs/generation/slat_flow_txt_dit_B_64l8p2_fp16.json"&gt;&lt;code&gt;generation/slat_flow_txt_dit_B_64l8p2_fp16.json&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/microsoft/TRELLIS-text-base/blob/main/ckpts/slat_flow_txt_dit_B_64l8p2_fp16.safetensors"&gt;Denoiser&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Base text-conditioned SLat Flow Model&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/TRELLIS/main/configs/generation/ss_flow_txt_dit_L_16l8_fp16.json"&gt;&lt;code&gt;generation/ss_flow_txt_dit_L_16l8_fp16.json&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/microsoft/TRELLIS-text-large/blob/main/ckpts/ss_flow_txt_dit_L_16l8_fp16.safetensors"&gt;Denoiser&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Large text-conditioned sparse structure Flow Model&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/TRELLIS/main/configs/generation/slat_flow_txt_dit_L_64l8p2_fp16.json"&gt;&lt;code&gt;generation/slat_flow_txt_dit_L_64l8p2_fp16.json&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/microsoft/TRELLIS-text-large/blob/main/ckpts/slat_flow_txt_dit_L_64l8p2_fp16.safetensors"&gt;Denoiser&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Large text-conditioned SLat Flow Model&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/TRELLIS/main/configs/generation/ss_flow_txt_dit_XL_16l8_fp16.json"&gt;&lt;code&gt;generation/ss_flow_txt_dit_XL_16l8_fp16.json&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/microsoft/TRELLIS-text-xlarge/blob/main/ckpts/ss_flow_txt_dit_XL_16l8_fp16.safetensors"&gt;Denoiser&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Extra-large text-conditioned sparse structure Flow Model&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/TRELLIS/main/configs/generation/slat_flow_txt_dit_XL_64l8p2_fp16.json"&gt;&lt;code&gt;generation/slat_flow_txt_dit_XL_64l8p2_fp16.json&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/microsoft/TRELLIS-text-xlarge/blob/main/ckpts/slat_flow_txt_dit_XL_64l8p2_fp16.safetensors"&gt;Denoiser&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Extra-large text-conditioned SLat Flow Model&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Command-Line Options&lt;/h3&gt; 
&lt;p&gt;The training script can be run as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;usage: train.py [-h] --config CONFIG --output_dir OUTPUT_DIR [--load_dir LOAD_DIR] [--ckpt CKPT] [--data_dir DATA_DIR] [--auto_retry AUTO_RETRY] [--tryrun] [--profile] [--num_nodes NUM_NODES] [--node_rank NODE_RANK] [--num_gpus NUM_GPUS] [--master_addr MASTER_ADDR] [--master_port MASTER_PORT]

options:
  -h, --help                    show this help message and exit
  --config CONFIG               Experiment config file
  --output_dir OUTPUT_DIR       Output directory
  --load_dir LOAD_DIR           Load directory, default to output_dir
  --ckpt CKPT                   Checkpoint step to resume training, default to latest
  --data_dir DATA_DIR           Data directory
  --auto_retry AUTO_RETRY       Number of retries on error
  --tryrun                      Try run without training
  --profile                     Profile training
  --num_nodes NUM_NODES         Number of nodes
  --node_rank NODE_RANK         Node rank
  --num_gpus NUM_GPUS           Number of GPUs per node, default to all
  --master_addr MASTER_ADDR     Master address for distributed training
  --master_port MASTER_PORT     Port for distributed training
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Example Training Commands&lt;/h3&gt; 
&lt;h4&gt;Single-node Training&lt;/h4&gt; 
&lt;p&gt;To train a image-to-3D stage 2 model with a single machine.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python train.py \
  --config configs/vae/slat_vae_dec_mesh_swin8_B_64l8_fp16.json \
  --output_dir outputs/slat_vae_dec_mesh_swin8_B_64l8_fp16_1node \
  --data_dir /path/to/your/dataset1,/path/to/your/dataset2 \
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The script will automatically distribute the training across all available GPUs. Specify the number of GPUs with the &lt;code&gt;--num_gpus&lt;/code&gt; flag if you want to limit the number of GPUs used.&lt;/p&gt; 
&lt;h4&gt;Multi-node Training&lt;/h4&gt; 
&lt;p&gt;To train a image-to-3D stage 2 model with multiple GPUs across nodes (e.g., 2 nodes):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python train.py \
  --config configs/generation/slat_flow_img_dit_L_64l8p2_fp16.json \
  --output_dir outputs/slat_flow_img_dit_L_64l8p2_fp16_2nodes \
  --data_dir /path/to/your/dataset1,/path/to/your/dataset2 \
  --num_nodes 2 \
  --node_rank 0 \
  --master_addr $MASTER_ADDR \
  --master_port $MASTER_PORT
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Be sure to adjust &lt;code&gt;node_rank&lt;/code&gt;, &lt;code&gt;master_addr&lt;/code&gt;, and &lt;code&gt;master_port&lt;/code&gt; for each node accordingly.&lt;/p&gt; 
&lt;h4&gt;Resuming Training&lt;/h4&gt; 
&lt;p&gt;By default, training will resume from the latest saved checkpoint in the same output directory. To specify a specific checkpoint to resume from, use the &lt;code&gt;--load_dir&lt;/code&gt; and &lt;code&gt;--ckpt&lt;/code&gt; flags:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python train.py \
  --config configs/generation/slat_flow_img_dit_L_64l8p2_fp16.json \
  --output_dir outputs/slat_flow_img_dit_L_64l8p2_fp16_resume \
  --data_dir /path/to/your/dataset1,/path/to/your/dataset2 \
  --load_dir /path/to/your/checkpoint \
  --ckpt [step]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Additional Options&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Auto Retry:&lt;/strong&gt; Use the &lt;code&gt;--auto_retry&lt;/code&gt; flag to specify the number of retries in case of intermittent errors.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Dry Run:&lt;/strong&gt; The &lt;code&gt;--tryrun&lt;/code&gt; flag allows you to check your configuration and environment without launching full training.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Profiling:&lt;/strong&gt; Enable profiling with the &lt;code&gt;--profile&lt;/code&gt; flag to gain insights into training performance and diagnose bottlenecks.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Adjust the file paths and parameters to match your experimental setup.&lt;/p&gt; 
&lt;!-- License --&gt; 
&lt;h2&gt;âš–ï¸ License&lt;/h2&gt; 
&lt;p&gt;TRELLIS models and the majority of the code are licensed under the &lt;a href="https://raw.githubusercontent.com/microsoft/TRELLIS/main/LICENSE"&gt;MIT License&lt;/a&gt;. The following submodules may have different licenses:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/JeffreyXiang/diffoctreerast"&gt;&lt;strong&gt;diffoctreerast&lt;/strong&gt;&lt;/a&gt;: We developed a CUDA-based real-time differentiable octree renderer for rendering radiance fields as part of this project. This renderer is derived from the &lt;a href="https://github.com/graphdeco-inria/diff-gaussian-rasterization"&gt;diff-gaussian-rasterization&lt;/a&gt; project and is available under the &lt;a href="https://github.com/JeffreyXiang/diffoctreerast/raw/master/LICENSE"&gt;LICENSE&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/MaxtirError/FlexiCubes"&gt;&lt;strong&gt;Modified Flexicubes&lt;/strong&gt;&lt;/a&gt;: In this project, we used a modified version of &lt;a href="https://github.com/nv-tlabs/FlexiCubes"&gt;Flexicubes&lt;/a&gt; to support vertex attributes. This modified version is licensed under the &lt;a href="https://github.com/nv-tlabs/FlexiCubes/raw/main/LICENSE.txt"&gt;LICENSE&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- Citation --&gt; 
&lt;h2&gt;ğŸ“œ Citation&lt;/h2&gt; 
&lt;p&gt;If you find this work helpful, please consider citing our paper:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@article{xiang2024structured,
    title   = {Structured 3D Latents for Scalable and Versatile 3D Generation},
    author  = {Xiang, Jianfeng and Lv, Zelong and Xu, Sicheng and Deng, Yu and Wang, Ruicheng and Zhang, Bowen and Chen, Dong and Tong, Xin and Yang, Jiaolong},
    journal = {arXiv preprint arXiv:2412.01506},
    year    = {2024}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>TNTwise/REAL-Video-Enhancer</title>
      <link>https://github.com/TNTwise/REAL-Video-Enhancer</link>
      <description>&lt;p&gt;Interpolate, Upscale, Decompress, and Denoise videos easily on Linux/Windows/MacOS.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;REAL Video Enhancer&lt;/h1&gt; 
&lt;p&gt;&lt;img src="https://api.visitorbadge.io/api/visitors?path=https%3A%2F%2Fgithub.com%2FTNTwise%2FREAL-Video-enhancer%2F&amp;amp;countColor=%23263759" alt="Visitors" /&gt; &lt;a href="https://github.com/qwertyquerty/pypresence"&gt;&lt;img src="https://img.shields.io/badge/using-pypresence-00bb88.svg?style=for-the-badge&amp;amp;logo=discord&amp;amp;logoWidth=20" alt="pypresence" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://img.shields.io/github/license/tntwise/real-video-enhancer" alt="license" /&gt; &lt;img src="https://img.shields.io/badge/Version-2.3.8-blue" alt="Version" /&gt; &lt;img src="https://img.shields.io/github/downloads/tntwise/REAL-Video-Enhancer/total.svg?label=downloads%40total" alt="downloads_total" /&gt; &lt;a href="https://discord.gg/hwGHXga8ck"&gt; &lt;img src="https://img.shields.io/discord/1041502781808328704?label=Discord" alt="Discord Shield" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;a href="https://flathub.org/apps/io.github.tntwise.REAL-Video-Enhancer"&gt; &lt;img src="https://dl.flathub.org/assets/badges/flathub-badge-en.svg?sanitize=true" height="50px" /&gt; &lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://github.com/TNTwise/REAL-Video-Enhancer/raw/2.0/icons/logo-v2.svg?sanitize=true" width="25%" /&gt; &lt;/p&gt; 
&lt;h1&gt;Table of Contents&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/TNTwise/REAL-Video-Enhancer/2.3.9/#introduction"&gt;Introduction&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/TNTwise/REAL-Video-Enhancer/2.3.9/#Features"&gt;Features&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/TNTwise/REAL-Video-Enhancer/2.3.9/#hardware-requirements"&gt;Hardware Requirements&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/TNTwise/REAL-Video-Enhancer/2.3.9/#models"&gt;Models&lt;/a&gt;&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/TNTwise/REAL-Video-Enhancer/2.3.9/#interpolate-models"&gt;Interpolate Models&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/TNTwise/REAL-Video-Enhancer/2.3.9/#upscale-models"&gt;Upscale Models&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/TNTwise/REAL-Video-Enhancer/2.3.9/#backends"&gt;Backends&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/TNTwise/REAL-Video-Enhancer/2.3.9/#faq"&gt;FAQ&lt;/a&gt;&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/TNTwise/REAL-Video-Enhancer/2.3.9/#general-application-usage"&gt;General App Usage&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/TNTwise/REAL-Video-Enhancer/2.3.9/#tensorrt-related-questions"&gt;TensorRT&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/TNTwise/REAL-Video-Enhancer/2.3.9/#rocm-related-questions"&gt;ROCm&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/TNTwise/REAL-Video-Enhancer/2.3.9/#ncnn-related-questions"&gt;NCNN&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/TNTwise/REAL-Video-Enhancer/2.3.9/#cloning"&gt;Cloning&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/TNTwise/REAL-Video-Enhancer/2.3.9/#building"&gt;Building&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/TNTwise/REAL-Video-Enhancer/2.3.9/#colab-notebook"&gt;Colab Notebook&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/TNTwise/REAL-Video-Enhancer/2.3.9/#credits"&gt;Credits&lt;/a&gt;&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/TNTwise/REAL-Video-Enhancer/2.3.9/#people"&gt;People&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/TNTwise/REAL-Video-Enhancer/2.3.9/#software"&gt;Software&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Introduction&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;REAL Video Enhancer&lt;/strong&gt; is a redesigned and enhanced version of the original Rife ESRGAN App for Linux. This program offers convenient access to frame interpolation and upscaling functionalities on Windows, Linux and MacOS, and is an alternative to outdated software like &lt;a rel="noopener noreferrer" href="https://nmkd.itch.io/flowframes" target="_blank"&gt;Flowframes&lt;/a&gt; or &lt;a rel="noopener noreferrer" href="https://github.com/mafiosnik777/enhancr" target="_blank"&gt;enhancr&lt;/a&gt;.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://github.com/TNTwise/REAL-Video-Enhancer/raw/v2-main/screenshots/demo.png?raw=true" width="100%" /&gt; &lt;/p&gt; 
&lt;h1&gt;Features: &lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt; Windows support. &lt;strong&gt;!!! NOTICE !!!&lt;/strong&gt; The bin can be detected as a trojan. This is a false positive caused by pyinstaller.&lt;/li&gt; 
 &lt;li&gt; Ubuntu 22.04+ support on Executable and Flatpak. (20.04 can work but is now legacy) &lt;/li&gt; 
 &lt;li&gt; MacOS 14+ arm/x86 support &lt;/li&gt; 
 &lt;li&gt; Discord RPC support for Discord system package and Discord flatpak. &lt;/li&gt; 
 &lt;li&gt; Scene change detection to preserve sharp transitions. &lt;/li&gt; 
 &lt;li&gt; Preview that shows latest frame that has been rendered. &lt;/li&gt; 
 &lt;li&gt; TensorRT and NCNN for efficient inference across many GPUs. &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Hardware/Software Requirements&lt;/h1&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th&gt;Minimum&lt;/th&gt; 
   &lt;th&gt;Recommended&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;CPU&lt;/td&gt; 
   &lt;td&gt;Dual Core x64 bit&lt;/td&gt; 
   &lt;td&gt;Quad Core x64 bit&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;GPU&lt;/td&gt; 
   &lt;td&gt;Vulkan 1.3 capable device&lt;/td&gt; 
   &lt;td&gt;Nvidia RTX GPU (20 series and up)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;VRAM&lt;/td&gt; 
   &lt;td&gt;4 GB - NCNN&lt;/td&gt; 
   &lt;td&gt;8 GB - TensorRT (Nvidia, why keep making 8gb cards?)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;RAM&lt;/td&gt; 
   &lt;td&gt;16 GB&lt;/td&gt; 
   &lt;td&gt;32 GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Storage&lt;/td&gt; 
   &lt;td&gt;1 GB free - NCNN&lt;/td&gt; 
   &lt;td&gt;16 GB free - TensorRT&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Operating System&lt;/td&gt; 
   &lt;td&gt;Windows 10/11 64bit / MacOS 14+&lt;/td&gt; 
   &lt;td&gt;Any modern Linux distro (Ubuntu 22.04+)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;Models:&lt;/h1&gt; 
&lt;h3&gt;Interpolate Models:&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Author&lt;/th&gt; 
   &lt;th&gt;Link&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;RIFE 4.6,4.7,4.15,4.18,4.22,4.22-lite,4.25&lt;/td&gt; 
   &lt;td&gt;Hzwer&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/hzwer/Practical-RIFE"&gt;Practical-RIFE&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;GMFSS&lt;/td&gt; 
   &lt;td&gt;98mxr&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/98mxr/GMFSS_Fortuna"&gt;GMFSS_Fortuna&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;IFRNet&lt;/td&gt; 
   &lt;td&gt;ltkong218&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ltkong218/IFRNet"&gt;IFRnet&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Upscale Models:&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Author&lt;/th&gt; 
   &lt;th&gt;Link&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;4x-SPANkendata&lt;/td&gt; 
   &lt;td&gt;Crustaceous D&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://openmodeldb.info/models/4x-SPANkendata"&gt;4x-SPANkendata&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;4x-ClearRealityV1&lt;/td&gt; 
   &lt;td&gt;Kim2091&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://openmodeldb.info/models/4x-ClearRealityV1"&gt;4x-ClearRealityV1&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;4x-Nomos8k-SPAN series&lt;/td&gt; 
   &lt;td&gt;Helaman&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://openmodeldb.info/models/4x-Nomos8k-span-otf-strong"&gt;4x-Nomos8k-SPAN series&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;2x-OpenProteus&lt;/td&gt; 
   &lt;td&gt;SiroSky&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Sirosky/Upscale-Hub/releases/tag/OpenProteus"&gt;OpenProteus&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;2x-AnimeJaNai V2 and V3 Sharp&lt;/td&gt; 
   &lt;td&gt;The Database&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/the-database/mpv-upscale-2x_animejanai"&gt;AnimeJanai&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;2x-AniSD&lt;/td&gt; 
   &lt;td&gt;SiroSky&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Sirosky/Upscale-Hub/releases/tag/AniSD"&gt;AniSD&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;AnimeSR&lt;/td&gt; 
   &lt;td&gt;Tencent ARC&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/TencentARC/AnimeSR"&gt;AnimeSR&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Decompression Models:&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Author&lt;/th&gt; 
   &lt;th&gt;Link&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;DeH264&lt;/td&gt; 
   &lt;td&gt;Helaman&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Phhofm/models/releases/tag/1xDeH264_realplksr"&gt;1xDeH264_realplksr&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Denoise Models:&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Author&lt;/th&gt; 
   &lt;th&gt;Link&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;DRUnet&lt;/td&gt; 
   &lt;td&gt;cszn&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/cszn/DPIR"&gt;DRUnet&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;DnCNN&lt;/td&gt; 
   &lt;td&gt;czsn&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/cszn/DPIR"&gt;DnCNN&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;Backends&lt;/h1&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Backend&lt;/th&gt; 
   &lt;th&gt;Hardware&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;TensorRT&lt;/td&gt; 
   &lt;td&gt;NVIDIA RTX GPUs&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;PyTorch&lt;/td&gt; 
   &lt;td&gt;CUDA 12.6 and ROCm 6.2 capable GPUs&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;NCNN&lt;/td&gt; 
   &lt;td&gt;Vulkan 1.3 capable GPUs&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;FAQ&lt;/h1&gt; 
&lt;h3&gt;General Application Usage&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Question&lt;/th&gt; 
   &lt;th&gt;Answer&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;What does this program attempt to accomplish?&lt;/td&gt; 
   &lt;td&gt;Fast, efficient and easily accessable video interpolation (Ex: 24-&amp;gt;48FPS) and video upscaling (Ex: 1920-&amp;gt;3840)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Why is it failing to recognize installed backends?&lt;/td&gt; 
   &lt;td&gt;REAL Video Enhancer uses PIP and portable python for inference, this can sometimes have issues installing. Please attempt reinstalling the app before creating an issue.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;TensorRT related questions&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Why does it take so long to begin inference?&lt;/td&gt; 
   &lt;td&gt;TensorRT uses advanced optimization at the beginning of inference based on your device, this is only done once per resolution of video inputed.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Why does the optimization and inference fail?&lt;/td&gt; 
   &lt;td&gt;The most common way an optimization can fail is &lt;strong&gt;Limited VRAM&lt;/strong&gt; There is no fix to this except using CUDA or NCNN instead.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;ROCm related questions&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Why am I getting (Insert Error here)?&lt;/td&gt; 
   &lt;td&gt;ROCM is buggy, please take a look at &lt;a href="https://github.com/TNTwise/REAL-Video-Enhancer/wiki/ROCm-Help"&gt;ROCm Help&lt;/a&gt;.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;NCNN related questions&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Why am I getting (Insert Vulkan Error here)?&lt;/td&gt; 
   &lt;td&gt;This usually is an OOM (Out Of Memory) error, this can indicate a weak iGPU or very old GPU, I recommeding trying out the &lt;a href="https://github.com/TNTwise/REAL-Video-Enhancer-Colab"&gt;Colab Notebook&lt;/a&gt; instead.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;Cloning:&lt;/h1&gt; 
&lt;pre&gt;&lt;code&gt;# Nightly
git clone --recurse-submodules https://github.com/TNTwise/REAL-Video-Enhancer 

# Stable
git clone --recurse-submodules https://github.com/TNTwise/REAL-Video-Enhancer --branch 2.3.8
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;Building:&lt;/h1&gt; 
&lt;p&gt;3 supported build methods: &lt;/p&gt; 
&lt;p&gt; - pyinstaller (recommended for Win/Mac) &lt;br /&gt; - cx_freeze (recommended for Linux) &lt;br /&gt; - nuitka (experimental) &lt;/p&gt; 
&lt;p&gt;supported python versions: &lt;/p&gt; 
&lt;p&gt; - 3.10 3.11, 3.12 &lt;br /&gt; &lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;python3 build.py --build BUILD_OPTION --copy_backend
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;Colab Notebook&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/tntwise/REAL-Video-Enhancer-Colab"&gt;Colab Notebook&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Credits:&lt;/h1&gt; 
&lt;h3&gt;People:&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Person&lt;/th&gt; 
   &lt;th&gt;For&lt;/th&gt; 
   &lt;th&gt;Link&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;NevermindNilas&lt;/td&gt; 
   &lt;td&gt;Some backend and reference code and working with me on many projects&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/NevermindNilas/"&gt;https://github.com/NevermindNilas/&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Styler00dollar&lt;/td&gt; 
   &lt;td&gt;RIFE ncnn models (4.1-4.5, 4.7-4.12-lite), Sudo Shuffle Span and benchmarking&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/styler00dollar"&gt;https://github.com/styler00dollar&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;HolyWu&lt;/td&gt; 
   &lt;td&gt;TensorRT engine generation code, inference optimizations, and RIFE jagged lines fixes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/HolyWu/"&gt;https://github.com/HolyWu/&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Rick Astley&lt;/td&gt; 
   &lt;td&gt;Amazing music&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=dQw4w9WgXcQ"&gt;https://www.youtube.com/watch?v=dQw4w9WgXcQ&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Software:&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Software Used&lt;/th&gt; 
   &lt;th&gt;For&lt;/th&gt; 
   &lt;th&gt;Link&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;FFmpeg&lt;/td&gt; 
   &lt;td&gt;Multimedia framework for handling video, audio, and other media files&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ffmpeg.org/"&gt;https://ffmpeg.org/&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;QT&lt;/td&gt; 
   &lt;td&gt;GUI framework&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://qt.io/"&gt;https://qt.io/&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;FFMpeg Builds&lt;/td&gt; 
   &lt;td&gt;Pre-compiled builds of FFMpeg.&lt;/td&gt; 
   &lt;td&gt;Windows/Linux: &lt;a href="https://github.com/BtbN/FFmpeg-Builds"&gt;https://github.com/BtbN/FFmpeg-Builds&lt;/a&gt;, MacOS: &lt;a href="https://github.com/eko5624/mpv-mac"&gt;https://github.com/eko5624/mpv-mac&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;PyTorch&lt;/td&gt; 
   &lt;td&gt;Neural Network Inference (CUDA/ROCm/TensorRT)&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://pytorch.org/"&gt;https://pytorch.org/&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;NCNN&lt;/td&gt; 
   &lt;td&gt;Neural Network Inference (Vulkan)&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/tencent/ncnn"&gt;https://github.com/tencent/ncnn&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;RIFE&lt;/td&gt; 
   &lt;td&gt;Real-Time Intermediate Flow Estimation for Video Frame Interpolation&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/hzwer/Practical-RIFE"&gt;https://github.com/hzwer/Practical-RIFE&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;rife-ncnn-vulkan&lt;/td&gt; 
   &lt;td&gt;Video frame interpolation implementation using NCNN and Vulkan&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/nihui/rife-ncnn-vulkan"&gt;https://github.com/nihui/rife-ncnn-vulkan&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;rife ncnn vulkan python&lt;/td&gt; 
   &lt;td&gt;Python bindings for RIFE NCNN Vulkan implementation&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/media2x/rife-ncnn-vulkan-python"&gt;https://github.com/media2x/rife-ncnn-vulkan-python&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;GMFSS&lt;/td&gt; 
   &lt;td&gt;GMFlow based Anime VFI&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/98mxr/GMFSS_Fortuna"&gt;https://github.com/98mxr/GMFSS_Fortuna&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;GIMM&lt;/td&gt; 
   &lt;td&gt;Motion Modeling Realistic VFI&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/GSeanCDAT/GIMM-VFI"&gt;https://github.com/GSeanCDAT/GIMM-VFI&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ncnn python&lt;/td&gt; 
   &lt;td&gt;Python bindings for NCNN Vulkan framework&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://pypi.org/project/ncnn"&gt;https://pypi.org/project/ncnn&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Real-ESRGAN&lt;/td&gt; 
   &lt;td&gt;Upscaling&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/xinntao/Real-ESRGAN"&gt;https://github.com/xinntao/Real-ESRGAN&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;SPAN&lt;/td&gt; 
   &lt;td&gt;Upscaling&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/hongyuanyu/SPAN"&gt;https://github.com/hongyuanyu/SPAN&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Spandrel&lt;/td&gt; 
   &lt;td&gt;CUDA upscaling model architecture support&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/chaiNNer-org/spandrel"&gt;https://github.com/chaiNNer-org/spandrel&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ChaiNNer&lt;/td&gt; 
   &lt;td&gt;Model Scale Detection&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/chaiNNer-org/chainner"&gt;https://github.com/chaiNNer-org/chainner&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;cx_Freeze&lt;/td&gt; 
   &lt;td&gt;Tool for creating standalone executables from Python scripts (Linux build)&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/marcelotduarte/cx_Freeze"&gt;https://github.com/marcelotduarte/cx_Freeze&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;PyInstaller&lt;/td&gt; 
   &lt;td&gt;Tool for creating standalone executables from Python scripts (Windows/Mac builds)&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/pyinstaller/pyinstaller"&gt;https://github.com/pyinstaller/pyinstaller&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Feather Icons&lt;/td&gt; 
   &lt;td&gt;Open source icons library&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/feathericons/feather"&gt;https://github.com/feathericons/feather&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;PySceneDetect&lt;/td&gt; 
   &lt;td&gt;Transition detection library for python&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Breakthrough/PySceneDetect/"&gt;https://github.com/Breakthrough/PySceneDetect/&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Python Standalone Builds&lt;/td&gt; 
   &lt;td&gt;Backend inference using portable python, helps when porting to different platforms.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/indygreg/python-build-standalone"&gt;https://github.com/indygreg/python-build-standalone&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;Star History&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#tntwise/real-video-enhancer&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=tntwise/real-video-enhancer&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>topoteretes/cognee</title>
      <link>https://github.com/topoteretes/cognee</link>
      <description>&lt;p&gt;Memory for AI Agents in 6 lines of code&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;a href="https://github.com/topoteretes/cognee"&gt; &lt;img src="https://raw.githubusercontent.com/topoteretes/cognee/refs/heads/dev/assets/cognee-logo-transparent.png" alt="Cognee Logo" height="60" /&gt; &lt;/a&gt; 
 &lt;br /&gt; 
 &lt;p&gt;Cognee - Accurate and Persistent AI Memory&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;a href="https://www.youtube.com/watch?v=1bezuvLwJmw&amp;amp;t=2s"&gt;Demo&lt;/a&gt; . &lt;a href="https://docs.cognee.ai/"&gt;Docs&lt;/a&gt; . &lt;a href="https://cognee.ai"&gt;Learn More&lt;/a&gt; Â· &lt;a href="https://discord.gg/NQPKmU5CCg"&gt;Join Discord&lt;/a&gt; Â· &lt;a href="https://www.reddit.com/r/AIMemory/"&gt;Join r/AIMemory&lt;/a&gt; . &lt;a href="https://github.com/topoteretes/cognee-community"&gt;Community Plugins &amp;amp; Add-ons&lt;/a&gt; &lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://GitHub.com/topoteretes/cognee/network/"&gt;&lt;img src="https://img.shields.io/github/forks/topoteretes/cognee.svg?style=social&amp;amp;label=Fork&amp;amp;maxAge=2592000" alt="GitHub forks" /&gt;&lt;/a&gt; &lt;a href="https://GitHub.com/topoteretes/cognee/stargazers/"&gt;&lt;img src="https://img.shields.io/github/stars/topoteretes/cognee.svg?style=social&amp;amp;label=Star&amp;amp;maxAge=2592000" alt="GitHub stars" /&gt;&lt;/a&gt; &lt;a href="https://GitHub.com/topoteretes/cognee/commit/"&gt;&lt;img src="https://badgen.net/github/commits/topoteretes/cognee" alt="GitHub commits" /&gt;&lt;/a&gt; &lt;a href="https://github.com/topoteretes/cognee/tags/"&gt;&lt;img src="https://badgen.net/github/tag/topoteretes/cognee" alt="GitHub tag" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/cognee"&gt;&lt;img src="https://static.pepy.tech/badge/cognee" alt="Downloads" /&gt;&lt;/a&gt; &lt;a href="https://github.com/topoteretes/cognee/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/topoteretes/cognee?colorA=00C586&amp;amp;colorB=000000" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://github.com/topoteretes/cognee/graphs/contributors"&gt;&lt;img src="https://img.shields.io/github/contributors/topoteretes/cognee?colorA=00C586&amp;amp;colorB=000000" alt="Contributors" /&gt;&lt;/a&gt; &lt;a href="https://github.com/sponsors/topoteretes"&gt;&lt;img src="https://img.shields.io/badge/Sponsor-â¤ï¸-ff69b4.svg" alt="Sponsor" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt; &lt;a href="https://www.producthunt.com/posts/cognee?embed=true&amp;amp;utm_source=badge-top-post-badge&amp;amp;utm_medium=badge&amp;amp;utm_souce=badge-cognee" target="_blank" style="display:inline-block; margin-right:10px;"&gt; &lt;img src="https://api.producthunt.com/widgets/embed-image/v1/top-post-badge.svg?post_id=946346&amp;amp;theme=light&amp;amp;period=daily&amp;amp;t=1744472480704" alt="cognee - Memory for AI Agents  in 5 lines of code | Product Hunt" width="250" height="54" /&gt; &lt;/a&gt; &lt;a href="https://trendshift.io/repositories/13955" target="_blank" style="display:inline-block;"&gt; &lt;img src="https://trendshift.io/api/badge/repositories/13955" alt="topoteretes%2Fcognee | Trendshift" width="250" height="55" /&gt; &lt;/a&gt; &lt;/p&gt; 
 &lt;p&gt;Use your data to build personalized and dynamic memory for AI Agents. Cognee lets you replace RAG with scalable and modular ECL (Extract, Cognify, Load) pipelines.&lt;/p&gt; 
 &lt;p align="center"&gt; ğŸŒ Available Languages : 
  &lt;!-- Keep these links. Translations will automatically update with the README. --&gt; &lt;a href="https://www.readme-i18n.com/topoteretes/cognee?lang=de"&gt;Deutsch&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/topoteretes/cognee?lang=es"&gt;EspaÃ±ol&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/topoteretes/cognee?lang=fr"&gt;FranÃ§ais&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/topoteretes/cognee?lang=ja"&gt;æ—¥æœ¬èª&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/topoteretes/cognee?lang=ko"&gt;í•œêµ­ì–´&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/topoteretes/cognee?lang=pt"&gt;PortuguÃªs&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/topoteretes/cognee?lang=ru"&gt;Ğ ÑƒÑÑĞºĞ¸Ğ¹&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/topoteretes/cognee?lang=zh"&gt;ä¸­æ–‡&lt;/a&gt; &lt;/p&gt; 
 &lt;div style="text-align: center"&gt; 
  &lt;img src="https://raw.githubusercontent.com/topoteretes/cognee/refs/heads/main/assets/cognee_benefits.png" alt="Why cognee?" width="50%" /&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;h2&gt;About Cognee&lt;/h2&gt; 
&lt;p&gt;Cognee is an open-source tool and platform that transforms your raw data into persistent and dynamic AI memory for Agents. It combines vector search with graph databases to make your documents both searchable by meaning and connected by relationships.&lt;/p&gt; 
&lt;p&gt;You can use Cognee in two ways:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://docs.cognee.ai/getting-started/installation"&gt;Self-host Cognee Open Source&lt;/a&gt;, which stores all data locally by default.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://platform.cognee.ai/"&gt;Connect to Cognee Cloud&lt;/a&gt;, and get the same OSS stack on managed infrastructure for easier development and productionization.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Cognee Open Source (self-hosted):&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Interconnects any type of data â€” including past conversations, files, images, and audio transcriptions&lt;/li&gt; 
 &lt;li&gt;Replaces traditional RAG systems with a unified memory layer built on graphs and vectors&lt;/li&gt; 
 &lt;li&gt;Reduces developer effort and infrastructure cost while improving quality and precision&lt;/li&gt; 
 &lt;li&gt;Provides Pythonic data pipelines for ingestion from 30+ data sources&lt;/li&gt; 
 &lt;li&gt;Offers high customizability through user-defined tasks, modular pipelines, and built-in search endpoints&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Cognee Cloud (managed):&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Hosted web UI dashboard&lt;/li&gt; 
 &lt;li&gt;Automatic version updates&lt;/li&gt; 
 &lt;li&gt;Resource usage analytics&lt;/li&gt; 
 &lt;li&gt;GDPR compliant, enterprise-grade security&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Basic Usage &amp;amp; Feature Guide&lt;/h2&gt; 
&lt;p&gt;To learn more, &lt;a href="https://colab.research.google.com/drive/12Vi9zID-M3fpKpKiaqDBvkk98ElkRPWy?usp=sharing"&gt;check out this short, end-to-end Colab walkthrough&lt;/a&gt; of Cognee's core features.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://colab.research.google.com/drive/12Vi9zID-M3fpKpKiaqDBvkk98ElkRPWy?usp=sharing"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Quickstart&lt;/h2&gt; 
&lt;p&gt;Letâ€™s try Cognee in just a few lines of code. For detailed setup and configuration, see the &lt;a href="https://docs.cognee.ai/getting-started/installation#environment-configuration"&gt;Cognee Docs&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.10 to 3.12&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Step 1: Install Cognee&lt;/h3&gt; 
&lt;p&gt;You can install Cognee with &lt;strong&gt;pip&lt;/strong&gt;, &lt;strong&gt;poetry&lt;/strong&gt;, &lt;strong&gt;uv&lt;/strong&gt;, or your preferred Python package manager.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv pip install cognee
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Step 2: Configure the LLM&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import os
os.environ["LLM_API_KEY"] = "YOUR OPENAI_API_KEY"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, create a &lt;code&gt;.env&lt;/code&gt; file using our &lt;a href="https://github.com/topoteretes/cognee/raw/main/.env.template"&gt;template&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;To integrate other LLM providers, see our &lt;a href="https://docs.cognee.ai/setup-configuration/llm-providers"&gt;LLM Provider Documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Step 3: Run the Pipeline&lt;/h3&gt; 
&lt;p&gt;Cognee will take your documents, generate a knowledge graph from them and then query the graph based on combined relationships.&lt;/p&gt; 
&lt;p&gt;Now, run a minimal pipeline:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import cognee
import asyncio


async def main():
    # Add text to cognee
    await cognee.add("Cognee turns documents into AI memory.")

    # Generate the knowledge graph
    await cognee.cognify()

    # Add memory algorithms to the graph
    await cognee.memify()

    # Query the knowledge graph
    results = await cognee.search("What does Cognee do?")

    # Display the results
    for result in results:
        print(result)


if __name__ == '__main__':
    asyncio.run(main())

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;As you can see, the output is generated from the document we previously stored in Cognee:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;  Cognee turns documents into AI memory.
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Use the Cognee CLI&lt;/h3&gt; 
&lt;p&gt;As an alternative, you can get started with these essential commands:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cognee-cli add "Cognee turns documents into AI memory."

cognee-cli cognify

cognee-cli search "What does Cognee do?"
cognee-cli delete --all

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To open the local UI, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cognee-cli -ui
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Demos &amp;amp; Examples&lt;/h2&gt; 
&lt;p&gt;See Cognee in action:&lt;/p&gt; 
&lt;h3&gt;Persistent Agent Memory&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/e113b628-7212-4a2b-b288-0be39a93a1c3"&gt;Cognee Memory for LangGraph Agents&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Simple GraphRAG&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/f2186b2e-305a-42b0-9c2d-9f4473f15df8"&gt;Watch Demo&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Cognee with Ollama&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/39672858-f774-4136-b957-1e2de67b8981"&gt;Watch Demo&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Community &amp;amp; Support&lt;/h2&gt; 
&lt;h3&gt;Contributing&lt;/h3&gt; 
&lt;p&gt;We welcome contributions from the community! Your input helps make Cognee better for everyone. See &lt;a href="https://raw.githubusercontent.com/topoteretes/cognee/main/CONTRIBUTING.md"&gt;&lt;code&gt;CONTRIBUTING.md&lt;/code&gt;&lt;/a&gt; to get started.&lt;/p&gt; 
&lt;h3&gt;Code of Conduct&lt;/h3&gt; 
&lt;p&gt;We're committed to fostering an inclusive and respectful community. Read our &lt;a href="https://github.com/topoteretes/cognee/raw/main/CODE_OF_CONDUCT.md"&gt;Code of Conduct&lt;/a&gt; for guidelines.&lt;/p&gt; 
&lt;h2&gt;Research &amp;amp; Citation&lt;/h2&gt; 
&lt;p&gt;We recently published a research paper on optimizing knowledge graphs for LLM reasoning:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{markovic2025optimizinginterfaceknowledgegraphs,
      title={Optimizing the Interface Between Knowledge Graphs and LLMs for Complex Reasoning},
      author={Vasilije Markovic and Lazar Obradovic and Laszlo Hajdu and Jovan Pavlovic},
      year={2025},
      eprint={2505.24478},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2505.24478},
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>huggingface/trl</title>
      <link>https://github.com/huggingface/trl</link>
      <description>&lt;p&gt;Train transformer language models with reinforcement learning.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;TRL - Transformer Reinforcement Learning&lt;/h1&gt; 
&lt;div style="text-align: center"&gt; 
 &lt;img src="https://huggingface.co/datasets/trl-lib/documentation-images/resolve/main/trl_banner_dark.png" alt="TRL Banner" /&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;br /&gt; 
&lt;h3 align="center"&gt; &lt;p&gt;A comprehensive library to post-train foundation models&lt;/p&gt; &lt;/h3&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/huggingface/trl/raw/main/LICENSE"&gt;&lt;img alt="License" src="https://img.shields.io/github/license/huggingface/trl.svg?color=blue" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/docs/trl/index"&gt;&lt;img alt="Documentation" src="https://img.shields.io/website?label=documentation&amp;amp;url=https%3A%2F%2Fhuggingface.co%2Fdocs%2Ftrl%2Findex&amp;amp;down_color=red&amp;amp;down_message=offline&amp;amp;up_color=blue&amp;amp;up_message=online" /&gt;&lt;/a&gt; &lt;a href="https://github.com/huggingface/trl/releases"&gt;&lt;img alt="GitHub release" src="https://img.shields.io/github/release/huggingface/trl.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/trl-lib"&gt;&lt;img alt="Hugging Face Hub" src="https://img.shields.io/badge/ğŸ¤—%20Hub-trl--lib-yellow" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;ğŸ‰ What's New&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;OpenEnv Integration:&lt;/strong&gt; TRL now supports &lt;strong&gt;&lt;a href="https://huggingface.co/blog/openenv"&gt;OpenEnv&lt;/a&gt;&lt;/strong&gt;, the open-source framework from Meta for defining, deploying, and interacting with environments in reinforcement learning and agentic workflows.&lt;/p&gt; 
&lt;p&gt;Explore how to seamlessly integrate TRL with OpenEnv in our &lt;a href="https://raw.githubusercontent.com/huggingface/trl/main/openenv"&gt;dedicated documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Overview&lt;/h2&gt; 
&lt;p&gt;TRL is a cutting-edge library designed for post-training foundation models using advanced techniques like Supervised Fine-Tuning (SFT), Proximal Policy Optimization (PPO), and Direct Preference Optimization (DPO). Built on top of the &lt;a href="https://github.com/huggingface/transformers"&gt;ğŸ¤— Transformers&lt;/a&gt; ecosystem, TRL supports a variety of model architectures and modalities, and can be scaled-up across various hardware setups.&lt;/p&gt; 
&lt;h2&gt;Highlights&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Trainers&lt;/strong&gt;: Various fine-tuning methods are easily accessible via trainers like &lt;a href="https://huggingface.co/docs/trl/sft_trainer"&gt;&lt;code&gt;SFTTrainer&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://huggingface.co/docs/trl/grpo_trainer"&gt;&lt;code&gt;GRPOTrainer&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://huggingface.co/docs/trl/dpo_trainer"&gt;&lt;code&gt;DPOTrainer&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://huggingface.co/docs/trl/reward_trainer"&gt;&lt;code&gt;RewardTrainer&lt;/code&gt;&lt;/a&gt; and more.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Efficient and scalable&lt;/strong&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Leverages &lt;a href="https://github.com/huggingface/accelerate"&gt;ğŸ¤— Accelerate&lt;/a&gt; to scale from single GPU to multi-node clusters using methods like &lt;a href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html"&gt;DDP&lt;/a&gt; and &lt;a href="https://github.com/deepspeedai/DeepSpeed"&gt;DeepSpeed&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;Full integration with &lt;a href="https://github.com/huggingface/peft"&gt;ğŸ¤— PEFT&lt;/a&gt; enables training on large models with modest hardware via quantization and LoRA/QLoRA.&lt;/li&gt; 
   &lt;li&gt;Integrates &lt;a href="https://github.com/unslothai/unsloth"&gt;ğŸ¦¥ Unsloth&lt;/a&gt; for accelerating training using optimized kernels.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Command Line Interface (CLI)&lt;/strong&gt;: A simple interface lets you fine-tune with models without needing to write code.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;Python Package&lt;/h3&gt; 
&lt;p&gt;Install the library using &lt;code&gt;pip&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install trl
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;From source&lt;/h3&gt; 
&lt;p&gt;If you want to use the latest features before an official release, you can install TRL from source:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install git+https://github.com/huggingface/trl.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Repository&lt;/h3&gt; 
&lt;p&gt;If you want to use the examples you can clone the repository with the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/huggingface/trl.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;p&gt;For more flexibility and control over training, TRL provides dedicated trainer classes to post-train language models or PEFT adapters on a custom dataset. Each trainer in TRL is a light wrapper around the ğŸ¤— Transformers trainer and natively supports distributed training methods like DDP, DeepSpeed ZeRO, and FSDP.&lt;/p&gt; 
&lt;h3&gt;&lt;code&gt;SFTTrainer&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;Here is a basic example of how to use the &lt;a href="https://huggingface.co/docs/trl/sft_trainer"&gt;&lt;code&gt;SFTTrainer&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from trl import SFTTrainer
from datasets import load_dataset

dataset = load_dataset("trl-lib/Capybara", split="train")

trainer = SFTTrainer(
    model="Qwen/Qwen2.5-0.5B",
    train_dataset=dataset,
)
trainer.train()
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;&lt;code&gt;GRPOTrainer&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/docs/trl/grpo_trainer"&gt;&lt;code&gt;GRPOTrainer&lt;/code&gt;&lt;/a&gt; implements the &lt;a href="https://huggingface.co/papers/2402.03300"&gt;Group Relative Policy Optimization (GRPO) algorithm&lt;/a&gt; that is more memory-efficient than PPO and was used to train &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-R1"&gt;Deepseek AI's R1&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from datasets import load_dataset
from trl import GRPOTrainer

dataset = load_dataset("trl-lib/tldr", split="train")

# Dummy reward function: count the number of unique characters in the completions
def reward_num_unique_chars(completions, **kwargs):
    return [len(set(c)) for c in completions]

trainer = GRPOTrainer(
    model="Qwen/Qwen2-0.5B-Instruct",
    reward_funcs=reward_num_unique_chars,
    train_dataset=dataset,
)
trainer.train()
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;&lt;code&gt;DPOTrainer&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/docs/trl/dpo_trainer"&gt;&lt;code&gt;DPOTrainer&lt;/code&gt;&lt;/a&gt; implements the popular &lt;a href="https://huggingface.co/papers/2305.18290"&gt;Direct Preference Optimization (DPO) algorithm&lt;/a&gt; that was used to post-train &lt;a href="https://huggingface.co/papers/2407.21783"&gt;Llama 3&lt;/a&gt; and many other models. Here is a basic example of how to use the &lt;code&gt;DPOTrainer&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer
from trl import DPOConfig, DPOTrainer

model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen2.5-0.5B-Instruct")
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-0.5B-Instruct")
dataset = load_dataset("trl-lib/ultrafeedback_binarized", split="train")
training_args = DPOConfig(output_dir="Qwen2.5-0.5B-DPO")
trainer = DPOTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
    processing_class=tokenizer
)
trainer.train()
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;&lt;code&gt;RewardTrainer&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;Here is a basic example of how to use the &lt;a href="https://huggingface.co/docs/trl/reward_trainer"&gt;&lt;code&gt;RewardTrainer&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from trl import RewardTrainer
from datasets import load_dataset

dataset = load_dataset("trl-lib/ultrafeedback_binarized", split="train")

trainer = RewardTrainer(
    model="Qwen/Qwen2.5-0.5B-Instruct",
    train_dataset=dataset,
)
trainer.train()
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Command Line Interface (CLI)&lt;/h2&gt; 
&lt;p&gt;You can use the TRL Command Line Interface (CLI) to quickly get started with post-training methods like Supervised Fine-Tuning (SFT) or Direct Preference Optimization (DPO):&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;SFT:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;trl sft --model_name_or_path Qwen/Qwen2.5-0.5B \
    --dataset_name trl-lib/Capybara \
    --output_dir Qwen2.5-0.5B-SFT
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;DPO:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;trl dpo --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
    --dataset_name argilla/Capybara-Preferences \
    --output_dir Qwen2.5-0.5B-DPO 
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Read more about CLI in the &lt;a href="https://huggingface.co/docs/trl/clis"&gt;relevant documentation section&lt;/a&gt; or use &lt;code&gt;--help&lt;/code&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;Development&lt;/h2&gt; 
&lt;p&gt;If you want to contribute to &lt;code&gt;trl&lt;/code&gt; or customize it to your needs make sure to read the &lt;a href="https://github.com/huggingface/trl/raw/main/CONTRIBUTING.md"&gt;contribution guide&lt;/a&gt; and make sure you make a dev install:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/huggingface/trl.git
cd trl/
pip install -e .[dev]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Experimental&lt;/h2&gt; 
&lt;p&gt;A minimal incubation area is available under &lt;code&gt;trl.experimental&lt;/code&gt; for unstable / fast-evolving features. Anything there may change or be removed in any release without notice.&lt;/p&gt; 
&lt;p&gt;Example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from trl.experimental.new_trainer import NewTrainer
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Read more in the &lt;a href="https://huggingface.co/docs/trl/experimental_overview"&gt;Experimental docs&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{vonwerra2022trl,
  author = {Leandro von Werra and Younes Belkada and Lewis Tunstall and Edward Beeching and Tristan Thrush and Nathan Lambert and Shengyi Huang and Kashif Rasul and Quentin GallouÃ©dec},
  title = {TRL: Transformer Reinforcement Learning},
  year = {2020},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/huggingface/trl}}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This repository's source code is available under the &lt;a href="https://raw.githubusercontent.com/huggingface/trl/main/LICENSE"&gt;Apache-2.0 License&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>localstack/localstack</title>
      <link>https://github.com/localstack/localstack</link>
      <description>&lt;p&gt;ğŸ’» A fully functional local AWS cloud stack. Develop and test your cloud &amp; Serverless apps offline&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;span&gt;âš¡&lt;/span&gt; We are thrilled to announce the release of &lt;a href="https://blog.localstack.cloud/localstack-for-aws-release-v-4-9-0/"&gt;LocalStack 4.9&lt;/a&gt; &lt;span&gt;âš¡&lt;/span&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/localstack/localstack/main/docs/localstack-readme-banner.svg?sanitize=true" alt="LocalStack - The Leading Platform for Local Cloud Development" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/localstack/localstack/actions/workflows/aws-main.yml?query=branch%3Amain"&gt;&lt;img alt="GitHub Actions" src="https://github.com/localstack/localstack/actions/workflows/aws-main.yml/badge.svg?branch=main" /&gt;&lt;/a&gt; &lt;a href="https://coveralls.io/github/localstack/localstack?branch=main"&gt;&lt;img alt="Coverage Status" src="https://coveralls.io/repos/github/localstack/localstack/badge.svg?branch=main" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/localstack/"&gt;&lt;img alt="PyPI Version" src="https://img.shields.io/pypi/v/localstack?color=blue" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/localstack/localstack"&gt;&lt;img alt="Docker Pulls" src="https://img.shields.io/docker/pulls/localstack/localstack" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/localstack"&gt;&lt;img alt="PyPi downloads" src="https://static.pepy.tech/badge/localstack" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/localstack/localstack/main/#backers"&gt;&lt;img alt="Backers on Open Collective" src="https://opencollective.com/localstack/backers/badge.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/localstack/localstack/main/#sponsors"&gt;&lt;img alt="Sponsors on Open Collective" src="https://opencollective.com/localstack/sponsors/badge.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://img.shields.io/pypi/l/localstack.svg"&gt;&lt;img alt="PyPI License" src="https://img.shields.io/pypi/l/localstack.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://github.com/psf/black"&gt;&lt;img alt="Code style: black" src="https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://github.com/astral-sh/ruff"&gt;&lt;img alt="Ruff" src="https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json" /&gt;&lt;/a&gt; &lt;a href="https://bsky.app/profile/localstack.cloud"&gt;&lt;img alt="Bluesky" src="https://img.shields.io/badge/bluesky-Follow-blue?logo=bluesky" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; LocalStack is a cloud software development framework to develop and test your AWS applications locally. &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://raw.githubusercontent.com/localstack/localstack/main/#overview"&gt;Overview&lt;/a&gt; â€¢ &lt;a href="https://raw.githubusercontent.com/localstack/localstack/main/#install"&gt;Install&lt;/a&gt; â€¢ &lt;a href="https://raw.githubusercontent.com/localstack/localstack/main/#quickstart"&gt;Quickstart&lt;/a&gt; â€¢ &lt;a href="https://raw.githubusercontent.com/localstack/localstack/main/#running"&gt;Run&lt;/a&gt; â€¢ &lt;a href="https://raw.githubusercontent.com/localstack/localstack/main/#usage"&gt;Usage&lt;/a&gt; â€¢ &lt;a href="https://raw.githubusercontent.com/localstack/localstack/main/#releases"&gt;Releases&lt;/a&gt; â€¢ &lt;a href="https://raw.githubusercontent.com/localstack/localstack/main/#contributing"&gt;Contributing&lt;/a&gt; &lt;br /&gt; &lt;a href="https://docs.localstack.cloud" target="_blank"&gt;ğŸ“– Docs&lt;/a&gt; â€¢ &lt;a href="https://app.localstack.cloud" target="_blank"&gt;ğŸ’» Pro version&lt;/a&gt; â€¢ &lt;a href="https://docs.localstack.cloud/references/coverage/" target="_blank"&gt;â˜‘ï¸ LocalStack coverage&lt;/a&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;h1&gt;Overview&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://localstack.cloud"&gt;LocalStack&lt;/a&gt; is a cloud service emulator that runs in a single container on your laptop or in your CI environment. With LocalStack, you can run your AWS applications or Lambdas entirely on your local machine without connecting to a remote cloud provider! Whether you are testing complex CDK applications or Terraform configurations, or just beginning to learn about AWS services, LocalStack helps speed up and simplify your testing and development workflow.&lt;/p&gt; 
&lt;p&gt;LocalStack supports a growing number of AWS services, like AWS Lambda, S3, DynamoDB, Kinesis, SQS, SNS, and many more! The &lt;a href="https://localstack.cloud/pricing"&gt;Pro version of LocalStack&lt;/a&gt; supports additional APIs and advanced features. You can find a comprehensive list of supported APIs on our &lt;a href="https://docs.localstack.cloud/user-guide/aws/feature-coverage/"&gt;â˜‘ï¸ Feature Coverage&lt;/a&gt; page.&lt;/p&gt; 
&lt;p&gt;LocalStack also provides additional features to make your life as a cloud developer easier! Check out LocalStack's &lt;a href="https://docs.localstack.cloud/user-guide/"&gt;User Guides&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h2&gt;Install&lt;/h2&gt; 
&lt;p&gt;The quickest way to get started with LocalStack is by using the LocalStack CLI. It enables you to start and manage the LocalStack Docker container directly through your command line. Ensure that your machine has a functional &lt;a href="https://docs.docker.com/get-docker/"&gt;&lt;code&gt;docker&lt;/code&gt; environment&lt;/a&gt; installed before proceeding.&lt;/p&gt; 
&lt;h3&gt;Brew (macOS or Linux with Homebrew)&lt;/h3&gt; 
&lt;p&gt;Install the LocalStack CLI through our &lt;a href="https://github.com/localstack/homebrew-tap"&gt;official LocalStack Brew Tap&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;brew install localstack/tap/localstack-cli
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Binary download (macOS, Linux, Windows)&lt;/h3&gt; 
&lt;p&gt;If Brew is not installed on your machine, you can download the pre-built LocalStack CLI binary directly:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Visit &lt;a href="https://github.com/localstack/localstack-cli/releases/latest"&gt;localstack/localstack-cli&lt;/a&gt; and download the latest release for your platform.&lt;/li&gt; 
 &lt;li&gt;Extract the downloaded archive to a directory included in your &lt;code&gt;PATH&lt;/code&gt; variable: 
  &lt;ul&gt; 
   &lt;li&gt;For macOS/Linux, use the command: &lt;code&gt;sudo tar xvzf ~/Downloads/localstack-cli-*-darwin-*-onefile.tar.gz -C /usr/local/bin&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;PyPI (macOS, Linux, Windows)&lt;/h3&gt; 
&lt;p&gt;LocalStack is developed using Python. To install the LocalStack CLI using &lt;code&gt;pip&lt;/code&gt;, run the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python3 -m pip install localstack
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The &lt;code&gt;localstack-cli&lt;/code&gt; installation enables you to run the Docker image containing the LocalStack runtime. To interact with the local AWS services, you need to install the &lt;code&gt;awslocal&lt;/code&gt; CLI separately. For installation guidelines, refer to the &lt;a href="https://docs.localstack.cloud/user-guide/integrations/aws-cli/#localstack-aws-cli-awslocal"&gt;&lt;code&gt;awslocal&lt;/code&gt; documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: Do not use &lt;code&gt;sudo&lt;/code&gt; or run as &lt;code&gt;root&lt;/code&gt; user. LocalStack must be installed and started entirely under a local non-root user. If you have problems with permissions in macOS High Sierra, install with &lt;code&gt;pip install --user localstack&lt;/code&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Quickstart&lt;/h2&gt; 
&lt;p&gt;Start LocalStack inside a Docker container by running:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt; % localstack start -d

     __                     _______ __             __
    / /   ____  _________ _/ / ___// /_____ ______/ /__
   / /   / __ \/ ___/ __ `/ /\__ \/ __/ __ `/ ___/ //_/
  / /___/ /_/ / /__/ /_/ / /___/ / /_/ /_/ / /__/ ,&amp;lt;
 /_____/\____/\___/\__,_/_//____/\__/\__,_/\___/_/|_|

- LocalStack CLI: 4.9.0
- Profile: default
- App: https://app.localstack.cloud

[17:00:15] starting LocalStack in Docker mode ğŸ³               localstack.py:512
           preparing environment                               bootstrap.py:1322
           configuring container                               bootstrap.py:1330
           starting container                                  bootstrap.py:1340
[17:00:16] detaching                                           bootstrap.py:1344
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can query the status of respective services on LocalStack by running:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;% localstack status services
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ Service                  â”ƒ Status      â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ acm                      â”‚ âœ” available â”‚
â”‚ apigateway               â”‚ âœ” available â”‚
â”‚ cloudformation           â”‚ âœ” available â”‚
â”‚ cloudwatch               â”‚ âœ” available â”‚
â”‚ config                   â”‚ âœ” available â”‚
â”‚ dynamodb                 â”‚ âœ” available â”‚
...
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To use SQS, a fully managed distributed message queuing service, on LocalStack, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;% awslocal sqs create-queue --queue-name sample-queue
{
    "QueueUrl": "http://sqs.us-east-1.localhost.localstack.cloud:4566/000000000000/sample-queue"
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Learn more about &lt;a href="https://docs.localstack.cloud/references/coverage/"&gt;LocalStack AWS services&lt;/a&gt; and using them with LocalStack's &lt;code&gt;awslocal&lt;/code&gt; CLI.&lt;/p&gt; 
&lt;h2&gt;Running&lt;/h2&gt; 
&lt;p&gt;You can run LocalStack through the following options:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.localstack.cloud/getting-started/installation/#localstack-cli"&gt;LocalStack CLI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.localstack.cloud/getting-started/installation/#docker"&gt;Docker&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.localstack.cloud/getting-started/installation/#docker-compose"&gt;Docker Compose&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.localstack.cloud/getting-started/installation/#helm"&gt;Helm&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;p&gt;To start using LocalStack, check out our &lt;a href="https://docs.localstack.cloud"&gt;documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.localstack.cloud/references/configuration/"&gt;LocalStack Configuration&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.localstack.cloud/user-guide/ci/"&gt;LocalStack in CI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.localstack.cloud/user-guide/integrations/"&gt;LocalStack Integrations&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.localstack.cloud/user-guide/tools/"&gt;LocalStack Tools&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.localstack.cloud/references/"&gt;Understanding LocalStack&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.localstack.cloud/getting-started/faq/"&gt;Frequently Asked Questions&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To use LocalStack with a graphical user interface, you can use the following UI clients:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://app.localstack.cloud"&gt;LocalStack Web Application&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.localstack.cloud/user-guide/tools/localstack-desktop/"&gt;LocalStack Desktop&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.localstack.cloud/user-guide/tools/localstack-docker-extension/"&gt;LocalStack Docker Extension&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Releases&lt;/h2&gt; 
&lt;p&gt;Please refer to &lt;a href="https://github.com/localstack/localstack/releases"&gt;GitHub releases&lt;/a&gt; to see the complete list of changes for each release. For extended release notes, please refer to the &lt;a href="https://docs.localstack.cloud/references/changelog/"&gt;changelog&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;If you are interested in contributing to LocalStack:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Start by reading our &lt;a href="https://raw.githubusercontent.com/localstack/localstack/main/docs/CONTRIBUTING.md"&gt;contributing guide&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Check out our &lt;a href="https://raw.githubusercontent.com/localstack/localstack/main/docs/development-environment-setup/README.md"&gt;development environment setup guide&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Navigate our codebase and &lt;a href="https://github.com/localstack/localstack/issues"&gt;open issues&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We are thankful for all the contributions and feedback we receive.&lt;/p&gt; 
&lt;h2&gt;Get in touch&lt;/h2&gt; 
&lt;p&gt;Get in touch with the LocalStack Team to report ğŸ &lt;a href="https://github.com/localstack/localstack/issues/new/choose"&gt;issues&lt;/a&gt;, upvote ğŸ‘ &lt;a href="https://github.com/localstack/localstack/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc+"&gt;feature requests&lt;/a&gt;, ğŸ™‹ğŸ½ ask &lt;a href="https://docs.localstack.cloud/getting-started/help-and-support/"&gt;support questions&lt;/a&gt;, or ğŸ—£ï¸ discuss local cloud development:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://localstack.cloud/slack/"&gt;LocalStack Slack Community&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/localstack/localstack/issues"&gt;LocalStack GitHub Issue tracker&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Contributors&lt;/h3&gt; 
&lt;p&gt;We are thankful to all the people who have contributed to this project.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/localstack/localstack/graphs/contributors"&gt;&lt;img src="https://opencollective.com/localstack/contributors.svg?width=890" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Backers&lt;/h3&gt; 
&lt;p&gt;We are also grateful to all our backers who have donated to the project. You can become a backer on &lt;a href="https://opencollective.com/localstack#backer"&gt;Open Collective&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://opencollective.com/localstack#backers" target="_blank"&gt;&lt;img src="https://opencollective.com/localstack/backers.svg?width=890" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Sponsors&lt;/h3&gt; 
&lt;p&gt;You can also support this project by becoming a sponsor on &lt;a href="https://opencollective.com/localstack#sponsor"&gt;Open Collective&lt;/a&gt;. Your logo will show up here along with a link to your website.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://opencollective.com/localstack/sponsor/0/website" target="_blank"&gt;&lt;img src="https://opencollective.com/localstack/sponsor/0/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/localstack/sponsor/1/website" target="_blank"&gt;&lt;img src="https://opencollective.com/localstack/sponsor/1/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/localstack/sponsor/2/website" target="_blank"&gt;&lt;img src="https://opencollective.com/localstack/sponsor/2/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/localstack/sponsor/3/website" target="_blank"&gt;&lt;img src="https://opencollective.com/localstack/sponsor/3/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/localstack/sponsor/4/website" target="_blank"&gt;&lt;img src="https://opencollective.com/localstack/sponsor/4/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/localstack/sponsor/5/website" target="_blank"&gt;&lt;img src="https://opencollective.com/localstack/sponsor/5/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/localstack/sponsor/6/website" target="_blank"&gt;&lt;img src="https://opencollective.com/localstack/sponsor/6/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/localstack/sponsor/7/website" target="_blank"&gt;&lt;img src="https://opencollective.com/localstack/sponsor/7/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/localstack/sponsor/8/website" target="_blank"&gt;&lt;img src="https://opencollective.com/localstack/sponsor/8/avatar.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://opencollective.com/localstack/sponsor/9/website" target="_blank"&gt;&lt;img src="https://opencollective.com/localstack/sponsor/9/avatar.svg?sanitize=true" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Copyright (c) 2017-2025 LocalStack maintainers and contributors.&lt;/p&gt; 
&lt;p&gt;Copyright (c) 2016 Atlassian and others.&lt;/p&gt; 
&lt;p&gt;This version of LocalStack is released under the Apache License, Version 2.0 (see &lt;a href="https://raw.githubusercontent.com/localstack/localstack/main/LICENSE.txt"&gt;LICENSE&lt;/a&gt;). By downloading and using this software you agree to the &lt;a href="https://raw.githubusercontent.com/localstack/localstack/main/docs/end_user_license_agreement"&gt;End-User License Agreement (EULA)&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>usestrix/strix</title>
      <link>https://github.com/usestrix/strix</link>
      <description>&lt;p&gt;âœ¨ Open-source AI hackers for your apps ğŸ‘¨ğŸ»â€ğŸ’»&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="https://usestrix.com/"&gt; &lt;img src="https://raw.githubusercontent.com/usestrix/strix/main/.github/logo.png" width="150" alt="Strix Logo" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h1 align="center"&gt; Strix &lt;/h1&gt; 
&lt;h2 align="center"&gt;Open-source AI Hackers to secure your Apps&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://pypi.org/project/strix-agent/"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/strix-agent?color=3776AB" alt="Python" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/strix-agent/"&gt;&lt;img src="https://img.shields.io/pypi/v/strix-agent?color=10b981" alt="PyPI" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/projects/strix-agent"&gt;&lt;img src="https://static.pepy.tech/personalized-badge/strix-agent?period=total&amp;amp;units=INTERNATIONAL_SYSTEM&amp;amp;left_color=GREY&amp;amp;right_color=RED&amp;amp;left_text=Downloads" alt="PyPI Downloads" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/usestrix/strix/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/license-Apache%202.0-blue.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/usestrix/strix"&gt;&lt;img src="https://img.shields.io/github/stars/usestrix/strix" alt="GitHub Stars" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/J48Fzuh7"&gt;&lt;img src="https://img.shields.io/badge/Discord-%235865F2.svg?&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://usestrix.com"&gt;&lt;img src="https://img.shields.io/badge/Website-usestrix.com-2d3748.svg?sanitize=true" alt="Website" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span&gt;â­&lt;/span&gt; &lt;em&gt;Love Strix? Give us a star to help other developers discover it!&lt;/em&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/usestrix/strix/main/.github/screenshot.png" alt="Strix Demo" width="800" style="border-radius: 16px; box-shadow: 0 20px 40px rgba(0, 0, 0, 0.3), 0 0 0 1px rgba(255, 255, 255, 0.1), inset 0 1px 0 rgba(255, 255, 255, 0.2); transform: perspective(1000px) rotateX(2deg); transition: transform 0.3s ease;" /&gt; 
&lt;/div&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] &lt;strong&gt;New!&lt;/strong&gt; Strix now integrates seamlessly with GitHub Actions and CI/CD pipelines. Automatically scan for vulnerabilities on every pull request and block insecure code before it reaches production!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ¦‰ Strix Overview&lt;/h2&gt; 
&lt;p&gt;Strix are autonomous AI agents that act just like real hackers - they run your code dynamically, find vulnerabilities, and validate them through actual proof-of-concepts. Built for developers and security teams who need fast, accurate security testing without the overhead of manual pentesting or the false positives of static analysis tools.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Full hacker toolkit&lt;/strong&gt; out of the box&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Teams of agents&lt;/strong&gt; that collaborate and scale&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Real validation&lt;/strong&gt; with PoCs, not false positives&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Developerâ€‘first&lt;/strong&gt; CLI with actionable reports&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Autoâ€‘fix &amp;amp; reporting&lt;/strong&gt; to accelerate remediation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h3&gt;ğŸ¯ Use Cases&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Detect and validate critical vulnerabilities in your applications.&lt;/li&gt; 
 &lt;li&gt;Get penetration tests done in hours, not weeks, with compliance reports.&lt;/li&gt; 
 &lt;li&gt;Automate bug bounty research and generate PoCs for faster reporting.&lt;/li&gt; 
 &lt;li&gt;Run tests in CI/CD to block vulnerabilities before reaching production.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h3&gt;ğŸš€ Quick Start&lt;/h3&gt; 
&lt;p&gt;Prerequisites:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Docker (running)&lt;/li&gt; 
 &lt;li&gt;Python 3.12+&lt;/li&gt; 
 &lt;li&gt;An LLM provider key (or a local LLM)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install
pipx install strix-agent

# Configure AI provider
export STRIX_LLM="openai/gpt-5"
export LLM_API_KEY="your-api-key"

# Run security assessment
strix --target ./app-directory
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;First run pulls the sandbox Docker image. Results are saved under &lt;code&gt;agent_runs/&amp;lt;run-name&amp;gt;&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;â˜ï¸ Cloud Hosted&lt;/h3&gt; 
&lt;p&gt;Want to skip the setup? Try our cloud-hosted version: &lt;strong&gt;&lt;a href="https://usestrix.com"&gt;usestrix.com&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;âœ¨ Features&lt;/h2&gt; 
&lt;h3&gt;ğŸ› ï¸ Agentic Security Tools&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ”Œ Full HTTP Proxy&lt;/strong&gt; - Full request/response manipulation and analysis&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸŒ Browser Automation&lt;/strong&gt; - Multi-tab browser for testing of XSS, CSRF, auth flows&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ’» Terminal Environments&lt;/strong&gt; - Interactive shells for command execution and testing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ Python Runtime&lt;/strong&gt; - Custom exploit development and validation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ” Reconnaissance&lt;/strong&gt; - Automated OSINT and attack surface mapping&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ“ Code Analysis&lt;/strong&gt; - Static and dynamic analysis capabilities&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ“ Knowledge Management&lt;/strong&gt; - Structured findings and attack documentation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ¯ Comprehensive Vulnerability Detection&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Access Control&lt;/strong&gt; - IDOR, privilege escalation, auth bypass&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Injection Attacks&lt;/strong&gt; - SQL, NoSQL, command injection&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Server-Side&lt;/strong&gt; - SSRF, XXE, deserialization flaws&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Client-Side&lt;/strong&gt; - XSS, prototype pollution, DOM vulnerabilities&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Business Logic&lt;/strong&gt; - Race conditions, workflow manipulation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Authentication&lt;/strong&gt; - JWT vulnerabilities, session management&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Infrastructure&lt;/strong&gt; - Misconfigurations, exposed services&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ•¸ï¸ Graph of Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Distributed Workflows&lt;/strong&gt; - Specialized agents for different attacks and assets&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Scalable Testing&lt;/strong&gt; - Parallel execution for fast comprehensive coverage&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Dynamic Coordination&lt;/strong&gt; - Agents collaborate and share discoveries&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ’» Usage Examples&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Local codebase analysis
strix --target ./app-directory

# Repository security review
strix --target https://github.com/org/repo

# Web application assessment
strix --target https://your-app.com

# Multi-target white-box testing (source code + deployed app)
strix -t https://github.com/org/app -t https://your-app.com

# Test multiple environments simultaneously
strix -t https://dev.your-app.com -t https://staging.your-app.com -t https://prod.your-app.com

# Focused testing with instructions
strix --target api.your-app.com --instruction "Prioritize authentication and authorization testing"

# Testing with credentials
strix --target https://your-app.com --instruction "Test with credentials: testuser/testpass. Focus on privilege escalation and access control bypasses."
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;âš™ï¸ Configuration&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export STRIX_LLM="openai/gpt-5"
export LLM_API_KEY="your-api-key"

# Optional
export LLM_API_BASE="your-api-base-url"  # if using a local model, e.g. Ollama, LMStudio
export PERPLEXITY_API_KEY="your-api-key"  # for search capabilities
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href="https://docs.litellm.ai/docs/providers"&gt;ğŸ“š View supported AI models&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;ğŸ¤– Headless Mode&lt;/h3&gt; 
&lt;p&gt;Run Strix programmatically without interactive UI using the &lt;code&gt;-n/--non-interactive&lt;/code&gt; flagâ€”perfect for servers and automated jobs. The CLI prints real-time vulnerability findings, and the final report before exiting. Exits with non-zero code when vulnerabilities are found.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;strix -n --target https://your-app.com --instruction "Focus on authentication and authorization vulnerabilities"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;ğŸ”„ CI/CD (GitHub Actions)&lt;/h3&gt; 
&lt;p&gt;Strix can be added to your pipeline to run a security test on pull requests with a lightweight GitHub Actions workflow:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;name: strix-penetration-test

on:
  pull_request:

jobs:
  security-scan:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Install Strix
        run: pipx install strix-agent

      - name: Run Strix
        env:
          STRIX_LLM: ${{ secrets.STRIX_LLM }}
          LLM_API_KEY: ${{ secrets.LLM_API_KEY }}

        run: strix -n -t ./
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ğŸ† Enterprise Platform&lt;/h2&gt; 
&lt;p&gt;Our managed platform provides:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ“ˆ Executive Dashboards&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ§  Custom Fine-Tuned Models&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;âš™ï¸ CI/CD Integration&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ” Large-Scale Scanning&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ”Œ Third-Party Integrations&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ¯ Enterprise Support&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://usestrix.com"&gt;&lt;strong&gt;Get Enterprise Demo â†’&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;ğŸ”’ Security Architecture&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Container Isolation&lt;/strong&gt; - All testing in sandboxed Docker environments&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Local Processing&lt;/strong&gt; - Testing runs locally, no data sent to external services&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING] Only test systems you own or have permission to test. You are responsible for using Strix ethically and legally.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;ğŸ¤ Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions from the community! There are several ways to contribute:&lt;/p&gt; 
&lt;h3&gt;Code Contributions&lt;/h3&gt; 
&lt;p&gt;See our &lt;a href="https://raw.githubusercontent.com/usestrix/strix/main/CONTRIBUTING.md"&gt;Contributing Guide&lt;/a&gt; for details on:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Setting up your development environment&lt;/li&gt; 
 &lt;li&gt;Running tests and quality checks&lt;/li&gt; 
 &lt;li&gt;Submitting pull requests&lt;/li&gt; 
 &lt;li&gt;Code style guidelines&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Prompt Modules Collection&lt;/h3&gt; 
&lt;p&gt;Help expand our collection of specialized prompt modules for AI agents:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Advanced testing techniques for vulnerabilities, frameworks, and technologies&lt;/li&gt; 
 &lt;li&gt;See &lt;a href="https://raw.githubusercontent.com/usestrix/strix/main/strix/prompts/README.md"&gt;Prompt Modules Documentation&lt;/a&gt; for guidelines&lt;/li&gt; 
 &lt;li&gt;Submit via &lt;a href="https://github.com/usestrix/strix/pulls"&gt;pull requests&lt;/a&gt; or &lt;a href="https://github.com/usestrix/strix/issues"&gt;issues&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸŒŸ Support the Project&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Love Strix?&lt;/strong&gt; Give us a â­ on GitHub!&lt;/p&gt; 
&lt;h2&gt;ğŸ‘¥ Join Our Community&lt;/h2&gt; 
&lt;p&gt;Have questions? Found a bug? Want to contribute? &lt;strong&gt;&lt;a href="https://discord.gg/J48Fzuh7"&gt;Join our Discord!&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>IDEA-Research/GroundingDINO</title>
      <link>https://github.com/IDEA-Research/GroundingDINO</link>
      <description>&lt;p&gt;[ECCV 2024] Official implementation of the paper "Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection"&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/IDEA-Research/GroundingDINO/main/.asset/grounding_dino_logo.png" width="30%" /&gt; 
&lt;/div&gt; 
&lt;h1&gt;&lt;span&gt;ğŸ¦•&lt;/span&gt; Grounding DINO&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://paperswithcode.com/sota/zero-shot-object-detection-on-mscoco?p=grounding-dino-marrying-dino-with-grounded"&gt;&lt;img src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/grounding-dino-marrying-dino-with-grounded/zero-shot-object-detection-on-mscoco" alt="PWC" /&gt;&lt;/a&gt; &lt;a href="https://paperswithcode.com/sota/zero-shot-object-detection-on-odinw?p=grounding-dino-marrying-dino-with-grounded"&gt;&lt;img src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/grounding-dino-marrying-dino-with-grounded/zero-shot-object-detection-on-odinw" alt="PWC" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;a href="https://paperswithcode.com/sota/object-detection-on-coco-minival?p=grounding-dino-marrying-dino-with-grounded"&gt;&lt;img src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/grounding-dino-marrying-dino-with-grounded/object-detection-on-coco-minival" alt="PWC" /&gt;&lt;/a&gt; &lt;a href="https://paperswithcode.com/sota/object-detection-on-coco?p=grounding-dino-marrying-dino-with-grounded"&gt;&lt;img src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/grounding-dino-marrying-dino-with-grounded/object-detection-on-coco" alt="PWC" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/IDEA-Research"&gt;IDEA-CVR, IDEA-Research&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="http://www.lsl.zone/"&gt;Shilong Liu&lt;/a&gt;, &lt;a href="https://scholar.google.com/citations?user=U_cvvUwAAAAJ&amp;amp;hl=zh-CN&amp;amp;oi=ao"&gt;Zhaoyang Zeng&lt;/a&gt;, &lt;a href="https://rentainhe.github.io/"&gt;Tianhe Ren&lt;/a&gt;, &lt;a href="https://scholar.google.com/citations?user=ybRe9GcAAAAJ&amp;amp;hl=zh-CN"&gt;Feng Li&lt;/a&gt;, &lt;a href="https://scholar.google.com/citations?user=B8hPxMQAAAAJ&amp;amp;hl=zh-CN"&gt;Hao Zhang&lt;/a&gt;, &lt;a href="https://github.com/yangjie-cv"&gt;Jie Yang&lt;/a&gt;, &lt;a href="https://scholar.google.com/citations?user=Zd7WmXUAAAAJ&amp;amp;hl=zh-CN&amp;amp;oi=ao"&gt;Chunyuan Li&lt;/a&gt;, &lt;a href="https://jwyang.github.io/"&gt;Jianwei Yang&lt;/a&gt;, &lt;a href="https://scholar.google.com/citations?hl=en&amp;amp;user=dxN1_X0AAAAJ&amp;amp;view_op=list_works&amp;amp;sortby=pubdate"&gt;Hang Su&lt;/a&gt;, &lt;a href="https://scholar.google.com/citations?hl=en&amp;amp;user=axsP38wAAAAJ"&gt;Jun Zhu&lt;/a&gt;, &lt;a href="https://www.leizhang.org/"&gt;Lei Zhang&lt;/a&gt;&lt;sup&gt;&lt;span&gt;ğŸ“§&lt;/span&gt;&lt;/sup&gt;.&lt;/p&gt; 
&lt;p&gt;[&lt;a href="https://arxiv.org/abs/2303.05499"&gt;&lt;code&gt;Paper&lt;/code&gt;&lt;/a&gt;] [&lt;a href="https://huggingface.co/spaces/ShilongLiu/Grounding_DINO_demo"&gt;&lt;code&gt;Demo&lt;/code&gt;&lt;/a&gt;] [&lt;a href="https://raw.githubusercontent.com/IDEA-Research/GroundingDINO/main/#black_nib-citation"&gt;&lt;code&gt;BibTex&lt;/code&gt;&lt;/a&gt;]&lt;/p&gt; 
&lt;p&gt;PyTorch implementation and pretrained models for Grounding DINO. For details, see the paper &lt;strong&gt;&lt;a href="https://arxiv.org/abs/2303.05499"&gt;Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ”¥ &lt;strong&gt;&lt;a href="https://github.com/IDEA-Research/Grounded-SAM-2"&gt;Grounded SAM 2&lt;/a&gt;&lt;/strong&gt; is released now, which combines Grounding DINO with &lt;a href="https://github.com/facebookresearch/segment-anything-2"&gt;SAM 2&lt;/a&gt; for any object tracking in open-world scenarios.&lt;/li&gt; 
 &lt;li&gt;ğŸ”¥ &lt;strong&gt;&lt;a href="https://github.com/IDEA-Research/Grounding-DINO-1.5-API"&gt;Grounding DINO 1.5&lt;/a&gt;&lt;/strong&gt; is released now, which is IDEA Research's &lt;strong&gt;Most Capable&lt;/strong&gt; Open-World Object Detection Model!&lt;/li&gt; 
 &lt;li&gt;ğŸ”¥ &lt;strong&gt;&lt;a href="https://arxiv.org/abs/2303.05499"&gt;Grounding DINO&lt;/a&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;a href="https://arxiv.org/abs/2401.14159"&gt;Grounded SAM&lt;/a&gt;&lt;/strong&gt; are now supported in Huggingface. For more convenient use, you can refer to &lt;a href="https://huggingface.co/docs/transformers/model_doc/grounding-dino"&gt;this documentation&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;span&gt;ğŸŒ&lt;/span&gt; Helpful Tutorial&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;span&gt;ğŸ‡&lt;/span&gt; [&lt;a href="https://arxiv.org/abs/2303.05499"&gt;Read our arXiv Paper&lt;/a&gt;]&lt;/li&gt; 
 &lt;li&gt;&lt;span&gt;ğŸ&lt;/span&gt; [&lt;a href="https://youtu.be/wxWDt5UiwY8"&gt;Watch our simple introduction video on YouTube&lt;/a&gt;]&lt;/li&gt; 
 &lt;li&gt;&lt;span&gt;ğŸŒ¼&lt;/span&gt; &amp;nbsp;[&lt;a href="https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/zero-shot-object-detection-with-grounding-dino.ipynb"&gt;Try the Colab Demo&lt;/a&gt;]&lt;/li&gt; 
 &lt;li&gt;&lt;span&gt;ğŸŒ»&lt;/span&gt; [&lt;a href="https://huggingface.co/spaces/ShilongLiu/Grounding_DINO_demo"&gt;Try our Official Huggingface Demo&lt;/a&gt;]&lt;/li&gt; 
 &lt;li&gt;&lt;span&gt;ğŸ&lt;/span&gt; [&lt;a href="https://youtu.be/cMa77r3YrDk"&gt;Watch the Step by Step Tutorial about GroundingDINO by Roboflow AI&lt;/a&gt;]&lt;/li&gt; 
 &lt;li&gt;&lt;span&gt;ğŸ„&lt;/span&gt; [&lt;a href="https://youtu.be/C4NqaRBz_Kw"&gt;GroundingDINO: Automated Dataset Annotation and Evaluation by Roboflow AI&lt;/a&gt;]&lt;/li&gt; 
 &lt;li&gt;&lt;span&gt;ğŸŒº&lt;/span&gt; [&lt;a href="https://youtu.be/oEQYStnF2l8"&gt;Accelerate Image Annotation with SAM and GroundingDINO by Roboflow AI&lt;/a&gt;]&lt;/li&gt; 
 &lt;li&gt;&lt;span&gt;ğŸ’®&lt;/span&gt; [&lt;a href="https://github.com/autodistill/autodistill"&gt;Autodistill: Train YOLOv8 with ZERO Annotations based on Grounding-DINO and Grounded-SAM by Roboflow AI&lt;/a&gt;]&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- Grounding DINO Methods | 
[![arXiv](https://img.shields.io/badge/arXiv-2303.05499-b31b1b.svg)](https://arxiv.org/abs/2303.05499) 
[![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://youtu.be/wxWDt5UiwY8) --&gt; 
&lt;!-- Grounding DINO Demos |
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/zero-shot-object-detection-with-grounding-dino.ipynb) --&gt; 
&lt;!-- [![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://youtu.be/cMa77r3YrDk)
[![HuggingFace space](https://img.shields.io/badge/ğŸ¤—-HuggingFace%20Space-cyan.svg)](https://huggingface.co/spaces/ShilongLiu/Grounding_DINO_demo)
[![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://youtu.be/oEQYStnF2l8)
[![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://youtu.be/C4NqaRBz_Kw) --&gt; 
&lt;h2&gt;&lt;span&gt;âœ¨&lt;/span&gt; Highlight Projects&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/UX-Decoder/Semantic-SAM"&gt;Semantic-SAM: a universal image segmentation model to enable segment and recognize anything at any desired granularity.&lt;/a&gt;,&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OptimalScale/DetGPT"&gt;DetGPT: Detect What You Need via Reasoning&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/IDEA-Research/Grounded-Segment-Anything"&gt;Grounded-SAM: Marrying Grounding DINO with Segment Anything&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/IDEA-Research/GroundingDINO/main/demo/image_editing_with_groundingdino_stablediffusion.ipynb"&gt;Grounding DINO with Stable Diffusion&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/IDEA-Research/GroundingDINO/main/demo/image_editing_with_groundingdino_gligen.ipynb"&gt;Grounding DINO with GLIGEN for Controllable Image Editing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/IDEA-Research/OpenSeeD"&gt;OpenSeeD: A Simple and Strong Openset Segmentation Model&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once"&gt;SEEM: Segment Everything Everywhere All at Once&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/microsoft/X-Decoder/tree/xgpt"&gt;X-GPT: Conversational Visual Agent supported by X-Decoder&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/gligen/GLIGEN"&gt;GLIGEN: Open-Set Grounded Text-to-Image Generation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/haotian-liu/LLaVA"&gt;LLaVA: Large Language and Vision Assistant&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- Extensions | [Grounding DINO with Segment Anything](https://github.com/IDEA-Research/Grounded-Segment-Anything); [Grounding DINO with Stable Diffusion](demo/image_editing_with_groundingdino_stablediffusion.ipynb); [Grounding DINO with GLIGEN](demo/image_editing_with_groundingdino_gligen.ipynb)  --&gt; 
&lt;!-- Official PyTorch implementation of [Grounding DINO](https://arxiv.org/abs/2303.05499), a stronger open-set object detector. Code is available now! --&gt; 
&lt;h2&gt;&lt;span&gt;ğŸ’¡&lt;/span&gt; Highlight&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Open-Set Detection.&lt;/strong&gt; Detect &lt;strong&gt;everything&lt;/strong&gt; with language!&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;High Performance.&lt;/strong&gt; COCO zero-shot &lt;strong&gt;52.5 AP&lt;/strong&gt; (training without COCO data!). COCO fine-tune &lt;strong&gt;63.0 AP&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Flexible.&lt;/strong&gt; Collaboration with Stable Diffusion for Image Editting.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;span&gt;ğŸ”¥&lt;/span&gt; News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;2023/07/18&lt;/code&gt;&lt;/strong&gt;: We release &lt;a href="https://github.com/UX-Decoder/Semantic-SAM"&gt;Semantic-SAM&lt;/a&gt;, a universal image segmentation model to enable segment and recognize anything at any desired granularity. &lt;strong&gt;Code&lt;/strong&gt; and &lt;strong&gt;checkpoint&lt;/strong&gt; are available!&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;2023/06/17&lt;/code&gt;&lt;/strong&gt;: We provide an example to evaluate Grounding DINO on COCO zero-shot performance.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;2023/04/15&lt;/code&gt;&lt;/strong&gt;: Refer to &lt;a href="https://github.com/Computer-Vision-in-the-Wild/CVinW_Readings"&gt;CV in the Wild Readings&lt;/a&gt; for those who are interested in open-set recognition!&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;2023/04/08&lt;/code&gt;&lt;/strong&gt;: We release &lt;a href="https://raw.githubusercontent.com/IDEA-Research/GroundingDINO/main/demo/image_editing_with_groundingdino_gligen.ipynb"&gt;demos&lt;/a&gt; to combine &lt;a href="https://arxiv.org/abs/2303.05499"&gt;Grounding DINO&lt;/a&gt; with &lt;a href="https://github.com/gligen/GLIGEN"&gt;GLIGEN&lt;/a&gt; for more controllable image editings.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;2023/04/08&lt;/code&gt;&lt;/strong&gt;: We release &lt;a href="https://raw.githubusercontent.com/IDEA-Research/GroundingDINO/main/demo/image_editing_with_groundingdino_stablediffusion.ipynb"&gt;demos&lt;/a&gt; to combine &lt;a href="https://arxiv.org/abs/2303.05499"&gt;Grounding DINO&lt;/a&gt; with &lt;a href="https://github.com/Stability-AI/StableDiffusion"&gt;Stable Diffusion&lt;/a&gt; for image editings.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;2023/04/06&lt;/code&gt;&lt;/strong&gt;: We build a new demo by marrying GroundingDINO with &lt;a href="https://github.com/facebookresearch/segment-anything"&gt;Segment-Anything&lt;/a&gt; named &lt;strong&gt;&lt;a href="https://github.com/IDEA-Research/Grounded-Segment-Anything"&gt;Grounded-Segment-Anything&lt;/a&gt;&lt;/strong&gt; aims to support segmentation in GroundingDINO.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;2023/03/28&lt;/code&gt;&lt;/strong&gt;: A YouTube &lt;a href="https://youtu.be/cMa77r3YrDk"&gt;video&lt;/a&gt; about Grounding DINO and basic object detection prompt engineering. [&lt;a href="https://github.com/SkalskiP"&gt;SkalskiP&lt;/a&gt;]&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;2023/03/28&lt;/code&gt;&lt;/strong&gt;: Add a &lt;a href="https://huggingface.co/spaces/ShilongLiu/Grounding_DINO_demo"&gt;demo&lt;/a&gt; on Hugging Face Space!&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;2023/03/27&lt;/code&gt;&lt;/strong&gt;: Support CPU-only mode. Now the model can run on machines without GPUs.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;2023/03/25&lt;/code&gt;&lt;/strong&gt;: A &lt;a href="https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/zero-shot-object-detection-with-grounding-dino.ipynb"&gt;demo&lt;/a&gt; for Grounding DINO is available at Colab. [&lt;a href="https://github.com/SkalskiP"&gt;SkalskiP&lt;/a&gt;]&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;2023/03/22&lt;/code&gt;&lt;/strong&gt;: Code is available Now!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details open&gt; 
 &lt;summary&gt;&lt;font size="4"&gt; Description &lt;/font&gt;&lt;/summary&gt; 
 &lt;a href="https://arxiv.org/abs/2303.05499"&gt;Paper&lt;/a&gt; introduction. 
 &lt;img src="https://raw.githubusercontent.com/IDEA-Research/GroundingDINO/main/.asset/hero_figure.png" alt="ODinW" width="100%" /&gt; Marrying 
 &lt;a href="https://github.com/IDEA-Research/GroundingDINO"&gt;Grounding DINO&lt;/a&gt; and 
 &lt;a href="https://github.com/gligen/GLIGEN"&gt;GLIGEN&lt;/a&gt; 
 &lt;img src="https://huggingface.co/ShilongLiu/GroundingDINO/resolve/main/GD_GLIGEN.png" alt="gd_gligen" width="100%" /&gt; 
&lt;/details&gt; 
&lt;h2&gt;&lt;span&gt;â­&lt;/span&gt; Explanations/Tips for Grounding DINO Inputs and Outputs&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Grounding DINO accepts an &lt;code&gt;(image, text)&lt;/code&gt; pair as inputs.&lt;/li&gt; 
 &lt;li&gt;It outputs &lt;code&gt;900&lt;/code&gt; (by default) object boxes. Each box has similarity scores across all input words. (as shown in Figures below.)&lt;/li&gt; 
 &lt;li&gt;We defaultly choose the boxes whose highest similarities are higher than a &lt;code&gt;box_threshold&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;We extract the words whose similarities are higher than the &lt;code&gt;text_threshold&lt;/code&gt; as predicted labels.&lt;/li&gt; 
 &lt;li&gt;If you want to obtain objects of specific phrases, like the &lt;code&gt;dogs&lt;/code&gt; in the sentence &lt;code&gt;two dogs with a stick.&lt;/code&gt;, you can select the boxes with highest text similarities with &lt;code&gt;dogs&lt;/code&gt; as final outputs.&lt;/li&gt; 
 &lt;li&gt;Note that each word can be split to &lt;strong&gt;more than one&lt;/strong&gt; tokens with different tokenlizers. The number of words in a sentence may not equal to the number of text tokens.&lt;/li&gt; 
 &lt;li&gt;We suggest separating different category names with &lt;code&gt;.&lt;/code&gt; for Grounding DINO. &lt;img src="https://raw.githubusercontent.com/IDEA-Research/GroundingDINO/main/.asset/model_explan1.PNG" alt="model_explain1" /&gt; &lt;img src="https://raw.githubusercontent.com/IDEA-Research/GroundingDINO/main/.asset/model_explan2.PNG" alt="model_explain2" /&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;span&gt;ğŸ·&lt;/span&gt; TODO&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Release inference code and demo.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Release checkpoints.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Grounding DINO with Stable Diffusion and GLIGEN demos.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Release training codes.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;span&gt;ğŸ› &lt;/span&gt; Install&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol start="0"&gt; 
 &lt;li&gt;If you have a CUDA environment, please make sure the environment variable &lt;code&gt;CUDA_HOME&lt;/code&gt; is set. It will be compiled under CPU-only mode if no CUDA available.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Please make sure following the installation steps strictly, otherwise the program may produce:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;NameError: name '_C' is not defined
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If this happened, please reinstalled the groundingDINO by reclone the git and do all the installation steps again.&lt;/p&gt; 
&lt;h4&gt;how to check cuda:&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;echo $CUDA_HOME
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If it print nothing, then it means you haven't set up the path/&lt;/p&gt; 
&lt;p&gt;Run this so the environment variable will be set under current shell.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export CUDA_HOME=/path/to/cuda-11.3
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Notice the version of cuda should be aligned with your CUDA runtime, for there might exists multiple cuda at the same time.&lt;/p&gt; 
&lt;p&gt;If you want to set the CUDA_HOME permanently, store it using:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;echo 'export CUDA_HOME=/path/to/cuda' &amp;gt;&amp;gt; ~/.bashrc
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;after that, source the bashrc file and check CUDA_HOME:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;source ~/.bashrc
echo $CUDA_HOME
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;In this example, /path/to/cuda-11.3 should be replaced with the path where your CUDA toolkit is installed. You can find this by typing &lt;strong&gt;which nvcc&lt;/strong&gt; in your terminal:&lt;/p&gt; 
&lt;p&gt;For instance, if the output is /usr/local/cuda/bin/nvcc, then:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export CUDA_HOME=/usr/local/cuda
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Installation:&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;1.Clone the GroundingDINO repository from GitHub.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/IDEA-Research/GroundingDINO.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Change the current directory to the GroundingDINO folder.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd GroundingDINO/
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Install the required dependencies in the current directory.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="4"&gt; 
 &lt;li&gt;Download pre-trained model weights.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;mkdir weights
cd weights
wget -q https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth
cd ..
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;&lt;span&gt;â–¶&lt;/span&gt; Demo&lt;/h2&gt; 
&lt;p&gt;Check your GPU ID (only if you're using a GPU)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;nvidia-smi
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Replace &lt;code&gt;{GPU ID}&lt;/code&gt;, &lt;code&gt;image_you_want_to_detect.jpg&lt;/code&gt;, and &lt;code&gt;"dir you want to save the output"&lt;/code&gt; with appropriate values in the following command&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;CUDA_VISIBLE_DEVICES={GPU ID} python demo/inference_on_a_image.py \
-c groundingdino/config/GroundingDINO_SwinT_OGC.py \
-p weights/groundingdino_swint_ogc.pth \
-i image_you_want_to_detect.jpg \
-o "dir you want to save the output" \
-t "chair"
 [--cpu-only] # open it for cpu mode
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you would like to specify the phrases to detect, here is a demo:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;CUDA_VISIBLE_DEVICES={GPU ID} python demo/inference_on_a_image.py \
-c groundingdino/config/GroundingDINO_SwinT_OGC.py \
-p ./groundingdino_swint_ogc.pth \
-i .asset/cat_dog.jpeg \
-o logs/1111 \
-t "There is a cat and a dog in the image ." \
--token_spans "[[[9, 10], [11, 14]], [[19, 20], [21, 24]]]"
 [--cpu-only] # open it for cpu mode
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The token_spans specify the start and end positions of a phrases. For example, the first phrase is &lt;code&gt;[[9, 10], [11, 14]]&lt;/code&gt;. &lt;code&gt;"There is a cat and a dog in the image ."[9:10] = 'a'&lt;/code&gt;, &lt;code&gt;"There is a cat and a dog in the image ."[11:14] = 'cat'&lt;/code&gt;. Hence it refers to the phrase &lt;code&gt;a cat&lt;/code&gt; . Similarly, the &lt;code&gt;[[19, 20], [21, 24]]&lt;/code&gt; refers to the phrase &lt;code&gt;a dog&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;See the &lt;code&gt;demo/inference_on_a_image.py&lt;/code&gt; for more details.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Running with Python:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from groundingdino.util.inference import load_model, load_image, predict, annotate
import cv2

model = load_model("groundingdino/config/GroundingDINO_SwinT_OGC.py", "weights/groundingdino_swint_ogc.pth")
IMAGE_PATH = "weights/dog-3.jpeg"
TEXT_PROMPT = "chair . person . dog ."
BOX_TRESHOLD = 0.35
TEXT_TRESHOLD = 0.25

image_source, image = load_image(IMAGE_PATH)

boxes, logits, phrases = predict(
    model=model,
    image=image,
    caption=TEXT_PROMPT,
    box_threshold=BOX_TRESHOLD,
    text_threshold=TEXT_TRESHOLD
)

annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)
cv2.imwrite("annotated_image.jpg", annotated_frame)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Web UI&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;We also provide a demo code to integrate Grounding DINO with Gradio Web UI. See the file &lt;code&gt;demo/gradio_app.py&lt;/code&gt; for more details.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Notebooks&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;We release &lt;a href="https://raw.githubusercontent.com/IDEA-Research/GroundingDINO/main/demo/image_editing_with_groundingdino_gligen.ipynb"&gt;demos&lt;/a&gt; to combine &lt;a href="https://arxiv.org/abs/2303.05499"&gt;Grounding DINO&lt;/a&gt; with &lt;a href="https://github.com/gligen/GLIGEN"&gt;GLIGEN&lt;/a&gt; for more controllable image editings.&lt;/li&gt; 
 &lt;li&gt;We release &lt;a href="https://raw.githubusercontent.com/IDEA-Research/GroundingDINO/main/demo/image_editing_with_groundingdino_stablediffusion.ipynb"&gt;demos&lt;/a&gt; to combine &lt;a href="https://arxiv.org/abs/2303.05499"&gt;Grounding DINO&lt;/a&gt; with &lt;a href="https://github.com/Stability-AI/StableDiffusion"&gt;Stable Diffusion&lt;/a&gt; for image editings.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;COCO Zero-shot Evaluations&lt;/h2&gt; 
&lt;p&gt;We provide an example to evaluate Grounding DINO zero-shot performance on COCO. The results should be &lt;strong&gt;48.5&lt;/strong&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;CUDA_VISIBLE_DEVICES=0 \
python demo/test_ap_on_coco.py \
 -c groundingdino/config/GroundingDINO_SwinT_OGC.py \
 -p weights/groundingdino_swint_ogc.pth \
 --anno_path /path/to/annoataions/ie/instances_val2017.json \
 --image_dir /path/to/imagedir/ie/val2017
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;&lt;span&gt;ğŸ§³&lt;/span&gt; Checkpoints&lt;/h2&gt; 
&lt;!-- insert a table --&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr style="text-align: right;"&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th&gt;name&lt;/th&gt; 
   &lt;th&gt;backbone&lt;/th&gt; 
   &lt;th&gt;Data&lt;/th&gt; 
   &lt;th&gt;box AP on COCO&lt;/th&gt; 
   &lt;th&gt;Checkpoint&lt;/th&gt; 
   &lt;th&gt;Config&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;th&gt;1&lt;/th&gt; 
   &lt;td&gt;GroundingDINO-T&lt;/td&gt; 
   &lt;td&gt;Swin-T&lt;/td&gt; 
   &lt;td&gt;O365,GoldG,Cap4M&lt;/td&gt; 
   &lt;td&gt;48.4 (zero-shot) / 57.2 (fine-tune)&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth"&gt;GitHub link&lt;/a&gt; | &lt;a href="https://huggingface.co/ShilongLiu/GroundingDINO/resolve/main/groundingdino_swint_ogc.pth"&gt;HF link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/IDEA-Research/GroundingDINO/raw/main/groundingdino/config/GroundingDINO_SwinT_OGC.py"&gt;link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;th&gt;2&lt;/th&gt; 
   &lt;td&gt;GroundingDINO-B&lt;/td&gt; 
   &lt;td&gt;Swin-B&lt;/td&gt; 
   &lt;td&gt;COCO,O365,GoldG,Cap4M,OpenImage,ODinW-35,RefCOCO&lt;/td&gt; 
   &lt;td&gt;56.7 &lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha2/groundingdino_swinb_cogcoor.pth"&gt;GitHub link&lt;/a&gt; | &lt;a href="https://huggingface.co/ShilongLiu/GroundingDINO/resolve/main/groundingdino_swinb_cogcoor.pth"&gt;HF link&lt;/a&gt; &lt;/td&gt;
   &lt;td&gt;&lt;a href="https://github.com/IDEA-Research/GroundingDINO/raw/main/groundingdino/config/GroundingDINO_SwinB_cfg.py"&gt;link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;&lt;span&gt;ğŸ–&lt;/span&gt; Results&lt;/h2&gt; 
&lt;details open&gt; 
 &lt;summary&gt;&lt;font size="4"&gt; COCO Object Detection Results &lt;/font&gt;&lt;/summary&gt; 
 &lt;img src="https://raw.githubusercontent.com/IDEA-Research/GroundingDINO/main/.asset/COCO.png" alt="COCO" width="100%" /&gt; 
&lt;/details&gt; 
&lt;details open&gt; 
 &lt;summary&gt;&lt;font size="4"&gt; ODinW Object Detection Results &lt;/font&gt;&lt;/summary&gt; 
 &lt;img src="https://raw.githubusercontent.com/IDEA-Research/GroundingDINO/main/.asset/ODinW.png" alt="ODinW" width="100%" /&gt; 
&lt;/details&gt; 
&lt;details open&gt; 
 &lt;summary&gt;&lt;font size="4"&gt; Marrying Grounding DINO with &lt;a href="https://github.com/Stability-AI/StableDiffusion"&gt;Stable Diffusion&lt;/a&gt; for Image Editing &lt;/font&gt;&lt;/summary&gt; See our example 
 &lt;a href="https://github.com/IDEA-Research/GroundingDINO/raw/main/demo/image_editing_with_groundingdino_stablediffusion.ipynb"&gt;notebook&lt;/a&gt; for more details. 
 &lt;img src="https://raw.githubusercontent.com/IDEA-Research/GroundingDINO/main/.asset/GD_SD.png" alt="GD_SD" width="100%" /&gt; 
&lt;/details&gt; 
&lt;details open&gt; 
 &lt;summary&gt;&lt;font size="4"&gt; Marrying Grounding DINO with &lt;a href="https://github.com/gligen/GLIGEN"&gt;GLIGEN&lt;/a&gt; for more Detailed Image Editing. &lt;/font&gt;&lt;/summary&gt; See our example 
 &lt;a href="https://github.com/IDEA-Research/GroundingDINO/raw/main/demo/image_editing_with_groundingdino_gligen.ipynb"&gt;notebook&lt;/a&gt; for more details. 
 &lt;img src="https://raw.githubusercontent.com/IDEA-Research/GroundingDINO/main/.asset/GD_GLIGEN.png" alt="GD_GLIGEN" width="100%" /&gt; 
&lt;/details&gt; 
&lt;h2&gt;&lt;span&gt;ğŸ¦•&lt;/span&gt; Model: Grounding DINO&lt;/h2&gt; 
&lt;p&gt;Includes: a text backbone, an image backbone, a feature enhancer, a language-guided query selection, and a cross-modality decoder.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/IDEA-Research/GroundingDINO/main/.asset/arch.png" alt="arch" /&gt;&lt;/p&gt; 
&lt;h2&gt;&lt;span&gt;â™¥&lt;/span&gt; Acknowledgement&lt;/h2&gt; 
&lt;p&gt;Our model is related to &lt;a href="https://github.com/IDEA-Research/DINO"&gt;DINO&lt;/a&gt; and &lt;a href="https://github.com/microsoft/GLIP"&gt;GLIP&lt;/a&gt;. Thanks for their great work!&lt;/p&gt; 
&lt;p&gt;We also thank great previous work including DETR, Deformable DETR, SMCA, Conditional DETR, Anchor DETR, Dynamic DETR, DAB-DETR, DN-DETR, etc. More related work are available at &lt;a href="https://github.com/IDEACVR/awesome-detection-transformer"&gt;Awesome Detection Transformer&lt;/a&gt;. A new toolbox &lt;a href="https://github.com/IDEA-Research/detrex"&gt;detrex&lt;/a&gt; is available as well.&lt;/p&gt; 
&lt;p&gt;Thanks &lt;a href="https://github.com/Stability-AI/StableDiffusion"&gt;Stable Diffusion&lt;/a&gt; and &lt;a href="https://github.com/gligen/GLIGEN"&gt;GLIGEN&lt;/a&gt; for their awesome models.&lt;/p&gt; 
&lt;h2&gt;&lt;span&gt;âœ’&lt;/span&gt; Citation&lt;/h2&gt; 
&lt;p&gt;If you find our work helpful for your research, please consider citing the following BibTeX entry.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@article{liu2023grounding,
  title={Grounding dino: Marrying dino with grounded pre-training for open-set object detection},
  author={Liu, Shilong and Zeng, Zhaoyang and Ren, Tianhe and Li, Feng and Zhang, Hao and Yang, Jie and Li, Chunyuan and Yang, Jianwei and Su, Hang and Zhu, Jun and others},
  journal={arXiv preprint arXiv:2303.05499},
  year={2023}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>AsyncFuncAI/deepwiki-open</title>
      <link>https://github.com/AsyncFuncAI/deepwiki-open</link>
      <description>&lt;p&gt;Open Source DeepWiki: AI-Powered Wiki Generator for GitHub/Gitlab/Bitbucket Repositories. Join the discord: https://discord.gg/gMwThUMeme&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;DeepWiki-Open&lt;/h1&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/AsyncFuncAI/deepwiki-open/main/screenshots/Deepwiki.png" alt="DeepWiki Banner" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;DeepWiki&lt;/strong&gt; is my own implementation attempt of DeepWiki, automatically creates beautiful, interactive wikis for any GitHub, GitLab, or BitBucket repository! Just enter a repo name, and DeepWiki will:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Analyze the code structure&lt;/li&gt; 
 &lt;li&gt;Generate comprehensive documentation&lt;/li&gt; 
 &lt;li&gt;Create visual diagrams to explain how everything works&lt;/li&gt; 
 &lt;li&gt;Organize it all into an easy-to-navigate wiki&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;a href="https://buymeacoffee.com/sheing"&gt;&lt;img src="https://www.buymeacoffee.com/assets/img/custom_images/orange_img.png" alt="&amp;quot;Buy Me A Coffee&amp;quot;" /&gt;&lt;/a&gt; &lt;a href="https://tip.md/sng-asyncfunc"&gt;&lt;img src="https://tip.md/badge.svg?sanitize=true" alt="Tip in Crypto" /&gt;&lt;/a&gt; &lt;a href="https://x.com/sashimikun_void"&gt;&lt;img src="https://img.shields.io/badge/Twitter-1DA1F2?style=for-the-badge&amp;amp;logo=twitter&amp;amp;logoColor=white" alt="Twitter/X" /&gt;&lt;/a&gt; &lt;a href="https://discord.com/invite/VQMBGR8u5v"&gt;&lt;img src="https://img.shields.io/badge/Discord-7289DA?style=for-the-badge&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/AsyncFuncAI/deepwiki-open/main/README.md"&gt;English&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/AsyncFuncAI/deepwiki-open/main/README.zh.md"&gt;ç®€ä½“ä¸­æ–‡&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/AsyncFuncAI/deepwiki-open/main/README.zh-tw.md"&gt;ç¹é«”ä¸­æ–‡&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/AsyncFuncAI/deepwiki-open/main/README.ja.md"&gt;æ—¥æœ¬èª&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/AsyncFuncAI/deepwiki-open/main/README.es.md"&gt;EspaÃ±ol&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/AsyncFuncAI/deepwiki-open/main/README.kr.md"&gt;í•œêµ­ì–´&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/AsyncFuncAI/deepwiki-open/main/README.vi.md"&gt;Tiáº¿ng Viá»‡t&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/AsyncFuncAI/deepwiki-open/main/README.pt-br.md"&gt;PortuguÃªs Brasileiro&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/AsyncFuncAI/deepwiki-open/main/README.fr.md"&gt;FranÃ§ais&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/AsyncFuncAI/deepwiki-open/main/README.ru.md"&gt;Ğ ÑƒÑÑĞºĞ¸Ğ¹&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;âœ¨ Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Instant Documentation&lt;/strong&gt;: Turn any GitHub, GitLab or BitBucket repo into a wiki in seconds&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Private Repository Support&lt;/strong&gt;: Securely access private repositories with personal access tokens&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Smart Analysis&lt;/strong&gt;: AI-powered understanding of code structure and relationships&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Beautiful Diagrams&lt;/strong&gt;: Automatic Mermaid diagrams to visualize architecture and data flow&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Easy Navigation&lt;/strong&gt;: Simple, intuitive interface to explore the wiki&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Ask Feature&lt;/strong&gt;: Chat with your repository using RAG-powered AI to get accurate answers&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;DeepResearch&lt;/strong&gt;: Multi-turn research process that thoroughly investigates complex topics&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multiple Model Providers&lt;/strong&gt;: Support for Google Gemini, OpenAI, OpenRouter, and local Ollama models&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Flexible Embeddings&lt;/strong&gt;: Choose between OpenAI, Google AI, or local Ollama embeddings for optimal performance&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸš€ Quick Start (Super Easy!)&lt;/h2&gt; 
&lt;h3&gt;Option 1: Using Docker&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Clone the repository
git clone https://github.com/AsyncFuncAI/deepwiki-open.git
cd deepwiki-open

# Create a .env file with your API keys
echo "GOOGLE_API_KEY=your_google_api_key" &amp;gt; .env
echo "OPENAI_API_KEY=your_openai_api_key" &amp;gt;&amp;gt; .env
# Optional: Use Google AI embeddings instead of OpenAI (recommended if using Google models)
echo "DEEPWIKI_EMBEDDER_TYPE=google" &amp;gt;&amp;gt; .env
# Optional: Add OpenRouter API key if you want to use OpenRouter models
echo "OPENROUTER_API_KEY=your_openrouter_api_key" &amp;gt;&amp;gt; .env
# Optional: Add Ollama host if not local. defaults to http://localhost:11434
echo "OLLAMA_HOST=your_ollama_host" &amp;gt;&amp;gt; .env
# Optional: Add Azure API key, endpoint and version if you want to use azure openai models
echo "AZURE_OPENAI_API_KEY=your_azure_openai_api_key" &amp;gt;&amp;gt; .env
echo "AZURE_OPENAI_ENDPOINT=your_azure_openai_endpoint" &amp;gt;&amp;gt; .env
echo "AZURE_OPENAI_VERSION=your_azure_openai_version" &amp;gt;&amp;gt; .env
# Run with Docker Compose
docker-compose up
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For detailed instructions on using DeepWiki with Ollama and Docker, see &lt;a href="https://raw.githubusercontent.com/AsyncFuncAI/deepwiki-open/main/Ollama-instruction.md"&gt;Ollama Instructions&lt;/a&gt;.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ğŸ’¡ &lt;strong&gt;Where to get these keys:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Get a Google API key from &lt;a href="https://makersuite.google.com/app/apikey"&gt;Google AI Studio&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Get an OpenAI API key from &lt;a href="https://platform.openai.com/api-keys"&gt;OpenAI Platform&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Get Azure OpenAI credentials from &lt;a href="https://portal.azure.com/"&gt;Azure Portal&lt;/a&gt; - create an Azure OpenAI resource and get the API key, endpoint, and API version&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Option 2: Manual Setup (Recommended)&lt;/h3&gt; 
&lt;h4&gt;Step 1: Set Up Your API Keys&lt;/h4&gt; 
&lt;p&gt;Create a &lt;code&gt;.env&lt;/code&gt; file in the project root with these keys:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;GOOGLE_API_KEY=your_google_api_key
OPENAI_API_KEY=your_openai_api_key
# Optional: Use Google AI embeddings (recommended if using Google models)
DEEPWIKI_EMBEDDER_TYPE=google
# Optional: Add this if you want to use OpenRouter models
OPENROUTER_API_KEY=your_openrouter_api_key
# Optional: Add this if you want to use Azure OpenAI models
AZURE_OPENAI_API_KEY=your_azure_openai_api_key
AZURE_OPENAI_ENDPOINT=your_azure_openai_endpoint
AZURE_OPENAI_VERSION=your_azure_openai_version
# Optional: Add Ollama host if not local. default: http://localhost:11434
OLLAMA_HOST=your_ollama_host
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Step 2: Start the Backend&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install Python dependencies
python -m pip install poetry==1.8.2 &amp;amp;&amp;amp; poetry install -C api

# Start the API server
python -m api.main
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Step 3: Start the Frontend&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install JavaScript dependencies
npm install
# or
yarn install

# Start the web app
npm run dev
# or
yarn dev
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Step 4: Use DeepWiki!&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;Open &lt;a href="http://localhost:3000"&gt;http://localhost:3000&lt;/a&gt; in your browser&lt;/li&gt; 
 &lt;li&gt;Enter a GitHub, GitLab, or Bitbucket repository (like &lt;code&gt;https://github.com/openai/codex&lt;/code&gt;, &lt;code&gt;https://github.com/microsoft/autogen&lt;/code&gt;, &lt;code&gt;https://gitlab.com/gitlab-org/gitlab&lt;/code&gt;, or &lt;code&gt;https://bitbucket.org/redradish/atlassian_app_versions&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;For private repositories, click "+ Add access tokens" and enter your GitHub or GitLab personal access token&lt;/li&gt; 
 &lt;li&gt;Click "Generate Wiki" and watch the magic happen!&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;ğŸ” How It Works&lt;/h2&gt; 
&lt;p&gt;DeepWiki uses AI to:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Clone and analyze the GitHub, GitLab, or Bitbucket repository (including private repos with token authentication)&lt;/li&gt; 
 &lt;li&gt;Create embeddings of the code for smart retrieval&lt;/li&gt; 
 &lt;li&gt;Generate documentation with context-aware AI (using Google Gemini, OpenAI, OpenRouter, Azure OpenAI, or local Ollama models)&lt;/li&gt; 
 &lt;li&gt;Create visual diagrams to explain code relationships&lt;/li&gt; 
 &lt;li&gt;Organize everything into a structured wiki&lt;/li&gt; 
 &lt;li&gt;Enable intelligent Q&amp;amp;A with the repository through the Ask feature&lt;/li&gt; 
 &lt;li&gt;Provide in-depth research capabilities with DeepResearch&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-mermaid"&gt;graph TD
    A[User inputs GitHub/GitLab/Bitbucket repo] --&amp;gt; AA{Private repo?}
    AA --&amp;gt;|Yes| AB[Add access token]
    AA --&amp;gt;|No| B[Clone Repository]
    AB --&amp;gt; B
    B --&amp;gt; C[Analyze Code Structure]
    C --&amp;gt; D[Create Code Embeddings]

    D --&amp;gt; M{Select Model Provider}
    M --&amp;gt;|Google Gemini| E1[Generate with Gemini]
    M --&amp;gt;|OpenAI| E2[Generate with OpenAI]
    M --&amp;gt;|OpenRouter| E3[Generate with OpenRouter]
    M --&amp;gt;|Local Ollama| E4[Generate with Ollama]
    M --&amp;gt;|Azure| E5[Generate with Azure]

    E1 --&amp;gt; E[Generate Documentation]
    E2 --&amp;gt; E
    E3 --&amp;gt; E
    E4 --&amp;gt; E
    E5 --&amp;gt; E

    D --&amp;gt; F[Create Visual Diagrams]
    E --&amp;gt; G[Organize as Wiki]
    F --&amp;gt; G
    G --&amp;gt; H[Interactive DeepWiki]

    classDef process stroke-width:2px;
    classDef data stroke-width:2px;
    classDef result stroke-width:2px;
    classDef decision stroke-width:2px;

    class A,D data;
    class AA,M decision;
    class B,C,E,F,G,AB,E1,E2,E3,E4,E5 process;
    class H result;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ğŸ› ï¸ Project Structure&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;deepwiki/
â”œâ”€â”€ api/                  # Backend API server
â”‚   â”œâ”€â”€ main.py           # API entry point
â”‚   â”œâ”€â”€ api.py            # FastAPI implementation
â”‚   â”œâ”€â”€ rag.py            # Retrieval Augmented Generation
â”‚   â”œâ”€â”€ data_pipeline.py  # Data processing utilities
â”‚   â””â”€â”€ requirements.txt  # Python dependencies
â”‚
â”œâ”€â”€ src/                  # Frontend Next.js app
â”‚   â”œâ”€â”€ app/              # Next.js app directory
â”‚   â”‚   â””â”€â”€ page.tsx      # Main application page
â”‚   â””â”€â”€ components/       # React components
â”‚       â””â”€â”€ Mermaid.tsx   # Mermaid diagram renderer
â”‚
â”œâ”€â”€ public/               # Static assets
â”œâ”€â”€ package.json          # JavaScript dependencies
â””â”€â”€ .env                  # Environment variables (create this)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ğŸ¤– Provider-Based Model Selection System&lt;/h2&gt; 
&lt;p&gt;DeepWiki now implements a flexible provider-based model selection system supporting multiple LLM providers:&lt;/p&gt; 
&lt;h3&gt;Supported Providers and Models&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Google&lt;/strong&gt;: Default &lt;code&gt;gemini-2.5-flash&lt;/code&gt;, also supports &lt;code&gt;gemini-2.5-flash-lite&lt;/code&gt;, &lt;code&gt;gemini-2.5-pro&lt;/code&gt;, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;OpenAI&lt;/strong&gt;: Default &lt;code&gt;gpt-5-nano&lt;/code&gt;, also supports &lt;code&gt;gpt-5&lt;/code&gt;, &lt;code&gt;4o&lt;/code&gt;, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;OpenRouter&lt;/strong&gt;: Access to multiple models via a unified API, including Claude, Llama, Mistral, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Azure OpenAI&lt;/strong&gt;: Default &lt;code&gt;gpt-4o&lt;/code&gt;, also supports &lt;code&gt;o4-mini&lt;/code&gt;, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Ollama&lt;/strong&gt;: Support for locally running open-source models like &lt;code&gt;llama3&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Environment Variables&lt;/h3&gt; 
&lt;p&gt;Each provider requires its corresponding API key environment variables:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;# API Keys
GOOGLE_API_KEY=your_google_api_key        # Required for Google Gemini models
OPENAI_API_KEY=your_openai_api_key        # Required for OpenAI models
OPENROUTER_API_KEY=your_openrouter_api_key # Required for OpenRouter models
AZURE_OPENAI_API_KEY=your_azure_openai_api_key  #Required for Azure OpenAI models
AZURE_OPENAI_ENDPOINT=your_azure_openai_endpoint  #Required for Azure OpenAI models
AZURE_OPENAI_VERSION=your_azure_openai_version  #Required for Azure OpenAI models

# OpenAI API Base URL Configuration
OPENAI_BASE_URL=https://custom-api-endpoint.com/v1  # Optional, for custom OpenAI API endpoints

# Ollama host
OLLAMA_HOST=your_ollama_host # Optional, if Ollama is not local. default: http://localhost:11434

# Configuration Directory
DEEPWIKI_CONFIG_DIR=/path/to/custom/config/dir  # Optional, for custom config file location
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Configuration Files&lt;/h3&gt; 
&lt;p&gt;DeepWiki uses JSON configuration files to manage various aspects of the system:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;generator.json&lt;/code&gt;&lt;/strong&gt;: Configuration for text generation models&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Defines available model providers (Google, OpenAI, OpenRouter, Azure, Ollama)&lt;/li&gt; 
   &lt;li&gt;Specifies default and available models for each provider&lt;/li&gt; 
   &lt;li&gt;Contains model-specific parameters like temperature and top_p&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;embedder.json&lt;/code&gt;&lt;/strong&gt;: Configuration for embedding models and text processing&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Defines embedding models for vector storage&lt;/li&gt; 
   &lt;li&gt;Contains retriever configuration for RAG&lt;/li&gt; 
   &lt;li&gt;Specifies text splitter settings for document chunking&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;repo.json&lt;/code&gt;&lt;/strong&gt;: Configuration for repository handling&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Contains file filters to exclude certain files and directories&lt;/li&gt; 
   &lt;li&gt;Defines repository size limits and processing rules&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;By default, these files are located in the &lt;code&gt;api/config/&lt;/code&gt; directory. You can customize their location using the &lt;code&gt;DEEPWIKI_CONFIG_DIR&lt;/code&gt; environment variable.&lt;/p&gt; 
&lt;h3&gt;Custom Model Selection for Service Providers&lt;/h3&gt; 
&lt;p&gt;The custom model selection feature is specifically designed for service providers who need to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;You can offer multiple AI model choices to users within your organization&lt;/li&gt; 
 &lt;li&gt;You can quickly adapt to the rapidly evolving LLM landscape without code changes&lt;/li&gt; 
 &lt;li&gt;You can support specialized or fine-tuned models that aren't in the predefined list&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Service providers can implement their model offerings by selecting from the predefined options or entering custom model identifiers in the frontend interface.&lt;/p&gt; 
&lt;h3&gt;Base URL Configuration for Enterprise Private Channels&lt;/h3&gt; 
&lt;p&gt;The OpenAI Client's base_url configuration is designed primarily for enterprise users with private API channels. This feature:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Enables connection to private or enterprise-specific API endpoints&lt;/li&gt; 
 &lt;li&gt;Allows organizations to use their own self-hosted or custom-deployed LLM services&lt;/li&gt; 
 &lt;li&gt;Supports integration with third-party OpenAI API-compatible services&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Coming Soon&lt;/strong&gt;: In future updates, DeepWiki will support a mode where users need to provide their own API keys in requests. This will allow enterprise customers with private channels to use their existing API arrangements without sharing credentials with the DeepWiki deployment.&lt;/p&gt; 
&lt;h2&gt;ğŸ§© Using OpenAI-Compatible Embedding Models (e.g., Alibaba Qwen)&lt;/h2&gt; 
&lt;p&gt;If you want to use embedding models compatible with the OpenAI API (such as Alibaba Qwen), follow these steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Replace the contents of &lt;code&gt;api/config/embedder.json&lt;/code&gt; with those from &lt;code&gt;api/config/embedder_openai_compatible.json&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;In your project root &lt;code&gt;.env&lt;/code&gt; file, set the relevant environment variables, for example: &lt;pre&gt;&lt;code&gt;OPENAI_API_KEY=your_api_key
OPENAI_BASE_URL=your_openai_compatible_endpoint
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;The program will automatically substitute placeholders in embedder.json with the values from your environment variables.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;This allows you to seamlessly switch to any OpenAI-compatible embedding service without code changes.&lt;/p&gt; 
&lt;h2&gt;ğŸ§  Using Google AI Embeddings&lt;/h2&gt; 
&lt;p&gt;DeepWiki now supports Google AI's latest embedding models as an alternative to OpenAI embeddings. This provides better integration when you're already using Google Gemini models for text generation.&lt;/p&gt; 
&lt;h3&gt;Features&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Latest Model&lt;/strong&gt;: Uses Google's &lt;code&gt;text-embedding-004&lt;/code&gt; model&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Same API Key&lt;/strong&gt;: Uses your existing &lt;code&gt;GOOGLE_API_KEY&lt;/code&gt; (no additional setup required)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Better Integration&lt;/strong&gt;: Optimized for use with Google Gemini text generation models&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Task-Specific&lt;/strong&gt;: Supports semantic similarity, retrieval, and classification tasks&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Batch Processing&lt;/strong&gt;: Efficient processing of multiple texts&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;How to Enable Google AI Embeddings&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Option 1: Environment Variable (Recommended)&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Set the embedder type in your &lt;code&gt;.env&lt;/code&gt; file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Your existing Google API key
GOOGLE_API_KEY=your_google_api_key

# Enable Google AI embeddings
DEEPWIKI_EMBEDDER_TYPE=google
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Option 2: Docker Environment&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run -p 8001:8001 -p 3000:3000 \
  -e GOOGLE_API_KEY=your_google_api_key \
  -e DEEPWIKI_EMBEDDER_TYPE=google \
  -v ~/.adalflow:/root/.adalflow \
  ghcr.io/asyncfuncai/deepwiki-open:latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Option 3: Docker Compose&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Add to your &lt;code&gt;.env&lt;/code&gt; file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;GOOGLE_API_KEY=your_google_api_key
DEEPWIKI_EMBEDDER_TYPE=google
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker-compose up
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Available Embedder Types&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;API Key Required&lt;/th&gt; 
   &lt;th&gt;Notes&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;openai&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;OpenAI embeddings (default)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Uses &lt;code&gt;text-embedding-3-small&lt;/code&gt; model&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;google&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Google AI embeddings&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;GOOGLE_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Uses &lt;code&gt;text-embedding-004&lt;/code&gt; model&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ollama&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Local Ollama embeddings&lt;/td&gt; 
   &lt;td&gt;None&lt;/td&gt; 
   &lt;td&gt;Requires local Ollama installation&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Why Use Google AI Embeddings?&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Consistency&lt;/strong&gt;: If you're using Google Gemini for text generation, using Google embeddings provides better semantic consistency&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Performance&lt;/strong&gt;: Google's latest embedding model offers excellent performance for retrieval tasks&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Cost&lt;/strong&gt;: Competitive pricing compared to OpenAI&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;No Additional Setup&lt;/strong&gt;: Uses the same API key as your text generation models&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Switching Between Embedders&lt;/h3&gt; 
&lt;p&gt;You can easily switch between different embedding providers:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Use OpenAI embeddings (default)
export DEEPWIKI_EMBEDDER_TYPE=openai

# Use Google AI embeddings
export DEEPWIKI_EMBEDDER_TYPE=google

# Use local Ollama embeddings
export DEEPWIKI_EMBEDDER_TYPE=ollama
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: When switching embedders, you may need to regenerate your repository embeddings as different models produce different vector spaces.&lt;/p&gt; 
&lt;h3&gt;Logging&lt;/h3&gt; 
&lt;p&gt;DeepWiki uses Python's built-in &lt;code&gt;logging&lt;/code&gt; module for diagnostic output. You can configure the verbosity and log file destination via environment variables:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Default&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;LOG_LEVEL&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL).&lt;/td&gt; 
   &lt;td&gt;INFO&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;LOG_FILE_PATH&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Path to the log file. If set, logs will be written to this file.&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;api/logs/application.log&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;To enable debug logging and direct logs to a custom file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export LOG_LEVEL=DEBUG
export LOG_FILE_PATH=./debug.log
python -m api.main
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or with Docker Compose:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;LOG_LEVEL=DEBUG LOG_FILE_PATH=./debug.log docker-compose up
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;When running with Docker Compose, the container's &lt;code&gt;api/logs&lt;/code&gt; directory is bind-mounted to &lt;code&gt;./api/logs&lt;/code&gt; on your host (see the &lt;code&gt;volumes&lt;/code&gt; section in &lt;code&gt;docker-compose.yml&lt;/code&gt;), ensuring log files persist across restarts.&lt;/p&gt; 
&lt;p&gt;Alternatively, you can store these settings in your &lt;code&gt;.env&lt;/code&gt; file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;LOG_LEVEL=DEBUG
LOG_FILE_PATH=./debug.log
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then simply run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker-compose up
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Logging Path Security Considerations:&lt;/strong&gt; In production environments, ensure the &lt;code&gt;api/logs&lt;/code&gt; directory and any custom log file path are secured with appropriate filesystem permissions and access controls. The application enforces that &lt;code&gt;LOG_FILE_PATH&lt;/code&gt; resides within the project's &lt;code&gt;api/logs&lt;/code&gt; directory to prevent path traversal or unauthorized writes.&lt;/p&gt; 
&lt;h2&gt;ğŸ› ï¸ Advanced Setup&lt;/h2&gt; 
&lt;h3&gt;Environment Variables&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Required&lt;/th&gt; 
   &lt;th&gt;Note&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;GOOGLE_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Google Gemini API key for AI generation and embeddings&lt;/td&gt; 
   &lt;td&gt;No&lt;/td&gt; 
   &lt;td&gt;Required for Google Gemini models and Google AI embeddings&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;OpenAI API key for embeddings and models&lt;/td&gt; 
   &lt;td&gt;Conditional&lt;/td&gt; 
   &lt;td&gt;Required if using OpenAI embeddings or models&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENROUTER_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;OpenRouter API key for alternative models&lt;/td&gt; 
   &lt;td&gt;No&lt;/td&gt; 
   &lt;td&gt;Required only if you want to use OpenRouter models&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;AZURE_OPENAI_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Azure OpenAI API key&lt;/td&gt; 
   &lt;td&gt;No&lt;/td&gt; 
   &lt;td&gt;Required only if you want to use Azure OpenAI models&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;AZURE_OPENAI_ENDPOINT&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Azure OpenAI endpoint&lt;/td&gt; 
   &lt;td&gt;No&lt;/td&gt; 
   &lt;td&gt;Required only if you want to use Azure OpenAI models&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;AZURE_OPENAI_VERSION&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Azure OpenAI version&lt;/td&gt; 
   &lt;td&gt;No&lt;/td&gt; 
   &lt;td&gt;Required only if you want to use Azure OpenAI models&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OLLAMA_HOST&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Ollama Host (default: &lt;a href="http://localhost:11434"&gt;http://localhost:11434&lt;/a&gt;)&lt;/td&gt; 
   &lt;td&gt;No&lt;/td&gt; 
   &lt;td&gt;Required only if you want to use external Ollama server&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;DEEPWIKI_EMBEDDER_TYPE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Embedder type: &lt;code&gt;openai&lt;/code&gt;, &lt;code&gt;google&lt;/code&gt;, or &lt;code&gt;ollama&lt;/code&gt; (default: &lt;code&gt;openai&lt;/code&gt;)&lt;/td&gt; 
   &lt;td&gt;No&lt;/td&gt; 
   &lt;td&gt;Controls which embedding provider to use&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;PORT&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Port for the API server (default: 8001)&lt;/td&gt; 
   &lt;td&gt;No&lt;/td&gt; 
   &lt;td&gt;If you host API and frontend on the same machine, make sure change port of &lt;code&gt;SERVER_BASE_URL&lt;/code&gt; accordingly&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;SERVER_BASE_URL&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Base URL for the API server (default: &lt;a href="http://localhost:8001"&gt;http://localhost:8001&lt;/a&gt;)&lt;/td&gt; 
   &lt;td&gt;No&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;DEEPWIKI_AUTH_MODE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Set to &lt;code&gt;true&lt;/code&gt; or &lt;code&gt;1&lt;/code&gt; to enable authorization mode.&lt;/td&gt; 
   &lt;td&gt;No&lt;/td&gt; 
   &lt;td&gt;Defaults to &lt;code&gt;false&lt;/code&gt;. If enabled, &lt;code&gt;DEEPWIKI_AUTH_CODE&lt;/code&gt; is required.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;DEEPWIKI_AUTH_CODE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;The secret code required for wiki generation when &lt;code&gt;DEEPWIKI_AUTH_MODE&lt;/code&gt; is enabled.&lt;/td&gt; 
   &lt;td&gt;No&lt;/td&gt; 
   &lt;td&gt;Only used if &lt;code&gt;DEEPWIKI_AUTH_MODE&lt;/code&gt; is &lt;code&gt;true&lt;/code&gt; or &lt;code&gt;1&lt;/code&gt;.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;API Key Requirements:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;If using &lt;code&gt;DEEPWIKI_EMBEDDER_TYPE=openai&lt;/code&gt; (default): &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; is required&lt;/li&gt; 
 &lt;li&gt;If using &lt;code&gt;DEEPWIKI_EMBEDDER_TYPE=google&lt;/code&gt;: &lt;code&gt;GOOGLE_API_KEY&lt;/code&gt; is required&lt;/li&gt; 
 &lt;li&gt;If using &lt;code&gt;DEEPWIKI_EMBEDDER_TYPE=ollama&lt;/code&gt;: No API key required (local processing)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Other API keys are only required when configuring and using models from the corresponding providers.&lt;/p&gt; 
&lt;h2&gt;Authorization Mode&lt;/h2&gt; 
&lt;p&gt;DeepWiki can be configured to run in an authorization mode, where wiki generation requires a valid authorization code. This is useful if you want to control who can use the generation feature. Restricts frontend initiation and protects cache deletion, but doesn't fully prevent backend generation if API endpoints are hit directly.&lt;/p&gt; 
&lt;p&gt;To enable authorization mode, set the following environment variables:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;DEEPWIKI_AUTH_MODE&lt;/code&gt;: Set this to &lt;code&gt;true&lt;/code&gt; or &lt;code&gt;1&lt;/code&gt;. When enabled, the frontend will display an input field for the authorization code.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;DEEPWIKI_AUTH_CODE&lt;/code&gt;: Set this to the desired secret code. Restricts frontend initiation and protects cache deletion, but doesn't fully prevent backend generation if API endpoints are hit directly.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If &lt;code&gt;DEEPWIKI_AUTH_MODE&lt;/code&gt; is not set or is set to &lt;code&gt;false&lt;/code&gt; (or any other value than &lt;code&gt;true&lt;/code&gt;/&lt;code&gt;1&lt;/code&gt;), the authorization feature will be disabled, and no code will be required.&lt;/p&gt; 
&lt;h3&gt;Docker Setup&lt;/h3&gt; 
&lt;p&gt;You can use Docker to run DeepWiki:&lt;/p&gt; 
&lt;h4&gt;Running the Container&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Pull the image from GitHub Container Registry
docker pull ghcr.io/asyncfuncai/deepwiki-open:latest

# Run the container with environment variables
docker run -p 8001:8001 -p 3000:3000 \
  -e GOOGLE_API_KEY=your_google_api_key \
  -e OPENAI_API_KEY=your_openai_api_key \
  -e OPENROUTER_API_KEY=your_openrouter_api_key \
  -e OLLAMA_HOST=your_ollama_host \
  -e AZURE_OPENAI_API_KEY=your_azure_openai_api_key \
  -e AZURE_OPENAI_ENDPOINT=your_azure_openai_endpoint \
  -e AZURE_OPENAI_VERSION=your_azure_openai_version \

  -v ~/.adalflow:/root/.adalflow \
  ghcr.io/asyncfuncai/deepwiki-open:latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This command also mounts &lt;code&gt;~/.adalflow&lt;/code&gt; on your host to &lt;code&gt;/root/.adalflow&lt;/code&gt; in the container. This path is used to store:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Cloned repositories (&lt;code&gt;~/.adalflow/repos/&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Their embeddings and indexes (&lt;code&gt;~/.adalflow/databases/&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Cached generated wiki content (&lt;code&gt;~/.adalflow/wikicache/&lt;/code&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This ensures that your data persists even if the container is stopped or removed.&lt;/p&gt; 
&lt;p&gt;Or use the provided &lt;code&gt;docker-compose.yml&lt;/code&gt; file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Edit the .env file with your API keys first
docker-compose up
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;(The &lt;code&gt;docker-compose.yml&lt;/code&gt; file is pre-configured to mount &lt;code&gt;~/.adalflow&lt;/code&gt; for data persistence, similar to the &lt;code&gt;docker run&lt;/code&gt; command above.)&lt;/p&gt; 
&lt;h4&gt;Using a .env file with Docker&lt;/h4&gt; 
&lt;p&gt;You can also mount a .env file to the container:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Create a .env file with your API keys
echo "GOOGLE_API_KEY=your_google_api_key" &amp;gt; .env
echo "OPENAI_API_KEY=your_openai_api_key" &amp;gt;&amp;gt; .env
echo "OPENROUTER_API_KEY=your_openrouter_api_key" &amp;gt;&amp;gt; .env
echo "AZURE_OPENAI_API_KEY=your_azure_openai_api_key" &amp;gt;&amp;gt; .env
echo "AZURE_OPENAI_ENDPOINT=your_azure_openai_endpoint" &amp;gt;&amp;gt; .env
echo "AZURE_OPENAI_VERSION=your_azure_openai_version"  &amp;gt;&amp;gt;.env
echo "OLLAMA_HOST=your_ollama_host" &amp;gt;&amp;gt; .env

# Run the container with the .env file mounted
docker run -p 8001:8001 -p 3000:3000 \
  -v $(pwd)/.env:/app/.env \
  -v ~/.adalflow:/root/.adalflow \
  ghcr.io/asyncfuncai/deepwiki-open:latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This command also mounts &lt;code&gt;~/.adalflow&lt;/code&gt; on your host to &lt;code&gt;/root/.adalflow&lt;/code&gt; in the container. This path is used to store:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Cloned repositories (&lt;code&gt;~/.adalflow/repos/&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Their embeddings and indexes (&lt;code&gt;~/.adalflow/databases/&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Cached generated wiki content (&lt;code&gt;~/.adalflow/wikicache/&lt;/code&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This ensures that your data persists even if the container is stopped or removed.&lt;/p&gt; 
&lt;h4&gt;Building the Docker image locally&lt;/h4&gt; 
&lt;p&gt;If you want to build the Docker image locally:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Clone the repository
git clone https://github.com/AsyncFuncAI/deepwiki-open.git
cd deepwiki-open

# Build the Docker image
docker build -t deepwiki-open .

# Run the container
docker run -p 8001:8001 -p 3000:3000 \
  -e GOOGLE_API_KEY=your_google_api_key \
  -e OPENAI_API_KEY=your_openai_api_key \
  -e OPENROUTER_API_KEY=your_openrouter_api_key \
  -e AZURE_OPENAI_API_KEY=your_azure_openai_api_key \
  -e AZURE_OPENAI_ENDPOINT=your_azure_openai_endpoint \
  -e AZURE_OPENAI_VERSION=your_azure_openai_version \
  -e OLLAMA_HOST=your_ollama_host \
  deepwiki-open
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Using Self-Signed Certificates in Docker&lt;/h4&gt; 
&lt;p&gt;If you're in an environment that uses self-signed certificates, you can include them in the Docker build:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Create a directory for your certificates (default is &lt;code&gt;certs&lt;/code&gt; in your project root)&lt;/li&gt; 
 &lt;li&gt;Copy your &lt;code&gt;.crt&lt;/code&gt; or &lt;code&gt;.pem&lt;/code&gt; certificate files into this directory&lt;/li&gt; 
 &lt;li&gt;Build the Docker image:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Build with default certificates directory (certs)
docker build .

# Or build with a custom certificates directory
docker build --build-arg CUSTOM_CERT_DIR=my-custom-certs .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;API Server Details&lt;/h3&gt; 
&lt;p&gt;The API server provides:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Repository cloning and indexing&lt;/li&gt; 
 &lt;li&gt;RAG (Retrieval Augmented Generation)&lt;/li&gt; 
 &lt;li&gt;Streaming chat completions&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For more details, see the &lt;a href="https://raw.githubusercontent.com/AsyncFuncAI/deepwiki-open/main/api/README.md"&gt;API README&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;ğŸ”Œ OpenRouter Integration&lt;/h2&gt; 
&lt;p&gt;DeepWiki now supports &lt;a href="https://openrouter.ai/"&gt;OpenRouter&lt;/a&gt; as a model provider, giving you access to hundreds of AI models through a single API:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Multiple Model Options&lt;/strong&gt;: Access models from OpenAI, Anthropic, Google, Meta, Mistral, and more&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Simple Configuration&lt;/strong&gt;: Just add your OpenRouter API key and select the model you want to use&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Cost Efficiency&lt;/strong&gt;: Choose models that fit your budget and performance needs&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Easy Switching&lt;/strong&gt;: Toggle between different models without changing your code&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;How to Use OpenRouter with DeepWiki&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Get an API Key&lt;/strong&gt;: Sign up at &lt;a href="https://openrouter.ai/"&gt;OpenRouter&lt;/a&gt; and get your API key&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Add to Environment&lt;/strong&gt;: Add &lt;code&gt;OPENROUTER_API_KEY=your_key&lt;/code&gt; to your &lt;code&gt;.env&lt;/code&gt; file&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Enable in UI&lt;/strong&gt;: Check the "Use OpenRouter API" option on the homepage&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Select Model&lt;/strong&gt;: Choose from popular models like GPT-4o, Claude 3.5 Sonnet, Gemini 2.0, and more&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;OpenRouter is particularly useful if you want to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Try different models without signing up for multiple services&lt;/li&gt; 
 &lt;li&gt;Access models that might be restricted in your region&lt;/li&gt; 
 &lt;li&gt;Compare performance across different model providers&lt;/li&gt; 
 &lt;li&gt;Optimize for cost vs. performance based on your needs&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ¤– Ask &amp;amp; DeepResearch Features&lt;/h2&gt; 
&lt;h3&gt;Ask Feature&lt;/h3&gt; 
&lt;p&gt;The Ask feature allows you to chat with your repository using Retrieval Augmented Generation (RAG):&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Context-Aware Responses&lt;/strong&gt;: Get accurate answers based on the actual code in your repository&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;RAG-Powered&lt;/strong&gt;: The system retrieves relevant code snippets to provide grounded responses&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Real-Time Streaming&lt;/strong&gt;: See responses as they're generated for a more interactive experience&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Conversation History&lt;/strong&gt;: The system maintains context between questions for more coherent interactions&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;DeepResearch Feature&lt;/h3&gt; 
&lt;p&gt;DeepResearch takes repository analysis to the next level with a multi-turn research process:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;In-Depth Investigation&lt;/strong&gt;: Thoroughly explores complex topics through multiple research iterations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Structured Process&lt;/strong&gt;: Follows a clear research plan with updates and a comprehensive conclusion&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Automatic Continuation&lt;/strong&gt;: The AI automatically continues research until reaching a conclusion (up to 5 iterations)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Research Stages&lt;/strong&gt;: 
  &lt;ol&gt; 
   &lt;li&gt;&lt;strong&gt;Research Plan&lt;/strong&gt;: Outlines the approach and initial findings&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Research Updates&lt;/strong&gt;: Builds on previous iterations with new insights&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Final Conclusion&lt;/strong&gt;: Provides a comprehensive answer based on all iterations&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To use DeepResearch, simply toggle the "Deep Research" switch in the Ask interface before submitting your question.&lt;/p&gt; 
&lt;h2&gt;ğŸ“± Screenshots&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/AsyncFuncAI/deepwiki-open/main/screenshots/Interface.png" alt="DeepWiki Main Interface" /&gt; &lt;em&gt;The main interface of DeepWiki&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/AsyncFuncAI/deepwiki-open/main/screenshots/privaterepo.png" alt="Private Repository Support" /&gt; &lt;em&gt;Access private repositories with personal access tokens&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/AsyncFuncAI/deepwiki-open/main/screenshots/DeepResearch.png" alt="DeepResearch Feature" /&gt; &lt;em&gt;DeepResearch conducts multi-turn investigations for complex topics&lt;/em&gt;&lt;/p&gt; 
&lt;h3&gt;Demo Video&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://youtu.be/zGANs8US8B4"&gt;&lt;img src="https://img.youtube.com/vi/zGANs8US8B4/0.jpg" alt="DeepWiki Demo Video" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Watch DeepWiki in action!&lt;/em&gt;&lt;/p&gt; 
&lt;h2&gt;â“ Troubleshooting&lt;/h2&gt; 
&lt;h3&gt;API Key Issues&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;"Missing environment variables"&lt;/strong&gt;: Make sure your &lt;code&gt;.env&lt;/code&gt; file is in the project root and contains the required API keys&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;"API key not valid"&lt;/strong&gt;: Check that you've copied the full key correctly with no extra spaces&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;"OpenRouter API error"&lt;/strong&gt;: Verify your OpenRouter API key is valid and has sufficient credits&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;"Azure OpenAI API error"&lt;/strong&gt;: Verify your Azure OpenAI credentials (API key, endpoint, and version) are correct and the service is properly deployed&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Connection Problems&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;"Cannot connect to API server"&lt;/strong&gt;: Make sure the API server is running on port 8001&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;"CORS error"&lt;/strong&gt;: The API is configured to allow all origins, but if you're having issues, try running both frontend and backend on the same machine&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Generation Issues&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;"Error generating wiki"&lt;/strong&gt;: For very large repositories, try a smaller one first&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;"Invalid repository format"&lt;/strong&gt;: Make sure you're using a valid GitHub, GitLab or Bitbucket URL format&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;"Could not fetch repository structure"&lt;/strong&gt;: For private repositories, ensure you've entered a valid personal access token with appropriate permissions&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;"Diagram rendering error"&lt;/strong&gt;: The app will automatically try to fix broken diagrams&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Common Solutions&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Restart both servers&lt;/strong&gt;: Sometimes a simple restart fixes most issues&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Check console logs&lt;/strong&gt;: Open browser developer tools to see any JavaScript errors&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Check API logs&lt;/strong&gt;: Look at the terminal where the API is running for Python errors&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;ğŸ¤ Contributing&lt;/h2&gt; 
&lt;p&gt;Contributions are welcome! Feel free to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Open issues for bugs or feature requests&lt;/li&gt; 
 &lt;li&gt;Submit pull requests to improve the code&lt;/li&gt; 
 &lt;li&gt;Share your feedback and ideas&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ“„ License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the MIT License - see the &lt;a href="https://raw.githubusercontent.com/AsyncFuncAI/deepwiki-open/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;h2&gt;â­ Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#AsyncFuncAI/deepwiki-open&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=AsyncFuncAI/deepwiki-open&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>dlt-hub/dlt</title>
      <link>https://github.com/dlt-hub/dlt</link>
      <description>&lt;p&gt;data load tool (dlt) is an open source Python library that makes data loading easy ğŸ› ï¸&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt; &lt;strong&gt;data load tool (dlt) â€” the open-source Python library that automates all your tedious data loading tasks&lt;/strong&gt; &lt;/h1&gt; 
&lt;p align="center"&gt; Be it a Google Colab notebook, AWS Lambda function, an Airflow DAG, your local laptop,&lt;br /&gt;or a GPT-4 assisted development playgroundâ€”&lt;strong&gt;dlt&lt;/strong&gt; can be dropped in anywhere. &lt;/p&gt; 
&lt;h3 align="center"&gt; &lt;p&gt;ğŸš€ Join our thriving community of likeminded developers and build the future together!&lt;/p&gt; &lt;/h3&gt; 
&lt;div align="center"&gt; 
 &lt;a target="_blank" href="https://dlthub.com/community" style="background:none"&gt; &lt;img src="https://img.shields.io/badge/slack-join-dlt.svg?labelColor=191937&amp;amp;color=6F6FF7&amp;amp;logo=slack" style="width: 260px;" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a target="_blank" href="https://pypi.org/project/dlt/" style="background:none"&gt; &lt;img src="https://img.shields.io/pypi/v/dlt?labelColor=191937&amp;amp;color=6F6FF7" /&gt; &lt;/a&gt; 
 &lt;a target="_blank" href="https://pypi.org/project/dlt/" style="background:none"&gt; &lt;img src="https://img.shields.io/pypi/pyversions/dlt?labelColor=191937&amp;amp;color=6F6FF7" /&gt; &lt;/a&gt; 
 &lt;a target="_blank" href="https://pypi.org/project/dlt/" style="background:none"&gt; &lt;img src="https://img.shields.io/pypi/dm/dlt?labelColor=191937&amp;amp;color=6F6FF7" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;dlt supports Python 3.9 through Python 3.14. Note that some optional extras are not yet available for Python 3.14, so support for this version is considered experimental.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;pip install dlt
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;p&gt;Load chess game data from chess.com API and save it in DuckDB:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import dlt
from dlt.sources.helpers import requests

# Create a dlt pipeline that will load
# chess player data to the DuckDB destination
pipeline = dlt.pipeline(
    pipeline_name='chess_pipeline',
    destination='duckdb',
    dataset_name='player_data'
)

# Grab some player data from Chess.com API
data = []
for player in ['magnuscarlsen', 'rpragchess']:
    response = requests.get(f'https://api.chess.com/pub/player/{player}')
    response.raise_for_status()
    data.append(response.json())

# Extract, normalize, and load the data
pipeline.run(data, table_name='player')
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Try it out in our &lt;strong&gt;&lt;a href="https://colab.research.google.com/drive/1NfSB1DpwbbHX9_t5vlalBTf13utwpMGx?usp=sharing"&gt;Colab Demo&lt;/a&gt;&lt;/strong&gt; or directly on our wasm-based &lt;a href="https://dlthub.com/docs/tutorial/playground"&gt;playground&lt;/a&gt; in our docs.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;p&gt;dlt is an open-source Python library that loads data from various, often messy data sources into well-structured datasets. It provides lightweight Python interfaces to extract, load, inspect, and transform data. dlt and dlt docs are built from the ground up to be used with LLMs: the &lt;a href="https://dlthub.com/docs/dlt-ecosystem/llm-tooling/llm-native-workflow"&gt;LLM-native workflow&lt;/a&gt; will take your pipeline code to data in a notebook for over &lt;a href="https://dlthub.com/workspace"&gt;5000 sources&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;dlt is designed to be easy to use, flexible, and scalable:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;dlt extracts data from &lt;a href="https://dlthub.com/docs/tutorial/rest-api"&gt;REST APIs&lt;/a&gt;, &lt;a href="https://dlthub.com/docs/tutorial/sql-database"&gt;SQL databases&lt;/a&gt;, &lt;a href="https://dlthub.com/docs/tutorial/filesystem"&gt;cloud storage&lt;/a&gt;, &lt;a href="https://dlthub.com/docs/tutorial/load-data-from-an-api"&gt;Python data structures&lt;/a&gt;, and &lt;a href="https://dlthub.com/docs/dlt-ecosystem/verified-sources"&gt;many more&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;dlt infers &lt;a href="https://dlthub.com/docs/general-usage/schema"&gt;schemas&lt;/a&gt; and &lt;a href="https://dlthub.com/docs/general-usage/schema/#data-types"&gt;data types&lt;/a&gt;, &lt;a href="https://dlthub.com/docs/general-usage/schema/#data-normalizer"&gt;normalizes the data&lt;/a&gt;, and handles nested data structures.&lt;/li&gt; 
 &lt;li&gt;dlt supports a variety of &lt;a href="https://dlthub.com/docs/dlt-ecosystem/destinations/"&gt;popular destinations&lt;/a&gt; and has an interface to add &lt;a href="https://dlthub.com/docs/dlt-ecosystem/destinations/destination"&gt;custom destinations&lt;/a&gt; to create reverse ETL pipelines.&lt;/li&gt; 
 &lt;li&gt;dlt automates pipeline maintenance with &lt;a href="https://dlthub.com/docs/general-usage/incremental-loading"&gt;incremental loading&lt;/a&gt;, &lt;a href="https://dlthub.com/docs/general-usage/schema-evolution"&gt;schema evolution&lt;/a&gt;, and &lt;a href="https://dlthub.com/docs/general-usage/schema-contracts"&gt;schema and data contracts&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;dlt supports &lt;a href="https://dlthub.com/docs/general-usage/dataset-access/"&gt;Python and SQL data access&lt;/a&gt;, &lt;a href="https://dlthub.com/docs/dlt-ecosystem/transformations"&gt;transformations&lt;/a&gt;, &lt;a href="https://dlthub.com/docs/general-usage/dashboard.md"&gt;pipeline inspection&lt;/a&gt;, and &lt;a href="https://dlthub.com/docs/general-usage/dataset-access/marimo"&gt;visualizing data in Marimo Notebooks&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;dlt can be deployed anywhere Python runs, be it on &lt;a href="https://dlthub.com/docs/walkthroughs/deploy-a-pipeline/deploy-with-airflow-composer"&gt;Airflow&lt;/a&gt;, &lt;a href="https://dlthub.com/docs/walkthroughs/deploy-a-pipeline/deploy-with-google-cloud-functions"&gt;serverless functions&lt;/a&gt;, or any other cloud deployment of your choice.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;For detailed usage and configuration, please refer to the &lt;a href="https://dlthub.com/docs"&gt;official documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;p&gt;You can find examples for various use cases in the &lt;a href="https://raw.githubusercontent.com/dlt-hub/dlt/devel/docs/examples"&gt;examples&lt;/a&gt; folder, or in the &lt;a href="https://dlthub.com/docs/examples"&gt;code examples section&lt;/a&gt; of our docs page.&lt;/p&gt; 
&lt;h2&gt;Adding as dependency&lt;/h2&gt; 
&lt;p&gt;&lt;code&gt;dlt&lt;/code&gt; follows the semantic versioning with the &lt;a href="https://peps.python.org/pep-0440/#semantic-versioning"&gt;&lt;code&gt;MAJOR.MINOR.PATCH&lt;/code&gt;&lt;/a&gt; pattern.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;major&lt;/code&gt; means breaking changes and removed deprecations&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;minor&lt;/code&gt; new features, sometimes automatic migrations&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;patch&lt;/code&gt; bug fixes&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We suggest that you allow only &lt;code&gt;patch&lt;/code&gt; level updates automatically:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Using the &lt;a href="https://packaging.python.org/en/latest/specifications/version-specifiers/#compatible-release"&gt;Compatible Release Specifier&lt;/a&gt;. For example &lt;strong&gt;dlt~=1.0&lt;/strong&gt; allows only versions &lt;strong&gt;&amp;gt;=1.0&lt;/strong&gt; and less than &lt;strong&gt;&amp;lt;1.1&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Poetry &lt;a href="https://python-poetry.org/docs/dependency-specification/"&gt;caret requirements&lt;/a&gt;. For example &lt;strong&gt;^1.0&lt;/strong&gt; allows only versions &lt;strong&gt;&amp;gt;=1.0&lt;/strong&gt; to &lt;strong&gt;&amp;lt;1.0&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Please also see our &lt;a href="https://github.com/dlt-hub/dlt/releases"&gt;release notes&lt;/a&gt; for notable changes between versions.&lt;/p&gt; 
&lt;h2&gt;Get Involved&lt;/h2&gt; 
&lt;p&gt;The dlt project is quickly growing, and we're excited to have you join our community! Here's how you can get involved:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Connect with the Community&lt;/strong&gt;: Join other dlt users and contributors on our &lt;a href="https://dlthub.com/community"&gt;Slack&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Report issues and suggest features&lt;/strong&gt;: Please use the &lt;a href="https://github.com/dlt-hub/dlt/issues"&gt;GitHub Issues&lt;/a&gt; to report bugs or suggest new features. Before creating a new issue, make sure to search the tracker for possible duplicates and add a comment if you find one.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Track progress of our work and our plans&lt;/strong&gt;: Please check out our &lt;a href="https://github.com/orgs/dlt-hub/projects/9"&gt;public Github project&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Improve documentation&lt;/strong&gt;: Help us enhance the dlt documentation.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contribute code&lt;/h2&gt; 
&lt;p&gt;Please read &lt;a href="https://raw.githubusercontent.com/dlt-hub/dlt/devel/CONTRIBUTING.md"&gt;CONTRIBUTING&lt;/a&gt; before you make a PR.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“£ &lt;strong&gt;New destinations are unlikely to be merged&lt;/strong&gt; due to high maintenance cost (but we are happy to improve SQLAlchemy destination to handle more dialects)&lt;/li&gt; 
 &lt;li&gt;Significant changes require tests and docs and in many cases writing tests will be more laborious than writing code&lt;/li&gt; 
 &lt;li&gt;Bugfixes and improvements are welcome! You'll get help with writing tests and docs + a decent review.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;&lt;code&gt;dlt&lt;/code&gt; is released under the &lt;a href="https://raw.githubusercontent.com/dlt-hub/dlt/devel/LICENSE.txt"&gt;Apache 2.0 License&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>666ghj/BettaFish</title>
      <link>https://github.com/666ghj/BettaFish</link>
      <description>&lt;p&gt;å¾®èˆ†ï¼šäººäººå¯ç”¨çš„å¤šAgentèˆ†æƒ…åˆ†æåŠ©æ‰‹ï¼Œæ‰“ç ´ä¿¡æ¯èŒ§æˆ¿ï¼Œè¿˜åŸèˆ†æƒ…åŸè²Œï¼Œé¢„æµ‹æœªæ¥èµ°å‘ï¼Œè¾…åŠ©å†³ç­–ï¼ä»0å®ç°ï¼Œä¸ä¾èµ–ä»»ä½•æ¡†æ¶ã€‚&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/666ghj/BettaFish/main/static/image/logo_compressed.png" alt="Weibo Public Opinion Analysis System Logo" width="100%" /&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/15286" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/15286" alt="666ghj%2FBettaFish | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://aihubmix.com/?aff=8Ds9" target="_blank"&gt;&lt;img src="https://raw.githubusercontent.com/666ghj/BettaFish/main/static/image/logo_aihubmix.png" alt="666ghj%2FBettaFish | Trendshift" height="40" /&gt;&lt;/a&gt;â€‚ &lt;a href="https://lioncc.ai/" target="_blank"&gt;&lt;img src="https://raw.githubusercontent.com/666ghj/BettaFish/main/static/image/logo_loincc.png" alt="666ghj%2FBettaFish | Trendshift" height="40" /&gt;&lt;/a&gt;â€‚ &lt;a href="https://share.302.ai/P66Qe3" target="_blank"&gt;&lt;img src="https://raw.githubusercontent.com/666ghj/BettaFish/main/static/image/logo_302ai.png" alt="666ghj%2FBettaFish | Trendshift" height="40" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/666ghj/Weibo_PublicOpinion_AnalysisSystem/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/666ghj/Weibo_PublicOpinion_AnalysisSystem?style=flat-square" alt="GitHub Stars" /&gt;&lt;/a&gt; &lt;a href="https://github.com/666ghj/Weibo_PublicOpinion_AnalysisSystem/watchers"&gt;&lt;img src="https://img.shields.io/github/watchers/666ghj/Weibo_PublicOpinion_AnalysisSystem?style=flat-square" alt="GitHub Watchers" /&gt;&lt;/a&gt; &lt;a href="https://github.com/666ghj/Weibo_PublicOpinion_AnalysisSystem/network"&gt;&lt;img src="https://img.shields.io/github/forks/666ghj/Weibo_PublicOpinion_AnalysisSystem?style=flat-square" alt="GitHub Forks" /&gt;&lt;/a&gt; &lt;a href="https://github.com/666ghj/Weibo_PublicOpinion_AnalysisSystem/issues"&gt;&lt;img src="https://img.shields.io/github/issues/666ghj/Weibo_PublicOpinion_AnalysisSystem?style=flat-square" alt="GitHub Issues" /&gt;&lt;/a&gt; &lt;a href="https://github.com/666ghj/Weibo_PublicOpinion_AnalysisSystem/pulls"&gt;&lt;img src="https://img.shields.io/github/issues-pr/666ghj/Weibo_PublicOpinion_AnalysisSystem?style=flat-square" alt="GitHub Pull Requests" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/666ghj/Weibo_PublicOpinion_AnalysisSystem/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/666ghj/Weibo_PublicOpinion_AnalysisSystem?style=flat-square" alt="GitHub License" /&gt;&lt;/a&gt; &lt;a href="https://github.com/666ghj/Weibo_PublicOpinion_AnalysisSystem"&gt;&lt;img src="https://img.shields.io/badge/version-v1.0.0-green.svg?style=flat-square" alt="Version" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/"&gt;&lt;img src="https://img.shields.io/badge/Docker-Build-2496ED?style=flat-square&amp;amp;logo=docker&amp;amp;logoColor=white" alt="Docker" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/666ghj/BettaFish/main/README-EN.md"&gt;English&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/666ghj/BettaFish/main/README.md"&gt;ä¸­æ–‡æ–‡æ¡£&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;âš¡ é¡¹ç›®æ¦‚è¿°&lt;/h2&gt; 
&lt;p&gt;â€œ&lt;strong&gt;å¾®èˆ†&lt;/strong&gt;â€ æ˜¯ä¸€ä¸ªä»0å®ç°çš„åˆ›æ–°å‹ å¤šæ™ºèƒ½ä½“ èˆ†æƒ…åˆ†æç³»ç»Ÿï¼Œå¸®åŠ©å¤§å®¶ç ´é™¤ä¿¡æ¯èŒ§æˆ¿ï¼Œè¿˜åŸèˆ†æƒ…åŸè²Œï¼Œé¢„æµ‹æœªæ¥èµ°å‘ï¼Œè¾…åŠ©å†³ç­–ã€‚ç”¨æˆ·åªéœ€åƒèŠå¤©ä¸€æ ·æå‡ºåˆ†æéœ€æ±‚ï¼Œæ™ºèƒ½ä½“å¼€å§‹å…¨è‡ªåŠ¨åˆ†æ å›½å†…å¤–30+ä¸»æµç¤¾åª’ ä¸ æ•°ç™¾ä¸‡æ¡å¤§ä¼—è¯„è®ºã€‚&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;â€œå¾®èˆ†â€è°éŸ³â€œå¾®é±¼â€ï¼ŒBettaFishæ˜¯ä¸€ç§ä½“å‹å¾ˆå°ä½†éå¸¸å¥½æ–—ã€æ¼‚äº®çš„é±¼ï¼Œå®ƒè±¡å¾ç€â€œå°è€Œå¼ºå¤§ï¼Œä¸ç•æŒ‘æˆ˜â€&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;æŸ¥çœ‹ç³»ç»Ÿä»¥â€œæ­¦æ±‰å¤§å­¦èˆ†æƒ…â€ä¸ºä¾‹ï¼Œç”Ÿæˆçš„ç ”ç©¶æŠ¥å‘Šï¼š&lt;a href="https://raw.githubusercontent.com/666ghj/BettaFish/main/final_reports/final_report__20250827_131630.html"&gt;æ­¦æ±‰å¤§å­¦å“ç‰Œå£°èª‰æ·±åº¦åˆ†ææŠ¥å‘Š&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;æŸ¥çœ‹ç³»ç»Ÿä»¥â€œæ­¦æ±‰å¤§å­¦èˆ†æƒ…â€ä¸ºä¾‹ï¼Œä¸€æ¬¡å®Œæ•´è¿è¡Œçš„è§†é¢‘ï¼š&lt;a href="https://www.bilibili.com/video/BV1TH1WBxEWN/?vd_source=da3512187e242ce17dceee4c537ec7a6#reply279744466833"&gt;è§†é¢‘-æ­¦æ±‰å¤§å­¦å“ç‰Œå£°èª‰æ·±åº¦åˆ†ææŠ¥å‘Š&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;ä¸ä»…ä»…ä½“ç°åœ¨æŠ¥å‘Šè´¨é‡ä¸Šï¼Œç›¸æ¯”åŒç±»äº§å“ï¼Œæˆ‘ä»¬æ‹¥æœ‰ğŸš€å…­å¤§ä¼˜åŠ¿ï¼š&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;AIé©±åŠ¨çš„å…¨åŸŸç›‘æ§&lt;/strong&gt;ï¼šAIçˆ¬è™«é›†ç¾¤7x24å°æ—¶ä¸é—´æ–­ä½œä¸šï¼Œå…¨é¢è¦†ç›–å¾®åšã€å°çº¢ä¹¦ã€æŠ–éŸ³ã€å¿«æ‰‹ç­‰10+å›½å†…å¤–å…³é”®ç¤¾åª’ã€‚ä¸ä»…å®æ—¶æ•è·çƒ­ç‚¹å†…å®¹ï¼Œæ›´èƒ½ä¸‹é’»è‡³æµ·é‡ç”¨æˆ·è¯„è®ºï¼Œè®©æ‚¨å¬åˆ°æœ€çœŸå®ã€æœ€å¹¿æ³›çš„å¤§ä¼—å£°éŸ³ã€‚&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;è¶…è¶ŠLLMçš„å¤åˆåˆ†æå¼•æ“&lt;/strong&gt;ï¼šæˆ‘ä»¬ä¸ä»…ä¾èµ–è®¾è®¡çš„5ç±»ä¸“ä¸šAgentï¼Œæ›´èåˆäº†å¾®è°ƒæ¨¡å‹ã€ç»Ÿè®¡æ¨¡å‹ç­‰ä¸­é—´ä»¶ã€‚é€šè¿‡å¤šæ¨¡å‹ååŒå·¥ä½œï¼Œç¡®ä¿äº†åˆ†æç»“æœçš„æ·±åº¦ã€å‡†åº¦ä¸å¤šç»´è§†è§’ã€‚&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;å¼ºå¤§çš„å¤šæ¨¡æ€èƒ½åŠ›&lt;/strong&gt;ï¼šçªç ´å›¾æ–‡é™åˆ¶ï¼Œèƒ½æ·±åº¦è§£ææŠ–éŸ³ã€å¿«æ‰‹ç­‰çŸ­è§†é¢‘å†…å®¹ï¼Œå¹¶ç²¾å‡†æå–ç°ä»£æœç´¢å¼•æ“ä¸­çš„å¤©æ°”ã€æ—¥å†ã€è‚¡ç¥¨ç­‰ç»“æ„åŒ–å¤šæ¨¡æ€ä¿¡æ¯å¡ç‰‡ï¼Œè®©æ‚¨å…¨é¢æŒæ¡èˆ†æƒ…åŠ¨æ€ã€‚&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Agentâ€œè®ºå›â€åä½œæœºåˆ¶&lt;/strong&gt;ï¼šä¸ºä¸åŒAgentèµ‹äºˆç‹¬ç‰¹çš„å·¥å…·é›†ä¸æ€ç»´æ¨¡å¼ï¼Œå¼•å…¥è¾©è®ºä¸»æŒäººæ¨¡å‹ï¼Œé€šè¿‡â€œè®ºå›â€æœºåˆ¶è¿›è¡Œé“¾å¼æ€ç»´ç¢°æ’ä¸è¾©è®ºã€‚è¿™ä¸ä»…é¿å…äº†å•ä¸€æ¨¡å‹çš„æ€ç»´å±€é™ä¸äº¤æµå¯¼è‡´çš„åŒè´¨åŒ–ï¼Œæ›´å‚¬ç”Ÿå‡ºæ›´é«˜è´¨é‡çš„é›†ä½“æ™ºèƒ½ä¸å†³ç­–æ”¯æŒã€‚&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;å…¬ç§åŸŸæ•°æ®æ— ç¼èåˆ&lt;/strong&gt;ï¼šå¹³å°ä¸ä»…åˆ†æå…¬å¼€èˆ†æƒ…ï¼Œè¿˜æä¾›é«˜å®‰å…¨æ€§çš„æ¥å£ï¼Œæ”¯æŒæ‚¨å°†å†…éƒ¨ä¸šåŠ¡æ•°æ®åº“ä¸èˆ†æƒ…æ•°æ®æ— ç¼é›†æˆã€‚æ‰“é€šæ•°æ®å£å’ï¼Œä¸ºå‚ç›´ä¸šåŠ¡æä¾›â€œå¤–éƒ¨è¶‹åŠ¿+å†…éƒ¨æ´å¯Ÿâ€çš„å¼ºå¤§åˆ†æèƒ½åŠ›ã€‚&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;è½»é‡åŒ–ä¸é«˜æ‰©å±•æ€§æ¡†æ¶&lt;/strong&gt;ï¼šåŸºäºçº¯Pythonæ¨¡å—åŒ–è®¾è®¡ï¼Œå®ç°è½»é‡åŒ–ã€ä¸€é”®å¼éƒ¨ç½²ã€‚ä»£ç ç»“æ„æ¸…æ™°ï¼Œå¼€å‘è€…å¯è½»æ¾é›†æˆè‡ªå®šä¹‰æ¨¡å‹ä¸ä¸šåŠ¡é€»è¾‘ï¼Œå®ç°å¹³å°çš„å¿«é€Ÿæ‰©å±•ä¸æ·±åº¦å®šåˆ¶ã€‚&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;å§‹äºèˆ†æƒ…ï¼Œè€Œä¸æ­¢äºèˆ†æƒ…&lt;/strong&gt;ã€‚â€œå¾®èˆ†â€çš„ç›®æ ‡ï¼Œæ˜¯æˆä¸ºé©±åŠ¨ä¸€åˆ‡ä¸šåŠ¡åœºæ™¯çš„ç®€æ´é€šç”¨çš„æ•°æ®åˆ†æå¼•æ“ã€‚&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ä¸¾ä¸ªä¾‹å­. ä½ åªéœ€ç®€å•ä¿®æ”¹Agentå·¥å…·é›†çš„apiå‚æ•°ä¸promptï¼Œå°±å¯ä»¥æŠŠä»–å˜æˆä¸€ä¸ªé‡‘èé¢†åŸŸçš„å¸‚åœºåˆ†æç³»ç»Ÿ&lt;/p&gt; 
 &lt;p&gt;é™„ä¸€ä¸ªæ¯”è¾ƒæ´»è·ƒçš„Lç«™é¡¹ç›®è®¨è®ºå¸–ï¼š&lt;a href="https://linux.do/t/topic/1009280"&gt;https://linux.do/t/topic/1009280&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/666ghj/BettaFish/main/static/image/system_schematic.png" alt="banner" width="800" /&gt; 
 &lt;p&gt;å‘Šåˆ«ä¼ ç»Ÿçš„æ•°æ®çœ‹æ¿ï¼Œåœ¨â€œå¾®èˆ†â€ï¼Œä¸€åˆ‡ç”±ä¸€ä¸ªç®€å•çš„é—®é¢˜å¼€å§‹ï¼Œæ‚¨åªéœ€åƒå¯¹è¯ä¸€æ ·ï¼Œæå‡ºæ‚¨çš„åˆ†æéœ€æ±‚&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸª„ èµåŠ©å•†&lt;/h2&gt; 
&lt;p&gt;LLMæ¨¡å‹APIèµåŠ©ï¼š&lt;a href="https://aihubmix.com/?aff=8Ds9" target="_blank"&gt;&lt;img src="https://raw.githubusercontent.com/666ghj/BettaFish/main/static/image/logo_aihubmix.png" alt="666ghj%2FBettaFish | Trendshift" height="40" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;æ‰€ç½—é—¨åšå®¢LionCC.aiï¼›ç¼–ç¨‹æ‹¼è½¦codecodex.aiï¼›ç¼–ç¨‹ç®—åŠ›VibeCodingAPI.aiï¼š&lt;span style="margin-left: 10px"&gt;&lt;a href="https://aihubmix.com/?aff=8Ds9" target="_blank"&gt;&lt;img src="https://raw.githubusercontent.com/666ghj/BettaFish/main/static/image/logo_loincc.png" alt="666ghj%2FBettaFish | Trendshift" height="40" /&gt;&lt;/a&gt;&lt;/span&gt;&lt;/summary&gt; 1. æ‰€ç½—é—¨åšå®¢LionCC.aiå·²æ›´æ–°ã€ŠBettaFish å¾®èˆ†ç³»ç»Ÿ - LionCC API éƒ¨ç½²é…ç½®å®Œå…¨æŒ‡å—ã€‹æ­£åœ¨äºŒå¼€ä¼˜åŒ–ä¸€é”®éƒ¨ç½²å’Œäº‘æœåŠ¡å™¨è°ƒç”¨æ–¹æ¡ˆã€‚ 2. VibeCodingapi.aiç‹®å­ç®—åŠ›å¹³å°å·²ç»é€‚é…ã€ŠBettaFish å¾®èˆ†ç³»ç»Ÿã€‹æ‰€æœ‰LLMæ¨¡å‹å«claude codeå’Œopenai codexå’Œgemini cliç¼–ç¨‹å¼€å‘ä¸‰å·¨å¤´ç®—åŠ›ã€‚é¢åº¦ä»·æ ¼ï¼Œåªè¦ä¸€æ¯”ä¸€ï¼ˆ100å…ƒç­‰äº100ç¾åˆ€é¢åº¦ï¼‰ 3. Codecodex.aiç‹®å­ç¼–ç¨‹æ‹¼è½¦ç³»ç»Ÿï¼Œå·²å®ç°æ— IPé—¨æ§›ç»•è¿‡claude codeå’Œopenai codexå°é”ï¼ŒæŒ‰å®˜æ–¹éƒ¨ç½²æ•™ç¨‹ååˆ‡æ¢BASE_URLè°ƒç”¨åœ°å€å’ŒToken keyè°ƒç”¨å¯†é’¥å³å¯ä½¿ç”¨æœ€å¼ºç¼–ç¨‹æ¨¡å‹ã€‚ 
 &lt;p&gt;æ‰€ç½—é—¨LionCCèµåŠ©BettaFish å¾®èˆ†ç¦åˆ©ï¼šæ‰“å¼€codecodex.aiç‹®å­ç¼–ç¨‹é¢‘é“æ‰«ç åŠ å…¥å¾®ä¿¡ç¤¾ç¾¤ï¼Œæ³¨å†ŒVibeCodingapi.aiç‹®å­ç®—åŠ›ï¼Œç»Ÿä¸€é€20åˆ€APIé¢åº¦ï¼ˆä»…é™å‰ä¸€åƒåï¼‰&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;æŒ‰ç”¨é‡ä»˜è´¹çš„ä¼ä¸šçº§AIèµ„æºå¹³å°ï¼Œæä¾›å¸‚åœºä¸Šå…¨é¢çš„AIæ¨¡å‹å’ŒAPIï¼Œä»¥åŠå¤šç§åœ¨çº¿AIåº”ç”¨ï¼š&lt;span style="margin-left: 10px"&gt;&lt;a href="https://share.302.ai/P66Qe3" target="_blank"&gt;&lt;img src="https://raw.githubusercontent.com/666ghj/BettaFish/main/static/image/logo_302ai.png" alt="666ghj%2FBettaFish | Trendshift" height="40" /&gt;&lt;/a&gt;&lt;/span&gt;&lt;/summary&gt; 
 &lt;img src="https://raw.githubusercontent.com/666ghj/BettaFish/main/static/image/banner_302ai_ch.jpg" alt="banner" /&gt;302.AIæ˜¯ä¸€ä¸ªæŒ‰ç”¨é‡ä»˜è´¹çš„ä¼ä¸šçº§AIèµ„æºå¹³å°ï¼Œæä¾›å¸‚åœºä¸Šæœ€æ–°ã€æœ€å…¨é¢çš„AIæ¨¡å‹å’ŒAPIï¼Œä»¥åŠå¤šç§å¼€ç®±å³ç”¨çš„åœ¨çº¿AIåº”ç”¨ã€‚ 
&lt;/details&gt; 
&lt;h2&gt;ğŸ—ï¸ ç³»ç»Ÿæ¶æ„&lt;/h2&gt; 
&lt;h3&gt;æ•´ä½“æ¶æ„å›¾&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Insight Agent&lt;/strong&gt; ç§æœ‰æ•°æ®åº“æŒ–æ˜ï¼šç§æœ‰èˆ†æƒ…æ•°æ®åº“æ·±åº¦åˆ†æAIä»£ç†&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Media Agent&lt;/strong&gt; å¤šæ¨¡æ€å†…å®¹åˆ†æï¼šå…·å¤‡å¼ºå¤§å¤šæ¨¡æ€èƒ½åŠ›çš„AIä»£ç†&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Query Agent&lt;/strong&gt; ç²¾å‡†ä¿¡æ¯æœç´¢ï¼šå…·å¤‡å›½å†…å¤–ç½‘é¡µæœç´¢èƒ½åŠ›çš„AIä»£ç†&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Report Agent&lt;/strong&gt; æ™ºèƒ½æŠ¥å‘Šç”Ÿæˆï¼šå†…ç½®æ¨¡æ¿çš„å¤šè½®æŠ¥å‘Šç”ŸæˆAIä»£ç†&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/666ghj/BettaFish/main/static/image/framework.png" alt="banner" width="800" /&gt; 
&lt;/div&gt; 
&lt;h3&gt;ä¸€æ¬¡å®Œæ•´åˆ†ææµç¨‹&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;æ­¥éª¤&lt;/th&gt; 
   &lt;th&gt;é˜¶æ®µåç§°&lt;/th&gt; 
   &lt;th&gt;ä¸»è¦æ“ä½œ&lt;/th&gt; 
   &lt;th&gt;å‚ä¸ç»„ä»¶&lt;/th&gt; 
   &lt;th&gt;å¾ªç¯ç‰¹æ€§&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;1&lt;/td&gt; 
   &lt;td&gt;ç”¨æˆ·æé—®&lt;/td&gt; 
   &lt;td&gt;Flaskä¸»åº”ç”¨æ¥æ”¶æŸ¥è¯¢&lt;/td&gt; 
   &lt;td&gt;Flaskä¸»åº”ç”¨&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;å¹¶è¡Œå¯åŠ¨&lt;/td&gt; 
   &lt;td&gt;ä¸‰ä¸ªAgentåŒæ—¶å¼€å§‹å·¥ä½œ&lt;/td&gt; 
   &lt;td&gt;Query Agentã€Media Agentã€Insight Agent&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;3&lt;/td&gt; 
   &lt;td&gt;åˆæ­¥åˆ†æ&lt;/td&gt; 
   &lt;td&gt;å„Agentä½¿ç”¨ä¸“å±å·¥å…·è¿›è¡Œæ¦‚è§ˆæœç´¢&lt;/td&gt; 
   &lt;td&gt;å„Agent + ä¸“å±å·¥å…·é›†&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;4&lt;/td&gt; 
   &lt;td&gt;ç­–ç•¥åˆ¶å®š&lt;/td&gt; 
   &lt;td&gt;åŸºäºåˆæ­¥ç»“æœåˆ¶å®šåˆ†å—ç ”ç©¶ç­–ç•¥&lt;/td&gt; 
   &lt;td&gt;å„Agentå†…éƒ¨å†³ç­–æ¨¡å—&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5-N&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;å¾ªç¯é˜¶æ®µ&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;è®ºå›åä½œ + æ·±åº¦ç ”ç©¶&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;ForumEngine + æ‰€æœ‰Agent&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;å¤šè½®å¾ªç¯&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5.1&lt;/td&gt; 
   &lt;td&gt;æ·±åº¦ç ”ç©¶&lt;/td&gt; 
   &lt;td&gt;å„AgentåŸºäºè®ºå›ä¸»æŒäººå¼•å¯¼è¿›è¡Œä¸“é¡¹æœç´¢&lt;/td&gt; 
   &lt;td&gt;å„Agent + åæ€æœºåˆ¶ + è®ºå›å¼•å¯¼&lt;/td&gt; 
   &lt;td&gt;æ¯è½®å¾ªç¯&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5.2&lt;/td&gt; 
   &lt;td&gt;è®ºå›åä½œ&lt;/td&gt; 
   &lt;td&gt;ForumEngineç›‘æ§Agentå‘è¨€å¹¶ç”Ÿæˆä¸»æŒäººæ€»ç»“&lt;/td&gt; 
   &lt;td&gt;ForumEngine + LLMä¸»æŒäºº&lt;/td&gt; 
   &lt;td&gt;æ¯è½®å¾ªç¯&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5.3&lt;/td&gt; 
   &lt;td&gt;äº¤æµèåˆ&lt;/td&gt; 
   &lt;td&gt;å„Agentæ ¹æ®è®¨è®ºè°ƒæ•´ç ”ç©¶æ–¹å‘&lt;/td&gt; 
   &lt;td&gt;å„Agent + forum_readerå·¥å…·&lt;/td&gt; 
   &lt;td&gt;æ¯è½®å¾ªç¯&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;N+1&lt;/td&gt; 
   &lt;td&gt;ç»“æœæ•´åˆ&lt;/td&gt; 
   &lt;td&gt;Report Agentæ”¶é›†æ‰€æœ‰åˆ†æç»“æœå’Œè®ºå›å†…å®¹&lt;/td&gt; 
   &lt;td&gt;Report Agent&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;N+2&lt;/td&gt; 
   &lt;td&gt;æŠ¥å‘Šç”Ÿæˆ&lt;/td&gt; 
   &lt;td&gt;åŠ¨æ€é€‰æ‹©æ¨¡æ¿å’Œæ ·å¼ï¼Œå¤šè½®ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š&lt;/td&gt; 
   &lt;td&gt;Report Agent + æ¨¡æ¿å¼•æ“&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;é¡¹ç›®ä»£ç ç»“æ„æ ‘&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;Weibo_PublicOpinion_AnalysisSystem/
â”œâ”€â”€ QueryEngine/                   # å›½å†…å¤–æ–°é—»å¹¿åº¦æœç´¢Agent
â”‚   â”œâ”€â”€ agent.py                   # Agentä¸»é€»è¾‘
â”‚   â”œâ”€â”€ llms/                      # LLMæ¥å£å°è£…
â”‚   â”œâ”€â”€ nodes/                     # å¤„ç†èŠ‚ç‚¹
â”‚   â”œâ”€â”€ tools/                     # æœç´¢å·¥å…·
â”‚   â”œâ”€â”€ utils/                     # å·¥å…·å‡½æ•°
â”‚   â””â”€â”€ ...                        # å…¶ä»–æ¨¡å—
â”œâ”€â”€ MediaEngine/                   # å¼ºå¤§çš„å¤šæ¨¡æ€ç†è§£Agent
â”‚   â”œâ”€â”€ agent.py                   # Agentä¸»é€»è¾‘
â”‚   â”œâ”€â”€ nodes/                     # å¤„ç†èŠ‚ç‚¹
â”‚   â”œâ”€â”€ llms/                      # LLMæ¥å£
â”‚   â”œâ”€â”€ tools/                     # æœç´¢å·¥å…·
â”‚   â”œâ”€â”€ utils/                     # å·¥å…·å‡½æ•°
â”‚   â””â”€â”€ ...                        # å…¶ä»–æ¨¡å—
â”œâ”€â”€ InsightEngine/                 # ç§æœ‰æ•°æ®åº“æŒ–æ˜Agent
â”‚   â”œâ”€â”€ agent.py                   # Agentä¸»é€»è¾‘
â”‚   â”œâ”€â”€ llms/                      # LLMæ¥å£å°è£…
â”‚   â”‚   â””â”€â”€ base.py                # ç»Ÿä¸€çš„ OpenAI å…¼å®¹å®¢æˆ·ç«¯
â”‚   â”œâ”€â”€ nodes/                     # å¤„ç†èŠ‚ç‚¹
â”‚   â”‚   â”œâ”€â”€ base_node.py           # åŸºç¡€èŠ‚ç‚¹ç±»
â”‚   â”‚   â”œâ”€â”€ formatting_node.py     # æ ¼å¼åŒ–èŠ‚ç‚¹
â”‚   â”‚   â”œâ”€â”€ report_structure_node.py # æŠ¥å‘Šç»“æ„èŠ‚ç‚¹
â”‚   â”‚   â”œâ”€â”€ search_node.py         # æœç´¢èŠ‚ç‚¹
â”‚   â”‚   â””â”€â”€ summary_node.py        # æ€»ç»“èŠ‚ç‚¹
â”‚   â”œâ”€â”€ tools/                     # æ•°æ®åº“æŸ¥è¯¢å’Œåˆ†æå·¥å…·
â”‚   â”‚   â”œâ”€â”€ keyword_optimizer.py   # Qwenå…³é”®è¯ä¼˜åŒ–ä¸­é—´ä»¶
â”‚   â”‚   â”œâ”€â”€ search.py              # æ•°æ®åº“æ“ä½œå·¥å…·é›†
â”‚   â”‚   â””â”€â”€ sentiment_analyzer.py  # æƒ…æ„Ÿåˆ†æé›†æˆå·¥å…·
â”‚   â”œâ”€â”€ state/                     # çŠ¶æ€ç®¡ç†
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ state.py               # AgentçŠ¶æ€å®šä¹‰
â”‚   â”œâ”€â”€ prompts/                   # æç¤ºè¯æ¨¡æ¿
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ prompts.py             # å„ç±»æç¤ºè¯
â”‚   â””â”€â”€ utils/                     # å·¥å…·å‡½æ•°
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ config.py              # é…ç½®ç®¡ç†
â”‚       â””â”€â”€ text_processing.py     # æ–‡æœ¬å¤„ç†å·¥å…·
â”œâ”€â”€ ReportEngine/                  # å¤šè½®æŠ¥å‘Šç”ŸæˆAgent
â”‚   â”œâ”€â”€ agent.py                   # Agentä¸»é€»è¾‘
â”‚   â”œâ”€â”€ llms/                      # LLMæ¥å£
â”‚   â”œâ”€â”€ nodes/                     # æŠ¥å‘Šç”ŸæˆèŠ‚ç‚¹
â”‚   â”‚   â”œâ”€â”€ template_selection.py  # æ¨¡æ¿é€‰æ‹©èŠ‚ç‚¹
â”‚   â”‚   â””â”€â”€ html_generation.py     # HTMLç”ŸæˆèŠ‚ç‚¹
â”‚   â”œâ”€â”€ report_template/           # æŠ¥å‘Šæ¨¡æ¿åº“
â”‚   â”‚   â”œâ”€â”€ ç¤¾ä¼šå…¬å…±çƒ­ç‚¹äº‹ä»¶åˆ†æ.md
â”‚   â”‚   â”œâ”€â”€ å•†ä¸šå“ç‰Œèˆ†æƒ…ç›‘æµ‹.md
â”‚   â”‚   â””â”€â”€ ...                    # æ›´å¤šæ¨¡æ¿
â”‚   â””â”€â”€ flask_interface.py         # Flask APIæ¥å£
â”œâ”€â”€ ForumEngine/                   # è®ºå›å¼•æ“ç®€æ˜“å®ç°
â”‚   â”œâ”€â”€ monitor.py                 # æ—¥å¿—ç›‘æ§å’Œè®ºå›ç®¡ç†
â”‚   â””â”€â”€ llm_host.py                # è®ºå›ä¸»æŒäººLLMæ¨¡å—
â”œâ”€â”€ MindSpider/                    # å¾®åšçˆ¬è™«ç³»ç»Ÿ
â”‚   â”œâ”€â”€ main.py                    # çˆ¬è™«ä¸»ç¨‹åº
â”‚   â”œâ”€â”€ config.py                  # çˆ¬è™«é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ BroadTopicExtraction/      # è¯é¢˜æå–æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ database_manager.py    # æ•°æ®åº“ç®¡ç†å™¨
â”‚   â”‚   â”œâ”€â”€ get_today_news.py      # ä»Šæ—¥æ–°é—»è·å–
â”‚   â”‚   â”œâ”€â”€ main.py                # è¯é¢˜æå–ä¸»ç¨‹åº
â”‚   â”‚   â””â”€â”€ topic_extractor.py     # è¯é¢˜æå–å™¨
â”‚   â”œâ”€â”€ DeepSentimentCrawling/     # æ·±åº¦èˆ†æƒ…çˆ¬å–
â”‚   â”‚   â”œâ”€â”€ keyword_manager.py     # å…³é”®è¯ç®¡ç†å™¨
â”‚   â”‚   â”œâ”€â”€ main.py                # æ·±åº¦çˆ¬å–ä¸»ç¨‹åº
â”‚   â”‚   â”œâ”€â”€ MediaCrawler/          # åª’ä½“çˆ¬è™«æ ¸å¿ƒ
â”‚   â”‚   â””â”€â”€ platform_crawler.py    # å¹³å°çˆ¬è™«ç®¡ç†
â”‚   â””â”€â”€ schema/                    # æ•°æ®åº“ç»“æ„
â”‚       â”œâ”€â”€ db_manager.py          # æ•°æ®åº“ç®¡ç†å™¨
â”‚       â”œâ”€â”€ init_database.py       # æ•°æ®åº“åˆå§‹åŒ–
â”‚       â””â”€â”€ mindspider_tables.sql  # æ•°æ®åº“è¡¨ç»“æ„
â”œâ”€â”€ SentimentAnalysisModel/        # æƒ…æ„Ÿåˆ†ææ¨¡å‹é›†åˆ
â”‚   â”œâ”€â”€ WeiboSentiment_Finetuned/  # å¾®è°ƒBERT/GPT-2æ¨¡å‹
â”‚   â”œâ”€â”€ WeiboMultilingualSentiment/# å¤šè¯­è¨€æƒ…æ„Ÿåˆ†æï¼ˆæ¨èï¼‰
â”‚   â”œâ”€â”€ WeiboSentiment_SmallQwen/  # å°å‚æ•°Qwen3å¾®è°ƒ
â”‚   â””â”€â”€ WeiboSentiment_MachineLearning/ # ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•
â”œâ”€â”€ SingleEngineApp/               # å•ç‹¬Agentçš„Streamlitåº”ç”¨
â”‚   â”œâ”€â”€ query_engine_streamlit_app.py
â”‚   â”œâ”€â”€ media_engine_streamlit_app.py
â”‚   â””â”€â”€ insight_engine_streamlit_app.py
â”œâ”€â”€ templates/                     # Flaskæ¨¡æ¿
â”‚   â””â”€â”€ index.html                 # ä¸»ç•Œé¢å‰ç«¯
â”œâ”€â”€ static/                        # é™æ€èµ„æº
â”œâ”€â”€ logs/                          # è¿è¡Œæ—¥å¿—ç›®å½•
â”œâ”€â”€ final_reports/                 # æœ€ç»ˆç”Ÿæˆçš„HTMLæŠ¥å‘Šæ–‡ä»¶
â”œâ”€â”€ utils/                         # é€šç”¨å·¥å…·å‡½æ•°
â”‚   â”œâ”€â”€ forum_reader.py            # Agenté—´è®ºå›é€šä¿¡
â”‚   â””â”€â”€ retry_helper.py            # ç½‘ç»œè¯·æ±‚é‡è¯•æœºåˆ¶å·¥å…·
â”œâ”€â”€ app.py                         # Flaskä¸»åº”ç”¨å…¥å£
â”œâ”€â”€ config.py                      # å…¨å±€é…ç½®æ–‡ä»¶
â””â”€â”€ requirements.txt               # Pythonä¾èµ–åŒ…æ¸…å•
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ğŸš€ å¿«é€Ÿå¼€å§‹&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;å¦‚æœä½ æ˜¯åˆæ¬¡å­¦ä¹ ä¸€ä¸ªAgentç³»ç»Ÿçš„æ­å»ºï¼Œå¯ä»¥ä»ä¸€ä¸ªéå¸¸ç®€å•çš„demoå¼€å§‹ï¼š&lt;a href="https://github.com/666ghj/DeepSearchAgent-Demo"&gt;Deep Search Agent Demo&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;ç¯å¢ƒè¦æ±‚&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;æ“ä½œç³»ç»Ÿ&lt;/strong&gt;: Windowsã€Linuxã€MacOS&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Pythonç‰ˆæœ¬&lt;/strong&gt;: 3.9+&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Conda&lt;/strong&gt;: Anacondaæˆ–Miniconda&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;æ•°æ®åº“&lt;/strong&gt;: MySQLï¼ˆå¯é€‰æ‹©æˆ‘ä»¬çš„äº‘æ•°æ®åº“æœåŠ¡ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;å†…å­˜&lt;/strong&gt;: å»ºè®®2GBä»¥ä¸Š&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;1. åˆ›å»ºç¯å¢ƒ&lt;/h3&gt; 
&lt;h4&gt;å¦‚æœä½¿ç”¨Conda&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# åˆ›å»ºcondaç¯å¢ƒ
conda create -n your_conda_name python=3.11
conda activate your_conda_name
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;å¦‚æœä½¿ç”¨uv&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# åˆ›å»ºuvç¯å¢ƒ
uv venv --python 3.11 # åˆ›å»º3.11ç¯å¢ƒ
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. å®‰è£…ä¾èµ–åŒ…&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# åŸºç¡€ä¾èµ–å®‰è£…
pip install -r requirements.txt

# uvç‰ˆæœ¬å‘½ä»¤ï¼ˆæ›´å¿«é€Ÿå®‰è£…ï¼‰
uv pip install -r requirements.txt
# å¦‚æœä¸æƒ³ä½¿ç”¨æœ¬åœ°æƒ…æ„Ÿåˆ†ææ¨¡å‹ï¼ˆç®—åŠ›éœ€æ±‚å¾ˆå°ï¼Œé»˜è®¤å®‰è£…cpuç‰ˆæœ¬ï¼‰ï¼Œå¯ä»¥å°†è¯¥æ–‡ä»¶ä¸­çš„â€œæœºå™¨å­¦ä¹ â€éƒ¨åˆ†æ³¨é‡Šæ‰å†æ‰§è¡ŒæŒ‡ä»¤
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;3. å®‰è£…Playwrightæµè§ˆå™¨é©±åŠ¨&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# å®‰è£…æµè§ˆå™¨é©±åŠ¨ï¼ˆç”¨äºçˆ¬è™«åŠŸèƒ½ï¼‰
playwright install chromium
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;4. é…ç½®ç³»ç»Ÿ&lt;/h3&gt; 
&lt;h4&gt;4.1 é…ç½®APIå¯†é’¥&lt;/h4&gt; 
&lt;p&gt;å¤åˆ¶ä¸€ä»½ é¡¹ç›®æ ¹ç›®å½• &lt;code&gt;.env.example&lt;/code&gt; æ–‡ä»¶ï¼Œå‘½åä¸º &lt;code&gt;.env&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;ç¼–è¾‘ &lt;code&gt;.env&lt;/code&gt; æ–‡ä»¶ï¼Œå¡«å…¥æ‚¨çš„APIå¯†é’¥ï¼ˆæ‚¨ä¹Ÿå¯ä»¥é€‰æ‹©è‡ªå·±çš„æ¨¡å‹ã€æœç´¢ä»£ç†ï¼Œè¯¦æƒ…è§æ ¹ç›®å½•.env.exampleæ–‡ä»¶å†…æˆ–æ ¹ç›®å½•config.pyä¸­çš„è¯´æ˜ï¼‰ï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# MySQLæ•°æ®åº“é…ç½®
DB_HOST = "localhost"
DB_PORT = 3306
DB_USER = "your_username"
DB_PASSWORD = "your_password"
DB_NAME = "your_db_name"
DB_CHARSET = "utf8mb4"

# LLMé…ç½®
# æ‚¨å¯ä»¥æ›´æ”¹æ¯ä¸ªéƒ¨åˆ†LLMä½¿ç”¨çš„APIï¼Œåªè¦å…¼å®¹OpenAIè¯·æ±‚æ ¼å¼éƒ½å¯ä»¥

# Insight Agent
INSIGHT_ENGINE_API_KEY = "your_api_key"
INSIGHT_ENGINE_BASE_URL = "https://api.moonshot.cn/v1"
INSIGHT_ENGINE_MODEL_NAME = "kimi-k2-0711-preview"
# Media Agent
...
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;æ¨èLLM APIä¾›åº”å•†ï¼š&lt;a href="https://aihubmix.com/?aff=8Ds9"&gt;æ¨ç†æ—¶ä»£&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;4.2 æ•°æ®åº“åˆå§‹åŒ–&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;é€‰æ‹©1ï¼šä½¿ç”¨æœ¬åœ°æ•°æ®åº“&lt;/strong&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;del&gt;MindSpiderçˆ¬è™«ç³»ç»Ÿè·Ÿèˆ†æƒ…ç³»ç»Ÿæ˜¯å„è‡ªç‹¬ç«‹çš„ï¼Œæ‰€ä»¥éœ€è¦å†å»&lt;code&gt;MindSpider\config.py&lt;/code&gt;é…ç½®ä¸€ä¸‹ï¼Œå¤åˆ¶&lt;code&gt;MindSpider&lt;/code&gt;æ–‡ä»¶å¤¹ä¸‹çš„ &lt;code&gt;config.py.example&lt;/code&gt; æ–‡ä»¶ï¼Œå‘½åä¸º &lt;code&gt;config.py&lt;/code&gt;&lt;/del&gt;&lt;br /&gt; ç°ç‰ˆæœ¬å·²æ›´æ”¹ä¸ºåŸºäºç¯å¢ƒå˜é‡é…ç½®ï¼Œè¯·å¤åˆ¶é¡¹ç›®æ ¹ç›®å½•.env.exampleæ–‡ä»¶ä¸º.envæ–‡ä»¶ï¼Œå¹¶åœ¨å…¶ä¸­å¡«å†™å„é¡¹é…ç½®&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# æœ¬åœ°MySQLæ•°æ®åº“åˆå§‹åŒ–
cd MindSpider
# é¡¹ç›®åˆå§‹åŒ–
python main.py --setup

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;é€‰æ‹©2ï¼šä½¿ç”¨äº‘æ•°æ®åº“æœåŠ¡ï¼ˆæ¨èï¼‰&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;æˆ‘ä»¬æä¾›ä¾¿æ·çš„äº‘æ•°æ®åº“æœåŠ¡ï¼ŒåŒ…å«æ—¥å‡10ä¸‡+çœŸå®èˆ†æƒ…æ•°æ®ï¼Œç›®å‰&lt;strong&gt;å…è´¹ç”³è¯·&lt;/strong&gt;ï¼&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;çœŸå®èˆ†æƒ…æ•°æ®ï¼Œå®æ—¶æ›´æ–°&lt;/li&gt; 
 &lt;li&gt;å¤šç»´åº¦æ ‡ç­¾åˆ†ç±»&lt;/li&gt; 
 &lt;li&gt;é«˜å¯ç”¨äº‘ç«¯æœåŠ¡&lt;/li&gt; 
 &lt;li&gt;ä¸“ä¸šæŠ€æœ¯æ”¯æŒ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;è”ç³»æˆ‘ä»¬ç”³è¯·å…è´¹äº‘æ•°æ®åº“è®¿é—®ï¼šğŸ“§ &lt;a href="mailto:670939375@qq.com"&gt;670939375@qq.com&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ä¸ºè¿›è¡Œæ•°æ®åˆè§„æ€§å®¡æŸ¥ä¸æœåŠ¡å‡çº§ï¼Œäº‘æ•°æ®åº“è‡ª2025å¹´10æœˆ1æ—¥èµ·æš‚åœæ¥æ”¶æ–°çš„ä½¿ç”¨ç”³è¯·&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;5. å¯åŠ¨ç³»ç»Ÿ&lt;/h3&gt; 
&lt;h4&gt;5.1 å®Œæ•´ç³»ç»Ÿå¯åŠ¨ï¼ˆæ¨èï¼‰&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹ï¼Œæ¿€æ´»condaç¯å¢ƒ
conda activate your_conda_name

# å¯åŠ¨ä¸»åº”ç”¨å³å¯
python app.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;uv ç‰ˆæœ¬å¯åŠ¨å‘½ä»¤&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹ï¼Œæ¿€æ´»uvç¯å¢ƒ
.venv\Scripts\activate

# å¯åŠ¨ä¸»åº”ç”¨å³å¯
python app.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;æ³¨1ï¼šä¸€æ¬¡è¿è¡Œç»ˆæ­¢åï¼Œstreamlit appå¯èƒ½ç»“æŸå¼‚å¸¸ä»ç„¶å ç”¨ç«¯å£ï¼Œæ­¤æ—¶æœç´¢å ç”¨ç«¯å£çš„è¿›ç¨‹killæ‰å³å¯&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;æ³¨2ï¼šæ•°æ®çˆ¬å–éœ€è¦å•ç‹¬æ“ä½œï¼Œè§5.3æŒ‡å¼•&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;æ³¨3ï¼šå¦‚æœæœåŠ¡å™¨è¿œç¨‹éƒ¨ç½²å‡ºç°é¡µé¢æ˜¾ç¤ºé—®é¢˜ï¼Œè§&lt;a href="https://github.com/666ghj/BettaFish/pull/45"&gt;PR#45&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;è®¿é—® &lt;a href="http://localhost:5000"&gt;http://localhost:5000&lt;/a&gt; å³å¯ä½¿ç”¨å®Œæ•´ç³»ç»Ÿ&lt;/p&gt; 
&lt;h4&gt;5.2 å•ç‹¬å¯åŠ¨æŸä¸ªAgent&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# å¯åŠ¨QueryEngine
streamlit run SingleEngineApp/query_engine_streamlit_app.py --server.port 8503

# å¯åŠ¨MediaEngine  
streamlit run SingleEngineApp/media_engine_streamlit_app.py --server.port 8502

# å¯åŠ¨InsightEngine
streamlit run SingleEngineApp/insight_engine_streamlit_app.py --server.port 8501
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;5.3 çˆ¬è™«ç³»ç»Ÿå•ç‹¬ä½¿ç”¨&lt;/h4&gt; 
&lt;p&gt;è¿™éƒ¨åˆ†æœ‰è¯¦ç»†çš„é…ç½®æ–‡æ¡£ï¼š&lt;a href="https://raw.githubusercontent.com/666ghj/BettaFish/main/MindSpider/README.md"&gt;MindSpiderä½¿ç”¨è¯´æ˜&lt;/a&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="MindSpider\img\example.png" alt="banner" width="600" /&gt; 
 &lt;p&gt;MindSpider è¿è¡Œç¤ºä¾‹&lt;/p&gt; 
&lt;/div&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# è¿›å…¥çˆ¬è™«ç›®å½•
cd MindSpider

# é¡¹ç›®åˆå§‹åŒ–
python main.py --setup

# è¿è¡Œè¯é¢˜æå–ï¼ˆè·å–çƒ­ç‚¹æ–°é—»å’Œå…³é”®è¯ï¼‰
python main.py --broad-topic

# è¿è¡Œå®Œæ•´çˆ¬è™«æµç¨‹
python main.py --complete --date 2024-01-20

# ä»…è¿è¡Œè¯é¢˜æå–
python main.py --broad-topic --date 2024-01-20

# ä»…è¿è¡Œæ·±åº¦çˆ¬å–
python main.py --deep-sentiment --platforms xhs dy wb
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;âš™ï¸ é«˜çº§é…ç½®ï¼ˆå·²è¿‡æ—¶ï¼Œå·²ç»ç»Ÿä¸€ä¸ºé¡¹ç›®æ ¹ç›®å½•.envæ–‡ä»¶ç®¡ç†ï¼Œå…¶ä»–å­agentè‡ªåŠ¨ç»§æ‰¿æ ¹ç›®å½•é…ç½®ï¼‰&lt;/h2&gt; 
&lt;h3&gt;ä¿®æ”¹å…³é”®å‚æ•°&lt;/h3&gt; 
&lt;h4&gt;Agenté…ç½®å‚æ•°&lt;/h4&gt; 
&lt;p&gt;æ¯ä¸ªAgentéƒ½æœ‰ä¸“é—¨çš„é…ç½®æ–‡ä»¶ï¼Œå¯æ ¹æ®éœ€æ±‚è°ƒæ•´ï¼Œä¸‹é¢æ˜¯éƒ¨åˆ†ç¤ºä¾‹ï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# QueryEngine/utils/config.py
class Config:
    max_reflections = 2           # åæ€è½®æ¬¡
    max_search_results = 15       # æœ€å¤§æœç´¢ç»“æœæ•°
    max_content_length = 8000     # æœ€å¤§å†…å®¹é•¿åº¦
    
# MediaEngine/utils/config.py  
class Config:
    comprehensive_search_limit = 10  # ç»¼åˆæœç´¢é™åˆ¶
    web_search_limit = 15           # ç½‘é¡µæœç´¢é™åˆ¶
    
# InsightEngine/utils/config.py
class Config:
    default_search_topic_globally_limit = 200    # å…¨å±€æœç´¢é™åˆ¶
    default_get_comments_limit = 500             # è¯„è®ºè·å–é™åˆ¶
    max_search_results_for_llm = 50              # ä¼ ç»™LLMçš„æœ€å¤§ç»“æœæ•°
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;æƒ…æ„Ÿåˆ†ææ¨¡å‹é…ç½®&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# InsightEngine/tools/sentiment_analyzer.py
SENTIMENT_CONFIG = {
    'model_type': 'multilingual',     # å¯é€‰: 'bert', 'multilingual', 'qwen'ç­‰
    'confidence_threshold': 0.8,      # ç½®ä¿¡åº¦é˜ˆå€¼
    'batch_size': 32,                 # æ‰¹å¤„ç†å¤§å°
    'max_sequence_length': 512,       # æœ€å¤§åºåˆ—é•¿åº¦
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;æ¥å…¥ä¸åŒçš„LLMæ¨¡å‹&lt;/h3&gt; 
&lt;p&gt;æ”¯æŒä»»æ„openAIè°ƒç”¨æ ¼å¼çš„LLMæä¾›å•†ï¼Œåªéœ€è¦åœ¨/config.pyä¸­å¡«å†™å¯¹åº”çš„KEYã€BASE_URLã€MODEL_NAMEå³å¯ã€‚&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ä»€ä¹ˆæ˜¯openAIè°ƒç”¨æ ¼å¼ï¼Ÿä¸‹é¢æä¾›ä¸€ä¸ªç®€å•çš„ä¾‹å­ï¼š&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from openai import OpenAI

client = OpenAI(api_key="your_api_key", 
               base_url="https://api.siliconflow.cn/v1")

response = client.chat.completions.create(
   model="Qwen/Qwen2.5-72B-Instruct",
   messages=[
       {'role': 'user', 
        'content': "æ¨ç†æ¨¡å‹ä¼šç»™å¸‚åœºå¸¦æ¥å“ªäº›æ–°çš„æœºä¼š"}
   ],
)

complete_response = response.choices[0].message.content
print(complete_response)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;æ›´æ”¹æƒ…æ„Ÿåˆ†ææ¨¡å‹&lt;/h3&gt; 
&lt;p&gt;ç³»ç»Ÿé›†æˆäº†å¤šç§æƒ…æ„Ÿåˆ†ææ–¹æ³•ï¼Œå¯æ ¹æ®éœ€æ±‚é€‰æ‹©ï¼š&lt;/p&gt; 
&lt;h4&gt;1. å¤šè¯­è¨€æƒ…æ„Ÿåˆ†æ&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd SentimentAnalysisModel/WeiboMultilingualSentiment
python predict.py --text "This product is amazing!" --lang "en"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;2. å°å‚æ•°Qwen3å¾®è°ƒ&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd SentimentAnalysisModel/WeiboSentiment_SmallQwen
python predict_universal.py --text "è¿™æ¬¡æ´»åŠ¨åŠå¾—å¾ˆæˆåŠŸ"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;3. åŸºäºBERTçš„å¾®è°ƒæ¨¡å‹&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# ä½¿ç”¨BERTä¸­æ–‡æ¨¡å‹
cd SentimentAnalysisModel/WeiboSentiment_Finetuned/BertChinese-Lora
python predict.py --text "è¿™ä¸ªäº§å“çœŸçš„å¾ˆä¸é”™"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;4. GPT-2 LoRAå¾®è°ƒæ¨¡å‹&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd SentimentAnalysisModel/WeiboSentiment_Finetuned/GPT2-Lora
python predict.py --text "ä»Šå¤©å¿ƒæƒ…ä¸å¤ªå¥½"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;5. ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd SentimentAnalysisModel/WeiboSentiment_MachineLearning
python predict.py --model_type "svm" --text "æœåŠ¡æ€åº¦éœ€è¦æ”¹è¿›"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;æ¥å…¥è‡ªå®šä¹‰ä¸šåŠ¡æ•°æ®åº“&lt;/h3&gt; 
&lt;h4&gt;1. ä¿®æ”¹æ•°æ®åº“è¿æ¥é…ç½®&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# config.py ä¸­æ·»åŠ æ‚¨çš„ä¸šåŠ¡æ•°æ®åº“é…ç½®
BUSINESS_DB_HOST = "your_business_db_host"
BUSINESS_DB_PORT = 3306
BUSINESS_DB_USER = "your_business_user"
BUSINESS_DB_PASSWORD = "your_business_password"
BUSINESS_DB_NAME = "your_business_database"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;2. åˆ›å»ºè‡ªå®šä¹‰æ•°æ®è®¿é—®å·¥å…·&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# InsightEngine/tools/custom_db_tool.py
class CustomBusinessDBTool:
    """è‡ªå®šä¹‰ä¸šåŠ¡æ•°æ®åº“æŸ¥è¯¢å·¥å…·"""
    
    def __init__(self):
        self.connection_config = {
            'host': config.BUSINESS_DB_HOST,
            'port': config.BUSINESS_DB_PORT,
            'user': config.BUSINESS_DB_USER,
            'password': config.BUSINESS_DB_PASSWORD,
            'database': config.BUSINESS_DB_NAME,
        }
    
    def search_business_data(self, query: str, table: str):
        """æŸ¥è¯¢ä¸šåŠ¡æ•°æ®"""
        # å®ç°æ‚¨çš„ä¸šåŠ¡é€»è¾‘
        pass
    
    def get_customer_feedback(self, product_id: str):
        """è·å–å®¢æˆ·åé¦ˆæ•°æ®"""
        # å®ç°å®¢æˆ·åé¦ˆæŸ¥è¯¢é€»è¾‘
        pass
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;3. é›†æˆåˆ°InsightEngine&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# InsightEngine/agent.py ä¸­é›†æˆè‡ªå®šä¹‰å·¥å…·
from .tools.custom_db_tool import CustomBusinessDBTool

class DeepSearchAgent:
    def __init__(self, config=None):
        # ... å…¶ä»–åˆå§‹åŒ–ä»£ç 
        self.custom_db_tool = CustomBusinessDBTool()
    
    def execute_custom_search(self, query: str):
        """æ‰§è¡Œè‡ªå®šä¹‰ä¸šåŠ¡æ•°æ®æœç´¢"""
        return self.custom_db_tool.search_business_data(query, "your_table")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;è‡ªå®šä¹‰æŠ¥å‘Šæ¨¡æ¿&lt;/h3&gt; 
&lt;h4&gt;1. åœ¨Webç•Œé¢ä¸­ä¸Šä¼ &lt;/h4&gt; 
&lt;p&gt;ç³»ç»Ÿæ”¯æŒä¸Šä¼ è‡ªå®šä¹‰æ¨¡æ¿æ–‡ä»¶ï¼ˆ.mdæˆ–.txtæ ¼å¼ï¼‰ï¼Œå¯åœ¨ç”ŸæˆæŠ¥å‘Šæ—¶é€‰æ‹©ä½¿ç”¨ã€‚&lt;/p&gt; 
&lt;h4&gt;2. åˆ›å»ºæ¨¡æ¿æ–‡ä»¶&lt;/h4&gt; 
&lt;p&gt;åœ¨ &lt;code&gt;ReportEngine/report_template/&lt;/code&gt; ç›®å½•ä¸‹åˆ›å»ºæ–°çš„æ¨¡æ¿ï¼Œæˆ‘ä»¬çš„Agentä¼šè‡ªè¡Œé€‰ç”¨æœ€åˆé€‚çš„æ¨¡æ¿ã€‚&lt;/p&gt; 
&lt;h2&gt;ğŸ¤ è´¡çŒ®æŒ‡å—&lt;/h2&gt; 
&lt;p&gt;æˆ‘ä»¬æ¬¢è¿æ‰€æœ‰å½¢å¼çš„è´¡çŒ®ï¼&lt;/p&gt; 
&lt;h3&gt;å¦‚ä½•è´¡çŒ®&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Forké¡¹ç›®&lt;/strong&gt;åˆ°æ‚¨çš„GitHubè´¦å·&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;åˆ›å»ºFeatureåˆ†æ”¯&lt;/strong&gt;ï¼š&lt;code&gt;git checkout -b feature/AmazingFeature&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;æäº¤æ›´æ”¹&lt;/strong&gt;ï¼š&lt;code&gt;git commit -m 'Add some AmazingFeature'&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;æ¨é€åˆ°åˆ†æ”¯&lt;/strong&gt;ï¼š&lt;code&gt;git push origin feature/AmazingFeature&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;å¼€å¯Pull Request&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;å¼€å‘è§„èŒƒ&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ä»£ç éµå¾ªPEP8è§„èŒƒ&lt;/li&gt; 
 &lt;li&gt;æäº¤ä¿¡æ¯ä½¿ç”¨æ¸…æ™°çš„ä¸­è‹±æ–‡æè¿°&lt;/li&gt; 
 &lt;li&gt;æ–°åŠŸèƒ½éœ€è¦åŒ…å«ç›¸åº”çš„æµ‹è¯•ç”¨ä¾‹&lt;/li&gt; 
 &lt;li&gt;æ›´æ–°ç›¸å…³æ–‡æ¡£&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ¦– ä¸‹ä¸€æ­¥å¼€å‘è®¡åˆ’&lt;/h2&gt; 
&lt;p&gt;ç°åœ¨ç³»ç»Ÿåªå®Œæˆäº†"ä¸‰æ¿æ–§"ä¸­çš„å‰ä¸¤æ­¥ï¼Œå³ï¼šè¾“å…¥è¦æ±‚-&amp;gt;è¯¦ç»†åˆ†æï¼Œè¿˜ç¼ºå°‘ä¸€æ­¥é¢„æµ‹ï¼Œç›´æ¥å°†ä»–ç»§ç»­äº¤ç»™LLMæ˜¯ä¸å…·æœ‰è¯´æœåŠ›çš„ã€‚&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/666ghj/BettaFish/main/static/image/banner_compressed.png" alt="banner" width="800" /&gt; 
&lt;/div&gt; 
&lt;p&gt;ç›®å‰æˆ‘ä»¬ç»è¿‡å¾ˆé•¿ä¸€æ®µæ—¶é—´çš„çˆ¬å–æ”¶é›†ï¼Œæ‹¥æœ‰äº†å¤§é‡å…¨ç½‘è¯é¢˜çƒ­åº¦éšæ—¶é—´ã€çˆ†ç‚¹ç­‰çš„å˜åŒ–è¶‹åŠ¿çƒ­åº¦æ•°æ®ï¼Œå·²ç»å…·å¤‡äº†å¯ä»¥å¼€å‘é¢„æµ‹æ¨¡å‹çš„æ¡ä»¶ã€‚æˆ‘ä»¬å›¢é˜Ÿå°†è¿ç”¨æ—¶åºæ¨¡å‹ã€å›¾ç¥ç»ç½‘ç»œã€å¤šæ¨¡æ€èåˆç­‰é¢„æµ‹æ¨¡å‹æŠ€æœ¯å‚¨å¤‡äºæ­¤ï¼Œå®ç°çœŸæ­£åŸºäºæ•°æ®é©±åŠ¨çš„èˆ†æƒ…é¢„æµ‹åŠŸèƒ½ã€‚&lt;/p&gt; 
&lt;h2&gt;âš ï¸ å…è´£å£°æ˜&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;é‡è¦æé†’ï¼šæœ¬é¡¹ç›®ä»…ä¾›å­¦ä¹ ã€å­¦æœ¯ç ”ç©¶å’Œæ•™è‚²ç›®çš„ä½¿ç”¨&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;åˆè§„æ€§å£°æ˜&lt;/strong&gt;ï¼š&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;æœ¬é¡¹ç›®ä¸­çš„æ‰€æœ‰ä»£ç ã€å·¥å…·å’ŒåŠŸèƒ½å‡ä»…ä¾›å­¦ä¹ ã€å­¦æœ¯ç ”ç©¶å’Œæ•™è‚²ç›®çš„ä½¿ç”¨&lt;/li&gt; 
   &lt;li&gt;ä¸¥ç¦å°†æœ¬é¡¹ç›®ç”¨äºä»»ä½•å•†ä¸šç”¨é€”æˆ–ç›ˆåˆ©æ€§æ´»åŠ¨&lt;/li&gt; 
   &lt;li&gt;ä¸¥ç¦å°†æœ¬é¡¹ç›®ç”¨äºä»»ä½•è¿æ³•ã€è¿è§„æˆ–ä¾µçŠ¯ä»–äººæƒç›Šçš„è¡Œä¸º&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;çˆ¬è™«åŠŸèƒ½å…è´£&lt;/strong&gt;ï¼š&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;é¡¹ç›®ä¸­çš„çˆ¬è™«åŠŸèƒ½ä»…ç”¨äºæŠ€æœ¯å­¦ä¹ å’Œç ”ç©¶ç›®çš„&lt;/li&gt; 
   &lt;li&gt;ä½¿ç”¨è€…å¿…é¡»éµå®ˆç›®æ ‡ç½‘ç«™çš„robots.txtåè®®å’Œä½¿ç”¨æ¡æ¬¾&lt;/li&gt; 
   &lt;li&gt;ä½¿ç”¨è€…å¿…é¡»éµå®ˆç›¸å…³æ³•å¾‹æ³•è§„ï¼Œä¸å¾—è¿›è¡Œæ¶æ„çˆ¬å–æˆ–æ•°æ®æ»¥ç”¨&lt;/li&gt; 
   &lt;li&gt;å› ä½¿ç”¨çˆ¬è™«åŠŸèƒ½äº§ç”Ÿçš„ä»»ä½•æ³•å¾‹åæœç”±ä½¿ç”¨è€…è‡ªè¡Œæ‰¿æ‹…&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;æ•°æ®ä½¿ç”¨å…è´£&lt;/strong&gt;ï¼š&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;é¡¹ç›®æ¶‰åŠçš„æ•°æ®åˆ†æåŠŸèƒ½ä»…ä¾›å­¦æœ¯ç ”ç©¶ä½¿ç”¨&lt;/li&gt; 
   &lt;li&gt;ä¸¥ç¦å°†åˆ†æç»“æœç”¨äºå•†ä¸šå†³ç­–æˆ–ç›ˆåˆ©ç›®çš„&lt;/li&gt; 
   &lt;li&gt;ä½¿ç”¨è€…åº”ç¡®ä¿æ‰€åˆ†ææ•°æ®çš„åˆæ³•æ€§å’Œåˆè§„æ€§&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;æŠ€æœ¯å…è´£&lt;/strong&gt;ï¼š&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;æœ¬é¡¹ç›®æŒ‰"ç°çŠ¶"æä¾›ï¼Œä¸æä¾›ä»»ä½•æ˜ç¤ºæˆ–æš—ç¤ºçš„ä¿è¯&lt;/li&gt; 
   &lt;li&gt;ä½œè€…ä¸å¯¹ä½¿ç”¨æœ¬é¡¹ç›®é€ æˆçš„ä»»ä½•ç›´æ¥æˆ–é—´æ¥æŸå¤±æ‰¿æ‹…è´£ä»»&lt;/li&gt; 
   &lt;li&gt;ä½¿ç”¨è€…åº”è‡ªè¡Œè¯„ä¼°é¡¹ç›®çš„é€‚ç”¨æ€§å’Œé£é™©&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;è´£ä»»é™åˆ¶&lt;/strong&gt;ï¼š&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;ä½¿ç”¨è€…åœ¨ä½¿ç”¨æœ¬é¡¹ç›®å‰åº”å……åˆ†äº†è§£ç›¸å…³æ³•å¾‹æ³•è§„&lt;/li&gt; 
   &lt;li&gt;ä½¿ç”¨è€…åº”ç¡®ä¿å…¶ä½¿ç”¨è¡Œä¸ºç¬¦åˆå½“åœ°æ³•å¾‹æ³•è§„è¦æ±‚&lt;/li&gt; 
   &lt;li&gt;å› è¿åæ³•å¾‹æ³•è§„ä½¿ç”¨æœ¬é¡¹ç›®è€Œäº§ç”Ÿçš„ä»»ä½•åæœç”±ä½¿ç”¨è€…è‡ªè¡Œæ‰¿æ‹…&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;è¯·åœ¨ä½¿ç”¨æœ¬é¡¹ç›®å‰ä»”ç»†é˜…è¯»å¹¶ç†è§£ä¸Šè¿°å…è´£å£°æ˜ã€‚ä½¿ç”¨æœ¬é¡¹ç›®å³è¡¨ç¤ºæ‚¨å·²åŒæ„å¹¶æ¥å—ä¸Šè¿°æ‰€æœ‰æ¡æ¬¾ã€‚&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;ğŸ“„ è®¸å¯è¯&lt;/h2&gt; 
&lt;p&gt;æœ¬é¡¹ç›®é‡‡ç”¨ &lt;a href="https://raw.githubusercontent.com/666ghj/BettaFish/main/LICENSE"&gt;GPL-2.0è®¸å¯è¯&lt;/a&gt;ã€‚è¯¦ç»†ä¿¡æ¯è¯·å‚é˜…LICENSEæ–‡ä»¶ã€‚&lt;/p&gt; 
&lt;h2&gt;ğŸ‰ æ”¯æŒä¸è”ç³»&lt;/h2&gt; 
&lt;h3&gt;è·å–å¸®åŠ©&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;é¡¹ç›®ä¸»é¡µ&lt;/strong&gt;ï¼š&lt;a href="https://github.com/666ghj/Weibo_PublicOpinion_AnalysisSystem"&gt;GitHubä»“åº“&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;é—®é¢˜åé¦ˆ&lt;/strong&gt;ï¼š&lt;a href="https://github.com/666ghj/Weibo_PublicOpinion_AnalysisSystem/issues"&gt;Issuesé¡µé¢&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;åŠŸèƒ½å»ºè®®&lt;/strong&gt;ï¼š&lt;a href="https://github.com/666ghj/Weibo_PublicOpinion_AnalysisSystem/discussions"&gt;Discussionsé¡µé¢&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;è”ç³»æ–¹å¼&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“§ &lt;strong&gt;é‚®ç®±&lt;/strong&gt;ï¼š&lt;a href="mailto:670939375@qq.com"&gt;670939375@qq.com&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;å•†åŠ¡åˆä½œ&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ä¼ä¸šå®šåˆ¶å¼€å‘&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;å¤§æ•°æ®æœåŠ¡&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;å­¦æœ¯åˆä½œ&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;æŠ€æœ¯åŸ¹è®­&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ‘¥ è´¡çŒ®è€…&lt;/h2&gt; 
&lt;p&gt;æ„Ÿè°¢ä»¥ä¸‹ä¼˜ç§€çš„è´¡çŒ®è€…ä»¬ï¼š&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/666ghj/Weibo_PublicOpinion_AnalysisSystem/graphs/contributors"&gt;&lt;img src="https://contrib.rocks/image?repo=666ghj/Weibo_PublicOpinion_AnalysisSystem" alt="Contributors" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;ğŸ“ˆ é¡¹ç›®ç»Ÿè®¡&lt;/h2&gt; 
&lt;a href="https://www.star-history.com/#666ghj/BettaFish&amp;amp;type=date&amp;amp;legend=top-left"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=666ghj/BettaFish&amp;amp;type=date&amp;amp;theme=dark&amp;amp;legend=top-left" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=666ghj/BettaFish&amp;amp;type=date&amp;amp;legend=top-left" /&gt; 
  &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=666ghj/BettaFish&amp;amp;type=date&amp;amp;legend=top-left" /&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;p&gt;&lt;img src="https://repobeats.axiom.co/api/embed/e04e3eea4674edc39c148a7845c8d09c1b7b1922.svg?sanitize=true" alt="Alt" title="Repobeats analytics image" /&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>marimo-team/marimo</title>
      <link>https://github.com/marimo-team/marimo</link>
      <description>&lt;p&gt;A reactive notebook for Python â€” run reproducible experiments, query with SQL, execute as a script, deploy as an app, and version with git. Stored as pure Python. All in a modern, AI-native editor.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/marimo-logotype-thick.svg?sanitize=true" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;em&gt;A reactive Python notebook that's reproducible, git-friendly, and deployable as scripts or apps.&lt;/em&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://docs.marimo.io" target="_blank"&gt;&lt;strong&gt;Docs&lt;/strong&gt;&lt;/a&gt; Â· &lt;a href="https://marimo.io/discord?ref=readme" target="_blank"&gt;&lt;strong&gt;Discord&lt;/strong&gt;&lt;/a&gt; Â· &lt;a href="https://docs.marimo.io/examples/" target="_blank"&gt;&lt;strong&gt;Examples&lt;/strong&gt;&lt;/a&gt; Â· &lt;a href="https://marimo.io/gallery/" target="_blank"&gt;&lt;strong&gt;Gallery&lt;/strong&gt;&lt;/a&gt; Â· &lt;a href="https://www.youtube.com/@marimo-team/" target="_blank"&gt;&lt;strong&gt;YouTube&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;b&gt;English&lt;/b&gt; &lt;b&gt; | &lt;/b&gt; &lt;a href="https://github.com/marimo-team/marimo/raw/main/README_Traditional_Chinese.md" target="_blank"&gt;&lt;b&gt;ç¹é«”ä¸­æ–‡&lt;/b&gt;&lt;/a&gt; &lt;b&gt; | &lt;/b&gt; &lt;a href="https://github.com/marimo-team/marimo/raw/main/README_Chinese.md" target="_blank"&gt;&lt;b&gt;ç®€ä½“ä¸­æ–‡&lt;/b&gt;&lt;/a&gt; &lt;b&gt; | &lt;/b&gt; &lt;a href="https://github.com/marimo-team/marimo/raw/main/README_Japanese.md" target="_blank"&gt;&lt;b&gt;æ—¥æœ¬èª&lt;/b&gt;&lt;/a&gt; &lt;b&gt; | &lt;/b&gt; &lt;a href="https://github.com/marimo-team/marimo/raw/main/README_Spanish.md" target="_blank"&gt;&lt;b&gt;EspaÃ±ol&lt;/b&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://pypi.org/project/marimo/"&gt;&lt;img src="https://img.shields.io/pypi/v/marimo?color=%2334D058&amp;amp;label=pypi" /&gt;&lt;/a&gt; &lt;a href="https://anaconda.org/conda-forge/marimo"&gt;&lt;img src="https://img.shields.io/conda/vn/conda-forge/marimo.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://marimo.io/discord?ref=readme"&gt;&lt;img src="https://shields.io/discord/1059888774789730424" alt="discord" /&gt;&lt;/a&gt; &lt;img alt="Pepy Total Downloads" src="https://img.shields.io/pepy/dt/marimo?label=pypi%20%7C%20downloads" /&gt; &lt;img alt="Conda Downloads" src="https://img.shields.io/conda/d/conda-forge/marimo" /&gt; &lt;a href="https://github.com/marimo-team/marimo/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/pypi/l/marimo" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;marimo&lt;/strong&gt; is a reactive Python notebook: run a cell or interact with a UI element, and marimo automatically runs dependent cells (or &lt;a href="https://raw.githubusercontent.com/marimo-team/marimo/main/#expensive-notebooks"&gt;marks them as stale&lt;/a&gt;), keeping code and outputs consistent. marimo notebooks are stored as pure Python (with first-class SQL support), executable as scripts, and deployable as apps.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Highlights&lt;/strong&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸš€ &lt;strong&gt;batteries-included:&lt;/strong&gt; replaces &lt;code&gt;jupyter&lt;/code&gt;, &lt;code&gt;streamlit&lt;/code&gt;, &lt;code&gt;jupytext&lt;/code&gt;, &lt;code&gt;ipywidgets&lt;/code&gt;, &lt;code&gt;papermill&lt;/code&gt;, and more&lt;/li&gt; 
 &lt;li&gt;âš¡ï¸ &lt;strong&gt;reactive&lt;/strong&gt;: run a cell, and marimo reactively &lt;a href="https://docs.marimo.io/guides/reactivity.html"&gt;runs all dependent cells&lt;/a&gt; or &lt;a href="https://raw.githubusercontent.com/marimo-team/marimo/main/#expensive-notebooks"&gt;marks them as stale&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸ–ï¸ &lt;strong&gt;interactive:&lt;/strong&gt; &lt;a href="https://docs.marimo.io/guides/interactivity.html"&gt;bind sliders, tables, plots, and more&lt;/a&gt; to Python â€” no callbacks required&lt;/li&gt; 
 &lt;li&gt;ğŸ &lt;strong&gt;git-friendly:&lt;/strong&gt; stored as &lt;code&gt;.py&lt;/code&gt; files&lt;/li&gt; 
 &lt;li&gt;ğŸ›¢ï¸ &lt;strong&gt;designed for data&lt;/strong&gt;: query dataframes, databases, warehouses, or lakehouses &lt;a href="https://docs.marimo.io/guides/working_with_data/sql.html"&gt;with SQL&lt;/a&gt;, filter and search &lt;a href="https://docs.marimo.io/guides/working_with_data/dataframes.html"&gt;dataframes&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸ¤– &lt;strong&gt;AI-native&lt;/strong&gt;: &lt;a href="https://docs.marimo.io/guides/generate_with_ai/"&gt;generate cells with AI&lt;/a&gt; tailored for data work&lt;/li&gt; 
 &lt;li&gt;ğŸ”¬ &lt;strong&gt;reproducible:&lt;/strong&gt; &lt;a href="https://docs.marimo.io/guides/reactivity.html#no-hidden-state"&gt;no hidden state&lt;/a&gt;, deterministic execution, &lt;a href="https://docs.marimo.io/guides/package_management/"&gt;built-in package management&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸƒ &lt;strong&gt;executable:&lt;/strong&gt; &lt;a href="https://docs.marimo.io/guides/scripts.html"&gt;execute as a Python script&lt;/a&gt;, parameterized by CLI args&lt;/li&gt; 
 &lt;li&gt;ğŸ›œ &lt;strong&gt;shareable&lt;/strong&gt;: &lt;a href="https://docs.marimo.io/guides/apps.html"&gt;deploy as an interactive web app&lt;/a&gt; or &lt;a href="https://docs.marimo.io/guides/apps.html#slides-layout"&gt;slides&lt;/a&gt;, &lt;a href="https://docs.marimo.io/guides/wasm.html"&gt;run in the browser via WASM&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸ§© &lt;strong&gt;reusable:&lt;/strong&gt; &lt;a href="https://docs.marimo.io/guides/reusing_functions/"&gt;import functions and classes&lt;/a&gt; from one notebook to another&lt;/li&gt; 
 &lt;li&gt;ğŸ§ª &lt;strong&gt;testable:&lt;/strong&gt; &lt;a href="https://docs.marimo.io/guides/testing/"&gt;run pytest&lt;/a&gt; on notebooks&lt;/li&gt; 
 &lt;li&gt;âŒ¨ï¸ &lt;strong&gt;a modern editor&lt;/strong&gt;: &lt;a href="https://docs.marimo.io/guides/editor_features/ai_completion.html#github-copilot"&gt;GitHub Copilot&lt;/a&gt;, &lt;a href="https://docs.marimo.io/guides/editor_features/ai_completion.html"&gt;AI assistants&lt;/a&gt;, vim keybindings, variable explorer, and &lt;a href="https://docs.marimo.io/guides/editor_features/index.html"&gt;more&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;pip install marimo &amp;amp;&amp;amp; marimo tutorial intro
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Try marimo at &lt;a href="https://marimo.app/l/c7h6pz"&gt;our online playground&lt;/a&gt;, which runs entirely in the browser!&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Jump to the &lt;a href="https://raw.githubusercontent.com/marimo-team/marimo/main/#quickstart"&gt;quickstart&lt;/a&gt; for a primer on our CLI.&lt;/em&gt;&lt;/p&gt; 
&lt;h2&gt;A reactive programming environment&lt;/h2&gt; 
&lt;p&gt;marimo guarantees your notebook code, outputs, and program state are consistent. This &lt;a href="https://docs.marimo.io/faq.html#faq-problems"&gt;solves many problems&lt;/a&gt; associated with traditional notebooks like Jupyter.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;A reactive programming environment.&lt;/strong&gt; Run a cell and marimo &lt;em&gt;reacts&lt;/em&gt; by automatically running the cells that reference its variables, eliminating the error-prone task of manually re-running cells. Delete a cell and marimo scrubs its variables from program memory, eliminating hidden state.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/reactive.gif" width="700px" /&gt; 
&lt;p&gt;&lt;a name="expensive-notebooks"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Compatible with expensive notebooks.&lt;/strong&gt; marimo lets you &lt;a href="https://docs.marimo.io/guides/configuration/runtime_configuration.html"&gt;configure the runtime to be lazy&lt;/a&gt;, marking affected cells as stale instead of automatically running them. This gives you guarantees on program state while preventing accidental execution of expensive cells.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Synchronized UI elements.&lt;/strong&gt; Interact with &lt;a href="https://docs.marimo.io/guides/interactivity.html"&gt;UI elements&lt;/a&gt; like &lt;a href="https://docs.marimo.io/api/inputs/slider.html#slider"&gt;sliders&lt;/a&gt;, &lt;a href="https://docs.marimo.io/api/inputs/dropdown.html"&gt;dropdowns&lt;/a&gt;, &lt;a href="https://docs.marimo.io/api/inputs/dataframe.html"&gt;dataframe transformers&lt;/a&gt;, and &lt;a href="https://docs.marimo.io/api/inputs/chat.html"&gt;chat interfaces&lt;/a&gt;, and the cells that use them are automatically re-run with their latest values.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/readme-ui.gif" width="700px" /&gt; 
&lt;p&gt;&lt;strong&gt;Interactive dataframes.&lt;/strong&gt; &lt;a href="https://docs.marimo.io/guides/working_with_data/dataframes.html"&gt;Page through, search, filter, and sort&lt;/a&gt; millions of rows blazingly fast, no code required.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/docs-df.gif" width="700px" /&gt; 
&lt;p&gt;&lt;strong&gt;Generate cells with data-aware AI.&lt;/strong&gt; &lt;a href="https://docs.marimo.io/guides/editor_features/ai_completion/"&gt;Generate code with an AI assistant&lt;/a&gt; that is highly specialized for working with data, with context about your variables in memory; &lt;a href="https://docs.marimo.io/guides/generate_with_ai/text_to_notebook/"&gt;zero-shot entire notebooks&lt;/a&gt;. Customize the system prompt, bring your own API keys, or use local models.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/readme-generate-with-ai.gif" width="700px" /&gt; 
&lt;p&gt;&lt;strong&gt;Query data with SQL.&lt;/strong&gt; Build &lt;a href="https://docs.marimo.io/guides/working_with_data/sql.html"&gt;SQL&lt;/a&gt; queries that depend on Python values and execute them against dataframes, databases, lakehouses, CSVs, Google Sheets, or anything else using our built-in SQL engine, which returns the result as a Python dataframe.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/readme-sql-cell.png" width="700px" /&gt; 
&lt;p&gt;Your notebooks are still pure Python, even if they use SQL.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Dynamic markdown.&lt;/strong&gt; Use markdown parametrized by Python variables to tell dynamic stories that depend on Python data.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Built-in package management.&lt;/strong&gt; marimo has built-in support for all major package managers, letting you &lt;a href="https://docs.marimo.io/guides/editor_features/package_management.html"&gt;install packages on import&lt;/a&gt;. marimo can even &lt;a href="https://docs.marimo.io/guides/package_management/inlining_dependencies/"&gt;serialize package requirements&lt;/a&gt; in notebook files, and auto install them in isolated venv sandboxes.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Deterministic execution order.&lt;/strong&gt; Notebooks are executed in a deterministic order, based on variable references instead of cells' positions on the page. Organize your notebooks to best fit the stories you'd like to tell.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Performant runtime.&lt;/strong&gt; marimo runs only those cells that need to be run by statically analyzing your code.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Batteries-included.&lt;/strong&gt; marimo comes with GitHub Copilot, AI assistants, Ruff code formatting, HTML export, fast code completion, a &lt;a href="https://marketplace.visualstudio.com/items?itemName=marimo-team.vscode-marimo"&gt;VS Code extension&lt;/a&gt;, an interactive dataframe viewer, and &lt;a href="https://docs.marimo.io/guides/editor_features/index.html"&gt;many more&lt;/a&gt; quality-of-life features.&lt;/p&gt; 
&lt;h2&gt;Quickstart&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;The &lt;a href="https://www.youtube.com/watch?v=3N6lInzq5MI&amp;amp;list=PLNJXGo8e1XT9jP7gPbRdm1XwloZVFvLEq"&gt;marimo concepts playlist&lt;/a&gt; on our &lt;a href="https://www.youtube.com/@marimo-team"&gt;YouTube channel&lt;/a&gt; gives an overview of many features.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Installation.&lt;/strong&gt; In a terminal, run&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install marimo  # or conda install -c conda-forge marimo
marimo tutorial intro
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To install with additional dependencies that unlock SQL cells, AI completion, and more, run&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install marimo[recommended]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Create notebooks.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Create or edit notebooks with&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;marimo edit
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Run apps.&lt;/strong&gt; Run your notebook as a web app, with Python code hidden and uneditable:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;marimo run your_notebook.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;img src="https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/docs-model-comparison.gif" style="border-radius: 8px" width="450px" /&gt; 
&lt;p&gt;&lt;strong&gt;Execute as scripts.&lt;/strong&gt; Execute a notebook as a script at the command line:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python your_notebook.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Automatically convert Jupyter notebooks.&lt;/strong&gt; Automatically convert Jupyter notebooks to marimo notebooks with the CLI&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;marimo convert your_notebook.ipynb &amp;gt; your_notebook.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or use our &lt;a href="https://marimo.io/convert"&gt;web interface&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Tutorials.&lt;/strong&gt; List all tutorials:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;marimo tutorial --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Share cloud-based notebooks.&lt;/strong&gt; Use &lt;a href="https://molab.marimo.io/notebooks"&gt;molab&lt;/a&gt;, a cloud-based marimo notebook service similar to Google Colab, to create and share notebook links.&lt;/p&gt; 
&lt;h2&gt;Questions?&lt;/h2&gt; 
&lt;p&gt;See the &lt;a href="https://docs.marimo.io/faq.html"&gt;FAQ&lt;/a&gt; at our docs.&lt;/p&gt; 
&lt;h2&gt;Learn more&lt;/h2&gt; 
&lt;p&gt;marimo is easy to get started with, with lots of room for power users. For example, here's an embedding visualizer made in marimo (&lt;a href="https://marimo.io/videos/landing/full.mp4"&gt;video&lt;/a&gt;):&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/embedding.gif" width="700px" /&gt; 
&lt;p&gt;Check out our &lt;a href="https://docs.marimo.io"&gt;docs&lt;/a&gt;, &lt;a href="https://docs.marimo.io/examples/"&gt;usage examples&lt;/a&gt;, and our &lt;a href="https://marimo.io/gallery"&gt;gallery&lt;/a&gt; to learn more.&lt;/p&gt; 
&lt;table border="0"&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td&gt; &lt;a target="_blank" href="https://docs.marimo.io/getting_started/key_concepts.html"&gt; &lt;img src="https://docs.marimo.io/_static/reactive.gif" style="max-height: 150px; width: auto; display: block" /&gt; &lt;/a&gt; &lt;/td&gt; 
   &lt;td&gt; &lt;a target="_blank" href="https://docs.marimo.io/api/inputs/index.html"&gt; &lt;img src="https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/readme-ui.gif" style="max-height: 150px; width: auto; display: block" /&gt; &lt;/a&gt; &lt;/td&gt; 
   &lt;td&gt; &lt;a target="_blank" href="https://docs.marimo.io/guides/working_with_data/plotting.html"&gt; &lt;img src="https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/docs-intro.gif" style="max-height: 150px; width: auto; display: block" /&gt; &lt;/a&gt; &lt;/td&gt; 
   &lt;td&gt; &lt;a target="_blank" href="https://docs.marimo.io/api/layouts/index.html"&gt; &lt;img src="https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/outputs.gif" style="max-height: 150px; width: auto; display: block" /&gt; &lt;/a&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt; &lt;a target="_blank" href="https://docs.marimo.io/getting_started/key_concepts.html"&gt; Tutorial &lt;/a&gt; &lt;/td&gt; 
   &lt;td&gt; &lt;a target="_blank" href="https://docs.marimo.io/api/inputs/index.html"&gt; Inputs &lt;/a&gt; &lt;/td&gt; 
   &lt;td&gt; &lt;a target="_blank" href="https://docs.marimo.io/guides/working_with_data/plotting.html"&gt; Plots &lt;/a&gt; &lt;/td&gt; 
   &lt;td&gt; &lt;a target="_blank" href="https://docs.marimo.io/api/layouts/index.html"&gt; Layout &lt;/a&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt; &lt;a target="_blank" href="https://marimo.app/l/c7h6pz"&gt; &lt;img src="https://marimo.io/shield.svg?sanitize=true" /&gt; &lt;/a&gt; &lt;/td&gt; 
   &lt;td&gt; &lt;a target="_blank" href="https://marimo.app/l/0ue871"&gt; &lt;img src="https://marimo.io/shield.svg?sanitize=true" /&gt; &lt;/a&gt; &lt;/td&gt; 
   &lt;td&gt; &lt;a target="_blank" href="https://marimo.app/l/lxp1jk"&gt; &lt;img src="https://marimo.io/shield.svg?sanitize=true" /&gt; &lt;/a&gt; &lt;/td&gt; 
   &lt;td&gt; &lt;a target="_blank" href="https://marimo.app/l/14ovyr"&gt; &lt;img src="https://marimo.io/shield.svg?sanitize=true" /&gt; &lt;/a&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We appreciate all contributions! You don't need to be an expert to help out. Please see &lt;a href="https://github.com/marimo-team/marimo/raw/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for more details on how to get started.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Questions? Reach out to us &lt;a href="https://marimo.io/discord?ref=readme"&gt;on Discord&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Community&lt;/h2&gt; 
&lt;p&gt;We're building a community. Come hang out with us!&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸŒŸ &lt;a href="https://github.com/marimo-team/marimo"&gt;Star us on GitHub&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸ’¬ &lt;a href="https://marimo.io/discord?ref=readme"&gt;Chat with us on Discord&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸ“§ &lt;a href="https://marimo.io/newsletter"&gt;Subscribe to our Newsletter&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;â˜ï¸ &lt;a href="https://marimo.io/cloud"&gt;Join our Cloud Waitlist&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;âœï¸ &lt;a href="https://github.com/marimo-team/marimo/discussions"&gt;Start a GitHub Discussion&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸ¦‹ &lt;a href="https://bsky.app/profile/marimo.io"&gt;Follow us on Bluesky&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸ¦ &lt;a href="https://twitter.com/marimo_io"&gt;Follow us on Twitter&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸ¥ &lt;a href="https://www.youtube.com/@marimo-team"&gt;Subscribe on YouTube&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸ•´ï¸ &lt;a href="https://www.linkedin.com/company/marimo-io"&gt;Follow us on LinkedIn&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;A NumFOCUS affiliated project.&lt;/strong&gt; marimo is a core part of the broader Python ecosystem and is a member of the NumFOCUS community, which includes projects such as NumPy, SciPy, and Matplotlib.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/numfocus_affiliated_project.png" height="40px" /&gt; 
&lt;h2&gt;Inspiration âœ¨&lt;/h2&gt; 
&lt;p&gt;marimo is a &lt;strong&gt;reinvention&lt;/strong&gt; of the Python notebook as a reproducible, interactive, and shareable Python program, instead of an error-prone JSON scratchpad.&lt;/p&gt; 
&lt;p&gt;We believe that the tools we use shape the way we think â€” better tools, for better minds. With marimo, we hope to provide the Python community with a better programming environment to do research and communicate it; to experiment with code and share it; to learn computational science and teach it.&lt;/p&gt; 
&lt;p&gt;Our inspiration comes from many places and projects, especially &lt;a href="https://github.com/fonsp/Pluto.jl"&gt;Pluto.jl&lt;/a&gt;, &lt;a href="https://observablehq.com/tutorials"&gt;ObservableHQ&lt;/a&gt;, and &lt;a href="http://worrydream.com/"&gt;Bret Victor's essays&lt;/a&gt;. marimo is part of a greater movement toward reactive dataflow programming. From &lt;a href="https://github.com/ipyflow/ipyflow"&gt;IPyflow&lt;/a&gt;, &lt;a href="https://github.com/streamlit/streamlit"&gt;streamlit&lt;/a&gt;, &lt;a href="https://github.com/tensorflow/tensorflow"&gt;TensorFlow&lt;/a&gt;, &lt;a href="https://github.com/pytorch/pytorch/tree/main"&gt;PyTorch&lt;/a&gt;, &lt;a href="https://github.com/google/jax"&gt;JAX&lt;/a&gt;, and &lt;a href="https://github.com/facebook/react"&gt;React&lt;/a&gt;, the ideas of functional, declarative, and reactive programming are transforming a broad range of tools for the better.&lt;/p&gt; 
&lt;p align="right"&gt; &lt;img src="https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/marimo-logotype-horizontal.png" height="200px" /&gt; &lt;/p&gt;</description>
    </item>
    
    <item>
      <title>triton-inference-server/server</title>
      <link>https://github.com/triton-inference-server/server</link>
      <description>&lt;p&gt;The Triton Inference Server provides an optimized cloud and edge inferencing solution.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href="https://opensource.org/licenses/BSD-3-Clause"&gt;&lt;img src="https://img.shields.io/badge/License-BSD3-lightgrey.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING] You are currently on the &lt;code&gt;main&lt;/code&gt; branch which tracks under-development progress towards the next release. The current release is version &lt;a href="https://github.com/triton-inference-server/server/releases/latest"&gt;2.62.0&lt;/a&gt; and corresponds to the 25.10 container release on NVIDIA GPU Cloud (NGC).&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h1&gt;Triton Inference Server&lt;/h1&gt; 
&lt;p&gt;Triton Inference Server is an open source inference serving software that streamlines AI inferencing. Triton enables teams to deploy any AI model from multiple deep learning and machine learning frameworks, including TensorRT, PyTorch, ONNX, OpenVINO, Python, RAPIDS FIL, and more. Triton Inference Server supports inference across cloud, data center, edge and embedded devices on NVIDIA GPUs, x86 and ARM CPU, or AWS Inferentia. Triton Inference Server delivers optimized performance for many query types, including real time, batched, ensembles and audio/video streaming. Triton inference Server is part of &lt;a href="https://www.nvidia.com/en-us/data-center/products/ai-enterprise/"&gt;NVIDIA AI Enterprise&lt;/a&gt;, a software platform that accelerates the data science pipeline and streamlines the development and deployment of production AI.&lt;/p&gt; 
&lt;p&gt;Major features include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/triton-inference-server/backend#where-can-i-find-all-the-backends-that-are-available-for-triton"&gt;Supports multiple deep learning frameworks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/triton-inference-server/fil_backend"&gt;Supports multiple machine learning frameworks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/triton-inference-server/server/main/docs/user_guide/architecture.md#concurrent-model-execution"&gt;Concurrent model execution&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/triton-inference-server/server/main/docs/user_guide/model_configuration.md#dynamic-batcher"&gt;Dynamic batching&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/triton-inference-server/server/main/docs/user_guide/model_configuration.md#sequence-batcher"&gt;Sequence batching&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/triton-inference-server/server/main/docs/user_guide/architecture.md#implicit-state-management"&gt;implicit state management&lt;/a&gt; for stateful models&lt;/li&gt; 
 &lt;li&gt;Provides &lt;a href="https://github.com/triton-inference-server/backend"&gt;Backend API&lt;/a&gt; that allows adding custom backends and pre/post processing operations&lt;/li&gt; 
 &lt;li&gt;Supports writing custom backends in python, a.k.a. &lt;a href="https://github.com/triton-inference-server/backend/raw/main/docs/python_based_backends.md#python-based-backends"&gt;Python-based backends.&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Model pipelines using &lt;a href="https://raw.githubusercontent.com/triton-inference-server/server/main/docs/user_guide/architecture.md#ensemble-models"&gt;Ensembling&lt;/a&gt; or &lt;a href="https://github.com/triton-inference-server/python_backend#business-logic-scripting"&gt;Business Logic Scripting (BLS)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/triton-inference-server/server/main/docs/customization_guide/inference_protocols.md"&gt;HTTP/REST and GRPC inference protocols&lt;/a&gt; based on the community developed &lt;a href="https://github.com/kserve/kserve/tree/master/docs/predict-api/v2"&gt;KServe protocol&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;A &lt;a href="https://raw.githubusercontent.com/triton-inference-server/server/main/docs/customization_guide/inference_protocols.md#in-process-triton-server-api"&gt;C API&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/triton-inference-server/server/main/docs/customization_guide/inference_protocols.md#java-bindings-for-in-process-triton-server-api"&gt;Java API&lt;/a&gt; allow Triton to link directly into your application for edge and other in-process use cases&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/triton-inference-server/server/main/docs/user_guide/metrics.md"&gt;Metrics&lt;/a&gt; indicating GPU utilization, server throughput, server latency, and more&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;New to Triton Inference Server?&lt;/strong&gt; Make use of &lt;a href="https://github.com/triton-inference-server/tutorials"&gt;these tutorials&lt;/a&gt; to begin your Triton journey!&lt;/p&gt; 
&lt;p&gt;Join the &lt;a href="https://www.nvidia.com/en-us/deep-learning-ai/triton-tensorrt-newsletter/"&gt;Triton and TensorRT community&lt;/a&gt; and stay current on the latest product updates, bug fixes, content, best practices, and more. Need enterprise support? NVIDIA global support is available for Triton Inference Server with the &lt;a href="https://www.nvidia.com/en-us/data-center/products/ai-enterprise/"&gt;NVIDIA AI Enterprise software suite&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Serve a Model in 3 Easy Steps&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Step 1: Create the example model repository
git clone -b r25.10 https://github.com/triton-inference-server/server.git
cd server/docs/examples
./fetch_models.sh

# Step 2: Launch triton from the NGC Triton container
docker run --gpus=1 --rm --net=host -v ${PWD}/model_repository:/models nvcr.io/nvidia/tritonserver:25.10-py3 tritonserver --model-repository=/models --model-control-mode explicit --load-model densenet_onnx

# Step 3: Sending an Inference Request
# In a separate console, launch the image_client example from the NGC Triton SDK container
docker run -it --rm --net=host nvcr.io/nvidia/tritonserver:25.10-py3-sdk /workspace/install/bin/image_client -m densenet_onnx -c 3 -s INCEPTION /workspace/images/mug.jpg

# Inference should return the following
Image '/workspace/images/mug.jpg':
    15.346230 (504) = COFFEE MUG
    13.224326 (968) = CUP
    10.422965 (505) = COFFEEPOT
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Please read the &lt;a href="https://raw.githubusercontent.com/triton-inference-server/server/main/docs/getting_started/quickstart.md"&gt;QuickStart&lt;/a&gt; guide for additional information regarding this example. The quickstart guide also contains an example of how to launch Triton on &lt;a href="https://raw.githubusercontent.com/triton-inference-server/server/main/docs/getting_started/quickstart.md#run-on-cpu-only-system"&gt;CPU-only systems&lt;/a&gt;. New to Triton and wondering where to get started? Watch the &lt;a href="https://youtu.be/NQDtfSi5QF4"&gt;Getting Started video&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Examples and Tutorials&lt;/h2&gt; 
&lt;p&gt;Check out &lt;a href="https://www.nvidia.com/en-us/data-center/products/ai-enterprise-suite/trial/"&gt;NVIDIA LaunchPad&lt;/a&gt; for free access to a set of hands-on labs with Triton Inference Server hosted on NVIDIA infrastructure.&lt;/p&gt; 
&lt;p&gt;Specific end-to-end examples for popular models, such as ResNet, BERT, and DLRM are located in the &lt;a href="https://github.com/NVIDIA/DeepLearningExamples"&gt;NVIDIA Deep Learning Examples&lt;/a&gt; page on GitHub. The &lt;a href="https://developer.nvidia.com/nvidia-triton-inference-server"&gt;NVIDIA Developer Zone&lt;/a&gt; contains additional documentation, presentations, and examples.&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;h3&gt;Build and Deploy&lt;/h3&gt; 
&lt;p&gt;The recommended way to build and use Triton Inference Server is with Docker images.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/triton-inference-server/server/main/docs/customization_guide/build.md#building-with-docker"&gt;Install Triton Inference Server with Docker containers&lt;/a&gt; (&lt;em&gt;Recommended&lt;/em&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/triton-inference-server/server/main/docs/customization_guide/build.md#building-without-docker"&gt;Install Triton Inference Server without Docker containers&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/triton-inference-server/server/main/docs/customization_guide/compose.md"&gt;Build a custom Triton Inference Server Docker container&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/triton-inference-server/server/main/docs/customization_guide/build.md#building-on-unsupported-platforms"&gt;Build Triton Inference Server from source&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/triton-inference-server/server/main/docs/customization_guide/build.md#building-for-windows-10"&gt;Build Triton Inference Server for Windows 10&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Examples for deploying Triton Inference Server with Kubernetes and Helm on &lt;a href="https://raw.githubusercontent.com/triton-inference-server/server/main/deploy/gcp/README.md"&gt;GCP&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/triton-inference-server/server/main/deploy/aws/README.md"&gt;AWS&lt;/a&gt;, and &lt;a href="https://raw.githubusercontent.com/triton-inference-server/server/main/deploy/fleetcommand/README.md"&gt;NVIDIA FleetCommand&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/triton-inference-server/server/main/docs/customization_guide/deploy.md"&gt;Secure Deployment Considerations&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Using Triton&lt;/h3&gt; 
&lt;h4&gt;Preparing Models for Triton Inference Server&lt;/h4&gt; 
&lt;p&gt;The first step in using Triton to serve your models is to place one or more models into a &lt;a href="https://raw.githubusercontent.com/triton-inference-server/server/main/docs/user_guide/model_repository.md"&gt;model repository&lt;/a&gt;. Depending on the type of the model and on what Triton capabilities you want to enable for the model, you may need to create a &lt;a href="https://raw.githubusercontent.com/triton-inference-server/server/main/docs/user_guide/model_configuration.md"&gt;model configuration&lt;/a&gt; for the model.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/triton-inference-server/server/main/docs/user_guide/custom_operations.md"&gt;Add custom operations to Triton if needed by your model&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Enable model pipelining with &lt;a href="https://raw.githubusercontent.com/triton-inference-server/server/main/docs/user_guide/architecture.md#ensemble-models"&gt;Model Ensemble&lt;/a&gt; and &lt;a href="https://github.com/triton-inference-server/python_backend#business-logic-scripting"&gt;Business Logic Scripting (BLS)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Optimize your models setting &lt;a href="https://raw.githubusercontent.com/triton-inference-server/server/main/docs/user_guide/architecture.md#models-and-schedulers"&gt;scheduling and batching&lt;/a&gt; parameters and &lt;a href="https://raw.githubusercontent.com/triton-inference-server/server/main/docs/user_guide/model_configuration.md#instance-groups"&gt;model instances&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Use the &lt;a href="https://github.com/triton-inference-server/model_analyzer"&gt;Model Analyzer tool&lt;/a&gt; to help optimize your model configuration with profiling&lt;/li&gt; 
 &lt;li&gt;Learn how to &lt;a href="https://raw.githubusercontent.com/triton-inference-server/server/main/docs/user_guide/model_management.md"&gt;explicitly manage what models are available by loading and unloading models&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Configure and Use Triton Inference Server&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Read the &lt;a href="https://raw.githubusercontent.com/triton-inference-server/server/main/docs/getting_started/quickstart.md"&gt;Quick Start Guide&lt;/a&gt; to run Triton Inference Server on both GPU and CPU&lt;/li&gt; 
 &lt;li&gt;Triton supports multiple execution engines, called &lt;a href="https://github.com/triton-inference-server/backend#where-can-i-find-all-the-backends-that-are-available-for-triton"&gt;backends&lt;/a&gt;, including &lt;a href="https://github.com/triton-inference-server/tensorrt_backend"&gt;TensorRT&lt;/a&gt;, &lt;a href="https://github.com/triton-inference-server/pytorch_backend"&gt;PyTorch&lt;/a&gt;, &lt;a href="https://github.com/triton-inference-server/onnxruntime_backend"&gt;ONNX&lt;/a&gt;, &lt;a href="https://github.com/triton-inference-server/openvino_backend"&gt;OpenVINO&lt;/a&gt;, &lt;a href="https://github.com/triton-inference-server/python_backend"&gt;Python&lt;/a&gt;, and more&lt;/li&gt; 
 &lt;li&gt;Not all the above backends are supported on every platform supported by Triton. Look at the &lt;a href="https://github.com/triton-inference-server/backend/raw/main/docs/backend_platform_support_matrix.md"&gt;Backend-Platform Support Matrix&lt;/a&gt; to learn which backends are supported on your target platform.&lt;/li&gt; 
 &lt;li&gt;Learn how to &lt;a href="https://raw.githubusercontent.com/triton-inference-server/server/main/docs/user_guide/optimization.md"&gt;optimize performance&lt;/a&gt; using the &lt;a href="https://github.com/triton-inference-server/perf_analyzer/raw/main/README.md"&gt;Performance Analyzer&lt;/a&gt; and &lt;a href="https://github.com/triton-inference-server/model_analyzer"&gt;Model Analyzer&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Learn how to &lt;a href="https://raw.githubusercontent.com/triton-inference-server/server/main/docs/user_guide/model_management.md"&gt;manage loading and unloading models&lt;/a&gt; in Triton&lt;/li&gt; 
 &lt;li&gt;Send requests directly to Triton with the &lt;a href="https://raw.githubusercontent.com/triton-inference-server/server/main/docs/customization_guide/inference_protocols.md#httprest-and-grpc-protocols"&gt;HTTP/REST JSON-based or gRPC protocols&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Client Support and Examples&lt;/h4&gt; 
&lt;p&gt;A Triton &lt;em&gt;client&lt;/em&gt; application sends inference and other requests to Triton. The &lt;a href="https://github.com/triton-inference-server/client"&gt;Python and C++ client libraries&lt;/a&gt; provide APIs to simplify this communication.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Review client examples for &lt;a href="https://github.com/triton-inference-server/client/raw/main/src/c%2B%2B/examples"&gt;C++&lt;/a&gt;, &lt;a href="https://github.com/triton-inference-server/client/raw/main/src/python/examples"&gt;Python&lt;/a&gt;, and &lt;a href="https://github.com/triton-inference-server/client/raw/main/src/java/src/main/java/triton/client/examples"&gt;Java&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Configure &lt;a href="https://github.com/triton-inference-server/client#http-options"&gt;HTTP&lt;/a&gt; and &lt;a href="https://github.com/triton-inference-server/client#grpc-options"&gt;gRPC&lt;/a&gt; client options&lt;/li&gt; 
 &lt;li&gt;Send input data (e.g. a jpeg image) directly to Triton in the &lt;a href="https://github.com/triton-inference-server/server/raw/main/docs/protocol/extension_binary_data.md#raw-binary-request"&gt;body of an HTTP request without any additional metadata&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Extend Triton&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/triton-inference-server/server/main/docs/user_guide/architecture.md"&gt;Triton Inference Server's architecture&lt;/a&gt; is specifically designed for modularity and flexibility&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/triton-inference-server/server/main/docs/customization_guide/compose.md"&gt;Customize Triton Inference Server container&lt;/a&gt; for your use case&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/triton-inference-server/backend"&gt;Create custom backends&lt;/a&gt; in either &lt;a href="https://github.com/triton-inference-server/backend/raw/main/README.md#triton-backend-api"&gt;C/C++&lt;/a&gt; or &lt;a href="https://github.com/triton-inference-server/python_backend"&gt;Python&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Create &lt;a href="https://raw.githubusercontent.com/triton-inference-server/server/main/docs/user_guide/decoupled_models.md"&gt;decoupled backends and models&lt;/a&gt; that can send multiple responses for a request or not send any responses for a request&lt;/li&gt; 
 &lt;li&gt;Use a &lt;a href="https://raw.githubusercontent.com/triton-inference-server/server/main/docs/customization_guide/repository_agents.md"&gt;Triton repository agent&lt;/a&gt; to add functionality that operates when a model is loaded and unloaded, such as authentication, decryption, or conversion&lt;/li&gt; 
 &lt;li&gt;Deploy Triton on &lt;a href="https://raw.githubusercontent.com/triton-inference-server/server/main/docs/user_guide/jetson.md"&gt;Jetson and JetPack&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/triton-inference-server/python_backend/tree/main/inferentia"&gt;Use Triton on AWS Inferentia&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Additional Documentation&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/triton-inference-server/server/main/docs/user_guide/faq.md"&gt;FAQ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/triton-inference-server/server/main/docs/README.md#user-guide"&gt;User Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/triton-inference-server/server/main/docs/README.md#customization-guide"&gt;Customization Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.nvidia.com/deeplearning/triton-inference-server/release-notes/index.html"&gt;Release Notes&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.nvidia.com/deeplearning/dgx/support-matrix/index.html"&gt;GPU, Driver, and CUDA Support Matrix&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Contributions to Triton Inference Server are more than welcome. To contribute please review the &lt;a href="https://raw.githubusercontent.com/triton-inference-server/server/main/CONTRIBUTING.md"&gt;contribution guidelines&lt;/a&gt;. If you have a backend, client, example or similar contribution that is not modifying the core of Triton, then you should file a PR in the &lt;a href="https://github.com/triton-inference-server/contrib"&gt;contrib repo&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Reporting problems, asking questions&lt;/h2&gt; 
&lt;p&gt;We appreciate any feedback, questions or bug reporting regarding this project. When posting &lt;a href="https://github.com/triton-inference-server/server/issues"&gt;issues in GitHub&lt;/a&gt;, follow the process outlined in the &lt;a href="https://stackoverflow.com/help/mcve"&gt;Stack Overflow document&lt;/a&gt;. Ensure posted examples are:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;minimal â€“ use as little code as possible that still produces the same problem&lt;/li&gt; 
 &lt;li&gt;complete â€“ provide all parts needed to reproduce the problem. Check if you can strip external dependencies and still show the problem. The less time we spend on reproducing problems the more time we have to fix it&lt;/li&gt; 
 &lt;li&gt;verifiable â€“ test the code you're about to provide to make sure it reproduces the problem. Remove all other problems that are not related to your request/question.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For issues, please use the provided bug report and feature request templates.&lt;/p&gt; 
&lt;p&gt;For questions, we recommend posting in our community &lt;a href="https://github.com/triton-inference-server/server/discussions"&gt;GitHub Discussions.&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;For more information&lt;/h2&gt; 
&lt;p&gt;Please refer to the &lt;a href="https://developer.nvidia.com/nvidia-triton-inference-server"&gt;NVIDIA Developer Triton page&lt;/a&gt; for more information.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>rossant/awesome-math</title>
      <link>https://github.com/rossant/awesome-math</link>
      <description>&lt;p&gt;A curated list of awesome mathematics resources&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Awesome Math &lt;a href="https://github.com/sindresorhus/awesome"&gt;&lt;img src="https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg?sanitize=true" alt="Awesome" /&gt;&lt;/a&gt;&lt;/h1&gt; 
&lt;p&gt;A curated list of awesome mathematics resources.&lt;/p&gt; 
&lt;p&gt;All resources are freely available except those with a ğŸ’² icon.&lt;/p&gt; 
&lt;h1&gt;Contents&lt;/h1&gt; 
&lt;!-- START_TOC --&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#contents"&gt;Contents&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#general-resources"&gt;General Resources&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#learning-platforms"&gt;Learning Platforms&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#learn-to-learn"&gt;Learn to Learn&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#youtube-series"&gt;Youtube Series&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#tools"&gt;Tools&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#questions-and-answers"&gt;Questions and Answers&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#encyclopedia"&gt;Encyclopedia&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#books"&gt;Books&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#magazines"&gt;Magazines&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#blogs"&gt;Blogs&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#meetings-and-conferences"&gt;Meetings and Conferences&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#misc"&gt;Misc&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#branches-of-mathematics"&gt;Branches of Mathematics&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#foundations-of-mathematics"&gt;Foundations of Mathematics&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#transition-to-pure-rigour-math"&gt;Transition To Pure Rigour Math&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#set-theory"&gt;Set Theory&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#logic"&gt;Logic&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#category-theory"&gt;Category Theory&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#type-theory"&gt;Type Theory&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#homotopy-type-theory"&gt;Homotopy Type Theory&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#surreal-numbers"&gt;Surreal Numbers&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#number-theory"&gt;Number Theory&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#algebraic-number-theory"&gt;Algebraic Number Theory&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#analytic-number-theory"&gt;Analytic Number Theory&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#algebra"&gt;Algebra&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#abstract-algebra"&gt;Abstract Algebra&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#group-theory"&gt;Group Theory&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#linear-algebra"&gt;Linear Algebra&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#ring-theory"&gt;Ring Theory&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#galois-theory"&gt;Galois Theory&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#lie-algebras"&gt;Lie Algebras&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#combinatorics"&gt;Combinatorics&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#graph-theory"&gt;Graph Theory&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#geometry-and-topology"&gt;Geometry and Topology&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#differential-geometry"&gt;Differential Geometry&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#algebraic-geometry"&gt;Algebraic Geometry&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#algebraic-statistics"&gt;Algebraic Statistics&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#topology"&gt;Topology&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#algebraic-topology"&gt;Algebraic Topology&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#analysis"&gt;Analysis&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#real-analysis"&gt;Real Analysis&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#harmonic-analysis"&gt;Harmonic Analysis&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#complex-analysis"&gt;Complex Analysis&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#functional-analysis"&gt;Functional Analysis&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#measure-theory"&gt;Measure Theory&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#ordinary-differential-equations"&gt;Ordinary Differential Equations&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#partial-differential-equations"&gt;Partial Differential Equations&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#chaos-theory"&gt;Chaos Theory&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#probability-and-statistics"&gt;Probability and Statistics&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#probability-theory"&gt;Probability Theory&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#statistics"&gt;Statistics&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#statistical-learning"&gt;Statistical Learning&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#stochastic-processes"&gt;Stochastic processes&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#numerical-analysis"&gt;Numerical Analysis&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#signal-processing"&gt;Signal processing&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#mathematics-for-computer-science"&gt;Mathematics for Computer Science&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#mathematical-biology"&gt;Mathematical Biology&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#mathematical-physics"&gt;Mathematical Physics&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#students-lecture-notes"&gt;Students Lecture Notes&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#related-awesome-lists"&gt;Related Awesome Lists&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#license"&gt;License&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- END_TOC --&gt; 
&lt;h1&gt;General Resources&lt;/h1&gt; 
&lt;h2&gt;Learning Platforms&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.khanacademy.org/math"&gt;Khan Academy&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.coursera.org/courses?query=mathematics&amp;amp;languages=en"&gt;Coursera&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://ocw.mit.edu/courses/mathematics/"&gt;MIT OpenCourseWare&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.edx.org/course/subject/math"&gt;edX&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://brilliant.org/courses/#math-foundational"&gt;Brilliant&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://misterwootube.com/"&gt;WooTube&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://mathigon.org/"&gt;Mathigon&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://calculus.org/"&gt;Calculus.org&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ximera.osu.edu/"&gt;Ximera&lt;/a&gt; : free interactive mathematics textbooks (Ohio State University)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.almostfun.org/lessons/"&gt;Almost Fun&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/c/OxfordMathematics"&gt;Oxford Mathematics&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://mathacademy.com/"&gt;Math Academy&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Learn to Learn&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/nelson-brochado/understanding-math"&gt;Understanding Mathematics&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Youtube Series&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/@BrandonFoltz"&gt;Brandon Foltz&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/channel/UCtYLUTtgS3k1Fg4y5tAhLbw"&gt;StatQuest&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/@3blue1brown"&gt;3Blue1Brown&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/@iit"&gt;NPTEL&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/@patrickjmt"&gt;PatrickJMT&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/@ProfessorLeonard"&gt;Professor Leonard&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://www.youtube.com/playlist?list=PLDesaqWTN6ESsmwELdrzhcGiRhk5DjwLP"&gt;Precalculus - College Algebra/Trigonometry&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.youtube.com/playlist?list=PLF797E961509B4EB5"&gt;Calculus 1&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.youtube.com/playlist?list=PLDesaqWTN6EQ2J4vgsN1HyBeRADEh4Cw-"&gt;Calculus 2&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.youtube.com/playlist?list=PLDesaqWTN6ESk16YRmzuJ8f6-rnuy0Ry7"&gt;Calculus 3&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.youtube.com/playlist?list=PLDesaqWTN6ESPaHy2QUKVaXNZuQNxkYQ_"&gt;Differential Equations&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.youtube.com/playlist?list=PLDesaqWTN6ETc1ZwHWijCBcZ2gOvS2tTN"&gt;To The Point Math&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/@crashcourse"&gt;Crash Course&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/@harvard"&gt;Harvard&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/@mitocw"&gt;MIT OpenCourseWare&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/@Mathologer"&gt;Mathologer&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/@TheMathDistrict"&gt;The Math District&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/@mathematicalmonk"&gt;Mathematical Monk&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/@TheMathSorcerer"&gt;The Math Sorcerer&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Tools&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.symbolab.com/"&gt;Symbolab&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.desmos.com/calculator"&gt;Desmos&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://www.mathwords.com/"&gt;Math Words&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://www.wolframalpha.com/"&gt;Wolfram Alpha&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://maxima.sourceforge.io/"&gt;Maxima&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.sympy.org/"&gt;Sympy&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://www.sagemath.org/"&gt;Sagemath&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Nonanti/MathFlow"&gt;MathFlow&lt;/a&gt; - C# math expression library with symbolic computation (differentiation, simplification, equation solving)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://unitconverters.net"&gt;Unit Converter&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.geogebra.org/?lang=en"&gt;GeoGebra&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://www2.macaulay2.com/Macaulay2/"&gt;Macaulay2&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.singular.uni-kl.de/"&gt;Singular&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.gnu.org/software/octave/"&gt;GNU Octave&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://magma.maths.usyd.edu.au/magma/"&gt;Magma&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.maplesoft.com/products/Maple/"&gt;Maple&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.mathworks.com/products/matlab.html"&gt;Matlab&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.wolfram.com/mathematica/"&gt;Wolfram Mathematica&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://freemathapp.org"&gt;Free Math&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://chrome.google.com/webstore/detail/xhub/anidddebgkllnnnnjfkmjcaallemhjee"&gt;xhub&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.copypastemathjax.com/"&gt;CopyPasteMathjax&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.financecharts.com/pages/5724-retirement-calculators-and-stock-market-tips"&gt;Finance calculators&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://mathcheap.xyz"&gt;Mathcheap&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://midpointcalculator.co"&gt;Midpoint Calculator&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://quartilecalculator.net"&gt;Quartiles Calculator&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://corca.io/"&gt;Corca Editor&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Questions and Answers&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="http://math.stackexchange.com/"&gt;Mathematics Stack Exchange&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://mathoverflow.net/"&gt;MathOverflow&lt;/a&gt; - for professional mathematicians&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Encyclopedia&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.encyclopediaofmath.org"&gt;Encyclopedia of Mathematics&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://planetmath.org/"&gt;Planetmath&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://proofwiki.org/wiki/Main_Page"&gt;ProofWiki&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://mathworld.wolfram.com/"&gt;Wolfram Mathworld&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://oeis.org"&gt;The On-Line Encyclopedia of Integer Sequences&lt;/a&gt; - Great compendium of many different integer sequences. Founded 1964 by N. J. A. Sloane.&lt;/li&gt; 
 &lt;li&gt;ğŸ’² &lt;a href="https://press.princeton.edu/books/hardcover/9780691118802/the-princeton-companion-to-mathematics"&gt;The Princeton Companion to Mathematics&lt;/a&gt; - Timothy Gowers (Professor, Fields medallist), June Barrow-Green (Professor), and Imre Leader (Professor).&lt;/li&gt; 
 &lt;li&gt;ğŸ’² &lt;a href="https://link.springer.com/book/10.1007/978-3-662-52844-0"&gt;Encyclopedia of Distances (4th Edition)&lt;/a&gt; - Michel Marie Deza, Elena Deza.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Books&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://archive.org/details/TarasovCalculus"&gt;Calculus: Basic Concepts for High Schools&lt;/a&gt; - L.V. Tarasov&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://www.cis.upenn.edu/~jean/math-basics.pdf"&gt;Basics of Algebra, Topology, and Differential Calculus&lt;/a&gt; - Jean Gallier (University of Pennsylvania)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://people.math.gatech.edu/%7Ecain/notes/calculus.html"&gt;Multivariable Calculus&lt;/a&gt; - G. Cain, J. Herod (Georgia Tech)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://en.wikibooks.org/wiki/Wikibooks:Mathematics_bookshelf"&gt;Wikibooks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://people.math.gatech.edu/~cain/textbooks/onlinebooks.html"&gt;Online Mathematics Textbooks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://www.wallace.ccfaculty.org/book/Beginning_and_Intermediate_Algebra.pdf"&gt;Beginning and Intermediate Algebra&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/EbookFoundation/free-programming-books/raw/master/books/free-programming-books-subjects.md#mathematics"&gt;Free Mathematics Books&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://www.mecmath.net/trig/trigbook.pdf"&gt;Trigonometry&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.manning.com/books/math-for-frontend-web-dev"&gt;Math for Frontend Web Dev&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.manning.com/books/grokking-statistics"&gt;Grokking Statistics&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Magazines&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.quantamagazine.org/mathematics/"&gt;Quanta Magazine&lt;/a&gt; - Features latest research breakthroughs in an accessible style for non-experts.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.ams.org/journals/bull/all_issues.html"&gt;Bulletin of the American Mathematical Society&lt;/a&gt; - Expository articles on contemporary mathematical research, written in a way that gives insight to mathematicians who may not be experts in the particular topic.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://www.ams.org/cgi-bin/notices/amsnotices.pl?article_id=fullissue&amp;amp;article_type=gallery&amp;amp;gallery_type=fullissue"&gt;Notices of the American Mathematical Society&lt;/a&gt; - Publicizes activities of the Society and features surveys, reports, news, announcements, and opinions on industry trends, academia, and research.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://euromathsoc.org/magazine"&gt;European Mathematical Society Magazine&lt;/a&gt; - The Magazine features announcements about meetings and conferences, articles outlining current trends in scientific development, reports on member societies, and many other informational items.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ima.org.uk/publications/mathematics-today/"&gt;Mathematics Today by Institute of Mathematics and its Applications&lt;/a&gt; - News, opinions, and articles related to mathematics, so the reader stays updated.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://cms.math.ca/publications/crux/"&gt;Crux Mathematicorum by Canadian Mathematical Society&lt;/a&gt; - source of unique and challenging mathematical problems designed for the secondary and undergraduate levels. It includes an Olympiad Corner which is helpful for math competitions.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Blogs&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://betterexplained.com/"&gt;BetterExplained&lt;/a&gt; - Maintained by Kalid Azad&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://ilovemaths.com/"&gt;ILoveMaths&lt;/a&gt; - For grades 6 thru 12 in K-12 system&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.3blue1brown.com/"&gt;3blue1brown&lt;/a&gt; - Animated Maths&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.mathsisfun.com"&gt;Mathsisfun&lt;/a&gt; simple text lightweight site for students up to highschool&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://calculus123.com/wiki/Peter_Saveliev"&gt;MathematicsIsAScience&lt;/a&gt; - Peter Saveliev (Professor of mathematics at Marshall University, Huntington WV, USA)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Meetings and Conferences&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://mathsjam.com/"&gt;MathsJam&lt;/a&gt; - monthly local recreational maths/puzzle meetups and an annual gathering in Staffordshire, England&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://talkingmathsinpublic.uk/"&gt;Talking Maths in Public&lt;/a&gt; - a conference for maths communicators, running every two years, usually in the UK&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.bridgesmathart.org/"&gt;Bridges&lt;/a&gt; - an annual conference on mathematical connections in art, music, architecture, and culture. The 2025 meeting is in Eindhoven, Netherlands.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Misc&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Areas_of_mathematics"&gt;Areas of mathematics on Wikipedia&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://tutorial.math.lamar.edu/"&gt;Paul's Online Math Notes&lt;/a&gt; - Paul Dawkins (Lamar University)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://faculty.atu.edu/mfinan/nnotes.html"&gt;List of electronic textbooks&lt;/a&gt; - Marcel B. Finan (Arkansas Tech University)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://at.yorku.ca/topology/"&gt;Topology Atlas&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://djm.cc/library/Recreations_in_Mathematics_Licks_edited.pdf"&gt;Recreations in Math&lt;/a&gt; - H. E. Licks (1917)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://djm.cc/library/Magic_Squares_Cubes_Andrews_edited.pdf"&gt;Magic Squares and Cubes&lt;/a&gt; - W. S. Andrews (1917)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://web.stanford.edu/~boyd/cvxbook/"&gt;Convex Optimization&lt;/a&gt; - Stephen Boyd and Lieven Vandenberghe&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://fabricebaudoin.wordpress.com/"&gt;Fabrice Baudoin's Notes&lt;/a&gt; - Both research and lecture notes on many topics, Including Diffusions on foliated manifold, Stochastic Calculus, Global analysis in Dirichlet spaces, and more.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Branches of Mathematics&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;Content Format&lt;/strong&gt; &lt;br /&gt; ğŸ“– Books &lt;br /&gt; ğŸ¥ Videos &lt;br /&gt; ğŸ“ Lecture notes, slides, articles, papers&lt;/p&gt; 
&lt;h2&gt;Foundations of Mathematics&lt;/h2&gt; 
&lt;h3&gt;Transition To Pure Rigour Math&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.trillia.com/zakon1.html"&gt;Basic Concepts of Mathematics&lt;/a&gt; - Elias Zakon&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://richardhammack.github.io/BookOfProof/"&gt;Book of Proof&lt;/a&gt; - Richard Hammak (Virginia Commonwealth University)&lt;/li&gt; 
 &lt;li&gt;ğŸ“– &lt;a href="https://ia800501.us.archive.org/7/items/how-to-prove-it-a-structured-approach-daniel-j.-velleman/How%20to%20Prove%20It%20A%20Structured%20Approach%20%28Daniel%20J.%20Velleman%29.pdf"&gt;How to Prove It: A Structured Approach (3rd Edition)&lt;/a&gt; - Daniel J. Velleman (Professor).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Set Theory&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.cosc.brocku.ca/~duentsch/papers/methprimer1.html"&gt;Sets, Relations, Functions&lt;/a&gt; - Ivo DÃ¼ntsch, GÃ¼nther Gediga&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.toronto.edu/weiss/set_theory.pdf"&gt;An Introduction to Set Theory&lt;/a&gt; - William A. R. Weiss&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.settheory.net/"&gt;Set Theory and Foundations of Mathematics&lt;/a&gt; - Sylvain Poirier&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://plato.stanford.edu/entries/set-theory/"&gt;Set Theory on the Stanford Encyclopedia of Philosophy&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Logic&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://pdfs.semanticscholar.org/6967/f52773d9c2ccfc94658657a5761e0f00e95a.pdf"&gt;Introduction to Logic&lt;/a&gt; - Michael Genesereth, Eric Kao (Stanford University)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://www.fecundity.com/codex/forallx.pdf"&gt;An Introduction to Formal Logic&lt;/a&gt; - P.D. Magnus (University at Albany)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://euclid.trentu.ca/math/sb/pcml/pcml-16.pdf"&gt;A Problem Course in Mathematical Logic&lt;/a&gt; - Stefan Bilaniuk (Trent University)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://poincare.matf.bg.ac.rs/~zarkom/Book_Math__Cutland_Computability.pdf"&gt;Computability - An introduction to recursive function theory&lt;/a&gt; - Nigel Cutland (University of Hull)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://homepages.uc.edu/~martinj/Symbolic_Logic/341%20Syllabus,%20Textbook,%20Handouts,%20Notes/LPL%20textbook.pdf"&gt;Language, Proof, and Logic&lt;/a&gt; - Jon Barwise, John Etchemendy&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.mathematik.uni-muenchen.de/~schwicht/lectures/logic/ws03/ml.pdf"&gt;Mathematical Logic&lt;/a&gt; - Helmut Schwichtenberg&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.personal.psu.edu/t20/notes/logic.pdf"&gt;Mathematical Logic&lt;/a&gt; - Stephen G. Simpson (Pennsylvania State University)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://maude.sip.ucm.es/~miguelpt/papers/flogic.pdf"&gt;Formal Logic&lt;/a&gt; - Miguel Palomino&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://web.math.princeton.edu/~nelson/books/pa.pdf"&gt;Predictive Arithmetic&lt;/a&gt; - Edward Nelson&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://people.uleth.ca/~dave.morris/books/proofs+concepts.html"&gt;Proofs and Concepts: the fundamentals of abstract mathematics&lt;/a&gt; - Joy Morris, Dave Morris&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://www.tedsundstrom.com/mathreasoning"&gt;Mathematical Reasoning: Writing and Proof&lt;/a&gt; - Ted Sundstrom&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://leanprover.github.io/logic_and_proof/"&gt;Logic and Proof&lt;/a&gt; - Jeremy Avigad, Robert Y. Lewis, and Floris van Doorn&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://teorth.github.io/QED"&gt;QED - an interactive textbook&lt;/a&gt; - Terence Tao&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://builds.openlogicproject.org/"&gt;Open Logic Textbook&lt;/a&gt; - collaborative effort, main contributors listed &lt;a href="https://openlogicproject.org/people/"&gt;here&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Category Theory&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.mathematik.tu-darmstadt.de/~streicher/CTCL.pdf"&gt;Introduction to Category Theory and Categorical Logic&lt;/a&gt; - Thomas Streicher&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.cs.man.ac.uk/~hsimmons/zCATS.pdf"&gt;An Introduction to Category Theory&lt;/a&gt; - Harold Simmons&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.211.4754&amp;amp;rep=rep1&amp;amp;type=pdf"&gt;Category Theory&lt;/a&gt; - Steve Awodey (Carnegie Mellon University)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.mathematik.uni-muenchen.de/~pareigis/Vorlesungen/04SS/Cats1.pdf"&gt;Category Theory&lt;/a&gt; - B. Pareigis&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://web.archive.org/web/20181221233252/http://www.math.mcgill.ca/triples/Barr-Wells-ctcs.pdf"&gt;Category Theory for Computing Science&lt;/a&gt; - Michael Barr, Charles Wells&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.tac.mta.ca/tac/reprints/articles/12/tr12.pdf"&gt;Toposes, Triples and Theories&lt;/a&gt; - Michael Barr, Charles Wells&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.tac.mta.ca/tac/reprints/articles/3/tr3abs.html"&gt;Abelian Categories&lt;/a&gt; - Peter Freyd&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.tac.mta.ca/tac/reprints/articles/7/tr7abs.html"&gt;Categories and Groupoids&lt;/a&gt; - P. J. Higgins&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.tac.mta.ca/tac/reprints/articles/10/tr10abs.html"&gt;Basic Concepts of Enriched Category Theory&lt;/a&gt; - G. M. Kelley&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.tac.mta.ca/tac/reprints/articles/17/tr17abs.html"&gt;Abstract and Concrete Categories: The Joy of Cats&lt;/a&gt; - Jiri Adamek, Horst Herrlich, George Strecker&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://math.mit.edu/~dspivak/teaching/sp18/7Sketches.pdf"&gt;Seven Sketches in Compositionality: An Invitation to Applied Category Theory&lt;/a&gt; - Brendan Fong and David I. Spivak (MIT)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.jhu.edu/~eriehl/context/"&gt;Category Theory in Context&lt;/a&gt; - Emily Riehl (John Hopkins University)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Type Theory&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.paultaylor.eu/stable/prot.pdf"&gt;Proofs and Types&lt;/a&gt; - Jean-Yves Girard&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://archive-pml.github.io/martin-lof/pdfs/Bibliopolis-Book-retypeset-1984.pdf"&gt;Intuitionistic Type Theory&lt;/a&gt; - Per Martin-Lof&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://www.cs.kent.ac.uk/people/staff/sjt/TTFP/"&gt;Type Theory and Functional Programming&lt;/a&gt; - Simon Thompson&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.cse.chalmers.se/research/group/logic/book/book.pdf"&gt;Programming in Martin-Lofâ€™s Type Theory&lt;/a&gt; - Bengt Nordstrom, Kent Petersson, Jan M. Smith&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Homotopy Type Theory&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://hottheory.files.wordpress.com/2013/03/hott-online-611-ga1a258c.pdf"&gt;Homotopy Type Theory&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Surreal Numbers&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.harvard.edu/~knill/teaching/mathe320_2015_fall/blog15/surreal1.pdf"&gt;Surreal Numbers - How two ex-students turned on to pure mathematics and found total happiness&lt;/a&gt; - D. E. Knuth&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://web.mit.edu/sp.268/www/2010/surreal.pdf"&gt;Surreal Numbers and Games&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.ohio.edu/people/ehrlich/ConwayNames.pdf"&gt;Conway names, the simplicity hierarchy and the surreal number tree&lt;/a&gt; - Philip Ehrlich&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Number Theory&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://wstein.org/ent/ent.pdf"&gt;Elementary Number Theory: Primes, Congruences, and Secrets&lt;/a&gt; - William Stein&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://math.utoledo.edu/~codenth/Spring_13/3200/ENT-books/Elementary_Number_Theory-Clark.pdf"&gt;Elementary Number Theory&lt;/a&gt; - W. Edwin Clark (University of South Florida)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.maths.qmul.ac.uk/~pjc/notes/nt.pdf"&gt;A Course on Number Theory&lt;/a&gt; - Peter J. Cameron&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://shoup.net/ntb/ntb-v2.pdf"&gt;A Computational Introduction to Number Theory and Algebra&lt;/a&gt; - Victor Shoup&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://alpha.math.uga.edu/~pete/4400FULL.pdf"&gt;Number Theory: A Contemporary Introduction&lt;/a&gt; - Pete L. Clark&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.trillia.com/moser-number.html"&gt;An Introduction to the Theory of Numbers&lt;/a&gt; - Leo Moser&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://www.poritz.net/jonathan/share/yaintt/"&gt;Yet Another Introductory Number Theory Textbook&lt;/a&gt; - Jonathan A. Poritz&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Algebraic Number Theory&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://feog.github.io/ANT10.pdf"&gt;Introduction to Algebraic Number Theory&lt;/a&gt; - F. Oggier&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.jmilne.org/math/CourseNotes/ANT.pdf"&gt;Algebraic Number Theory&lt;/a&gt; - J.S. Milne&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://people.math.gatech.edu/~mbaker/pdf/ANTBook.pdf"&gt;Algebraic Number Theory Course Notes&lt;/a&gt; - Matthew Baker (Georgia Tech)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.uiuc.edu/~r-ash/ANT.html"&gt;A Course In Algebraic Number Theory&lt;/a&gt; - Robert Ash&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Analytic Number Theory&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.uiuc.edu/~hildebr/ant/main.pdf"&gt;Introduction to Analytic Number Theory&lt;/a&gt; - A.J. Hildebrand (University of Illinois)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://math.nsc.ru/~vdovin/lectures/numth_eng.pdf"&gt;Elements of Analytic Number Theory&lt;/a&gt; - P. S. Kolesnikov, E. P. Vdovin (Novosibirsk)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.mathematik.uni-muenchen.de/~forster/v/ann/annth_all.pdf"&gt;Analytic Number Theory&lt;/a&gt; - Otto Forster (LMU Munich)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www2.math.uu.se/~astrombe/analtalt08/www_notes.pdf"&gt;Analytic Number Theory - Lecture Notes based on Davenportâ€™s book&lt;/a&gt; - Andreas StrÃ¶mbergsson&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Algebra&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.uwaterloo.ca/~snburris/htdocs/ualg.html"&gt;A Course in Universal Algebra&lt;/a&gt; - S. Burris, H.P. Sankappanavar&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://faculty.math.illinois.edu/~r-ash/ComAlg.html"&gt;A Course in Commutative Algebra&lt;/a&gt; - Robert Ash&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://djm.cc/library/First_Algebra_Hawkes_Luby_Touton_edited.pdf"&gt;First Course in Algebra&lt;/a&gt; - Herbert E. Hawkes, William A. Luby, Frank C. Touton (1910)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://djm.cc/library/Second_Algebra_Hawkes_Luby_Touton_edited.pdf"&gt;Second Course in Algebra&lt;/a&gt; - Herbert E. Hawkes, William A. Luby, Frank C. Touton (1911)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://djm.cc/library/Algebra_Elementary_Text-Book_Part_I_Chrystal_edited.pdf"&gt;Algebra: An Elementary Text-Book, Part I&lt;/a&gt; - G. Chrystal (1904)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://djm.cc/library/Algebra_Elementary_Text-Book_Part_II_Chrystal_edited02.pdf"&gt;Algebra: An Elementary Text-Book, Part II&lt;/a&gt; - G. Chrystal (1900)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://jamesbrennan.org/algebra"&gt;Understanding Algebra&lt;/a&gt; - James W. Brennan&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Abstract Algebra&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://zodml.org/sites/default/files/Introduction_to_Abstract_Algebra_0.pdf"&gt;Introduction to Abstract Algebra&lt;/a&gt; - D. S. Malik, John N. Mordeson, M.K. Sen (Creighton University)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://aleph0.clarku.edu/~djoyce/ma225/algebra.pdf"&gt;Introduction to Modern Algebra&lt;/a&gt; - David Joyce (Clark University)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://feog.github.io/AA11.pdf"&gt;Algebraic Methods&lt;/a&gt; - F. Oggier&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://abstract.ups.edu/download/aata-20150812.pdf"&gt;Abstract Algebra : Theory and Applications&lt;/a&gt; - Thomas W. Judson, Robert A. Beezer (Austin State University)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.maths.usyd.edu.au/u/bobh/UoS/rfwhole.pdf"&gt;An Undergraduate Course in Abstract Algebra&lt;/a&gt; - Robert Howlett&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.miami.edu/~ec/book"&gt;Elements of Abstract and Linear Algebra&lt;/a&gt; - E.H. Connell (University of Miami)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.uiuc.edu/~r-ash/Algebra.html"&gt;Abstract Algebra: The Basic Graduate Year&lt;/a&gt; - Robert Ash&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://web.archive.org/web/20150528171650/extension.harvard.edu/open-learning-initiative/abstract-algebra"&gt;Abstract Algebra: Harvard Extension (Archived)&lt;/a&gt; - Benedict Gross&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://www.youtube.com/playlist?list=PLA58AC5CABC1321A3"&gt;Abstract Algebra: Harvard Extension Videos&lt;/a&gt; - Benedict Gross&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Group Theory&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://www2.bc.edu/mark-reeder/Groups.pdf"&gt;Notes on Group Theory&lt;/a&gt; - Mark Reeder&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.jmilne.org/math/CourseNotes/GT.pdf"&gt;Group Theory&lt;/a&gt; - J.S. Milne&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.maths.qmul.ac.uk/~pjc/notes/gt.pdf"&gt;Notes on Finite Group Theory&lt;/a&gt; - Peter J. Cameron&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.cns.gatech.edu/GroupTheory/index.html"&gt;Group Theory&lt;/a&gt; - Pedrag Civitanovic&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Linear Algebra&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.ubc.ca/~carrell/NB.pdf"&gt;Fundamentals of Linear Algebra&lt;/a&gt; - James B. Carrell&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://web.archive.org/web/20140824074655/http://mathstat.helsinki.fi/~fluch/linear_algebra_1-sp07/la1.pdf"&gt;Linear Algebra and Matrices&lt;/a&gt; - Martin Fluch&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.maths.usyd.edu.au/u/bobh/UoS/MATH2902/vswhole.pdf"&gt;Vector Space Theory&lt;/a&gt; - Robert Howlett&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://joshua.smcvt.edu/linearalgebra"&gt;Linear Algebra&lt;/a&gt; - Jim Hefferon&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://github.com/juanklopper/MIT_OCW_Linear_Algebra_18_06"&gt;MIT OpenCourseWare Lectures on Linear Algebra (18.06) as Jupyter Notebooks&lt;/a&gt; - Juan Klopper&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.numbertheory.org/book/"&gt;Elementary Linear Algebra&lt;/a&gt; - Keith Matthews&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://linear.ups.edu/"&gt;A First Courses in Linear Algebra&lt;/a&gt; - Rob Breezer&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://www.math.ucdavis.edu/~linear/"&gt;Linear Algebra&lt;/a&gt; - David Cherney, Tom Denton, Andrew Waldron&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://oaktrust.library.tamu.edu/handle/1969.1/2502"&gt;Introduction to vectors and tensors, Vol 1: linear and multilinear algebra&lt;/a&gt; - Ray M Bowen, C. C. Wang&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://oaktrust.library.tamu.edu/handle/1969.1/3609"&gt;Introduction to vectors and tensors, Vol 2: vector and tensor analysis&lt;/a&gt; - Ray M Bowen, C. C. Wang&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://web.stanford.edu/~boyd/vmls/vmls.pdf"&gt;Introduction to Applied Linear Algebra&lt;/a&gt; - Stephen Boyd (Stanford University), Lieven Vandenberghe (UCLA)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://www.math.brown.edu/~treil/papers/LADW/LADW_2017-09-04.pdf"&gt;Linear Algebra Done Wrong&lt;/a&gt; - Sergei Treil&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://immersivemath.com/ila/index.html"&gt;Immersive Linear Algebra&lt;/a&gt; - J. StrÃ¶m, K. Ã…strÃ¶m, and T. Akenine-MÃ¶ller&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://textbooks.math.gatech.edu/ila/"&gt;Interactive Linear Algebra&lt;/a&gt; - Dan Margalit and Joseph Rabinoff&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://people.math.gatech.edu/~herod/Hspace/Hspace.html"&gt;Linear Algebra, Infinite Dimensions, and Maple&lt;/a&gt; - James Herod&lt;/li&gt; 
 &lt;li&gt;ğŸ“– &lt;a href="https://linear.axler.net/"&gt;Linear Algebra Done Right&lt;/a&gt; - Sheldon Axler&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Ring Theory&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.uni-duesseldorf.de/~wisbauer/book.pdf"&gt;Foundations of Module and Ring Theory&lt;/a&gt; - Robert Wisbauer (University of DÃ¼sseldorf)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Galois Theory&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.maths.gla.ac.uk/~ajb/dvi-ps/Galois.pdf"&gt;An Introduction to Galois Theory&lt;/a&gt; - Andrew Baker (University of Glasgow)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.jmilne.org/math/CourseNotes/FT.pdf"&gt;Fields and Galois Theory&lt;/a&gt; - J.S. Milne&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://homepages.warwick.ac.uk/~masda/MA3D5/Galois.pdf"&gt;Galois theory&lt;/a&gt; - Miles Reid&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://eclass.uoa.gr/modules/document/file.php/MATH594/Stewart%20Galois%204th%20edition.pdf"&gt;Galois Theory&lt;/a&gt; - Ian Stewart&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://arxiv.org/pdf/2408.07499"&gt;Galois Theory&lt;/a&gt; â€” Tom Leinster (University of Edinburgh)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Lie Algebras&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.harvard.edu/~shlomo/docs/lie_algebras.pdf"&gt;Lie Algebras&lt;/a&gt; - Shlomo Sternberg&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Combinatorics&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://web.math.utk.edu/~wagner/papers/comb.pdf"&gt;Basic Combinatorics&lt;/a&gt; - Carl G. Wagner (University of Tennessee)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://people.math.gatech.edu/~trotter/book.pdf"&gt;Applied Combinatorics&lt;/a&gt; - Mitchel T. Keller, William T. Trotter&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.maths.qmul.ac.uk/~pjc/notes/comb.pdf"&gt;Notes on Combinatorics&lt;/a&gt; - Peter J. Cameron&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://algo.inria.fr/flajolet/Publications/book.pdf"&gt;Analytic Combinatorics&lt;/a&gt; - Philippe Flajolet, Robert Sedgewick&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.upenn.edu/~wilf/DownldGF.html"&gt;generatingfunctionology&lt;/a&gt; - Herbert Wilf&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Graph Theory&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.personal.psu.edu/cxg286/Math485.pdf"&gt;Graph Theory: Lecture Notes&lt;/a&gt; - Christopher Griffin&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.cs.unibo.it/babaoglu/courses/cas00-01/tutorials/GraphTheory.pdf"&gt;Graph Theory&lt;/a&gt; - Reinhard Diestel&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://learngraphtheory.org/"&gt;Graph Theory : Interactive Algorithm Visualizer | Graph Theory Learning Platform&lt;/a&gt; - Hadjoudj Mohammed Islam&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Geometry and Topology&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://polly.phys.msu.ru/~belyaev/geometry.pdf"&gt;Fundamentals of Geometry&lt;/a&gt; - Oleg A. Belyaev&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://www.math.upenn.edu/~wilf/AeqB.html"&gt;A=B&lt;/a&gt; - M. Petkovsek, H. Wilf, D. Zeilberger&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://aleph0.clarku.edu/~djoyce/java/elements/toc.html"&gt;Elements&lt;/a&gt; - Euclid&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://starrhorse.com/euclid/"&gt;Euclid's Elements Redux&lt;/a&gt; - Daniel Callahan&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.ubc.ca/~cass/graphics/manual/"&gt;Mathematical Illustrations&lt;/a&gt; - Bill Casselman&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://www.c82.net/euclid/"&gt;Byrne's Euclid&lt;/a&gt; - Oliver Byrne&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://djm.cc/library/Plane_Geometry_Wentworth_Smith_edited.pdf"&gt;Plane Geometry&lt;/a&gt; - George Wentworth and David Eugene Smith (1913)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://djm.cc/library/Plane_Spherical_Trigonometry_Wentworth_Smith_edited_2.pdf"&gt;Planes and Spherical Trigonometry&lt;/a&gt; - George Wentworth and David Eugene Smith (1915)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://djm.cc/library/Coordinate_Geometry_Fine_Thompson_edited03.pdf"&gt;Coordinate Geometry&lt;/a&gt; - Henry Buchard Fine and Henry Dallas Thompson (1911)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://djm.cc/library/Analytic_Geometry_Siceloff_Wentworth_Smith_edited.pdf"&gt;Analytic Geometry&lt;/a&gt; - Lewis Parker Siceloff, George Wentworth, David Eugene Smith (1922)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Differential Geometry&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://people.math.ethz.ch/~salamon/PREPRINTS/diffgeo.pdf"&gt;Introduction to Differential Geometry&lt;/a&gt; - Joel W. Robbin, Dietmar A. Salamon&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://www.cis.upenn.edu/~jean/gbooks/manif.html"&gt;Notes on Differential Geometry and Lie Groups&lt;/a&gt; - Jean Gallier (University of Pennsylvania)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.mat.univie.ac.at/~michor/dgbook.pdf"&gt;Topics in Differential Geometry&lt;/a&gt; - Peter W. Michor&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://mysite.science.uottawa.ca/rossmann/Differential%20Geometry%20book_files/Diffgeo.pdf"&gt;Lectures on Differential Geometry&lt;/a&gt; - Wulf Rossmann&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.matematik.lu.se/matematiklu/personal/sigma/Riemann.pdf"&gt;An Introduction to Riemannian Geometry&lt;/a&gt; - Sigmundur Gudmundsson (Lund University)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://msri.org/publications/books/gt3m/"&gt;The Geometry and Topology of Three-Manifolds&lt;/a&gt; - W. Thurston&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.harvard.edu/~shlomo/docs/semi_riemannian_geometry.pdf"&gt;Semi-Riemann Geometry and General Relativity&lt;/a&gt; - Shlomo Sternberg&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.cs.cmu.edu/~kmcrane/Projects/DDG/paper.pdf"&gt;Discrete Differential Geometry&lt;/a&gt; - Keenan Crane&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Algebraic Geometry&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://ksda.ccny.cuny.edu/PostedPapers/rickksda1107.pdf"&gt;A Brief Introduction to Algebraic Geometry&lt;/a&gt; - R.C. Churchill&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.lsa.umich.edu/~idolga/631.pdf"&gt;Introduction to Algebraic Geometry&lt;/a&gt; - Igor V. Dolgachev&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://math.stanford.edu/~vakil/216blog/FOAGjun1113public.pdf"&gt;Foundations of Algebraic Geometry&lt;/a&gt; - Ravi Vakil&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.cis.upenn.edu/~jean/algeoms.pdf"&gt;Algebraic Geometry&lt;/a&gt; - Jean Gallier, Stephen S. Shatz (University of Pennsylvania)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.jmilne.org/math/CourseNotes/AG.pdf"&gt;Algebraic Geometry&lt;/a&gt; - J.S. Milne&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.mathematik.uni-kl.de/~gathmann/class/alggeom-2002/main.pdf"&gt;Algebraic Geometry&lt;/a&gt; - Andreas Gathmann (University of Kaiserslautern)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://stacks.math.columbia.edu/"&gt;The Stacks Project&lt;/a&gt; - Maintained by Aise Johan de Jong (Columbia)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Algebraic Statistics&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://math.berkeley.edu/~bernd/owl.pdf"&gt;Lectures on Algebraic Statistics&lt;/a&gt; - Mathias Drton, Bernd Sturmfels, Seth Sullivant&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://www3.diism.unisi.it/~chiantini/did/00Book.pdf"&gt;An Introduction to Algebraic Statistics&lt;/a&gt; - Cristiano Bocci, Luca Chiantini and Anthony V. Geramita&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://tore.tuhh.de/dspace-cris-server/api/core/bitstreams/a0c378d5-ce8e-442a-8891-9e7f763b4279/content"&gt;Algebraic Statistics&lt;/a&gt; - Karl-Heinz Zimmermann&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://yaroslavvb.com/papers/pachter-algebraic.pdf"&gt;Algebraic Statistics for Computational Biology&lt;/a&gt; - Pachter, and Sturmfels.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Topology&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://www.math.upenn.edu/~ghrist/notes.html"&gt;Elementary Applied Topology&lt;/a&gt; - Robert Ghrist (UPenn)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.colostate.edu/~renzo/teaching/Topology10/Notes.pdf"&gt;Introduction to Topology&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.bme.hu/~kalex/Teaching/Spring10/Topology/TopNotes_Spring10.pdf"&gt;Introduction to Topology&lt;/a&gt; - Alex KÃ¼ronya&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.clemson.edu/~jimlb/Teaching/2009-10/Math986/Topology.pdf"&gt;Introductory Topology&lt;/a&gt; - Jim L. Brown&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://webusers.imj-prg.fr/~pierre.schapira/lectnotes/Topo.pdf"&gt;General Topology&lt;/a&gt; - Pierre Schapira (Paris VI University)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.pdmi.ras.ru/~olegviro/topoman/eng-book-nopfs.pdf"&gt;Elementary Topology Problem Textbook&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.ku.dk/~moller/e03/3gt/notes/gtnotes.pdf"&gt;General Topology&lt;/a&gt; - Jesper M. MÃ¸ller&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://mathonline.wikidot.com/topology"&gt;Topology Topics&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Algebraic Topology&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.cornell.edu/~hatcher/AT/AT.pdf"&gt;Algebraic Topology&lt;/a&gt; - Allen Hatcher&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.uchicago.edu/~may/CONCISE/ConciseRevised.pdf"&gt;A Concise Course in Algebraic Topology&lt;/a&gt; - J. P. May&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.muni.cz/~cadek/at/at.pdf"&gt;Introduction to Algebraic Topology&lt;/a&gt; - Martin Cadek&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://webusers.imj-prg.fr/~pierre.schapira/lectnotes/AlTo.pdf"&gt;Algebra and Topology&lt;/a&gt; - Pierre Schapira (Paris VI University)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.indiana.edu/~jfdavis/teaching/m623/book.pdf"&gt;Lecture Notes in Algebraic Topology&lt;/a&gt; - James F. Davis, Paul Kirk (Indiana University)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://www.ma.utexas.edu/ibl1/courses/resources/12_15_07_grad_alg_top_mooremethod.pdf"&gt;Algebraic Topology&lt;/a&gt; - Michael Starbird&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.nus.edu.sg/~matwujie/ma5209.pdf"&gt;Lecture Notes on Algebraic Topology&lt;/a&gt; - Jie Wu&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Analysis&lt;/h2&gt; 
&lt;h3&gt;Real Analysis&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://ocw.mit.edu/resources/res-18-001-calculus-online-textbook-spring-2005/textbook/"&gt;MIT OpenCourseWare Lectures on Calculus&lt;/a&gt; - G. Strang&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.wisc.edu/~keisler/calc.html"&gt;Elementary Calculus: An Approach Using Infinitesimals&lt;/a&gt; - Professor H. Jerome Keisler&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://www.math.ucdavis.edu/~hunter/intro_analysis_pdf/intro_analysis.pdf"&gt;An Introduction to Real Analysis&lt;/a&gt; - John K. Hunter (University of California at Davis)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://ramanujan.math.trinity.edu/wtrench/texts/TRENCH_REAL_ANALYSIS.PDF"&gt;Introduction to Real Analysis&lt;/a&gt; - William F. Trench (Trinity University, Texas)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.jirka.org/ra/realanal.pdf"&gt;Basic Analysis: Introduction to Real Analysis&lt;/a&gt; - JiÅ™Ã­ Lebl&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://prac.im.pwr.wroc.pl/~kwasnicki/pl/stuff/tbb-hyper.pdf"&gt;Elementary Real Analysis&lt;/a&gt; - Thomson, Bruckner&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://ms.mcmaster.ca/~sawyer/Publications/Real_Analysis.pdf"&gt;Lecture Notes in Real Analysis&lt;/a&gt; - Eric T. Sawyer (McMaster University)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://math.harvard.edu/~ctm/papers/home/text/class/harvard/212a/course/course.pdf"&gt;Real Analysis&lt;/a&gt; - C. McMullen&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://bass.math.uconn.edu/3rd.pdf"&gt;Real Analysis for Graduate Students&lt;/a&gt; - Richard F. Bass&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.purdue.edu/~torres/pubs/Modern-real-analysis.pdf"&gt;Modern Real Analysis&lt;/a&gt; - William P. Ziemer (Indiana University)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.trillia.com/zakon-analysisI.html"&gt;Mathematical Analysis Vol I&lt;/a&gt; - Elias Zakon&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.trillia.com/zakon-analysisII.html"&gt;Mathematical Analysis Vol II&lt;/a&gt; - Elias Zakon&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.harvard.edu/~shlomo/docs/Advanced_Calculus.pdf"&gt;Advanced Calculus&lt;/a&gt; - Lynn Loomis, Schlomo Sternberg&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://spot.colorado.edu/~baggett/analysis.html"&gt; Analysis of Functions of a Single Variable&lt;/a&gt; - Lawerence Baggett&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.synechism.org/wp/the-calculus-of-functions-of-several-variables/"&gt;The Calculus of Functions of Several Variables&lt;/a&gt; - Dan Sloughter&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://web.pdx.edu/~erdman/PTAC/problemtext_pdf.pdf"&gt;A ProblemText in Advanced Calculus&lt;/a&gt; - John M. Erdman&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://hdl.handle.net/2027/spo.5597602.0001.001"&gt;Calculus and Linear Algebra. Vol. 1&lt;/a&gt; - Wilfred Kaplan, Donald J. Lewis&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://quod.lib.umich.edu/s/spobooks/5597602.0002.001"&gt;Calculus and Linear Algebra. Vol. 2&lt;/a&gt; - Wilfred Kaplan, Donald J. Lewis&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.odu.edu/~jhh/counter10.html"&gt;Introduction to Calculus I and II&lt;/a&gt; - J.H. Heinbockel&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://faculty.gvsu.edu/boelkinm/Home/Active_Calculus.html"&gt;Active Calculus&lt;/a&gt; - Matt Boelkins&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://math.berkeley.edu/~gbergman/ug.hndts/#Rudin"&gt;Supplements to the Exercises in Chapters 1-7 of Walter Rudin's "Principles of Mathematical Analysis"&lt;/a&gt; - George M. Bergman&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://calculusmadeeasy.org/"&gt;Calculus Made Easy&lt;/a&gt; - Silvanus P. Thompson (1910)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://djm.cc/library/Elements_Differential_Integral_Calculus_Granville_edited_2.pdf"&gt;Elements of Differential and Integral Calculus&lt;/a&gt; - William Anthony Granville (1911)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://stitz-zeager.com/szprecalculus07042013.pdf"&gt;Precalculus&lt;/a&gt; - Carl Stitz, Jeff Zeager&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Harmonic Analysis&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.uiuc.edu/~laugesen/545/545Lectures.pdf"&gt;Harmonic Analysis Lecture Notes&lt;/a&gt; - Richard S. Laugesen (University of Illinois at Urbanaâ€“Champaign)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.uchicago.edu/~schlag/harmonicnotes.pdf"&gt;Harmonic Analysis&lt;/a&gt; - W. Schlag&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://see.stanford.edu/materials/lsoftaee261/book-fall-07.pdf"&gt;Lecture Notes: Fourier Transform and its Applications&lt;/a&gt; - Brad Osgood&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.reed.edu/physics/courses/Physics331.f08/pdf/Fourier.pdf"&gt;Fourier Analysis&lt;/a&gt; - Lucas Illing&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://ccrma.stanford.edu/~jos/mdft"&gt;Mathematics of the Discrete Fourier Transform (DFT) with Audio Applications&lt;/a&gt; - Julius O. Smith III (Stanford University)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Complex Analysis&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://mtaylor.web.unc.edu/wp-content/uploads/sites/16915/2018/04/complex.pdf"&gt;Introduction to Complex Analysis&lt;/a&gt; - Michael Taylor&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.uiuc.edu/~jpda/jpd-complex-geometry-book-5-refs-bip.pdf"&gt;An Introduction to Complex Analysis and Geometry&lt;/a&gt; - John P. D'Angelo (University of Illinois)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://math.sfsu.edu/beck/papers/complex.pdf"&gt;A First Course in Complex Analysis&lt;/a&gt; - Matthias Beck, Gerald Marchesi, Dennis Pixton, Lucas Sabalka&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.wustl.edu/~sk/books/guide.pdf"&gt;A Guide to Complex Variables&lt;/a&gt; - Steven G. Krantz&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.maths.manchester.ac.uk/~cwalkden/complex-analysis/complex_analysis.pdf"&gt;Complex Analysis&lt;/a&gt; - Charles Walkden&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.ku.dk/noter/filer/koman-12.pdf"&gt;Complex Analysis&lt;/a&gt; - Christian Berg&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://people.math.sc.edu/girardi/m7034/book/AshComplexVariablesWithHyperlinks.pdf"&gt;Complex Variables&lt;/a&gt; - R. B. Ash, W.P. Novinger&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.maths.lth.se/matematiklu/personal/olofsson/CompHT06.pdf"&gt;Complex Analysis&lt;/a&gt; - Christer Bennewitz&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://web.archive.org/web/20150620124453/https://www.math.washington.edu/~marshall/math_536/Notes.pdf"&gt;Complex Analysis&lt;/a&gt; - Donald E. Marshall&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://gauss.math.yale.edu/~ws442/complex.pdf"&gt;A Concise Course in Complex Analysis and Riemann Surfaces&lt;/a&gt; - Wilhelm Schlag&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://people.math.gatech.edu/%7Ecain/winter99/complex.html"&gt;Complex Analysis&lt;/a&gt; - G. Cain (Georgia Tech)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://complex-analysis.com/"&gt;Complex Analysis&lt;/a&gt; - Juan Carlos Ponce Campuzano&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Functional Analysis&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://www.math.uwaterloo.ca/~lwmarcou/notes/pmath453.pdf"&gt;An Introduction to Functional Analysis&lt;/a&gt; - Laurent W. Marcoux (University of Waterloo)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://users.math.msu.edu/users/jeffrey/920/920notes.pdf"&gt;Functional Analysis: Lecture Notes&lt;/a&gt; - Jeff Schenker (Michigan State University)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://archive.org/details/TB_Ward___Functional_analysis_lecture_notes"&gt;Functional Analysis Lecture Notes&lt;/a&gt; - T.B. Ward (University of East Anglia)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.maths.lancs.ac.uk/~belton/www/notes/fa_notes.pdf"&gt;Functional Analysis&lt;/a&gt; - Alexander C. R. Belton&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://www.mat.univie.ac.at/~gerald/ftp/book-fa/fa.pdf"&gt;Topics in Real and Functional Analysis&lt;/a&gt; - Gerald Teschl&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www2.math.ou.edu/~cremling/teaching/lecturenotes/fa-new/LN-I.pdf"&gt;Functional Analysis&lt;/a&gt; - Christian Remling&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.harvard.edu/~shlomo/docs/Real_Variables.pdf"&gt;Theory of Functions of a Real Variable&lt;/a&gt; - Shlomo Sternberg&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://spot.colorado.edu/~baggett/functional.html"&gt;Functional Analysis&lt;/a&gt; - Lawerence Baggett&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Measure Theory&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://terrytao.files.wordpress.com/2012/12/gsm-126-tao5-measure-book.pdf"&gt;An Introduction to Measure Theory&lt;/a&gt; - Terence Tao (UCLA)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.mat.uniroma2.it/~cannarsa/cam_0607.pdf"&gt;Lecture Notes on Measure Theory and Functional Analysis&lt;/a&gt; - P. Cannarsa, T. Dâ€™Aprile&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.chalmers.se/~borell/MeasureTheory.pdf"&gt;Lecture Notes in Measure Theory&lt;/a&gt; - Christer Borell&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.gold-saucer.org/math/lebesgue/lebesgue.pdf"&gt;A Crash Course on the Lebesgue Integral and Measure Theory&lt;/a&gt; - Steve Cheng&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://www.math.ucdavis.edu/~hunter/measure_theory/measure_notes.pdf"&gt;Measure Theory&lt;/a&gt; - John K. Hunter (University of California at Davis)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://people.math.ethz.ch/~salamon/PREPRINTS/measure.pdf"&gt;Measure and Integration&lt;/a&gt; - Dietmar A. Salamon (ETH ZÃ¼rich)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.ucsd.edu/~bdriver/240-00-01/Lecture_Notes/measurep.pdf"&gt;Lecture notes: Measure Theory&lt;/a&gt; - Bruce K. Driver&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Ordinary Differential Equations&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.synechism.org/wp/difference-equations-to-differential-equations/"&gt;Difference Equations To Differential Equations&lt;/a&gt; - Dan Sloughter&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://www.math.uni-bielefeld.de/~grigor/odelec2008.pdf"&gt;Ordinary Differential Equation&lt;/a&gt; - Alexander Grigorian (University of Bielefeld)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.cs.bgu.ac.il/~leonid/ode_bio_files/Ionascu_LectNotes.pdf"&gt;Ordinary Differential Equations: Lecture Notes&lt;/a&gt; - Eugen J. Ionascu&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.lmu.de/~philip/publications/lectureNotes/ODE.pdf"&gt;Ordinary Differential Equations&lt;/a&gt; - Peter Philip&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://users.math.msu.edu/users/gnagy/teaching/ode.pdf"&gt;Ordinary Differential Equations&lt;/a&gt; - Gabriel Nagy&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.mat.univie.ac.at/~gerald/ftp/book-ode/ode.pdf"&gt;Ordinary Differential Equations and Dynamical Systems&lt;/a&gt; - Gerald Teschl&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://leipper.org/manuals/zip-fill/dn-difeq-notes.pdf"&gt;Notes on Differential Equations&lt;/a&gt; - Bob Terrell&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://digitalcommons.trinity.edu/mono/8/"&gt;Elementary Differential Equations&lt;/a&gt; - William F. Trench&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://digitalcommons.trinity.edu/mono/9/"&gt;Elementary Differential Equations With Boundary Value Problems&lt;/a&gt; - William F. Trench&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.jirka.org/diffyqs/"&gt;Notes on Diffy Qs: Differential Equations for Engineers&lt;/a&gt; - JiÅ™Ã­ Lebl&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://djm.cc/library/Differential_Equations_Phillips_edited.pdf"&gt;Differential Equations&lt;/a&gt; - H. B. Phillips (1922)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Partial Differential Equations&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://www.math.ucdavis.edu/~hunter/pdes/pde_notes.pdf"&gt;Notes on Partial Differential Equations&lt;/a&gt; - John K. Hunter (University of California at Davis)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.uni-leipzig.de/~miersemann/pdebook.pdf"&gt;Partial Differential Equations: Lecture Notes&lt;/a&gt; - Erich Miersemann (Leipzig University)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.mathphysics.com/pde/"&gt;Linear Methods of Applied Mathematics&lt;/a&gt; - E. Harrell, J. Herod (Georgia Tech)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Chaos Theory&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://archive.org/details/chaosmakingnewsc0000unse"&gt;Chaos: Making a New Science&lt;/a&gt; - James Gleick&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://archive.org/details/complexityguided0000mitc?utm_source=chatgpt.com"&gt;Complexity: A Guided Tour&lt;/a&gt; - Melanie Mitchell (Oxford University)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Probability and Statistics&lt;/h2&gt; 
&lt;h3&gt;Probability Theory&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/amsbook.mac.pdf"&gt;Introduction to Probability&lt;/a&gt; - Charles M. Grinstead, J. Laurie Snell&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://vfu.bg/en/e-Learning/Math--Bertsekas_Tsitsiklis_Introduction_to_probability.pdf"&gt;Introduction to Probability&lt;/a&gt; - Dimitri P. Bertsekas, John N. Tsitsiklis (MIT)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.maths.uq.edu.au/~kroese/asitp.pdf"&gt;A Short Introduction to Probability&lt;/a&gt; - Dirk P. Kroese (University of Queensland)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://www.math.duke.edu/~rtd/PTE/PTE4_1.pdf"&gt;Probability: Theory and Examples&lt;/a&gt; - Rick Durrett&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://github.com/mavam/stat-cookbook/releases/download/0.2.3/stat-cookbook.pdf"&gt;Probability and Statistics Cookbook&lt;/a&gt; - Matthias Vallentin (UC Berkeley)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.wzchen.com/probability-cheatsheet/"&gt;The Only Probability Cheatsheet You'll Ever Need&lt;/a&gt; - William Chen&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.ellerman.org/Davids-Stuff/Maths/Rota-Baclawski-Prob-Theory-79.pdf"&gt;An Introduction to Probability and Random Processes&lt;/a&gt; - Gian-Carlo Rota, Kenneth Baclawski&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://arxiv.org/pdf/1906.01803.pdf"&gt;Foundations of Constructive Probability Theory&lt;/a&gt; - Yuen-Kwok Chan&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Statistics&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://homepages.math.uic.edu/~rgmartin/Teaching/Stat411/Notes/411notes.pdf"&gt;Lecture Notes on Statistical Theory&lt;/a&gt; - Ryan Martin (University of Illinois)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www-library.desy.de/preparch/books/vstatmp_engl.pdf"&gt;Introduction to Statistics and Data Analysis for Physicists&lt;/a&gt; - Gerhard Bohm, GÃ¼nter Zech&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.iiserpune.ac.in/~ayan/MTH201/Sahoo_textbook.pdf"&gt;Probability and Mathematical Statistics&lt;/a&gt; - Prasanna Sahoo (University of Louisville)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://math.arizona.edu/~faris/stat.pdf"&gt;Lectures on Statistics&lt;/a&gt; - William G. Faris&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://pages.pomona.edu/~ajr04747/Fall2009/Math152/Notes/Math152NotesFall09.pdf"&gt;Statistical Theory&lt;/a&gt; - Adolfo J. Rumbos&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://mason.gmu.edu/~jgentle/books/MathStat.pdf"&gt;Theory of Statistics&lt;/a&gt; - James E. Gentle (George Mason University)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://math.arizona.edu/~jwatkins/notests.pdf"&gt;Theory of Statistics&lt;/a&gt; - Joseph C. Watkins (University of Arizona)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://web.archive.org/web/20130523134625/http://www.aiaccess.net/e_gm.htm"&gt;Glossary of Data Modeling&lt;/a&gt; - AI Access&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.ats.ucla.edu/stat/papers/"&gt;Statistics Papers&lt;/a&gt; - List of statistics papers curated by the Institute for Digital Research and Education (IDRE) at UCLA on methods such as bootstrap and factor invariance.&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://itl.nist.gov/div898/handbook/index.htm"&gt;NIST Handbook of Statistical Methods&lt;/a&gt; - Resource on practical statistics directed towards scientists and engineers.&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://vassarstats.net/textbook/"&gt;Concepts and Applications of Inferential Statistics&lt;/a&gt; - Richard Lowry&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.cosc.brocku.ca/~duentsch/papers/methprimer2.html"&gt;Rough set data analysis: A road to non-invasive knowledge discovery&lt;/a&gt; - Ivo DÃ¼ntsch, GÃ¼nther Gediga&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://statsthinking21.org/"&gt;Statistical Thinking for the 21st Century&lt;/a&gt; - Russell A. Poldrack&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://jonathanweisberg.org/vip/"&gt;Odds and Ends: Introducing Probability &amp;amp; Decision with a Visual Emphasis&lt;/a&gt; - Jonathan Weisberg&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://seeing-theory.brown.edu/"&gt;Seeing Theory&lt;/a&gt; - Daniel Kunin, Jingru Guo, Tyler Dae Devlin, and Daniel Xiang&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://www.statisticsdonewrong.com/"&gt;Statistics Done Wrong&lt;/a&gt; - Alex Reinhart&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://link.springer.com/book/10.1007/978-0-387-21736-9"&gt;All of Statistics: A Concise Course in Statistical Inference&lt;/a&gt; - Larry Wasserman&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Statistical Learning&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www-bcf.usc.edu/~gareth/ISL/ISLR%20First%20Printing.pdf"&gt;An Introduction to Statistical Learning with Applications in R&lt;/a&gt; - Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://web.stanford.edu/~hastie/Papers/ESLII.pdf"&gt;The Elements of Statistical Learning&lt;/a&gt; - Trevor Hastie, Robert Tibshirani, Jerome Friedman&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://web.stanford.edu/class/cs229t/notes.pdf"&gt;Statistical Learning Theory&lt;/a&gt; - Percy Liang&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://webdocs.cs.ualberta.ca/~sutton/book/bookdraft2016sep.pdf"&gt;Reinforcement Learning: An Introduction&lt;/a&gt; - Richard S. Sutton, Andrew G. Barto&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Stochastic processes&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.tifr.res.in/~publ/ln/tifr24.pdf"&gt;Lectures on Stochastic Processes&lt;/a&gt; - K. Ito (Tata Institute of Fundamental Research, Bombay)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.harvard.edu/~knill/teaching/math144_1994/probability.pdf"&gt;Probability and Stochastic Processes with Applications&lt;/a&gt; - Oliver Knill (Harvard University)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://statweb.stanford.edu/~adembo/math-136/nnotes.pdf"&gt;Stochastic Processes&lt;/a&gt; - Amir Dembo (Stanford University)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.mi.fu-berlin.de/wiki/pub/CompMolBio/MarkovKetten15/stochastic_processes_2011.pdf"&gt;Lecture Notes on Stochastic Processes&lt;/a&gt; - Frank NoÃ©, Bettina Keller and Jan-Hendrik Prinz (Freie UniversitÃ¤t Berlin)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://www.ma.utexas.edu/users/gordanz/notes/introduction_to_stochastic_processes.pdf"&gt;Introduction to Stochastic Processes - Lecture Notes&lt;/a&gt; - Gordan Å½itkoviÄ‡ (University of Texas)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://www.math.uwaterloo.ca/~mscott/Little_Notes.pdf"&gt;Applied Stochastic Processes in science and engineering&lt;/a&gt; - Matt Scott (University of Waterloo)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.leidenuniv.nl/~spieksma/colleges/sp-master/sp-hvz1.pdf"&gt;An Introduction to Stochastic Processes in Continuous Time&lt;/a&gt; - Flora Spieksma (Leiden University)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://pages.uoregon.edu/dlevin/MARKOV/markovmixing.pdf"&gt;Markov Chains and Mixing Times&lt;/a&gt; - David A. Levin, Yuval Peres, Elizabeth L. Wilmer&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.stat.yale.edu/~pollard/Books/1984book/pollard1984.pdf"&gt;Convergence of Stochastic Processes&lt;/a&gt; - David Pollard&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Numerical Analysis&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.umd.edu/~dlevy/resources/notes.pdf"&gt;Introduction to Numerical Analysis&lt;/a&gt; - Doron Levy (University of Maryland)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.ima.umn.edu/~arnold/597.00-01/nabook.pdf"&gt;A Concise Introduction to Numerical Analysis&lt;/a&gt; - Douglas N. Arnold (University of Minnesota)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://people.cs.uchicago.edu/~ridg/newna/nalrs.pdf"&gt;Numerical Analysis&lt;/a&gt; - L. Ridgway Scott&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://uknowledge.uky.edu/cgi/viewcontent.cgi?article=1000&amp;amp;context=math_textbooks"&gt;Lectures In Basic Computational Numerical Analysis&lt;/a&gt; - J. M. McDonough (University of Kentucky)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://user.math.uni-bremen.de/schmi/SS04/YSU_Notes.pdf"&gt;Advanced Numerical Methods and Their Applications to Industrial Problems: Adaptive Finite Element Methods&lt;/a&gt; - Alfred Schmidt, Arsen Narimanyan&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://ece.uwaterloo.ca/~dwharder/nm/"&gt;Numerical Analysis for Engineers&lt;/a&gt; - Douglas Wilhelm Harder&lt;/li&gt; 
 &lt;li&gt;ğŸ“ğŸ¥ &lt;a href="https://www.cs.utexas.edu/users/flame/laff/alaff/frontmatter.html"&gt;Advanced Linear Algebra: Foundations to Frontiers&lt;/a&gt; - Robert van de Geijn, Margaret Myers (University of Texas at Austin)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Signal processing&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.ece.rutgers.edu/~orfanidi/intro2sp/orfanidis-i2sp.pdf"&gt;Introduction to Signal Processing&lt;/a&gt; - Sophocles J. Orfanidis (Rutgers University)&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.fourierandwavelets.org/FSP_v1.1_2014.pdf"&gt;Foundations of Signal Processing&lt;/a&gt; - Martin Vetterli, Jelena Kovacevic, Vivek K Goyal&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://ee.stanford.edu/~gray/sp.pdf"&gt;An Introduction to Statistical Signal Processing&lt;/a&gt; - Robert M. Gray, Lee D. Davisson&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://greenteapress.com/wp/think-dsp/"&gt;Think DSP&lt;/a&gt; - Allen B. Downey&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://www.uio.no/studier/emner/matnat/math/MAT-INF2360/v15/kompendium/applinalgpython.pdf"&gt;Linear algebra, signal processing, and wavelets. A unified approach.&lt;/a&gt; - Ã˜yvind Ryan (University of Oslo)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Mathematics for Computer Science&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://people.csail.mit.edu/meyer/mcs.pdf"&gt;Mathematics for Computer Science&lt;/a&gt; - Eric Lehman, F. Thomson Leighton, Albert R. Meyer&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.upenn.edu/%7Ewilf/AlgComp3.html"&gt;Algorithms and Complexity&lt;/a&gt; - H. Wilf&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://people.eecs.berkeley.edu/~varaiya/papers_ps.dir/NOO.pdf"&gt;Lecture Notes on Optimization&lt;/a&gt; - Pravin Varaiya&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.inference.org.uk/mackay/itila/book.html"&gt;Information Theory, Inference, and Learning Algorithms&lt;/a&gt; - David J. C. MacKay&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="https://hypertextbook.com/chaos/"&gt;The Chaos Textbook: Mathematics in the age of the computer&lt;/a&gt; - Glenn Elert&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Mathematical Biology&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.math.ust.hk/~machas/mathematical-biology.pdf"&gt;Mathematical Biology&lt;/a&gt; - Jeffrey Chasnov&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Mathematical Physics&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://oaktrust.library.tamu.edu/handle/1969.1/2501"&gt;Introduction to Continuum Mechanics&lt;/a&gt; - Ray. M. Bowen&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.physics.miami.edu/nearing/mathmethods/"&gt;Mathematical Tools for Physics&lt;/a&gt; - James Nearing&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;a href="http://www.malaspina.com/etext/heavens.htm"&gt;Mechanism of the Heavens (1831)&lt;/a&gt; - Mary Somerville&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Students Lecture Notes&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://web.evanchen.cc/coursework.html"&gt;Evan Chen&lt;/a&gt; - MIT. 2012 ~ 2018. Covers Combinatorics, Number Theory, Honors Algebra, Set Theory, Real Analysis, Graph Theory, and more.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://dec41.user.srcf.net/notes/"&gt;Dexter Chua&lt;/a&gt; - Harvard. 2013 ~ 2018. Covers Analysis, Probability, Linear Algebra, Complex Analysis, Numerical Analysis, Statistics, Optimization, Algebraic Topology, Quantum Field Theory, and more.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Related Awesome Lists&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/mostafatouny/awesome-theoretical-computer-science"&gt;Theoretical Computer Science&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;License&lt;/h1&gt; 
&lt;p&gt;&lt;a href="http://creativecommons.org/publicdomain/zero/1.0/"&gt;&lt;img src="http://i.creativecommons.org/p/zero/1.0/88x31.png" alt="CC0" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;To the extent possible under law, &lt;a href="http://cyrille.rossant.net"&gt;Cyrille Rossant&lt;/a&gt; has waived all copyright and related or neighboring rights to this work.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>