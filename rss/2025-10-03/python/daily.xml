<rss version="2.0">
  <channel>
    <title>GitHub Python Daily Trending</title>
    <description>Daily Trending of Python in GitHub</description>
    <pubDate>Thu, 02 Oct 2025 01:37:02 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>jsvine/pdfplumber</title>
      <link>https://github.com/jsvine/pdfplumber</link>
      <description>&lt;p&gt;Plumb a PDF for detailed information about each char, rectangle, line, et cetera — and easily extract text and tables.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;pdfplumber&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://pypi.python.org/pypi/pdfplumber"&gt;&lt;img src="https://img.shields.io/pypi/v/pdfplumber.svg?sanitize=true" alt="Version" /&gt;&lt;/a&gt; &lt;img src="https://github.com/jsvine/pdfplumber/workflows/Tests/badge.svg?sanitize=true" alt="Tests" /&gt; &lt;a href="https://codecov.io/gh/jsvine/pdfplumber/branch/stable"&gt;&lt;img src="https://codecov.io/gh/jsvine/pdfplumber/branch/stable/graph/badge.svg?sanitize=true" alt="Code coverage" /&gt;&lt;/a&gt; &lt;a href="https://pypi.python.org/pypi/pdfplumber"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/pdfplumber.svg?sanitize=true" alt="Support Python versions" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Plumb a PDF for detailed information about each text character, rectangle, and line. Plus: Table extraction and visual debugging.&lt;/p&gt; 
&lt;p&gt;Works best on machine-generated, rather than scanned, PDFs. Built on &lt;a href="https://github.com/goulu/pdfminer"&gt;&lt;code&gt;pdfminer.six&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Currently &lt;a href="https://raw.githubusercontent.com/jsvine/pdfplumber/stable/tests/"&gt;tested&lt;/a&gt; on &lt;a href="https://raw.githubusercontent.com/jsvine/pdfplumber/stable/.github/workflows/tests.yml"&gt;Python 3.8, 3.9, 3.10, 3.11&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Translations of this document are available in: &lt;a href="https://github.com/hbh112233abc/pdfplumber/raw/stable/README-CN.md"&gt;Chinese (by @hbh112233abc)&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;To report a bug&lt;/strong&gt; or request a feature, please &lt;a href="https://github.com/jsvine/pdfplumber/issues/new/choose"&gt;file an issue&lt;/a&gt;. &lt;strong&gt;To ask a question&lt;/strong&gt; or request assistance with a specific PDF, please &lt;a href="https://github.com/jsvine/pdfplumber/discussions"&gt;use the discussions forum&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/jsvine/pdfplumber/stable/#installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/jsvine/pdfplumber/stable/#command-line-interface"&gt;Command line interface&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/jsvine/pdfplumber/stable/#python-library"&gt;Python library&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/jsvine/pdfplumber/stable/#visual-debugging"&gt;Visual debugging&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/jsvine/pdfplumber/stable/#extracting-text"&gt;Extracting text&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/jsvine/pdfplumber/stable/#extracting-tables"&gt;Extracting tables&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/jsvine/pdfplumber/stable/#extracting-form-values"&gt;Extracting form values&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/jsvine/pdfplumber/stable/#demonstrations"&gt;Demonstrations&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/jsvine/pdfplumber/stable/#comparison-to-other-libraries"&gt;Comparison to other libraries&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/jsvine/pdfplumber/stable/#acknowledgments--contributors"&gt;Acknowledgments / Contributors&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/jsvine/pdfplumber/stable/#contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;pip install pdfplumber
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Command line interface&lt;/h2&gt; 
&lt;h3&gt;Basic example&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;curl "https://raw.githubusercontent.com/jsvine/pdfplumber/stable/examples/pdfs/background-checks.pdf" &amp;gt; background-checks.pdf
pdfplumber background-checks.pdf &amp;gt; background-checks.csv
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The output will be a CSV containing info about every character, line, and rectangle in the PDF.&lt;/p&gt; 
&lt;h3&gt;Options&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Argument&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--format [format]&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;csv&lt;/code&gt;, &lt;code&gt;json&lt;/code&gt;, or &lt;code&gt;text&lt;/code&gt;. The &lt;code&gt;csv&lt;/code&gt; and &lt;code&gt;json&lt;/code&gt; formats return information about each object. Of those two, the &lt;code&gt;json&lt;/code&gt; format returns more information; it includes PDF-level and page-level metadata, plus dictionary-nested attributes. The &lt;code&gt;text&lt;/code&gt; option returns a plain-text representation of the PDF, using &lt;code&gt;Page.extract_text(layout=True)&lt;/code&gt;.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--pages [list of pages]&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;A space-delimited, &lt;code&gt;1&lt;/code&gt;-indexed list of pages or hyphenated page ranges. E.g., &lt;code&gt;1, 11-15&lt;/code&gt;, which would return data for pages 1, 11, 12, 13, 14, and 15.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--types [list of object types to extract]&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Choices are &lt;code&gt;char&lt;/code&gt;, &lt;code&gt;rect&lt;/code&gt;, &lt;code&gt;line&lt;/code&gt;, &lt;code&gt;curve&lt;/code&gt;, &lt;code&gt;image&lt;/code&gt;, &lt;code&gt;annot&lt;/code&gt;, et cetera. Defaults to all available.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--laparams&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;A JSON-formatted string (e.g., &lt;code&gt;'{"detect_vertical": true}'&lt;/code&gt;) to pass to &lt;code&gt;pdfplumber.open(..., laparams=...)&lt;/code&gt;.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--precision [integer]&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;The number of decimal places to round floating-point numbers. Defaults to no rounding.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Python library&lt;/h2&gt; 
&lt;h3&gt;Basic example&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import pdfplumber

with pdfplumber.open("path/to/file.pdf") as pdf:
    first_page = pdf.pages[0]
    print(first_page.chars[0])
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Loading a PDF&lt;/h3&gt; 
&lt;p&gt;To start working with a PDF, call &lt;code&gt;pdfplumber.open(x)&lt;/code&gt;, where &lt;code&gt;x&lt;/code&gt; can be a:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;path to your PDF file&lt;/li&gt; 
 &lt;li&gt;file object, loaded as bytes&lt;/li&gt; 
 &lt;li&gt;file-like object, loaded as bytes&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The &lt;code&gt;open&lt;/code&gt; method returns an instance of the &lt;code&gt;pdfplumber.PDF&lt;/code&gt; class.&lt;/p&gt; 
&lt;p&gt;To load a password-protected PDF, pass the &lt;code&gt;password&lt;/code&gt; keyword argument, e.g., &lt;code&gt;pdfplumber.open("file.pdf", password = "test")&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;To set layout analysis parameters to &lt;code&gt;pdfminer.six&lt;/code&gt;'s layout engine, pass the &lt;code&gt;laparams&lt;/code&gt; keyword argument, e.g., &lt;code&gt;pdfplumber.open("file.pdf", laparams = { "line_overlap": 0.7 })&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;To &lt;a href="https://unicode.org/reports/tr15/"&gt;pre-normalize Unicode text&lt;/a&gt;, pass &lt;code&gt;unicode_norm=...&lt;/code&gt;, where &lt;code&gt;...&lt;/code&gt; is one of the &lt;a href="https://unicode.org/reports/tr15/#Normalization_Forms_Table"&gt;four Unicode normalization forms&lt;/a&gt;: &lt;code&gt;"NFC"&lt;/code&gt;, &lt;code&gt;"NFD"&lt;/code&gt;, &lt;code&gt;"NFKC"&lt;/code&gt;, or &lt;code&gt;"NFKD"&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Invalid metadata values are treated as a warning by default. If that is not intended, pass &lt;code&gt;strict_metadata=True&lt;/code&gt; to the &lt;code&gt;open&lt;/code&gt; method and &lt;code&gt;pdfplumber.open&lt;/code&gt; will raise an exception if it is unable to parse the metadata.&lt;/p&gt; 
&lt;h3&gt;The &lt;code&gt;pdfplumber.PDF&lt;/code&gt; class&lt;/h3&gt; 
&lt;p&gt;The top-level &lt;code&gt;pdfplumber.PDF&lt;/code&gt; class represents a single PDF and has two main properties:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Property&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;.metadata&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;A dictionary of metadata key/value pairs, drawn from the PDF's &lt;code&gt;Info&lt;/code&gt; trailers. Typically includes "CreationDate," "ModDate," "Producer," et cetera.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;.pages&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;A list containing one &lt;code&gt;pdfplumber.Page&lt;/code&gt; instance per page loaded.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;... and also has the following method:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Method&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;.close()&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Calling this method calls &lt;code&gt;Page.close()&lt;/code&gt; on each page, and also closes the file stream (except in cases when the stream is external, i.e., already opened and passed directly to &lt;code&gt;pdfplumber&lt;/code&gt;).&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;The &lt;code&gt;pdfplumber.Page&lt;/code&gt; class&lt;/h3&gt; 
&lt;p&gt;The &lt;code&gt;pdfplumber.Page&lt;/code&gt; class is at the core of &lt;code&gt;pdfplumber&lt;/code&gt;. Most things you'll do with &lt;code&gt;pdfplumber&lt;/code&gt; will revolve around this class. It has these main properties:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Property&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;.page_number&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;The sequential page number, starting with &lt;code&gt;1&lt;/code&gt; for the first page, &lt;code&gt;2&lt;/code&gt; for the second, and so on.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;.width&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;The page's width.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;.height&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;The page's height.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;.objects&lt;/code&gt; / &lt;code&gt;.chars&lt;/code&gt; / &lt;code&gt;.lines&lt;/code&gt; / &lt;code&gt;.rects&lt;/code&gt; / &lt;code&gt;.curves&lt;/code&gt; / &lt;code&gt;.images&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Each of these properties is a list, and each list contains one dictionary for each such object embedded on the page. For more detail, see "&lt;a href="https://raw.githubusercontent.com/jsvine/pdfplumber/stable/#objects"&gt;Objects&lt;/a&gt;" below.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;... and these main methods:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Method&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;.crop(bounding_box, relative=False, strict=True)&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Returns a version of the page cropped to the bounding box, which should be expressed as 4-tuple with the values &lt;code&gt;(x0, top, x1, bottom)&lt;/code&gt;. Cropped pages retain objects that fall at least partly within the bounding box. If an object falls only partly within the box, its dimensions are sliced to fit the bounding box. If &lt;code&gt;relative=True&lt;/code&gt;, the bounding box is calculated as an offset from the top-left of the page's bounding box, rather than an absolute positioning. (See &lt;a href="https://github.com/jsvine/pdfplumber/issues/245"&gt;Issue #245&lt;/a&gt; for a visual example and explanation.) When &lt;code&gt;strict=True&lt;/code&gt; (the default), the crop's bounding box must fall entirely within the page's bounding box.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;.within_bbox(bounding_box, relative=False, strict=True)&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Similar to &lt;code&gt;.crop&lt;/code&gt;, but only retains objects that fall &lt;em&gt;entirely within&lt;/em&gt; the bounding box.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;.outside_bbox(bounding_box, relative=False, strict=True)&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Similar to &lt;code&gt;.crop&lt;/code&gt; and &lt;code&gt;.within_bbox&lt;/code&gt;, but only retains objects that fall &lt;em&gt;entirely outside&lt;/em&gt; the bounding box.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;.filter(test_function)&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Returns a version of the page with only the &lt;code&gt;.objects&lt;/code&gt; for which &lt;code&gt;test_function(obj)&lt;/code&gt; returns &lt;code&gt;True&lt;/code&gt;.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;... and also has the following method:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Method&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;.close()&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;By default, &lt;code&gt;Page&lt;/code&gt; objects cache their layout and object information to avoid having to reprocess it. When parsing large PDFs, however, these cached properties can require a lot of memory. You can use this method to flush the cache and release the memory.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Additional methods are described in the sections below:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/jsvine/pdfplumber/stable/#visual-debugging"&gt;Visual debugging&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/jsvine/pdfplumber/stable/#extracting-text"&gt;Extracting text&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/jsvine/pdfplumber/stable/#extracting-tables"&gt;Extracting tables&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Objects&lt;/h3&gt; 
&lt;p&gt;Each instance of &lt;code&gt;pdfplumber.PDF&lt;/code&gt; and &lt;code&gt;pdfplumber.Page&lt;/code&gt; provides access to several types of PDF objects, all derived from &lt;a href="https://github.com/pdfminer/pdfminer.six/"&gt;&lt;code&gt;pdfminer.six&lt;/code&gt;&lt;/a&gt; PDF parsing. The following properties each return a Python list of the matching objects:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;.chars&lt;/code&gt;, each representing a single text character.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;.lines&lt;/code&gt;, each representing a single 1-dimensional line.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;.rects&lt;/code&gt;, each representing a single 2-dimensional rectangle.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;.curves&lt;/code&gt;, each representing any series of connected points that &lt;code&gt;pdfminer.six&lt;/code&gt; does not recognize as a line or rectangle.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;.images&lt;/code&gt;, each representing an image.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;.annots&lt;/code&gt;, each representing a single PDF annotation (cf. Section 8.4 of the &lt;a href="https://www.adobe.com/content/dam/acom/en/devnet/acrobat/pdfs/pdf_reference_1-7.pdf"&gt;official PDF specification&lt;/a&gt; for details)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;.hyperlinks&lt;/code&gt;, each representing a single PDF annotation of the subtype &lt;code&gt;Link&lt;/code&gt; and having an &lt;code&gt;URI&lt;/code&gt; action attribute&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Each object is represented as a simple Python &lt;code&gt;dict&lt;/code&gt;, with the following properties:&lt;/p&gt; 
&lt;h4&gt;&lt;code&gt;char&lt;/code&gt; properties&lt;/h4&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Property&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;page_number&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Page number on which this character was found.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;text&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;E.g., "z", or "Z" or " ".&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;fontname&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Name of the character's font face.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;size&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Font size.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;adv&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Equal to text width * the font size * scaling factor.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;upright&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Whether the character is upright.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;height&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Height of the character.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;width&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Width of the character.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;x0&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Distance of left side of character from left side of page.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;x1&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Distance of right side of character from left side of page.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;y0&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Distance of bottom of character from bottom of page.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;y1&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Distance of top of character from bottom of page.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;top&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Distance of top of character from top of page.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;bottom&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Distance of bottom of the character from top of page.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;doctop&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Distance of top of character from top of document.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;matrix&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;The "current transformation matrix" for this character. (See below for details.)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;mcid&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;The &lt;a href="https://ghostscript.com/~robin/pdf_reference17.pdf#page=850"&gt;marked content&lt;/a&gt; section ID for this character if any (otherwise &lt;code&gt;None&lt;/code&gt;). &lt;em&gt;Experimental attribute.&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;tag&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;The &lt;a href="https://ghostscript.com/~robin/pdf_reference17.pdf#page=850"&gt;marked content&lt;/a&gt; section tag for this character if any (otherwise &lt;code&gt;None&lt;/code&gt;). &lt;em&gt;Experimental attribute.&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ncs&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;TKTK&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;stroking_pattern&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;TKTK&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;non_stroking_pattern&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;TKTK&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;stroking_color&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;The color of the character's outline (i.e., stroke). See &lt;a href="https://raw.githubusercontent.com/jsvine/pdfplumber/stable/docs/colors.md"&gt;docs/colors.md&lt;/a&gt; for details.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;non_stroking_color&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;The character's interior color. See &lt;a href="https://raw.githubusercontent.com/jsvine/pdfplumber/stable/docs/colors.md"&gt;docs/colors.md&lt;/a&gt; for details.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;object_type&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;"char"&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: A character’s &lt;code&gt;matrix&lt;/code&gt; property represents the “current transformation matrix,” as described in Section 4.2.2 of the &lt;a href="https://ghostscript.com/~robin/pdf_reference17.pdf"&gt;PDF Reference&lt;/a&gt; (6th Ed.). The matrix controls the character’s scale, skew, and positional translation. Rotation is a combination of scale and skew, but in most cases can be considered equal to the x-axis skew. The &lt;code&gt;pdfplumber.ctm&lt;/code&gt; submodule defines a class, &lt;code&gt;CTM&lt;/code&gt;, that assists with these calculations. For instance:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from pdfplumber.ctm import CTM
my_char = pdf.pages[0].chars[3]
my_char_ctm = CTM(*my_char["matrix"])
my_char_rotation = my_char_ctm.skew_x
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;&lt;code&gt;line&lt;/code&gt; properties&lt;/h4&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Property&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;page_number&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Page number on which this line was found.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;height&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Height of line.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;width&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Width of line.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;x0&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Distance of left-side extremity from left side of page.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;x1&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Distance of right-side extremity from left side of page.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;y0&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Distance of bottom extremity from bottom of page.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;y1&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Distance of top extremity bottom of page.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;top&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Distance of top of line from top of page.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;bottom&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Distance of bottom of the line from top of page.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;doctop&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Distance of top of line from top of document.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;linewidth&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Thickness of line.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;stroking_color&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;The color of the line. See &lt;a href="https://raw.githubusercontent.com/jsvine/pdfplumber/stable/docs/colors.md"&gt;docs/colors.md&lt;/a&gt; for details.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;non_stroking_color&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;The non-stroking color specified for the line’s path. See &lt;a href="https://raw.githubusercontent.com/jsvine/pdfplumber/stable/docs/colors.md"&gt;docs/colors.md&lt;/a&gt; for details.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;mcid&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;The &lt;a href="https://ghostscript.com/~robin/pdf_reference17.pdf#page=850"&gt;marked content&lt;/a&gt; section ID for this line if any (otherwise &lt;code&gt;None&lt;/code&gt;). &lt;em&gt;Experimental attribute.&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;tag&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;The &lt;a href="https://ghostscript.com/~robin/pdf_reference17.pdf#page=850"&gt;marked content&lt;/a&gt; section tag for this line if any (otherwise &lt;code&gt;None&lt;/code&gt;). &lt;em&gt;Experimental attribute.&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;object_type&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;"line"&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h4&gt;&lt;code&gt;rect&lt;/code&gt; properties&lt;/h4&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Property&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;page_number&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Page number on which this rectangle was found.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;height&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Height of rectangle.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;width&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Width of rectangle.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;x0&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Distance of left side of rectangle from left side of page.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;x1&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Distance of right side of rectangle from left side of page.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;y0&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Distance of bottom of rectangle from bottom of page.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;y1&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Distance of top of rectangle from bottom of page.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;top&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Distance of top of rectangle from top of page.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;bottom&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Distance of bottom of the rectangle from top of page.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;doctop&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Distance of top of rectangle from top of document.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;linewidth&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Thickness of line.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;stroking_color&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;The color of the rectangle's outline. See &lt;a href="https://raw.githubusercontent.com/jsvine/pdfplumber/stable/docs/colors.md"&gt;docs/colors.md&lt;/a&gt; for details.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;non_stroking_color&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;The rectangle’s fill color. See &lt;a href="https://raw.githubusercontent.com/jsvine/pdfplumber/stable/docs/colors.md"&gt;docs/colors.md&lt;/a&gt; for details.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;mcid&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;The &lt;a href="https://ghostscript.com/~robin/pdf_reference17.pdf#page=850"&gt;marked content&lt;/a&gt; section ID for this rect if any (otherwise &lt;code&gt;None&lt;/code&gt;). &lt;em&gt;Experimental attribute.&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;tag&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;The &lt;a href="https://ghostscript.com/~robin/pdf_reference17.pdf#page=850"&gt;marked content&lt;/a&gt; section tag for this rect if any (otherwise &lt;code&gt;None&lt;/code&gt;). &lt;em&gt;Experimental attribute.&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;object_type&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;"rect"&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h4&gt;&lt;code&gt;curve&lt;/code&gt; properties&lt;/h4&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Property&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;page_number&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Page number on which this curve was found.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;pts&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;A list of &lt;code&gt;(x, top)&lt;/code&gt; tuples indicating the &lt;em&gt;points on the curve&lt;/em&gt;.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;path&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;A list of &lt;code&gt;(cmd, *(x, top))&lt;/code&gt; tuples &lt;em&gt;describing the full path description&lt;/em&gt;, including (for example) control points used in Bezier curves.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;height&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Height of curve's bounding box.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;width&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Width of curve's bounding box.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;x0&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Distance of curve's left-most point from left side of page.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;x1&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Distance of curve's right-most point from left side of the page.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;y0&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Distance of curve's lowest point from bottom of page.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;y1&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Distance of curve's highest point from bottom of page.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;top&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Distance of curve's highest point from top of page.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;bottom&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Distance of curve's lowest point from top of page.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;doctop&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Distance of curve's highest point from top of document.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;linewidth&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Thickness of line.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;fill&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Whether the shape defined by the curve's path is filled.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;stroking_color&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;The color of the curve's outline. See &lt;a href="https://raw.githubusercontent.com/jsvine/pdfplumber/stable/docs/colors.md"&gt;docs/colors.md&lt;/a&gt; for details.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;non_stroking_color&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;The curve’s fill color. See &lt;a href="https://raw.githubusercontent.com/jsvine/pdfplumber/stable/docs/colors.md"&gt;docs/colors.md&lt;/a&gt; for details.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;dash&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;A &lt;code&gt;([dash_array], dash_phase)&lt;/code&gt; tuple describing the curve's dash style. See &lt;a href="https://ghostscript.com/~robin/pdf_reference17.pdf#page=218"&gt;Table 4.6 of the PDF specification&lt;/a&gt; for details.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;mcid&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;The &lt;a href="https://ghostscript.com/~robin/pdf_reference17.pdf#page=850"&gt;marked content&lt;/a&gt; section ID for this curve if any (otherwise &lt;code&gt;None&lt;/code&gt;). &lt;em&gt;Experimental attribute.&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;tag&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;The &lt;a href="https://ghostscript.com/~robin/pdf_reference17.pdf#page=850"&gt;marked content&lt;/a&gt; section tag for this curve if any (otherwise &lt;code&gt;None&lt;/code&gt;). &lt;em&gt;Experimental attribute.&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;object_type&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;"curve"&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h4&gt;Derived properties&lt;/h4&gt; 
&lt;p&gt;Additionally, both &lt;code&gt;pdfplumber.PDF&lt;/code&gt; and &lt;code&gt;pdfplumber.Page&lt;/code&gt; provide access to several derived lists of objects: &lt;code&gt;.rect_edges&lt;/code&gt; (which decomposes each rectangle into its four lines), &lt;code&gt;.curve_edges&lt;/code&gt; (which does the same for &lt;code&gt;curve&lt;/code&gt; objects), and &lt;code&gt;.edges&lt;/code&gt; (which combines &lt;code&gt;.rect_edges&lt;/code&gt;, &lt;code&gt;.curve_edges&lt;/code&gt;, and &lt;code&gt;.lines&lt;/code&gt;).&lt;/p&gt; 
&lt;h4&gt;&lt;code&gt;image&lt;/code&gt; properties&lt;/h4&gt; 
&lt;p&gt;&lt;em&gt;Note: Although the positioning and characteristics of &lt;code&gt;image&lt;/code&gt; objects are available via &lt;code&gt;pdfplumber&lt;/code&gt;, this library does not provide direct support for reconstructing image content. For that, please see &lt;a href="https://github.com/jsvine/pdfplumber/discussions/496#discussioncomment-1259772"&gt;this suggestion&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Property&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;page_number&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Page number on which the image was found.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;height&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Height of the image.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;width&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Width of the image.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;x0&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Distance of left side of the image from left side of page.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;x1&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Distance of right side of the image from left side of page.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;y0&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Distance of bottom of the image from bottom of page.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;y1&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Distance of top of the image from bottom of page.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;top&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Distance of top of the image from top of page.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;bottom&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Distance of bottom of the image from top of page.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;doctop&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Distance of top of rectangle from top of document.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;srcsize&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;The image original dimensions, as a &lt;code&gt;(width, height)&lt;/code&gt; tuple.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;colorspace&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Color domain of the image (e.g., RGB).&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;bits&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;The number of bits per color component; e.g., 8 corresponds to 255 possible values for each color component (R, G, and B in an RGB color space).&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;stream&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Pixel values of the image, as a &lt;code&gt;pdfminer.pdftypes.PDFStream&lt;/code&gt; object.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;imagemask&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;A nullable boolean; if &lt;code&gt;True&lt;/code&gt;, "specifies that the image data is to be used as a stencil mask for painting in the current color."&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;name&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;"The name by which this image XObject is referenced in the XObject subdictionary of the current resource dictionary." &lt;a href="https://ghostscript.com/~robin/pdf_reference17.pdf#page=340"&gt;🔗&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;mcid&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;The &lt;a href="https://ghostscript.com/~robin/pdf_reference17.pdf#page=850"&gt;marked content&lt;/a&gt; section ID for this image if any (otherwise &lt;code&gt;None&lt;/code&gt;). &lt;em&gt;Experimental attribute.&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;tag&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;The &lt;a href="https://ghostscript.com/~robin/pdf_reference17.pdf#page=850"&gt;marked content&lt;/a&gt; section tag for this image if any (otherwise &lt;code&gt;None&lt;/code&gt;). &lt;em&gt;Experimental attribute.&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;object_type&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;"image"&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Obtaining higher-level layout objects via &lt;code&gt;pdfminer.six&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;If you pass the &lt;code&gt;pdfminer.six&lt;/code&gt;-handling &lt;code&gt;laparams&lt;/code&gt; parameter to &lt;code&gt;pdfplumber.open(...)&lt;/code&gt;, then each page's &lt;code&gt;.objects&lt;/code&gt; dictionary will also contain &lt;code&gt;pdfminer.six&lt;/code&gt;'s higher-level layout objects, such as &lt;code&gt;"textboxhorizontal"&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Visual debugging&lt;/h2&gt; 
&lt;p&gt;&lt;code&gt;pdfplumber&lt;/code&gt;'s visual debugging tools can be helpful in understanding the structure of a PDF and the objects that have been extracted from it.&lt;/p&gt; 
&lt;h3&gt;Creating a &lt;code&gt;PageImage&lt;/code&gt; with &lt;code&gt;.to_image()&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;To turn any page (including cropped pages) into an &lt;code&gt;PageImage&lt;/code&gt; object, call &lt;code&gt;my_page.to_image()&lt;/code&gt;. You can optionally pass &lt;em&gt;one&lt;/em&gt; of the following keyword arguments:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;resolution&lt;/code&gt;: The desired number pixels per inch. Default: &lt;code&gt;72&lt;/code&gt;. Type: &lt;code&gt;int&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;width&lt;/code&gt;: The desired image width in pixels. Default: unset, determined by &lt;code&gt;resolution&lt;/code&gt;. Type: &lt;code&gt;int&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;height&lt;/code&gt;: The desired image width in pixels. Default: unset, determined by &lt;code&gt;resolution&lt;/code&gt;. Type: &lt;code&gt;int&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;antialias&lt;/code&gt;: Whether to use antialiasing when creating the image. Setting to &lt;code&gt;True&lt;/code&gt; creates images with less-jagged text and graphics, but with larger file sizes. Default: &lt;code&gt;False&lt;/code&gt;. Type: &lt;code&gt;bool&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;force_mediabox&lt;/code&gt;: Use the page's &lt;code&gt;.mediabox&lt;/code&gt; dimensions, rather than the &lt;code&gt;.cropbox&lt;/code&gt; dimensions. Default: &lt;code&gt;False&lt;/code&gt;. Type: &lt;code&gt;bool&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For instance:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;im = my_pdf.pages[0].to_image(resolution=150)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;From a script or REPL, &lt;code&gt;im.show()&lt;/code&gt; will open the image in your local image viewer. But &lt;code&gt;PageImage&lt;/code&gt; objects also play nicely with Jupyter notebooks; they automatically render as cell outputs. For example:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/jsvine/pdfplumber/stable/examples/screenshots/visual-debugging-in-jupyter.png" alt="Visual debugging in Jupyter" title="Visual debugging in Jupyter" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Note&lt;/em&gt;: &lt;code&gt;.to_image(...)&lt;/code&gt; works as expected with &lt;code&gt;Page.crop(...)&lt;/code&gt;/&lt;code&gt;CroppedPage&lt;/code&gt; instances, but is unable to incorporate changes made via &lt;code&gt;Page.filter(...)&lt;/code&gt;/&lt;code&gt;FilteredPage&lt;/code&gt; instances.&lt;/p&gt; 
&lt;h3&gt;Basic &lt;code&gt;PageImage&lt;/code&gt; methods&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Method&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;im.reset()&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Clears anything you've drawn so far.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;im.copy()&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Copies the image to a new &lt;code&gt;PageImage&lt;/code&gt; object.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;im.show()&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Opens the image in your local image viewer.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;im.save(path_or_fileobject, format="PNG", quantize=True, colors=256, bits=8)&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Saves the annotated image as a PNG file. The default arguments quantize the image to a palette of 256 colors, saving the PNG with 8-bit color depth. You can disable quantization by passing &lt;code&gt;quantize=False&lt;/code&gt; or adjust the size of the color palette by passing &lt;code&gt;colors=N&lt;/code&gt;.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Drawing methods&lt;/h3&gt; 
&lt;p&gt;You can pass explicit coordinates or any &lt;code&gt;pdfplumber&lt;/code&gt; PDF object (e.g., char, line, rect) to these methods.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Single-object method&lt;/th&gt; 
   &lt;th&gt;Bulk method&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;im.draw_line(line, stroke={color}, stroke_width=1)&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;im.draw_lines(list_of_lines, **kwargs)&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Draws a line from a &lt;code&gt;line&lt;/code&gt;, &lt;code&gt;curve&lt;/code&gt;, or a 2-tuple of 2-tuples (e.g., &lt;code&gt;((x, y), (x, y))&lt;/code&gt;).&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;im.draw_vline(location, stroke={color}, stroke_width=1)&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;im.draw_vlines(list_of_locations, **kwargs)&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Draws a vertical line at the x-coordinate indicated by &lt;code&gt;location&lt;/code&gt;.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;im.draw_hline(location, stroke={color}, stroke_width=1)&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;im.draw_hlines(list_of_locations, **kwargs)&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Draws a horizontal line at the y-coordinate indicated by &lt;code&gt;location&lt;/code&gt;.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;im.draw_rect(bbox_or_obj, fill={color}, stroke={color}, stroke_width=1)&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;im.draw_rects(list_of_rects, **kwargs)&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Draws a rectangle from a &lt;code&gt;rect&lt;/code&gt;, &lt;code&gt;char&lt;/code&gt;, etc., or 4-tuple bounding box.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;im.draw_circle(center_or_obj, radius=5, fill={color}, stroke={color})&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;im.draw_circles(list_of_circles, **kwargs)&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Draws a circle at &lt;code&gt;(x, y)&lt;/code&gt; coordinate or at the center of a &lt;code&gt;char&lt;/code&gt;, &lt;code&gt;rect&lt;/code&gt;, etc.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Note: The methods above are built on Pillow's &lt;a href="http://pillow.readthedocs.io/en/latest/reference/ImageDraw.html"&gt;&lt;code&gt;ImageDraw&lt;/code&gt; methods&lt;/a&gt;, but the parameters have been tweaked for consistency with SVG's &lt;code&gt;fill&lt;/code&gt;/&lt;code&gt;stroke&lt;/code&gt;/&lt;code&gt;stroke_width&lt;/code&gt; nomenclature.&lt;/p&gt; 
&lt;h3&gt;Visually debugging the table-finder&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;im.debug_tablefinder(table_settings={})&lt;/code&gt; will return a version of the PageImage with the detected lines (in red), intersections (circles), and tables (light blue) overlaid.&lt;/p&gt; 
&lt;h2&gt;Extracting text&lt;/h2&gt; 
&lt;p&gt;&lt;code&gt;pdfplumber&lt;/code&gt; can extract text from any given page (including cropped and derived pages). It can also attempt to preserve the layout of that text, as well as to identify the coordinates of words and search queries. &lt;code&gt;Page&lt;/code&gt; objects can call the following text-extraction methods:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Method&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;.extract_text(x_tolerance=3, x_tolerance_ratio=None, y_tolerance=3, layout=False, x_density=7.25, y_density=13, line_dir_render=None, char_dir_render=None, **kwargs)&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Collates all of the page's character objects into a single string.
    &lt;ul&gt;
     &lt;li&gt;&lt;p&gt;When &lt;code&gt;layout=False&lt;/code&gt;: Adds spaces where the difference between the &lt;code&gt;x1&lt;/code&gt; of one character and the &lt;code&gt;x0&lt;/code&gt; of the next is greater than &lt;code&gt;x_tolerance&lt;/code&gt;. (If &lt;code&gt;x_tolerance_ratio&lt;/code&gt; is not &lt;code&gt;None&lt;/code&gt;, the extractor uses a dynamic &lt;code&gt;x_tolerance&lt;/code&gt; equal to &lt;code&gt;x_tolerance_ratio * previous_character["size"]&lt;/code&gt;.) Adds newline characters where the difference between the &lt;code&gt;doctop&lt;/code&gt; of one character and the &lt;code&gt;doctop&lt;/code&gt; of the next is greater than &lt;code&gt;y_tolerance&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
     &lt;li&gt;&lt;p&gt;When &lt;code&gt;layout=True&lt;/code&gt; (&lt;em&gt;experimental feature&lt;/em&gt;): Attempts to mimic the structural layout of the text on the page(s), using &lt;code&gt;x_density&lt;/code&gt; and &lt;code&gt;y_density&lt;/code&gt; to determine the minimum number of characters/newlines per "point," the PDF unit of measurement. Passing &lt;code&gt;line_dir_render="ttb"/"btt"/"ltr"/"rtl"&lt;/code&gt; and/or &lt;code&gt;char_dir_render="ttb"/"btt"/"ltr"/"rtl"&lt;/code&gt; will output the the lines/characters in a different direction than the default. All remaining &lt;code&gt;**kwargs&lt;/code&gt; are passed to &lt;code&gt;.extract_words(...)&lt;/code&gt; (see below), the first step in calculating the layout.&lt;/p&gt;&lt;/li&gt;
    &lt;/ul&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;.extract_text_simple(x_tolerance=3, y_tolerance=3)&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;A slightly faster but less flexible version of &lt;code&gt;.extract_text(...)&lt;/code&gt;, using a simpler logic.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;.extract_words(x_tolerance=3, x_tolerance_ratio=None, y_tolerance=3, keep_blank_chars=False, use_text_flow=False, line_dir="ttb", char_dir="ltr", line_dir_rotated="ttb", char_dir_rotated="ltr", extra_attrs=[], split_at_punctuation=False, expand_ligatures=True, return_chars=False)&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Returns a list of all word-looking things and their bounding boxes. Words are considered to be sequences of characters where (for "upright" characters) the difference between the &lt;code&gt;x1&lt;/code&gt; of one character and the &lt;code&gt;x0&lt;/code&gt; of the next is less than or equal to &lt;code&gt;x_tolerance&lt;/code&gt; &lt;em&gt;and&lt;/em&gt; where the &lt;code&gt;doctop&lt;/code&gt; of one character and the &lt;code&gt;doctop&lt;/code&gt; of the next is less than or equal to &lt;code&gt;y_tolerance&lt;/code&gt;. (If &lt;code&gt;x_tolerance_ratio&lt;/code&gt; is not &lt;code&gt;None&lt;/code&gt;, the extractor uses a dynamic &lt;code&gt;x_tolerance&lt;/code&gt; equal to &lt;code&gt;x_tolerance_ratio * previous_character["size"]&lt;/code&gt;.) A similar approach is taken for non-upright characters, but instead measuring the vertical, rather than horizontal, distances between them. Changing &lt;code&gt;keep_blank_chars&lt;/code&gt; to &lt;code&gt;True&lt;/code&gt; will mean that blank characters are treated as part of a word, not as a space between words. Changing &lt;code&gt;use_text_flow&lt;/code&gt; to &lt;code&gt;True&lt;/code&gt; will use the PDF's underlying flow of characters as a guide for ordering and segmenting the words, rather than presorting the characters by x/y position. (This mimics how dragging a cursor highlights text in a PDF; as with that, the order does not always appear to be logical.) The arguments &lt;code&gt;line_dir&lt;/code&gt; and &lt;code&gt;char_dir&lt;/code&gt; tell this method the direction in which lines/characters are expected to be read; valid options are "ttb" (top-to-bottom), "btt" (bottom-to-top), "ltr" (left-to-right), and "rtl" (right-to-left). The &lt;code&gt;line_dir_rotated&lt;/code&gt; and &lt;code&gt;char_dir_rotated&lt;/code&gt; arguments are similar, but for text that has been rotated. Passing a list of &lt;code&gt;extra_attrs&lt;/code&gt; (e.g., &lt;code&gt;["fontname", "size"]&lt;/code&gt; will restrict each words to characters that share exactly the same value for each of those &lt;a href="https://raw.githubusercontent.com/jsvine/pdfplumber/stable/#char-properties"&gt;attributes&lt;/a&gt;, and the resulting word dicts will indicate those attributes. Setting &lt;code&gt;split_at_punctuation&lt;/code&gt; to &lt;code&gt;True&lt;/code&gt; will enforce breaking tokens at punctuations specified by &lt;code&gt;string.punctuation&lt;/code&gt;; or you can specify the list of separating punctuation by pass a string, e.g., &lt;code&gt;split_at_punctuation='!"&amp;amp;'()*+,.:;&amp;lt;=&amp;gt;?@[]^`{|}~'&lt;/code&gt;. Unless you set &lt;code&gt;expand_ligatures=False&lt;/code&gt;, ligatures such as &lt;code&gt;ﬁ&lt;/code&gt; will be expanded into their constituent letters (e.g., &lt;code&gt;fi&lt;/code&gt;). Passing &lt;code&gt;return_chars=True&lt;/code&gt; will add, to each word dictionary, a list of its constituent characters, as a list in the &lt;code&gt;"chars"&lt;/code&gt; field.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;.extract_text_lines(layout=False, strip=True, return_chars=True, **kwargs)&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;em&gt;Experimental feature&lt;/em&gt; that returns a list of dictionaries representing the lines of text on the page. The &lt;code&gt;strip&lt;/code&gt; parameter works analogously to Python's &lt;code&gt;str.strip()&lt;/code&gt; method, and returns &lt;code&gt;text&lt;/code&gt; attributes without their surrounding whitespace. (Only relevant when &lt;code&gt;layout = True&lt;/code&gt;.) Setting &lt;code&gt;return_chars&lt;/code&gt; to &lt;code&gt;False&lt;/code&gt; will exclude the individual character objects from the returned text-line dicts. The remaining &lt;code&gt;**kwargs&lt;/code&gt; are those you would pass to &lt;code&gt;.extract_text(layout=True, ...)&lt;/code&gt;.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;.search(pattern, regex=True, case=True, main_group=0, return_groups=True, return_chars=True, layout=False, **kwargs)&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;em&gt;Experimental feature&lt;/em&gt; that allows you to search a page's text, returning a list of all instances that match the query. For each instance, the response dictionary object contains the matching text, any regex group matches, the bounding box coordinates, and the char objects themselves. &lt;code&gt;pattern&lt;/code&gt; can be a compiled regular expression, an uncompiled regular expression, or a non-regex string. If &lt;code&gt;regex&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt;, the pattern is treated as a non-regex string. If &lt;code&gt;case&lt;/code&gt; is &lt;code&gt;False&lt;/code&gt;, the search is performed in a case-insensitive manner. Setting &lt;code&gt;main_group&lt;/code&gt; restricts the results to a specific regex group within the &lt;code&gt;pattern&lt;/code&gt; (default of &lt;code&gt;0&lt;/code&gt; means the entire match). Setting &lt;code&gt;return_groups&lt;/code&gt; and/or &lt;code&gt;return_chars&lt;/code&gt; to &lt;code&gt;False&lt;/code&gt; will exclude the lists of the matched regex groups and/or characters from being added (as &lt;code&gt;"groups"&lt;/code&gt; and &lt;code&gt;"chars"&lt;/code&gt; to the return dicts). The &lt;code&gt;layout&lt;/code&gt; parameter operates as it does for &lt;code&gt;.extract_text(...)&lt;/code&gt;. The remaining &lt;code&gt;**kwargs&lt;/code&gt; are those you would pass to &lt;code&gt;.extract_text(layout=True, ...)&lt;/code&gt;. &lt;strong&gt;Note&lt;/strong&gt;: Zero-width and all-whitespace matches are discarded, because they (generally) have no explicit position on the page.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;.dedupe_chars(tolerance=1, extra_attrs=("fontname", "size"))&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Returns a version of the page with duplicate chars —&amp;nbsp;those sharing the same text, positioning (within &lt;code&gt;tolerance&lt;/code&gt; x/y), and &lt;code&gt;extra_attrs&lt;/code&gt; as other characters —&amp;nbsp;removed. (See &lt;a href="https://github.com/jsvine/pdfplumber/issues/71"&gt;Issue #71&lt;/a&gt; to understand the motivation.)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Extracting tables&lt;/h2&gt; 
&lt;p&gt;&lt;code&gt;pdfplumber&lt;/code&gt;'s approach to table detection borrows heavily from &lt;a href="https://trepo.tuni.fi/bitstream/handle/123456789/21520/Nurminen.pdf?sequence=3"&gt;Anssi Nurminen's master's thesis&lt;/a&gt;, and is inspired by &lt;a href="https://github.com/tabulapdf/tabula-extractor/issues/16"&gt;Tabula&lt;/a&gt;. It works like this:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;For any given PDF page, find the lines that are (a) explicitly defined and/or (b) implied by the alignment of words on the page.&lt;/li&gt; 
 &lt;li&gt;Merge overlapping, or nearly-overlapping, lines.&lt;/li&gt; 
 &lt;li&gt;Find the intersections of all those lines.&lt;/li&gt; 
 &lt;li&gt;Find the most granular set of rectangles (i.e., cells) that use these intersections as their vertices.&lt;/li&gt; 
 &lt;li&gt;Group contiguous cells into tables.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Table-extraction methods&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;pdfplumber.Page&lt;/code&gt; objects can call the following table methods:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Method&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;.find_tables(table_settings={})&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Returns a list of &lt;code&gt;Table&lt;/code&gt; objects. The &lt;code&gt;Table&lt;/code&gt; object provides access to the &lt;code&gt;.cells&lt;/code&gt;, &lt;code&gt;.rows&lt;/code&gt;, &lt;code&gt;.columns&lt;/code&gt;, and &lt;code&gt;.bbox&lt;/code&gt; properties, as well as the &lt;code&gt;.extract(x_tolerance=3, y_tolerance=3)&lt;/code&gt; method.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;.find_table(table_settings={})&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Similar to &lt;code&gt;.find_tables(...)&lt;/code&gt;, but returns the &lt;em&gt;largest&lt;/em&gt; table on the page, as a &lt;code&gt;Table&lt;/code&gt; object. If multiple tables have the same size —&amp;nbsp;as measured by the number of cells —&amp;nbsp;this method returns the table closest to the top of the page.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;.extract_tables(table_settings={})&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Returns the text extracted from &lt;em&gt;all&lt;/em&gt; tables found on the page, represented as a list of lists of lists, with the structure &lt;code&gt;table -&amp;gt; row -&amp;gt; cell&lt;/code&gt;.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;.extract_table(table_settings={})&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Returns the text extracted from the &lt;em&gt;largest&lt;/em&gt; table on the page (see &lt;code&gt;.find_table(...)&lt;/code&gt; above), represented as a list of lists, with the structure &lt;code&gt;row -&amp;gt; cell&lt;/code&gt;.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;.debug_tablefinder(table_settings={})&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Returns an instance of the &lt;code&gt;TableFinder&lt;/code&gt; class, with access to the &lt;code&gt;.edges&lt;/code&gt;, &lt;code&gt;.intersections&lt;/code&gt;, &lt;code&gt;.cells&lt;/code&gt;, and &lt;code&gt;.tables&lt;/code&gt; properties.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;pdf = pdfplumber.open("path/to/my.pdf")
page = pdf.pages[0]
page.extract_table()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/jsvine/pdfplumber/stable/examples/notebooks/extract-table-ca-warn-report.ipynb"&gt;Click here for a more detailed example.&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Table-extraction settings&lt;/h3&gt; 
&lt;p&gt;By default, &lt;code&gt;extract_tables&lt;/code&gt; uses the page's vertical and horizontal lines (or rectangle edges) as cell-separators. But the method is highly customizable via the &lt;code&gt;table_settings&lt;/code&gt; argument. The possible settings, and their defaults:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;{
    "vertical_strategy": "lines", 
    "horizontal_strategy": "lines",
    "explicit_vertical_lines": [],
    "explicit_horizontal_lines": [],
    "snap_tolerance": 3,
    "snap_x_tolerance": 3,
    "snap_y_tolerance": 3,
    "join_tolerance": 3,
    "join_x_tolerance": 3,
    "join_y_tolerance": 3,
    "edge_min_length": 3,
    "min_words_vertical": 3,
    "min_words_horizontal": 1,
    "intersection_tolerance": 3,
    "intersection_x_tolerance": 3,
    "intersection_y_tolerance": 3,
    "text_tolerance": 3,
    "text_x_tolerance": 3,
    "text_y_tolerance": 3,
    "text_*": …, # See below
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Setting&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;"vertical_strategy"&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Either &lt;code&gt;"lines"&lt;/code&gt;, &lt;code&gt;"lines_strict"&lt;/code&gt;, &lt;code&gt;"text"&lt;/code&gt;, or &lt;code&gt;"explicit"&lt;/code&gt;. See explanation below.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;"horizontal_strategy"&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Either &lt;code&gt;"lines"&lt;/code&gt;, &lt;code&gt;"lines_strict"&lt;/code&gt;, &lt;code&gt;"text"&lt;/code&gt;, or &lt;code&gt;"explicit"&lt;/code&gt;. See explanation below.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;"explicit_vertical_lines"&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;A list of vertical lines that explicitly demarcate cells in the table. Can be used in combination with any of the strategies above. Items in the list should be either numbers —&amp;nbsp;indicating the &lt;code&gt;x&lt;/code&gt; coordinate of a line the full height of the page —&amp;nbsp;or &lt;code&gt;line&lt;/code&gt;/&lt;code&gt;rect&lt;/code&gt;/&lt;code&gt;curve&lt;/code&gt; objects.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;"explicit_horizontal_lines"&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;A list of horizontal lines that explicitly demarcate cells in the table. Can be used in combination with any of the strategies above. Items in the list should be either numbers —&amp;nbsp;indicating the &lt;code&gt;y&lt;/code&gt; coordinate of a line the full height of the page —&amp;nbsp;or &lt;code&gt;line&lt;/code&gt;/&lt;code&gt;rect&lt;/code&gt;/&lt;code&gt;curve&lt;/code&gt; objects.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;"snap_tolerance"&lt;/code&gt;, &lt;code&gt;"snap_x_tolerance"&lt;/code&gt;, &lt;code&gt;"snap_y_tolerance"&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Parallel lines within &lt;code&gt;snap_tolerance&lt;/code&gt; points will be "snapped" to the same horizontal or vertical position.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;"join_tolerance"&lt;/code&gt;, &lt;code&gt;"join_x_tolerance"&lt;/code&gt;, &lt;code&gt;"join_y_tolerance"&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Line segments on the same infinite line, and whose ends are within &lt;code&gt;join_tolerance&lt;/code&gt; of one another, will be "joined" into a single line segment.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;"edge_min_length"&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Edges shorter than &lt;code&gt;edge_min_length&lt;/code&gt; will be discarded before attempting to reconstruct the table.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;"min_words_vertical"&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;When using &lt;code&gt;"vertical_strategy": "text"&lt;/code&gt;, at least &lt;code&gt;min_words_vertical&lt;/code&gt; words must share the same alignment.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;"min_words_horizontal"&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;When using &lt;code&gt;"horizontal_strategy": "text"&lt;/code&gt;, at least &lt;code&gt;min_words_horizontal&lt;/code&gt; words must share the same alignment.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;"intersection_tolerance"&lt;/code&gt;, &lt;code&gt;"intersection_x_tolerance"&lt;/code&gt;, &lt;code&gt;"intersection_y_tolerance"&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;When combining edges into cells, orthogonal edges must be within &lt;code&gt;intersection_tolerance&lt;/code&gt; points to be considered intersecting.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;"text_*"&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;All settings prefixed with &lt;code&gt;text_&lt;/code&gt; are then used when extracting text from each discovered table. All possible arguments to &lt;code&gt;Page.extract_text(...)&lt;/code&gt; are also valid here.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;"text_x_tolerance"&lt;/code&gt;, &lt;code&gt;"text_y_tolerance"&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;These &lt;code&gt;text_&lt;/code&gt;-prefixed settings &lt;em&gt;also&lt;/em&gt; apply to the table-identification algorithm when the &lt;code&gt;text&lt;/code&gt; strategy is used. I.e., when that algorithm searches for words, it will expect the individual letters in each word to be no more than &lt;code&gt;text_x_tolerance&lt;/code&gt;/&lt;code&gt;text_y_tolerance&lt;/code&gt; points apart.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Table-extraction strategies&lt;/h3&gt; 
&lt;p&gt;Both &lt;code&gt;vertical_strategy&lt;/code&gt; and &lt;code&gt;horizontal_strategy&lt;/code&gt; accept the following options:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Strategy&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;"lines"&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Use the page's graphical lines —&amp;nbsp;including the sides of rectangle objects —&amp;nbsp;as the borders of potential table-cells.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;"lines_strict"&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Use the page's graphical lines —&amp;nbsp;but &lt;em&gt;not&lt;/em&gt; the sides of rectangle objects —&amp;nbsp;as the borders of potential table-cells.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;"text"&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;For &lt;code&gt;vertical_strategy&lt;/code&gt;: Deduce the (imaginary) lines that connect the left, right, or center of words on the page, and use those lines as the borders of potential table-cells. For &lt;code&gt;horizontal_strategy&lt;/code&gt;, the same but using the tops of words.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;"explicit"&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Only use the lines explicitly defined in &lt;code&gt;explicit_vertical_lines&lt;/code&gt; / &lt;code&gt;explicit_horizontal_lines&lt;/code&gt;.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Notes&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Often it's helpful to crop a page —&amp;nbsp;&lt;code&gt;Page.crop(bounding_box)&lt;/code&gt; —&amp;nbsp;before trying to extract the table.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Table extraction for &lt;code&gt;pdfplumber&lt;/code&gt; was radically redesigned for &lt;code&gt;v0.5.0&lt;/code&gt;, and introduced breaking changes.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Extracting form values&lt;/h2&gt; 
&lt;p&gt;Sometimes PDF files can contain forms that include inputs that people can fill out and save. While values in form fields appear like other text in a PDF file, form data is handled differently. If you want the gory details, see page 671 of this &lt;a href="https://opensource.adobe.com/dc-acrobat-sdk-docs/pdfstandards/pdfreference1.7old.pdf"&gt;specification&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;pdfplumber&lt;/code&gt; doesn't have an interface for working with form data, but you can access it using &lt;code&gt;pdfplumber&lt;/code&gt;'s wrappers around &lt;code&gt;pdfminer&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;For example, this snippet will retrieve form field names and values and store them in a dictionary.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import pdfplumber
from pdfplumber.utils.pdfinternals import resolve_and_decode, resolve

pdf = pdfplumber.open("document_with_form.pdf")

def parse_field_helper(form_data, field, prefix=None):
    """ appends any PDF AcroForm field/value pairs in `field` to provided `form_data` list

        if `field` has child fields, those will be parsed recursively.
    """
    resolved_field = field.resolve()
    field_name = '.'.join(filter(lambda x: x, [prefix, resolve_and_decode(resolved_field.get("T"))]))
    if "Kids" in resolved_field:
        for kid_field in resolved_field["Kids"]:
            parse_field_helper(form_data, kid_field, prefix=field_name)
    if "T" in resolved_field or "TU" in resolved_field:
        # "T" is a field-name, but it's sometimes absent.
        # "TU" is the "alternate field name" and is often more human-readable
        # your PDF may have one, the other, or both.
        alternate_field_name  = resolve_and_decode(resolved_field.get("TU")) if resolved_field.get("TU") else None
        field_value = resolve_and_decode(resolved_field["V"]) if 'V' in resolved_field else None
        form_data.append([field_name, alternate_field_name, field_value])


form_data = []
fields = resolve(resolve(pdf.doc.catalog["AcroForm"])["Fields"])
for field in fields:
    parse_field_helper(form_data, field)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Once you run this script, &lt;code&gt;form_data&lt;/code&gt; is a list containing a three-element tuple for each form element. For instance, a PDF form with a city and state field might look like this.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;[
 ['STATE.0', 'enter STATE', 'CA'],
 ['section 2  accident infoRmation.1.0',
  'enter city of accident',
  'SAN FRANCISCO']
]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Thanks to &lt;a href="https://github.com/jeremybmerrill"&gt;@jeremybmerrill&lt;/a&gt; for helping to maintain the form-parsing code above.&lt;/em&gt;&lt;/p&gt; 
&lt;h2&gt;Demonstrations&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/jsvine/pdfplumber/stable/examples/notebooks/extract-table-ca-warn-report.ipynb"&gt;Using &lt;code&gt;extract_table&lt;/code&gt; on a California Worker Adjustment and Retraining Notification (WARN) report&lt;/a&gt;. Demonstrates basic visual debugging and table extraction.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/jsvine/pdfplumber/stable/examples/notebooks/extract-table-nics.ipynb"&gt;Using &lt;code&gt;extract_table&lt;/code&gt; on the FBI's National Instant Criminal Background Check System PDFs&lt;/a&gt;. Demonstrates how to use visual debugging to find optimal table extraction settings. Also demonstrates &lt;code&gt;Page.crop(...)&lt;/code&gt; and &lt;code&gt;Page.extract_text(...).&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/jsvine/pdfplumber/stable/examples/notebooks/ag-energy-roundup-curves.ipynb"&gt;Inspecting and visualizing &lt;code&gt;curve&lt;/code&gt; objects&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/jsvine/pdfplumber/stable/examples/notebooks/san-jose-pd-firearm-report.ipynb"&gt;Extracting fixed-width data from a San Jose PD firearm search report&lt;/a&gt;, an example of using &lt;code&gt;Page.extract_text(...)&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Comparison to other libraries&lt;/h2&gt; 
&lt;p&gt;Several other Python libraries help users to extract information from PDFs. As a broad overview, &lt;code&gt;pdfplumber&lt;/code&gt; distinguishes itself from other PDF processing libraries by combining these features:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Easy access to detailed information about each PDF object&lt;/li&gt; 
 &lt;li&gt;Higher-level, customizable methods for extracting text and tables&lt;/li&gt; 
 &lt;li&gt;Tightly integrated visual debugging&lt;/li&gt; 
 &lt;li&gt;Other useful utility functions, such as filtering objects via a crop-box&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;It's also helpful to know what features &lt;code&gt;pdfplumber&lt;/code&gt; does &lt;strong&gt;not&lt;/strong&gt; provide:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;PDF &lt;em&gt;generation&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;PDF &lt;em&gt;modification&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;Optical character recognition (OCR)&lt;/li&gt; 
 &lt;li&gt;Strong support for extracting tables from OCR'ed documents&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Specific comparisons&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/pdfminer/pdfminer.six"&gt;&lt;code&gt;pdfminer.six&lt;/code&gt;&lt;/a&gt; provides the foundation for &lt;code&gt;pdfplumber&lt;/code&gt;. It primarily focuses on parsing PDFs, analyzing PDF layouts and object positioning, and extracting text. It does not provide tools for table extraction or visual debugging. License: &lt;a href="https://github.com/pdfminer/pdfminer.six?tab=MIT-1-ov-file"&gt;MIT&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/mstamy2/PyPDF2"&gt;&lt;code&gt;PyPDF2&lt;/code&gt;&lt;/a&gt; is a pure-Python library "capable of splitting, merging, cropping, and transforming the pages of PDF files. It can also add custom data, viewing options, and passwords to PDF files." It can extract page text, but does not provide easy access to shape objects (rectangles, lines, etc.), table-extraction, or visually debugging tools. License: &lt;a href="https://github.com/py-pdf/pypdf?tab=License-1-ov-file#readme"&gt;BSD&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://pymupdf.readthedocs.io/"&gt;&lt;code&gt;pymupdf&lt;/code&gt;&lt;/a&gt; is substantially faster than &lt;code&gt;pdfminer.six&lt;/code&gt; (and thus also &lt;code&gt;pdfplumber&lt;/code&gt;) and can generate and modify PDFs, but the library requires installation of non-Python software (MuPDF). It also does not enable easy access to shape objects (rectangles, lines, etc.), and does not provide table-extraction or visual debugging tools. License: &lt;a href="https://pymupdf.readthedocs.io/en/latest/about.html#license-and-copyright"&gt;AGPL&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/camelot-dev/camelot"&gt;&lt;code&gt;camelot&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://github.com/chezou/tabula-py"&gt;&lt;code&gt;tabula-py&lt;/code&gt;&lt;/a&gt;, and &lt;a href="https://github.com/drj11/pdftables"&gt;&lt;code&gt;pdftables&lt;/code&gt;&lt;/a&gt; all focus primarily on extracting tables. In some cases, they may be better suited to the particular tables you are trying to extract. License: &lt;a href="https://github.com/camelot-dev/camelot?tab=MIT-1-ov-file#readme"&gt;MIT&lt;/a&gt; (&lt;code&gt;camelot&lt;/code&gt;), &lt;a href="https://github.com/chezou/tabula-py?tab=MIT-1-ov-file#readme"&gt;MIT&lt;/a&gt; (&lt;code&gt;tabula-py&lt;/code&gt;), &lt;a href="https://github.com/drj11/pdftables?tab=BSD-2-Clause-1-ov-file#readme"&gt;BSD&lt;/a&gt; (&lt;code&gt;pdftables&lt;/code&gt;).&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Acknowledgments / Contributors&lt;/h2&gt; 
&lt;p&gt;Many thanks to the following users who've contributed ideas, features, and fixes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/jsfenfen"&gt;Jacob Fenton&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/dannguyen"&gt;Dan Nguyen&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/jeffbarrera"&gt;Jeff Barrera&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/boblannon"&gt;Bob Lannon&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/dustindall"&gt;Dustin Tindall&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Yevgnen"&gt;@yevgnen&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/meldonization"&gt;@meldonization&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OisinMoran"&gt;Oisín Moran&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/samkit-jain"&gt;Samkit Jain&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/frascuchon"&gt;Francisco Aranda&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/cheungpat"&gt;Kwok-kuen Cheung&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ubmarco"&gt;Marco&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/idan-david"&gt;Idan David&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/xv44586"&gt;@xv44586&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/alexreg"&gt;Alexander Regueiro&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/trifling"&gt;Daniel Peña&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/bobluda"&gt;@bobluda&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ramcdona"&gt;@ramcdona&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/johnhuge"&gt;@johnhuge&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/jhonatan-lopes"&gt;Jhonatan Lopes&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ethanscorey"&gt;Ethan Corey&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/lolipopshock"&gt;Shannon Shen&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/toshi1127"&gt;Matsumoto Toshi&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/jwestwsj"&gt;John West&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/dhdaines"&gt;David Huggins-Daines&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/jeremybmerrill"&gt;Jeremy B. Merrill&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/echedey-ls"&gt;Echedey Luis&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/afriedman412"&gt;Andy Friedman&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aronweiler"&gt;Aron Weiler&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/QuentinAndre11"&gt;Quentin André&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/leorouxx"&gt;Léo Roux&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/wodny"&gt;@wodny&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/stolarczyk"&gt;Michal Stolarczyk&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/brandonrobertz"&gt;Brandon Roberts&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ennamarie19"&gt;@ennamarie19&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Pull requests are welcome, but please submit a proposal issue first, as the library is in active development.&lt;/p&gt; 
&lt;p&gt;Current maintainers:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/jsvine"&gt;Jeremy Singer-Vine&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/samkit-jain"&gt;Samkit Jain&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>anthropics/claude-agent-sdk-python</title>
      <link>https://github.com/anthropics/claude-agent-sdk-python</link>
      <description>&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Claude Agent SDK for Python&lt;/h1&gt; 
&lt;p&gt;Python SDK for Claude Agent. See the &lt;a href="https://docs.anthropic.com/en/docs/claude-code/sdk/sdk-python"&gt;Claude Agent SDK documentation&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install claude-agent-sdk
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Prerequisites:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.10+&lt;/li&gt; 
 &lt;li&gt;Node.js&lt;/li&gt; 
 &lt;li&gt;Claude Code: &lt;code&gt;npm install -g @anthropic-ai/claude-code&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import anyio
from claude_agent_sdk import query

async def main():
    async for message in query(prompt="What is 2 + 2?"):
        print(message)

anyio.run(main)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Basic Usage: query()&lt;/h2&gt; 
&lt;p&gt;&lt;code&gt;query()&lt;/code&gt; is an async function for querying Claude Code. It returns an &lt;code&gt;AsyncIterator&lt;/code&gt; of response messages. See &lt;a href="https://raw.githubusercontent.com/anthropics/claude-agent-sdk-python/main/src/claude_agent_sdk/query.py"&gt;src/claude_agent_sdk/query.py&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from claude_agent_sdk import query, ClaudeAgentOptions, AssistantMessage, TextBlock

# Simple query
async for message in query(prompt="Hello Claude"):
    if isinstance(message, AssistantMessage):
        for block in message.content:
            if isinstance(block, TextBlock):
                print(block.text)

# With options
options = ClaudeAgentOptions(
    system_prompt="You are a helpful assistant",
    max_turns=1
)

async for message in query(prompt="Tell me a joke", options=options):
    print(message)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Using Tools&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;options = ClaudeAgentOptions(
    allowed_tools=["Read", "Write", "Bash"],
    permission_mode='acceptEdits'  # auto-accept file edits
)

async for message in query(
    prompt="Create a hello.py file",
    options=options
):
    # Process tool use and results
    pass
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Working Directory&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from pathlib import Path

options = ClaudeAgentOptions(
    cwd="/path/to/project"  # or Path("/path/to/project")
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ClaudeSDKClient&lt;/h2&gt; 
&lt;p&gt;&lt;code&gt;ClaudeSDKClient&lt;/code&gt; supports bidirectional, interactive conversations with Claude Code. See &lt;a href="https://raw.githubusercontent.com/anthropics/claude-agent-sdk-python/main/src/claude_agent_sdk/client.py"&gt;src/claude_agent_sdk/client.py&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Unlike &lt;code&gt;query()&lt;/code&gt;, &lt;code&gt;ClaudeSDKClient&lt;/code&gt; additionally enables &lt;strong&gt;custom tools&lt;/strong&gt; and &lt;strong&gt;hooks&lt;/strong&gt;, both of which can be defined as Python functions.&lt;/p&gt; 
&lt;h3&gt;Custom Tools (as In-Process SDK MCP Servers)&lt;/h3&gt; 
&lt;p&gt;A &lt;strong&gt;custom tool&lt;/strong&gt; is a Python function that you can offer to Claude, for Claude to invoke as needed.&lt;/p&gt; 
&lt;p&gt;Custom tools are implemented in-process MCP servers that run directly within your Python application, eliminating the need for separate processes that regular MCP servers require.&lt;/p&gt; 
&lt;p&gt;For an end-to-end example, see &lt;a href="https://raw.githubusercontent.com/anthropics/claude-agent-sdk-python/main/examples/mcp_calculator.py"&gt;MCP Calculator&lt;/a&gt;.&lt;/p&gt; 
&lt;h4&gt;Creating a Simple Tool&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from claude_agent_sdk import tool, create_sdk_mcp_server, ClaudeAgentOptions, ClaudeSDKClient

# Define a tool using the @tool decorator
@tool("greet", "Greet a user", {"name": str})
async def greet_user(args):
    return {
        "content": [
            {"type": "text", "text": f"Hello, {args['name']}!"}
        ]
    }

# Create an SDK MCP server
server = create_sdk_mcp_server(
    name="my-tools",
    version="1.0.0",
    tools=[greet_user]
)

# Use it with Claude
options = ClaudeAgentOptions(
    mcp_servers={"tools": server},
    allowed_tools=["mcp__tools__greet"]
)

async with ClaudeSDKClient(options=options) as client:
    await client.query("Greet Alice")

    # Extract and print response
    async for msg in client.receive_response():
        print(msg)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Benefits Over External MCP Servers&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;No subprocess management&lt;/strong&gt; - Runs in the same process as your application&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Better performance&lt;/strong&gt; - No IPC overhead for tool calls&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Simpler deployment&lt;/strong&gt; - Single Python process instead of multiple&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Easier debugging&lt;/strong&gt; - All code runs in the same process&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Type safety&lt;/strong&gt; - Direct Python function calls with type hints&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Migration from External Servers&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# BEFORE: External MCP server (separate process)
options = ClaudeAgentOptions(
    mcp_servers={
        "calculator": {
            "type": "stdio",
            "command": "python",
            "args": ["-m", "calculator_server"]
        }
    }
)

# AFTER: SDK MCP server (in-process)
from my_tools import add, subtract  # Your tool functions

calculator = create_sdk_mcp_server(
    name="calculator",
    tools=[add, subtract]
)

options = ClaudeAgentOptions(
    mcp_servers={"calculator": calculator}
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Mixed Server Support&lt;/h4&gt; 
&lt;p&gt;You can use both SDK and external MCP servers together:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;options = ClaudeAgentOptions(
    mcp_servers={
        "internal": sdk_server,      # In-process SDK server
        "external": {                # External subprocess server
            "type": "stdio",
            "command": "external-server"
        }
    }
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Hooks&lt;/h3&gt; 
&lt;p&gt;A &lt;strong&gt;hook&lt;/strong&gt; is a Python function that the Claude Code &lt;em&gt;application&lt;/em&gt; (&lt;em&gt;not&lt;/em&gt; Claude) invokes at specific points of the Claude agent loop. Hooks can provide deterministic processing and automated feedback for Claude. Read more in &lt;a href="https://docs.anthropic.com/en/docs/claude-code/hooks"&gt;Claude Code Hooks Reference&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For more examples, see examples/hooks.py.&lt;/p&gt; 
&lt;h4&gt;Example&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from claude_agent_sdk import ClaudeAgentOptions, ClaudeSDKClient, HookMatcher

async def check_bash_command(input_data, tool_use_id, context):
    tool_name = input_data["tool_name"]
    tool_input = input_data["tool_input"]
    if tool_name != "Bash":
        return {}
    command = tool_input.get("command", "")
    block_patterns = ["foo.sh"]
    for pattern in block_patterns:
        if pattern in command:
            return {
                "hookSpecificOutput": {
                    "hookEventName": "PreToolUse",
                    "permissionDecision": "deny",
                    "permissionDecisionReason": f"Command contains invalid pattern: {pattern}",
                }
            }
    return {}

options = ClaudeAgentOptions(
    allowed_tools=["Bash"],
    hooks={
        "PreToolUse": [
            HookMatcher(matcher="Bash", hooks=[check_bash_command]),
        ],
    }
)

async with ClaudeSDKClient(options=options) as client:
    # Test 1: Command with forbidden pattern (will be blocked)
    await client.query("Run the bash command: ./foo.sh --help")
    async for msg in client.receive_response():
        print(msg)

    print("\n" + "=" * 50 + "\n")

    # Test 2: Safe command that should work
    await client.query("Run the bash command: echo 'Hello from hooks example!'")
    async for msg in client.receive_response():
        print(msg)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Types&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/anthropics/claude-agent-sdk-python/main/src/claude_agent_sdk/types.py"&gt;src/claude_agent_sdk/types.py&lt;/a&gt; for complete type definitions:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;ClaudeAgentOptions&lt;/code&gt; - Configuration options&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;AssistantMessage&lt;/code&gt;, &lt;code&gt;UserMessage&lt;/code&gt;, &lt;code&gt;SystemMessage&lt;/code&gt;, &lt;code&gt;ResultMessage&lt;/code&gt; - Message types&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;TextBlock&lt;/code&gt;, &lt;code&gt;ToolUseBlock&lt;/code&gt;, &lt;code&gt;ToolResultBlock&lt;/code&gt; - Content blocks&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Error Handling&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from claude_agent_sdk import (
    ClaudeSDKError,      # Base error
    CLINotFoundError,    # Claude Code not installed
    CLIConnectionError,  # Connection issues
    ProcessError,        # Process failed
    CLIJSONDecodeError,  # JSON parsing issues
)

try:
    async for message in query(prompt="Hello"):
        pass
except CLINotFoundError:
    print("Please install Claude Code")
except ProcessError as e:
    print(f"Process failed with exit code: {e.exit_code}")
except CLIJSONDecodeError as e:
    print(f"Failed to parse response: {e}")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/anthropics/claude-agent-sdk-python/main/src/claude_agent_sdk/_errors.py"&gt;src/claude_agent_sdk/_errors.py&lt;/a&gt; for all error types.&lt;/p&gt; 
&lt;h2&gt;Available Tools&lt;/h2&gt; 
&lt;p&gt;See the &lt;a href="https://docs.anthropic.com/en/docs/claude-code/settings#tools-available-to-claude"&gt;Claude Code documentation&lt;/a&gt; for a complete list of available tools.&lt;/p&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/anthropics/claude-agent-sdk-python/main/examples/quick_start.py"&gt;examples/quick_start.py&lt;/a&gt; for a complete working example.&lt;/p&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/anthropics/claude-agent-sdk-python/main/examples/streaming_mode.py"&gt;examples/streaming_mode.py&lt;/a&gt; for comprehensive examples involving &lt;code&gt;ClaudeSDKClient&lt;/code&gt;. You can even run interactive examples in IPython from &lt;a href="https://raw.githubusercontent.com/anthropics/claude-agent-sdk-python/main/examples/streaming_mode_ipython.py"&gt;examples/streaming_mode_ipython.py&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Migrating from Claude Code SDK&lt;/h2&gt; 
&lt;p&gt;If you're upgrading from the Claude Code SDK (versions &amp;lt; 0.1.0), please see the &lt;a href="https://raw.githubusercontent.com/anthropics/claude-agent-sdk-python/main/CHANGELOG.md#010"&gt;CHANGELOG.md&lt;/a&gt; for details on breaking changes and new features, including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;ClaudeCodeOptions&lt;/code&gt; → &lt;code&gt;ClaudeAgentOptions&lt;/code&gt; rename&lt;/li&gt; 
 &lt;li&gt;Merged system prompt configuration&lt;/li&gt; 
 &lt;li&gt;Settings isolation and explicit control&lt;/li&gt; 
 &lt;li&gt;New programmatic subagents and session forking features&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;MIT&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>apify/crawlee-python</title>
      <link>https://github.com/apify/crawlee-python</link>
      <description>&lt;p&gt;Crawlee—A web scraping and browser automation library for Python to build reliable crawlers. Extract data for AI, LLMs, RAG, or GPTs. Download HTML, PDF, JPG, PNG, and other files from websites. Works with BeautifulSoup, Playwright, and raw HTTP. Both headful and headless mode. With proxy rotation.&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt; &lt;a href="https://crawlee.dev"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="https://raw.githubusercontent.com/apify/crawlee-python/master/website/static/img/crawlee-dark.svg?sanitize=true" /&gt; 
   &lt;img alt="Crawlee" src="https://raw.githubusercontent.com/apify/crawlee-python/master/website/static/img/crawlee-light.svg?sanitize=true" width="500" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; &lt;br /&gt; &lt;small&gt;A web scraping and browser automation library&lt;/small&gt; &lt;/h1&gt; 
&lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/11169" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/11169" alt="apify%2Fcrawlee-python | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://badge.fury.io/py/crawlee" rel="nofollow"&gt; &lt;img src="https://badge.fury.io/py/crawlee.svg?sanitize=true" alt="PyPI version" style="max-width: 100%;" /&gt; &lt;/a&gt; &lt;a href="https://pypi.org/project/crawlee/" rel="nofollow"&gt; &lt;img src="https://img.shields.io/pypi/dm/crawlee" alt="PyPI - Downloads" style="max-width: 100%;" /&gt; &lt;/a&gt; &lt;a href="https://pypi.org/project/crawlee/" rel="nofollow"&gt; &lt;img src="https://img.shields.io/pypi/pyversions/crawlee" alt="PyPI - Python Version" style="max-width: 100%;" /&gt; &lt;/a&gt; &lt;a href="https://discord.gg/jyEM2PRvMU" rel="nofollow"&gt; &lt;img src="https://img.shields.io/discord/801163717915574323?label=discord" alt="Chat on discord" style="max-width: 100%;" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;Crawlee covers your crawling and scraping end-to-end and &lt;strong&gt;helps you build reliable scrapers. Fast.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Your crawlers will appear almost human-like and fly under the radar of modern bot protections even with the default configuration. Crawlee gives you the tools to crawl the web for links, scrape data and persistently store it in machine-readable formats, without having to worry about the technical details. And thanks to rich configuration options, you can tweak almost any aspect of Crawlee to suit your project's needs if the default settings don't cut it.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;👉 &lt;strong&gt;View full documentation, guides and examples on the &lt;a href="https://crawlee.dev/python/"&gt;Crawlee project website&lt;/a&gt;&lt;/strong&gt; 👈&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;We also have a TypeScript implementation of the Crawlee, which you can explore and utilize for your projects. Visit our GitHub repository for more information &lt;a href="https://github.com/apify/crawlee"&gt;Crawlee for JS/TS on GitHub&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;We recommend visiting the &lt;a href="https://crawlee.dev/python/docs/introduction"&gt;Introduction tutorial&lt;/a&gt; in Crawlee documentation for more information.&lt;/p&gt; 
&lt;p&gt;Crawlee is available as &lt;a href="https://pypi.org/project/crawlee/"&gt;&lt;code&gt;crawlee&lt;/code&gt;&lt;/a&gt; package on PyPI. This package includes the core functionality, while additional features are available as optional extras to keep dependencies and package size minimal.&lt;/p&gt; 
&lt;p&gt;To install Crawlee with all features, run the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python -m pip install 'crawlee[all]'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then, install the &lt;a href="https://playwright.dev/"&gt;Playwright&lt;/a&gt; dependencies:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;playwright install
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Verify that Crawlee is successfully installed:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python -c 'import crawlee; print(crawlee.__version__)'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For detailed installation instructions see the &lt;a href="https://crawlee.dev/python/docs/introduction/setting-up"&gt;Setting up&lt;/a&gt; documentation page.&lt;/p&gt; 
&lt;h3&gt;With Crawlee CLI&lt;/h3&gt; 
&lt;p&gt;The quickest way to get started with Crawlee is by using the Crawlee CLI and selecting one of the prepared templates. First, ensure you have &lt;a href="https://pypi.org/project/uv/"&gt;uv&lt;/a&gt; installed:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;uv --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If &lt;a href="https://pypi.org/project/uv/"&gt;uv&lt;/a&gt; is not installed, follow the official &lt;a href="https://docs.astral.sh/uv/getting-started/installation/"&gt;installation guide&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Then, run the CLI and choose from the available templates:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;uvx 'crawlee[cli]' create my-crawler
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you already have &lt;code&gt;crawlee&lt;/code&gt; installed, you can spin it up by running:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;crawlee create my-crawler
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;p&gt;Here are some practical examples to help you get started with different types of crawlers in Crawlee. Each example demonstrates how to set up and run a crawler for specific use cases, whether you need to handle simple HTML pages or interact with JavaScript-heavy sites. A crawler run will create a &lt;code&gt;storage/&lt;/code&gt; directory in your current working directory.&lt;/p&gt; 
&lt;h3&gt;BeautifulSoupCrawler&lt;/h3&gt; 
&lt;p&gt;The &lt;a href="https://crawlee.dev/python/api/class/BeautifulSoupCrawler"&gt;&lt;code&gt;BeautifulSoupCrawler&lt;/code&gt;&lt;/a&gt; downloads web pages using an HTTP library and provides HTML-parsed content to the user. By default it uses &lt;a href="https://crawlee.dev/python/api/class/HttpxHttpClient"&gt;&lt;code&gt;HttpxHttpClient&lt;/code&gt;&lt;/a&gt; for HTTP communication and &lt;a href="https://pypi.org/project/beautifulsoup4/"&gt;BeautifulSoup&lt;/a&gt; for parsing HTML. It is ideal for projects that require efficient extraction of data from HTML content. This crawler has very good performance since it does not use a browser. However, if you need to execute client-side JavaScript, to get your content, this is not going to be enough and you will need to use &lt;a href="https://crawlee.dev/python/api/class/PlaywrightCrawler"&gt;&lt;code&gt;PlaywrightCrawler&lt;/code&gt;&lt;/a&gt;. Also if you want to use this crawler, make sure you install &lt;code&gt;crawlee&lt;/code&gt; with &lt;code&gt;beautifulsoup&lt;/code&gt; extra.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import asyncio

from crawlee.crawlers import BeautifulSoupCrawler, BeautifulSoupCrawlingContext


async def main() -&amp;gt; None:
    crawler = BeautifulSoupCrawler(
        # Limit the crawl to max requests. Remove or increase it for crawling all links.
        max_requests_per_crawl=10,
    )

    # Define the default request handler, which will be called for every request.
    @crawler.router.default_handler
    async def request_handler(context: BeautifulSoupCrawlingContext) -&amp;gt; None:
        context.log.info(f'Processing {context.request.url} ...')

        # Extract data from the page.
        data = {
            'url': context.request.url,
            'title': context.soup.title.string if context.soup.title else None,
        }

        # Push the extracted data to the default dataset.
        await context.push_data(data)

        # Enqueue all links found on the page.
        await context.enqueue_links()

    # Run the crawler with the initial list of URLs.
    await crawler.run(['https://crawlee.dev'])


if __name__ == '__main__':
    asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;PlaywrightCrawler&lt;/h3&gt; 
&lt;p&gt;The &lt;a href="https://crawlee.dev/python/api/class/PlaywrightCrawler"&gt;&lt;code&gt;PlaywrightCrawler&lt;/code&gt;&lt;/a&gt; uses a headless browser to download web pages and provides an API for data extraction. It is built on &lt;a href="https://playwright.dev/"&gt;Playwright&lt;/a&gt;, an automation library designed for managing headless browsers. It excels at retrieving web pages that rely on client-side JavaScript for content generation, or tasks requiring interaction with JavaScript-driven content. For scenarios where JavaScript execution is unnecessary or higher performance is required, consider using the &lt;a href="https://crawlee.dev/python/api/class/BeautifulSoupCrawler"&gt;&lt;code&gt;BeautifulSoupCrawler&lt;/code&gt;&lt;/a&gt;. Also if you want to use this crawler, make sure you install &lt;code&gt;crawlee&lt;/code&gt; with &lt;code&gt;playwright&lt;/code&gt; extra.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import asyncio

from crawlee.crawlers import PlaywrightCrawler, PlaywrightCrawlingContext


async def main() -&amp;gt; None:
    crawler = PlaywrightCrawler(
        # Limit the crawl to max requests. Remove or increase it for crawling all links.
        max_requests_per_crawl=10,
    )

    # Define the default request handler, which will be called for every request.
    @crawler.router.default_handler
    async def request_handler(context: PlaywrightCrawlingContext) -&amp;gt; None:
        context.log.info(f'Processing {context.request.url} ...')

        # Extract data from the page.
        data = {
            'url': context.request.url,
            'title': await context.page.title(),
        }

        # Push the extracted data to the default dataset.
        await context.push_data(data)

        # Enqueue all links found on the page.
        await context.enqueue_links()

    # Run the crawler with the initial list of requests.
    await crawler.run(['https://crawlee.dev'])


if __name__ == '__main__':
    asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;More examples&lt;/h3&gt; 
&lt;p&gt;Explore our &lt;a href="https://crawlee.dev/python/docs/examples"&gt;Examples&lt;/a&gt; page in the Crawlee documentation for a wide range of additional use cases and demonstrations.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;p&gt;Why Crawlee is the preferred choice for web scraping and crawling?&lt;/p&gt; 
&lt;h3&gt;Why use Crawlee instead of just a random HTTP library with an HTML parser?&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Unified interface for &lt;strong&gt;HTTP &amp;amp; headless browser&lt;/strong&gt; crawling.&lt;/li&gt; 
 &lt;li&gt;Automatic &lt;strong&gt;parallel crawling&lt;/strong&gt; based on available system resources.&lt;/li&gt; 
 &lt;li&gt;Written in Python with &lt;strong&gt;type hints&lt;/strong&gt; - enhances DX (IDE autocompletion) and reduces bugs (static type checking).&lt;/li&gt; 
 &lt;li&gt;Automatic &lt;strong&gt;retries&lt;/strong&gt; on errors or when you’re getting blocked.&lt;/li&gt; 
 &lt;li&gt;Integrated &lt;strong&gt;proxy rotation&lt;/strong&gt; and session management.&lt;/li&gt; 
 &lt;li&gt;Configurable &lt;strong&gt;request routing&lt;/strong&gt; - direct URLs to the appropriate handlers.&lt;/li&gt; 
 &lt;li&gt;Persistent &lt;strong&gt;queue for URLs&lt;/strong&gt; to crawl.&lt;/li&gt; 
 &lt;li&gt;Pluggable &lt;strong&gt;storage&lt;/strong&gt; of both tabular data and files.&lt;/li&gt; 
 &lt;li&gt;Robust &lt;strong&gt;error handling&lt;/strong&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Why to use Crawlee rather than Scrapy?&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Asyncio-based&lt;/strong&gt; – Leveraging the standard &lt;a href="https://docs.python.org/3/library/asyncio.html"&gt;Asyncio&lt;/a&gt; library, Crawlee delivers better performance and seamless compatibility with other modern asynchronous libraries.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Type hints&lt;/strong&gt; – Newer project built with modern Python, and complete type hint coverage for a better developer experience.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Simple integration&lt;/strong&gt; – Crawlee crawlers are regular Python scripts, requiring no additional launcher executor. This flexibility allows to integrate a crawler directly into other applications.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;State persistence&lt;/strong&gt; – Supports state persistence during interruptions, saving time and costs by avoiding the need to restart scraping pipelines from scratch after an issue.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Organized data storages&lt;/strong&gt; – Allows saving of multiple types of results in a single scraping run. Offers several storing options (see &lt;a href="https://crawlee.dev/python/api/class/Dataset"&gt;datasets&lt;/a&gt; &amp;amp; &lt;a href="https://crawlee.dev/python/api/class/KeyValueStore"&gt;key-value stores&lt;/a&gt;).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Running on the Apify platform&lt;/h2&gt; 
&lt;p&gt;Crawlee is open-source and runs anywhere, but since it's developed by &lt;a href="https://apify.com"&gt;Apify&lt;/a&gt;, it's easy to set up on the Apify platform and run in the cloud. Visit the &lt;a href="https://docs.apify.com/sdk/python/"&gt;Apify SDK website&lt;/a&gt; to learn more about deploying Crawlee to the Apify platform.&lt;/p&gt; 
&lt;h2&gt;Support&lt;/h2&gt; 
&lt;p&gt;If you find any bug or issue with Crawlee, please &lt;a href="https://github.com/apify/crawlee-python/issues"&gt;submit an issue on GitHub&lt;/a&gt;. For questions, you can ask on &lt;a href="https://stackoverflow.com/questions/tagged/apify"&gt;Stack Overflow&lt;/a&gt;, in GitHub Discussions or you can join our &lt;a href="https://discord.com/invite/jyEM2PRvMU"&gt;Discord server&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Your code contributions are welcome, and you'll be praised for eternity! If you have any ideas for improvements, either submit an issue or create a pull request. For contribution guidelines and the code of conduct, see &lt;a href="https://github.com/apify/crawlee-python/raw/master/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the Apache License 2.0 - see the &lt;a href="https://github.com/apify/crawlee-python/raw/master/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>frappe/erpnext</title>
      <link>https://github.com/frappe/erpnext</link>
      <description>&lt;p&gt;Free and Open Source Enterprise Resource Planning (ERP)&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;a href="https://frappe.io/erpnext"&gt; &lt;img src="https://raw.githubusercontent.com/frappe/erpnext/develop/erpnext/public/images/v16/erpnext.svg?sanitize=true" alt="ERPNext Logo" height="80px" width="80xp" /&gt; &lt;/a&gt; 
 &lt;h2&gt;ERPNext&lt;/h2&gt; 
 &lt;p align="center"&gt; &lt;/p&gt;
 &lt;p&gt;Powerful, Intuitive and Open-Source ERP&lt;/p&gt; 
 &lt;p&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://frappe.school"&gt;&lt;img src="https://img.shields.io/badge/Frappe%20School-Learn%20ERPNext-blue?style=flat-square" alt="Learn on Frappe School" /&gt;&lt;/a&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://github.com/frappe/erpnext/actions/workflows/server-tests-mariadb.yml"&gt;&lt;img src="https://github.com/frappe/erpnext/actions/workflows/server-tests-mariadb.yml/badge.svg?event=schedule" alt="CI" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/frappe/erpnext-worker"&gt;&lt;img src="https://img.shields.io/docker/pulls/frappe/erpnext-worker.svg?sanitize=true" alt="docker pulls" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/frappe/erpnext/develop/erpnext/public/images/v16/hero_image.png" /&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://erpnext-demo.frappe.cloud/api/method/erpnext_demo.erpnext_demo.auth.login_demo"&gt;Live Demo&lt;/a&gt; - 
 &lt;a href="https://frappe.io/erpnext"&gt;Website&lt;/a&gt; - 
 &lt;a href="https://docs.frappe.io/erpnext/"&gt;Documentation&lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;ERPNext&lt;/h2&gt; 
&lt;p&gt;100% Open-Source ERP system to help you run your business.&lt;/p&gt; 
&lt;h3&gt;Motivation&lt;/h3&gt; 
&lt;p&gt;Running a business is a complex task - handling invoices, tracking stock, managing personnel and even more ad-hoc activities. In a market where software is sold separately to manage each of these tasks, ERPNext does all of the above and more, for free.&lt;/p&gt; 
&lt;h3&gt;Key Features&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Accounting&lt;/strong&gt;: All the tools you need to manage cash flow in one place, right from recording transactions to summarizing and analyzing financial reports.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Order Management&lt;/strong&gt;: Track inventory levels, replenish stock, and manage sales orders, customers, suppliers, shipments, deliverables, and order fulfillment.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Manufacturing&lt;/strong&gt;: Simplifies the production cycle, helps track material consumption, exhibits capacity planning, handles subcontracting, and more!&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Asset Management&lt;/strong&gt;: From purchase to perishment, IT infrastructure to equipment. Cover every branch of your organization, all in one centralized system.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Projects&lt;/strong&gt;: Delivery both internal and external Projects on time, budget and Profitability. Track tasks, timesheets, and issues by project.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details open&gt; 
 &lt;summary&gt;More&lt;/summary&gt; 
 &lt;img src="https://erpnext.com/files/v16_bom.png" /&gt; 
 &lt;img src="https://erpnext.com/files/v16_stock_summary.png" /&gt; 
 &lt;img src="https://erpnext.com/files/v16_job_card.png" /&gt; 
 &lt;img src="https://erpnext.com/files/v16_tasks.png" /&gt; 
&lt;/details&gt; 
&lt;h3&gt;Under the Hood&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/frappe/frappe"&gt;&lt;strong&gt;Frappe Framework&lt;/strong&gt;&lt;/a&gt;: A full-stack web application framework written in Python and Javascript. The framework provides a robust foundation for building web applications, including a database abstraction layer, user authentication, and a REST API.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/frappe/frappe-ui"&gt;&lt;strong&gt;Frappe UI&lt;/strong&gt;&lt;/a&gt;: A Vue-based UI library, to provide a modern user interface. The Frappe UI library provides a variety of components that can be used to build single-page applications on top of the Frappe Framework.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Production Setup&lt;/h2&gt; 
&lt;h3&gt;Managed Hosting&lt;/h3&gt; 
&lt;p&gt;You can try &lt;a href="https://frappecloud.com"&gt;Frappe Cloud&lt;/a&gt;, a simple, user-friendly and sophisticated &lt;a href="https://github.com/frappe/press"&gt;open-source&lt;/a&gt; platform to host Frappe applications with peace of mind.&lt;/p&gt; 
&lt;p&gt;It takes care of installation, setup, upgrades, monitoring, maintenance and support of your Frappe deployments. It is a fully featured developer platform with an ability to manage and control multiple Frappe deployments.&lt;/p&gt; 
&lt;div&gt; 
 &lt;a href="https://erpnext-demo.frappe.cloud/app/home" target="_blank"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="https://frappe.io/files/try-on-fc-white.png" /&gt; 
   &lt;img src="https://frappe.io/files/try-on-fc-black.png" alt="Try on Frappe Cloud" height="28" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h3&gt;Self-Hosted&lt;/h3&gt; 
&lt;h4&gt;Docker&lt;/h4&gt; 
&lt;p&gt;Prerequisites: docker, docker-compose, git. Refer &lt;a href="https://docs.docker.com"&gt;Docker Documentation&lt;/a&gt; for more details on Docker setup.&lt;/p&gt; 
&lt;p&gt;Run following commands:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;git clone https://github.com/frappe/frappe_docker
cd frappe_docker
docker compose -f pwd.yml up -d
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After a couple of minutes, site should be accessible on your localhost port: 8080. Use below default login credentials to access the site.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Username: Administrator&lt;/li&gt; 
 &lt;li&gt;Password: admin&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See &lt;a href="https://github.com/frappe/frappe_docker?tab=readme-ov-file#to-run-on-arm64-architecture-follow-this-instructions"&gt;Frappe Docker&lt;/a&gt; for ARM based docker setup.&lt;/p&gt; 
&lt;h2&gt;Development Setup&lt;/h2&gt; 
&lt;h3&gt;Manual Install&lt;/h3&gt; 
&lt;p&gt;The Easy Way: our install script for bench will install all dependencies (e.g. MariaDB). See &lt;a href="https://github.com/frappe/bench"&gt;https://github.com/frappe/bench&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;p&gt;New passwords will be created for the ERPNext "Administrator" user, the MariaDB root user, and the frappe user (the script displays the passwords and saves them to ~/frappe_passwords.txt).&lt;/p&gt; 
&lt;h3&gt;Local&lt;/h3&gt; 
&lt;p&gt;To setup the repository locally follow the steps mentioned below:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Setup bench by following the &lt;a href="https://frappeframework.com/docs/user/en/installation"&gt;Installation Steps&lt;/a&gt; and start the server&lt;/p&gt; &lt;pre&gt;&lt;code&gt;bench start
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;In a separate terminal window, run the following commands:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Create a new site
bench new-site erpnext.localhost
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Get the ERPNext app and install it&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Get the ERPNext app
bench get-app https://github.com/frappe/erpnext

# Install the app
bench --site erpnext.localhost install-app erpnext
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Open the URL &lt;code&gt;http://erpnext.localhost:8000/app&lt;/code&gt; in your browser, you should see the app running&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Learning and community&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://school.frappe.io"&gt;Frappe School&lt;/a&gt; - Learn Frappe Framework and ERPNext from the various courses by the maintainers or from the community.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.erpnext.com/"&gt;Official documentation&lt;/a&gt; - Extensive documentation for ERPNext.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://discuss.erpnext.com/"&gt;Discussion Forum&lt;/a&gt; - Engage with community of ERPNext users and service providers.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://erpnext_public.t.me"&gt;Telegram Group&lt;/a&gt; - Get instant help from huge community of users.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://github.com/frappe/erpnext/wiki/Issue-Guidelines"&gt;Issue Guidelines&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://erpnext.com/security"&gt;Report Security Vulnerabilities&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/frappe/erpnext/wiki/Contribution-Guidelines"&gt;Pull Request Requirements&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://crowdin.com/project/frappe"&gt;Translations&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Logo and Trademark Policy&lt;/h2&gt; 
&lt;p&gt;Please read our &lt;a href="https://raw.githubusercontent.com/frappe/erpnext/develop/TRADEMARK_POLICY.md"&gt;Logo and Trademark Policy&lt;/a&gt;.&lt;/p&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;div align="center" style="padding-top: 0.75rem;"&gt; 
 &lt;a href="https://frappe.io" target="_blank"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="https://frappe.io/files/Frappe-white.png" /&gt; 
   &lt;img src="https://frappe.io/files/Frappe-black.png" alt="Frappe Technologies" height="28" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>hacksider/Deep-Live-Cam</title>
      <link>https://github.com/hacksider/Deep-Live-Cam</link>
      <description>&lt;p&gt;real time face swap and one-click video deepfake with only a single image&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt;Deep-Live-Cam&lt;/h1&gt; 
&lt;p align="center"&gt; Real-time face swap and video deepfake with a single click and only a single image. &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/11395" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/11395" alt="hacksider%2FDeep-Live-Cam | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/media/demo.gif" alt="Demo GIF" width="800" /&gt; &lt;/p&gt; 
&lt;h2&gt;Disclaimer&lt;/h2&gt; 
&lt;p&gt;This deepfake software is designed to be a productive tool for the AI-generated media industry. It can assist artists in animating custom characters, creating engaging content, and even using models for clothing design.&lt;/p&gt; 
&lt;p&gt;We are aware of the potential for unethical applications and are committed to preventative measures. A built-in check prevents the program from processing inappropriate media (nudity, graphic content, sensitive material like war footage, etc.). We will continue to develop this project responsibly, adhering to the law and ethics. We may shut down the project or add watermarks if legally required.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Ethical Use: Users are expected to use this software responsibly and legally. If using a real person's face, obtain their consent and clearly label any output as a deepfake when sharing online.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Content Restrictions: The software includes built-in checks to prevent processing inappropriate media, such as nudity, graphic content, or sensitive material.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Legal Compliance: We adhere to all relevant laws and ethical guidelines. If legally required, we may shut down the project or add watermarks to the output.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;User Responsibility: We are not responsible for end-user actions. Users must ensure their use of the software aligns with ethical standards and legal requirements.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;By using this software, you agree to these terms and commit to using it in a manner that respects the rights and dignity of others.&lt;/p&gt; 
&lt;p&gt;Users are expected to use this software responsibly and legally. If using a real person's face, obtain their consent and clearly label any output as a deepfake when sharing online. We are not responsible for end-user actions.&lt;/p&gt; 
&lt;h2&gt;Exclusive v2.2 Quick Start - Pre-built (Windows/Mac Silicon)&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://deeplivecam.net/index.php/quickstart"&gt; &lt;img src="https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/media/Download.png" width="285" height="77" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;a href="https://deeplivecam.net/index.php/quickstart"&gt; &lt;h5&gt;This is the fastest build you can get if you have a discrete NVIDIA or AMD GPU or Mac Silicon, And you'll receive special priority support.&lt;/h5&gt; &lt;h6&gt;These Pre-builts are perfect for non-technical users or those who don't have time to, or can't manually install all the requirements. Just a heads-up: this is an open-source project, so you can also install it manually.&lt;/h6&gt; &lt;h2&gt;TLDR; Live Deepfake in just 3 Clicks&lt;/h2&gt; &lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/af825228-852c-411b-b787-ffd9aac72fc6" alt="easysteps" /&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Select a face&lt;/li&gt; 
  &lt;li&gt;Select which camera to use&lt;/li&gt; 
  &lt;li&gt;Press live!&lt;/li&gt; 
 &lt;/ol&gt; &lt;h2&gt;Features &amp;amp; Uses - Everything is in real-time&lt;/h2&gt; &lt;h3&gt;Mouth Mask&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Retain your original mouth for accurate movement using Mouth Mask&lt;/strong&gt;&lt;/p&gt; &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/media/ludwig.gif" alt="resizable-gif" /&gt; &lt;/p&gt; &lt;h3&gt;Face Mapping&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Use different faces on multiple subjects simultaneously&lt;/strong&gt;&lt;/p&gt; &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/media/streamers.gif" alt="face_mapping_source" /&gt; &lt;/p&gt; &lt;h3&gt;Your Movie, Your Face&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Watch movies with any face in real-time&lt;/strong&gt;&lt;/p&gt; &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/media/movie.gif" alt="movie" /&gt; &lt;/p&gt; &lt;h3&gt;Live Show&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Run Live shows and performances&lt;/strong&gt;&lt;/p&gt; &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/media/live_show.gif" alt="show" /&gt; &lt;/p&gt; &lt;h3&gt;Memes&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Create Your Most Viral Meme Yet&lt;/strong&gt;&lt;/p&gt; &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/hacksider/Deep-Live-Cam/main/media/meme.gif" alt="show" width="450" /&gt; &lt;br /&gt; &lt;sub&gt;Created using Many Faces feature in Deep-Live-Cam&lt;/sub&gt; &lt;/p&gt; &lt;h3&gt;Omegle&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Surprise people on Omegle&lt;/strong&gt;&lt;/p&gt; &lt;p align="center"&gt; 
  &lt;video src="https://github.com/user-attachments/assets/2e9b9b82-fa04-4b70-9f56-b1f68e7672d0" width="450" controls&gt;&lt;/video&gt; &lt;/p&gt; &lt;h2&gt;Installation (Manual)&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;Please be aware that the installation requires technical skills and is not for beginners. Consider downloading the quickstart version.&lt;/strong&gt;&lt;/p&gt; &lt;/a&gt;
&lt;details&gt;
 &lt;a href="https://deeplivecam.net/index.php/quickstart"&gt; &lt;summary&gt;Click to see the process&lt;/summary&gt; &lt;h3&gt;Installation&lt;/h3&gt; &lt;p&gt;This is more likely to work on your computer but will be slower as it utilizes the CPU.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. Set up Your Platform&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Python (3.11 recommended)&lt;/li&gt; 
   &lt;li&gt;pip&lt;/li&gt; 
   &lt;li&gt;git&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=OlNWCpFdVMA"&gt;ffmpeg&lt;/a&gt; - &lt;code&gt;iex (irm ffmpeg.tc.ht)&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://visualstudio.microsoft.com/visual-cpp-build-tools/"&gt;Visual Studio 2022 Runtimes (Windows)&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt;&lt;/a&gt; 
 &lt;p&gt;&lt;strong&gt;2. Clone the Repository&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/hacksider/Deep-Live-Cam.git
cd Deep-Live-Cam
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;3. Download the Models&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/hacksider/deep-live-cam/resolve/main/GFPGANv1.4.pth"&gt;GFPGANv1.4&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/hacksider/deep-live-cam/resolve/main/inswapper_128_fp16.onnx"&gt;inswapper_128_fp16.onnx&lt;/a&gt;&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;Place these files in the "&lt;strong&gt;models&lt;/strong&gt;" folder.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;4. Install Dependencies&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;We highly recommend using a &lt;code&gt;venv&lt;/code&gt; to avoid issues.&lt;/p&gt; 
 &lt;p&gt;For Windows:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;python -m venv venv
venv\Scripts\activate
pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;For Linux:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Ensure you use the installed Python 3.10
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;For macOS:&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;Apple Silicon (M1/M2/M3) requires specific setup:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Install Python 3.11 (specific version is important)
brew install python@3.11

# Install tkinter package (required for the GUI)
brew install python-tk@3.10

# Create and activate virtual environment with Python 3.11
python3.11 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;** In case something goes wrong and you need to reinstall the virtual environment **&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Deactivate the virtual environment
rm -rf venv

# Reinstall the virtual environment
python -m venv venv
source venv/bin/activate

# install the dependencies again
pip install -r requirements.txt

# gfpgan and basicsrs issue fix
pip install git+https://github.com/xinntao/BasicSR.git@master
pip uninstall gfpgan -y
pip install git+https://github.com/TencentARC/GFPGAN.git@master
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Run:&lt;/strong&gt; If you don't have a GPU, you can run Deep-Live-Cam using &lt;code&gt;python run.py&lt;/code&gt;. Note that initial execution will download models (~300MB).&lt;/p&gt; 
 &lt;h3&gt;GPU Acceleration&lt;/h3&gt; 
 &lt;p&gt;&lt;strong&gt;CUDA Execution Provider (Nvidia)&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Install &lt;a href="https://developer.nvidia.com/cuda-12-8-0-download-archive"&gt;CUDA Toolkit 12.8.0&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Install &lt;a href="https://developer.nvidia.com/rdp/cudnn-archive"&gt;cuDNN v8.9.7 for CUDA 12.x&lt;/a&gt; (required for onnxruntime-gpu): 
   &lt;ul&gt; 
    &lt;li&gt;Download cuDNN v8.9.7 for CUDA 12.x&lt;/li&gt; 
    &lt;li&gt;Make sure the cuDNN bin directory is in your system PATH&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;Install dependencies:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;pip install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128
pip uninstall onnxruntime onnxruntime-gpu
pip install onnxruntime-gpu==1.21.0
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="3"&gt; 
  &lt;li&gt;Usage:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;python run.py --execution-provider cuda
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;CoreML Execution Provider (Apple Silicon)&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;Apple Silicon (M1/M2/M3) specific installation:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Make sure you've completed the macOS setup above using Python 3.10.&lt;/li&gt; 
  &lt;li&gt;Install dependencies:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;pip uninstall onnxruntime onnxruntime-silicon
pip install onnxruntime-silicon==1.13.1
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="3"&gt; 
  &lt;li&gt;Usage (important: specify Python 3.10):&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;python3.10 run.py --execution-provider coreml
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Important Notes for macOS:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;You &lt;strong&gt;must&lt;/strong&gt; use Python 3.10, not newer versions like 3.11 or 3.13&lt;/li&gt; 
  &lt;li&gt;Always run with &lt;code&gt;python3.10&lt;/code&gt; command not just &lt;code&gt;python&lt;/code&gt; if you have multiple Python versions installed&lt;/li&gt; 
  &lt;li&gt;If you get error about &lt;code&gt;_tkinter&lt;/code&gt; missing, reinstall the tkinter package: &lt;code&gt;brew reinstall python-tk@3.10&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;If you get model loading errors, check that your models are in the correct folder&lt;/li&gt; 
  &lt;li&gt;If you encounter conflicts with other Python versions, consider uninstalling them: &lt;pre&gt;&lt;code class="language-bash"&gt;# List all installed Python versions
brew list | grep python

# Uninstall conflicting versions if needed
brew uninstall --ignore-dependencies python@3.11 python@3.13

# Keep only Python 3.11
brew cleanup
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;CoreML Execution Provider (Apple Legacy)&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Install dependencies:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;pip uninstall onnxruntime onnxruntime-coreml
pip install onnxruntime-coreml==1.21.0
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="2"&gt; 
  &lt;li&gt;Usage:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;python run.py --execution-provider coreml
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;DirectML Execution Provider (Windows)&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Install dependencies:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;pip uninstall onnxruntime onnxruntime-directml
pip install onnxruntime-directml==1.21.0
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="2"&gt; 
  &lt;li&gt;Usage:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;python run.py --execution-provider directml
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;OpenVINO™ Execution Provider (Intel)&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Install dependencies:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;pip uninstall onnxruntime onnxruntime-openvino
pip install onnxruntime-openvino==1.21.0
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="2"&gt; 
  &lt;li&gt;Usage:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;python run.py --execution-provider openvino
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;1. Image/Video Mode&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Execute &lt;code&gt;python run.py&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Choose a source face image and a target image/video.&lt;/li&gt; 
 &lt;li&gt;Click "Start".&lt;/li&gt; 
 &lt;li&gt;The output will be saved in a directory named after the target video.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;2. Webcam Mode&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Execute &lt;code&gt;python run.py&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Select a source face image.&lt;/li&gt; 
 &lt;li&gt;Click "Live".&lt;/li&gt; 
 &lt;li&gt;Wait for the preview to appear (10-30 seconds).&lt;/li&gt; 
 &lt;li&gt;Use a screen capture tool like OBS to stream.&lt;/li&gt; 
 &lt;li&gt;To change the face, select a new source image.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Command Line Arguments (Unmaintained)&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;options:
  -h, --help                                               show this help message and exit
  -s SOURCE_PATH, --source SOURCE_PATH                     select a source image
  -t TARGET_PATH, --target TARGET_PATH                     select a target image or video
  -o OUTPUT_PATH, --output OUTPUT_PATH                     select output file or directory
  --frame-processor FRAME_PROCESSOR [FRAME_PROCESSOR ...]  frame processors (choices: face_swapper, face_enhancer, ...)
  --keep-fps                                               keep original fps
  --keep-audio                                             keep original audio
  --keep-frames                                            keep temporary frames
  --many-faces                                             process every face
  --map-faces                                              map source target faces
  --mouth-mask                                             mask the mouth region
  --video-encoder {libx264,libx265,libvpx-vp9}             adjust output video encoder
  --video-quality [0-51]                                   adjust output video quality
  --live-mirror                                            the live camera display as you see it in the front-facing camera frame
  --live-resizable                                         the live camera frame is resizable
  --max-memory MAX_MEMORY                                  maximum amount of RAM in GB
  --execution-provider {cpu} [{cpu} ...]                   available execution provider (choices: cpu, ...)
  --execution-threads EXECUTION_THREADS                    number of execution threads
  -v, --version                                            show program's version number and exit
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Looking for a CLI mode? Using the -s/--source argument will make the run program in cli mode.&lt;/p&gt; 
&lt;h2&gt;Press&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;We are always open to criticism and are ready to improve, that's why we didn't cherry-pick anything.&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://arstechnica.com/information-technology/2024/08/new-ai-tool-enables-real-time-face-swapping-on-webcams-raising-fraud-concerns/"&gt;&lt;em&gt;"Deep-Live-Cam goes viral, allowing anyone to become a digital doppelganger"&lt;/em&gt;&lt;/a&gt; - Ars Technica&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://dataconomy.com/2024/08/15/what-is-deep-live-cam-github-deepfake/"&gt;&lt;em&gt;"Thanks Deep Live Cam, shapeshifters are among us now"&lt;/em&gt;&lt;/a&gt; - Dataconomy&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.newsbytesapp.com/news/science/deep-live-cam-ai-impersonation-tool-goes-viral/story"&gt;&lt;em&gt;"This free AI tool lets you become anyone during video-calls"&lt;/em&gt;&lt;/a&gt; - NewsBytes&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.creativebloq.com/ai/ok-this-viral-ai-live-stream-software-is-truly-terrifying"&gt;&lt;em&gt;"OK, this viral AI live stream software is truly terrifying"&lt;/em&gt;&lt;/a&gt; - Creative Bloq&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://petapixel.com/2024/08/14/deep-live-cam-deepfake-ai-tool-lets-you-become-anyone-in-a-video-call-with-single-photo-mark-zuckerberg-jd-vance-elon-musk/"&gt;&lt;em&gt;"Deepfake AI Tool Lets You Become Anyone in a Video Call With Single Photo"&lt;/em&gt;&lt;/a&gt; - PetaPixel&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.techeblog.com/deep-live-cam-ai-transform-face/"&gt;&lt;em&gt;"Deep-Live-Cam Uses AI to Transform Your Face in Real-Time, Celebrities Included"&lt;/em&gt;&lt;/a&gt; - TechEBlog&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://telegrafi.com/en/a-tool-that-makes-you-look-like-anyone-during-a-video-call-is-going-viral-on-the-Internet/"&gt;&lt;em&gt;"An AI tool that "makes you look like anyone" during a video call is going viral online"&lt;/em&gt;&lt;/a&gt; - Telegrafi&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://decrypt.co/244565/this-deepfake-tool-turning-images-into-livestreams-is-topping-the-github-charts"&gt;&lt;em&gt;"This Deepfake Tool Turning Images Into Livestreams is Topping the GitHub Charts"&lt;/em&gt;&lt;/a&gt; - Emerge&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.digitalmusicnews.com/2024/08/15/face-swapping-ai-real-time-mimic/"&gt;&lt;em&gt;"New Real-Time Face-Swapping AI Allows Anyone to Mimic Famous Faces"&lt;/em&gt;&lt;/a&gt; - Digital Music News&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.diyphotography.net/this-real-time-webcam-deepfake-tool-raises-alarms-about-the-future-of-identity-theft/"&gt;&lt;em&gt;"This real-time webcam deepfake tool raises alarms about the future of identity theft"&lt;/em&gt;&lt;/a&gt; - DIYPhotography&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/watch?time_continue=1074&amp;amp;v=py4Tc-Y8BcY"&gt;&lt;em&gt;"That's Crazy, Oh God. That's Fucking Freaky Dude... That's So Wild Dude"&lt;/em&gt;&lt;/a&gt; - SomeOrdinaryGamers&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/live/mFsCe7AIxq8?feature=shared&amp;amp;t=2686"&gt;&lt;em&gt;"Alright look look look, now look chat, we can do any face we want to look like chat"&lt;/em&gt;&lt;/a&gt; - IShowSpeed&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=wnCghLjqv3s&amp;amp;t=551s"&gt;&lt;em&gt;"They do a pretty good job matching poses, expression and even the lighting"&lt;/em&gt;&lt;/a&gt; - TechLinked (LTT)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.golem.de/news/deepfakes-als-sean-connery-an-der-redaktionskonferenz-teilnahm-2408-188172.html"&gt;&lt;em&gt;"Als Sean Connery an der Redaktionskonferenz teilnahm"&lt;/em&gt;&lt;/a&gt; - Golem.de (German)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Credits&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://ffmpeg.org/"&gt;ffmpeg&lt;/a&gt;: for making video-related operations easy&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/deepinsight"&gt;deepinsight&lt;/a&gt;: for their &lt;a href="https://github.com/deepinsight/insightface"&gt;insightface&lt;/a&gt; project which provided a well-made library and models. Please be reminded that the &lt;a href="https://github.com/deepinsight/insightface?tab=readme-ov-file#license"&gt;use of the model is for non-commercial research purposes only&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/havok2-htwo"&gt;havok2-htwo&lt;/a&gt;: for sharing the code for webcam&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/GosuDRM"&gt;GosuDRM&lt;/a&gt;: for the open version of roop&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pereiraroland26"&gt;pereiraroland26&lt;/a&gt;: Multiple faces support&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/vic4key"&gt;vic4key&lt;/a&gt;: For supporting/contributing to this project&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/kier007"&gt;kier007&lt;/a&gt;: for improving the user experience&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/qitianai"&gt;qitianai&lt;/a&gt;: for multi-lingual support&lt;/li&gt; 
 &lt;li&gt;and &lt;a href="https://github.com/hacksider/Deep-Live-Cam/graphs/contributors"&gt;all developers&lt;/a&gt; behind libraries used in this project.&lt;/li&gt; 
 &lt;li&gt;Footnote: Please be informed that the base author of the code is &lt;a href="https://github.com/s0md3v/roop"&gt;s0md3v&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;All the wonderful users who helped make this project go viral by starring the repo ❤️&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://github.com/hacksider/Deep-Live-Cam/stargazers"&gt;&lt;img src="https://reporoster.com/stars/hacksider/Deep-Live-Cam" alt="Stargazers" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Contributions&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://repobeats.axiom.co/api/embed/fec8e29c45dfdb9c5916f3a7830e1249308d20e1.svg?sanitize=true" alt="Alt" title="Repobeats analytics image" /&gt;&lt;/p&gt; 
&lt;h2&gt;Stars to the Moon 🚀&lt;/h2&gt; 
&lt;a href="https://star-history.com/#hacksider/deep-live-cam&amp;amp;Date"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=hacksider/deep-live-cam&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=hacksider/deep-live-cam&amp;amp;type=Date" /&gt; 
  &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=hacksider/deep-live-cam&amp;amp;type=Date" /&gt; 
 &lt;/picture&gt; &lt;/a&gt;</description>
    </item>
    
    <item>
      <title>Byaidu/PDFMathTranslate</title>
      <link>https://github.com/Byaidu/PDFMathTranslate</link>
      <description>&lt;p&gt;PDF scientific paper translation with preserved formats - 基于 AI 完整保留排版的 PDF 文档全文双语翻译，支持 Google/DeepL/Ollama/OpenAI 等服务，提供 CLI/GUI/MCP/Docker/Zotero&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p&gt;English | &lt;a href="https://raw.githubusercontent.com/Byaidu/PDFMathTranslate/main/docs/README_zh-CN.md"&gt;简体中文&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/Byaidu/PDFMathTranslate/main/docs/README_zh-TW.md"&gt;繁體中文&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/Byaidu/PDFMathTranslate/main/docs/README_ja-JP.md"&gt;日本語&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/Byaidu/PDFMathTranslate/main/docs/README_ko-KR.md"&gt;한국어&lt;/a&gt;&lt;/p&gt; 
 &lt;img src="https://raw.githubusercontent.com/Byaidu/PDFMathTranslate/main/docs/images/banner.png" width="320px" alt="PDF2ZH" /&gt; 
 &lt;h2 id="title"&gt;PDFMathTranslate&lt;/h2&gt; 
 &lt;p&gt; 
  &lt;!-- PyPI --&gt; &lt;a href="https://pypi.org/project/pdf2zh/"&gt; &lt;img src="https://img.shields.io/pypi/v/pdf2zh" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/projects/pdf2zh"&gt; &lt;img src="https://static.pepy.tech/badge/pdf2zh" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/byaidu/pdf2zh"&gt; &lt;img src="https://img.shields.io/docker/pulls/byaidu/pdf2zh" /&gt;&lt;/a&gt; &lt;a href="https://hellogithub.com/repository/8ec2cfd3ef744762bf531232fa32bc47" target="_blank"&gt;&lt;img src="https://api.hellogithub.com/v1/widgets/recommend.svg?rid=8ec2cfd3ef744762bf531232fa32bc47&amp;amp;claim_uid=JQ0yfeBNjaTuqDU&amp;amp;theme=small" alt="Featured｜HelloGitHub" /&gt;&lt;/a&gt; &lt;a href="https://gitcode.com/Byaidu/PDFMathTranslate/overview"&gt; &lt;img src="https://gitcode.com/Byaidu/PDFMathTranslate/star/badge.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/spaces/reycn/PDFMathTranslate-Docker"&gt; &lt;img src="https://img.shields.io/badge/%F0%9F%A4%97-Online%20Demo-FF9E0D" /&gt;&lt;/a&gt; &lt;a href="https://www.modelscope.cn/studios/AI-ModelScope/PDFMathTranslate"&gt; &lt;img src="https://img.shields.io/badge/ModelScope-Demo-blue" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Byaidu/PDFMathTranslate/pulls"&gt; &lt;img src="https://img.shields.io/badge/contributions-welcome-green" /&gt;&lt;/a&gt; &lt;a href="https://t.me/+Z9_SgnxmsmA5NzBl"&gt; &lt;img src="https://img.shields.io/badge/Telegram-2CA5E0?style=flat-squeare&amp;amp;logo=telegram&amp;amp;logoColor=white" /&gt;&lt;/a&gt; 
  &lt;!-- License --&gt; &lt;a href="https://raw.githubusercontent.com/Byaidu/PDFMathTranslate/main/LICENSE"&gt; &lt;img src="https://img.shields.io/github/license/Byaidu/PDFMathTranslate" /&gt;&lt;/a&gt; &lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/12424" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/12424" alt="Byaidu%2FPDFMathTranslate | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2 id="updates"&gt;1. What does this do?&lt;/h2&gt; 
&lt;p&gt;Scientific PDF document translation preserving layouts.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;📊 Preserve formulas, charts, table of contents, and annotations.&lt;/li&gt; 
 &lt;li&gt;🌐 Support &lt;a href="https://raw.githubusercontent.com/Byaidu/PDFMathTranslate/main/#usage"&gt;multiple languages&lt;/a&gt;, and diverse &lt;a href="https://raw.githubusercontent.com/Byaidu/PDFMathTranslate/main/#usage"&gt;translation services&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;🤖 Provides &lt;a href="https://raw.githubusercontent.com/Byaidu/PDFMathTranslate/main/#usage"&gt;commandline tool&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/Byaidu/PDFMathTranslate/main/#install"&gt;interactive user interface&lt;/a&gt;, and &lt;a href="https://raw.githubusercontent.com/Byaidu/PDFMathTranslate/main/#install"&gt;Docker&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/Byaidu/PDFMathTranslate/main/docs/images/preview.gif" width="80%" /&gt; 
&lt;/div&gt; 
&lt;h2 id="updates"&gt;2. Recent Updates&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;[May 9, 2025] pdf2zh 2.0 Preview Version &lt;a href="https://github.com/Byaidu/PDFMathTranslate/issues/586"&gt;#586&lt;/a&gt;: The Windows ZIP file and Docker image are now available.&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;[!NOTE]&lt;/p&gt; 
   &lt;p&gt;2.0 Moved to a new repository under the organization: &lt;a href="https://github.com/PDFMathTranslate/PDFMathTranslate-next"&gt;PDFMathTranslate/PDFMathTranslate-next&lt;/a&gt;&lt;/p&gt; 
   &lt;p&gt;Version 2.0 official release has been published.&lt;/p&gt; 
  &lt;/blockquote&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[Mar. 3, 2025] Experimental support for the new backend &lt;a href="https://github.com/funstory-ai/BabelDOC"&gt;BabelDOC&lt;/a&gt; WebUI added as an experimental option (by &lt;a href="https://github.com/awwaawwa"&gt;@awwaawwa&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[Feb. 22 2025] Better release CI and well-packaged windows-amd64 exe (by &lt;a href="https://github.com/awwaawwa"&gt;@awwaawwa&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2 id="use-section"&gt;3. Use 🌟&lt;/h2&gt; 
&lt;h3 id="demo"&gt;3.1 Online Service 🌟&lt;/h3&gt; 
&lt;p&gt;You can try our application out using either of the following demos:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://pdf2zh.com/"&gt;Public free service&lt;/a&gt; online without installation &lt;em&gt;(recommended)&lt;/em&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://app.immersivetranslate.com/babel-doc/"&gt;Immersive Translate - BabelDOC&lt;/a&gt; 1000 free pages per month. &lt;em&gt;(recommended)&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/spaces/reycn/PDFMathTranslate-Docker"&gt;Demo hosted on HuggingFace&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.modelscope.cn/studios/AI-ModelScope/PDFMathTranslate"&gt;Demo hosted on ModelScope&lt;/a&gt; without installation.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Note that the computing resources of the demo are limited, so please avoid abusing them.&lt;/p&gt; 
&lt;h3 id="install"&gt;3.2 Local Installation&lt;/h3&gt; 
&lt;p&gt;For different use cases, we provide distinct methods to use our program:&lt;/p&gt; 
&lt;details open&gt; 
 &lt;summary&gt;3.2.1 Python: Install using uv&lt;/summary&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;Python installed (3.10 &amp;lt;= version &amp;lt;= 3.12)&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Install our package:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pip install uv
uv tool install --python 3.12 pdf2zh
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Execute translation, files generated in &lt;a href="https://chatgpt.com/share/6745ed36-9acc-800e-8a90-59204bd13444"&gt;current working directory&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pdf2zh document.pdf
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;3.2.2 Python: Install using pip&lt;/summary&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;Python installed (3.10 &amp;lt;= version &amp;lt;= 3.12)&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Install our package:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pip install pdf2zh
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Execute translation, files generated in &lt;a href="https://chatgpt.com/share/6745ed36-9acc-800e-8a90-59204bd13444"&gt;current working directory&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pdf2zh document.pdf
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;3.3.3 Python: Graphic user interface&lt;/summary&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;Python installed (3.10 &amp;lt;= version &amp;lt;= 3.12)&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Install our package:&lt;/p&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;pip install pdf2zh
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="3"&gt; 
  &lt;li&gt; &lt;p&gt;Start using in browser:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pdf2zh -i
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;If your browser has not been started automatically, goto&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;http://localhost:7860/
&lt;/code&gt;&lt;/pre&gt; &lt;img src="https://raw.githubusercontent.com/Byaidu/PDFMathTranslate/main/docs/images/gui.gif" width="500" /&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/Byaidu/PDFMathTranslate/main/docs/README_GUI.md"&gt;documentation for GUI&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;3.2.4 Application: On Windows&lt;/summary&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;Download pdf2zh-version-win64.zip from &lt;a href="https://github.com/Byaidu/PDFMathTranslate/releases"&gt;release page&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Unzip and double-click &lt;code&gt;pdf2zh.exe&lt;/code&gt; to run.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;[!TIP]&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;If you're using Windows and cannot open the file after downloading, please install &lt;a href="https://aka.ms/vs/17/release/vc_redist.x64.exe"&gt;vc_redist.x64.exe&lt;/a&gt; and try again.&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/blockquote&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;3.2.5 Reference manager: Zotero Plugin&lt;/summary&gt; 
 &lt;p&gt;See &lt;a href="https://github.com/guaguastandup/zotero-pdf2zh"&gt;Zotero PDF2zh&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;3.2.6 Docker: Containerized Deployment&lt;/summary&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;Pull and run:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;docker pull byaidu/pdf2zh
docker run -d -p 7860:7860 byaidu/pdf2zh
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Open in browser:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;http://localhost:7860/
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;For docker deployment on cloud service:&lt;/p&gt; 
 &lt;div&gt; 
  &lt;a href="https://www.heroku.com/deploy?template=https://github.com/Byaidu/PDFMathTranslate"&gt; &lt;img src="https://www.herokucdn.com/deploy/button.svg?sanitize=true" alt="Deploy" height="26" /&gt;&lt;/a&gt; 
  &lt;a href="https://render.com/deploy"&gt; &lt;img src="https://render.com/images/deploy-to-render-button.svg?sanitize=true" alt="Deploy to Koyeb" height="26" /&gt;&lt;/a&gt; 
  &lt;a href="https://zeabur.com/templates/5FQIGX?referralCode=reycn"&gt; &lt;img src="https://zeabur.com/button.svg?sanitize=true" alt="Deploy on Zeabur" height="26" /&gt;&lt;/a&gt; 
  &lt;a href="https://template.sealos.io/deploy?templateName=pdf2zh"&gt; &lt;img src="https://sealos.io/Deploy-on-Sealos.svg?sanitize=true" alt="Deploy on Sealos" height="26" /&gt;&lt;/a&gt; 
  &lt;a href="https://app.koyeb.com/deploy?type=git&amp;amp;builder=buildpack&amp;amp;repository=github.com/Byaidu/PDFMathTranslate&amp;amp;branch=main&amp;amp;name=pdf-math-translate"&gt; &lt;img src="https://www.koyeb.com/static/images/deploy/button.svg?sanitize=true" alt="Deploy to Koyeb" height="26" /&gt;&lt;/a&gt; 
 &lt;/div&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;[!TIP]&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;If you cannot access Docker Hub, please try the image on &lt;a href="https://github.com/Byaidu/PDFMathTranslate/pkgs/container/pdfmathtranslate"&gt;GitHub Container Registry&lt;/a&gt;.&lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;pre&gt;&lt;code class="language-bash"&gt;docker pull ghcr.io/byaidu/pdfmathtranslate
docker run -d -p 7860:7860 ghcr.io/byaidu/pdfmathtranslate
&lt;/code&gt;&lt;/pre&gt; 
 &lt;/blockquote&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;3.2.* Solutions for network issues in installation&lt;/summary&gt; 
 &lt;p&gt;Users in specific regions may encounter network difficulties when loading the AI model. The current program relies on the AI model (&lt;code&gt;wybxc/DocLayout-YOLO-DocStructBench-onnx&lt;/code&gt;), and some users are unable to download it due to these network issues.&lt;/p&gt; 
 &lt;p&gt;To address issues with downloading this model, use the following environment variable as a workaround:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;set HF_ENDPOINT=https://hf-mirror.com
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;For PowerShell user:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;$env:HF_ENDPOINT = https://hf-mirror.com
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;If the solution does not work to you / you encountered other issues, please refer to &lt;a href="https://github.com/Byaidu/PDFMathTranslate/wiki#-faq--%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98"&gt;Frequently Asked Questions&lt;/a&gt;.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2 id="usage"&gt;4. Technical Details&lt;/h2&gt; 
&lt;h3&gt;4.1 Advanced options&lt;/h3&gt; 
&lt;p&gt;Execute the translation command in the command line to generate the translated document &lt;code&gt;example-mono.pdf&lt;/code&gt; and the bilingual document &lt;code&gt;example-dual.pdf&lt;/code&gt; in the current working directory. Use Google as the default translation service. More support translation services can find &lt;a href="https://github.com/Byaidu/PDFMathTranslate/raw/main/docs/ADVANCED.md#services"&gt;HERE&lt;/a&gt;.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/Byaidu/PDFMathTranslate/main/docs/images/cmd.explained.png" width="580px" alt="cmd" /&gt; 
&lt;p&gt;In the following table, we list all advanced options for reference:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Option&lt;/th&gt; 
   &lt;th&gt;Function&lt;/th&gt; 
   &lt;th&gt;Example&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;files&lt;/td&gt; 
   &lt;td&gt;Local files&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;pdf2zh ~/local.pdf&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;links&lt;/td&gt; 
   &lt;td&gt;Online files&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;pdf2zh http://arxiv.org/paper.pdf&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-i&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/Byaidu/PDFMathTranslate/main/#gui"&gt;Enter GUI&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;pdf2zh -i&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-p&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Byaidu/PDFMathTranslate/raw/main/docs/ADVANCED.md#partial"&gt;Partial document translation&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;pdf2zh example.pdf -p 1&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-li&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Byaidu/PDFMathTranslate/raw/main/docs/ADVANCED.md#languages"&gt;Source language&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;pdf2zh example.pdf -li en&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-lo&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Byaidu/PDFMathTranslate/raw/main/docs/ADVANCED.md#languages"&gt;Target language&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;pdf2zh example.pdf -lo zh&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-s&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Byaidu/PDFMathTranslate/raw/main/docs/ADVANCED.md#services"&gt;Translation service&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;pdf2zh example.pdf -s deepl&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-t&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Byaidu/PDFMathTranslate/raw/main/docs/ADVANCED.md#threads"&gt;Multi-threads&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;pdf2zh example.pdf -t 1&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-o&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Output dir&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;pdf2zh example.pdf -o output&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-f&lt;/code&gt;, &lt;code&gt;-c&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Byaidu/PDFMathTranslate/raw/main/docs/ADVANCED.md#exceptions"&gt;Exceptions&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;pdf2zh example.pdf -f "(MS.*)"&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-cp&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Compatibility Mode&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;pdf2zh example.pdf --compatible&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--skip-subset-fonts&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Byaidu/PDFMathTranslate/raw/main/docs/ADVANCED.md#font-subset"&gt;Skip font subset&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;pdf2zh example.pdf --skip-subset-fonts&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--ignore-cache&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Byaidu/PDFMathTranslate/raw/main/docs/ADVANCED.md#cache"&gt;Ignore translate cache&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;pdf2zh example.pdf --ignore-cache&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--share&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Public link&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;pdf2zh -i --share&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--authorized&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Byaidu/PDFMathTranslate/raw/main/docs/ADVANCED.md#auth"&gt;Authorization&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;pdf2zh -i --authorized users.txt [auth.html]&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--prompt&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Byaidu/PDFMathTranslate/raw/main/docs/ADVANCED.md#prompt"&gt;Custom Prompt&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;pdf2zh --prompt [prompt.txt]&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--onnx&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;[Use Custom DocLayout-YOLO ONNX model]&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;pdf2zh --onnx [onnx/model/path]&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--serverport&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;[Use Custom WebUI port]&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;pdf2zh --serverport 7860&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--dir&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;[batch translate]&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;pdf2zh --dir /path/to/translate/&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--config&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Byaidu/PDFMathTranslate/raw/main/docs/ADVANCED.md#cofig"&gt;configuration file&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;pdf2zh --config /path/to/config/config.json&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--serverport&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;[custom gradio server port]&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;pdf2zh --serverport 7860&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--babeldoc&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Use Experimental backend &lt;a href="https://funstory-ai.github.io/BabelDOC/"&gt;BabelDOC&lt;/a&gt; to translate&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;pdf2zh --babeldoc&lt;/code&gt; -s openai example.pdf&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--mcp&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Enable MCP STDIO mode&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;pdf2zh --mcp&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--sse&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Enable MCP SSE mode&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;pdf2zh --mcp --sse&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;For detailed explanations, please refer to our document about &lt;a href="https://raw.githubusercontent.com/Byaidu/PDFMathTranslate/main/docs/ADVANCED.md"&gt;Advanced Usage&lt;/a&gt; for a full list of each option.&lt;/p&gt; 
&lt;h3 id="downstream"&gt;4.2 Downstream Development&lt;/h3&gt; For downstream applications, please refer to our document about [API Details](./docs/APIS.md) for further information about: 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Byaidu/PDFMathTranslate/main/docs/APIS.md#api-python"&gt;Python API&lt;/a&gt;, how to use the program in other Python programs&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Byaidu/PDFMathTranslate/main/docs/APIS.md#api-http"&gt;HTTP API&lt;/a&gt;, how to communicate with a server with the program installed&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3 id="downstream"&gt;4.3 Differences between two major forks&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/Byaidu/PDFMathTranslate"&gt;Byaidu/PDFMathTranslate&lt;/a&gt;: The present and the original project for stable release.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/PDFMathTranslate/PDFMathTranslate-next"&gt;PDFMathTranslate/PDFMathTranslate-next&lt;/a&gt;: A fork with web-ui and additional features. This fork handles a large number of marginal cases, improves PDF compatibility, and optimizes cross-column and cross-page semantic consistency, dynamic scaling, and dynamic scaling consistency, among many other translation quality improvements. However, this fork is intended solely for development and does not address compatibility issues and is not designed for community-contributions.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2 id="information"&gt;5. Project Information&lt;/h2&gt; 
&lt;h3 id="citation"&gt;5.1 Citation&lt;/h3&gt; 
&lt;p&gt;This work has been accepted by the &lt;em&gt;Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing: System Demonstrations&lt;/em&gt; (EMNLP 2025).&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Pre-print version: &lt;a href="https://arxiv.org/abs/2507.03009"&gt;PDFMathTranslate: Scientific Document Translation Preserving Layouts&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;@online{ouyang2025pdfmathtranslate,
  title = {{{PDFMathTranslate}}: {{Scientific Document Translation Preserving Layouts}}},
  shorttitle = {{{PDFMathTranslate}}},
  author = {Ouyang, Rongxin and Chu, Chang and Xin, Zhikuang and Ma, Xiangyao},
  date = {2025-07-08},
  eprint = {2507.03009},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2507.03009},
  url = {http://arxiv.org/abs/2507.03009},
  urldate = {2025-08-27},
  pubstate = {prepublished}
}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;The citation for the EMNLP proceedings will be provided upon release.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- ```
@inproceedings{zheng-etal-2024-openresearcher,
    title = "{O}pen{R}esearcher: Unleashing {AI} for Accelerated Scientific Research",
    author = "Ouyang, Rongxin  and
      Chu, Chang and
      Xin, Zhikuang and
      Ma, Xiangyao",
    editor = "TBD",
    booktitle = "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = nov,
    year = "2025",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/TBD/",
    doi = "TBD",
    pages = "TBD",
    abstract = "Language barriers in scientific documents hinder the diffusion and development of science and technologies. However, prior efforts in translating such documents largely overlooked the information in layouts. To bridge the gap, we introduce PDFMathTranslate, the world’s first open-source software for translating scientific documents while preserving layouts. Leveraging the most recent advances in large language models and precise layout detection, we contribute to the community with key improvements in precision, flexibility, and efficiency. The work is open-sourced at https://github.com/byaidu/pdfmathtranslate with more than 222k downloads."
}
``` --&gt; 
&lt;h3 id="acknowledgement"&gt;5.2 Acknowledgement&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://immersivetranslate.com"&gt;Immersive Translation&lt;/a&gt; sponsors monthly Pro membership redemption codes for active contributors to this project, see details at: &lt;a href="https://github.com/funstory-ai/BabelDOC/raw/main/docs/CONTRIBUTOR_REWARD.md"&gt;CONTRIBUTOR_REWARD.md&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;New backend: &lt;a href="https://github.com/funstory-ai/BabelDOC"&gt;BabelDOC&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Document merging: &lt;a href="https://github.com/pymupdf/PyMuPDF"&gt;PyMuPDF&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Document parsing: &lt;a href="https://github.com/pdfminer/pdfminer.six"&gt;Pdfminer.six&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Document extraction: &lt;a href="https://github.com/opendatalab/MinerU"&gt;MinerU&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Document Preview: &lt;a href="https://github.com/freddyaboulton/gradio-pdf"&gt;Gradio PDF&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Multi-threaded translation: &lt;a href="https://github.com/SUSYUSTC/MathTranslate"&gt;MathTranslate&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Layout parsing: &lt;a href="https://github.com/opendatalab/DocLayout-YOLO"&gt;DocLayout-YOLO&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Document standard: &lt;a href="https://zxyle.github.io/PDF-Explained/"&gt;PDF Explained&lt;/a&gt;, &lt;a href="https://pdfa.org/resource/pdf-cheat-sheets/"&gt;PDF Cheat Sheets&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Multilingual Font: &lt;a href="https://github.com/satbyy/go-noto-universal"&gt;Go Noto Universal&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3 id="contrib"&gt;5.3 Contributors&lt;/h3&gt; 
&lt;a href="https://github.com/Byaidu/PDFMathTranslate/graphs/contributors"&gt; &lt;img src="https://opencollective.com/PDFMathTranslate/contributors.svg?width=890&amp;amp;button=false" /&gt; &lt;/a&gt; 
&lt;p&gt;&lt;img src="https://repobeats.axiom.co/api/embed/dfa7583da5332a11468d686fbd29b92320a6a869.svg?sanitize=true" alt="Alt" title="Repobeats analytics image" /&gt;&lt;/p&gt; 
&lt;p&gt;For details on how to contribute, please consult the &lt;a href="https://github.com/Byaidu/PDFMathTranslate/wiki/Contribution-Guide---%E8%B4%A1%E7%8C%AE%E6%8C%87%E5%8D%97"&gt;Contribution Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h3 id="star_hist"&gt;5.4 Star History&lt;/h3&gt; 
&lt;a href="https://star-history.com/#Byaidu/PDFMathTranslate&amp;amp;Date"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=Byaidu/PDFMathTranslate&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=Byaidu/PDFMathTranslate&amp;amp;type=Date" /&gt; 
  &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=Byaidu/PDFMathTranslate&amp;amp;type=Date" /&gt; 
 &lt;/picture&gt; &lt;/a&gt;</description>
    </item>
    
    <item>
      <title>volatilityfoundation/volatility3</title>
      <link>https://github.com/volatilityfoundation/volatility3</link>
      <description>&lt;p&gt;Volatility 3.0 development&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Volatility 3: The volatile memory extraction framework&lt;/h1&gt; 
&lt;p&gt;Volatility is the world's most widely used framework for extracting digital artifacts from volatile memory (RAM) samples. The extraction techniques are performed completely independent of the system being investigated but offer visibility into the runtime state of the system. The framework is intended to introduce people to the techniques and complexities associated with extracting digital artifacts from volatile memory samples and provide a platform for further work into this exciting area of research.&lt;/p&gt; 
&lt;p&gt;In 2019, the Volatility Foundation released a complete rewrite of the framework, Volatility 3. The project was intended to address many of the technical and performance challenges associated with the original code base that became apparent over the previous 10 years. Another benefit of the rewrite is that Volatility 3 could be released under a custom license that was more aligned with the goals of the Volatility community, the Volatility Software License (VSL). See the &lt;a href="https://www.volatilityfoundation.org/license/vsl-v1.0"&gt;LICENSE&lt;/a&gt; file for more details.&lt;/p&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Install the required dependencies:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;pip install --user -e ".[full]"
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;See available options:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;vol -h
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;To get more information on a Windows memory sample and to make sure Volatility supports that sample type, run &lt;code&gt;vol -f &amp;lt;imagepath&amp;gt; windows.info&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;vol -f /home/user/samples/stuxnet.vmem windows.info
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run some other plugins. The &lt;code&gt;-f&lt;/code&gt; or &lt;code&gt;--single-location&lt;/code&gt; is not strictly required, but most plugins expect a single sample. Some also require/accept other options. Run &lt;code&gt;vol &amp;lt;plugin&amp;gt; -h&lt;/code&gt; for more information on a particular command.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Installing&lt;/h2&gt; 
&lt;p&gt;Volatility 3 requires Python 3.8.0 or later and is published on the &lt;a href="https://pypi.org/project/volatility3"&gt;PyPi registry&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip install volatility3
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you want to use the latest development version of Volatility 3 we recommend you manually clone this repository and install an editable version of the project. We recommend you use a virtual environment to keep installed dependencies separate from system packages.&lt;/p&gt; 
&lt;p&gt;The latest stable version of Volatility will always be the &lt;code&gt;stable&lt;/code&gt; branch of the GitHub repository. The default branch is &lt;code&gt;develop&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;git clone https://github.com/volatilityfoundation/volatility3.git
cd volatility3/
python3 -m venv venv &amp;amp;&amp;amp; . venv/bin/activate
pip install -e ".[dev]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Symbol Tables&lt;/h2&gt; 
&lt;p&gt;Symbol table packs for the various operating systems are available for download at:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://downloads.volatilityfoundation.org/volatility3/symbols/windows.zip"&gt;https://downloads.volatilityfoundation.org/volatility3/symbols/windows.zip&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://downloads.volatilityfoundation.org/volatility3/symbols/mac.zip"&gt;https://downloads.volatilityfoundation.org/volatility3/symbols/mac.zip&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://downloads.volatilityfoundation.org/volatility3/symbols/linux.zip"&gt;https://downloads.volatilityfoundation.org/volatility3/symbols/linux.zip&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;The hashes to verify whether any of the symbol pack files have downloaded successfully or have changed can be found at:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://downloads.volatilityfoundation.org/volatility3/symbols/SHA256SUMS"&gt;https://downloads.volatilityfoundation.org/volatility3/symbols/SHA256SUMS&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://downloads.volatilityfoundation.org/volatility3/symbols/SHA1SUMS"&gt;https://downloads.volatilityfoundation.org/volatility3/symbols/SHA1SUMS&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://downloads.volatilityfoundation.org/volatility3/symbols/MD5SUMS"&gt;https://downloads.volatilityfoundation.org/volatility3/symbols/MD5SUMS&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Symbol tables zip files must be placed, as named, into the &lt;code&gt;volatility3/symbols&lt;/code&gt; directory (or just the symbols directory next to the executable file).&lt;/p&gt; 
&lt;p&gt;Windows symbols that cannot be found will be queried, downloaded, generated and cached. Mac and Linux symbol tables must be manually produced by a tool such as &lt;a href="https://github.com/volatilityfoundation/dwarf2json"&gt;dwarf2json&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Important: The first run of volatility with new symbol files will require the cache to be updated. The symbol packs contain a large number of symbol files and so may take some time to update! However, this process only needs to be run once on each new symbol file, so assuming the pack stays in the same location will not need to be done again. Please also note it can be interrupted and next run will restart itself.&lt;/p&gt; 
&lt;p&gt;Please note: These are representative and are complete up to the point of creation for Windows and Mac. Due to the ease of compiling Linux kernels and the inability to uniquely distinguish them, an exhaustive set of Linux symbol tables cannot easily be supplied.&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;The framework is documented through doc strings and can be built using sphinx.&lt;/p&gt; 
&lt;p&gt;The latest generated copy of the documentation can be found at: &lt;a href="https://volatility3.readthedocs.io/en/latest/"&gt;https://volatility3.readthedocs.io/en/latest/&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Licensing and Copyright&lt;/h2&gt; 
&lt;p&gt;Copyright (C) 2007-2025 Volatility Foundation&lt;/p&gt; 
&lt;p&gt;All Rights Reserved&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.volatilityfoundation.org/license/vsl-v1.0"&gt;https://www.volatilityfoundation.org/license/vsl-v1.0&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Bugs and Support&lt;/h2&gt; 
&lt;p&gt;If you think you've found a bug, please report it at:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/volatilityfoundation/volatility3/issues"&gt;https://github.com/volatilityfoundation/volatility3/issues&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;In order to help us solve your issues as quickly as possible, please include the following information when filing a bug:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;The version of Volatility you're using&lt;/li&gt; 
 &lt;li&gt;The operating system used to run Volatility&lt;/li&gt; 
 &lt;li&gt;The version of Python used to run Volatility&lt;/li&gt; 
 &lt;li&gt;The suspected operating system of the memory sample&lt;/li&gt; 
 &lt;li&gt;The complete command line you used to run Volatility&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For community support, please join us on Slack:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.volatilityfoundation.org/slack"&gt;https://www.volatilityfoundation.org/slack&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Contact&lt;/h2&gt; 
&lt;p&gt;For information or requests, contact:&lt;/p&gt; 
&lt;p&gt;Volatility Foundation&lt;/p&gt; 
&lt;p&gt;Web: &lt;a href="https://www.volatilityfoundation.org"&gt;https://www.volatilityfoundation.org&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Blog: &lt;a href="https://volatility-labs.blogspot.com"&gt;https://volatility-labs.blogspot.com&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Email: volatility (at) volatilityfoundation (dot) org&lt;/p&gt; 
&lt;p&gt;Twitter: &lt;a href="https://twitter.com/volatility"&gt;@volatility&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>lukas-blecher/LaTeX-OCR</title>
      <link>https://github.com/lukas-blecher/LaTeX-OCR</link>
      <description>&lt;p&gt;pix2tex: Using a ViT to convert images of equations into LaTeX code.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;pix2tex - LaTeX OCR&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/lukas-blecher/LaTeX-OCR"&gt;&lt;img src="https://img.shields.io/github/license/lukas-blecher/LaTeX-OCR" alt="GitHub" /&gt;&lt;/a&gt; &lt;a href="https://pix2tex.readthedocs.io/en/latest/?badge=latest"&gt;&lt;img src="https://readthedocs.org/projects/pix2tex/badge/?version=latest" alt="Documentation Status" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/pix2tex"&gt;&lt;img src="https://img.shields.io/pypi/v/pix2tex?logo=pypi" alt="PyPI" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/pix2tex"&gt;&lt;img src="https://img.shields.io/pypi/dm/pix2tex?logo=pypi" alt="PyPI - Downloads" /&gt;&lt;/a&gt; &lt;a href="https://github.com/lukas-blecher/LaTeX-OCR/releases"&gt;&lt;img src="https://img.shields.io/github/downloads/lukas-blecher/LaTeX-OCR/total?color=blue&amp;amp;logo=github" alt="GitHub all releases" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/lukasblecher/pix2tex"&gt;&lt;img src="https://img.shields.io/docker/pulls/lukasblecher/pix2tex?logo=docker" alt="Docker Pulls" /&gt;&lt;/a&gt; &lt;a href="https://colab.research.google.com/github/lukas-blecher/LaTeX-OCR/blob/main/notebooks/LaTeX_OCR_test.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/spaces/lukbl/LaTeX-OCR"&gt;&lt;img src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue" alt="Hugging Face Spaces" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;The goal of this project is to create a learning based system that takes an image of a math formula and returns corresponding LaTeX code.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/55287601/109183599-69431f00-778e-11eb-9809-d42b9451e018.png" alt="header" /&gt;&lt;/p&gt; 
&lt;h2&gt;Using the model&lt;/h2&gt; 
&lt;p&gt;To run the model you need Python 3.7+&lt;/p&gt; 
&lt;p&gt;If you don't have PyTorch installed. Follow their instructions &lt;a href="https://pytorch.org/get-started/locally/"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Install the package &lt;code&gt;pix2tex&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install "pix2tex[gui]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Model checkpoints will be downloaded automatically.&lt;/p&gt; 
&lt;p&gt;There are three ways to get a prediction from an image.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;You can use the command line tool by calling &lt;code&gt;pix2tex&lt;/code&gt;. Here you can parse already existing images from the disk and images in your clipboard.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Thanks to &lt;a href="https://github.com/katie-lim"&gt;@katie-lim&lt;/a&gt;, you can use a nice user interface as a quick way to get the model prediction. Just call the GUI with &lt;code&gt;latexocr&lt;/code&gt;. From here you can take a screenshot and the predicted latex code is rendered using &lt;a href="https://www.mathjax.org/"&gt;MathJax&lt;/a&gt; and copied to your clipboard.&lt;/p&gt; &lt;p&gt;Under linux, it is possible to use the GUI with &lt;code&gt;gnome-screenshot&lt;/code&gt; (which comes with multiple monitor support). For other Wayland compositers, &lt;code&gt;grim&lt;/code&gt; and &lt;code&gt;slurp&lt;/code&gt; will be used for wlroots-based Wayland compositers and &lt;code&gt;spectacle&lt;/code&gt; for KDE Plasma. Note that &lt;code&gt;gnome-screenshot&lt;/code&gt; is not compatible with wlroots or Qt based compositers. Since &lt;code&gt;gnome-screenshot&lt;/code&gt; will be preferred when available, you may have to set the environment variable &lt;code&gt;SCREENSHOT_TOOL&lt;/code&gt; to &lt;code&gt;grim&lt;/code&gt; or &lt;code&gt;spectacle&lt;/code&gt; in these cases (other available values are &lt;code&gt;gnome-screenshot&lt;/code&gt; and &lt;code&gt;pil&lt;/code&gt;).&lt;/p&gt; &lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/55287601/117812740-77b7b780-b262-11eb-81f6-fc19766ae2ae.gif" alt="demo" /&gt;&lt;/p&gt; &lt;p&gt;If the model is unsure about the what's in the image it might output a different prediction every time you click "Retry". With the &lt;code&gt;temperature&lt;/code&gt; parameter you can control this behavior (low temperature will produce the same result).&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;You can use an API. This has additional dependencies. Install via &lt;code&gt;pip install -U "pix2tex[api]"&lt;/code&gt; and run&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;python -m pix2tex.api.run
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;to start a &lt;a href="https://streamlit.io/"&gt;Streamlit&lt;/a&gt; demo that connects to the API at port 8502. There is also a docker image available for the API: &lt;a href="https://hub.docker.com/r/lukasblecher/pix2tex"&gt;https://hub.docker.com/r/lukasblecher/pix2tex&lt;/a&gt; &lt;a href="https://hub.docker.com/r/lukasblecher/pix2tex"&gt;&lt;img src="https://img.shields.io/docker/image-size/lukasblecher/pix2tex?logo=docker" alt="Docker Image Size (latest by date)" /&gt;&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;docker pull lukasblecher/pix2tex:api
docker run --rm -p 8502:8502 lukasblecher/pix2tex:api
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To also run the streamlit demo run&lt;/p&gt; &lt;pre&gt;&lt;code&gt;docker run --rm -it -p 8501:8501 --entrypoint python lukasblecher/pix2tex:api pix2tex/api/run.py
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;and navigate to &lt;a href="http://localhost:8501/"&gt;http://localhost:8501/&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Use from within Python&lt;/p&gt; &lt;pre&gt;&lt;code class="language-python"&gt;from PIL import Image
from pix2tex.cli import LatexOCR

img = Image.open('path/to/image.png')
model = LatexOCR()
print(model(img))
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;The model works best with images of smaller resolution. That's why I added a preprocessing step where another neural network predicts the optimal resolution of the input image. This model will automatically resize the custom image to best resemble the training data and thus increase performance of images found in the wild. Still it's not perfect and might not be able to handle huge images optimally, so don't zoom in all the way before taking a picture.&lt;/p&gt; 
&lt;p&gt;Always double check the result carefully. You can try to redo the prediction with an other resolution if the answer was wrong.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Want to use the package?&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;I'm trying to compile a documentation right now.&lt;/p&gt; 
&lt;p&gt;Visit here: &lt;a href="https://pix2tex.readthedocs.io/"&gt;https://pix2tex.readthedocs.io/&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Training the model &lt;a href="https://colab.research.google.com/github/lukas-blecher/LaTeX-OCR/blob/main/notebooks/LaTeX_OCR_training.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;Install a couple of dependencies &lt;code&gt;pip install "pix2tex[train]"&lt;/code&gt;.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;First we need to combine the images with their ground truth labels. I wrote a dataset class (which needs further improving) that saves the relative paths to the images with the LaTeX code they were rendered with. To generate the dataset pickle file run&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code&gt;python -m pix2tex.dataset.dataset --equations path_to_textfile --images path_to_images --out dataset.pkl
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To use your own tokenizer pass it via &lt;code&gt;--tokenizer&lt;/code&gt; (See below).&lt;/p&gt; 
&lt;p&gt;You can find my generated training data on the &lt;a href="https://drive.google.com/drive/folders/13CA4vAmOmD_I_dSbvLp-Lf0s6KiaNfuO"&gt;Google Drive&lt;/a&gt; as well (formulae.zip - images, math.txt - labels). Repeat the step for the validation and test data. All use the same label text file.&lt;/p&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Edit the &lt;code&gt;data&lt;/code&gt; (and &lt;code&gt;valdata&lt;/code&gt;) entry in the config file to the newly generated &lt;code&gt;.pkl&lt;/code&gt; file. Change other hyperparameters if you want to. See &lt;code&gt;pix2tex/model/settings/config.yaml&lt;/code&gt; for a template.&lt;/li&gt; 
 &lt;li&gt;Now for the actual training run&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code&gt;python -m pix2tex.train --config path_to_config_file
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you want to use your own data you might be interested in creating your own tokenizer with&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;python -m pix2tex.dataset.dataset --equations path_to_textfile --vocab-size 8000 --out tokenizer.json
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Don't forget to update the path to the tokenizer in the config file and set &lt;code&gt;num_tokens&lt;/code&gt; to your vocabulary size.&lt;/p&gt; 
&lt;h2&gt;Model&lt;/h2&gt; 
&lt;p&gt;The model consist of a ViT [&lt;a href="https://raw.githubusercontent.com/lukas-blecher/LaTeX-OCR/main/#References"&gt;1&lt;/a&gt;] encoder with a ResNet backbone and a Transformer [&lt;a href="https://raw.githubusercontent.com/lukas-blecher/LaTeX-OCR/main/#References"&gt;2&lt;/a&gt;] decoder.&lt;/p&gt; 
&lt;h3&gt;Performance&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;BLEU score&lt;/th&gt; 
   &lt;th&gt;normed edit distance&lt;/th&gt; 
   &lt;th&gt;token accuracy&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;0.88&lt;/td&gt; 
   &lt;td&gt;0.10&lt;/td&gt; 
   &lt;td&gt;0.60&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Data&lt;/h2&gt; 
&lt;p&gt;We need paired data for the network to learn. Luckily there is a lot of LaTeX code on the internet, e.g. &lt;a href="https://www.wikipedia.org"&gt;wikipedia&lt;/a&gt;, &lt;a href="https://www.arxiv.org"&gt;arXiv&lt;/a&gt;. We also use the formulae from the &lt;a href="https://zenodo.org/record/56198#.V2px0jXT6eA"&gt;im2latex-100k&lt;/a&gt; [&lt;a href="https://raw.githubusercontent.com/lukas-blecher/LaTeX-OCR/main/#References"&gt;3&lt;/a&gt;] dataset. All of it can be found &lt;a href="https://drive.google.com/drive/folders/13CA4vAmOmD_I_dSbvLp-Lf0s6KiaNfuO"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Dataset Requirements&lt;/h3&gt; 
&lt;p&gt;In order to render the math in many different fonts we use XeLaTeX, generate a PDF and finally convert it to a PNG. For the last step we need to use some third party tools:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.ctan.org/pkg/xetex"&gt;XeLaTeX&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://imagemagick.org/"&gt;ImageMagick&lt;/a&gt; with &lt;a href="https://www.ghostscript.com/index.html"&gt;Ghostscript&lt;/a&gt;. (for converting pdf to png)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nodejs.org/"&gt;Node.js&lt;/a&gt; to run &lt;a href="https://github.com/KaTeX/KaTeX"&gt;KaTeX&lt;/a&gt; (for normalizing Latex code)&lt;/li&gt; 
 &lt;li&gt;Python 3.7+ &amp;amp; dependencies (specified in &lt;code&gt;setup.py&lt;/code&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Fonts&lt;/h3&gt; 
&lt;p&gt;Latin Modern Math, GFSNeohellenicMath.otf, Asana Math, XITS Math, Cambria Math&lt;/p&gt; 
&lt;h2&gt;TODO&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; add more evaluation metrics&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; create a GUI&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; add beam search&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; support handwritten formulae (kinda done, see training colab notebook)&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; reduce model size (distillation)&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; find optimal hyperparameters&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; tweak model structure&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; fix data scraping and scrape more data&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; trace the model (&lt;a href="https://github.com/lukas-blecher/LaTeX-OCR/issues/2"&gt;#2&lt;/a&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contribution&lt;/h2&gt; 
&lt;p&gt;Contributions of any kind are welcome.&lt;/p&gt; 
&lt;h2&gt;Acknowledgment&lt;/h2&gt; 
&lt;p&gt;Code taken and modified from &lt;a href="https://github.com/lucidrains"&gt;lucidrains&lt;/a&gt;, &lt;a href="https://github.com/rwightman/pytorch-image-models"&gt;rwightman&lt;/a&gt;, &lt;a href="https://github.com/harvardnlp/im2markup"&gt;im2markup&lt;/a&gt;, &lt;a href="https://github.com/soskek/arxiv_leaks"&gt;arxiv_leaks&lt;/a&gt;, &lt;a href="https://github.com/pkra/MathJax-single-file"&gt;pkra: Mathjax&lt;/a&gt;, &lt;a href="https://github.com/harupy/snipping-tool"&gt;harupy: snipping tool&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;References&lt;/h2&gt; 
&lt;p&gt;[1] &lt;a href="https://arxiv.org/abs/2010.11929"&gt;An Image is Worth 16x16 Words&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[2] &lt;a href="https://arxiv.org/abs/1706.03762"&gt;Attention Is All You Need&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[3] &lt;a href="https://arxiv.org/abs/1609.04938v2"&gt;Image-to-Markup Generation with Coarse-to-Fine Attention&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>harry0703/MoneyPrinterTurbo</title>
      <link>https://github.com/harry0703/MoneyPrinterTurbo</link>
      <description>&lt;p&gt;利用AI大模型，一键生成高清短视频 Generate short videos with one click using AI LLM.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h1 align="center"&gt;MoneyPrinterTurbo 💸&lt;/h1&gt; 
 &lt;p align="center"&gt; &lt;a href="https://github.com/harry0703/MoneyPrinterTurbo/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/harry0703/MoneyPrinterTurbo.svg?style=for-the-badge" alt="Stargazers" /&gt;&lt;/a&gt; &lt;a href="https://github.com/harry0703/MoneyPrinterTurbo/issues"&gt;&lt;img src="https://img.shields.io/github/issues/harry0703/MoneyPrinterTurbo.svg?style=for-the-badge" alt="Issues" /&gt;&lt;/a&gt; &lt;a href="https://github.com/harry0703/MoneyPrinterTurbo/network/members"&gt;&lt;img src="https://img.shields.io/github/forks/harry0703/MoneyPrinterTurbo.svg?style=for-the-badge" alt="Forks" /&gt;&lt;/a&gt; &lt;a href="https://github.com/harry0703/MoneyPrinterTurbo/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/harry0703/MoneyPrinterTurbo.svg?style=for-the-badge" alt="License" /&gt;&lt;/a&gt; &lt;/p&gt; 
 &lt;br /&gt; 
 &lt;h3&gt;简体中文 | &lt;a href="https://raw.githubusercontent.com/harry0703/MoneyPrinterTurbo/main/README-en.md"&gt;English&lt;/a&gt;&lt;/h3&gt; 
 &lt;div align="center"&gt; 
  &lt;a href="https://trendshift.io/repositories/8731" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/8731" alt="harry0703%2FMoneyPrinterTurbo | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; 
 &lt;/div&gt; 
 &lt;br /&gt; 只需提供一个视频 
 &lt;b&gt;主题&lt;/b&gt; 或 
 &lt;b&gt;关键词&lt;/b&gt; ，就可以全自动生成视频文案、视频素材、视频字幕、视频背景音乐，然后合成一个高清的短视频。 
 &lt;br /&gt; 
 &lt;h4&gt;Web界面&lt;/h4&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/harry0703/MoneyPrinterTurbo/main/docs/webui.jpg" alt="" /&gt;&lt;/p&gt; 
 &lt;h4&gt;API界面&lt;/h4&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/harry0703/MoneyPrinterTurbo/main/docs/api.jpg" alt="" /&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;特别感谢 🙏&lt;/h2&gt; 
&lt;p&gt;由于该项目的 &lt;strong&gt;部署&lt;/strong&gt; 和 &lt;strong&gt;使用&lt;/strong&gt;，对于一些小白用户来说，还是 &lt;strong&gt;有一定的门槛&lt;/strong&gt;，在此特别感谢 &lt;strong&gt;录咖（AI智能 多媒体服务平台）&lt;/strong&gt; 网站基于该项目，提供的免费&lt;code&gt;AI视频生成器&lt;/code&gt;服务，可以不用部署，直接在线使用，非常方便。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;中文版：&lt;a href="https://reccloud.cn"&gt;https://reccloud.cn&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;英文版：&lt;a href="https://reccloud.com"&gt;https://reccloud.com&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/harry0703/MoneyPrinterTurbo/main/docs/reccloud.cn.jpg" alt="" /&gt;&lt;/p&gt; 
&lt;h2&gt;感谢赞助 🙏&lt;/h2&gt; 
&lt;p&gt;感谢佐糖 &lt;a href="https://picwish.cn"&gt;https://picwish.cn&lt;/a&gt; 对该项目的支持和赞助，使得该项目能够持续的更新和维护。&lt;/p&gt; 
&lt;p&gt;佐糖专注于&lt;strong&gt;图像处理领域&lt;/strong&gt;，提供丰富的&lt;strong&gt;图像处理工具&lt;/strong&gt;，将复杂操作极致简化，真正实现让图像处理更简单。&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/harry0703/MoneyPrinterTurbo/main/docs/picwish.jpg" alt="picwish.jpg" /&gt;&lt;/p&gt; 
&lt;h2&gt;功能特性 🎯&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; 完整的 &lt;strong&gt;MVC架构&lt;/strong&gt;，代码 &lt;strong&gt;结构清晰&lt;/strong&gt;，易于维护，支持 &lt;code&gt;API&lt;/code&gt; 和 &lt;code&gt;Web界面&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; 支持视频文案 &lt;strong&gt;AI自动生成&lt;/strong&gt;，也可以&lt;strong&gt;自定义文案&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; 支持多种 &lt;strong&gt;高清视频&lt;/strong&gt; 尺寸 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; 竖屏 9:16，&lt;code&gt;1080x1920&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; 横屏 16:9，&lt;code&gt;1920x1080&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; 支持 &lt;strong&gt;批量视频生成&lt;/strong&gt;，可以一次生成多个视频，然后选择一个最满意的&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; 支持 &lt;strong&gt;视频片段时长&lt;/strong&gt; 设置，方便调节素材切换频率&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; 支持 &lt;strong&gt;中文&lt;/strong&gt; 和 &lt;strong&gt;英文&lt;/strong&gt; 视频文案&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; 支持 &lt;strong&gt;多种语音&lt;/strong&gt; 合成，可 &lt;strong&gt;实时试听&lt;/strong&gt; 效果&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; 支持 &lt;strong&gt;字幕生成&lt;/strong&gt;，可以调整 &lt;code&gt;字体&lt;/code&gt;、&lt;code&gt;位置&lt;/code&gt;、&lt;code&gt;颜色&lt;/code&gt;、&lt;code&gt;大小&lt;/code&gt;，同时支持&lt;code&gt;字幕描边&lt;/code&gt;设置&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; 支持 &lt;strong&gt;背景音乐&lt;/strong&gt;，随机或者指定音乐文件，可设置&lt;code&gt;背景音乐音量&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; 视频素材来源 &lt;strong&gt;高清&lt;/strong&gt;，而且 &lt;strong&gt;无版权&lt;/strong&gt;，也可以使用自己的 &lt;strong&gt;本地素材&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; 支持 &lt;strong&gt;OpenAI&lt;/strong&gt;、&lt;strong&gt;Moonshot&lt;/strong&gt;、&lt;strong&gt;Azure&lt;/strong&gt;、&lt;strong&gt;gpt4free&lt;/strong&gt;、&lt;strong&gt;one-api&lt;/strong&gt;、&lt;strong&gt;通义千问&lt;/strong&gt;、&lt;strong&gt;Google Gemini&lt;/strong&gt;、&lt;strong&gt;Ollama&lt;/strong&gt;、&lt;strong&gt;DeepSeek&lt;/strong&gt;、 &lt;strong&gt;文心一言&lt;/strong&gt;, &lt;strong&gt;Pollinations&lt;/strong&gt; 等多种模型接入 
  &lt;ul&gt; 
   &lt;li&gt;中国用户建议使用 &lt;strong&gt;DeepSeek&lt;/strong&gt; 或 &lt;strong&gt;Moonshot&lt;/strong&gt; 作为大模型提供商（国内可直接访问，不需要VPN。注册就送额度，基本够用）&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;后期计划 📅&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; GPT-SoVITS 配音支持&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; 优化语音合成，利用大模型，使其合成的声音，更加自然，情绪更加丰富&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; 增加视频转场效果，使其看起来更加的流畅&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; 增加更多视频素材来源，优化视频素材和文案的匹配度&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; 增加视频长度选项：短、中、长&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; 支持更多的语音合成服务商，比如 OpenAI TTS&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; 自动上传到YouTube平台&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;视频演示 📺&lt;/h2&gt; 
&lt;h3&gt;竖屏 9:16&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;
    &lt;g-emoji class="g-emoji" alias="arrow_forward"&gt;
     ▶️
    &lt;/g-emoji&gt; 《如何增加生活的乐趣》&lt;/th&gt; 
   &lt;th align="center"&gt;
    &lt;g-emoji class="g-emoji" alias="arrow_forward"&gt;
     ▶️
    &lt;/g-emoji&gt; 《金钱的作用》&lt;br /&gt;更真实的合成声音&lt;/th&gt; 
   &lt;th align="center"&gt;
    &lt;g-emoji class="g-emoji" alias="arrow_forward"&gt;
     ▶️
    &lt;/g-emoji&gt; 《生命的意义是什么》&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;
    &lt;video src="https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/a84d33d5-27a2-4aba-8fd0-9fb2bd91c6a6"&gt;&lt;/video&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;
    &lt;video src="https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/af2f3b0b-002e-49fe-b161-18ba91c055e8"&gt;&lt;/video&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;
    &lt;video src="https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/112c9564-d52b-4472-99ad-970b75f66476"&gt;&lt;/video&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;横屏 16:9&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;
    &lt;g-emoji class="g-emoji" alias="arrow_forward"&gt;
     ▶️
    &lt;/g-emoji&gt;《生命的意义是什么》&lt;/th&gt; 
   &lt;th align="center"&gt;
    &lt;g-emoji class="g-emoji" alias="arrow_forward"&gt;
     ▶️
    &lt;/g-emoji&gt;《为什么要运动》&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;
    &lt;video src="https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/346ebb15-c55f-47a9-a653-114f08bb8073"&gt;&lt;/video&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;
    &lt;video src="https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/271f2fae-8283-44a0-8aa0-0ed8f9a6fa87"&gt;&lt;/video&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;配置要求 📦&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;建议最低 CPU &lt;strong&gt;4核&lt;/strong&gt; 或以上，内存 &lt;strong&gt;4G&lt;/strong&gt; 或以上，显卡非必须&lt;/li&gt; 
 &lt;li&gt;Windows 10 或 MacOS 11.0 以上系统&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;快速开始 🚀&lt;/h2&gt; 
&lt;h3&gt;在 Google Colab 中运行&lt;/h3&gt; 
&lt;p&gt;免去本地环境配置，点击直接在 Google Colab 中快速体验 MoneyPrinterTurbo&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://colab.research.google.com/github/harry0703/MoneyPrinterTurbo/blob/main/docs/MoneyPrinterTurbo.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open in Colab" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Windows一键启动包&lt;/h3&gt; 
&lt;p&gt;下载一键启动包，解压直接使用（路径不要有 &lt;strong&gt;中文&lt;/strong&gt;、&lt;strong&gt;特殊字符&lt;/strong&gt;、&lt;strong&gt;空格&lt;/strong&gt;）&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;百度网盘（v1.2.6）: &lt;a href="https://pan.baidu.com/s/1wg0UaIyXpO3SqIpaq790SQ?pwd=sbqx"&gt;https://pan.baidu.com/s/1wg0UaIyXpO3SqIpaq790SQ?pwd=sbqx&lt;/a&gt; 提取码: sbqx&lt;/li&gt; 
 &lt;li&gt;Google Drive (v1.2.6): &lt;a href="https://drive.google.com/file/d/1HsbzfT7XunkrCrHw5ncUjFX8XX4zAuUh/view?usp=sharing"&gt;https://drive.google.com/file/d/1HsbzfT7XunkrCrHw5ncUjFX8XX4zAuUh/view?usp=sharing&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;下载后，建议先&lt;strong&gt;双击执行&lt;/strong&gt; &lt;code&gt;update.bat&lt;/code&gt; 更新到&lt;strong&gt;最新代码&lt;/strong&gt;，然后双击 &lt;code&gt;start.bat&lt;/code&gt; 启动&lt;/p&gt; 
&lt;p&gt;启动后，会自动打开浏览器（如果打开是空白，建议换成 &lt;strong&gt;Chrome&lt;/strong&gt; 或者 &lt;strong&gt;Edge&lt;/strong&gt; 打开）&lt;/p&gt; 
&lt;h2&gt;安装部署 📥&lt;/h2&gt; 
&lt;h3&gt;前提条件&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;尽量不要使用 &lt;strong&gt;中文路径&lt;/strong&gt;，避免出现一些无法预料的问题&lt;/li&gt; 
 &lt;li&gt;请确保你的 &lt;strong&gt;网络&lt;/strong&gt; 是正常的，VPN需要打开&lt;code&gt;全局流量&lt;/code&gt;模式&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;① 克隆代码&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;git clone https://github.com/harry0703/MoneyPrinterTurbo.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;② 修改配置文件（可选，建议启动后也可以在 WebUI 里面配置）&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;将 &lt;code&gt;config.example.toml&lt;/code&gt; 文件复制一份，命名为 &lt;code&gt;config.toml&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;按照 &lt;code&gt;config.toml&lt;/code&gt; 文件中的说明，配置好 &lt;code&gt;pexels_api_keys&lt;/code&gt; 和 &lt;code&gt;llm_provider&lt;/code&gt;，并根据 llm_provider 对应的服务商，配置相关的 API Key&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Docker部署 🐳&lt;/h3&gt; 
&lt;h4&gt;① 启动Docker&lt;/h4&gt; 
&lt;p&gt;如果未安装 Docker，请先安装 &lt;a href="https://www.docker.com/products/docker-desktop/"&gt;https://www.docker.com/products/docker-desktop/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;如果是Windows系统，请参考微软的文档：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://learn.microsoft.com/zh-cn/windows/wsl/install"&gt;https://learn.microsoft.com/zh-cn/windows/wsl/install&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://learn.microsoft.com/zh-cn/windows/wsl/tutorials/wsl-containers"&gt;https://learn.microsoft.com/zh-cn/windows/wsl/tutorials/wsl-containers&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;cd MoneyPrinterTurbo
docker-compose up
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;注意：最新版的docker安装时会自动以插件的形式安装docker compose，启动命令调整为docker compose up&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;② 访问Web界面&lt;/h4&gt; 
&lt;p&gt;打开浏览器，访问 &lt;a href="http://0.0.0.0:8501"&gt;http://0.0.0.0:8501&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;③ 访问API文档&lt;/h4&gt; 
&lt;p&gt;打开浏览器，访问 &lt;a href="http://0.0.0.0:8080/docs"&gt;http://0.0.0.0:8080/docs&lt;/a&gt; 或者 &lt;a href="http://0.0.0.0:8080/redoc"&gt;http://0.0.0.0:8080/redoc&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;手动部署 📦&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;视频教程&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;完整的使用演示：&lt;a href="https://v.douyin.com/iFhnwsKY/"&gt;https://v.douyin.com/iFhnwsKY/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;如何在Windows上部署：&lt;a href="https://v.douyin.com/iFyjoW3M"&gt;https://v.douyin.com/iFyjoW3M&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;① 创建虚拟环境&lt;/h4&gt; 
&lt;p&gt;建议使用 &lt;a href="https://conda.io/projects/conda/en/latest/user-guide/install/index.html"&gt;conda&lt;/a&gt; 创建 python 虚拟环境&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;git clone https://github.com/harry0703/MoneyPrinterTurbo.git
cd MoneyPrinterTurbo
conda create -n MoneyPrinterTurbo python=3.11
conda activate MoneyPrinterTurbo
pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;② 安装好 ImageMagick&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Windows:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;下载 &lt;a href="https://imagemagick.org/script/download.php"&gt;https://imagemagick.org/script/download.php&lt;/a&gt; 选择Windows版本，切记一定要选择 &lt;strong&gt;静态库&lt;/strong&gt; 版本，比如 ImageMagick-7.1.1-32-Q16-x64-&lt;strong&gt;static&lt;/strong&gt;.exe&lt;/li&gt; 
   &lt;li&gt;安装下载好的 ImageMagick，&lt;strong&gt;注意不要修改安装路径&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;修改 &lt;code&gt;配置文件 config.toml&lt;/code&gt; 中的 &lt;code&gt;imagemagick_path&lt;/code&gt; 为你的 &lt;strong&gt;实际安装路径&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;MacOS:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;brew install imagemagick
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Ubuntu&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;sudo apt-get install imagemagick
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;CentOS&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;sudo yum install ImageMagick
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;③ 启动Web界面 🌐&lt;/h4&gt; 
&lt;p&gt;注意需要到 MoneyPrinterTurbo 项目 &lt;code&gt;根目录&lt;/code&gt; 下执行以下命令&lt;/p&gt; 
&lt;h6&gt;Windows&lt;/h6&gt; 
&lt;pre&gt;&lt;code class="language-bat"&gt;webui.bat
&lt;/code&gt;&lt;/pre&gt; 
&lt;h6&gt;MacOS or Linux&lt;/h6&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;sh webui.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;启动后，会自动打开浏览器（如果打开是空白，建议换成 &lt;strong&gt;Chrome&lt;/strong&gt; 或者 &lt;strong&gt;Edge&lt;/strong&gt; 打开）&lt;/p&gt; 
&lt;h4&gt;④ 启动API服务 🚀&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;python main.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;启动后，可以查看 &lt;code&gt;API文档&lt;/code&gt; &lt;a href="http://127.0.0.1:8080/docs"&gt;http://127.0.0.1:8080/docs&lt;/a&gt; 或者 &lt;a href="http://127.0.0.1:8080/redoc"&gt;http://127.0.0.1:8080/redoc&lt;/a&gt; 直接在线调试接口，快速体验。&lt;/p&gt; 
&lt;h2&gt;语音合成 🗣&lt;/h2&gt; 
&lt;p&gt;所有支持的声音列表，可以查看：&lt;a href="https://raw.githubusercontent.com/harry0703/MoneyPrinterTurbo/main/docs/voice-list.txt"&gt;声音列表&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;2024-04-16 v1.1.2 新增了9种Azure的语音合成声音，需要配置API KEY，该声音合成的更加真实。&lt;/p&gt; 
&lt;h2&gt;字幕生成 📜&lt;/h2&gt; 
&lt;p&gt;当前支持2种字幕生成方式：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;edge&lt;/strong&gt;: 生成&lt;code&gt;速度快&lt;/code&gt;，性能更好，对电脑配置没有要求，但是质量可能不稳定&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;whisper&lt;/strong&gt;: 生成&lt;code&gt;速度慢&lt;/code&gt;，性能较差，对电脑配置有一定要求，但是&lt;code&gt;质量更可靠&lt;/code&gt;。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;可以修改 &lt;code&gt;config.toml&lt;/code&gt; 配置文件中的 &lt;code&gt;subtitle_provider&lt;/code&gt; 进行切换&lt;/p&gt; 
&lt;p&gt;建议使用 &lt;code&gt;edge&lt;/code&gt; 模式，如果生成的字幕质量不好，再切换到 &lt;code&gt;whisper&lt;/code&gt; 模式&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;注意：&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ol&gt; 
 &lt;li&gt;whisper 模式下需要到 HuggingFace 下载一个模型文件，大约 3GB 左右，请确保网络通畅&lt;/li&gt; 
 &lt;li&gt;如果留空，表示不生成字幕。&lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;由于国内无法访问 HuggingFace，可以使用以下方法下载 &lt;code&gt;whisper-large-v3&lt;/code&gt; 的模型文件&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;下载地址：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;百度网盘: &lt;a href="https://pan.baidu.com/s/11h3Q6tsDtjQKTjUu3sc5cA?pwd=xjs9"&gt;https://pan.baidu.com/s/11h3Q6tsDtjQKTjUu3sc5cA?pwd=xjs9&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;夸克网盘：&lt;a href="https://pan.quark.cn/s/3ee3d991d64b"&gt;https://pan.quark.cn/s/3ee3d991d64b&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;模型下载后解压，整个目录放到 &lt;code&gt;.\MoneyPrinterTurbo\models&lt;/code&gt; 里面， 最终的文件路径应该是这样: &lt;code&gt;.\MoneyPrinterTurbo\models\whisper-large-v3&lt;/code&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;MoneyPrinterTurbo  
  ├─models
  │   └─whisper-large-v3
  │          config.json
  │          model.bin
  │          preprocessor_config.json
  │          tokenizer.json
  │          vocabulary.json
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;背景音乐 🎵&lt;/h2&gt; 
&lt;p&gt;用于视频的背景音乐，位于项目的 &lt;code&gt;resource/songs&lt;/code&gt; 目录下。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;当前项目里面放了一些默认的音乐，来自于 YouTube 视频，如有侵权，请删除。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;字幕字体 🅰&lt;/h2&gt; 
&lt;p&gt;用于视频字幕的渲染，位于项目的 &lt;code&gt;resource/fonts&lt;/code&gt; 目录下，你也可以放进去自己的字体。&lt;/p&gt; 
&lt;h2&gt;常见问题 🤔&lt;/h2&gt; 
&lt;h3&gt;❓RuntimeError: No ffmpeg exe could be found&lt;/h3&gt; 
&lt;p&gt;通常情况下，ffmpeg 会被自动下载，并且会被自动检测到。 但是如果你的环境有问题，无法自动下载，可能会遇到如下错误：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;RuntimeError: No ffmpeg exe could be found.
Install ffmpeg on your system, or set the IMAGEIO_FFMPEG_EXE environment variable.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;此时你可以从 &lt;a href="https://www.gyan.dev/ffmpeg/builds/"&gt;https://www.gyan.dev/ffmpeg/builds/&lt;/a&gt; 下载ffmpeg，解压后，设置 &lt;code&gt;ffmpeg_path&lt;/code&gt; 为你的实际安装路径即可。&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-toml"&gt;[app]
# 请根据你的实际路径设置，注意 Windows 路径分隔符为 \\
ffmpeg_path = "C:\\Users\\harry\\Downloads\\ffmpeg.exe"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;❓ImageMagick的安全策略阻止了与临时文件@/tmp/tmpur5hyyto.txt相关的操作&lt;/h3&gt; 
&lt;p&gt;可以在ImageMagick的配置文件policy.xml中找到这些策略。 这个文件通常位于 /etc/ImageMagick-&lt;code&gt;X&lt;/code&gt;/ 或 ImageMagick 安装目录的类似位置。 修改包含&lt;code&gt;pattern="@"&lt;/code&gt;的条目，将&lt;code&gt;rights="none"&lt;/code&gt;更改为&lt;code&gt;rights="read|write"&lt;/code&gt;以允许对文件的读写操作。&lt;/p&gt; 
&lt;h3&gt;❓OSError: [Errno 24] Too many open files&lt;/h3&gt; 
&lt;p&gt;这个问题是由于系统打开文件数限制导致的，可以通过修改系统的文件打开数限制来解决。&lt;/p&gt; 
&lt;p&gt;查看当前限制&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;ulimit -n
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;如果过低，可以调高一些，比如&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;ulimit -n 10240
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;❓Whisper 模型下载失败，出现如下错误&lt;/h3&gt; 
&lt;p&gt;LocalEntryNotfoundEror: Cannot find an appropriate cached snapshotfolderfor the specified revision on the local disk and outgoing trafic has been disabled. To enablerepo look-ups and downloads online, pass 'local files only=False' as input.&lt;/p&gt; 
&lt;p&gt;或者&lt;/p&gt; 
&lt;p&gt;An error occured while synchronizing the model Systran/faster-whisper-large-v3 from the Hugging Face Hub: An error happened while trying to locate the files on the Hub and we cannot find the appropriate snapshot folder for the specified revision on the local disk. Please check your internet connection and try again. Trying to load the model directly from the local cache, if it exists.&lt;/p&gt; 
&lt;p&gt;解决方法：&lt;a href="https://raw.githubusercontent.com/harry0703/MoneyPrinterTurbo/main/#%E5%AD%97%E5%B9%95%E7%94%9F%E6%88%90-"&gt;点击查看如何从网盘手动下载模型&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;反馈建议 📢&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;可以提交 &lt;a href="https://github.com/harry0703/MoneyPrinterTurbo/issues"&gt;issue&lt;/a&gt; 或者 &lt;a href="https://github.com/harry0703/MoneyPrinterTurbo/pulls"&gt;pull request&lt;/a&gt;。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;许可证 📝&lt;/h2&gt; 
&lt;p&gt;点击查看 &lt;a href="https://raw.githubusercontent.com/harry0703/MoneyPrinterTurbo/main/LICENSE"&gt;&lt;code&gt;LICENSE&lt;/code&gt;&lt;/a&gt; 文件&lt;/p&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#harry0703/MoneyPrinterTurbo&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=harry0703/MoneyPrinterTurbo&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>YILING0013/AI_NovelGenerator</title>
      <link>https://github.com/YILING0013/AI_NovelGenerator</link>
      <description>&lt;p&gt;使用ai生成多章节的长篇小说，自动衔接上下文、伏笔&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;📖 自动小说生成工具&lt;/h1&gt; 
&lt;blockquote&gt; 
 &lt;ul&gt; 
  &lt;li&gt;当前没有什么精力维护该项目，本身该项目并无任何收益，以及临近毕业，有很多内容要忙，如果后面有时间的话，再考虑基于更新的技术去重构吧。——2025/9/24&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;✨ &lt;strong&gt;核心功能&lt;/strong&gt; ✨&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;功能模块&lt;/th&gt; 
    &lt;th&gt;关键能力&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;🎨 小说设定工坊&lt;/td&gt; 
    &lt;td&gt;世界观架构 / 角色设定 / 剧情蓝图&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;📖 智能章节生成&lt;/td&gt; 
    &lt;td&gt;多阶段生成保障剧情连贯性&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;🧠 状态追踪系统&lt;/td&gt; 
    &lt;td&gt;角色发展轨迹 / 伏笔管理系统&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;🔍 语义检索引擎&lt;/td&gt; 
    &lt;td&gt;基于向量的长程上下文一致性维护&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;📚 知识库集成&lt;/td&gt; 
    &lt;td&gt;支持本地文档参考&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;✅ 自动审校机制&lt;/td&gt; 
    &lt;td&gt;检测剧情矛盾与逻辑冲突&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;🖥 可视化工作台&lt;/td&gt; 
    &lt;td&gt;全流程GUI操作，配置/生成/审校一体化&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;一款基于大语言模型的多功能小说生成器，助您高效创作逻辑严谨、设定统一的长篇故事&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr /&gt; 
&lt;h2&gt;📑 目录导航&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/YILING0013/AI_NovelGenerator/main/#-%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87"&gt;环境准备&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/YILING0013/AI_NovelGenerator/main/#-%E9%A1%B9%E7%9B%AE%E6%9E%B6%E6%9E%84"&gt;项目架构&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/YILING0013/AI_NovelGenerator/main/#%E2%9A%99%EF%B8%8F-%E9%85%8D%E7%BD%AE%E6%8C%87%E5%8D%97"&gt;配置指南&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/YILING0013/AI_NovelGenerator/main/#%F0%9F%9A%80-%E8%BF%90%E8%A1%8C%E8%AF%B4%E6%98%8E"&gt;运行说明&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/YILING0013/AI_NovelGenerator/main/#%F0%9F%93%98-%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B"&gt;使用教程&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/YILING0013/AI_NovelGenerator/main/#%E2%9D%93-%E7%96%91%E9%9A%BE%E8%A7%A3%E7%AD%94"&gt;疑难解答&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr /&gt; 
&lt;h2&gt;🛠 环境准备&lt;/h2&gt; 
&lt;p&gt;确保满足以下运行条件：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Python 3.9+&lt;/strong&gt; 运行环境（推荐3.10-3.12之间）&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;pip&lt;/strong&gt; 包管理工具&lt;/li&gt; 
 &lt;li&gt;有效API密钥： 
  &lt;ul&gt; 
   &lt;li&gt;云端服务：OpenAI / DeepSeek 等&lt;/li&gt; 
   &lt;li&gt;本地服务：Ollama 等兼容 OpenAI 的接口&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;📥 安装说明&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;下载项目&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;通过 &lt;a href="https://github.com"&gt;GitHub&lt;/a&gt; 下载项目 ZIP 文件，或使用以下命令克隆本项目： &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/YILING0013/AI_NovelGenerator
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;安装编译工具（可选）&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;如果对某些包无法正常安装，访问 &lt;a href="https://visualstudio.microsoft.com/zh-hans/visual-cpp-build-tools/"&gt;Visual Studio Build Tools&lt;/a&gt; 下载并安装C++编译工具，用于构建部分模块包；&lt;/li&gt; 
   &lt;li&gt;安装时，默认只包含 MSBuild 工具，需手动勾选左上角列表栏中的 &lt;strong&gt;C++ 桌面开发&lt;/strong&gt; 选项。&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;安装依赖并运行&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;打开终端，进入项目源文件目录： &lt;pre&gt;&lt;code class="language-bash"&gt;cd AI_NovelGenerator
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt;安装项目依赖： &lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt;安装完成后，运行主程序： &lt;pre&gt;&lt;code class="language-bash"&gt;python main.py
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;如果缺失部分依赖，后续&lt;strong&gt;手动执行&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;pip install XXX
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;进行安装即可&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;🗂 项目架构&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;novel-generator/
├── main.py                      # 入口文件, 运行 GUI
├── consistency_checker.py       # 一致性检查, 防止剧情冲突
|—— chapter_directory_parser.py  # 目录解析
|—— embedding_adapters.py        # Embedding 接口封装
|—— llm_adapters.py              # LLM 接口封装
├── prompt_definitions.py        # 定义 AI 提示词
├── utils.py                     # 常用工具函数, 文件操作
├── config_manager.py            # 管理配置 (API Key, Base URL)
├── config.json                  # 用户配置文件 (可选)
├── novel_generator/             # 章节生成核心逻辑
├── ui/                          # 图形界面
└── vectorstore/                 # (可选) 本地向量数据库存储
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;⚙️ 配置指南&lt;/h2&gt; 
&lt;h3&gt;📌 基础配置（config.json）&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
    "api_key": "sk-XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX",
    "base_url": "https://api.openai.com/v1",
    "interface_format": "OpenAI",
    "model_name": "gpt-4o-mini",
    "temperature": 0.7,
    "max_tokens": 4096,
    "embedding_api_key": "sk-XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX",
    "embedding_interface_format": "OpenAI",
    "embedding_url": "https://api.openai.com/v1",
    "embedding_model_name": "text-embedding-ada-002",
    "embedding_retrieval_k": 4,
    "topic": "星穹铁道主角星穿越到原神提瓦特大陆，拯救提瓦特大陆，并与其中的角色展开爱恨情仇的小说",
    "genre": "玄幻",
    "num_chapters": 120,
    "word_number": 4000,
    "filepath": "D:/AI_NovelGenerator/filepath"
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;🔧 配置说明&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;生成模型配置&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;api_key&lt;/code&gt;: 大模型服务的API密钥&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;base_url&lt;/code&gt;: API终端地址（本地服务填Ollama等地址）&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;interface_format&lt;/code&gt;: 接口模式&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;model_name&lt;/code&gt;: 主生成模型名称（如gpt-4, claude-3等）&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;temperature&lt;/code&gt;: 创意度参数（0-1，越高越有创造性）&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;max_tokens&lt;/code&gt;: 模型最大回复长度&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Embedding模型配置&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;embedding_model_name&lt;/code&gt;: 模型名称（如Ollama的nomic-embed-text）&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;embedding_url&lt;/code&gt;: 服务地址&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;embedding_retrieval_k&lt;/code&gt;:&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;小说参数配置&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;topic&lt;/code&gt;: 核心故事主题&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;genre&lt;/code&gt;: 作品类型&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;num_chapters&lt;/code&gt;: 总章节数&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;word_number&lt;/code&gt;: 单章目标字数&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;filepath&lt;/code&gt;: 生成文件存储路径&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr /&gt; 
&lt;h2&gt;🚀 运行说明&lt;/h2&gt; 
&lt;h3&gt;&lt;strong&gt;方式 1：使用 Python 解释器&lt;/strong&gt;&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python main.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;执行后，GUI 将会启动，你可以在图形界面中进行各项操作。&lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;方式 2：打包为可执行文件&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;如果你想在无 Python 环境的机器上使用本工具，可以使用 &lt;strong&gt;PyInstaller&lt;/strong&gt; 进行打包：&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install pyinstaller
pyinstaller main.spec
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;打包完成后，会在 &lt;code&gt;dist/&lt;/code&gt; 目录下生成可执行文件（如 Windows 下的 &lt;code&gt;main.exe&lt;/code&gt;）。&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;📘 使用教程&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;启动后，先完成基本参数设置：&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;API Key &amp;amp; Base URL&lt;/strong&gt;（如 &lt;code&gt;https://api.openai.com/v1&lt;/code&gt;）&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;模型名称&lt;/strong&gt;（如 &lt;code&gt;gpt-3.5-turbo&lt;/code&gt;、&lt;code&gt;gpt-4o&lt;/code&gt; 等）&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Temperature&lt;/strong&gt; (0~1，决定文字创意程度)&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;主题(Topic)&lt;/strong&gt;（如 “废土世界的 AI 叛乱”）&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;类型(Genre)&lt;/strong&gt;（如 “科幻”/“魔幻”/“都市幻想”）&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;章节数&lt;/strong&gt;、&lt;strong&gt;每章字数&lt;/strong&gt;（如 10 章，每章约 3000 字）&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;保存路径&lt;/strong&gt;（建议创建一个新的输出文件夹）&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;点击「Step1. 生成设定」&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;系统将基于主题、类型、章节数等信息，生成： 
    &lt;ul&gt; 
     &lt;li&gt;&lt;code&gt;Novel_setting.txt&lt;/code&gt;：包含世界观、角色信息、雷点暗线等。&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;可以在生成后的 &lt;code&gt;Novel_setting.txt&lt;/code&gt; 中查看或修改设定内容。&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;点击「Step2. 生成目录」&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;系统会根据已完成的 &lt;code&gt;Novel_setting.txt&lt;/code&gt; 内容，为全部章节生成： 
    &lt;ul&gt; 
     &lt;li&gt;&lt;code&gt;Novel_directory.txt&lt;/code&gt;：包括每章标题和简要提示。&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;可以在生成后的文件中查看、修改或补充章节标题和描述。&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;点击「Step3. 生成章节草稿」&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;在生成章节之前，你可以： 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;设置章节号&lt;/strong&gt;（如写第 1 章，就填 &lt;code&gt;1&lt;/code&gt;）&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;在“本章指导”输入框&lt;/strong&gt;中提供对本章剧情的任何期望或提示&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;点击按钮后，系统将： 
    &lt;ul&gt; 
     &lt;li&gt;自动读取前文设定、&lt;code&gt;Novel_directory.txt&lt;/code&gt;、以及已定稿章节&lt;/li&gt; 
     &lt;li&gt;调用向量检索回顾剧情，保证上下文连贯&lt;/li&gt; 
     &lt;li&gt;生成本章大纲 (&lt;code&gt;outline_X.txt&lt;/code&gt;) 及正文 (&lt;code&gt;chapter_X.txt&lt;/code&gt;)&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;生成完成后，你可在左侧的文本框查看、编辑本章草稿内容。&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;点击「Step4. 定稿当前章节」&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;系统将： 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;更新全局摘要&lt;/strong&gt;（写入 &lt;code&gt;global_summary.txt&lt;/code&gt;）&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;更新角色状态&lt;/strong&gt;（写入 &lt;code&gt;character_state.txt&lt;/code&gt;）&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;更新向量检索库&lt;/strong&gt;（保证后续章节可以调用最新信息）&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;更新剧情要点&lt;/strong&gt;（如 &lt;code&gt;plot_arcs.txt&lt;/code&gt;）&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;定稿完成后，你可以在 &lt;code&gt;chapter_X.txt&lt;/code&gt; 中看到定稿后的文本。&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;一致性检查（可选）&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;点击「[可选] 一致性审校」按钮，对最新章节进行冲突检测，如角色逻辑、剧情前后矛盾等。&lt;/li&gt; 
   &lt;li&gt;若有冲突，会在日志区输出详细提示。&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;重复第 4-6 步&lt;/strong&gt; 直到所有章节生成并定稿！&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;向量检索配置提示&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;embedding模型需要显示指定接口和模型名称；&lt;/li&gt; 
  &lt;li&gt;使用&lt;strong&gt;本地Ollama&lt;/strong&gt;的&lt;strong&gt;Embedding&lt;/strong&gt;时需提前启动Ollama服务： &lt;pre&gt;&lt;code class="language-bash"&gt;ollama serve  # 启动服务
ollama pull nomic-embed-text  # 下载/启用模型
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt;切换不同Embedding模型后建议清空vectorstore目录&lt;/li&gt; 
  &lt;li&gt;云端Embedding需确保对应API权限已开通&lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/blockquote&gt; 
&lt;hr /&gt; 
&lt;h2&gt;❓ 疑难解答&lt;/h2&gt; 
&lt;h3&gt;Q1: Expecting value: line 1 column 1 (char 0)&lt;/h3&gt; 
&lt;p&gt;该问题大概率由于API未正确响应造成，也许响应了一个html？其它内容，导致出现该报错；&lt;/p&gt; 
&lt;h3&gt;Q2: HTTP/1.1 504 Gateway Timeout？&lt;/h3&gt; 
&lt;p&gt;确认接口是否稳定；&lt;/p&gt; 
&lt;h3&gt;Q3: 如何切换不同的Embedding提供商？&lt;/h3&gt; 
&lt;p&gt;在GUI界面中对应输入即可。&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;如有更多问题或需求，欢迎在&lt;strong&gt;项目 Issues&lt;/strong&gt; 中提出。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>PostHog/posthog</title>
      <link>https://github.com/PostHog/posthog</link>
      <description>&lt;p&gt;🦔 PostHog provides open-source web &amp; product analytics, session recording, feature flagging and A/B testing that you can self-host. Get started - free.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img alt="posthoglogo" src="https://user-images.githubusercontent.com/65415371/205059737-c8a4f836-4889-4654-902e-f302b187b6a0.png" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://posthog.com/contributors"&gt;&lt;img alt="GitHub contributors" src="https://img.shields.io/github/contributors/posthog/posthog" /&gt;&lt;/a&gt; &lt;a href="http://makeapullrequest.com"&gt;&lt;img alt="PRs Welcome" src="https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=shields" /&gt;&lt;/a&gt; &lt;img alt="Docker Pulls" src="https://img.shields.io/docker/pulls/posthog/posthog" /&gt; &lt;a href="https://github.com/PostHog/posthog/commits/master"&gt;&lt;img alt="GitHub commit activity" src="https://img.shields.io/github/commit-activity/m/posthog/posthog" /&gt; &lt;/a&gt; &lt;a href="https://github.com/PostHog/posthog/issues?q=is%3Aissue%20state%3Aclosed"&gt;&lt;img alt="GitHub closed issues" src="https://img.shields.io/github/issues-closed/posthog/posthog" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://posthog.com/docs"&gt;Docs&lt;/a&gt; - &lt;a href="https://posthog.com/community"&gt;Community&lt;/a&gt; - &lt;a href="https://posthog.com/roadmap"&gt;Roadmap&lt;/a&gt; - &lt;a href="https://posthog.com/why"&gt;Why PostHog?&lt;/a&gt; - &lt;a href="https://posthog.com/changelog"&gt;Changelog&lt;/a&gt; - &lt;a href="https://github.com/PostHog/posthog/issues/new?assignees=&amp;amp;labels=bug&amp;amp;template=bug_report.md"&gt;Bug reports&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://www.youtube.com/watch?v=2jQco8hEvTI"&gt; &lt;img src="https://res.cloudinary.com/dmukukwp6/image/upload/demo_thumb_68d0d8d56d" alt="PostHog Demonstration" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;PostHog is an all-in-one, open source platform for building successful products&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://posthog.com/"&gt;PostHog&lt;/a&gt; provides every tool you need to build a successful product including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://posthog.com/product-analytics"&gt;Product analytics&lt;/a&gt;: Autocapture or manually instrument event-based analytics to understand user behavior and analyze data with visualization or SQL.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://posthog.com/web-analytics"&gt;Web analytics&lt;/a&gt;: Monitor web traffic and user sessions with a GA-like dashboard. Easily monitor conversion, web vitals, and revenue.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://posthog.com/session-replay"&gt;Session replays&lt;/a&gt;: Watch real user sessions of interactions with your website or mobile app to diagnose issues and understand user behavior.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://posthog.com/feature-flags"&gt;Feature flags&lt;/a&gt;: Safely roll out features to select users or cohorts with feature flags.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://posthog.com/experiments"&gt;Experiments&lt;/a&gt;: Test changes and measure their statistical impact on goal metrics. Set up experiments with no-code too.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://posthog.com/error-tracking"&gt;Error tracking&lt;/a&gt;: Track errors, get alerts, and resolve issues to improve your product.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://posthog.com/surveys"&gt;Surveys&lt;/a&gt;: Ask anything with our collection of no-code survey templates, or build custom surveys with our survey builder.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://posthog.com/data-warehouse"&gt;Data warehouse&lt;/a&gt;: Sync data from external tools like Stripe, Hubspot, your data warehouse, and more. Query it alongside your product data.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://posthog.com/cdp"&gt;Data pipelines&lt;/a&gt;: Run custom filters and transformations on your incoming data. Send it to 25+ tools or any webhook in real time or batch export large amounts to your warehouse.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://posthog.com/docs/llm-analytics"&gt;LLM analytics&lt;/a&gt;: Capture traces, generations, latency, and cost for your LLM-powered app.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Best of all, all of this is free to use with a &lt;a href="https://posthog.com/pricing"&gt;generous monthly free tier&lt;/a&gt; for each product. Get started by signing up for &lt;a href="https://us.posthog.com/signup"&gt;PostHog Cloud US&lt;/a&gt; or &lt;a href="https://eu.posthog.com/signup"&gt;PostHog Cloud EU&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PostHog/posthog/master/#posthog-is-an-all-in-one-open-source-platform-for-building-successful-products"&gt;PostHog is an all-in-one, open source platform for building successful products&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PostHog/posthog/master/#table-of-contents"&gt;Table of Contents&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PostHog/posthog/master/#getting-started-with-posthog"&gt;Getting started with PostHog&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PostHog/posthog/master/#posthog-cloud-recommended"&gt;PostHog Cloud (Recommended)&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PostHog/posthog/master/#self-hosting-the-open-source-hobby-deploy-advanced"&gt;Self-hosting the open-source hobby deploy (Advanced)&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PostHog/posthog/master/#setting-up-posthog"&gt;Setting up PostHog&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PostHog/posthog/master/#learning-more-about-posthog"&gt;Learning more about PostHog&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PostHog/posthog/master/#contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PostHog/posthog/master/#open-source-vs-paid"&gt;Open-source vs. paid&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/PostHog/posthog/master/#were-hiring"&gt;We’re hiring!&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Getting started with PostHog&lt;/h2&gt; 
&lt;h3&gt;PostHog Cloud (Recommended)&lt;/h3&gt; 
&lt;p&gt;The fastest and most reliable way to get started with PostHog is signing up for free to&amp;nbsp;&lt;a href="https://us.posthog.com/signup"&gt;PostHog Cloud&lt;/a&gt; or &lt;a href="https://eu.posthog.com/signup"&gt;PostHog Cloud EU&lt;/a&gt;. Your first 1 million events, 5k recordings, 1M flag requests, 100k exceptions, and 1500 survey responses are free every month, after which you pay based on usage.&lt;/p&gt; 
&lt;h3&gt;Self-hosting the open-source hobby deploy (Advanced)&lt;/h3&gt; 
&lt;p&gt;If you want to self-host PostHog, you can deploy a hobby instance in one line on Linux with Docker (recommended 4GB memory):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/posthog/posthog/HEAD/bin/deploy-hobby)"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Open source deployments should scale to approximately 100k events per month, after which we recommend &lt;a href="https://posthog.com/docs/migrate/migrate-to-cloud"&gt;migrating to a PostHog Cloud&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;We &lt;em&gt;do not&lt;/em&gt; provide customer support or offer guarantees for open source deployments. See our &lt;a href="https://posthog.com/docs/self-host"&gt;self-hosting docs&lt;/a&gt;, &lt;a href="https://posthog.com/docs/self-host/deploy/troubleshooting"&gt;troubleshooting guide&lt;/a&gt;, and &lt;a href="https://posthog.com/docs/self-host/open-source/disclaimer"&gt;disclaimer&lt;/a&gt; for more info.&lt;/p&gt; 
&lt;h2&gt;Setting up PostHog&lt;/h2&gt; 
&lt;p&gt;Once you've got a PostHog instance, you can set it up by installing our &lt;a href="https://posthog.com/docs/getting-started/install?tab=snippet"&gt;JavaScript web snippet&lt;/a&gt;, one of &lt;a href="https://posthog.com/docs/getting-started/install?tab=sdks"&gt;our SDKs&lt;/a&gt;, or by &lt;a href="https://posthog.com/docs/getting-started/install?tab=api"&gt;using our API&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;We have SDKs and libraries for popular languages and frameworks like:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Frontend&lt;/th&gt; 
   &lt;th&gt;Mobile&lt;/th&gt; 
   &lt;th&gt;Backend&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://posthog.com/docs/libraries/js"&gt;JavaScript&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://posthog.com/docs/libraries/react-native"&gt;React Native&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://posthog.com/docs/libraries/python"&gt;Python&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://posthog.com/docs/libraries/next-js"&gt;Next.js&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://posthog.com/docs/libraries/android"&gt;Android&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://posthog.com/docs/libraries/node"&gt;Node&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://posthog.com/docs/libraries/react"&gt;React&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://posthog.com/docs/libraries/ios"&gt;iOS&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://posthog.com/docs/libraries/php"&gt;PHP&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://posthog.com/docs/libraries/vue-js"&gt;Vue&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://posthog.com/docs/libraries/flutter"&gt;Flutter&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://posthog.com/docs/libraries/ruby"&gt;Ruby&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Beyond this, we have docs and guides for &lt;a href="https://posthog.com/docs/libraries/go"&gt;Go&lt;/a&gt;, &lt;a href="https://posthog.com/docs/libraries/dotnet"&gt;.NET/C#&lt;/a&gt;, &lt;a href="https://posthog.com/docs/libraries/django"&gt;Django&lt;/a&gt;, &lt;a href="https://posthog.com/docs/libraries/angular"&gt;Angular&lt;/a&gt;, &lt;a href="https://posthog.com/docs/libraries/wordpress"&gt;WordPress&lt;/a&gt;, &lt;a href="https://posthog.com/docs/libraries/webflow"&gt;Webflow&lt;/a&gt;, and more.&lt;/p&gt; 
&lt;p&gt;Once you've installed PostHog, see our &lt;a href="https://posthog.com/docs/product-os"&gt;product docs&lt;/a&gt; for more information on how to set up &lt;a href="https://posthog.com/docs/product-analytics/capture-events"&gt;product analytics&lt;/a&gt;, &lt;a href="https://posthog.com/docs/web-analytics/getting-started"&gt;web analytics&lt;/a&gt;, &lt;a href="https://posthog.com/docs/session-replay/how-to-watch-recordings"&gt;session replays&lt;/a&gt;, &lt;a href="https://posthog.com/docs/feature-flags/creating-feature-flags"&gt;feature flags&lt;/a&gt;, &lt;a href="https://posthog.com/docs/experiments/creating-an-experiment"&gt;experiments&lt;/a&gt;, &lt;a href="https://posthog.com/docs/error-tracking/installation#setting-up-exception-autocapture"&gt;error tracking&lt;/a&gt;, &lt;a href="https://posthog.com/docs/surveys/installation"&gt;surveys&lt;/a&gt;, &lt;a href="https://posthog.com/docs/cdp/sources"&gt;data warehouse&lt;/a&gt;, and more.&lt;/p&gt; 
&lt;h2&gt;Learning more about PostHog&lt;/h2&gt; 
&lt;p&gt;Our code isn't the only thing that's open source 😳. We also open source our &lt;a href="https://posthog.com/handbook"&gt;company handbook&lt;/a&gt; which details our &lt;a href="https://posthog.com/handbook/why-does-posthog-exist"&gt;strategy&lt;/a&gt;, &lt;a href="https://posthog.com/handbook/company/culture"&gt;ways of working&lt;/a&gt;, and &lt;a href="https://posthog.com/handbook/team-structure"&gt;processes&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Curious about how to make the most of PostHog? We wrote a guide to &lt;a href="https://posthog.com/docs/new-to-posthog/getting-hogpilled"&gt;winning with PostHog&lt;/a&gt; which walks you through the basics of &lt;a href="https://posthog.com/docs/new-to-posthog/activation"&gt;measuring activation&lt;/a&gt;, &lt;a href="https://posthog.com/docs/new-to-posthog/retention"&gt;tracking retention&lt;/a&gt;, and &lt;a href="https://posthog.com/docs/new-to-posthog/revenue"&gt;capturing revenue&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We &amp;lt;3 contributions big and small:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Vote on features or get early access to beta functionality in our &lt;a href="https://posthog.com/roadmap"&gt;roadmap&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Open a PR (see our instructions on &lt;a href="https://posthog.com/handbook/engineering/developing-locally"&gt;developing PostHog locally&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;Submit a &lt;a href="https://github.com/PostHog/posthog/issues/new?assignees=&amp;amp;labels=enhancement%2C+feature&amp;amp;template=feature_request.md"&gt;feature request&lt;/a&gt; or &lt;a href="https://github.com/PostHog/posthog/issues/new?assignees=&amp;amp;labels=bug&amp;amp;template=bug_report.md"&gt;bug report&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Open-source vs. paid&lt;/h2&gt; 
&lt;p&gt;This repo is available under the &lt;a href="https://github.com/PostHog/posthog/raw/master/LICENSE"&gt;MIT expat license&lt;/a&gt;, except for the &lt;code&gt;ee&lt;/code&gt; directory (which has its &lt;a href="https://github.com/PostHog/posthog/raw/master/ee/LICENSE"&gt;license here&lt;/a&gt;) if applicable.&lt;/p&gt; 
&lt;p&gt;Need &lt;em&gt;absolutely 💯% FOSS&lt;/em&gt;? Check out our &lt;a href="https://github.com/PostHog/posthog-foss"&gt;posthog-foss&lt;/a&gt; repository, which is purged of all proprietary code and features.&lt;/p&gt; 
&lt;p&gt;The pricing for our paid plan is completely transparent and available on &lt;a href="https://posthog.com/pricing"&gt;our pricing page&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;We’re hiring!&lt;/h2&gt; 
&lt;img src="https://res.cloudinary.com/dmukukwp6/image/upload/v1/posthog.com/src/components/Home/images/mission-control-hog" alt="Hedgehog working on a Mission Control Center" width="350px" /&gt; 
&lt;p&gt;Hey! If you're reading this, you've proven yourself as a dedicated README reader.&lt;/p&gt; 
&lt;p&gt;You might also make a great addition to our team. We're growing fast &lt;a href="https://posthog.com/careers"&gt;and would love for you to join us&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>isaac-sim/IsaacLab</title>
      <link>https://github.com/isaac-sim/IsaacLab</link>
      <description>&lt;p&gt;Unified framework for robot learning built on NVIDIA Isaac Sim&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/isaac-sim/IsaacLab/main/docs/source/_static/isaaclab.jpg" alt="Isaac Lab" /&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h1&gt;Isaac Lab&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://docs.isaacsim.omniverse.nvidia.com/latest/index.html"&gt;&lt;img src="https://img.shields.io/badge/IsaacSim-5.0.0-silver.svg?sanitize=true" alt="IsaacSim" /&gt;&lt;/a&gt; &lt;a href="https://docs.python.org/3/whatsnew/3.11.html"&gt;&lt;img src="https://img.shields.io/badge/python-3.11-blue.svg?sanitize=true" alt="Python" /&gt;&lt;/a&gt; &lt;a href="https://releases.ubuntu.com/22.04/"&gt;&lt;img src="https://img.shields.io/badge/platform-linux--64-orange.svg?sanitize=true" alt="Linux platform" /&gt;&lt;/a&gt; &lt;a href="https://www.microsoft.com/en-us/"&gt;&lt;img src="https://img.shields.io/badge/platform-windows--64-orange.svg?sanitize=true" alt="Windows platform" /&gt;&lt;/a&gt; &lt;a href="https://github.com/isaac-sim/IsaacLab/actions/workflows/pre-commit.yaml"&gt;&lt;img src="https://img.shields.io/github/actions/workflow/status/isaac-sim/IsaacLab/pre-commit.yaml?logo=pre-commit&amp;amp;logoColor=white&amp;amp;label=pre-commit&amp;amp;color=brightgreen" alt="pre-commit" /&gt;&lt;/a&gt; &lt;a href="https://github.com/isaac-sim/IsaacLab/actions/workflows/docs.yaml"&gt;&lt;img src="https://img.shields.io/github/actions/workflow/status/isaac-sim/IsaacLab/docs.yaml?label=docs&amp;amp;color=brightgreen" alt="docs status" /&gt;&lt;/a&gt; &lt;a href="https://opensource.org/licenses/BSD-3-Clause"&gt;&lt;img src="https://img.shields.io/badge/license-BSD--3-yellow.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://opensource.org/license/apache-2-0"&gt;&lt;img src="https://img.shields.io/badge/license-Apache--2.0-yellow.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Isaac Lab&lt;/strong&gt; is a GPU-accelerated, open-source framework designed to unify and simplify robotics research workflows, such as reinforcement learning, imitation learning, and motion planning. Built on &lt;a href="https://docs.isaacsim.omniverse.nvidia.com/latest/index.html"&gt;NVIDIA Isaac Sim&lt;/a&gt;, it combines fast and accurate physics and sensor simulation, making it an ideal choice for sim-to-real transfer in robotics.&lt;/p&gt; 
&lt;p&gt;Isaac Lab provides developers with a range of essential features for accurate sensor simulation, such as RTX-based cameras, LIDAR, or contact sensors. The framework's GPU acceleration enables users to run complex simulations and computations faster, which is key for iterative processes like reinforcement learning and data-intensive tasks. Moreover, Isaac Lab can run locally or be distributed across the cloud, offering flexibility for large-scale deployments.&lt;/p&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;p&gt;Isaac Lab offers a comprehensive set of tools and environments designed to facilitate robot learning:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Robots&lt;/strong&gt;: A diverse collection of robots, from manipulators, quadrupeds, to humanoids, with 16 commonly available models.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Environments&lt;/strong&gt;: Ready-to-train implementations of more than 30 environments, which can be trained with popular reinforcement learning frameworks such as RSL RL, SKRL, RL Games, or Stable Baselines. We also support multi-agent reinforcement learning.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Physics&lt;/strong&gt;: Rigid bodies, articulated systems, deformable objects&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Sensors&lt;/strong&gt;: RGB/depth/segmentation cameras, camera annotations, IMU, contact sensors, ray casters.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;h3&gt;Documentation&lt;/h3&gt; 
&lt;p&gt;Our &lt;a href="https://isaac-sim.github.io/IsaacLab"&gt;documentation page&lt;/a&gt; provides everything you need to get started, including detailed tutorials and step-by-step guides. Follow these links to learn more about:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://isaac-sim.github.io/IsaacLab/main/source/setup/installation/index.html#local-installation"&gt;Installation steps&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://isaac-sim.github.io/IsaacLab/main/source/overview/reinforcement-learning/rl_existing_scripts.html"&gt;Reinforcement learning&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://isaac-sim.github.io/IsaacLab/main/source/tutorials/index.html"&gt;Tutorials&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://isaac-sim.github.io/IsaacLab/main/source/overview/environments.html"&gt;Available environments&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Isaac Sim Version Dependency&lt;/h2&gt; 
&lt;p&gt;Isaac Lab is built on top of Isaac Sim and requires specific versions of Isaac Sim that are compatible with each release of Isaac Lab. Below, we outline the recent Isaac Lab releases and GitHub branches and their corresponding dependency versions for Isaac Sim.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Isaac Lab Version&lt;/th&gt; 
   &lt;th&gt;Isaac Sim Version&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;main&lt;/code&gt; branch&lt;/td&gt; 
   &lt;td&gt;Isaac Sim 4.5 / 5.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;v2.2.X&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Isaac Sim 4.5 / 5.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;v2.1.X&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Isaac Sim 4.5&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;v2.0.X&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Isaac Sim 4.5&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Contributing to Isaac Lab&lt;/h2&gt; 
&lt;p&gt;We wholeheartedly welcome contributions from the community to make this framework mature and useful for everyone. These may happen as bug reports, feature requests, or code contributions. For details, please check our &lt;a href="https://isaac-sim.github.io/IsaacLab/main/source/refs/contributing.html"&gt;contribution guidelines&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Show &amp;amp; Tell: Share Your Inspiration&lt;/h2&gt; 
&lt;p&gt;We encourage you to utilize our &lt;a href="https://github.com/isaac-sim/IsaacLab/discussions/categories/show-and-tell"&gt;Show &amp;amp; Tell&lt;/a&gt; area in the &lt;code&gt;Discussions&lt;/code&gt; section of this repository. This space is designed for you to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Share the tutorials you've created&lt;/li&gt; 
 &lt;li&gt;Showcase your learning content&lt;/li&gt; 
 &lt;li&gt;Present exciting projects you've developed&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;By sharing your work, you'll inspire others and contribute to the collective knowledge of our community. Your contributions can spark new ideas and collaborations, fostering innovation in robotics and simulation.&lt;/p&gt; 
&lt;h2&gt;Troubleshooting&lt;/h2&gt; 
&lt;p&gt;Please see the &lt;a href="https://isaac-sim.github.io/IsaacLab/main/source/refs/troubleshooting.html"&gt;troubleshooting&lt;/a&gt; section for common fixes or &lt;a href="https://github.com/isaac-sim/IsaacLab/issues"&gt;submit an issue&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For issues related to Isaac Sim, we recommend checking its &lt;a href="https://docs.omniverse.nvidia.com/app_isaacsim/app_isaacsim/overview.html"&gt;documentation&lt;/a&gt; or opening a question on its &lt;a href="https://forums.developer.nvidia.com/c/agx-autonomous-machines/isaac/67"&gt;forums&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Support&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Please use GitHub &lt;a href="https://github.com/isaac-sim/IsaacLab/discussions"&gt;Discussions&lt;/a&gt; for discussing ideas, asking questions, and requests for new features.&lt;/li&gt; 
 &lt;li&gt;Github &lt;a href="https://github.com/isaac-sim/IsaacLab/issues"&gt;Issues&lt;/a&gt; should only be used to track executable pieces of work with a definite scope and a clear deliverable. These can be fixing bugs, documentation issues, new features, or general updates.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Connect with the NVIDIA Omniverse Community&lt;/h2&gt; 
&lt;p&gt;Do you have a project or resource you'd like to share more widely? We'd love to hear from you! Reach out to the NVIDIA Omniverse Community team at &lt;a href="mailto:OmniverseCommunity@nvidia.com"&gt;OmniverseCommunity@nvidia.com&lt;/a&gt; to explore opportunities to spotlight your work.&lt;/p&gt; 
&lt;p&gt;You can also join the conversation on the &lt;a href="https://discord.com/invite/nvidiaomniverse"&gt;Omniverse Discord&lt;/a&gt; to connect with other developers, share your projects, and help grow a vibrant, collaborative ecosystem where creativity and technology intersect. Your contributions can make a meaningful impact on the Isaac Lab community and beyond!&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;The Isaac Lab framework is released under &lt;a href="https://raw.githubusercontent.com/isaac-sim/IsaacLab/main/LICENSE"&gt;BSD-3 License&lt;/a&gt;. The &lt;code&gt;isaaclab_mimic&lt;/code&gt; extension and its corresponding standalone scripts are released under &lt;a href="https://raw.githubusercontent.com/isaac-sim/IsaacLab/main/LICENSE-mimic"&gt;Apache 2.0&lt;/a&gt;. The license files of its dependencies and assets are present in the &lt;a href="https://raw.githubusercontent.com/isaac-sim/IsaacLab/main/docs/licenses"&gt;&lt;code&gt;docs/licenses&lt;/code&gt;&lt;/a&gt; directory.&lt;/p&gt; 
&lt;p&gt;Note that Isaac Lab requires Isaac Sim, which includes components under proprietary licensing terms. Please see the &lt;a href="https://raw.githubusercontent.com/isaac-sim/IsaacLab/main/docs/licenses/dependencies/isaacsim-license.txt"&gt;Isaac Sim license&lt;/a&gt; for information on Isaac Sim licensing.&lt;/p&gt; 
&lt;p&gt;Note that the &lt;code&gt;isaaclab_mimic&lt;/code&gt; extension requires cuRobo, which has proprietary licensing terms that can be found in &lt;a href="https://raw.githubusercontent.com/isaac-sim/IsaacLab/main/docs/licenses/dependencies/cuRobo-license.txt"&gt;&lt;code&gt;docs/licenses/dependencies/cuRobo-license.txt&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Acknowledgement&lt;/h2&gt; 
&lt;p&gt;Isaac Lab development initiated from the &lt;a href="https://isaac-orbit.github.io/"&gt;Orbit&lt;/a&gt; framework. We would appreciate if you would cite it in academic publications as well:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@article{mittal2023orbit,
   author={Mittal, Mayank and Yu, Calvin and Yu, Qinxi and Liu, Jingzhou and Rudin, Nikita and Hoeller, David and Yuan, Jia Lin and Singh, Ritvik and Guo, Yunrong and Mazhar, Hammad and Mandlekar, Ajay and Babich, Buck and State, Gavriel and Hutter, Marco and Garg, Animesh},
   journal={IEEE Robotics and Automation Letters},
   title={Orbit: A Unified Simulation Framework for Interactive Robot Learning Environments},
   year={2023},
   volume={8},
   number={6},
   pages={3740-3747},
   doi={10.1109/LRA.2023.3270034}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>commaai/openpilot</title>
      <link>https://github.com/commaai/openpilot</link>
      <description>&lt;p&gt;openpilot is an operating system for robotics. Currently, it upgrades the driver assistance system on 300+ supported cars.&lt;/p&gt;&lt;hr&gt;&lt;div align="center" style="text-align: center;"&gt; 
 &lt;h1&gt;openpilot&lt;/h1&gt; 
 &lt;p&gt; &lt;b&gt;openpilot is an operating system for robotics.&lt;/b&gt; &lt;br /&gt; Currently, it upgrades the driver assistance system in 300+ supported cars. &lt;/p&gt; 
 &lt;h3&gt; &lt;a href="https://docs.comma.ai"&gt;Docs&lt;/a&gt; &lt;span&gt; · &lt;/span&gt; &lt;a href="https://docs.comma.ai/contributing/roadmap/"&gt;Roadmap&lt;/a&gt; &lt;span&gt; · &lt;/span&gt; &lt;a href="https://github.com/commaai/openpilot/raw/master/docs/CONTRIBUTING.md"&gt;Contribute&lt;/a&gt; &lt;span&gt; · &lt;/span&gt; &lt;a href="https://discord.comma.ai"&gt;Community&lt;/a&gt; &lt;span&gt; · &lt;/span&gt; &lt;a href="https://comma.ai/shop"&gt;Try it on a comma 3X&lt;/a&gt; &lt;/h3&gt; 
 &lt;p&gt;Quick start: &lt;code&gt;bash &amp;lt;(curl -fsSL openpilot.comma.ai)&lt;/code&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/commaai/openpilot/actions/workflows/selfdrive_tests.yaml"&gt;&lt;img src="https://github.com/commaai/openpilot/actions/workflows/selfdrive_tests.yaml/badge.svg?sanitize=true" alt="openpilot tests" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/commaai/openpilot/master/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/License-MIT-yellow.svg?sanitize=true" alt="License: MIT" /&gt;&lt;/a&gt; &lt;a href="https://x.com/comma_ai"&gt;&lt;img src="https://img.shields.io/twitter/follow/comma_ai" alt="X Follow" /&gt;&lt;/a&gt; &lt;a href="https://discord.comma.ai"&gt;&lt;img src="https://img.shields.io/discord/469524606043160576" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://youtu.be/NmBfgOanCyk" title="Video By Greer Viau"&gt;&lt;img src="https://github.com/commaai/openpilot/assets/8762862/2f7112ae-f748-4f39-b617-fabd689c3772" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://youtu.be/VHKyqZ7t8Gw" title="Video By Logan LeGrand"&gt;&lt;img src="https://github.com/commaai/openpilot/assets/8762862/92351544-2833-40d7-9e0b-7ef7ae37ec4c" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://youtu.be/SUIZYzxtMQs" title="A drive to Taco Bell"&gt;&lt;img src="https://github.com/commaai/openpilot/assets/8762862/05ceefc5-2628-439c-a9b2-89ce77dc6f63" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;Using openpilot in a car&lt;/h2&gt; 
&lt;p&gt;To use openpilot in a car, you need four things:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Supported Device:&lt;/strong&gt; a comma 3X, available at &lt;a href="https://comma.ai/shop/comma-3x"&gt;comma.ai/shop&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Software:&lt;/strong&gt; The setup procedure for the comma 3X allows users to enter a URL for custom software. Use the URL &lt;code&gt;openpilot.comma.ai&lt;/code&gt; to install the release version.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Supported Car:&lt;/strong&gt; Ensure that you have one of &lt;a href="https://raw.githubusercontent.com/commaai/openpilot/master/docs/CARS.md"&gt;the 275+ supported cars&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Car Harness:&lt;/strong&gt; You will also need a &lt;a href="https://comma.ai/shop/car-harness"&gt;car harness&lt;/a&gt; to connect your comma 3X to your car.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;We have detailed instructions for &lt;a href="https://comma.ai/setup"&gt;how to install the harness and device in a car&lt;/a&gt;. Note that it's possible to run openpilot on &lt;a href="https://blog.comma.ai/self-driving-car-for-free/"&gt;other hardware&lt;/a&gt;, although it's not plug-and-play.&lt;/p&gt; 
&lt;h3&gt;Branches&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;branch&lt;/th&gt; 
   &lt;th&gt;URL&lt;/th&gt; 
   &lt;th&gt;description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;release3&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;openpilot.comma.ai&lt;/td&gt; 
   &lt;td&gt;This is openpilot's release branch.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;release3-staging&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;openpilot-test.comma.ai&lt;/td&gt; 
   &lt;td&gt;This is the staging branch for releases. Use it to get new releases slightly early.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;nightly&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;openpilot-nightly.comma.ai&lt;/td&gt; 
   &lt;td&gt;This is the bleeding edge development branch. Do not expect this to be stable.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;nightly-dev&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;installer.comma.ai/commaai/nightly-dev&lt;/td&gt; 
   &lt;td&gt;Same as nightly, but includes experimental development features for some cars.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;To start developing openpilot&lt;/h2&gt; 
&lt;p&gt;openpilot is developed by &lt;a href="https://comma.ai/"&gt;comma&lt;/a&gt; and by users like you. We welcome both pull requests and issues on &lt;a href="http://github.com/commaai/openpilot"&gt;GitHub&lt;/a&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Join the &lt;a href="https://discord.comma.ai"&gt;community Discord&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Check out &lt;a href="https://raw.githubusercontent.com/commaai/openpilot/master/docs/CONTRIBUTING.md"&gt;the contributing docs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Check out the &lt;a href="https://raw.githubusercontent.com/commaai/openpilot/master/tools/"&gt;openpilot tools&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Code documentation lives at &lt;a href="https://docs.comma.ai"&gt;https://docs.comma.ai&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Information about running openpilot lives on the &lt;a href="https://github.com/commaai/openpilot/wiki"&gt;community wiki&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Want to get paid to work on openpilot? &lt;a href="https://comma.ai/jobs#open-positions"&gt;comma is hiring&lt;/a&gt; and offers lots of &lt;a href="https://comma.ai/bounties"&gt;bounties&lt;/a&gt; for external contributors.&lt;/p&gt; 
&lt;h2&gt;Safety and Testing&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;openpilot observes &lt;a href="https://en.wikipedia.org/wiki/ISO_26262"&gt;ISO26262&lt;/a&gt; guidelines, see &lt;a href="https://raw.githubusercontent.com/commaai/openpilot/master/docs/SAFETY.md"&gt;SAFETY.md&lt;/a&gt; for more details.&lt;/li&gt; 
 &lt;li&gt;openpilot has software-in-the-loop &lt;a href="https://raw.githubusercontent.com/commaai/openpilot/master/.github/workflows/selfdrive_tests.yaml"&gt;tests&lt;/a&gt; that run on every commit.&lt;/li&gt; 
 &lt;li&gt;The code enforcing the safety model lives in panda and is written in C, see &lt;a href="https://github.com/commaai/panda#code-rigor"&gt;code rigor&lt;/a&gt; for more details.&lt;/li&gt; 
 &lt;li&gt;panda has software-in-the-loop &lt;a href="https://github.com/commaai/panda/tree/master/tests/safety"&gt;safety tests&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Internally, we have a hardware-in-the-loop Jenkins test suite that builds and unit tests the various processes.&lt;/li&gt; 
 &lt;li&gt;panda has additional hardware-in-the-loop &lt;a href="https://github.com/commaai/panda/raw/master/Jenkinsfile"&gt;tests&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;We run the latest openpilot in a testing closet containing 10 comma devices continuously replaying routes.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;MIT Licensed&lt;/summary&gt; 
 &lt;p&gt;openpilot is released under the MIT license. Some parts of the software are released under other licenses as specified.&lt;/p&gt; 
 &lt;p&gt;Any user of this software shall indemnify and hold harmless Comma.ai, Inc. and its directors, officers, employees, agents, stockholders, affiliates, subcontractors and customers from and against all allegations, claims, actions, suits, demands, damages, liabilities, obligations, losses, settlements, judgments, costs and expenses (including without limitation attorneys’ fees and costs) which arise out of, relate to or result from any use of this software by user.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;THIS IS ALPHA QUALITY SOFTWARE FOR RESEARCH PURPOSES ONLY. THIS IS NOT A PRODUCT. YOU ARE RESPONSIBLE FOR COMPLYING WITH LOCAL LAWS AND REGULATIONS. NO WARRANTY EXPRESSED OR IMPLIED.&lt;/strong&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;User Data and comma Account&lt;/summary&gt; 
 &lt;p&gt;By default, openpilot uploads the driving data to our servers. You can also access your data through &lt;a href="https://connect.comma.ai/"&gt;comma connect&lt;/a&gt;. We use your data to train better models and improve openpilot for everyone.&lt;/p&gt; 
 &lt;p&gt;openpilot is open source software: the user is free to disable data collection if they wish to do so.&lt;/p&gt; 
 &lt;p&gt;openpilot logs the road-facing cameras, CAN, GPS, IMU, magnetometer, thermal sensors, crashes, and operating system logs. The driver-facing camera and microphone are only logged if you explicitly opt-in in settings.&lt;/p&gt; 
 &lt;p&gt;By using openpilot, you agree to &lt;a href="https://comma.ai/privacy"&gt;our Privacy Policy&lt;/a&gt;. You understand that use of this software or its related services will generate certain types of user data, which may be logged and stored at the sole discretion of comma. By accepting this agreement, you grant an irrevocable, perpetual, worldwide right to comma for the use of this data.&lt;/p&gt; 
&lt;/details&gt;</description>
    </item>
    
    <item>
      <title>OpenBB-finance/OpenBB</title>
      <link>https://github.com/OpenBB-finance/OpenBB</link>
      <description>&lt;p&gt;Financial data platform for analysts, quants and AI agents.&lt;/p&gt;&lt;hr&gt;&lt;br /&gt; 
&lt;img src="https://github.com/OpenBB-finance/OpenBB/raw/develop/images/platform-light.svg?raw=true#gh-light-mode-only" alt="OpenBB Platform logo" width="600" /&gt; 
&lt;img src="https://github.com/OpenBB-finance/OpenBB/raw/develop/images/platform-dark.svg?raw=true#gh-dark-mode-only" alt="OpenBB Platform logo" width="600" /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;a href="https://x.com/openbb_finance"&gt;&lt;img src="https://img.shields.io/twitter/url/https/twitter.com/openbb_finance.svg?style=social&amp;amp;label=Follow%20%40openbb_finance" alt="Twitter" /&gt;&lt;/a&gt; &lt;a href="https://discord.com/invite/xPHTuHCmuV"&gt;&lt;img src="https://img.shields.io/discord/831165782750789672" alt="Discord Shield" /&gt;&lt;/a&gt; &lt;a href="https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/OpenBB-finance/OpenBB"&gt;&lt;img src="https://img.shields.io/static/v1?label=Dev%20Containers&amp;amp;message=Open&amp;amp;color=blue&amp;amp;logo=visualstudiocode" alt="Open in Dev Containers" /&gt;&lt;/a&gt; &lt;a href="https://codespaces.new/OpenBB-finance/OpenBB"&gt; &lt;img src="https://github.com/codespaces/badge.svg?sanitize=true" height="20" /&gt; &lt;/a&gt; &lt;a target="_blank" href="https://colab.research.google.com/github/OpenBB-finance/OpenBB/blob/develop/examples/googleColab.ipynb"&gt; &lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt; &lt;/a&gt; &lt;a href="https://pypi.org/project/openbb/"&gt;&lt;img src="https://img.shields.io/pypi/v/openbb?color=blue&amp;amp;label=PyPI%20Package" alt="PyPI" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;The first financial Platform that is open source.&lt;/p&gt; 
&lt;p&gt;The OpenBB Platform offers access to equity, options, crypto, forex, macro economy, fixed income, and more while also offering a broad range of extensions to enhance the user experience according to their needs.&lt;/p&gt; 
&lt;p&gt;Get started with: &lt;code&gt;pip install openbb&lt;/code&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from openbb import obb
output = obb.equity.price.historical("AAPL")
df = output.to_dataframe()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can sign up to the &lt;a href="https://my.openbb.co/login"&gt;OpenBB Hub&lt;/a&gt; to get the most out of the OpenBB ecosystem.&lt;/p&gt; 
&lt;p&gt;Data integrations available can be found here: &lt;a href="https://docs.openbb.co/platform/reference"&gt;https://docs.openbb.co/platform/reference&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;OpenBB Workspace&lt;/h2&gt; 
&lt;p&gt;While the OpenBB Platform is all about an integration to dozens of different data vendors, the interface is either Python or a CLI.&lt;/p&gt; 
&lt;p&gt;If you want an enterprise UI to visualize this datasets and use AI agents on top, you can find OpenBB Workspace at &lt;a href="https://pro.openbb.co"&gt;https://pro.openbb.co&lt;/a&gt;.&lt;/p&gt; 
&lt;a href="https://pro.openbb.co"&gt; 
 &lt;div align="center"&gt; 
  &lt;img src="https://openbb-cms.directus.app/assets/f69b6aaf-0821-4bc8-a43c-715e03a924ef.png" alt="Logo" width="1000" /&gt; 
 &lt;/div&gt; &lt;/a&gt; 
&lt;p&gt;Data integration:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;You can learn more about adding data to the OpenBB workspace from the &lt;a href="https://docs.openbb.co/workspace"&gt;docs&lt;/a&gt; or &lt;a href="https://github.com/OpenBB-finance/backends-for-openbb"&gt;this open source repository&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;AI Agents integration:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;You can learn more about adding AI agents to the OpenBB workspace from &lt;a href="https://github.com/OpenBB-finance/agents-for-openbb"&gt;this open source repository&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Integrating OpenBB Platform to the OpenBB Workspace&lt;/h3&gt; 
&lt;p&gt;Connect this library to the OpenBB Workspace with a few simple commands, in a Python (3.9.21 - 3.12) environment.&lt;/p&gt; 
&lt;h4&gt;Run OpenBB Platform backend&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Install the packages.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;pip install "openbb[all]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Start the API server over localhost.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;openbb-api
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will launch a FastAPI server, via Uvicorn, at &lt;code&gt;127.0.0.1:6900&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;You can check that it works by going to &lt;a href="http://127.0.0.1:6900"&gt;http://127.0.0.1:6900&lt;/a&gt;.&lt;/p&gt; 
&lt;h4&gt;Integrate OpenBB Platform backend to OpenBB Workspace&lt;/h4&gt; 
&lt;p&gt;Sign-in to the &lt;a href="https://pro.openbb.co/"&gt;OpenBB Workspace&lt;/a&gt;, and follow the following steps:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/75cffb4a-5e95-470a-b9d0-6ffd4067e069" alt="CleanShot 2025-05-17 at 09 51 56@2x" /&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Go to the "Apps" tab&lt;/li&gt; 
 &lt;li&gt;Click on "Connect backend"&lt;/li&gt; 
 &lt;li&gt;Fill in the form with: Name: OpenBB Platform URL: &lt;a href="http://127.0.0.1:6900"&gt;http://127.0.0.1:6900&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Click on "Test". You should get a "Test successful" with the number of apps found.&lt;/li&gt; 
 &lt;li&gt;Click on "Add".&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;That's it.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;!-- TABLE OF CONTENTS --&gt; 
&lt;details closed="closed"&gt; 
 &lt;summary&gt;&lt;h2 style="display: inline-block"&gt;Table of Contents&lt;/h2&gt;&lt;/summary&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#1-installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#2-contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#3-license"&gt;License&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#4-disclaimer"&gt;Disclaimer&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#5-contacts"&gt;Contacts&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#6-star-history"&gt;Star History&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#7-contributors"&gt;Contributors&lt;/a&gt;&lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/details&gt; 
&lt;h2&gt;1. Installation&lt;/h2&gt; 
&lt;p&gt;The OpenBB Platform can be installed as a &lt;a href="https://pypi.org/project/openbb/"&gt;PyPI package&lt;/a&gt; by running &lt;code&gt;pip install openbb&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;or by cloning the repository directly with &lt;code&gt;git clone https://github.com/OpenBB-finance/OpenBB.git&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Please find more about the installation process, in the &lt;a href="https://docs.openbb.co/platform/installation"&gt;OpenBB Documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;OpenBB Platform CLI installation&lt;/h3&gt; 
&lt;p&gt;The OpenBB Platform CLI is a command-line interface that allows you to access the OpenBB Platform directly from your command line.&lt;/p&gt; 
&lt;p&gt;It can be installed by running &lt;code&gt;pip install openbb-cli&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;or by cloning the repository directly with &lt;code&gt;git clone https://github.com/OpenBB-finance/OpenBB.git&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Please find more about the installation process in the &lt;a href="https://docs.openbb.co/cli/installation"&gt;OpenBB Documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;2. Contributing&lt;/h2&gt; 
&lt;p&gt;There are three main ways of contributing to this project. (Hopefully you have starred the project by now ⭐️)&lt;/p&gt; 
&lt;h3&gt;Become a Contributor&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;More information on our &lt;a href="https://docs.openbb.co/platform/developer_guide/misc/contributing"&gt;Contributing Documentation&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Create a GitHub ticket&lt;/h3&gt; 
&lt;p&gt;Before creating a ticket make sure the one you are creating doesn't exist already &lt;a href="https://github.com/OpenBB-finance/OpenBB/issues"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&amp;amp;labels=bug&amp;amp;template=bug_report.md&amp;amp;title=%5BBug%5D"&gt;Report bug&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&amp;amp;labels=enhancement&amp;amp;template=enhancement.md&amp;amp;title=%5BIMPROVE%5D"&gt;Suggest improvement&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&amp;amp;labels=new+feature&amp;amp;template=feature_request.md&amp;amp;title=%5BFR%5D"&gt;Request a feature&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Provide feedback&lt;/h3&gt; 
&lt;p&gt;We are most active on &lt;a href="https://openbb.co/discord"&gt;our Discord&lt;/a&gt;, but feel free to reach out to us in any of &lt;a href="https://openbb.co/links"&gt;our social media&lt;/a&gt; for feedback.&lt;/p&gt; 
&lt;h2&gt;3. License&lt;/h2&gt; 
&lt;p&gt;Distributed under the AGPLv3 License. See &lt;a href="https://github.com/OpenBB-finance/OpenBB/raw/main/LICENSE"&gt;LICENSE&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h2&gt;4. Disclaimer&lt;/h2&gt; 
&lt;p&gt;Trading in financial instruments involves high risks including the risk of losing some, or all, of your investment amount, and may not be suitable for all investors.&lt;/p&gt; 
&lt;p&gt;Before deciding to trade in a financial instrument you should be fully informed of the risks and costs associated with trading the financial markets, carefully consider your investment objectives, level of experience, and risk appetite, and seek professional advice where needed.&lt;/p&gt; 
&lt;p&gt;The data contained in the OpenBB Platform is not necessarily accurate.&lt;/p&gt; 
&lt;p&gt;OpenBB and any provider of the data contained in this website will not accept liability for any loss or damage as a result of your trading, or your reliance on the information displayed.&lt;/p&gt; 
&lt;p&gt;All names, logos, and brands of third parties that may be referenced in our sites, products or documentation are trademarks of their respective owners. Unless otherwise specified, OpenBB and its products and services are not endorsed by, sponsored by, or affiliated with these third parties.&lt;/p&gt; 
&lt;p&gt;Our use of these names, logos, and brands is for identification purposes only, and does not imply any such endorsement, sponsorship, or affiliation.&lt;/p&gt; 
&lt;h2&gt;5. Contacts&lt;/h2&gt; 
&lt;p&gt;If you have any questions about the platform or anything OpenBB, feel free to email us at &lt;code&gt;support@openbb.co&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;If you want to say hi, or are interested in partnering with us, feel free to reach us at &lt;code&gt;hello@openbb.co&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Any of our social media platforms: &lt;a href="https://openbb.co/links"&gt;openbb.co/links&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;6. Star History&lt;/h2&gt; 
&lt;p&gt;This is a proxy of our growth and that we are just getting started.&lt;/p&gt; 
&lt;p&gt;But for more metrics important to us check &lt;a href="https://openbb.co/open"&gt;openbb.co/open&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://api.star-history.com/svg?repos=openbb-finance/OpenBB&amp;amp;type=Date&amp;amp;theme=dark"&gt;&lt;img src="https://api.star-history.com/svg?repos=openbb-finance/OpenBB&amp;amp;type=Date&amp;amp;theme=dark" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;7. Contributors&lt;/h2&gt; 
&lt;p&gt;OpenBB wouldn't be OpenBB without you. If we are going to disrupt financial industry, every contribution counts. Thank you for being part of this journey.&lt;/p&gt; 
&lt;a href="https://github.com/OpenBB-finance/OpenBB/graphs/contributors"&gt; &lt;img src="https://contributors-img.web.app/image?repo=OpenBB-finance/OpenBB" width="800" /&gt; &lt;/a&gt; 
&lt;!-- MARKDOWN LINKS &amp; IMAGES --&gt; 
&lt;!-- https://www.markdownguide.org/basic-syntax/#reference-style-links --&gt;</description>
    </item>
    
    <item>
      <title>opendatalab/MinerU</title>
      <link>https://github.com/opendatalab/MinerU</link>
      <description>&lt;p&gt;Transforms complex documents like PDFs into LLM-ready markdown/JSON for your Agentic workflows.&lt;/p&gt;&lt;hr&gt;&lt;div align="center" xmlns="http://www.w3.org/1999/html"&gt; 
 &lt;!-- logo --&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/opendatalab/MinerU/master/docs/images/MinerU-logo.png" width="300px" style="vertical-align:middle;" /&gt; &lt;/p&gt; 
 &lt;!-- icon --&gt; 
 &lt;p&gt;&lt;a href="https://github.com/opendatalab/MinerU"&gt;&lt;img src="https://img.shields.io/github/stars/opendatalab/MinerU.svg?sanitize=true" alt="stars" /&gt;&lt;/a&gt; &lt;a href="https://github.com/opendatalab/MinerU"&gt;&lt;img src="https://img.shields.io/github/forks/opendatalab/MinerU.svg?sanitize=true" alt="forks" /&gt;&lt;/a&gt; &lt;a href="https://github.com/opendatalab/MinerU/issues"&gt;&lt;img src="https://img.shields.io/github/issues-raw/opendatalab/MinerU" alt="open issues" /&gt;&lt;/a&gt; &lt;a href="https://github.com/opendatalab/MinerU/issues"&gt;&lt;img src="https://img.shields.io/github/issues-closed-raw/opendatalab/MinerU" alt="issue resolution" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/mineru/"&gt;&lt;img src="https://img.shields.io/pypi/v/mineru" alt="PyPI version" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/mineru/"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/mineru" alt="PyPI - Python Version" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/mineru"&gt;&lt;img src="https://static.pepy.tech/badge/mineru" alt="Downloads" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/mineru"&gt;&lt;img src="https://static.pepy.tech/badge/mineru/month" alt="Downloads" /&gt;&lt;/a&gt; &lt;a href="https://mineru.net/OpenSourceTools/Extractor?source=github"&gt;&lt;img src="https://img.shields.io/badge/webapp_on_mineru.net-blue?logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTM0IiBoZWlnaHQ9IjEzNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBkPSJtMTIyLDljMCw1LTQsOS05LDlzLTktNC05LTksNC05LDktOSw5LDQsOSw5eiIgZmlsbD0idXJsKCNhKSIvPjxwYXRoIGQ9Im0xMjIsOWMwLDUtNCw5LTksOXMtOS00LTktOSw0LTksOS05LDksNCw5LDl6IiBmaWxsPSIjMDEwMTAxIi8+PHBhdGggZD0ibTkxLDE4YzAsNS00LDktOSw5cy05LTQtOS05LDQtOSw5LTksOSw0LDksOXoiIGZpbGw9InVybCgjYikiLz48cGF0aCBkPSJtOTEsMThjMCw1LTQsOS05LDlzLTktNC05LTksNC05LDktOSw5LDQsOSw5eiIgZmlsbD0iIzAxMDEwMSIvPjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJtMzksNjJjMCwxNiw4LDMwLDIwLDM4LDctNiwxMi0xNiwxMi0yNlY0OWMwLTQsMy03LDYtOGw0Ni0xMmM1LTEsMTEsMywxMSw4djMxYzAsMzctMzAsNjYtNjYsNjYtMzcsMC02Ni0zMC02Ni02NlY0NmMwLTQsMy03LDYtOGwyMC02YzUtMSwxMSwzLDExLDh2MjF6bS0yOSw2YzAsMTYsNiwzMCwxNyw0MCwzLDEsNSwxLDgsMSw1LDAsMTAtMSwxNS0zQzM3LDk1LDI5LDc5LDI5LDYyVjQybC0xOSw1djIweiIgZmlsbD0idXJsKCNjKSIvPjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJtMzksNjJjMCwxNiw4LDMwLDIwLDM4LDctNiwxMi0xNiwxMi0yNlY0OWMwLTQsMy03LDYtOGw0Ni0xMmM1LTEsMTEsMywxMSw4djMxYzAsMzctMzAsNjYtNjYsNjYtMzcsMC02Ni0zMC02Ni02NlY0NmMwLTQsMy03LDYtOGwyMC02YzUtMSwxMSwzLDExLDh2MjF6bS0yOSw2YzAsMTYsNiwzMCwxNyw0MCwzLDEsNSwxLDgsMSw1LDAsMTAtMSwxNS0zQzM3LDk1LDI5LDc5LDI5LDYyVjQybC0xOSw1djIweiIgZmlsbD0iIzAxMDEwMSIvPjxkZWZzPjxsaW5lYXJHcmFkaWVudCBpZD0iYSIgeDE9Ijg0IiB5MT0iNDEiIHgyPSI3NSIgeTI9IjEyMCIgZ3JhZGllbnRVbml0cz0idXNlclNwYWNlT25Vc2UiPjxzdG9wIHN0b3AtY29sb3I9IiNmZmYiLz48c3RvcCBvZmZzZXQ9IjEiIHN0b3AtY29sb3I9IiMyZTJlMmUiLz48L2xpbmVhckdyYWRpZW50PjxsaW5lYXJHcmFkaWVudCBpZD0iYiIgeDE9Ijg0IiB5MT0iNDEiIHgyPSI3NSIgeTI9IjEyMCIgZ3JhZGllbnRVbml0cz0idXNlclNwYWNlT25Vc2UiPjxzdG9wIHN0b3AtY29sb3I9IiNmZmYiLz48c3RvcCBvZmZzZXQ9IjEiIHN0b3AtY29sb3I9IiMyZTJlMmUiLz48L2xpbmVhckdyYWRpZW50PjxsaW5lYXJHcmFkaWVudCBpZD0iYyIgeDE9Ijg0IiB5MT0iNDEiIHgyPSI3NSIgeTI9IjEyMCIgZ3JhZGllbnRVbml0cz0idXNlclNwYWNlT25Vc2UiPjxzdG9wIHN0b3AtY29sb3I9IiNmZmYiLz48c3RvcCBvZmZzZXQ9IjEiIHN0b3AtY29sb3I9IiMyZTJlMmUiLz48L2xpbmVhckdyYWRpZW50PjwvZGVmcz48L3N2Zz4=&amp;amp;labelColor=white" alt="OpenDataLab" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/spaces/opendatalab/MinerU"&gt;&lt;img src="https://img.shields.io/badge/Demo_on_HuggingFace-yellow.svg?logo=data:image/png;base64,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&amp;amp;labelColor=white" alt="HuggingFace" /&gt;&lt;/a&gt; &lt;a href="https://www.modelscope.cn/studios/OpenDataLab/MinerU"&gt;&lt;img src="https://img.shields.io/badge/Demo_on_ModelScope-purple?logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIzIiBoZWlnaHQ9IjIwMCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCiA8Zz4KICA8dGl0bGU+TGF5ZXIgMTwvdGl0bGU+CiAgPHBhdGggaWQ9InN2Z18xNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTAsODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTUiIGZpbGw9IiM2MjRhZmYiIGQ9Im05OS4xNCwxMTUuNDlsMjUuNjUsMGwwLDI1LjY1bC0yNS42NSwwbDAsLTI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTYiIGZpbGw9IiM2MjRhZmYiIGQ9Im0xNzYuMDksMTQxLjE0bC0yNS42NDk5OSwwbDAsMjIuMTlsNDcuODQsMGwwLC00Ny44NGwtMjIuMTksMGwwLDI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTciIGZpbGw9IiMzNmNmZDEiIGQ9Im0xMjQuNzksODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTgiIGZpbGw9IiMzNmNmZDEiIGQ9Im0wLDY0LjE5bDI1LjY1LDBsMCwyNS42NWwtMjUuNjUsMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzE5IiBmaWxsPSIjNjI0YWZmIiBkPSJtMTk4LjI4LDg5Ljg0bDI1LjY0OTk5LDBsMCwyNS42NDk5OWwtMjUuNjQ5OTksMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIwIiBmaWxsPSIjMzZjZmQxIiBkPSJtMTk4LjI4LDY0LjE5bDI1LjY0OTk5LDBsMCwyNS42NWwtMjUuNjQ5OTksMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIxIiBmaWxsPSIjNjI0YWZmIiBkPSJtMTUwLjQ0LDQybDAsMjIuMTlsMjUuNjQ5OTksMGwwLDI1LjY1bDIyLjE5LDBsMCwtNDcuODRsLTQ3Ljg0LDB6Ii8+CiAgPHBhdGggaWQ9InN2Z18yMiIgZmlsbD0iIzM2Y2ZkMSIgZD0ibTczLjQ5LDg5Ljg0bDI1LjY1LDBsMCwyNS42NDk5OWwtMjUuNjUsMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIzIiBmaWxsPSIjNjI0YWZmIiBkPSJtNDcuODQsNjQuMTlsMjUuNjUsMGwwLC0yMi4xOWwtNDcuODQsMGwwLDQ3Ljg0bDIyLjE5LDBsMCwtMjUuNjV6Ii8+CiAgPHBhdGggaWQ9InN2Z18yNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTQ3Ljg0LDExNS40OWwtMjIuMTksMGwwLDQ3Ljg0bDQ3Ljg0LDBsMCwtMjIuMTlsLTI1LjY1LDBsMCwtMjUuNjV6Ii8+CiA8L2c+Cjwvc3ZnPg==&amp;amp;labelColor=white" alt="ModelScope" /&gt;&lt;/a&gt; &lt;a href="https://colab.research.google.com/gist/myhloli/a3cb16570ab3cfeadf9d8f0ac91b4fca/mineru_demo.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Colab" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2409.18839"&gt;&lt;img src="https://img.shields.io/badge/MinerU-Technical%20Report-b31b1b.svg?logo=arXiv" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2509.22186"&gt;&lt;img src="https://img.shields.io/badge/MinerU2.5-Technical%20Report-b31b1b.svg?logo=arXiv" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/opendatalab/MinerU"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/11174" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/11174" alt="opendatalab%2FMinerU | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;!-- language --&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/opendatalab/MinerU/master/README.md"&gt;English&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/opendatalab/MinerU/master/README_zh-CN.md"&gt;简体中文&lt;/a&gt;&lt;/p&gt; 
 &lt;!-- hot link --&gt; 
 &lt;p align="center"&gt; 🚀&lt;a href="https://mineru.net/?source=github"&gt;Access MinerU Now→✅ Zero-Install Web Version ✅ Full-Featured Desktop Client ✅ Instant API Access; Skip deployment headaches – get all product formats in one click. Developers, dive in!&lt;/a&gt; &lt;/p&gt; 
 &lt;!-- join us --&gt; 
 &lt;p align="center"&gt; 👋 join us on &lt;a href="https://discord.gg/Tdedn9GTXq" target="_blank"&gt;Discord&lt;/a&gt; and &lt;a href="https://mineru.net/community-portal/?aliasId=3c430f94" target="_blank"&gt;WeChat&lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h1&gt;Changelog&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;2025/09/26 2.5.4 released&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;🎉🎉 The MinerU2.5 &lt;a href="https://arxiv.org/abs/2509.22186"&gt;Technical Report&lt;/a&gt; is now available! We welcome you to read it for a comprehensive overview of its model architecture, training strategy, data engineering and evaluation results.&lt;/li&gt; 
   &lt;li&gt;Fixed an issue where some &lt;code&gt;PDF&lt;/code&gt; files were mistakenly identified as &lt;code&gt;AI&lt;/code&gt; files, causing parsing failures&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025/09/20 2.5.3 Released&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Dependency version range adjustment to enable Turing and earlier architecture GPUs to use vLLM acceleration for MinerU2.5 model inference.&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;pipeline&lt;/code&gt; backend compatibility fixes for torch 2.8.0.&lt;/li&gt; 
   &lt;li&gt;Reduced default concurrency for vLLM async backend to lower server pressure and avoid connection closure issues caused by high load.&lt;/li&gt; 
   &lt;li&gt;More compatibility-related details can be found in the &lt;a href="https://github.com/opendatalab/MinerU/discussions/3548"&gt;announcement&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025/09/19 2.5.2 Released&lt;/p&gt; &lt;p&gt;We are officially releasing MinerU2.5, currently the most powerful multimodal large model for document parsing. With only 1.2B parameters, MinerU2.5's accuracy on the OmniDocBench benchmark comprehensively surpasses top-tier multimodal models like Gemini 2.5 Pro, GPT-4o, and Qwen2.5-VL-72B. It also significantly outperforms leading specialized models such as dots.ocr, MonkeyOCR, and PP-StructureV3. The model has been released on &lt;a href="https://huggingface.co/opendatalab/MinerU2.5-2509-1.2B"&gt;HuggingFace&lt;/a&gt; and &lt;a href="https://modelscope.cn/models/opendatalab/MinerU2.5-2509-1.2B"&gt;ModelScope&lt;/a&gt; platforms. Welcome to download and use!&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Core Highlights: 
    &lt;ul&gt; 
     &lt;li&gt;SOTA Performance with Extreme Efficiency: As a 1.2B model, it achieves State-of-the-Art (SOTA) results that exceed models in the 10B and 100B+ classes, redefining the performance-per-parameter standard in document AI.&lt;/li&gt; 
     &lt;li&gt;Advanced Architecture for Across-the-Board Leadership: By combining a two-stage inference pipeline (decoupling layout analysis from content recognition) with a native high-resolution architecture, it achieves SOTA performance across five key areas: layout analysis, text recognition, formula recognition, table recognition, and reading order.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Key Capability Enhancements: 
    &lt;ul&gt; 
     &lt;li&gt;Layout Detection: Delivers more complete results by accurately covering non-body content like headers, footers, and page numbers. It also provides more precise element localization and natural format reconstruction for lists and references.&lt;/li&gt; 
     &lt;li&gt;Table Parsing: Drastically improves parsing for challenging cases, including rotated tables, borderless/semi-structured tables, and long/complex tables.&lt;/li&gt; 
     &lt;li&gt;Formula Recognition: Significantly boosts accuracy for complex, long-form, and hybrid Chinese-English formulas, greatly enhancing the parsing capability for mathematical documents.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;p&gt;Additionally, with the release of vlm 2.5, we have made some adjustments to the repository:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;The vlm backend has been upgraded to version 2.5, supporting the MinerU2.5 model and no longer compatible with the MinerU2.0-2505-0.9B model. The last version supporting the 2.0 model is mineru-2.2.2.&lt;/li&gt; 
   &lt;li&gt;VLM inference-related code has been moved to &lt;a href="https://github.com/opendatalab/mineru-vl-utils"&gt;mineru_vl_utils&lt;/a&gt;, reducing coupling with the main mineru repository and facilitating independent iteration in the future.&lt;/li&gt; 
   &lt;li&gt;The vlm accelerated inference framework has been switched from &lt;code&gt;sglang&lt;/code&gt; to &lt;code&gt;vllm&lt;/code&gt;, achieving full compatibility with the vllm ecosystem, allowing users to use the MinerU2.5 model and accelerated inference on any platform that supports the vllm framework.&lt;/li&gt; 
   &lt;li&gt;Due to major upgrades in the vlm model supporting more layout types, we have made some adjustments to the structure of the parsing intermediate file &lt;code&gt;middle.json&lt;/code&gt; and result file &lt;code&gt;content_list.json&lt;/code&gt;. Please refer to the &lt;a href="https://opendatalab.github.io/MinerU/reference/output_files/"&gt;documentation&lt;/a&gt; for details.&lt;/li&gt; 
  &lt;/ul&gt; &lt;p&gt;Other repository optimizations:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Removed file extension whitelist validation for input files. When input files are PDF documents or images, there are no longer requirements for file extensions, improving usability.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;History Log&lt;/summary&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/09/10 2.2.2 Released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Fixed the issue where the new table recognition model would affect the overall parsing task when some table parsing failed&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/09/08 2.2.1 Released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Fixed the issue where some newly added models were not downloaded when using the model download command.&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/09/05 2.2.0 Released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt; Major Updates 
    &lt;ul&gt; 
     &lt;li&gt;In this version, we focused on improving table parsing accuracy by introducing a new &lt;a href="https://github.com/RapidAI/TableStructureRec"&gt;wired table recognition model&lt;/a&gt; and a brand-new hybrid table structure parsing algorithm, significantly enhancing the table recognition capabilities of the &lt;code&gt;pipeline&lt;/code&gt; backend.&lt;/li&gt; 
     &lt;li&gt;We also added support for cross-page table merging, which is supported by both &lt;code&gt;pipeline&lt;/code&gt; and &lt;code&gt;vlm&lt;/code&gt; backends, further improving the completeness and accuracy of table parsing.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt; Other Updates 
    &lt;ul&gt; 
     &lt;li&gt;The &lt;code&gt;pipeline&lt;/code&gt; backend now supports 270-degree rotated table parsing, bringing support for table parsing in 0/90/270-degree orientations&lt;/li&gt; 
     &lt;li&gt;&lt;code&gt;pipeline&lt;/code&gt; added OCR capability support for Thai and Greek, and updated the English OCR model to the latest version. English recognition accuracy improved by 11%, Thai recognition model accuracy is 82.68%, and Greek recognition model accuracy is 89.28% (by PPOCRv5)&lt;/li&gt; 
     &lt;li&gt;Added &lt;code&gt;bbox&lt;/code&gt; field (mapped to 0-1000 range) in the output &lt;code&gt;content_list.json&lt;/code&gt;, making it convenient for users to directly obtain position information for each content block&lt;/li&gt; 
     &lt;li&gt;Removed the &lt;code&gt;pipeline_old_linux&lt;/code&gt; installation option, no longer supporting legacy Linux systems such as &lt;code&gt;CentOS 7&lt;/code&gt;, to provide better support for &lt;code&gt;uv&lt;/code&gt;'s &lt;code&gt;sync&lt;/code&gt;/&lt;code&gt;run&lt;/code&gt; commands&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/08/01 2.1.10 Released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Fixed an issue in the &lt;code&gt;pipeline&lt;/code&gt; backend where block overlap caused the parsing results to deviate from expectations #3232&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/07/30 2.1.9 Released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;transformers&lt;/code&gt; 4.54.1 version adaptation&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/07/28 2.1.8 Released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;sglang&lt;/code&gt; 0.4.9.post5 version adaptation&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/07/27 2.1.7 Released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;transformers&lt;/code&gt; 4.54.0 version adaptation&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/07/26 2.1.6 Released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Fixed table parsing issues in handwritten documents when using &lt;code&gt;vlm&lt;/code&gt; backend&lt;/li&gt; 
   &lt;li&gt;Fixed visualization box position drift issue when document is rotated #3175&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/07/24 2.1.5 Released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;sglang&lt;/code&gt; 0.4.9 version adaptation, synchronously upgrading the dockerfile base image to sglang 0.4.9.post3&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/07/23 2.1.4 Released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Bug Fixes&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Fixed the issue of excessive memory consumption during the &lt;code&gt;MFR&lt;/code&gt; step in the &lt;code&gt;pipeline&lt;/code&gt; backend under certain scenarios #2771&lt;/li&gt; 
     &lt;li&gt;Fixed the inaccurate matching between &lt;code&gt;image&lt;/code&gt;/&lt;code&gt;table&lt;/code&gt; and &lt;code&gt;caption&lt;/code&gt;/&lt;code&gt;footnote&lt;/code&gt; under certain conditions #3129&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/07/16 2.1.1 Released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Bug fixes&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Fixed text block content loss issue that could occur in certain &lt;code&gt;pipeline&lt;/code&gt; scenarios #3005&lt;/li&gt; 
     &lt;li&gt;Fixed issue where &lt;code&gt;sglang-client&lt;/code&gt; required unnecessary packages like &lt;code&gt;torch&lt;/code&gt; #2968&lt;/li&gt; 
     &lt;li&gt;Updated &lt;code&gt;dockerfile&lt;/code&gt; to fix incomplete text content parsing due to missing fonts in Linux #2915&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Usability improvements&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Updated &lt;code&gt;compose.yaml&lt;/code&gt; to facilitate direct startup of &lt;code&gt;sglang-server&lt;/code&gt;, &lt;code&gt;mineru-api&lt;/code&gt;, and &lt;code&gt;mineru-gradio&lt;/code&gt; services&lt;/li&gt; 
     &lt;li&gt;Launched brand new &lt;a href="https://opendatalab.github.io/MinerU/"&gt;online documentation site&lt;/a&gt;, simplified readme, providing better documentation experience&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/07/05 2.1.0 Released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;This is the first major update of MinerU 2, which includes a large number of new features and improvements, covering significant performance optimizations, user experience enhancements, and bug fixes. The detailed update contents are as follows:&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Performance Optimizations:&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Significantly improved preprocessing speed for documents with specific resolutions (around 2000 pixels on the long side).&lt;/li&gt; 
     &lt;li&gt;Greatly enhanced post-processing speed when the &lt;code&gt;pipeline&lt;/code&gt; backend handles batch processing of documents with fewer pages (&amp;lt;10 pages).&lt;/li&gt; 
     &lt;li&gt;Layout analysis speed of the &lt;code&gt;pipeline&lt;/code&gt; backend has been increased by approximately 20%.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Experience Enhancements:&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Built-in ready-to-use &lt;code&gt;fastapi service&lt;/code&gt; and &lt;code&gt;gradio webui&lt;/code&gt;. For detailed usage instructions, please refer to &lt;a href="https://opendatalab.github.io/MinerU/usage/quick_usage/#advanced-usage-via-api-webui-sglang-clientserver"&gt;Documentation&lt;/a&gt;.&lt;/li&gt; 
     &lt;li&gt;Adapted to &lt;code&gt;sglang&lt;/code&gt; version &lt;code&gt;0.4.8&lt;/code&gt;, significantly reducing the GPU memory requirements for the &lt;code&gt;vlm-sglang&lt;/code&gt; backend. It can now run on graphics cards with as little as &lt;code&gt;8GB GPU memory&lt;/code&gt; (Turing architecture or newer).&lt;/li&gt; 
     &lt;li&gt;Added transparent parameter passing for all commands related to &lt;code&gt;sglang&lt;/code&gt;, allowing the &lt;code&gt;sglang-engine&lt;/code&gt; backend to receive all &lt;code&gt;sglang&lt;/code&gt; parameters consistently with the &lt;code&gt;sglang-server&lt;/code&gt;.&lt;/li&gt; 
     &lt;li&gt;Supports feature extensions based on configuration files, including &lt;code&gt;custom formula delimiters&lt;/code&gt;, &lt;code&gt;enabling heading classification&lt;/code&gt;, and &lt;code&gt;customizing local model directories&lt;/code&gt;. For detailed usage instructions, please refer to &lt;a href="https://opendatalab.github.io/MinerU/usage/quick_usage/#extending-mineru-functionality-with-configuration-files"&gt;Documentation&lt;/a&gt;.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;New Features:&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Updated the &lt;code&gt;pipeline&lt;/code&gt; backend with the PP-OCRv5 multilingual text recognition model, supporting text recognition in 37 languages such as French, Spanish, Portuguese, Russian, and Korean, with an average accuracy improvement of over 30%. &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/algorithm/PP-OCRv5/PP-OCRv5_multi_languages.html"&gt;Details&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;Introduced limited support for vertical text layout in the &lt;code&gt;pipeline&lt;/code&gt; backend.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/06/20 2.0.6 Released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Fixed occasional parsing interruptions caused by invalid block content in &lt;code&gt;vlm&lt;/code&gt; mode&lt;/li&gt; 
   &lt;li&gt;Fixed parsing interruptions caused by incomplete table structures in &lt;code&gt;vlm&lt;/code&gt; mode&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/06/17 2.0.5 Released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Fixed the issue where models were still required to be downloaded in the &lt;code&gt;sglang-client&lt;/code&gt; mode&lt;/li&gt; 
   &lt;li&gt;Fixed the issue where the &lt;code&gt;sglang-client&lt;/code&gt; mode unnecessarily depended on packages like &lt;code&gt;torch&lt;/code&gt; during runtime.&lt;/li&gt; 
   &lt;li&gt;Fixed the issue where only the first instance would take effect when attempting to launch multiple &lt;code&gt;sglang-client&lt;/code&gt; instances via multiple URLs within the same process&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/06/15 2.0.3 released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Fixed a configuration file key-value update error that occurred when downloading model type was set to &lt;code&gt;all&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Fixed the issue where the formula and table feature toggle switches were not working in &lt;code&gt;command line mode&lt;/code&gt;, causing the features to remain enabled.&lt;/li&gt; 
   &lt;li&gt;Fixed compatibility issues with sglang version 0.4.7 in the &lt;code&gt;sglang-engine&lt;/code&gt; mode.&lt;/li&gt; 
   &lt;li&gt;Updated Dockerfile and installation documentation for deploying the full version of MinerU in sglang environment&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/06/13 2.0.0 Released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;New Architecture&lt;/strong&gt;: MinerU 2.0 has been deeply restructured in code organization and interaction methods, significantly improving system usability, maintainability, and extensibility. 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Removal of Third-party Dependency Limitations&lt;/strong&gt;: Completely eliminated the dependency on &lt;code&gt;pymupdf&lt;/code&gt;, moving the project toward a more open and compliant open-source direction.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Ready-to-use, Easy Configuration&lt;/strong&gt;: No need to manually edit JSON configuration files; most parameters can now be set directly via command line or API.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Automatic Model Management&lt;/strong&gt;: Added automatic model download and update mechanisms, allowing users to complete model deployment without manual intervention.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Offline Deployment Friendly&lt;/strong&gt;: Provides built-in model download commands, supporting deployment requirements in completely offline environments.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Streamlined Code Structure&lt;/strong&gt;: Removed thousands of lines of redundant code, simplified class inheritance logic, significantly improving code readability and development efficiency.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Unified Intermediate Format Output&lt;/strong&gt;: Adopted standardized &lt;code&gt;middle_json&lt;/code&gt; format, compatible with most secondary development scenarios based on this format, ensuring seamless ecosystem business migration.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;New Model&lt;/strong&gt;: MinerU 2.0 integrates our latest small-parameter, high-performance multimodal document parsing model, achieving end-to-end high-speed, high-precision document understanding. 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;Small Model, Big Capabilities&lt;/strong&gt;: With parameters under 1B, yet surpassing traditional 72B-level vision-language models (VLMs) in parsing accuracy.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Multiple Functions in One&lt;/strong&gt;: A single model covers multilingual recognition, handwriting recognition, layout analysis, table parsing, formula recognition, reading order sorting, and other core tasks.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Ultimate Inference Speed&lt;/strong&gt;: Achieves peak throughput exceeding 10,000 tokens/s through &lt;code&gt;sglang&lt;/code&gt; acceleration on a single NVIDIA 4090 card, easily handling large-scale document processing requirements.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Online Experience&lt;/strong&gt;: You can experience our brand-new VLM model on &lt;a href="https://mineru.net/OpenSourceTools/Extractor"&gt;MinerU.net&lt;/a&gt;, &lt;a href="https://huggingface.co/spaces/opendatalab/MinerU"&gt;Hugging Face&lt;/a&gt;, and &lt;a href="https://www.modelscope.cn/studios/OpenDataLab/MinerU"&gt;ModelScope&lt;/a&gt;.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Incompatible Changes Notice&lt;/strong&gt;: To improve overall architectural rationality and long-term maintainability, this version contains some incompatible changes: 
    &lt;ul&gt; 
     &lt;li&gt;Python package name changed from &lt;code&gt;magic-pdf&lt;/code&gt; to &lt;code&gt;mineru&lt;/code&gt;, and the command-line tool changed from &lt;code&gt;magic-pdf&lt;/code&gt; to &lt;code&gt;mineru&lt;/code&gt;. Please update your scripts and command calls accordingly.&lt;/li&gt; 
     &lt;li&gt;For modular system design and ecosystem consistency considerations, MinerU 2.0 no longer includes the LibreOffice document conversion module. If you need to process Office documents, we recommend converting them to PDF format through an independently deployed LibreOffice service before proceeding with subsequent parsing operations.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/05/24 Release 1.3.12&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Added support for PPOCRv5 models, updated &lt;code&gt;ch_server&lt;/code&gt; model to &lt;code&gt;PP-OCRv5_rec_server&lt;/code&gt;, and &lt;code&gt;ch_lite&lt;/code&gt; model to &lt;code&gt;PP-OCRv5_rec_mobile&lt;/code&gt; (model update required) 
    &lt;ul&gt; 
     &lt;li&gt;In testing, we found that PPOCRv5(server) has some improvement for handwritten documents, but has slightly lower accuracy than v4_server_doc for other document types, so the default ch model remains unchanged as &lt;code&gt;PP-OCRv4_server_rec_doc&lt;/code&gt;.&lt;/li&gt; 
     &lt;li&gt;Since PPOCRv5 has enhanced recognition capabilities for handwriting and special characters, you can manually choose the PPOCRv5 model for Japanese-Traditional Chinese mixed scenarios and handwritten documents&lt;/li&gt; 
     &lt;li&gt;You can select the appropriate model through the lang parameter &lt;code&gt;lang='ch_server'&lt;/code&gt; (Python API) or &lt;code&gt;--lang ch_server&lt;/code&gt; (command line): 
      &lt;ul&gt; 
       &lt;li&gt;&lt;code&gt;ch&lt;/code&gt;: &lt;code&gt;PP-OCRv4_server_rec_doc&lt;/code&gt; (default) (Chinese/English/Japanese/Traditional Chinese mixed/15K dictionary)&lt;/li&gt; 
       &lt;li&gt;&lt;code&gt;ch_server&lt;/code&gt;: &lt;code&gt;PP-OCRv5_rec_server&lt;/code&gt; (Chinese/English/Japanese/Traditional Chinese mixed + handwriting/18K dictionary)&lt;/li&gt; 
       &lt;li&gt;&lt;code&gt;ch_lite&lt;/code&gt;: &lt;code&gt;PP-OCRv5_rec_mobile&lt;/code&gt; (Chinese/English/Japanese/Traditional Chinese mixed + handwriting/18K dictionary)&lt;/li&gt; 
       &lt;li&gt;&lt;code&gt;ch_server_v4&lt;/code&gt;: &lt;code&gt;PP-OCRv4_rec_server&lt;/code&gt; (Chinese/English mixed/6K dictionary)&lt;/li&gt; 
       &lt;li&gt;&lt;code&gt;ch_lite_v4&lt;/code&gt;: &lt;code&gt;PP-OCRv4_rec_mobile&lt;/code&gt; (Chinese/English mixed/6K dictionary)&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Added support for handwritten documents through optimized layout recognition of handwritten text areas 
    &lt;ul&gt; 
     &lt;li&gt;This feature is supported by default, no additional configuration required&lt;/li&gt; 
     &lt;li&gt;You can refer to the instructions above to manually select the PPOCRv5 model for better handwritten document parsing results&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;The &lt;code&gt;huggingface&lt;/code&gt; and &lt;code&gt;modelscope&lt;/code&gt; demos have been updated to versions that support handwriting recognition and PPOCRv5 models, which you can experience online&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/04/29 Release 1.3.10&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Added support for custom formula delimiters, which can be configured by modifying the &lt;code&gt;latex-delimiter-config&lt;/code&gt; section in the &lt;code&gt;magic-pdf.json&lt;/code&gt; file in your user directory.&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/04/27 Release 1.3.9&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Optimized formula parsing functionality, improved formula rendering success rate&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/04/23 Release 1.3.8&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;The default &lt;code&gt;ocr&lt;/code&gt; model (&lt;code&gt;ch&lt;/code&gt;) has been updated to &lt;code&gt;PP-OCRv4_server_rec_doc&lt;/code&gt; (model update required) 
    &lt;ul&gt; 
     &lt;li&gt;&lt;code&gt;PP-OCRv4_server_rec_doc&lt;/code&gt; is trained on a mixture of more Chinese document data and PP-OCR training data based on &lt;code&gt;PP-OCRv4_server_rec&lt;/code&gt;, adding recognition capabilities for some traditional Chinese characters, Japanese, and special characters. It can recognize over 15,000 characters and improves both document-specific and general text recognition abilities.&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://paddlepaddle.github.io/PaddleX/latest/module_usage/tutorials/ocr_modules/text_recognition.html#_3"&gt;Performance comparison of PP-OCRv4_server_rec_doc/PP-OCRv4_server_rec/PP-OCRv4_mobile_rec&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;After verification, the &lt;code&gt;PP-OCRv4_server_rec_doc&lt;/code&gt; model shows significant accuracy improvements in Chinese/English/Japanese/Traditional Chinese in both single language and mixed language scenarios, with comparable speed to &lt;code&gt;PP-OCRv4_server_rec&lt;/code&gt;, making it suitable for most use cases.&lt;/li&gt; 
     &lt;li&gt;In some pure English scenarios, &lt;code&gt;PP-OCRv4_server_rec_doc&lt;/code&gt; may have word adhesion issues, while &lt;code&gt;PP-OCRv4_server_rec&lt;/code&gt; performs better in these cases. Therefore, we've kept the &lt;code&gt;PP-OCRv4_server_rec&lt;/code&gt; model, which users can access by adding the parameter &lt;code&gt;lang='ch_server'&lt;/code&gt; (Python API) or &lt;code&gt;--lang ch_server&lt;/code&gt; (command line).&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/04/22 Release 1.3.7&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Fixed the issue where the lang parameter was ineffective during table parsing model initialization&lt;/li&gt; 
   &lt;li&gt;Fixed the significant speed reduction of OCR and table parsing in &lt;code&gt;cpu&lt;/code&gt; mode&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/04/16 Release 1.3.4&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Slightly improved OCR-det speed by removing some unnecessary blocks&lt;/li&gt; 
   &lt;li&gt;Fixed page-internal sorting errors caused by footnotes in certain cases&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/04/12 Release 1.3.2&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Fixed dependency version incompatibility issues when installing on Windows with Python 3.13&lt;/li&gt; 
   &lt;li&gt;Optimized memory usage during batch inference&lt;/li&gt; 
   &lt;li&gt;Improved parsing of tables rotated 90 degrees&lt;/li&gt; 
   &lt;li&gt;Enhanced parsing of oversized tables in financial report samples&lt;/li&gt; 
   &lt;li&gt;Fixed the occasional word adhesion issue in English text areas when OCR language is not specified (model update required)&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/04/08 Release 1.3.1&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Fixed several compatibility issues 
    &lt;ul&gt; 
     &lt;li&gt;Added support for Python 3.13&lt;/li&gt; 
     &lt;li&gt;Made final adaptations for outdated Linux systems (such as CentOS 7) with no guarantee of continued support in future versions, &lt;a href="https://github.com/opendatalab/MinerU/issues/1004"&gt;installation instructions&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/04/03 Release 1.3.0&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Installation and compatibility optimizations 
    &lt;ul&gt; 
     &lt;li&gt;Resolved compatibility issues caused by &lt;code&gt;detectron2&lt;/code&gt; by removing &lt;code&gt;layoutlmv3&lt;/code&gt; usage in layout&lt;/li&gt; 
     &lt;li&gt;Extended torch version compatibility to 2.2~2.6 (excluding 2.5)&lt;/li&gt; 
     &lt;li&gt;Added CUDA compatibility for versions 11.8/12.4/12.6/12.8 (CUDA version determined by torch), solving compatibility issues for users with 50-series and H-series GPUs&lt;/li&gt; 
     &lt;li&gt;Extended Python compatibility to versions 3.10~3.12, fixing the issue of automatic downgrade to version 0.6.1 when installing in non-3.10 environments&lt;/li&gt; 
     &lt;li&gt;Optimized offline deployment process, eliminating the need to download any model files after successful deployment&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Performance optimizations 
    &lt;ul&gt; 
     &lt;li&gt;Enhanced parsing speed for batches of small files by supporting batch processing of multiple PDF files (&lt;a href="https://raw.githubusercontent.com/opendatalab/MinerU/master/demo/batch_demo.py"&gt;script example&lt;/a&gt;), with formula parsing speed improved by up to 1400% and overall parsing speed improved by up to 500% compared to version 1.0.1&lt;/li&gt; 
     &lt;li&gt;Reduced memory usage and improved parsing speed by optimizing MFR model loading and usage (requires re-running the &lt;a href="https://raw.githubusercontent.com/opendatalab/MinerU/master/docs/how_to_download_models_zh_cn.md"&gt;model download process&lt;/a&gt; to get incremental updates to model files)&lt;/li&gt; 
     &lt;li&gt;Optimized GPU memory usage, requiring only 6GB minimum to run this project&lt;/li&gt; 
     &lt;li&gt;Improved running speed on MPS devices&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Parsing effect optimizations 
    &lt;ul&gt; 
     &lt;li&gt;Updated MFR model to &lt;code&gt;unimernet(2503)&lt;/code&gt;, fixing line break loss issues in multi-line formulas&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Usability optimizations 
    &lt;ul&gt; 
     &lt;li&gt;Completely replaced the &lt;code&gt;paddle&lt;/code&gt; framework and &lt;code&gt;paddleocr&lt;/code&gt; in the project by using &lt;code&gt;paddleocr2torch&lt;/code&gt;, resolving conflicts between &lt;code&gt;paddle&lt;/code&gt; and &lt;code&gt;torch&lt;/code&gt;, as well as thread safety issues caused by the &lt;code&gt;paddle&lt;/code&gt; framework&lt;/li&gt; 
     &lt;li&gt;Added real-time progress bar display during parsing, allowing precise tracking of parsing progress and making the waiting process more bearable&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/03/03 1.2.1 released&lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Fixed the impact on punctuation marks during full-width to half-width conversion of letters and numbers&lt;/li&gt; 
   &lt;li&gt;Fixed caption matching inaccuracies in certain scenarios&lt;/li&gt; 
   &lt;li&gt;Fixed formula span loss issues in certain scenarios&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/02/24 1.2.0 released&lt;/summary&gt; 
  &lt;p&gt;This version includes several fixes and improvements to enhance parsing efficiency and accuracy:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Performance Optimization&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Increased classification speed for PDF documents in auto mode.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Parsing Optimization&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Improved parsing logic for documents containing watermarks, significantly enhancing the parsing results for such documents.&lt;/li&gt; 
     &lt;li&gt;Enhanced the matching logic for multiple images/tables and captions within a single page, improving the accuracy of image-text matching in complex layouts.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Bug Fixes&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Fixed an issue where image/table spans were incorrectly filled into text blocks under certain conditions.&lt;/li&gt; 
     &lt;li&gt;Resolved an issue where title blocks were empty in some cases.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/01/22 1.1.0 released&lt;/summary&gt; 
  &lt;p&gt;In this version we have focused on improving parsing accuracy and efficiency:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Model capability upgrade&lt;/strong&gt; (requires re-executing the &lt;a href="https://github.com/opendatalab/MinerU/raw/master/docs/how_to_download_models_en.md"&gt;model download process&lt;/a&gt; to obtain incremental updates of model files) 
    &lt;ul&gt; 
     &lt;li&gt;The layout recognition model has been upgraded to the latest &lt;code&gt;doclayout_yolo(2501)&lt;/code&gt; model, improving layout recognition accuracy.&lt;/li&gt; 
     &lt;li&gt;The formula parsing model has been upgraded to the latest &lt;code&gt;unimernet(2501)&lt;/code&gt; model, improving formula recognition accuracy.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Performance optimization&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;On devices that meet certain configuration requirements (16GB+ VRAM), by optimizing resource usage and restructuring the processing pipeline, overall parsing speed has been increased by more than 50%.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Parsing effect optimization&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Added a new heading classification feature (testing version, enabled by default) to the online demo (&lt;a href="https://mineru.net/OpenSourceTools/Extractor"&gt;mineru.net&lt;/a&gt;/&lt;a href="https://huggingface.co/spaces/opendatalab/MinerU"&gt;huggingface&lt;/a&gt;/&lt;a href="https://www.modelscope.cn/studios/OpenDataLab/MinerU"&gt;modelscope&lt;/a&gt;), which supports hierarchical classification of headings, thereby enhancing document structuring.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2025/01/10 1.0.1 released&lt;/summary&gt; 
  &lt;p&gt;This is our first official release, where we have introduced a completely new API interface and enhanced compatibility through extensive refactoring, as well as a brand new automatic language identification feature:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;New API Interface&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;For the data-side API, we have introduced the Dataset class, designed to provide a robust and flexible data processing framework. This framework currently supports a variety of document formats, including images (.jpg and .png), PDFs, Word documents (.doc and .docx), and PowerPoint presentations (.ppt and .pptx). It ensures effective support for data processing tasks ranging from simple to complex.&lt;/li&gt; 
     &lt;li&gt;For the user-side API, we have meticulously designed the MinerU processing workflow as a series of composable Stages. Each Stage represents a specific processing step, allowing users to define new Stages according to their needs and creatively combine these stages to customize their data processing workflows.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Enhanced Compatibility&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;By optimizing the dependency environment and configuration items, we ensure stable and efficient operation on ARM architecture Linux systems.&lt;/li&gt; 
     &lt;li&gt;We have deeply integrated with Huawei Ascend NPU acceleration, providing autonomous and controllable high-performance computing capabilities. This supports the localization and development of AI application platforms in China. &lt;a href="https://github.com/opendatalab/MinerU/raw/master/docs/README_Ascend_NPU_Acceleration_zh_CN.md"&gt;Ascend NPU Acceleration&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Automatic Language Identification&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;By introducing a new language recognition model, setting the &lt;code&gt;lang&lt;/code&gt; configuration to &lt;code&gt;auto&lt;/code&gt; during document parsing will automatically select the appropriate OCR language model, improving the accuracy of scanned document parsing.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2024/11/22 0.10.0 released&lt;/summary&gt; 
  &lt;p&gt;Introducing hybrid OCR text extraction capabilities:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Significantly improved parsing performance in complex text distribution scenarios such as dense formulas, irregular span regions, and text represented by images.&lt;/li&gt; 
   &lt;li&gt;Combines the dual advantages of accurate content extraction and faster speed in text mode, and more precise span/line region recognition in OCR mode.&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2024/11/15 0.9.3 released&lt;/summary&gt; 
  &lt;p&gt;Integrated &lt;a href="https://github.com/RapidAI/RapidTable"&gt;RapidTable&lt;/a&gt; for table recognition, improving single-table parsing speed by more than 10 times, with higher accuracy and lower GPU memory usage.&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2024/11/06 0.9.2 released&lt;/summary&gt; 
  &lt;p&gt;Integrated the &lt;a href="https://huggingface.co/U4R/StructTable-InternVL2-1B"&gt;StructTable-InternVL2-1B&lt;/a&gt; model for table recognition functionality.&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2024/10/31 0.9.0 released&lt;/summary&gt; 
  &lt;p&gt;This is a major new version with extensive code refactoring, addressing numerous issues, improving performance, reducing hardware requirements, and enhancing usability:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Refactored the sorting module code to use &lt;a href="https://github.com/ppaanngggg/layoutreader"&gt;layoutreader&lt;/a&gt; for reading order sorting, ensuring high accuracy in various layouts.&lt;/li&gt; 
   &lt;li&gt;Refactored the paragraph concatenation module to achieve good results in cross-column, cross-page, cross-figure, and cross-table scenarios.&lt;/li&gt; 
   &lt;li&gt;Refactored the list and table of contents recognition functions, significantly improving the accuracy of list blocks and table of contents blocks, as well as the parsing of corresponding text paragraphs.&lt;/li&gt; 
   &lt;li&gt;Refactored the matching logic for figures, tables, and descriptive text, greatly enhancing the accuracy of matching captions and footnotes to figures and tables, and reducing the loss rate of descriptive text to near zero.&lt;/li&gt; 
   &lt;li&gt;Added multi-language support for OCR, supporting detection and recognition of 84 languages. For the list of supported languages, see &lt;a href="https://paddlepaddle.github.io/PaddleOCR/latest/en/ppocr/blog/multi_languages.html#5-support-languages-and-abbreviations"&gt;OCR Language Support List&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;Added memory recycling logic and other memory optimization measures, significantly reducing memory usage. The memory requirement for enabling all acceleration features except table acceleration (layout/formula/OCR) has been reduced from 16GB to 8GB, and the memory requirement for enabling all acceleration features has been reduced from 24GB to 10GB.&lt;/li&gt; 
   &lt;li&gt;Optimized configuration file feature switches, adding an independent formula detection switch to significantly improve speed and parsing results when formula detection is not needed.&lt;/li&gt; 
   &lt;li&gt;Integrated &lt;a href="https://github.com/opendatalab/PDF-Extract-Kit"&gt;PDF-Extract-Kit 1.0&lt;/a&gt;: 
    &lt;ul&gt; 
     &lt;li&gt;Added the self-developed &lt;code&gt;doclayout_yolo&lt;/code&gt; model, which speeds up processing by more than 10 times compared to the original solution while maintaining similar parsing effects, and can be freely switched with &lt;code&gt;layoutlmv3&lt;/code&gt; via the configuration file.&lt;/li&gt; 
     &lt;li&gt;Upgraded formula parsing to &lt;code&gt;unimernet 0.2.1&lt;/code&gt;, improving formula parsing accuracy while significantly reducing memory usage.&lt;/li&gt; 
     &lt;li&gt;Due to the repository change for &lt;code&gt;PDF-Extract-Kit 1.0&lt;/code&gt;, you need to re-download the model. Please refer to &lt;a href="https://github.com/opendatalab/MinerU/raw/master/docs/how_to_download_models_en.md"&gt;How to Download Models&lt;/a&gt; for detailed steps.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2024/09/27 Version 0.8.1 released&lt;/summary&gt; 
  &lt;p&gt;Fixed some bugs, and providing a &lt;a href="https://github.com/opendatalab/MinerU/raw/master/projects/web_demo/README.md"&gt;localized deployment version&lt;/a&gt; of the &lt;a href="https://opendatalab.com/OpenSourceTools/Extractor/PDF/"&gt;online demo&lt;/a&gt; and the &lt;a href="https://github.com/opendatalab/MinerU/raw/master/projects/web/README.md"&gt;front-end interface&lt;/a&gt;.&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2024/09/09 Version 0.8.0 released&lt;/summary&gt; 
  &lt;p&gt;Supporting fast deployment with Dockerfile, and launching demos on Huggingface and Modelscope.&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2024/08/30 Version 0.7.1 released&lt;/summary&gt; 
  &lt;p&gt;Add paddle tablemaster table recognition option&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2024/08/09 Version 0.7.0b1 released&lt;/summary&gt; 
  &lt;p&gt;Simplified installation process, added table recognition functionality&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2024/08/01 Version 0.6.2b1 released&lt;/summary&gt; 
  &lt;p&gt;Optimized dependency conflict issues and installation documentation&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;2024/07/05 Initial open-source release&lt;/summary&gt; 
 &lt;/details&gt; 
&lt;/details&gt; 
&lt;h1&gt;MinerU&lt;/h1&gt; 
&lt;h2&gt;Project Introduction&lt;/h2&gt; 
&lt;p&gt;MinerU is a tool that converts PDFs into machine-readable formats (e.g., markdown, JSON), allowing for easy extraction into any format. MinerU was born during the pre-training process of &lt;a href="https://github.com/InternLM/InternLM"&gt;InternLM&lt;/a&gt;. We focus on solving symbol conversion issues in scientific literature and hope to contribute to technological development in the era of large models. Compared to well-known commercial products, MinerU is still young. If you encounter any issues or if the results are not as expected, please submit an issue on &lt;a href="https://github.com/opendatalab/MinerU/issues"&gt;issue&lt;/a&gt; and &lt;strong&gt;attach the relevant PDF&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/4bea02c9-6d54-4cd6-97ed-dff14340982c"&gt;https://github.com/user-attachments/assets/4bea02c9-6d54-4cd6-97ed-dff14340982c&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Remove headers, footers, footnotes, page numbers, etc., to ensure semantic coherence.&lt;/li&gt; 
 &lt;li&gt;Output text in human-readable order, suitable for single-column, multi-column, and complex layouts.&lt;/li&gt; 
 &lt;li&gt;Preserve the structure of the original document, including headings, paragraphs, lists, etc.&lt;/li&gt; 
 &lt;li&gt;Extract images, image descriptions, tables, table titles, and footnotes.&lt;/li&gt; 
 &lt;li&gt;Automatically recognize and convert formulas in the document to LaTeX format.&lt;/li&gt; 
 &lt;li&gt;Automatically recognize and convert tables in the document to HTML format.&lt;/li&gt; 
 &lt;li&gt;Automatically detect scanned PDFs and garbled PDFs and enable OCR functionality.&lt;/li&gt; 
 &lt;li&gt;OCR supports detection and recognition of 84 languages.&lt;/li&gt; 
 &lt;li&gt;Supports multiple output formats, such as multimodal and NLP Markdown, JSON sorted by reading order, and rich intermediate formats.&lt;/li&gt; 
 &lt;li&gt;Supports various visualization results, including layout visualization and span visualization, for efficient confirmation of output quality.&lt;/li&gt; 
 &lt;li&gt;Supports running in a pure CPU environment, and also supports GPU(CUDA)/NPU(CANN)/MPS acceleration&lt;/li&gt; 
 &lt;li&gt;Compatible with Windows, Linux, and Mac platforms.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Quick Start&lt;/h1&gt; 
&lt;p&gt;If you encounter any installation issues, please first consult the &lt;a href="https://raw.githubusercontent.com/opendatalab/MinerU/master/#faq"&gt;FAQ&lt;/a&gt;. &lt;br /&gt; If the parsing results are not as expected, refer to the &lt;a href="https://raw.githubusercontent.com/opendatalab/MinerU/master/#known-issues"&gt;Known Issues&lt;/a&gt;. &lt;br /&gt;&lt;/p&gt; 
&lt;h2&gt;Online Experience&lt;/h2&gt; 
&lt;h3&gt;Official online web application&lt;/h3&gt; 
&lt;p&gt;The official online version has the same functionality as the client, with a beautiful interface and rich features, requires login to use&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://mineru.net/OpenSourceTools/Extractor?source=github"&gt;&lt;img src="https://img.shields.io/badge/webapp_on_mineru.net-blue?logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTM0IiBoZWlnaHQ9IjEzNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBkPSJtMTIyLDljMCw1LTQsOS05LDlzLTktNC05LTksNC05LDktOSw5LDQsOSw5eiIgZmlsbD0idXJsKCNhKSIvPjxwYXRoIGQ9Im0xMjIsOWMwLDUtNCw5LTksOXMtOS00LTktOSw0LTksOS05LDksNCw5LDl6IiBmaWxsPSIjMDEwMTAxIi8+PHBhdGggZD0ibTkxLDE4YzAsNS00LDktOSw5cy05LTQtOS05LDQtOSw5LTksOSw0LDksOXoiIGZpbGw9InVybCgjYikiLz48cGF0aCBkPSJtOTEsMThjMCw1LTQsOS05LDlzLTktNC05LTksNC05LDktOSw5LDQsOSw5eiIgZmlsbD0iIzAxMDEwMSIvPjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJtMzksNjJjMCwxNiw4LDMwLDIwLDM4LDctNiwxMi0xNiwxMi0yNlY0OWMwLTQsMy03LDYtOGw0Ni0xMmM1LTEsMTEsMywxMSw4djMxYzAsMzctMzAsNjYtNjYsNjYtMzcsMC02Ni0zMC02Ni02NlY0NmMwLTQsMy03LDYtOGwyMC02YzUtMSwxMSwzLDExLDh2MjF6bS0yOSw2YzAsMTYsNiwzMCwxNyw0MCwzLDEsNSwxLDgsMSw1LDAsMTAtMSwxNS0zQzM3LDk1LDI5LDc5LDI5LDYyVjQybC0xOSw1djIweiIgZmlsbD0idXJsKCNjKSIvPjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJtMzksNjJjMCwxNiw4LDMwLDIwLDM4LDctNiwxMi0xNiwxMi0yNlY0OWMwLTQsMy03LDYtOGw0Ni0xMmM1LTEsMTEsMywxMSw4djMxYzAsMzctMzAsNjYtNjYsNjYtMzcsMC02Ni0zMC02Ni02NlY0NmMwLTQsMy03LDYtOGwyMC02YzUtMSwxMSwzLDExLDh2MjF6bS0yOSw2YzAsMTYsNiwzMCwxNyw0MCwzLDEsNSwxLDgsMSw1LDAsMTAtMSwxNS0zQzM3LDk1LDI5LDc5LDI5LDYyVjQybC0xOSw1djIweiIgZmlsbD0iIzAxMDEwMSIvPjxkZWZzPjxsaW5lYXJHcmFkaWVudCBpZD0iYSIgeDE9Ijg0IiB5MT0iNDEiIHgyPSI3NSIgeTI9IjEyMCIgZ3JhZGllbnRVbml0cz0idXNlclNwYWNlT25Vc2UiPjxzdG9wIHN0b3AtY29sb3I9IiNmZmYiLz48c3RvcCBvZmZzZXQ9IjEiIHN0b3AtY29sb3I9IiMyZTJlMmUiLz48L2xpbmVhckdyYWRpZW50PjxsaW5lYXJHcmFkaWVudCBpZD0iYiIgeDE9Ijg0IiB5MT0iNDEiIHgyPSI3NSIgeTI9IjEyMCIgZ3JhZGllbnRVbml0cz0idXNlclNwYWNlT25Vc2UiPjxzdG9wIHN0b3AtY29sb3I9IiNmZmYiLz48c3RvcCBvZmZzZXQ9IjEiIHN0b3AtY29sb3I9IiMyZTJlMmUiLz48L2xpbmVhckdyYWRpZW50PjxsaW5lYXJHcmFkaWVudCBpZD0iYyIgeDE9Ijg0IiB5MT0iNDEiIHgyPSI3NSIgeTI9IjEyMCIgZ3JhZGllbnRVbml0cz0idXNlclNwYWNlT25Vc2UiPjxzdG9wIHN0b3AtY29sb3I9IiNmZmYiLz48c3RvcCBvZmZzZXQ9IjEiIHN0b3AtY29sb3I9IiMyZTJlMmUiLz48L2xpbmVhckdyYWRpZW50PjwvZGVmcz48L3N2Zz4=&amp;amp;labelColor=white" alt="OpenDataLab" /&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Gradio-based online demo&lt;/h3&gt; 
&lt;p&gt;A WebUI developed based on Gradio, with a simple interface and only core parsing functionality, no login required&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.modelscope.cn/studios/OpenDataLab/MinerU"&gt;&lt;img src="https://img.shields.io/badge/Demo_on_ModelScope-purple?logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIzIiBoZWlnaHQ9IjIwMCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCiA8Zz4KICA8dGl0bGU+TGF5ZXIgMTwvdGl0bGU+CiAgPHBhdGggaWQ9InN2Z18xNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTAsODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTUiIGZpbGw9IiM2MjRhZmYiIGQ9Im05OS4xNCwxMTUuNDlsMjUuNjUsMGwwLDI1LjY1bC0yNS42NSwwbDAsLTI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTYiIGZpbGw9IiM2MjRhZmYiIGQ9Im0xNzYuMDksMTQxLjE0bC0yNS42NDk5OSwwbDAsMjIuMTlsNDcuODQsMGwwLC00Ny44NGwtMjIuMTksMGwwLDI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTciIGZpbGw9IiMzNmNmZDEiIGQ9Im0xMjQuNzksODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTgiIGZpbGw9IiMzNmNmZDEiIGQ9Im0wLDY0LjE5bDI1LjY1LDBsMCwyNS42NWwtMjUuNjUsMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzE5IiBmaWxsPSIjNjI0YWZmIiBkPSJtMTk4LjI4LDg5Ljg0bDI1LjY0OTk5LDBsMCwyNS42NDk5OWwtMjUuNjQ5OTksMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIwIiBmaWxsPSIjMzZjZmQxIiBkPSJtMTk4LjI4LDY0LjE5bDI1LjY0OTk5LDBsMCwyNS42NWwtMjUuNjQ5OTksMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIxIiBmaWxsPSIjNjI0YWZmIiBkPSJtMTUwLjQ0LDQybDAsMjIuMTlsMjUuNjQ5OTksMGwwLDI1LjY1bDIyLjE5LDBsMCwtNDcuODRsLTQ3Ljg0LDB6Ii8+CiAgPHBhdGggaWQ9InN2Z18yMiIgZmlsbD0iIzM2Y2ZkMSIgZD0ibTczLjQ5LDg5Ljg0bDI1LjY1LDBsMCwyNS42NDk5OWwtMjUuNjUsMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIzIiBmaWxsPSIjNjI0YWZmIiBkPSJtNDcuODQsNjQuMTlsMjUuNjUsMGwwLC0yMi4xOWwtNDcuODQsMGwwLDQ3Ljg0bDIyLjE5LDBsMCwtMjUuNjV6Ii8+CiAgPHBhdGggaWQ9InN2Z18yNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTQ3Ljg0LDExNS40OWwtMjIuMTksMGwwLDQ3Ljg0bDQ3Ljg0LDBsMCwtMjIuMTlsLTI1LjY1LDBsMCwtMjUuNjV6Ii8+CiA8L2c+Cjwvc3ZnPg==&amp;amp;labelColor=white" alt="ModelScope" /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/spaces/opendatalab/MinerU"&gt;&lt;img src="https://img.shields.io/badge/Demo_on_HuggingFace-yellow.svg?logo=data:image/png;base64,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&amp;amp;labelColor=white" alt="HuggingFace" /&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Local Deployment&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING] &lt;strong&gt;Pre-installation Notice—Hardware and Software Environment Support&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;To ensure the stability and reliability of the project, we only optimize and test for specific hardware and software environments during development. This ensures that users deploying and running the project on recommended system configurations will get the best performance with the fewest compatibility issues.&lt;/p&gt; 
 &lt;p&gt;By focusing resources on the mainline environment, our team can more efficiently resolve potential bugs and develop new features.&lt;/p&gt; 
 &lt;p&gt;In non-mainline environments, due to the diversity of hardware and software configurations, as well as third-party dependency compatibility issues, we cannot guarantee 100% project availability. Therefore, for users who wish to use this project in non-recommended environments, we suggest carefully reading the documentation and FAQ first. Most issues already have corresponding solutions in the FAQ. We also encourage community feedback to help us gradually expand support.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td&gt;Parsing Backend&lt;/td&gt; 
   &lt;td&gt;pipeline&lt;/td&gt; 
   &lt;td&gt;vlm-transformers&lt;/td&gt; 
   &lt;td&gt;vlm-vllm&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Operating System&lt;/td&gt; 
   &lt;td&gt;Linux / Windows / macOS&lt;/td&gt; 
   &lt;td&gt;Linux / Windows&lt;/td&gt; 
   &lt;td&gt;Linux / Windows (via WSL2)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;CPU Inference Support&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td colspan="2"&gt;❌&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;GPU Requirements&lt;/td&gt; 
   &lt;td&gt;Turing architecture and later, 6GB+ VRAM or Apple Silicon&lt;/td&gt; 
   &lt;td colspan="2"&gt;Turing architecture and later, 8GB+ VRAM&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Memory Requirements&lt;/td&gt; 
   &lt;td colspan="3"&gt;Minimum 16GB+, recommended 32GB+&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Disk Space Requirements&lt;/td&gt; 
   &lt;td colspan="3"&gt;20GB+, SSD recommended&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Python Version&lt;/td&gt; 
   &lt;td colspan="3"&gt;3.10-3.13&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h3&gt;Install MinerU&lt;/h3&gt; 
&lt;h4&gt;Install MinerU using pip or uv&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install --upgrade pip
pip install uv
uv pip install -U "mineru[core]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Install MinerU from source code&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/opendatalab/MinerU.git
cd MinerU
uv pip install -e .[core]
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] &lt;code&gt;mineru[core]&lt;/code&gt; includes all core features except &lt;code&gt;vLLM&lt;/code&gt; acceleration, compatible with Windows / Linux / macOS systems, suitable for most users. If you need to use &lt;code&gt;vLLM&lt;/code&gt; acceleration for VLM model inference or install a lightweight client on edge devices, please refer to the documentation &lt;a href="https://opendatalab.github.io/MinerU/quick_start/extension_modules/"&gt;Extension Modules Installation Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr /&gt; 
&lt;h4&gt;Deploy MinerU using Docker&lt;/h4&gt; 
&lt;p&gt;MinerU provides a convenient Docker deployment method, which helps quickly set up the environment and solve some tricky environment compatibility issues. You can get the &lt;a href="https://opendatalab.github.io/MinerU/quick_start/docker_deployment/"&gt;Docker Deployment Instructions&lt;/a&gt; in the documentation.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Using MinerU&lt;/h3&gt; 
&lt;p&gt;The simplest command line invocation is:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;mineru -p &amp;lt;input_path&amp;gt; -o &amp;lt;output_path&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can use MinerU for PDF parsing through various methods such as command line, API, and WebUI. For detailed instructions, please refer to the &lt;a href="https://opendatalab.github.io/MinerU/usage/"&gt;Usage Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;TODO&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Reading order based on the model&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Recognition of &lt;code&gt;index&lt;/code&gt; and &lt;code&gt;list&lt;/code&gt; in the main text&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Table recognition&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Heading Classification&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Handwritten Text Recognition&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Vertical Text Recognition&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Latin Accent Mark Recognition&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Code block recognition in the main text&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://raw.githubusercontent.com/opendatalab/MinerU/master/docs/chemical_knowledge_introduction/introduction.pdf"&gt;Chemical formula recognition&lt;/a&gt;(mineru.net)&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Geometric shape recognition&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Known Issues&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;Reading order is determined by the model based on the spatial distribution of readable content, and may be out of order in some areas under extremely complex layouts.&lt;/li&gt; 
 &lt;li&gt;Limited support for vertical text.&lt;/li&gt; 
 &lt;li&gt;Tables of contents and lists are recognized through rules, and some uncommon list formats may not be recognized.&lt;/li&gt; 
 &lt;li&gt;Code blocks are not yet supported in the layout model.&lt;/li&gt; 
 &lt;li&gt;Comic books, art albums, primary school textbooks, and exercises cannot be parsed well.&lt;/li&gt; 
 &lt;li&gt;Table recognition may result in row/column recognition errors in complex tables.&lt;/li&gt; 
 &lt;li&gt;OCR recognition may produce inaccurate characters in PDFs of lesser-known languages (e.g., diacritical marks in Latin script, easily confused characters in Arabic script).&lt;/li&gt; 
 &lt;li&gt;Some formulas may not render correctly in Markdown.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;FAQ&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;If you encounter any issues during usage, you can first check the &lt;a href="https://opendatalab.github.io/MinerU/faq/"&gt;FAQ&lt;/a&gt; for solutions.&lt;/li&gt; 
 &lt;li&gt;If your issue remains unresolved, you may also use &lt;a href="https://deepwiki.com/opendatalab/MinerU"&gt;DeepWiki&lt;/a&gt; to interact with an AI assistant, which can address most common problems.&lt;/li&gt; 
 &lt;li&gt;If you still cannot resolve the issue, you are welcome to join our community via &lt;a href="https://discord.gg/Tdedn9GTXq"&gt;Discord&lt;/a&gt; or &lt;a href="https://mineru.net/community-portal/?aliasId=3c430f94"&gt;WeChat&lt;/a&gt; to discuss with other users and developers.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;All Thanks To Our Contributors&lt;/h1&gt; 
&lt;a href="https://github.com/opendatalab/MinerU/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=opendatalab/MinerU" /&gt; &lt;/a&gt; 
&lt;h1&gt;License Information&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/opendatalab/MinerU/master/LICENSE.md"&gt;LICENSE.md&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Currently, some models in this project are trained based on YOLO. However, since YOLO follows the AGPL license, it may impose restrictions on certain use cases. In future iterations, we plan to explore and replace these with models under more permissive licenses to enhance user-friendliness and flexibility.&lt;/p&gt; 
&lt;h1&gt;Acknowledgments&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/opendatalab/PDF-Extract-Kit"&gt;PDF-Extract-Kit&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/opendatalab/DocLayout-YOLO"&gt;DocLayout-YOLO&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/opendatalab/UniMERNet"&gt;UniMERNet&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/RapidAI/RapidTable"&gt;RapidTable&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/RapidAI/TableStructureRec"&gt;TableStructureRec&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/PaddlePaddle/PaddleOCR"&gt;PaddleOCR&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/frotms/PaddleOCR2Pytorch"&gt;PaddleOCR2Pytorch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ppaanngggg/layoutreader"&gt;layoutreader&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Sanster/xy-cut"&gt;xy-cut&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/LlmKira/fast-langdetect"&gt;fast-langdetect&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pypdfium2-team/pypdfium2"&gt;pypdfium2&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/datalab-to/pdftext"&gt;pdftext&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pdfminer/pdfminer.six"&gt;pdfminer.six&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/py-pdf/pypdf"&gt;pypdf&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/google/magika"&gt;magika&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Citation&lt;/h1&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{niu2025mineru25decoupledvisionlanguagemodel,
      title={MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing}, 
      author={Junbo Niu and Zheng Liu and Zhuangcheng Gu and Bin Wang and Linke Ouyang and Zhiyuan Zhao and Tao Chu and Tianyao He and Fan Wu and Qintong Zhang and Zhenjiang Jin and Guang Liang and Rui Zhang and Wenzheng Zhang and Yuan Qu and Zhifei Ren and Yuefeng Sun and Yuanhong Zheng and Dongsheng Ma and Zirui Tang and Boyu Niu and Ziyang Miao and Hejun Dong and Siyi Qian and Junyuan Zhang and Jingzhou Chen and Fangdong Wang and Xiaomeng Zhao and Liqun Wei and Wei Li and Shasha Wang and Ruiliang Xu and Yuanyuan Cao and Lu Chen and Qianqian Wu and Huaiyu Gu and Lindong Lu and Keming Wang and Dechen Lin and Guanlin Shen and Xuanhe Zhou and Linfeng Zhang and Yuhang Zang and Xiaoyi Dong and Jiaqi Wang and Bo Zhang and Lei Bai and Pei Chu and Weijia Li and Jiang Wu and Lijun Wu and Zhenxiang Li and Guangyu Wang and Zhongying Tu and Chao Xu and Kai Chen and Yu Qiao and Bowen Zhou and Dahua Lin and Wentao Zhang and Conghui He},
      year={2025},
      eprint={2509.22186},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2509.22186}, 
}

@misc{wang2024mineruopensourcesolutionprecise,
      title={MinerU: An Open-Source Solution for Precise Document Content Extraction}, 
      author={Bin Wang and Chao Xu and Xiaomeng Zhao and Linke Ouyang and Fan Wu and Zhiyuan Zhao and Rui Xu and Kaiwen Liu and Yuan Qu and Fukai Shang and Bo Zhang and Liqun Wei and Zhihao Sui and Wei Li and Botian Shi and Yu Qiao and Dahua Lin and Conghui He},
      year={2024},
      eprint={2409.18839},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2409.18839}, 
}

@article{he2024opendatalab,
  title={Opendatalab: Empowering general artificial intelligence with open datasets},
  author={He, Conghui and Li, Wei and Jin, Zhenjiang and Xu, Chao and Wang, Bin and Lin, Dahua},
  journal={arXiv preprint arXiv:2407.13773},
  year={2024}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;Star History&lt;/h1&gt; 
&lt;a&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=opendatalab/MinerU&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=opendatalab/MinerU&amp;amp;type=Date" /&gt; 
  &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=opendatalab/MinerU&amp;amp;type=Date" /&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;h1&gt;Links&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenDCAI/DataFlow"&gt;Easy Data Preparation with latest LLMs-based Operators and Pipelines&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/opendatalab/Vis3"&gt;Vis3 (OSS browser based on s3)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/opendatalab/labelU"&gt;LabelU (A Lightweight Multi-modal Data Annotation Tool)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/opendatalab/LabelLLM"&gt;LabelLLM (An Open-source LLM Dialogue Annotation Platform)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/opendatalab/PDF-Extract-Kit"&gt;PDF-Extract-Kit (A Comprehensive Toolkit for High-Quality PDF Content Extraction)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/opendatalab/OmniDocBench"&gt;OmniDocBench (A Comprehensive Benchmark for Document Parsing and Evaluation)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/opendatalab/magic-html"&gt;Magic-HTML (Mixed web page extraction tool)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/InternLM/magic-doc"&gt;Magic-Doc (Fast speed ppt/pptx/doc/docx/pdf extraction tool)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/MigoXLab/dingo"&gt;Dingo: A Comprehensive AI Data Quality Evaluation Tool&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>google-deepmind/mujoco_warp</title>
      <link>https://github.com/google-deepmind/mujoco_warp</link>
      <description>&lt;p&gt;GPU-optimized version of the MuJoCo physics simulator, designed for NVIDIA hardware.&lt;/p&gt;&lt;hr&gt;&lt;p&gt; &lt;a href="https://github.com/google-deepmind/mujoco_warp/actions/workflows/ci.yml?query=branch%3Amain" alt="GitHub Actions"&gt; &lt;img src="https://img.shields.io/github/actions/workflow/status/google-deepmind/mujoco_warp/ci.yml?branch=main" /&gt; &lt;/a&gt; &lt;a href="https://github.com/google-deepmind/mujoco_warp/raw/main/LICENSE" alt="License"&gt; &lt;img src="https://img.shields.io/github/license/google-deepmind/mujoco_warp" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h1&gt;MuJoCo Warp (MJWarp)&lt;/h1&gt; 
&lt;p&gt;MJWarp is a GPU-optimized version of the &lt;a href="https://github.com/google-deepmind/mujoco"&gt;MuJoCo&lt;/a&gt; physics simulator, designed for NVIDIA hardware.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] MJWarp is in Beta and under active development:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;MJWarp developers will triage and respond to &lt;a href="https://github.com/google-deepmind/mujoco_warp/issues"&gt;bug report and feature requests&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;MJWarp is mostly feature complete but requires performance optimization, documentation, and testing.&lt;/li&gt; 
  &lt;li&gt;The intended audience during Beta are physics engine enthusiasts and learning framework integrators.&lt;/li&gt; 
  &lt;li&gt;Machine learning / robotics researchers who just want to train policies should wait for the &lt;a href="https://mujoco.readthedocs.io/en/stable/mjx.html"&gt;MJX&lt;/a&gt; or &lt;a href="https://isaac-sim.github.io/IsaacLab/main/index.html"&gt;Isaac&lt;/a&gt;/&lt;a href="https://github.com/newton-physics/newton"&gt;Newton&lt;/a&gt; integrations, which are coming soon.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;MJWarp uses &lt;a href="https://github.com/NVIDIA/warp"&gt;NVIDIA Warp&lt;/a&gt; to circumvent many of the &lt;a href="https://mujoco.readthedocs.io/en/stable/mjx.html#mjx-the-sharp-bits"&gt;sharp bits&lt;/a&gt; in &lt;a href="https://mujoco.readthedocs.io/en/stable/mjx.html#"&gt;MuJoCo MJX&lt;/a&gt;. MJWarp is integrated into both &lt;a href="https://mujoco.readthedocs.io/en/stable/mjx.html"&gt;MJX&lt;/a&gt; and &lt;a href="https://github.com/newton-physics/newton"&gt;Newton&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;MJWarp is maintained by &lt;a href="https://deepmind.google/"&gt;Google DeepMind&lt;/a&gt; and &lt;a href="https://www.nvidia.com/"&gt;NVIDIA&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Installing for development&lt;/h1&gt; 
&lt;p&gt;MuJoCo Warp is currently supported on Windows or Linux on x86-64 architecture (to be expanded to more platforms and architectures soon).&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Linux&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/google-deepmind/mujoco_warp.git
cd mujoco_warp
python3 -m venv env
source env/bin/activate
pip install --upgrade pip
pip install uv
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Windows&lt;/strong&gt; (native Python only, not MSYS2 or WSL)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-powershell"&gt;git clone https://github.com/google-deepmind/mujoco_warp.git
cd mujoco_warp
python -m venv env
.\env\Scripts\Activate.ps1  # For MSYS2 Python: env\bin\activate
pip install --upgrade pip
pip install uv
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then install MJWarp in editable mode for local development:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;uv pip install -e .[dev,cuda]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Now make sure everything is working:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pytest
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Should print out something like &lt;code&gt;XX passed in XX.XXs&lt;/code&gt; at the end!&lt;/p&gt; 
&lt;p&gt;If you plan to write Warp kernels for MJWarp, please use the &lt;code&gt;kernel_analyzer&lt;/code&gt; vscode plugin located in &lt;code&gt;contrib/kernel_analyzer&lt;/code&gt;. Please see the &lt;code&gt;README.md&lt;/code&gt; there for details on how to install it and use it. The same kernel analyzer will be run on any PR you open, so it's important to fix any issues it reports.&lt;/p&gt; 
&lt;h1&gt;Compatibility&lt;/h1&gt; 
&lt;p&gt;The following features are implemented:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Category&lt;/th&gt; 
   &lt;th&gt;Feature&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Dynamics&lt;/td&gt; 
   &lt;td&gt;Forward, Inverse&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Transmission&lt;/td&gt; 
   &lt;td&gt;All&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Actuator Dynamics&lt;/td&gt; 
   &lt;td&gt;All except &lt;code&gt;USER&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Actuator Gain&lt;/td&gt; 
   &lt;td&gt;All except &lt;code&gt;USER&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Actuator Bias&lt;/td&gt; 
   &lt;td&gt;All except &lt;code&gt;USER&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Geom&lt;/td&gt; 
   &lt;td&gt;All&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Constraint&lt;/td&gt; 
   &lt;td&gt;All&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Equality&lt;/td&gt; 
   &lt;td&gt;All except &lt;code&gt;FLEX&lt;/code&gt;, &lt;code&gt;DISTANCE&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Integrator&lt;/td&gt; 
   &lt;td&gt;All except &lt;code&gt;IMPLICIT&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Cone&lt;/td&gt; 
   &lt;td&gt;All&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Condim&lt;/td&gt; 
   &lt;td&gt;All&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Solver&lt;/td&gt; 
   &lt;td&gt;All except &lt;code&gt;PGS&lt;/code&gt;, &lt;code&gt;noslip&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Fluid Model&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;BOX&lt;/code&gt; only&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Tendon Wrap&lt;/td&gt; 
   &lt;td&gt;All&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Sensors&lt;/td&gt; 
   &lt;td&gt;All except &lt;code&gt;GEOMDIST&lt;/code&gt;, &lt;code&gt;GEOMNORMAL&lt;/code&gt;, &lt;code&gt;GEOMFROMTO&lt;/code&gt;, &lt;code&gt;PLUGIN&lt;/code&gt;, &lt;code&gt;USER&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Flex&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;VERTCOLLIDE&lt;/code&gt;, &lt;code&gt;ELASTICITY&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Mass matrix format&lt;/td&gt; 
   &lt;td&gt;Sparse and Dense&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Jacobian format&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;DENSE&lt;/code&gt; only (row-sparse, no islanding yet)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;a href="https://nvidia.github.io/warp/modules/differentiability.html#differentiability"&gt;Differentiability via Warp&lt;/a&gt; is not currently available.&lt;/p&gt; 
&lt;h1&gt;Viewing simulations&lt;/h1&gt; 
&lt;p&gt;Explore MuJoCo Warp simulations using an interactive viewer:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;mjwarp-viewer benchmark/humanoid/humanoid.xml
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will open a window on your local machine that uses the &lt;a href="https://mujoco.readthedocs.io/en/stable/programming/visualization.html"&gt;MuJoCo native visualizer&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Benchmarking&lt;/h1&gt; 
&lt;p&gt;Benchmark as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;mjwarp-testspeed benchmark/humanoid/humanoid.xml
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To get a full trace of the physics steps (e.g. timings of the subcomponents) run the following:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;mjwarp-testspeed benchmark/humanoid/humanoid.xml --event_trace=True
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;code&gt;mjwarp-testspeed&lt;/code&gt; has many configuration options, see &lt;code&gt;mjwarp-testspeed --help&lt;/code&gt; for details.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>