<rss version="2.0">
  <channel>
    <title>GitHub Python Daily Trending</title>
    <description>Daily Trending of Python in GitHub</description>
    <pubDate>Sun, 21 Dec 2025 01:38:51 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>exo-explore/exo</title>
      <link>https://github.com/exo-explore/exo</link>
      <description>&lt;p&gt;Run your own AI cluster at home with everyday devices ğŸ“±ğŸ’» ğŸ–¥ï¸âŒš&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="/docs/exo-logo-black-bg.jpg" /&gt; 
  &lt;img alt="exo logo" src="https://raw.githubusercontent.com/exo-explore/exo/main/docs/exo-logo-transparent.png" width="50%" height="50%" /&gt; 
 &lt;/picture&gt; 
 &lt;p&gt;exo: Run your own AI cluster at home with everyday devices. Maintained by &lt;a href="https://x.com/exolabs"&gt;exo labs&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/exo-explore/exo/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/exo-explore/exo" alt="GitHub Repo stars" /&gt;&lt;/a&gt; &lt;a href="https://www.apache.org/licenses/LICENSE-2.0.html"&gt;&lt;img src="https://img.shields.io/badge/License-Apache2.0-blue.svg?sanitize=true" alt="License: Apache-2.0" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/11849" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/11849" alt="exo-explore%2Fexo | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;p&gt;EXO connects all your devices into an AI cluster. It pools together the resources of all your devices in order to run large models. Not only does EXO enable running models larger than would fit on a single device, but with &lt;a href="https://x.com/exolabs/status/2001817749744476256?s=20"&gt;day-0 support for RDMA over Thunderbolt&lt;/a&gt;, makes models run faster as you add more devices.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Automatic Device Discovery&lt;/strong&gt;: Devices running EXO automatically discover each other - no manual configuration.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;RDMA over Thunderbolt&lt;/strong&gt;: EXO ships with &lt;a href="https://x.com/exolabs/status/2001817749744476256?s=20"&gt;day-0 support for RDMA over Thunderbolt 5&lt;/a&gt;, enabling 99% reduction in latency between devices.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Topology-Aware Auto Parallel&lt;/strong&gt;: EXO figures out the best way to split your model across all available devices based on a realtime view of your device topology. It takes into account device resources and network latency/bandwidth between each link.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Tensor Parallelism&lt;/strong&gt;: EXO supports sharding models, for up to 1.8x speedup on 2 devices and 3.2x speedup on 4 devices.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MLX Support&lt;/strong&gt;: EXO uses &lt;a href="https://github.com/ml-explore/mlx"&gt;MLX&lt;/a&gt; as an inference backend and &lt;a href="https://ml-explore.github.io/mlx/build/html/usage/distributed.html"&gt;MLX distributed&lt;/a&gt; for distributed communication.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Benchmarks&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;Qwen3-235B (8-bit) on 4 Ã— M3 Ultra Mac Studio with Tensor Parallel RDMA&lt;/summary&gt; 
 &lt;img src="https://raw.githubusercontent.com/exo-explore/exo/main/docs/benchmarks/jeffgeerling/mac-studio-cluster-ai-full-1-qwen3-235b.jpeg" alt="Benchmark - Qwen3-235B (8-bit) on 4 Ã— M3 Ultra Mac Studio with Tensor Parallel RDMA" width="80%" /&gt; 
 &lt;p&gt; &lt;strong&gt;Source:&lt;/strong&gt; &lt;a href="https://www.jeffgeerling.com/blog/2025/15-tb-vram-on-mac-studio-rdma-over-thunderbolt-5"&gt;Jeff Geerling: 15 TB VRAM on Mac Studio â€“ RDMA over Thunderboltâ€¯5&lt;/a&gt; &lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;DeepSeek v3.1 671B (8-bit) on 4 Ã— M3 Ultra Mac Studio with Tensor Parallel RDMA&lt;/summary&gt; 
 &lt;img src="https://raw.githubusercontent.com/exo-explore/exo/main/docs/benchmarks/jeffgeerling/mac-studio-cluster-ai-full-2-deepseek-3.1-671b.jpeg" alt="Benchmark - DeepSeek v3.1 671B (8-bit) on 4 Ã— M3 Ultra Mac Studio with Tensor Parallel RDMA" width="80%" /&gt; 
 &lt;p&gt; &lt;strong&gt;Source:&lt;/strong&gt; &lt;a href="https://www.jeffgeerling.com/blog/2025/15-tb-vram-on-mac-studio-rdma-over-thunderbolt-5"&gt;Jeff Geerling: 15 TB VRAM on Mac Studio â€“ RDMA over Thunderboltâ€¯5&lt;/a&gt; &lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Kimi K2 Thinking (native 4-bit) on 4 Ã— M3 Ultra Mac Studio with Tensor Parallel RDMA&lt;/summary&gt; 
 &lt;img src="https://raw.githubusercontent.com/exo-explore/exo/main/docs/benchmarks/jeffgeerling/mac-studio-cluster-ai-full-3-kimi-k2-thinking.jpeg" alt="Benchmark - Kimi K2 Thinking (native 4-bit) on 4 Ã— M3 Ultra Mac Studio with Tensor Parallel RDMA" width="80%" /&gt; 
 &lt;p&gt; &lt;strong&gt;Source:&lt;/strong&gt; &lt;a href="https://www.jeffgeerling.com/blog/2025/15-tb-vram-on-mac-studio-rdma-over-thunderbolt-5"&gt;Jeff Geerling: 15 TB VRAM on Mac Studio â€“ RDMA over Thunderboltâ€¯5&lt;/a&gt; &lt;/p&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;p&gt;Devices running EXO automatically discover each other, without needing any manual configuration. Each device provides an API and a dashboard for interacting with your cluster (runs at &lt;code&gt;http://localhost:52415&lt;/code&gt;).&lt;/p&gt; 
&lt;p&gt;There are two ways to run EXO:&lt;/p&gt; 
&lt;h3&gt;Run from Source (Mac &amp;amp; Linux)&lt;/h3&gt; 
&lt;p&gt;Clone the repo, build the dashboard, and run EXO:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd dashboard &amp;amp;&amp;amp; npm install &amp;amp;&amp;amp; npm run build
uv run exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;One-liner:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/exo-explore/exo &amp;amp;&amp;amp; cd exo/dashboard &amp;amp;&amp;amp; npm i &amp;amp;&amp;amp; npm run build &amp;amp;&amp;amp; cd .. &amp;amp;&amp;amp; uv run exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h3&gt;macOS App&lt;/h3&gt; 
&lt;p&gt;EXO ships a macOS app that runs in the background on your Mac.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/exo-explore/exo/main/docs/macos-app-one-macbook.png" alt="EXO macOS App - running on a MacBook" width="35%" /&gt; 
&lt;p&gt;The macOS app requires macOS Tahoe 26.2 or later.&lt;/p&gt; 
&lt;p&gt;Download the latest build here: &lt;a href="https://assets.exolabs.net/EXO-latest.dmg"&gt;EXO-latest.dmg&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The app will ask for permission to modify system settings and install a new Network profile. Improvements to this are being worked on.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Hardware Accelerator Support&lt;/h2&gt; 
&lt;p&gt;On macOS, EXO uses the GPU. On Linux, EXO currently runs on CPU. We are working on extending hardware accelerator support. If you'd like support for a new hardware platform, please search for an existing feature request and add a thumbs up so we know what hardware is important to the community.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for guidelines on how to contribute to EXO.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>lintsinghua/DeepAudit</title>
      <link>https://github.com/lintsinghua/DeepAudit</link>
      <description>&lt;p&gt;DeepAuditï¼šäººäººæ‹¥æœ‰çš„ AI é»‘å®¢æˆ˜é˜Ÿï¼Œè®©æ¼æ´æŒ–æ˜è§¦æ‰‹å¯åŠã€‚å›½å†…é¦–ä¸ªå¼€æºçš„ä»£ç æ¼æ´æŒ–æ˜å¤šæ™ºèƒ½ä½“ç³»ç»Ÿã€‚å°ç™½ä¸€é”®éƒ¨ç½²è¿è¡Œï¼Œè‡ªä¸»åä½œå®¡è®¡ + è‡ªåŠ¨åŒ–æ²™ç®± PoC éªŒè¯ã€‚æ”¯æŒ Ollama ç§æœ‰éƒ¨ç½² ï¼Œä¸€é”®ç”ŸæˆæŠ¥å‘Šã€‚â€‹è®©å®‰å…¨ä¸å†æ˜‚è´µï¼Œè®©å®¡è®¡ä¸å†å¤æ‚ã€‚&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;DeepAudit - äººäººæ‹¥æœ‰çš„ AI å®¡è®¡æˆ˜é˜Ÿï¼Œè®©æ¼æ´æŒ–æ˜è§¦æ‰‹å¯åŠ ğŸ¦¸â€â™‚ï¸&lt;/h1&gt; 
&lt;p align="center"&gt; &lt;strong&gt;ç®€ä½“ä¸­æ–‡&lt;/strong&gt; | &lt;a href="https://raw.githubusercontent.com/lintsinghua/DeepAudit/v3.0.0/README_EN.md"&gt;English&lt;/a&gt; &lt;/p&gt; 
&lt;div style="width: 100%; max-width: 600px; margin: 0 auto;"&gt; 
 &lt;img src="https://raw.githubusercontent.com/lintsinghua/DeepAudit/v3.0.0/frontend/public/images/logo.png" alt="DeepAudit Logo" style="width: 100%; height: auto; display: block; margin: 0 auto;" /&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/lintsinghua/DeepAudit/releases"&gt;&lt;img src="https://img.shields.io/badge/version-3.0.2-blue.svg?sanitize=true" alt="Version" /&gt;&lt;/a&gt; &lt;a href="https://www.gnu.org/licenses/agpl-3.0"&gt;&lt;img src="https://img.shields.io/badge/License-AGPL--3.0-blue.svg?sanitize=true" alt="License: AGPL-3.0" /&gt;&lt;/a&gt; &lt;a href="https://reactjs.org/"&gt;&lt;img src="https://img.shields.io/badge/React-18-61dafb.svg?sanitize=true" alt="React" /&gt;&lt;/a&gt; &lt;a href="https://www.typescriptlang.org/"&gt;&lt;img src="https://img.shields.io/badge/TypeScript-5.7-3178c6.svg?sanitize=true" alt="TypeScript" /&gt;&lt;/a&gt; &lt;a href="https://fastapi.tiangolo.com/"&gt;&lt;img src="https://img.shields.io/badge/FastAPI-0.100+-009688.svg?sanitize=true" alt="FastAPI" /&gt;&lt;/a&gt; &lt;a href="https://www.python.org/"&gt;&lt;img src="https://img.shields.io/badge/Python-3.11+-3776ab.svg?sanitize=true" alt="Python" /&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/lintsinghua/DeepAudit"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/lintsinghua/DeepAudit/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/lintsinghua/DeepAudit?style=social" alt="Stars" /&gt;&lt;/a&gt; &lt;a href="https://github.com/lintsinghua/DeepAudit/network/members"&gt;&lt;img src="https://img.shields.io/github/forks/lintsinghua/DeepAudit?style=social" alt="Forks" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/15634" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/15634" alt="lintsinghua%2FDeepAudit | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/lintsinghua/DeepAudit/v3.0.0/frontend/public/DeepAudit.gif" alt="DeepAudit Demo" width="90%" /&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ“¸ ç•Œé¢é¢„è§ˆ&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;h3&gt;ğŸ¤– Agent å®¡è®¡å…¥å£&lt;/h3&gt; 
 &lt;img src="frontend/public/images/README-show/Agentå®¡è®¡å…¥å£ï¼ˆé¦–é¡µï¼‰.png" alt="Agentå®¡è®¡å…¥å£" width="90%" /&gt; 
 &lt;p&gt;&lt;em&gt;é¦–é¡µå¿«é€Ÿè¿›å…¥ Multi-Agent æ·±åº¦å®¡è®¡&lt;/em&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td width="50%" align="center"&gt; &lt;strong&gt;ğŸ“‹ å®¡è®¡æµæ—¥å¿—&lt;/strong&gt;&lt;br /&gt;&lt;br /&gt; &lt;img src="frontend/public/images/README-show/å®¡è®¡æµæ—¥å¿—.png" alt="å®¡è®¡æµæ—¥å¿—" width="95%" /&gt;&lt;br /&gt; &lt;em&gt;å®æ—¶æŸ¥çœ‹ Agent æ€è€ƒä¸æ‰§è¡Œè¿‡ç¨‹&lt;/em&gt; &lt;/td&gt; 
   &lt;td width="50%" align="center"&gt; &lt;strong&gt;ğŸ›ï¸ æ™ºèƒ½ä»ªè¡¨ç›˜&lt;/strong&gt;&lt;br /&gt;&lt;br /&gt; &lt;img src="frontend/public/images/README-show/ä»ªè¡¨ç›˜.png" alt="ä»ªè¡¨ç›˜" width="95%" /&gt;&lt;br /&gt; &lt;em&gt;ä¸€çœ¼æŒæ¡é¡¹ç›®å®‰å…¨æ€åŠ¿&lt;/em&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%" align="center"&gt; &lt;strong&gt;âš¡ å³æ—¶åˆ†æ&lt;/strong&gt;&lt;br /&gt;&lt;br /&gt; &lt;img src="frontend/public/images/README-show/å³æ—¶åˆ†æ.png" alt="å³æ—¶åˆ†æ" width="95%" /&gt;&lt;br /&gt; &lt;em&gt;ç²˜è´´ä»£ç  / ä¸Šä¼ æ–‡ä»¶ï¼Œç§’å‡ºç»“æœ&lt;/em&gt; &lt;/td&gt; 
   &lt;td width="50%" align="center"&gt; &lt;strong&gt;ğŸ—‚ï¸ é¡¹ç›®ç®¡ç†&lt;/strong&gt;&lt;br /&gt;&lt;br /&gt; &lt;img src="frontend/public/images/README-show/é¡¹ç›®ç®¡ç†.png" alt="é¡¹ç›®ç®¡ç†" width="95%" /&gt;&lt;br /&gt; &lt;em&gt;GitHub/GitLab å¯¼å…¥ï¼Œå¤šé¡¹ç›®ååŒç®¡ç†&lt;/em&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;div align="center"&gt; 
 &lt;h3&gt;ğŸ“Š ä¸“ä¸šæŠ¥å‘Š&lt;/h3&gt; 
 &lt;img src="frontend/public/images/README-show/å®¡è®¡æŠ¥å‘Šç¤ºä¾‹.png" alt="å®¡è®¡æŠ¥å‘Š" width="90%" /&gt; 
 &lt;p&gt;&lt;em&gt;ä¸€é”®å¯¼å‡º PDF / Markdown / JSON&lt;/em&gt;ï¼ˆå›¾ä¸­ä¸ºå¿«é€Ÿæ¨¡å¼ï¼ŒéAgentæ¨¡å¼æŠ¥å‘Šï¼‰&lt;/p&gt; 
 &lt;p&gt;ğŸ‘‰ &lt;a href="https://lintsinghua.github.io/"&gt;æŸ¥çœ‹Agentå®¡è®¡å®Œæ•´æŠ¥å‘Šç¤ºä¾‹&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;âš¡ é¡¹ç›®æ¦‚è¿°&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;DeepAudit&lt;/strong&gt; æ˜¯ä¸€ä¸ªåŸºäº &lt;strong&gt;Multi-Agent åä½œæ¶æ„&lt;/strong&gt;çš„ä¸‹ä¸€ä»£ä»£ç å®‰å…¨å®¡è®¡å¹³å°ã€‚å®ƒä¸ä»…ä»…æ˜¯ä¸€ä¸ªé™æ€æ‰«æå·¥å…·ï¼Œè€Œæ˜¯æ¨¡æ‹Ÿå®‰å…¨ä¸“å®¶çš„æ€ç»´æ¨¡å¼ï¼Œé€šè¿‡å¤šä¸ªæ™ºèƒ½ä½“ï¼ˆ&lt;strong&gt;Orchestrator&lt;/strong&gt;, &lt;strong&gt;Recon&lt;/strong&gt;, &lt;strong&gt;Analysis&lt;/strong&gt;, &lt;strong&gt;Verification&lt;/strong&gt;ï¼‰çš„è‡ªä¸»åä½œï¼Œå®ç°å¯¹ä»£ç çš„æ·±åº¦ç†è§£ã€æ¼æ´æŒ–æ˜å’Œ &lt;strong&gt;è‡ªåŠ¨åŒ–æ²™ç®± PoC éªŒè¯&lt;/strong&gt;ã€‚&lt;/p&gt; 
&lt;p&gt;æˆ‘ä»¬è‡´åŠ›äºè§£å†³ä¼ ç»Ÿ SAST å·¥å…·çš„ä¸‰å¤§ç—›ç‚¹ï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;è¯¯æŠ¥ç‡é«˜&lt;/strong&gt; â€” ç¼ºä¹è¯­ä¹‰ç†è§£ï¼Œå¤§é‡è¯¯æŠ¥æ¶ˆè€—äººåŠ›&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ä¸šåŠ¡é€»è¾‘ç›²ç‚¹&lt;/strong&gt; â€” æ— æ³•ç†è§£è·¨æ–‡ä»¶è°ƒç”¨å’Œå¤æ‚é€»è¾‘&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ç¼ºä¹éªŒè¯æ‰‹æ®µ&lt;/strong&gt; â€” ä¸çŸ¥é“æ¼æ´æ˜¯å¦çœŸå®å¯åˆ©ç”¨&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;ç”¨æˆ·åªéœ€å¯¼å…¥é¡¹ç›®ï¼ŒDeepAudit ä¾¿å…¨è‡ªåŠ¨å¼€å§‹å·¥ä½œï¼šè¯†åˆ«æŠ€æœ¯æ ˆ â†’ åˆ†ææ½œåœ¨é£é™© â†’ ç”Ÿæˆè„šæœ¬ â†’ æ²™ç®±éªŒè¯ â†’ ç”ŸæˆæŠ¥å‘Šï¼Œæœ€ç»ˆè¾“å‡ºä¸€ä»½ä¸“ä¸šå®¡è®¡æŠ¥å‘Šã€‚&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;æ ¸å¿ƒç†å¿µ&lt;/strong&gt;: è®© AI åƒé»‘å®¢ä¸€æ ·æ”»å‡»ï¼Œåƒä¸“å®¶ä¸€æ ·é˜²å¾¡ã€‚&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;ğŸ’¡ ä¸ºä»€ä¹ˆé€‰æ‹© DeepAuditï¼Ÿ&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="left"&gt;ğŸ˜« ä¼ ç»Ÿå®¡è®¡çš„ç—›ç‚¹&lt;/th&gt; 
    &lt;th align="left"&gt;ğŸ’¡ DeepAudit è§£å†³æ–¹æ¡ˆ&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;&lt;strong&gt;äººå·¥å®¡è®¡æ•ˆç‡ä½&lt;/strong&gt;&lt;br /&gt;è·¨ä¸ä¸Š CI/CD ä»£ç è¿­ä»£é€Ÿåº¦ï¼Œæ‹–æ…¢å‘å¸ƒæµç¨‹&lt;/td&gt; 
    &lt;td align="left"&gt;&lt;strong&gt;ğŸ¤– Multi-Agent è‡ªä¸»å®¡è®¡&lt;/strong&gt;&lt;br /&gt;AI è‡ªåŠ¨ç¼–æ’å®¡è®¡ç­–ç•¥ï¼Œå…¨å¤©å€™è‡ªåŠ¨åŒ–æ‰§è¡Œ&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;&lt;strong&gt;ä¼ ç»Ÿå·¥å…·è¯¯æŠ¥å¤š&lt;/strong&gt;&lt;br /&gt;ç¼ºä¹è¯­ä¹‰ç†è§£ï¼Œæ¯å¤©èŠ±è´¹å¤§é‡æ—¶é—´æ¸…æ´—å™ªéŸ³&lt;/td&gt; 
    &lt;td align="left"&gt;&lt;strong&gt;ğŸ§  RAG çŸ¥è¯†åº“å¢å¼º&lt;/strong&gt;&lt;br /&gt;ç»“åˆä»£ç è¯­ä¹‰ä¸ä¸Šä¸‹æ–‡ï¼Œå¤§å¹…é™ä½è¯¯æŠ¥ç‡&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;&lt;strong&gt;æ•°æ®éšç§æ‹…å¿§&lt;/strong&gt;&lt;br /&gt;æ‹…å¿ƒæ ¸å¿ƒæºç æ³„éœ²ç»™äº‘ç«¯ AIï¼Œæ— æ³•æ»¡è¶³åˆè§„è¦æ±‚&lt;/td&gt; 
    &lt;td align="left"&gt;&lt;strong&gt;ğŸ”’ æ”¯æŒ Ollama æœ¬åœ°éƒ¨ç½²&lt;/strong&gt;&lt;br /&gt;æ•°æ®ä¸å‡ºå†…ç½‘ï¼Œæ”¯æŒ Llama3/DeepSeek ç­‰æœ¬åœ°æ¨¡å‹&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="left"&gt;&lt;strong&gt;æ— æ³•ç¡®è®¤çœŸå®æ€§&lt;/strong&gt;&lt;br /&gt;å¤–åŒ…é¡¹ç›®æ¼æ´å¤šï¼Œä¸çŸ¥é“å“ªäº›æ¼æ´çœŸå®å¯è¢«åˆ©ç”¨&lt;/td&gt; 
    &lt;td align="left"&gt;&lt;strong&gt;ğŸ’¥ æ²™ç®± PoC éªŒè¯&lt;/strong&gt;&lt;br /&gt;è‡ªåŠ¨ç”Ÿæˆå¹¶æ‰§è¡Œæ”»å‡»è„šæœ¬ï¼Œç¡®è®¤æ¼æ´çœŸå®å±å®³&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ—ï¸ ç³»ç»Ÿæ¶æ„&lt;/h2&gt; 
&lt;h3&gt;æ•´ä½“æ¶æ„å›¾&lt;/h3&gt; 
&lt;p&gt;DeepAudit é‡‡ç”¨å¾®æœåŠ¡æ¶æ„ï¼Œæ ¸å¿ƒç”± Multi-Agent å¼•æ“é©±åŠ¨ã€‚&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="frontend/public/images/README-show/æ¶æ„å›¾.png" alt="DeepAudit æ¶æ„å›¾" width="90%" /&gt; 
&lt;/div&gt; 
&lt;h3&gt;ğŸ”„ å®¡è®¡å·¥ä½œæµ&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;æ­¥éª¤&lt;/th&gt; 
   &lt;th align="center"&gt;é˜¶æ®µ&lt;/th&gt; 
   &lt;th align="center"&gt;è´Ÿè´£ Agent&lt;/th&gt; 
   &lt;th align="left"&gt;ä¸»è¦åŠ¨ä½œ&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;1&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;ç­–ç•¥è§„åˆ’&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Orchestrator&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;æ¥æ”¶å®¡è®¡ä»»åŠ¡ï¼Œåˆ†æé¡¹ç›®ç±»å‹ï¼Œåˆ¶å®šå®¡è®¡è®¡åˆ’ï¼Œä¸‹å‘ä»»åŠ¡ç»™å­ Agent&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;2&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;ä¿¡æ¯æ”¶é›†&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Recon Agent&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;æ‰«æé¡¹ç›®ç»“æ„ï¼Œè¯†åˆ«æ¡†æ¶/åº“/APIï¼Œæå–æ”»å‡»é¢ï¼ˆEntry Pointsï¼‰&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;3&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;æ¼æ´æŒ–æ˜&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Analysis Agent&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;ç»“åˆ RAG çŸ¥è¯†åº“ä¸ AST åˆ†æï¼Œæ·±åº¦å®¡æŸ¥ä»£ç ï¼Œå‘ç°æ½œåœ¨æ¼æ´&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;4&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;PoC éªŒè¯&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Verification Agent&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;strong&gt;(å…³é”®)&lt;/strong&gt; ç¼–å†™ PoC è„šæœ¬ï¼Œåœ¨ Docker æ²™ç®±ä¸­æ‰§è¡Œã€‚å¦‚å¤±è´¥åˆ™è‡ªæˆ‘ä¿®æ­£é‡è¯•&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;5&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;æŠ¥å‘Šç”Ÿæˆ&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Orchestrator&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;æ±‡æ€»æ‰€æœ‰å‘ç°ï¼Œå‰”é™¤è¢«éªŒè¯ä¸ºè¯¯æŠ¥çš„æ¼æ´ï¼Œç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;ğŸ“‚ é¡¹ç›®ä»£ç ç»“æ„&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;DeepAudit/
â”œâ”€â”€ backend/                        # Python FastAPI åç«¯
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ agents/                 # Multi-Agent æ ¸å¿ƒé€»è¾‘
â”‚   â”‚   â”‚   â”œâ”€â”€ orchestrator.py     # æ€»æŒ‡æŒ¥ï¼šä»»åŠ¡ç¼–æ’
â”‚   â”‚   â”‚   â”œâ”€â”€ recon.py            # ä¾¦å¯Ÿå…µï¼šèµ„äº§è¯†åˆ«
â”‚   â”‚   â”‚   â”œâ”€â”€ analysis.py         # åˆ†æå¸ˆï¼šæ¼æ´æŒ–æ˜
â”‚   â”‚   â”‚   â””â”€â”€ verification.py     # éªŒè¯è€…ï¼šæ²™ç®± PoC
â”‚   â”‚   â”œâ”€â”€ core/                   # æ ¸å¿ƒé…ç½®ä¸æ²™ç®±æ¥å£
â”‚   â”‚   â”œâ”€â”€ models/                 # æ•°æ®åº“æ¨¡å‹
â”‚   â”‚   â””â”€â”€ services/               # RAG, LLM æœåŠ¡å°è£…
â”‚   â””â”€â”€ tests/                      # å•å…ƒæµ‹è¯•
â”œâ”€â”€ frontend/                       # React + TypeScript å‰ç«¯
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/             # UI ç»„ä»¶åº“
â”‚   â”‚   â”œâ”€â”€ pages/                  # é¡µé¢è·¯ç”±
â”‚   â”‚   â””â”€â”€ stores/                 # Zustand çŠ¶æ€ç®¡ç†
â”œâ”€â”€ docker/                         # Docker éƒ¨ç½²é…ç½®
â”‚   â”œâ”€â”€ sandbox/                    # å®‰å…¨æ²™ç®±é•œåƒæ„å»º
â”‚   â””â”€â”€ postgres/                   # æ•°æ®åº“åˆå§‹åŒ–
â””â”€â”€ docs/                           # è¯¦ç»†æ–‡æ¡£
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸš€ å¿«é€Ÿå¼€å§‹&lt;/h2&gt; 
&lt;h3&gt;æ–¹å¼ä¸€ï¼šä¸€è¡Œå‘½ä»¤éƒ¨ç½²ï¼ˆæ¨èï¼‰&lt;/h3&gt; 
&lt;p&gt;ä½¿ç”¨é¢„æ„å»ºçš„ Docker é•œåƒï¼Œæ— éœ€å…‹éš†ä»£ç ï¼Œä¸€è¡Œå‘½ä»¤å³å¯å¯åŠ¨ï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -fsSL https://raw.githubusercontent.com/lintsinghua/DeepAudit/v3.0.0/docker-compose.prod.yml | docker compose -f - up -d
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ğŸ‡¨ğŸ‡³ å›½å†…åŠ é€Ÿéƒ¨ç½²ï¼ˆä½œè€…äº²æµ‹éå¸¸æ— æ•Œä¹‹å¿«ï¼‰&lt;/h2&gt; 
&lt;p&gt;ä½¿ç”¨å—äº¬å¤§å­¦é•œåƒç«™åŠ é€Ÿæ‹‰å– Docker é•œåƒï¼ˆå°† &lt;code&gt;ghcr.io&lt;/code&gt; æ›¿æ¢ä¸º &lt;code&gt;ghcr.nju.edu.cn&lt;/code&gt;ï¼‰ï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# å›½å†…åŠ é€Ÿç‰ˆ - ä½¿ç”¨å—äº¬å¤§å­¦ GHCR é•œåƒç«™
curl -fsSL https://raw.githubusercontent.com/lintsinghua/DeepAudit/v3.0.0/docker-compose.prod.cn.yml | docker compose -f - up -d
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;æ‰‹åŠ¨æ‹‰å–é•œåƒï¼ˆå¦‚éœ€å•ç‹¬æ‹‰å–ï¼‰ï¼ˆç‚¹å‡»å±•å¼€ï¼‰&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# å‰ç«¯é•œåƒ
docker pull ghcr.nju.edu.cn/lintsinghua/deepaudit-frontend:latest

# åç«¯é•œåƒ
docker pull ghcr.nju.edu.cn/lintsinghua/deepaudit-backend:latest

# æ²™ç®±é•œåƒ
docker pull ghcr.nju.edu.cn/lintsinghua/deepaudit-sandbox:latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ğŸ’¡ é•œåƒæºç”± &lt;a href="https://mirrors.nju.edu.cn/"&gt;å—äº¬å¤§å­¦å¼€æºé•œåƒç«™&lt;/a&gt; æä¾›æ”¯æŒ&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ğŸ‰ &lt;strong&gt;å¯åŠ¨æˆåŠŸï¼&lt;/strong&gt; è®¿é—® &lt;a href="http://localhost:3000"&gt;http://localhost:3000&lt;/a&gt; å¼€å§‹ä½“éªŒã€‚&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr /&gt; 
&lt;h3&gt;æ–¹å¼äºŒï¼šå…‹éš†ä»£ç éƒ¨ç½²&lt;/h3&gt; 
&lt;p&gt;é€‚åˆéœ€è¦è‡ªå®šä¹‰é…ç½®æˆ–äºŒæ¬¡å¼€å‘çš„ç”¨æˆ·ï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 1. å…‹éš†é¡¹ç›®
git clone https://github.com/lintsinghua/DeepAudit.git &amp;amp;&amp;amp; cd DeepAudit

# 2. é…ç½®ç¯å¢ƒå˜é‡
cp backend/env.example backend/.env
# ç¼–è¾‘ backend/.env å¡«å…¥ä½ çš„ LLM API Key

# 3. ä¸€é”®å¯åŠ¨
docker compose up -d
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;é¦–æ¬¡å¯åŠ¨ä¼šè‡ªåŠ¨æ„å»ºæ²™ç®±é•œåƒï¼Œå¯èƒ½éœ€è¦å‡ åˆ†é’Ÿã€‚&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ”§ æºç å¼€å‘æŒ‡å—&lt;/h2&gt; 
&lt;p&gt;é€‚åˆå¼€å‘è€…è¿›è¡ŒäºŒæ¬¡å¼€å‘è°ƒè¯•ã€‚&lt;/p&gt; 
&lt;h3&gt;ç¯å¢ƒè¦æ±‚&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.11+&lt;/li&gt; 
 &lt;li&gt;Node.js 20+&lt;/li&gt; 
 &lt;li&gt;PostgreSQL 15+&lt;/li&gt; 
 &lt;li&gt;Docker (ç”¨äºæ²™ç®±)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;1. åç«¯å¯åŠ¨&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd backend
# ä½¿ç”¨ uv ç®¡ç†ç¯å¢ƒï¼ˆæ¨èï¼‰
uv sync
source .venv/bin/activate

# å¯åŠ¨ API æœåŠ¡
uvicorn app.main:app --reload
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. å‰ç«¯å¯åŠ¨&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd frontend
pnpm install
pnpm dev
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;3. æ²™ç®±ç¯å¢ƒ&lt;/h3&gt; 
&lt;p&gt;å¼€å‘æ¨¡å¼ä¸‹éœ€è¦æœ¬åœ° Docker æ‹‰å–æ²™ç®±é•œåƒï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# æ ‡å‡†æ‹‰å–
docker pull ghcr.io/lintsinghua/deepaudit-sandbox:latest

# å›½å†…åŠ é€Ÿï¼ˆå—äº¬å¤§å­¦é•œåƒç«™ï¼‰
docker pull ghcr.nju.edu.cn/lintsinghua/deepaudit-sandbox:latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ¤– Multi-Agent æ™ºèƒ½å®¡è®¡&lt;/h2&gt; 
&lt;h3&gt;æ”¯æŒçš„æ¼æ´ç±»å‹&lt;/h3&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td&gt; 
    &lt;table&gt; 
     &lt;thead&gt; 
      &lt;tr&gt; 
       &lt;th&gt;æ¼æ´ç±»å‹&lt;/th&gt; 
       &lt;th&gt;æè¿°&lt;/th&gt; 
      &lt;/tr&gt; 
     &lt;/thead&gt; 
     &lt;tbody&gt; 
      &lt;tr&gt; 
       &lt;td&gt;&lt;code&gt;sql_injection&lt;/code&gt;&lt;/td&gt; 
       &lt;td&gt;SQL æ³¨å…¥&lt;/td&gt; 
      &lt;/tr&gt; 
      &lt;tr&gt; 
       &lt;td&gt;&lt;code&gt;xss&lt;/code&gt;&lt;/td&gt; 
       &lt;td&gt;è·¨ç«™è„šæœ¬æ”»å‡»&lt;/td&gt; 
      &lt;/tr&gt; 
      &lt;tr&gt; 
       &lt;td&gt;&lt;code&gt;command_injection&lt;/code&gt;&lt;/td&gt; 
       &lt;td&gt;å‘½ä»¤æ³¨å…¥&lt;/td&gt; 
      &lt;/tr&gt; 
      &lt;tr&gt; 
       &lt;td&gt;&lt;code&gt;path_traversal&lt;/code&gt;&lt;/td&gt; 
       &lt;td&gt;è·¯å¾„éå†&lt;/td&gt; 
      &lt;/tr&gt; 
      &lt;tr&gt; 
       &lt;td&gt;&lt;code&gt;ssrf&lt;/code&gt;&lt;/td&gt; 
       &lt;td&gt;æœåŠ¡ç«¯è¯·æ±‚ä¼ªé€ &lt;/td&gt; 
      &lt;/tr&gt; 
      &lt;tr&gt; 
       &lt;td&gt;&lt;code&gt;xxe&lt;/code&gt;&lt;/td&gt; 
       &lt;td&gt;XML å¤–éƒ¨å®ä½“æ³¨å…¥&lt;/td&gt; 
      &lt;/tr&gt; 
     &lt;/tbody&gt; 
    &lt;/table&gt; &lt;/td&gt; 
   &lt;td&gt; 
    &lt;table&gt; 
     &lt;thead&gt; 
      &lt;tr&gt; 
       &lt;th&gt;æ¼æ´ç±»å‹&lt;/th&gt; 
       &lt;th&gt;æè¿°&lt;/th&gt; 
      &lt;/tr&gt; 
     &lt;/thead&gt; 
     &lt;tbody&gt; 
      &lt;tr&gt; 
       &lt;td&gt;&lt;code&gt;insecure_deserialization&lt;/code&gt;&lt;/td&gt; 
       &lt;td&gt;ä¸å®‰å…¨ååºåˆ—åŒ–&lt;/td&gt; 
      &lt;/tr&gt; 
      &lt;tr&gt; 
       &lt;td&gt;&lt;code&gt;hardcoded_secret&lt;/code&gt;&lt;/td&gt; 
       &lt;td&gt;ç¡¬ç¼–ç å¯†é’¥&lt;/td&gt; 
      &lt;/tr&gt; 
      &lt;tr&gt; 
       &lt;td&gt;&lt;code&gt;weak_crypto&lt;/code&gt;&lt;/td&gt; 
       &lt;td&gt;å¼±åŠ å¯†ç®—æ³•&lt;/td&gt; 
      &lt;/tr&gt; 
      &lt;tr&gt; 
       &lt;td&gt;&lt;code&gt;authentication_bypass&lt;/code&gt;&lt;/td&gt; 
       &lt;td&gt;è®¤è¯ç»•è¿‡&lt;/td&gt; 
      &lt;/tr&gt; 
      &lt;tr&gt; 
       &lt;td&gt;&lt;code&gt;authorization_bypass&lt;/code&gt;&lt;/td&gt; 
       &lt;td&gt;æˆæƒç»•è¿‡&lt;/td&gt; 
      &lt;/tr&gt; 
      &lt;tr&gt; 
       &lt;td&gt;&lt;code&gt;idor&lt;/code&gt;&lt;/td&gt; 
       &lt;td&gt;ä¸å®‰å…¨ç›´æ¥å¯¹è±¡å¼•ç”¨&lt;/td&gt; 
      &lt;/tr&gt; 
     &lt;/tbody&gt; 
    &lt;/table&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ğŸ“– è¯¦ç»†æ–‡æ¡£è¯·æŸ¥çœ‹ &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lintsinghua/DeepAudit/v3.0.0/docs/AGENT_AUDIT.md"&gt;Agent å®¡è®¡æŒ‡å—&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ”Œ æ”¯æŒçš„ LLM å¹³å°&lt;/h2&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td align="center" width="33%"&gt; &lt;h3&gt;ğŸŒ å›½é™…å¹³å°&lt;/h3&gt; &lt;p&gt; OpenAI GPT-4o / GPT-4&lt;br /&gt; Claude 3.5 Sonnet / Opus&lt;br /&gt; Google Gemini Pro&lt;br /&gt; DeepSeek V3 &lt;/p&gt; &lt;/td&gt; 
   &lt;td align="center" width="33%"&gt; &lt;h3&gt;ğŸ‡¨ğŸ‡³ å›½å†…å¹³å°&lt;/h3&gt; &lt;p&gt; é€šä¹‰åƒé—® Qwen&lt;br /&gt; æ™ºè°± GLM-4&lt;br /&gt; Moonshot Kimi&lt;br /&gt; æ–‡å¿ƒä¸€è¨€ Â· MiniMax Â· è±†åŒ… &lt;/p&gt; &lt;/td&gt; 
   &lt;td align="center" width="33%"&gt; &lt;h3&gt;ğŸ  æœ¬åœ°éƒ¨ç½²&lt;/h3&gt; &lt;p&gt; &lt;strong&gt;Ollama&lt;/strong&gt;&lt;br /&gt; Llama3 Â· Qwen2.5 Â· CodeLlama&lt;br /&gt; DeepSeek-Coder Â· Codestral&lt;br /&gt; &lt;em&gt;ä»£ç ä¸å‡ºå†…ç½‘&lt;/em&gt; &lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ğŸ’¡ æ”¯æŒ API ä¸­è½¬ç«™ï¼Œè§£å†³ç½‘ç»œè®¿é—®é—®é¢˜ | è¯¦ç»†é…ç½® â†’ &lt;a href="https://raw.githubusercontent.com/lintsinghua/DeepAudit/v3.0.0/docs/LLM_PROVIDERS.md"&gt;LLM å¹³å°æ”¯æŒ&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ¯ åŠŸèƒ½çŸ©é˜µ&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;åŠŸèƒ½&lt;/th&gt; 
   &lt;th&gt;è¯´æ˜&lt;/th&gt; 
   &lt;th&gt;æ¨¡å¼&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ğŸ¤– &lt;strong&gt;Agent æ·±åº¦å®¡è®¡&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Multi-Agent åä½œï¼Œè‡ªä¸»ç¼–æ’å®¡è®¡ç­–ç•¥&lt;/td&gt; 
   &lt;td&gt;Agent&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ğŸ§  &lt;strong&gt;RAG çŸ¥è¯†å¢å¼º&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;ä»£ç è¯­ä¹‰ç†è§£ï¼ŒCWE/CVE çŸ¥è¯†åº“æ£€ç´¢&lt;/td&gt; 
   &lt;td&gt;Agent&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ğŸ”’ &lt;strong&gt;æ²™ç®± PoC éªŒè¯&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Docker éš”ç¦»æ‰§è¡Œï¼ŒéªŒè¯æ¼æ´æœ‰æ•ˆæ€§&lt;/td&gt; 
   &lt;td&gt;Agent&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ğŸ—‚ï¸ &lt;strong&gt;é¡¹ç›®ç®¡ç†&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;GitHub/GitLab å¯¼å…¥ï¼ŒZIP ä¸Šä¼ ï¼Œ10+ è¯­è¨€æ”¯æŒ&lt;/td&gt; 
   &lt;td&gt;é€šç”¨&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;âš¡ &lt;strong&gt;å³æ—¶åˆ†æ&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;ä»£ç ç‰‡æ®µç§’çº§åˆ†æï¼Œç²˜è´´å³ç”¨&lt;/td&gt; 
   &lt;td&gt;é€šç”¨&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ğŸ” &lt;strong&gt;äº”ç»´æ£€æµ‹&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Bug Â· å®‰å…¨ Â· æ€§èƒ½ Â· é£æ ¼ Â· å¯ç»´æŠ¤æ€§&lt;/td&gt; 
   &lt;td&gt;é€šç”¨&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ğŸ’¡ &lt;strong&gt;What-Why-How&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;ç²¾å‡†å®šä½ + åŸå› è§£é‡Š + ä¿®å¤å»ºè®®&lt;/td&gt; 
   &lt;td&gt;é€šç”¨&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ğŸ“‹ &lt;strong&gt;å®¡è®¡è§„åˆ™&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;å†…ç½® OWASP Top 10ï¼Œæ”¯æŒè‡ªå®šä¹‰è§„åˆ™é›†&lt;/td&gt; 
   &lt;td&gt;é€šç”¨&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ğŸ“ &lt;strong&gt;æç¤ºè¯æ¨¡æ¿&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;å¯è§†åŒ–ç®¡ç†ï¼Œæ”¯æŒä¸­è‹±æ–‡åŒè¯­&lt;/td&gt; 
   &lt;td&gt;é€šç”¨&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ğŸ“Š &lt;strong&gt;æŠ¥å‘Šå¯¼å‡º&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;PDF / Markdown / JSON ä¸€é”®å¯¼å‡º&lt;/td&gt; 
   &lt;td&gt;é€šç”¨&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;âš™ï¸ &lt;strong&gt;è¿è¡Œæ—¶é…ç½®&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;æµè§ˆå™¨é…ç½® LLMï¼Œæ— éœ€é‡å¯æœåŠ¡&lt;/td&gt; 
   &lt;td&gt;é€šç”¨&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;ğŸ¦– å‘å±•è·¯çº¿å›¾&lt;/h2&gt; 
&lt;p&gt;æˆ‘ä»¬æ­£åœ¨æŒç»­æ¼”è¿›ï¼Œæœªæ¥å°†æ”¯æŒæ›´å¤šè¯­è¨€å’Œæ›´å¼ºå¤§çš„ Agent èƒ½åŠ›ã€‚&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; åŸºç¡€é™æ€åˆ†æï¼Œé›†æˆ Semgrep&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; å¼•å…¥ RAG çŸ¥è¯†åº“ï¼Œæ”¯æŒ Docker å®‰å…¨æ²™ç®±&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Multi-Agent åä½œæ¶æ„&lt;/strong&gt; (Current)&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; æ”¯æŒæ›´çœŸå®çš„æ¨¡æ‹ŸæœåŠ¡ç¯å¢ƒï¼Œè¿›è¡Œæ›´çœŸå®æ¼æ´éªŒè¯æµç¨‹&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; æ²™ç®±ä»function_callä¼˜åŒ–é›†æˆä¸ºç¨³å®šMCPæœåŠ¡&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;strong&gt;è‡ªåŠ¨ä¿®å¤ (Auto-Fix)&lt;/strong&gt;: Agent ç›´æ¥æäº¤ PR ä¿®å¤æ¼æ´&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;strong&gt;å¢é‡PRå®¡è®¡&lt;/strong&gt;: æŒç»­è·Ÿè¸ª PR å˜æ›´ï¼Œæ™ºèƒ½åˆ†ææ¼æ´ï¼Œå¹¶é›†æˆCI/CDæµç¨‹&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;strong&gt;ä¼˜åŒ–RAG&lt;/strong&gt;: æ”¯æŒè‡ªå®šä¹‰çŸ¥è¯†åº“&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ¤ è´¡çŒ®ä¸ç¤¾åŒº&lt;/h2&gt; 
&lt;h3&gt;è´¡çŒ®æŒ‡å—&lt;/h3&gt; 
&lt;p&gt;æˆ‘ä»¬éå¸¸æ¬¢è¿æ‚¨çš„è´¡çŒ®ï¼æ— è®ºæ˜¯æäº¤ Issueã€PR è¿˜æ˜¯å®Œå–„æ–‡æ¡£ã€‚ è¯·æŸ¥çœ‹ &lt;a href="https://raw.githubusercontent.com/lintsinghua/DeepAudit/v3.0.0/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; äº†è§£è¯¦æƒ…ã€‚&lt;/p&gt; 
&lt;h3&gt;ğŸ“¬ è”ç³»ä½œè€…&lt;/h3&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;strong&gt;æ¬¢è¿å¤§å®¶æ¥å’Œæˆ‘äº¤æµæ¢è®¨ï¼æ— è®ºæ˜¯æŠ€æœ¯é—®é¢˜ã€åŠŸèƒ½å»ºè®®è¿˜æ˜¯åˆä½œæ„å‘ï¼Œéƒ½æœŸå¾…ä¸ä½ æ²Ÿé€š~&lt;/strong&gt;&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;è”ç³»æ–¹å¼&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;ğŸ“§ &lt;strong&gt;é‚®ç®±&lt;/strong&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;strong&gt;&lt;a href="mailto:lintsinghua@qq.com"&gt;lintsinghua@qq.com&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;ğŸ™ &lt;strong&gt;GitHub&lt;/strong&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://github.com/lintsinghua"&gt;@lintsinghua&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸ“„ è®¸å¯è¯&lt;/h2&gt; 
&lt;p&gt;æœ¬é¡¹ç›®é‡‡ç”¨ &lt;a href="https://raw.githubusercontent.com/lintsinghua/DeepAudit/v3.0.0/LICENSE"&gt;AGPL-3.0 License&lt;/a&gt; å¼€æºã€‚&lt;/p&gt; 
&lt;h2&gt;ğŸ“ˆ é¡¹ç›®çƒ­åº¦&lt;/h2&gt; 
&lt;a href="https://star-history.com/#lintsinghua/DeepAudit&amp;amp;Date"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=lintsinghua/DeepAudit&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=lintsinghua/DeepAudit&amp;amp;type=Date" /&gt; 
  &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=lintsinghua/DeepAudit&amp;amp;type=Date" /&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;strong&gt;Made with â¤ï¸ by &lt;a href="https://github.com/lintsinghua"&gt;lintsinghua&lt;/a&gt;&lt;/strong&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;è‡´è°¢&lt;/h2&gt; 
&lt;p&gt;æ„Ÿè°¢ä»¥ä¸‹å¼€æºé¡¹ç›®çš„æ”¯æŒï¼š&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://fastapi.tiangolo.com/"&gt;FastAPI&lt;/a&gt; Â· &lt;a href="https://langchain.com/"&gt;LangChain&lt;/a&gt; Â· &lt;a href="https://langchain-ai.github.io/langgraph/"&gt;LangGraph&lt;/a&gt; Â· &lt;a href="https://www.trychroma.com/"&gt;ChromaDB&lt;/a&gt; Â· &lt;a href="https://litellm.ai/"&gt;LiteLLM&lt;/a&gt; Â· &lt;a href="https://tree-sitter.github.io/"&gt;Tree-sitter&lt;/a&gt; Â· &lt;a href="https://github.com/LoRexxar/Kunlun-M"&gt;Kunlun-M&lt;/a&gt; Â· &lt;a href="https://github.com/usestrix/strix"&gt;Strix&lt;/a&gt; Â· &lt;a href="https://react.dev/"&gt;React&lt;/a&gt; Â· &lt;a href="https://vitejs.dev/"&gt;Vite&lt;/a&gt; Â· &lt;a href="https://www.radix-ui.com/"&gt;Radix UI&lt;/a&gt; Â· &lt;a href="https://tailwindcss.com/"&gt;TailwindCSS&lt;/a&gt; Â· &lt;a href="https://ui.shadcn.com/"&gt;shadcn/ui&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;âš ï¸ é‡è¦å®‰å…¨å£°æ˜&lt;/h2&gt; 
&lt;h3&gt;æ³•å¾‹åˆè§„å£°æ˜&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;ç¦æ­¢&lt;strong&gt;ä»»ä½•æœªç»æˆæƒçš„æ¼æ´æµ‹è¯•ã€æ¸—é€æµ‹è¯•æˆ–å®‰å…¨è¯„ä¼°&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;æœ¬é¡¹ç›®ä»…ä¾›ç½‘ç»œç©ºé—´å®‰å…¨å­¦æœ¯ç ”ç©¶ã€æ•™å­¦å’Œå­¦ä¹ ä½¿ç”¨&lt;/li&gt; 
 &lt;li&gt;ä¸¥ç¦å°†æœ¬é¡¹ç›®ç”¨äºä»»ä½•éæ³•ç›®çš„æˆ–æœªç»æˆæƒçš„å®‰å…¨æµ‹è¯•&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;æ¼æ´ä¸ŠæŠ¥è´£ä»»&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;å‘ç°ä»»ä½•å®‰å…¨æ¼æ´æ—¶ï¼Œè¯·åŠæ—¶é€šè¿‡åˆæ³•æ¸ é“ä¸ŠæŠ¥&lt;/li&gt; 
 &lt;li&gt;ä¸¥ç¦åˆ©ç”¨å‘ç°çš„æ¼æ´è¿›è¡Œéæ³•æ´»åŠ¨&lt;/li&gt; 
 &lt;li&gt;éµå®ˆå›½å®¶ç½‘ç»œå®‰å…¨æ³•å¾‹æ³•è§„ï¼Œç»´æŠ¤ç½‘ç»œç©ºé—´å®‰å…¨&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;ä½¿ç”¨é™åˆ¶&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ä»…é™åœ¨æˆæƒç¯å¢ƒä¸‹ç”¨äºæ•™è‚²å’Œç ”ç©¶ç›®çš„&lt;/li&gt; 
 &lt;li&gt;ç¦æ­¢ç”¨äºå¯¹æœªæˆæƒç³»ç»Ÿè¿›è¡Œå®‰å…¨æµ‹è¯•&lt;/li&gt; 
 &lt;li&gt;ä½¿ç”¨è€…éœ€å¯¹è‡ªèº«è¡Œä¸ºæ‰¿æ‹…å…¨éƒ¨æ³•å¾‹è´£ä»»&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;å…è´£å£°æ˜&lt;/h3&gt; 
&lt;p&gt;ä½œè€…ä¸å¯¹ä»»ä½•å› ä½¿ç”¨æœ¬é¡¹ç›®è€Œå¯¼è‡´çš„ç›´æ¥æˆ–é—´æ¥æŸå¤±è´Ÿè´£ï¼Œä½¿ç”¨è€…éœ€å¯¹è‡ªèº«è¡Œä¸ºæ‰¿æ‹…å…¨éƒ¨æ³•å¾‹è´£ä»»ã€‚&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ“– è¯¦ç»†å®‰å…¨æ”¿ç­–&lt;/h2&gt; 
&lt;p&gt;æœ‰å…³å®‰è£…æ”¿ç­–ã€å…è´£å£°æ˜ã€ä»£ç éšç§ã€APIä½¿ç”¨å®‰å…¨å’Œæ¼æ´æŠ¥å‘Šçš„è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… &lt;a href="https://raw.githubusercontent.com/lintsinghua/DeepAudit/v3.0.0/DISCLAIMER.md"&gt;DISCLAIMER.md&lt;/a&gt; å’Œ &lt;a href="https://raw.githubusercontent.com/lintsinghua/DeepAudit/v3.0.0/SECURITY.md"&gt;SECURITY.md&lt;/a&gt; æ–‡ä»¶ã€‚&lt;/p&gt; 
&lt;h3&gt;å¿«é€Ÿå‚è€ƒ&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ”’ &lt;strong&gt;ä»£ç éšç§è­¦å‘Š&lt;/strong&gt;: æ‚¨çš„ä»£ç å°†è¢«å‘é€åˆ°æ‰€é€‰æ‹©çš„LLMæœåŠ¡å•†æœåŠ¡å™¨&lt;/li&gt; 
 &lt;li&gt;ğŸ›¡ï¸ &lt;strong&gt;æ•æ„Ÿä»£ç å¤„ç†&lt;/strong&gt;: ä½¿ç”¨æœ¬åœ°æ¨¡å‹å¤„ç†æ•æ„Ÿä»£ç &lt;/li&gt; 
 &lt;li&gt;âš ï¸ &lt;strong&gt;åˆè§„è¦æ±‚&lt;/strong&gt;: éµå®ˆæ•°æ®ä¿æŠ¤å’Œéšç§æ³•å¾‹æ³•è§„&lt;/li&gt; 
 &lt;li&gt;ğŸ“§ &lt;strong&gt;æ¼æ´æŠ¥å‘Š&lt;/strong&gt;: å‘ç°å®‰å…¨é—®é¢˜è¯·é€šè¿‡åˆæ³•æ¸ é“ä¸ŠæŠ¥&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>swisskyrepo/PayloadsAllTheThings</title>
      <link>https://github.com/swisskyrepo/PayloadsAllTheThings</link>
      <description>&lt;p&gt;A list of useful payloads and bypass for Web Application Security and Pentest/CTF&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Payloads All The Things&lt;/h1&gt; 
&lt;p&gt;A list of useful payloads and bypasses for Web Application Security. Feel free to improve with your payloads and techniques!&lt;/p&gt; 
&lt;p&gt;You can also contribute with a &lt;span&gt;ğŸ»&lt;/span&gt; IRL, or using the sponsor button.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/sponsors/swisskyrepo"&gt;&lt;img src="https://img.shields.io/static/v1?label=Sponsor&amp;amp;message=%E2%9D%A4&amp;amp;logo=GitHub&amp;amp;link=https://github.com/sponsors/swisskyrepo" alt="Sponsor" /&gt;&lt;/a&gt; &lt;a href="https://twitter.com/intent/tweet?text=Payloads%20All%20The%20Things,%20a%20list%20of%20useful%20payloads%20and%20bypasses%20for%20Web%20Application%20Security%20-%20by%20@pentest_swissky&amp;amp;url=https://github.com/swisskyrepo/PayloadsAllTheThings/"&gt;&lt;img src="https://img.shields.io/twitter/url/http/shields.io.svg?style=social" alt="Tweet" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;An alternative display version is available at &lt;a href="https://swisskyrepo.github.io/PayloadsAllTheThings/"&gt;PayloadsAllTheThingsWeb&lt;/a&gt;.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/swisskyrepo/PayloadsAllTheThings/master/.github/banner.png" alt="banner" /&gt; &lt;/p&gt; 
&lt;h2&gt;&lt;span&gt;ğŸ“–&lt;/span&gt; Documentation&lt;/h2&gt; 
&lt;p&gt;Every section contains the following files, you can use the &lt;code&gt;_template_vuln&lt;/code&gt; folder to create a new chapter:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;README.md - vulnerability description and how to exploit it, including several payloads&lt;/li&gt; 
 &lt;li&gt;Intruder - a set of files to give to Burp Intruder&lt;/li&gt; 
 &lt;li&gt;Images - pictures for the README.md&lt;/li&gt; 
 &lt;li&gt;Files - some files referenced in the README.md&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You might also like the other projects from the AllTheThings family :&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://swisskyrepo.github.io/InternalAllTheThings/"&gt;InternalAllTheThings&lt;/a&gt; - Active Directory and Internal Pentest Cheatsheets&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://swisskyrepo.github.io/HardwareAllTheThings/"&gt;HardwareAllTheThings&lt;/a&gt; - Hardware/IOT Pentesting Wiki&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You want more? Check the &lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/raw/master/_LEARNING_AND_SOCIALS/BOOKS.md"&gt;Books&lt;/a&gt; and &lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/raw/master/_LEARNING_AND_SOCIALS/YOUTUBE.md"&gt;YouTube channel&lt;/a&gt; selections.&lt;/p&gt; 
&lt;h2&gt;&lt;span&gt;ğŸ§‘ğŸ’»&lt;/span&gt; Contributions&lt;/h2&gt; 
&lt;p&gt;Be sure to read &lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/raw/master/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/swisskyrepo/PayloadsAllTheThings/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=swisskyrepo/PayloadsAllTheThings&amp;amp;max=36" alt="sponsors-list" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;Thanks again for your contribution! &lt;span&gt;â¤ï¸&lt;/span&gt;&lt;/p&gt; 
&lt;h2&gt;&lt;span&gt;ğŸ»&lt;/span&gt; Sponsors&lt;/h2&gt; 
&lt;p&gt;This project is proudly sponsored by these companies.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Logo&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://serpapi.com"&gt;&lt;img src="https://avatars.githubusercontent.com/u/34724717?s=40&amp;amp;v=4" alt="sponsor-serpapi" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;SerpApi&lt;/strong&gt; is a real time API to access Google search results. It solves the issues of having to rent proxies, solving captchas, and JSON parsing.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://projectdiscovery.io/"&gt;&lt;img src="https://avatars.githubusercontent.com/u/50994705?s=40&amp;amp;v=4" alt="sponsor-projectdiscovery" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;ProjectDiscovery&lt;/strong&gt; - Detect real, exploitable vulnerabilities. Harness the power of Nuclei for fast and accurate findings without false positives.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.vaadata.com/"&gt;&lt;img src="https://avatars.githubusercontent.com/u/48131541?s=40&amp;amp;v=4" alt="sponsor-vaadata" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;VAADATA&lt;/strong&gt; - Ethical Hacking Services&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt;</description>
    </item>
    
    <item>
      <title>goauthentik/authentik</title>
      <link>https://github.com/goauthentik/authentik</link>
      <description>&lt;p&gt;The authentication glue you need.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://goauthentik.io/img/icon_top_brand_colour.svg?sanitize=true" height="150" alt="authentik logo" /&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;a href="https://goauthentik.io/discord"&gt;&lt;img src="https://img.shields.io/discord/809154715984199690?label=Discord&amp;amp;style=for-the-badge" alt="Join Discord" /&gt;&lt;/a&gt; &lt;a href="https://github.com/goauthentik/authentik/actions/workflows/ci-main.yml"&gt;&lt;img src="https://img.shields.io/github/actions/workflow/status/goauthentik/authentik/ci-main.yml?branch=main&amp;amp;label=core%20build&amp;amp;style=for-the-badge" alt="GitHub Workflow Status" /&gt;&lt;/a&gt; &lt;a href="https://github.com/goauthentik/authentik/actions/workflows/ci-outpost.yml"&gt;&lt;img src="https://img.shields.io/github/actions/workflow/status/goauthentik/authentik/ci-outpost.yml?branch=main&amp;amp;label=outpost%20build&amp;amp;style=for-the-badge" alt="GitHub Workflow Status" /&gt;&lt;/a&gt; &lt;a href="https://github.com/goauthentik/authentik/actions/workflows/ci-web.yml"&gt;&lt;img src="https://img.shields.io/github/actions/workflow/status/goauthentik/authentik/ci-web.yml?branch=main&amp;amp;label=web%20build&amp;amp;style=for-the-badge" alt="GitHub Workflow Status" /&gt;&lt;/a&gt; &lt;a href="https://codecov.io/gh/goauthentik/authentik"&gt;&lt;img src="https://img.shields.io/codecov/c/gh/goauthentik/authentik?style=for-the-badge" alt="Code Coverage" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/docker/v/authentik/server?sort=semver&amp;amp;style=for-the-badge" alt="Latest version" /&gt; &lt;a href="https://explore.transifex.com/authentik/authentik/"&gt;&lt;img src="https://img.shields.io/badge/Help%20translate-transifex-blue?style=for-the-badge" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;What is authentik?&lt;/h2&gt; 
&lt;p&gt;authentik is an open-source Identity Provider (IdP) for modern SSO. It supports SAML, OAuth2/OIDC, LDAP, RADIUS, and more, designed for self-hosting from small labs to large production clusters.&lt;/p&gt; 
&lt;p&gt;Our &lt;a href="https://goauthentik.io/pricing"&gt;enterprise offering&lt;/a&gt; is available for organizations to securely replace existing IdPs such as Okta, Auth0, Entra ID, and Ping Identity for robust, large-scale identity management.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Docker Compose: recommended for small/test setups. See the &lt;a href="https://docs.goauthentik.io/docs/install-config/install/docker-compose/"&gt;documentation&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Kubernetes (Helm Chart): recommended for larger setups. See the &lt;a href="https://docs.goauthentik.io/docs/install-config/install/kubernetes/"&gt;documentation&lt;/a&gt; and the Helm chart &lt;a href="https://github.com/goauthentik/helm"&gt;repository&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;AWS CloudFormation: deploy on AWS using our official templates. See the &lt;a href="https://docs.goauthentik.io/docs/install-config/install/aws/"&gt;documentation&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;DigitalOcean Marketplace: one-click deployment via the official Marketplace app. See the &lt;a href="https://marketplace.digitalocean.com/apps/authentik"&gt;app listing&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Screenshots&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Light&lt;/th&gt; 
   &lt;th&gt;Dark&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src="https://docs.goauthentik.io/img/screen_apps_light.jpg" alt="" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://docs.goauthentik.io/img/screen_apps_dark.jpg" alt="" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src="https://docs.goauthentik.io/img/screen_admin_light.jpg" alt="" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://docs.goauthentik.io/img/screen_admin_dark.jpg" alt="" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Development and contributions&lt;/h2&gt; 
&lt;p&gt;See the &lt;a href="https://docs.goauthentik.io/docs/developer-docs/"&gt;Developer Documentation&lt;/a&gt; for information about setting up local build environments, testing your contributions, and our contribution process.&lt;/p&gt; 
&lt;h2&gt;Security&lt;/h2&gt; 
&lt;p&gt;Please see &lt;a href="https://raw.githubusercontent.com/goauthentik/authentik/main/SECURITY.md"&gt;SECURITY.md&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Adoption&lt;/h2&gt; 
&lt;p&gt;Using authentik? We'd love to hear your story and feature your logo. Email us at &lt;a href="mailto:hello@goauthentik.io"&gt;hello@goauthentik.io&lt;/a&gt; or open a GitHub Issue/PR!&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/goauthentik/authentik/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/License-MIT-green?style=for-the-badge" alt="MIT License" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/goauthentik/authentik/main/website/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/License-CC%20BY--SA%204.0-lightgrey?style=for-the-badge" alt="CC BY-SA 4.0" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/goauthentik/authentik/main/authentik/enterprise/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/License-EE-orange?style=for-the-badge" alt="authentik EE License" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>sgl-project/mini-sglang</title>
      <link>https://github.com/sgl-project/mini-sglang</link>
      <description>&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img width="400" src="https://raw.githubusercontent.com/sgl-project/mini-sglang/main/assets/logo.png" /&gt; &lt;/p&gt; 
&lt;h1&gt;Mini-SGLang&lt;/h1&gt; 
&lt;p&gt;A &lt;strong&gt;lightweight yet high-performance&lt;/strong&gt; inference framework for Large Language Models.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;Mini-SGLang is a compact implementation of &lt;a href="https://github.com/sgl-project/sglang"&gt;SGLang&lt;/a&gt;, designed to demystify the complexities of modern LLM serving systems. With a compact codebase of &lt;strong&gt;~5,000 lines of Python&lt;/strong&gt;, it serves as both a capable inference engine and a transparent reference for researchers and developers.&lt;/p&gt; 
&lt;h2&gt;âœ¨ Key Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;High Performance&lt;/strong&gt;: Achieves state-of-the-art throughput and latency with advanced optimizations.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Lightweight &amp;amp; Readable&lt;/strong&gt;: A clean, modular, and fully type-annotated codebase that is easy to understand and modify.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Advanced Optimizations&lt;/strong&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Radix Cache&lt;/strong&gt;: Reuses KV cache for shared prefixes across requests.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Chunked Prefill&lt;/strong&gt;: Reduces peak memory usage for long-context serving.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Overlap Scheduling&lt;/strong&gt;: Hides CPU scheduling overhead with GPU computation.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Tensor Parallelism&lt;/strong&gt;: Scales inference across multiple GPUs.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Optimized Kernels&lt;/strong&gt;: Integrates &lt;strong&gt;FlashAttention&lt;/strong&gt; and &lt;strong&gt;FlashInfer&lt;/strong&gt; for maximum efficiency.&lt;/li&gt; 
   &lt;li&gt;...&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸš€ Quick Start&lt;/h2&gt; 
&lt;h3&gt;1. Environment Setup&lt;/h3&gt; 
&lt;p&gt;We recommend using &lt;code&gt;uv&lt;/code&gt; for a fast and reliable installation (note that &lt;code&gt;uv&lt;/code&gt; does not conflict with &lt;code&gt;conda&lt;/code&gt;).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Create a virtual environment (Python 3.10+ recommended)
uv venv --python=3.12
source .venv/bin/activate
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Prerequisites&lt;/strong&gt;: Mini-SGLang relies on CUDA kernels that are JIT-compiled. Ensure you have the &lt;strong&gt;NVIDIA CUDA Toolkit&lt;/strong&gt; installed and that its version matches your driver's version. You can check your driver's CUDA capability with &lt;code&gt;nvidia-smi&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;2. Installation&lt;/h3&gt; 
&lt;p&gt;Install Mini-SGLang directly from the source:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/sgl-project/mini-sglang.git
cd mini-sglang &amp;amp;&amp;amp; uv venv --python=3.12 &amp;amp;&amp;amp; source .venv/bin/activate
uv pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;3. Online Serving&lt;/h3&gt; 
&lt;p&gt;Launch an OpenAI-compatible API server with a single command.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Deploy Qwen/Qwen3-0.6B on a single GPU
python -m minisgl --model "Qwen/Qwen3-0.6B"

# Deploy meta-llama/Llama-3.1-70B-Instruct on 4 GPUs with Tensor Parallelism, on port 30000
python -m minisgl --model "meta-llama/Llama-3.1-70B-Instruct" --tp 4 --port 30000
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Once the server is running, you can send requests using standard tools like &lt;code&gt;curl&lt;/code&gt; or any OpenAI-compatible client.&lt;/p&gt; 
&lt;h3&gt;4. Interactive Shell&lt;/h3&gt; 
&lt;p&gt;Chat with your model directly in the terminal by adding the &lt;code&gt;--shell&lt;/code&gt; flag.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m minisgl --model "Qwen/Qwen3-0.6B" --shell
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src="https://lmsys.org/images/blog/minisgl/shell.png" alt="shell-example" /&gt;&lt;/p&gt; 
&lt;p&gt;You can also use &lt;code&gt;/reset&lt;/code&gt; to clear the chat history.&lt;/p&gt; 
&lt;h2&gt;Benchmark&lt;/h2&gt; 
&lt;h3&gt;Offline inference&lt;/h3&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/sgl-project/mini-sglang/main/benchmark/offline/bench.py"&gt;bench.py&lt;/a&gt; for more details. Set &lt;code&gt;MINISGL_DISABLE_OVERLAP_SCHEDULING=1&lt;/code&gt; for ablation study on overlap scheduling.&lt;/p&gt; 
&lt;p&gt;Test Configuration:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Hardware: 1xH200 GPU.&lt;/li&gt; 
 &lt;li&gt;Model: Qwen3-0.6B, Qwen3-14B&lt;/li&gt; 
 &lt;li&gt;Total Requests: 256 sequences&lt;/li&gt; 
 &lt;li&gt;Input Length: Randomly sampled between 100-1024 tokens&lt;/li&gt; 
 &lt;li&gt;Output Length: Randomly sampled between 100-1024 tokens&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://lmsys.org/images/blog/minisgl/offline.png" alt="offline" /&gt;&lt;/p&gt; 
&lt;h3&gt;Online inference&lt;/h3&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/sgl-project/mini-sglang/main/benchmark/online/bench_qwen.py"&gt;benchmark_qwen.py&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;p&gt;Test Configuration:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Hardware: 4xH200 GPU, connected by NVLink.&lt;/li&gt; 
 &lt;li&gt;Model: Qwen3-32B&lt;/li&gt; 
 &lt;li&gt;Dataset: &lt;a href="https://github.com/alibaba-edu/qwen-bailian-usagetraces-anon/raw/main/qwen_traceA_blksz_16.jsonl"&gt;Qwen trace&lt;/a&gt;, replaying first 1000 requests.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Launch command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Mini-SGLang
python -m minisgl --model "Qwen/Qwen3-32B" --tp 4 --cache naive

# SGLang
python3 -m sglang.launch_server --model "Qwen/Qwen3-32B" --tp 4 \
    --disable-radix --port 1919 --decode-attention flashinfer
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src="https://lmsys.org/images/blog/minisgl/online.png" alt="online" /&gt;&lt;/p&gt; 
&lt;h2&gt;ğŸ“š Learn More&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/sgl-project/mini-sglang/main/docs/features.md"&gt;Detailed Features&lt;/a&gt;&lt;/strong&gt;: Explore all available features and command-line arguments.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/sgl-project/mini-sglang/main/docs/structures.md"&gt;System Architecture&lt;/a&gt;&lt;/strong&gt;: Dive deep into the design and data flow of Mini-SGLang.&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>roboflow/rf-detr</title>
      <link>https://github.com/roboflow/rf-detr</link>
      <description>&lt;p&gt;RF-DETR is a real-time object detection and segmentation model architecture developed by Roboflow, SOTA on COCO and designed for fine-tuning.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;RF-DETR: SOTA Real-Time Detection and Segmentation Model&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://badge.fury.io/py/rfdetr"&gt;&lt;img src="https://badge.fury.io/py/rfdetr.svg?sanitize=true" alt="version" /&gt;&lt;/a&gt; &lt;a href="https://pypistats.org/packages/rfdetr"&gt;&lt;img src="https://img.shields.io/pypi/dm/rfdetr" alt="downloads" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2511.09554"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2511.09554-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://badge.fury.io/py/rfdetr"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/rfdetr" alt="python-version" /&gt;&lt;/a&gt; &lt;a href="https://github.com/roboflow/rfdetr/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/license-Apache%202.0-blue" alt="license" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/spaces/SkalskiP/RF-DETR"&gt;&lt;img src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue" alt="hf space" /&gt;&lt;/a&gt; &lt;a href="https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-finetune-rf-detr-on-detection-dataset.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="colab" /&gt;&lt;/a&gt; &lt;a href="https://blog.roboflow.com/rf-detr"&gt;&lt;img src="https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true" alt="roboflow" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/GbfgXGJ8Bk"&gt;&lt;img src="https://img.shields.io/discord/1159501506232451173?logo=discord&amp;amp;label=discord&amp;amp;labelColor=fff&amp;amp;color=5865f2&amp;amp;link=https%3A%2F%2Fdiscord.gg%2FGbfgXGJ8Bk" alt="discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;RF-DETR is a real-time, transformer-based object detection and instance segmentation model architecture developed by Roboflow and released under the Apache 2.0 license.&lt;/p&gt; 
&lt;p&gt;RF-DETR is the first real-time model to exceed 60 AP on the &lt;a href="https://cocodataset.org/#home"&gt;Microsoft COCO object detection benchmark&lt;/a&gt; alongside competitive performance at base sizes. It also achieves state-of-the-art performance on &lt;a href="https://github.com/roboflow/rf100-vl"&gt;RF100-VL&lt;/a&gt;, an object detection benchmark that measures model domain adaptability to real world problems. RF-DETR is fastest and most accurate for its size when compared current real-time objection models.&lt;/p&gt; 
&lt;p&gt;On image segmentation, RF-DETR Seg (Preview) is 3x faster and more accurate than the largest YOLO when evaluated on the Microsoft COCO Segmentation benchmark, defining a new real-time state-of-the-art for the industry-standard benchmark in segmentation model evaluation.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://youtu.be/-OvpdLAElFA"&gt;&lt;img src="https://github.com/user-attachments/assets/555a45c3-96e8-4d8a-ad29-f23403c8edfd" alt="rf-detr-tutorial-banner" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;2025/11/13&lt;/code&gt;: We release a paper representing our work on RF-DETR, &lt;a href="https://arxiv.org/abs/2511.09554"&gt;RF-DETR: Neural Architecture Search for Real-Time Detection Transformers&lt;/a&gt;, on Arxiv.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;2025/10/02&lt;/code&gt;: We release RF-DETR-Seg (Preview), a preview of our instance segmentation head for RF-DETR.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;2025/07/23&lt;/code&gt;: We release three new checkpoints for RF-DETR: Nano, Small, and Medium. 
  &lt;ul&gt; 
   &lt;li&gt;RF-DETR Base is now deprecated. We recommend using RF-DETR Medium which offers subtantially better accuracy at comparable latency.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;2025/05/16&lt;/code&gt;: We release an 'optimize_for_inference' method which speeds up native PyTorch by up to 2x, depending on platform.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;2025/04/03&lt;/code&gt;: We release early stopping, gradient checkpointing, metrics saving, training resume, TensorBoard and W&amp;amp;B logging support.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;2025/03/20&lt;/code&gt;: We release RF-DETR real-time object detection model. &lt;strong&gt;Code and checkpoint for RF-DETR-large and RF-DETR-base are available.&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Results&lt;/h2&gt; 
&lt;p&gt;RF-DETR achieves state-of-the-art performance on both the Microsoft COCO and the RF100-VL benchmarks.&lt;/p&gt; 
&lt;p&gt;The below tables shows how RF-DETR performs when validated on the Microsoft COCO benchmark for object detection and image segmentation.&lt;/p&gt; 
&lt;h3&gt;Object Detection Benchmarks&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://media.roboflow.com/rfdetr/pareto1.png" alt="rf-detr-coco-rf100-vl-9" /&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;Architecture&lt;/th&gt; 
   &lt;th align="center"&gt;COCO AP&lt;sub&gt;50&lt;/sub&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;COCO AP&lt;sub&gt;50:95&lt;/sub&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;RF100VL AP&lt;sub&gt;50&lt;/sub&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;RF100VL AP&lt;sub&gt;50:95&lt;/sub&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;Latency (ms)&lt;/th&gt; 
   &lt;th align="center"&gt;Params (M)&lt;/th&gt; 
   &lt;th align="center"&gt;Resolution&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;RF-DETR-N&lt;/td&gt; 
   &lt;td align="center"&gt;67.6&lt;/td&gt; 
   &lt;td align="center"&gt;48.4&lt;/td&gt; 
   &lt;td align="center"&gt;84.1&lt;/td&gt; 
   &lt;td align="center"&gt;57.1&lt;/td&gt; 
   &lt;td align="center"&gt;2.32&lt;/td&gt; 
   &lt;td align="center"&gt;30.5&lt;/td&gt; 
   &lt;td align="center"&gt;384x384&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;RF-DETR-S&lt;/td&gt; 
   &lt;td align="center"&gt;72.1&lt;/td&gt; 
   &lt;td align="center"&gt;53.0&lt;/td&gt; 
   &lt;td align="center"&gt;85.9&lt;/td&gt; 
   &lt;td align="center"&gt;59.6&lt;/td&gt; 
   &lt;td align="center"&gt;3.52&lt;/td&gt; 
   &lt;td align="center"&gt;32.1&lt;/td&gt; 
   &lt;td align="center"&gt;512x512&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;RF-DETR-M&lt;/td&gt; 
   &lt;td align="center"&gt;73.6&lt;/td&gt; 
   &lt;td align="center"&gt;54.7&lt;/td&gt; 
   &lt;td align="center"&gt;86.6&lt;/td&gt; 
   &lt;td align="center"&gt;60.6&lt;/td&gt; 
   &lt;td align="center"&gt;4.52&lt;/td&gt; 
   &lt;td align="center"&gt;33.7&lt;/td&gt; 
   &lt;td align="center"&gt;576x576&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;YOLO11-N&lt;/td&gt; 
   &lt;td align="center"&gt;52.0&lt;/td&gt; 
   &lt;td align="center"&gt;37.4&lt;/td&gt; 
   &lt;td align="center"&gt;81.4&lt;/td&gt; 
   &lt;td align="center"&gt;55.3&lt;/td&gt; 
   &lt;td align="center"&gt;2.49&lt;/td&gt; 
   &lt;td align="center"&gt;2.6&lt;/td&gt; 
   &lt;td align="center"&gt;640x640&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;YOLO11-S&lt;/td&gt; 
   &lt;td align="center"&gt;59.7&lt;/td&gt; 
   &lt;td align="center"&gt;44.4&lt;/td&gt; 
   &lt;td align="center"&gt;82.3&lt;/td&gt; 
   &lt;td align="center"&gt;56.2&lt;/td&gt; 
   &lt;td align="center"&gt;3.16&lt;/td&gt; 
   &lt;td align="center"&gt;9.4&lt;/td&gt; 
   &lt;td align="center"&gt;640x640&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;YOLO11-M&lt;/td&gt; 
   &lt;td align="center"&gt;64.1&lt;/td&gt; 
   &lt;td align="center"&gt;48.6&lt;/td&gt; 
   &lt;td align="center"&gt;82.5&lt;/td&gt; 
   &lt;td align="center"&gt;56.5&lt;/td&gt; 
   &lt;td align="center"&gt;5.13&lt;/td&gt; 
   &lt;td align="center"&gt;20.1&lt;/td&gt; 
   &lt;td align="center"&gt;640x640&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;YOLO11-L&lt;/td&gt; 
   &lt;td align="center"&gt;65.3&lt;/td&gt; 
   &lt;td align="center"&gt;50.2&lt;/td&gt; 
   &lt;td align="center"&gt;x&lt;/td&gt; 
   &lt;td align="center"&gt;x&lt;/td&gt; 
   &lt;td align="center"&gt;6.65&lt;/td&gt; 
   &lt;td align="center"&gt;25.3&lt;/td&gt; 
   &lt;td align="center"&gt;640x640&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;YOLO11-X&lt;/td&gt; 
   &lt;td align="center"&gt;66.5&lt;/td&gt; 
   &lt;td align="center"&gt;51.2&lt;/td&gt; 
   &lt;td align="center"&gt;x&lt;/td&gt; 
   &lt;td align="center"&gt;x&lt;/td&gt; 
   &lt;td align="center"&gt;11.92&lt;/td&gt; 
   &lt;td align="center"&gt;56.9&lt;/td&gt; 
   &lt;td align="center"&gt;640x640&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;LW-DETR-T&lt;/td&gt; 
   &lt;td align="center"&gt;60.7&lt;/td&gt; 
   &lt;td align="center"&gt;42.9&lt;/td&gt; 
   &lt;td align="center"&gt;x&lt;/td&gt; 
   &lt;td align="center"&gt;x&lt;/td&gt; 
   &lt;td align="center"&gt;1.91&lt;/td&gt; 
   &lt;td align="center"&gt;12.1&lt;/td&gt; 
   &lt;td align="center"&gt;640x640&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;LW-DETR-S&lt;/td&gt; 
   &lt;td align="center"&gt;66.8&lt;/td&gt; 
   &lt;td align="center"&gt;48.0&lt;/td&gt; 
   &lt;td align="center"&gt;84.5&lt;/td&gt; 
   &lt;td align="center"&gt;58.0&lt;/td&gt; 
   &lt;td align="center"&gt;2.62&lt;/td&gt; 
   &lt;td align="center"&gt;14.6&lt;/td&gt; 
   &lt;td align="center"&gt;640x640&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;LW-DETR-M&lt;/td&gt; 
   &lt;td align="center"&gt;72.0&lt;/td&gt; 
   &lt;td align="center"&gt;52.6&lt;/td&gt; 
   &lt;td align="center"&gt;85.2&lt;/td&gt; 
   &lt;td align="center"&gt;59.4&lt;/td&gt; 
   &lt;td align="center"&gt;4.49&lt;/td&gt; 
   &lt;td align="center"&gt;28.2&lt;/td&gt; 
   &lt;td align="center"&gt;640x640&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;D-FINE-N&lt;/td&gt; 
   &lt;td align="center"&gt;60.2&lt;/td&gt; 
   &lt;td align="center"&gt;42.7&lt;/td&gt; 
   &lt;td align="center"&gt;83.6&lt;/td&gt; 
   &lt;td align="center"&gt;57.7&lt;/td&gt; 
   &lt;td align="center"&gt;2.12&lt;/td&gt; 
   &lt;td align="center"&gt;3.8&lt;/td&gt; 
   &lt;td align="center"&gt;640x640&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;D-FINE-S&lt;/td&gt; 
   &lt;td align="center"&gt;67.6&lt;/td&gt; 
   &lt;td align="center"&gt;50.7&lt;/td&gt; 
   &lt;td align="center"&gt;84.5&lt;/td&gt; 
   &lt;td align="center"&gt;59.9&lt;/td&gt; 
   &lt;td align="center"&gt;3.55&lt;/td&gt; 
   &lt;td align="center"&gt;10.2&lt;/td&gt; 
   &lt;td align="center"&gt;640x640&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;D-FINE-M&lt;/td&gt; 
   &lt;td align="center"&gt;72.6&lt;/td&gt; 
   &lt;td align="center"&gt;55.1&lt;/td&gt; 
   &lt;td align="center"&gt;84.6&lt;/td&gt; 
   &lt;td align="center"&gt;60.2&lt;/td&gt; 
   &lt;td align="center"&gt;5.68&lt;/td&gt; 
   &lt;td align="center"&gt;19.2&lt;/td&gt; 
   &lt;td align="center"&gt;640x640&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;a href="https://rfdetr.roboflow.com/learn/benchmarks/"&gt;See our benchmark notes in the RF-DETR documentation.&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;We are actively working on RF-DETR Large and X-Large models using the same techniques we used to achieve the strong accuracy that RF-DETR Medium attains. This is why RF-DETR Large and X-Large is not yet reported on our pareto charts and why we haven't benchmarked other models at similar sizes. Check back in the next few weeks for the launch of new RF-DETR Large and X-Large models.&lt;/em&gt;&lt;/p&gt; 
&lt;h3&gt;Instance Segmentation Benchmarks&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://media.roboflow.com/rfdetr/pareto_segmentation.png" alt="rf-detr-coco-rf100-vl-9" /&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model Name&lt;/th&gt; 
   &lt;th&gt;Reported Latency&lt;/th&gt; 
   &lt;th&gt;Reported mAP&lt;/th&gt; 
   &lt;th&gt;Measured Latency&lt;/th&gt; 
   &lt;th&gt;Measured mAP&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;RF-DETR Seg-Preview@312&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;3.3&lt;/td&gt; 
   &lt;td&gt;39.4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;YOLO11n-Seg&lt;/td&gt; 
   &lt;td&gt;1.8&lt;/td&gt; 
   &lt;td&gt;32.0&lt;/td&gt; 
   &lt;td&gt;3.6&lt;/td&gt; 
   &lt;td&gt;30.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;YOLOv8n-Seg&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;30.5&lt;/td&gt; 
   &lt;td&gt;3.5&lt;/td&gt; 
   &lt;td&gt;28.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;RF-DETR Seg-Preview@384&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;4.5&lt;/td&gt; 
   &lt;td&gt;42.7&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;YOLO11s-Seg&lt;/td&gt; 
   &lt;td&gt;2.9&lt;/td&gt; 
   &lt;td&gt;37.8&lt;/td&gt; 
   &lt;td&gt;4.6&lt;/td&gt; 
   &lt;td&gt;35.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;YOLOv8s-Seg&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;36.8&lt;/td&gt; 
   &lt;td&gt;4.2&lt;/td&gt; 
   &lt;td&gt;34.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;RF-DETR Seg-Preview@432&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;5.6&lt;/td&gt; 
   &lt;td&gt;44.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;YOLO11m-Seg&lt;/td&gt; 
   &lt;td&gt;6.3&lt;/td&gt; 
   &lt;td&gt;41.5&lt;/td&gt; 
   &lt;td&gt;6.9&lt;/td&gt; 
   &lt;td&gt;38.5&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;YOLOv8m-Seg&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;40.8&lt;/td&gt; 
   &lt;td&gt;7.0&lt;/td&gt; 
   &lt;td&gt;37.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;YOLO11l-Seg&lt;/td&gt; 
   &lt;td&gt;7.8&lt;/td&gt; 
   &lt;td&gt;42.9&lt;/td&gt; 
   &lt;td&gt;8.3&lt;/td&gt; 
   &lt;td&gt;39.5&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;YOLOv8l-Seg&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;42.6&lt;/td&gt; 
   &lt;td&gt;9.7&lt;/td&gt; 
   &lt;td&gt;39.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;YOLO11x-Seg&lt;/td&gt; 
   &lt;td&gt;15.8&lt;/td&gt; 
   &lt;td&gt;43.8&lt;/td&gt; 
   &lt;td&gt;13.7&lt;/td&gt; 
   &lt;td&gt;40.1&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;YOLOv8x-Seg&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;43.4&lt;/td&gt; 
   &lt;td&gt;14.0&lt;/td&gt; 
   &lt;td&gt;39.5&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;For more information on measuring end-to-end latency for models, see our open source &lt;a href="https://github.com/roboflow/single_artifact_benchmarking"&gt;Single Artifact Benchmarking tool&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;To install RF-DETR, install the &lt;code&gt;rfdetr&lt;/code&gt; package in a &lt;a href="https://www.python.org/"&gt;&lt;strong&gt;Python&amp;gt;=3.9&lt;/strong&gt;&lt;/a&gt; environment with &lt;code&gt;pip&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install rfdetr
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;Install from source&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p&gt;By installing RF-DETR from source, you can explore the most recent features and enhancements that have not yet been officially released. Please note that these updates are still in development and may not be as stable as the latest published release.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;pip install git+https://github.com/roboflow/rf-detr.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;Inference&lt;/h2&gt; 
&lt;p&gt;The easiest path to deployment is using Roboflow's &lt;a href="https://github.com/roboflow/inference"&gt;Inference&lt;/a&gt; package.&lt;/p&gt; 
&lt;p&gt;The code below lets you run &lt;code&gt;rfdetr-base&lt;/code&gt; on an image:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import os
import supervision as sv
from inference import get_model
from PIL import Image
from io import BytesIO
import requests

url = "https://media.roboflow.com/dog.jpeg"
image = Image.open(BytesIO(requests.get(url).content))

model = get_model("rfdetr-base")

predictions = model.infer(image, confidence=0.5)[0]

detections = sv.Detections.from_inference(predictions)

labels = [prediction.class_name for prediction in predictions.predictions]

annotated_image = image.copy()
annotated_image = sv.BoxAnnotator(color=sv.ColorPalette.ROBOFLOW).annotate(annotated_image, detections)
annotated_image = sv.LabelAnnotator(color=sv.ColorPalette.ROBOFLOW).annotate(annotated_image, detections, labels)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To use segmentation, use the &lt;code&gt;rfdetr-seg-preview&lt;/code&gt; model ID. This model will return segmentation masks from a RF-DETR-Seg (Preview) model trained on the Microsoft COCO dataset.&lt;/p&gt; 
&lt;h2&gt;Predict&lt;/h2&gt; 
&lt;p&gt;You can also use the .predict method to perform inference during local development. The &lt;code&gt;.predict()&lt;/code&gt; method accepts various input formats, including file paths, PIL images, NumPy arrays, and torch tensors. Please ensure inputs use RGB channel order. For &lt;code&gt;torch.Tensor&lt;/code&gt; inputs specifically, they must have a shape of &lt;code&gt;(3, H, W)&lt;/code&gt; with values normalized to the &lt;code&gt;[0..1)&lt;/code&gt; range. If you don't plan to modify the image or batch size dynamically at runtime, you can also use &lt;code&gt;.optimize_for_inference()&lt;/code&gt; to get up to 2x end-to-end speedup, depending on platform.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import io
import requests
import supervision as sv
from PIL import Image
from rfdetr import RFDETRBase
from rfdetr.util.coco_classes import COCO_CLASSES

model = RFDETRBase()

model.optimize_for_inference()

url = "https://media.roboflow.com/notebooks/examples/dog-2.jpeg"

image = Image.open(io.BytesIO(requests.get(url).content))
detections = model.predict(image, threshold=0.5)

labels = [
    f"{COCO_CLASSES[class_id]} {confidence:.2f}"
    for class_id, confidence
    in zip(detections.class_id, detections.confidence)
]

annotated_image = image.copy()
annotated_image = sv.BoxAnnotator().annotate(annotated_image, detections)
annotated_image = sv.LabelAnnotator().annotate(annotated_image, detections, labels)

sv.plot_image(annotated_image)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Train a Model&lt;/h3&gt; 
&lt;p&gt;You can fine-tune an RF-DETR Nano, Small, Medium, and Base model with a custom dataset using the &lt;code&gt;rfdetr&lt;/code&gt; Python package.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://rfdetr.roboflow.com/learn/train/"&gt;Learn how to train an RF-DETR model.&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;Visit our &lt;a href="https://rfdetr.roboflow.com"&gt;documentation website&lt;/a&gt; to learn more about how to use RF-DETR.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Both the code and the weights pretrained on the COCO dataset are released under the &lt;a href="https://github.com/roboflow/r-flow/raw/main/LICENSE"&gt;Apache 2.0 license&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;p&gt;Our work is built upon &lt;a href="https://arxiv.org/pdf/2406.03459"&gt;LW-DETR&lt;/a&gt;, &lt;a href="https://arxiv.org/pdf/2304.07193"&gt;DINOv2&lt;/a&gt;, and &lt;a href="https://arxiv.org/pdf/2010.04159"&gt;Deformable DETR&lt;/a&gt;. Thanks to their authors for their excellent work!&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you find our work helpful for your research, please consider citing the following BibTeX entry.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{rf-detr,
    title={RF-DETR: Neural Architecture Search for Real-Time Detection Transformers}, 
    author={Isaac Robinson and Peter Robicheaux and Matvei Popov and Deva Ramanan and Neehar Peri},
    year={2025},
    eprint={2511.09554},
    archivePrefix={arXiv},
    primaryClass={cs.CV},
    url={https://arxiv.org/abs/2511.09554}, 
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Contribute&lt;/h2&gt; 
&lt;p&gt;We welcome and appreciate all contributions! If you notice any issues or bugs, have questions, or would like to suggest new features, please &lt;a href="https://github.com/roboflow/rf-detr/issues/new"&gt;open an issue&lt;/a&gt; or pull request. By sharing your ideas and improvements, you help make RF-DETR better for everyone.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://youtube.com/roboflow"&gt; &lt;img src="https://media.roboflow.com/notebooks/template/icons/purple/youtube.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949634652" width="3%" /&gt; &lt;/a&gt; 
 &lt;img src="https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png" width="3%" /&gt; 
 &lt;a href="https://roboflow.com"&gt; &lt;img src="https://media.roboflow.com/notebooks/template/icons/purple/roboflow-app.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949746649" width="3%" /&gt; &lt;/a&gt; 
 &lt;img src="https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png" width="3%" /&gt; 
 &lt;a href="https://www.linkedin.com/company/roboflow-ai/"&gt; &lt;img src="https://media.roboflow.com/notebooks/template/icons/purple/linkedin.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949633691" width="3%" /&gt; &lt;/a&gt; 
 &lt;img src="https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png" width="3%" /&gt; 
 &lt;a href="https://docs.roboflow.com"&gt; &lt;img src="https://media.roboflow.com/notebooks/template/icons/purple/knowledge.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949634511" width="3%" /&gt; &lt;/a&gt; 
 &lt;img src="https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png" width="3%" /&gt; 
 &lt;a href="https://discuss.roboflow.com"&gt; &lt;img src="https://media.roboflow.com/notebooks/template/icons/purple/forum.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949633584" width="3%" /&gt; &lt;img src="https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png" width="3%" /&gt; &lt;/a&gt;
 &lt;a href="https://blog.roboflow.com"&gt; &lt;img src="https://media.roboflow.com/notebooks/template/icons/purple/blog.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949633605" width="3%" /&gt; &lt;/a&gt;  
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>fla-org/flash-linear-attention</title>
      <link>https://github.com/fla-org/flash-linear-attention</link>
      <description>&lt;p&gt;ğŸš€ Efficient implementations of state-of-the-art linear attention models&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h1&gt;ğŸ’¥ Flash Linear Attention&lt;/h1&gt; 
 &lt;p&gt;&lt;a href="https://huggingface.co/fla-hub"&gt;&lt;img src="https://img.shields.io/badge/-Models-gray.svg?logo=huggingface&amp;amp;style=flat-square" alt="hf_model" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/vDaJTmKNcS"&gt;&lt;img src="https://img.shields.io/badge/Discord-%235865F2.svg?&amp;amp;logo=discord&amp;amp;logoColor=white&amp;amp;style=flat-square" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;This repo aims at providing a collection of efficient Triton-based implementations for state-of-the-art linear attention models. &lt;strong&gt;All implementations are written purely in PyTorch and Triton, making them platform-agnostic.&lt;/strong&gt; Currently verified platforms include NVIDIA, AMD, and Intel. &lt;strong&gt;Any pull requests are welcome!&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img width="400" alt="image" src="https://github.com/fla-org/flash-linear-attention/assets/18402347/02ff2e26-1495-4088-b701-e72cd65ac6cf" /&gt; 
&lt;/div&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/#news"&gt;News&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/#models"&gt;Models&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/#installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/#usage"&gt;Usage&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/#token-mixing"&gt;Token Mixing&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/#fused-modules"&gt;Fused Modules&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/#generation"&gt;Generation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/#hybrid-models"&gt;Hybrid Models&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/#training"&gt;Training&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/#evaluation"&gt;Evaluation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/#benchmarks"&gt;Benchmarks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/#citation"&gt;Citation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/#star-history"&gt;Star History&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/#acknowledgements"&gt;Acknowledgements&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;$\texttt{[2025-10]}$:&lt;/strong&gt; ğŸŒ‘ Add Kimi Delta Attention implementation to &lt;code&gt;fla&lt;/code&gt; (&lt;a href="https://arxiv.org/abs/2510.26692"&gt;paper&lt;/a&gt;).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;$\texttt{[2025-09]}$:&lt;/strong&gt; ğŸŒ² Add DeltaFormer implementation to &lt;code&gt;fla&lt;/code&gt; (&lt;a href="https://arxiv.org/abs/2505.19488v1"&gt;paper&lt;/a&gt;).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;$\texttt{[2025-09]}$:&lt;/strong&gt; ğŸ» Thrilled to announce that &lt;a href="https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/fla/ops/gated_delta_rule"&gt;GDN&lt;/a&gt; has been integrated into Qwen3-Next. Check out their &lt;a href="https://qwen.ai/blog?id=4074cca80393150c248e508aa62983f9cb7d27cd&amp;amp;from=research.latest-advancements-list"&gt;blog post&lt;/a&gt; for more infos!&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;$\texttt{[2025-08]}$:&lt;/strong&gt; ğŸŒ² Add Log-Linear Attention implementation to &lt;code&gt;fla&lt;/code&gt; (&lt;a href="https://arxiv.org/abs/2506.04761"&gt;paper&lt;/a&gt;).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;$\texttt{[2025-08]}$:&lt;/strong&gt; ğŸ“ Add MoM implementation to &lt;code&gt;fla&lt;/code&gt; (&lt;a href="https://arxiv.org/abs/2502.13685"&gt;paper&lt;/a&gt;).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;$\texttt{[2025-07]}$:&lt;/strong&gt; ğŸ³ Add MLA implementation to &lt;code&gt;fla&lt;/code&gt; (&lt;a href="https://arxiv.org/abs/2405.04434"&gt;paper&lt;/a&gt;).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;$\texttt{[2025-07]}$:&lt;/strong&gt; ğŸ›£ï¸ Added PaTH Attention to fla (&lt;a href="https://arxiv.org/abs/2505.16381"&gt;paper&lt;/a&gt;).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;$\texttt{[2025-06]}$:&lt;/strong&gt; ğŸ‰ Added MesaNet to fla (&lt;a href="https://arxiv.org/abs/2506.05233"&gt;paper&lt;/a&gt;).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;$\texttt{[2025-06]}$:&lt;/strong&gt; ğŸ Add Comba implementation to &lt;code&gt;fla&lt;/code&gt; (&lt;a href="https://arxiv.org/abs/2506.02475"&gt;paper&lt;/a&gt;).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;$\texttt{[2025-05]}$:&lt;/strong&gt; ğŸ‰ Add Rodimus* implementation to &lt;code&gt;fla&lt;/code&gt; (&lt;a href="https://arxiv.org/abs/2410.06577"&gt;paper&lt;/a&gt;).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;$\texttt{[2025-04]}$:&lt;/strong&gt; ğŸ‰ Add DeltaProduct implementation to &lt;code&gt;fla&lt;/code&gt; (&lt;a href="https://arxiv.org/abs/2502.10297"&gt;paper&lt;/a&gt;).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;$\texttt{[2025-04]}$:&lt;/strong&gt; ğŸ‰ Add FoX implementation to &lt;code&gt;fla&lt;/code&gt; (&lt;a href="https://arxiv.org/abs/2503.02130"&gt;paper&lt;/a&gt;).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;$\texttt{[2025-03]}$:&lt;/strong&gt; &lt;del&gt;We have changed the default &lt;code&gt;initializer_range&lt;/code&gt; to the magic ğŸ³ 0.006&lt;/del&gt; The &lt;code&gt;initializer_range&lt;/code&gt; was rolled back to the default value of 0.02. For actual training, we recommend trying both.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;$\texttt{[2025-02]}$:&lt;/strong&gt; ğŸ³ Add NSA implementations to &lt;code&gt;fla&lt;/code&gt;. See kernels &lt;a href="https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/fla/ops/nsa"&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;$\texttt{[2025-01]}$:&lt;/strong&gt; ğŸ”¥ We are migrating to &lt;code&gt;torchtitan&lt;/code&gt;-based training framework. Check out the &lt;a href="https://github.com/fla-org/flame"&gt;flame&lt;/a&gt; repo for more details.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;$\texttt{[2025-01]}$:&lt;/strong&gt; ğŸ¦… Add RWKV7 implementations (both kernels and models) to &lt;code&gt;fla&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;$\texttt{[2024-12]}$:&lt;/strong&gt; Integrated &lt;code&gt;flash-bidirectional-attention&lt;/code&gt; to &lt;code&gt;fla-org&lt;/code&gt; (&lt;a href="https://github.com/fla-org/flash-bidirectional-linear-attention"&gt;repo&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;$\texttt{[2024-12]}$:&lt;/strong&gt; ğŸ‰ Add Gated DeltaNet implementation to &lt;code&gt;fla&lt;/code&gt; (&lt;a href="https://arxiv.org/abs/2412.06464"&gt;paper&lt;/a&gt;).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;$\texttt{[2024-12]}$:&lt;/strong&gt; ğŸš€ &lt;code&gt;fla&lt;/code&gt; now officially supports kernels with variable-length inputs.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;$\texttt{[2024-11]}$:&lt;/strong&gt; The inputs are now switched from head-first to seq-first format.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;$\texttt{[2024-11]}$:&lt;/strong&gt; ğŸ’¥ &lt;code&gt;fla&lt;/code&gt; now provides a flexible way for training hybrid models.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;$\texttt{[2024-10]}$:&lt;/strong&gt; ğŸ”¥ Announcing &lt;code&gt;flame&lt;/code&gt;, a minimal and scalable framework for training &lt;code&gt;fla&lt;/code&gt; models. Check out the details &lt;a href="https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/training/README.md"&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;$\texttt{[2024-09]}$:&lt;/strong&gt; &lt;code&gt;fla&lt;/code&gt; now includes a fused linear and cross-entropy layer, significantly reducing memory usage during training.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;$\texttt{[2024-09]}$:&lt;/strong&gt; ğŸ‰ Add GSA implementation to &lt;code&gt;fla&lt;/code&gt; (&lt;a href="https://arxiv.org/abs/2409.07146"&gt;paper&lt;/a&gt;).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;$\texttt{[2024-05]}$:&lt;/strong&gt; ğŸ‰ Add DeltaNet implementation to &lt;code&gt;fla&lt;/code&gt; (&lt;a href="https://arxiv.org/abs/2102.11174"&gt;paper&lt;/a&gt;).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;$\texttt{[2024-05]}$:&lt;/strong&gt; ğŸ’¥ &lt;code&gt;fla&lt;/code&gt; v0.1: a variety of subquadratic kernels/layers/models integrated (RetNet/GLA/Mamba/HGRN/HGRN2/RWKV6, etc., see &lt;a href="https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/#models"&gt;Models&lt;/a&gt;).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;$\texttt{[2023-12]}$:&lt;/strong&gt; ğŸ’¥ Launched &lt;code&gt;fla&lt;/code&gt;, offering a collection of implementations for state-of-the-art linear attention models.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Models&lt;/h2&gt; 
&lt;p&gt;Roughly sorted according to the timeline supported in &lt;code&gt;fla&lt;/code&gt;. The recommended training mode is &lt;code&gt;chunk&lt;/code&gt; when available.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Year&lt;/th&gt; 
   &lt;th align="left"&gt;Venue&lt;/th&gt; 
   &lt;th align="left"&gt;Model&lt;/th&gt; 
   &lt;th align="left"&gt;Paper&lt;/th&gt; 
   &lt;th align="left"&gt;Code&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;2023&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;RetNet&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2307.08621"&gt;Retentive network: a successor to transformer for large language models&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/microsoft/torchscale/tree/main"&gt;official&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/fla-org/flash-linear-attention/raw/main/fla/layers/multiscale_retention.py"&gt;fla&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;2024&lt;/td&gt; 
   &lt;td align="left"&gt;ICML&lt;/td&gt; 
   &lt;td align="left"&gt;GLA&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2312.06635"&gt;Gated Linear Attention Transformers with Hardware-Efficient Training&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/berlino/gated_linear_attention"&gt;official&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/fla-org/flash-linear-attention/raw/main/fla/layers/gla.py"&gt;fla&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;2024&lt;/td&gt; 
   &lt;td align="left"&gt;ICML&lt;/td&gt; 
   &lt;td align="left"&gt;Based&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2402.18668"&gt;Simple linear attention language models balance the recall-throughput tradeoff&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/HazyResearch/based"&gt;official&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/fla-org/flash-linear-attention/raw/main/fla/layers/based.py"&gt;fla&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;2024&lt;/td&gt; 
   &lt;td align="left"&gt;ACL&lt;/td&gt; 
   &lt;td align="left"&gt;Rebased&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2402.10644"&gt;Linear Transformers with Learnable Kernel Functions are Better In-Context Models&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/corl-team/rebased/"&gt;official&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/fla-org/flash-linear-attention/raw/main/fla/layers/rebased.py"&gt;fla&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;2024&lt;/td&gt; 
   &lt;td align="left"&gt;NeurIPS&lt;/td&gt; 
   &lt;td align="left"&gt;DeltaNet&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2406.06484"&gt;Parallelizing Linear Transformers with Delta Rule over Sequence Length&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/fla-org/flash-linear-attention/raw/main/fla/layers/delta_net.py"&gt;official&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/fla-org/flash-linear-attention/raw/main/fla/layers/delta_net.py"&gt;fla&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;2022&lt;/td&gt; 
   &lt;td align="left"&gt;ACL&lt;/td&gt; 
   &lt;td align="left"&gt;ABC&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2110.02488"&gt;ABC: Attention with Bounded-memory Control&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/fla-org/flash-linear-attention/raw/main/fla/layers/abc.py"&gt;fla&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;2023&lt;/td&gt; 
   &lt;td align="left"&gt;NeurIPS&lt;/td&gt; 
   &lt;td align="left"&gt;HGRN&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://openreview.net/forum?id=P1TCHxJwLB"&gt;Hierarchically Gated Recurrent Neural Network for Sequence Modeling&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/OpenNLPLab/HGRN"&gt;official&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/fla-org/flash-linear-attention/raw/main/fla/layers/hgrn.py"&gt;fla&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;2024&lt;/td&gt; 
   &lt;td align="left"&gt;COLM&lt;/td&gt; 
   &lt;td align="left"&gt;HGRN2&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2404.07904"&gt;HGRN2: Gated Linear RNNs with State Expansion&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/OpenNLPLab/HGRN2"&gt;official&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/fla-org/flash-linear-attention/raw/main/fla/layers/hgrn2.py"&gt;fla&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;2024&lt;/td&gt; 
   &lt;td align="left"&gt;COLM&lt;/td&gt; 
   &lt;td align="left"&gt;RWKV6&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2404.05892"&gt;Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/RWKV/RWKV-LM"&gt;official&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/fla-org/flash-linear-attention/raw/main/fla/layers/rwkv6.py"&gt;fla&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;2024&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;LightNet&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2405.21022"&gt;You Only Scan Once: Efficient Multi-dimension Sequential Modeling with LightNet&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/OpenNLPLab/LightNet"&gt;official&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/fla-org/flash-linear-attention/raw/main/fla/layers/lightnet.py"&gt;fla&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;2025&lt;/td&gt; 
   &lt;td align="left"&gt;ICLR&lt;/td&gt; 
   &lt;td align="left"&gt;Samba&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2406.07522"&gt;Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/microsoft/Samba"&gt;official&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/fla-org/flash-linear-attention/raw/main/fla/models/samba"&gt;fla&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;2024&lt;/td&gt; 
   &lt;td align="left"&gt;ICML&lt;/td&gt; 
   &lt;td align="left"&gt;Mamba2&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2405.21060"&gt;Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/state-spaces/mamba"&gt;official&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/fla-org/flash-linear-attention/raw/main/fla/models/mamba2"&gt;fla&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;2024&lt;/td&gt; 
   &lt;td align="left"&gt;NeurIPS&lt;/td&gt; 
   &lt;td align="left"&gt;GSA&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2409.07146"&gt;Gated Slot Attention for Efficient Linear-Time Sequence Modeling&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/fla-org/flash-linear-attention/tree/main/fla/models/gsa"&gt;official&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/fla-org/flash-linear-attention/tree/main/fla/models/gsa"&gt;fla&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;2025&lt;/td&gt; 
   &lt;td align="left"&gt;ICLR&lt;/td&gt; 
   &lt;td align="left"&gt;Gated DeltaNet&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2412.06464"&gt;Gated Delta Networks: Improving Mamba2 with Delta Rule&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/NVlabs/GatedDeltaNet"&gt;official&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/fla-org/flash-linear-attention/tree/main/fla/ops/gated_delta_rule"&gt;fla&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;2025&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;RWKV7&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2503.14456"&gt;RWKV-7 "Goose" with Expressive Dynamic State Evolution&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/BlinkDL/RWKV-LM/tree/main/RWKV-v7"&gt;official&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/fla-org/flash-linear-attention/tree/main/fla/ops/rwkv7"&gt;fla&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;2025&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;NSA&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2502.11089"&gt;Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/fla-org/flash-linear-attention/tree/main/fla/ops/nsa"&gt;fla&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;2025&lt;/td&gt; 
   &lt;td align="left"&gt;ICLR&lt;/td&gt; 
   &lt;td align="left"&gt;FoX&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2503.02130"&gt;Forgetting Transformer: Softmax Attention with a Forget Gate&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/zhixuan-lin/forgetting-transformer"&gt;official&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/fla-org/flash-linear-attention/tree/main/fla/ops/forgetting_attn"&gt;fla&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;2025&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;DeltaProduct&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2502.10297"&gt;DeltaProduct: Improving State-Tracking in Linear RNNs via Householder Products&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/fla-org/flash-linear-attention/tree/main/fla/layers/gated_deltaproduct.py"&gt;fla&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;2025&lt;/td&gt; 
   &lt;td align="left"&gt;ICLR&lt;/td&gt; 
   &lt;td align="left"&gt;Rodimus*&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2410.06577"&gt;Rodimus*: Breaking the Accuracy-Efficiency Trade-Off with Efficient Attentions&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/codefuse-ai/rodimus"&gt;official&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/fla-org/flash-linear-attention/raw/main/fla/layers/rodimus.py"&gt;fla&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;2025&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;MesaNet&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2506.05233"&gt;MesaNet: Sequence Modeling by Locally Optimal Test-Time Training&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/fla-org/flash-linear-attention/raw/main/fla/layers/mesa_net.py"&gt;fla&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;2025&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Comba&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2506.02475"&gt;Comba: Improving Bilinear RNNs with Closed-loop Control&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/AwesomeSeq/Comba-triton"&gt;official&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/fla-org/flash-linear-attention/raw/main/fla/layers/comba.py"&gt;fla&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;2025&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;PaTH&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2505.16381"&gt;PaTH Attention: Position Encoding via Accumulating Householder Transformations&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/fla-org/flash-linear-attention/raw/main/fla/layers/path_attn.py"&gt;fla&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;2025&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;MoM&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2502.13685"&gt;MoM: Linear Sequence Modeling with Mixture-of-Memories&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/OpenSparseLLMs/MoM"&gt;official&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/fla-org/flash-linear-attention/raw/main/fla/layers/mom.py"&gt;fla&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;2025&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Log-Linear Attention&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2506.04761"&gt;Log-Linear Attention&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/HanGuo97/log-linear-attention"&gt;official&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/fla-org/flash-linear-attention/tree/main/fla/ops/log_linear_attn"&gt;fla&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;2025&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;DeltaFormer&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2505.19488v1"&gt;Understanding Transformer from the Perspective of Associative Memory&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/fla-org/flash-linear-attention/raw/main/fla/layers/deltaformer.py"&gt;fla&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;2025&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;KDA&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2510.26692"&gt;Kimi Linear: An Expressive, Efficient Attention Architecture&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/fla-org/flash-linear-attention/tree/main/fla/ops/kda"&gt;fla&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/fla-org/flash-linear-attention/actions/workflows/nvidia-4090.yml"&gt;&lt;img src="https://github.com/fla-org/flash-linear-attention/actions/workflows/nvidia-4090.yml/badge.svg?branch=main&amp;amp;event=push" alt="nvidia-4090-ci" /&gt;&lt;/a&gt; &lt;a href="https://github.com/fla-org/flash-linear-attention/actions/workflows/nvidia-a100.yml"&gt;&lt;img src="https://github.com/fla-org/flash-linear-attention/actions/workflows/nvidia-a100.yml/badge.svg?branch=main" alt="nvidia-a100-ci" /&gt;&lt;/a&gt; &lt;a href="https://github.com/fla-org/flash-linear-attention/actions/workflows/nvidia-h100.yml"&gt;&lt;img src="https://github.com/fla-org/flash-linear-attention/actions/workflows/nvidia-h100.yml/badge.svg?branch=main&amp;amp;event=push" alt="nvidia-h100-ci" /&gt;&lt;/a&gt; &lt;a href="https://github.com/fla-org/flash-linear-attention/actions/workflows/intel-b580.yml"&gt;&lt;img src="https://github.com/fla-org/flash-linear-attention/actions/workflows/intel-b580.yml/badge.svg?event=push" alt="intel-b580-ci" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;The following requirements should be satisfied&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://pytorch.org/"&gt;PyTorch&lt;/a&gt; &amp;gt;= 2.5&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/openai/triton"&gt;Triton&lt;/a&gt; &amp;gt;=3.0 (or nightly version, see &lt;a href="https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/FAQs.md"&gt;FAQs&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://einops.rocks/"&gt;einops&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/huggingface/transformers"&gt;transformers&lt;/a&gt; &amp;gt;=4.45.0&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/huggingface/datasets"&gt;datasets&lt;/a&gt; &amp;gt;=3.3.0&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Starting from v0.3.2, the packages published on PyPI are &lt;code&gt;fla-core&lt;/code&gt; and &lt;code&gt;flash-linear-attention&lt;/code&gt;. The former contains all our customized kernels and only depends on PyTorch, Triton, and einops. The latter is an extension package of the former, containing &lt;code&gt;fla/layers&lt;/code&gt; and &lt;code&gt;fla/models&lt;/code&gt;, and depends on transformers. We also provide Triton implementations for conv1d operations, so causal-conv1d is not required.&lt;/p&gt; 
&lt;p&gt;You can install &lt;code&gt;fla&lt;/code&gt; with pip:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;pip install flash-linear-attention
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;As &lt;code&gt;fla&lt;/code&gt; is actively developed now, for the latest features and updates, an alternative way is to install the package from source. Note that installing from git uses the default mode, so you need to uninstall both &lt;code&gt;fla-core&lt;/code&gt; and &lt;code&gt;flash-linear-attention&lt;/code&gt; first:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# uninstall both packages first to ensure a successful upgrade
pip uninstall fla-core flash-linear-attention -y &amp;amp;&amp;amp; pip install -U git+https://github.com/fla-org/flash-linear-attention
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or manage &lt;code&gt;fla&lt;/code&gt; with submodules&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;git submodule add https://github.com/fla-org/flash-linear-attention.git 3rdparty/flash-linear-attention
ln -s 3rdparty/flash-linear-attention/fla fla
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you have installed &lt;code&gt;triton-nightly&lt;/code&gt; and &lt;code&gt;torch&lt;/code&gt; pre version, please use the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;pip install einops ninja datasets transformers numpy
# uninstall both packages first to ensure a successful upgrade
pip uninstall fla-core flash-linear-attention -y &amp;amp;&amp;amp; pip install -U --no-use-pep517 git+https://github.com/fla-org/flash-linear-attention --no-deps
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://deepwiki.com/fla-org/flash-linear-attention"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Token Mixing&lt;/h3&gt; 
&lt;p&gt;We provide ``token mixing'' linear attention layers in &lt;code&gt;fla.layers&lt;/code&gt; for you to use. You can replace the standard multihead attention layer in your model with other linear attention layers. Example usage is as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;&amp;gt;&amp;gt;&amp;gt; import torch
&amp;gt;&amp;gt;&amp;gt; from fla.layers import MultiScaleRetention
&amp;gt;&amp;gt;&amp;gt; batch_size, num_heads, seq_len, hidden_size = 32, 4, 2048, 1024
&amp;gt;&amp;gt;&amp;gt; device, dtype = 'cuda:0', torch.bfloat16
&amp;gt;&amp;gt;&amp;gt; retnet = MultiScaleRetention(hidden_size=hidden_size, num_heads=num_heads).to(device=device, dtype=dtype)
&amp;gt;&amp;gt;&amp;gt; retnet
MultiScaleRetention(
  (q_proj): Linear(in_features=1024, out_features=1024, bias=False)
  (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
  (v_proj): Linear(in_features=1024, out_features=2048, bias=False)
  (g_proj): Linear(in_features=1024, out_features=2048, bias=False)
  (o_proj): Linear(in_features=2048, out_features=1024, bias=False)
  (g_norm_swish_gate): FusedRMSNormGated(512, eps=1e-05, activation=swish)
  (rotary): RotaryEmbedding(dim=256, base=10000.0, interleaved=False, pos_idx_in_fp32=True)
)
&amp;gt;&amp;gt;&amp;gt; x = torch.randn(batch_size, seq_len, hidden_size).to(device=device, dtype=dtype)
&amp;gt;&amp;gt;&amp;gt; y, *_ = retnet(x)
&amp;gt;&amp;gt;&amp;gt; y.shape
torch.Size([32, 2048, 1024])
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;We provide the implementations of models that are compatible with ğŸ¤— Transformers library. Here's an example of how to initialize a GLA model from the default configs in &lt;code&gt;fla&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;&amp;gt;&amp;gt;&amp;gt; from fla.models import GLAConfig
&amp;gt;&amp;gt;&amp;gt; from transformers import AutoModelForCausalLM
&amp;gt;&amp;gt;&amp;gt; config = GLAConfig()
&amp;gt;&amp;gt;&amp;gt; config
GLAConfig {
  "attn": null,
  "attn_mode": "chunk",
  "bos_token_id": 1,
  "clamp_min": null,
  "conv_size": 4,
  "elementwise_affine": true,
  "eos_token_id": 2,
  "expand_k": 0.5,
  "expand_v": 1,
  "feature_map": null,
  "fuse_cross_entropy": true,
  "fuse_norm": true,
  "fuse_swiglu": true,
  "hidden_act": "swish",
  "hidden_ratio": 4,
  "hidden_size": 2048,
  "initializer_range": 0.006,
  "intermediate_size": null,
  "max_position_embeddings": 2048,
  "model_type": "gla",
  "norm_eps": 1e-06,
  "num_heads": 4,
  "num_hidden_layers": 24,
  "num_kv_heads": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.50.1",
  "use_cache": true,
  "use_gk": true,
  "use_gv": false,
  "use_output_gate": true,
  "use_short_conv": false,
  "vocab_size": 32000
}

&amp;gt;&amp;gt;&amp;gt; AutoModelForCausalLM.from_config(config)
GLAForCausalLM(
  (model): GLAModel(
    (embeddings): Embedding(32000, 2048)
    (layers): ModuleList(
      (0-23): 24 x GLABlock(
        (attn_norm): RMSNorm(2048, eps=1e-06)
        (attn): GatedLinearAttention(
          (q_proj): Linear(in_features=2048, out_features=1024, bias=False)
          (k_proj): Linear(in_features=2048, out_features=1024, bias=False)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (g_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (gk_proj): Sequential(
            (0): Linear(in_features=2048, out_features=16, bias=False)
            (1): Linear(in_features=16, out_features=1024, bias=True)
          )
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          (g_norm_swish_gate): FusedRMSNormGated(512, eps=1e-06, activation=swish)
        )
        (mlp_norm): RMSNorm(2048, eps=1e-06)
        (mlp): GatedMLP(
          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)
          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)
          (swiglu_linear): SwiGLULinear()
        )
      )
    )
    (norm): RMSNorm(2048, eps=1e-06)
  )
  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Fused Modules&lt;/h3&gt; 
&lt;p&gt;We offer a collection of fused modules in &lt;code&gt;fla.modules&lt;/code&gt; to facilitate faster training:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/fla/modules/rotary.py"&gt;&lt;code&gt;Rotary Embedding&lt;/code&gt;&lt;/a&gt;: rotary positional embeddings as adopted by the Llama architecture, a.k.a., Transformer++.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/fla/modules/layernorm.py"&gt;&lt;code&gt;Norm Layers&lt;/code&gt;&lt;/a&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;RMSNorm&lt;/code&gt;, &lt;code&gt;LayerNorm&lt;/code&gt; and &lt;code&gt;GroupNorm&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;RMSNormLinear&lt;/code&gt;, &lt;code&gt;LayerNormLinear&lt;/code&gt; and &lt;code&gt;GroupNormLinear&lt;/code&gt; to reduce memory usage of intermediate tensors for improved memory efficiency.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/fla/modules/fused_norm_gate.py"&gt;&lt;code&gt;Norm Layers with Gating&lt;/code&gt;&lt;/a&gt;: combine norm layers with element-wise sigmoid or swish gating, as used by RetNet/GLA.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/fla/modules/fused_cross_entropy.py"&gt;&lt;code&gt;Cross Entropy&lt;/code&gt;&lt;/a&gt;: faster Triton implementation of cross entropy loss.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/fla/modules/fused_linear_cross_entropy.py"&gt;&lt;code&gt;Linear Cross Entropy&lt;/code&gt;&lt;/a&gt;: fused linear layer and cross entropy loss to avoid the materialization of large logits tensors. Also refer to implementations by &lt;a href="https://github.com/mgmalek/efficient_cross_entropy"&gt;mgmalek&lt;/a&gt; and &lt;a href="https://github.com/linkedin/Liger-Kernel/raw/main/src/liger_kernel/ops/fused_linear_cross_entropy.py"&gt;Liger-Kernel&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/fla/modules/fused_kl_div.py"&gt;&lt;code&gt;Linear KL Divergence&lt;/code&gt;&lt;/a&gt;: fused linear layer and KL divergence loss in a similar vein as CE loss.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] You can control using &lt;code&gt;fuse_linear_cross_entropy&lt;/code&gt; in the model configuration to enable/disable the fused linear cross entropy loss.&lt;/p&gt; 
 &lt;p&gt;This fused implementation is more memory-efficient but may reduce numerical precision. Due to this trade-off, it is disabled by default. If you enable this feature and encounter training instability (e.g., loss divergence), we recommend disabling it to see if the issue is resolved.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Generation&lt;/h3&gt; 
&lt;p&gt;Upon successfully pretraining a model, it becomes accessible for generating text using the ğŸ¤— text generation APIs. In the following, we give a generation example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;&amp;gt;&amp;gt;&amp;gt; import fla
&amp;gt;&amp;gt;&amp;gt; from transformers import AutoModelForCausalLM, AutoTokenizer
&amp;gt;&amp;gt;&amp;gt; name = 'fla-hub/gla-1.3B-100B'
&amp;gt;&amp;gt;&amp;gt; tokenizer = AutoTokenizer.from_pretrained(name)
&amp;gt;&amp;gt;&amp;gt; model = AutoModelForCausalLM.from_pretrained(name).cuda()
&amp;gt;&amp;gt;&amp;gt; input_prompt = "Power goes with permanence. Impermanence is impotence. And rotation is castration."
&amp;gt;&amp;gt;&amp;gt; input_ids = tokenizer(input_prompt, return_tensors="pt").input_ids.cuda()
&amp;gt;&amp;gt;&amp;gt; outputs = model.generate(input_ids, max_length=64)
&amp;gt;&amp;gt;&amp;gt; tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;We also provide a simple script &lt;a href="https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/benchmarks/benchmark_generation.py"&gt;here&lt;/a&gt; for benchmarking the generation speed. Simply run it by:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;$ python -m benchmarks.benchmark_generation \
  --path 'fla-hub/gla-1.3B-100B' \
  --repetition_penalty 2. \
  --prompt="Hello everyone, I'm Songlin Yang"

Prompt:
Hello everyone, I'm Songlin Yang
Generated:
Hello everyone, I'm Songlin Yang.
I am a 20 year old girl from China who is currently studying in the United States of America for my Master degree and also working as an English teacher at school here on campus since last summer (1st semester). My main goal to be able do well with this course so that we can have

Prompt length: 10, generation length: 64
Total prompt processing + decoding time: 4593ms
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;All of the pretrained models currently available can be found in &lt;a href="https://huggingface.co/fla-hub"&gt;&lt;code&gt;fla-hub&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;&amp;gt;&amp;gt;&amp;gt; from huggingface_hub import list_models
&amp;gt;&amp;gt;&amp;gt; for model in list_models(author='fla-hub'): print(model.id)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Hybrid Models&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;fla&lt;/code&gt; provides a flexible method to incorporate standard attention layers into existing linear attention models. This is easily achieved by specifying the &lt;code&gt;attn&lt;/code&gt; argument in the model configuration.&lt;/p&gt; 
&lt;p&gt;For example, to create a 2-layer Samba model with interleaved Mamba and local attention layers, using a sliding window size of 2048:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;&amp;gt;&amp;gt;&amp;gt; from fla.models import SambaConfig
&amp;gt;&amp;gt;&amp;gt; from transformers import AutoModelForCausalLM
&amp;gt;&amp;gt;&amp;gt; config = SambaConfig(num_hidden_layers=2)
&amp;gt;&amp;gt;&amp;gt; config.attn = {
  'layers': [1],
  'num_heads': 18,
  'num_kv_heads': 18,
  'qkv_bias': False,
  'rope_theta': 10000.,
  'window_size': 2048
}
&amp;gt;&amp;gt;&amp;gt; config
SambaConfig {
  "attn": {
    "layers": [
      1
    ],
    "num_heads": 18,
    "num_kv_heads": 18,
    "qkv_bias": false,
    "rope_theta": 10000.0,
    "window_size": 2048
  },
  "bos_token_id": 1,
  "conv_kernel": 4,
  "eos_token_id": 2,
  "expand": 2,
  "fuse_cross_entropy": true,
  "fuse_norm": true,
  "fuse_swiglu": true,
  "hidden_act": "swish",
  "hidden_ratio": 4,
  "hidden_size": 2304,
  "initializer_range": 0.02,
  "intermediate_size": 4608,
  "max_position_embeddings": 2048,
  "model_type": "samba",
  "norm_eps": 1e-05,
  "num_hidden_layers": 2,
  "pad_token_id": 0,
  "rescale_prenorm_residual": false,
  "residual_in_fp32": false,
  "state_size": 16,
  "tie_word_embeddings": false,
  "time_step_floor": 0.0001,
  "time_step_init_scheme": "random",
  "time_step_max": 0.1,
  "time_step_min": 0.001,
  "time_step_rank": 144,
  "time_step_scale": 1.0,
  "transformers_version": "4.50.1",
  "use_bias": false,
  "use_cache": true,
  "use_conv_bias": true,
  "vocab_size": 32000
}

&amp;gt;&amp;gt;&amp;gt; AutoModelForCausalLM.from_config(config)
SambaForCausalLM(
  (backbone): SambaModel(
    (embeddings): Embedding(32000, 2304)
    (layers): ModuleList(
      (0): SambaBlock(
        (mixer_norm): RMSNorm(2304, eps=1e-05)
        (mixer): Mamba(
          (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)
          (in_proj): Linear(in_features=2304, out_features=9216, bias=False)
          (x_proj): Linear(in_features=4608, out_features=176, bias=False)
          (dt_proj): Linear(in_features=144, out_features=4608, bias=True)
          (out_proj): Linear(in_features=4608, out_features=2304, bias=False)
        )
        (mlp_norm): RMSNorm(2304, eps=1e-05)
        (mlp): GatedMLP(
          (gate_proj): Linear(in_features=2304, out_features=6144, bias=False)
          (up_proj): Linear(in_features=2304, out_features=6144, bias=False)
          (down_proj): Linear(in_features=6144, out_features=2304, bias=False)
          (swiglu_linear): SwiGLULinear()
        )
      )
      (1): SambaBlock(
        (mixer_norm): RMSNorm(2304, eps=1e-05)
        (mixer): Attention(
          (q_proj): Linear(in_features=2304, out_features=2304, bias=False)
          (k_proj): Linear(in_features=2304, out_features=2304, bias=False)
          (v_proj): Linear(in_features=2304, out_features=2304, bias=False)
          (o_proj): Linear(in_features=2304, out_features=2304, bias=False)
          (rotary): RotaryEmbedding(dim=128, base=10000.0, interleaved=False, pos_idx_in_fp32=True)
        )
        (mlp_norm): RMSNorm(2304, eps=1e-05)
        (mlp): GatedMLP(
          (gate_proj): Linear(in_features=2304, out_features=6144, bias=False)
          (up_proj): Linear(in_features=2304, out_features=6144, bias=False)
          (down_proj): Linear(in_features=6144, out_features=2304, bias=False)
          (swiglu_linear): SwiGLULinear()
        )
      )
    )
    (norm_f): RMSNorm(2304, eps=1e-05)
  )
  (lm_head): Linear(in_features=2304, out_features=32000, bias=False)
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;During inference, you &lt;strong&gt;DO NOT&lt;/strong&gt; need to revise anything for generation! The model will produce output as-is, without any need for additional configurations or modifications.&lt;/p&gt; 
&lt;h2&gt;Training&lt;/h2&gt; 
&lt;p&gt;We provide a minimal framework called &lt;a href="https://github.com/fla-org/flame"&gt;ğŸ”¥ &lt;code&gt;flame&lt;/code&gt;&lt;/a&gt; built on top of &lt;code&gt;torchtitan&lt;/code&gt;, for efficient training of &lt;code&gt;fla&lt;/code&gt; models.&lt;/p&gt; 
&lt;p&gt;Checkout &lt;a href="https://github.com/fla-org/flash-linear-attention/raw/main/examples/training.md"&gt;the GLA example&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;Evaluation&lt;/h2&gt; 
&lt;p&gt;The &lt;a href="https://github.com/EleutherAI/lm-evaluation-harness"&gt;lm-evaluation-harness&lt;/a&gt; library allows you to easily perform (zero-shot) model evaluations. Follow the steps below to use this library:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Install &lt;code&gt;lm_eval&lt;/code&gt; following &lt;a href="https://github.com/EleutherAI/lm-evaluation-harness/raw/main/README.md"&gt;their instructions&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run evaluation with:&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;$ MODEL='fla-hub/gla-1.3B-100B'
$ python -m evals.harness --model hf \
    --model_args pretrained=$MODEL,dtype=bfloat16 \
    --tasks wikitext,lambada_openai,piqa,hellaswag,winogrande,arc_easy,arc_challenge,boolq,sciq,copa,openbookqa \
    --batch_size 64 \
    --num_fewshot 0 \
    --device cuda \
    --show_config
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;We've made &lt;code&gt;fla&lt;/code&gt; compatible with hf-style evaluations, you can call &lt;a href="https://raw.githubusercontent.com/fla-org/flash-linear-attention/main/evals/harness.py"&gt;evals.harness&lt;/a&gt; to finish the evaluations. Running the command above will provide the task results reported in the GLA paper.&lt;/p&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Multi-GPU Evaluation with Hugging Face accelerate ğŸš€&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;To perform data-parallel evaluation (where each GPU loads a separate full copy of the model), we leverage the accelerate launcher as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;$ MODEL='fla-hub/gla-1.3B-100B'
$ accelerate launch -m evals.harness --model hf  \
    --model_args pretrained=$MODEL,dtype=bfloat16,trust_remote_code=True  \
    --tasks wikitext,lambada_openai,piqa,hellaswag,winogrande,arc_easy,arc_challenge,boolq,sciq,copa,openbookqa \
    --batch_size 64  \
    --num_fewshot 0  \
    --device cuda  \
    --show_config  \
    --trust_remote_code
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="4"&gt; 
 &lt;li&gt;ğŸ“ RULER Benchmark suite&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;The RULER benchmarks are commonly used for evaluating model performance on long-context tasks. You can evaluate &lt;code&gt;fla&lt;/code&gt; models on RULER directly using &lt;code&gt;lm-evaluation-harness&lt;/code&gt;. RULER is only available in a relatively recent version of &lt;code&gt;lm-evaluation-harness&lt;/code&gt;, so make sure you have the latest version installed.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;git clone --depth 1 https://github.com/EleutherAI/lm-evaluation-harness
cd lm-evaluation-harness
pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then, install the necessary dependencies for RULER:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;pip install lm_eval["ruler"]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;and run evaluation by (e.g., 32k contexts):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;$ accelerate launch -m evals.harness \
    --output_path $OUTPUT \
    --tasks niah_single_1,niah_single_2,niah_single_3,niah_multikey_1,niah_multikey_2,niah_multikey_3,niah_multiquery,niah_multivalue,ruler_vt,ruler_cwe,ruler_fwe,ruler_qa_hotpot,ruler_qa_squad \
    --model_args pretrained=$MODEL,dtype=bfloat16,max_length=32768,trust_remote_code=True \
    --metadata='{"max_seq_lengths":[4096,8192,16384,32768]}' \
    --batch_size 2 \
    --show_config  \
    --trust_remote_code
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If a GPU can't load a full copy of the model, please refer to &lt;a href="https://github.com/EleutherAI/lm-evaluation-harness?tab=readme-ov-file#multi-gpu-evaluation-with-hugging-face-accelerate"&gt;this link&lt;/a&gt; for FSDP settings.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Tip] If you are using &lt;code&gt;lm-evaluation-harness&lt;/code&gt; as an external library and can't find (almost) any tasks available, before calling &lt;code&gt;lm_eval.evaluate()&lt;/code&gt; or &lt;code&gt;lm_eval.simple_evaluate()&lt;/code&gt;, simply run the following to load the library's stock tasks!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;&amp;gt;&amp;gt;&amp;gt; from lm_eval.tasks import TaskManager; TaskManager().initialize_tasks()
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Benchmarks&lt;/h2&gt; 
&lt;p&gt;We compared our Triton-based RetNet implementation with CUDA-based FlashAttention2, using a batch size of 8, 32 heads, and a head dimension of 128, across different sequence lengths. These tests were conducted on a single H100 80GB GPU, as illustrated in the following graph&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;# you might have to first install `fla` to enable its import via `pip install -e .`
$ python benchmark_retention.py
Performance:
         T  chunk_fwd  parallel_fwd  flash_fwd  chunk_fwdbwd  parallel_fwdbwd  flash_fwdbwd
0    128.0   0.264032      0.243536   0.083488      1.301856         1.166784      0.320704
1    256.0   0.273472      0.252848   0.094304      1.345872         1.300608      0.807936
2    512.0   0.303600      0.278896   0.098112      1.503168         1.433184      0.857216
3   1024.0   0.357248      0.367360   0.156528      1.773552         2.303424      1.160864
4   2048.0   0.454624      0.605616   0.340928      2.283728         4.483360      1.955936
5   4096.0   0.638960      1.378016   1.004992      3.374720        12.271215      4.813776
6   8192.0   1.012352      4.201344   3.625008      5.581808        40.833618     15.023697
7  16384.0   1.748512     14.489664  13.710080     10.191552       153.093765     54.336864
&lt;/code&gt;&lt;/pre&gt; 
&lt;div align="center"&gt; 
 &lt;img width="500" alt="image" src="https://github.com/user-attachments/assets/c2607015-63af-43d1-90d1-ad5fe1670a03" /&gt; 
&lt;/div&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you find this repository helpful, please cite our work:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bib"&gt;@software{yang2024fla,
  title  = {FLA: A Triton-Based Library for Hardware-Efficient Implementations of Linear Attention Mechanism},
  author = {Yang, Songlin and Zhang, Yu},
  url    = {https://github.com/fla-org/flash-linear-attention},
  month  = jan,
  year   = {2024}
}

@misc{zhang2025kda,
    title         = {Kimi Linear: An Expressive, Efficient Attention Architecture},
    author        = {Zhang, Yu  and Lin, Zongyu  and Yao, Xingcheng  and Hu, Jiaxi  and Meng, Fanqing  and Liu, Chengyin  and Men, Xin  and Yang, Songlin  and Li, Zhiyuan  and Li, Wentao  and Lu, Enzhe  and Liu, Weizhou  and Chen, Yanru  and Xu, Weixin  and Yu, Longhui  and Wang, Yejie  and Fan, Yu  and Zhong, Longguang  and Yuan, Enming  and Zhang, Dehao  and Zhang, Yizhi  and T. Liu, Y.  and Wang, Haiming  and Fang, Shengjun  and He, Weiran  and Liu, Shaowei  and Li, Yiwei  and Su, Jianlin  and Qiu, Jiezhong  and Pang, Bo  and Yan, Junjie  and Jiang, Zhejun  and Huang, Weixiao  and Yin, Bohong  and You, Jiacheng  and Wei, Chu  and Wang, Zhengtao  and Hong, Chao  and Chen, Yutian  and Chen, Guanduo  and Wang, Yucheng  and Zheng, Huabin  and Wang, Feng  and Liu, Yibo  and Dong, Mengnan  and Zhang, Zheng  and Pan, Siyuan  and Wu, Wenhao  and Wu, Yuhao  and Guan, Longyu  and Tao, Jiawen  and Fu, Guohong  and Xu, Xinran  and Wang, Yuzhi  and Lai, Guokun  and Wu, Yuxin  and Zhou, Xinyu  and Yang, Zhilin  and Du, Yulun},
    year          = {2025},
    eprint        = {2510.26692},
    archivePrefix = {arXiv},
    primaryClass  = {cs.CL}
}

@inproceedings{yang2025path,
  title     = {PaTH Attention: Position Encoding via Accumulating Householder Transformations},
  author    = {Yang, Songlin  and Shen, Yikang and Wen, Kaiyue and Tan, Shawn  and Mishra, Mayank  and Ren, Liliang  and Panda, Rameswar  and Kim, Yoon},
  booktitle = {Proceedings of NeurIPS},
  year      = {2025}
}

@inproceedings{yang2024gdn,
  title     = {Gated Delta Networks: Improving Mamba2 with Delta Rule},
  author    = {Yang, Songlin  and Kautz, Jan  and Hatamizadeh, Ali},
  booktitle = {Proceedings of ICLR},
  year      = {2025}
}

@inproceedings{yang2024deltanet,
  title     = {Parallelizing Linear Transformers with the Delta Rule over Sequence Length},
  author    = {Yang, Songlin and Wang, Bailin and Zhang, Yu and Shen, Yikang and Kim, Yoon},
  booktitle = {Proceedings of NeurIPS},
  year      = {2024}
}

@inproceedings{zhang2024gsa,
  title     = {Gated Slot Attention for Efficient Linear-Time Sequence Modeling},
  author    = {Zhang, Yu and Yang, Songlin and Zhu, Ruijie and Zhang, Yue and Cui, Leyang and Wang, Yiqiao and Wang, Bolun and Shi, Freda and Wang, Bailin and Bi, Wei and Zhou, Peng and Fu, Guohong},
  booktitle = {Proceedings of NeurIPS},
  year      = {2024}
}

@inproceedings{qin2024hgrn2,
  title     = {HGRN2: Gated Linear RNNs with State Expansion},
  author    = {Qin, Zhen and Yang, Songlin and Sun, Weixuan and Shen, Xuyang and Li, Dong and Sun, Weigao and Zhong, Yiran},
  booktitle = {Proceedings of COLM},
  year      = {2024}
}

@inproceedings{yang2024gla,
  title     = {Gated Linear Attention Transformers with Hardware-Efficient Training},
  author    = {Yang, Songlin and Wang, Bailin and Shen, Yikang and Panda, Rameswar and Kim, Yoon},
  booktitle = {Proceedings of ICML},
  year      = {2024}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/fla-org/flash-linear-attention/stargazers"&gt;&lt;img src="https://bytecrank.com/nastyox/reporoster/php/stargazersSVG.php?user=fla-org&amp;amp;repo=flash-linear-attention" alt="Stargazers repo roster for @fla-org/flash-linear-attention" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#fla-org/flash-linear-attention&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=fla-org/flash-linear-attention&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;p&gt;We extend our gratitude to &lt;a href="https://www.bitdeer.com/"&gt;Bitdeer&lt;/a&gt; and &lt;a href="https://www.moonshot.ai/"&gt;Moonshot AI&lt;/a&gt; for their support in maintaining and powering our project infrastructure.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>GreyDGL/PentestGPT</title>
      <link>https://github.com/GreyDGL/PentestGPT</link>
      <description>&lt;p&gt;A GPT-empowered penetration testing tool&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a name="readme-top"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;!-- PROJECT SHIELDS --&gt; 
&lt;p&gt;&lt;a href="https://github.com/GreyDGL/PentestGPT/graphs/contributors"&gt;&lt;img src="https://img.shields.io/github/contributors/GreyDGL/PentestGPT.svg?style=for-the-badge" alt="Contributors" /&gt;&lt;/a&gt; &lt;a href="https://github.com/GreyDGL/PentestGPT/network/members"&gt;&lt;img src="https://img.shields.io/github/forks/GreyDGL/PentestGPT.svg?style=for-the-badge" alt="Forks" /&gt;&lt;/a&gt; &lt;a href="https://github.com/GreyDGL/PentestGPT/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/GreyDGL/PentestGPT.svg?style=for-the-badge" alt="Stargazers" /&gt;&lt;/a&gt; &lt;a href="https://github.com/GreyDGL/PentestGPT/issues"&gt;&lt;img src="https://img.shields.io/github/issues/GreyDGL/PentestGPT.svg?style=for-the-badge" alt="Issues" /&gt;&lt;/a&gt; &lt;a href="https://github.com/GreyDGL/PentestGPT/raw/master/LICENSE.md"&gt;&lt;img src="https://img.shields.io/github/license/GreyDGL/PentestGPT.svg?style=for-the-badge" alt="MIT License" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/eC34CEfEkK"&gt;&lt;img src="https://dcbadge.vercel.app/api/server/eC34CEfEkK" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;!-- PROJECT LOGO --&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt; 
 &lt;h3 align="center"&gt;PentestGPT&lt;/h3&gt; 
 &lt;p align="center"&gt; AI-Powered Autonomous Penetration Testing Agent &lt;br /&gt; &lt;strong&gt;Published at USENIX Security 2024&lt;/strong&gt; &lt;br /&gt; &lt;br /&gt; &lt;a href="https://www.usenix.org/conference/usenixsecurity24/presentation/deng"&gt;Research Paper&lt;/a&gt; Â· &lt;a href="https://github.com/GreyDGL/PentestGPT/issues"&gt;Report Bug&lt;/a&gt; Â· &lt;a href="https://github.com/GreyDGL/PentestGPT/issues"&gt;Request Feature&lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;!-- ABOUT THE PROJECT --&gt; 
&lt;p&gt;&lt;a href="https://trendshift.io/repositories/3770" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/3770" alt="GreyDGL%2FPentestGPT | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING] &lt;strong&gt;PentestGPT is a research prototype only&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;PentestGPT is a research prototype that pioneered the use of GenAI in cybersecurity. Please be aware of third-party services claiming to offer paid PentestGPT products - the original project is free and open-source.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Demo&lt;/h2&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://asciinema.org/a/761661"&gt;&lt;img src="https://asciinema.org/a/761661.svg?sanitize=true" alt="Installation Demo" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=RUNmoXqBwVg"&gt;Watch on YouTube&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;PentestGPT in Action&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://asciinema.org/a/761663"&gt;&lt;img src="https://asciinema.org/a/761663.svg?sanitize=true" alt="PentestGPT Demo" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=cWi3Yb7RmZA"&gt;Watch on YouTube&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;What's New in v1.0 (Agentic Upgrade)&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Autonomous Agent&lt;/strong&gt; - Agentic pipeline for intelligent, autonomous penetration testing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Session Persistence&lt;/strong&gt; - Save and resume penetration testing sessions&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Docker-First&lt;/strong&gt; - Isolated, reproducible environment with security tools pre-installed&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;In Progress&lt;/strong&gt;: Multi-model support for OpenAI, Gemini, and other LLM providers&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;AI-Powered Challenge Solver&lt;/strong&gt; - Leverages LLM advanced reasoning to perform penetration testing and CTFs&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Live Walkthrough&lt;/strong&gt; - Tracks steps in real-time as the agent works through challenges&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-Category Support&lt;/strong&gt; - Web, Crypto, Reversing, Forensics, PWN, Privilege Escalation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Real-Time Feedback&lt;/strong&gt; - Watch the AI work with live activity updates&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Extensible Architecture&lt;/strong&gt; - Clean, modular design ready for future enhancements&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Docker&lt;/strong&gt; (required) - &lt;a href="https://docs.docker.com/get-docker/"&gt;Install Docker&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;LLM Provider&lt;/strong&gt; (choose one): 
  &lt;ul&gt; 
   &lt;li&gt;Anthropic API Key from &lt;a href="https://console.anthropic.com/"&gt;console.anthropic.com&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Claude OAuth Login (requires Claude subscription)&lt;/li&gt; 
   &lt;li&gt;OpenRouter for alternative models at &lt;a href="https://openrouter.ai/keys"&gt;openrouter.ai&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://docs.google.com/document/d/1ixK7x-wlr5t5TYZJdfm75UME5KnPCpS46boLkUXKg1w/edit?usp=sharing"&gt;Tutorial: Using Local Models with Claude Code&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Clone and build
git clone --recurse-submodules https://github.com/GreyDGL/PentestGPT.git
cd PentestGPT
make install

# Configure authentication (first time only)
make config

# Connect to container
make connect
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: The &lt;code&gt;--recurse-submodules&lt;/code&gt; flag downloads the benchmark suite. If you already cloned without it, run: &lt;code&gt;git submodule update --init --recursive&lt;/code&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Try a Benchmark&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run pentestgpt-benchmark start XBEN-037-24 
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then connect into the container and run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pentestgpt --target http://host.docker.internal:8000
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Commands Reference&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Command&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;make install&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Build the Docker image&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;make config&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Configure API key (first-time setup)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;make connect&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Connect to container (main entry point)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;make stop&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Stop container (config persists)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;make clean-docker&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Remove everything including config&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Interactive TUI mode (default)
pentestgpt --target 10.10.11.234

# Non-interactive mode
pentestgpt --target 10.10.11.100 --non-interactive

# With challenge context
pentestgpt --target 10.10.11.50 --instruction "WordPress site, focus on plugin vulnerabilities"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Keyboard Shortcuts:&lt;/strong&gt; &lt;code&gt;F1&lt;/code&gt; Help | &lt;code&gt;Ctrl+P&lt;/code&gt; Pause/Resume | &lt;code&gt;Ctrl+Q&lt;/code&gt; Quit&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Using Local LLMs&lt;/h2&gt; 
&lt;p&gt;PentestGPT supports routing requests to local LLM servers (LM Studio, Ollama, text-generation-webui, etc.) running on your host machine.&lt;/p&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Local LLM server with an OpenAI-compatible API endpoint 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;LM Studio&lt;/strong&gt;: Enable server mode (default port 1234)&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Ollama&lt;/strong&gt;: Run &lt;code&gt;ollama serve&lt;/code&gt; (default port 11434)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Setup&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Configure PentestGPT for local LLM
make config
# Select option 4: Local LLM

# Start your local LLM server on the host machine
# Then connect to the container
make connect
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Customizing Models&lt;/h3&gt; 
&lt;p&gt;Edit &lt;code&gt;scripts/ccr-config-template.json&lt;/code&gt; to customize:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;localLLM.api_base_url&lt;/code&gt;&lt;/strong&gt;: Your LLM server URL (default: &lt;code&gt;host.docker.internal:1234&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;localLLM.models&lt;/code&gt;&lt;/strong&gt;: Available model names on your server&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Router section&lt;/strong&gt;: Which models handle which operations&lt;/li&gt; 
&lt;/ul&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Route&lt;/th&gt; 
   &lt;th&gt;Purpose&lt;/th&gt; 
   &lt;th&gt;Default Model&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;default&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;General tasks&lt;/td&gt; 
   &lt;td&gt;openai/gpt-oss-20b&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;background&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Background operations&lt;/td&gt; 
   &lt;td&gt;openai/gpt-oss-20b&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;think&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Reasoning-heavy tasks&lt;/td&gt; 
   &lt;td&gt;qwen/qwen3-coder-30b&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;longContext&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Large context handling&lt;/td&gt; 
   &lt;td&gt;qwen/qwen3-coder-30b&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;webSearch&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Web search operations&lt;/td&gt; 
   &lt;td&gt;openai/gpt-oss-20b&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Troubleshooting&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Connection refused&lt;/strong&gt;: Ensure your LLM server is running and listening on the configured port&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Docker networking&lt;/strong&gt;: Use &lt;code&gt;host.docker.internal&lt;/code&gt; (not &lt;code&gt;localhost&lt;/code&gt;) to access host services from Docker&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Check CCR logs&lt;/strong&gt;: Inside the container, run &lt;code&gt;cat /tmp/ccr.log&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Telemetry&lt;/h2&gt; 
&lt;p&gt;PentestGPT collects anonymous usage data to help improve the tool. This data is sent to our &lt;a href="https://langfuse.com"&gt;Langfuse&lt;/a&gt; project and includes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Session metadata (target type, duration, completion status)&lt;/li&gt; 
 &lt;li&gt;Tool execution patterns (which tools are used, not the actual commands)&lt;/li&gt; 
 &lt;li&gt;Flag detection events (that a flag was found, not the flag content)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;No sensitive data is collected&lt;/strong&gt; - command outputs, credentials, or actual flag values are never transmitted.&lt;/p&gt; 
&lt;h3&gt;Opting Out&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Via command line flag
pentestgpt --target 10.10.11.234 --no-telemetry

# Via environment variable
export LANGFUSE_ENABLED=false
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Benchmarks&lt;/h2&gt; 
&lt;p&gt;PentestGPT includes 100+ vulnerability challenges for testing and development.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pentestgpt-benchmark list                    # List all benchmarks
pentestgpt-benchmark list --levels 1         # Filter by difficulty
pentestgpt-benchmark list --tags sqli        # Filter by vulnerability type
pentestgpt-benchmark start XBEN-037-24       # Start a benchmark
pentestgpt-benchmark status                  # Check running benchmarks
pentestgpt-benchmark stop XBEN-037-24        # Stop a benchmark
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Available Tags:&lt;/strong&gt; &lt;code&gt;sqli&lt;/code&gt;, &lt;code&gt;xss&lt;/code&gt;, &lt;code&gt;idor&lt;/code&gt;, &lt;code&gt;ssti&lt;/code&gt;, &lt;code&gt;ssrf&lt;/code&gt;, &lt;code&gt;lfi&lt;/code&gt;, &lt;code&gt;rce&lt;/code&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Development&lt;/h2&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;uv&lt;/strong&gt; (required) - Python package manager: &lt;code&gt;curl -LsSf https://astral.sh/uv/install.sh | sh&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Claude Code CLI&lt;/strong&gt; - Configure with &lt;code&gt;claude login&lt;/code&gt; or &lt;code&gt;export ANTHROPIC_API_KEY='your-key'&lt;/code&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://docs.google.com/document/d/1ixK7x-wlr5t5TYZJdfm75UME5KnPCpS46boLkUXKg1w/edit?usp=sharing"&gt;Tutorial: Using Local Models with Claude Code&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Local Development&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv sync                                      # Install dependencies
uv run pentestgpt --target 10.10.11.234      # Run locally
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Project Commands&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;make test          # Run pytest
make lint          # Run ruff linter
make typecheck     # Run mypy
make ci            # Run full CI simulation (lint, format, typecheck, test, build)
make ci-quick      # Quick CI without build step
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Legacy Version&lt;/h2&gt; 
&lt;p&gt;The previous multi-LLM version (v0.15) supporting OpenAI, Gemini, Deepseek, and Ollama is archived in &lt;a href="https://raw.githubusercontent.com/GreyDGL/PentestGPT/main/legacy/"&gt;&lt;code&gt;legacy/&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd legacy &amp;amp;&amp;amp; pip install -e . &amp;amp;&amp;amp; pentestgpt --reasoning gpt-4o
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you use PentestGPT in your research, please cite our paper:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@inproceedings{299699,
  author = {Gelei Deng and Yi Liu and VÃ­ctor Mayoral-Vilches and Peng Liu and Yuekang Li and Yuan Xu and Tianwei Zhang and Yang Liu and Martin Pinzger and Stefan Rass},
  title = {{PentestGPT}: Evaluating and Harnessing Large Language Models for Automated Penetration Testing},
  booktitle = {33rd USENIX Security Symposium (USENIX Security 24)},
  year = {2024},
  isbn = {978-1-939133-44-1},
  address = {Philadelphia, PA},
  pages = {847--864},
  url = {https://www.usenix.org/conference/usenixsecurity24/presentation/deng},
  publisher = {USENIX Association},
  month = aug
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Distributed under the MIT License. See &lt;code&gt;LICENSE.md&lt;/code&gt; for more information.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Disclaimer&lt;/strong&gt;: This tool is for educational purposes and authorized security testing only. The authors do not condone any illegal use. Use at your own risk.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Contact&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Gelei Deng&lt;/strong&gt; - &lt;a href="https://www.linkedin.com/in/gelei-deng-225a10112/"&gt;&lt;img src="https://img.shields.io/badge/-LinkedIn-black.svg?style=for-the-badge&amp;amp;logo=linkedin&amp;amp;colorB=555" alt="LinkedIn" /&gt;&lt;/a&gt; - &lt;a href="mailto:gelei.deng@ntu.edu.sg"&gt;gelei.deng@ntu.edu.sg&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Yi Liu&lt;/strong&gt; - &lt;a href="mailto:yi009@e.ntu.edu.sg"&gt;yi009@e.ntu.edu.sg&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Yuekang Li&lt;/strong&gt; - &lt;a href="mailto:yuekang.li@unsw.edu.au"&gt;yuekang.li@unsw.edu.au&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;VÃ­ctor Mayoral Vilches&lt;/strong&gt; - &lt;a href="https://www.linkedin.com/in/vmayoral/"&gt;&lt;img src="https://img.shields.io/badge/-LinkedIn-black.svg?style=for-the-badge&amp;amp;logo=linkedin&amp;amp;colorB=555" alt="LinkedIn" /&gt;&lt;/a&gt; - &lt;a href="mailto:v.mayoralv@gmail.com"&gt;v.mayoralv@gmail.com&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Peng Liu&lt;/strong&gt; - &lt;a href="mailto:liu_peng@i2r.a-star.edu.sg"&gt;liu_peng@i2r.a-star.edu.sg&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Acknowledgments&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Research supported by &lt;a href="https://www.quantstamp.com/"&gt;Quantstamp&lt;/a&gt; and &lt;a href="https://www.ntu.edu.sg/"&gt;NTU Singapore&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="right"&gt;(&lt;a href="https://raw.githubusercontent.com/GreyDGL/PentestGPT/main/#readme-top"&gt;back to top&lt;/a&gt;)&lt;/p&gt; 
&lt;!-- MARKDOWN LINKS &amp; IMAGES --&gt;</description>
    </item>
    
    <item>
      <title>atlassian/atlassian-mcp-server</title>
      <link>https://github.com/atlassian/atlassian-mcp-server</link>
      <description>&lt;p&gt;Remote MCP Server that securely connects Jira and Confluence with your LLM, IDE, or agent platform of choice.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/atlassian/atlassian-mcp-server/main/images/atlassian_logo_brand_RGB.svg?sanitize=true" /&gt; &lt;/p&gt; 
&lt;h1&gt;Atlassian MCP Server&lt;/h1&gt; 
&lt;p&gt;The Model Context Protocol (MCP) is a new, standardized protocol designed by Anthropic to manage context between large language models (LLMs) and external systems. This repository offers an MCP Server for Atlassian.&lt;/p&gt; 
&lt;p&gt;The Remote MCP Server is a cloud-based bridge between your Atlassian Cloud site and compatible external tools. Once configured, it enables those tools to interact with Jira and Confluence data in real-time. This functionality is powered by secure &lt;strong&gt;OAuth 2.1 authorization&lt;/strong&gt;, which ensures all actions respect the userâ€™s existing access controls.&lt;/p&gt; 
&lt;p&gt;The Remote MCP Server helps bring Atlassian data into your existing workflows:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Summarize and search&lt;/strong&gt; Jira and Confluence content without switching tools.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Create and update&lt;/strong&gt; issues or pages based on natural language commands.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Bulk process tasks&lt;/strong&gt; like generating tickets from meeting notes or specs. Itâ€™s designed to support developers, content creators, and project managers working within IDEs or AI platforms.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Before you start&lt;/h2&gt; 
&lt;p&gt;Ensure your environment meets the necessary requirements to successfully set up the Remote MCP Server. This section outlines the technical prerequisites, access considerations, and security details.&lt;/p&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;p&gt;Before connecting to the Remote MCP Server, review the setup requirements for your environment:&lt;/p&gt; 
&lt;h4&gt;Cloud-based Setup&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;An Atlassian Cloud site with Jira and/or Confluence&lt;/li&gt; 
 &lt;li&gt;Access to an AI Client (Claude for Teams for example)&lt;/li&gt; 
 &lt;li&gt;A modern browser to complete the OAuth 2.0 authorization flow&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Desktop Setup for Local Clients&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;An Atlassian Cloud site with Jira and/or Confluence&lt;/li&gt; 
 &lt;li&gt;A supported IDE (for example, Claude desktop, VS Code, or Cursor) or a custom MCP-compatible client&lt;/li&gt; 
 &lt;li&gt;Node.js v18+ installed to run the local MCP proxy (mcp-remote). Download from &lt;a href="https://nodejs.org/en"&gt;nodejs.org&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;A modern browser for completing the OAuth login&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Rate limits&lt;/h3&gt; 
&lt;p&gt;Usage of the MCP is subject to rate limits:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Standard plan&lt;/strong&gt;: Moderate usage thresholds.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Premium/Enterprise plans&lt;/strong&gt;: Higher usage quotas (1,000 requests/hour plus per-user limits).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Data and security&lt;/h3&gt; 
&lt;p&gt;Security is a core focus of the Remote MCP Server:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;All traffic is encrypted via HTTPS using TLS 1.2 or later.&lt;/li&gt; 
 &lt;li&gt;OAuth 2.0 ensures secure authentication and access control.&lt;/li&gt; 
 &lt;li&gt;Data access respects Jira and Confluence user permissions.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;How It Works&lt;/h2&gt; 
&lt;h3&gt;Architecture and Communication&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;A supported client connects to the server endpoint:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code&gt;https://mcp.atlassian.com/v1/sse
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;A secure browser-based OAuth 2.0 flow is triggered.&lt;/li&gt; 
 &lt;li&gt;Once authorized, the client streams contextual data and receives real-time responses from Jira or Confluence.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Permission Management&lt;/h3&gt; 
&lt;p&gt;Access is granted only to data that the user already has permission to view in Atlassian Cloud. All actions respect existing project or space-level roles. OAuth tokens are scoped and session-based. Once connected, you can perform a variety of useful tasks from within your supported client.&lt;/p&gt; 
&lt;h3&gt;Example Workflows&lt;/h3&gt; 
&lt;h4&gt;Jira workflows&lt;/h4&gt; 
&lt;p&gt;Use these examples to understand how to interact with Jira:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Search&lt;/strong&gt;: â€œFind all open bugs in Project Alpha.â€&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Create/update&lt;/strong&gt;: â€œCreate a story titled â€˜Redesign onboardingâ€™.â€&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Bulk create&lt;/strong&gt;: â€œMake five Jira issues from these notes.â€&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Confluence workflows&lt;/h4&gt; 
&lt;p&gt;Access and manage documentation content directly:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Summarize&lt;/strong&gt;: â€œSummarize the Q2 planning page.â€&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Create&lt;/strong&gt;: â€œCreate a page titled â€˜Team Goals Q3â€™.â€&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Navigate&lt;/strong&gt;: â€œWhat spaces do I have access to?â€&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Combined Tasks&lt;/h4&gt; 
&lt;p&gt;Integrate actions across Jira and Confluence:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Link content&lt;/strong&gt;: â€œLink these three Jira tickets to the â€˜Release Planâ€™ page.â€&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Note] Actual capabilities vary depending on your permission level and client platform.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Managing access&lt;/h3&gt; 
&lt;p&gt;If you're an admin preparing your team to use the Remote MCP Server, keep the following considerations in mind:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Ensure users have product access to Jira and/or Confluence via Atlassian Admin.&lt;/li&gt; 
 &lt;li&gt;Authorization tokens are tied to the userâ€™s current product permissionsâ€”check these if data isnâ€™t accessible.&lt;/li&gt; 
 &lt;li&gt;App authorizations can be revoked by end users through their profile settings or by admins in the &lt;a href="https://support.atlassian.com/security-and-access-policies/docs/manage-your-users-third-party-apps/"&gt;Connect apps section of Atlassian Admin&lt;/a&gt; for site-level control.&lt;/li&gt; 
 &lt;li&gt;Consider establishing usage guidelines or policies for teams leveraging AI-driven content generation.&lt;/li&gt; 
 &lt;li&gt;Reach out to your Atlassian account representative for advice on OAuth scope control and long-term support planning.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Setting up Atlassian MCP Server&lt;/h2&gt; 
&lt;h3&gt;Cloud-based Clients&lt;/h3&gt; 
&lt;p&gt;Depending on the tool you're using, the setup process may vary. We recommend you navigate to the exact instructions for connecting to an MCP client through the tool's documentation. Here is an example for &lt;a href="https://support.atlassian.com/rovo/docs/setting-up-claude-ai/"&gt;setting up Claude.ai&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Desktop/Local Clients&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] If youâ€™re using VSCode, you can also install it directly by browsing their &lt;a href="https://code.visualstudio.com/mcp"&gt;curated list of MCP servers&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ol&gt; 
 &lt;li&gt;Open your terminal&lt;/li&gt; 
 &lt;li&gt;Run the following command to start the proxy and begin authentication:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npx -y mcp-remote https://mcp.atlassian.com/v1/sse
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] If this command doesn't work due to a version-related issue, try specifying an older version of mcp-remote. The example below uses version 0.1.13, but you may use another version if needed:&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npx -y mcp-remote@0.1.13 https://mcp.atlassian.com/v1/sse
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;A browser window will open. Log in using your Atlassian credentials and approve the required permissions.&lt;/li&gt; 
 &lt;li&gt;Once authorized, return to your IDE and configure the MCP server settings by adding the following atlassian entry to the server configuration file (e.g. &lt;code&gt;mcp.json&lt;/code&gt;, &lt;code&gt;mcp_config.json&lt;/code&gt;):&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;"mcp.servers": {
  "atlassian": {
    "command": "npx",
    "args": ["-y", "mcp-remote", "https://mcp.atlassian.com/v1/sse"]
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="4"&gt; 
 &lt;li&gt;Save and reload your client's MCP extension or plugin.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Support and feedback&lt;/h2&gt; 
&lt;p&gt;Your feedback plays a crucial role in shaping the Remote MCP Server. If you encounter bugs, limitations, or have suggestions:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Visit the &lt;a href="https://support.atlassian.com/"&gt;Atlassian Support Portal&lt;/a&gt; to report issues.&lt;/li&gt; 
 &lt;li&gt;Share your experiences and feature requests on the &lt;a href="https://community.atlassian.com/"&gt;Atlassian Community&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Enterprise customers can contact their Atlassian Customer Success Manager for advanced support and roadmap discussions.&lt;/li&gt; 
 &lt;li&gt;Weâ€™re excited to collaborate with you to improve this capability before its general availability.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Guides&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.atlassian.com/blog/announcements/remote-mcp-server"&gt;Introducing Atlassian's MCP server&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://support.atlassian.com/rovo/docs/setting-up-claude-ai/"&gt;Setting up Claude.ai&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://support.atlassian.com/rovo/docs/setting-up-ides/"&gt;Setting up IDEs (like VS Code or Cursor)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://support.atlassian.com/rovo/docs/authentication-and-authorization/"&gt;Understanding Authentication &amp;amp; OAuth&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://support.atlassian.com/rovo/docs/troubleshooting-and-verifying-your-setup/"&gt;Troubleshooting and verifying your setup&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>scipy/scipy</title>
      <link>https://github.com/scipy/scipy</link>
      <description>&lt;p&gt;SciPy library main repository&lt;/p&gt;&lt;hr&gt;&lt;p&gt;.. image:: &lt;a href="https://raw.githubusercontent.com/scipy/scipy/main/doc/source/_static/logo.svg"&gt;https://raw.githubusercontent.com/scipy/scipy/main/doc/source/_static/logo.svg&lt;/a&gt; :target: &lt;a href="https://scipy.org"&gt;https://scipy.org&lt;/a&gt; :width: 110 :height: 110 :align: left&lt;/p&gt; 
&lt;p&gt;.. image:: &lt;a href="https://img.shields.io/badge/powered%20by-NumFOCUS-orange.svg?style=flat&amp;amp;colorA=E1523D&amp;amp;colorB=007D8A"&gt;https://img.shields.io/badge/powered%20by-NumFOCUS-orange.svg?style=flat&amp;amp;colorA=E1523D&amp;amp;colorB=007D8A&lt;/a&gt; :target: &lt;a href="https://numfocus.org"&gt;https://numfocus.org&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;.. image:: &lt;a href="https://img.shields.io/pypi/dm/scipy.svg?label=Pypi%20downloads"&gt;https://img.shields.io/pypi/dm/scipy.svg?label=Pypi%20downloads&lt;/a&gt; :target: &lt;a href="https://pypi.org/project/scipy/"&gt;https://pypi.org/project/scipy/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;.. image:: &lt;a href="https://img.shields.io/conda/dn/conda-forge/scipy.svg?label=Conda%20downloads"&gt;https://img.shields.io/conda/dn/conda-forge/scipy.svg?label=Conda%20downloads&lt;/a&gt; :target: &lt;a href="https://anaconda.org/conda-forge/scipy"&gt;https://anaconda.org/conda-forge/scipy&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;.. image:: &lt;a href="https://img.shields.io/badge/stackoverflow-Ask%20questions-blue.svg"&gt;https://img.shields.io/badge/stackoverflow-Ask%20questions-blue.svg&lt;/a&gt; :target: &lt;a href="https://stackoverflow.com/questions/tagged/scipy"&gt;https://stackoverflow.com/questions/tagged/scipy&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;.. image:: &lt;a href="https://img.shields.io/badge/DOI-10.1038%2Fs41592--019--0686--2-blue.svg"&gt;https://img.shields.io/badge/DOI-10.1038%2Fs41592--019--0686--2-blue.svg&lt;/a&gt; :target: &lt;a href="https://www.nature.com/articles/s41592-019-0686-2"&gt;https://www.nature.com/articles/s41592-019-0686-2&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;.. image:: &lt;a href="https://insights.linuxfoundation.org/api/badge/health-score?project=scipy"&gt;https://insights.linuxfoundation.org/api/badge/health-score?project=scipy&lt;/a&gt; :target: &lt;a href="https://insights.linuxfoundation.org/project/scipy"&gt;https://insights.linuxfoundation.org/project/scipy&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;SciPy (pronounced "Sigh Pie") is an open-source software for mathematics, science, and engineering. It includes modules for statistics, optimization, integration, linear algebra, Fourier transforms, signal and image processing, ODE solvers, and more.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Website:&lt;/strong&gt; &lt;a href="https://scipy.org"&gt;https://scipy.org&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Documentation:&lt;/strong&gt; &lt;a href="https://docs.scipy.org/doc/scipy/"&gt;https://docs.scipy.org/doc/scipy/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Development version of the documentation:&lt;/strong&gt; &lt;a href="https://scipy.github.io/devdocs"&gt;https://scipy.github.io/devdocs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;SciPy development forum:&lt;/strong&gt; &lt;a href="https://discuss.scientific-python.org/c/contributor/scipy"&gt;https://discuss.scientific-python.org/c/contributor/scipy&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Stack Overflow:&lt;/strong&gt; &lt;a href="https://stackoverflow.com/questions/tagged/scipy"&gt;https://stackoverflow.com/questions/tagged/scipy&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Source code:&lt;/strong&gt; &lt;a href="https://github.com/scipy/scipy"&gt;https://github.com/scipy/scipy&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Contributing:&lt;/strong&gt; &lt;a href="https://scipy.github.io/devdocs/dev/index.html"&gt;https://scipy.github.io/devdocs/dev/index.html&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Bug reports:&lt;/strong&gt; &lt;a href="https://github.com/scipy/scipy/issues"&gt;https://github.com/scipy/scipy/issues&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Code of Conduct:&lt;/strong&gt; &lt;a href="https://docs.scipy.org/doc/scipy/dev/conduct/code_of_conduct.html"&gt;https://docs.scipy.org/doc/scipy/dev/conduct/code_of_conduct.html&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Report a security vulnerability:&lt;/strong&gt; &lt;a href="https://tidelift.com/docs/security"&gt;https://tidelift.com/docs/security&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Citing in your work:&lt;/strong&gt; &lt;a href="https://www.scipy.org/citing-scipy/"&gt;https://www.scipy.org/citing-scipy/&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;SciPy is built to work with NumPy arrays, and provides many user-friendly and efficient numerical routines, such as routines for numerical integration and optimization. Together, they run on all popular operating systems, are quick to install, and are free of charge. NumPy and SciPy are easy to use, but powerful enough to be depended upon by some of the world's leading scientists and engineers. If you need to manipulate numbers on a computer and display or publish the results, give SciPy a try!&lt;/p&gt; 
&lt;p&gt;For the installation instructions, see &lt;code&gt;our install guide &amp;lt;https://scipy.org/install/&amp;gt;&lt;/code&gt;__.&lt;/p&gt; 
&lt;h2&gt;Call for Contributions&lt;/h2&gt; 
&lt;p&gt;We appreciate and welcome contributions. Small improvements or fixes are always appreciated; issues labeled as "good first issue" may be a good starting point. Have a look at &lt;code&gt;our contributing guide &amp;lt;https://scipy.github.io/devdocs/dev/index.html&amp;gt;&lt;/code&gt;__.&lt;/p&gt; 
&lt;p&gt;Writing code isnâ€™t the only way to contribute to SciPy. You can also:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;review pull requests&lt;/li&gt; 
 &lt;li&gt;triage issues&lt;/li&gt; 
 &lt;li&gt;develop tutorials, presentations, and other educational materials&lt;/li&gt; 
 &lt;li&gt;maintain and improve &lt;code&gt;our website &amp;lt;https://github.com/scipy/scipy.org&amp;gt;&lt;/code&gt;__&lt;/li&gt; 
 &lt;li&gt;develop graphic design for our brand assets and promotional materials&lt;/li&gt; 
 &lt;li&gt;help with outreach and onboard new contributors&lt;/li&gt; 
 &lt;li&gt;write grant proposals and help with other fundraising efforts&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If youâ€™re unsure where to start or how your skills fit in, reach out! You can ask on the &lt;code&gt;forum &amp;lt;https://discuss.scientific-python.org/c/contributor/scipy&amp;gt;&lt;/code&gt;__ or here, on GitHub, by leaving a comment on a relevant issue that is already open.&lt;/p&gt; 
&lt;p&gt;If you are new to contributing to open source, &lt;code&gt;this guide &amp;lt;https://opensource.guide/how-to-contribute/&amp;gt;&lt;/code&gt;__ helps explain why, what, and how to get involved.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>NVIDIA/NeMo-Agent-Toolkit</title>
      <link>https://github.com/NVIDIA/NeMo-Agent-Toolkit</link>
      <description>&lt;p&gt;The NVIDIA NeMo Agent toolkit is an open-source library for efficiently connecting and optimizing teams of AI agents.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/NVIDIA/NeMo-Agent-Toolkit/develop/docs/source/_static/banner.png" alt="NVIDIA NeMo Agent Toolkit" title="NeMo Agent Toolkit banner image" /&gt;&lt;/p&gt; 
&lt;h1&gt;NVIDIA NeMo Agent Toolkit&lt;/h1&gt; 
&lt;!-- vale off (due to hyperlinks) --&gt; 
&lt;p&gt;&lt;a href="https://opensource.org/licenses/Apache-2.0"&gt;&lt;img src="https://img.shields.io/badge/License-Apache%202.0-green.svg?sanitize=true" alt="License: Apache 2.0" /&gt;&lt;/a&gt; &lt;a href="https://github.com/NVIDIA/NeMo-Agent-Toolkit/releases"&gt;&lt;img src="https://img.shields.io/github/v/release/NVIDIA/NeMo-Agent-Toolkit" alt="GitHub Release" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/nvidia-nat/"&gt;&lt;img src="https://img.shields.io/pypi/v/nvidia-nat" alt="PyPI version" /&gt;&lt;/a&gt; &lt;a href="https://github.com/NVIDIA/NeMo-Agent-Toolkit/issues"&gt;&lt;img src="https://img.shields.io/github/issues/NVIDIA/NeMo-Agent-Toolkit" alt="GitHub issues" /&gt;&lt;/a&gt; &lt;a href="https://github.com/NVIDIA/NeMo-Agent-Toolkit/pulls"&gt;&lt;img src="https://img.shields.io/github/issues-pr/NVIDIA/NeMo-Agent-Toolkit" alt="GitHub pull requests" /&gt;&lt;/a&gt; &lt;a href="https://github.com/NVIDIA/NeMo-Agent-Toolkit"&gt;&lt;img src="https://img.shields.io/github/stars/NVIDIA/NeMo-Agent-Toolkit" alt="GitHub Repo stars" /&gt;&lt;/a&gt; &lt;a href="https://github.com/NVIDIA/NeMo-Agent-Toolkit/network/members"&gt;&lt;img src="https://img.shields.io/github/forks/NVIDIA/NeMo-Agent-Toolkit" alt="GitHub forks" /&gt;&lt;/a&gt; &lt;a href="https://colab.research.google.com/github/NVIDIA/NeMo-Agent-Toolkit/"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open in Colab" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;!-- vale on --&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;em&gt;NVIDIA NeMo Agent Toolkit is a flexible, lightweight, and unifying library that allows you to easily connect existing enterprise agents to data sources and tools across any framework.&lt;/em&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] NeMo Agent Toolkit was previously known as the Agent Intelligence (AIQ) toolkit, and 
  &lt;!-- vale off --&gt;AgentIQ
  &lt;!-- vale on --&gt;. The library was renamed to better reflect the purpose of the toolkit and to align with the NVIDIA NeMo family of products. The core technologies, performance, and roadmap remain unchanged and the API is fully compatible with previous releases. Please refer to the &lt;a href="https://raw.githubusercontent.com/NVIDIA/NeMo-Agent-Toolkit/develop/docs/source/resources/migration-guide.md"&gt;Migration Guide&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;ğŸ”¥ New Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/NeMo-Agent-Toolkit/develop/docs/source/improve-workflows/optimizer.md"&gt;&lt;strong&gt;Automatic Hyperparameter Tuning:&lt;/strong&gt;&lt;/a&gt; Automatically tune the parameters and prompts of your agents, tools, and workflows to maximize performance, minimize cost, and increase accuracy.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/NeMo-Agent-Toolkit/develop/docs/source/components/integrations/frameworks.md#adk-google-agent-development-kit"&gt;&lt;strong&gt;Google ADK Support:&lt;/strong&gt;&lt;/a&gt; Users of Google's Agent Development Kit (ADK) framework are now supported in NeMo Agent Toolkit.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/NeMo-Agent-Toolkit/develop/docs/source/components/auth/mcp-auth/index.md"&gt;&lt;strong&gt;MCP Authorization:&lt;/strong&gt;&lt;/a&gt; NeMo Agent Toolkit now supports MCP authorization. This allows you to use NeMo Agent Toolkit with MCP authorization when using the streamable HTTP protocol.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/NeMo-Agent-Toolkit/develop/docs/source/build-workflows/functions-and-function-groups/function-groups.md"&gt;&lt;strong&gt;Function Groups:&lt;/strong&gt;&lt;/a&gt; NeMo Agent Toolkit now supports Function Groups, allowing you to package multiple related functions together to share configuration, context, and resources.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/NeMo-Agent-Toolkit/develop/docs/source/components/integrations/frameworks.md#strands"&gt;&lt;strong&gt;Amazon Bedrock AgentCore and Strands Agents Support:&lt;/strong&gt;&lt;/a&gt; NeMo Agent Toolkit now supports building agents using Strands Agents framework and deploying them securely on Amazon Bedrock AgentCore runtime.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;âœ¨ Key Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;ğŸ§© &lt;a href="https://raw.githubusercontent.com/NVIDIA/NeMo-Agent-Toolkit/develop/docs/source/components/integrations/frameworks.md"&gt;&lt;strong&gt;Framework Agnostic:&lt;/strong&gt;&lt;/a&gt; NeMo Agent Toolkit works side-by-side and around existing agentic frameworks, such as &lt;a href="https://www.langchain.com/"&gt;LangChain&lt;/a&gt;, &lt;a href="https://www.llamaindex.ai/"&gt;LlamaIndex&lt;/a&gt;, &lt;a href="https://www.crewai.com/"&gt;CrewAI&lt;/a&gt;, &lt;a href="https://learn.microsoft.com/en-us/semantic-kernel/"&gt;Microsoft Semantic Kernel&lt;/a&gt;, and &lt;a href="https://google.github.io/adk-docs/"&gt;Google ADK&lt;/a&gt;, as well as custom enterprise agentic frameworks and simple Python agents. This allows you to use your current technology stack without replatforming. NeMo Agent Toolkit complements any existing agentic framework or memory tool you're using and isn't tied to any specific agentic framework, LLM provider, or data source.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;ğŸ” &lt;a href="https://raw.githubusercontent.com/NVIDIA/NeMo-Agent-Toolkit/develop/docs/source/components/sharing-components.md"&gt;&lt;strong&gt;Reusability:&lt;/strong&gt;&lt;/a&gt; Every agent, tool, and agentic workflow in this library exists as a function call that works together in complex software applications. The composability between these agents, tools, and workflows allows you to build once and reuse in different scenarios.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;âš¡ &lt;a href="https://raw.githubusercontent.com/NVIDIA/NeMo-Agent-Toolkit/develop/docs/source/get-started/tutorials/customize-a-workflow.md"&gt;&lt;strong&gt;Rapid Development:&lt;/strong&gt;&lt;/a&gt; Start with a pre-built agent, tool, or workflow, and customize it to your needs. This allows you and your development teams to move quickly if you're already developing with agents.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;ğŸ“ˆ &lt;a href="https://raw.githubusercontent.com/NVIDIA/NeMo-Agent-Toolkit/develop/docs/source/improve-workflows/profiler.md"&gt;&lt;strong&gt;Profiling:&lt;/strong&gt;&lt;/a&gt; Use the profiler to profile entire workflows down to the tool and agent level, track input/output tokens and timings, and identify bottlenecks. While we encourage you to wrap (decorate) every tool and agent to get the most out of the profiler, you have the freedom to integrate your tools, agents, and workflows to whatever level you want. You start small and go to where you believe you'll see the most value and expand from there.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;ğŸ” &lt;a href="https://raw.githubusercontent.com/NVIDIA/NeMo-Agent-Toolkit/develop/docs/source/run-workflows/observe/observe.md"&gt;&lt;strong&gt;Observability:&lt;/strong&gt;&lt;/a&gt; Monitor and debug your workflows with dedicated integrations for popular observability platforms such as Phoenix, Weave, and Langfuse, plus compatibility with OpenTelemetry-based observability platforms. Track performance, trace execution flows, and gain insights into your agent behaviors.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;ğŸ§ª &lt;a href="https://raw.githubusercontent.com/NVIDIA/NeMo-Agent-Toolkit/develop/docs/source/improve-workflows/evaluate.md"&gt;&lt;strong&gt;Evaluation System:&lt;/strong&gt;&lt;/a&gt; Validate and maintain accuracy of agentic workflows with built-in evaluation tools.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;ğŸ’¬ &lt;a href="https://raw.githubusercontent.com/NVIDIA/NeMo-Agent-Toolkit/develop/docs/source/run-workflows/launching-ui.md"&gt;&lt;strong&gt;User Interface:&lt;/strong&gt;&lt;/a&gt; Use the NeMo Agent Toolkit UI chat interface to interact with your agents, visualize output, and debug workflows.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;ğŸ”— &lt;a href="https://raw.githubusercontent.com/NVIDIA/NeMo-Agent-Toolkit/develop/docs/source/build-workflows/mcp-client.md"&gt;&lt;strong&gt;Full MCP Support:&lt;/strong&gt;&lt;/a&gt; Compatible with &lt;a href="https://modelcontextprotocol.io/"&gt;Model Context Protocol (MCP)&lt;/a&gt;. You can use NeMo Agent Toolkit as an &lt;a href="https://raw.githubusercontent.com/NVIDIA/NeMo-Agent-Toolkit/develop/docs/source/build-workflows/mcp-client.md"&gt;MCP client&lt;/a&gt; to connect to and use tools served by remote MCP servers. You can also use NeMo Agent Toolkit as an &lt;a href="https://raw.githubusercontent.com/NVIDIA/NeMo-Agent-Toolkit/develop/docs/source/run-workflows/mcp-server.md"&gt;MCP server&lt;/a&gt; to publish tools via MCP.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;With NeMo Agent Toolkit, you can move quickly, experiment freely, and ensure reliability across all your agent-driven projects.&lt;/p&gt; 
&lt;h2&gt;ğŸš€ Installation&lt;/h2&gt; 
&lt;p&gt;Before you begin using NeMo Agent Toolkit, ensure that you have Python 3.11, 3.12, or 3.13 installed on your system.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] For users who want to run the examples, it's required to clone the repository and install from source to get the necessary files required to run the examples. Please refer to the &lt;a href="https://raw.githubusercontent.com/NVIDIA/NeMo-Agent-Toolkit/develop/examples/README.md"&gt;Examples&lt;/a&gt; documentation for more information.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;To install the latest stable version of NeMo Agent Toolkit from PyPI, run the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install nvidia-nat
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;NeMo Agent Toolkit has many optional dependencies which can be installed with the core package. Optional dependencies are grouped by framework and can be installed with the core package. For example, to install the LangChain/LangGraph plugin, run the following:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install "nvidia-nat[langchain]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or for all optional dependencies:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install "nvidia-nat[all]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Detailed installation instructions, including the full list of optional dependencies, can be found in the &lt;a href="https://raw.githubusercontent.com/NVIDIA/NeMo-Agent-Toolkit/develop/docs/source/get-started/installation.md"&gt;Installation Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;ğŸŒŸ Hello World Example&lt;/h2&gt; 
&lt;p&gt;Before getting started, it's possible to run this simple workflow and many other examples in Google Colab with no setup. Click here to open the introduction notebook: &lt;a href="https://colab.research.google.com/github/NVIDIA/NeMo-Agent-Toolkit/"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open in Colab" /&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Ensure you have set the &lt;code&gt;NVIDIA_API_KEY&lt;/code&gt; environment variable to allow the example to use NVIDIA NIMs. An API key can be obtained by visiting &lt;a href="https://build.nvidia.com/"&gt;&lt;code&gt;build.nvidia.com&lt;/code&gt;&lt;/a&gt; and creating an account.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;export NVIDIA_API_KEY=&amp;lt;your_api_key&amp;gt;
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Create the NeMo Agent Toolkit workflow configuration file. This file will define the agents, tools, and workflows that will be used in the example. Save the following as &lt;code&gt;workflow.yml&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-yaml"&gt;functions:
   # Add a tool to search wikipedia
   wikipedia_search:
      _type: wiki_search
      max_results: 2

llms:
   # Tell NeMo Agent Toolkit which LLM to use for the agent
   nim_llm:
      _type: nim
      model_name: meta/llama-3.1-70b-instruct
      temperature: 0.0

workflow:
   # Use an agent that 'reasons' and 'acts'
   _type: react_agent
   # Give it access to our wikipedia search tool
   tool_names: [wikipedia_search]
   # Tell it which LLM to use
   llm_name: nim_llm
   # Make it verbose
   verbose: true
   # Retry up to 3 times
   parse_agent_response_max_retries: 3
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run the Hello World example using the &lt;code&gt;nat&lt;/code&gt; CLI and the &lt;code&gt;workflow.yml&lt;/code&gt; file.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;nat run --config_file workflow.yml --input "List five subspecies of Aardvarks"
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This will run the workflow and output the results to the console.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-console"&gt;Workflow Result:
['Here are five subspecies of Aardvarks:\n\n1. Orycteropus afer afer (Southern aardvark)\n2. O. a. adametzi  Grote, 1921 (Western aardvark)\n3. O. a. aethiopicus  Sundevall, 1843\n4. O. a. angolensis  Zukowsky &amp;amp; Haltenorth, 1957\n5. O. a. erikssoni  LÃ¶nnberg, 1906']
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;ğŸ“š Additional Resources&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“– &lt;a href="https://docs.nvidia.com/nemo/agent-toolkit/latest"&gt;Documentation&lt;/a&gt;: Explore the full documentation for NeMo Agent Toolkit.&lt;/li&gt; 
 &lt;li&gt;ğŸ§­ &lt;a href="https://raw.githubusercontent.com/NVIDIA/NeMo-Agent-Toolkit/develop/docs/source/get-started/installation.md"&gt;Get Started Guide&lt;/a&gt;: Set up your environment and start building with NeMo Agent toolkit.&lt;/li&gt; 
 &lt;li&gt;ğŸ¤ &lt;a href="https://raw.githubusercontent.com/NVIDIA/NeMo-Agent-Toolkit/develop/docs/source/resources/contributing/index.md"&gt;Contributing&lt;/a&gt;: Learn how to contribute to NeMo Agent Toolkit and set up your development environment.&lt;/li&gt; 
 &lt;li&gt;ğŸ§ª &lt;a href="https://raw.githubusercontent.com/NVIDIA/NeMo-Agent-Toolkit/develop/examples/README.md"&gt;Examples&lt;/a&gt;: Explore examples of NeMo Agent Toolkit workflows located in the &lt;a href="https://raw.githubusercontent.com/NVIDIA/NeMo-Agent-Toolkit/develop/examples"&gt;&lt;code&gt;examples&lt;/code&gt;&lt;/a&gt; directory of the source repository.&lt;/li&gt; 
 &lt;li&gt;ğŸ› ï¸ &lt;a href="https://raw.githubusercontent.com/NVIDIA/NeMo-Agent-Toolkit/develop/docs/source/get-started/tutorials/customize-a-workflow.md"&gt;Create and Customize NeMo Agent Toolkit Workflows&lt;/a&gt;: Learn how to create and customize NeMo Agent Toolkit workflows.&lt;/li&gt; 
 &lt;li&gt;ğŸ¯ &lt;a href="https://raw.githubusercontent.com/NVIDIA/NeMo-Agent-Toolkit/develop/docs/source/improve-workflows/evaluate.md"&gt;Evaluate with NeMo Agent Toolkit&lt;/a&gt;: Learn how to evaluate your NeMo Agent Toolkit workflows.&lt;/li&gt; 
 &lt;li&gt;ğŸ†˜ &lt;a href="https://raw.githubusercontent.com/NVIDIA/NeMo-Agent-Toolkit/develop/docs/source/resources/troubleshooting.md"&gt;Troubleshooting&lt;/a&gt;: Get help with common issues.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ›£ï¸ Roadmap&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Automatic Reinforcement Learning (RL) to fine-tune LLMs for a specific agent.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Integration with &lt;a href="https://github.com/NVIDIA/NeMo-Guardrails"&gt;NeMo Guardrails&lt;/a&gt; to secure any function in an agent workflow.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; End-to-end acceleration using intelligent integrations with &lt;a href="https://github.com/ai-dynamo/dynamo"&gt;NVIDIA Dynamo&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ’¬ Feedback&lt;/h2&gt; 
&lt;p&gt;We would love to hear from you! Please file an issue on &lt;a href="https://github.com/NVIDIA/NeMo-Agent-Toolkit/issues"&gt;GitHub&lt;/a&gt; if you have any feedback or feature requests.&lt;/p&gt; 
&lt;h2&gt;ğŸ¤ Acknowledgements&lt;/h2&gt; 
&lt;p&gt;We would like to thank the following groups for their contribution to the toolkit:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.synopsys.com/"&gt;Synopsys&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Google ADK framework support.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://wandb.ai/site/weave/"&gt;W&amp;amp;B Weave Team&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Contributions to the evaluation and telemetry system.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;In addition, we would like to thank the following open source projects that made NeMo Agent Toolkit possible:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/crewAIInc/crewAI"&gt;CrewAI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/tiangolo/fastapi"&gt;FastAPI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/google/adk-python"&gt;Google Agent Development Kit (ADK)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/langchain-ai/langchain"&gt;LangChain&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/run-llama/llama_index"&gt;Llama-Index&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/mem0ai/mem0"&gt;Mem0ai&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/minio/minio"&gt;MinIO&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/modelcontextprotocol/modelcontextprotocol"&gt;Model Context Protocol (MCP)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/open-telemetry/opentelemetry-python"&gt;OpenTelemetry&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/arize-ai/phoenix"&gt;Phoenix&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/explodinggradients/ragas"&gt;Ragas&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/redis/redis-py"&gt;Redis&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/microsoft/semantic-kernel"&gt;Semantic Kernel&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/astral-sh/uv"&gt;uv&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/wandb/weave"&gt;Weave&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>google/langextract</title>
      <link>https://github.com/google/langextract</link>
      <description>&lt;p&gt;A Python library for extracting structured information from unstructured text using LLMs with precise source grounding and interactive visualization.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="https://github.com/google/langextract"&gt; &lt;img src="https://raw.githubusercontent.com/google/langextract/main/docs/_static/logo.svg?sanitize=true" alt="LangExtract Logo" width="128" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h1&gt;LangExtract&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://pypi.org/project/langextract/"&gt;&lt;img src="https://img.shields.io/pypi/v/langextract.svg?sanitize=true" alt="PyPI version" /&gt;&lt;/a&gt; &lt;a href="https://github.com/google/langextract"&gt;&lt;img src="https://img.shields.io/github/stars/google/langextract.svg?style=social&amp;amp;label=Star" alt="GitHub stars" /&gt;&lt;/a&gt; &lt;img src="https://github.com/google/langextract/actions/workflows/ci.yaml/badge.svg?sanitize=true" alt="Tests" /&gt; &lt;a href="https://doi.org/10.5281/zenodo.17015089"&gt;&lt;img src="https://zenodo.org/badge/DOI/10.5281/zenodo.17015089.svg?sanitize=true" alt="DOI" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#introduction"&gt;Introduction&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#why-langextract"&gt;Why LangExtract?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#quick-start"&gt;Quick Start&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#api-key-setup-for-cloud-models"&gt;API Key Setup for Cloud Models&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#adding-custom-model-providers"&gt;Adding Custom Model Providers&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#using-openai-models"&gt;Using OpenAI Models&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#using-local-llms-with-ollama"&gt;Using Local LLMs with Ollama&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#more-examples"&gt;More Examples&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#romeo-and-juliet-full-text-extraction"&gt;&lt;em&gt;Romeo and Juliet&lt;/em&gt; Full Text Extraction&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#medication-extraction"&gt;Medication Extraction&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#radiology-report-structuring-radextract"&gt;Radiology Report Structuring: RadExtract&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#community-providers"&gt;Community Providers&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#testing"&gt;Testing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/google/langextract/main/#disclaimer"&gt;Disclaimer&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;LangExtract is a Python library that uses LLMs to extract structured information from unstructured text documents based on user-defined instructions. It processes materials such as clinical notes or reports, identifying and organizing key details while ensuring the extracted data corresponds to the source text.&lt;/p&gt; 
&lt;h2&gt;Why LangExtract?&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Precise Source Grounding:&lt;/strong&gt; Maps every extraction to its exact location in the source text, enabling visual highlighting for easy traceability and verification.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Reliable Structured Outputs:&lt;/strong&gt; Enforces a consistent output schema based on your few-shot examples, leveraging controlled generation in supported models like Gemini to guarantee robust, structured results.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Optimized for Long Documents:&lt;/strong&gt; Overcomes the "needle-in-a-haystack" challenge of large document extraction by using an optimized strategy of text chunking, parallel processing, and multiple passes for higher recall.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Interactive Visualization:&lt;/strong&gt; Instantly generates a self-contained, interactive HTML file to visualize and review thousands of extracted entities in their original context.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Flexible LLM Support:&lt;/strong&gt; Supports your preferred models, from cloud-based LLMs like the Google Gemini family to local open-source models via the built-in Ollama interface.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Adaptable to Any Domain:&lt;/strong&gt; Define extraction tasks for any domain using just a few examples. LangExtract adapts to your needs without requiring any model fine-tuning.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Leverages LLM World Knowledge:&lt;/strong&gt; Utilize precise prompt wording and few-shot examples to influence how the extraction task may utilize LLM knowledge. The accuracy of any inferred information and its adherence to the task specification are contingent upon the selected LLM, the complexity of the task, the clarity of the prompt instructions, and the nature of the prompt examples.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Using cloud-hosted models like Gemini requires an API key. See the &lt;a href="https://raw.githubusercontent.com/google/langextract/main/#api-key-setup-for-cloud-models"&gt;API Key Setup&lt;/a&gt; section for instructions on how to get and configure your key.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Extract structured information with just a few lines of code.&lt;/p&gt; 
&lt;h3&gt;1. Define Your Extraction Task&lt;/h3&gt; 
&lt;p&gt;First, create a prompt that clearly describes what you want to extract. Then, provide a high-quality example to guide the model.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import langextract as lx
import textwrap

# 1. Define the prompt and extraction rules
prompt = textwrap.dedent("""\
    Extract characters, emotions, and relationships in order of appearance.
    Use exact text for extractions. Do not paraphrase or overlap entities.
    Provide meaningful attributes for each entity to add context.""")

# 2. Provide a high-quality example to guide the model
examples = [
    lx.data.ExampleData(
        text="ROMEO. But soft! What light through yonder window breaks? It is the east, and Juliet is the sun.",
        extractions=[
            lx.data.Extraction(
                extraction_class="character",
                extraction_text="ROMEO",
                attributes={"emotional_state": "wonder"}
            ),
            lx.data.Extraction(
                extraction_class="emotion",
                extraction_text="But soft!",
                attributes={"feeling": "gentle awe"}
            ),
            lx.data.Extraction(
                extraction_class="relationship",
                extraction_text="Juliet is the sun",
                attributes={"type": "metaphor"}
            ),
        ]
    )
]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. Run the Extraction&lt;/h3&gt; 
&lt;p&gt;Provide your input text and the prompt materials to the &lt;code&gt;lx.extract&lt;/code&gt; function.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# The input text to be processed
input_text = "Lady Juliet gazed longingly at the stars, her heart aching for Romeo"

# Run the extraction
result = lx.extract(
    text_or_documents=input_text,
    prompt_description=prompt,
    examples=examples,
    model_id="gemini-2.5-flash",
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Model Selection&lt;/strong&gt;: &lt;code&gt;gemini-2.5-flash&lt;/code&gt; is the recommended default, offering an excellent balance of speed, cost, and quality. For highly complex tasks requiring deeper reasoning, &lt;code&gt;gemini-2.5-pro&lt;/code&gt; may provide superior results. For large-scale or production use, a Tier 2 Gemini quota is suggested to increase throughput and avoid rate limits. See the &lt;a href="https://ai.google.dev/gemini-api/docs/rate-limits#tier-2"&gt;rate-limit documentation&lt;/a&gt; for details.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Model Lifecycle&lt;/strong&gt;: Note that Gemini models have a lifecycle with defined retirement dates. Users should consult the &lt;a href="https://cloud.google.com/vertex-ai/generative-ai/docs/learn/model-versions"&gt;official model version documentation&lt;/a&gt; to stay informed about the latest stable and legacy versions.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;3. Visualize the Results&lt;/h3&gt; 
&lt;p&gt;The extractions can be saved to a &lt;code&gt;.jsonl&lt;/code&gt; file, a popular format for working with language model data. LangExtract can then generate an interactive HTML visualization from this file to review the entities in context.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Save the results to a JSONL file
lx.io.save_annotated_documents([result], output_name="extraction_results.jsonl", output_dir=".")

# Generate the visualization from the file
html_content = lx.visualize("extraction_results.jsonl")
with open("visualization.html", "w") as f:
    if hasattr(html_content, 'data'):
        f.write(html_content.data)  # For Jupyter/Colab
    else:
        f.write(html_content)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This creates an animated and interactive HTML file:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/google/langextract/main/docs/_static/romeo_juliet_basic.gif" alt="Romeo and Juliet Basic Visualization " /&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note on LLM Knowledge Utilization:&lt;/strong&gt; This example demonstrates extractions that stay close to the text evidence - extracting "longing" for Lady Juliet's emotional state and identifying "yearning" from "gazed longingly at the stars." The task could be modified to generate attributes that draw more heavily from the LLM's world knowledge (e.g., adding &lt;code&gt;"identity": "Capulet family daughter"&lt;/code&gt; or &lt;code&gt;"literary_context": "tragic heroine"&lt;/code&gt;). The balance between text-evidence and knowledge-inference is controlled by your prompt instructions and example attributes.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Scaling to Longer Documents&lt;/h3&gt; 
&lt;p&gt;For larger texts, you can process entire documents directly from URLs with parallel processing and enhanced sensitivity:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Process Romeo &amp;amp; Juliet directly from Project Gutenberg
result = lx.extract(
    text_or_documents="https://www.gutenberg.org/files/1513/1513-0.txt",
    prompt_description=prompt,
    examples=examples,
    model_id="gemini-2.5-flash",
    extraction_passes=3,    # Improves recall through multiple passes
    max_workers=20,         # Parallel processing for speed
    max_char_buffer=1000    # Smaller contexts for better accuracy
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This approach can extract hundreds of entities from full novels while maintaining high accuracy. The interactive visualization seamlessly handles large result sets, making it easy to explore hundreds of entities from the output JSONL file. &lt;strong&gt;&lt;a href="https://github.com/google/langextract/raw/main/docs/examples/longer_text_example.md"&gt;See the full &lt;em&gt;Romeo and Juliet&lt;/em&gt; extraction example â†’&lt;/a&gt;&lt;/strong&gt; for detailed results and performance insights.&lt;/p&gt; 
&lt;h3&gt;Vertex AI Batch Processing&lt;/h3&gt; 
&lt;p&gt;Save costs on large-scale tasks by enabling Vertex AI Batch API: &lt;code&gt;language_model_params={"vertexai": True, "batch": {"enabled": True}}&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;See an example of the Vertex AI Batch API usage in &lt;a href="https://raw.githubusercontent.com/google/langextract/main/docs/examples/batch_api_example.md"&gt;this example&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;From PyPI&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install langextract
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Recommended for most users. For isolated environments, consider using a virtual environment:&lt;/em&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m venv langextract_env
source langextract_env/bin/activate  # On Windows: langextract_env\Scripts\activate
pip install langextract
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;From Source&lt;/h3&gt; 
&lt;p&gt;LangExtract uses modern Python packaging with &lt;code&gt;pyproject.toml&lt;/code&gt; for dependency management:&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Installing with &lt;code&gt;-e&lt;/code&gt; puts the package in development mode, allowing you to modify the code without reinstalling.&lt;/em&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/google/langextract.git
cd langextract

# For basic installation:
pip install -e .

# For development (includes linting tools):
pip install -e ".[dev]"

# For testing (includes pytest):
pip install -e ".[test]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Docker&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker build -t langextract .
docker run --rm -e LANGEXTRACT_API_KEY="your-api-key" langextract python your_script.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;API Key Setup for Cloud Models&lt;/h2&gt; 
&lt;p&gt;When using LangExtract with cloud-hosted models (like Gemini or OpenAI), you'll need to set up an API key. On-device models don't require an API key. For developers using local LLMs, LangExtract offers built-in support for Ollama and can be extended to other third-party APIs by updating the inference endpoints.&lt;/p&gt; 
&lt;h3&gt;API Key Sources&lt;/h3&gt; 
&lt;p&gt;Get API keys from:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://aistudio.google.com/app/apikey"&gt;AI Studio&lt;/a&gt; for Gemini models&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://cloud.google.com/vertex-ai/generative-ai/docs/sdks/overview"&gt;Vertex AI&lt;/a&gt; for enterprise use&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://platform.openai.com/api-keys"&gt;OpenAI Platform&lt;/a&gt; for OpenAI models&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Setting up API key in your environment&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Option 1: Environment Variable&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export LANGEXTRACT_API_KEY="your-api-key-here"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Option 2: .env File (Recommended)&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Add your API key to a &lt;code&gt;.env&lt;/code&gt; file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Add API key to .env file
cat &amp;gt;&amp;gt; .env &amp;lt;&amp;lt; 'EOF'
LANGEXTRACT_API_KEY=your-api-key-here
EOF

# Keep your API key secure
echo '.env' &amp;gt;&amp;gt; .gitignore
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;In your Python code:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import langextract as lx

result = lx.extract(
    text_or_documents=input_text,
    prompt_description="Extract information...",
    examples=[...],
    model_id="gemini-2.5-flash"
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Option 3: Direct API Key (Not Recommended for Production)&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;You can also provide the API key directly in your code, though this is not recommended for production use:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;result = lx.extract(
    text_or_documents=input_text,
    prompt_description="Extract information...",
    examples=[...],
    model_id="gemini-2.5-flash",
    api_key="your-api-key-here"  # Only use this for testing/development
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Option 4: Vertex AI (Service Accounts)&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Use &lt;a href="https://cloud.google.com/vertex-ai/docs/start/introduction-unified-platform"&gt;Vertex AI&lt;/a&gt; for authentication with service accounts:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;result = lx.extract(
    text_or_documents=input_text,
    prompt_description="Extract information...",
    examples=[...],
    model_id="gemini-2.5-flash",
    language_model_params={
        "vertexai": True,
        "project": "your-project-id",
        "location": "global"  # or regional endpoint
    }
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Adding Custom Model Providers&lt;/h2&gt; 
&lt;p&gt;LangExtract supports custom LLM providers via a lightweight plugin system. You can add support for new models without changing core code.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Add new model support independently of the core library&lt;/li&gt; 
 &lt;li&gt;Distribute your provider as a separate Python package&lt;/li&gt; 
 &lt;li&gt;Keep custom dependencies isolated&lt;/li&gt; 
 &lt;li&gt;Override or extend built-in providers via priority-based resolution&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See the detailed guide in &lt;a href="https://raw.githubusercontent.com/google/langextract/main/langextract/providers/README.md"&gt;Provider System Documentation&lt;/a&gt; to learn how to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Register a provider with &lt;code&gt;@registry.register(...)&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Publish an entry point for discovery&lt;/li&gt; 
 &lt;li&gt;Optionally provide a schema with &lt;code&gt;get_schema_class()&lt;/code&gt; for structured output&lt;/li&gt; 
 &lt;li&gt;Integrate with the factory via &lt;code&gt;create_model(...)&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Using OpenAI Models&lt;/h2&gt; 
&lt;p&gt;LangExtract supports OpenAI models (requires optional dependency: &lt;code&gt;pip install langextract[openai]&lt;/code&gt;):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import langextract as lx

result = lx.extract(
    text_or_documents=input_text,
    prompt_description=prompt,
    examples=examples,
    model_id="gpt-4o",  # Automatically selects OpenAI provider
    api_key=os.environ.get('OPENAI_API_KEY'),
    fence_output=True,
    use_schema_constraints=False
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note: OpenAI models require &lt;code&gt;fence_output=True&lt;/code&gt; and &lt;code&gt;use_schema_constraints=False&lt;/code&gt; because LangExtract doesn't implement schema constraints for OpenAI yet.&lt;/p&gt; 
&lt;h2&gt;Using Local LLMs with Ollama&lt;/h2&gt; 
&lt;p&gt;LangExtract supports local inference using Ollama, allowing you to run models without API keys:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import langextract as lx

result = lx.extract(
    text_or_documents=input_text,
    prompt_description=prompt,
    examples=examples,
    model_id="gemma2:2b",  # Automatically selects Ollama provider
    model_url="http://localhost:11434",
    fence_output=False,
    use_schema_constraints=False
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Quick setup:&lt;/strong&gt; Install Ollama from &lt;a href="https://ollama.com/"&gt;ollama.com&lt;/a&gt;, run &lt;code&gt;ollama pull gemma2:2b&lt;/code&gt;, then &lt;code&gt;ollama serve&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;For detailed installation, Docker setup, and examples, see &lt;a href="https://raw.githubusercontent.com/google/langextract/main/examples/ollama/"&gt;&lt;code&gt;examples/ollama/&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;More Examples&lt;/h2&gt; 
&lt;p&gt;Additional examples of LangExtract in action:&lt;/p&gt; 
&lt;h3&gt;&lt;em&gt;Romeo and Juliet&lt;/em&gt; Full Text Extraction&lt;/h3&gt; 
&lt;p&gt;LangExtract can process complete documents directly from URLs. This example demonstrates extraction from the full text of &lt;em&gt;Romeo and Juliet&lt;/em&gt; from Project Gutenberg (147,843 characters), showing parallel processing, sequential extraction passes, and performance optimization for long document processing.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/google/langextract/raw/main/docs/examples/longer_text_example.md"&gt;View &lt;em&gt;Romeo and Juliet&lt;/em&gt; Full Text Example â†’&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h3&gt;Medication Extraction&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Disclaimer:&lt;/strong&gt; This demonstration is for illustrative purposes of LangExtract's baseline capability only. It does not represent a finished or approved product, is not intended to diagnose or suggest treatment of any disease or condition, and should not be used for medical advice.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;LangExtract excels at extracting structured medical information from clinical text. These examples demonstrate both basic entity recognition (medication names, dosages, routes) and relationship extraction (connecting medications to their attributes), showing LangExtract's effectiveness for healthcare applications.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/google/langextract/raw/main/docs/examples/medication_examples.md"&gt;View Medication Examples â†’&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h3&gt;Radiology Report Structuring: RadExtract&lt;/h3&gt; 
&lt;p&gt;Explore RadExtract, a live interactive demo on HuggingFace Spaces that shows how LangExtract can automatically structure radiology reports. Try it directly in your browser with no setup required.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://huggingface.co/spaces/google/radextract"&gt;View RadExtract Demo â†’&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;Community Providers&lt;/h2&gt; 
&lt;p&gt;Extend LangExtract with custom model providers! Check out our &lt;a href="https://raw.githubusercontent.com/google/langextract/main/COMMUNITY_PROVIDERS.md"&gt;Community Provider Plugins&lt;/a&gt; registry to discover providers created by the community or add your own.&lt;/p&gt; 
&lt;p&gt;For detailed instructions on creating a provider plugin, see the &lt;a href="https://raw.githubusercontent.com/google/langextract/main/examples/custom_provider_plugin/"&gt;Custom Provider Plugin Example&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Contributions are welcome! See &lt;a href="https://github.com/google/langextract/raw/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; to get started with development, testing, and pull requests. You must sign a &lt;a href="https://cla.developers.google.com/about"&gt;Contributor License Agreement&lt;/a&gt; before submitting patches.&lt;/p&gt; 
&lt;h2&gt;Testing&lt;/h2&gt; 
&lt;p&gt;To run tests locally from the source:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Clone the repository
git clone https://github.com/google/langextract.git
cd langextract

# Install with test dependencies
pip install -e ".[test]"

# Run all tests
pytest tests
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or reproduce the full CI matrix locally with tox:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;tox  # runs pylint + pytest on Python 3.10 and 3.11
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Ollama Integration Testing&lt;/h3&gt; 
&lt;p&gt;If you have Ollama installed locally, you can run integration tests:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Test Ollama integration (requires Ollama running with gemma2:2b model)
tox -e ollama-integration
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This test will automatically detect if Ollama is available and run real inference tests.&lt;/p&gt; 
&lt;h2&gt;Development&lt;/h2&gt; 
&lt;h3&gt;Code Formatting&lt;/h3&gt; 
&lt;p&gt;This project uses automated formatting tools to maintain consistent code style:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Auto-format all code
./autoformat.sh

# Or run formatters separately
isort langextract tests --profile google --line-length 80
pyink langextract tests --config pyproject.toml
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Pre-commit Hooks&lt;/h3&gt; 
&lt;p&gt;For automatic formatting checks:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pre-commit install  # One-time setup
pre-commit run --all-files  # Manual run
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Linting&lt;/h3&gt; 
&lt;p&gt;Run linting before submitting PRs:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pylint --rcfile=.pylintrc langextract tests
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/google/langextract/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for full development guidelines.&lt;/p&gt; 
&lt;h2&gt;Disclaimer&lt;/h2&gt; 
&lt;p&gt;This is not an officially supported Google product. If you use LangExtract in production or publications, please cite accordingly and acknowledge usage. Use is subject to the &lt;a href="https://github.com/google/langextract/raw/main/LICENSE"&gt;Apache 2.0 License&lt;/a&gt;. For health-related applications, use of LangExtract is also subject to the &lt;a href="https://developers.google.com/health-ai-developer-foundations/terms"&gt;Health AI Developer Foundations Terms of Use&lt;/a&gt;.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;strong&gt;Happy Extracting!&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>astral-sh/ty</title>
      <link>https://github.com/astral-sh/ty</link>
      <description>&lt;p&gt;An extremely fast Python type checker and language server, written in Rust.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ty&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/astral-sh/ty"&gt;&lt;img src="https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ty/main/assets/badge/v0.json" alt="ty" /&gt;&lt;/a&gt; &lt;a href="https://pypi.python.org/pypi/ty"&gt;&lt;img src="https://img.shields.io/pypi/v/ty.svg?sanitize=true" alt="PyPI" /&gt;&lt;/a&gt; &lt;a href="https://discord.com/invite/astral-sh"&gt;&lt;img src="https://img.shields.io/badge/Discord-%235865F2.svg?logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;An extremely fast Python type checker and language server, written in Rust.&lt;/p&gt; 
&lt;br /&gt; 
&lt;p align="center"&gt; &lt;img alt="Shows a bar chart with benchmark results." width="500px" src="https://raw.githubusercontent.com/astral-sh/ty/main/docs/assets/ty-benchmark-cli.svg?sanitize=true" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;i&gt;Type checking the &lt;a href="https://github.com/home-assistant/core"&gt;home-assistant&lt;/a&gt; project without caching.&lt;/i&gt; &lt;/p&gt; 
&lt;br /&gt; 
&lt;p&gt;ty is backed by &lt;a href="https://astral.sh"&gt;Astral&lt;/a&gt;, the creators of &lt;a href="https://github.com/astral-sh/uv"&gt;uv&lt;/a&gt; and &lt;a href="https://github.com/astral-sh/ruff"&gt;Ruff&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Highlights&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;10x - 100x faster than mypy and Pyright&lt;/li&gt; 
 &lt;li&gt;Comprehensive &lt;a href="https://docs.astral.sh/ty/features/diagnostics/"&gt;diagnostics&lt;/a&gt; with rich contextual information&lt;/li&gt; 
 &lt;li&gt;Configurable &lt;a href="https://docs.astral.sh/ty/rules/"&gt;rule levels&lt;/a&gt;, &lt;a href="https://docs.astral.sh/ty/reference/configuration/#overrides"&gt;per-file overrides&lt;/a&gt;, &lt;a href="https://docs.astral.sh/ty/suppression/"&gt;suppression comments&lt;/a&gt;, and first-class project support&lt;/li&gt; 
 &lt;li&gt;Designed for adoption, with support for &lt;a href="https://docs.astral.sh/ty/features/type-system/#redeclarations"&gt;redeclarations&lt;/a&gt; and &lt;a href="https://docs.astral.sh/ty/features/type-system/#gradual-guarantee"&gt;partially typed code&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.astral.sh/ty/features/language-server/"&gt;Language server&lt;/a&gt; with code navigation, completions, code actions, auto-import, inlay hints, on-hover help, etc.&lt;/li&gt; 
 &lt;li&gt;Fine-grained &lt;a href="https://docs.astral.sh/ty/features/language-server/#fine-grained-incrementality"&gt;incremental analysis&lt;/a&gt; designed for fast updates when editing files in an IDE&lt;/li&gt; 
 &lt;li&gt;Editor integrations for &lt;a href="https://docs.astral.sh/ty/editors/#vs-code"&gt;VS Code&lt;/a&gt;, &lt;a href="https://docs.astral.sh/ty/editors/#pycharm"&gt;PyCharm&lt;/a&gt;, &lt;a href="https://docs.astral.sh/ty/editors/#neovim"&gt;Neovim&lt;/a&gt; and more&lt;/li&gt; 
 &lt;li&gt;Advanced typing features like first-class &lt;a href="https://docs.astral.sh/ty/features/type-system/#intersection-types"&gt;intersection types&lt;/a&gt;, advanced &lt;a href="https://docs.astral.sh/ty/features/type-system/#top-and-bottom-materializations"&gt;type narrowing&lt;/a&gt;, and &lt;a href="https://docs.astral.sh/ty/features/type-system/#reachability-based-on-types"&gt;sophisticated reachability analysis&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Getting started&lt;/h2&gt; 
&lt;p&gt;Run ty with &lt;a href="https://docs.astral.sh/uv/guides/tools/#running-tools"&gt;uvx&lt;/a&gt; to get started quickly:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;uvx ty check
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or, check out the &lt;a href="https://play.ty.dev"&gt;ty playground&lt;/a&gt; to try it out in your browser.&lt;/p&gt; 
&lt;p&gt;To learn more about using ty, see the &lt;a href="https://docs.astral.sh/ty/"&gt;documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;To install ty, see the &lt;a href="https://docs.astral.sh/ty/installation/"&gt;installation&lt;/a&gt; documentation.&lt;/p&gt; 
&lt;p&gt;To add the ty language server to your editor, see the &lt;a href="https://docs.astral.sh/ty/editors/"&gt;editor integration&lt;/a&gt; guide.&lt;/p&gt; 
&lt;h2&gt;Getting help&lt;/h2&gt; 
&lt;p&gt;If you have questions or want to report a bug, please open an &lt;a href="https://github.com/astral-sh/ty/issues"&gt;issue&lt;/a&gt; in this repository.&lt;/p&gt; 
&lt;p&gt;You may also join our &lt;a href="https://discord.com/invite/astral-sh"&gt;Discord server&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Development of this project takes place in the &lt;a href="https://github.com/astral-sh/ruff"&gt;Ruff&lt;/a&gt; repository at this time. Please &lt;a href="https://github.com/astral-sh/ruff/pulls"&gt;open pull requests&lt;/a&gt; there for changes to anything in the &lt;code&gt;ruff&lt;/code&gt; submodule (which includes all of the Rust source code).&lt;/p&gt; 
&lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/astral-sh/ty/main/CONTRIBUTING.md"&gt;contributing guide&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;FAQ&lt;/h2&gt; 
&lt;!-- We intentionally use smaller headings for the FAQ items --&gt; 
&lt;!-- markdownlint-disable MD001 --&gt; 
&lt;h4&gt;Why is ty doing _____?&lt;/h4&gt; 
&lt;p&gt;See our &lt;a href="https://docs.astral.sh/ty/reference/typing-faq"&gt;typing FAQ&lt;/a&gt;.&lt;/p&gt; 
&lt;h4&gt;How do you pronounce ty?&lt;/h4&gt; 
&lt;p&gt;It's pronounced as "tee - why" (&lt;a href="https://en.wikipedia.org/wiki/Help:IPA/English#Key"&gt;&lt;code&gt;/tiË waÉª/&lt;/code&gt;&lt;/a&gt;)&lt;/p&gt; 
&lt;h4&gt;How should I stylize ty?&lt;/h4&gt; 
&lt;p&gt;Just "ty", please.&lt;/p&gt; 
&lt;!-- markdownlint-enable MD001 --&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;ty is licensed under the MIT license (&lt;a href="https://raw.githubusercontent.com/astral-sh/ty/main/LICENSE"&gt;LICENSE&lt;/a&gt; or &lt;a href="https://opensource.org/licenses/MIT"&gt;https://opensource.org/licenses/MIT&lt;/a&gt;).&lt;/p&gt; 
&lt;p&gt;Unless you explicitly state otherwise, any contribution intentionally submitted for inclusion in ty by you, as defined in the MIT license, shall be licensed as above, without any additional terms or conditions.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a target="_blank" href="https://astral.sh" style="background:none"&gt; &lt;img src="https://raw.githubusercontent.com/astral-sh/uv/main/assets/svg/Astral.svg?sanitize=true" alt="Made by Astral" /&gt; &lt;/a&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>comfyanonymous/ComfyUI</title>
      <link>https://github.com/comfyanonymous/ComfyUI</link>
      <description>&lt;p&gt;The most powerful and modular diffusion model GUI, api and backend with a graph/nodes interface.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h1&gt;ComfyUI&lt;/h1&gt; 
 &lt;p&gt;&lt;strong&gt;The most powerful and modular visual AI engine and application.&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://www.comfy.org/"&gt;&lt;img src="https://img.shields.io/badge/ComfyOrg-4285F4?style=flat" alt="Website" /&gt;&lt;/a&gt; &lt;a href="https://www.comfy.org/discord"&gt;&lt;img src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fdiscord.com%2Fapi%2Finvites%2Fcomfyorg%3Fwith_counts%3Dtrue&amp;amp;query=%24.approximate_member_count&amp;amp;logo=discord&amp;amp;logoColor=white&amp;amp;label=Discord&amp;amp;color=green&amp;amp;suffix=%20total" alt="Dynamic JSON Badge" /&gt;&lt;/a&gt; &lt;a href="https://x.com/ComfyUI"&gt;&lt;img src="https://img.shields.io/twitter/follow/ComfyUI" alt="Twitter" /&gt;&lt;/a&gt; &lt;a href="https://app.element.io/#/room/%23comfyui_space%3Amatrix.org"&gt;&lt;img src="https://img.shields.io/badge/Matrix-000000?style=flat&amp;amp;logo=matrix&amp;amp;logoColor=white" alt="Matrix" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;a href="https://github.com/comfyanonymous/ComfyUI/releases"&gt;&lt;img src="https://img.shields.io/github/v/release/comfyanonymous/ComfyUI?style=flat&amp;amp;sort=semver" alt="" /&gt;&lt;/a&gt; &lt;a href="https://github.com/comfyanonymous/ComfyUI/releases"&gt;&lt;img src="https://img.shields.io/github/release-date/comfyanonymous/ComfyUI?style=flat" alt="" /&gt;&lt;/a&gt; &lt;a href="https://github.com/comfyanonymous/ComfyUI/releases"&gt;&lt;img src="https://img.shields.io/github/downloads/comfyanonymous/ComfyUI/total?style=flat" alt="" /&gt;&lt;/a&gt; &lt;a href="https://github.com/comfyanonymous/ComfyUI/releases"&gt;&lt;img src="https://img.shields.io/github/downloads/comfyanonymous/ComfyUI/latest/total?style=flat&amp;amp;label=downloads%40latest" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;!-- Workaround to display total user from https://github.com/badges/shields/issues/4500#issuecomment-2060079995 --&gt; 
 &lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/7ccaf2c1-9b72-41ae-9a89-5688c94b7abe" alt="ComfyUI Screenshot" /&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;ComfyUI lets you design and execute advanced stable diffusion pipelines using a graph/nodes/flowchart based interface. Available on Windows, Linux, and macOS.&lt;/p&gt; 
&lt;h2&gt;Get Started&lt;/h2&gt; 
&lt;h4&gt;&lt;a href="https://www.comfy.org/download"&gt;Desktop Application&lt;/a&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;The easiest way to get started.&lt;/li&gt; 
 &lt;li&gt;Available on Windows &amp;amp; macOS.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;&lt;a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/#installing"&gt;Windows Portable Package&lt;/a&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Get the latest commits and completely portable.&lt;/li&gt; 
 &lt;li&gt;Available on Windows.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;&lt;a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/#manual-install-windows-linux"&gt;Manual Install&lt;/a&gt;&lt;/h4&gt; 
&lt;p&gt;Supports all operating systems and GPU types (NVIDIA, AMD, Intel, Apple Silicon, Ascend).&lt;/p&gt; 
&lt;h2&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/"&gt;Examples&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;See what ComfyUI can do with the &lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/"&gt;example workflows&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Nodes/graph/flowchart interface to experiment and create complex Stable Diffusion workflows without needing to code anything.&lt;/li&gt; 
 &lt;li&gt;Image Models 
  &lt;ul&gt; 
   &lt;li&gt;SD1.x, SD2.x (&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/unclip/"&gt;unCLIP&lt;/a&gt;)&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/sdxl/"&gt;SDXL&lt;/a&gt;, &lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/sdturbo/"&gt;SDXL Turbo&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/stable_cascade/"&gt;Stable Cascade&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/sd3/"&gt;SD3 and SD3.5&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Pixart Alpha and Sigma&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/aura_flow/"&gt;AuraFlow&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/hunyuan_dit/"&gt;HunyuanDiT&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/flux/"&gt;Flux&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/lumina2/"&gt;Lumina Image 2.0&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/hidream/"&gt;HiDream&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/qwen_image/"&gt;Qwen Image&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/hunyuan_image/"&gt;Hunyuan Image 2.1&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/flux2/"&gt;Flux 2&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/z_image/"&gt;Z Image&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Image Editing Models 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/omnigen/"&gt;Omnigen 2&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/flux/#flux-kontext-image-editing-model"&gt;Flux Kontext&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/hidream/#hidream-e11"&gt;HiDream E1.1&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/qwen_image/#edit-model"&gt;Qwen Image Edit&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Video Models 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/video/"&gt;Stable Video Diffusion&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/mochi/"&gt;Mochi&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/ltxv/"&gt;LTX-Video&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/hunyuan_video/"&gt;Hunyuan Video&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/wan/"&gt;Wan 2.1&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/wan22/"&gt;Wan 2.2&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://docs.comfy.org/tutorials/video/hunyuan/hunyuan-video-1-5"&gt;Hunyuan Video 1.5&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Audio Models 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/audio/"&gt;Stable Audio&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/audio/"&gt;ACE Step&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;3D Models 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://docs.comfy.org/tutorials/3d/hunyuan3D-2"&gt;Hunyuan3D 2.0&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Asynchronous Queue system&lt;/li&gt; 
 &lt;li&gt;Many optimizations: Only re-executes the parts of the workflow that changes between executions.&lt;/li&gt; 
 &lt;li&gt;Smart memory management: can automatically run large models on GPUs with as low as 1GB vram with smart offloading.&lt;/li&gt; 
 &lt;li&gt;Works even if you don't have a GPU with: &lt;code&gt;--cpu&lt;/code&gt; (slow)&lt;/li&gt; 
 &lt;li&gt;Can load ckpt and safetensors: All in one checkpoints or standalone diffusion models, VAEs and CLIP models.&lt;/li&gt; 
 &lt;li&gt;Safe loading of ckpt, pt, pth, etc.. files.&lt;/li&gt; 
 &lt;li&gt;Embeddings/Textual inversion&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/lora/"&gt;Loras (regular, locon and loha)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/hypernetworks/"&gt;Hypernetworks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Loading full workflows (with seeds) from generated PNG, WebP and FLAC files.&lt;/li&gt; 
 &lt;li&gt;Saving/Loading workflows as Json files.&lt;/li&gt; 
 &lt;li&gt;Nodes interface can be used to create complex workflows like one for &lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/2_pass_txt2img/"&gt;Hires fix&lt;/a&gt; or much more advanced ones.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/area_composition/"&gt;Area Composition&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/inpaint/"&gt;Inpainting&lt;/a&gt; with both regular and inpainting models.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/controlnet/"&gt;ControlNet and T2I-Adapter&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/upscale_models/"&gt;Upscale Models (ESRGAN, ESRGAN variants, SwinIR, Swin2SR, etc...)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/gligen/"&gt;GLIGEN&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/model_merging/"&gt;Model Merging&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/lcm/"&gt;LCM models and Loras&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Latent previews with &lt;a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/#how-to-show-high-quality-previews"&gt;TAESD&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Works fully offline: core will never download anything unless you want to.&lt;/li&gt; 
 &lt;li&gt;Optional API nodes to use paid models from external providers through the online &lt;a href="https://docs.comfy.org/tutorials/api-nodes/overview"&gt;Comfy API&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/extra_model_paths.yaml.example"&gt;Config file&lt;/a&gt; to set the search paths for models.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Workflow examples can be found on the &lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/"&gt;Examples page&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Release Process&lt;/h2&gt; 
&lt;p&gt;ComfyUI follows a weekly release cycle targeting Monday but this regularly changes because of model releases or large changes to the codebase. There are three interconnected repositories:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/comfyanonymous/ComfyUI"&gt;ComfyUI Core&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Releases a new stable version (e.g., v0.7.0) roughly every week.&lt;/li&gt; 
   &lt;li&gt;Starting from v0.4.0 patch versions will be used for fixes backported onto the current stable release.&lt;/li&gt; 
   &lt;li&gt;Minor versions will be used for releases off the master branch.&lt;/li&gt; 
   &lt;li&gt;Patch versions may still be used for releases on the master branch in cases where a backport would not make sense.&lt;/li&gt; 
   &lt;li&gt;Commits outside of the stable release tags may be very unstable and break many custom nodes.&lt;/li&gt; 
   &lt;li&gt;Serves as the foundation for the desktop release&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/Comfy-Org/desktop"&gt;ComfyUI Desktop&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Builds a new release using the latest stable core version&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/Comfy-Org/ComfyUI_frontend"&gt;ComfyUI Frontend&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Weekly frontend updates are merged into the core repository&lt;/li&gt; 
   &lt;li&gt;Features are frozen for the upcoming core release&lt;/li&gt; 
   &lt;li&gt;Development continues for the next release cycle&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Shortcuts&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Keybind&lt;/th&gt; 
   &lt;th&gt;Explanation&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;Enter&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Queue up current graph for generation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;Shift&lt;/code&gt; + &lt;code&gt;Enter&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Queue up current graph as first for generation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;Alt&lt;/code&gt; + &lt;code&gt;Enter&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Cancel current generation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;Z&lt;/code&gt;/&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;Y&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Undo/Redo&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;S&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Save workflow&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;O&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Load workflow&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;A&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Select all nodes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Alt &lt;/code&gt;+ &lt;code&gt;C&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Collapse/uncollapse selected nodes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;M&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Mute/unmute selected nodes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;B&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Bypass selected nodes (acts like the node was removed from the graph and the wires reconnected through)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Delete&lt;/code&gt;/&lt;code&gt;Backspace&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Delete selected nodes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;Backspace&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Delete the current graph&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Space&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Move the canvas around when held and moving the cursor&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt;/&lt;code&gt;Shift&lt;/code&gt; + &lt;code&gt;Click&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Add clicked node to selection&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;C&lt;/code&gt;/&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;V&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Copy and paste selected nodes (without maintaining connections to outputs of unselected nodes)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;C&lt;/code&gt;/&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;Shift&lt;/code&gt; + &lt;code&gt;V&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Copy and paste selected nodes (maintaining connections from outputs of unselected nodes to inputs of pasted nodes)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Shift&lt;/code&gt; + &lt;code&gt;Drag&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Move multiple selected nodes at the same time&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;D&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Load default graph&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Alt&lt;/code&gt; + &lt;code&gt;+&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Canvas Zoom in&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Alt&lt;/code&gt; + &lt;code&gt;-&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Canvas Zoom out&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;Shift&lt;/code&gt; + LMB + Vertical drag&lt;/td&gt; 
   &lt;td&gt;Canvas Zoom in/out&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;P&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Pin/Unpin selected nodes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;G&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Group selected nodes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Q&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Toggle visibility of the queue&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;H&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Toggle visibility of history&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;R&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Refresh graph&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;F&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Show/Hide menu&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;.&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Fit view to selection (Whole graph when nothing is selected)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Double-Click LMB&lt;/td&gt; 
   &lt;td&gt;Open node quick search palette&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Shift&lt;/code&gt; + Drag&lt;/td&gt; 
   &lt;td&gt;Move multiple wires at once&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;Alt&lt;/code&gt; + LMB&lt;/td&gt; 
   &lt;td&gt;Disconnect all wires from clicked slot&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;code&gt;Ctrl&lt;/code&gt; can also be replaced with &lt;code&gt;Cmd&lt;/code&gt; instead for macOS users&lt;/p&gt; 
&lt;h1&gt;Installing&lt;/h1&gt; 
&lt;h2&gt;Windows Portable&lt;/h2&gt; 
&lt;p&gt;There is a portable standalone build for Windows that should work for running on Nvidia GPUs or for running on your CPU only on the &lt;a href="https://github.com/comfyanonymous/ComfyUI/releases"&gt;releases page&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;&lt;a href="https://github.com/comfyanonymous/ComfyUI/releases/latest/download/ComfyUI_windows_portable_nvidia.7z"&gt;Direct link to download&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Simply download, extract with &lt;a href="https://7-zip.org"&gt;7-Zip&lt;/a&gt; or with the windows explorer on recent windows versions and run. For smaller models you normally only need to put the checkpoints (the huge ckpt/safetensors files) in: ComfyUI\models\checkpoints but many of the larger models have multiple files. Make sure to follow the instructions to know which subfolder to put them in ComfyUI\models\&lt;/p&gt; 
&lt;p&gt;If you have trouble extracting it, right click the file -&amp;gt; properties -&amp;gt; unblock&lt;/p&gt; 
&lt;p&gt;Update your Nvidia drivers if it doesn't start.&lt;/p&gt; 
&lt;h4&gt;Alternative Downloads:&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://github.com/comfyanonymous/ComfyUI/releases/latest/download/ComfyUI_windows_portable_amd.7z"&gt;Experimental portable for AMD GPUs&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/comfyanonymous/ComfyUI/releases/latest/download/ComfyUI_windows_portable_nvidia_cu128.7z"&gt;Portable with pytorch cuda 12.8 and python 3.12&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/comfyanonymous/ComfyUI/releases/latest/download/ComfyUI_windows_portable_nvidia_cu126.7z"&gt;Portable with pytorch cuda 12.6 and python 3.12&lt;/a&gt; (Supports Nvidia 10 series and older GPUs).&lt;/p&gt; 
&lt;h4&gt;How do I share models between another UI and ComfyUI?&lt;/h4&gt; 
&lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/extra_model_paths.yaml.example"&gt;Config file&lt;/a&gt; to set the search paths for models. In the standalone windows build you can find this file in the ComfyUI directory. Rename this file to extra_model_paths.yaml and edit it with your favorite text editor.&lt;/p&gt; 
&lt;h2&gt;&lt;a href="https://docs.comfy.org/comfy-cli/getting-started"&gt;comfy-cli&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;You can install and start ComfyUI using comfy-cli:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install comfy-cli
comfy install
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Manual Install (Windows, Linux)&lt;/h2&gt; 
&lt;p&gt;Python 3.14 works but you may encounter issues with the torch compile node. The free threaded variant is still missing some dependencies.&lt;/p&gt; 
&lt;p&gt;Python 3.13 is very well supported. If you have trouble with some custom node dependencies on 3.13 you can try 3.12&lt;/p&gt; 
&lt;h3&gt;Instructions:&lt;/h3&gt; 
&lt;p&gt;Git clone this repo.&lt;/p&gt; 
&lt;p&gt;Put your SD checkpoints (the huge ckpt/safetensors files) in: models/checkpoints&lt;/p&gt; 
&lt;p&gt;Put your VAE in: models/vae&lt;/p&gt; 
&lt;h3&gt;AMD GPUs (Linux)&lt;/h3&gt; 
&lt;p&gt;AMD users can install rocm and pytorch with pip if you don't have it already installed, this is the command to install the stable version:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.4&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;This is the command to install the nightly with ROCm 7.0 which might have some performance improvements:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/rocm7.1&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;AMD GPUs (Experimental: Windows and Linux), RDNA 3, 3.5 and 4 only.&lt;/h3&gt; 
&lt;p&gt;These have less hardware support than the builds above but they work on windows. You also need to install the pytorch version specific to your hardware.&lt;/p&gt; 
&lt;p&gt;RDNA 3 (RX 7000 series):&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;pip install --pre torch torchvision torchaudio --index-url https://rocm.nightlies.amd.com/v2/gfx110X-dgpu/&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;RDNA 3.5 (Strix halo/Ryzen AI Max+ 365):&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;pip install --pre torch torchvision torchaudio --index-url https://rocm.nightlies.amd.com/v2/gfx1151/&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;RDNA 4 (RX 9000 series):&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;pip install --pre torch torchvision torchaudio --index-url https://rocm.nightlies.amd.com/v2/gfx120X-all/&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Intel GPUs (Windows and Linux)&lt;/h3&gt; 
&lt;p&gt;Intel Arc GPU users can install native PyTorch with torch.xpu support using pip. More information can be found &lt;a href="https://pytorch.org/docs/main/notes/get_start_xpu.html"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;To install PyTorch xpu, use the following command:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;code&gt;pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/xpu&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;This is the command to install the Pytorch xpu nightly which might have some performance improvements:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/xpu&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;NVIDIA&lt;/h3&gt; 
&lt;p&gt;Nvidia users should install stable pytorch using this command:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu130&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;This is the command to install pytorch nightly instead which might have performance improvements.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu130&lt;/code&gt;&lt;/p&gt; 
&lt;h4&gt;Troubleshooting&lt;/h4&gt; 
&lt;p&gt;If you get the "Torch not compiled with CUDA enabled" error, uninstall torch with:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;pip uninstall torch&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;And install it again with the command above.&lt;/p&gt; 
&lt;h3&gt;Dependencies&lt;/h3&gt; 
&lt;p&gt;Install the dependencies by opening your terminal inside the ComfyUI folder and:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;After this you should have everything installed and can proceed to running ComfyUI.&lt;/p&gt; 
&lt;h3&gt;Others:&lt;/h3&gt; 
&lt;h4&gt;Apple Mac silicon&lt;/h4&gt; 
&lt;p&gt;You can install ComfyUI in Apple Mac silicon (M1 or M2) with any recent macOS version.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install pytorch nightly. For instructions, read the &lt;a href="https://developer.apple.com/metal/pytorch/"&gt;Accelerated PyTorch training on Mac&lt;/a&gt; Apple Developer guide (make sure to install the latest pytorch nightly).&lt;/li&gt; 
 &lt;li&gt;Follow the &lt;a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/#manual-install-windows-linux"&gt;ComfyUI manual installation&lt;/a&gt; instructions for Windows and Linux.&lt;/li&gt; 
 &lt;li&gt;Install the ComfyUI &lt;a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/#dependencies"&gt;dependencies&lt;/a&gt;. If you have another Stable Diffusion UI &lt;a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/#i-already-have-another-ui-for-stable-diffusion-installed-do-i-really-have-to-install-all-of-these-dependencies"&gt;you might be able to reuse the dependencies&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Launch ComfyUI by running &lt;code&gt;python main.py&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Remember to add your models, VAE, LoRAs etc. to the corresponding Comfy folders, as discussed in &lt;a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/#manual-install-windows-linux"&gt;ComfyUI manual installation&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Ascend NPUs&lt;/h4&gt; 
&lt;p&gt;For models compatible with Ascend Extension for PyTorch (torch_npu). To get started, ensure your environment meets the prerequisites outlined on the &lt;a href="https://ascend.github.io/docs/sources/ascend/quick_install.html"&gt;installation&lt;/a&gt; page. Here's a step-by-step guide tailored to your platform and installation method:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Begin by installing the recommended or newer kernel version for Linux as specified in the Installation page of torch-npu, if necessary.&lt;/li&gt; 
 &lt;li&gt;Proceed with the installation of Ascend Basekit, which includes the driver, firmware, and CANN, following the instructions provided for your specific platform.&lt;/li&gt; 
 &lt;li&gt;Next, install the necessary packages for torch-npu by adhering to the platform-specific instructions on the &lt;a href="https://ascend.github.io/docs/sources/pytorch/install.html#pytorch"&gt;Installation&lt;/a&gt; page.&lt;/li&gt; 
 &lt;li&gt;Finally, adhere to the &lt;a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/#manual-install-windows-linux"&gt;ComfyUI manual installation&lt;/a&gt; guide for Linux. Once all components are installed, you can run ComfyUI as described earlier.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Cambricon MLUs&lt;/h4&gt; 
&lt;p&gt;For models compatible with Cambricon Extension for PyTorch (torch_mlu). Here's a step-by-step guide tailored to your platform and installation method:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install the Cambricon CNToolkit by adhering to the platform-specific instructions on the &lt;a href="https://www.cambricon.com/docs/sdk_1.15.0/cntoolkit_3.7.2/cntoolkit_install_3.7.2/index.html"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Next, install the PyTorch(torch_mlu) following the instructions on the &lt;a href="https://www.cambricon.com/docs/sdk_1.15.0/cambricon_pytorch_1.17.0/user_guide_1.9/index.html"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Launch ComfyUI by running &lt;code&gt;python main.py&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Iluvatar Corex&lt;/h4&gt; 
&lt;p&gt;For models compatible with Iluvatar Extension for PyTorch. Here's a step-by-step guide tailored to your platform and installation method:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install the Iluvatar Corex Toolkit by adhering to the platform-specific instructions on the &lt;a href="https://support.iluvatar.com/#/DocumentCentre?id=1&amp;amp;nameCenter=2&amp;amp;productId=520117912052801536"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Launch ComfyUI by running &lt;code&gt;python main.py&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;&lt;a href="https://github.com/Comfy-Org/ComfyUI-Manager/tree/manager-v4"&gt;ComfyUI-Manager&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;ComfyUI-Manager&lt;/strong&gt; is an extension that allows you to easily install, update, and manage custom nodes for ComfyUI.&lt;/p&gt; 
&lt;h3&gt;Setup&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Install the manager dependencies:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r manager_requirements.txt
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Enable the manager with the &lt;code&gt;--enable-manager&lt;/code&gt; flag when running ComfyUI:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;python main.py --enable-manager
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Command Line Options&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Flag&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--enable-manager&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Enable ComfyUI-Manager&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--enable-manager-legacy-ui&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Use the legacy manager UI instead of the new UI (requires &lt;code&gt;--enable-manager&lt;/code&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--disable-manager-ui&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Disable the manager UI and endpoints while keeping background features like security checks and scheduled installation completion (requires &lt;code&gt;--enable-manager&lt;/code&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;Running&lt;/h1&gt; 
&lt;p&gt;&lt;code&gt;python main.py&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;For AMD cards not officially supported by ROCm&lt;/h3&gt; 
&lt;p&gt;Try running it with this command if you have issues:&lt;/p&gt; 
&lt;p&gt;For 6700, 6600 and maybe other RDNA2 or older: &lt;code&gt;HSA_OVERRIDE_GFX_VERSION=10.3.0 python main.py&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;For AMD 7600 and maybe other RDNA3 cards: &lt;code&gt;HSA_OVERRIDE_GFX_VERSION=11.0.0 python main.py&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;AMD ROCm Tips&lt;/h3&gt; 
&lt;p&gt;You can enable experimental memory efficient attention on recent pytorch in ComfyUI on some AMD GPUs using this command, it should already be enabled by default on RDNA3. If this improves speed for you on latest pytorch on your GPU please report it so that I can enable it by default.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL=1 python main.py --use-pytorch-cross-attention&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;You can also try setting this env variable &lt;code&gt;PYTORCH_TUNABLEOP_ENABLED=1&lt;/code&gt; which might speed things up at the cost of a very slow initial run.&lt;/p&gt; 
&lt;h1&gt;Notes&lt;/h1&gt; 
&lt;p&gt;Only parts of the graph that have an output with all the correct inputs will be executed.&lt;/p&gt; 
&lt;p&gt;Only parts of the graph that change from each execution to the next will be executed, if you submit the same graph twice only the first will be executed. If you change the last part of the graph only the part you changed and the part that depends on it will be executed.&lt;/p&gt; 
&lt;p&gt;Dragging a generated png on the webpage or loading one will give you the full workflow including seeds that were used to create it.&lt;/p&gt; 
&lt;p&gt;You can use () to change emphasis of a word or phrase like: (good code:1.2) or (bad code:0.8). The default emphasis for () is 1.1. To use () characters in your actual prompt escape them like \( or \).&lt;/p&gt; 
&lt;p&gt;You can use {day|night}, for wildcard/dynamic prompts. With this syntax "{wild|card|test}" will be randomly replaced by either "wild", "card" or "test" by the frontend every time you queue the prompt. To use {} characters in your actual prompt escape them like: \{ or \}.&lt;/p&gt; 
&lt;p&gt;Dynamic prompts also support C-style comments, like &lt;code&gt;// comment&lt;/code&gt; or &lt;code&gt;/* comment */&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;To use a textual inversion concepts/embeddings in a text prompt put them in the models/embeddings directory and use them in the CLIPTextEncode node like this (you can omit the .pt extension):&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;embedding:embedding_filename.pt&lt;/code&gt;&lt;/p&gt; 
&lt;h2&gt;How to show high-quality previews?&lt;/h2&gt; 
&lt;p&gt;Use &lt;code&gt;--preview-method auto&lt;/code&gt; to enable previews.&lt;/p&gt; 
&lt;p&gt;The default installation includes a fast latent preview method that's low-resolution. To enable higher-quality previews with &lt;a href="https://github.com/madebyollin/taesd"&gt;TAESD&lt;/a&gt;, download the &lt;a href="https://github.com/madebyollin/taesd/"&gt;taesd_decoder.pth, taesdxl_decoder.pth, taesd3_decoder.pth and taef1_decoder.pth&lt;/a&gt; and place them in the &lt;code&gt;models/vae_approx&lt;/code&gt; folder. Once they're installed, restart ComfyUI and launch it with &lt;code&gt;--preview-method taesd&lt;/code&gt; to enable high-quality previews.&lt;/p&gt; 
&lt;h2&gt;How to use TLS/SSL?&lt;/h2&gt; 
&lt;p&gt;Generate a self-signed certificate (not appropriate for shared/production use) and key by running the command: &lt;code&gt;openssl req -x509 -newkey rsa:4096 -keyout key.pem -out cert.pem -sha256 -days 3650 -nodes -subj "/C=XX/ST=StateName/L=CityName/O=CompanyName/OU=CompanySectionName/CN=CommonNameOrHostname"&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Use &lt;code&gt;--tls-keyfile key.pem --tls-certfile cert.pem&lt;/code&gt; to enable TLS/SSL, the app will now be accessible with &lt;code&gt;https://...&lt;/code&gt; instead of &lt;code&gt;http://...&lt;/code&gt;.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Note: Windows users can use &lt;a href="https://github.com/alexisrolland/docker-openssl"&gt;alexisrolland/docker-openssl&lt;/a&gt; or one of the &lt;a href="https://wiki.openssl.org/index.php/Binaries"&gt;3rd party binary distributions&lt;/a&gt; to run the command example above. &lt;br /&gt;&lt;br /&gt;If you use a container, note that the volume mount &lt;code&gt;-v&lt;/code&gt; can be a relative path so &lt;code&gt;... -v ".\:/openssl-certs" ...&lt;/code&gt; would create the key &amp;amp; cert files in the current directory of your command prompt or powershell terminal.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Support and dev channel&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://comfy.org/discord"&gt;Discord&lt;/a&gt;: Try the #help or #feedback channels.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://app.element.io/#/room/%23comfyui_space%3Amatrix.org"&gt;Matrix space: #comfyui_space:matrix.org&lt;/a&gt; (it's like discord but open source).&lt;/p&gt; 
&lt;p&gt;See also: &lt;a href="https://www.comfy.org/"&gt;https://www.comfy.org/&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Frontend Development&lt;/h2&gt; 
&lt;p&gt;As of August 15, 2024, we have transitioned to a new frontend, which is now hosted in a separate repository: &lt;a href="https://github.com/Comfy-Org/ComfyUI_frontend"&gt;ComfyUI Frontend&lt;/a&gt;. This repository now hosts the compiled JS (from TS/Vue) under the &lt;code&gt;web/&lt;/code&gt; directory.&lt;/p&gt; 
&lt;h3&gt;Reporting Issues and Requesting Features&lt;/h3&gt; 
&lt;p&gt;For any bugs, issues, or feature requests related to the frontend, please use the &lt;a href="https://github.com/Comfy-Org/ComfyUI_frontend"&gt;ComfyUI Frontend repository&lt;/a&gt;. This will help us manage and address frontend-specific concerns more efficiently.&lt;/p&gt; 
&lt;h3&gt;Using the Latest Frontend&lt;/h3&gt; 
&lt;p&gt;The new frontend is now the default for ComfyUI. However, please note:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;The frontend in the main ComfyUI repository is updated fortnightly.&lt;/li&gt; 
 &lt;li&gt;Daily releases are available in the separate frontend repository.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;To use the most up-to-date frontend version:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;For the latest daily release, launch ComfyUI with this command line argument:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;--front-end-version Comfy-Org/ComfyUI_frontend@latest
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;For a specific version, replace &lt;code&gt;latest&lt;/code&gt; with the desired version number:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;--front-end-version Comfy-Org/ComfyUI_frontend@1.2.2
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;This approach allows you to easily switch between the stable fortnightly release and the cutting-edge daily updates, or even specific versions for testing purposes.&lt;/p&gt; 
&lt;h3&gt;Accessing the Legacy Frontend&lt;/h3&gt; 
&lt;p&gt;If you need to use the legacy frontend for any reason, you can access it using the following command line argument:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;--front-end-version Comfy-Org/ComfyUI_legacy_frontend@latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will use a snapshot of the legacy frontend preserved in the &lt;a href="https://github.com/Comfy-Org/ComfyUI_legacy_frontend"&gt;ComfyUI Legacy Frontend repository&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;QA&lt;/h1&gt; 
&lt;h3&gt;Which GPU should I buy for this?&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/comfyanonymous/ComfyUI/wiki/Which-GPU-should-I-buy-for-ComfyUI"&gt;See this page for some recommendations&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>