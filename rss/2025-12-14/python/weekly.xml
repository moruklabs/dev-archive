<rss version="2.0">
  <channel>
    <title>GitHub Python Weekly Trending</title>
    <description>Weekly Trending of Python in GitHub</description>
    <pubDate>Sat, 13 Dec 2025 01:45:35 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>strands-agents/sdk-python</title>
      <link>https://github.com/strands-agents/sdk-python</link>
      <description>&lt;p&gt;A model-driven approach to building AI agents in just a few lines of code.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;div&gt; 
  &lt;a href="https://strandsagents.com"&gt; &lt;img src="https://strandsagents.com/latest/assets/logo-github.svg?sanitize=true" alt="Strands Agents" width="55px" height="105px" /&gt; &lt;/a&gt; 
 &lt;/div&gt; 
 &lt;h1&gt; Strands Agents &lt;/h1&gt; 
 &lt;h2&gt; A model-driven approach to building AI agents in just a few lines of code. &lt;/h2&gt; 
 &lt;div align="center"&gt; 
  &lt;a href="https://github.com/strands-agents/sdk-python/graphs/commit-activity"&gt;&lt;img alt="GitHub commit activity" src="https://img.shields.io/github/commit-activity/m/strands-agents/sdk-python" /&gt;&lt;/a&gt; 
  &lt;a href="https://github.com/strands-agents/sdk-python/issues"&gt;&lt;img alt="GitHub open issues" src="https://img.shields.io/github/issues/strands-agents/sdk-python" /&gt;&lt;/a&gt; 
  &lt;a href="https://github.com/strands-agents/sdk-python/pulls"&gt;&lt;img alt="GitHub open pull requests" src="https://img.shields.io/github/issues-pr/strands-agents/sdk-python" /&gt;&lt;/a&gt; 
  &lt;a href="https://github.com/strands-agents/sdk-python/raw/main/LICENSE"&gt;&lt;img alt="License" src="https://img.shields.io/github/license/strands-agents/sdk-python" /&gt;&lt;/a&gt; 
  &lt;a href="https://pypi.org/project/strands-agents/"&gt;&lt;img alt="PyPI version" src="https://img.shields.io/pypi/v/strands-agents" /&gt;&lt;/a&gt; 
  &lt;a href="https://python.org"&gt;&lt;img alt="Python versions" src="https://img.shields.io/pypi/pyversions/strands-agents" /&gt;&lt;/a&gt; 
 &lt;/div&gt; 
 &lt;p&gt; &lt;a href="https://strandsagents.com/"&gt;Documentation&lt;/a&gt; â—† &lt;a href="https://github.com/strands-agents/samples"&gt;Samples&lt;/a&gt; â—† &lt;a href="https://github.com/strands-agents/sdk-python"&gt;Python SDK&lt;/a&gt; â—† &lt;a href="https://github.com/strands-agents/tools"&gt;Tools&lt;/a&gt; â—† &lt;a href="https://github.com/strands-agents/agent-builder"&gt;Agent Builder&lt;/a&gt; â—† &lt;a href="https://github.com/strands-agents/mcp-server"&gt;MCP Server&lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;Strands Agents is a simple yet powerful SDK that takes a model-driven approach to building and running AI agents. From simple conversational assistants to complex autonomous workflows, from local development to production deployment, Strands Agents scales with your needs.&lt;/p&gt; 
&lt;h2&gt;Feature Overview&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Lightweight &amp;amp; Flexible&lt;/strong&gt;: Simple agent loop that just works and is fully customizable&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Model Agnostic&lt;/strong&gt;: Support for Amazon Bedrock, Anthropic, Gemini, LiteLLM, Llama, Ollama, OpenAI, Writer, and custom providers&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Advanced Capabilities&lt;/strong&gt;: Multi-agent systems, autonomous agents, and streaming support&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Built-in MCP&lt;/strong&gt;: Native support for Model Context Protocol (MCP) servers, enabling access to thousands of pre-built tools&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install Strands Agents
pip install strands-agents strands-agents-tools
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from strands import Agent
from strands_tools import calculator
agent = Agent(tools=[calculator])
agent("What is the square root of 1764")
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: For the default Amazon Bedrock model provider, you'll need AWS credentials configured and model access enabled for Claude 4 Sonnet in the us-west-2 region. See the &lt;a href="https://strandsagents.com/"&gt;Quickstart Guide&lt;/a&gt; for details on configuring other model providers.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;Ensure you have Python 3.10+ installed, then:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Create and activate virtual environment
python -m venv .venv
source .venv/bin/activate  # On Windows use: .venv\Scripts\activate

# Install Strands and tools
pip install strands-agents strands-agents-tools
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Features at a Glance&lt;/h2&gt; 
&lt;h3&gt;Python-Based Tools&lt;/h3&gt; 
&lt;p&gt;Easily build tools using Python decorators:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from strands import Agent, tool

@tool
def word_count(text: str) -&amp;gt; int:
    """Count words in text.

    This docstring is used by the LLM to understand the tool's purpose.
    """
    return len(text.split())

agent = Agent(tools=[word_count])
response = agent("How many words are in this sentence?")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Hot Reloading from Directory:&lt;/strong&gt; Enable automatic tool loading and reloading from the &lt;code&gt;./tools/&lt;/code&gt; directory:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from strands import Agent

# Agent will watch ./tools/ directory for changes
agent = Agent(load_tools_from_directory=True)
response = agent("Use any tools you find in the tools directory")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;MCP Support&lt;/h3&gt; 
&lt;p&gt;Seamlessly integrate Model Context Protocol (MCP) servers:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from strands import Agent
from strands.tools.mcp import MCPClient
from mcp import stdio_client, StdioServerParameters

aws_docs_client = MCPClient(
    lambda: stdio_client(StdioServerParameters(command="uvx", args=["awslabs.aws-documentation-mcp-server@latest"]))
)

with aws_docs_client:
   agent = Agent(tools=aws_docs_client.list_tools_sync())
   response = agent("Tell me about Amazon Bedrock and how to use it with Python")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Multiple Model Providers&lt;/h3&gt; 
&lt;p&gt;Support for various model providers:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from strands import Agent
from strands.models import BedrockModel
from strands.models.ollama import OllamaModel
from strands.models.llamaapi import LlamaAPIModel
from strands.models.gemini import GeminiModel
from strands.models.llamacpp import LlamaCppModel

# Bedrock
bedrock_model = BedrockModel(
  model_id="us.amazon.nova-pro-v1:0",
  temperature=0.3,
  streaming=True, # Enable/disable streaming
)
agent = Agent(model=bedrock_model)
agent("Tell me about Agentic AI")

# Google Gemini
gemini_model = GeminiModel(
  client_args={
    "api_key": "your_gemini_api_key",
  },
  model_id="gemini-2.5-flash",
  params={"temperature": 0.7}
)
agent = Agent(model=gemini_model)
agent("Tell me about Agentic AI")

# Ollama
ollama_model = OllamaModel(
  host="http://localhost:11434",
  model_id="llama3"
)
agent = Agent(model=ollama_model)
agent("Tell me about Agentic AI")

# Llama API
llama_model = LlamaAPIModel(
    model_id="Llama-4-Maverick-17B-128E-Instruct-FP8",
)
agent = Agent(model=llama_model)
response = agent("Tell me about Agentic AI")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Built-in providers:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/latest/user-guide/concepts/model-providers/amazon-bedrock/"&gt;Amazon Bedrock&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/latest/user-guide/concepts/model-providers/anthropic/"&gt;Anthropic&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/latest/user-guide/concepts/model-providers/gemini/"&gt;Gemini&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/latest/user-guide/concepts/model-providers/cohere/"&gt;Cohere&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/latest/user-guide/concepts/model-providers/litellm/"&gt;LiteLLM&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/latest/user-guide/concepts/model-providers/llamacpp/"&gt;llama.cpp&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/latest/user-guide/concepts/model-providers/llamaapi/"&gt;LlamaAPI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/latest/user-guide/concepts/model-providers/mistral/"&gt;MistralAI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/latest/user-guide/concepts/model-providers/ollama/"&gt;Ollama&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/latest/user-guide/concepts/model-providers/openai/"&gt;OpenAI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/latest/user-guide/concepts/model-providers/sagemaker/"&gt;SageMaker&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/latest/user-guide/concepts/model-providers/writer/"&gt;Writer&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Custom providers can be implemented using &lt;a href="https://strandsagents.com/latest/user-guide/concepts/model-providers/custom_model_provider/"&gt;Custom Providers&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Example tools&lt;/h3&gt; 
&lt;p&gt;Strands offers an optional strands-agents-tools package with pre-built tools for quick experimentation:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from strands import Agent
from strands_tools import calculator
agent = Agent(tools=[calculator])
agent("What is the square root of 1764")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;It's also available on GitHub via &lt;a href="https://github.com/strands-agents/tools"&gt;strands-agents/tools&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Bidirectional Streaming&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;âš ï¸ Experimental Feature&lt;/strong&gt;: Bidirectional streaming is currently in experimental status. APIs may change in future releases as we refine the feature based on user feedback and evolving model capabilities.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Build real-time voice and audio conversations with persistent streaming connections. Unlike traditional request-response patterns, bidirectional streaming maintains long-running conversations where users can interrupt, provide continuous input, and receive real-time audio responses. Get started with your first BidiAgent by following the &lt;a href="https://strandsagents.com/latest/documentation/docs/user-guide/concepts/experimental/bidirectional-streaming/quickstart"&gt;Quickstart&lt;/a&gt; guide.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Supported Model Providers:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Amazon Nova Sonic (&lt;code&gt;amazon.nova-sonic-v1:0&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Google Gemini Live (&lt;code&gt;gemini-2.5-flash-native-audio-preview-09-2025&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;OpenAI Realtime API (&lt;code&gt;gpt-realtime&lt;/code&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Quick Example:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import asyncio
from strands.experimental.bidi import BidiAgent
from strands.experimental.bidi.models import BidiNovaSonicModel
from strands.experimental.bidi.io import BidiAudioIO, BidiTextIO
from strands.experimental.bidi.tools import stop_conversation
from strands_tools import calculator

async def main():
    # Create bidirectional agent with audio model
    model = BidiNovaSonicModel()
    agent = BidiAgent(model=model, tools=[calculator, stop_conversation])

    # Setup audio and text I/O
    audio_io = BidiAudioIO()
    text_io = BidiTextIO()

    # Run with real-time audio streaming
    # Say "stop conversation" to gracefully end the conversation
    await agent.run(
        inputs=[audio_io.input()],
        outputs=[audio_io.output(), text_io.output()]
    )

if __name__ == "__main__":
    asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Configuration Options:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Configure audio settings
model = BidiNovaSonicModel(
    provider_config={
        "audio": {
            "input_rate": 16000,
            "output_rate": 16000,
            "voice": "matthew"
        },
        "inference": {
            "max_tokens": 2048,
            "temperature": 0.7
        }
    }
)

# Configure I/O devices
audio_io = BidiAudioIO(
    input_device_index=0,  # Specific microphone
    output_device_index=1,  # Specific speaker
    input_buffer_size=10,
    output_buffer_size=10
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;For detailed guidance &amp;amp; examples, explore our documentation:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/"&gt;User Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/latest/user-guide/quickstart/"&gt;Quick Start Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/latest/user-guide/concepts/agents/agent-loop/"&gt;Agent Loop&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/latest/examples/"&gt;Examples&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/latest/api-reference/agent/"&gt;API Reference&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/latest/user-guide/deploy/operating-agents-in-production/"&gt;Production &amp;amp; Deployment Guide&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing â¤ï¸&lt;/h2&gt; 
&lt;p&gt;We welcome contributions! See our &lt;a href="https://raw.githubusercontent.com/strands-agents/sdk-python/main/CONTRIBUTING.md"&gt;Contributing Guide&lt;/a&gt; for details on:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Reporting bugs &amp;amp; features&lt;/li&gt; 
 &lt;li&gt;Development setup&lt;/li&gt; 
 &lt;li&gt;Contributing via Pull Requests&lt;/li&gt; 
 &lt;li&gt;Code of Conduct&lt;/li&gt; 
 &lt;li&gt;Reporting of security issues&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the Apache License 2.0 - see the &lt;a href="https://raw.githubusercontent.com/strands-agents/sdk-python/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;h2&gt;Security&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/strands-agents/sdk-python/main/CONTRIBUTING.md#security-issue-notifications"&gt;CONTRIBUTING&lt;/a&gt; for more information.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>MODSetter/SurfSense</title>
      <link>https://github.com/MODSetter/SurfSense</link>
      <description>&lt;p&gt;Open source alternative to NotebookLM, Perplexity, and Glean. Connects to search engines, Slack, Linear, Jira, ClickUp, Notion, YouTube, GitHub, Discord, and more. Join our Discord: https://discord.gg/ejRNvftDp9&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/e236b764-0ddc-42ff-a1f1-8fbb3d2e0e65" alt="new_header" /&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://discord.gg/ejRNvftDp9"&gt; &lt;img src="https://img.shields.io/discord/1359368468260192417" alt="Discord" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/MODSetter/SurfSense/main/README.md"&gt;English&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/MODSetter/SurfSense/main/README.zh-CN.md"&gt;ç®€ä½“ä¸­æ–‡&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h1&gt;SurfSense&lt;/h1&gt; 
&lt;p&gt;While tools like NotebookLM and Perplexity are impressive and highly effective for conducting research on any topic/query, SurfSense elevates this capability by integrating with your personal knowledge base. It is a highly customizable AI research agent, connected to external sources such as Search Engines (SearxNG, Tavily, LinkUp), Slack, Linear, Jira, ClickUp, Confluence, BookStack, Gmail, Notion, YouTube, GitHub, Discord, Airtable, Google Calendar, Luma, Elasticsearch and more to come.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://trendshift.io/repositories/13606" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/13606" alt="MODSetter%2FSurfSense | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;h1&gt;Video&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/d9221908-e0de-4b2f-ac3a-691cf4b202da"&gt;https://github.com/user-attachments/assets/d9221908-e0de-4b2f-ac3a-691cf4b202da&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Podcast Sample&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/a0a16566-6967-4374-ac51-9b3e07fbecd7"&gt;https://github.com/user-attachments/assets/a0a16566-6967-4374-ac51-9b3e07fbecd7&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;h3&gt;ğŸ’¡ &lt;strong&gt;Idea&lt;/strong&gt;:&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Have your own highly customizable private NotebookLM and Perplexity integrated with external sources.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ“ &lt;strong&gt;Multiple File Format Uploading Support&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Save content from your own personal files &lt;em&gt;(Documents, images, videos and supports &lt;strong&gt;50+ file extensions&lt;/strong&gt;)&lt;/em&gt; to your own personal knowledge base .&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ” &lt;strong&gt;Powerful Search&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Quickly research or find anything in your saved content .&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ’¬ &lt;strong&gt;Chat with your Saved Content&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Interact in Natural Language and get cited answers.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ“„ &lt;strong&gt;Cited Answers&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Get Cited answers just like Perplexity.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ”” &lt;strong&gt;Privacy &amp;amp; Local LLM Support&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Works Flawlessly with Ollama local LLMs.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ  &lt;strong&gt;Self Hostable&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Open source and easy to deploy locally.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ‘¥ &lt;strong&gt;Team Collaboration with RBAC&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Role-Based Access Control for Search Spaces&lt;/li&gt; 
 &lt;li&gt;Invite team members with customizable roles (Owner, Admin, Editor, Viewer)&lt;/li&gt; 
 &lt;li&gt;Granular permissions for documents, chats, connectors, and settings&lt;/li&gt; 
 &lt;li&gt;Share knowledge bases securely within your organization&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ™ï¸ Podcasts&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Blazingly fast podcast generation agent. (Creates a 3-minute podcast in under 20 seconds.)&lt;/li&gt; 
 &lt;li&gt;Convert your chat conversations into engaging audio content&lt;/li&gt; 
 &lt;li&gt;Support for local TTS providers (Kokoro TTS)&lt;/li&gt; 
 &lt;li&gt;Support for multiple TTS providers (OpenAI, Azure, Google Vertex AI)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ“Š &lt;strong&gt;Advanced RAG Techniques&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Supports 100+ LLM's&lt;/li&gt; 
 &lt;li&gt;Supports 6000+ Embedding Models.&lt;/li&gt; 
 &lt;li&gt;Supports all major Rerankers (Pinecode, Cohere, Flashrank etc)&lt;/li&gt; 
 &lt;li&gt;Uses Hierarchical Indices (2 tiered RAG setup).&lt;/li&gt; 
 &lt;li&gt;Utilizes Hybrid Search (Semantic + Full Text Search combined with Reciprocal Rank Fusion).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;â„¹ï¸ &lt;strong&gt;External Sources&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Search Engines (Tavily, LinkUp)&lt;/li&gt; 
 &lt;li&gt;SearxNG (self-hosted instances)&lt;/li&gt; 
 &lt;li&gt;Slack&lt;/li&gt; 
 &lt;li&gt;Linear&lt;/li&gt; 
 &lt;li&gt;Jira&lt;/li&gt; 
 &lt;li&gt;ClickUp&lt;/li&gt; 
 &lt;li&gt;Confluence&lt;/li&gt; 
 &lt;li&gt;BookStack&lt;/li&gt; 
 &lt;li&gt;Notion&lt;/li&gt; 
 &lt;li&gt;Gmail&lt;/li&gt; 
 &lt;li&gt;Youtube Videos&lt;/li&gt; 
 &lt;li&gt;GitHub&lt;/li&gt; 
 &lt;li&gt;Discord&lt;/li&gt; 
 &lt;li&gt;Airtable&lt;/li&gt; 
 &lt;li&gt;Google Calendar&lt;/li&gt; 
 &lt;li&gt;Luma&lt;/li&gt; 
 &lt;li&gt;Elasticsearch&lt;/li&gt; 
 &lt;li&gt;and more to come.....&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ“„ &lt;strong&gt;Supported File Extensions&lt;/strong&gt;&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;ETL Service&lt;/th&gt; 
   &lt;th&gt;Formats&lt;/th&gt; 
   &lt;th&gt;Notes&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;LlamaCloud&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;50+ formats&lt;/td&gt; 
   &lt;td&gt;Documents, presentations, spreadsheets, images&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Unstructured&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;34+ formats&lt;/td&gt; 
   &lt;td&gt;Core formats + email support&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Docling&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Core formats&lt;/td&gt; 
   &lt;td&gt;Local processing, no API key required&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Audio/Video&lt;/strong&gt; (via STT Service): &lt;code&gt;.mp3&lt;/code&gt;, &lt;code&gt;.wav&lt;/code&gt;, &lt;code&gt;.mp4&lt;/code&gt;, &lt;code&gt;.webm&lt;/code&gt;, etc.&lt;/p&gt; 
&lt;h3&gt;ğŸ”– Cross Browser Extension&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;The SurfSense extension can be used to save any webpage you like.&lt;/li&gt; 
 &lt;li&gt;Its main usecase is to save any webpages protected beyond authentication.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;FEATURE REQUESTS AND FUTURE&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;SurfSense is actively being developed.&lt;/strong&gt; While it's not yet production-ready, you can help us speed up the process.&lt;/p&gt; 
&lt;p&gt;Join the &lt;a href="https://discord.gg/ejRNvftDp9"&gt;SurfSense Discord&lt;/a&gt; and help shape the future of SurfSense!&lt;/p&gt; 
&lt;h2&gt;ğŸš€ Roadmap&lt;/h2&gt; 
&lt;p&gt;Stay up to date with our development progress and upcoming features!&lt;br /&gt; Check out our public roadmap and contribute your ideas or feedback:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;ğŸ“‹ Roadmap Discussion:&lt;/strong&gt; &lt;a href="https://github.com/MODSetter/SurfSense/discussions/565"&gt;SurfSense 2025-2026 Roadmap: Deep Agents, Real-Time Collaboration &amp;amp; MCP Servers&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;ğŸ“Š Kanban Board:&lt;/strong&gt; &lt;a href="https://github.com/users/MODSetter/projects/3"&gt;SurfSense Project Board&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;How to get started?&lt;/h2&gt; 
&lt;h3&gt;Quick Start with Docker ğŸ³&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] For production deployments, use the full &lt;a href="https://www.surfsense.com/docs/docker-installation"&gt;Docker Compose setup&lt;/a&gt; which offers more control and scalability.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;Linux/macOS:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run -d -p 3000:3000 -p 8000:8000 \
  -v surfsense-data:/data \
  --name surfsense \
  --restart unless-stopped \
  ghcr.io/modsetter/surfsense:latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Windows (PowerShell):&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-powershell"&gt;docker run -d -p 3000:3000 -p 8000:8000 `
  -v surfsense-data:/data `
  --name surfsense `
  --restart unless-stopped `
  ghcr.io/modsetter/surfsense:latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;With Custom Configuration (e.g., OpenAI Embeddings):&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run -d -p 3000:3000 -p 8000:8000 \
  -v surfsense-data:/data \
  -e EMBEDDING_MODEL=openai://text-embedding-ada-002 \
  -e OPENAI_API_KEY=your_openai_api_key \
  --name surfsense \
  --restart unless-stopped \
  ghcr.io/modsetter/surfsense:latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After starting, access SurfSense at:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Frontend&lt;/strong&gt;: &lt;a href="http://localhost:3000"&gt;http://localhost:3000&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Backend API&lt;/strong&gt;: &lt;a href="http://localhost:8000"&gt;http://localhost:8000&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;API Docs&lt;/strong&gt;: &lt;a href="http://localhost:8000/docs"&gt;http://localhost:8000/docs&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Useful Commands:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker logs -f surfsense      # View logs
docker stop surfsense         # Stop
docker start surfsense        # Start
docker rm surfsense           # Remove (data preserved in volume)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Installation Options&lt;/h3&gt; 
&lt;p&gt;SurfSense provides multiple options to get started:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://www.surfsense.com/login"&gt;SurfSense Cloud&lt;/a&gt;&lt;/strong&gt; - The easiest way to try SurfSense without any setup.&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;No installation required&lt;/li&gt; 
   &lt;li&gt;Instant access to all features&lt;/li&gt; 
   &lt;li&gt;Perfect for getting started quickly&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Quick Start Docker (Above)&lt;/strong&gt; - Single command to get SurfSense running locally.&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;All-in-one image with PostgreSQL, Redis, and all services bundled&lt;/li&gt; 
   &lt;li&gt;Perfect for evaluation, development, and small deployments&lt;/li&gt; 
   &lt;li&gt;Data persisted via Docker volume&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://www.surfsense.com/docs/docker-installation"&gt;Docker Compose (Production)&lt;/a&gt;&lt;/strong&gt; - Full stack deployment with separate services.&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Includes pgAdmin for database management through a web UI&lt;/li&gt; 
   &lt;li&gt;Supports environment variable customization via &lt;code&gt;.env&lt;/code&gt; file&lt;/li&gt; 
   &lt;li&gt;Flexible deployment options (full stack or core services only)&lt;/li&gt; 
   &lt;li&gt;Better for production with separate scaling of services&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://www.surfsense.com/docs/manual-installation"&gt;Manual Installation&lt;/a&gt;&lt;/strong&gt; - For users who prefer more control over their setup or need to customize their deployment.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Docker and manual installation guides include detailed OS-specific instructions for Windows, macOS, and Linux.&lt;/p&gt; 
&lt;p&gt;Before self-hosting installation, make sure to complete the &lt;a href="https://www.surfsense.com/docs/"&gt;prerequisite setup steps&lt;/a&gt; including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Auth setup (optional - defaults to LOCAL auth)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;File Processing ETL Service&lt;/strong&gt; (optional - defaults to Docling): 
  &lt;ul&gt; 
   &lt;li&gt;Docling (default, local processing, no API key required, supports PDF, Office docs, images, HTML, CSV)&lt;/li&gt; 
   &lt;li&gt;Unstructured.io API key (supports 34+ formats)&lt;/li&gt; 
   &lt;li&gt;LlamaIndex API key (enhanced parsing, supports 50+ formats)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Other API keys as needed for your use case&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Screenshots&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Research Agent&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/e22c5d86-f511-4c72-8c50-feba0c1561b4" alt="updated_researcher" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Search Spaces&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/e254c38c-f937-44b6-9e9d-770db583d099" alt="search_spaces" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Manage Documents&lt;/strong&gt; &lt;img src="https://github.com/user-attachments/assets/7001e306-eb06-4009-89c6-8fadfdc3fc4d" alt="documents" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Podcast Agent&lt;/strong&gt; &lt;img src="https://github.com/user-attachments/assets/6cb82ffd-9e14-4172-bc79-67faf34c4c1c" alt="podcasts" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Agent Chat&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/bb352d52-1c6d-4020-926b-722d0b98b491" alt="git_chat" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Browser Extension&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/1f042b7a-6349-422b-94fb-d40d0df16c40" alt="ext1" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/a9b9f1aa-2677-404d-b0a0-c1b2dddf24a7" alt="ext2" /&gt;&lt;/p&gt; 
&lt;h2&gt;Tech Stack&lt;/h2&gt; 
&lt;h3&gt;&lt;strong&gt;BackEnd&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;FastAPI&lt;/strong&gt;: Modern, fast web framework for building APIs with Python&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;PostgreSQL with pgvector&lt;/strong&gt;: Database with vector search capabilities for similarity searches&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;SQLAlchemy&lt;/strong&gt;: SQL toolkit and ORM (Object-Relational Mapping) for database interactions&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Alembic&lt;/strong&gt;: A database migrations tool for SQLAlchemy.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;FastAPI Users&lt;/strong&gt;: Authentication and user management with JWT and OAuth support&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;LangGraph&lt;/strong&gt;: Framework for developing AI-agents.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;LangChain&lt;/strong&gt;: Framework for developing AI-powered applications.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;LLM Integration&lt;/strong&gt;: Integration with LLM models through LiteLLM&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Rerankers&lt;/strong&gt;: Advanced result ranking for improved search relevance&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Hybrid Search&lt;/strong&gt;: Combines vector similarity and full-text search for optimal results using Reciprocal Rank Fusion (RRF)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Vector Embeddings&lt;/strong&gt;: Document and text embeddings for semantic search&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;pgvector&lt;/strong&gt;: PostgreSQL extension for efficient vector similarity operations&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Redis&lt;/strong&gt;: In-memory data structure store used as message broker and result backend for Celery&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Celery&lt;/strong&gt;: Distributed task queue for handling asynchronous background jobs (document processing, podcast generation, etc.)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Flower&lt;/strong&gt;: Real-time monitoring and administration tool for Celery task queues&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Chonkie&lt;/strong&gt;: Advanced document chunking and embedding library&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Uses &lt;code&gt;AutoEmbeddings&lt;/code&gt; for flexible embedding model selection&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;LateChunker&lt;/code&gt; for optimized document chunking based on embedding model's max sequence length&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h3&gt;&lt;strong&gt;FrontEnd&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Next.js 15.2.3&lt;/strong&gt;: React framework featuring App Router, server components, automatic code-splitting, and optimized rendering.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;React 19.0.0&lt;/strong&gt;: JavaScript library for building user interfaces.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;TypeScript&lt;/strong&gt;: Static type-checking for JavaScript, enhancing code quality and developer experience.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Vercel AI SDK Kit UI Stream Protocol&lt;/strong&gt;: To create scalable chat UI.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Tailwind CSS 4.x&lt;/strong&gt;: Utility-first CSS framework for building custom UI designs.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Shadcn&lt;/strong&gt;: Headless components library.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Lucide React&lt;/strong&gt;: Icon set implemented as React components.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Framer Motion&lt;/strong&gt;: Animation library for React.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Sonner&lt;/strong&gt;: Toast notification library.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Geist&lt;/strong&gt;: Font family from Vercel.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;React Hook Form&lt;/strong&gt;: Form state management and validation.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Zod&lt;/strong&gt;: TypeScript-first schema validation with static type inference.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;@hookform/resolvers&lt;/strong&gt;: Resolvers for using validation libraries with React Hook Form.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;@tanstack/react-table&lt;/strong&gt;: Headless UI for building powerful tables &amp;amp; datagrids.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;strong&gt;DevOps&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Docker&lt;/strong&gt;: Container platform for consistent deployment across environments&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Docker Compose&lt;/strong&gt;: Tool for defining and running multi-container Docker applications&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;pgAdmin&lt;/strong&gt;: Web-based PostgreSQL administration tool included in Docker setup&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;strong&gt;Extension&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;Manifest v3 on Plasmo&lt;/p&gt; 
&lt;h2&gt;Contribute&lt;/h2&gt; 
&lt;p&gt;Contributions are very welcome! A contribution can be as small as a â­ or even finding and creating issues. Fine-tuning the Backend is always desired.&lt;/p&gt; 
&lt;p&gt;For detailed contribution guidelines, please see our &lt;a href="https://raw.githubusercontent.com/MODSetter/SurfSense/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; file.&lt;/p&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;a href="https://www.star-history.com/#MODSetter/SurfSense&amp;amp;Date"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=MODSetter/SurfSense&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=MODSetter/SurfSense&amp;amp;type=Date" /&gt; 
  &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=MODSetter/SurfSense&amp;amp;type=Date" /&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;hr /&gt; 
&lt;hr /&gt; 
&lt;p align="center"&gt; &lt;img src="https://github.com/user-attachments/assets/329c9bc2-6005-4aed-a629-700b5ae296b4" alt="Catalyst Project" width="200" /&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;hr /&gt;</description>
    </item>
    
    <item>
      <title>srbhr/Resume-Matcher</title>
      <link>https://github.com/srbhr/Resume-Matcher</link>
      <description>&lt;p&gt;Improve your resumes with Resume Matcher. Get insights, keyword suggestions and tune your resumes to job descriptions.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://www.resumematcher.fyi"&gt;&lt;img src="https://raw.githubusercontent.com/srbhr/Resume-Matcher/main/assets/page_2.png" alt="Resume Matcher" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;h1&gt;Resume Matcher&lt;/h1&gt; 
 &lt;p&gt;&lt;a href="https://dsc.gg/resume-matcher"&gt;ğ™¹ğš˜ğš’ğš— ğ™³ğš’ğšœğšŒğš˜ğš›ğš&lt;/a&gt; âœ¦ &lt;a href="https://resumematcher.fyi"&gt;ğš†ğšğš‹ğšœğš’ğšğš&lt;/a&gt; âœ¦ &lt;a href="https://raw.githubusercontent.com/srbhr/Resume-Matcher/main/#how-to-install"&gt;ğ™·ğš˜ğš  ğšğš˜ ğ™¸ğš—ğšœğšğšŠğš•ğš•&lt;/a&gt; âœ¦ &lt;a href="https://raw.githubusercontent.com/srbhr/Resume-Matcher/main/#contributors"&gt;ğ™²ğš˜ğš—ğšğš›ğš’ğš‹ğšğšğš˜ğš›ğšœ&lt;/a&gt; âœ¦ &lt;a href="https://raw.githubusercontent.com/srbhr/Resume-Matcher/main/#support-the-development-by-donating"&gt;ğ™³ğš˜ğš—ğšŠğšğš&lt;/a&gt; âœ¦ &lt;a href="https://twitter.com/ssrbhr"&gt;ğšƒğš ğš’ğšğšğšğš›/ğš‡&lt;/a&gt; âœ¦ &lt;a href="https://www.linkedin.com/company/resume-matcher/"&gt;ğ™»ğš’ğš—ğš”ğšğšğ™¸ğš—&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Stop getting auto-rejected by ATS bots.&lt;/strong&gt; Resume Matcher is the AI-powered platform that reverse-engineers hiring algorithms to show you exactly how to tailor your resume. Get the keywords, formatting, and insights that actually get you past the first screen and into human hands.&lt;/p&gt; 
 &lt;p&gt;Hoping to make this, &lt;strong&gt;VS Code for making resumes&lt;/strong&gt;.&lt;/p&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;img src="https://img.shields.io/github/stars/srbhr/Resume-Matcher?labelColor=black&amp;amp;style=for-the-badge&amp;amp;color=c20a71" alt="Stars" /&gt; &lt;img src="https://img.shields.io/github/license/srbhr/Resume-Matcher?labelColor=black&amp;amp;style=for-the-badge&amp;amp;color=c20a71" alt="Apache 2.0" /&gt; &lt;img src="https://img.shields.io/github/forks/srbhr/Resume-Matcher?labelColor=black&amp;amp;style=for-the-badge&amp;amp;color=c20a71" alt="Forks" /&gt; &lt;img src="https://img.shields.io/badge/Version-0.1%20Veridis%20Quo-FFF?labelColor=black&amp;amp;logo=LinkedIn&amp;amp;style=for-the-badge&amp;amp;color=c20a71" alt="version" /&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://dsc.gg/resume-matcher"&gt;&lt;img src="https://img.shields.io/discord/1122069176962531400?labelColor=black&amp;amp;logo=discord&amp;amp;logoColor=c20a71&amp;amp;style=for-the-badge&amp;amp;color=c20a71" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://resumematcher.fyi"&gt;&lt;img src="https://img.shields.io/badge/website-Resume%20Matcher-FFF?labelColor=black&amp;amp;style=for-the-badge&amp;amp;color=c20a71" alt="Website" /&gt;&lt;/a&gt; &lt;a href="https://www.linkedin.com/company/resume-matcher/"&gt;&lt;img src="https://img.shields.io/badge/LinkedIn-Resume%20Matcher-FFF?labelColor=black&amp;amp;logo=LinkedIn&amp;amp;style=for-the-badge&amp;amp;color=c20a71" alt="LinkedIn" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/565" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/565" alt="srbhr%2FResume-Matcher | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://vercel.com/oss/program-badge.svg?sanitize=true" alt="Vercel OSS Program" /&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT]&lt;/p&gt; 
 &lt;p&gt;This project is in active development. New features are being added continuously, and we welcome contributions from the community. There are some breaking changes on the &lt;code&gt;main&lt;/code&gt; branch. If you have any suggestions or feature requests, please feel free to open an issue on GitHub or discuss it on our &lt;a href="https://dsc.gg/resume-matcher"&gt;Discord&lt;/a&gt; server.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Getting started with Resume Matcher&lt;/h2&gt; 
&lt;p&gt;Resume Matcher is designed to help you optimize your resume with the aim to highlight your skills and experience in a way that resonates with potential employers.&lt;/p&gt; 
&lt;p&gt;We're actively working on improving the platform, building towards a &lt;strong&gt;VS Code for making resumes&lt;/strong&gt;, and adding new features. The best way to stay updated is to join the Discord discussion and be part of the active development community.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Join our &lt;a href="https://dsc.gg/resume-matcher"&gt;Discord&lt;/a&gt; community ğŸ‘‡ &lt;a href="https://dsc.gg/resume-matcher"&gt;&lt;img src="https://raw.githubusercontent.com/srbhr/Resume-Matcher/main/assets/resume_matcher_discord.png" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Follow us on &lt;a href="https://www.linkedin.com/company/resume-matcher/"&gt;LinkedIn&lt;/a&gt; âœ¨ &lt;a href="https://www.linkedin.com/company/resume-matcher/"&gt;&lt;img src="https://raw.githubusercontent.com/srbhr/Resume-Matcher/main/assets/resume_matcher_linkedin.png" alt="LinkedIn" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;â­ Star Resume Matcher to support the development and get updates on GitHub. &lt;img src="https://raw.githubusercontent.com/srbhr/Resume-Matcher/main/assets/star_resume_matcher.png" alt="Star Resume Matcher" /&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/srbhr/Resume-Matcher/main/assets/resume_matcher_features.png" alt="resume_matcher_features" /&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Works locally&lt;/strong&gt;: No need to upload your resume to a server. Everything runs on your machine with open source AI models by Ollama.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ATS Compatibility&lt;/strong&gt;: Get a detailed analysis of your resume's compatibility with ATS systems.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Instant Match Score&lt;/strong&gt;: Upload resume &amp;amp; job description for a quick match score and key improvement areas.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Keyword Optimizer&lt;/strong&gt;: Align your resume with job keywords and identify critical content gaps.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Guided Improvements&lt;/strong&gt;: Get clear suggestions to make your resume stand out.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Roadmap&lt;/h3&gt; 
&lt;p&gt;If you have any suggestions or feature requests, please feel free to open an issue on GitHub. And discuss it on our &lt;a href="https://dsc.gg/resume-matcher"&gt;Discord&lt;/a&gt; server.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Visual keyword highlighting.&lt;/li&gt; 
 &lt;li&gt;AI Canvas, which can help to craft impactful, metric-driven resume content.&lt;/li&gt; 
 &lt;li&gt;Multi-job description optimization.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;How to Install&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/srbhr/Resume-Matcher/main/assets/how_to_install_resumematcher.png" alt="Installation" /&gt;&lt;/p&gt; 
&lt;p&gt;Follow the instructions in the &lt;a href="https://raw.githubusercontent.com/srbhr/Resume-Matcher/main/SETUP.md"&gt;SETUP.md&lt;/a&gt; file to set up the project locally. The setup script will install all the necessary dependencies and configure your environment.&lt;/p&gt; 
&lt;p&gt;The project is built using:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;FastAPI for the backend.&lt;/li&gt; 
 &lt;li&gt;Next.js for the frontend.&lt;/li&gt; 
 &lt;li&gt;Ollama for local AI model serving.&lt;/li&gt; 
 &lt;li&gt;Tailwind CSS for styling.&lt;/li&gt; 
 &lt;li&gt;SQLite for the database.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Technology&lt;/th&gt; 
   &lt;th&gt;Info/Version&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Python&lt;/td&gt; 
   &lt;td&gt;3.12+&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Next.js&lt;/td&gt; 
   &lt;td&gt;15+&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ollama&lt;/td&gt; 
   &lt;td&gt;0.6.7&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Join Us and Contribute&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/srbhr/Resume-Matcher/main/assets/how_to_contribute.png" alt="how to contribute" /&gt;&lt;/p&gt; 
&lt;p&gt;We welcome contributions from everyone! Whether you're a developer, designer, or just someone who wants to help out. All the contributors are listed in the &lt;a href="https://resumematcher.fyi/about"&gt;about page&lt;/a&gt; on our website and on the GitHub Readme here.&lt;/p&gt; 
&lt;p&gt;Check out the roadmap if you would like to work on the features that are planned for the future. If you have any suggestions or feature requests, please feel free to open an issue on GitHub and discuss it on our &lt;a href="https://dsc.gg/resume-matcher"&gt;Discord&lt;/a&gt; server.&lt;/p&gt; 
&lt;h2&gt;Contributors&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/srbhr/Resume-Matcher/main/assets/contributors.png" alt="Contributors" /&gt;&lt;/p&gt; 
&lt;a href="https://github.com/srbhr/Resume-Matcher/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=srbhr/Resume-Matcher" /&gt; &lt;/a&gt; 
&lt;h2&gt;Support the Development by Donating&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/srbhr/Resume-Matcher/main/assets/supporting_resume_matcher.png" alt="donate" /&gt;&lt;/p&gt; 
&lt;p&gt;If you would like to support the development of Resume Matcher, you can do so by donating. Your contributions will help us keep the project alive and continue adding new features.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Platform&lt;/th&gt; 
   &lt;th&gt;Link&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;GitHub&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/sponsors/srbhr"&gt;&lt;img src="https://img.shields.io/github/sponsors/srbhr?style=for-the-badge&amp;amp;color=c20a71&amp;amp;labelColor=black&amp;amp;logo=github" alt="GitHub Sponsors" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Buy Me a Coffee&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.buymeacoffee.com/srbhr"&gt;&lt;img src="https://img.shields.io/badge/Buy%20Me%20a%20Coffee-ffdd00?style=for-the-badge&amp;amp;logo=buy-me-a-coffee&amp;amp;color=c20a72&amp;amp;logoColor=white" alt="BuyMeACoffee" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;kbd&gt;Star History&lt;/kbd&gt;&lt;/summary&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=srbhr/resume-matcher&amp;amp;theme=dark&amp;amp;type=Date" /&gt; 
  &lt;img width="100%" src="https://api.star-history.com/svg?repos=srbhr/resume-matcher&amp;amp;theme=dark&amp;amp;type=Date" /&gt; 
 &lt;/picture&gt; 
&lt;/details&gt; 
&lt;h2&gt;Resume Matcher is a part of &lt;a href="https://vercel.com/oss"&gt;Vercel Open Source Program&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://vercel.com/oss/program-badge.svg?sanitize=true" alt="Vercel OSS Program" /&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>zhu-xlab/GlobalBuildingAtlas</title>
      <link>https://github.com/zhu-xlab/GlobalBuildingAtlas</link>
      <description>&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;GlobalBuildingAtlas&lt;/h1&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;In this project, we provide the level of detail 1 (LoD1) data of buildings across the globe.&lt;/p&gt; 
&lt;p&gt;A overview of the dataset is illustrated bellow:&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/zhu-xlab/GlobalBuildingAtlas/main/figures/overview.png" width="800" /&gt; 
&lt;h2&gt;Access to the Data&lt;/h2&gt; 
&lt;h3&gt;Web Feature Service (WFS)&lt;/h3&gt; 
&lt;p&gt;A WFS is provided so that one can access the data using other websites or GIS softwares such as QGIS and ArcGIS.&lt;/p&gt; 
&lt;p&gt;Url: &lt;code&gt;https://tubvsig-so2sat-vm1.srv.mwn.de/geoserver/ows?&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Web Viewer&lt;/h3&gt; 
&lt;p&gt;A web interface for viewing the data is available at: &lt;a href="https://tubvsig-so2sat-vm1.srv.mwn.de"&gt;website&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Note: Over the past few days, our web viewer has received nearly 280,000 access requests. Due to this unusually high traffic, some data may not load completely, which may result in a significant portion of buildings not being displayed.&lt;/p&gt; 
&lt;h3&gt;Full Data Download&lt;/h3&gt; 
&lt;p&gt;The full data can be downloaded from &lt;a href="https://mediatum.ub.tum.de/1782307"&gt;mediaTUM&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Development Code&lt;/h2&gt; 
&lt;h3&gt;Global Building Polygon Generation using Satellite Data (Sec. 4.3)&lt;/h3&gt; 
&lt;p&gt;For codes related to building map extraction, regularization, polygonization, and simplification, i.e., generating building polygons from satellite images (Sec. 4.3.2, Sec. 4.3.3, and Sec. 4.3.4), please refer to &lt;code&gt;./im2bf&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Global Building Height Estimation (Sec. 4.4)&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;For codes related to monocular height estimation using HTC-DC Net (Sec. 4.4.2), please refer to &lt;code&gt;./im2bh&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;For codes related to the global inference and uncertainty quantification (Sec. 4.4.3), please refer to &lt;code&gt;./infer_height&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Global LoD1 Building Model Generation (Sec. 4.5)&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;For codes related to quality-guided building polygon fusion (Sec. 4.5.1), please refer to &lt;code&gt;./fuse_bf&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;For codes related to LoD1 building model generation (Sec. 4.5.2), please refer to &lt;code&gt;./make_lod1&lt;/code&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Visualization Code&lt;/h2&gt; 
&lt;p&gt;For codes to reproduce the plots in the manuscript, please refer to &lt;code&gt;./make_plots&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Code License&lt;/h2&gt; 
&lt;p&gt;MIT with Commons Clause (no commercial use allowed). See &lt;a href="https://github.com/zhu-xlab/GlobalBuildingAtlas/raw/main/LICENSE"&gt;LICENSE&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;How to cite&lt;/h2&gt; 
&lt;p&gt;If you find this dataset helpful in your work, please cite the following paper.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@Article{essd-17-6647-2025,
AUTHOR = {Zhu, X. X. and Chen, S. and Zhang, F. and Shi, Y. and Wang, Y.},
TITLE = {GlobalBuildingAtlas: an open global and complete dataset of building polygons, heights and LoD1 3D models},
JOURNAL = {Earth System Science Data},
VOLUME = {17},
YEAR = {2025},
NUMBER = {12},
PAGES = {6647--6668},
URL = {https://essd.copernicus.org/articles/17/6647/2025/},
DOI = {10.5194/essd-17-6647-2025}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>infiniflow/ragflow</title>
      <link>https://github.com/infiniflow/ragflow</link>
      <description>&lt;p&gt;RAGFlow is a leading open-source Retrieval-Augmented Generation (RAG) engine that fuses cutting-edge RAG with Agent capabilities to create a superior context layer for LLMs&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;a href="https://demo.ragflow.io/"&gt; &lt;img src="https://raw.githubusercontent.com/infiniflow/ragflow/main/web/src/assets/logo-with-text.svg?sanitize=true" width="520" alt="ragflow logo" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;p align="center"&gt; &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/README.md"&gt;&lt;img alt="README in English" src="https://img.shields.io/badge/English-DBEDFA" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/README_zh.md"&gt;&lt;img alt="ç®€ä½“ä¸­æ–‡ç‰ˆè‡ªè¿°æ–‡ä»¶" src="https://img.shields.io/badge/ç®€ä½“ä¸­æ–‡-DFE0E5" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/README_tzh.md"&gt;&lt;img alt="ç¹é«”ç‰ˆä¸­æ–‡è‡ªè¿°æ–‡ä»¶" src="https://img.shields.io/badge/ç¹é«”ä¸­æ–‡-DFE0E5" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/README_ja.md"&gt;&lt;img alt="æ—¥æœ¬èªã®README" src="https://img.shields.io/badge/æ—¥æœ¬èª-DFE0E5" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/README_ko.md"&gt;&lt;img alt="í•œêµ­ì–´" src="https://img.shields.io/badge/í•œêµ­ì–´-DFE0E5" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/README_id.md"&gt;&lt;img alt="Bahasa Indonesia" src="https://img.shields.io/badge/Bahasa Indonesia-DFE0E5" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/README_pt_br.md"&gt;&lt;img alt="PortuguÃªs(Brasil)" src="https://img.shields.io/badge/PortuguÃªs(Brasil)-DFE0E5" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://x.com/intent/follow?screen_name=infiniflowai" target="_blank"&gt; &lt;img src="https://img.shields.io/twitter/follow/infiniflow?logo=X&amp;amp;color=%20%23f5f5f5" alt="follow on X(Twitter)" /&gt; &lt;/a&gt; &lt;a href="https://demo.ragflow.io" target="_blank"&gt; &lt;img alt="Static Badge" src="https://img.shields.io/badge/Online-Demo-4e6b99" /&gt; &lt;/a&gt; &lt;a href="https://hub.docker.com/r/infiniflow/ragflow" target="_blank"&gt; &lt;img src="https://img.shields.io/docker/pulls/infiniflow/ragflow?label=Docker%20Pulls&amp;amp;color=0db7ed&amp;amp;logo=docker&amp;amp;logoColor=white&amp;amp;style=flat-square" alt="docker pull infiniflow/ragflow:v0.22.1" /&gt; &lt;/a&gt; &lt;a href="https://github.com/infiniflow/ragflow/releases/latest"&gt; &lt;img src="https://img.shields.io/github/v/release/infiniflow/ragflow?color=blue&amp;amp;label=Latest%20Release" alt="Latest Release" /&gt; &lt;/a&gt; &lt;a href="https://github.com/infiniflow/ragflow/raw/main/LICENSE"&gt; &lt;img height="21" src="https://img.shields.io/badge/License-Apache--2.0-ffffff?labelColor=d4eaf7&amp;amp;color=2e6cc4" alt="license" /&gt; &lt;/a&gt; &lt;a href="https://deepwiki.com/infiniflow/ragflow"&gt; &lt;img alt="Ask DeepWiki" src="https://deepwiki.com/badge.svg?sanitize=true" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h4 align="center"&gt; &lt;a href="https://ragflow.io/docs/dev/"&gt;Document&lt;/a&gt; | &lt;a href="https://github.com/infiniflow/ragflow/issues/4214"&gt;Roadmap&lt;/a&gt; | &lt;a href="https://twitter.com/infiniflowai"&gt;Twitter&lt;/a&gt; | &lt;a href="https://discord.gg/NjYzJD3GM3"&gt;Discord&lt;/a&gt; | &lt;a href="https://demo.ragflow.io"&gt;Demo&lt;/a&gt; &lt;/h4&gt; 
&lt;div align="center" style="margin-top:20px;margin-bottom:20px;"&gt; 
 &lt;img src="https://raw.githubusercontent.com/infiniflow/ragflow-docs/refs/heads/image/image/ragflow-octoverse.png" width="1200" /&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://trendshift.io/repositories/9064" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/9064" alt="infiniflow%2Fragflow | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;details open&gt; 
 &lt;summary&gt;&lt;b&gt;ğŸ“• Table of Contents&lt;/b&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;ğŸ’¡ &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/#-what-is-ragflow"&gt;What is RAGFlow?&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;ğŸ® &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/#-demo"&gt;Demo&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;ğŸ“Œ &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/#-latest-updates"&gt;Latest Updates&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;ğŸŒŸ &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/#-key-features"&gt;Key Features&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;ğŸ” &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/#-system-architecture"&gt;System Architecture&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;ğŸ¬ &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/#-get-started"&gt;Get Started&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;ğŸ”§ &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/#-configurations"&gt;Configurations&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;ğŸ”§ &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/#-build-a-docker-image"&gt;Build a Docker image&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;ğŸ”¨ &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/#-launch-service-from-source-for-development"&gt;Launch service from source for development&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;ğŸ“š &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/#-documentation"&gt;Documentation&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;ğŸ“œ &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/#-roadmap"&gt;Roadmap&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;ğŸ„ &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/#-community"&gt;Community&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;ğŸ™Œ &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/#-contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;ğŸ’¡ What is RAGFlow?&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://ragflow.io/"&gt;RAGFlow&lt;/a&gt; is a leading open-source Retrieval-Augmented Generation (RAG) engine that fuses cutting-edge RAG with Agent capabilities to create a superior context layer for LLMs. It offers a streamlined RAG workflow adaptable to enterprises of any scale. Powered by a converged context engine and pre-built agent templates, RAGFlow enables developers to transform complex data into high-fidelity, production-ready AI systems with exceptional efficiency and precision.&lt;/p&gt; 
&lt;h2&gt;ğŸ® Demo&lt;/h2&gt; 
&lt;p&gt;Try our demo at &lt;a href="https://demo.ragflow.io"&gt;https://demo.ragflow.io&lt;/a&gt;.&lt;/p&gt; 
&lt;div align="center" style="margin-top:20px;margin-bottom:20px;"&gt; 
 &lt;img src="https://raw.githubusercontent.com/infiniflow/ragflow-docs/refs/heads/image/image/chunking.gif" width="1200" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/infiniflow/ragflow-docs/refs/heads/image/image/agentic-dark.gif" width="1200" /&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸ”¥ Latest Updates&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;2025-11-19 Supports Gemini 3 Pro.&lt;/li&gt; 
 &lt;li&gt;2025-11-12 Supports data synchronization from Confluence, S3, Notion, Discord, Google Drive.&lt;/li&gt; 
 &lt;li&gt;2025-10-23 Supports MinerU &amp;amp; Docling as document parsing methods.&lt;/li&gt; 
 &lt;li&gt;2025-10-15 Supports orchestrable ingestion pipeline.&lt;/li&gt; 
 &lt;li&gt;2025-08-08 Supports OpenAI's latest GPT-5 series models.&lt;/li&gt; 
 &lt;li&gt;2025-08-01 Supports agentic workflow and MCP.&lt;/li&gt; 
 &lt;li&gt;2025-05-23 Adds a Python/JavaScript code executor component to Agent.&lt;/li&gt; 
 &lt;li&gt;2025-05-05 Supports cross-language query.&lt;/li&gt; 
 &lt;li&gt;2025-03-19 Supports using a multi-modal model to make sense of images within PDF or DOCX files.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ‰ Stay Tuned&lt;/h2&gt; 
&lt;p&gt;â­ï¸ Star our repository to stay up-to-date with exciting new features and improvements! Get instant notifications for new releases! ğŸŒŸ&lt;/p&gt; 
&lt;div align="center" style="margin-top:20px;margin-bottom:20px;"&gt; 
 &lt;img src="https://github.com/user-attachments/assets/18c9707e-b8aa-4caf-a154-037089c105ba" width="1200" /&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸŒŸ Key Features&lt;/h2&gt; 
&lt;h3&gt;ğŸ­ &lt;strong&gt;"Quality in, quality out"&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/deepdoc/README.md"&gt;Deep document understanding&lt;/a&gt;-based knowledge extraction from unstructured data with complicated formats.&lt;/li&gt; 
 &lt;li&gt;Finds "needle in a data haystack" of literally unlimited tokens.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ± &lt;strong&gt;Template-based chunking&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Intelligent and explainable.&lt;/li&gt; 
 &lt;li&gt;Plenty of template options to choose from.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸŒ± &lt;strong&gt;Grounded citations with reduced hallucinations&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Visualization of text chunking to allow human intervention.&lt;/li&gt; 
 &lt;li&gt;Quick view of the key references and traceable citations to support grounded answers.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ” &lt;strong&gt;Compatibility with heterogeneous data sources&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Supports Word, slides, excel, txt, images, scanned copies, structured data, web pages, and more.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ›€ &lt;strong&gt;Automated and effortless RAG workflow&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Streamlined RAG orchestration catered to both personal and large businesses.&lt;/li&gt; 
 &lt;li&gt;Configurable LLMs as well as embedding models.&lt;/li&gt; 
 &lt;li&gt;Multiple recall paired with fused re-ranking.&lt;/li&gt; 
 &lt;li&gt;Intuitive APIs for seamless integration with business.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ” System Architecture&lt;/h2&gt; 
&lt;div align="center" style="margin-top:20px;margin-bottom:20px;"&gt; 
 &lt;img src="https://github.com/user-attachments/assets/31b0dd6f-ca4f-445a-9457-70cb44a381b2" width="1000" /&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸ¬ Get Started&lt;/h2&gt; 
&lt;h3&gt;ğŸ“ Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;CPU &amp;gt;= 4 cores&lt;/li&gt; 
 &lt;li&gt;RAM &amp;gt;= 16 GB&lt;/li&gt; 
 &lt;li&gt;Disk &amp;gt;= 50 GB&lt;/li&gt; 
 &lt;li&gt;Docker &amp;gt;= 24.0.0 &amp;amp; Docker Compose &amp;gt;= v2.26.1&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://gvisor.dev/docs/user_guide/install/"&gt;gVisor&lt;/a&gt;: Required only if you intend to use the code executor (sandbox) feature of RAGFlow.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] If you have not installed Docker on your local machine (Windows, Mac, or Linux), see &lt;a href="https://docs.docker.com/engine/install/"&gt;Install Docker Engine&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;ğŸš€ Start up the server&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Ensure &lt;code&gt;vm.max_map_count&lt;/code&gt; &amp;gt;= 262144:&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;To check the value of &lt;code&gt;vm.max_map_count&lt;/code&gt;:&lt;/p&gt; 
   &lt;pre&gt;&lt;code class="language-bash"&gt;$ sysctl vm.max_map_count
&lt;/code&gt;&lt;/pre&gt; 
   &lt;p&gt;Reset &lt;code&gt;vm.max_map_count&lt;/code&gt; to a value at least 262144 if it is not.&lt;/p&gt; 
   &lt;pre&gt;&lt;code class="language-bash"&gt;# In this case, we set it to 262144:
$ sudo sysctl -w vm.max_map_count=262144
&lt;/code&gt;&lt;/pre&gt; 
   &lt;p&gt;This change will be reset after a system reboot. To ensure your change remains permanent, add or update the &lt;code&gt;vm.max_map_count&lt;/code&gt; value in &lt;strong&gt;/etc/sysctl.conf&lt;/strong&gt; accordingly:&lt;/p&gt; 
   &lt;pre&gt;&lt;code class="language-bash"&gt;vm.max_map_count=262144
&lt;/code&gt;&lt;/pre&gt; 
  &lt;/blockquote&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Clone the repo:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;$ git clone https://github.com/infiniflow/ragflow.git
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Start up the server using the pre-built Docker images:&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!CAUTION] All Docker images are built for x86 platforms. We don't currently offer Docker images for ARM64. If you are on an ARM64 platform, follow &lt;a href="https://ragflow.io/docs/dev/build_docker_image"&gt;this guide&lt;/a&gt; to build a Docker image compatible with your system.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;The command below downloads the &lt;code&gt;v0.22.1&lt;/code&gt; edition of the RAGFlow Docker image. See the following table for descriptions of different RAGFlow editions. To download a RAGFlow edition different from &lt;code&gt;v0.22.1&lt;/code&gt;, update the &lt;code&gt;RAGFLOW_IMAGE&lt;/code&gt; variable accordingly in &lt;strong&gt;docker/.env&lt;/strong&gt; before using &lt;code&gt;docker compose&lt;/code&gt; to start the server.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;   $ cd ragflow/docker
  
   # git checkout v0.22.1
   # Optional: use a stable tag (see releases: https://github.com/infiniflow/ragflow/releases)
   # This step ensures the **entrypoint.sh** file in the code matches the Docker image version.
   
   # Use CPU for DeepDoc tasks:
   $ docker compose -f docker-compose.yml up -d

   # To use GPU to accelerate DeepDoc tasks:
   # sed -i '1i DEVICE=gpu' .env
   # docker compose -f docker-compose.yml up -d
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Note: Prior to &lt;code&gt;v0.22.0&lt;/code&gt;, we provided both images with embedding models and slim images without embedding models. Details as follows:&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;RAGFlow image tag&lt;/th&gt; 
   &lt;th&gt;Image size (GB)&lt;/th&gt; 
   &lt;th&gt;Has embedding models?&lt;/th&gt; 
   &lt;th&gt;Stable?&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;v0.21.1&lt;/td&gt; 
   &lt;td&gt;â‰ˆ9&lt;/td&gt; 
   &lt;td&gt;âœ”ï¸&lt;/td&gt; 
   &lt;td&gt;Stable release&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;v0.21.1-slim&lt;/td&gt; 
   &lt;td&gt;â‰ˆ2&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
   &lt;td&gt;Stable release&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Starting with &lt;code&gt;v0.22.0&lt;/code&gt;, we ship only the slim edition and no longer append the &lt;strong&gt;-slim&lt;/strong&gt; suffix to the image tag.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ol start="4"&gt; 
 &lt;li&gt; &lt;p&gt;Check the server status after having the server up and running:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;$ docker logs -f docker-ragflow-cpu-1
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;The following output confirms a successful launch of the system:&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;
      ____   ___    ______ ______ __
     / __ \ /   |  / ____// ____// /____  _      __
    / /_/ // /| | / / __ / /_   / // __ \| | /| / /
   / _, _// ___ |/ /_/ // __/  / // /_/ /| |/ |/ /
  /_/ |_|/_/  |_|\____//_/    /_/ \____/ |__/|__/

 * Running on all addresses (0.0.0.0)
&lt;/code&gt;&lt;/pre&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;If you skip this confirmation step and directly log in to RAGFlow, your browser may prompt a &lt;code&gt;network anormal&lt;/code&gt; error because, at that moment, your RAGFlow may not be fully initialized.&lt;/p&gt; 
  &lt;/blockquote&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;In your web browser, enter the IP address of your server and log in to RAGFlow.&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;With the default settings, you only need to enter &lt;code&gt;http://IP_OF_YOUR_MACHINE&lt;/code&gt; (&lt;strong&gt;sans&lt;/strong&gt; port number) as the default HTTP serving port &lt;code&gt;80&lt;/code&gt; can be omitted when using the default configurations.&lt;/p&gt; 
  &lt;/blockquote&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;In &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/service_conf.yaml.template"&gt;service_conf.yaml.template&lt;/a&gt;, select the desired LLM factory in &lt;code&gt;user_default_llm&lt;/code&gt; and update the &lt;code&gt;API_KEY&lt;/code&gt; field with the corresponding API key.&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;See &lt;a href="https://ragflow.io/docs/dev/llm_api_key_setup"&gt;llm_api_key_setup&lt;/a&gt; for more information.&lt;/p&gt; 
  &lt;/blockquote&gt; &lt;p&gt;&lt;em&gt;The show is on!&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;ğŸ”§ Configurations&lt;/h2&gt; 
&lt;p&gt;When it comes to system configurations, you will need to manage the following files:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/.env"&gt;.env&lt;/a&gt;: Keeps the fundamental setups for the system, such as &lt;code&gt;SVR_HTTP_PORT&lt;/code&gt;, &lt;code&gt;MYSQL_PASSWORD&lt;/code&gt;, and &lt;code&gt;MINIO_PASSWORD&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/service_conf.yaml.template"&gt;service_conf.yaml.template&lt;/a&gt;: Configures the back-end services. The environment variables in this file will be automatically populated when the Docker container starts. Any environment variables set within the Docker container will be available for use, allowing you to customize service behavior based on the deployment environment.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/docker-compose.yml"&gt;docker-compose.yml&lt;/a&gt;: The system relies on &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/docker-compose.yml"&gt;docker-compose.yml&lt;/a&gt; to start up.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;The &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/README.md"&gt;./docker/README&lt;/a&gt; file provides a detailed description of the environment settings and service configurations which can be used as &lt;code&gt;${ENV_VARS}&lt;/code&gt; in the &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/service_conf.yaml.template"&gt;service_conf.yaml.template&lt;/a&gt; file.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;To update the default HTTP serving port (80), go to &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/docker-compose.yml"&gt;docker-compose.yml&lt;/a&gt; and change &lt;code&gt;80:80&lt;/code&gt; to &lt;code&gt;&amp;lt;YOUR_SERVING_PORT&amp;gt;:80&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Updates to the above configurations require a reboot of all containers to take effect:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;$ docker compose -f docker-compose.yml up -d
&lt;/code&gt;&lt;/pre&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Switch doc engine from Elasticsearch to Infinity&lt;/h3&gt; 
&lt;p&gt;RAGFlow uses Elasticsearch by default for storing full text and vectors. To switch to &lt;a href="https://github.com/infiniflow/infinity/"&gt;Infinity&lt;/a&gt;, follow these steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Stop all running containers:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;$ docker compose -f docker/docker-compose.yml down -v
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING] &lt;code&gt;-v&lt;/code&gt; will delete the docker container volumes, and the existing data will be cleared.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt; &lt;p&gt;Set &lt;code&gt;DOC_ENGINE&lt;/code&gt; in &lt;strong&gt;docker/.env&lt;/strong&gt; to &lt;code&gt;infinity&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Start the containers:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;$ docker compose -f docker-compose.yml up -d
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING] Switching to Infinity on a Linux/arm64 machine is not yet officially supported.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;ğŸ”§ Build a Docker image&lt;/h2&gt; 
&lt;p&gt;This image is approximately 2 GB in size and relies on external LLM and embedding services.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/infiniflow/ragflow.git
cd ragflow/
docker build --platform linux/amd64 -f Dockerfile -t infiniflow/ragflow:nightly .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ğŸ”¨ Launch service from source for development&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Install &lt;code&gt;uv&lt;/code&gt; and &lt;code&gt;pre-commit&lt;/code&gt;, or skip this step if they are already installed:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pipx install uv pre-commit
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Clone the source code and install Python dependencies:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/infiniflow/ragflow.git
cd ragflow/
uv sync --python 3.12 # install RAGFlow dependent python modules
uv run download_deps.py
pre-commit install
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Launch the dependent services (MinIO, Elasticsearch, Redis, and MySQL) using Docker Compose:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;docker compose -f docker/docker-compose-base.yml up -d
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Add the following line to &lt;code&gt;/etc/hosts&lt;/code&gt; to resolve all hosts specified in &lt;strong&gt;docker/.env&lt;/strong&gt; to &lt;code&gt;127.0.0.1&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;127.0.0.1       es01 infinity mysql minio redis sandbox-executor-manager
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;If you cannot access HuggingFace, set the &lt;code&gt;HF_ENDPOINT&lt;/code&gt; environment variable to use a mirror site:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;export HF_ENDPOINT=https://hf-mirror.com
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;If your operating system does not have jemalloc, please install it as follows:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# Ubuntu
sudo apt-get install libjemalloc-dev
# CentOS
sudo yum install jemalloc
# OpenSUSE
sudo zypper install jemalloc
# macOS
sudo brew install jemalloc
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Launch backend service:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;source .venv/bin/activate
export PYTHONPATH=$(pwd)
bash docker/launch_backend_service.sh
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install frontend dependencies:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cd web
npm install
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Launch frontend service:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;npm run dev
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;The following output confirms a successful launch of the system:&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/0daf462c-a24d-4496-a66f-92533534e187" alt="" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Stop RAGFlow front-end and back-end service after development is complete:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pkill -f "ragflow_server.py|task_executor.py"
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;ğŸ“š Documentation&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://ragflow.io/docs/dev/"&gt;Quickstart&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ragflow.io/docs/dev/configurations"&gt;Configuration&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ragflow.io/docs/dev/release_notes"&gt;Release notes&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ragflow.io/docs/dev/category/guides"&gt;User guides&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ragflow.io/docs/dev/category/developers"&gt;Developer guides&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ragflow.io/docs/dev/category/references"&gt;References&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ragflow.io/docs/dev/faq"&gt;FAQs&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ“œ Roadmap&lt;/h2&gt; 
&lt;p&gt;See the &lt;a href="https://github.com/infiniflow/ragflow/issues/4214"&gt;RAGFlow Roadmap 2025&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;ğŸ„ Community&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://discord.gg/NjYzJD3GM3"&gt;Discord&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://twitter.com/infiniflowai"&gt;Twitter&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/orgs/infiniflow/discussions"&gt;GitHub Discussions&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ™Œ Contributing&lt;/h2&gt; 
&lt;p&gt;RAGFlow flourishes via open-source collaboration. In this spirit, we embrace diverse contributions from the community. If you would like to be a part, review our &lt;a href="https://ragflow.io/docs/dev/contributing"&gt;Contribution Guidelines&lt;/a&gt; first.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>anthropics/claude-quickstarts</title>
      <link>https://github.com/anthropics/claude-quickstarts</link>
      <description>&lt;p&gt;A collection of projects designed to help developers quickly get started with building deployable applications using the Claude API&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Claude Quickstarts&lt;/h1&gt; 
&lt;p&gt;Claude Quickstarts is a collection of projects designed to help developers quickly get started with building applications using the Claude API. Each quickstart provides a foundation that you can easily build upon and customize for your specific needs.&lt;/p&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;To use these quickstarts, you'll need an Claude API key. If you don't have one yet, you can sign up for free at &lt;a href="https://console.anthropic.com"&gt;console.anthropic.com&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Available Quickstarts&lt;/h2&gt; 
&lt;h3&gt;Customer Support Agent&lt;/h3&gt; 
&lt;p&gt;A customer support agent powered by Claude. This project demonstrates how to leverage Claude's natural language understanding and generation capabilities to create an AI-assisted customer support system with access to a knowledge base.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/anthropics/claude-quickstarts/main/customer-support-agent"&gt;Go to Customer Support Agent Quickstart&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Financial Data Analyst&lt;/h3&gt; 
&lt;p&gt;A financial data analyst powered by Claude. This project demonstrates how to leverage Claude's capabilities with interactive data visualization to analyze financial data via chat.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/anthropics/claude-quickstarts/main/financial-data-analyst"&gt;Go to Financial Data Analyst Quickstart&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Computer Use Demo&lt;/h3&gt; 
&lt;p&gt;An environment and tools that Claude can use to control a desktop computer. This project demonstrates how to leverage the computer use capabilities of Claude, including support for the latest &lt;code&gt;computer_use_20251124&lt;/code&gt; tool version with zoom actions.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/anthropics/claude-quickstarts/main/computer-use-demo"&gt;Go to Computer Use Demo Quickstart&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Autonomous Coding Agent&lt;/h3&gt; 
&lt;p&gt;An autonomous coding agent powered by the Claude Agent SDK. This project demonstrates a two-agent pattern (initializer + coding agent) that can build complete applications over multiple sessions, with progress persisted via git and a feature list that the agent works through incrementally.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/anthropics/claude-quickstarts/main/autonomous-coding"&gt;Go to Autonomous Coding Agent Quickstart&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;General Usage&lt;/h2&gt; 
&lt;p&gt;Each quickstart project comes with its own README and setup instructions. Generally, you'll follow these steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Clone this repository&lt;/li&gt; 
 &lt;li&gt;Navigate to the specific quickstart directory&lt;/li&gt; 
 &lt;li&gt;Install the required dependencies&lt;/li&gt; 
 &lt;li&gt;Set up your Claude API key as an environment variable&lt;/li&gt; 
 &lt;li&gt;Run the quickstart application&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Explore Further&lt;/h2&gt; 
&lt;p&gt;To deepen your understanding of working with Claude and the Claude API, check out these resources:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.claude.com"&gt;Claude API Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/anthropics/claude-cookbooks"&gt;Claude Cookbooks&lt;/a&gt; - A collection of code snippets and guides for common tasks&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/anthropics/courses/tree/master/anthropic_api_fundamentals"&gt;Claude API Fundamentals Course&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions to the Claude Quickstarts repository! If you have ideas for new quickstart projects or improvements to existing ones, please open an issue or submit a pull request.&lt;/p&gt; 
&lt;h2&gt;Community and Support&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Join our &lt;a href="https://www.anthropic.com/discord"&gt;Anthropic Discord community&lt;/a&gt; for discussions and support&lt;/li&gt; 
 &lt;li&gt;Check out the &lt;a href="https://support.anthropic.com"&gt;Anthropic support documentation&lt;/a&gt; for additional help&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the MIT License - see the &lt;a href="https://raw.githubusercontent.com/anthropics/claude-quickstarts/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>datawhalechina/hello-agents</title>
      <link>https://github.com/datawhalechina/hello-agents</link>
      <description>&lt;p&gt;ğŸ“š ã€Šä»é›¶å¼€å§‹æ„å»ºæ™ºèƒ½ä½“ã€‹â€”â€”ä»é›¶å¼€å§‹çš„æ™ºèƒ½ä½“åŸç†ä¸å®è·µæ•™ç¨‹&lt;/p&gt;&lt;hr&gt;&lt;div align="right"&gt; 
 &lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/README_EN.md"&gt;English&lt;/a&gt; | ä¸­æ–‡ 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/images/hello-agents.png" alt="alt text" width="100%" /&gt; 
 &lt;h1&gt;Hello-Agents&lt;/h1&gt; 
 &lt;h3&gt;ğŸ¤– ã€Šä»é›¶å¼€å§‹æ„å»ºæ™ºèƒ½ä½“ã€‹&lt;/h3&gt; 
 &lt;p&gt;&lt;em&gt;ä»åŸºç¡€ç†è®ºåˆ°å®é™…åº”ç”¨ï¼Œå…¨é¢æŒæ¡æ™ºèƒ½ä½“ç³»ç»Ÿçš„è®¾è®¡ä¸å®ç°&lt;/em&gt;&lt;/p&gt; 
 &lt;img src="https://img.shields.io/github/stars/datawhalechina/Hello-Agents?style=flat&amp;amp;logo=github" alt="GitHub stars" /&gt; 
 &lt;img src="https://img.shields.io/github/forks/datawhalechina/Hello-Agents?style=flat&amp;amp;logo=github" alt="GitHub forks" /&gt; 
 &lt;img src="https://img.shields.io/badge/language-Chinese-brightgreen?style=flat" alt="Language" /&gt; 
 &lt;a href="https://github.com/datawhalechina/Hello-Agents"&gt;&lt;img src="https://img.shields.io/badge/GitHub-Project-blue?style=flat&amp;amp;logo=github" alt="GitHub Project" /&gt;&lt;/a&gt; 
 &lt;a href="https://datawhalechina.github.io/hello-agents/"&gt;&lt;img src="https://img.shields.io/badge/åœ¨çº¿é˜…è¯»-Online%20Reading-green?style=flat&amp;amp;logo=gitbook" alt="Online Reading" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ¯ é¡¹ç›®ä»‹ç»&lt;/h2&gt; 
&lt;p&gt;â€ƒâ€ƒå¦‚æœè¯´ 2024 å¹´æ˜¯"ç™¾æ¨¡å¤§æˆ˜"çš„å…ƒå¹´ï¼Œé‚£ä¹ˆ 2025 å¹´æ— ç–‘å¼€å¯äº†"Agent å…ƒå¹´"ã€‚æŠ€æœ¯çš„ç„¦ç‚¹æ­£ä»è®­ç»ƒæ›´å¤§çš„åŸºç¡€æ¨¡å‹ï¼Œè½¬å‘æ„å»ºæ›´èªæ˜çš„æ™ºèƒ½ä½“åº”ç”¨ã€‚ç„¶è€Œï¼Œå½“å‰ç³»ç»Ÿæ€§ã€é‡å®è·µçš„æ•™ç¨‹å´æåº¦åŒ®ä¹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å‘èµ·äº† Hello-Agents é¡¹ç›®ï¼Œå¸Œæœ›èƒ½ä¸ºç¤¾åŒºæä¾›ä¸€æœ¬ä»é›¶å¼€å§‹ã€ç†è®ºä¸å®æˆ˜å¹¶é‡çš„æ™ºèƒ½ä½“ç³»ç»Ÿæ„å»ºæŒ‡å—ã€‚&lt;/p&gt; 
&lt;p&gt;â€ƒâ€ƒHello-Agents æ˜¯ Datawhale ç¤¾åŒºçš„&lt;strong&gt;ç³»ç»Ÿæ€§æ™ºèƒ½ä½“å­¦ä¹ æ•™ç¨‹&lt;/strong&gt;ã€‚å¦‚ä»Š Agent æ„å»ºä¸»è¦åˆ†ä¸ºä¸¤æ´¾ï¼Œä¸€æ´¾æ˜¯ Difyï¼ŒCozeï¼Œn8n è¿™ç±»è½¯ä»¶å·¥ç¨‹ç±» Agentï¼Œå…¶æœ¬è´¨æ˜¯æµç¨‹é©±åŠ¨çš„è½¯ä»¶å¼€å‘ï¼ŒLLM ä½œä¸ºæ•°æ®å¤„ç†çš„åç«¯ï¼›å¦ä¸€æ´¾åˆ™æ˜¯ AI åŸç”Ÿçš„ Agentï¼Œå³çœŸæ­£ä»¥ AI é©±åŠ¨çš„ Agentã€‚æœ¬æ•™ç¨‹æ—¨åœ¨å¸¦é¢†å¤§å®¶æ·±å…¥ç†è§£å¹¶æ„å»ºåè€…â€”â€”çœŸæ­£çš„ AI Native Agentã€‚æ•™ç¨‹å°†å¸¦é¢†ä½ ç©¿é€æ¡†æ¶è¡¨è±¡ï¼Œä»æ™ºèƒ½ä½“çš„æ ¸å¿ƒåŸç†å‡ºå‘ï¼Œæ·±å…¥å…¶æ ¸å¿ƒæ¶æ„ï¼Œç†è§£å…¶ç»å…¸èŒƒå¼ï¼Œå¹¶æœ€ç»ˆäº²æ‰‹æ„å»ºèµ·å±äºè‡ªå·±çš„å¤šæ™ºèƒ½ä½“åº”ç”¨ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼Œæœ€å¥½çš„å­¦ä¹ æ–¹å¼å°±æ˜¯åŠ¨æ‰‹å®è·µã€‚å¸Œæœ›è¿™æœ¬æ•™ç¨‹èƒ½æˆä¸ºä½ æ¢ç´¢æ™ºèƒ½ä½“ä¸–ç•Œçš„èµ·ç‚¹ï¼Œèƒ½å¤Ÿä»ä¸€åå¤§è¯­è¨€æ¨¡å‹çš„"ä½¿ç”¨è€…"ï¼Œèœ•å˜ä¸ºä¸€åæ™ºèƒ½ä½“ç³»ç»Ÿçš„"æ„å»ºè€…"ã€‚&lt;/p&gt; 
&lt;h2&gt;ğŸ“š å¿«é€Ÿå¼€å§‹&lt;/h2&gt; 
&lt;h3&gt;åœ¨çº¿é˜…è¯»&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://datawhalechina.github.io/hello-agents/"&gt;ğŸŒ ç‚¹å‡»è¿™é‡Œå¼€å§‹åœ¨çº¿é˜…è¯»&lt;/a&gt;&lt;/strong&gt; - æ— éœ€ä¸‹è½½ï¼Œéšæ—¶éšåœ°å­¦ä¹ &lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://book.heterocat.com.cn/"&gt;ğŸ“– Cookbook(æµ‹è¯•ç‰ˆ)&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h3&gt;æœ¬åœ°é˜…è¯»&lt;/h3&gt; 
&lt;p&gt;å¦‚æœæ‚¨å¸Œæœ›åœ¨æœ¬åœ°é˜…è¯»æˆ–è´¡çŒ®å†…å®¹ï¼Œè¯·å‚è€ƒä¸‹æ–¹çš„å­¦ä¹ æŒ‡å—ã€‚&lt;/p&gt; 
&lt;h3&gt;âœ¨ ä½ å°†æ”¶è·ä»€ä¹ˆï¼Ÿ&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“– &lt;strong&gt;Datawhale å¼€æºå…è´¹&lt;/strong&gt; å®Œå…¨å…è´¹å­¦ä¹ æœ¬é¡¹ç›®æ‰€æœ‰å†…å®¹ï¼Œä¸ç¤¾åŒºå…±åŒæˆé•¿&lt;/li&gt; 
 &lt;li&gt;ğŸ” &lt;strong&gt;ç†è§£æ ¸å¿ƒåŸç†&lt;/strong&gt; æ·±å…¥ç†è§£æ™ºèƒ½ä½“çš„æ¦‚å¿µã€å†å²ä¸ç»å…¸èŒƒå¼&lt;/li&gt; 
 &lt;li&gt;ğŸ—ï¸ &lt;strong&gt;äº²æ‰‹å®ç°&lt;/strong&gt; æŒæ¡çƒ­é—¨ä½ä»£ç å¹³å°å’Œæ™ºèƒ½ä½“ä»£ç æ¡†æ¶çš„ä½¿ç”¨&lt;/li&gt; 
 &lt;li&gt;ğŸ› ï¸ &lt;strong&gt;è‡ªç ”æ¡†æ¶&lt;a href="https://github.com/jjyaoao/helloagents"&gt;HelloAgents&lt;/a&gt;&lt;/strong&gt; åŸºäº Openai åŸç”Ÿ API ä»é›¶æ„å»ºä¸€ä¸ªè‡ªå·±çš„æ™ºèƒ½ä½“æ¡†æ¶&lt;/li&gt; 
 &lt;li&gt;âš™ï¸ &lt;strong&gt;æŒæ¡é«˜çº§æŠ€èƒ½&lt;/strong&gt; ä¸€æ­¥æ­¥å®ç°ä¸Šä¸‹æ–‡å·¥ç¨‹ã€Memoryã€åè®®ã€è¯„ä¼°ç­‰ç³»ç»Ÿæ€§æŠ€æœ¯&lt;/li&gt; 
 &lt;li&gt;ğŸ¤ &lt;strong&gt;æ¨¡å‹è®­ç»ƒ&lt;/strong&gt; æŒæ¡ Agentic RLï¼Œä» SFT åˆ° GRPO çš„å…¨æµç¨‹å®æˆ˜è®­ç»ƒ LLM&lt;/li&gt; 
 &lt;li&gt;ğŸš€ &lt;strong&gt;é©±åŠ¨çœŸå®æ¡ˆä¾‹&lt;/strong&gt; å®æˆ˜å¼€å‘æ™ºèƒ½æ—…è¡ŒåŠ©æ‰‹ã€èµ›åšå°é•‡ç­‰ç»¼åˆé¡¹ç›®&lt;/li&gt; 
 &lt;li&gt;ğŸ“– &lt;strong&gt;æ±‚èŒé¢è¯•&lt;/strong&gt; å­¦ä¹ æ™ºèƒ½ä½“æ±‚èŒç›¸å…³é¢è¯•é—®é¢˜&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ“– å†…å®¹å¯¼èˆª&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;ç« èŠ‚&lt;/th&gt; 
   &lt;th&gt;å…³é”®å†…å®¹&lt;/th&gt; 
   &lt;th&gt;çŠ¶æ€&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/%E5%89%8D%E8%A8%80.md"&gt;å‰è¨€&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;é¡¹ç›®çš„ç¼˜èµ·ã€èƒŒæ™¯åŠè¯»è€…å»ºè®®&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ç¬¬ä¸€éƒ¨åˆ†ï¼šæ™ºèƒ½ä½“ä¸è¯­è¨€æ¨¡å‹åŸºç¡€&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter1/%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%88%9D%E8%AF%86%E6%99%BA%E8%83%BD%E4%BD%93.md"&gt;ç¬¬ä¸€ç«  åˆè¯†æ™ºèƒ½ä½“&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;æ™ºèƒ½ä½“å®šä¹‰ã€ç±»å‹ã€èŒƒå¼ä¸åº”ç”¨&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter2/%E7%AC%AC%E4%BA%8C%E7%AB%A0%20%E6%99%BA%E8%83%BD%E4%BD%93%E5%8F%91%E5%B1%95%E5%8F%B2.md"&gt;ç¬¬äºŒç«  æ™ºèƒ½ä½“å‘å±•å²&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ä»ç¬¦å·ä¸»ä¹‰åˆ° LLM é©±åŠ¨çš„æ™ºèƒ½ä½“æ¼”è¿›&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter3/%E7%AC%AC%E4%B8%89%E7%AB%A0%20%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80.md"&gt;ç¬¬ä¸‰ç«  å¤§è¯­è¨€æ¨¡å‹åŸºç¡€&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Transformerã€æç¤ºã€ä¸»æµ LLM åŠå…¶å±€é™&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ç¬¬äºŒéƒ¨åˆ†ï¼šæ„å»ºä½ çš„å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter4/%E7%AC%AC%E5%9B%9B%E7%AB%A0%20%E6%99%BA%E8%83%BD%E4%BD%93%E7%BB%8F%E5%85%B8%E8%8C%83%E5%BC%8F%E6%9E%84%E5%BB%BA.md"&gt;ç¬¬å››ç«  æ™ºèƒ½ä½“ç»å…¸èŒƒå¼æ„å»º&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;æ‰‹æŠŠæ‰‹å®ç° ReActã€Plan-and-Solveã€Reflection&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter5/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%9F%BA%E4%BA%8E%E4%BD%8E%E4%BB%A3%E7%A0%81%E5%B9%B3%E5%8F%B0%E7%9A%84%E6%99%BA%E8%83%BD%E4%BD%93%E6%90%AD%E5%BB%BA.md"&gt;ç¬¬äº”ç«  åŸºäºä½ä»£ç å¹³å°çš„æ™ºèƒ½ä½“æ­å»º&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;äº†è§£ Cozeã€Difyã€n8n ç­‰ä½ä»£ç æ™ºèƒ½ä½“å¹³å°ä½¿ç”¨&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter6/%E7%AC%AC%E5%85%AD%E7%AB%A0%20%E6%A1%86%E6%9E%B6%E5%BC%80%E5%8F%91%E5%AE%9E%E8%B7%B5.md"&gt;ç¬¬å…­ç«  æ¡†æ¶å¼€å‘å®è·µ&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;AutoGenã€AgentScopeã€LangGraph ç­‰ä¸»æµæ¡†æ¶åº”ç”¨&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter7/%E7%AC%AC%E4%B8%83%E7%AB%A0%20%E6%9E%84%E5%BB%BA%E4%BD%A0%E7%9A%84Agent%E6%A1%86%E6%9E%B6.md"&gt;ç¬¬ä¸ƒç«  æ„å»ºä½ çš„Agentæ¡†æ¶&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ä» 0 å¼€å§‹æ„å»ºæ™ºèƒ½ä½“æ¡†æ¶&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ç¬¬ä¸‰éƒ¨åˆ†ï¼šé«˜çº§çŸ¥è¯†æ‰©å±•&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter8/%E7%AC%AC%E5%85%AB%E7%AB%A0%20%E8%AE%B0%E5%BF%86%E4%B8%8E%E6%A3%80%E7%B4%A2.md"&gt;ç¬¬å…«ç«  è®°å¿†ä¸æ£€ç´¢&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;è®°å¿†ç³»ç»Ÿï¼ŒRAGï¼Œå­˜å‚¨&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter9/%E7%AC%AC%E4%B9%9D%E7%AB%A0%20%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B.md"&gt;ç¬¬ä¹ç«  ä¸Šä¸‹æ–‡å·¥ç¨‹&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;æŒç»­äº¤äº’çš„"æƒ…å¢ƒç†è§£"&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter10/%E7%AC%AC%E5%8D%81%E7%AB%A0%20%E6%99%BA%E8%83%BD%E4%BD%93%E9%80%9A%E4%BF%A1%E5%8D%8F%E8%AE%AE.md"&gt;ç¬¬åç«  æ™ºèƒ½ä½“é€šä¿¡åè®®&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;MCPã€A2Aã€ANP ç­‰åè®®è§£æ&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter11/%E7%AC%AC%E5%8D%81%E4%B8%80%E7%AB%A0%20Agentic-RL.md"&gt;ç¬¬åä¸€ç«  Agentic-RL&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ä» SFT åˆ° GRPO çš„ LLM è®­ç»ƒå®æˆ˜&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter12/%E7%AC%AC%E5%8D%81%E4%BA%8C%E7%AB%A0%20%E6%99%BA%E8%83%BD%E4%BD%93%E6%80%A7%E8%83%BD%E8%AF%84%E4%BC%B0.md"&gt;ç¬¬åäºŒç«  æ™ºèƒ½ä½“æ€§èƒ½è¯„ä¼°&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;æ ¸å¿ƒæŒ‡æ ‡ã€åŸºå‡†æµ‹è¯•ä¸è¯„ä¼°æ¡†æ¶&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ç¬¬å››éƒ¨åˆ†ï¼šç»¼åˆæ¡ˆä¾‹è¿›é˜¶&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter13/%E7%AC%AC%E5%8D%81%E4%B8%89%E7%AB%A0%20%E6%99%BA%E8%83%BD%E6%97%85%E8%A1%8C%E5%8A%A9%E6%89%8B.md"&gt;ç¬¬åä¸‰ç«  æ™ºèƒ½æ—…è¡ŒåŠ©æ‰‹&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;MCP ä¸å¤šæ™ºèƒ½ä½“åä½œçš„çœŸå®ä¸–ç•Œåº”ç”¨&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter14/%E7%AC%AC%E5%8D%81%E5%9B%9B%E7%AB%A0%20%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A0%94%E7%A9%B6%E6%99%BA%E8%83%BD%E4%BD%93.md"&gt;ç¬¬åå››ç«  è‡ªåŠ¨åŒ–æ·±åº¦ç ”ç©¶æ™ºèƒ½ä½“&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;DeepResearch Agent å¤ç°ä¸è§£æ&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter15/%E7%AC%AC%E5%8D%81%E4%BA%94%E7%AB%A0%20%E6%9E%84%E5%BB%BA%E8%B5%9B%E5%8D%9A%E5%B0%8F%E9%95%87.md"&gt;ç¬¬åäº”ç«  æ„å»ºèµ›åšå°é•‡&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Agent ä¸æ¸¸æˆçš„ç»“åˆï¼Œæ¨¡æ‹Ÿç¤¾ä¼šåŠ¨æ€&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ç¬¬äº”éƒ¨åˆ†ï¼šæ¯•ä¸šè®¾è®¡åŠæœªæ¥å±•æœ›&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter16/%E7%AC%AC%E5%8D%81%E5%85%AD%E7%AB%A0%20%E6%AF%95%E4%B8%9A%E8%AE%BE%E8%AE%A1.md"&gt;ç¬¬åå…­ç«  æ¯•ä¸šè®¾è®¡&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;æ„å»ºå±äºä½ çš„å®Œæ•´å¤šæ™ºèƒ½ä½“åº”ç”¨&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;ç¤¾åŒºè´¡çŒ®ç²¾é€‰ (Community Blog)&lt;/h3&gt; 
&lt;p&gt;â€ƒâ€ƒæ¬¢è¿å¤§å®¶å°†åœ¨å­¦ä¹  Hello-Agents æˆ– Agent ç›¸å…³æŠ€æœ¯ä¸­çš„ç‹¬åˆ°è§è§£ã€å®è·µæ€»ç»“ï¼Œä»¥ PR çš„å½¢å¼è´¡çŒ®åˆ°ç¤¾åŒºç²¾é€‰ã€‚å¦‚æœæ˜¯ç‹¬ç«‹äºæ­£æ–‡çš„å†…å®¹ï¼Œä¹Ÿå¯ä»¥æŠ•ç¨¿è‡³ Extra-Chapterï¼&lt;strong&gt;æœŸå¾…ä½ çš„ç¬¬ä¸€æ¬¡è´¡çŒ®ï¼&lt;/strong&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;ç¤¾åŒºç²¾é€‰&lt;/th&gt; 
   &lt;th&gt;å†…å®¹æ€»ç»“&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/datawhalechina/hello-agents/raw/main/Extra-Chapter/Extra01-%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93.md"&gt;01-Agenté¢è¯•é¢˜æ€»ç»“&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Agent å²—ä½ç›¸å…³é¢è¯•é—®é¢˜&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/datawhalechina/hello-agents/raw/main/Extra-Chapter/Extra01-%E5%8F%82%E8%80%83%E7%AD%94%E6%A1%88.md"&gt;01-Agenté¢è¯•é¢˜ç­”æ¡ˆ&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ç›¸å…³é¢è¯•é—®é¢˜ç­”æ¡ˆ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/datawhalechina/hello-agents/raw/main/Extra-Chapter/Extra02-%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%E8%A1%A5%E5%85%85%E7%9F%A5%E8%AF%86.md"&gt;02-ä¸Šä¸‹æ–‡å·¥ç¨‹å†…å®¹è¡¥å……&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ä¸Šä¸‹æ–‡å·¥ç¨‹å†…å®¹æ‰©å±•&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/datawhalechina/hello-agents/raw/main/Extra-Chapter/Extra03-Dify%E6%99%BA%E8%83%BD%E4%BD%93%E5%88%9B%E5%BB%BA%E4%BF%9D%E5%A7%86%E7%BA%A7%E6%93%8D%E4%BD%9C%E6%B5%81%E7%A8%8B.md"&gt;03-Difyæ™ºèƒ½ä½“åˆ›å»ºä¿å§†çº§æ•™ç¨‹&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Difyæ™ºèƒ½ä½“åˆ›å»ºä¿å§†çº§æ•™ç¨‹&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/datawhalechina/hello-agents/raw/main/Extra-Chapter/Extra04-DatawhaleFAQ.md"&gt;04-Hello-agentsè¯¾ç¨‹å¸¸è§é—®é¢˜&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Datawhaleè¯¾ç¨‹å¸¸è§é—®é¢˜&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;PDF ç‰ˆæœ¬ä¸‹è½½&lt;/h3&gt; 
&lt;p&gt;â€ƒâ€ƒ&lt;em&gt;&lt;strong&gt;æœ¬ Hello-Agents PDF æ•™ç¨‹å®Œå…¨å¼€æºå…è´¹ã€‚ä¸ºé˜²æ­¢å„ç±»è¥é”€å·åŠ æ°´å°åè´©å–ç»™å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåˆå­¦è€…ï¼Œæˆ‘ä»¬ç‰¹åœ°åœ¨ PDF æ–‡ä»¶ä¸­é¢„å…ˆæ·»åŠ äº†ä¸å½±å“é˜…è¯»çš„ Datawhale å¼€æºæ ‡å¿—æ°´å°ï¼Œæ•¬è¯·è°…è§£ï½&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;em&gt;Hello-Agents PDF : &lt;a href="https://github.com/datawhalechina/hello-agents/releases/tag/V1.0.0"&gt;https://github.com/datawhalechina/hello-agents/releases/tag/V1.0.0&lt;/a&gt;&lt;/em&gt;&lt;br /&gt; &lt;em&gt;Hello-Agents PDF å›½å†…ä¸‹è½½åœ°å€ : &lt;a href="https://www.datawhale.cn/learn/summary/239"&gt;https://www.datawhale.cn/learn/summary/239&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;ğŸ’¡ å¦‚ä½•å­¦ä¹ &lt;/h2&gt; 
&lt;p&gt;â€ƒâ€ƒæ¬¢è¿ä½ ï¼Œæœªæ¥çš„æ™ºèƒ½ç³»ç»Ÿæ„å»ºè€…ï¼åœ¨å¼€å¯è¿™æ®µæ¿€åŠ¨äººå¿ƒçš„æ—…ç¨‹ä¹‹å‰ï¼Œè¯·å…è®¸æˆ‘ä»¬ç»™ä½ ä¸€äº›æ¸…æ™°çš„æŒ‡å¼•ã€‚&lt;/p&gt; 
&lt;p&gt;â€ƒâ€ƒæœ¬é¡¹ç›®å†…å®¹å…¼é¡¾ç†è®ºä¸å®æˆ˜ï¼Œæ—¨åœ¨å¸®åŠ©ä½ ç³»ç»Ÿæ€§åœ°æŒæ¡ä»å•ä¸ªæ™ºèƒ½ä½“åˆ°å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„è®¾è®¡ä¸å¼€å‘å…¨æµç¨‹ã€‚å› æ­¤ï¼Œå°¤å…¶é€‚åˆæœ‰ä¸€å®šç¼–ç¨‹åŸºç¡€çš„ &lt;strong&gt;AI å¼€å‘è€…ã€è½¯ä»¶å·¥ç¨‹å¸ˆã€åœ¨æ ¡å­¦ç”Ÿ&lt;/strong&gt; ä»¥åŠå¯¹å‰æ²¿ AI æŠ€æœ¯æŠ±æœ‰æµ“åšå…´è¶£çš„ &lt;strong&gt;è‡ªå­¦è€…&lt;/strong&gt;ã€‚åœ¨å­¦ä¹ æœ¬é¡¹ç›®ä¹‹å‰ï¼Œæˆ‘ä»¬å¸Œæœ›ä½ å…·å¤‡åŸºç¡€çš„ Python ç¼–ç¨‹èƒ½åŠ›ï¼Œå¹¶å¯¹å¤§è¯­è¨€æ¨¡å‹æœ‰åŸºæœ¬çš„æ¦‚å¿µæ€§äº†è§£ï¼ˆä¾‹å¦‚ï¼ŒçŸ¥é“å¦‚ä½•é€šè¿‡ API è°ƒç”¨ä¸€ä¸ª LLMï¼‰ã€‚é¡¹ç›®çš„é‡ç‚¹æ˜¯åº”ç”¨ä¸æ„å»ºï¼Œå› æ­¤ä½ æ— éœ€å…·å¤‡æ·±åšçš„ç®—æ³•æˆ–æ¨¡å‹è®­ç»ƒèƒŒæ™¯ã€‚&lt;/p&gt; 
&lt;p&gt;â€ƒâ€ƒé¡¹ç›®åˆ†ä¸ºäº”å¤§éƒ¨åˆ†ï¼Œæ¯ä¸€éƒ¨åˆ†éƒ½æ˜¯é€šå¾€ä¸‹ä¸€é˜¶æ®µçš„åšå®é˜¶æ¢¯ï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ç¬¬ä¸€éƒ¨åˆ†ï¼šæ™ºèƒ½ä½“ä¸è¯­è¨€æ¨¡å‹åŸºç¡€&lt;/strong&gt;ï¼ˆç¬¬ä¸€ç« ï½ç¬¬ä¸‰ç« ï¼‰ï¼Œæˆ‘ä»¬å°†ä»æ™ºèƒ½ä½“çš„å®šä¹‰ã€ç±»å‹ä¸å‘å±•å†å²è®²èµ·ï¼Œä¸ºä½ æ¢³ç†"æ™ºèƒ½ä½“"è¿™ä¸€æ¦‚å¿µçš„æ¥é¾™å»è„‰ã€‚éšåï¼Œæˆ‘ä»¬ä¼šå¿«é€Ÿå·©å›ºå¤§è¯­è¨€æ¨¡å‹çš„æ ¸å¿ƒçŸ¥è¯†ï¼Œä¸ºä½ çš„å®è·µä¹‹æ—…æ‰“ä¸‹åšå®çš„ç†è®ºåœ°åŸºã€‚&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ç¬¬äºŒéƒ¨åˆ†ï¼šæ„å»ºä½ çš„å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“&lt;/strong&gt;ï¼ˆç¬¬å››ç« ï½ç¬¬ä¸ƒç« ï¼‰ï¼Œè¿™æ˜¯ä½ åŠ¨æ‰‹å®è·µçš„èµ·ç‚¹ã€‚ä½ å°†äº²æ‰‹å®ç° ReAct ç­‰ç»å…¸èŒƒå¼ï¼Œä½“éªŒ Coze ç­‰ä½ä»£ç å¹³å°çš„ä¾¿æ·ï¼Œå¹¶æŒæ¡ Langgraph ç­‰ä¸»æµæ¡†æ¶çš„åº”ç”¨ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬è¿˜ä¼šå¸¦ä½ ä»é›¶å¼€å§‹æ„å»ºä¸€ä¸ªå±äºè‡ªå·±çš„æ™ºèƒ½ä½“æ¡†æ¶ï¼Œè®©ä½ å…¼å…·â€œç”¨è½®å­â€ä¸â€œé€ è½®å­â€çš„èƒ½åŠ›ã€‚&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ç¬¬ä¸‰éƒ¨åˆ†ï¼šé«˜çº§çŸ¥è¯†æ‰©å±•&lt;/strong&gt;ï¼ˆç¬¬å…«ç« ï½ç¬¬åäºŒç« ï¼‰ï¼Œåœ¨è¿™ä¸€éƒ¨åˆ†ï¼Œä½ çš„æ™ºèƒ½ä½“å°†â€œå­¦ä¼šâ€æ€è€ƒä¸åä½œã€‚æˆ‘ä»¬å°†ä½¿ç”¨ç¬¬äºŒéƒ¨åˆ†çš„è‡ªç ”æ¡†æ¶ï¼Œæ·±å…¥æ¢ç´¢è®°å¿†ä¸æ£€ç´¢ã€ä¸Šä¸‹æ–‡å·¥ç¨‹ã€Agent è®­ç»ƒç­‰æ ¸å¿ƒæŠ€æœ¯ï¼Œå¹¶å­¦ä¹ å¤šæ™ºèƒ½ä½“é—´çš„é€šä¿¡åè®®ã€‚æœ€ç»ˆï¼Œä½ å°†æŒæ¡è¯„ä¼°æ™ºèƒ½ä½“ç³»ç»Ÿæ€§èƒ½çš„ä¸“ä¸šæ–¹æ³•ã€‚&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ç¬¬å››éƒ¨åˆ†ï¼šç»¼åˆæ¡ˆä¾‹è¿›é˜¶&lt;/strong&gt;ï¼ˆç¬¬åä¸‰ç« ï½ç¬¬åäº”ç« ï¼‰ï¼Œè¿™é‡Œæ˜¯ç†è®ºä¸å®è·µçš„äº¤æ±‡ç‚¹ã€‚ä½ å°†æŠŠæ‰€å­¦èä¼šè´¯é€šï¼Œäº²æ‰‹æ‰“é€ æ™ºèƒ½æ—…è¡ŒåŠ©æ‰‹ã€è‡ªåŠ¨åŒ–æ·±åº¦ç ”ç©¶æ™ºèƒ½ä½“ï¼Œä¹ƒè‡³ä¸€ä¸ªæ¨¡æ‹Ÿç¤¾ä¼šåŠ¨æ€çš„èµ›åšå°é•‡ï¼Œåœ¨çœŸå®æœ‰è¶£çš„é¡¹ç›®ä¸­æ·¬ç‚¼ä½ çš„æ„å»ºèƒ½åŠ›ã€‚&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ç¬¬äº”éƒ¨åˆ†ï¼šæ¯•ä¸šè®¾è®¡åŠæœªæ¥å±•æœ›&lt;/strong&gt;ï¼ˆç¬¬åå…­ç« ï¼‰ï¼Œåœ¨æ—…ç¨‹çš„ç»ˆç‚¹ï¼Œä½ å°†è¿æ¥ä¸€ä¸ªæ¯•ä¸šè®¾è®¡ï¼Œæ„å»ºä¸€ä¸ªå®Œæ•´çš„ã€å±äºä½ è‡ªå·±çš„å¤šæ™ºèƒ½ä½“åº”ç”¨ï¼Œå…¨é¢æ£€éªŒä½ çš„å­¦ä¹ æˆæœã€‚æˆ‘ä»¬è¿˜å°†ä¸ä½ ä¸€åŒå±•æœ›æ™ºèƒ½ä½“çš„æœªæ¥ï¼Œæ¢ç´¢æ¿€åŠ¨äººå¿ƒçš„å‰æ²¿æ–¹å‘ã€‚&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;â€ƒâ€ƒæ™ºèƒ½ä½“æ˜¯ä¸€ä¸ªé£é€Ÿå‘å±•ä¸”æåº¦ä¾èµ–å®è·µçš„é¢†åŸŸã€‚ä¸ºäº†è·å¾—æœ€ä½³çš„å­¦ä¹ æ•ˆæœï¼Œæˆ‘ä»¬åœ¨é¡¹ç›®çš„&lt;code&gt;code&lt;/code&gt;æ–‡ä»¶å¤¹å†…æä¾›äº†é…å¥—çš„å…¨éƒ¨ä»£ç ï¼Œå¼ºçƒˆå»ºè®®ä½ &lt;strong&gt;å°†ç†è®ºä¸å®è·µç›¸ç»“åˆ&lt;/strong&gt;ã€‚è¯·åŠ¡å¿…äº²æ‰‹è¿è¡Œã€è°ƒè¯•ç”šè‡³ä¿®æ”¹é¡¹ç›®é‡Œæä¾›çš„æ¯ä¸€ä»½ä»£ç ã€‚æ¬¢è¿ä½ éšæ—¶å…³æ³¨ Datawhale ä»¥åŠå…¶ä»– Agent ç›¸å…³ç¤¾åŒºï¼Œå½“é‡åˆ°é—®é¢˜æ—¶ï¼Œä½ å¯ä»¥éšæ—¶åœ¨æœ¬é¡¹ç›®çš„ issue åŒºæé—®ã€‚&lt;/p&gt; 
&lt;p&gt;â€ƒâ€ƒç°åœ¨ï¼Œå‡†å¤‡å¥½è¿›å…¥æ™ºèƒ½ä½“çš„å¥‡å¦™ä¸–ç•Œäº†å—ï¼Ÿè®©æˆ‘ä»¬å³åˆ»å¯ç¨‹ï¼&lt;/p&gt; 
&lt;h2&gt;ä¸‹ä¸€æ­¥è§„åˆ’&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;[]è‹±æ–‡ç‰ˆæ•™ç¨‹&lt;/li&gt; 
 &lt;li&gt;[]åŒè¯­è§†é¢‘è¯¾ç¨‹[è‹±æ–‡+ä¸­æ–‡]ï¼ˆå°†ä¼šæ›´åŠ ç»†è‡´ï¼Œå®è·µè¯¾å¸¦é¢†å¤§å®¶ä»è®¾è®¡æ€è·¯åˆ°å®æ–½ï¼Œæˆäººä»¥é±¼ä¹Ÿæˆäººä»¥æ¸”ï¼‰&lt;/li&gt; 
 &lt;li&gt;[]å…±åˆ›ç¬¬16ç« ï¼ˆæ‰“é€ å„ç±»Agentåº”ç”¨,æ›´æ‰“é€ Agentç”Ÿæ€ï¼‰&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ¤ å¦‚ä½•è´¡çŒ®&lt;/h2&gt; 
&lt;p&gt;æˆ‘ä»¬æ˜¯ä¸€ä¸ªå¼€æ”¾çš„å¼€æºç¤¾åŒºï¼Œæ¬¢è¿ä»»ä½•å½¢å¼çš„è´¡çŒ®ï¼&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ› &lt;strong&gt;æŠ¥å‘Š Bug&lt;/strong&gt; - å‘ç°å†…å®¹æˆ–ä»£ç é—®é¢˜ï¼Œè¯·æäº¤ Issue&lt;/li&gt; 
 &lt;li&gt;ğŸ’¡ &lt;strong&gt;æå‡ºå»ºè®®&lt;/strong&gt; - å¯¹é¡¹ç›®æœ‰å¥½æƒ³æ³•ï¼Œæ¬¢è¿å‘èµ·è®¨è®º&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;strong&gt;å®Œå–„å†…å®¹&lt;/strong&gt; - å¸®åŠ©æ”¹è¿›æ•™ç¨‹ï¼Œæäº¤ä½ çš„ Pull Request&lt;/li&gt; 
 &lt;li&gt;âœï¸ &lt;strong&gt;åˆ†äº«å®è·µ&lt;/strong&gt; - åœ¨"ç¤¾åŒºè´¡çŒ®ç²¾é€‰"ä¸­åˆ†äº«ä½ çš„å­¦ä¹ ç¬”è®°å’Œé¡¹ç›®&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ™ è‡´è°¢&lt;/h2&gt; 
&lt;h3&gt;æ ¸å¿ƒè´¡çŒ®è€…&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/jjyaoao"&gt;é™ˆæ€å·-é¡¹ç›®è´Ÿè´£äºº&lt;/a&gt; (Datawhale æˆå‘˜, å…¨æ–‡å†™ä½œå’Œæ ¡å¯¹)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/fengju0213"&gt;å­™éŸ¬-é¡¹ç›®è´Ÿè´£äºº&lt;/a&gt; (Datawhale æˆå‘˜, ç¬¬ä¹ç« å†…å®¹å’Œæ ¡å¯¹)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Tsumugii24"&gt;å§œèˆ’å‡¡-é¡¹ç›®è´Ÿè´£äºº&lt;/a&gt;ï¼ˆDatawhale æˆå‘˜, ç« èŠ‚ä¹ é¢˜è®¾è®¡å’Œæ ¡å¯¹ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/HeteroCat"&gt;é»„ä½©æ—-Datawhaleæ„å‘æˆå‘˜&lt;/a&gt; (Agent å¼€å‘å·¥ç¨‹å¸ˆ, ç¬¬äº”ç« å†…å®¹è´¡çŒ®è€…)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/fancyboi999"&gt;æ›¾é‘«æ°‘-Agentå·¥ç¨‹å¸ˆ&lt;/a&gt; (ç‰›å®¢ç§‘æŠ€, ç¬¬åå››ç« æ¡ˆä¾‹å¼€å‘)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://xinzhongzhu.github.io/"&gt;æœ±ä¿¡å¿ -æŒ‡å¯¼ä¸“å®¶&lt;/a&gt; (Datawhaleé¦–å¸­ç§‘å­¦å®¶-æµ™æ±Ÿå¸ˆèŒƒå¤§å­¦æ­å·äººå·¥æ™ºèƒ½ç ”ç©¶é™¢æ•™æˆ)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Extra-Chapter è´¡çŒ®è€…&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/WHQAQ11"&gt;WH&lt;/a&gt; (å†…å®¹è´¡çŒ®è€…)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/thunderbolt-fire"&gt;å‘¨å¥¥æ°-DWè´¡çŒ®è€…å›¢é˜Ÿ&lt;/a&gt; (è¥¿å®‰äº¤é€šå¤§å­¦, Extra02 å†…å®¹è´¡çŒ®)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Tasselszcx"&gt;å¼ å®¸æ—­-ä¸ªäººå¼€å‘è€…&lt;/a&gt;(å¸å›½ç†å·¥å­¦é™¢, Extra03 å†…å®¹è´¡çŒ®)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/XiaoMa-PM"&gt;é»„å®æ™—-DWè´¡çŒ®è€…å›¢é˜Ÿ&lt;/a&gt; (æ·±åœ³å¤§å­¦, Extra04 å†…å®¹è´¡çŒ®)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ç‰¹åˆ«æ„Ÿè°¢&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;æ„Ÿè°¢ &lt;a href="https://github.com/Sm1les"&gt;@Sm1les&lt;/a&gt; å¯¹æœ¬é¡¹ç›®çš„å¸®åŠ©ä¸æ”¯æŒ&lt;/li&gt; 
 &lt;li&gt;æ„Ÿè°¢æ‰€æœ‰ä¸ºæœ¬é¡¹ç›®åšå‡ºè´¡çŒ®çš„å¼€å‘è€…ä»¬ â¤ï¸&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align="center" style="margin-top: 30px;"&gt; 
 &lt;a href="https://github.com/datawhalechina/Hello-Agents/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=datawhalechina/Hello-Agents" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/images/star-history-20251212.png" alt="Datawhale" width="90%" /&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;â­ å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·ç»™æˆ‘ä»¬ä¸€ä¸ª Starï¼&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;å…³äº Datawhale&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/images/datawhale.png" alt="Datawhale" width="30%" /&gt; 
 &lt;p&gt;æ‰«æäºŒç»´ç å…³æ³¨ Datawhale å…¬ä¼—å·ï¼Œè·å–æ›´å¤šä¼˜è´¨å¼€æºå†…å®¹&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ“œ å¼€æºåè®®&lt;/h2&gt; 
&lt;p&gt;æœ¬ä½œå“é‡‡ç”¨&lt;a href="http://creativecommons.org/licenses/by-nc-sa/4.0/"&gt;çŸ¥è¯†å…±äº«ç½²å-éå•†ä¸šæ€§ä½¿ç”¨-ç›¸åŒæ–¹å¼å…±äº« 4.0 å›½é™…è®¸å¯åè®®&lt;/a&gt;è¿›è¡Œè®¸å¯ã€‚&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>BeehiveInnovations/pal-mcp-server</title>
      <link>https://github.com/BeehiveInnovations/pal-mcp-server</link>
      <description>&lt;p&gt;The power of Claude Code / GeminiCLI / CodexCLI + [Gemini / OpenAI / OpenRouter / Azure / Grok / Ollama / Custom Model / All Of The Above] working as one.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;PAL MCP: Many Workflows. One Context.&lt;/h1&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;em&gt;Your AI's PAL â€“ a Provider Abstraction Layer&lt;/em&gt;&lt;br /&gt; &lt;sub&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/name-change.md"&gt;Formerly known as Zen MCP&lt;/a&gt;&lt;/sub&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/0d26061e-5f21-4ab1-b7d0-f883ddc2c3da"&gt;PAL in action&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;ğŸ‘‰ &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/#-watch-tools-in-action"&gt;Watch more examples&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
 &lt;h3&gt;Your CLI + Multiple Models = Your AI Dev Team&lt;/h3&gt; 
 &lt;p&gt;&lt;strong&gt;Use the ğŸ¤– CLI you love:&lt;/strong&gt;&lt;br /&gt; &lt;a href="https://www.anthropic.com/claude-code"&gt;Claude Code&lt;/a&gt; Â· &lt;a href="https://github.com/google-gemini/gemini-cli"&gt;Gemini CLI&lt;/a&gt; Â· &lt;a href="https://github.com/openai/codex"&gt;Codex CLI&lt;/a&gt; Â· &lt;a href="https://qwenlm.github.io/qwen-code-docs/"&gt;Qwen Code CLI&lt;/a&gt; Â· &lt;a href="https://cursor.com"&gt;Cursor&lt;/a&gt; Â· &lt;em&gt;and more&lt;/em&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;With multiple models within a single prompt:&lt;/strong&gt;&lt;br /&gt; Gemini Â· OpenAI Â· Anthropic Â· Grok Â· Azure Â· Ollama Â· OpenRouter Â· DIAL Â· On-Device Model&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ†• Now with CLI-to-CLI Bridge&lt;/h2&gt; 
&lt;p&gt;The new &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/tools/clink.md"&gt;&lt;code&gt;clink&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt; (CLI + Link) tool connects external AI CLIs directly into your workflow:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Connect external CLIs&lt;/strong&gt; like &lt;a href="https://github.com/google-gemini/gemini-cli"&gt;Gemini CLI&lt;/a&gt;, &lt;a href="https://github.com/openai/codex"&gt;Codex CLI&lt;/a&gt;, and &lt;a href="https://www.anthropic.com/claude-code"&gt;Claude Code&lt;/a&gt; directly into your workflow&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;CLI Subagents&lt;/strong&gt; - Launch isolated CLI instances from &lt;em&gt;within&lt;/em&gt; your current CLI! Claude Code can spawn Codex subagents, Codex can spawn Gemini CLI subagents, etc. Offload heavy tasks (code reviews, bug hunting) to fresh contexts while your main session's context window remains unpolluted. Each subagent returns only final results.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Context Isolation&lt;/strong&gt; - Run separate investigations without polluting your primary workspace&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Role Specialization&lt;/strong&gt; - Spawn &lt;code&gt;planner&lt;/code&gt;, &lt;code&gt;codereviewer&lt;/code&gt;, or custom role agents with specialized system prompts&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Full CLI Capabilities&lt;/strong&gt; - Web search, file inspection, MCP tool access, latest documentation lookups&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Seamless Continuity&lt;/strong&gt; - Sub-CLIs participate as first-class members with full conversation context between tools&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Codex spawns Codex subagent for isolated code review in fresh context
clink with codex codereviewer to audit auth module for security issues
# Subagent reviews in isolation, returns final report without cluttering your context as codex reads each file and walks the directory structure

# Consensus from different AI models â†’ Implementation handoff with full context preservation between tools
Use consensus with gpt-5 and gemini-pro to decide: dark mode or offline support next
Continue with clink gemini - implement the recommended feature
# Gemini receives full debate context and starts coding immediately
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;ğŸ‘‰ &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/tools/clink.md"&gt;Learn more about clink&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Why PAL MCP?&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Why rely on one AI model when you can orchestrate them all?&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;A Model Context Protocol server that supercharges tools like &lt;a href="https://www.anthropic.com/claude-code"&gt;Claude Code&lt;/a&gt;, &lt;a href="https://developers.openai.com/codex/cli"&gt;Codex CLI&lt;/a&gt;, and IDE clients such as &lt;a href="https://cursor.com"&gt;Cursor&lt;/a&gt; or the &lt;a href="https://marketplace.visualstudio.com/items?itemName=Anthropic.claude-vscode"&gt;Claude Dev VS Code extension&lt;/a&gt;. &lt;strong&gt;PAL MCP connects your favorite AI tool to multiple AI models&lt;/strong&gt; for enhanced code analysis, problem-solving, and collaborative development.&lt;/p&gt; 
&lt;h3&gt;True AI Collaboration with Conversation Continuity&lt;/h3&gt; 
&lt;p&gt;PAL supports &lt;strong&gt;conversation threading&lt;/strong&gt; so your CLI can &lt;strong&gt;discuss ideas with multiple AI models, exchange reasoning, get second opinions, and even run collaborative debates between models&lt;/strong&gt; to help you reach deeper insights and better solutions.&lt;/p&gt; 
&lt;p&gt;Your CLI always stays in control but gets perspectives from the best AI for each subtask. Context carries forward seamlessly across tools and models, enabling complex workflows like: code reviews with multiple models â†’ automated planning â†’ implementation â†’ pre-commit validation.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;You're in control.&lt;/strong&gt; Your CLI of choice orchestrates the AI team, but you decide the workflow. Craft powerful prompts that bring in Gemini Pro, GPT 5, Flash, or local offline models exactly when needed.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Reasons to Use PAL MCP&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;A typical workflow with Claude Code as an example:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Multi-Model Orchestration&lt;/strong&gt; - Claude coordinates with Gemini Pro, O3, GPT-5, and 50+ other models to get the best analysis for each task&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Context Revival Magic&lt;/strong&gt; - Even after Claude's context resets, continue conversations seamlessly by having other models "remind" Claude of the discussion&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Guided Workflows&lt;/strong&gt; - Enforces systematic investigation phases that prevent rushed analysis and ensure thorough code examination&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Extended Context Windows&lt;/strong&gt; - Break Claude's limits by delegating to Gemini (1M tokens) or O3 (200K tokens) for massive codebases&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;True Conversation Continuity&lt;/strong&gt; - Full context flows across tools and models - Gemini remembers what O3 said 10 steps ago&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Model-Specific Strengths&lt;/strong&gt; - Extended thinking with Gemini Pro, blazing speed with Flash, strong reasoning with O3, privacy with local Ollama&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Professional Code Reviews&lt;/strong&gt; - Multi-pass analysis with severity levels, actionable feedback, and consensus from multiple AI experts&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Smart Debugging Assistant&lt;/strong&gt; - Systematic root cause analysis with hypothesis tracking and confidence levels&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Automatic Model Selection&lt;/strong&gt; - Claude intelligently picks the right model for each subtask (or you can specify)&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Vision Capabilities&lt;/strong&gt; - Analyze screenshots, diagrams, and visual content with vision-enabled models&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Local Model Support&lt;/strong&gt; - Run Llama, Mistral, or other models locally for complete privacy and zero API costs&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Bypass MCP Token Limits&lt;/strong&gt; - Automatically works around MCP's 25K limit for large prompts and responses&lt;/p&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;&lt;strong&gt;The Killer Feature:&lt;/strong&gt; When Claude's context resets, just ask to "continue with O3" - the other model's response magically revives Claude's understanding without re-ingesting documents!&lt;/p&gt; 
 &lt;h4&gt;Example: Multi-Model Code Review Workflow&lt;/h4&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;code&gt;Perform a codereview using gemini pro and o3 and use planner to generate a detailed plan, implement the fixes and do a final precommit check by continuing from the previous codereview&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;This triggers a &lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/tools/codereview.md"&gt;&lt;code&gt;codereview&lt;/code&gt;&lt;/a&gt; workflow where Claude walks the code, looking for all kinds of issues&lt;/li&gt; 
  &lt;li&gt;After multiple passes, collects relevant code and makes note of issues along the way&lt;/li&gt; 
  &lt;li&gt;Maintains a &lt;code&gt;confidence&lt;/code&gt; level between &lt;code&gt;exploring&lt;/code&gt;, &lt;code&gt;low&lt;/code&gt;, &lt;code&gt;medium&lt;/code&gt;, &lt;code&gt;high&lt;/code&gt; and &lt;code&gt;certain&lt;/code&gt; to track how confidently it's been able to find and identify issues&lt;/li&gt; 
  &lt;li&gt;Generates a detailed list of critical -&amp;gt; low issues&lt;/li&gt; 
  &lt;li&gt;Shares the relevant files, findings, etc with &lt;strong&gt;Gemini Pro&lt;/strong&gt; to perform a deep dive for a second &lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/tools/codereview.md"&gt;&lt;code&gt;codereview&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Comes back with a response and next does the same with o3, adding to the prompt if a new discovery comes to light&lt;/li&gt; 
  &lt;li&gt;When done, Claude takes in all the feedback and combines a single list of all critical -&amp;gt; low issues, including good patterns in your code. The final list includes new findings or revisions in case Claude misunderstood or missed something crucial and one of the other models pointed this out&lt;/li&gt; 
  &lt;li&gt;It then uses the &lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/tools/planner.md"&gt;&lt;code&gt;planner&lt;/code&gt;&lt;/a&gt; workflow to break the work down into simpler steps if a major refactor is required&lt;/li&gt; 
  &lt;li&gt;Claude then performs the actual work of fixing highlighted issues&lt;/li&gt; 
  &lt;li&gt;When done, Claude returns to Gemini Pro for a &lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/tools/precommit.md"&gt;&lt;code&gt;precommit&lt;/code&gt;&lt;/a&gt; review&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;All within a single conversation thread! Gemini Pro in step 11 &lt;em&gt;knows&lt;/em&gt; what was recommended by O3 in step 7! Taking that context and review into consideration to aid with its final pre-commit review.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Think of it as Claude Code &lt;em&gt;for&lt;/em&gt; Claude Code.&lt;/strong&gt; This MCP isn't magic. It's just &lt;strong&gt;super-glue&lt;/strong&gt;.&lt;/p&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;&lt;strong&gt;Remember:&lt;/strong&gt; Claude stays in full control â€” but &lt;strong&gt;YOU&lt;/strong&gt; call the shots. PAL is designed to have Claude engage other models only when needed â€” and to follow through with meaningful back-and-forth. &lt;strong&gt;You're&lt;/strong&gt; the one who crafts the powerful prompt that makes Claude bring in Gemini, Flash, O3 â€” or fly solo. You're the guide. The prompter. The puppeteer.&lt;/p&gt; 
  &lt;h4&gt;You are the AI - &lt;strong&gt;Actually Intelligent&lt;/strong&gt;.&lt;/h4&gt; 
 &lt;/blockquote&gt; 
&lt;/details&gt; 
&lt;h4&gt;Recommended AI Stack&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;For Claude Code Users&lt;/summary&gt; 
 &lt;p&gt;For best results when using &lt;a href="https://claude.ai/code"&gt;Claude Code&lt;/a&gt;:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Sonnet 4.5&lt;/strong&gt; - All agentic work and orchestration&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Gemini 3.0 Pro&lt;/strong&gt; OR &lt;strong&gt;GPT-5.2 / Pro&lt;/strong&gt; - Deep thinking, additional code reviews, debugging and validations, pre-commit analysis&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;For Codex Users&lt;/summary&gt; 
 &lt;p&gt;For best results when using &lt;a href="https://developers.openai.com/codex/cli"&gt;Codex CLI&lt;/a&gt;:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;GPT-5.2 Codex Medium&lt;/strong&gt; - All agentic work and orchestration&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Gemini 3.0 Pro&lt;/strong&gt; OR &lt;strong&gt;GPT-5.2-Pro&lt;/strong&gt; - Deep thinking, additional code reviews, debugging and validations, pre-commit analysis&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;Quick Start (5 minutes)&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Prerequisites:&lt;/strong&gt; Python 3.10+, Git, &lt;a href="https://docs.astral.sh/uv/getting-started/installation/"&gt;uv installed&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;1. Get API Keys&lt;/strong&gt; (choose one or more):&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://openrouter.ai/"&gt;OpenRouter&lt;/a&gt;&lt;/strong&gt; - Access multiple models with one API&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://makersuite.google.com/app/apikey"&gt;Gemini&lt;/a&gt;&lt;/strong&gt; - Google's latest models&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://platform.openai.com/api-keys"&gt;OpenAI&lt;/a&gt;&lt;/strong&gt; - O3, GPT-5 series&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://learn.microsoft.com/azure/ai-services/openai/"&gt;Azure OpenAI&lt;/a&gt;&lt;/strong&gt; - Enterprise deployments of GPT-4o, GPT-4.1, GPT-5 family&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://console.x.ai/"&gt;X.AI&lt;/a&gt;&lt;/strong&gt; - Grok models&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://dialx.ai/"&gt;DIAL&lt;/a&gt;&lt;/strong&gt; - Vendor-agnostic model access&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://ollama.ai/"&gt;Ollama&lt;/a&gt;&lt;/strong&gt; - Local models (free)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;2. Install&lt;/strong&gt; (choose one):&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Option A: Clone and Automatic Setup&lt;/strong&gt; (recommended)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/BeehiveInnovations/pal-mcp-server.git
cd pal-mcp-server

# Handles everything: setup, config, API keys from system environment. 
# Auto-configures Claude Desktop, Claude Code, Gemini CLI, Codex CLI, Qwen CLI
# Enable / disable additional settings in .env
./run-server.sh  
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Option B: Instant Setup with &lt;a href="https://docs.astral.sh/uv/getting-started/installation/"&gt;uvx&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;// Add to ~/.claude/settings.json or .mcp.json
// Don't forget to add your API keys under env
{
  "mcpServers": {
    "pal": {
      "command": "bash",
      "args": ["-c", "for p in $(which uvx 2&amp;gt;/dev/null) $HOME/.local/bin/uvx /opt/homebrew/bin/uvx /usr/local/bin/uvx uvx; do [ -x \"$p\" ] &amp;amp;&amp;amp; exec \"$p\" --from git+https://github.com/BeehiveInnovations/pal-mcp-server.git pal-mcp-server; done; echo 'uvx not found' &amp;gt;&amp;amp;2; exit 1"],
      "env": {
        "PATH": "/usr/local/bin:/usr/bin:/bin:/opt/homebrew/bin:~/.local/bin",
        "GEMINI_API_KEY": "your-key-here",
        "DISABLED_TOOLS": "analyze,refactor,testgen,secaudit,docgen,tracer",
        "DEFAULT_MODEL": "auto"
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;3. Start Using!&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;"Use pal to analyze this code for security issues with gemini pro"
"Debug this error with o3 and then get flash to suggest optimizations"
"Plan the migration strategy with pal, get consensus from multiple models"
"clink with cli_name=\"gemini\" role=\"planner\" to draft a phased rollout plan"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;ğŸ‘‰ &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/getting-started.md"&gt;Complete Setup Guide&lt;/a&gt;&lt;/strong&gt; with detailed installation, configuration for Gemini / Codex / Qwen, and troubleshooting ğŸ‘‰ &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/getting-started.md#ide-clients"&gt;Cursor &amp;amp; VS Code Setup&lt;/a&gt;&lt;/strong&gt; for IDE integration instructions ğŸ“º &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/#-watch-tools-in-action"&gt;Watch tools in action&lt;/a&gt;&lt;/strong&gt; to see real-world examples&lt;/p&gt; 
&lt;h2&gt;Provider Configuration&lt;/h2&gt; 
&lt;p&gt;PAL activates any provider that has credentials in your &lt;code&gt;.env&lt;/code&gt;. See &lt;code&gt;.env.example&lt;/code&gt; for deeper customization.&lt;/p&gt; 
&lt;h2&gt;Core Tools&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Each tool comes with its own multi-step workflow, parameters, and descriptions that consume valuable context window space even when not in use. To optimize performance, some tools are disabled by default. See &lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/#tool-configuration"&gt;Tool Configuration&lt;/a&gt; below to enable them.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;Collaboration &amp;amp; Planning&lt;/strong&gt; &lt;em&gt;(Enabled by default)&lt;/em&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/tools/clink.md"&gt;&lt;code&gt;clink&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt; - Bridge requests to external AI CLIs (Gemini planner, codereviewer, etc.)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/tools/chat.md"&gt;&lt;code&gt;chat&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt; - Brainstorm ideas, get second opinions, validate approaches. With capable models (GPT-5.2 Pro, Gemini 3.0 Pro), generates complete code / implementation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/tools/thinkdeep.md"&gt;&lt;code&gt;thinkdeep&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt; - Extended reasoning, edge case analysis, alternative perspectives&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/tools/planner.md"&gt;&lt;code&gt;planner&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt; - Break down complex projects into structured, actionable plans&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/tools/consensus.md"&gt;&lt;code&gt;consensus&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt; - Get expert opinions from multiple AI models with stance steering&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Code Analysis &amp;amp; Quality&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/tools/debug.md"&gt;&lt;code&gt;debug&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt; - Systematic investigation and root cause analysis&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/tools/precommit.md"&gt;&lt;code&gt;precommit&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt; - Validate changes before committing, prevent regressions&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/tools/codereview.md"&gt;&lt;code&gt;codereview&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt; - Professional reviews with severity levels and actionable feedback&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/tools/analyze.md"&gt;&lt;code&gt;analyze&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt; &lt;em&gt;(disabled by default - &lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/#tool-configuration"&gt;enable&lt;/a&gt;)&lt;/em&gt; - Understand architecture, patterns, dependencies across entire codebases&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Development Tools&lt;/strong&gt; &lt;em&gt;(Disabled by default - &lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/#tool-configuration"&gt;enable&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/tools/refactor.md"&gt;&lt;code&gt;refactor&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt; - Intelligent code refactoring with decomposition focus&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/tools/testgen.md"&gt;&lt;code&gt;testgen&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt; - Comprehensive test generation with edge cases&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/tools/secaudit.md"&gt;&lt;code&gt;secaudit&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt; - Security audits with OWASP Top 10 analysis&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/tools/docgen.md"&gt;&lt;code&gt;docgen&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt; - Generate documentation with complexity analysis&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Utilities&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/tools/apilookup.md"&gt;&lt;code&gt;apilookup&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt; - Forces current-year API/SDK documentation lookups in a sub-process (saves tokens within the current context window), prevents outdated training data responses&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/tools/challenge.md"&gt;&lt;code&gt;challenge&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt; - Prevent "You're absolutely right!" responses with critical analysis&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/tools/tracer.md"&gt;&lt;code&gt;tracer&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt; &lt;em&gt;(disabled by default - &lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/#tool-configuration"&gt;enable&lt;/a&gt;)&lt;/em&gt; - Static analysis prompts for call-flow mapping&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b id="tool-configuration"&gt;ğŸ‘‰ Tool Configuration&lt;/b&gt;&lt;/summary&gt; 
 &lt;h3&gt;Default Configuration&lt;/h3&gt; 
 &lt;p&gt;To optimize context window usage, only essential tools are enabled by default:&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Enabled by default:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;chat&lt;/code&gt;, &lt;code&gt;thinkdeep&lt;/code&gt;, &lt;code&gt;planner&lt;/code&gt;, &lt;code&gt;consensus&lt;/code&gt; - Core collaboration tools&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;codereview&lt;/code&gt;, &lt;code&gt;precommit&lt;/code&gt;, &lt;code&gt;debug&lt;/code&gt; - Essential code quality tools&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;apilookup&lt;/code&gt; - Rapid API/SDK information lookup&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;challenge&lt;/code&gt; - Critical thinking utility&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;Disabled by default:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;analyze&lt;/code&gt;, &lt;code&gt;refactor&lt;/code&gt;, &lt;code&gt;testgen&lt;/code&gt;, &lt;code&gt;secaudit&lt;/code&gt;, &lt;code&gt;docgen&lt;/code&gt;, &lt;code&gt;tracer&lt;/code&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;Enabling Additional Tools&lt;/h3&gt; 
 &lt;p&gt;To enable additional tools, remove them from the &lt;code&gt;DISABLED_TOOLS&lt;/code&gt; list:&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Option 1: Edit your .env file&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Default configuration (from .env.example)
DISABLED_TOOLS=analyze,refactor,testgen,secaudit,docgen,tracer

# To enable specific tools, remove them from the list
# Example: Enable analyze tool
DISABLED_TOOLS=refactor,testgen,secaudit,docgen,tracer

# To enable ALL tools
DISABLED_TOOLS=
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Option 2: Configure in MCP settings&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;// In ~/.claude/settings.json or .mcp.json
{
  "mcpServers": {
    "pal": {
      "env": {
        // Tool configuration
        "DISABLED_TOOLS": "refactor,testgen,secaudit,docgen,tracer",
        "DEFAULT_MODEL": "pro",
        "DEFAULT_THINKING_MODE_THINKDEEP": "high",
        
        // API configuration
        "GEMINI_API_KEY": "your-gemini-key",
        "OPENAI_API_KEY": "your-openai-key",
        "OPENROUTER_API_KEY": "your-openrouter-key",
        
        // Logging and performance
        "LOG_LEVEL": "INFO",
        "CONVERSATION_TIMEOUT_HOURS": "6",
        "MAX_CONVERSATION_TURNS": "50"
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Option 3: Enable all tools&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;// Remove or empty the DISABLED_TOOLS to enable everything
{
  "mcpServers": {
    "pal": {
      "env": {
        "DISABLED_TOOLS": ""
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Essential tools (&lt;code&gt;version&lt;/code&gt;, &lt;code&gt;listmodels&lt;/code&gt;) cannot be disabled&lt;/li&gt; 
  &lt;li&gt;After changing tool configuration, restart your Claude session for changes to take effect&lt;/li&gt; 
  &lt;li&gt;Each tool adds to context window usage, so only enable what you need&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;ğŸ“º Watch Tools In Action&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Chat Tool&lt;/b&gt; - Collaborative decision making and multi-turn conversations&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;Picking Redis vs Memcached:&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/41076cfe-dd49-4dfc-82f5-d7461b34705d"&gt;Chat Redis or Memcached_web.webm&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Multi-turn conversation with continuation:&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/37bd57ca-e8a6-42f7-b5fb-11de271e95db"&gt;Chat With Gemini_web.webm&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Consensus Tool&lt;/b&gt; - Multi-model debate and decision making&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;Multi-model consensus debate:&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/76a23dd5-887a-4382-9cf0-642f5cf6219e"&gt;PAL Consensus Debate&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;PreCommit Tool&lt;/b&gt; - Comprehensive change validation&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;Pre-commit validation workflow:&lt;/strong&gt;&lt;/p&gt; 
 &lt;div align="center"&gt; 
  &lt;img src="https://github.com/user-attachments/assets/584adfa6-d252-49b4-b5b0-0cd6e97fb2c6" width="950" /&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;API Lookup Tool&lt;/b&gt; - Current vs outdated API documentation&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;Without PAL - outdated APIs:&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/01a79dc9-ad16-4264-9ce1-76a56c3580ee"&gt;API without PAL&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;With PAL - current APIs:&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/5c847326-4b66-41f7-8f30-f380453dce22"&gt;API with PAL&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Challenge Tool&lt;/b&gt; - Critical thinking vs reflexive agreement&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;Without PAL:&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/64f3c9fb-7ca9-4876-b687-25e847edfd87" alt="without_pal@2x" /&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;With PAL:&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/9d72f444-ba53-4ab1-83e5-250062c6ee70" alt="with_pal@2x" /&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;AI Orchestration&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Auto model selection&lt;/strong&gt; - Claude picks the right AI for each task&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-model workflows&lt;/strong&gt; - Chain different models in single conversations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Conversation continuity&lt;/strong&gt; - Context preserved across tools and models&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/context-revival.md"&gt;Context revival&lt;/a&gt;&lt;/strong&gt; - Continue conversations even after context resets&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Model Support&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Multiple providers&lt;/strong&gt; - Gemini, OpenAI, Azure, X.AI, OpenRouter, DIAL, Ollama&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Latest models&lt;/strong&gt; - GPT-5, Gemini 3.0 Pro, O3, Grok-4, local Llama&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/advanced-usage.md#thinking-modes"&gt;Thinking modes&lt;/a&gt;&lt;/strong&gt; - Control reasoning depth vs cost&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Vision support&lt;/strong&gt; - Analyze images, diagrams, screenshots&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Developer Experience&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Guided workflows&lt;/strong&gt; - Systematic investigation prevents rushed analysis&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Smart file handling&lt;/strong&gt; - Auto-expand directories, manage token limits&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Web search integration&lt;/strong&gt; - Access current documentation and best practices&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/advanced-usage.md#working-with-large-prompts"&gt;Large prompt support&lt;/a&gt;&lt;/strong&gt; - Bypass MCP's 25K token limit&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Example Workflows&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Multi-model Code Review:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;"Perform a codereview using gemini pro and o3, then use planner to create a fix strategy"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;â†’ Claude reviews code systematically â†’ Consults Gemini Pro â†’ Gets O3's perspective â†’ Creates unified action plan&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Collaborative Debugging:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;"Debug this race condition with max thinking mode, then validate the fix with precommit"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;â†’ Deep investigation â†’ Expert analysis â†’ Solution implementation â†’ Pre-commit validation&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Architecture Planning:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;"Plan our microservices migration, get consensus from pro and o3 on the approach"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;â†’ Structured planning â†’ Multiple expert opinions â†’ Consensus building â†’ Implementation roadmap&lt;/p&gt; 
&lt;p&gt;ğŸ‘‰ &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/advanced-usage.md"&gt;Advanced Usage Guide&lt;/a&gt;&lt;/strong&gt; for complex workflows, model configuration, and power-user features&lt;/p&gt; 
&lt;h2&gt;Quick Links&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;ğŸ“– Documentation&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/index.md"&gt;Docs Overview&lt;/a&gt; - High-level map of major guides&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/getting-started.md"&gt;Getting Started&lt;/a&gt; - Complete setup guide&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/tools/"&gt;Tools Reference&lt;/a&gt; - All tools with examples&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/advanced-usage.md"&gt;Advanced Usage&lt;/a&gt; - Power user features&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/configuration.md"&gt;Configuration&lt;/a&gt; - Environment variables, restrictions&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/adding_providers.md"&gt;Adding Providers&lt;/a&gt; - Provider-specific setup (OpenAI, Azure, custom gateways)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/model_ranking.md"&gt;Model Ranking Guide&lt;/a&gt; - How intelligence scores drive auto-mode suggestions&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;ğŸ”§ Setup &amp;amp; Support&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/wsl-setup.md"&gt;WSL Setup&lt;/a&gt; - Windows users&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/troubleshooting.md"&gt;Troubleshooting&lt;/a&gt; - Common issues&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/docs/contributions.md"&gt;Contributing&lt;/a&gt; - Code standards, PR process&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Apache 2.0 License - see &lt;a href="https://raw.githubusercontent.com/BeehiveInnovations/pal-mcp-server/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;h2&gt;Acknowledgments&lt;/h2&gt; 
&lt;p&gt;Built with the power of &lt;strong&gt;Multi-Model AI&lt;/strong&gt; collaboration ğŸ¤&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;A&lt;/strong&gt;ctual &lt;strong&gt;I&lt;/strong&gt;ntelligence by real Humans&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://modelcontextprotocol.com"&gt;MCP (Model Context Protocol)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://developers.openai.com/codex/cli"&gt;Codex CLI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://claude.ai/code"&gt;Claude Code&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ai.google.dev/"&gt;Gemini&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openai.com/"&gt;OpenAI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://learn.microsoft.com/azure/ai-services/openai/"&gt;Azure OpenAI&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Star History&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://www.star-history.com/#BeehiveInnovations/pal-mcp-server&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=BeehiveInnovations/pal-mcp-server&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>CVHub520/X-AnyLabeling</title>
      <link>https://github.com/CVHub520/X-AnyLabeling</link>
      <description>&lt;p&gt;Effortless data labeling with AI support from Segment Anything and other awesome models.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p&gt; &lt;a href="https://github.com/CVHub520/X-AnyLabeling/" target="_blank"&gt; &lt;img alt="X-AnyLabeling" height="200px" src="https://github.com/user-attachments/assets/0714a182-92bd-4b47-b48d-1c5d7c225176" /&gt;&lt;/a&gt; &lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/README.md"&gt;English&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/README_zh-CN.md"&gt;ç®€ä½“ä¸­æ–‡&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p align="center"&gt; &lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/License-LGPL%20v3-blue.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href=""&gt;&lt;img src="https://img.shields.io/github/v/release/CVHub520/X-AnyLabeling?color=ffa" /&gt;&lt;/a&gt; &lt;a href=""&gt;&lt;img src="https://img.shields.io/pypi/v/x-anylabeling-cvhub?logo=pypi&amp;amp;logoColor=white" /&gt;&lt;/a&gt; &lt;a href=""&gt;&lt;img src="https://img.shields.io/badge/python-3.10+-aff.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href=""&gt;&lt;img src="https://img.shields.io/badge/os-linux%2C%20win%2C%20mac-pink.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href=""&gt;&lt;img src="https://img.shields.io/github/downloads/CVHub520/X-AnyLabeling/total?label=downloads" /&gt;&lt;/a&gt; &lt;a href="https://modelscope.cn/collections/X-AnyLabeling-7b0e1798bcda43"&gt;&lt;img src="https://img.shields.io/badge/modelscope-X--AnyLabeling-6750FF?link=https%3A%2F%2Fmodelscope.cn%2Fcollections%2FX-AnyLabeling-7b0e1798bcda43" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/18329471/234640541-a6a65fbc-d7a5-4ec3-9b65-55305b01a7aa.png" alt="" /&gt;&lt;/p&gt; 
&lt;img src="https://github.com/user-attachments/assets/8b5f290a-dddf-410c-a004-21e5a7bcd1cc" width="100%" /&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Auto-Training&lt;/strong&gt;&lt;/summary&gt; 
 &lt;video src="https://github.com/user-attachments/assets/c0ab2056-2743-4a2c-ba93-13f478d3481e" width="100%" controls&gt; 
 &lt;/video&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Auto-Labeling&lt;/strong&gt;&lt;/summary&gt; 
 &lt;video src="https://github.com/user-attachments/assets/f517fa94-c49c-4f05-864e-96b34f592079" width="100%" controls&gt; 
 &lt;/video&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Detect Anything&lt;/strong&gt;&lt;/summary&gt; 
 &lt;img src="https://github.com/user-attachments/assets/7f43bcec-96fd-48d1-bd36-9e5a440a66f6" width="100%" /&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Segment Anything&lt;/strong&gt;&lt;/summary&gt; 
 &lt;img src="https://github.com/user-attachments/assets/208dc9ed-b8c9-4127-9e5b-e76f53892f03" width="100%" /&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Promptable Concept Grounding&lt;/strong&gt;&lt;/summary&gt; 
 &lt;video src="https://github.com/user-attachments/assets/52cbdb5d-cc60-4be5-826f-903ea4330ca8" width="100%" controls&gt; 
 &lt;/video&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;VQA&lt;/strong&gt;&lt;/summary&gt; 
 &lt;video src="https://github.com/user-attachments/assets/53adcff4-b962-41b7-a408-3afecd8d8c82" width="100%" controls&gt; 
 &lt;/video&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Chatbot&lt;/strong&gt;&lt;/summary&gt; 
 &lt;img src="https://github.com/user-attachments/assets/56c9a20b-c836-47aa-8b54-bad5bb99b735" width="100%" /&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Image Classifier&lt;/strong&gt;&lt;/summary&gt; 
 &lt;video src="https://github.com/user-attachments/assets/0652adfb-48a4-4219-9b18-16ff5ce31be0" width="100%" controls&gt; 
 &lt;/video&gt; 
&lt;/details&gt; 
&lt;h2&gt;ğŸ¥³ What's New&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Add support for &lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/grounding/sam3/README.md"&gt;Segment Anything 3&lt;/a&gt; model with text and visual promptable segmentation (#1207)&lt;/li&gt; 
 &lt;li&gt;Add TinyObj mode for Segment Anything Model to improve small object detection accuracy in high-resolution images by local cropping (#1193)&lt;/li&gt; 
 &lt;li&gt;For more details, please refer to the &lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/CHANGELOG.md"&gt;CHANGELOG&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;X-AnyLabeling&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;X-AnyLabeling&lt;/strong&gt; is a powerful annotation tool that integrates an AI engine for fast and automatic labeling. It's designed for multi-modal data engineers, offering industrial-grade solutions for complex tasks.&lt;/p&gt; 
&lt;img src="https://github.com/user-attachments/assets/632e629b-0dec-407b-95a6-728052e1dd7b" width="100%" /&gt; 
&lt;p&gt;Also, we highly recommend trying out &lt;a href="https://github.com/CVHub520/X-AnyLabeling-Server"&gt;X-AnyLabeling-Server&lt;/a&gt;, a simple, lightweight, and extensible framework that enables remote inference capabilities for X-AnyLabeling.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;img src="https://github.com/user-attachments/assets/c65db18f-167b-49e8-bea3-fcf4b43a8ffd" width="100%" /&gt; 
&lt;ul&gt; 
 &lt;li&gt;Supports remote inference service.&lt;/li&gt; 
 &lt;li&gt;Processes both &lt;code&gt;images&lt;/code&gt; and &lt;code&gt;videos&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Accelerates inference with &lt;code&gt;GPU&lt;/code&gt; support.&lt;/li&gt; 
 &lt;li&gt;Allows custom models and secondary development.&lt;/li&gt; 
 &lt;li&gt;Supports one-click inference for all images in the current task.&lt;/li&gt; 
 &lt;li&gt;Supports import/export for formats like COCO, VOC, YOLO, DOTA, MOT, MASK, PPOCR, MMGD, VLM-R1.&lt;/li&gt; 
 &lt;li&gt;Handles tasks like &lt;code&gt;classification&lt;/code&gt;, &lt;code&gt;detection&lt;/code&gt;, &lt;code&gt;segmentation&lt;/code&gt;, &lt;code&gt;caption&lt;/code&gt;, &lt;code&gt;rotation&lt;/code&gt;, &lt;code&gt;tracking&lt;/code&gt;, &lt;code&gt;estimation&lt;/code&gt;, &lt;code&gt;ocr&lt;/code&gt;, &lt;code&gt;vqa&lt;/code&gt;, &lt;code&gt;grounding&lt;/code&gt; and so on.&lt;/li&gt; 
 &lt;li&gt;Supports diverse annotation styles: &lt;code&gt;polygons&lt;/code&gt;, &lt;code&gt;rectangles&lt;/code&gt;, &lt;code&gt;rotated boxes&lt;/code&gt;, &lt;code&gt;circles&lt;/code&gt;, &lt;code&gt;lines&lt;/code&gt;, &lt;code&gt;points&lt;/code&gt;, and annotations for &lt;code&gt;text detection&lt;/code&gt;, &lt;code&gt;recognition&lt;/code&gt;, and &lt;code&gt;KIE&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Model library&lt;/h3&gt; 
&lt;img src="https://github.com/user-attachments/assets/7da2da2e-f182-4a1b-85f6-bfd0dfcc6a1b" width="100%" /&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;&lt;strong&gt;Task Category&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align="left"&gt;&lt;strong&gt;Supported Models&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;ğŸ–¼ï¸ Image Classification&lt;/td&gt; 
   &lt;td align="left"&gt;YOLOv5-Cls, YOLOv8-Cls, YOLO11-Cls, InternImage, PULC&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;ğŸ¯ Object Detection&lt;/td&gt; 
   &lt;td align="left"&gt;YOLOv5/6/7/8/9/10, YOLO11/12, YOLOX, YOLO-NAS, D-FINE, DAMO-YOLO, Gold_YOLO, RT-DETR, RF-DETR, DEIMv2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;ğŸ–Œï¸ Instance Segmentation&lt;/td&gt; 
   &lt;td align="left"&gt;YOLOv5-Seg, YOLOv8-Seg, YOLO11-Seg, Hyper-YOLO-Seg, RF-DETR-Seg&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;ğŸƒ Pose Estimation&lt;/td&gt; 
   &lt;td align="left"&gt;YOLOv8-Pose, YOLO11-Pose, DWPose, RTMO&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;ğŸ‘£ Tracking&lt;/td&gt; 
   &lt;td align="left"&gt;Bot-SORT, ByteTrack&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;ğŸ”„ Rotated Object Detection&lt;/td&gt; 
   &lt;td align="left"&gt;YOLOv5-Obb, YOLOv8-Obb, YOLO11-Obb&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;ğŸ“ Depth Estimation&lt;/td&gt; 
   &lt;td align="left"&gt;Depth Anything&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;ğŸ§© Segment Anything&lt;/td&gt; 
   &lt;td align="left"&gt;SAM 1/2/3, SAM-HQ, SAM-Med2D, EdgeSAM, EfficientViT-SAM, MobileSAM&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;âœ‚ï¸ Image Matting&lt;/td&gt; 
   &lt;td align="left"&gt;RMBG 1.4/2.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;ğŸ’¡ Proposal&lt;/td&gt; 
   &lt;td align="left"&gt;UPN&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;ğŸ·ï¸ Tagging&lt;/td&gt; 
   &lt;td align="left"&gt;RAM, RAM++&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;ğŸ“„ OCR&lt;/td&gt; 
   &lt;td align="left"&gt;PP-OCRv4, PP-OCRv5&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;ğŸ—£ï¸ Vision Foundation Models&lt;/td&gt; 
   &lt;td align="left"&gt;Florence2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;ğŸ‘ï¸ Vision Language Models&lt;/td&gt; 
   &lt;td align="left"&gt;Qwen3-VL, Gemini, ChatGPT&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;ğŸ›£ï¸ Land Detection&lt;/td&gt; 
   &lt;td align="left"&gt;CLRNet&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;ğŸ“ Grounding&lt;/td&gt; 
   &lt;td align="left"&gt;CountGD, GeCO, Grounding DINO, YOLO-World, YOLOE&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;ğŸ“š Other&lt;/td&gt; 
   &lt;td align="left"&gt;ğŸ‘‰ &lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/docs/en/model_zoo.md"&gt;model_zoo&lt;/a&gt; ğŸ‘ˆ&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Docs&lt;/h2&gt; 
&lt;ol start="0"&gt; 
 &lt;li&gt;&lt;a href="https://github.com/CVHub520/X-AnyLabeling-Server"&gt;Remote Inference Service&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/docs/en/get_started.md"&gt;Installation &amp;amp; Quickstart&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/docs/en/user_guide.md"&gt;Usage&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/docs/en/cli.md"&gt;Command Line Interface&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/docs/en/custom_model.md"&gt;Customize a model&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/docs/en/chatbot.md"&gt;Chatbot&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/docs/en/vqa.md"&gt;VQA&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/docs/en/image_classifier.md"&gt;Multi-class Image Classifier&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;img src="https://github.com/user-attachments/assets/0d67311c-f441-44b6-9ee0-932f25f51b1c" width="100%" /&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/classification/"&gt;Classification&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/classification/image-level/README.md"&gt;Image-Level&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/classification/shape-level/README.md"&gt;Shape-Level&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/detection/"&gt;Detection&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/detection/hbb/README.md"&gt;HBB Object Detection&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/detection/obb/README.md"&gt;OBB Object Detection&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/segmentation/README.md"&gt;Segmentation&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/segmentation/instance_segmentation/"&gt;Instance Segmentation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/segmentation/binary_semantic_segmentation/"&gt;Binary Semantic Segmentation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/segmentation/multiclass_semantic_segmentation/"&gt;Multiclass Semantic Segmentation&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/description/"&gt;Description&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/description/tagging/README.md"&gt;Tagging&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/description/captioning/README.md"&gt;Captioning&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/estimation/"&gt;Estimation&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/estimation/pose_estimation/README.md"&gt;Pose Estimation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/estimation/depth_estimation/README.md"&gt;Depth Estimation&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/optical_character_recognition/"&gt;OCR&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/optical_character_recognition/text_recognition/"&gt;Text Recognition&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/optical_character_recognition/key_information_extraction/README.md"&gt;Key Information Extraction&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/multiple_object_tracking/README.md"&gt;MOT&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/multiple_object_tracking/README.md"&gt;Tracking by HBB Object Detection&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/multiple_object_tracking/README.md"&gt;Tracking by OBB Object Detection&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/multiple_object_tracking/README.md"&gt;Tracking by Instance Segmentation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/multiple_object_tracking/README.md"&gt;Tracking by Pose Estimation&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/interactive_video_object_segmentation/README.md"&gt;iVOS&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/matting/"&gt;Matting&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/matting/image_matting/README.md"&gt;Image Matting&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/vision_language/"&gt;Vision-Language&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/vision_language/florence2/README.md"&gt;Florence 2&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/counting/"&gt;Counting&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/counting/geco/README.md"&gt;GeCo&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/grounding/"&gt;Grounding&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/grounding/yoloe/README.md"&gt;YOLOE&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/grounding/sam3/README.md"&gt;SAM 3&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/training/"&gt;Training&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/examples/training/ultralytics/README.md"&gt;Ultralytics&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contribute&lt;/h2&gt; 
&lt;p&gt;We believe in open collaboration! &lt;strong&gt;Xâ€‘AnyLabeling&lt;/strong&gt; continues to grow with the support of the community. Whether you're fixing bugs, improving documentation, or adding new features, your contributions make a real impact.&lt;/p&gt; 
&lt;p&gt;To get started, please read our &lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/CONTRIBUTING.md"&gt;Contributing Guide&lt;/a&gt; and make sure to agree to the &lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/CLA.md"&gt;Contributor License Agreement (CLA)&lt;/a&gt; before submitting a pull request.&lt;/p&gt; 
&lt;p&gt;If you find this project helpful, please consider giving it a â­ï¸ star! Have questions or suggestions? Open an &lt;a href="https://github.com/CVHub520/X-AnyLabeling/issues"&gt;issue&lt;/a&gt; or email us at &lt;a href="mailto:cv_hub@163.com"&gt;cv_hub@163.com&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;A huge thank you ğŸ™ to everyone helping to make Xâ€‘AnyLabeling better.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the &lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/LICENSE"&gt;GPL-3.0 license&lt;/a&gt; and is completely open source and free. The original intention is to enable more developers, researchers, and enterprises to conveniently use this AI application platform, promoting the development of the entire industry. We encourage everyone to use it freely (including commercial use), and you can also add features based on this project and commercialize it, but you must retain the brand identity and indicate the source project address.&lt;/p&gt; 
&lt;p&gt;Additionally, to understand the ecosystem and usage of X-AnyLabeling, if you use this project for academic, research, teaching, or enterprise purposes, please fill out the &lt;a href="https://forms.gle/MZCKhU7UJ4TRSWxR7"&gt;registration form&lt;/a&gt;. This registration is only for statistical purposes and will not incur any fees. We will strictly keep all information confidential.&lt;/p&gt; 
&lt;p&gt;X-AnyLabeling is independently developed and maintained by an individual. If this project has been helpful to you, we welcome your support through the donation links below to help sustain the project's continued development. Your support is the greatest encouragement! If you have any questions about the project or would like to collaborate, please feel free to contact via WeChat: ww10874 or email provided above.&lt;/p&gt; 
&lt;h2&gt;Sponsors&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://ko-fi.com/cvhub520"&gt;buy-me-a-coffee&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/CVHub520/X-AnyLabeling/raw/main/README_zh-CN.md#%E8%B5%9E%E5%8A%A9"&gt;Wechat/Alipay&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Acknowledgement&lt;/h2&gt; 
&lt;p&gt;I extend my heartfelt thanks to the developers and contributors of &lt;a href="https://github.com/vietanhdev/anylabeling"&gt;AnyLabeling&lt;/a&gt;, &lt;a href="https://github.com/wkentaro/labelme"&gt;LabelMe&lt;/a&gt;, &lt;a href="https://github.com/tzutalin/labelImg"&gt;LabelImg&lt;/a&gt;, &lt;a href="https://github.com/cgvict/roLabelImg"&gt;roLabelImg&lt;/a&gt;, &lt;a href="https://github.com/PFCCLab/PPOCRLabel"&gt;PPOCRLabel&lt;/a&gt; and &lt;a href="https://github.com/opencv/cvat"&gt;CVAT&lt;/a&gt;, whose work has been crucial to the success of this project.&lt;/p&gt; 
&lt;h2&gt;Citing&lt;/h2&gt; 
&lt;p&gt;If you use this software in your research, please cite it as below:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@misc{X-AnyLabeling,
  year = {2023},
  author = {Wei Wang},
  publisher = {Github},
  organization = {CVHub},
  journal = {Github repository},
  title = {Advanced Auto Labeling Solution with Added Features},
  howpublished = {\url{https://github.com/CVHub520/X-AnyLabeling}}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;img src="https://api.star-history.com/svg?repos=CVHub520/X-AnyLabeling&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/p&gt; 
&lt;div align="center"&gt;
 &lt;a href="https://raw.githubusercontent.com/CVHub520/X-AnyLabeling/main/#top"&gt;ğŸ” Back to Top&lt;/a&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>RosettaCommons/foundry</title>
      <link>https://github.com/RosettaCommons/foundry</link>
      <description>&lt;p&gt;Central repository for biomolecular foundation models with shared trainers and pipeline components&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Protein design with Foundry&lt;/h1&gt; 
&lt;p&gt;Foundry provides tooling and infrastructure for using and training all classes of models for protein design, including design (RFD3), inverse folding (ProteinMPNN) and protein folding (RF3).&lt;/p&gt; 
&lt;p&gt;All models within Foundry rely on &lt;a href="https://github.com/RosettaCommons/atomworks"&gt;AtomWorks&lt;/a&gt; - a unified framework for manipulating and processing biomolecular structures - for both training and inference.&lt;/p&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;h3&gt;Quickstart guide&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Installation&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install "rc-foundry[all]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Downloading weights&lt;/strong&gt; Models can be downloaded to a target folder with:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;foundry install base-models --checkpoint-dir &amp;lt;path/to/ckpt/dir&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;where &lt;code&gt;checkpoint-dir&lt;/code&gt; will be &lt;code&gt;~/.foundry/checkpoints&lt;/code&gt; by default. Foundry always searches &lt;code&gt;~/.foundry/checkpoints&lt;/code&gt; plus any colon-separated entries in &lt;code&gt;$FOUNDRY_CHECKPOINT_DIRS&lt;/code&gt; during inference or subsequent commands to find checkpoints. &lt;code&gt;base-models&lt;/code&gt; installs the latest RFD3, RF3 and MPNN variants - you can also download all of the models supported (including multiple checkpoints of RF3) with &lt;code&gt;all&lt;/code&gt;, or by listing the models sequentially (e.g. &lt;code&gt;foundry install rfd3 rf3 ...&lt;/code&gt;). To list the registry of available checkpoints:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;foundry list-available
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To check what you already have downloaded (searches &lt;code&gt;~/.foundry/checkpoints&lt;/code&gt; plus &lt;code&gt;$FOUNDRY_CHECKPOINT_DIRS&lt;/code&gt; if set):&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;foundry list-installed
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;em&gt;See &lt;code&gt;examples/all.ipynb&lt;/code&gt; for how to run each model and design proteins end-to-end in a notebook.&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Google Colab&lt;/h3&gt; 
&lt;p&gt;For an interactive Google Colab notebook walking through a basic design pipeline with RFD3, MPNN, and RF3, please see the &lt;a href="https://colab.research.google.com/drive/1ZwIMV3n9h0ZOnIXX0GyKUuoiahgifBxh?usp=sharing"&gt;IPD Design Pipeline Tutorial&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;RFdiffusion3 (RFD3)&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://www.biorxiv.org/content/10.1101/2025.09.18.676967v2"&gt;RFdiffusion3&lt;/a&gt; is an all-atom generative model capable of designing protein structures under complex constraints.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/RosettaCommons/foundry/production/docs/_static/cover.png" alt="RFdiffusion3 generation trajectory." width="700" /&gt; 
&lt;/div&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;em&gt;See &lt;a href="https://raw.githubusercontent.com/RosettaCommons/foundry/production/models/rfd3/README.md"&gt;models/rfd3/README.md&lt;/a&gt; for complete documentation.&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;RosettaFold3 (RF3)&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://doi.org/10.1101/2025.08.14.670328"&gt;RF3&lt;/a&gt; is a structure prediction neural network that narrows the gap between closed-source AF-3 and open-source alternatives.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/RosettaCommons/foundry/production/docs/_static/prot_dna.png" alt="Protein-DNA complex prediction" width="400" /&gt; 
&lt;/div&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;em&gt;See &lt;a href="https://raw.githubusercontent.com/RosettaCommons/foundry/production/models/rf3/README.md"&gt;models/rf3/README.md&lt;/a&gt; for complete documentation.&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;ProteinMPNN&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://www.science.org/doi/10.1126/science.add2187"&gt;ProteinMPNN&lt;/a&gt; and &lt;a href="https://www.nature.com/articles/s41592-025-02626-1"&gt;LigandMPNN&lt;/a&gt; are lightweight inverse-folding models which can be use to design diverse sequences for backbones under constrained conditions.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;em&gt;See &lt;a href="https://raw.githubusercontent.com/RosettaCommons/foundry/production/models/mpnn/README.md"&gt;models/mpnn/README.md&lt;/a&gt; for complete documentation.&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Development&lt;/h2&gt; 
&lt;h3&gt;Code Organization&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Strict dependency flow:&lt;/strong&gt; &lt;code&gt;foundry&lt;/code&gt; â†’ &lt;code&gt;atomworks&lt;/code&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;atomworks&lt;/strong&gt;: Structure I/O, preprocessing, featurization&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;foundry&lt;/strong&gt;: Model architectures, training, inference endpoints&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;models/&amp;lt;model&amp;gt;:&lt;/strong&gt; Released models.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;For Core Developers (Multiple Packages)&lt;/h4&gt; 
&lt;p&gt;Install both &lt;code&gt;foundry&lt;/code&gt; and models in editable mode for development:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv pip install -e '.[all,dev]'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This approach allows you to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Modify &lt;code&gt;foundry&lt;/code&gt; shared utilities and see changes immediately&lt;/li&gt; 
 &lt;li&gt;Work on specific models without installing all models&lt;/li&gt; 
 &lt;li&gt;Add new models as independent packages in &lt;code&gt;models/&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Running tests is not currently supported, test files may be missing.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Adding New Models&lt;/h3&gt; 
&lt;p&gt;To add a new model:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Create &lt;code&gt;models/&amp;lt;model_name&amp;gt;/&lt;/code&gt; directory with its own &lt;code&gt;pyproject.toml&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Add &lt;code&gt;foundry&lt;/code&gt; as a dependency&lt;/li&gt; 
 &lt;li&gt;Implement model-specific code in &lt;code&gt;models/&amp;lt;model_name&amp;gt;/src/&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Users can install with: &lt;code&gt;uv pip install -e ./models/&amp;lt;model_name&amp;gt;&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Pre-commit Formatting&lt;/h3&gt; 
&lt;p&gt;We ship a &lt;code&gt;.pre-commit-config.yaml&lt;/code&gt; that runs &lt;code&gt;make format&lt;/code&gt; (via &lt;code&gt;ruff format&lt;/code&gt;) before each commit. Enable it once per clone:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install pre-commit  # if not already installed
pre-commit install
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After installation the hook automatically formats the repo whenever you &lt;code&gt;git commit&lt;/code&gt;. Use &lt;code&gt;pre-commit run --all-files&lt;/code&gt; to apply it manually.&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you use this repository code or data in your work, please cite the relavant work as below:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@article{corley2025accelerating,
  title={Accelerating biomolecular modeling with atomworks and rf3},
  author={Corley, Nathaniel and Mathis, Simon and Krishna, Rohith and Bauer, Magnus S and Thompson, Tuscan R and Ahern, Woody and Kazman, Maxwell W and Brent, Rafael I and Didi, Kieran and Kubaney, Andrew and others},
  journal={bioRxiv},
  year={2025}
}

@article {butcher2025_rfdiffusion3,
    author = {Butcher, Jasper and Krishna, Rohith and Mitra, Raktim and Brent, Rafael Isaac and Li, Yanjing and Corley, Nathaniel and Kim, Paul T and Funk, Jonathan and Mathis, Simon Valentin and Salike, Saman and Muraishi, Aiko and Eisenach, Helen and Thompson, Tuscan Rock and Chen, Jie and Politanska, Yuliya and Sehgal, Enisha and Coventry, Brian and Zhang, Odin and Qiang, Bo and Didi, Kieran and Kazman, Maxwell and DiMaio, Frank and Baker, David},
    title = {De novo Design of All-atom Biomolecular Interactions with RFdiffusion3},
    elocation-id = {2025.09.18.676967},
    year = {2025},
    doi = {10.1101/2025.09.18.676967},
    publisher = {Cold Spring Harbor Laboratory},
    URL = {https://www.biorxiv.org/content/early/2025/11/19/2025.09.18.676967},
    eprint = {https://www.biorxiv.org/content/early/2025/11/19/2025.09.18.676967.full.pdf},
    journal = {bioRxiv}
}

@article{dauparas2022robust,
  title={Robust deep learning--based protein sequence design using ProteinMPNN},
  author={Dauparas, Justas and Anishchenko, Ivan and Bennett, Nathaniel and Bai, Hua and Ragotte, Robert J and Milles, Lukas F and Wicky, Basile IM and Courbet, Alexis and de Haas, Rob J and Bethel, Neville and others},
  journal={Science},
  volume={378},
  number={6615},
  pages={49--56},
  year={2022},
  publisher={American Association for the Advancement of Science}
}

@article{dauparas2025atomic,
  title={Atomic context-conditioned protein sequence design using LigandMPNN},
  author={Dauparas, Justas and Lee, Gyu Rie and Pecoraro, Robert and An, Linna and Anishchenko, Ivan and Glasscock, Cameron and Baker, David},
  journal={Nature Methods},
  pages={1--7},
  year={2025},
  publisher={Nature Publishing Group US New York}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Acknowledgments&lt;/h2&gt; 
&lt;p&gt;We thank Rachel Clune and Hope Woods from the RosettaCommons for their collaboration on the codebase, documentation, tutorials and examples.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>microsoft/VibeVoice</title>
      <link>https://github.com/microsoft/VibeVoice</link>
      <description>&lt;p&gt;Open-Source Frontier Voice AI&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h2&gt;ğŸ™ï¸ VibeVoice: Open-Source Frontier Voice AI&lt;/h2&gt; 
 &lt;p&gt;&lt;a href="https://microsoft.github.io/VibeVoice"&gt;&lt;img src="https://img.shields.io/badge/Project-Page-blue?logo=microsoft" alt="Project Page" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/collections/microsoft/vibevoice-68a2ef24a875c44be47b034f"&gt;&lt;img src="https://img.shields.io/badge/HuggingFace-Collection-orange?logo=huggingface" alt="Hugging Face" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/pdf/2508.19205"&gt;&lt;img src="https://img.shields.io/badge/Technical-Report-red?logo=adobeacrobatreader" alt="Technical Report" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="Figures/VibeVoice_logo_white.png" /&gt; 
  &lt;img src="https://raw.githubusercontent.com/microsoft/VibeVoice/main/Figures/VibeVoice_logo.png" alt="VibeVoice Logo" width="300" /&gt; 
 &lt;/picture&gt; 
&lt;/div&gt; 
&lt;div align="left"&gt; 
 &lt;h3&gt;ğŸ“° News&lt;/h3&gt; 
 &lt;img src="https://img.shields.io/badge/Status-New-brightgreen?style=flat" alt="New" /&gt; 
 &lt;img src="https://img.shields.io/badge/Feature-Realtime_TTS-blue?style=flat&amp;amp;logo=soundcharts" alt="Realtime TTS" /&gt; 
 &lt;p&gt;&lt;strong&gt;2025-12-03: ğŸ“£ We open-sourced &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-realtime-0.5b.md"&gt;&lt;strong&gt;VibeVoiceâ€‘Realtimeâ€‘0.5B&lt;/strong&gt;&lt;/a&gt;, a realâ€‘time textâ€‘toâ€‘speech model that supports streaming text input and robust long-form speech generation. Try it on &lt;a href="https://colab.research.google.com/github/microsoft/VibeVoice/blob/main/demo/vibevoice_realtime_colab.ipynb"&gt;Colab&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;2025-12-09: ğŸ“£ Weâ€™ve added experimental speakers in nine languages (DE, FR, IT, JP, KR, NL, PL, PT, ES) for explorationâ€”welcome to try them out and share your feedback.&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;To mitigate deepfake risks and ensure low latency for the first speech chunk, voice prompts are provided in an embedded format. For users requiring voice customization, please reach out to our team. We will also be expanding the range of available speakers. &lt;br /&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/0901d274-f6ae-46ef-a0fd-3c4fba4f76dc"&gt;https://github.com/user-attachments/assets/0901d274-f6ae-46ef-a0fd-3c4fba4f76dc&lt;/a&gt;&lt;/p&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;(Launch your own realtime demo via the websocket example in &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-realtime-0.5b.md#usage-1-launch-real-time-websocket-demo"&gt;Usage&lt;/a&gt;).&lt;/p&gt; 
 &lt;/blockquote&gt; 
&lt;/div&gt; 
&lt;p&gt;2025-09-05: VibeVoice is an open-source research framework intended to advance collaboration in the speech synthesis community. After release, we discovered instances where the tool was used in ways inconsistent with the stated intent. Since responsible use of AI is one of Microsoftâ€™s guiding principles, we have disabled this repo until we are confident that out-of-scope use is no longer possible.&lt;/p&gt; 
&lt;h3&gt;Overview&lt;/h3&gt; 
&lt;p&gt;VibeVoice is a novel framework designed for generating &lt;strong&gt;expressive&lt;/strong&gt;, &lt;strong&gt;long-form&lt;/strong&gt;, &lt;strong&gt;multi-speaker&lt;/strong&gt; conversational audio, such as podcasts, from text. It addresses significant challenges in traditional Text-to-Speech (TTS) systems, particularly in scalability, speaker consistency, and natural turn-taking.&lt;/p&gt; 
&lt;p&gt;VibeVoice currently includes two model variants:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Long-form multi-speaker model&lt;/strong&gt;: Synthesizes conversational/single-speaker speech up to &lt;strong&gt;90 minutes&lt;/strong&gt; with up to &lt;strong&gt;4 distinct speakers&lt;/strong&gt;, surpassing the typical 1â€“2 speaker limits of many prior models.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-realtime-0.5b.md"&gt;Realtime streaming TTS model&lt;/a&gt;&lt;/strong&gt;: Produces initial audible speech in ~&lt;strong&gt;300 ms&lt;/strong&gt; and supports &lt;strong&gt;streaming text input&lt;/strong&gt; for single-speaker &lt;strong&gt;real-time&lt;/strong&gt; speech generation; designed for low-latency generation.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;A core innovation of VibeVoice is its use of continuous speech tokenizers (Acoustic and Semantic) operating at an ultra-low frame rate of 7.5 Hz. These tokenizers efficiently preserve audio fidelity while significantly boosting computational efficiency for processing long sequences. VibeVoice employs a &lt;a href="https://arxiv.org/abs/2412.08635"&gt;next-token diffusion&lt;/a&gt; framework, leveraging a Large Language Model (LLM) to understand textual context and dialogue flow, and a diffusion head to generate high-fidelity acoustic details.&lt;/p&gt; 
&lt;p align="left"&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/VibeVoice/main/Figures/MOS-preference.png" alt="MOS Preference Results" height="260px" /&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/VibeVoice/main/Figures/VibeVoice.jpg" alt="VibeVoice Overview" height="250px" style="margin-right: 10px;" /&gt; &lt;/p&gt; 
&lt;h3&gt;ğŸµ Demo Examples&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Video Demo&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;We produced this video with &lt;a href="https://github.com/Wan-Video/Wan2.2"&gt;Wan2.2&lt;/a&gt;. We sincerely appreciate the Wan-Video team for their great work.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;English&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/0967027c-141e-4909-bec8-091558b1b784"&gt;https://github.com/user-attachments/assets/0967027c-141e-4909-bec8-091558b1b784&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Chinese&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/322280b7-3093-4c67-86e3-10be4746c88f"&gt;https://github.com/user-attachments/assets/322280b7-3093-4c67-86e3-10be4746c88f&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Cross-Lingual&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/838d8ad9-a201-4dde-bb45-8cd3f59ce722"&gt;https://github.com/user-attachments/assets/838d8ad9-a201-4dde-bb45-8cd3f59ce722&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Spontaneous Singing&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/6f27a8a5-0c60-4f57-87f3-7dea2e11c730"&gt;https://github.com/user-attachments/assets/6f27a8a5-0c60-4f57-87f3-7dea2e11c730&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Long Conversation with 4 people&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/a357c4b6-9768-495c-a576-1618f6275727"&gt;https://github.com/user-attachments/assets/a357c4b6-9768-495c-a576-1618f6275727&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;For more examples, see the &lt;a href="https://microsoft.github.io/VibeVoice"&gt;Project Page&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Risks and limitations&lt;/h2&gt; 
&lt;p&gt;While efforts have been made to optimize it through various techniques, it may still produce outputs that are unexpected, biased, or inaccurate. VibeVoice inherits any biases, errors, or omissions produced by its base model (specifically, Qwen2.5 1.5b in this release). Potential for Deepfakes and Disinformation: High-quality synthetic speech can be misused to create convincing fake audio content for impersonation, fraud, or spreading disinformation. Users must ensure transcripts are reliable, check content accuracy, and avoid using generated content in misleading ways. Users are expected to use the generated content and to deploy the models in a lawful manner, in full compliance with all applicable laws and regulations in the relevant jurisdictions. It is best practice to disclose the use of AI when sharing AI-generated content.&lt;/p&gt; 
&lt;p&gt;English and Chinese only: Transcripts in languages other than English or Chinese may result in unexpected audio outputs.&lt;/p&gt; 
&lt;p&gt;Non-Speech Audio: The model focuses solely on speech synthesis and does not handle background noise, music, or other sound effects.&lt;/p&gt; 
&lt;p&gt;Overlapping Speech: The current model does not explicitly model or generate overlapping speech segments in conversations.&lt;/p&gt; 
&lt;p&gt;We do not recommend using VibeVoice in commercial or real-world applications without further testing and development. This model is intended for research and development purposes only. Please use responsibly.&lt;/p&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://api.star-history.com/svg?repos=Microsoft/vibevoice&amp;amp;type=date&amp;amp;legend=top-left" alt="Star History Chart" /&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>trustedsec/social-engineer-toolkit</title>
      <link>https://github.com/trustedsec/social-engineer-toolkit</link>
      <description>&lt;p&gt;The Social-Engineer Toolkit (SET) repository from TrustedSec - All new versions of SET will be deployed here.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;The Social-Engineer Toolkit (SET)&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;Copyright &lt;span&gt;Â©&lt;/span&gt; 2020&lt;/li&gt; 
 &lt;li&gt;Written by: David Kennedy (ReL1K) @HackingDave&lt;/li&gt; 
 &lt;li&gt;Company: &lt;a href="https://www.trustedsec.com"&gt;TrustedSec&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;h2&gt;Description&lt;/h2&gt; 
&lt;p&gt;The Social-Engineer Toolkit is an open-source penetration testing framework designed for social engineering. SET has a number of custom attack vectors that allow you to make a believable attack quickly. SET is a product of TrustedSec, LLC â€“ an information security consulting firm located in Cleveland, Ohio.&lt;/p&gt; 
&lt;p&gt;DISCLAIMER: This is &lt;em&gt;only&lt;/em&gt; for testing purposes and can only be used where strict consent has been given. Do not use this for illegal purposes, period. Please read the LICENSE under readme/LICENSE for the licensing of SET.&lt;/p&gt; 
&lt;h4&gt;Supported platforms:&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Linux&lt;/li&gt; 
 &lt;li&gt;Mac OS X (experimental)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Installation&lt;/h1&gt; 
&lt;h2&gt;Install via requirements.txt&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip3 install -r requirements.txt
python3 setup.py 
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Install SET&lt;/h2&gt; 
&lt;p&gt;=======&lt;/p&gt; 
&lt;h4&gt;Mac OS X&lt;/h4&gt; 
&lt;p&gt;You will need to use a virtual environment for the Python install if you are using an M2 Macbook with the following instructions in your CLI within the social-engineer-toolkit directory.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;    # to install dependencies, run the following:
    python3 -m venv path/to/venv
    source path/to/venv/bin/activate
    python3 -m pip install -r requirements.txt

    # to install SET
    sudo python3 setup.py 
&lt;/code&gt;&lt;/pre&gt; 
&lt;br /&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h4&gt;Windows 10 WSL/WSL2 Kali Linux&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo apt install set -y
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Kali Linux on Windows 10 is a minimal installation so it doesn't have any tools installed. You can easily install Social Engineer Toolkit on WSL/WSL2 without needing pip using the above command.&lt;/p&gt; 
&lt;h4&gt;Linux&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/trustedsec/social-engineer-toolkit/ setoolkit/
cd setoolkit
pip3 install -r requirements.txt
python setup.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;br /&gt; 
&lt;h2&gt;SET Tutorial&lt;/h2&gt; 
&lt;p&gt;For a full document on how to use SET, &lt;a href="https://github.com/trustedsec/social-engineer-toolkit/raw/master/readme/User_Manual.pdf"&gt;visit the SET user manual&lt;/a&gt;.&lt;/p&gt; 
&lt;br /&gt; 
&lt;h2&gt;Bugs and enhancements&lt;/h2&gt; 
&lt;p&gt;For bug reports or enhancements, please open an &lt;a href="https://github.com/trustedsec/social-engineer-toolkit/issues"&gt;issue&lt;/a&gt; here. &lt;br /&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>google/adk-samples</title>
      <link>https://github.com/google/adk-samples</link>
      <description>&lt;p&gt;A collection of sample agents built with Agent Development Kit (ADK)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Agent Development Kit (ADK) Samples&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/google/adk-samples/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/License-Apache_2.0-blue.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;img src="https://github.com/google/adk-docs/raw/main/docs/assets/agent-development-kit.png" alt="Agent Development Kit Logo" width="150" /&gt; 
&lt;p&gt;Welcome to the ADK Sample Agents repository! This collection provides ready-to-use agents built on top of the &lt;a href="https://google.github.io/adk-docs/"&gt;Agent Development Kit&lt;/a&gt;, designed to accelerate your development process. These agents cover a range of common use cases and complexities, from simple conversational bots to complex multi-agent workflows.&lt;/p&gt; 
&lt;h2&gt;âœ¨ Getting Started&lt;/h2&gt; 
&lt;p&gt;This repo contains ADK sample agents for &lt;strong&gt;Python&lt;/strong&gt;, &lt;strong&gt;Go&lt;/strong&gt; and &lt;strong&gt;Java.&lt;/strong&gt; Navigate to the &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/google/adk-samples/main/python/"&gt;Python&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/google/adk-samples/main/go/"&gt;Go&lt;/a&gt;&lt;/strong&gt;, and &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/google/adk-samples/main/java/"&gt;Java&lt;/a&gt;&lt;/strong&gt; subfolders to see language-specific setup instructions, and learn more about the available sample agents.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] The agents in this repository are built using the &lt;strong&gt;Agent Development Kit (ADK)&lt;/strong&gt;. Before you can run any of the samples, you must have the ADK installed. For instructions, please refer to the &lt;a href="https://google.github.io/adk-docs/get-started"&gt;&lt;strong&gt;ADK Installation Guide&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;To learn more, check out the &lt;a href="https://google.github.io/adk-docs/"&gt;ADK Documentation&lt;/a&gt;, and the GitHub repositories for each language:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/google/adk-python"&gt;ADK Python&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/google/adk-go"&gt;ADK Go&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/google/adk-java"&gt;ADK Java&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸŒ³ Repository Structure&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;â”œâ”€â”€ go
â”‚&amp;nbsp;&amp;nbsp; â”œâ”€â”€ agents
â”‚&amp;nbsp;&amp;nbsp; â”‚&amp;nbsp;&amp;nbsp; â”œâ”€â”€ llm-auditor
â”‚&amp;nbsp;&amp;nbsp; â””â”€â”€ README.md
â”œâ”€â”€ java
â”‚&amp;nbsp;&amp;nbsp; â”œâ”€â”€ agents
â”‚&amp;nbsp;&amp;nbsp; â”‚&amp;nbsp;&amp;nbsp; â”œâ”€â”€ software-bug-assistant
â”‚&amp;nbsp;&amp;nbsp; â”‚&amp;nbsp;&amp;nbsp; â””â”€â”€ time-series-forecasting
â”‚&amp;nbsp;&amp;nbsp; â””â”€â”€ README.md
â”œâ”€â”€ python
â”‚&amp;nbsp;&amp;nbsp; â”œâ”€â”€ agents
â”‚&amp;nbsp;&amp;nbsp; â”‚&amp;nbsp;&amp;nbsp; â”œâ”€â”€ academic-research
â”‚&amp;nbsp;&amp;nbsp; â”‚&amp;nbsp;&amp;nbsp; â”œâ”€â”€ antom-payment
â”‚&amp;nbsp;&amp;nbsp; â”‚&amp;nbsp;&amp;nbsp; â”œâ”€â”€ blog-writer
â”‚&amp;nbsp;&amp;nbsp; â”‚&amp;nbsp;&amp;nbsp; â”œâ”€â”€ brand-search-optimization
â”‚&amp;nbsp;&amp;nbsp; â”‚&amp;nbsp;&amp;nbsp; â”œâ”€â”€ camel
â”‚&amp;nbsp;&amp;nbsp; â”‚&amp;nbsp;&amp;nbsp; â”œâ”€â”€ customer-service
â”‚&amp;nbsp;&amp;nbsp; â”‚&amp;nbsp;&amp;nbsp; â”œâ”€â”€ data-engineering
â”‚&amp;nbsp;&amp;nbsp; â”‚&amp;nbsp;&amp;nbsp; â”œâ”€â”€ data-science
â”‚&amp;nbsp;&amp;nbsp; â”‚&amp;nbsp;&amp;nbsp; â”œâ”€â”€ financial-advisor
â”‚&amp;nbsp;&amp;nbsp; â”‚&amp;nbsp;&amp;nbsp; â”œâ”€â”€ fomc-research
â”‚   â”‚   â”œâ”€â”€ gemini-fullstack
â”‚   â”‚   â”œâ”€â”€ deep-search
â”‚   â”‚   â”œâ”€â”€ google-trends-agent
â”‚&amp;nbsp;&amp;nbsp; â”‚&amp;nbsp;&amp;nbsp; â”œâ”€â”€ image-scoring
â”‚   â”‚   â”œâ”€â”€ llm-auditor
â”‚   â”‚   â”œâ”€â”€ machine-learning-engineering
â”‚   â”‚   â”œâ”€â”€ marketing-agency
â”‚   â”‚   â”œâ”€â”€ medical-pre-authorization
â”‚   â”‚   â”œâ”€â”€ personalized-shopping
â”‚&amp;nbsp;&amp;nbsp; â”‚&amp;nbsp;&amp;nbsp; â”œâ”€â”€ plumber-data-engineering-assistant
â”‚   â”‚   â”œâ”€â”€ RAG
â”‚   â”‚   â”œâ”€â”€ realtime-conversational-agent
â”‚   â”‚   â”œâ”€â”€ safety-plugins
â”‚   â”‚   â”œâ”€â”€ short-movie-agents
â”‚   â”‚   â”œâ”€â”€ software-bug-assistant
â”‚   â”‚   â”œâ”€â”€ travel-concierge
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â””â”€â”€ README.md
â””â”€â”€ README.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;â„¹ï¸ Getting help&lt;/h2&gt; 
&lt;p&gt;If you have any questions or if you found any problems with this repository, please report through &lt;a href="https://github.com/google/adk-samples/issues"&gt;GitHub issues&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;ğŸ¤ Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions from the community! Whether it's bug reports, feature requests, documentation improvements, or code contributions, please see our &lt;a href="https://github.com/google/adk-samples/raw/main/CONTRIBUTING.md"&gt;&lt;strong&gt;Contributing Guidelines&lt;/strong&gt;&lt;/a&gt; to get started.&lt;/p&gt; 
&lt;h2&gt;ğŸ“„ License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the Apache 2.0 License - see the &lt;a href="https://github.com/google/adk-samples/raw/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;h2&gt;Disclaimers&lt;/h2&gt; 
&lt;p&gt;This is not an officially supported Google product. This project is not eligible for the &lt;a href="https://bughunters.google.com/open-source-security"&gt;Google Open Source Software Vulnerability Rewards Program&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;This project is intended for demonstration purposes only. It is not intended for use in a production environment.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>