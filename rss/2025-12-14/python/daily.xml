<rss version="2.0">
  <channel>
    <title>GitHub Python Daily Trending</title>
    <description>Daily Trending of Python in GitHub</description>
    <pubDate>Sat, 13 Dec 2025 01:37:07 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>521xueweihan/HelloGitHub</title>
      <link>https://github.com/521xueweihan/HelloGitHub</link>
      <description>&lt;p&gt;åˆ†äº« GitHub ä¸Šæœ‰è¶£ã€å…¥é—¨çº§çš„å¼€æºé¡¹ç›®ã€‚Share interesting, entry-level open source projects on GitHub.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/readme.gif" /&gt; &lt;br /&gt;ä¸­æ–‡ | &lt;a href="https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/README_en.md"&gt;English&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/README_ja.md"&gt;æ—¥æœ¬èª&lt;/a&gt; &lt;br /&gt;åˆ†äº« GitHub ä¸Šæœ‰è¶£ã€å…¥é—¨çº§çš„å¼€æºé¡¹ç›®ã€‚ &lt;br /&gt;å…´è¶£æ˜¯æœ€å¥½çš„è€å¸ˆï¼ŒHelloGitHub å¸®ä½ æ‰¾åˆ°å¼€æºçš„ä¹è¶£ï¼ &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://hellogithub.com/repository/d4aae58ddbf34f0799bf3e8f965e0d70" target="_blank"&gt;&lt;img src="https://abroad.hellogithub.com/v1/widgets/recommend.svg?rid=d4aae58ddbf34f0799bf3e8f965e0d70&amp;amp;claim_uid=8MKvZoxaWt" alt="Featuredï½œHelloGitHub" style="width: 250px; height: 54px;" width="250" height="54" /&gt;&lt;/a&gt;&lt;br /&gt; &lt;a href="https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/weixin.png"&gt;&lt;img src="https://img.shields.io/badge/Talk-%E5%BE%AE%E4%BF%A1%E7%BE%A4-brightgreen.svg?style=popout-square" alt="WeiXin" /&gt;&lt;/a&gt; &lt;a href="https://github.com/521xueweihan/HelloGitHub/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/521xueweihan/HelloGitHub.svg?style=popout-square" alt="GitHub stars" /&gt;&lt;/a&gt; &lt;a href="https://github.com/521xueweihan/HelloGitHub/issues"&gt;&lt;img src="https://img.shields.io/github/issues/521xueweihan/HelloGitHub.svg?style=popout-square" alt="GitHub issues" /&gt;&lt;/a&gt; &lt;a href="https://weibo.com/hellogithub"&gt;&lt;img src="https://img.shields.io/badge/%E6%96%B0%E6%B5%AA-Weibo-red.svg?style=popout-square" alt="Sina Weibo" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;ç®€ä»‹&lt;/h2&gt; 
&lt;p&gt;HelloGitHub åˆ†äº« GitHub ä¸Šæœ‰è¶£ã€å…¥é—¨çº§çš„å¼€æºé¡¹ç›®ã€‚&lt;strong&gt;æ¯æœˆ 28 å·&lt;/strong&gt;ä»¥æœˆåˆŠçš„å½¢å¼&lt;a href="https://mp.weixin.qq.com/mp/appmsgalbum?__biz=MzA5MzYyNzQ0MQ==&amp;amp;action=getalbum&amp;amp;album_id=1331197538447310849#wechat_redirect"&gt;æ›´æ–°å‘å¸ƒ&lt;/a&gt;ï¼Œå†…å®¹åŒ…æ‹¬ï¼š&lt;strong&gt;æœ‰è¶£ã€å…¥é—¨çº§çš„å¼€æºé¡¹ç›®&lt;/strong&gt;ã€&lt;strong&gt;å¼€æºä¹¦ç±&lt;/strong&gt;ã€&lt;strong&gt;å®æˆ˜é¡¹ç›®&lt;/strong&gt;ã€&lt;strong&gt;ä¼ä¸šçº§é¡¹ç›®&lt;/strong&gt;ç­‰ï¼Œè®©ä½ ç”¨å¾ˆçŸ­æ—¶é—´æ„Ÿå—åˆ°å¼€æºçš„é­…åŠ›ï¼Œçˆ±ä¸Šå¼€æºï¼&lt;/p&gt; 
&lt;h2&gt;å†…å®¹&lt;/h2&gt; 
&lt;p&gt;è·å¾—æ›´å¥½çš„é˜…è¯»ä½“éªŒ &lt;a href="https://hellogithub.com/"&gt;å®˜ç½‘&lt;/a&gt; æˆ– &lt;a href="https://cdn.jsdelivr.net/gh/521xueweihan/img_logo@main/logo/weixin.png"&gt;HelloGitHub å…¬ä¼—å·&lt;/a&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;span&gt;ğŸ“‡&lt;/span&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;span&gt;ğŸƒ&lt;/span&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;span&gt;ğŸº&lt;/span&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;span&gt;ğŸ¥&lt;/span&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;img alt="octocat" src="https://github.githubassets.com/images/icons/emoji/octocat.png?v8" /&gt;)&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub116.md"&gt;ç¬¬ 116 æœŸ&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub115.md"&gt;ç¬¬ 115 æœŸ&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub114.md"&gt;ç¬¬ 114 æœŸ&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub113.md"&gt;ç¬¬ 113 æœŸ&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub112.md"&gt;ç¬¬ 112 æœŸ&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub111.md"&gt;ç¬¬ 111 æœŸ&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub110.md"&gt;ç¬¬ 110 æœŸ&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub109.md"&gt;ç¬¬ 109 æœŸ&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub108.md"&gt;ç¬¬ 108 æœŸ&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub107.md"&gt;ç¬¬ 107 æœŸ&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub106.md"&gt;ç¬¬ 106 æœŸ&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub105.md"&gt;ç¬¬ 105 æœŸ&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub104.md"&gt;ç¬¬ 104 æœŸ&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub103.md"&gt;ç¬¬ 103 æœŸ&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub102.md"&gt;ç¬¬ 102 æœŸ&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub101.md"&gt;ç¬¬ 101 æœŸ&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub100.md"&gt;ç¬¬ 100 æœŸ&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub99.md"&gt;ç¬¬ 99 æœŸ&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub98.md"&gt;ç¬¬ 98 æœŸ&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub97.md"&gt;ç¬¬ 97 æœŸ&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub96.md"&gt;ç¬¬ 96 æœŸ&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub95.md"&gt;ç¬¬ 95 æœŸ&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub94.md"&gt;ç¬¬ 94 æœŸ&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub93.md"&gt;ç¬¬ 93 æœŸ&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub92.md"&gt;ç¬¬ 92 æœŸ&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub91.md"&gt;ç¬¬ 91 æœŸ&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub90.md"&gt;ç¬¬ 90 æœŸ&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub89.md"&gt;ç¬¬ 89 æœŸ&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub88.md"&gt;ç¬¬ 88 æœŸ&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub87.md"&gt;ç¬¬ 87 æœŸ&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub86.md"&gt;ç¬¬ 86 æœŸ&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub85.md"&gt;ç¬¬ 85 æœŸ&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub84.md"&gt;ç¬¬ 84 æœŸ&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub83.md"&gt;ç¬¬ 83 æœŸ&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub82.md"&gt;ç¬¬ 82 æœŸ&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/521xueweihan/HelloGitHub/master/content/HelloGitHub81.md"&gt;ç¬¬ 81 æœŸ&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;æ¬¢è¿&lt;a href="https://hellogithub.com/periodical"&gt;æ¨èæˆ–è‡ªè&lt;/a&gt;é¡¹ç›®æˆä¸º &lt;strong&gt;HelloGitHub&lt;/strong&gt; çš„&lt;a href="https://github.com/521xueweihan/HelloGitHub/raw/master/content/contributors.md"&gt;è´¡çŒ®è€…&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;èµåŠ©&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center" style="width: 80px;"&gt; &lt;a href="https://www.compshare.cn/?utm_term=logo&amp;amp;utm_campaign=hellogithub&amp;amp;utm_source=otherdsp&amp;amp;utm_medium=display&amp;amp;ytag=logo_hellogithub_otherdsp_display"&gt; &lt;img src="https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/ucloud.png" width="60px" /&gt;&lt;br /&gt; &lt;sub&gt;UCloud&lt;/sub&gt;&lt;br /&gt; &lt;sub&gt;è¶…å€¼çš„GPUäº‘æœåŠ¡&lt;/sub&gt; &lt;/a&gt; &lt;/th&gt; 
   &lt;th align="center" style="width: 80px;"&gt; &lt;a href="https://www.upyun.com/?from=hellogithub"&gt; &lt;img src="https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/upyun.png" width="60px" /&gt;&lt;br /&gt; &lt;sub&gt;CDN&lt;/sub&gt;&lt;br /&gt; &lt;sub&gt;å¼€å¯å…¨ç½‘åŠ é€Ÿ&lt;/sub&gt; &lt;/a&gt; &lt;/th&gt; 
   &lt;th align="center" style="width: 80px;"&gt; &lt;a href="https://github.com/OpenIMSDK/Open-IM-Server"&gt; &lt;img src="https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/im.png" width="60px" /&gt;&lt;br /&gt; &lt;sub&gt;OpenIM&lt;/sub&gt;&lt;br /&gt; &lt;sub&gt;å¼€æºIMåŠ›äº‰No.1&lt;/sub&gt; &lt;/a&gt; &lt;/th&gt; 
   &lt;th align="center" style="width: 80px;"&gt; &lt;a href="https://www.qiniu.com/products/ai-token-api?utm_source=hello"&gt; &lt;img src="https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/qiniu.jpg" width="60px" /&gt;&lt;br /&gt; &lt;sub&gt;ä¸ƒç‰›äº‘&lt;/sub&gt;&lt;br /&gt; &lt;sub&gt;ç™¾ä¸‡ Token å…è´¹ä½“éªŒ&lt;/sub&gt; &lt;/a&gt; &lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
&lt;/table&gt; 
&lt;h2&gt;å£°æ˜&lt;/h2&gt; 
&lt;p&gt;&lt;a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/deed.zh"&gt;&lt;img alt="çŸ¥è¯†å…±äº«è®¸å¯åè®®" style="border-width: 0" src="https://licensebuttons.net/l/by-nc-nd/4.0/88x31.png" /&gt;&lt;/a&gt;&lt;br /&gt;æœ¬ä½œå“é‡‡ç”¨ &lt;a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/deed.zh"&gt;ç½²å-éå•†ä¸šæ€§ä½¿ç”¨-ç¦æ­¢æ¼”ç» 4.0 å›½é™…&lt;/a&gt; è¿›è¡Œè®¸å¯ã€‚&lt;a href="mailto:595666367@qq.com"&gt;è”ç³»æˆ‘&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>mindsdb/mindsdb</title>
      <link>https://github.com/mindsdb/mindsdb</link>
      <description>&lt;p&gt;Federated query engine for AI - The only MCP Server you'll ever need&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a name="readme-top"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://pypi.org/project/MindsDB/" target="_blank"&gt;&lt;img src="https://badge.fury.io/py/MindsDB.svg?sanitize=true" alt="MindsDB Release" /&gt;&lt;/a&gt; 
 &lt;a href="https://www.python.org/downloads/" target="_blank"&gt;&lt;img src="https://img.shields.io/badge/python-3.10.x%7C%203.11.x%7C%203.12.x%7C%203.13.x-brightgreen.svg?sanitize=true" alt="Python supported" /&gt;&lt;/a&gt; 
 &lt;a href="https://hub.docker.com/u/mindsdb" target="_blank"&gt;&lt;img src="https://img.shields.io/docker/pulls/mindsdb/mindsdb" alt="Docker pulls" /&gt;&lt;/a&gt; 
 &lt;br /&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/3068" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/3068" alt="mindsdb%2Fmindsdb | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;a href="https://github.com/mindsdb/mindsdb"&gt; &lt;img src="https://raw.githubusercontent.com/mindsdb/mindsdb/main/docs/assets/mindsdb_logo.png" alt="MindsDB" width="300" /&gt; &lt;/a&gt; 
 &lt;p align="center"&gt; &lt;br /&gt; &lt;a href="https://www.mindsdb.com?utm_medium=community&amp;amp;utm_source=github&amp;amp;utm_campaign=mindsdb%20repo"&gt;Website&lt;/a&gt; Â· &lt;a href="https://docs.mindsdb.com?utm_medium=community&amp;amp;utm_source=github&amp;amp;utm_campaign=mindsdb%20repo"&gt;Docs&lt;/a&gt; Â· &lt;a href="https://mindsdb.com/contact"&gt;Contact us for a Demo&lt;/a&gt; Â· &lt;a href="https://mindsdb.com/joincommunity?utm_medium=community&amp;amp;utm_source=github&amp;amp;utm_campaign=mindsdb%20repo"&gt;Community Slack&lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;p&gt;MindsDB enables humans, AI, agents, and applications to get highly accurate answers across large scale data sources.&lt;/p&gt; 
&lt;a href="https://www.youtube.com/watch?v=MX3OKpnsoLM" target="_blank"&gt; &lt;img src="https://github.com/user-attachments/assets/119e7b82-f901-4214-a26f-ff7c5ad86064" alt="MindsDB Demo" /&gt; &lt;/a&gt; 
&lt;h2&gt;Install MindsDB Server&lt;/h2&gt; 
&lt;p&gt;MindsDB is an open-source server that can be deployed anywhere - from your laptop to the cloud, and everywhere in between. And yes, you can customize it to your heart's content.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.mindsdb.com/setup/self-hosted/docker-desktop"&gt;Using Docker Desktop&lt;/a&gt;. This is the fastest and recommended way to get started and have it all running.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.mindsdb.com/setup/self-hosted/docker"&gt;Using Docker&lt;/a&gt;. This is also simple, but gives you more flexibility on how to further customize your server.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://docs.mindsdb.com/mcp/overview"&gt;MindsDB has an MCP server built in&lt;/a&gt; that enables your MCP applications to connect, unify and respond to questions over large-scale federated dataâ€”spanning databases, data warehouses, and SaaS applications.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h1&gt;Core Philosophy: Connect, Unify, Respond&lt;/h1&gt; 
&lt;p&gt;MindsDB's architecture is built around three fundamental capabilities:&lt;/p&gt; 
&lt;h2&gt;&lt;a href="https://docs.mindsdb.com/integrations/data-overview"&gt;Connect&lt;/a&gt; Your Data&lt;/h2&gt; 
&lt;p&gt;You can connect to hundreds of enterprise &lt;a href="https://docs.mindsdb.com/integrations/data-overview"&gt;data sources (learn more)&lt;/a&gt;. These integrations allow MindsDB to access data wherever it resides, forming the foundation for all other capabilities.&lt;/p&gt; 
&lt;h2&gt;&lt;a href="https://docs.mindsdb.com/mindsdb_sql/overview"&gt;Unify&lt;/a&gt; Your Data&lt;/h2&gt; 
&lt;p&gt;In many situations, itâ€™s important to be able to prepare and unify data before generating responses from it. MindsDB SQL offers knowledge bases and views that allow indexing and organizing structured and unstructured data as if it were unified in a single system.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.mindsdb.com/mindsdb_sql/knowledge-bases"&gt;&lt;strong&gt;KNOWLEDGE BASES&lt;/strong&gt;&lt;/a&gt; â€“ Index and organize unstructured data for efficient Q&amp;amp;A.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.mindsdb.com/mindsdb_sql/sql/create/view"&gt;&lt;strong&gt;VIEWS&lt;/strong&gt;&lt;/a&gt; â€“ Simplify data access by creating unified views across different sources (no-ETL).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Unification of data can be automated using JOBs&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.mindsdb.com/mindsdb_sql/sql/create/jobs"&gt;&lt;strong&gt;JOBS&lt;/strong&gt;&lt;/a&gt; â€“ Schedule synchronization and transformation tasks for real-time processing.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;a href="https://docs.mindsdb.com/mindsdb_sql/agents/agent"&gt;Respond&lt;/a&gt; From Your Data&lt;/h2&gt; 
&lt;p&gt;Chat with Your Data&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.mindsdb.com/mindsdb_sql/agents/agent"&gt;&lt;strong&gt;AGENTS&lt;/strong&gt;&lt;/a&gt; â€“ Configure built-in agents specialized in answering questions over your connected and unified data.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.mindsdb.com/mcp/overview"&gt;&lt;strong&gt;MCP&lt;/strong&gt;&lt;/a&gt; â€“ Connect to MindsDB through the MCP (Model Context Protocol) for seamless interaction.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ¤ Contribute&lt;/h2&gt; 
&lt;p&gt;Interested in contributing to MindsDB? Follow our &lt;a href="https://docs.mindsdb.com/contribute/install?utm_medium=community&amp;amp;utm_source=github&amp;amp;utm_campaign=mindsdb%20repo"&gt;installation guide for development&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You can find our &lt;a href="https://docs.mindsdb.com/contribute/contribute?utm_medium=community&amp;amp;utm_source=github&amp;amp;utm_campaign=mindsdb%20repo"&gt;contribution guide here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;We welcome suggestions! Feel free to open new issues with your ideas, and weâ€™ll guide you.&lt;/p&gt; 
&lt;p&gt;This project adheres to a &lt;a href="https://github.com/mindsdb/mindsdb/raw/main/CODE_OF_CONDUCT.md"&gt;Contributor Code of Conduct&lt;/a&gt;. By participating, you agree to follow its terms.&lt;/p&gt; 
&lt;p&gt;Also, check out our &lt;a href="https://mindsdb.com/community?utm_medium=community&amp;amp;utm_source=github&amp;amp;utm_campaign=mindsdb%20repo"&gt;community rewards and programs&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;ğŸ¤ Support&lt;/h2&gt; 
&lt;p&gt;If you find a bug, please submit an &lt;a href="https://github.com/mindsdb/mindsdb/issues/new/choose"&gt;issue on GitHub&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Hereâ€™s how you can get community support:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Ask a question in our &lt;a href="https://mindsdb.com/joincommunity?utm_medium=community&amp;amp;utm_source=github&amp;amp;utm_campaign=mindsdb%20repo"&gt;Slack Community&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Join our &lt;a href="https://github.com/mindsdb/mindsdb/discussions"&gt;GitHub Discussions&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Post on &lt;a href="https://stackoverflow.com/questions/tagged/mindsdb"&gt;Stack Overflow&lt;/a&gt; with the MindsDB tag.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For commercial support, please &lt;a href="https://mindsdb.com/contact?utm_medium=community&amp;amp;utm_source=github&amp;amp;utm_campaign=mindsdb%20repo"&gt;contact the MindsDB team&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;ğŸ’š Current Contributors&lt;/h2&gt; 
&lt;a href="https://github.com/mindsdb/mindsdb/graphs/contributors"&gt; &lt;img src="https://contributors-img.web.app/image?repo=mindsdb/mindsdb" /&gt; &lt;/a&gt; 
&lt;p&gt;Generated with &lt;a href="https://contributors-img.web.app"&gt;contributors-img&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;ğŸ”” Subscribe for Updates&lt;/h2&gt; 
&lt;p&gt;Join our &lt;a href="https://mindsdb.com/joincommunity"&gt;Slack community&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>infiniflow/ragflow</title>
      <link>https://github.com/infiniflow/ragflow</link>
      <description>&lt;p&gt;RAGFlow is a leading open-source Retrieval-Augmented Generation (RAG) engine that fuses cutting-edge RAG with Agent capabilities to create a superior context layer for LLMs&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;a href="https://demo.ragflow.io/"&gt; &lt;img src="https://raw.githubusercontent.com/infiniflow/ragflow/main/web/src/assets/logo-with-text.svg?sanitize=true" width="520" alt="ragflow logo" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;p align="center"&gt; &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/README.md"&gt;&lt;img alt="README in English" src="https://img.shields.io/badge/English-DBEDFA" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/README_zh.md"&gt;&lt;img alt="ç®€ä½“ä¸­æ–‡ç‰ˆè‡ªè¿°æ–‡ä»¶" src="https://img.shields.io/badge/ç®€ä½“ä¸­æ–‡-DFE0E5" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/README_tzh.md"&gt;&lt;img alt="ç¹é«”ç‰ˆä¸­æ–‡è‡ªè¿°æ–‡ä»¶" src="https://img.shields.io/badge/ç¹é«”ä¸­æ–‡-DFE0E5" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/README_ja.md"&gt;&lt;img alt="æ—¥æœ¬èªã®README" src="https://img.shields.io/badge/æ—¥æœ¬èª-DFE0E5" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/README_ko.md"&gt;&lt;img alt="í•œêµ­ì–´" src="https://img.shields.io/badge/í•œêµ­ì–´-DFE0E5" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/README_id.md"&gt;&lt;img alt="Bahasa Indonesia" src="https://img.shields.io/badge/Bahasa Indonesia-DFE0E5" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/README_pt_br.md"&gt;&lt;img alt="PortuguÃªs(Brasil)" src="https://img.shields.io/badge/PortuguÃªs(Brasil)-DFE0E5" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://x.com/intent/follow?screen_name=infiniflowai" target="_blank"&gt; &lt;img src="https://img.shields.io/twitter/follow/infiniflow?logo=X&amp;amp;color=%20%23f5f5f5" alt="follow on X(Twitter)" /&gt; &lt;/a&gt; &lt;a href="https://demo.ragflow.io" target="_blank"&gt; &lt;img alt="Static Badge" src="https://img.shields.io/badge/Online-Demo-4e6b99" /&gt; &lt;/a&gt; &lt;a href="https://hub.docker.com/r/infiniflow/ragflow" target="_blank"&gt; &lt;img src="https://img.shields.io/docker/pulls/infiniflow/ragflow?label=Docker%20Pulls&amp;amp;color=0db7ed&amp;amp;logo=docker&amp;amp;logoColor=white&amp;amp;style=flat-square" alt="docker pull infiniflow/ragflow:v0.22.1" /&gt; &lt;/a&gt; &lt;a href="https://github.com/infiniflow/ragflow/releases/latest"&gt; &lt;img src="https://img.shields.io/github/v/release/infiniflow/ragflow?color=blue&amp;amp;label=Latest%20Release" alt="Latest Release" /&gt; &lt;/a&gt; &lt;a href="https://github.com/infiniflow/ragflow/raw/main/LICENSE"&gt; &lt;img height="21" src="https://img.shields.io/badge/License-Apache--2.0-ffffff?labelColor=d4eaf7&amp;amp;color=2e6cc4" alt="license" /&gt; &lt;/a&gt; &lt;a href="https://deepwiki.com/infiniflow/ragflow"&gt; &lt;img alt="Ask DeepWiki" src="https://deepwiki.com/badge.svg?sanitize=true" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h4 align="center"&gt; &lt;a href="https://ragflow.io/docs/dev/"&gt;Document&lt;/a&gt; | &lt;a href="https://github.com/infiniflow/ragflow/issues/4214"&gt;Roadmap&lt;/a&gt; | &lt;a href="https://twitter.com/infiniflowai"&gt;Twitter&lt;/a&gt; | &lt;a href="https://discord.gg/NjYzJD3GM3"&gt;Discord&lt;/a&gt; | &lt;a href="https://demo.ragflow.io"&gt;Demo&lt;/a&gt; &lt;/h4&gt; 
&lt;div align="center" style="margin-top:20px;margin-bottom:20px;"&gt; 
 &lt;img src="https://raw.githubusercontent.com/infiniflow/ragflow-docs/refs/heads/image/image/ragflow-octoverse.png" width="1200" /&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://trendshift.io/repositories/9064" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/9064" alt="infiniflow%2Fragflow | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;details open&gt; 
 &lt;summary&gt;&lt;b&gt;ğŸ“• Table of Contents&lt;/b&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;ğŸ’¡ &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/#-what-is-ragflow"&gt;What is RAGFlow?&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;ğŸ® &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/#-demo"&gt;Demo&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;ğŸ“Œ &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/#-latest-updates"&gt;Latest Updates&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;ğŸŒŸ &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/#-key-features"&gt;Key Features&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;ğŸ” &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/#-system-architecture"&gt;System Architecture&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;ğŸ¬ &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/#-get-started"&gt;Get Started&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;ğŸ”§ &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/#-configurations"&gt;Configurations&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;ğŸ”§ &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/#-build-a-docker-image"&gt;Build a Docker image&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;ğŸ”¨ &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/#-launch-service-from-source-for-development"&gt;Launch service from source for development&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;ğŸ“š &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/#-documentation"&gt;Documentation&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;ğŸ“œ &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/#-roadmap"&gt;Roadmap&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;ğŸ„ &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/#-community"&gt;Community&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;ğŸ™Œ &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/#-contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;ğŸ’¡ What is RAGFlow?&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://ragflow.io/"&gt;RAGFlow&lt;/a&gt; is a leading open-source Retrieval-Augmented Generation (RAG) engine that fuses cutting-edge RAG with Agent capabilities to create a superior context layer for LLMs. It offers a streamlined RAG workflow adaptable to enterprises of any scale. Powered by a converged context engine and pre-built agent templates, RAGFlow enables developers to transform complex data into high-fidelity, production-ready AI systems with exceptional efficiency and precision.&lt;/p&gt; 
&lt;h2&gt;ğŸ® Demo&lt;/h2&gt; 
&lt;p&gt;Try our demo at &lt;a href="https://demo.ragflow.io"&gt;https://demo.ragflow.io&lt;/a&gt;.&lt;/p&gt; 
&lt;div align="center" style="margin-top:20px;margin-bottom:20px;"&gt; 
 &lt;img src="https://raw.githubusercontent.com/infiniflow/ragflow-docs/refs/heads/image/image/chunking.gif" width="1200" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/infiniflow/ragflow-docs/refs/heads/image/image/agentic-dark.gif" width="1200" /&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸ”¥ Latest Updates&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;2025-11-19 Supports Gemini 3 Pro.&lt;/li&gt; 
 &lt;li&gt;2025-11-12 Supports data synchronization from Confluence, S3, Notion, Discord, Google Drive.&lt;/li&gt; 
 &lt;li&gt;2025-10-23 Supports MinerU &amp;amp; Docling as document parsing methods.&lt;/li&gt; 
 &lt;li&gt;2025-10-15 Supports orchestrable ingestion pipeline.&lt;/li&gt; 
 &lt;li&gt;2025-08-08 Supports OpenAI's latest GPT-5 series models.&lt;/li&gt; 
 &lt;li&gt;2025-08-01 Supports agentic workflow and MCP.&lt;/li&gt; 
 &lt;li&gt;2025-05-23 Adds a Python/JavaScript code executor component to Agent.&lt;/li&gt; 
 &lt;li&gt;2025-05-05 Supports cross-language query.&lt;/li&gt; 
 &lt;li&gt;2025-03-19 Supports using a multi-modal model to make sense of images within PDF or DOCX files.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ‰ Stay Tuned&lt;/h2&gt; 
&lt;p&gt;â­ï¸ Star our repository to stay up-to-date with exciting new features and improvements! Get instant notifications for new releases! ğŸŒŸ&lt;/p&gt; 
&lt;div align="center" style="margin-top:20px;margin-bottom:20px;"&gt; 
 &lt;img src="https://github.com/user-attachments/assets/18c9707e-b8aa-4caf-a154-037089c105ba" width="1200" /&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸŒŸ Key Features&lt;/h2&gt; 
&lt;h3&gt;ğŸ­ &lt;strong&gt;"Quality in, quality out"&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/deepdoc/README.md"&gt;Deep document understanding&lt;/a&gt;-based knowledge extraction from unstructured data with complicated formats.&lt;/li&gt; 
 &lt;li&gt;Finds "needle in a data haystack" of literally unlimited tokens.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ± &lt;strong&gt;Template-based chunking&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Intelligent and explainable.&lt;/li&gt; 
 &lt;li&gt;Plenty of template options to choose from.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸŒ± &lt;strong&gt;Grounded citations with reduced hallucinations&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Visualization of text chunking to allow human intervention.&lt;/li&gt; 
 &lt;li&gt;Quick view of the key references and traceable citations to support grounded answers.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ” &lt;strong&gt;Compatibility with heterogeneous data sources&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Supports Word, slides, excel, txt, images, scanned copies, structured data, web pages, and more.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ›€ &lt;strong&gt;Automated and effortless RAG workflow&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Streamlined RAG orchestration catered to both personal and large businesses.&lt;/li&gt; 
 &lt;li&gt;Configurable LLMs as well as embedding models.&lt;/li&gt; 
 &lt;li&gt;Multiple recall paired with fused re-ranking.&lt;/li&gt; 
 &lt;li&gt;Intuitive APIs for seamless integration with business.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ” System Architecture&lt;/h2&gt; 
&lt;div align="center" style="margin-top:20px;margin-bottom:20px;"&gt; 
 &lt;img src="https://github.com/user-attachments/assets/31b0dd6f-ca4f-445a-9457-70cb44a381b2" width="1000" /&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸ¬ Get Started&lt;/h2&gt; 
&lt;h3&gt;ğŸ“ Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;CPU &amp;gt;= 4 cores&lt;/li&gt; 
 &lt;li&gt;RAM &amp;gt;= 16 GB&lt;/li&gt; 
 &lt;li&gt;Disk &amp;gt;= 50 GB&lt;/li&gt; 
 &lt;li&gt;Docker &amp;gt;= 24.0.0 &amp;amp; Docker Compose &amp;gt;= v2.26.1&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://gvisor.dev/docs/user_guide/install/"&gt;gVisor&lt;/a&gt;: Required only if you intend to use the code executor (sandbox) feature of RAGFlow.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] If you have not installed Docker on your local machine (Windows, Mac, or Linux), see &lt;a href="https://docs.docker.com/engine/install/"&gt;Install Docker Engine&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;ğŸš€ Start up the server&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Ensure &lt;code&gt;vm.max_map_count&lt;/code&gt; &amp;gt;= 262144:&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;To check the value of &lt;code&gt;vm.max_map_count&lt;/code&gt;:&lt;/p&gt; 
   &lt;pre&gt;&lt;code class="language-bash"&gt;$ sysctl vm.max_map_count
&lt;/code&gt;&lt;/pre&gt; 
   &lt;p&gt;Reset &lt;code&gt;vm.max_map_count&lt;/code&gt; to a value at least 262144 if it is not.&lt;/p&gt; 
   &lt;pre&gt;&lt;code class="language-bash"&gt;# In this case, we set it to 262144:
$ sudo sysctl -w vm.max_map_count=262144
&lt;/code&gt;&lt;/pre&gt; 
   &lt;p&gt;This change will be reset after a system reboot. To ensure your change remains permanent, add or update the &lt;code&gt;vm.max_map_count&lt;/code&gt; value in &lt;strong&gt;/etc/sysctl.conf&lt;/strong&gt; accordingly:&lt;/p&gt; 
   &lt;pre&gt;&lt;code class="language-bash"&gt;vm.max_map_count=262144
&lt;/code&gt;&lt;/pre&gt; 
  &lt;/blockquote&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Clone the repo:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;$ git clone https://github.com/infiniflow/ragflow.git
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Start up the server using the pre-built Docker images:&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!CAUTION] All Docker images are built for x86 platforms. We don't currently offer Docker images for ARM64. If you are on an ARM64 platform, follow &lt;a href="https://ragflow.io/docs/dev/build_docker_image"&gt;this guide&lt;/a&gt; to build a Docker image compatible with your system.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;The command below downloads the &lt;code&gt;v0.22.1&lt;/code&gt; edition of the RAGFlow Docker image. See the following table for descriptions of different RAGFlow editions. To download a RAGFlow edition different from &lt;code&gt;v0.22.1&lt;/code&gt;, update the &lt;code&gt;RAGFLOW_IMAGE&lt;/code&gt; variable accordingly in &lt;strong&gt;docker/.env&lt;/strong&gt; before using &lt;code&gt;docker compose&lt;/code&gt; to start the server.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;   $ cd ragflow/docker
  
   # git checkout v0.22.1
   # Optional: use a stable tag (see releases: https://github.com/infiniflow/ragflow/releases)
   # This step ensures the **entrypoint.sh** file in the code matches the Docker image version.
   
   # Use CPU for DeepDoc tasks:
   $ docker compose -f docker-compose.yml up -d

   # To use GPU to accelerate DeepDoc tasks:
   # sed -i '1i DEVICE=gpu' .env
   # docker compose -f docker-compose.yml up -d
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Note: Prior to &lt;code&gt;v0.22.0&lt;/code&gt;, we provided both images with embedding models and slim images without embedding models. Details as follows:&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;RAGFlow image tag&lt;/th&gt; 
   &lt;th&gt;Image size (GB)&lt;/th&gt; 
   &lt;th&gt;Has embedding models?&lt;/th&gt; 
   &lt;th&gt;Stable?&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;v0.21.1&lt;/td&gt; 
   &lt;td&gt;â‰ˆ9&lt;/td&gt; 
   &lt;td&gt;âœ”ï¸&lt;/td&gt; 
   &lt;td&gt;Stable release&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;v0.21.1-slim&lt;/td&gt; 
   &lt;td&gt;â‰ˆ2&lt;/td&gt; 
   &lt;td&gt;âŒ&lt;/td&gt; 
   &lt;td&gt;Stable release&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Starting with &lt;code&gt;v0.22.0&lt;/code&gt;, we ship only the slim edition and no longer append the &lt;strong&gt;-slim&lt;/strong&gt; suffix to the image tag.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ol start="4"&gt; 
 &lt;li&gt; &lt;p&gt;Check the server status after having the server up and running:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;$ docker logs -f docker-ragflow-cpu-1
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;The following output confirms a successful launch of the system:&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;
      ____   ___    ______ ______ __
     / __ \ /   |  / ____// ____// /____  _      __
    / /_/ // /| | / / __ / /_   / // __ \| | /| / /
   / _, _// ___ |/ /_/ // __/  / // /_/ /| |/ |/ /
  /_/ |_|/_/  |_|\____//_/    /_/ \____/ |__/|__/

 * Running on all addresses (0.0.0.0)
&lt;/code&gt;&lt;/pre&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;If you skip this confirmation step and directly log in to RAGFlow, your browser may prompt a &lt;code&gt;network anormal&lt;/code&gt; error because, at that moment, your RAGFlow may not be fully initialized.&lt;/p&gt; 
  &lt;/blockquote&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;In your web browser, enter the IP address of your server and log in to RAGFlow.&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;With the default settings, you only need to enter &lt;code&gt;http://IP_OF_YOUR_MACHINE&lt;/code&gt; (&lt;strong&gt;sans&lt;/strong&gt; port number) as the default HTTP serving port &lt;code&gt;80&lt;/code&gt; can be omitted when using the default configurations.&lt;/p&gt; 
  &lt;/blockquote&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;In &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/service_conf.yaml.template"&gt;service_conf.yaml.template&lt;/a&gt;, select the desired LLM factory in &lt;code&gt;user_default_llm&lt;/code&gt; and update the &lt;code&gt;API_KEY&lt;/code&gt; field with the corresponding API key.&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;See &lt;a href="https://ragflow.io/docs/dev/llm_api_key_setup"&gt;llm_api_key_setup&lt;/a&gt; for more information.&lt;/p&gt; 
  &lt;/blockquote&gt; &lt;p&gt;&lt;em&gt;The show is on!&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;ğŸ”§ Configurations&lt;/h2&gt; 
&lt;p&gt;When it comes to system configurations, you will need to manage the following files:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/.env"&gt;.env&lt;/a&gt;: Keeps the fundamental setups for the system, such as &lt;code&gt;SVR_HTTP_PORT&lt;/code&gt;, &lt;code&gt;MYSQL_PASSWORD&lt;/code&gt;, and &lt;code&gt;MINIO_PASSWORD&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/service_conf.yaml.template"&gt;service_conf.yaml.template&lt;/a&gt;: Configures the back-end services. The environment variables in this file will be automatically populated when the Docker container starts. Any environment variables set within the Docker container will be available for use, allowing you to customize service behavior based on the deployment environment.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/docker-compose.yml"&gt;docker-compose.yml&lt;/a&gt;: The system relies on &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/docker-compose.yml"&gt;docker-compose.yml&lt;/a&gt; to start up.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;The &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/README.md"&gt;./docker/README&lt;/a&gt; file provides a detailed description of the environment settings and service configurations which can be used as &lt;code&gt;${ENV_VARS}&lt;/code&gt; in the &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/service_conf.yaml.template"&gt;service_conf.yaml.template&lt;/a&gt; file.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;To update the default HTTP serving port (80), go to &lt;a href="https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/docker-compose.yml"&gt;docker-compose.yml&lt;/a&gt; and change &lt;code&gt;80:80&lt;/code&gt; to &lt;code&gt;&amp;lt;YOUR_SERVING_PORT&amp;gt;:80&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Updates to the above configurations require a reboot of all containers to take effect:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;$ docker compose -f docker-compose.yml up -d
&lt;/code&gt;&lt;/pre&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Switch doc engine from Elasticsearch to Infinity&lt;/h3&gt; 
&lt;p&gt;RAGFlow uses Elasticsearch by default for storing full text and vectors. To switch to &lt;a href="https://github.com/infiniflow/infinity/"&gt;Infinity&lt;/a&gt;, follow these steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Stop all running containers:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;$ docker compose -f docker/docker-compose.yml down -v
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING] &lt;code&gt;-v&lt;/code&gt; will delete the docker container volumes, and the existing data will be cleared.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt; &lt;p&gt;Set &lt;code&gt;DOC_ENGINE&lt;/code&gt; in &lt;strong&gt;docker/.env&lt;/strong&gt; to &lt;code&gt;infinity&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Start the containers:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;$ docker compose -f docker-compose.yml up -d
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING] Switching to Infinity on a Linux/arm64 machine is not yet officially supported.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;ğŸ”§ Build a Docker image&lt;/h2&gt; 
&lt;p&gt;This image is approximately 2 GB in size and relies on external LLM and embedding services.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/infiniflow/ragflow.git
cd ragflow/
docker build --platform linux/amd64 -f Dockerfile -t infiniflow/ragflow:nightly .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ğŸ”¨ Launch service from source for development&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Install &lt;code&gt;uv&lt;/code&gt; and &lt;code&gt;pre-commit&lt;/code&gt;, or skip this step if they are already installed:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pipx install uv pre-commit
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Clone the source code and install Python dependencies:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/infiniflow/ragflow.git
cd ragflow/
uv sync --python 3.12 # install RAGFlow dependent python modules
uv run download_deps.py
pre-commit install
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Launch the dependent services (MinIO, Elasticsearch, Redis, and MySQL) using Docker Compose:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;docker compose -f docker/docker-compose-base.yml up -d
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Add the following line to &lt;code&gt;/etc/hosts&lt;/code&gt; to resolve all hosts specified in &lt;strong&gt;docker/.env&lt;/strong&gt; to &lt;code&gt;127.0.0.1&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;127.0.0.1       es01 infinity mysql minio redis sandbox-executor-manager
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;If you cannot access HuggingFace, set the &lt;code&gt;HF_ENDPOINT&lt;/code&gt; environment variable to use a mirror site:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;export HF_ENDPOINT=https://hf-mirror.com
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;If your operating system does not have jemalloc, please install it as follows:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# Ubuntu
sudo apt-get install libjemalloc-dev
# CentOS
sudo yum install jemalloc
# OpenSUSE
sudo zypper install jemalloc
# macOS
sudo brew install jemalloc
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Launch backend service:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;source .venv/bin/activate
export PYTHONPATH=$(pwd)
bash docker/launch_backend_service.sh
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install frontend dependencies:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cd web
npm install
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Launch frontend service:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;npm run dev
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;The following output confirms a successful launch of the system:&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/0daf462c-a24d-4496-a66f-92533534e187" alt="" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Stop RAGFlow front-end and back-end service after development is complete:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pkill -f "ragflow_server.py|task_executor.py"
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;ğŸ“š Documentation&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://ragflow.io/docs/dev/"&gt;Quickstart&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ragflow.io/docs/dev/configurations"&gt;Configuration&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ragflow.io/docs/dev/release_notes"&gt;Release notes&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ragflow.io/docs/dev/category/guides"&gt;User guides&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ragflow.io/docs/dev/category/developers"&gt;Developer guides&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ragflow.io/docs/dev/category/references"&gt;References&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ragflow.io/docs/dev/faq"&gt;FAQs&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ“œ Roadmap&lt;/h2&gt; 
&lt;p&gt;See the &lt;a href="https://github.com/infiniflow/ragflow/issues/4214"&gt;RAGFlow Roadmap 2025&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;ğŸ„ Community&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://discord.gg/NjYzJD3GM3"&gt;Discord&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://twitter.com/infiniflowai"&gt;Twitter&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/orgs/infiniflow/discussions"&gt;GitHub Discussions&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ™Œ Contributing&lt;/h2&gt; 
&lt;p&gt;RAGFlow flourishes via open-source collaboration. In this spirit, we embrace diverse contributions from the community. If you would like to be a part, review our &lt;a href="https://ragflow.io/docs/dev/contributing"&gt;Contribution Guidelines&lt;/a&gt; first.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>aurelio-labs/semantic-router</title>
      <link>https://github.com/aurelio-labs/semantic-router</link>
      <description>&lt;p&gt;Superfast AI decision making and intelligent processing of multi-modal data.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href="https://aurelio.ai"&gt;&lt;img src="https://i.ibb.co.com/g423grt/semantic-router-banner.png" alt="Semantic Router" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt; &lt;img alt="PyPI - Python Version" src="https://img.shields.io/pypi/pyversions/semantic-router?logo=python&amp;amp;logoColor=gold" /&gt; &lt;a href="https://github.com/aurelio-labs/semantic-router/graphs/contributors"&gt;&lt;img alt="GitHub Contributors" src="https://img.shields.io/github/contributors/aurelio-labs/semantic-router" /&gt; &lt;/a&gt;&lt;a href="https://github.com/aurelio-labs/semantic-router/commits/main"&gt;&lt;img alt="GitHub Last Commit" src="https://img.shields.io/github/last-commit/aurelio-labs/semantic-router" /&gt; &lt;img alt="" src="https://img.shields.io/github/repo-size/aurelio-labs/semantic-router" /&gt; &lt;/a&gt;&lt;a href="https://github.com/aurelio-labs/semantic-router/issues"&gt;&lt;img alt="GitHub Issues" src="https://img.shields.io/github/issues/aurelio-labs/semantic-router" /&gt; &lt;/a&gt;&lt;a href="https://github.com/aurelio-labs/semantic-router/pulls"&gt;&lt;img alt="GitHub Pull Requests" src="https://img.shields.io/github/issues-pr/aurelio-labs/semantic-router" /&gt; &lt;img src="https://codecov.io/gh/aurelio-labs/semantic-router/graph/badge.svg?token=H8OOMV2TUF" /&gt; &lt;/a&gt;&lt;a href="https://github.com/aurelio-labs/semantic-router/raw/main/LICENSE"&gt;&lt;img alt="Github License" src="https://img.shields.io/badge/License-MIT-yellow.svg?sanitize=true" /&gt; &lt;/a&gt;&lt;/p&gt;
&lt;a href="https://github.com/aurelio-labs/semantic-router/raw/main/LICENSE"&gt; &lt;p&gt;Semantic Router is a superfast decision-making layer for your LLMs and agents. Rather than waiting for slow LLM generations to make tool-use decisions, we use the magic of semantic vector space to make those decisions â€” &lt;em&gt;routing&lt;/em&gt; our requests using &lt;em&gt;semantic&lt;/em&gt; meaning.&lt;/p&gt; &lt;/a&gt;
&lt;h4&gt;&lt;a href="https://github.com/aurelio-labs/semantic-router/raw/main/LICENSE"&gt;&lt;/a&gt;&lt;a href="https://docs.aurelio.ai/semantic-router/get-started/introduction"&gt;Read the Docs&lt;/a&gt;&lt;/h4&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Quickstart&lt;/h2&gt; 
&lt;p&gt;To get started with &lt;em&gt;semantic-router&lt;/em&gt; we install it like so:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install -qU semantic-router
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;â—ï¸ &lt;em&gt;If wanting to use a fully local version of semantic router you can use &lt;code&gt;HuggingFaceEncoder&lt;/code&gt; and &lt;code&gt;LlamaCppLLM&lt;/code&gt; (&lt;code&gt;pip install -qU "semantic-router[local]"&lt;/code&gt;, see &lt;a href="https://github.com/aurelio-labs/semantic-router/raw/main/docs/05-local-execution.ipynb"&gt;here&lt;/a&gt;). To use the &lt;code&gt;HybridRouteLayer&lt;/code&gt; you must &lt;code&gt;pip install -qU "semantic-router[hybrid]"&lt;/code&gt;.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;We begin by defining a set of &lt;code&gt;Route&lt;/code&gt; objects. These are the decision paths that the semantic router can decide to use, let's try two simple routes for now â€” one for talk on &lt;em&gt;politics&lt;/em&gt; and another for &lt;em&gt;chitchat&lt;/em&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from semantic_router import Route

# we could use this as a guide for our chatbot to avoid political conversations
politics = Route(
    name="politics",
    utterances=[
        "isn't politics the best thing ever",
        "why don't you tell me about your political opinions",
        "don't you just love the president",
        "they're going to destroy this country!",
        "they will save the country!",
    ],
)

# this could be used as an indicator to our chatbot to switch to a more
# conversational prompt
chitchat = Route(
    name="chitchat",
    utterances=[
        "how's the weather today?",
        "how are things going?",
        "lovely weather today",
        "the weather is horrendous",
        "let's go to the chippy",
    ],
)

# we place both of our decisions together into single list
routes = [politics, chitchat]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;We have our routes ready, now we initialize an embedding / encoder model. We currently support a &lt;code&gt;CohereEncoder&lt;/code&gt; and &lt;code&gt;OpenAIEncoder&lt;/code&gt; â€” more encoders will be added soon. To initialize them we do:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import os
from semantic_router.encoders import CohereEncoder, OpenAIEncoder

# for Cohere
os.environ["COHERE_API_KEY"] = "&amp;lt;YOUR_API_KEY&amp;gt;"
encoder = CohereEncoder()

# or for OpenAI
os.environ["OPENAI_API_KEY"] = "&amp;lt;YOUR_API_KEY&amp;gt;"
encoder = OpenAIEncoder()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;With our &lt;code&gt;routes&lt;/code&gt; and &lt;code&gt;encoder&lt;/code&gt; defined we now create a &lt;code&gt;RouteLayer&lt;/code&gt;. The route layer handles our semantic decision making.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from semantic_router.routers import SemanticRouter

rl = SemanticRouter(encoder=encoder, routes=routes, auto_sync="local")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;We can now use our route layer to make super fast decisions based on user queries. Let's try with two queries that should trigger our route decisions:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;rl("don't you love politics?").name
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code&gt;[Out]: 'politics'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Correct decision, let's try another:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;rl("how's the weather today?").name
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code&gt;[Out]: 'chitchat'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;We get both decisions correct! Now lets try sending an unrelated query:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;rl("I'm interested in learning about llama 2").name
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code&gt;[Out]:
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;In this case, no decision could be made as we had no matches â€” so our route layer returned &lt;code&gt;None&lt;/code&gt;!&lt;/p&gt; 
&lt;h2&gt;Integrations&lt;/h2&gt; 
&lt;p&gt;The &lt;em&gt;encoders&lt;/em&gt; of semantic router include easy-to-use integrations with &lt;a href="https://github.com/aurelio-labs/semantic-router/raw/main/semantic_router/encoders/cohere.py"&gt;Cohere&lt;/a&gt;, &lt;a href="https://github.com/aurelio-labs/semantic-router/raw/main/docs/encoders/openai-embed-3.ipynb"&gt;OpenAI&lt;/a&gt;, &lt;a href="https://github.com/aurelio-labs/semantic-router/raw/main/docs/encoders/huggingface.ipynb"&gt;Hugging Face&lt;/a&gt;, &lt;a href="https://github.com/aurelio-labs/semantic-router/raw/main/docs/encoders/fastembed.ipynb"&gt;FastEmbed&lt;/a&gt;, and &lt;a href="https://github.com/aurelio-labs/semantic-router/tree/main/semantic_router/encoders"&gt;more&lt;/a&gt; â€” we even support &lt;a href="https://github.com/aurelio-labs/semantic-router/raw/main/docs/07-multi-modal.ipynb"&gt;multi-modality&lt;/a&gt;!.&lt;/p&gt; 
&lt;p&gt;Our utterance vector space also integrates with &lt;a href="https://github.com/aurelio-labs/semantic-router/raw/main/docs/indexes/pinecone.ipynb"&gt;Pinecone&lt;/a&gt; and &lt;a href="https://github.com/aurelio-labs/semantic-router/raw/main/docs/indexes/qdrant.ipynb"&gt;Qdrant&lt;/a&gt;!&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ“š Resources&lt;/h2&gt; 
&lt;h3&gt;Docs&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Notebook&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/aurelio-labs/semantic-router/raw/main/docs/00-introduction.ipynb"&gt;Introduction&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Introduction to Semantic Router and static routes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/aurelio-labs/semantic-router/raw/main/docs/02-dynamic-routes.ipynb"&gt;Dynamic Routes&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Dynamic routes for parameter generation and functionc calls&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/aurelio-labs/semantic-router/raw/main/docs/01-save-load-from-file.ipynb"&gt;Save/Load Layers&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;How to save and load &lt;code&gt;RouteLayer&lt;/code&gt; from file&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/aurelio-labs/semantic-router/raw/main/docs/03-basic-langchain-agent.ipynb"&gt;LangChain Integration&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;How to integrate Semantic Router with LangChain Agents&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/aurelio-labs/semantic-router/raw/main/docs/05-local-execution.ipynb"&gt;Local Execution&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Fully local Semantic Router with dynamic routes â€” &lt;em&gt;local models such as Mistral 7B outperform GPT-3.5 in most tests&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/aurelio-labs/semantic-router/raw/main/docs/06-threshold-optimization.ipynb"&gt;Route Optimization&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;How to train route layer thresholds to optimize performance&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/aurelio-labs/semantic-router/raw/main/docs/07-multi-modal.ipynb"&gt;Multi-Modal Routes&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Using multi-modal routes to identify Shrek vs. not-Shrek pictures&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Online Course&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://www.aurelio.ai/course/semantic-router"&gt;&lt;img src="https://github.com/aurelio-labs/assets/raw/main/images/aurelio-1080p-header-dark-semantic-router.jpg" alt="Semantic Router Course" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Community&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Dimitrios Manias, Ali Chouman, Abdallah Shami, &lt;a href="https://arxiv.org/abs/2404.15869"&gt;Semantic Routing for Enhanced Performance of LLM-Assisted Intent-Based 5G Core Network Management and Orchestration&lt;/a&gt;, IEEE GlobeCom 2024&lt;/li&gt; 
 &lt;li&gt;Julian Horsey, &lt;a href="https://www.geeky-gadgets.com/semantic-router-superfast-decision-layer-for-llms-and-ai-agents/"&gt;Semantic Router superfast decision layer for LLMs and AI agents&lt;/a&gt;, Geeky Gadgets&lt;/li&gt; 
 &lt;li&gt;azhar, &lt;a href="https://medium.com/ai-insights-cobet/beyond-basic-chatbots-how-semantic-router-is-changing-the-game-783dd959a32d"&gt;Beyond Basic Chatbots: How Semantic Router is Changing the Game&lt;/a&gt;, AI Insights @ Medium&lt;/li&gt; 
 &lt;li&gt;Daniel Avila, &lt;a href="https://blog.codegpt.co/semantic-router-enhancing-control-in-llm-conversations-68ce905c8d33"&gt;Semantic Router: Enhancing Control in LLM Conversations&lt;/a&gt;, CodeGPT @ Medium&lt;/li&gt; 
 &lt;li&gt;Yogendra Sisodia, &lt;a href="https://medium.com/@scholarly360/stop-chat-gpt-from-going-rogue-in-production-with-semantic-router-937a4768ae19"&gt;Stop Chat-GPT From Going Rogue In Production With Semantic Router&lt;/a&gt;, Medium&lt;/li&gt; 
 &lt;li&gt;Aniket Hingane, &lt;a href="https://medium.com/@learn-simplified/llm-apps-why-you-must-know-semantic-router-in-2024-part-1-bfbda81374c5"&gt;LLM Apps: Why you Must Know Semantic Router in 2024: Part 1&lt;/a&gt;, Medium&lt;/li&gt; 
 &lt;li&gt;Adrien Sales, &lt;a href="https://dev.to/adriens/semantic-router-w-ollamagemma2-real-life-10ms-hotline-challenge-1i3f"&gt;ğŸ”€ Semantic Router w. ollama/gemma2 : real life 10ms hotline challenge ğŸ¤¯&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Adrien Sales, &lt;a href="https://www.kaggle.com/code/adriensales/semantic-router-ollama-gemma2-hotline/notebook"&gt;Kaggle Notebook ğŸ”€ Semantic Router: &lt;code&gt;ollama&lt;/code&gt;/ &lt;code&gt;gemma2:9b&lt;/code&gt; hotline&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>datawhalechina/hello-agents</title>
      <link>https://github.com/datawhalechina/hello-agents</link>
      <description>&lt;p&gt;ğŸ“š ã€Šä»é›¶å¼€å§‹æ„å»ºæ™ºèƒ½ä½“ã€‹â€”â€”ä»é›¶å¼€å§‹çš„æ™ºèƒ½ä½“åŸç†ä¸å®è·µæ•™ç¨‹&lt;/p&gt;&lt;hr&gt;&lt;div align="right"&gt; 
 &lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/README_EN.md"&gt;English&lt;/a&gt; | ä¸­æ–‡ 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/images/hello-agents.png" alt="alt text" width="100%" /&gt; 
 &lt;h1&gt;Hello-Agents&lt;/h1&gt; 
 &lt;h3&gt;ğŸ¤– ã€Šä»é›¶å¼€å§‹æ„å»ºæ™ºèƒ½ä½“ã€‹&lt;/h3&gt; 
 &lt;p&gt;&lt;em&gt;ä»åŸºç¡€ç†è®ºåˆ°å®é™…åº”ç”¨ï¼Œå…¨é¢æŒæ¡æ™ºèƒ½ä½“ç³»ç»Ÿçš„è®¾è®¡ä¸å®ç°&lt;/em&gt;&lt;/p&gt; 
 &lt;img src="https://img.shields.io/github/stars/datawhalechina/Hello-Agents?style=flat&amp;amp;logo=github" alt="GitHub stars" /&gt; 
 &lt;img src="https://img.shields.io/github/forks/datawhalechina/Hello-Agents?style=flat&amp;amp;logo=github" alt="GitHub forks" /&gt; 
 &lt;img src="https://img.shields.io/badge/language-Chinese-brightgreen?style=flat" alt="Language" /&gt; 
 &lt;a href="https://github.com/datawhalechina/Hello-Agents"&gt;&lt;img src="https://img.shields.io/badge/GitHub-Project-blue?style=flat&amp;amp;logo=github" alt="GitHub Project" /&gt;&lt;/a&gt; 
 &lt;a href="https://datawhalechina.github.io/hello-agents/"&gt;&lt;img src="https://img.shields.io/badge/åœ¨çº¿é˜…è¯»-Online%20Reading-green?style=flat&amp;amp;logo=gitbook" alt="Online Reading" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ¯ é¡¹ç›®ä»‹ç»&lt;/h2&gt; 
&lt;p&gt;â€ƒâ€ƒå¦‚æœè¯´ 2024 å¹´æ˜¯"ç™¾æ¨¡å¤§æˆ˜"çš„å…ƒå¹´ï¼Œé‚£ä¹ˆ 2025 å¹´æ— ç–‘å¼€å¯äº†"Agent å…ƒå¹´"ã€‚æŠ€æœ¯çš„ç„¦ç‚¹æ­£ä»è®­ç»ƒæ›´å¤§çš„åŸºç¡€æ¨¡å‹ï¼Œè½¬å‘æ„å»ºæ›´èªæ˜çš„æ™ºèƒ½ä½“åº”ç”¨ã€‚ç„¶è€Œï¼Œå½“å‰ç³»ç»Ÿæ€§ã€é‡å®è·µçš„æ•™ç¨‹å´æåº¦åŒ®ä¹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å‘èµ·äº† Hello-Agents é¡¹ç›®ï¼Œå¸Œæœ›èƒ½ä¸ºç¤¾åŒºæä¾›ä¸€æœ¬ä»é›¶å¼€å§‹ã€ç†è®ºä¸å®æˆ˜å¹¶é‡çš„æ™ºèƒ½ä½“ç³»ç»Ÿæ„å»ºæŒ‡å—ã€‚&lt;/p&gt; 
&lt;p&gt;â€ƒâ€ƒHello-Agents æ˜¯ Datawhale ç¤¾åŒºçš„&lt;strong&gt;ç³»ç»Ÿæ€§æ™ºèƒ½ä½“å­¦ä¹ æ•™ç¨‹&lt;/strong&gt;ã€‚å¦‚ä»Š Agent æ„å»ºä¸»è¦åˆ†ä¸ºä¸¤æ´¾ï¼Œä¸€æ´¾æ˜¯ Difyï¼ŒCozeï¼Œn8n è¿™ç±»è½¯ä»¶å·¥ç¨‹ç±» Agentï¼Œå…¶æœ¬è´¨æ˜¯æµç¨‹é©±åŠ¨çš„è½¯ä»¶å¼€å‘ï¼ŒLLM ä½œä¸ºæ•°æ®å¤„ç†çš„åç«¯ï¼›å¦ä¸€æ´¾åˆ™æ˜¯ AI åŸç”Ÿçš„ Agentï¼Œå³çœŸæ­£ä»¥ AI é©±åŠ¨çš„ Agentã€‚æœ¬æ•™ç¨‹æ—¨åœ¨å¸¦é¢†å¤§å®¶æ·±å…¥ç†è§£å¹¶æ„å»ºåè€…â€”â€”çœŸæ­£çš„ AI Native Agentã€‚æ•™ç¨‹å°†å¸¦é¢†ä½ ç©¿é€æ¡†æ¶è¡¨è±¡ï¼Œä»æ™ºèƒ½ä½“çš„æ ¸å¿ƒåŸç†å‡ºå‘ï¼Œæ·±å…¥å…¶æ ¸å¿ƒæ¶æ„ï¼Œç†è§£å…¶ç»å…¸èŒƒå¼ï¼Œå¹¶æœ€ç»ˆäº²æ‰‹æ„å»ºèµ·å±äºè‡ªå·±çš„å¤šæ™ºèƒ½ä½“åº”ç”¨ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼Œæœ€å¥½çš„å­¦ä¹ æ–¹å¼å°±æ˜¯åŠ¨æ‰‹å®è·µã€‚å¸Œæœ›è¿™æœ¬æ•™ç¨‹èƒ½æˆä¸ºä½ æ¢ç´¢æ™ºèƒ½ä½“ä¸–ç•Œçš„èµ·ç‚¹ï¼Œèƒ½å¤Ÿä»ä¸€åå¤§è¯­è¨€æ¨¡å‹çš„"ä½¿ç”¨è€…"ï¼Œèœ•å˜ä¸ºä¸€åæ™ºèƒ½ä½“ç³»ç»Ÿçš„"æ„å»ºè€…"ã€‚&lt;/p&gt; 
&lt;h2&gt;ğŸ“š å¿«é€Ÿå¼€å§‹&lt;/h2&gt; 
&lt;h3&gt;åœ¨çº¿é˜…è¯»&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://datawhalechina.github.io/hello-agents/"&gt;ğŸŒ ç‚¹å‡»è¿™é‡Œå¼€å§‹åœ¨çº¿é˜…è¯»&lt;/a&gt;&lt;/strong&gt; - æ— éœ€ä¸‹è½½ï¼Œéšæ—¶éšåœ°å­¦ä¹ &lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://book.heterocat.com.cn/"&gt;ğŸ“– Cookbook(æµ‹è¯•ç‰ˆ)&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h3&gt;æœ¬åœ°é˜…è¯»&lt;/h3&gt; 
&lt;p&gt;å¦‚æœæ‚¨å¸Œæœ›åœ¨æœ¬åœ°é˜…è¯»æˆ–è´¡çŒ®å†…å®¹ï¼Œè¯·å‚è€ƒä¸‹æ–¹çš„å­¦ä¹ æŒ‡å—ã€‚&lt;/p&gt; 
&lt;h3&gt;âœ¨ ä½ å°†æ”¶è·ä»€ä¹ˆï¼Ÿ&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“– &lt;strong&gt;Datawhale å¼€æºå…è´¹&lt;/strong&gt; å®Œå…¨å…è´¹å­¦ä¹ æœ¬é¡¹ç›®æ‰€æœ‰å†…å®¹ï¼Œä¸ç¤¾åŒºå…±åŒæˆé•¿&lt;/li&gt; 
 &lt;li&gt;ğŸ” &lt;strong&gt;ç†è§£æ ¸å¿ƒåŸç†&lt;/strong&gt; æ·±å…¥ç†è§£æ™ºèƒ½ä½“çš„æ¦‚å¿µã€å†å²ä¸ç»å…¸èŒƒå¼&lt;/li&gt; 
 &lt;li&gt;ğŸ—ï¸ &lt;strong&gt;äº²æ‰‹å®ç°&lt;/strong&gt; æŒæ¡çƒ­é—¨ä½ä»£ç å¹³å°å’Œæ™ºèƒ½ä½“ä»£ç æ¡†æ¶çš„ä½¿ç”¨&lt;/li&gt; 
 &lt;li&gt;ğŸ› ï¸ &lt;strong&gt;è‡ªç ”æ¡†æ¶&lt;a href="https://github.com/jjyaoao/helloagents"&gt;HelloAgents&lt;/a&gt;&lt;/strong&gt; åŸºäº Openai åŸç”Ÿ API ä»é›¶æ„å»ºä¸€ä¸ªè‡ªå·±çš„æ™ºèƒ½ä½“æ¡†æ¶&lt;/li&gt; 
 &lt;li&gt;âš™ï¸ &lt;strong&gt;æŒæ¡é«˜çº§æŠ€èƒ½&lt;/strong&gt; ä¸€æ­¥æ­¥å®ç°ä¸Šä¸‹æ–‡å·¥ç¨‹ã€Memoryã€åè®®ã€è¯„ä¼°ç­‰ç³»ç»Ÿæ€§æŠ€æœ¯&lt;/li&gt; 
 &lt;li&gt;ğŸ¤ &lt;strong&gt;æ¨¡å‹è®­ç»ƒ&lt;/strong&gt; æŒæ¡ Agentic RLï¼Œä» SFT åˆ° GRPO çš„å…¨æµç¨‹å®æˆ˜è®­ç»ƒ LLM&lt;/li&gt; 
 &lt;li&gt;ğŸš€ &lt;strong&gt;é©±åŠ¨çœŸå®æ¡ˆä¾‹&lt;/strong&gt; å®æˆ˜å¼€å‘æ™ºèƒ½æ—…è¡ŒåŠ©æ‰‹ã€èµ›åšå°é•‡ç­‰ç»¼åˆé¡¹ç›®&lt;/li&gt; 
 &lt;li&gt;ğŸ“– &lt;strong&gt;æ±‚èŒé¢è¯•&lt;/strong&gt; å­¦ä¹ æ™ºèƒ½ä½“æ±‚èŒç›¸å…³é¢è¯•é—®é¢˜&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ“– å†…å®¹å¯¼èˆª&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;ç« èŠ‚&lt;/th&gt; 
   &lt;th&gt;å…³é”®å†…å®¹&lt;/th&gt; 
   &lt;th&gt;çŠ¶æ€&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/%E5%89%8D%E8%A8%80.md"&gt;å‰è¨€&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;é¡¹ç›®çš„ç¼˜èµ·ã€èƒŒæ™¯åŠè¯»è€…å»ºè®®&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ç¬¬ä¸€éƒ¨åˆ†ï¼šæ™ºèƒ½ä½“ä¸è¯­è¨€æ¨¡å‹åŸºç¡€&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter1/%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%88%9D%E8%AF%86%E6%99%BA%E8%83%BD%E4%BD%93.md"&gt;ç¬¬ä¸€ç«  åˆè¯†æ™ºèƒ½ä½“&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;æ™ºèƒ½ä½“å®šä¹‰ã€ç±»å‹ã€èŒƒå¼ä¸åº”ç”¨&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter2/%E7%AC%AC%E4%BA%8C%E7%AB%A0%20%E6%99%BA%E8%83%BD%E4%BD%93%E5%8F%91%E5%B1%95%E5%8F%B2.md"&gt;ç¬¬äºŒç«  æ™ºèƒ½ä½“å‘å±•å²&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ä»ç¬¦å·ä¸»ä¹‰åˆ° LLM é©±åŠ¨çš„æ™ºèƒ½ä½“æ¼”è¿›&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter3/%E7%AC%AC%E4%B8%89%E7%AB%A0%20%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80.md"&gt;ç¬¬ä¸‰ç«  å¤§è¯­è¨€æ¨¡å‹åŸºç¡€&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Transformerã€æç¤ºã€ä¸»æµ LLM åŠå…¶å±€é™&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ç¬¬äºŒéƒ¨åˆ†ï¼šæ„å»ºä½ çš„å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter4/%E7%AC%AC%E5%9B%9B%E7%AB%A0%20%E6%99%BA%E8%83%BD%E4%BD%93%E7%BB%8F%E5%85%B8%E8%8C%83%E5%BC%8F%E6%9E%84%E5%BB%BA.md"&gt;ç¬¬å››ç«  æ™ºèƒ½ä½“ç»å…¸èŒƒå¼æ„å»º&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;æ‰‹æŠŠæ‰‹å®ç° ReActã€Plan-and-Solveã€Reflection&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter5/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%9F%BA%E4%BA%8E%E4%BD%8E%E4%BB%A3%E7%A0%81%E5%B9%B3%E5%8F%B0%E7%9A%84%E6%99%BA%E8%83%BD%E4%BD%93%E6%90%AD%E5%BB%BA.md"&gt;ç¬¬äº”ç«  åŸºäºä½ä»£ç å¹³å°çš„æ™ºèƒ½ä½“æ­å»º&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;äº†è§£ Cozeã€Difyã€n8n ç­‰ä½ä»£ç æ™ºèƒ½ä½“å¹³å°ä½¿ç”¨&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter6/%E7%AC%AC%E5%85%AD%E7%AB%A0%20%E6%A1%86%E6%9E%B6%E5%BC%80%E5%8F%91%E5%AE%9E%E8%B7%B5.md"&gt;ç¬¬å…­ç«  æ¡†æ¶å¼€å‘å®è·µ&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;AutoGenã€AgentScopeã€LangGraph ç­‰ä¸»æµæ¡†æ¶åº”ç”¨&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter7/%E7%AC%AC%E4%B8%83%E7%AB%A0%20%E6%9E%84%E5%BB%BA%E4%BD%A0%E7%9A%84Agent%E6%A1%86%E6%9E%B6.md"&gt;ç¬¬ä¸ƒç«  æ„å»ºä½ çš„Agentæ¡†æ¶&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ä» 0 å¼€å§‹æ„å»ºæ™ºèƒ½ä½“æ¡†æ¶&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ç¬¬ä¸‰éƒ¨åˆ†ï¼šé«˜çº§çŸ¥è¯†æ‰©å±•&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter8/%E7%AC%AC%E5%85%AB%E7%AB%A0%20%E8%AE%B0%E5%BF%86%E4%B8%8E%E6%A3%80%E7%B4%A2.md"&gt;ç¬¬å…«ç«  è®°å¿†ä¸æ£€ç´¢&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;è®°å¿†ç³»ç»Ÿï¼ŒRAGï¼Œå­˜å‚¨&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter9/%E7%AC%AC%E4%B9%9D%E7%AB%A0%20%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B.md"&gt;ç¬¬ä¹ç«  ä¸Šä¸‹æ–‡å·¥ç¨‹&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;æŒç»­äº¤äº’çš„"æƒ…å¢ƒç†è§£"&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter10/%E7%AC%AC%E5%8D%81%E7%AB%A0%20%E6%99%BA%E8%83%BD%E4%BD%93%E9%80%9A%E4%BF%A1%E5%8D%8F%E8%AE%AE.md"&gt;ç¬¬åç«  æ™ºèƒ½ä½“é€šä¿¡åè®®&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;MCPã€A2Aã€ANP ç­‰åè®®è§£æ&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter11/%E7%AC%AC%E5%8D%81%E4%B8%80%E7%AB%A0%20Agentic-RL.md"&gt;ç¬¬åä¸€ç«  Agentic-RL&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ä» SFT åˆ° GRPO çš„ LLM è®­ç»ƒå®æˆ˜&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter12/%E7%AC%AC%E5%8D%81%E4%BA%8C%E7%AB%A0%20%E6%99%BA%E8%83%BD%E4%BD%93%E6%80%A7%E8%83%BD%E8%AF%84%E4%BC%B0.md"&gt;ç¬¬åäºŒç«  æ™ºèƒ½ä½“æ€§èƒ½è¯„ä¼°&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;æ ¸å¿ƒæŒ‡æ ‡ã€åŸºå‡†æµ‹è¯•ä¸è¯„ä¼°æ¡†æ¶&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ç¬¬å››éƒ¨åˆ†ï¼šç»¼åˆæ¡ˆä¾‹è¿›é˜¶&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter13/%E7%AC%AC%E5%8D%81%E4%B8%89%E7%AB%A0%20%E6%99%BA%E8%83%BD%E6%97%85%E8%A1%8C%E5%8A%A9%E6%89%8B.md"&gt;ç¬¬åä¸‰ç«  æ™ºèƒ½æ—…è¡ŒåŠ©æ‰‹&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;MCP ä¸å¤šæ™ºèƒ½ä½“åä½œçš„çœŸå®ä¸–ç•Œåº”ç”¨&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter14/%E7%AC%AC%E5%8D%81%E5%9B%9B%E7%AB%A0%20%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A0%94%E7%A9%B6%E6%99%BA%E8%83%BD%E4%BD%93.md"&gt;ç¬¬åå››ç«  è‡ªåŠ¨åŒ–æ·±åº¦ç ”ç©¶æ™ºèƒ½ä½“&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;DeepResearch Agent å¤ç°ä¸è§£æ&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter15/%E7%AC%AC%E5%8D%81%E4%BA%94%E7%AB%A0%20%E6%9E%84%E5%BB%BA%E8%B5%9B%E5%8D%9A%E5%B0%8F%E9%95%87.md"&gt;ç¬¬åäº”ç«  æ„å»ºèµ›åšå°é•‡&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Agent ä¸æ¸¸æˆçš„ç»“åˆï¼Œæ¨¡æ‹Ÿç¤¾ä¼šåŠ¨æ€&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ç¬¬äº”éƒ¨åˆ†ï¼šæ¯•ä¸šè®¾è®¡åŠæœªæ¥å±•æœ›&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter16/%E7%AC%AC%E5%8D%81%E5%85%AD%E7%AB%A0%20%E6%AF%95%E4%B8%9A%E8%AE%BE%E8%AE%A1.md"&gt;ç¬¬åå…­ç«  æ¯•ä¸šè®¾è®¡&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;æ„å»ºå±äºä½ çš„å®Œæ•´å¤šæ™ºèƒ½ä½“åº”ç”¨&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;ç¤¾åŒºè´¡çŒ®ç²¾é€‰ (Community Blog)&lt;/h3&gt; 
&lt;p&gt;â€ƒâ€ƒæ¬¢è¿å¤§å®¶å°†åœ¨å­¦ä¹  Hello-Agents æˆ– Agent ç›¸å…³æŠ€æœ¯ä¸­çš„ç‹¬åˆ°è§è§£ã€å®è·µæ€»ç»“ï¼Œä»¥ PR çš„å½¢å¼è´¡çŒ®åˆ°ç¤¾åŒºç²¾é€‰ã€‚å¦‚æœæ˜¯ç‹¬ç«‹äºæ­£æ–‡çš„å†…å®¹ï¼Œä¹Ÿå¯ä»¥æŠ•ç¨¿è‡³ Extra-Chapterï¼&lt;strong&gt;æœŸå¾…ä½ çš„ç¬¬ä¸€æ¬¡è´¡çŒ®ï¼&lt;/strong&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;ç¤¾åŒºç²¾é€‰&lt;/th&gt; 
   &lt;th&gt;å†…å®¹æ€»ç»“&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/datawhalechina/hello-agents/raw/main/Extra-Chapter/Extra01-%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93.md"&gt;01-Agenté¢è¯•é¢˜æ€»ç»“&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Agent å²—ä½ç›¸å…³é¢è¯•é—®é¢˜&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/datawhalechina/hello-agents/raw/main/Extra-Chapter/Extra01-%E5%8F%82%E8%80%83%E7%AD%94%E6%A1%88.md"&gt;01-Agenté¢è¯•é¢˜ç­”æ¡ˆ&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ç›¸å…³é¢è¯•é—®é¢˜ç­”æ¡ˆ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/datawhalechina/hello-agents/raw/main/Extra-Chapter/Extra02-%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%E8%A1%A5%E5%85%85%E7%9F%A5%E8%AF%86.md"&gt;02-ä¸Šä¸‹æ–‡å·¥ç¨‹å†…å®¹è¡¥å……&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ä¸Šä¸‹æ–‡å·¥ç¨‹å†…å®¹æ‰©å±•&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/datawhalechina/hello-agents/raw/main/Extra-Chapter/Extra03-Dify%E6%99%BA%E8%83%BD%E4%BD%93%E5%88%9B%E5%BB%BA%E4%BF%9D%E5%A7%86%E7%BA%A7%E6%93%8D%E4%BD%9C%E6%B5%81%E7%A8%8B.md"&gt;03-Difyæ™ºèƒ½ä½“åˆ›å»ºä¿å§†çº§æ•™ç¨‹&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Difyæ™ºèƒ½ä½“åˆ›å»ºä¿å§†çº§æ•™ç¨‹&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/datawhalechina/hello-agents/raw/main/Extra-Chapter/Extra04-DatawhaleFAQ.md"&gt;04-Hello-agentsè¯¾ç¨‹å¸¸è§é—®é¢˜&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Datawhaleè¯¾ç¨‹å¸¸è§é—®é¢˜&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;PDF ç‰ˆæœ¬ä¸‹è½½&lt;/h3&gt; 
&lt;p&gt;â€ƒâ€ƒ&lt;em&gt;&lt;strong&gt;æœ¬ Hello-Agents PDF æ•™ç¨‹å®Œå…¨å¼€æºå…è´¹ã€‚ä¸ºé˜²æ­¢å„ç±»è¥é”€å·åŠ æ°´å°åè´©å–ç»™å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåˆå­¦è€…ï¼Œæˆ‘ä»¬ç‰¹åœ°åœ¨ PDF æ–‡ä»¶ä¸­é¢„å…ˆæ·»åŠ äº†ä¸å½±å“é˜…è¯»çš„ Datawhale å¼€æºæ ‡å¿—æ°´å°ï¼Œæ•¬è¯·è°…è§£ï½&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;em&gt;Hello-Agents PDF : &lt;a href="https://github.com/datawhalechina/hello-agents/releases/tag/V1.0.0"&gt;https://github.com/datawhalechina/hello-agents/releases/tag/V1.0.0&lt;/a&gt;&lt;/em&gt;&lt;br /&gt; &lt;em&gt;Hello-Agents PDF å›½å†…ä¸‹è½½åœ°å€ : &lt;a href="https://www.datawhale.cn/learn/summary/239"&gt;https://www.datawhale.cn/learn/summary/239&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;ğŸ’¡ å¦‚ä½•å­¦ä¹ &lt;/h2&gt; 
&lt;p&gt;â€ƒâ€ƒæ¬¢è¿ä½ ï¼Œæœªæ¥çš„æ™ºèƒ½ç³»ç»Ÿæ„å»ºè€…ï¼åœ¨å¼€å¯è¿™æ®µæ¿€åŠ¨äººå¿ƒçš„æ—…ç¨‹ä¹‹å‰ï¼Œè¯·å…è®¸æˆ‘ä»¬ç»™ä½ ä¸€äº›æ¸…æ™°çš„æŒ‡å¼•ã€‚&lt;/p&gt; 
&lt;p&gt;â€ƒâ€ƒæœ¬é¡¹ç›®å†…å®¹å…¼é¡¾ç†è®ºä¸å®æˆ˜ï¼Œæ—¨åœ¨å¸®åŠ©ä½ ç³»ç»Ÿæ€§åœ°æŒæ¡ä»å•ä¸ªæ™ºèƒ½ä½“åˆ°å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„è®¾è®¡ä¸å¼€å‘å…¨æµç¨‹ã€‚å› æ­¤ï¼Œå°¤å…¶é€‚åˆæœ‰ä¸€å®šç¼–ç¨‹åŸºç¡€çš„ &lt;strong&gt;AI å¼€å‘è€…ã€è½¯ä»¶å·¥ç¨‹å¸ˆã€åœ¨æ ¡å­¦ç”Ÿ&lt;/strong&gt; ä»¥åŠå¯¹å‰æ²¿ AI æŠ€æœ¯æŠ±æœ‰æµ“åšå…´è¶£çš„ &lt;strong&gt;è‡ªå­¦è€…&lt;/strong&gt;ã€‚åœ¨å­¦ä¹ æœ¬é¡¹ç›®ä¹‹å‰ï¼Œæˆ‘ä»¬å¸Œæœ›ä½ å…·å¤‡åŸºç¡€çš„ Python ç¼–ç¨‹èƒ½åŠ›ï¼Œå¹¶å¯¹å¤§è¯­è¨€æ¨¡å‹æœ‰åŸºæœ¬çš„æ¦‚å¿µæ€§äº†è§£ï¼ˆä¾‹å¦‚ï¼ŒçŸ¥é“å¦‚ä½•é€šè¿‡ API è°ƒç”¨ä¸€ä¸ª LLMï¼‰ã€‚é¡¹ç›®çš„é‡ç‚¹æ˜¯åº”ç”¨ä¸æ„å»ºï¼Œå› æ­¤ä½ æ— éœ€å…·å¤‡æ·±åšçš„ç®—æ³•æˆ–æ¨¡å‹è®­ç»ƒèƒŒæ™¯ã€‚&lt;/p&gt; 
&lt;p&gt;â€ƒâ€ƒé¡¹ç›®åˆ†ä¸ºäº”å¤§éƒ¨åˆ†ï¼Œæ¯ä¸€éƒ¨åˆ†éƒ½æ˜¯é€šå¾€ä¸‹ä¸€é˜¶æ®µçš„åšå®é˜¶æ¢¯ï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ç¬¬ä¸€éƒ¨åˆ†ï¼šæ™ºèƒ½ä½“ä¸è¯­è¨€æ¨¡å‹åŸºç¡€&lt;/strong&gt;ï¼ˆç¬¬ä¸€ç« ï½ç¬¬ä¸‰ç« ï¼‰ï¼Œæˆ‘ä»¬å°†ä»æ™ºèƒ½ä½“çš„å®šä¹‰ã€ç±»å‹ä¸å‘å±•å†å²è®²èµ·ï¼Œä¸ºä½ æ¢³ç†"æ™ºèƒ½ä½“"è¿™ä¸€æ¦‚å¿µçš„æ¥é¾™å»è„‰ã€‚éšåï¼Œæˆ‘ä»¬ä¼šå¿«é€Ÿå·©å›ºå¤§è¯­è¨€æ¨¡å‹çš„æ ¸å¿ƒçŸ¥è¯†ï¼Œä¸ºä½ çš„å®è·µä¹‹æ—…æ‰“ä¸‹åšå®çš„ç†è®ºåœ°åŸºã€‚&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ç¬¬äºŒéƒ¨åˆ†ï¼šæ„å»ºä½ çš„å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“&lt;/strong&gt;ï¼ˆç¬¬å››ç« ï½ç¬¬ä¸ƒç« ï¼‰ï¼Œè¿™æ˜¯ä½ åŠ¨æ‰‹å®è·µçš„èµ·ç‚¹ã€‚ä½ å°†äº²æ‰‹å®ç° ReAct ç­‰ç»å…¸èŒƒå¼ï¼Œä½“éªŒ Coze ç­‰ä½ä»£ç å¹³å°çš„ä¾¿æ·ï¼Œå¹¶æŒæ¡ Langgraph ç­‰ä¸»æµæ¡†æ¶çš„åº”ç”¨ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬è¿˜ä¼šå¸¦ä½ ä»é›¶å¼€å§‹æ„å»ºä¸€ä¸ªå±äºè‡ªå·±çš„æ™ºèƒ½ä½“æ¡†æ¶ï¼Œè®©ä½ å…¼å…·â€œç”¨è½®å­â€ä¸â€œé€ è½®å­â€çš„èƒ½åŠ›ã€‚&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ç¬¬ä¸‰éƒ¨åˆ†ï¼šé«˜çº§çŸ¥è¯†æ‰©å±•&lt;/strong&gt;ï¼ˆç¬¬å…«ç« ï½ç¬¬åäºŒç« ï¼‰ï¼Œåœ¨è¿™ä¸€éƒ¨åˆ†ï¼Œä½ çš„æ™ºèƒ½ä½“å°†â€œå­¦ä¼šâ€æ€è€ƒä¸åä½œã€‚æˆ‘ä»¬å°†ä½¿ç”¨ç¬¬äºŒéƒ¨åˆ†çš„è‡ªç ”æ¡†æ¶ï¼Œæ·±å…¥æ¢ç´¢è®°å¿†ä¸æ£€ç´¢ã€ä¸Šä¸‹æ–‡å·¥ç¨‹ã€Agent è®­ç»ƒç­‰æ ¸å¿ƒæŠ€æœ¯ï¼Œå¹¶å­¦ä¹ å¤šæ™ºèƒ½ä½“é—´çš„é€šä¿¡åè®®ã€‚æœ€ç»ˆï¼Œä½ å°†æŒæ¡è¯„ä¼°æ™ºèƒ½ä½“ç³»ç»Ÿæ€§èƒ½çš„ä¸“ä¸šæ–¹æ³•ã€‚&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ç¬¬å››éƒ¨åˆ†ï¼šç»¼åˆæ¡ˆä¾‹è¿›é˜¶&lt;/strong&gt;ï¼ˆç¬¬åä¸‰ç« ï½ç¬¬åäº”ç« ï¼‰ï¼Œè¿™é‡Œæ˜¯ç†è®ºä¸å®è·µçš„äº¤æ±‡ç‚¹ã€‚ä½ å°†æŠŠæ‰€å­¦èä¼šè´¯é€šï¼Œäº²æ‰‹æ‰“é€ æ™ºèƒ½æ—…è¡ŒåŠ©æ‰‹ã€è‡ªåŠ¨åŒ–æ·±åº¦ç ”ç©¶æ™ºèƒ½ä½“ï¼Œä¹ƒè‡³ä¸€ä¸ªæ¨¡æ‹Ÿç¤¾ä¼šåŠ¨æ€çš„èµ›åšå°é•‡ï¼Œåœ¨çœŸå®æœ‰è¶£çš„é¡¹ç›®ä¸­æ·¬ç‚¼ä½ çš„æ„å»ºèƒ½åŠ›ã€‚&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ç¬¬äº”éƒ¨åˆ†ï¼šæ¯•ä¸šè®¾è®¡åŠæœªæ¥å±•æœ›&lt;/strong&gt;ï¼ˆç¬¬åå…­ç« ï¼‰ï¼Œåœ¨æ—…ç¨‹çš„ç»ˆç‚¹ï¼Œä½ å°†è¿æ¥ä¸€ä¸ªæ¯•ä¸šè®¾è®¡ï¼Œæ„å»ºä¸€ä¸ªå®Œæ•´çš„ã€å±äºä½ è‡ªå·±çš„å¤šæ™ºèƒ½ä½“åº”ç”¨ï¼Œå…¨é¢æ£€éªŒä½ çš„å­¦ä¹ æˆæœã€‚æˆ‘ä»¬è¿˜å°†ä¸ä½ ä¸€åŒå±•æœ›æ™ºèƒ½ä½“çš„æœªæ¥ï¼Œæ¢ç´¢æ¿€åŠ¨äººå¿ƒçš„å‰æ²¿æ–¹å‘ã€‚&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;â€ƒâ€ƒæ™ºèƒ½ä½“æ˜¯ä¸€ä¸ªé£é€Ÿå‘å±•ä¸”æåº¦ä¾èµ–å®è·µçš„é¢†åŸŸã€‚ä¸ºäº†è·å¾—æœ€ä½³çš„å­¦ä¹ æ•ˆæœï¼Œæˆ‘ä»¬åœ¨é¡¹ç›®çš„&lt;code&gt;code&lt;/code&gt;æ–‡ä»¶å¤¹å†…æä¾›äº†é…å¥—çš„å…¨éƒ¨ä»£ç ï¼Œå¼ºçƒˆå»ºè®®ä½ &lt;strong&gt;å°†ç†è®ºä¸å®è·µç›¸ç»“åˆ&lt;/strong&gt;ã€‚è¯·åŠ¡å¿…äº²æ‰‹è¿è¡Œã€è°ƒè¯•ç”šè‡³ä¿®æ”¹é¡¹ç›®é‡Œæä¾›çš„æ¯ä¸€ä»½ä»£ç ã€‚æ¬¢è¿ä½ éšæ—¶å…³æ³¨ Datawhale ä»¥åŠå…¶ä»– Agent ç›¸å…³ç¤¾åŒºï¼Œå½“é‡åˆ°é—®é¢˜æ—¶ï¼Œä½ å¯ä»¥éšæ—¶åœ¨æœ¬é¡¹ç›®çš„ issue åŒºæé—®ã€‚&lt;/p&gt; 
&lt;p&gt;â€ƒâ€ƒç°åœ¨ï¼Œå‡†å¤‡å¥½è¿›å…¥æ™ºèƒ½ä½“çš„å¥‡å¦™ä¸–ç•Œäº†å—ï¼Ÿè®©æˆ‘ä»¬å³åˆ»å¯ç¨‹ï¼&lt;/p&gt; 
&lt;h2&gt;ä¸‹ä¸€æ­¥è§„åˆ’&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;[]è‹±æ–‡ç‰ˆæ•™ç¨‹&lt;/li&gt; 
 &lt;li&gt;[]åŒè¯­è§†é¢‘è¯¾ç¨‹[è‹±æ–‡+ä¸­æ–‡]ï¼ˆå°†ä¼šæ›´åŠ ç»†è‡´ï¼Œå®è·µè¯¾å¸¦é¢†å¤§å®¶ä»è®¾è®¡æ€è·¯åˆ°å®æ–½ï¼Œæˆäººä»¥é±¼ä¹Ÿæˆäººä»¥æ¸”ï¼‰&lt;/li&gt; 
 &lt;li&gt;[]å…±åˆ›ç¬¬16ç« ï¼ˆæ‰“é€ å„ç±»Agentåº”ç”¨,æ›´æ‰“é€ Agentç”Ÿæ€ï¼‰&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ¤ å¦‚ä½•è´¡çŒ®&lt;/h2&gt; 
&lt;p&gt;æˆ‘ä»¬æ˜¯ä¸€ä¸ªå¼€æ”¾çš„å¼€æºç¤¾åŒºï¼Œæ¬¢è¿ä»»ä½•å½¢å¼çš„è´¡çŒ®ï¼&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ› &lt;strong&gt;æŠ¥å‘Š Bug&lt;/strong&gt; - å‘ç°å†…å®¹æˆ–ä»£ç é—®é¢˜ï¼Œè¯·æäº¤ Issue&lt;/li&gt; 
 &lt;li&gt;ğŸ’¡ &lt;strong&gt;æå‡ºå»ºè®®&lt;/strong&gt; - å¯¹é¡¹ç›®æœ‰å¥½æƒ³æ³•ï¼Œæ¬¢è¿å‘èµ·è®¨è®º&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;strong&gt;å®Œå–„å†…å®¹&lt;/strong&gt; - å¸®åŠ©æ”¹è¿›æ•™ç¨‹ï¼Œæäº¤ä½ çš„ Pull Request&lt;/li&gt; 
 &lt;li&gt;âœï¸ &lt;strong&gt;åˆ†äº«å®è·µ&lt;/strong&gt; - åœ¨"ç¤¾åŒºè´¡çŒ®ç²¾é€‰"ä¸­åˆ†äº«ä½ çš„å­¦ä¹ ç¬”è®°å’Œé¡¹ç›®&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ™ è‡´è°¢&lt;/h2&gt; 
&lt;h3&gt;æ ¸å¿ƒè´¡çŒ®è€…&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/jjyaoao"&gt;é™ˆæ€å·-é¡¹ç›®è´Ÿè´£äºº&lt;/a&gt; (Datawhale æˆå‘˜, å…¨æ–‡å†™ä½œå’Œæ ¡å¯¹)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/fengju0213"&gt;å­™éŸ¬-é¡¹ç›®è´Ÿè´£äºº&lt;/a&gt; (Datawhale æˆå‘˜, ç¬¬ä¹ç« å†…å®¹å’Œæ ¡å¯¹)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Tsumugii24"&gt;å§œèˆ’å‡¡-é¡¹ç›®è´Ÿè´£äºº&lt;/a&gt;ï¼ˆDatawhale æˆå‘˜, ç« èŠ‚ä¹ é¢˜è®¾è®¡å’Œæ ¡å¯¹ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/HeteroCat"&gt;é»„ä½©æ—-Datawhaleæ„å‘æˆå‘˜&lt;/a&gt; (Agent å¼€å‘å·¥ç¨‹å¸ˆ, ç¬¬äº”ç« å†…å®¹è´¡çŒ®è€…)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/fancyboi999"&gt;æ›¾é‘«æ°‘-Agentå·¥ç¨‹å¸ˆ&lt;/a&gt; (ç‰›å®¢ç§‘æŠ€, ç¬¬åå››ç« æ¡ˆä¾‹å¼€å‘)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://xinzhongzhu.github.io/"&gt;æœ±ä¿¡å¿ -æŒ‡å¯¼ä¸“å®¶&lt;/a&gt; (Datawhaleé¦–å¸­ç§‘å­¦å®¶-æµ™æ±Ÿå¸ˆèŒƒå¤§å­¦æ­å·äººå·¥æ™ºèƒ½ç ”ç©¶é™¢æ•™æˆ)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Extra-Chapter è´¡çŒ®è€…&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/WHQAQ11"&gt;WH&lt;/a&gt; (å†…å®¹è´¡çŒ®è€…)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/thunderbolt-fire"&gt;å‘¨å¥¥æ°-DWè´¡çŒ®è€…å›¢é˜Ÿ&lt;/a&gt; (è¥¿å®‰äº¤é€šå¤§å­¦, Extra02 å†…å®¹è´¡çŒ®)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Tasselszcx"&gt;å¼ å®¸æ—­-ä¸ªäººå¼€å‘è€…&lt;/a&gt;(å¸å›½ç†å·¥å­¦é™¢, Extra03 å†…å®¹è´¡çŒ®)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/XiaoMa-PM"&gt;é»„å®æ™—-DWè´¡çŒ®è€…å›¢é˜Ÿ&lt;/a&gt; (æ·±åœ³å¤§å­¦, Extra04 å†…å®¹è´¡çŒ®)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ç‰¹åˆ«æ„Ÿè°¢&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;æ„Ÿè°¢ &lt;a href="https://github.com/Sm1les"&gt;@Sm1les&lt;/a&gt; å¯¹æœ¬é¡¹ç›®çš„å¸®åŠ©ä¸æ”¯æŒ&lt;/li&gt; 
 &lt;li&gt;æ„Ÿè°¢æ‰€æœ‰ä¸ºæœ¬é¡¹ç›®åšå‡ºè´¡çŒ®çš„å¼€å‘è€…ä»¬ â¤ï¸&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align="center" style="margin-top: 30px;"&gt; 
 &lt;a href="https://github.com/datawhalechina/Hello-Agents/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=datawhalechina/Hello-Agents" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/images/star-history-20251212.png" alt="Datawhale" width="90%" /&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;â­ å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·ç»™æˆ‘ä»¬ä¸€ä¸ª Starï¼&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;å…³äº Datawhale&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/images/datawhale.png" alt="Datawhale" width="30%" /&gt; 
 &lt;p&gt;æ‰«æäºŒç»´ç å…³æ³¨ Datawhale å…¬ä¼—å·ï¼Œè·å–æ›´å¤šä¼˜è´¨å¼€æºå†…å®¹&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ“œ å¼€æºåè®®&lt;/h2&gt; 
&lt;p&gt;æœ¬ä½œå“é‡‡ç”¨&lt;a href="http://creativecommons.org/licenses/by-nc-sa/4.0/"&gt;çŸ¥è¯†å…±äº«ç½²å-éå•†ä¸šæ€§ä½¿ç”¨-ç›¸åŒæ–¹å¼å…±äº« 4.0 å›½é™…è®¸å¯åè®®&lt;/a&gt;è¿›è¡Œè®¸å¯ã€‚&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Flagsmith/flagsmith</title>
      <link>https://github.com/Flagsmith/flagsmith</link>
      <description>&lt;p&gt;Flagsmith is an open source feature flagging and remote config service. Self-host or use our hosted version at https://app.flagsmith.com.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href="https://www.flagsmith.com/"&gt;&lt;img src="https://raw.githubusercontent.com/Flagsmith/flagsmith/main/static-files/flagsmith-cover.png" alt="Feature Flag, Remote Config and A/B Testing platform, Flagsmith" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/Flagsmith/flagsmith/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/flagsmith/flagsmith" alt="Stars" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/u/flagsmith"&gt;&lt;img src="https://img.shields.io/docker/pulls/flagsmith/flagsmith" alt="Docker Pulls" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/flagsmith/flagsmith"&gt;&lt;img src="https://img.shields.io/docker/image-size/flagsmith/flagsmith" alt="Docker Image Size" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/hFhxNtXzgm"&gt;&lt;img src="https://img.shields.io/discord/517647859495993347" alt="Join the Discord chat" /&gt;&lt;/a&gt; &lt;a href="https://codecov.io/gh/Flagsmith/flagsmith"&gt;&lt;img src="https://codecov.io/gh/Flagsmith/flagsmith/branch/main/graph/badge.svg?token=IyGii7VSdc" alt="Coverage" /&gt;&lt;/a&gt; &lt;a href="https://opensource.org/licenses/BSD-3-Clause"&gt;&lt;img src="https://img.shields.io/badge/License-BSD_3--Clause-blue.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://depot.dev?utm_source=Flagsmith"&gt;&lt;img src="https://depot.dev/badges/built-with-depot.svg?sanitize=true" alt="Built with Depot" height="20" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://www.flagsmith.com/demo"&gt; &lt;img width="75%" height="75%" src="https://raw.githubusercontent.com/Flagsmith/flagsmith/main/static-files/ReadMe_Demo.gif" alt="Try our interactive demo" /&gt; &lt;/a&gt;&lt;/p&gt;
&lt;a href="https://www.flagsmith.com/demo"&gt; &lt;/a&gt;
&lt;p align="center"&gt;&lt;a href="https://www.flagsmith.com/demo"&gt; &lt;/a&gt;&lt;a href="https://www.flagsmith.com/demo"&gt;Try our interactive demo &lt;/a&gt; &lt;/p&gt; 
&lt;h1&gt;&lt;a href="https://flagsmith.com/"&gt;Flagsmith&lt;/a&gt; is an Open-Source Feature Flagging Tool to Ship Faster &amp;amp; Control Releases&lt;/h1&gt; 
&lt;p&gt;Change the way your team releases software. Roll out, segment, and optimiseâ€”with granular control. Stay secure with on-premise and private cloud hosting.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Feature flags: Release features behind the safety of a feature flag&lt;/li&gt; 
 &lt;li&gt;Make changes remotely: Easily toggle individual features on and off, and make changes without deploying new code&lt;/li&gt; 
 &lt;li&gt;A/B testing: Use segments to run A/B and multivariate tests on new features&lt;/li&gt; 
 &lt;li&gt;Segments: Release features to beta testers, collect feedback, and iterate&lt;/li&gt; 
 &lt;li&gt;Organisation management: Stay organised with orgs, projects, and roles for team members&lt;/li&gt; 
 &lt;li&gt;SDKs &amp;amp; frameworks: Choose from 15+ popular languages like Typescript, .NET, Java, and more. Integrate with any framework, including React, Next.js, and more&lt;/li&gt; 
 &lt;li&gt;Integrations: Use your favourite tools with Flagsmith&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Flagsmith makes it easy to create and manage feature flags across web, mobile, and server side applications. Just wrap a section of code with a flag, and then use Flagsmith to toggle that feature on or off for different environments, users or user segments.&lt;/p&gt; 
&lt;h2&gt;Get up and running in less than a minute:&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -o docker-compose.yml https://raw.githubusercontent.com/Flagsmith/flagsmith/main/docker-compose.yml
docker-compose -f docker-compose.yml up
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The application will bootstrap an admin user, organisation, and project for you. You'll find a link to set your password in your Compose logs:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-txt"&gt;Superuser "admin@example.com" created successfully.
Please go to the following page and choose a password: http://localhost:8000/password-reset/confirm/.../...
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Flagsmith/flagsmith/main/static-files/screenshot.png" alt="Flagsmith Screenshot" /&gt;&lt;/p&gt; 
&lt;h2&gt;Flagsmith Open Source&lt;/h2&gt; 
&lt;p&gt;The Flagsmith repository is comprised of two core components - the &lt;a href="https://github.com/Flagsmith/flagsmith/tree/main/api"&gt;REST API&lt;/a&gt; and the &lt;a href="https://github.com/Flagsmith/flagsmith/tree/main/frontend"&gt;frontend dashboard&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Further documentation for these can be found at:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.flagsmith.com/deployment/hosting/locally-api"&gt;API&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.flagsmith.com/deployment/hosting/locally-frontend"&gt;Frontend&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Flagsmith hosted SaaS&lt;/h2&gt; 
&lt;p&gt;You can try our hosted version for free at &lt;a href="https://flagsmith.com"&gt;https://flagsmith.com&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Community Resources + Contribution Guidelines&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.flagsmith.com/"&gt;Visit our docs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://discord.com/invite/hFhxNtXzgm"&gt;Chat with other developers on Discord&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;If you need help getting up and running, please &lt;a href="https://www.flagsmith.com/contact-us"&gt;get in touch&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We love contributions from the community and are always looking to improve! Here are our &lt;a href="https://docs.flagsmith.com/platform/contributing"&gt;contribution guidelines&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Open Source Philosophy&lt;/h2&gt; 
&lt;p&gt;The majority of our platform is open source under the &lt;a href="https://github.com/Flagsmith/flagsmith?tab=BSD-3-Clause-1-ov-file#readme"&gt;BSD-3-Clause license&lt;/a&gt;. A small number of repositories are under the MIT license.&lt;/p&gt; 
&lt;p&gt;We built Flagsmith as the open source feature flag tool we needed but couldn't find on GitHub. Our core functionality stays open, always. Read our &lt;a href="https://www.flagsmith.com/about-us"&gt;open letter to developers&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Open Source vs Paid&lt;/h2&gt; 
&lt;p&gt;With our core functionality being open, you can use our open-source feature flag and remote config management platform no matter what. Enterprise-level governance and management features are available with a valid Flagsmith Enterprise license.&lt;/p&gt; 
&lt;p&gt;To learn more, &lt;a href="https://www.flagsmith.com/contact-us"&gt;contact us&lt;/a&gt; or see our &lt;a href="https://docs.flagsmith.com/version-comparison"&gt;version comparison&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributors&lt;/h2&gt; 
&lt;p&gt;Thank you to the open source community for your contributions and for building this with us!&lt;/p&gt; 
&lt;a href="https://github.com/flagsmith/flagsmith/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=flagsmith/flagsmith" /&gt; &lt;/a&gt; 
&lt;p&gt;Made with &lt;a href="https://contrib.rocks"&gt;contrib.rocks&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>oomol-lab/pdf-craft</title>
      <link>https://github.com/oomol-lab/pdf-craft</link>
      <description>&lt;p&gt;PDF craft can convert PDF files into various other formats. This project will focus on processing PDF files of scanned books.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h1&gt;PDF Craft&lt;/h1&gt; 
 &lt;p&gt; &lt;a href="https://github.com/oomol-lab/pdf-craft/actions/workflows/build.yml" target="_blank"&gt;&lt;img src="https://img.shields.io/github/actions/workflow/status/oomol-lab/pdf-craft/build.yml" alt"ci" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/pdf-craft/" target="_blank"&gt;&lt;img src="https://img.shields.io/badge/pip_install-pdf--craft-blue" alt="pip install pdf-craft" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/pdf-craft/" target="_blank"&gt;&lt;img src="https://img.shields.io/pypi/v/pdf-craft.svg?sanitize=true" alt"pypi pdf-craft" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/pdf-craft/" target="_blank"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/pdf-craft.svg?sanitize=true" alt="python versions" /&gt;&lt;/a&gt; &lt;a href="https://github.com/oomol-lab/pdf-craft/raw/main/LICENSE" target="_blank"&gt;&lt;img src="https://img.shields.io/github/license/oomol-lab/pdf-craft" alt"license" /&gt;&lt;/a&gt; &lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://hub.oomol.com/package/pdf-craft?open=true" target="_blank"&gt;&lt;img src="https://static.oomol.com/assets/button.svg?sanitize=true" alt="Open in OOMOL Studio" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;English | &lt;a href="https://raw.githubusercontent.com/oomol-lab/pdf-craft/main/README_zh-CN.md"&gt;ä¸­æ–‡&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;pdf-craft converts PDF files into various other formats, with a focus on handling scanned book PDFs.&lt;/p&gt; 
&lt;p&gt;This project is based on &lt;a href="https://github.com/deepseek-ai/DeepSeek-OCR"&gt;DeepSeek OCR&lt;/a&gt; for document recognition. It supports the recognition of complex content such as tables and formulas. With GPU acceleration, pdf-craft can complete the entire conversion process from PDF to Markdown or EPUB locally. During the conversion, pdf-craft automatically identifies document structure, accurately extracts body text, and filters out interfering elements like headers and footers. For academic or technical documents containing footnotes, formulas, and tables, pdf-craft handles them properly, preserving these important elements (including images and other assets within footnotes). When converting to EPUB, the table of contents is automatically generated. The final Markdown or EPUB files maintain the content integrity and readability of the original book.&lt;/p&gt; 
&lt;h2&gt;Lightweight and Fast&lt;/h2&gt; 
&lt;p&gt;Starting from the official v1.0.0 release, pdf-craft fully embraces &lt;a href="https://github.com/deepseek-ai/DeepSeek-OCR"&gt;DeepSeek OCR&lt;/a&gt; and no longer relies on LLM for text correction. This change brings significant performance improvements: the entire conversion process is completed locally without network requests, eliminating the long waits and occasional network failures of the old version.&lt;/p&gt; 
&lt;p&gt;However, the new version has also removed the LLM text correction feature. If your use case still requires this functionality, you can continue using the old version &lt;a href="https://github.com/oomol-lab/pdf-craft/tree/v0.2.8"&gt;v0.2.8&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Online Demo&lt;/h3&gt; 
&lt;p&gt;We provide an &lt;a href="https://pdf.oomol.com/"&gt;online demo platform&lt;/a&gt; that lets you experience PDF Craft's conversion capabilities without any installation. You can directly upload PDF files and convert them.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://pdf.oomol.com/"&gt;&lt;img src="https://raw.githubusercontent.com/oomol-lab/pdf-craft/main/docs/images/website-en.png" alt="PDF Craft Online Demo" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu
pip install pdf-craft
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The above commands are for quick setup only. To actually use pdf-craft, you need to &lt;strong&gt;install Poppler&lt;/strong&gt; for PDF parsing (required for all use cases) and &lt;strong&gt;configure a CUDA environment&lt;/strong&gt; for OCR recognition (required for actual conversion). Please refer to the &lt;a href="https://raw.githubusercontent.com/oomol-lab/pdf-craft/main/docs/INSTALLATION.md"&gt;Installation Guide&lt;/a&gt; for detailed instructions.&lt;/p&gt; 
&lt;h3&gt;Quick Start&lt;/h3&gt; 
&lt;h4&gt;Convert to Markdown&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from pdf_craft import transform_markdown

transform_markdown(
    pdf_path="input.pdf",
    markdown_path="output.md",
    markdown_assets_path="images",
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/oomol-lab/pdf-craft/main/docs/images/pdf2md-en.png" alt="" /&gt;&lt;/p&gt; 
&lt;h4&gt;Convert to EPUB&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from pdf_craft import transform_epub, BookMeta

transform_epub(
    pdf_path="input.pdf",
    epub_path="output.epub",
    book_meta=BookMeta(
        title="Book Title",
        authors=["Author"],
    ),
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/oomol-lab/pdf-craft/main/docs/images/pdf2epub-en.png" alt="" /&gt;&lt;/p&gt; 
&lt;h2&gt;Detailed Usage&lt;/h2&gt; 
&lt;h3&gt;Convert to Markdown&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from pdf_craft import transform_markdown

transform_markdown(
    pdf_path="input.pdf",
    markdown_path="output.md",
    markdown_assets_path="images",
    analysing_path="temp",  # Optional: specify temporary folder
    ocr_size="gundam",  # Optional: tiny, small, base, large, gundam
    models_cache_path="models",  # Optional: model cache path
    includes_footnotes=True,  # Optional: include footnotes
    ignore_pdf_errors=False,  # Optional: continue on PDF rendering errors
    generate_plot=False,  # Optional: generate visualization charts
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Convert to EPUB&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from pdf_craft import transform_epub, BookMeta, TableRender, LaTeXRender

transform_epub(
    pdf_path="input.pdf",
    epub_path="output.epub",
    analysing_path="temp",  # Optional: specify temporary folder
    ocr_size="gundam",  # Optional: tiny, small, base, large, gundam
    models_cache_path="models",  # Optional: model cache path
    includes_cover=True,  # Optional: include cover
    includes_footnotes=True,  # Optional: include footnotes
    ignore_pdf_errors=False,  # Optional: continue on PDF rendering errors
    generate_plot=False,  # Optional: generate visualization charts
    book_meta=BookMeta(
        title="Book Title",
        authors=["Author 1", "Author 2"],
        publisher="Publisher",
        language="en",
    ),
    lan="en",  # Optional: language (zh/en)
    table_render=TableRender.HTML,  # Optional: table rendering method
    latex_render=LaTeXRender.MATHML,  # Optional: formula rendering method
    inline_latex=True,  # Optional: preserve inline LaTeX expressions
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Model Management&lt;/h3&gt; 
&lt;p&gt;pdf-craft depends on DeepSeek OCR models, which are automatically downloaded from Hugging Face on first run. You can control model storage and loading behavior through the &lt;code&gt;models_cache_path&lt;/code&gt; and &lt;code&gt;local_only&lt;/code&gt; parameters.&lt;/p&gt; 
&lt;h4&gt;Pre-download Models&lt;/h4&gt; 
&lt;p&gt;In production environments, it is recommended to download models in advance to avoid downloading on first run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from pdf_craft import predownload_models

predownload_models(
    models_cache_path="models",  # Specify model cache directory
    revision=None,  # Optional: specify model version
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Specify Model Cache Path&lt;/h4&gt; 
&lt;p&gt;By default, models are downloaded to the system's Hugging Face cache directory. You can customize the cache location through the &lt;code&gt;models_cache_path&lt;/code&gt; parameter:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from pdf_craft import transform_markdown

transform_markdown(
    pdf_path="input.pdf",
    markdown_path="output.md",
    models_cache_path="./my_models",  # Custom model cache directory
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Offline Mode&lt;/h4&gt; 
&lt;p&gt;If you have pre-downloaded the models, you can use &lt;code&gt;local_only=True&lt;/code&gt; to disable network downloads and ensure only local models are used:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from pdf_craft import transform_markdown

transform_markdown(
    pdf_path="input.pdf",
    markdown_path="output.md",
    models_cache_path="./my_models",
    local_only=True,  # Use local models only, do not download from network
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;API Reference&lt;/h2&gt; 
&lt;h3&gt;OCR Models&lt;/h3&gt; 
&lt;p&gt;The &lt;code&gt;ocr_size&lt;/code&gt; parameter accepts a &lt;code&gt;DeepSeekOCRSize&lt;/code&gt; type:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;tiny&lt;/code&gt; - Smallest model, fastest speed&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;small&lt;/code&gt; - Small model&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;base&lt;/code&gt; - Base model&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;large&lt;/code&gt; - Large model&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;gundam&lt;/code&gt; - Largest model, highest quality (default)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Table Rendering Methods&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;TableRender.HTML&lt;/code&gt; - HTML format (default)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;TableRender.CLIPPING&lt;/code&gt; - Clipping format (directly clips table images from the original PDF scan)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Formula Rendering Methods&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;LaTeXRender.MATHML&lt;/code&gt; - MathML format (default)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;LaTeXRender.SVG&lt;/code&gt; - SVG format&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;LaTeXRender.CLIPPING&lt;/code&gt; - Clipping format (directly clips formula images from the original PDF scan)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Inline LaTeX&lt;/h3&gt; 
&lt;p&gt;The &lt;code&gt;inline_latex&lt;/code&gt; parameter (EPUB only, default: &lt;code&gt;True&lt;/code&gt;) controls whether to preserve inline LaTeX expressions in the output. When enabled, inline mathematical formulas are preserved as LaTeX code, which can be rendered by compatible EPUB readers.&lt;/p&gt; 
&lt;h3&gt;Custom PDF Handler&lt;/h3&gt; 
&lt;p&gt;By default, pdf-craft uses Poppler (via &lt;code&gt;pdf2image&lt;/code&gt;) for PDF parsing and rendering. If Poppler is not in your system PATH, you can specify a custom path:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from pdf_craft import transform_markdown, DefaultPDFHandler

# Specify custom Poppler path
transform_markdown(
    pdf_path="input.pdf",
    markdown_path="output.md",
    pdf_handler=DefaultPDFHandler(poppler_path="/path/to/poppler/bin"),
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If not specified, pdf-craft will use Poppler from your system PATH. For advanced use cases, you can also implement the &lt;code&gt;PDFHandler&lt;/code&gt; protocol to use alternative PDF libraries.&lt;/p&gt; 
&lt;h3&gt;Error Handling&lt;/h3&gt; 
&lt;p&gt;You can use &lt;code&gt;ignore_pdf_errors=True&lt;/code&gt; to continue processing when individual pages fail to render, inserting a placeholder message for failed pages instead of stopping the entire conversion.&lt;/p&gt; 
&lt;h2&gt;Related Open Source Libraries&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/oomol-lab/epub-translator"&gt;epub-translator&lt;/a&gt; uses AI large language models to automatically translate EPUB e-books while 100% preserving the original book's format, illustrations, table of contents, and layout. It also generates bilingual versions for convenient language learning or international sharing. When combined with this library, you can convert and translate scanned PDF books. For a demonstration, see this &lt;a href="https://www.bilibili.com/video/BV1tMQZY5EYY"&gt;video: Convert PDF scanned books to EPUB format and translate to bilingual books&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the MIT License. See the &lt;a href="https://raw.githubusercontent.com/oomol-lab/pdf-craft/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;p&gt;Starting from v1.0.0, pdf-craft has fully migrated to DeepSeek OCR (MIT license), removing the previous AGPL-3.0 dependency, allowing the entire project to be released under the more permissive MIT license. Note that pdf-craft has a transitive dependency on easydict (LGPLv3) via DeepSeek OCR. Thanks to the community for their support and contributions!&lt;/p&gt; 
&lt;h2&gt;Acknowledgments&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/deepseek-ai/DeepSeek-OCR"&gt;DeepSeekOCR&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Moskize91/doc-page-extractor"&gt;doc-page-extractor&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>GoogleCloudPlatform/agent-starter-pack</title>
      <link>https://github.com/GoogleCloudPlatform/agent-starter-pack</link>
      <description>&lt;p&gt;Ship AI Agents to Google Cloud in minutes, not months. Production-ready templates with built-in CI/CD, evaluation, and observability.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ğŸš€ Agent Starter Pack&lt;/h1&gt; 
&lt;p&gt;&lt;img src="https://img.shields.io/pypi/v/agent-starter-pack?color=blue" alt="Version" /&gt; &lt;a href="https://youtu.be/jHt-ZVD660g"&gt;&lt;img src="https://img.shields.io/badge/1--Minute%20Overview-gray" alt="1-Minute Video Overview" /&gt;&lt;/a&gt; &lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/"&gt;&lt;img src="https://img.shields.io/badge/Documentation-gray" alt="Docs" /&gt;&lt;/a&gt; &lt;a href="https://studio.firebase.google.com/new?template=https%3A%2F%2Fgithub.com%2FGoogleCloudPlatform%2Fagent-starter-pack%2Ftree%2Fmain%2Fagent_starter_pack%2Fresources%2Fidx"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="https://cdn.firebasestudio.dev/btn/try_light_20.svg" /&gt; 
   &lt;source media="(prefers-color-scheme: light)" srcset="https://cdn.firebasestudio.dev/btn/try_dark_20.svg" /&gt; 
   &lt;img height="20" alt="Try in Firebase Studio" src="https://cdn.firebasestudio.dev/btn/try_blue_20.svg?sanitize=true" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; &lt;a href="https://shell.cloud.google.com/cloudshell/editor?cloudshell_git_repo=https%3A%2F%2Fgithub.com%2Feliasecchig%2Fasp-open-in-cloud-shell&amp;amp;cloudshell_print=open-in-cs"&gt;&lt;img src="https://img.shields.io/badge/Launch-in_Cloud_Shell-white" alt="Launch in Cloud Shell" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/github/stars/GoogleCloudPlatform/agent-starter-pack?color=yellow" alt="Stars" /&gt;&lt;/p&gt; 
&lt;p&gt;A Python package that provides &lt;strong&gt;production-ready templates&lt;/strong&gt; for GenAI agents on Google Cloud.&lt;/p&gt; 
&lt;p&gt;Focus on your agent logicâ€”the starter pack provides everything else: infrastructure, CI/CD, observability, and security.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;âš¡ï¸ Launch&lt;/th&gt; 
   &lt;th&gt;ğŸ§ª Experiment&lt;/th&gt; 
   &lt;th&gt;âœ… Deploy&lt;/th&gt; 
   &lt;th&gt;ğŸ› ï¸ Customize&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/GoogleCloudPlatform/agent-starter-pack/main/agent_starter_pack/agents/"&gt;Pre-built agent templates&lt;/a&gt; (ReAct, RAG, multi-agent, Live API).&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://cloud.google.com/vertex-ai/generative-ai/docs/models/evaluation-overview"&gt;Vertex AI evaluation&lt;/a&gt; and an interactive playground.&lt;/td&gt; 
   &lt;td&gt;Production-ready infra with &lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/guide/observability"&gt;monitoring, observability&lt;/a&gt;, and &lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/guide/deployment"&gt;CI/CD&lt;/a&gt; on &lt;a href="https://cloud.google.com/run"&gt;Cloud Run&lt;/a&gt; or &lt;a href="https://cloud.google.com/vertex-ai/generative-ai/docs/agent-engine/overview"&gt;Agent Engine&lt;/a&gt;.&lt;/td&gt; 
   &lt;td&gt;Extend and customize templates according to your needs. ğŸ†• Now integrating with &lt;a href="https://github.com/google-gemini/gemini-cli"&gt;Gemini CLI&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h2&gt;âš¡ Get Started in 1 Minute&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;From zero to production-ready agent in 60 seconds using &lt;a href="https://docs.astral.sh/uv/getting-started/installation/"&gt;&lt;code&gt;uv&lt;/code&gt;&lt;/a&gt;:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uvx agent-starter-pack create
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt; âœ¨ Alternative: Using pip&lt;/summary&gt; 
 &lt;p&gt;If you don't have &lt;a href="https://github.com/astral-sh/uv"&gt;&lt;code&gt;uv&lt;/code&gt;&lt;/a&gt; installed, you can use pip:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Create and activate a Python virtual environment
python -m venv .venv &amp;amp;&amp;amp; source .venv/bin/activate

# Install the agent starter pack
pip install --upgrade agent-starter-pack

# Create a new agent project
agent-starter-pack create
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p&gt;&lt;strong&gt;That's it!&lt;/strong&gt; You now have a fully functional agent projectâ€”complete with backend, frontend, and deployment infrastructureâ€”ready for you to explore and customize.&lt;/p&gt; 
&lt;h3&gt;ğŸ”§ Enhance Existing Agents&lt;/h3&gt; 
&lt;p&gt;Already have an agent? Add production-ready deployment and infrastructure by running this command in your project's root folder:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uvx agent-starter-pack enhance
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/guide/installation"&gt;Installation Guide&lt;/a&gt; for more options, or try with zero setup in &lt;a href="https://studio.firebase.google.com/new?template=https%3A%2F%2Fgithub.com%2FGoogleCloudPlatform%2Fagent-starter-pack%2Ftree%2Fmain%2Fsrc%2Fresources%2Fidx"&gt;Firebase Studio&lt;/a&gt; or &lt;a href="https://shell.cloud.google.com/cloudshell/editor?cloudshell_git_repo=https%3A%2F%2Fgithub.com%2Feliasecchig%2Fasp-open-in-cloud-shell&amp;amp;cloudshell_print=open-in-cs"&gt;Cloud Shell&lt;/a&gt;.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ¤– Agents&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Agent Name&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;adk_base&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;A base ReAct agent implemented using Google's &lt;a href="https://github.com/google/adk-python"&gt;Agent Development Kit&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;adk_a2a_base&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;An ADK agent with &lt;a href="https://a2a-protocol.org/"&gt;Agent2Agent (A2A) Protocol&lt;/a&gt; support for distributed agent communication and interoperability&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;agentic_rag&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;A RAG agent for document retrieval and Q&amp;amp;A. Supporting &lt;a href="https://cloud.google.com/generative-ai-app-builder/docs/enterprise-search-introduction"&gt;Vertex AI Search&lt;/a&gt; and &lt;a href="https://cloud.google.com/vertex-ai/docs/vector-search/overview"&gt;Vector Search&lt;/a&gt;.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;langgraph_base&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;A base ReAct agent implemented using LangChain's &lt;a href="https://github.com/langchain-ai/langgraph"&gt;LangGraph&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;adk_live&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;A real-time multimodal RAG agent powered by Gemini, supporting audio/video/text chat&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;More agents are on the way!&lt;/strong&gt; We are continuously expanding our &lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/agents/overview"&gt;agent library&lt;/a&gt;. Have a specific agent type in mind? &lt;a href="https://github.com/GoogleCloudPlatform/agent-starter-pack/issues/new?labels=enhancement"&gt;Raise an issue as a feature request!&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;ğŸ” ADK Samples&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Looking to explore more ADK examples? Check out the &lt;a href="https://github.com/google/adk-samples"&gt;ADK Samples Repository&lt;/a&gt; for additional examples and use cases demonstrating ADK's capabilities.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸŒŸ Community Showcase&lt;/h2&gt; 
&lt;p&gt;Explore amazing projects built with the Agent Starter Pack!&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/guide/community-showcase"&gt;View Community Showcase â†’&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;p&gt;The &lt;code&gt;agent-starter-pack&lt;/code&gt; offers key features to accelerate and simplify the development of your agent:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ”„ &lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/cli/setup_cicd"&gt;CI/CD Automation&lt;/a&gt;&lt;/strong&gt; - A single command to set up a complete CI/CD pipeline for all environments, supporting both &lt;strong&gt;Google Cloud Build&lt;/strong&gt; and &lt;strong&gt;GitHub Actions&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ“¥ &lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/guide/data-ingestion"&gt;Data Pipeline for RAG with Terraform/CI-CD&lt;/a&gt;&lt;/strong&gt; - Seamlessly integrate a data pipeline to process embeddings for RAG into your agent system. Supporting &lt;a href="https://cloud.google.com/generative-ai-app-builder/docs/enterprise-search-introduction"&gt;Vertex AI Search&lt;/a&gt; and &lt;a href="https://cloud.google.com/vertex-ai/docs/vector-search/overview"&gt;Vector Search&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/GoogleCloudPlatform/agent-starter-pack/main/docs/guide/remote-templating.md"&gt;Remote Templates&lt;/a&gt;&lt;/strong&gt;: Create and share your own agent starter packs templates from any Git repository.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ¤– Gemini CLI Integration&lt;/strong&gt; - Use the &lt;a href="https://github.com/google-gemini/gemini-cli"&gt;Gemini CLI&lt;/a&gt; and the included &lt;code&gt;GEMINI.md&lt;/code&gt; context file to ask questions about your template, agent architecture, and the path to production. Get instant guidance and code examples directly in your terminal.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;High-Level Architecture&lt;/h2&gt; 
&lt;p&gt;This starter pack covers all aspects of Agent development, from prototyping and evaluation to deployment and monitoring.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/GoogleCloudPlatform/agent-starter-pack/main/docs/images/ags_high_level_architecture.png" alt="High Level Architecture" title="Architecture" /&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ”§ Requirements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.10+&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://cloud.google.com/sdk/docs/install"&gt;Google Cloud SDK&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://developer.hashicorp.com/terraform/downloads"&gt;Terraform&lt;/a&gt; (for deployment)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.gnu.org/software/make/"&gt;Make&lt;/a&gt; (for development tasks)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ“š Documentation&lt;/h2&gt; 
&lt;p&gt;Visit our &lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/"&gt;documentation site&lt;/a&gt; for comprehensive guides and references!&lt;/p&gt; 
&lt;p&gt;ğŸ” &lt;strong&gt;New to the codebase?&lt;/strong&gt; Explore the &lt;a href="https://codewiki.google/github.com/googlecloudplatform/agent-starter-pack"&gt;CodeWiki&lt;/a&gt; for AI-powered code understanding and navigation.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/guide/getting-started"&gt;Getting Started Guide&lt;/a&gt; - First steps with agent-starter-pack&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/guide/installation"&gt;Installation Guide&lt;/a&gt; - Setting up your environment&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/guide/deployment"&gt;Deployment Guide&lt;/a&gt; - Taking your agent to production&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/agents/overview"&gt;Agent Templates Overview&lt;/a&gt; - Explore available agent patterns&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/cli/"&gt;CLI Reference&lt;/a&gt; - Command-line tool documentation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Video Walkthrough:&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://www.youtube.com/watch?v=9zqwym-N3lg"&gt;Exploring the Agent Starter Pack&lt;/a&gt;&lt;/strong&gt;: A comprehensive tutorial demonstrating how to rapidly deploy AI Agents using the Agent Starter Pack, covering architecture, templates, and step-by-step deployment.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://www.youtube.com/live/eZ-8UQ_t4YM?feature=shared&amp;amp;t=2791"&gt;6-minute introduction&lt;/a&gt;&lt;/strong&gt; (April 2024): Explaining the Agent Starter Pack and demonstrating its key features. Part of the Kaggle GenAI intensive course.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Looking for more examples and resources for Generative AI on Google Cloud? Check out the &lt;a href="https://github.com/GoogleCloudPlatform/generative-ai"&gt;GoogleCloudPlatform/generative-ai&lt;/a&gt; repository for notebooks, code samples, and more!&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Contributions are welcome! See the &lt;a href="https://raw.githubusercontent.com/GoogleCloudPlatform/agent-starter-pack/main/CONTRIBUTING.md"&gt;Contributing Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Feedback&lt;/h2&gt; 
&lt;p&gt;We value your input! Your feedback helps us improve this starter pack and make it more useful for the community.&lt;/p&gt; 
&lt;h3&gt;Getting Help&lt;/h3&gt; 
&lt;p&gt;If you encounter any issues or have specific suggestions, please first consider &lt;a href="https://github.com/GoogleCloudPlatform/generative-ai/issues"&gt;raising an issue&lt;/a&gt; on our GitHub repository.&lt;/p&gt; 
&lt;h3&gt;Share Your Experience&lt;/h3&gt; 
&lt;p&gt;For other types of feedback, or if you'd like to share a positive experience or success story using this starter pack, we'd love to hear from you! You can reach out to us at &lt;a href="mailto:agent-starter-pack@google.com"&gt;&lt;/a&gt;&lt;a href="mailto:agent-starter-pack@google.com"&gt;agent-starter-pack@google.com&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Thank you for your contributions!&lt;/p&gt; 
&lt;h2&gt;Disclaimer&lt;/h2&gt; 
&lt;p&gt;This repository is for demonstrative purposes only and is not an officially supported Google product.&lt;/p&gt; 
&lt;h2&gt;Terms of Service&lt;/h2&gt; 
&lt;p&gt;The agent-starter-pack templating CLI and the templates in this starter pack leverage Google Cloud APIs. When you use this starter pack, you'll be deploying resources in your own Google Cloud project and will be responsible for those resources. Please review the &lt;a href="https://cloud.google.com/terms/service-terms"&gt;Google Cloud Service Terms&lt;/a&gt; for details on the terms of service associated with these APIs.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>andrewyng/aisuite</title>
      <link>https://github.com/andrewyng/aisuite</link>
      <description>&lt;p&gt;Simple, unified interface to multiple Generative AI providers&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;aisuite&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://pypi.org/project/aisuite/"&gt;&lt;img src="https://img.shields.io/pypi/v/aisuite" alt="PyPI" /&gt;&lt;/a&gt; &lt;a href="https://github.com/psf/black"&gt;&lt;img src="https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true" alt="Code style: black" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;aisuite&lt;/code&gt; is a lightweight Python library that provides a &lt;strong&gt;unified API for working with multiple Generative AI providers&lt;/strong&gt;.&lt;br /&gt; It offers a consistent interface for models from &lt;em&gt;OpenAI, Anthropic, Google, Hugging Face, AWS, Cohere, Mistral, Ollama&lt;/em&gt;, and othersâ€”abstracting away SDK differences, authentication details, and parameter variations.&lt;br /&gt; Its design is modeled after OpenAIâ€™s API style, making it instantly familiar and easy to adopt.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;aisuite&lt;/code&gt; lets developers build and &lt;strong&gt;run LLM-based or agentic applications across providers&lt;/strong&gt; with minimal setup.&lt;br /&gt; While itâ€™s not a full-blown agents framework, it includes simple abstractions for creating standalone, lightweight agents.&lt;br /&gt; Itâ€™s designed for low learning curve â€” so you can focus on building AI systems, not integrating APIs.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;p&gt;&lt;code&gt;aisuite&lt;/code&gt; is designed to eliminate the complexity of working with multiple LLM providers while keeping your code simple and portable. Whether you're building a chatbot, an agentic application, or experimenting with different models, &lt;code&gt;aisuite&lt;/code&gt; provides the abstractions you need without getting in your way.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Unified API for multiple model providers&lt;/strong&gt; â€“ Write your code once and run it with any supported provider. Switch between OpenAI, Anthropic, Google, and others with a single parameter change.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Easy agentic app or agent creation&lt;/strong&gt; â€“ Build multi-turn agentic applications using a single parameter &lt;code&gt;max_turns&lt;/code&gt;. No need to manually manage tool execution loops.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Pass Tool calls easily&lt;/strong&gt; â€“ Pass real Python functions instead of JSON specs; aisuite handles schema generation and execution automatically.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MCP tools&lt;/strong&gt; â€“ Connect to MCP-based tools without writing boilerplate; aisuite handles connection, schema and execution seamlessly.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Modular and extensible provider architecture&lt;/strong&gt; â€“ Add support for new providers with minimal code. The plugin-style architecture makes extensions straightforward.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;You can install just the base &lt;code&gt;aisuite&lt;/code&gt; package, or install a provider's package along with &lt;code&gt;aisuite&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Install just the base package without any provider SDKs:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip install aisuite
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Install aisuite with a specific provider (e.g., Anthropic):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip install 'aisuite[anthropic]'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Install aisuite with all provider libraries:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip install 'aisuite[all]'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Setup&lt;/h2&gt; 
&lt;p&gt;To get started, you will need API Keys for the providers you intend to use. You'll need to install the provider-specific library either separately or when installing aisuite.&lt;/p&gt; 
&lt;p&gt;The API Keys can be set as environment variables, or can be passed as config to the aisuite Client constructor. You can use tools like &lt;a href="https://pypi.org/project/python-dotenv/"&gt;&lt;code&gt;python-dotenv&lt;/code&gt;&lt;/a&gt; or &lt;a href="https://direnv.net/"&gt;&lt;code&gt;direnv&lt;/code&gt;&lt;/a&gt; to set the environment variables manually. Please take a look at the &lt;code&gt;examples&lt;/code&gt; folder to see usage.&lt;/p&gt; 
&lt;p&gt;Here is a short example of using &lt;code&gt;aisuite&lt;/code&gt; to generate chat completion responses from gpt-4o and claude-3-5-sonnet.&lt;/p&gt; 
&lt;p&gt;Set the API keys.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;export OPENAI_API_KEY="your-openai-api-key"
export ANTHROPIC_API_KEY="your-anthropic-api-key"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Use the python client.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import aisuite as ai
client = ai.Client()

models = ["openai:gpt-4o", "anthropic:claude-3-5-sonnet-20240620"]

messages = [
    {"role": "system", "content": "Respond in Pirate English."},
    {"role": "user", "content": "Tell me a joke."},
]

for model in models:
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=0.75
    )
    print(response.choices[0].message.content)

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note that the model name in the create() call uses the format - &lt;code&gt;&amp;lt;provider&amp;gt;:&amp;lt;model-name&amp;gt;&lt;/code&gt;. &lt;code&gt;aisuite&lt;/code&gt; will call the appropriate provider with the right parameters based on the provider value. For a list of provider values, you can look at the directory - &lt;code&gt;aisuite/providers/&lt;/code&gt;. The list of supported providers are of the format - &lt;code&gt;&amp;lt;provider&amp;gt;_provider.py&lt;/code&gt; in that directory. We welcome providers to add support to this library by adding an implementation file in this directory. Please see section below for how to contribute.&lt;/p&gt; 
&lt;p&gt;For more examples, check out the &lt;code&gt;examples&lt;/code&gt; directory where you will find several notebooks that you can run to experiment with the interface.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Chat Completions&lt;/h2&gt; 
&lt;p&gt;The chat API provides a high-level abstraction for model interactions. It supports all core parameters (&lt;code&gt;temperature&lt;/code&gt;, &lt;code&gt;max_tokens&lt;/code&gt;, &lt;code&gt;tools&lt;/code&gt;, etc.) in a provider-agnostic way.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;response = client.chat.completions.create(
    model="google:gemini-pro",
    messages=[{"role": "user", "content": "Summarize this paragraph."}],
)
print(response.choices[0].message.content)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;code&gt;aisuite&lt;/code&gt; standardizes request and response structures so you can focus on logic rather than SDK differences.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Tool Calling &amp;amp; Agentic apps&lt;/h2&gt; 
&lt;p&gt;&lt;code&gt;aisuite&lt;/code&gt; provides a simple abstraction for tool/function calling that works across supported providers. This is in addition to the regular abstraction of passing JSON spec of the tool to the &lt;code&gt;tools&lt;/code&gt; parameter. The tool calling abstraction makes it easy to use tools with different LLMs without changing your code.&lt;/p&gt; 
&lt;p&gt;There are two ways to use tools with &lt;code&gt;aisuite&lt;/code&gt;:&lt;/p&gt; 
&lt;h3&gt;1. Manual Tool Handling&lt;/h3&gt; 
&lt;p&gt;This is the default behavior when &lt;code&gt;max_turns&lt;/code&gt; is not specified. In this mode, you have full control over the tool execution flow. You pass tools using the standard OpenAI JSON schema format, and &lt;code&gt;aisuite&lt;/code&gt; returns the LLM's tool call requests in the response. You're then responsible for executing the tools, processing results, and sending them back to the model in subsequent requests.&lt;/p&gt; 
&lt;p&gt;This approach is useful when you need:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Fine-grained control over tool execution logic&lt;/li&gt; 
 &lt;li&gt;Custom error handling or validation before executing tools&lt;/li&gt; 
 &lt;li&gt;The ability to selectively execute or skip certain tool calls&lt;/li&gt; 
 &lt;li&gt;Integration with existing tool execution pipelines&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can pass tools in the OpenAI tool format:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;def will_it_rain(location: str, time_of_day: str):
    """Check if it will rain in a location at a given time today.
    
    Args:
        location (str): Name of the city
        time_of_day (str): Time of the day in HH:MM format.
    """
    return "YES"

tools = [{
    "type": "function",
    "function": {
        "name": "will_it_rain",
        "description": "Check if it will rain in a location at a given time today",
        "parameters": {
            "type": "object",
            "properties": {
                "location": {
                    "type": "string",
                    "description": "Name of the city"
                },
                "time_of_day": {
                    "type": "string",
                    "description": "Time of the day in HH:MM format."
                }
            },
            "required": ["location", "time_of_day"]
        }
    }
}]

response = client.chat.completions.create(
    model="openai:gpt-4o",
    messages=messages,
    tools=tools
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. Automatic Tool Execution&lt;/h3&gt; 
&lt;p&gt;When &lt;code&gt;max_turns&lt;/code&gt; is specified, you can pass a list of callable Python functions as the &lt;code&gt;tools&lt;/code&gt; parameter. &lt;code&gt;aisuite&lt;/code&gt; will automatically handle the tool calling flow:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;def will_it_rain(location: str, time_of_day: str):
    """Check if it will rain in a location at a given time today.
    
    Args:
        location (str): Name of the city
        time_of_day (str): Time of the day in HH:MM format.
    """
    return "YES"

client = ai.Client()
messages = [{
    "role": "user",
    "content": "I live in San Francisco. Can you check for weather "
               "and plan an outdoor picnic for me at 2pm?"
}]

# Automatic tool execution with max_turns
response = client.chat.completions.create(
    model="openai:gpt-4o",
    messages=messages,
    tools=[will_it_rain],
    max_turns=2  # Maximum number of back-and-forth tool calls
)
print(response.choices[0].message.content)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;When &lt;code&gt;max_turns&lt;/code&gt; is specified, &lt;code&gt;aisuite&lt;/code&gt; will:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Send your message to the LLM&lt;/li&gt; 
 &lt;li&gt;Execute any tool calls the LLM requests&lt;/li&gt; 
 &lt;li&gt;Send the tool results back to the LLM&lt;/li&gt; 
 &lt;li&gt;Repeat until the conversation is complete or max_turns is reached&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;In addition to &lt;code&gt;response.choices[0].message&lt;/code&gt;, there is an additional field &lt;code&gt;response.choices[0].intermediate_messages&lt;/code&gt; which contains the list of all messages including tool interactions used. This can be used to continue the conversation with the model. For more detailed examples of tool calling, check out the &lt;code&gt;examples/tool_calling_abstraction.ipynb&lt;/code&gt; notebook.&lt;/p&gt; 
&lt;h3&gt;Model Context Protocol (MCP) Integration&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;aisuite&lt;/code&gt; natively supports &lt;strong&gt;MCP&lt;/strong&gt;, a standard protocol that allows LLMs to securely call external tools and access data. You can connect to MCP serversâ€”such as a filesystem or databaseâ€”and expose their tools directly to your model. Read more about MCP here - &lt;a href="https://modelcontextprotocol.io/docs/getting-started/intro"&gt;https://modelcontextprotocol.io/docs/getting-started/intro&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Install aisuite with MCP support:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip install 'aisuite[mcp]'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You'll also need an MCP server. For example, to use the filesystem server:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;npm install -g @modelcontextprotocol/server-filesystem
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;There are two ways to use MCP tools with aisuite:&lt;/p&gt; 
&lt;h4&gt;Option 1: Config Dict Format (Recommended for Simple Use Cases)&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import aisuite as ai

client = ai.Client()
response = client.chat.completions.create(
    model="openai:gpt-4o",
    messages=[{"role": "user", "content": "List the files in the current directory"}],
    tools=[{
        "type": "mcp",
        "name": "filesystem",
        "command": "npx",
        "args": ["-y", "@modelcontextprotocol/server-filesystem", "/path/to/directory"]
    }],
    max_turns=3
)

print(response.choices[0].message.content)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Option 2: Explicit MCPClient (Recommended for Advanced Use Cases)&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import aisuite as ai
from aisuite.mcp import MCPClient

# Create MCP client once, reuse across requests
mcp = MCPClient(
    command="npx",
    args=["-y", "@modelcontextprotocol/server-filesystem", "/path/to/directory"]
)

# Use with aisuite
client = ai.Client()
response = client.chat.completions.create(
    model="openai:gpt-4o",
    messages=[{"role": "user", "content": "List the files"}],
    tools=mcp.get_callable_tools(),
    max_turns=3
)

print(response.choices[0].message.content)
mcp.close()  # Clean up
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For detailed usage (security filters, tool prefixing, and &lt;code&gt;MCPClient&lt;/code&gt; management), see &lt;a href="https://raw.githubusercontent.com/andrewyng/aisuite/main/docs/mcp-tools.md"&gt;docs/mcp-tools.md&lt;/a&gt;. For detailed examples, see &lt;code&gt;examples/mcp_tools_example.ipynb&lt;/code&gt;.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Extending aisuite: Adding a Provider&lt;/h2&gt; 
&lt;p&gt;New providers can be added by implementing a lightweight adapter. The system uses a naming convention for discovery:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Element&lt;/th&gt; 
   &lt;th&gt;Convention&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Module file&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;&amp;lt;provider&amp;gt;_provider.py&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Class name&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;&amp;lt;Provider&amp;gt;Provider&lt;/code&gt; (capitalized)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# providers/openai_provider.py
class OpenaiProvider(BaseProvider):
    ...
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This convention ensures consistency and enables automatic loading of new integrations.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Contributions are welcome. Please review the &lt;a href="https://github.com/andrewyng/aisuite/raw/main/CONTRIBUTING.md"&gt;Contributing Guide&lt;/a&gt; and join our &lt;a href="https://discord.gg/T6Nvn8ExSb"&gt;Discord&lt;/a&gt; for discussions.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Released under the &lt;strong&gt;MIT License&lt;/strong&gt; â€” free for commercial and non-commercial use.&lt;/p&gt; 
&lt;hr /&gt;</description>
    </item>
    
    <item>
      <title>THUDM/slime</title>
      <link>https://github.com/THUDM/slime</link>
      <description>&lt;p&gt;slime is an LLM post-training framework for RL Scaling.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;slime&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/THUDM/slime/main/README_zh.md"&gt;ä¸­æ–‡ç‰ˆ&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://thudm.github.io/slime/"&gt;&lt;img src="https://img.shields.io/badge/docs-latest-brightgreen.svg?style=flat" alt="Documentation" /&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/THUDM/slime"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;slime&lt;/strong&gt; is an LLM post-training framework for RL scaling, providing two core capabilities:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;High-Performance Training&lt;/strong&gt;: Supports efficient training in various modes by connecting Megatron with SGLang;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Flexible Data Generation&lt;/strong&gt;: Enables arbitrary training data generation workflows through custom data generation interfaces and server-based engines.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;slime is the RL-framework behind &lt;a href="https://z.ai/blog/glm-4.5"&gt;GLM-4.5&lt;/a&gt; and &lt;a href="https://z.ai/blog/glm-4.6"&gt;GLM-4.6&lt;/a&gt; and apart from models from Z.ai, we also supports the following models:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Qwen3 series (Qwen3Next, Qwen3MoE, Qwen3), Qwen2.5 series;&lt;/li&gt; 
 &lt;li&gt;DeepSeek V3 series (DeepSeek V3, V3.1, DeepSeek R1);&lt;/li&gt; 
 &lt;li&gt;Llama 3.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Blogs&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Our vision: &lt;a href="https://lmsys.org/blog/2025-07-09-slime/"&gt;slime: An SGLang-Native Post-Training Framework for RL Scaling&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Our ideas on agentic training: &lt;a href="https://www.notion.so/Agent-Oriented-Design-An-Asynchronous-and-Decoupled-Framework-for-Agentic-RL-2278e692d081802cbdd5d37cef76a547"&gt;Agent-Oriented Design: An Asynchronous and Decoupled Framework for Agentic RL&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;v0.1.0 release note: &lt;a href="https://thudm.github.io/slime/blogs/release_v0.1.0.html"&gt;v0.1.0: Redefining High-Performance RL Training Frameworks&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/THUDM/slime/main/#architecture-overview"&gt;Architecture Overview&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/THUDM/slime/main/#quick-start"&gt;Quick Start&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/THUDM/slime/main/#projects-built-with-slime"&gt;Projects Built with slime&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/THUDM/slime/main/#arguments-walkthrough"&gt;Arguments Walkthrough&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/THUDM/slime/main/#developer-guide"&gt;Developer Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/THUDM/slime/main/#faq--acknowledgements"&gt;FAQ &amp;amp; Acknowledgements&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Architecture Overview&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/THUDM/slime/main/imgs/arch.png" alt="arch" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Module Descriptions&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;training (Megatron)&lt;/strong&gt;: Responsible for the main training process, reads data from the Data Buffer, and synchronizes parameters to the rollout module after training.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;rollout (SGLang + router)&lt;/strong&gt;: Generates new data (including rewards/verifier outputs) and stores it in the Data Buffer.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;data buffer&lt;/strong&gt;: A bridge module that manages prompt initialization, custom data, and rollout generation methods.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;p&gt;For a comprehensive quick start guide covering environment setup, data preparation, training startup, and key code analysis, please refer to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/THUDM/slime/main/docs/en/get_started/quick_start.md"&gt;Quick Start Guide&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We also provide examples for some use cases not covered in the quick start guide; please check &lt;a href="https://raw.githubusercontent.com/THUDM/slime/main/examples/"&gt;examples&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Projects Built upon slime&lt;/h2&gt; 
&lt;p&gt;slime has powered several novel research projects and production systems. Here are some notable examples:&lt;/p&gt; 
&lt;h3&gt;âš›ï¸ P1: Mastering Physics Olympiads with Reinforcement Learning&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://prime-rl.github.io/P1/"&gt;&lt;strong&gt;P1&lt;/strong&gt;&lt;/a&gt; is a family of open-source physics reasoning models trained entirely through reinforcement learning. P1 leverages slime as the RL post training framework, and introduces a multi-stage RL training algorithm that progressively enhances reasoning ability through adaptive learnability adjustment and stabilization mechanisms. Enpowered by this training paradigm, P1 delivers breakthrough performance in open-source physics reasoning.&lt;/p&gt; 
&lt;h3&gt;ğŸ“ˆRLVE: Scaling LM RL with Adaptive Verifiable Environments&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/Zhiyuan-Zeng/RLVE"&gt;&lt;strong&gt;RLVE&lt;/strong&gt;&lt;/a&gt; introduces an approach using verifiable environments that procedurally generate problems and provide algorithmically verifiable rewards, to scale up RL for language models (LMs). With joint training across 400 verifiable environments, RLVE enables each environment to dynamically adapt its problem difficulty distribution to the policy model's capabilities as training progresses.&lt;/p&gt; 
&lt;h3&gt;âš¡ TritonForge: Agentic RL Training Framework for Kernel Generation&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/RLsys-Foundation/TritonForge"&gt;&lt;strong&gt;TritonForge&lt;/strong&gt;&lt;/a&gt; leverages slime's SFT &amp;amp; RL capabilities to train LLMs that automatically generate optimized GPU kernels. By using a two-stage training approachâ€”supervised fine-tuning followed by reinforcement learning with multi-turn compilation feedbackâ€”TritonForge achieves remarkable results in converting PyTorch operations into high-performance Triton kernels.&lt;/p&gt; 
&lt;h3&gt;ğŸš€ APRIL: Accelerating RL Training with Active Partial Rollouts&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/RLsys-Foundation/APRIL"&gt;&lt;strong&gt;APRIL&lt;/strong&gt;&lt;/a&gt; introduces a system-level optimization that seamlessly integrates with slime to accelerate the rollout generation phase in RL training. By intelligently over-provisioning requests and actively managing partial completions, APRIL addresses the long-tail generation bottleneck that typically consumes over 90% of RL training time.&lt;/p&gt; 
&lt;p&gt;These projects showcase slime's versatilityâ€”from training code-generation models to optimizing RL training systemsâ€”making it a powerful foundation for both research and production deployments.&lt;/p&gt; 
&lt;h2&gt;Arguments Walkthrough&lt;/h2&gt; 
&lt;p&gt;Arguments in slime are divided into three categories:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Megatron arguments&lt;/strong&gt;: slime reads all arguments in Megatron. You can configure Megatron by passing arguments like &lt;code&gt;--tensor-model-parallel-size 2&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;SGLang arguments&lt;/strong&gt;: All arguments for the installed SGLang are supported. These arguments must be prefixed with &lt;code&gt;--sglang-&lt;/code&gt;. For example, &lt;code&gt;--mem-fraction-static&lt;/code&gt; should be passed as &lt;code&gt;--sglang-mem-fraction-static&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;slime-specific arguments&lt;/strong&gt;: Please refer to: &lt;a href="https://raw.githubusercontent.com/THUDM/slime/main/slime/utils/arguments.py"&gt;slime/utils/arguments.py&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;For complete usage instructions, please refer to the &lt;a href="https://raw.githubusercontent.com/THUDM/slime/main/docs/en/get_started/usage.md"&gt;Usage Documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Developer Guide&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Contributions are welcome!&lt;/strong&gt; If you have suggestions for new features, performance tuning, or feedback on user experience, feel free to submit an Issue or PR ğŸ˜Š&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Use &lt;a href="https://pre-commit.com/"&gt;pre-commit&lt;/a&gt; to ensure code style consistency for your commits:&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;apt install pre-commit -y
pre-commit install

# run pre-commit to ensure code style consistency
pre-commit run --all-files --show-diff-on-failure --color=always
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;For debugging tips, please refer to the &lt;a href="https://raw.githubusercontent.com/THUDM/slime/main/docs/en/developer_guide/debug.md"&gt;Debugging Guide&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;FAQ &amp;amp; Acknowledgements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;For frequently asked questions, please see the &lt;a href="https://raw.githubusercontent.com/THUDM/slime/main/docs/en/get_started/qa.md"&gt;Q&amp;amp;A&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Special thanks to the following projects &amp;amp; communities: SGLang, Megatronâ€‘LM, mbridge, OpenRLHF, veRL, Pai-Megatron-Patch and others.&lt;/li&gt; 
 &lt;li&gt;To quote slime, please use:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{slime_github,
  author       = {Zilin Zhu and Chengxing Xie and Xin Lv and slime Contributors},
  title        = {slime: An LLM post-training framework for RL Scaling},
  year         = {2025},
  howpublished = {\url{https://github.com/THUDM/slime}},
  note         = {GitHub repository. Corresponding author: Xin Lv},
  urldate      = {2025-06-19}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>TEN-framework/ten-framework</title>
      <link>https://github.com/TEN-framework/ten-framework</link>
      <description>&lt;p&gt;Open-source framework for conversational voice AI agents&lt;/p&gt;&lt;hr&gt;&lt;div align="center" id="readme-top"&gt; 
 &lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/799584b2-61ff-4255-bdd1-2548d0fdba52" alt="Image" /&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/TEN-framework/ten-framework/releases"&gt;&lt;img src="https://img.shields.io/github/v/release/ten-framework/ten-framework?color=369eff&amp;amp;labelColor=gray&amp;amp;logo=github&amp;amp;style=flat-square" alt="TEN Releases" /&gt;&lt;/a&gt; &lt;a href="https://coveralls.io/github/TEN-framework/ten-framework?branch=main"&gt;&lt;img src="https://coveralls.io/repos/github/TEN-framework/ten-framework/badge.svg?branch=main" alt="Coverage Status" /&gt;&lt;/a&gt; &lt;a href="https://github.com/TEN-framework/ten-framework/releases"&gt;&lt;img src="https://img.shields.io/github/release-date/ten-framework/ten-framework?labelColor=gray&amp;amp;style=flat-square" alt="Release Date" /&gt;&lt;/a&gt; &lt;a href="https://github.com/TEN-framework/ten-framework/graphs/commit-activity"&gt;&lt;img src="https://img.shields.io/github/commit-activity/m/TEN-framework/ten-framework?labelColor=gray&amp;amp;color=pink" alt="Commits" /&gt;&lt;/a&gt; &lt;a href="https://github.com/TEN-framework/ten-framework/issues"&gt;&lt;img src="https://img.shields.io/github/issues-search?query=repo%3ATEN-framework%2Ften-framework%20is%3Aclosed&amp;amp;label=issues%20closed&amp;amp;labelColor=gray&amp;amp;color=green" alt="Issues closed" /&gt;&lt;/a&gt; &lt;a href="https://github.com/TEN-framework/ten-framework/graphs/contributors"&gt;&lt;img src="https://img.shields.io/github/contributors/ten-framework/ten-framework?color=c4f042&amp;amp;labelColor=gray&amp;amp;style=flat-square" alt="Contributors" /&gt;&lt;/a&gt; &lt;a href="https://github.com/TEN-framework/ten-framework/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/License-Apache_2.0_with_certain_conditions-blue.svg?labelColor=%20%23155EEF&amp;amp;color=%20%23528bff" alt="GitHub license" /&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/TEN-framework/TEN-framework"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki" /&gt;&lt;/a&gt; &lt;a href="https://readmex.com/TEN-framework/ten-framework"&gt;&lt;img src="https://raw.githubusercontent.com/CodePhiliaX/resource-trusteeship/main/readmex.svg?sanitize=true" alt="ReadmeX" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/TEN-framework/ten-framework/raw/main/README.md"&gt;&lt;img src="https://img.shields.io/badge/English-lightgrey" alt="README in English" /&gt;&lt;/a&gt; &lt;a href="https://github.com/TEN-framework/ten-framework/raw/main/docs/README-CN.md"&gt;&lt;img src="https://img.shields.io/badge/%E7%AE%80%E4%BD%93%E4%B8%AD%E6%96%87-lightgrey" alt="ç®€ä½“ä¸­æ–‡æ“ä½œæŒ‡å—" /&gt;&lt;/a&gt; &lt;a href="https://github.com/TEN-framework/ten-framework/raw/main/docs/README-JP.md"&gt;&lt;img src="https://img.shields.io/badge/%E6%97%A5%E6%9C%AC%E8%AA%9E-lightgrey" alt="æ—¥æœ¬èªã®README" /&gt;&lt;/a&gt; &lt;a href="https://github.com/TEN-framework/ten-framework/raw/main/docs/README-KR.md"&gt;&lt;img src="https://img.shields.io/badge/%ED%95%9C%EA%B5%AD%EC%96%B4-lightgrey" alt="README in í•œêµ­ì–´" /&gt;&lt;/a&gt; &lt;a href="https://github.com/TEN-framework/ten-framework/raw/main/docs/README-ES.md"&gt;&lt;img src="https://img.shields.io/badge/Espa%C3%B1ol-lightgrey" alt="README en EspaÃ±ol" /&gt;&lt;/a&gt; &lt;a href="https://github.com/TEN-framework/ten-framework/raw/main/docs/README-FR.md"&gt;&lt;img src="https://img.shields.io/badge/Fran%C3%A7ais-lightgrey" alt="README en FranÃ§ais" /&gt;&lt;/a&gt; &lt;a href="https://github.com/TEN-framework/ten-framework/raw/main/docs/README-IT.md"&gt;&lt;img src="https://img.shields.io/badge/Italiano-lightgrey" alt="README in Italiano" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/11978"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/11978" alt="TEN-framework%2Ften_framework | Trendshift" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://theten.ai"&gt;Official Site&lt;/a&gt; â€¢ &lt;a href="https://theten.ai/docs"&gt;Documentation&lt;/a&gt; â€¢ &lt;a href="https://theten.ai/blog"&gt;Blog&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;details open&gt; 
 &lt;summary&gt;&lt;kbd&gt;Table of Contents&lt;/kbd&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#welcome-to-ten"&gt;Welcome to TEN&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#agent-examples"&gt;Agent Examples&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#quick-start-with-agent-examples"&gt;Quick Start with Agent Examples&lt;/a&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#localhost"&gt;Localhost&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#codespaces"&gt;Codespaces&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#agent-examples-self-hosting"&gt;Agent Examples Self-Hosting&lt;/a&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#deploying-with-docker"&gt;Deploying with Docker&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#deploying-with-other-cloud-services"&gt;Deploying with other cloud services&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#stay-tuned"&gt;Stay Tuned&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#ten-ecosystem"&gt;TEN Ecosystem&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#questions"&gt;Questions&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#contributing"&gt;Contributing&lt;/a&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#code-contributors"&gt;Code Contributors&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#contribution-guidelines"&gt;Contribution Guidelines&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#license"&gt;License&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;br /&gt; 
&lt;/details&gt; 
&lt;h2&gt;Welcome to TEN&lt;/h2&gt; 
&lt;p&gt;TEN is an open-source framework for real-time multimodal conversational AI.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#ten-ecosystem"&gt;TEN Ecosystem&lt;/a&gt; includes &lt;a href="https://github.com/ten-framework/ten-framework"&gt;TEN Framework&lt;/a&gt;, &lt;a href="https://github.com/TEN-framework/ten-framework/tree/main/ai_agents/agents/examples"&gt;Agent Examples&lt;/a&gt;, &lt;a href="https://github.com/ten-framework/ten-vad"&gt;VAD&lt;/a&gt;, &lt;a href="https://github.com/ten-framework/ten-turn-detection"&gt;Turn Detection&lt;/a&gt; and &lt;a href="https://github.com/ten-framework/portal"&gt;Portal&lt;/a&gt;.&lt;/p&gt; 
&lt;br /&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Community Channel&lt;/th&gt; 
   &lt;th&gt;Purpose&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://twitter.com/intent/follow?screen_name=TenFramework"&gt;&lt;img src="https://img.shields.io/twitter/follow/TenFramework?logo=X&amp;amp;color=%20%23f5f5f5" alt="Follow on X" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Follow TEN Framework on X for updates and announcements&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://discord.gg/VnPftUzAMJ"&gt;&lt;img src="https://img.shields.io/badge/Discord-Join%20TEN%20Community-5865F2?style=flat&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Discord TEN Community" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Join our Discord community to connect with developers&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.linkedin.com/company/ten-framework"&gt;&lt;img src="https://custom-icon-badges.demolab.com/badge/LinkedIn-TEN_Framework-0A66C2?logo=linkedin-white&amp;amp;logoColor=fff" alt="Follow on LinkedIn" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Follow TEN Framework on LinkedIn for updates and announcements&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/TEN-framework"&gt;&lt;img src="https://img.shields.io/badge/Hugging%20Face-TEN%20Framework-yellow?style=flat&amp;amp;logo=huggingface" alt="Hugging Face Space" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Join our Hugging Face community to explore our spaces and models&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/TEN-framework/ten-agent/discussions/170"&gt;&lt;img src="https://img.shields.io/badge/TEN_Framework-WeChat_Group-%2307C160?logo=wechat&amp;amp;labelColor=darkgreen&amp;amp;color=gray" alt="WeChat" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Join our WeChat group for Chinese community discussions&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;br /&gt; 
&lt;h2&gt;Agent Examples&lt;/h2&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/dce3db80-fb48-4e2a-8ac7-33f50bcffa32" alt="Image" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Multi-Purpose Voice Assistant&lt;/strong&gt; â€” This low-latency, high-quality real-time assistant supports both RTC and &lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/ai_agents/agents/examples/websocket-example"&gt;WebSocket&lt;/a&gt; connections, and you can extend it with &lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/ai_agents/agents/examples/voice-assistant-with-memU"&gt;Memory&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/ai_agents/agents/examples/voice-assistant-with-ten-vad"&gt;VAD&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/ai_agents/agents/examples/voice-assistant-with-turn-detection"&gt;Turn Detection&lt;/a&gt;, and other extensions.&lt;/p&gt; 
&lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/ai_agents/agents/examples/voice-assistant"&gt;Example code&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/aec54c94-ced9-4683-ae58-0a5a7ed803bd#gh-light-mode-only" alt="divider" /&gt; &lt;img src="https://github.com/user-attachments/assets/d57fad08-4f49-4a1c-bdfc-f659a5d86150#gh-dark-mode-only" alt="divider" /&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/51ab1504-b67c-49d4-8a7a-5582d9b254da" alt="Image" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Lip Sync Avatars&lt;/strong&gt; â€” Works with multiple avatar vendors, the main character features Kei, an anime character with MotionSync-powered lip sync, and also supports realistic avatars from Trulience, HeyGen, and Tavus.&lt;/p&gt; 
&lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/ai_agents/agents/examples/voice-assistant-live2d"&gt;Example code&lt;/a&gt; for different Live2D characters.&lt;/p&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/aec54c94-ced9-4683-ae58-0a5a7ed803bd#gh-light-mode-only" alt="divider" /&gt; &lt;img src="https://github.com/user-attachments/assets/d57fad08-4f49-4a1c-bdfc-f659a5d86150#gh-dark-mode-only" alt="divider" /&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/f94b21b8-9dda-4efc-9274-b028cc01296a" alt="Image" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Speech Diarization&lt;/strong&gt; â€” Real-time diarization that detects and labels speakers, the Who Likes What game shows an interactive use case.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/ai_agents/agents/examples/speechmatics-diarization"&gt;Example code&lt;/a&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/aec54c94-ced9-4683-ae58-0a5a7ed803bd#gh-light-mode-only" alt="divider" /&gt; &lt;img src="https://github.com/user-attachments/assets/d57fad08-4f49-4a1c-bdfc-f659a5d86150#gh-dark-mode-only" alt="divider" /&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/6ed5b04d-945a-4a30-a1cc-f8014b602b38" alt="Image" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;SIP Call&lt;/strong&gt; â€” SIP extension that enables phone calls powered by TEN.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/ai_agents/agents/examples/voice-assistant-sip-twilio"&gt;Example code&lt;/a&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/aec54c94-ced9-4683-ae58-0a5a7ed803bd#gh-light-mode-only" alt="divider" /&gt; &lt;img src="https://github.com/user-attachments/assets/d57fad08-4f49-4a1c-bdfc-f659a5d86150#gh-dark-mode-only" alt="divider" /&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/d793bc6c-c8de-4996-bd85-9ce88c69dd8d" alt="Image" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Transcription&lt;/strong&gt; â€” A transcription tool that transcribes audio to text.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/ai_agents/agents/examples/transcription"&gt;Example code&lt;/a&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/aec54c94-ced9-4683-ae58-0a5a7ed803bd#gh-light-mode-only" alt="divider" /&gt; &lt;img src="https://github.com/user-attachments/assets/d57fad08-4f49-4a1c-bdfc-f659a5d86150#gh-dark-mode-only" alt="divider" /&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/3d60f1ff-0f82-4fe7-b5c2-ac03d284f60c" alt="Image" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;ESP32-S3 Korvo V3&lt;/strong&gt; â€” Runs TEN agent example on the Espressif ESP32-S3 Korvo V3 development board to integrate LLM-powered communication with hardware.&lt;/p&gt; 
&lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/ai_agents/esp32-client"&gt;integration guide&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;br /&gt; 
&lt;div align="right"&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#readme-top"&gt;&lt;img src="https://img.shields.io/badge/-Back_to_top-gray?style=flat-square" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;Quick Start with Agent Examples&lt;/h2&gt; 
&lt;h3&gt;Localhost&lt;/h3&gt; 
&lt;h4&gt;Step â“µ - Prerequisites&lt;/h4&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Category&lt;/th&gt; 
   &lt;th&gt;Requirements&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Keys&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;â€¢ Agora &lt;a href="https://docs.agora.io/en/video-calling/get-started/manage-agora-account?platform=web#create-an-agora-project"&gt;App ID&lt;/a&gt; and &lt;a href="https://docs.agora.io/en/video-calling/get-started/manage-agora-account?platform=web#create-an-agora-project"&gt;App Certificate&lt;/a&gt;&lt;br /&gt;â€¢ &lt;a href="https://openai.com/index/openai-api/"&gt;OpenAI&lt;/a&gt; API key&lt;br /&gt;â€¢ &lt;a href="https://deepgram.com/"&gt;Deepgram&lt;/a&gt; ASR &lt;br /&gt;â€¢ &lt;a href="https://elevenlabs.io/"&gt;ElevenLabs&lt;/a&gt; TTS&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Installation&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;â€¢ &lt;a href="https://www.docker.com/"&gt;Docker&lt;/a&gt; / &lt;a href="https://docs.docker.com/compose/"&gt;Docker Compose&lt;/a&gt;&lt;br /&gt;â€¢ &lt;a href="https://nodejs.org/en"&gt;Node.js (LTS) v18&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Minimum System Requirements&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;â€¢ CPU &amp;gt;= 2 cores&lt;br /&gt;â€¢ RAM &amp;gt;= 4 GB&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/aec54c94-ced9-4683-ae58-0a5a7ed803bd#gh-light-mode-only" alt="divider" /&gt; &lt;img src="https://github.com/user-attachments/assets/d57fad08-4f49-4a1c-bdfc-f659a5d86150#gh-dark-mode-only" alt="divider" /&gt;&lt;/p&gt; 
&lt;!-- &gt; [!NOTE]
&gt; **macOS: Docker setting on Apple Silicon**
&gt;
&gt; Uncheck "Use Rosetta for x86/amd64 emulation" in Docker settings, it may result in slower build times on ARM, but performance will be normal when deployed to x64 servers. --&gt; 
&lt;h4&gt;Step â“¶ - Build agent examples in VM&lt;/h4&gt; 
&lt;h5&gt;1. Clone the repo, &lt;code&gt;cd&lt;/code&gt; into &lt;code&gt;ai_agents&lt;/code&gt;, and create a &lt;code&gt;.env&lt;/code&gt; file from &lt;code&gt;.env.example&lt;/code&gt;&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd ai_agents
cp ./.env.example ./.env
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;2. Set up the Agora App ID and App Certificate in &lt;code&gt;.env&lt;/code&gt;&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;AGORA_APP_ID=
AGORA_APP_CERTIFICATE=

# Deepgram (required for speech-to-text)
DEEPGRAM_API_KEY=

# OpenAI (required for language model)
OPENAI_API_KEY=

# ElevenLabs (required for text-to-speech)
ELEVENLABS_TTS_KEY=
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;3. Start agent development containers&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker compose up -d
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;4. Enter the container&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker exec -it ten_agent_dev bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;5. Build the agent with the default example (~5-8 min)&lt;/h5&gt; 
&lt;p&gt;Check the &lt;code&gt;agents/examples&lt;/code&gt; folder for additional samples. Start with one of these defaults:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# use the chained voice assistant
cd agents/examples/voice-assistant

# or use the speech-to-speech voice assistant in real time
cd agents/examples/voice-assistant-realtime
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;6. Start the web server&lt;/h5&gt; 
&lt;p&gt;Run &lt;code&gt;task build&lt;/code&gt; if you changed any local source code. This step is required for compiled languages (for example, TypeScript or Go) and not needed for Python.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;task install
task run
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;7. Access the agent&lt;/h5&gt; 
&lt;p&gt;Once the agent example is running, you can access the following interfaces:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;&lt;strong&gt;localhost:49483&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;strong&gt;localhost:3000&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;img src="https://github.com/user-attachments/assets/191a7c0a-d8e6-48f9-866f-6a70c58f0118" alt="Screenshot 1" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://github.com/user-attachments/assets/13e482b6-d907-4449-a779-9454bb24c0b1" alt="Screenshot 2" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ul&gt; 
 &lt;li&gt;TMAN Designer: &lt;a href="http://localhost:49483"&gt;localhost:49483&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Agent Examples UI: &lt;a href="http://localhost:3000"&gt;localhost:3000&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/aec54c94-ced9-4683-ae58-0a5a7ed803bd#gh-light-mode-only" alt="divider" /&gt; &lt;img src="https://github.com/user-attachments/assets/d57fad08-4f49-4a1c-bdfc-f659a5d86150#gh-dark-mode-only" alt="divider" /&gt;&lt;/p&gt; 
&lt;h4&gt;Step â“· - Customize your agent example&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;Open &lt;a href="http://localhost:49483"&gt;localhost:49483&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Right-click the STT, LLM, and TTS extensions.&lt;/li&gt; 
 &lt;li&gt;Open their properties and enter the corresponding API keys.&lt;/li&gt; 
 &lt;li&gt;Submit your changes, now you can see the updated Agent Example in &lt;a href="http://localhost:3000"&gt;localhost:3000&lt;/a&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/aec54c94-ced9-4683-ae58-0a5a7ed803bd#gh-light-mode-only" alt="divider" /&gt; &lt;img src="https://github.com/user-attachments/assets/d57fad08-4f49-4a1c-bdfc-f659a5d86150#gh-dark-mode-only" alt="divider" /&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;h4&gt;Run a transcriber app from TEN Manager without Docker (Beta)&lt;/h4&gt; 
&lt;p&gt;TEN also provides a transcriber app that you can run from TEN Manager without using Docker.&lt;/p&gt; 
&lt;p&gt;Check the &lt;a href="https://theten.ai/docs/ten_framework/getting-started/quick-start"&gt;quick start guide&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/aec54c94-ced9-4683-ae58-0a5a7ed803bd#gh-light-mode-only" alt="divider" /&gt; &lt;img src="https://github.com/user-attachments/assets/d57fad08-4f49-4a1c-bdfc-f659a5d86150#gh-dark-mode-only" alt="divider" /&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;h3&gt;Codespaces&lt;/h3&gt; 
&lt;p&gt;GitHub offers free Codespaces for each repository. You can run Agent Examples in Codespaces without using Docker. Codespaces typically start faster than local Docker environments.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://codespaces.new/ten-framework/ten-agent"&gt;&lt;img src="https://github.com/codespaces/badge.svg?sanitize=true" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Check out &lt;a href="https://theten.ai/docs/ten_agent_examples/setup_development_env/setting_up_development_inside_codespace"&gt;this guide&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;div align="right"&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#readme-top"&gt;&lt;img src="https://img.shields.io/badge/-Back_to_top-gray?style=flat-square" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;h2&gt;Agent Examples Self-Hosting&lt;/h2&gt; 
&lt;h3&gt;Deploying with Docker&lt;/h3&gt; 
&lt;p&gt;Once you have customized your agent (either by using the TMAN Designer or editing &lt;code&gt;property.json&lt;/code&gt; directly), you can deploy it by creating a release Docker image for your service.&lt;/p&gt; 
&lt;h5&gt;Release as Docker image&lt;/h5&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: The following commands need to be executed outside of any Docker container.&lt;/p&gt; 
&lt;h6&gt;Build image&lt;/h6&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd ai_agents
docker build -f agents/examples/&amp;lt;example-name&amp;gt;/Dockerfile -t example-app .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h6&gt;Run&lt;/h6&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run --rm -it --env-file .env -p 3000:3000 example-app
&lt;/code&gt;&lt;/pre&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/aec54c94-ced9-4683-ae58-0a5a7ed803bd#gh-light-mode-only" alt="divider" /&gt; &lt;img src="https://github.com/user-attachments/assets/d57fad08-4f49-4a1c-bdfc-f659a5d86150#gh-dark-mode-only" alt="divider" /&gt;&lt;/p&gt; 
&lt;h3&gt;Deploying with other cloud services&lt;/h3&gt; 
&lt;p&gt;You can split the deployment into two pieces when you want to host TEN on providers such as &lt;a href="https://vercel.com"&gt;Vercel&lt;/a&gt; or &lt;a href="https://www.netlify.com"&gt;Netlify&lt;/a&gt;.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Run the TEN backend on any container-friendly platform (a VM with Docker, Fly.io, Render, ECS, Cloud Run, or similar). Use the example Docker image without modifying it and expose port &lt;code&gt;8080&lt;/code&gt; from that service.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Deploy only the frontend to Vercel or Netlify. Point the project root to &lt;code&gt;ai_agents/agents/examples/&amp;lt;example&amp;gt;/frontend&lt;/code&gt;, run &lt;code&gt;pnpm install&lt;/code&gt; (or &lt;code&gt;bun install&lt;/code&gt;) followed by &lt;code&gt;pnpm build&lt;/code&gt; (or &lt;code&gt;bun run build&lt;/code&gt;), and keep the default &lt;code&gt;.next&lt;/code&gt; output directory.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Configure environment variables in your hosting dashboard so that &lt;code&gt;AGENT_SERVER_URL&lt;/code&gt; points to the backend URL, and add any &lt;code&gt;NEXT_PUBLIC_*&lt;/code&gt; keys the UI needs (for example, Agora credentials you surface to the browser).&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Ensure your backend accepts requests from the frontend origin â€” via open CORS or by using the built-in proxy middleware.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;With this setup, the backend handles long-running worker processes, while the hosted frontend simply forwards API traffic to it.&lt;/p&gt; 
&lt;div align="right"&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#readme-top"&gt;&lt;img src="https://img.shields.io/badge/-Back_to_top-gray?style=flat-square" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;h2&gt;Stay Tuned&lt;/h2&gt; 
&lt;p&gt;Get instant notifications for new releases and updates. Your support helps us grow and improve TEN!&lt;/p&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/72c6cc46-a2a2-484d-82a9-f3079269c815" alt="Image" /&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;div align="right"&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#readme-top"&gt;&lt;img src="https://img.shields.io/badge/-Back_to_top-gray?style=flat-square" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;h2&gt;TEN Ecosystem&lt;/h2&gt; 
&lt;br /&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Project&lt;/th&gt; 
   &lt;th&gt;Preview&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ten-framework/ten-framework"&gt;&lt;strong&gt;ï¸TEN Framework&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt;Open-source framework for conversational AI Agents.&lt;br /&gt;&lt;br /&gt;&lt;img src="https://img.shields.io/github/stars/ten-framework/ten-framework?color=ffcb47&amp;amp;labelColor=gray&amp;amp;style=flat-square&amp;amp;logo=github" alt="" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/user-attachments/assets/799584b2-61ff-4255-bdd1-2548d0fdba52" alt="" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ten-framework/ten-vad"&gt;&lt;strong&gt;TEN VAD&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt;Low-latency, lightweight and high-performance streaming voice activity detector (VAD).&lt;br /&gt;&lt;br /&gt;&lt;img src="https://img.shields.io/github/stars/ten-framework/ten-vad?color=ffcb47&amp;amp;labelColor=gray&amp;amp;style=flat-square&amp;amp;logo=github" alt="" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/user-attachments/assets/e504135e-67fd-4fa1-b0e4-d495358d8aa5" alt="" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ten-framework/ten-turn-detection"&gt;&lt;strong&gt;ï¸ TEN Turn Detection&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt;TEN Turn Detection enables full-duplex dialogue communication.&lt;br /&gt;&lt;br /&gt;&lt;img src="https://img.shields.io/github/stars/ten-framework/ten-turn-detection?color=ffcb47&amp;amp;labelColor=gray&amp;amp;style=flat-square&amp;amp;logo=github" alt="" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/user-attachments/assets/c72d82cc-3667-496c-8bd6-3d194a91c452" alt="" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/TEN-framework/ten-framework/tree/main/ai_agents/agents/examples"&gt;&lt;strong&gt;TEN Agent Examples&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt;Usecases powered by TEN.&lt;br /&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/user-attachments/assets/7f735633-c7f6-4432-b6b4-d2a2977ca588" alt="" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ten-framework/portal"&gt;&lt;strong&gt;TEN Portal&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt;The official site of the TEN Framework with documentation and a blog.&lt;br /&gt;&lt;br /&gt;&lt;img src="https://img.shields.io/github/stars/ten-framework/portal?color=ffcb47&amp;amp;labelColor=gray&amp;amp;style=flat-square&amp;amp;logo=github" alt="" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/user-attachments/assets/f56c75b9-722c-4156-902d-ae98ce2b3b5e" alt="" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;br /&gt; 
&lt;div align="right"&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#readme-top"&gt;&lt;img src="https://img.shields.io/badge/-Back_to_top-gray?style=flat-square" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;h2&gt;Questions&lt;/h2&gt; 
&lt;p&gt;TEN Framework is available on these AI-powered Q&amp;amp;A platforms. They can help you find answers quickly and accurately in multiple languages, covering everything from basic setup to advanced implementation details.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Service&lt;/th&gt; 
   &lt;th&gt;Link&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;DeepWiki&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://deepwiki.com/TEN-framework/TEN-framework"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ReadmeX&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://readmex.com/TEN-framework/ten-framework"&gt;&lt;img src="https://raw.githubusercontent.com/CodePhiliaX/resource-trusteeship/main/readmex.svg?sanitize=true" alt="ReadmeX" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;br /&gt; 
&lt;div align="right"&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#readme-top"&gt;&lt;img src="https://img.shields.io/badge/-Back_to_top-gray?style=flat-square" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome all forms of open-source collaboration! Whether you're fixing bugs, adding features, improving documentation, or sharing ideas, your contributions help advance personalized AI tools. Check out our GitHub Issues and Projects to find ways to contribute and show your skills. Together, we can build something amazing!&lt;/p&gt; 
&lt;br /&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP]&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Welcome all kinds of contributions&lt;/strong&gt; ğŸ™&lt;/p&gt; 
 &lt;p&gt;Join us in building TEN better! Every contribution makes a difference, from code to documentation. Share your TEN Agent projects on social media to inspire others!&lt;/p&gt; 
 &lt;p&gt;Connect with one of the TEN maintainers &lt;a href="https://x.com/elliotchen200"&gt;@elliotchen200&lt;/a&gt; on ğ• or &lt;a href="https://github.com/cyfyifanchen"&gt;@cyfyifanchen&lt;/a&gt; on GitHub for project updates, discussions, and collaboration opportunities.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/aec54c94-ced9-4683-ae58-0a5a7ed803bd#gh-light-mode-only" alt="divider" /&gt; &lt;img src="https://github.com/user-attachments/assets/d57fad08-4f49-4a1c-bdfc-f659a5d86150#gh-dark-mode-only" alt="divider" /&gt;&lt;/p&gt; 
&lt;h3&gt;Code Contributors&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/TEN-framework/ten-framework/graphs/contributors"&gt;&lt;img src="https://contrib.rocks/image?repo=TEN-framework/ten-framework" alt="TEN" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Contribution Guidelines&lt;/h3&gt; 
&lt;p&gt;Contributions are welcome! Please read the &lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/docs/code-of-conduct/contributing.md"&gt;contribution guidelines&lt;/a&gt; first.&lt;/p&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/aec54c94-ced9-4683-ae58-0a5a7ed803bd#gh-light-mode-only" alt="divider" /&gt; &lt;img src="https://github.com/user-attachments/assets/d57fad08-4f49-4a1c-bdfc-f659a5d86150#gh-dark-mode-only" alt="divider" /&gt;&lt;/p&gt; 
&lt;h3&gt;License&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;The entire TEN framework (except for the folders explicitly listed below) is released under the Apache License, Version 2.0, with additional restrictions. For details, please refer to the &lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/LICENSE"&gt;LICENSE&lt;/a&gt; file located in the root directory of the TEN framework.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;The components within the &lt;code&gt;packages&lt;/code&gt; directory are released under the Apache License, Version 2.0. For details, please refer to the &lt;code&gt;LICENSE&lt;/code&gt; file located in each package's root directory.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;The third-party libraries used by the TEN framework are listed and described in detail. For more information, please refer to the &lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/third_party/"&gt;third_party&lt;/a&gt; folder.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;div align="right"&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#readme-top"&gt;&lt;img src="https://img.shields.io/badge/-Back_to_top-gray?style=flat-square" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;!-- Navigation --&gt; 
&lt;!-- Header badges --&gt; 
&lt;!-- Localized READMEs --&gt; 
&lt;!-- Primary sites --&gt; 
&lt;!-- Welcome --&gt; 
&lt;!-- Community --&gt; 
&lt;!-- Agent examples --&gt; 
&lt;!-- Quick start --&gt; 
&lt;!-- Codespaces --&gt; 
&lt;!-- Deployment --&gt; 
&lt;!-- Stay tuned --&gt; 
&lt;!-- TEN ecosystem --&gt; 
&lt;!-- Contributing --&gt;</description>
    </item>
    
    <item>
      <title>SkyworkAI/SkyReels-V2</title>
      <link>https://github.com/SkyworkAI/SkyReels-V2</link>
      <description>&lt;p&gt;SkyReels-V2: Infinite-length Film Generative model&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/assets/logo2.png" alt="SkyReels Logo" width="50%" /&gt; &lt;/p&gt; 
&lt;h1 align="center"&gt;SkyReels V2: Infinite-Length Film Generative Model&lt;/h1&gt; 
&lt;p align="center"&gt; ğŸ“‘ &lt;a href="https://arxiv.org/pdf/2504.13074"&gt;Technical Report&lt;/a&gt; Â· ğŸ‘‹ &lt;a href="https://www.skyreels.ai/home?utm_campaign=github_SkyReels_V2" target="_blank"&gt;Playground&lt;/a&gt; Â· ğŸ’¬ &lt;a href="https://discord.gg/PwM6NYtccQ" target="_blank"&gt;Discord&lt;/a&gt; Â· ğŸ¤— &lt;a href="https://huggingface.co/collections/Skywork/skyreels-v2-6801b1b93df627d441d0d0d9" target="_blank"&gt;Hugging Face&lt;/a&gt; Â· ğŸ¤– &lt;a href="https://www.modelscope.cn/collections/SkyReels-V2-f665650130b144" target="_blank"&gt;ModelScope&lt;/a&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;Welcome to the &lt;strong&gt;SkyReels V2&lt;/strong&gt; repository! Here, you'll find the model weights and inference code for our infinite-length film generative models. To the best of our knowledge, it represents the first open-source video generative model employing &lt;strong&gt;AutoRegressive Diffusion-Forcing architecture&lt;/strong&gt; that achieves the &lt;strong&gt;SOTA performance&lt;/strong&gt; among publicly available models.&lt;/p&gt; 
&lt;h2&gt;ğŸ”¥ğŸ”¥ğŸ”¥ News!!&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Jun 1, 2025: ğŸ‰ We published the technical report, &lt;a href="https://arxiv.org/pdf/2506.00830"&gt;SkyReels-Audio: Omni Audio-Conditioned Talking Portraits in Video Diffusion Transformers&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;May 16, 2025: ğŸ”¥ We release the inference code for &lt;a href="https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/#ve"&gt;video extension&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/#se"&gt;start/end frame control&lt;/a&gt; in diffusion forcing model.&lt;/li&gt; 
 &lt;li&gt;Apr 24, 2025: ğŸ”¥ We release the 720P models, &lt;a href="https://huggingface.co/Skywork/SkyReels-V2-DF-14B-720P"&gt;SkyReels-V2-DF-14B-720P&lt;/a&gt; and &lt;a href="https://huggingface.co/Skywork/SkyReels-V2-I2V-14B-720P"&gt;SkyReels-V2-I2V-14B-720P&lt;/a&gt;. The former facilitates infinite-length autoregressive video generation, and the latter focuses on Image2Video synthesis.&lt;/li&gt; 
 &lt;li&gt;Apr 21, 2025: ğŸ‘‹ We release the inference code and model weights of &lt;a href="https://huggingface.co/collections/Skywork/skyreels-v2-6801b1b93df627d441d0d0d9"&gt;SkyReels-V2&lt;/a&gt; Series Models and the video captioning model &lt;a href="https://huggingface.co/Skywork/SkyCaptioner-V1"&gt;SkyCaptioner-V1&lt;/a&gt; .&lt;/li&gt; 
 &lt;li&gt;Apr 3, 2025: ğŸ”¥ We also release &lt;a href="https://github.com/SkyworkAI/SkyReels-A2"&gt;SkyReels-A2&lt;/a&gt;. This is an open-sourced controllable video generation framework capable of assembling arbitrary visual elements.&lt;/li&gt; 
 &lt;li&gt;Feb 18, 2025: ğŸ”¥ we released &lt;a href="https://github.com/SkyworkAI/SkyReels-A1"&gt;SkyReels-A1&lt;/a&gt;. This is an open-sourced and effective framework for portrait image animation.&lt;/li&gt; 
 &lt;li&gt;Feb 18, 2025: ğŸ”¥ We released &lt;a href="https://github.com/SkyworkAI/SkyReels-V1"&gt;SkyReels-V1&lt;/a&gt;. This is the first and most advanced open-source human-centric video foundation model.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ¥ Demos&lt;/h2&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td align="center"&gt; 
    &lt;video src="https://github.com/user-attachments/assets/f6f9f9a7-5d5f-433c-9d73-d8d593b7ad25" width="100%"&gt;&lt;/video&gt; &lt;/td&gt; 
   &lt;td align="center"&gt; 
    &lt;video src="https://github.com/user-attachments/assets/0eb13415-f4d9-4aaf-bcd3-3031851109b9" width="100%"&gt;&lt;/video&gt; &lt;/td&gt; 
   &lt;td align="center"&gt; 
    &lt;video src="https://github.com/user-attachments/assets/dcd16603-5bf4-4786-8e4d-1ed23889d07a" width="100%"&gt;&lt;/video&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; The demos above showcase 30-second videos generated using our SkyReels-V2 Diffusion Forcing model. 
&lt;h2&gt;ğŸ“‘ TODO List&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://arxiv.org/pdf/2504.13074"&gt;Technical Report&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Checkpoints of the 14B and 1.3B Models Series&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Single-GPU &amp;amp; Multi-GPU Inference Code&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://huggingface.co/Skywork/SkyCaptioner-V1"&gt;SkyCaptioner-V1&lt;/a&gt;: A Video Captioning Model&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Prompt Enhancer&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Diffusers integration&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Checkpoints of the 5B Models Series&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Checkpoints of the Camera Director Models&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Checkpoints of the Step &amp;amp; Guidance Distill Model&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸš€ Quickstart&lt;/h2&gt; 
&lt;h4&gt;Installation&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# clone the repository.
git clone https://github.com/SkyworkAI/SkyReels-V2
cd SkyReels-V2
# Install dependencies. Test environment uses Python 3.10.12.
pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Model Download&lt;/h4&gt; 
&lt;p&gt;You can download our models from Hugging Face:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Model Variant&lt;/th&gt; 
   &lt;th&gt;Recommended Height/Width/Frame&lt;/th&gt; 
   &lt;th&gt;Link&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="5"&gt;Diffusion Forcing&lt;/td&gt; 
   &lt;td&gt;1.3B-540P&lt;/td&gt; 
   &lt;td&gt;544 * 960 * 97f&lt;/td&gt; 
   &lt;td&gt;ğŸ¤— &lt;a href="https://huggingface.co/Skywork/SkyReels-V2-DF-1.3B-540P"&gt;Huggingface&lt;/a&gt; ğŸ¤– &lt;a href="https://www.modelscope.cn/models/Skywork/SkyReels-V2-DF-1.3B-540P"&gt;ModelScope&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5B-540P&lt;/td&gt; 
   &lt;td&gt;544 * 960 * 97f&lt;/td&gt; 
   &lt;td&gt;Coming Soon&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5B-720P&lt;/td&gt; 
   &lt;td&gt;720 * 1280 * 121f&lt;/td&gt; 
   &lt;td&gt;Coming Soon&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;14B-540P&lt;/td&gt; 
   &lt;td&gt;544 * 960 * 97f&lt;/td&gt; 
   &lt;td&gt;ğŸ¤— &lt;a href="https://huggingface.co/Skywork/SkyReels-V2-DF-14B-540P"&gt;Huggingface&lt;/a&gt; ğŸ¤– &lt;a href="https://www.modelscope.cn/models/Skywork/SkyReels-V2-DF-14B-540P"&gt;ModelScope&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;14B-720P&lt;/td&gt; 
   &lt;td&gt;720 * 1280 * 121f&lt;/td&gt; 
   &lt;td&gt;ğŸ¤— &lt;a href="https://huggingface.co/Skywork/SkyReels-V2-DF-14B-720P"&gt;Huggingface&lt;/a&gt; ğŸ¤– &lt;a href="https://www.modelscope.cn/models/Skywork/SkyReels-V2-DF-14B-720P"&gt;ModelScope&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="5"&gt;Text-to-Video&lt;/td&gt; 
   &lt;td&gt;1.3B-540P&lt;/td&gt; 
   &lt;td&gt;544 * 960 * 97f&lt;/td&gt; 
   &lt;td&gt;Coming Soon&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5B-540P&lt;/td&gt; 
   &lt;td&gt;544 * 960 * 97f&lt;/td&gt; 
   &lt;td&gt;Coming Soon&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5B-720P&lt;/td&gt; 
   &lt;td&gt;720 * 1280 * 121f&lt;/td&gt; 
   &lt;td&gt;Coming Soon&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;14B-540P&lt;/td&gt; 
   &lt;td&gt;544 * 960 * 97f&lt;/td&gt; 
   &lt;td&gt;ğŸ¤— &lt;a href="https://huggingface.co/Skywork/SkyReels-V2-T2V-14B-540P"&gt;Huggingface&lt;/a&gt; ğŸ¤– &lt;a href="https://www.modelscope.cn/models/Skywork/SkyReels-V2-T2V-14B-540P"&gt;ModelScope&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;14B-720P&lt;/td&gt; 
   &lt;td&gt;720 * 1280 * 121f&lt;/td&gt; 
   &lt;td&gt;ğŸ¤— &lt;a href="https://huggingface.co/Skywork/SkyReels-V2-T2V-14B-720P"&gt;Huggingface&lt;/a&gt; ğŸ¤– &lt;a href="https://www.modelscope.cn/models/Skywork/SkyReels-V2-T2V-14B-720P"&gt;ModelScope&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="5"&gt;Image-to-Video&lt;/td&gt; 
   &lt;td&gt;1.3B-540P&lt;/td&gt; 
   &lt;td&gt;544 * 960 * 97f&lt;/td&gt; 
   &lt;td&gt;ğŸ¤— &lt;a href="https://huggingface.co/Skywork/SkyReels-V2-I2V-1.3B-540P"&gt;Huggingface&lt;/a&gt; ğŸ¤– &lt;a href="https://www.modelscope.cn/models/Skywork/SkyReels-V2-I2V-1.3B-540P"&gt;ModelScope&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5B-540P&lt;/td&gt; 
   &lt;td&gt;544 * 960 * 97f&lt;/td&gt; 
   &lt;td&gt;Coming Soon&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5B-720P&lt;/td&gt; 
   &lt;td&gt;720 * 1280 * 121f&lt;/td&gt; 
   &lt;td&gt;Coming Soon&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;14B-540P&lt;/td&gt; 
   &lt;td&gt;544 * 960 * 97f&lt;/td&gt; 
   &lt;td&gt;ğŸ¤— &lt;a href="https://huggingface.co/Skywork/SkyReels-V2-I2V-14B-540P"&gt;Huggingface&lt;/a&gt; ğŸ¤– &lt;a href="https://www.modelscope.cn/models/Skywork/SkyReels-V2-I2V-14B-540P"&gt;ModelScope&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;14B-720P&lt;/td&gt; 
   &lt;td&gt;720 * 1280 * 121f&lt;/td&gt; 
   &lt;td&gt;ğŸ¤— &lt;a href="https://huggingface.co/Skywork/SkyReels-V2-I2V-14B-720P"&gt;Huggingface&lt;/a&gt; ğŸ¤– &lt;a href="https://www.modelscope.cn/models/Skywork/SkyReels-V2-I2V-14B-720P"&gt;ModelScope&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="3"&gt;Camera Director&lt;/td&gt; 
   &lt;td&gt;5B-540P&lt;/td&gt; 
   &lt;td&gt;544 * 960 * 97f&lt;/td&gt; 
   &lt;td&gt;Coming Soon&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5B-720P&lt;/td&gt; 
   &lt;td&gt;720 * 1280 * 121f&lt;/td&gt; 
   &lt;td&gt;Coming Soon&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;14B-720P&lt;/td&gt; 
   &lt;td&gt;720 * 1280 * 121f&lt;/td&gt; 
   &lt;td&gt;Coming Soon&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;After downloading, set the model path in your generation commands:&lt;/p&gt; 
&lt;h4&gt;Single GPU Inference&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Diffusion Forcing for Long Video Generation&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The &lt;a href="https://arxiv.org/abs/2407.01392"&gt;&lt;strong&gt;Diffusion Forcing&lt;/strong&gt;&lt;/a&gt; version model allows us to generate Infinite-Length videos. This model supports both &lt;strong&gt;text-to-video (T2V)&lt;/strong&gt; and &lt;strong&gt;image-to-video (I2V)&lt;/strong&gt; tasks, and it can perform inference in both synchronous and asynchronous modes. Here we demonstrate 2 running scripts as examples for long video generation. If you want to adjust the inference parameters, e.g., the duration of video, inference mode, read the Note below first.&lt;/p&gt; 
&lt;p&gt;synchronous generation for 10s video&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;model_id=Skywork/SkyReels-V2-DF-14B-540P
# synchronous inference
python3 generate_video_df.py \
  --model_id ${model_id} \
  --resolution 540P \
  --ar_step 0 \
  --base_num_frames 97 \
  --num_frames 257 \
  --overlap_history 17 \
  --prompt "A graceful white swan with a curved neck and delicate feathers swimming in a serene lake at dawn, its reflection perfectly mirrored in the still water as mist rises from the surface, with the swan occasionally dipping its head into the water to feed." \
  --addnoise_condition 20 \
  --offload \
  --teacache \
  --use_ret_steps \
  --teacache_thresh 0.3
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;asynchronous generation for 30s video&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;model_id=Skywork/SkyReels-V2-DF-14B-540P
# asynchronous inference
python3 generate_video_df.py \
  --model_id ${model_id} \
  --resolution 540P \
  --ar_step 5 \
  --causal_block_size 5 \
  --base_num_frames 97 \
  --num_frames 737 \
  --overlap_history 17 \
  --prompt "A graceful white swan with a curved neck and delicate feathers swimming in a serene lake at dawn, its reflection perfectly mirrored in the still water as mist rises from the surface, with the swan occasionally dipping its head into the water to feed." \
  --addnoise_condition 20 \
  --offload
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Text-to-video with &lt;code&gt;diffusers&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;import torch
from diffusers import AutoModel, SkyReelsV2DiffusionForcingPipeline, UniPCMultistepScheduler
from diffusers.utils import export_to_video

vae = AutoModel.from_pretrained("Skywork/SkyReels-V2-DF-14B-540P-Diffusers", subfolder="vae", torch_dtype=torch.float32)

pipeline = SkyReelsV2DiffusionForcingPipeline.from_pretrained(
    "Skywork/SkyReels-V2-DF-14B-540P-Diffusers",
    vae=vae,
    torch_dtype=torch.bfloat16
)
flow_shift = 8.0  # 8.0 for T2V, 5.0 for I2V
pipeline.scheduler = UniPCMultistepScheduler.from_config(pipeline.scheduler.config, flow_shift=flow_shift)
pipeline = pipeline.to("cuda")

prompt = "A cat and a dog baking a cake together in a kitchen. The cat is carefully measuring flour, while the dog is stirring the batter with a wooden spoon. The kitchen is cozy, with sunlight streaming through the window."

output = pipeline(
    prompt=prompt,
    num_inference_steps=30,
    height=544,  # 720 for 720P
    width=960,   # 1280 for 720P
    num_frames=97,
    base_num_frames=97,  # 121 for 720P
    ar_step=5,  # Controls asynchronous inference (0 for synchronous mode)
    causal_block_size=5,  # Number of frames in each block for asynchronous processing
    overlap_history=None,  # Number of frames to overlap for smooth transitions in long videos; 17 for long video generations
    addnoise_condition=20,  # Improves consistency in long video generation
).frames[0]
export_to_video(output, "T2V.mp4", fps=24, quality=8)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Image-to-video with &lt;code&gt;diffusers&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;import numpy as np
import torch
import torchvision.transforms.functional as TF
from diffusers import AutoencoderKLWan, SkyReelsV2DiffusionForcingImageToVideoPipeline, UniPCMultistepScheduler
from diffusers.utils import export_to_video, load_image

model_id = "Skywork/SkyReels-V2-DF-14B-720P-Diffusers"
vae = AutoencoderKLWan.from_pretrained(model_id, subfolder="vae", torch_dtype=torch.float32)
pipeline = SkyReelsV2DiffusionForcingImageToVideoPipeline.from_pretrained(
    model_id, vae=vae, torch_dtype=torch.bfloat16
)
flow_shift = 5.0  # 8.0 for T2V, 5.0 for I2V
pipeline.scheduler = UniPCMultistepScheduler.from_config(pipeline.scheduler.config, flow_shift=flow_shift)
pipeline.to("cuda")

first_frame = load_image("https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/flf2v_input_first_frame.png")
last_frame = load_image("https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/flf2v_input_last_frame.png")

def aspect_ratio_resize(image, pipeline, max_area=720 * 1280):
    aspect_ratio = image.height / image.width
    mod_value = pipeline.vae_scale_factor_spatial * pipeline.transformer.config.patch_size[1]
    height = round(np.sqrt(max_area * aspect_ratio)) // mod_value * mod_value
    width = round(np.sqrt(max_area / aspect_ratio)) // mod_value * mod_value
    image = image.resize((width, height))
    return image, height, width

def center_crop_resize(image, height, width):
    # Calculate resize ratio to match first frame dimensions
    resize_ratio = max(width / image.width, height / image.height)

    # Resize the image
    width = round(image.width * resize_ratio)
    height = round(image.height * resize_ratio)
    size = [width, height]
    image = TF.center_crop(image, size)

    return image, height, width

first_frame, height, width = aspect_ratio_resize(first_frame, pipeline)
if last_frame.size != first_frame.size:
    last_frame, _, _ = center_crop_resize(last_frame, height, width)

prompt = "CG animation style, a small blue bird takes off from the ground, flapping its wings. The bird's feathers are delicate, with a unique pattern on its chest. The background shows a blue sky with white clouds under bright sunshine. The camera follows the bird upward, capturing its flight and the vastness of the sky from a close-up, low-angle perspective."

output = pipeline(
    image=first_frame, last_image=last_frame, prompt=prompt, height=height, width=width, guidance_scale=5.0
).frames[0]
export_to_video(output, "output.mp4", fps=24, quality=8)
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;If you want to run the &lt;strong&gt;image-to-video (I2V)&lt;/strong&gt; task, add &lt;code&gt;--image ${image_path}&lt;/code&gt; to your command and it is also better to use &lt;strong&gt;text-to-video (T2V)&lt;/strong&gt;-like prompt which includes some descriptions of the first-frame image.&lt;/li&gt; 
  &lt;li&gt;For long video generation, you can just switch the &lt;code&gt;--num_frames&lt;/code&gt;, e.g., &lt;code&gt;--num_frames 257&lt;/code&gt; for 10s video, &lt;code&gt;--num_frames 377&lt;/code&gt; for 15s video, &lt;code&gt;--num_frames 737&lt;/code&gt; for 30s video, &lt;code&gt;--num_frames 1457&lt;/code&gt; for 60s video. The number is not strictly aligned with the logical frame number for specified time duration, but it is aligned with some training parameters, which means it may perform better. When you use asynchronous inference with causal_block_size &amp;gt; 1, the &lt;code&gt;--num_frames&lt;/code&gt; should be carefully set.&lt;/li&gt; 
  &lt;li&gt;You can use &lt;code&gt;--ar_step 5&lt;/code&gt; to enable asynchronous inference. When asynchronous inference, &lt;code&gt;--causal_block_size 5&lt;/code&gt; is recommended while it is not supposed to be set for synchronous generation. REMEMBER that the frame latent number inputted into the model in every iteration, e.g., base frame latent number (e.g., (97-1)//4+1=25 for base_num_frames=97) and (e.g., (237-97-(97-17)x1+17-1)//4+1=20 for base_num_frames=97, num_frames=237, overlap_history=17) for the last iteration, MUST be divided by causal_block_size. If you find it too hard to calculate and set proper values, just use our recommended setting above :). Asynchronous inference will take more steps to diffuse the whole sequence which means it will be SLOWER than synchronous mode. In our experiments, asynchronous inference may improve the instruction following and visual consistent performance.&lt;/li&gt; 
  &lt;li&gt;To reduce peak VRAM, just lower the &lt;code&gt;--base_num_frames&lt;/code&gt;, e.g., to 77 or 57, while keeping the same generative length &lt;code&gt;--num_frames&lt;/code&gt; you want to generate. This may slightly reduce video quality, and it should not be set too small.&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;--addnoise_condition&lt;/code&gt; is used to help smooth the long video generation by adding some noise to the clean condition. Too large noise can cause the inconsistency as well. 20 is a recommended value, and you may try larger ones, but it is recommended to not exceed 50.&lt;/li&gt; 
  &lt;li&gt;Generating a 540P video using the 1.3B model requires approximately 14.7GB peak VRAM, while the same resolution video using the 14B model demands around 51.2GB peak VRAM.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;span id="ve"&gt;Video Extention&lt;/span&gt;&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;model_id=Skywork/SkyReels-V2-DF-14B-540P
# video extention
python3 generate_video_df.py \
  --model_id ${model_id} \
  --resolution 540P \
  --ar_step 0 \
  --base_num_frames 97 \
  --num_frames 120 \
  --overlap_history 17 \
  --prompt ${prompt} \
  --addnoise_condition 20 \
  --offload \
  --use_ret_steps \
  --teacache \
  --teacache_thresh 0.3 \
  --video_path ${video_path}
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;When performing video extension, you need to pass the &lt;code&gt;--video_path ${video_path}&lt;/code&gt; parameter to specify the video to be extended.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;span id="se"&gt;Start/End Frame Control&lt;/span&gt;&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;model_id=Skywork/SkyReels-V2-DF-14B-540P
# start/end frame control
python3 generate_video_df.py \
  --model_id ${model_id} \
  --resolution 540P \
  --ar_step 0 \
  --base_num_frames 97 \
  --num_frames 97 \
  --overlap_history 17 \
  --prompt ${prompt} \
  --addnoise_condition 20 \
  --offload \
  --use_ret_steps \
  --teacache \
  --teacache_thresh 0.3 \
  --image ${image} \
  --end_image ${end_image}
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;When controlling the start and end frames, you need to pass the &lt;code&gt;--image ${image}&lt;/code&gt; parameter to control the generation of the start frame and the &lt;code&gt;--end_image ${end_image}&lt;/code&gt; parameter to control the generation of the end frame.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Video extension with &lt;code&gt;diffusers&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;import numpy as np
import torch
import torchvision.transforms.functional as TF
from diffusers import AutoencoderKLWan, SkyReelsV2DiffusionForcingVideoToVideoPipeline, UniPCMultistepScheduler
from diffusers.utils import export_to_video, load_video

model_id = "Skywork/SkyReels-V2-DF-14B-540P-Diffusers"
vae = AutoencoderKLWan.from_pretrained(model_id, subfolder="vae", torch_dtype=torch.float32)
pipeline = SkyReelsV2DiffusionForcingVideoToVideoPipeline.from_pretrained(
    model_id, vae=vae, torch_dtype=torch.bfloat16
)
flow_shift = 5.0  # 8.0 for T2V, 5.0 for I2V
pipeline.scheduler = UniPCMultistepScheduler.from_config(pipeline.scheduler.config, flow_shift=flow_shift)
pipeline.to("cuda")

video = load_video("input_video.mp4")

prompt = "CG animation style, a small blue bird takes off from the ground, flapping its wings. The bird's feathers are delicate, with a unique pattern on its chest. The background shows a blue sky with white clouds under bright sunshine. The camera follows the bird upward, capturing its flight and the vastness of the sky from a close-up, low-angle perspective."

output = pipeline(
    video=video, prompt=prompt, height=544, width=960, guidance_scale=5.0,
    num_inference_steps=30, num_frames=257, base_num_frames=97#, ar_step=5, causal_block_size=5,
).frames[0]
export_to_video(output, "output.mp4", fps=24, quality=8)
# Total frames will be the number of frames of given video + 257
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Text To Video &amp;amp; Image To Video&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# run Text-to-Video Generation
model_id=Skywork/SkyReels-V2-T2V-14B-540P
python3 generate_video.py \
  --model_id ${model_id} \
  --resolution 540P \
  --num_frames 97 \
  --guidance_scale 6.0 \
  --shift 8.0 \
  --fps 24 \
  --prompt "A serene lake surrounded by towering mountains, with a few swans gracefully gliding across the water and sunlight dancing on the surface." \
  --offload \
  --teacache \
  --use_ret_steps \
  --teacache_thresh 0.3
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;When using an &lt;strong&gt;image-to-video (I2V)&lt;/strong&gt; model, you must provide an input image using the &lt;code&gt;--image ${image_path}&lt;/code&gt; parameter. The &lt;code&gt;--guidance_scale 5.0&lt;/code&gt; and &lt;code&gt;--shift 3.0&lt;/code&gt; is recommended for I2V model.&lt;/li&gt; 
  &lt;li&gt;Generating a 540P video using the 1.3B model requires approximately 14.7GB peak VRAM, while the same resolution video using the 14B model demands around 43.4GB peak VRAM.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;T2V models with &lt;code&gt;diffusers&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;import torch
from diffusers import (
    SkyReelsV2Pipeline,
    UniPCMultistepScheduler,
    AutoencoderKLWan,
)
from diffusers.utils import export_to_video

# Load the pipeline
# Available models:
# - Skywork/SkyReels-V2-T2V-14B-540P-Diffusers
# - Skywork/SkyReels-V2-T2V-14B-720P-Diffusers
vae = AutoencoderKLWan.from_pretrained(
    "Skywork/SkyReels-V2-T2V-14B-720P-Diffusers",
    subfolder="vae",
    torch_dtype=torch.float32,
)
pipe = SkyReelsV2Pipeline.from_pretrained(
    "Skywork/SkyReels-V2-T2V-14B-720P-Diffusers",
    vae=vae,
    torch_dtype=torch.bfloat16,
)
flow_shift = 8.0  # 8.0 for T2V, 5.0 for I2V
pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config, flow_shift=flow_shift)
pipe = pipe.to("cuda")

prompt = "A cat and a dog baking a cake together in a kitchen. The cat is carefully measuring flour, while the dog is stirring the batter with a wooden spoon. The kitchen is cozy, with sunlight streaming through the window."

output = pipe(
    prompt=prompt,
    num_inference_steps=50,
    height=544,
    width=960,
    guidance_scale=6.0,  # 6.0 for T2V, 5.0 for I2V
    num_frames=97,
).frames[0]
export_to_video(output, "video.mp4", fps=24, quality=8)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;I2V models with &lt;code&gt;diffusers&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;import torch
from diffusers import (
    SkyReelsV2ImageToVideoPipeline,
    UniPCMultistepScheduler,
    AutoencoderKLWan,
)
from diffusers.utils import export_to_video
from PIL import Image

# Load the pipeline
# Available models:
# - Skywork/SkyReels-V2-I2V-1.3B-540P-Diffusers
# - Skywork/SkyReels-V2-I2V-14B-540P-Diffusers
# - Skywork/SkyReels-V2-I2V-14B-720P-Diffusers
vae = AutoencoderKLWan.from_pretrained(
    "Skywork/SkyReels-V2-I2V-14B-720P-Diffusers",
    subfolder="vae",
    torch_dtype=torch.float32,
)
pipe = SkyReelsV2ImageToVideoPipeline.from_pretrained(
    "Skywork/SkyReels-V2-I2V-14B-720P-Diffusers",
    vae=vae,
    torch_dtype=torch.bfloat16,
)
flow_shift = 5.0  # 8.0 for T2V, 5.0 for I2V
pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config, flow_shift=flow_shift)
pipe = pipe.to("cuda")

prompt = "A cat and a dog baking a cake together in a kitchen. The cat is carefully measuring flour, while the dog is stirring the batter with a wooden spoon. The kitchen is cozy, with sunlight streaming through the window."
image = Image.open("path/to/image.png")

output = pipe(
    image=image,
    prompt=prompt,
    num_inference_steps=50,
    height=544,
    width=960,
    guidance_scale=5.0,  # 6.0 for T2V, 5.0 for I2V
    num_frames=97,
).frames[0]
export_to_video(output, "video.mp4", fps=24, quality=8)
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Prompt Enhancer&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The prompt enhancer is implemented based on &lt;a href="https://huggingface.co/Qwen/Qwen2.5-32B-Instruct"&gt;Qwen2.5-32B-Instruct&lt;/a&gt; and is utilized via the &lt;code&gt;--prompt_enhancer&lt;/code&gt; parameter. It works ideally for short prompts, while for long prompts, it might generate an excessively lengthy prompt that could lead to over-saturation in the generative video. Note the peak memory of GPU is 64G+ if you use &lt;code&gt;--prompt_enhancer&lt;/code&gt;. If you want to obtain the enhanced prompt separately, you can also run the prompt_enhancer script separately for testing. The steps are as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;cd skyreels_v2_infer/pipelines
python3 prompt_enhancer.py --prompt "A serene lake surrounded by towering mountains, with a few swans gracefully gliding across the water and sunlight dancing on the surface."
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;--prompt_enhancer&lt;/code&gt; is not allowed if using &lt;code&gt;--use_usp&lt;/code&gt;. We recommend running the skyreels_v2_infer/pipelines/prompt_enhancer.py script first to generate enhanced prompt before enabling the &lt;code&gt;--use_usp&lt;/code&gt; parameter.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;Advanced Configuration Options&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Below are the key parameters you can customize for video generation:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;Parameter&lt;/th&gt; 
   &lt;th align="center"&gt;Recommended Value&lt;/th&gt; 
   &lt;th align="center"&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--prompt&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;Text description for generating your video&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--image&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;Path to input image for image-to-video generation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--resolution&lt;/td&gt; 
   &lt;td align="center"&gt;540P or 720P&lt;/td&gt; 
   &lt;td align="center"&gt;Output video resolution (select based on model type)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--num_frames&lt;/td&gt; 
   &lt;td align="center"&gt;97 or 121&lt;/td&gt; 
   &lt;td align="center"&gt;Total frames to generate (&lt;strong&gt;97 for 540P models&lt;/strong&gt;, &lt;strong&gt;121 for 720P models&lt;/strong&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--inference_steps&lt;/td&gt; 
   &lt;td align="center"&gt;50&lt;/td&gt; 
   &lt;td align="center"&gt;Number of denoising steps&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--fps&lt;/td&gt; 
   &lt;td align="center"&gt;24&lt;/td&gt; 
   &lt;td align="center"&gt;Frames per second in the output video&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--shift&lt;/td&gt; 
   &lt;td align="center"&gt;8.0 or 5.0&lt;/td&gt; 
   &lt;td align="center"&gt;Flow matching scheduler parameter (&lt;strong&gt;8.0 for T2V&lt;/strong&gt;, &lt;strong&gt;5.0 for I2V&lt;/strong&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--guidance_scale&lt;/td&gt; 
   &lt;td align="center"&gt;6.0 or 5.0&lt;/td&gt; 
   &lt;td align="center"&gt;Controls text adherence strength (&lt;strong&gt;6.0 for T2V&lt;/strong&gt;, &lt;strong&gt;5.0 for I2V&lt;/strong&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--seed&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;Fixed seed for reproducible results (omit for random generation)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--offload&lt;/td&gt; 
   &lt;td align="center"&gt;True&lt;/td&gt; 
   &lt;td align="center"&gt;Offloads model components to CPU to reduce VRAM usage (recommended)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--use_usp&lt;/td&gt; 
   &lt;td align="center"&gt;True&lt;/td&gt; 
   &lt;td align="center"&gt;Enables multi-GPU acceleration with xDiT USP&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--outdir&lt;/td&gt; 
   &lt;td align="center"&gt;./video_out&lt;/td&gt; 
   &lt;td align="center"&gt;Directory where generated videos will be saved&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--prompt_enhancer&lt;/td&gt; 
   &lt;td align="center"&gt;True&lt;/td&gt; 
   &lt;td align="center"&gt;Expand the prompt into a more detailed description&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--teacache&lt;/td&gt; 
   &lt;td align="center"&gt;False&lt;/td&gt; 
   &lt;td align="center"&gt;Enables teacache for faster inference&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--teacache_thresh&lt;/td&gt; 
   &lt;td align="center"&gt;0.2&lt;/td&gt; 
   &lt;td align="center"&gt;Higher speedup will cause to worse quality&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--use_ret_steps&lt;/td&gt; 
   &lt;td align="center"&gt;False&lt;/td&gt; 
   &lt;td align="center"&gt;Retention Steps for teacache&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Diffusion Forcing Additional Parameters&lt;/strong&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;Parameter&lt;/th&gt; 
   &lt;th align="center"&gt;Recommended Value&lt;/th&gt; 
   &lt;th align="center"&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--ar_step&lt;/td&gt; 
   &lt;td align="center"&gt;0&lt;/td&gt; 
   &lt;td align="center"&gt;Controls asynchronous inference (0 for synchronous mode)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--base_num_frames&lt;/td&gt; 
   &lt;td align="center"&gt;97 or 121&lt;/td&gt; 
   &lt;td align="center"&gt;Base frame count (&lt;strong&gt;97 for 540P&lt;/strong&gt;, &lt;strong&gt;121 for 720P&lt;/strong&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--overlap_history&lt;/td&gt; 
   &lt;td align="center"&gt;17&lt;/td&gt; 
   &lt;td align="center"&gt;Number of frames to overlap for smooth transitions in long videos&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--addnoise_condition&lt;/td&gt; 
   &lt;td align="center"&gt;20&lt;/td&gt; 
   &lt;td align="center"&gt;Improves consistency in long video generation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--causal_block_size&lt;/td&gt; 
   &lt;td align="center"&gt;5&lt;/td&gt; 
   &lt;td align="center"&gt;Recommended when using asynchronous inference (--ar_step &amp;gt; 0)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--video_path&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;Path to input video for video extension&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;--end_image&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;Path to input image for end frame control&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h4&gt;Multi-GPU inference using xDiT USP&lt;/h4&gt; 
&lt;p&gt;We use &lt;a href="https://github.com/xdit-project/xDiT"&gt;xDiT&lt;/a&gt; USP to accelerate inference. For example, to generate a video with 2 GPUs, you can use the following command:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Diffusion Forcing&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;model_id=Skywork/SkyReels-V2-DF-14B-540P
# diffusion forcing synchronous inference
torchrun --nproc_per_node=2 generate_video_df.py \
  --model_id ${model_id} \
  --resolution 540P \
  --ar_step 0 \
  --base_num_frames 97 \
  --num_frames 257 \
  --overlap_history 17 \
  --prompt "A graceful white swan with a curved neck and delicate feathers swimming in a serene lake at dawn, its reflection perfectly mirrored in the still water as mist rises from the surface, with the swan occasionally dipping its head into the water to feed." \
  --addnoise_condition 20 \
  --use_usp \
  --offload \
  --seed 42
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Text To Video &amp;amp; Image To Video&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# run Text-to-Video Generation
model_id=Skywork/SkyReels-V2-T2V-14B-540P
torchrun --nproc_per_node=2 generate_video.py \
  --model_id ${model_id} \
  --resolution 540P \
  --num_frames 97 \
  --guidance_scale 6.0 \
  --shift 8.0 \
  --fps 24 \
  --offload \
  --prompt "A serene lake surrounded by towering mountains, with a few swans gracefully gliding across the water and sunlight dancing on the surface." \
  --use_usp \
  --seed 42
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;When using an &lt;strong&gt;image-to-video (I2V)&lt;/strong&gt; model, you must provide an input image using the &lt;code&gt;--image ${image_path}&lt;/code&gt; parameter. The &lt;code&gt;--guidance_scale 5.0&lt;/code&gt; and &lt;code&gt;--shift 3.0&lt;/code&gt; is recommended for I2V model.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/#abstract"&gt;Abstract&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/#methodology-of-skyreels-v2"&gt;Methodology of SkyReels-V2&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/#key-contributions-of-skyreels-v2"&gt;Key Contributions of SkyReels-V2&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/#video-captioner"&gt;Video Captioner&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/#reinforcement-learning"&gt;Reinforcement Learning&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/#diffusion-forcing"&gt;Diffusion Forcing&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/#high-quality-supervised-fine-tuning-sft"&gt;High-Quality Supervised Fine-Tuning(SFT)&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/#performance"&gt;Performance&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/#acknowledgements"&gt;Acknowledgements&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/#citation"&gt;Citation&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Abstract&lt;/h2&gt; 
&lt;p&gt;Recent advances in video generation have been driven by diffusion models and autoregressive frameworks, yet critical challenges persist in harmonizing prompt adherence, visual quality, motion dynamics, and duration: compromises in motion dynamics to enhance temporal visual quality, constrained video duration (5-10 seconds) to prioritize resolution, and inadequate shot-aware generation stemming from general-purpose MLLMs' inability to interpret cinematic grammar, such as shot composition, actor expressions, and camera motions. These intertwined limitations hinder realistic long-form synthesis and professional film-style generation.&lt;/p&gt; 
&lt;p&gt;To address these limitations, we introduce SkyReels-V2, the world's first infinite-length film generative model using a Diffusion Forcing framework. Our approach synergizes Multi-modal Large Language Models (MLLM), Multi-stage Pretraining, Reinforcement Learning, and Diffusion Forcing techniques to achieve comprehensive optimization. Beyond its technical innovations, SkyReels-V2 enables multiple practical applications, including Story Generation, Image-to-Video Synthesis, Camera Director functionality, and multi-subject consistent video generation through our &lt;a href="https://github.com/SkyworkAI/SkyReels-A2"&gt;Skyreels-A2&lt;/a&gt; system.&lt;/p&gt; 
&lt;h2&gt;Methodology of SkyReels-V2&lt;/h2&gt; 
&lt;p&gt;The SkyReels-V2 methodology consists of several interconnected components. It starts with a comprehensive data processing pipeline that prepares various quality training data. At its core is the Video Captioner architecture, which provides detailed annotations for video content. The system employs a multi-task pretraining strategy to build fundamental video generation capabilities. Post-training optimization includes Reinforcement Learning to enhance motion quality, Diffusion Forcing Training for generating extended videos, and High-quality Supervised Fine-Tuning (SFT) stages for visual refinement. The model runs on optimized computational infrastructure for efficient training and inference. SkyReels-V2 supports multiple applications, including Story Generation, Image-to-Video Synthesis, Camera Director functionality, and Elements-to-Video Generation.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/assets/main_pipeline.jpg" alt="mainpipeline" width="100%" /&gt; &lt;/p&gt; 
&lt;h2&gt;Key Contributions of SkyReels-V2&lt;/h2&gt; 
&lt;h4&gt;Video Captioner&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/Skywork/SkyCaptioner-V1"&gt;SkyCaptioner-V1&lt;/a&gt; serves as our video captioning model for data annotation. This model is trained on the captioning result from the base model &lt;a href="https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct"&gt;Qwen2.5-VL-72B-Instruct&lt;/a&gt; and the sub-expert captioners on a balanced video data. The balanced video data is a carefully curated dataset of approximately 2 million videos to ensure conceptual balance and annotation quality. Built upon the &lt;a href="https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct"&gt;Qwen2.5-VL-7B-Instruct&lt;/a&gt; foundation model, &lt;a href="https://huggingface.co/Skywork/SkyCaptioner-V1"&gt;SkyCaptioner-V1&lt;/a&gt; is fine-tuned to enhance performance in domain-specific video captioning tasks. To compare the performance with the SOTA models, we conducted a manual assessment of accuracy across different captioning fields using a test set of 1,000 samples. The proposed &lt;a href="https://huggingface.co/Skywork/SkyCaptioner-V1"&gt;SkyCaptioner-V1&lt;/a&gt; achieves the highest average accuracy among the baseline models, and show a dramatic result in the shot related fields&lt;/p&gt; 
&lt;p align="center"&gt; &lt;/p&gt;
&lt;table align="center"&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;model&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct"&gt;Qwen2.5-VL-7B-Ins.&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct"&gt;Qwen2.5-VL-72B-Ins.&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://huggingface.co/omni-research/Tarsier2-Recap-7b"&gt;Tarsier2-Recap-7b&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://huggingface.co/Skywork/SkyCaptioner-V1"&gt;SkyCaptioner-V1&lt;/a&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Avg accuracy&lt;/td&gt; 
   &lt;td&gt;51.4%&lt;/td&gt; 
   &lt;td&gt;58.7%&lt;/td&gt; 
   &lt;td&gt;49.4%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;76.3%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;shot type&lt;/td&gt; 
   &lt;td&gt;76.8%&lt;/td&gt; 
   &lt;td&gt;82.5%&lt;/td&gt; 
   &lt;td&gt;60.2%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;93.7%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;shot angle&lt;/td&gt; 
   &lt;td&gt;60.0%&lt;/td&gt; 
   &lt;td&gt;73.7%&lt;/td&gt; 
   &lt;td&gt;52.4%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;89.8%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;shot position&lt;/td&gt; 
   &lt;td&gt;28.4%&lt;/td&gt; 
   &lt;td&gt;32.7%&lt;/td&gt; 
   &lt;td&gt;23.6%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;83.1%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;camera motion&lt;/td&gt; 
   &lt;td&gt;62.0%&lt;/td&gt; 
   &lt;td&gt;61.2%&lt;/td&gt; 
   &lt;td&gt;45.3%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;85.3%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;expression&lt;/td&gt; 
   &lt;td&gt;43.6%&lt;/td&gt; 
   &lt;td&gt;51.5%&lt;/td&gt; 
   &lt;td&gt;54.3%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;68.8%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td colspan="5" style="text-align: center; border-bottom: 1px solid #ddd; padding: 8px;"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;TYPES_type&lt;/td&gt; 
   &lt;td&gt;43.5%&lt;/td&gt; 
   &lt;td&gt;49.7%&lt;/td&gt; 
   &lt;td&gt;47.6%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;82.5%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;TYPES_sub_type&lt;/td&gt; 
   &lt;td&gt;38.9%&lt;/td&gt; 
   &lt;td&gt;44.9%&lt;/td&gt; 
   &lt;td&gt;45.9%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;75.4%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;appearance&lt;/td&gt; 
   &lt;td&gt;40.9%&lt;/td&gt; 
   &lt;td&gt;52.0%&lt;/td&gt; 
   &lt;td&gt;45.6%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;59.3%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;action&lt;/td&gt; 
   &lt;td&gt;32.4%&lt;/td&gt; 
   &lt;td&gt;52.0%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;69.8%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;68.8%&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;position&lt;/td&gt; 
   &lt;td&gt;35.4%&lt;/td&gt; 
   &lt;td&gt;48.6%&lt;/td&gt; 
   &lt;td&gt;45.5%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;57.5%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;is_main_subject&lt;/td&gt; 
   &lt;td&gt;58.5%&lt;/td&gt; 
   &lt;td&gt;68.7%&lt;/td&gt; 
   &lt;td&gt;69.7%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;80.9%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;environment&lt;/td&gt; 
   &lt;td&gt;70.4%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;72.7%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;61.4%&lt;/td&gt; 
   &lt;td&gt;70.5%&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;lighting&lt;/td&gt; 
   &lt;td&gt;77.1%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;80.0%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;21.2%&lt;/td&gt; 
   &lt;td&gt;76.5%&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;/p&gt; 
&lt;h4&gt;Reinforcement Learning&lt;/h4&gt; 
&lt;p&gt;Inspired by the previous success in LLM, we propose to enhance the performance of the generative model by Reinforcement Learning. Specifically, we focus on the motion quality because we find that the main drawback of our generative model is:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;the generative model does not handle well with large, deformable motions.&lt;/li&gt; 
 &lt;li&gt;the generated videos may violate the physical law.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To avoid the degradation in other metrics, such as text alignment and video quality, we ensure the preference data pairs have comparable text alignment and video quality, while only the motion quality varies. This requirement poses greater challenges in obtaining preference annotations due to the inherently higher costs of human annotation. To address this challenge, we propose a semi-automatic pipeline that strategically combines automatically generated motion pairs and human annotation results. This hybrid approach not only enhances the data scale but also improves alignment with human preferences through curated quality control. Leveraging this enhanced dataset, we first train a specialized reward model to capture the generic motion quality differences between paired samples. This learned reward function subsequently guides the sample selection process for Direct Preference Optimization (DPO), enhancing the motion quality of the generative model.&lt;/p&gt; 
&lt;h4&gt;Diffusion Forcing&lt;/h4&gt; 
&lt;p&gt;We introduce the Diffusion Forcing Transformer to unlock our modelâ€™s ability to generate long videos. Diffusion Forcing is a training and sampling strategy where each token is assigned an independent noise level. This allows tokens to be denoised according to arbitrary, per-token schedules. Conceptually, this approach functions as a form of partial masking: a token with zero noise is fully unmasked, while complete noise fully masks it. Diffusion Forcing trains the model to "unmask" any combination of variably noised tokens, using the cleaner tokens as conditional information to guide the recovery of noisy ones. Building on this, our Diffusion Forcing Transformer can extend video generation indefinitely based on the last frames of the previous segment. Note that the synchronous full sequence diffusion is a special case of Diffusion Forcing, where all tokens share the same noise level. This relationship allows us to fine-tune the Diffusion Forcing Transformer from a full-sequence diffusion model.&lt;/p&gt; 
&lt;h4&gt;High-Quality Supervised Fine-Tuning (SFT)&lt;/h4&gt; 
&lt;p&gt;We implement two sequential high-quality supervised fine-tuning (SFT) stages at 540p and 720p resolutions respectively, with the initial SFT phase conducted immediately after pretraining but prior to reinforcement learning (RL) stage.This first-stage SFT serves as a conceptual equilibrium trainer, building upon the foundation modelâ€™s pretraining outcomes that utilized only fps24 video data, while strategically removing FPS embedding components to streamline thearchitecture. Trained with the high-quality concept-balanced samples, this phase establishes optimized initialization parameters for subsequent training processes. Following this, we execute a secondary high-resolution SFT at 720p after completing the diffusion forcing stage, incorporating identical loss formulations and the higher-quality concept-balanced datasets by the manually filter. This final refinement phase focuses on resolution increase such that the overall video quality will be further enhanced.&lt;/p&gt; 
&lt;h2&gt;Performance&lt;/h2&gt; 
&lt;p&gt;To comprehensively evaluate our proposed method, we construct the SkyReels-Bench for human assessment and leveraged the open-source &lt;a href="https://github.com/Vchitect/VBench"&gt;V-Bench&lt;/a&gt; for automated evaluation. This allows us to compare our model with the state-of-the-art (SOTA) baselines, including both open-source and proprietary models.&lt;/p&gt; 
&lt;h4&gt;Human Evaluation&lt;/h4&gt; 
&lt;p&gt;For human evaluation, we design SkyReels-Bench with 1,020 text prompts, systematically assessing three dimensions: Instruction Adherence, Motion Quality, Consistency and Visual Quality. This benchmark is designed to evaluate both text-to-video (T2V) and image-to-video (I2V) generation models, providing comprehensive assessment across different generation paradigms. To ensure fairness, all models were evaluated under default settings with consistent resolutions, and no post-generation filtering was applied.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Text To Video Models&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="center"&gt; &lt;/p&gt;
&lt;table align="center"&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model Name&lt;/th&gt; 
   &lt;th&gt;Average&lt;/th&gt; 
   &lt;th&gt;Instruction Adherence&lt;/th&gt; 
   &lt;th&gt;Consistency&lt;/th&gt; 
   &lt;th&gt;Visual Quality&lt;/th&gt; 
   &lt;th&gt;Motion Quality&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://runwayml.com/research/introducing-gen-3-alpha"&gt;Runway-Gen3 Alpha&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2.53&lt;/td&gt; 
   &lt;td&gt;2.19&lt;/td&gt; 
   &lt;td&gt;2.57&lt;/td&gt; 
   &lt;td&gt;3.23&lt;/td&gt; 
   &lt;td&gt;2.11&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Tencent/HunyuanVideo"&gt;HunyuanVideo-13B&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2.82&lt;/td&gt; 
   &lt;td&gt;2.64&lt;/td&gt; 
   &lt;td&gt;2.81&lt;/td&gt; 
   &lt;td&gt;3.20&lt;/td&gt; 
   &lt;td&gt;2.61&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://klingai.com"&gt;Kling-1.6 STD Mode&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2.99&lt;/td&gt; 
   &lt;td&gt;2.77&lt;/td&gt; 
   &lt;td&gt;3.05&lt;/td&gt; 
   &lt;td&gt;3.39&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;2.76&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://hailuoai.video"&gt;Hailuo-01&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;3.0&lt;/td&gt; 
   &lt;td&gt;2.8&lt;/td&gt; 
   &lt;td&gt;3.08&lt;/td&gt; 
   &lt;td&gt;3.29&lt;/td&gt; 
   &lt;td&gt;2.74&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Wan-Video/Wan2.1"&gt;Wan2.1-14B&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;3.12&lt;/td&gt; 
   &lt;td&gt;2.91&lt;/td&gt; 
   &lt;td&gt;3.31&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;3.54&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;2.71&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;SkyReels-V2&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;3.14&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;3.15&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;3.35&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;3.34&lt;/td&gt; 
   &lt;td&gt;2.74&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;/p&gt; 
&lt;p&gt;The evaluation demonstrates that our model achieves significant advancements in &lt;strong&gt;instruction adherence (3.15)&lt;/strong&gt; compared to baseline methods, while maintaining competitive performance in &lt;strong&gt;motion quality (2.74)&lt;/strong&gt; without sacrificing the &lt;strong&gt;consistency (3.35)&lt;/strong&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Image To Video Models&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="center"&gt; &lt;/p&gt;
&lt;table align="center"&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Average&lt;/th&gt; 
   &lt;th&gt;Instruction Adherence&lt;/th&gt; 
   &lt;th&gt;Consistency&lt;/th&gt; 
   &lt;th&gt;Visual Quality&lt;/th&gt; 
   &lt;th&gt;Motion Quality&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Tencent/HunyuanVideo"&gt;HunyuanVideo-13B&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2.84&lt;/td&gt; 
   &lt;td&gt;2.97&lt;/td&gt; 
   &lt;td&gt;2.95&lt;/td&gt; 
   &lt;td&gt;2.87&lt;/td&gt; 
   &lt;td&gt;2.56&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Wan-Video/Wan2.1"&gt;Wan2.1-14B&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2.85&lt;/td&gt; 
   &lt;td&gt;3.10&lt;/td&gt; 
   &lt;td&gt;2.81&lt;/td&gt; 
   &lt;td&gt;3.00&lt;/td&gt; 
   &lt;td&gt;2.48&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://hailuoai.video"&gt;Hailuo-01&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;3.05&lt;/td&gt; 
   &lt;td&gt;3.31&lt;/td&gt; 
   &lt;td&gt;2.58&lt;/td&gt; 
   &lt;td&gt;3.55&lt;/td&gt; 
   &lt;td&gt;2.74&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://klingai.com"&gt;Kling-1.6 Pro Mode&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;3.4&lt;/td&gt; 
   &lt;td&gt;3.56&lt;/td&gt; 
   &lt;td&gt;3.03&lt;/td&gt; 
   &lt;td&gt;3.58&lt;/td&gt; 
   &lt;td&gt;3.41&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://runwayml.com/research/introducing-runway-gen-4"&gt;Runway-Gen4&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;3.39&lt;/td&gt; 
   &lt;td&gt;3.75&lt;/td&gt; 
   &lt;td&gt;3.2&lt;/td&gt; 
   &lt;td&gt;3.4&lt;/td&gt; 
   &lt;td&gt;3.37&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;SkyReels-V2-DF&lt;/td&gt; 
   &lt;td&gt;3.24&lt;/td&gt; 
   &lt;td&gt;3.64&lt;/td&gt; 
   &lt;td&gt;3.21&lt;/td&gt; 
   &lt;td&gt;3.18&lt;/td&gt; 
   &lt;td&gt;2.93&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;SkyReels-V2-I2V&lt;/td&gt; 
   &lt;td&gt;3.29&lt;/td&gt; 
   &lt;td&gt;3.42&lt;/td&gt; 
   &lt;td&gt;3.18&lt;/td&gt; 
   &lt;td&gt;3.56&lt;/td&gt; 
   &lt;td&gt;3.01&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;/p&gt; 
&lt;p&gt;Our results demonstrate that both &lt;strong&gt;SkyReels-V2-I2V (3.29)&lt;/strong&gt; and &lt;strong&gt;SkyReels-V2-DF (3.24)&lt;/strong&gt; achieve state-of-the-art performance among open-source models, significantly outperforming HunyuanVideo-13B (2.84) and Wan2.1-14B (2.85) across all quality dimensions. With an average score of 3.29, SkyReels-V2-I2V demonstrates comparable performance to proprietary models Kling-1.6 (3.4) and Runway-Gen4 (3.39).&lt;/p&gt; 
&lt;h4&gt;VBench&lt;/h4&gt; 
&lt;p&gt;To objectively compare SkyReels-V2 Model against other leading open-source Text-To-Video models, we conduct comprehensive evaluations using the public benchmark &lt;a href="https://github.com/Vchitect/VBench"&gt;V-Bench&lt;/a&gt;. Our evaluation specifically leverages the benchmarkâ€™s longer version prompt. For fair comparison with baseline models, we strictly follow their recommended setting for inference.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;/p&gt;
&lt;table align="center"&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Total Score&lt;/th&gt; 
   &lt;th&gt;Quality Score&lt;/th&gt; 
   &lt;th&gt;Semantic Score&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/hpcaitech/Open-Sora"&gt;OpenSora 2.0&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;81.5 %&lt;/td&gt; 
   &lt;td&gt;82.1 %&lt;/td&gt; 
   &lt;td&gt;78.2 %&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/THUDM/CogVideo"&gt;CogVideoX1.5-5B&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;80.3 %&lt;/td&gt; 
   &lt;td&gt;80.9 %&lt;/td&gt; 
   &lt;td&gt;77.9 %&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Tencent/HunyuanVideo"&gt;HunyuanVideo-13B&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;82.7 %&lt;/td&gt; 
   &lt;td&gt;84.4 %&lt;/td&gt; 
   &lt;td&gt;76.2 %&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Wan-Video/Wan2.1"&gt;Wan2.1-14B&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;83.7 %&lt;/td&gt; 
   &lt;td&gt;84.2 %&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;81.4 %&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;SkyReels-V2&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;83.9 %&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;84.7 %&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;80.8 %&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;/p&gt; 
&lt;p&gt;The VBench results demonstrate that SkyReels-V2 outperforms all compared models including HunyuanVideo-13B and Wan2.1-14B, With the highest &lt;strong&gt;total score (83.9%)&lt;/strong&gt; and &lt;strong&gt;quality score (84.7%)&lt;/strong&gt;. In this evaluation, the semantic score is slightly lower than Wan2.1-14B, while we outperform Wan2.1-14B in human evaluations, with the primary gap attributed to V-Benchâ€™s insufficient evaluation of shot-scenario semantic adherence.&lt;/p&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;p&gt;We would like to thank the contributors of &lt;a href="https://github.com/Wan-Video/Wan2.1"&gt;Wan 2.1&lt;/a&gt;, &lt;a href="https://github.com/xdit-project/xDiT"&gt;XDit&lt;/a&gt; and &lt;a href="https://qwenlm.github.io/blog/qwen2.5/"&gt;Qwen 2.5&lt;/a&gt; repositories, for their open research and contributions.&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{chen2025skyreelsv2infinitelengthfilmgenerative,
      title={SkyReels-V2: Infinite-length Film Generative Model}, 
      author={Guibin Chen and Dixuan Lin and Jiangping Yang and Chunze Lin and Junchen Zhu and Mingyuan Fan and Hao Zhang and Sheng Chen and Zheng Chen and Chengcheng Ma and Weiming Xiong and Wei Wang and Nuo Pang and Kang Kang and Zhiheng Xu and Yuzhe Jin and Yupeng Liang and Yubing Song and Peng Zhao and Boyuan Xu and Di Qiu and Debang Li and Zhengcong Fei and Yang Li and Yahui Zhou},
      year={2025},
      eprint={2504.13074},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2504.13074}, 
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
  </channel>
</rss>