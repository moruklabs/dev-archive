<rss version="2.0">
  <channel>
    <title>GitHub Python Weekly Trending</title>
    <description>Weekly Trending of Python in GitHub</description>
    <pubDate>Mon, 15 Sep 2025 02:15:08 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>emcie-co/parlant</title>
      <link>https://github.com/emcie-co/parlant</link>
      <description>&lt;p&gt;LLM agents built for control. Designed for real-world use. Deployed in minutes.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://github.com/emcie-co/parlant/blob/develop/docs/LogoTransparentLight.png?raw=true" /&gt; 
  &lt;img alt="Parlant - AI Agent Framework" src="https://github.com/emcie-co/parlant/raw/develop/docs/LogoTransparentDark.png?raw=true" width="400" /&gt; 
 &lt;/picture&gt; 
 &lt;h3&gt;Finally, LLM agents that actually follow instructions&lt;/h3&gt; 
 &lt;p&gt; &lt;a href="https://www.parlant.io/" target="_blank"&gt;ğŸŒ Website&lt;/a&gt; â€¢ &lt;a href="https://www.parlant.io/docs/quickstart/installation" target="_blank"&gt;âš¡ Quick Start&lt;/a&gt; â€¢ &lt;a href="https://discord.gg/duxWqxKk6J" target="_blank"&gt;ğŸ’¬ Discord&lt;/a&gt; â€¢ &lt;a href="https://www.parlant.io/docs/quickstart/examples" target="_blank"&gt;ğŸ“– Examples&lt;/a&gt; &lt;/p&gt; 
 &lt;p&gt; 
  &lt;!-- Keep these links. Translations will automatically update with the README. --&gt; &lt;a href="https://zdoc.app/de/emcie-co/parlant"&gt;Deutsch&lt;/a&gt; | &lt;a href="https://zdoc.app/es/emcie-co/parlant"&gt;EspaÃ±ol&lt;/a&gt; | &lt;a href="https://zdoc.app/fr/emcie-co/parlant"&gt;franÃ§ais&lt;/a&gt; | &lt;a href="https://zdoc.app/ja/emcie-co/parlant"&gt;æ—¥æœ¬èª&lt;/a&gt; | &lt;a href="https://zdoc.app/ko/emcie-co/parlant"&gt;í•œêµ­ì–´&lt;/a&gt; | &lt;a href="https://zdoc.app/pt/emcie-co/parlant"&gt;PortuguÃªs&lt;/a&gt; | &lt;a href="https://zdoc.app/ru/emcie-co/parlant"&gt;Ğ ÑƒÑÑĞºĞ¸Ğ¹&lt;/a&gt; | &lt;a href="https://zdoc.app/zh/emcie-co/parlant"&gt;ä¸­æ–‡&lt;/a&gt; &lt;/p&gt; 
 &lt;p&gt; &lt;a href="https://pypi.org/project/parlant/"&gt;&lt;img alt="PyPI" src="https://img.shields.io/pypi/v/parlant?color=blue" /&gt;&lt;/a&gt; &lt;img alt="Python 3.10+" src="https://img.shields.io/badge/python-3.10+-blue" /&gt; &lt;a href="https://opensource.org/licenses/Apache-2.0"&gt;&lt;img alt="License" src="https://img.shields.io/badge/license-Apache%202.0-green" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/duxWqxKk6J"&gt;&lt;img alt="Discord" src="https://img.shields.io/discord/1312378700993663007?color=7289da&amp;amp;logo=discord&amp;amp;logoColor=white" /&gt;&lt;/a&gt; &lt;img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/emcie-co/parlant?style=social" /&gt; &lt;/p&gt; 
 &lt;a href="https://trendshift.io/repositories/12768" target="_blank"&gt; &lt;img src="https://trendshift.io/api/badge/repositories/12768" alt="Trending on TrendShift" style="width: 250px; height: 55px;" width="250" height="55" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸ¯ The Problem Every AI Developer Faces&lt;/h2&gt; 
&lt;p&gt;You build an AI agent. It works great in testing. Then real users start talking to it and...&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;âŒ It ignores your carefully crafted system prompts&lt;/li&gt; 
 &lt;li&gt;âŒ It hallucinates responses in critical moments&lt;/li&gt; 
 &lt;li&gt;âŒ It can't handle edge cases consistently&lt;/li&gt; 
 &lt;li&gt;âŒ Each conversation feels like a roll of the dice&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Sound familiar?&lt;/strong&gt; You're not alone. This is the #1 pain point for developers building production AI agents.&lt;/p&gt; 
&lt;h2&gt;âš¡ The Solution: Stop Fighting Prompts, Teach Principles&lt;/h2&gt; 
&lt;p&gt;Parlant flips the script on AI agent development. Instead of hoping your LLM will follow instructions, &lt;strong&gt;Parlant ensures it&lt;/strong&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Traditional approach: Cross your fingers ğŸ¤
system_prompt = "You are a helpful assistant. Please follow these 47 rules..."

# Parlant approach: Ensured compliance âœ…
await agent.create_guideline(
    condition="Customer asks about refunds",
    action="Check order status first to see if eligible",
    tools=[check_order_status],
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href="https://www.parlant.io/blog/how-parlant-guarantees-compliance"&gt;âœ… Blog: How Parlant Ensures Agent Compliance&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;Parlant gives you all the structure you need to build customer-facing agents that behave exactly as your business requires:&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://parlant.io/docs/concepts/customization/journeys"&gt;Journeys&lt;/a&gt;&lt;/strong&gt;: Define clear customer journeys and how your agent should respond at each step.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://parlant.io/docs/concepts/customization/guidelines"&gt;Behavioral Guidelines&lt;/a&gt;&lt;/strong&gt;: Easily craft agent behavior; Parlant will match the relevant elements contextually.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://parlant.io/docs/concepts/customization/tools"&gt;Tool Use&lt;/a&gt;&lt;/strong&gt;: Attach external APIs, data fetchers, or backend services to specific interaction events.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://parlant.io/docs/concepts/customization/glossary"&gt;Domain Adaptation&lt;/a&gt;&lt;/strong&gt;: Teach your agent domain-specific terminology and craft personalized responses.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://parlant.io/docs/concepts/customization/canned-responses"&gt;Canned Responses&lt;/a&gt;&lt;/strong&gt;: Use response templates to eliminate hallucinations and guarantee style consistency.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://parlant.io/docs/advanced/explainability"&gt;Explainability&lt;/a&gt;&lt;/strong&gt;: Understand why and when each guideline was matched and followed.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align="center"&gt; 
 &lt;h2&gt;ğŸš€ Get Your Agent Running in 60 Seconds&lt;/h2&gt; 
&lt;/div&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install parlant
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import parlant.sdk as p

@p.tool
async def get_weather(context: p.ToolContext, city: str) -&amp;gt; p.ToolResult:
    # Your weather API logic here
    return p.ToolResult(f"Sunny, 72Â°F in {city}")

@p.tool
async def get_datetime(context: p.ToolContext) -&amp;gt; p.ToolResult:
    from datetime import datetime
    return p.ToolResult(datetime.now())

async def main():
    async with p.Server() as server:
        agent = await server.create_agent(
            name="WeatherBot",
            description="Helpful weather assistant"
        )

        # Have the agent's context be updated on every response (though
        # update interval is customizable) using a context variable.
        await agent.create_variable(name="current-datetime", tool=get_datetime)

        # Control and guide agent behavior with natural language
        await agent.create_guideline(
            condition="User asks about weather",
            action="Get current weather and provide a friendly response with suggestions",
            tools=[get_weather]
        )

        # Add other (reliably enforced) behavioral modeling elements
        # ...

        # ğŸ‰ Test playground ready at http://localhost:8800
        # Integrate the official React widget into your app,
        # or follow the tutorial to build your own frontend!

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;That's it!&lt;/strong&gt; Your agent is running with ensured rule-following behavior.&lt;/p&gt; 
&lt;h2&gt;ğŸ¬ See It In Action&lt;/h2&gt; 
&lt;img alt="Parlant Demo" src="https://github.com/emcie-co/parlant/raw/develop/docs/demo.gif?raw=true" width="100%" /&gt; 
&lt;h2&gt;ğŸ”¥ Why Developers Are Switching to Parlant&lt;/h2&gt; 
&lt;table width="100%"&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;ğŸ—ï¸ &lt;strong&gt;Traditional AI Frameworks&lt;/strong&gt;&lt;/h3&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;âš¡ &lt;strong&gt;Parlant&lt;/strong&gt;&lt;/h3&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%"&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Write complex system prompts&lt;/li&gt; 
     &lt;li&gt;Hope the LLM follows them&lt;/li&gt; 
     &lt;li&gt;Debug unpredictable behaviors&lt;/li&gt; 
     &lt;li&gt;Scale by prompt engineering&lt;/li&gt; 
     &lt;li&gt;Cross fingers for reliability&lt;/li&gt; 
    &lt;/ul&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Define rules in natural language&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Ensured&lt;/strong&gt; rule compliance&lt;/li&gt; 
     &lt;li&gt;Predictable, consistent behavior&lt;/li&gt; 
     &lt;li&gt;Scale by adding guidelines&lt;/li&gt; 
     &lt;li&gt;Production-ready from day one&lt;/li&gt; 
    &lt;/ul&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;ğŸ¯ Perfect For Your Use Case&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;Financial Services&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;Healthcare&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;E-commerce&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;Legal Tech&lt;/strong&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;Compliance-first design&lt;/td&gt; 
    &lt;td align="center"&gt;HIPAA-ready agents&lt;/td&gt; 
    &lt;td align="center"&gt;Customer service at scale&lt;/td&gt; 
    &lt;td align="center"&gt;Precise legal guidance&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;Built-in risk management&lt;/td&gt; 
    &lt;td align="center"&gt;Patient data protection&lt;/td&gt; 
    &lt;td align="center"&gt;Order processing automation&lt;/td&gt; 
    &lt;td align="center"&gt;Document review assistance&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸ› ï¸ Enterprise-Grade Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ§­ Conversational Journeys&lt;/strong&gt; - Lead the customer step-by-step to a goal&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ¯ Dynamic Guideline Matching&lt;/strong&gt; - Context-aware rule application&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ”§ Reliable Tool Integration&lt;/strong&gt; - APIs, databases, external services&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ“Š Conversation Analytics&lt;/strong&gt; - Deep insights into agent behavior&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ”„ Iterative Refinement&lt;/strong&gt; - Continuously improve agent responses&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ›¡ï¸ Built-in Guardrails&lt;/strong&gt; - Prevent hallucination and off-topic responses&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ“± React Widget&lt;/strong&gt; - &lt;a href="https://github.com/emcie-co/parlant-chat-react"&gt;Drop-in chat UI for any web app&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ” Full Explainability&lt;/strong&gt; - Understand every decision your agent makes&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ“ˆ Join 10,000+ Developers Building Better AI&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;strong&gt;Companies using Parlant:&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;Financial institutions â€¢ Healthcare providers â€¢ Legal firms â€¢ E-commerce platforms&lt;/em&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://star-history.com/#emcie-co/parlant&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=emcie-co/parlant&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸŒŸ What Developers Are Saying&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;em&gt;"By far the most elegant conversational AI framework that I've come across! Developing with Parlant is pure joy."&lt;/em&gt; &lt;strong&gt;â€” Vishal Ahuja, Senior Lead, Customer-Facing Conversational AI @ JPMorgan Chase&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;ğŸƒâ€â™‚ï¸ Quick Start Paths&lt;/h2&gt; 
&lt;table border="0"&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ğŸ¯ I want to test it myself&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.parlant.io/docs/quickstart/installation"&gt;â†’ 5-minute quickstart&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ğŸ› ï¸ I want to see an example&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.parlant.io/docs/quickstart/examples"&gt;â†’ Healthcare agent example&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ğŸš€ I want to get involved&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://discord.gg/duxWqxKk6J"&gt;â†’ Join our Discord community&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;ğŸ¤ Community &amp;amp; Support&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ’¬ &lt;strong&gt;&lt;a href="https://discord.gg/duxWqxKk6J"&gt;Discord Community&lt;/a&gt;&lt;/strong&gt; - Get help from the team and community&lt;/li&gt; 
 &lt;li&gt;ğŸ“– &lt;strong&gt;&lt;a href="https://parlant.io/docs/quickstart/installation"&gt;Documentation&lt;/a&gt;&lt;/strong&gt; - Comprehensive guides and examples&lt;/li&gt; 
 &lt;li&gt;ğŸ› &lt;strong&gt;&lt;a href="https://github.com/emcie-co/parlant/issues"&gt;GitHub Issues&lt;/a&gt;&lt;/strong&gt; - Bug reports and feature requests&lt;/li&gt; 
 &lt;li&gt;ğŸ“§ &lt;strong&gt;&lt;a href="https://parlant.io/contact"&gt;Direct Support&lt;/a&gt;&lt;/strong&gt; - Direct line to our engineering team&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ“„ License&lt;/h2&gt; 
&lt;p&gt;Apache 2.0 - Use it anywhere, including commercial projects.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;strong&gt;Ready to build AI agents that actually work?&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;â­ &lt;strong&gt;Star this repo&lt;/strong&gt; â€¢ ğŸš€ &lt;strong&gt;&lt;a href="https://parlant.io/"&gt;Try Parlant now&lt;/a&gt;&lt;/strong&gt; â€¢ ğŸ’¬ &lt;strong&gt;&lt;a href="https://discord.gg/duxWqxKk6J"&gt;Join Discord&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;Built with â¤ï¸ by the team at &lt;a href="https://emcie.co"&gt;Emcie&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>skyzh/tiny-llm</title>
      <link>https://github.com/skyzh/tiny-llm</link>
      <description>&lt;p&gt;A course of learning LLM inference serving on Apple Silicon for systems engineers: build a tiny vLLM + Qwen.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;tiny-llm - LLM Serving in a Week&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/skyzh/tiny-llm/actions/workflows/main.yml"&gt;&lt;img src="https://github.com/skyzh/tiny-llm/actions/workflows/main.yml/badge.svg?sanitize=true" alt="CI (main)" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;A course on LLM serving using MLX for system engineers. The codebase is solely (almost!) based on MLX array/matrix APIs without any high-level neural network APIs, so that we can build the model serving infrastructure from scratch and dig into the optimizations.&lt;/p&gt; 
&lt;p&gt;The goal is to learn the techniques behind efficiently serving a large language model (e.g., Qwen2 models).&lt;/p&gt; 
&lt;p&gt;In week 1, you will implement the necessary components in Python (only Python!) to use the Qwen2 model to generate responses (e.g., attention, RoPE, etc). In week 2, you will implement the inference system which is similar to but a much simpler version of vLLM (e.g., KV cache, continuous batching, flash attention, etc). In week 3, we will cover more advanced topics and how the model interacts with the outside world.&lt;/p&gt; 
&lt;p&gt;Why MLX: nowadays it's easier to get a macOS-based local development environment than setting up an NVIDIA GPU.&lt;/p&gt; 
&lt;p&gt;Why Qwen2: this was the first LLM I've interacted with -- it's the go-to example in the vllm documentation. I spent some time looking at the vllm source code and built some knowledge around it.&lt;/p&gt; 
&lt;h2&gt;Book&lt;/h2&gt; 
&lt;p&gt;The tiny-llm book is available at &lt;a href="https://skyzh.github.io/tiny-llm/"&gt;https://skyzh.github.io/tiny-llm/&lt;/a&gt;. You can follow the guide and start building.&lt;/p&gt; 
&lt;h2&gt;Community&lt;/h2&gt; 
&lt;p&gt;You may join skyzh's Discord server and study with the tiny-llm community.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://skyzh.dev/join/discord"&gt;&lt;img src="https://raw.githubusercontent.com/skyzh/tiny-llm/main/book/src/discord-badge.svg?sanitize=true" alt="Join skyzh's Discord Server" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Roadmap&lt;/h2&gt; 
&lt;p&gt;Week 1 is complete. Week 2 is in progress.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Week + Chapter&lt;/th&gt; 
   &lt;th&gt;Topic&lt;/th&gt; 
   &lt;th&gt;Code&lt;/th&gt; 
   &lt;th&gt;Test&lt;/th&gt; 
   &lt;th&gt;Doc&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;1.1&lt;/td&gt; 
   &lt;td&gt;Attention&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;1.2&lt;/td&gt; 
   &lt;td&gt;RoPE&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;1.3&lt;/td&gt; 
   &lt;td&gt;Grouped Query Attention&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;1.4&lt;/td&gt; 
   &lt;td&gt;RMSNorm and MLP&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;1.5&lt;/td&gt; 
   &lt;td&gt;Load the Model&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;1.6&lt;/td&gt; 
   &lt;td&gt;Generate Responses (aka Decoding)&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;1.7&lt;/td&gt; 
   &lt;td&gt;Sampling&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;2.1&lt;/td&gt; 
   &lt;td&gt;Key-Value Cache&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;2.2&lt;/td&gt; 
   &lt;td&gt;Quantized Matmul and Linear - CPU&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;ğŸš§&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;2.3&lt;/td&gt; 
   &lt;td&gt;Quantized Matmul and Linear - GPU&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;ğŸš§&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;2.4&lt;/td&gt; 
   &lt;td&gt;Flash Attention 2 - CPU&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;ğŸš§&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;2.5&lt;/td&gt; 
   &lt;td&gt;Flash Attention 2 - GPU&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;ğŸš§&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;2.6&lt;/td&gt; 
   &lt;td&gt;Continuous Batching&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;ğŸš§&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;2.7&lt;/td&gt; 
   &lt;td&gt;Chunked Prefill&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;ğŸš§&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;3.1&lt;/td&gt; 
   &lt;td&gt;Paged Attention - Part 1&lt;/td&gt; 
   &lt;td&gt;ğŸš§&lt;/td&gt; 
   &lt;td&gt;ğŸš§&lt;/td&gt; 
   &lt;td&gt;ğŸš§&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;3.2&lt;/td&gt; 
   &lt;td&gt;Paged Attention - Part 2&lt;/td&gt; 
   &lt;td&gt;ğŸš§&lt;/td&gt; 
   &lt;td&gt;ğŸš§&lt;/td&gt; 
   &lt;td&gt;ğŸš§&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;3.3&lt;/td&gt; 
   &lt;td&gt;MoE (Mixture of Experts)&lt;/td&gt; 
   &lt;td&gt;ğŸš§&lt;/td&gt; 
   &lt;td&gt;ğŸš§&lt;/td&gt; 
   &lt;td&gt;ğŸš§&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;3.4&lt;/td&gt; 
   &lt;td&gt;Speculative Decoding&lt;/td&gt; 
   &lt;td&gt;ğŸš§&lt;/td&gt; 
   &lt;td&gt;ğŸš§&lt;/td&gt; 
   &lt;td&gt;ğŸš§&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;3.5&lt;/td&gt; 
   &lt;td&gt;RAG Pipeline&lt;/td&gt; 
   &lt;td&gt;ğŸš§&lt;/td&gt; 
   &lt;td&gt;ğŸš§&lt;/td&gt; 
   &lt;td&gt;ğŸš§&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;3.6&lt;/td&gt; 
   &lt;td&gt;AI Agent / Tool Calling&lt;/td&gt; 
   &lt;td&gt;ğŸš§&lt;/td&gt; 
   &lt;td&gt;ğŸš§&lt;/td&gt; 
   &lt;td&gt;ğŸš§&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;3.7&lt;/td&gt; 
   &lt;td&gt;Long Context&lt;/td&gt; 
   &lt;td&gt;ğŸš§&lt;/td&gt; 
   &lt;td&gt;ğŸš§&lt;/td&gt; 
   &lt;td&gt;ğŸš§&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Other topics not covered: quantized/compressed kv cache, prefix/prompt cache; sampling, fine tuning; smaller kernels (softmax, silu, etc)&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>OpenPipe/ART</title>
      <link>https://github.com/OpenPipe/ART</link>
      <description>&lt;p&gt;Agent Reinforcement Trainer: train multi-step agents for real-world tasks using GRPO. Give your agents on-the-job training. Reinforcement learning for Qwen2.5, Qwen3, Llama, and more!&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://art.openpipe.ai"&gt;
   &lt;picture&gt; 
    &lt;img alt="ART logo" src="https://github.com/openpipe/art/raw/main/assets/ART_logo.png" width="160px" /&gt; 
   &lt;/picture&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;/p&gt;
 &lt;h1&gt;Agent Reinforcement Trainer&lt;/h1&gt; 
 &lt;p&gt;&lt;/p&gt; 
 &lt;p&gt; Train multi-step agents for real-world tasks using GRPO. &lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/openpipe/art/raw/main/CONTRIBUTING.md"&gt;&lt;img src="https://img.shields.io/badge/PRs-welcome-blue.svg?sanitize=true" alt="PRs-Welcome" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/openpipe-art/"&gt;&lt;img src="https://img.shields.io/pypi/v/openpipe-art?color=364fc7" alt="PyPI version" /&gt;&lt;/a&gt; &lt;a href="https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/2048/2048.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Train Agent" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://discord.gg/zbBHRUpwf4"&gt;&lt;img src="https://img.shields.io/badge/Join%20Discord-5865F2?style=plastic&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Join Discord" /&gt;&lt;/a&gt; &lt;a href="https://art.openpipe.ai"&gt;&lt;img src="https://img.shields.io/badge/Documentation-orange?style=plastic&amp;amp;logo=gitbook&amp;amp;logoColor=white" alt="Documentation" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸ“ RULER: Zero-Shot Agent Rewards&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;RULER&lt;/strong&gt; (Relative Universal LLM-Elicited Rewards) eliminates the need for hand-crafted reward functions by using an LLM-as-judge to automatically score agent trajectories. Simply define your task in the system prompt, and RULER handles the restâ€”&lt;strong&gt;no labeled data, expert feedback, or reward engineering required&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;âœ¨ &lt;strong&gt;Key Benefits:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;2-3x faster development&lt;/strong&gt; - Skip reward function engineering entirely&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;General-purpose&lt;/strong&gt; - Works across any task without modification&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Strong performance&lt;/strong&gt; - Matches or exceeds hand-crafted rewards in 3/4 benchmarks&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Easy integration&lt;/strong&gt; - Drop-in replacement for manual reward functions&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Before: Hours of reward engineering
def complex_reward_function(trajectory):
    # 50+ lines of careful scoring logic...
    pass

# After: One line with RULER
judged_group = await ruler_score_group(group, "openai/o3")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href="https://art.openpipe.ai/fundamentals/ruler"&gt;ğŸ“– Learn more about RULER â†’&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;ART Overview&lt;/h2&gt; 
&lt;p&gt;ART is an open-source RL framework that improves agent reliability by allowing LLMs to &lt;strong&gt;learn from experience&lt;/strong&gt;. ART provides an ergonomic harness for integrating GRPO into any python application. For a quick hands-on introduction, run one of the notebooks below. When you're ready to learn more, check out the &lt;a href="https://art.openpipe.ai"&gt;docs&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;ğŸ“’ Notebooks&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Agent Task&lt;/th&gt; 
   &lt;th&gt;Example Notebook&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Comparative Performance&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ARTâ€¢E LangGraph&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/langgraph/art-e-langgraph.ipynb"&gt;ğŸ‹ï¸ Train agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Qwen 2.5 7B learns to search emails using LangGraph&lt;/td&gt; 
   &lt;td&gt;[Link coming soon]&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;MCPâ€¢RL&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/mcp-rl/mcp-rl.ipynb"&gt;ğŸ‹ï¸ Train agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Qwen 2.5 3B masters the NWS MCP server&lt;/td&gt; 
   &lt;td&gt;[Link coming soon]&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ARTâ€¢E [RULER]&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/art-e.ipynb"&gt;ğŸ‹ï¸ Train agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Qwen 2.5 7B learns to search emails using RULER&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/openpipe/art/raw/main/assets/benchmarks/email_agent/accuracy-training-progress.svg?sanitize=true" height="72" /&gt; &lt;a href="https://raw.githubusercontent.com/OpenPipe/ART/main/dev/art-e/art_e/evaluate/display_benchmarks.ipynb"&gt;benchmarks&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;2048&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/2048/2048.ipynb"&gt;ğŸ‹ï¸ Train agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Qwen 2.5 3B learns to play 2048&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/openpipe/art/raw/main/assets/benchmarks/2048/accuracy-training-progress.svg?sanitize=true" height="72" /&gt; &lt;a href="https://raw.githubusercontent.com/OpenPipe/ART/main/examples/2048/display_benchmarks.ipynb"&gt;benchmarks&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Temporal Clue&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/temporal_clue/temporal-clue.ipynb"&gt;ğŸ‹ï¸ Train agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Qwen 2.5 7B learns to solve Temporal Clue&lt;/td&gt; 
   &lt;td&gt;[Link coming soon]&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Tic Tac Toe&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/tic_tac_toe/tic-tac-toe.ipynb"&gt;ğŸ‹ï¸ Train agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Qwen 2.5 3B learns to play Tic Tac Toe&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/openpipe/art/raw/main/assets/benchmarks/tic-tac-toe-local/accuracy-training-progress.svg?sanitize=true" height="72" /&gt; &lt;a href="https://raw.githubusercontent.com/OpenPipe/ART/main/examples/tic_tac_toe/display-benchmarks.ipynb"&gt;benchmarks&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Codenames&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/codenames/Codenames_RL.ipynb"&gt;ğŸ‹ï¸ Train agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Qwen 2.5 3B learns to play Codenames&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/openpipe/art/raw/main/assets/benchmarks/codenames/win_rate_over_time.png" height="72" /&gt; &lt;a href="https://github.com/OpenPipe/art-notebooks/raw/main/examples/codenames/Codenames_RL.ipynb"&gt;benchmarks&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;AutoRL [RULER]&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/auto_rl.ipynb"&gt;ğŸ‹ï¸ Train agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Train Qwen 2.5 7B to master any task&lt;/td&gt; 
   &lt;td&gt;[Link coming soon]&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;ğŸ“° ART News&lt;/h2&gt; 
&lt;p&gt;Explore our latest research and updates on building SOTA agents.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ—ï¸ &lt;strong&gt;&lt;a href="https://art.openpipe.ai/integrations/langgraph-integration"&gt;ART now integrates seamlessly with LangGraph&lt;/a&gt;&lt;/strong&gt; - Train your LangGraph agents with reinforcement learning for smarter multi-step reasoning and improved tool usage.&lt;/li&gt; 
 &lt;li&gt;ğŸ—ï¸ &lt;strong&gt;&lt;a href="https://x.com/corbtt/status/1953171838382817625"&gt;MCPâ€¢RL: Teach Your Model to Master Any MCP Server&lt;/a&gt;&lt;/strong&gt; - Automatically train models to effectively use MCP server tools through reinforcement learning.&lt;/li&gt; 
 &lt;li&gt;ğŸ—ï¸ &lt;strong&gt;&lt;a href="https://x.com/mattshumer_/status/1950572449025650733"&gt;AutoRL: Zero-Data Training for Any Task&lt;/a&gt;&lt;/strong&gt; - Train custom AI models without labeled data using automatic input generation and RULER evaluation.&lt;/li&gt; 
 &lt;li&gt;ğŸ—ï¸ &lt;strong&gt;&lt;a href="https://openpipe.ai/blog/ruler-easy-mode-for-rl-rewards"&gt;RULER: Easy Mode for RL Rewards&lt;/a&gt;&lt;/strong&gt; is now available for automatic reward generation in reinforcement learning.&lt;/li&gt; 
 &lt;li&gt;ğŸ—ï¸ &lt;strong&gt;&lt;a href="https://openpipe.ai/blog/art-e-mail-agent"&gt;ARTÂ·E: How We Built an Email Research Agent That Beats o3&lt;/a&gt;&lt;/strong&gt; demonstrates a Qwen 2.5 14B email agent outperforming OpenAI's o3.&lt;/li&gt; 
 &lt;li&gt;ğŸ—ï¸ &lt;strong&gt;&lt;a href="https://openpipe.ai/blog/art-trainer"&gt;ART Trainer: A New RL Trainer for Agents&lt;/a&gt;&lt;/strong&gt; enables easy training of LLM-based agents using GRPO.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://openpipe.ai/blog"&gt;ğŸ“– See all blog posts â†’&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Why ART?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ART provides convenient wrappers for introducing RL training into &lt;strong&gt;existing applications&lt;/strong&gt;. We abstract the training server into a modular service that your code doesn't need to interface with.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Train from anywhere.&lt;/strong&gt; Run the ART client on your laptop and let the ART server kick off an ephemeral GPU-enabled environment, or run on a local GPU.&lt;/li&gt; 
 &lt;li&gt;Integrations with hosted platforms like W&amp;amp;B, Langfuse, and OpenPipe provide flexible observability and &lt;strong&gt;simplify debugging&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;ART is customizable with &lt;strong&gt;intelligent defaults&lt;/strong&gt;. You can configure training parameters and inference engine configurations to meet specific needs, or take advantage of the defaults, which have been optimized for training efficiency and stability.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;ART agents can be trained from any client machine that runs python. To add to an existing project, run this command:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install openpipe-art
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ğŸ¤– ARTâ€¢E Agent&lt;/h2&gt; 
&lt;p&gt;Curious about how to use ART for a real-world task? Check out the &lt;a href="https://openpipe.ai/blog/art-e-mail-agent"&gt;ARTâ€¢E Agent&lt;/a&gt; blog post, where we detail how we trained Qwen 2.5 14B to beat o3 at email retrieval!&lt;/p&gt; 
&lt;img src="https://github.com/openpipe/art/raw/main/assets/ART_E_graphs.png" width="700" /&gt; 
&lt;h2&gt;ğŸ” Training Loop Overview&lt;/h2&gt; 
&lt;p&gt;ART's functionality is divided into a &lt;strong&gt;client&lt;/strong&gt; and a &lt;strong&gt;server&lt;/strong&gt;. The OpenAI-compatible client is responsible for interfacing between ART and your codebase. Using the client, you can pass messages and get completions from your LLM as it improves. The server runs independently on any machine with a GPU. It abstracts away the complexity of the inference and training portions of the RL loop while allowing for some custom configuration. An outline of the training loop is shown below:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Inference&lt;/strong&gt;&lt;/p&gt; 
  &lt;ol&gt; 
   &lt;li&gt;Your code uses the ART client to perform an agentic workflow (usually executing several rollouts in parallel to gather data faster).&lt;/li&gt; 
   &lt;li&gt;Completion requests are routed to the ART server, which runs the model's latest LoRA in vLLM.&lt;/li&gt; 
   &lt;li&gt;As the agent executes, each &lt;code&gt;system&lt;/code&gt;, &lt;code&gt;user&lt;/code&gt;, and &lt;code&gt;assistant&lt;/code&gt; message is stored in a Trajectory.&lt;/li&gt; 
   &lt;li&gt;When a rollout finishes, your code assigns a &lt;code&gt;reward&lt;/code&gt; to its Trajectory, indicating the performance of the LLM.&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Training&lt;/strong&gt;&lt;/p&gt; 
  &lt;ol&gt; 
   &lt;li&gt;When each rollout has finished, Trajectories are grouped and sent to the server. Inference is blocked while training executes.&lt;/li&gt; 
   &lt;li&gt;The server trains your model using GRPO, initializing from the latest checkpoint (or an empty LoRA on the first iteration).&lt;/li&gt; 
   &lt;li&gt;The server saves the newly trained LoRA to a local directory and loads it into vLLM.&lt;/li&gt; 
   &lt;li&gt;Inference is unblocked and the loop resumes at step 1.&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;This training loop runs until a specified number of inference and training iterations have completed.&lt;/p&gt; 
&lt;h2&gt;ğŸ§© Supported Models&lt;/h2&gt; 
&lt;p&gt;ART should work with most vLLM/HuggingFace-transformers compatible causal language models, or at least the ones supported by &lt;a href="https://docs.unsloth.ai/get-started/all-our-models"&gt;Unsloth&lt;/a&gt;. Gemma 3 does not appear to be supported for the time being. If any other model isn't working for you, please let us know on &lt;a href="https://discord.gg/zbBHRUpwf4"&gt;Discord&lt;/a&gt; or open an issue on &lt;a href="https://github.com/openpipe/art/issues"&gt;GitHub&lt;/a&gt;!&lt;/p&gt; 
&lt;h2&gt;ğŸ¤ Contributing&lt;/h2&gt; 
&lt;p&gt;ART is in active development, and contributions are most welcome! Please see the &lt;a href="https://raw.githubusercontent.com/OpenPipe/ART/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; file for more information.&lt;/p&gt; 
&lt;h2&gt;ğŸ“– Citation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{hilton2025art,
  author = {Brad Hilton and Kyle Corbitt and David Corbitt and Saumya Gandhi and Angky William and Bohdan Kovalenskyi and Andie Jones},
  title = {ART: Agent Reinforcement Trainer},
  year = {2025},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/openpipe/art}}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;âš–ï¸ License&lt;/h2&gt; 
&lt;p&gt;This repository's source code is available under the &lt;a href="https://raw.githubusercontent.com/OpenPipe/ART/main/LICENSE"&gt;Apache-2.0 License&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;ğŸ™ Credits&lt;/h2&gt; 
&lt;p&gt;ART stands on the shoulders of giants. While we owe many of the ideas and early experiments that led to ART's development to the open source RL community at large, we're especially grateful to the authors of the following projects:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/unslothai/unsloth"&gt;Unsloth&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/vllm-project/vllm"&gt;vLLM&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/huggingface/trl"&gt;trl&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pytorch/torchtune"&gt;torchtune&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/skypilot-org/skypilot"&gt;SkyPilot&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Finally, thank you to our partners who've helped us test ART in the wild! We're excited to see what you all build with it.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>NVIDIA/garak</title>
      <link>https://github.com/NVIDIA/garak</link>
      <description>&lt;p&gt;the LLM vulnerability scanner&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;garak, LLM vulnerability scanner&lt;/h1&gt; 
&lt;p&gt;&lt;em&gt;Generative AI Red-teaming &amp;amp; Assessment Kit&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;garak&lt;/code&gt; checks if an LLM can be made to fail in a way we don't want. &lt;code&gt;garak&lt;/code&gt; probes for hallucination, data leakage, prompt injection, misinformation, toxicity generation, jailbreaks, and many other weaknesses. If you know &lt;code&gt;nmap&lt;/code&gt; or &lt;code&gt;msf&lt;/code&gt; / Metasploit Framework, garak does somewhat similar things to them, but for LLMs.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;garak&lt;/code&gt; focuses on ways of making an LLM or dialog system fail. It combines static, dynamic, and adaptive probes to explore this.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;garak&lt;/code&gt;'s a free tool. We love developing it and are always interested in adding functionality to support applications.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://opensource.org/licenses/Apache-2.0"&gt;&lt;img src="https://img.shields.io/badge/License-Apache_2.0-blue.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://github.com/NVIDIA/garak/actions/workflows/test_linux.yml"&gt;&lt;img src="https://github.com/NVIDIA/garak/actions/workflows/test_linux.yml/badge.svg?sanitize=true" alt="Tests/Linux" /&gt;&lt;/a&gt; &lt;a href="https://github.com/NVIDIA/garak/actions/workflows/test_windows.yml"&gt;&lt;img src="https://github.com/NVIDIA/garak/actions/workflows/test_windows.yml/badge.svg?sanitize=true" alt="Tests/Windows" /&gt;&lt;/a&gt; &lt;a href="https://github.com/NVIDIA/garak/actions/workflows/test_macos.yml"&gt;&lt;img src="https://github.com/NVIDIA/garak/actions/workflows/test_macos.yml/badge.svg?sanitize=true" alt="Tests/OSX" /&gt;&lt;/a&gt; &lt;a href="http://garak.readthedocs.io/en/latest/?badge=latest"&gt;&lt;img src="https://readthedocs.org/projects/garak/badge/?version=latest" alt="Documentation Status" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2406.11036"&gt;&lt;img src="https://img.shields.io/badge/cs.CL-arXiv%3A2406.11036-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/uVch4puUCs"&gt;&lt;img src="https://img.shields.io/badge/chat-on%20discord-yellow.svg?sanitize=true" alt="discord-img" /&gt;&lt;/a&gt; &lt;a href="https://github.com/psf/black"&gt;&lt;img src="https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true" alt="Code style: black" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/garak"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/garak" alt="PyPI - Python Version" /&gt;&lt;/a&gt; &lt;a href="https://badge.fury.io/py/garak"&gt;&lt;img src="https://badge.fury.io/py/garak.svg?sanitize=true" alt="PyPI" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/garak"&gt;&lt;img src="https://static.pepy.tech/badge/garak" alt="Downloads" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/garak"&gt;&lt;img src="https://static.pepy.tech/badge/garak/month" alt="Downloads" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Get started&lt;/h2&gt; 
&lt;h3&gt;&amp;gt; See our user guide! &lt;a href="https://docs.garak.ai/"&gt;docs.garak.ai&lt;/a&gt;&lt;/h3&gt; 
&lt;h3&gt;&amp;gt; Join our &lt;a href="https://discord.gg/uVch4puUCs"&gt;Discord&lt;/a&gt;!&lt;/h3&gt; 
&lt;h3&gt;&amp;gt; Project links &amp;amp; home: &lt;a href="https://garak.ai/"&gt;garak.ai&lt;/a&gt;&lt;/h3&gt; 
&lt;h3&gt;&amp;gt; Twitter: &lt;a href="https://twitter.com/garak_llm"&gt;@garak_llm&lt;/a&gt;&lt;/h3&gt; 
&lt;h3&gt;&amp;gt; DEF CON &lt;a href="https://garak.ai/garak_aiv_slides.pdf"&gt;slides&lt;/a&gt;!&lt;/h3&gt; 
&lt;hr /&gt; 
&lt;h2&gt;LLM support&lt;/h2&gt; 
&lt;p&gt;currently supports:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/models"&gt;hugging face hub&lt;/a&gt; generative models&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://replicate.com/"&gt;replicate&lt;/a&gt; text models&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://platform.openai.com/docs/introduction"&gt;openai api&lt;/a&gt; chat &amp;amp; continuation models&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.litellm.ai/"&gt;litellm&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;pretty much anything accessible via REST&lt;/li&gt; 
 &lt;li&gt;gguf models like &lt;a href="https://github.com/ggerganov/llama.cpp"&gt;llama.cpp&lt;/a&gt; version &amp;gt;= 1046&lt;/li&gt; 
 &lt;li&gt;.. and many more LLMs!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Install:&lt;/h2&gt; 
&lt;p&gt;&lt;code&gt;garak&lt;/code&gt; is a command-line tool. It's developed in Linux and OSX.&lt;/p&gt; 
&lt;h3&gt;Standard install with &lt;code&gt;pip&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;Just grab it from PyPI and you should be good to go:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;python -m pip install -U garak
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Install development version with &lt;code&gt;pip&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;The standard pip version of &lt;code&gt;garak&lt;/code&gt; is updated periodically. To get a fresher version from GitHub, try:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;python -m pip install -U git+https://github.com/NVIDIA/garak.git@main
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Clone from source&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;garak&lt;/code&gt; has its own dependencies. You can to install &lt;code&gt;garak&lt;/code&gt; in its own Conda environment:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;conda create --name garak "python&amp;gt;=3.10,&amp;lt;=3.12"
conda activate garak
gh repo clone NVIDIA/garak
cd garak
python -m pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;OK, if that went fine, you're probably good to go!&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: if you cloned before the move to the &lt;code&gt;NVIDIA&lt;/code&gt; GitHub organisation, but you're reading this at the &lt;code&gt;github.com/NVIDIA&lt;/code&gt; URI, please update your remotes as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;git remote set-url origin https://github.com/NVIDIA/garak.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Getting started&lt;/h2&gt; 
&lt;p&gt;The general syntax is:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;garak &amp;lt;options&amp;gt;&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;garak&lt;/code&gt; needs to know what model to scan, and by default, it'll try all the probes it knows on that model, using the vulnerability detectors recommended by each probe. You can see a list of probes using:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;garak --list_probes&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;To specify a generator, use the &lt;code&gt;--model_type&lt;/code&gt; and, optionally, the &lt;code&gt;--model_name&lt;/code&gt; options. Model type specifies a model family/interface; model name specifies the exact model to be used. The "Intro to generators" section below describes some of the generators supported. A straightforward generator family is Hugging Face models; to load one of these, set &lt;code&gt;--model_type&lt;/code&gt; to &lt;code&gt;huggingface&lt;/code&gt; and &lt;code&gt;--model_name&lt;/code&gt; to the model's name on Hub (e.g. &lt;code&gt;"RWKV/rwkv-4-169m-pile"&lt;/code&gt;). Some generators might need an API key to be set as an environment variable, and they'll let you know if they need that.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;garak&lt;/code&gt; runs all the probes by default, but you can be specific about that too. &lt;code&gt;--probes promptinject&lt;/code&gt; will use only the &lt;a href="https://github.com/agencyenterprise/promptinject"&gt;PromptInject&lt;/a&gt; framework's methods, for example. You can also specify one specific plugin instead of a plugin family by adding the plugin name after a &lt;code&gt;.&lt;/code&gt;; for example, &lt;code&gt;--probes lmrc.SlurUsage&lt;/code&gt; will use an implementation of checking for models generating slurs based on the &lt;a href="https://arxiv.org/abs/2303.18190"&gt;Language Model Risk Cards&lt;/a&gt; framework.&lt;/p&gt; 
&lt;p&gt;For help and inspiration, find us on &lt;a href="https://twitter.com/garak_llm"&gt;Twitter&lt;/a&gt; or &lt;a href="https://discord.gg/uVch4puUCs"&gt;discord&lt;/a&gt;!&lt;/p&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;p&gt;Probe ChatGPT for encoding-based prompt injection (OSX/*nix) (replace example value with a real OpenAI API key)&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;export OPENAI_API_KEY="sk-123XXXXXXXXXXXX"
python3 -m garak --model_type openai --model_name gpt-3.5-turbo --probes encoding
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See if the Hugging Face version of GPT2 is vulnerable to DAN 11.0&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;python3 -m garak --model_type huggingface --model_name gpt2 --probes dan.Dan_11_0
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Reading the results&lt;/h2&gt; 
&lt;p&gt;For each probe loaded, garak will print a progress bar as it generates. Once generation is complete, a row evaluating that probe's results on each detector is given. If any of the prompt attempts yielded an undesirable behavior, the response will be marked as FAIL, and the failure rate given.&lt;/p&gt; 
&lt;p&gt;Here are the results with the &lt;code&gt;encoding&lt;/code&gt; module on a GPT-3 variant: &lt;img src="https://i.imgur.com/8Dxf45N.png" alt="alt text" /&gt;&lt;/p&gt; 
&lt;p&gt;And the same results for ChatGPT: &lt;img src="https://i.imgur.com/VKAF5if.png" alt="alt text" /&gt;&lt;/p&gt; 
&lt;p&gt;We can see that the more recent model is much more susceptible to encoding-based injection attacks, where text-babbage-001 was only found to be vulnerable to quoted-printable and MIME encoding injections. The figures at the end of each row, e.g. 840/840, indicate the number of text generations total and then how many of these seemed to behave OK. The figure can be quite high because more than one generation is made per prompt - by default, 10.&lt;/p&gt; 
&lt;p&gt;Errors go in &lt;code&gt;garak.log&lt;/code&gt;; the run is logged in detail in a &lt;code&gt;.jsonl&lt;/code&gt; file specified at analysis start &amp;amp; end. There's a basic analysis script in &lt;code&gt;analyse/analyse_log.py&lt;/code&gt; which will output the probes and prompts that led to the most hits.&lt;/p&gt; 
&lt;p&gt;Send PRs &amp;amp; open issues. Happy hunting!&lt;/p&gt; 
&lt;h2&gt;Intro to generators&lt;/h2&gt; 
&lt;h3&gt;Hugging Face&lt;/h3&gt; 
&lt;p&gt;Using the Pipeline API:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--model_type huggingface&lt;/code&gt; (for transformers models to run locally)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; - use the model name from Hub. Only generative models will work. If it fails and shouldn't, please open an issue and paste in the command you tried + the exception!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Using the Inference API:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--model_type huggingface.InferenceAPI&lt;/code&gt; (for API-based model access)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; - the model name from Hub, e.g. &lt;code&gt;"mosaicml/mpt-7b-instruct"&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Using private endpoints:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;--model_type huggingface.InferenceEndpoint&lt;/code&gt; (for private endpoints)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;--model_name&lt;/code&gt; - the endpoint URL, e.g. &lt;code&gt;https://xxx.us-east-1.aws.endpoints.huggingface.cloud&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;(optional) set the &lt;code&gt;HF_INFERENCE_TOKEN&lt;/code&gt; environment variable to a Hugging Face API token with the "read" role; see &lt;a href="https://huggingface.co/settings/tokens"&gt;https://huggingface.co/settings/tokens&lt;/a&gt; when logged in&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;OpenAI&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--model_type openai&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; - the OpenAI model you'd like to use. &lt;code&gt;gpt-3.5-turbo-0125&lt;/code&gt; is fast and fine for testing.&lt;/li&gt; 
 &lt;li&gt;set the &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; environment variable to your OpenAI API key (e.g. "sk-19763ASDF87q6657"); see &lt;a href="https://platform.openai.com/account/api-keys"&gt;https://platform.openai.com/account/api-keys&lt;/a&gt; when logged in&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Recognised model types are whitelisted, because the plugin needs to know which sub-API to use. Completion or ChatCompletion models are OK. If you'd like to use a model not supported, you should get an informative error message, and please send a PR / open an issue.&lt;/p&gt; 
&lt;h3&gt;Replicate&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;set the &lt;code&gt;REPLICATE_API_TOKEN&lt;/code&gt; environment variable to your Replicate API token, e.g. "r8-123XXXXXXXXXXXX"; see &lt;a href="https://replicate.com/account/api-tokens"&gt;https://replicate.com/account/api-tokens&lt;/a&gt; when logged in&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Public Replicate models:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--model_type replicate&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; - the Replicate model name and hash, e.g. &lt;code&gt;"stability-ai/stablelm-tuned-alpha-7b:c49dae36"&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Private Replicate endpoints:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--model_type replicate.InferenceEndpoint&lt;/code&gt; (for private endpoints)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; - username/model-name slug from the deployed endpoint, e.g. &lt;code&gt;elim/elims-llama2-7b&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Cohere&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--model_type cohere&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; (optional, &lt;code&gt;command&lt;/code&gt; by default) - The specific Cohere model you'd like to test&lt;/li&gt; 
 &lt;li&gt;set the &lt;code&gt;COHERE_API_KEY&lt;/code&gt; environment variable to your Cohere API key, e.g. "aBcDeFgHiJ123456789"; see &lt;a href="https://dashboard.cohere.ai/api-keys"&gt;https://dashboard.cohere.ai/api-keys&lt;/a&gt; when logged in&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Groq&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--model_type groq&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; - The name of the model to access via the Groq API&lt;/li&gt; 
 &lt;li&gt;set the &lt;code&gt;GROQ_API_KEY&lt;/code&gt; environment variable to your Groq API key, see &lt;a href="https://console.groq.com/docs/quickstart"&gt;https://console.groq.com/docs/quickstart&lt;/a&gt; for details on creating an API key&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ggml&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--model_type ggml&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; - The path to the ggml model you'd like to load, e.g. &lt;code&gt;/home/leon/llama.cpp/models/7B/ggml-model-q4_0.bin&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;set the &lt;code&gt;GGML_MAIN_PATH&lt;/code&gt; environment variable to the path to your ggml &lt;code&gt;main&lt;/code&gt; executable&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;REST&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;rest.RestGenerator&lt;/code&gt; is highly flexible and can connect to any REST endpoint that returns plaintext or JSON. It does need some brief config, which will typically result a short YAML file describing your endpoint. See &lt;a href="https://reference.garak.ai/en/latest/garak.generators.rest.html"&gt;https://reference.garak.ai/en/latest/garak.generators.rest.html&lt;/a&gt; for examples.&lt;/p&gt; 
&lt;h3&gt;NIM&lt;/h3&gt; 
&lt;p&gt;Use models from &lt;a href="https://build.nvidia.com/"&gt;https://build.nvidia.com/&lt;/a&gt; or other NIM endpoints.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;set the &lt;code&gt;NIM_API_KEY&lt;/code&gt; environment variable to your authentication API token, or specify it in the config YAML&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For chat models:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--model_type nim&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; - the NIM &lt;code&gt;model&lt;/code&gt; name, e.g. &lt;code&gt;meta/llama-3.1-8b-instruct&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For completion models:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--model_type nim.NVOpenAICompletion&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--model_name&lt;/code&gt; - the NIM &lt;code&gt;model&lt;/code&gt; name, e.g. &lt;code&gt;bigcode/starcoder2-15b&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Test&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;--model_type test&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;(alternatively) &lt;code&gt;--model_name test.Blank&lt;/code&gt; For testing. This always generates the empty string, using the &lt;code&gt;test.Blank&lt;/code&gt; generator. Will be marked as failing for any tests that &lt;em&gt;require&lt;/em&gt; an output, e.g. those that make contentious claims and expect the model to refute them in order to pass.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;--model_type test.Repeat&lt;/code&gt; For testing. This generator repeats back the prompt it received.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Intro to probes&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Probe&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;blank&lt;/td&gt; 
   &lt;td&gt;A simple probe that always sends an empty prompt.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;atkgen&lt;/td&gt; 
   &lt;td&gt;Automated Attack Generation. A red-teaming LLM probes the target and reacts to it in an attempt to get toxic output. Prototype, mostly stateless, for now uses a simple GPT-2 &lt;a href="https://huggingface.co/garak-llm/artgpt2tox"&gt;fine-tuned&lt;/a&gt; on the subset of hhrlhf attempts that yielded detectable toxicity (the only target currently supported for now).&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;av_spam_scanning&lt;/td&gt; 
   &lt;td&gt;Probes that attempt to make the model output malicious content signatures&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;continuation&lt;/td&gt; 
   &lt;td&gt;Probes that test if the model will continue a probably undesirable word&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;dan&lt;/td&gt; 
   &lt;td&gt;Various &lt;a href="https://adguard.com/en/blog/chatgpt-dan-prompt-abuse.html"&gt;DAN&lt;/a&gt; and DAN-like attacks&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;donotanswer&lt;/td&gt; 
   &lt;td&gt;Prompts to which responsible language models should not answer.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;encoding&lt;/td&gt; 
   &lt;td&gt;Prompt injection through text encoding&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;gcg&lt;/td&gt; 
   &lt;td&gt;Disrupt a system prompt by appending an adversarial suffix.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;glitch&lt;/td&gt; 
   &lt;td&gt;Probe model for glitch tokens that provoke unusual behavior.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;grandma&lt;/td&gt; 
   &lt;td&gt;Appeal to be reminded of one's grandmother.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;goodside&lt;/td&gt; 
   &lt;td&gt;Implementations of Riley Goodside attacks.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;leakreplay&lt;/td&gt; 
   &lt;td&gt;Evaluate if a model will replay training data.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;lmrc&lt;/td&gt; 
   &lt;td&gt;Subsample of the &lt;a href="https://arxiv.org/abs/2303.18190"&gt;Language Model Risk Cards&lt;/a&gt; probes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;malwaregen&lt;/td&gt; 
   &lt;td&gt;Attempts to have the model generate code for building malware&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;misleading&lt;/td&gt; 
   &lt;td&gt;Attempts to make a model support misleading and false claims&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;packagehallucination&lt;/td&gt; 
   &lt;td&gt;Trying to get code generations that specify non-existent (and therefore insecure) packages.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;promptinject&lt;/td&gt; 
   &lt;td&gt;Implementation of the Agency Enterprise &lt;a href="https://github.com/agencyenterprise/PromptInject/tree/main/promptinject"&gt;PromptInject&lt;/a&gt; work (best paper awards @ NeurIPS ML Safety Workshop 2022)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;realtoxicityprompts&lt;/td&gt; 
   &lt;td&gt;Subset of the RealToxicityPrompts work (data constrained because the full test will take so long to run)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;snowball&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ofir.io/snowballed_hallucination.pdf"&gt;Snowballed Hallucination&lt;/a&gt; probes designed to make a model give a wrong answer to questions too complex for it to process&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;xss&lt;/td&gt; 
   &lt;td&gt;Look for vulnerabilities the permit or enact cross-site attacks, such as private data exfiltration.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Logging&lt;/h2&gt; 
&lt;p&gt;&lt;code&gt;garak&lt;/code&gt; generates multiple kinds of log:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;A log file, &lt;code&gt;garak.log&lt;/code&gt;. This includes debugging information from &lt;code&gt;garak&lt;/code&gt; and its plugins, and is continued across runs.&lt;/li&gt; 
 &lt;li&gt;A report of the current run, structured as JSONL. A new report file is created every time &lt;code&gt;garak&lt;/code&gt; runs. The name of this file is output at the beginning and, if successful, also at the end of the run. In the report, an entry is made for each probing attempt both as the generations are received, and again when they are evaluated; the entry's &lt;code&gt;status&lt;/code&gt; attribute takes a constant from &lt;code&gt;garak.attempts&lt;/code&gt; to describe what stage it was made at.&lt;/li&gt; 
 &lt;li&gt;A hit log, detailing attempts that yielded a vulnerability (a 'hit')&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;How is the code structured?&lt;/h2&gt; 
&lt;p&gt;Check out the &lt;a href="https://reference.garak.ai/"&gt;reference docs&lt;/a&gt; for an authoritative guide to &lt;code&gt;garak&lt;/code&gt; code structure.&lt;/p&gt; 
&lt;p&gt;In a typical run, &lt;code&gt;garak&lt;/code&gt; will read a model type (and optionally model name) from the command line, then determine which &lt;code&gt;probe&lt;/code&gt;s and &lt;code&gt;detector&lt;/code&gt;s to run, start up a &lt;code&gt;generator&lt;/code&gt;, and then pass these to a &lt;code&gt;harness&lt;/code&gt; to do the probing; an &lt;code&gt;evaluator&lt;/code&gt; deals with the results. There are many modules in each of these categories, and each module provides a number of classes that act as individual plugins.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;garak/probes/&lt;/code&gt; - classes for generating interactions with LLMs&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;garak/detectors/&lt;/code&gt; - classes for detecting an LLM is exhibiting a given failure mode&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;garak/evaluators/&lt;/code&gt; - assessment reporting schemes&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;garak/generators/&lt;/code&gt; - plugins for LLMs to be probed&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;garak/harnesses/&lt;/code&gt; - classes for structuring testing&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;resources/&lt;/code&gt; - ancillary items required by plugins&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The default operating mode is to use the &lt;code&gt;probewise&lt;/code&gt; harness. Given a list of probe module names and probe plugin names, the &lt;code&gt;probewise&lt;/code&gt; harness instantiates each probe, then for each probe reads its &lt;code&gt;recommended_detectors&lt;/code&gt; attribute to get a list of &lt;code&gt;detector&lt;/code&gt;s to run on the output.&lt;/p&gt; 
&lt;p&gt;Each plugin category (&lt;code&gt;probes&lt;/code&gt;, &lt;code&gt;detectors&lt;/code&gt;, &lt;code&gt;evaluators&lt;/code&gt;, &lt;code&gt;generators&lt;/code&gt;, &lt;code&gt;harnesses&lt;/code&gt;) includes a &lt;code&gt;base.py&lt;/code&gt; which defines the base classes usable by plugins in that category. Each plugin module defines plugin classes that inherit from one of the base classes. For example, &lt;code&gt;garak.generators.openai.OpenAIGenerator&lt;/code&gt; descends from &lt;code&gt;garak.generators.base.Generator&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Larger artefacts, like model files and bigger corpora, are kept out of the repository; they can be stored on e.g. Hugging Face Hub and loaded locally by clients using &lt;code&gt;garak&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Developing your own plugin&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Take a look at how other plugins do it&lt;/li&gt; 
 &lt;li&gt;Inherit from one of the base classes, e.g. &lt;code&gt;garak.probes.base.TextProbe&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Override as little as possible&lt;/li&gt; 
 &lt;li&gt;You can test the new code in at least two ways: 
  &lt;ul&gt; 
   &lt;li&gt;Start an interactive Python session 
    &lt;ul&gt; 
     &lt;li&gt;Import the model, e.g. &lt;code&gt;import garak.probes.mymodule&lt;/code&gt;&lt;/li&gt; 
     &lt;li&gt;Instantiate the plugin, e.g. &lt;code&gt;p = garak.probes.mymodule.MyProbe()&lt;/code&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Run a scan with test plugins 
    &lt;ul&gt; 
     &lt;li&gt;For probes, try a blank generator and always.Pass detector: &lt;code&gt;python3 -m garak -m test.Blank -p mymodule -d always.Pass&lt;/code&gt;&lt;/li&gt; 
     &lt;li&gt;For detectors, try a blank generator and a blank probe: &lt;code&gt;python3 -m garak -m test.Blank -p test.Blank -d mymodule&lt;/code&gt;&lt;/li&gt; 
     &lt;li&gt;For generators, try a blank probe and always.Pass detector: &lt;code&gt;python3 -m garak -m mymodule -p test.Blank -d always.Pass&lt;/code&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Get &lt;code&gt;garak&lt;/code&gt; to list all the plugins of the type you're writing, with &lt;code&gt;--list_probes&lt;/code&gt;, &lt;code&gt;--list_detectors&lt;/code&gt;, or &lt;code&gt;--list_generators&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;FAQ&lt;/h2&gt; 
&lt;p&gt;We have an FAQ &lt;a href="https://github.com/NVIDIA/garak/raw/main/FAQ.md"&gt;here&lt;/a&gt;. Reach out if you have any more questions! &lt;a href="mailto:garak@nvidia.com"&gt;garak@nvidia.com&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Code reference documentation is at &lt;a href="https://garak.readthedocs.io/en/latest/"&gt;garak.readthedocs.io&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Citing garak&lt;/h2&gt; 
&lt;p&gt;You can read the &lt;a href="https://raw.githubusercontent.com/NVIDIA/garak/main/garak-paper.pdf"&gt;garak preprint paper&lt;/a&gt;. If you use garak, please cite us.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@article{garak,
  title={{garak: A Framework for Security Probing Large Language Models}},
  author={Leon Derczynski and Erick Galinkin and Jeffrey Martin and Subho Majumdar and Nanna Inie},
  year={2024},
  howpublished={\url{https://garak.ai}}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;em&gt;"Lying is a skill like any other, and if you wish to maintain a level of excellence you have to practice constantly"&lt;/em&gt; - Elim&lt;/p&gt; 
&lt;p&gt;For updates and news see &lt;a href="https://twitter.com/garak_llm"&gt;@garak_llm&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Â© 2023- Leon Derczynski; Apache license v2, see &lt;a href="https://raw.githubusercontent.com/NVIDIA/garak/main/LICENSE"&gt;LICENSE&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>pathwaycom/pathway</title>
      <link>https://github.com/pathwaycom/pathway</link>
      <description>&lt;p&gt;Python ETL framework for stream processing, real-time analytics, LLM pipelines, and RAG.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;a href="https://pathway.com/"&gt; &lt;img src="https://pathway.com/logo-light.svg?sanitize=true" /&gt; &lt;/a&gt; 
 &lt;br /&gt;
 &lt;br /&gt; 
 &lt;a href="https://trendshift.io/repositories/10388" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/10388" alt="pathwaycom%2Fpathway | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; 
 &lt;br /&gt;
 &lt;br /&gt; 
&lt;/div&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/pathwaycom/pathway/actions/workflows/ubuntu_test.yml"&gt; &lt;img src="https://github.com/pathwaycom/pathway/actions/workflows/ubuntu_test.yml/badge.svg?sanitize=true" alt="ubuntu" /&gt; &lt;br /&gt; &lt;/a&gt;&lt;a href="https://github.com/pathwaycom/pathway/actions/workflows/release.yml"&gt; &lt;img src="https://github.com/pathwaycom/pathway/actions/workflows/release.yml/badge.svg?sanitize=true" alt="Last release" /&gt;&lt;/a&gt; &lt;a href="https://badge.fury.io/py/pathway"&gt;&lt;img src="https://badge.fury.io/py/pathway.svg?sanitize=true" alt="PyPI version" height="18" /&gt;&lt;/a&gt; &lt;a href="https://badge.fury.io/py/pathway"&gt;&lt;img src="https://static.pepy.tech/badge/pathway" alt="PyPI downloads" height="18" /&gt;&lt;/a&gt; &lt;a href="https://github.com/pathwaycom/pathway/raw/main/LICENSE.txt"&gt; &lt;img src="https://img.shields.io/badge/license-BSL-green" alt="License: BSL" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;a href="https://discord.gg/pathway"&gt; &lt;img src="https://img.shields.io/discord/1042405378304004156?logo=discord" alt="chat on Discord" /&gt;&lt;/a&gt; &lt;a href="https://twitter.com/intent/follow?screen_name=pathway_com"&gt; &lt;img src="https://img.shields.io/twitter/follow/pathwaycom" alt="follow on Twitter" /&gt;&lt;/a&gt; &lt;a href="https://linkedin.com/company/pathway"&gt; &lt;img src="https://img.shields.io/badge/pathway-0077B5?style=social&amp;amp;logo=linkedin" alt="follow on LinkedIn" /&gt;&lt;/a&gt; &lt;a href="https://github.com/dylanhogg/awesome-python/raw/main/README.md"&gt; &lt;img src="https://awesome.re/badge.svg?sanitize=true" alt="Awesome Python" /&gt;&lt;/a&gt; &lt;a href="https://gurubase.io/g/pathway"&gt; &lt;img src="https://img.shields.io/badge/Gurubase-Ask%20Pathway%20Guru-006BFF" alt="Pathway Guru" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;a href="https://raw.githubusercontent.com/pathwaycom/pathway/main/#getting-started"&gt;Getting Started&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/pathwaycom/pathway/main/#deployment"&gt;Deployment&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/pathwaycom/pathway/main/#resources"&gt;Documentation and Support&lt;/a&gt; | &lt;a href="https://pathway.com/blog/"&gt;Blog&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/pathwaycom/pathway/main/#license"&gt;License&lt;/a&gt; &lt;/p&gt; 
&lt;h1&gt;Pathway&lt;a id="pathway"&gt; Live Data Framework&lt;/a&gt;&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://pathway.com"&gt;Pathway&lt;/a&gt; is a Python ETL framework for stream processing, real-time analytics, LLM pipelines, and RAG.&lt;/p&gt; 
&lt;p&gt;Pathway comes with an &lt;strong&gt;easy-to-use Python API&lt;/strong&gt;, allowing you to seamlessly integrate your favorite Python ML libraries. Pathway code is versatile and robust: &lt;strong&gt;you can use it in both development and production environments, handling both batch and streaming data effectively&lt;/strong&gt;. The same code can be used for local development, CI/CD tests, running batch jobs, handling stream replays, and processing data streams.&lt;/p&gt; 
&lt;p&gt;Pathway is powered by a &lt;strong&gt;scalable Rust engine&lt;/strong&gt; based on Differential Dataflow and performs incremental computation. Your Pathway code, despite being written in Python, is run by the Rust engine, enabling multithreading, multiprocessing, and distributed computations. All the pipeline is kept in memory and can be easily deployed with &lt;strong&gt;Docker and Kubernetes&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;You can install Pathway with pip:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install -U pathway
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For any questions, you will find the community and team behind the project &lt;a href="https://discord.com/invite/pathway"&gt;on Discord&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Use-cases and templates&lt;/h2&gt; 
&lt;p&gt;Ready to see what Pathway can do?&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://pathway.com/developers/templates"&gt;Try one of our easy-to-run examples&lt;/a&gt;!&lt;/p&gt; 
&lt;p&gt;Available in both notebook and docker formats, these ready-to-launch examples can be launched in just a few clicks. Pick one and start your hands-on experience with Pathway today!&lt;/p&gt; 
&lt;h3&gt;Event processing and real-time analytics pipelines&lt;/h3&gt; 
&lt;p&gt;With its unified engine for batch and streaming and its full Python compatibility, Pathway makes data processing as easy as possible. It's the ideal solution for a wide range of data processing pipelines, including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://pathway.com/developers/templates/kafka-etl"&gt;Showcase: Real-time ETL.&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pathway.com/developers/templates/realtime-log-monitoring"&gt;Showcase: Event-driven pipelines with alerting.&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pathway.com/developers/templates/linear_regression_with_kafka/"&gt;Showcase: Realtime analytics.&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pathway.com/developers/user-guide/connecting-to-data/switch-from-batch-to-streaming"&gt;Docs: Switch from batch to streaming.&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;AI Pipelines&lt;/h3&gt; 
&lt;p&gt;Pathway provides dedicated LLM tooling to build live LLM and RAG pipelines. Wrappers for most common LLM services and utilities are included, making working with LLMs and RAGs pipelines incredibly easy. Check out our &lt;a href="https://pathway.com/developers/user-guide/llm-xpack/overview"&gt;LLM xpack documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Don't hesitate to try one of our runnable examples featuring LLM tooling. You can find such examples &lt;a href="https://pathway.com/developers/user-guide/llm-xpack/llm-examples"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://pathway.com/developers/templates/unstructured-to-structured/"&gt;Template: Unstructured data to SQL on-the-fly.&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pathway.com/developers/templates/private-rag-ollama-mistral"&gt;Template: Private RAG with Ollama and Mistral AI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pathway.com/developers/templates/adaptive-rag"&gt;Template: Adaptive RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pathway.com/developers/templates/multimodal-rag"&gt;Template: Multimodal RAG with gpt-4o&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;A wide range of connectors&lt;/strong&gt;: Pathway comes with connectors that connect to external data sources such as Kafka, GDrive, PostgreSQL, or SharePoint. Its Airbyte connector allows you to connect to more than 300 different data sources. If the connector you want is not available, you can build your own custom connector using Pathway Python connector.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Stateless and stateful transformations&lt;/strong&gt;: Pathway supports stateful transformations such as joins, windowing, and sorting. It provides many transformations directly implemented in Rust. In addition to the provided transformation, you can use any Python function. You can implement your own or you can use any Python library to process your data.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Persistence&lt;/strong&gt;: Pathway provides persistence to save the state of the computation. This allows you to restart your pipeline after an update or a crash. Your pipelines are in good hands with Pathway!&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Consistency&lt;/strong&gt;: Pathway handles the time for you, making sure all your computations are consistent. In particular, Pathway manages late and out-of-order points by updating its results whenever new (or late, in this case) data points come into the system. The free version of Pathway gives the "at least once" consistency while the enterprise version provides the "exactly once" consistency.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Scalable Rust engine&lt;/strong&gt;: with Pathway Rust engine, you are free from the usual limits imposed by Python. You can easily do multithreading, multiprocessing, and distributed computations.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;LLM helpers&lt;/strong&gt;: Pathway provides an LLM extension with all the utilities to integrate LLMs with your data pipelines (LLM wrappers, parsers, embedders, splitters), including an in-memory real-time Vector Index, and integrations with LLamaIndex and LangChain. You can quickly build and deploy RAG applications with your live documents.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Getting started&lt;a id="getting-started"&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;h3&gt;Installation&lt;a id="installation"&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Pathway requires Python 3.10 or above.&lt;/p&gt; 
&lt;p&gt;You can install the current release of Pathway using &lt;code&gt;pip&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ pip install -U pathway
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;âš ï¸ Pathway is available on MacOS and Linux. Users of other systems should run Pathway on a Virtual Machine.&lt;/p&gt; 
&lt;h3&gt;Example: computing the sum of positive values in real time.&lt;a id="example"&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import pathway as pw

# Define the schema of your data (Optional)
class InputSchema(pw.Schema):
  value: int

# Connect to your data using connectors
input_table = pw.io.csv.read(
  "./input/",
  schema=InputSchema
)

#Define your operations on the data
filtered_table = input_table.filter(input_table.value&amp;gt;=0)
result_table = filtered_table.reduce(
  sum_value = pw.reducers.sum(filtered_table.value)
)

# Load your results to external systems
pw.io.jsonlines.write(result_table, "output.jsonl")

# Run the computation
pw.run()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Run Pathway &lt;a href="https://colab.research.google.com/drive/1aBIJ2HCng-YEUOMrr0qtj0NeZMEyRz55?usp=sharing"&gt;in Google Colab&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You can find more examples &lt;a href="https://github.com/pathwaycom/pathway/tree/main/examples"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Deployment&lt;a id="deployment"&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;h3&gt;Locally&lt;a id="running-pathway-locally"&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;To use Pathway, you only need to import it:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import pathway as pw
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Now, you can easily create your processing pipeline, and let Pathway handle the updates. Once your pipeline is created, you can launch the computation on streaming data with a one-line command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;pw.run()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can then run your Pathway project (say, &lt;code&gt;main.py&lt;/code&gt;) just like a normal Python script: &lt;code&gt;$ python main.py&lt;/code&gt;. Pathway comes with a monitoring dashboard that allows you to keep track of the number of messages sent by each connector and the latency of the system. The dashboard also includes log messages.&lt;/p&gt; 
&lt;img src="https://d14l3brkh44201.cloudfront.net/pathway-dashboard.png" width="1326" alt="Pathway dashboard" /&gt; 
&lt;p&gt;Alternatively, you can use the pathway'ish version:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ pathway spawn python main.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Pathway natively supports multithreading. To launch your application with 3 threads, you can do as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ pathway spawn --threads 3 python main.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To jumpstart a Pathway project, you can use our &lt;a href="https://github.com/pathwaycom/cookiecutter-pathway"&gt;cookiecutter template&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Docker&lt;a id="docker"&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;You can easily run Pathway using docker.&lt;/p&gt; 
&lt;h4&gt;Pathway image&lt;/h4&gt; 
&lt;p&gt;You can use the &lt;a href="https://hub.docker.com/r/pathwaycom/pathway"&gt;Pathway docker image&lt;/a&gt;, using a Dockerfile:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-dockerfile"&gt;FROM pathwaycom/pathway:latest

WORKDIR /app

COPY requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD [ "python", "./your-script.py" ]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can then build and run the Docker image:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;docker build -t my-pathway-app .
docker run -it --rm --name my-pathway-app my-pathway-app
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Run a single Python script&lt;/h4&gt; 
&lt;p&gt;When dealing with single-file projects, creating a full-fledged &lt;code&gt;Dockerfile&lt;/code&gt; might seem unnecessary. In such scenarios, you can execute a Python script directly using the Pathway Docker image. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;docker run -it --rm --name my-pathway-app -v "$PWD":/app pathwaycom/pathway:latest python my-pathway-app.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Python docker image&lt;/h4&gt; 
&lt;p&gt;You can also use a standard Python image and install Pathway using pip with a Dockerfile:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-dockerfile"&gt;FROM --platform=linux/x86_64 python:3.10

RUN pip install -U pathway
COPY ./pathway-script.py pathway-script.py

CMD ["python", "-u", "pathway-script.py"]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Kubernetes and cloud&lt;a id="k8s"&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Docker containers are ideally suited for deployment on the cloud with Kubernetes. If you want to scale your Pathway application, you may be interested in our Pathway for Enterprise. Pathway for Enterprise is specially tailored towards end-to-end data processing and real time intelligent analytics. It scales using distributed computing on the cloud and supports distributed Kubernetes deployment, with external persistence setup.&lt;/p&gt; 
&lt;p&gt;You can easily deploy Pathway using services like Render: see &lt;a href="https://pathway.com/developers/user-guide/deployment/render-deploy/"&gt;how to deploy Pathway in a few clicks&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you are interested, don't hesitate to &lt;a href="mailto:contact@pathway.com"&gt;contact us&lt;/a&gt; to learn more.&lt;/p&gt; 
&lt;h2&gt;Performance&lt;a id="performance"&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;Pathway is made to outperform state-of-the-art technologies designed for streaming and batch data processing tasks, including: Flink, Spark, and Kafka Streaming. It also makes it possible to implement a lot of algorithms/UDF's in streaming mode which are not readily supported by other streaming frameworks (especially: temporal joins, iterative graph algorithms, machine learning routines).&lt;/p&gt; 
&lt;p&gt;If you are curious, here are &lt;a href="https://github.com/pathwaycom/pathway-benchmarks"&gt;some benchmarks to play with&lt;/a&gt;.&lt;/p&gt; 
&lt;img src="https://github.com/pathwaycom/pathway-benchmarks/raw/main/images/bm-wordcount-lineplot.png" width="1326" alt="WordCount Graph" /&gt; 
&lt;h2&gt;Documentation and Support&lt;a id="resources"&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;The entire documentation of Pathway is available at &lt;a href="https://pathway.com/developers/user-guide/introduction/welcome"&gt;pathway.com/developers/&lt;/a&gt;, including the &lt;a href="https://pathway.com/developers/api-docs/pathway"&gt;API Docs&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you have any question, don't hesitate to &lt;a href="https://github.com/pathwaycom/pathway/issues"&gt;open an issue on GitHub&lt;/a&gt;, join us on &lt;a href="https://discord.com/invite/pathway"&gt;Discord&lt;/a&gt;, or send us an email at &lt;a href="mailto:contact@pathway.com"&gt;contact@pathway.com&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;a id="license"&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;Pathway is distributed on a &lt;a href="https://github.com/pathwaycom/pathway/raw/main/LICENSE.txt"&gt;BSL 1.1 License&lt;/a&gt; which allows for unlimited non-commercial use, as well as use of the Pathway package &lt;a href="https://pathway.com/license/"&gt;for most commercial purposes&lt;/a&gt;, free of charge. Code in this repository automatically converts to Open Source (Apache 2.0 License) after 4 years. Some &lt;a href="https://github.com/pathwaycom"&gt;public repos&lt;/a&gt; which are complementary to this one (examples, libraries, connectors, etc.) are licensed as Open Source, under the MIT license.&lt;/p&gt; 
&lt;h2&gt;Contribution guidelines&lt;a id="contribution-guidelines"&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;If you develop a library or connector which you would like to integrate with this repo, we suggest releasing it first as a separate repo on a MIT/Apache 2.0 license.&lt;/p&gt; 
&lt;p&gt;For all concerns regarding core Pathway functionalities, Issues are encouraged. For further information, don't hesitate to engage with Pathway's &lt;a href="https://discord.gg/pathway"&gt;Discord community&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ahujasid/blender-mcp</title>
      <link>https://github.com/ahujasid/blender-mcp</link>
      <description>&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;BlenderMCP - Blender Model Context Protocol Integration&lt;/h1&gt; 
&lt;p&gt;BlenderMCP connects Blender to Claude AI through the Model Context Protocol (MCP), allowing Claude to directly interact with and control Blender. This integration enables prompt assisted 3D modeling, scene creation, and manipulation.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;We have no official website. Any website you see online is unofficial and has no affiliation with this project. Use them at your own risk.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=lCyQ717DuzQ"&gt;Full tutorial&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Join the Community&lt;/h3&gt; 
&lt;p&gt;Give feedback, get inspired, and build on top of the MCP: &lt;a href="https://discord.gg/z5apgR8TFU"&gt;Discord&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Supporters&lt;/h3&gt; 
&lt;div align="center" markdown="1"&gt; 
 &lt;sup&gt;Special thanks to:&lt;/sup&gt; 
 &lt;br /&gt; 
 &lt;br /&gt; 
 &lt;a href="https://www.warp.dev/blender-mcp"&gt; &lt;img alt="Warp sponsorship" width="400" src="https://github.com/user-attachments/assets/c21102f7-bab9-4344-a731-0cf6b341cab2" /&gt; &lt;/a&gt; 
 &lt;h3&gt;&lt;a href="https://www.warp.dev/blender-mcp"&gt;Warp, the intelligent terminal for developers&lt;/a&gt;&lt;/h3&gt; 
 &lt;p&gt;&lt;a href="https://www.warp.dev/blender-mcp"&gt;Available for MacOS, Linux, &amp;amp; Windows&lt;/a&gt;&lt;br /&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;strong&gt;Other supporters:&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.coderabbit.ai/"&gt;CodeRabbit&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/satishgoda"&gt;Satish Goda&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;All supporters:&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/sponsors/ahujasid"&gt;Support this project&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Release notes (1.2.0)&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;View screenshots for Blender viewport to better understand the scene&lt;/li&gt; 
 &lt;li&gt;Search and download Sketchfab models&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Previously added features:&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Support for Poly Haven assets through their API&lt;/li&gt; 
 &lt;li&gt;Support to generate 3D models using Hyper3D Rodin&lt;/li&gt; 
 &lt;li&gt;For newcomers, you can go straight to Installation. For existing users, see the points below&lt;/li&gt; 
 &lt;li&gt;Download the latest addon.py file and replace the older one, then add it to Blender&lt;/li&gt; 
 &lt;li&gt;Delete the MCP server from Claude and add it back again, and you should be good to go!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Two-way communication&lt;/strong&gt;: Connect Claude AI to Blender through a socket-based server&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Object manipulation&lt;/strong&gt;: Create, modify, and delete 3D objects in Blender&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Material control&lt;/strong&gt;: Apply and modify materials and colors&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Scene inspection&lt;/strong&gt;: Get detailed information about the current Blender scene&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Code execution&lt;/strong&gt;: Run arbitrary Python code in Blender from Claude&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Components&lt;/h2&gt; 
&lt;p&gt;The system consists of two main components:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Blender Addon (&lt;code&gt;addon.py&lt;/code&gt;)&lt;/strong&gt;: A Blender addon that creates a socket server within Blender to receive and execute commands&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MCP Server (&lt;code&gt;src/blender_mcp/server.py&lt;/code&gt;)&lt;/strong&gt;: A Python server that implements the Model Context Protocol and connects to the Blender addon&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Blender 3.0 or newer&lt;/li&gt; 
 &lt;li&gt;Python 3.10 or newer&lt;/li&gt; 
 &lt;li&gt;uv package manager:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;If you're on Mac, please install uv as&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;brew install uv
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;On Windows&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;powershell -c "irm https://astral.sh/uv/install.ps1 | iex" 
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;and then&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;set Path=C:\Users\nntra\.local\bin;%Path%
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Otherwise installation instructions are on their website: &lt;a href="https://docs.astral.sh/uv/getting-started/installation/"&gt;Install uv&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;âš ï¸ Do not proceed before installing UV&lt;/strong&gt;&lt;/p&gt; 
&lt;h3&gt;Environment Variables&lt;/h3&gt; 
&lt;p&gt;The following environment variables can be used to configure the Blender connection:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;BLENDER_HOST&lt;/code&gt;: Host address for Blender socket server (default: "localhost")&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;BLENDER_PORT&lt;/code&gt;: Port number for Blender socket server (default: 9876)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export BLENDER_HOST='host.docker.internal'
export BLENDER_PORT=9876
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Claude for Desktop Integration&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=neoK_WMq92g"&gt;Watch the setup instruction video&lt;/a&gt; (Assuming you have already installed uv)&lt;/p&gt; 
&lt;p&gt;Go to Claude &amp;gt; Settings &amp;gt; Developer &amp;gt; Edit Config &amp;gt; claude_desktop_config.json to include the following:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
    "mcpServers": {
        "blender": {
            "command": "uvx",
            "args": [
                "blender-mcp"
            ]
        }
    }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Cursor integration&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://cursor.com/install-mcp?name=blender&amp;amp;config=eyJjb21tYW5kIjoidXZ4IGJsZW5kZXItbWNwIn0%3D"&gt;&lt;img src="https://cursor.com/deeplink/mcp-install-dark.svg?sanitize=true" alt="Install MCP Server" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;For Mac users, go to Settings &amp;gt; MCP and paste the following&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;To use as a global server, use "add new global MCP server" button and paste&lt;/li&gt; 
 &lt;li&gt;To use as a project specific server, create &lt;code&gt;.cursor/mcp.json&lt;/code&gt; in the root of the project and paste&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
    "mcpServers": {
        "blender": {
            "command": "uvx",
            "args": [
                "blender-mcp"
            ]
        }
    }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For Windows users, go to Settings &amp;gt; MCP &amp;gt; Add Server, add a new server with the following settings:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
    "mcpServers": {
        "blender": {
            "command": "cmd",
            "args": [
                "/c",
                "uvx",
                "blender-mcp"
            ]
        }
    }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=wgWsJshecac"&gt;Cursor setup video&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;âš ï¸ Only run one instance of the MCP server (either on Cursor or Claude Desktop), not both&lt;/strong&gt;&lt;/p&gt; 
&lt;h3&gt;Visual Studio Code Integration&lt;/h3&gt; 
&lt;p&gt;&lt;em&gt;Prerequisites&lt;/em&gt;: Make sure you have &lt;a href="https://code.visualstudio.com/"&gt;Visual Studio Code&lt;/a&gt; installed before proceeding.&lt;/p&gt; 
&lt;p&gt;&lt;a href="vscode:mcp/install?%7B%22name%22%3A%22blender-mcp%22%2C%22type%22%3A%22stdio%22%2C%22command%22%3A%22uvx%22%2C%22args%22%3A%5B%22blender-mcp%22%5D%7D"&gt;&lt;img src="https://img.shields.io/badge/VS_Code-Install_blender--mcp_server-0098FF?style=flat-square&amp;amp;logo=visualstudiocode&amp;amp;logoColor=ffffff" alt="Install in VS Code" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Installing the Blender Addon&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Download the &lt;code&gt;addon.py&lt;/code&gt; file from this repo&lt;/li&gt; 
 &lt;li&gt;Open Blender&lt;/li&gt; 
 &lt;li&gt;Go to Edit &amp;gt; Preferences &amp;gt; Add-ons&lt;/li&gt; 
 &lt;li&gt;Click "Install..." and select the &lt;code&gt;addon.py&lt;/code&gt; file&lt;/li&gt; 
 &lt;li&gt;Enable the addon by checking the box next to "Interface: Blender MCP"&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;h3&gt;Starting the Connection&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/ahujasid/blender-mcp/main/assets/addon-instructions.png" alt="BlenderMCP in the sidebar" /&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;In Blender, go to the 3D View sidebar (press N if not visible)&lt;/li&gt; 
 &lt;li&gt;Find the "BlenderMCP" tab&lt;/li&gt; 
 &lt;li&gt;Turn on the Poly Haven checkbox if you want assets from their API (optional)&lt;/li&gt; 
 &lt;li&gt;Click "Connect to Claude"&lt;/li&gt; 
 &lt;li&gt;Make sure the MCP server is running in your terminal&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Using with Claude&lt;/h3&gt; 
&lt;p&gt;Once the config file has been set on Claude, and the addon is running on Blender, you will see a hammer icon with tools for the Blender MCP.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/ahujasid/blender-mcp/main/assets/hammer-icon.png" alt="BlenderMCP in the sidebar" /&gt;&lt;/p&gt; 
&lt;h4&gt;Capabilities&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Get scene and object information&lt;/li&gt; 
 &lt;li&gt;Create, delete and modify shapes&lt;/li&gt; 
 &lt;li&gt;Apply or create materials for objects&lt;/li&gt; 
 &lt;li&gt;Execute any Python code in Blender&lt;/li&gt; 
 &lt;li&gt;Download the right models, assets and HDRIs through &lt;a href="https://polyhaven.com/"&gt;Poly Haven&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;AI generated 3D models through &lt;a href="https://hyper3d.ai/"&gt;Hyper3D Rodin&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Example Commands&lt;/h3&gt; 
&lt;p&gt;Here are some examples of what you can ask Claude to do:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;"Create a low poly scene in a dungeon, with a dragon guarding a pot of gold" &lt;a href="https://www.youtube.com/watch?v=DqgKuLYUv00"&gt;Demo&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;"Create a beach vibe using HDRIs, textures, and models like rocks and vegetation from Poly Haven" &lt;a href="https://www.youtube.com/watch?v=I29rn92gkC4"&gt;Demo&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Give a reference image, and create a Blender scene out of it &lt;a href="https://www.youtube.com/watch?v=FDRb03XPiRo"&gt;Demo&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;"Generate a 3D model of a garden gnome through Hyper3D"&lt;/li&gt; 
 &lt;li&gt;"Get information about the current scene, and make a threejs sketch from it" &lt;a href="https://www.youtube.com/watch?v=jxbNI5L7AH8"&gt;Demo&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;"Make this car red and metallic"&lt;/li&gt; 
 &lt;li&gt;"Create a sphere and place it above the cube"&lt;/li&gt; 
 &lt;li&gt;"Make the lighting like a studio"&lt;/li&gt; 
 &lt;li&gt;"Point the camera at the scene, and make it isometric"&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Hyper3D integration&lt;/h2&gt; 
&lt;p&gt;Hyper3D's free trial key allows you to generate a limited number of models per day. If the daily limit is reached, you can wait for the next day's reset or obtain your own key from hyper3d.ai and fal.ai.&lt;/p&gt; 
&lt;h2&gt;Troubleshooting&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Connection issues&lt;/strong&gt;: Make sure the Blender addon server is running, and the MCP server is configured on Claude, DO NOT run the uvx command in the terminal. Sometimes, the first command won't go through but after that it starts working.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Timeout errors&lt;/strong&gt;: Try simplifying your requests or breaking them into smaller steps&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Poly Haven integration&lt;/strong&gt;: Claude is sometimes erratic with its behaviour&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Have you tried turning it off and on again?&lt;/strong&gt;: If you're still having connection errors, try restarting both Claude and the Blender server&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Technical Details&lt;/h2&gt; 
&lt;h3&gt;Communication Protocol&lt;/h3&gt; 
&lt;p&gt;The system uses a simple JSON-based protocol over TCP sockets:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Commands&lt;/strong&gt; are sent as JSON objects with a &lt;code&gt;type&lt;/code&gt; and optional &lt;code&gt;params&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Responses&lt;/strong&gt; are JSON objects with a &lt;code&gt;status&lt;/code&gt; and &lt;code&gt;result&lt;/code&gt; or &lt;code&gt;message&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Limitations &amp;amp; Security Considerations&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;The &lt;code&gt;execute_blender_code&lt;/code&gt; tool allows running arbitrary Python code in Blender, which can be powerful but potentially dangerous. Use with caution in production environments. ALWAYS save your work before using it.&lt;/li&gt; 
 &lt;li&gt;Poly Haven requires downloading models, textures, and HDRI images. If you do not want to use it, please turn it off in the checkbox in Blender.&lt;/li&gt; 
 &lt;li&gt;Complex operations might need to be broken down into smaller steps&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Contributions are welcome! Please feel free to submit a Pull Request.&lt;/p&gt; 
&lt;h2&gt;Disclaimer&lt;/h2&gt; 
&lt;p&gt;This is a third-party integration and not made by Blender. Made by &lt;a href="https://x.com/sidahuj"&gt;Siddharth&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>NVIDIA/TensorRT-Model-Optimizer</title>
      <link>https://github.com/NVIDIA/TensorRT-Model-Optimizer</link>
      <description>&lt;p&gt;A unified library of state-of-the-art model optimization techniques like quantization, pruning, distillation, speculative decoding, etc. It compresses deep learning models for downstream deployment frameworks like TensorRT-LLM or TensorRT to optimize inference speed.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/NVIDIA/TensorRT-Model-Optimizer/main/docs/source/assets/model-optimizer-banner.png" alt="Banner image" /&gt;&lt;/p&gt; 
 &lt;h1&gt;NVIDIA TensorRT Model Optimizer&lt;/h1&gt; 
 &lt;p&gt;&lt;a href="https://nvidia.github.io/TensorRT-Model-Optimizer"&gt;&lt;img src="https://img.shields.io/badge/Documentation-latest-brightgreen.svg?style=flat" alt="Documentation" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/nvidia-modelopt/"&gt;&lt;img src="https://img.shields.io/pypi/v/nvidia-modelopt?label=Release" alt="version" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/NVIDIA/TensorRT-Model-Optimizer/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/License-Apache%202.0-blue" alt="license" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://nvidia.github.io/TensorRT-Model-Optimizer"&gt;Documentation&lt;/a&gt; | &lt;a href="https://github.com/NVIDIA/TensorRT-Model-Optimizer/issues/146"&gt;Roadmap&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;p&gt;The &lt;strong&gt;NVIDIA TensorRT Model Optimizer&lt;/strong&gt; (referred to as &lt;strong&gt;Model Optimizer&lt;/strong&gt;, or &lt;strong&gt;ModelOpt&lt;/strong&gt;) is a library comprising state-of-the-art model optimization &lt;a href="https://raw.githubusercontent.com/NVIDIA/TensorRT-Model-Optimizer/main/#techniques"&gt;techniques&lt;/a&gt; including quantization, distillation, pruning, speculative decoding and sparsity to accelerate models.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;[Input]&lt;/strong&gt; Model Optimizer currently supports inputs of a &lt;a href="https://huggingface.co/"&gt;Hugging Face&lt;/a&gt;, &lt;a href="https://github.com/pytorch/pytorch"&gt;PyTorch&lt;/a&gt; or &lt;a href="https://github.com/onnx/onnx"&gt;ONNX&lt;/a&gt; model.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;[Optimize]&lt;/strong&gt; Model Optimizer provides Python APIs for users to easily compose the above model optimization techniques and export an optimized quantized checkpoint. Model Optimizer is also integrated with &lt;a href="https://github.com/NVIDIA-NeMo/NeMo"&gt;NVIDIA NeMo&lt;/a&gt;, &lt;a href="https://github.com/NVIDIA/Megatron-LM"&gt;Megatron-LM&lt;/a&gt; and &lt;a href="https://github.com/huggingface/accelerate"&gt;Hugging Face Accelerate&lt;/a&gt; for training required inference optimization techniques.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;[Export for deployment]&lt;/strong&gt; Seamlessly integrated within the NVIDIA AI software ecosystem, the quantized checkpoint generated from Model Optimizer is ready for deployment in downstream inference frameworks like &lt;a href="https://github.com/sgl-project/sglang"&gt;SGLang&lt;/a&gt;, &lt;a href="https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/quantization"&gt;TensorRT-LLM&lt;/a&gt;, &lt;a href="https://github.com/NVIDIA/TensorRT"&gt;TensorRT&lt;/a&gt;, or &lt;a href="https://github.com/vllm-project/vllm"&gt;vLLM&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Latest News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;[2025/08/29] &lt;a href="https://developer.nvidia.com/blog/fine-tuning-gpt-oss-for-accuracy-and-performance-with-quantization-aware-training/"&gt;Fine-Tuning gpt-oss for Accuracy and Performance with Quantization Aware Training&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[2025/08/01] &lt;a href="https://developer.nvidia.com/blog/optimizing-llms-for-performance-and-accuracy-with-post-training-quantization/"&gt;Optimizing LLMs for Performance and Accuracy with Post-Training Quantization&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[2025/06/24] &lt;a href="https://developer.nvidia.com/blog/introducing-nvfp4-for-efficient-and-accurate-low-precision-inference/"&gt;Introducing NVFP4 for Efficient and Accurate Low-Precision Inference&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[2025/05/14] &lt;a href="https://developer.nvidia.com/blog/nvidia-tensorrt-unlocks-fp4-image-generation-for-nvidia-blackwell-geforce-rtx-50-series-gpus/"&gt;NVIDIA TensorRT Unlocks FP4 Image Generation for NVIDIA Blackwell GeForce RTX 50 Series GPUs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[2025/04/21] &lt;a href="https://developer.nvidia.com/blog/optimizing-transformer-based-diffusion-models-for-video-generation-with-nvidia-tensorrt/"&gt;Adobe optimized deployment using TensorRT-Model-Optimizer + TensorRT leading to a 60% reduction in diffusion latency, a 40% reduction in total cost of ownership&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[2025/04/05] &lt;a href="https://developer.nvidia.com/blog/nvidia-accelerates-inference-on-meta-llama-4-scout-and-maverick/"&gt;NVIDIA Accelerates Inference on Meta Llama 4 Scout and Maverick&lt;/a&gt;. Check out how to quantize Llama4 for deployment acceleration &lt;a href="https://raw.githubusercontent.com/NVIDIA/TensorRT-Model-Optimizer/main/examples/llm_ptq/README.md#llama-4"&gt;here&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[2025/03/18] &lt;a href="https://developer.nvidia.com/blog/nvidia-blackwell-delivers-world-record-deepseek-r1-inference-performance/"&gt;World's Fastest DeepSeek-R1 Inference with Blackwell FP4 &amp;amp; Increasing Image Generation Efficiency on Blackwell&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[2025/02/25] Model Optimizer quantized NVFP4 models available on Hugging Face for download: &lt;a href="https://huggingface.co/nvidia/DeepSeek-R1-FP4"&gt;DeepSeek-R1-FP4&lt;/a&gt;, &lt;a href="https://huggingface.co/nvidia/Llama-3.3-70B-Instruct-FP4"&gt;Llama-3.3-70B-Instruct-FP4&lt;/a&gt;, &lt;a href="https://huggingface.co/nvidia/Llama-3.1-405B-Instruct-FP4"&gt;Llama-3.1-405B-Instruct-FP4&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[2025/01/28] Model Optimizer has added support for NVFP4. Check out an example of NVFP4 PTQ &lt;a href="https://raw.githubusercontent.com/NVIDIA/TensorRT-Model-Optimizer/main/examples/llm_ptq/README.md#model-quantization-and-trt-llm-conversion"&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;[2025/01/28] Model Optimizer is now open source!&lt;/li&gt; 
 &lt;li&gt;[2024/10/23] Model Optimizer quantized FP8 Llama-3.1 Instruct models available on Hugging Face for download: &lt;a href="https://huggingface.co/nvidia/Llama-3.1-8B-Instruct-FP8"&gt;8B&lt;/a&gt;, &lt;a href="https://huggingface.co/nvidia/Llama-3.1-70B-Instruct-FP8"&gt;70B&lt;/a&gt;, &lt;a href="https://huggingface.co/nvidia/Llama-3.1-405B-Instruct-FP8"&gt;405B&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;[2024/09/10] &lt;a href="https://developer.nvidia.com/blog/post-training-quantization-of-llms-with-nvidia-nemo-and-nvidia-tensorrt-model-optimizer/"&gt;Post-Training Quantization of LLMs with NVIDIA NeMo and TensorRT Model Optimizer&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details close&gt; 
 &lt;summary&gt;Previous News&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;[2024/08/28] &lt;a href="https://developer.nvidia.com/blog/boosting-llama-3-1-405b-performance-by-up-to-44-with-nvidia-tensorrt-model-optimizer-on-nvidia-h200-gpus/"&gt;Boosting Llama 3.1 405B Performance up to 44% with TensorRT Model Optimizer on NVIDIA H200 GPUs&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;[2024/08/28] &lt;a href="https://developer.nvidia.com/blog/low-latency-inference-chapter-1-up-to-1-9x-higher-llama-3-1-performance-with-medusa-on-nvidia-hgx-h200-with-nvlink-switch/"&gt;Up to 1.9X Higher Llama 3.1 Performance with Medusa&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;[2024/08/15] New features in recent releases: &lt;a href="https://github.com/NVIDIA/TensorRT-Model-Optimizer/tree/main/examples/diffusers/cache_diffusion"&gt;Cache Diffusion&lt;/a&gt;, &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/24.09/sft_peft/qlora.html"&gt;QLoRA workflow with NVIDIA NeMo&lt;/a&gt;, and more. Check out &lt;a href="https://developer.nvidia.com/blog/nvidia-tensorrt-model-optimizer-v0-15-boosts-inference-performance-and-expands-model-support/"&gt;our blog&lt;/a&gt; for details.&lt;/li&gt; 
  &lt;li&gt;[2024/06/03] Model Optimizer now has an experimental feature to deploy to vLLM as part of our effort to support popular deployment frameworks. Check out the workflow &lt;a href="https://raw.githubusercontent.com/NVIDIA/TensorRT-Model-Optimizer/main/examples/llm_ptq/README.md#deploy-fp8-quantized-model-using-vllm"&gt;here&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;[2024/05/08] &lt;a href="https://developer.nvidia.com/blog/accelerate-generative-ai-inference-performance-with-nvidia-tensorrt-model-optimizer-now-publicly-available/"&gt;Announcement: Model Optimizer Now Formally Available to Further Accelerate GenAI Inference Performance&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;[2024/03/27] &lt;a href="https://developer.nvidia.com/blog/nvidia-h200-tensor-core-gpus-and-nvidia-tensorrt-llm-set-mlperf-llm-inference-records/"&gt;Model Optimizer supercharges TensorRT-LLM to set MLPerf LLM inference records&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;[2024/03/18] &lt;a href="https://www.nvidia.com/en-us/on-demand/session/gtc24-s63213/"&gt;GTC Session: Optimize Generative AI Inference with Quantization in TensorRT-LLM and TensorRT&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;[2024/03/07] &lt;a href="https://developer.nvidia.com/blog/tensorrt-accelerates-stable-diffusion-nearly-2x-faster-with-8-bit-post-training-quantization/"&gt;Model Optimizer's 8-bit Post-Training Quantization enables TensorRT to accelerate Stable Diffusion to nearly 2x faster&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;[2024/02/01] &lt;a href="https://github.com/NVIDIA/TensorRT-LLM/raw/main/docs/source/blogs/quantization-in-TRT-LLM.md"&gt;Speed up inference with Model Optimizer quantization techniques in TRT-LLM&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;Install&lt;/h2&gt; 
&lt;p&gt;To install stable release packages for Model Optimizer with &lt;code&gt;pip&lt;/code&gt; from &lt;a href="https://pypi.org/project/nvidia-modelopt/"&gt;PyPI&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install nvidia-modelopt[all]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To install from source in editable mode with all development dependencies or to test the latest changes, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Clone the Model Optimizer repository
git clone https://github.com/NVIDIA/TensorRT-Model-Optimizer.git
cd TensorRT-Model-Optimizer

pip install -e .[dev]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Visit our &lt;a href="https://nvidia.github.io/TensorRT-Model-Optimizer/getting_started/2_installation.html"&gt;installation guide&lt;/a&gt; for more fine-grained control on installed dependencies or view our pre-made &lt;a href="https://raw.githubusercontent.com/NVIDIA/TensorRT-Model-Optimizer/main/docker/README.md"&gt;dockerfiles&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h2&gt;Techniques&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;Technique&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;Examples&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;Docs&lt;/strong&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;Post Training Quantization&lt;/td&gt; 
    &lt;td align="center"&gt;Compress model size by 2x-4x, speeding up inference while preserving model quality!&lt;/td&gt; 
    &lt;td align="center"&gt;[&lt;a href="https://raw.githubusercontent.com/NVIDIA/TensorRT-Model-Optimizer/main/examples/llm_ptq/"&gt;LLMs&lt;/a&gt;] [&lt;a href="https://raw.githubusercontent.com/NVIDIA/TensorRT-Model-Optimizer/main/examples/diffusers/"&gt;diffusers&lt;/a&gt;] [&lt;a href="https://raw.githubusercontent.com/NVIDIA/TensorRT-Model-Optimizer/main/examples/vlm_ptq/"&gt;VLMs&lt;/a&gt;] [&lt;a href="https://raw.githubusercontent.com/NVIDIA/TensorRT-Model-Optimizer/main/examples/onnx_ptq/"&gt;onnx&lt;/a&gt;] [&lt;a href="https://raw.githubusercontent.com/NVIDIA/TensorRT-Model-Optimizer/main/examples/windows/"&gt;windows&lt;/a&gt;]&lt;/td&gt; 
    &lt;td align="center"&gt;[&lt;a href="https://nvidia.github.io/TensorRT-Model-Optimizer/guides/1_quantization.html"&gt;docs&lt;/a&gt;]&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;Quantization Aware Training&lt;/td&gt; 
    &lt;td align="center"&gt;Refine accuracy even further with a few training steps!&lt;/td&gt; 
    &lt;td align="center"&gt;[&lt;a href="https://raw.githubusercontent.com/NVIDIA/TensorRT-Model-Optimizer/main/examples/llm_qat#nemo-qatqad-simplified-flow-example"&gt;NeMo&lt;/a&gt;] [&lt;a href="https://raw.githubusercontent.com/NVIDIA/TensorRT-Model-Optimizer/main/examples/llm_qat/"&gt;Hugging Face&lt;/a&gt;]&lt;/td&gt; 
    &lt;td align="center"&gt;[&lt;a href="https://nvidia.github.io/TensorRT-Model-Optimizer/guides/1_quantization.html"&gt;docs&lt;/a&gt;]&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;Pruning&lt;/td&gt; 
    &lt;td align="center"&gt;Reduce your model size and accelerate inference by removing unnecessary weights!&lt;/td&gt; 
    &lt;td align="center"&gt;[&lt;a href="https://raw.githubusercontent.com/NVIDIA/TensorRT-Model-Optimizer/main/examples/pruning/"&gt;PyTorch&lt;/a&gt;]&lt;/td&gt; 
    &lt;td align="center"&gt;[&lt;a href="https://nvidia.github.io/TensorRT-Model-Optimizer/guides/3_pruning.html"&gt;docs&lt;/a&gt;]&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;Distillation&lt;/td&gt; 
    &lt;td align="center"&gt;Reduce deployment model size by teaching small models to behave like larger models!&lt;/td&gt; 
    &lt;td align="center"&gt;[&lt;a href="https://raw.githubusercontent.com/NVIDIA/TensorRT-Model-Optimizer/main/examples/llm_distill#knowledge-distillation-kd-for-nvidia-nemo-models"&gt;NeMo&lt;/a&gt;] [&lt;a href="https://raw.githubusercontent.com/NVIDIA/TensorRT-Model-Optimizer/main/examples/llm_distill/"&gt;Hugging Face&lt;/a&gt;]&lt;/td&gt; 
    &lt;td align="center"&gt;[&lt;a href="https://nvidia.github.io/TensorRT-Model-Optimizer/guides/4_distillation.html"&gt;docs&lt;/a&gt;]&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;Speculative Decoding&lt;/td&gt; 
    &lt;td align="center"&gt;Train draft modules to predict extra tokens during inference!&lt;/td&gt; 
    &lt;td align="center"&gt;[&lt;a href="https://raw.githubusercontent.com/NVIDIA/TensorRT-Model-Optimizer/main/examples/speculative_decoding#mlm-example"&gt;Megatron&lt;/a&gt;] [&lt;a href="https://raw.githubusercontent.com/NVIDIA/TensorRT-Model-Optimizer/main/examples/speculative_decoding/"&gt;Hugging Face&lt;/a&gt;]&lt;/td&gt; 
    &lt;td align="center"&gt;[&lt;a href="https://nvidia.github.io/TensorRT-Model-Optimizer/guides/5_speculative_decoding.html"&gt;docs&lt;/a&gt;]&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;Sparsity&lt;/td&gt; 
    &lt;td align="center"&gt;Efficiently compress your model by storing only its non-zero parameter values and their locations&lt;/td&gt; 
    &lt;td align="center"&gt;[&lt;a href="https://raw.githubusercontent.com/NVIDIA/TensorRT-Model-Optimizer/main/examples/llm_sparsity/"&gt;PyTorch&lt;/a&gt;]&lt;/td&gt; 
    &lt;td align="center"&gt;[&lt;a href="https://nvidia.github.io/TensorRT-Model-Optimizer/guides/6_sparsity.html"&gt;docs&lt;/a&gt;]&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h2&gt;Pre-Quantized Checkpoints&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Ready-to-deploy checkpoints [&lt;a href="https://huggingface.co/collections/nvidia/model-optimizer-66aa84f7966b3150262481a4"&gt;ğŸ¤— Hugging Face - Nvidia TensorRT Model Optimizer Collection&lt;/a&gt;]&lt;/li&gt; 
 &lt;li&gt;Deployable on &lt;a href="https://github.com/NVIDIA/TensorRT-LLM"&gt;TensorRT-LLM&lt;/a&gt;, &lt;a href="https://github.com/vllm-project/vllm"&gt;vLLM&lt;/a&gt; and &lt;a href="https://github.com/sgl-project/sglang"&gt;SGLang&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;More models coming soon!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Resources&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“… &lt;a href="https://github.com/NVIDIA/TensorRT-Model-Optimizer/issues/146"&gt;Roadmap&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸ“– &lt;a href="https://nvidia.github.io/TensorRT-Model-Optimizer"&gt;Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸ¯ &lt;a href="https://raw.githubusercontent.com/NVIDIA/TensorRT-Model-Optimizer/main/examples/benchmark.md"&gt;Benchmarks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸ’¡ &lt;a href="https://nvidia.github.io/TensorRT-Model-Optimizer/reference/0_changelog.html"&gt;Release Notes&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸ› &lt;a href="https://github.com/NVIDIA/TensorRT-Model-Optimizer/issues/new?template=1_bug_report.md"&gt;File a bug&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;âœ¨ &lt;a href="https://github.com/NVIDIA/TensorRT-Model-Optimizer/issues/new?template=2_feature_request.md"&gt;File a Feature Request&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Model Support Matrix&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model Type&lt;/th&gt; 
   &lt;th&gt;Support Matrix&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;LLM Quantization&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/TensorRT-Model-Optimizer/main/examples/llm_ptq/README.md#support-matrix"&gt;View Support Matrix&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Diffusers Quantization&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/TensorRT-Model-Optimizer/main/examples/diffusers/README.md#support-matrix"&gt;View Support Matrix&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;VLM Quantization&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/TensorRT-Model-Optimizer/main/examples/vlm_ptq/README.md#support-matrix"&gt;View Support Matrix&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ONNX Quantization&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/TensorRT-Model-Optimizer/main/examples/onnx_ptq/README.md#onnx-export-supported-llm-models"&gt;View Support Matrix&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Windows Quantization&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/TensorRT-Model-Optimizer/main/examples/windows/README.md#support-matrix"&gt;View Support Matrix&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Quantization Aware Training&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/TensorRT-Model-Optimizer/main/examples/llm_qat/README.md#support-matrix"&gt;View Support Matrix&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Pruning&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/TensorRT-Model-Optimizer/main/examples/pruning/README.md#support-matrix"&gt;View Support Matrix&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Distillation&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/TensorRT-Model-Optimizer/main/examples/llm_distill/README.md#support-matrix"&gt;View Support Matrix&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Speculative Decoding&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA/TensorRT-Model-Optimizer/main/examples/speculative_decoding/README.md#support-matrix"&gt;View Support Matrix&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Model Optimizer is now open source! We welcome any feedback, feature requests and PRs. Please read our &lt;a href="https://raw.githubusercontent.com/NVIDIA/TensorRT-Model-Optimizer/main/CONTRIBUTING.md"&gt;Contributing&lt;/a&gt; guidelines for details on how to contribute to this project.&lt;/p&gt; 
&lt;h3&gt;Top Contributors&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/NVIDIA/TensorRT-Model-Optimizer/graphs/contributors"&gt;&lt;img src="https://contrib.rocks/image?repo=NVIDIA/TensorRT-Model-Optimizer" alt="Contributors" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Happy optimizing!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>HKUDS/AutoAgent</title>
      <link>https://github.com/HKUDS/AutoAgent</link>
      <description>&lt;p&gt;"AutoAgent: Fully-Automated and Zero-Code LLM Agent Framework"&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a name="readme-top"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/AutoAgent_logo.svg?sanitize=true" alt="Logo" width="200" /&gt; 
 &lt;h1 align="center"&gt;AutoAgent: Fully-Automated &amp;amp; Zero-Code&lt;br /&gt; LLM Agent Framework &lt;/h1&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://autoagent-ai.github.io"&gt;&lt;img src="https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&amp;amp;color=FFE165&amp;amp;logo=homepage&amp;amp;logoColor=white" alt="Credits" /&gt;&lt;/a&gt; 
 &lt;a href="https://join.slack.com/t/metachain-workspace/shared_invite/zt-2zibtmutw-v7xOJObBf9jE2w3x7nctFQ"&gt;&lt;img src="https://img.shields.io/badge/Slack-Join%20Us-red?logo=slack&amp;amp;logoColor=white&amp;amp;style=for-the-badge" alt="Join our Slack community" /&gt;&lt;/a&gt; 
 &lt;a href="https://discord.gg/jQJdXyDB"&gt;&lt;img src="https://img.shields.io/badge/Discord-Join%20Us-purple?logo=discord&amp;amp;logoColor=white&amp;amp;style=for-the-badge" alt="Join our Discord community" /&gt;&lt;/a&gt; 
 &lt;a href="https://github.com/HKUDS/AutoAgent/raw/main/assets/autoagent-wechat.jpg"&gt;&lt;img src="https://img.shields.io/badge/Wechat-Join%20Us-green?logo=wechat&amp;amp;logoColor=white&amp;amp;style=for-the-badge" alt="Join our Wechat community" /&gt;&lt;/a&gt; 
 &lt;br /&gt; 
 &lt;a href="https://autoagent-ai.github.io/docs"&gt;&lt;img src="https://img.shields.io/badge/Documentation-000?logo=googledocs&amp;amp;logoColor=FFE165&amp;amp;style=for-the-badge" alt="Check out the documentation" /&gt;&lt;/a&gt; 
 &lt;a href="https://arxiv.org/abs/2502.05957"&gt;&lt;img src="https://img.shields.io/badge/Paper%20on%20Arxiv-000?logoColor=FFE165&amp;amp;logo=arxiv&amp;amp;style=for-the-badge" alt="Paper" /&gt;&lt;/a&gt; 
 &lt;a href="https://gaia-benchmark-leaderboard.hf.space/"&gt;&lt;img src="https://img.shields.io/badge/GAIA%20Benchmark-000?logoColor=FFE165&amp;amp;logo=huggingface&amp;amp;style=for-the-badge" alt="Evaluation Benchmark Score" /&gt;&lt;/a&gt; 
 &lt;hr /&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://trendshift.io/repositories/13954" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/13954" alt="HKUDS%2FAutoAgent | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;p&gt;Welcome to AutoAgent! AutoAgent is a &lt;strong&gt;Fully-Automated&lt;/strong&gt; and highly &lt;strong&gt;Self-Developing&lt;/strong&gt; framework that enables users to create and deploy LLM agents through &lt;strong&gt;Natural Language Alone&lt;/strong&gt;.&lt;/p&gt; 
&lt;h2&gt;âœ¨Key Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;ğŸ† Top Performers on the GAIA Benchmark &lt;br /&gt;AutoAgent has delivering comparable performance to many &lt;strong&gt;Deep Research Agents&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;âœ¨ Agent and Workflow Create with Ease &lt;br /&gt;AutoAgent leverages natural language to effortlessly build ready-to-use &lt;strong&gt;tools&lt;/strong&gt;, &lt;strong&gt;agents&lt;/strong&gt; and &lt;strong&gt;workflows&lt;/strong&gt; - no coding required.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;ğŸ“š Agentic-RAG with Native Self-Managing Vector Database &lt;br /&gt;AutoAgent equipped with a native self-managing vector database, outperforms industry-leading solutions like &lt;strong&gt;LangChain&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;ğŸŒ Universal LLM Support &lt;br /&gt;AutoAgent seamlessly integrates with &lt;strong&gt;A Wide Range&lt;/strong&gt; of LLMs (e.g., OpenAI, Anthropic, Deepseek, vLLM, Grok, Huggingface ...)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;ğŸ”€ Flexible Interaction &lt;br /&gt;Benefit from support for both &lt;strong&gt;function-calling&lt;/strong&gt; and &lt;strong&gt;ReAct&lt;/strong&gt; interaction modes.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;ğŸ¤– Dynamic, Extensible, Lightweight &lt;br /&gt;AutoAgent is your &lt;strong&gt;Personal AI Assistant&lt;/strong&gt;, designed to be dynamic, extensible, customized, and lightweight.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;ğŸš€ Unlock the Future of LLM Agents. Try ğŸ”¥AutoAgentğŸ”¥ Now!&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;!-- &lt;img src="./assets/AutoAgentnew-intro.pdf" alt="Logo" width="100%"&gt; --&gt; 
 &lt;figure&gt; 
  &lt;img src="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/autoagent-intro.svg?sanitize=true" alt="Logo" style="max-width: 100%; height: auto;" /&gt; 
  &lt;figcaption&gt;
   &lt;em&gt;Quick Overview of AutoAgent.&lt;/em&gt;
  &lt;/figcaption&gt; 
 &lt;/figure&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸ”¥ News&lt;/h2&gt; 
&lt;div class="scrollable"&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;[2025, Feb 17]&lt;/strong&gt;: &amp;nbsp;ğŸ‰ğŸ‰We've updated and released AutoAgent v0.2.0 (formerly known as MetaChain). Detailed changes include: 1) fix the bug of different LLM providers from issues; 2) add automatic installation of AutoAgent in the container environment according to issues; 3) add more easy-to-use commands for the CLI mode. 4) Rename the project to AutoAgent for better understanding.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;[2025, Feb 10]&lt;/strong&gt;: &amp;nbsp;ğŸ‰ğŸ‰We've released &lt;b&gt;MetaChain!&lt;/b&gt;, including framework, evaluation codes and CLI mode! Check our &lt;a href="https://arxiv.org/abs/2502.05957"&gt;paper&lt;/a&gt; for more details.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/div&gt; 
&lt;span id="table-of-contents"&gt;&lt;/span&gt; 
&lt;h2&gt;ğŸ“‘ Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#features"&gt;âœ¨ Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#news"&gt;ğŸ”¥ News&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#how-to-use"&gt;ğŸ” How to Use AutoAgent&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#user-mode"&gt;1. &lt;code&gt;user mode&lt;/code&gt; (SOTA ğŸ† Open Deep Research)&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#agent-editor"&gt;2. &lt;code&gt;agent editor&lt;/code&gt; (Agent Creation without Workflow)&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#workflow-editor"&gt;3. &lt;code&gt;workflow editor&lt;/code&gt; (Agent Creation with Workflow)&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#quick-start"&gt;âš¡ Quick Start&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#api-keys-setup"&gt;API Keys Setup&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#start-with-cli-mode"&gt;Start with CLI Mode&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#todo"&gt;â˜‘ï¸ Todo List&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#reproduce"&gt;ğŸ”¬ How To Reproduce the Results in the Paper&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#documentation"&gt;ğŸ“– Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#community"&gt;ğŸ¤ Join the Community&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#acknowledgements"&gt;ğŸ™ Acknowledgements&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/#cite"&gt;ğŸŒŸ Cite&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;span id="how-to-use"&gt;&lt;/span&gt; 
&lt;h2&gt;ğŸ” How to Use AutoAgent&lt;/h2&gt; 
&lt;span id="user-mode"&gt;&lt;/span&gt; 
&lt;h3&gt;1. &lt;code&gt;user mode&lt;/code&gt; (SOTA ğŸ† Open Deep Research)&lt;/h3&gt; 
&lt;p&gt;AutoAgent have an out-of-the-box multi-agent system, which you could choose &lt;code&gt;user mode&lt;/code&gt; in the start page to use it. This multi-agent system is a general AI assistant, having the same functionality with &lt;strong&gt;OpenAI's Deep Research&lt;/strong&gt; and the comparable performance with it in &lt;a href="https://gaia-benchmark-leaderboard.hf.space/"&gt;GAIA&lt;/a&gt; benchmark.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸš€ &lt;strong&gt;High Performance&lt;/strong&gt;: Matches Deep Research using Claude 3.5 rather than OpenAI's o3 model.&lt;/li&gt; 
 &lt;li&gt;ğŸ”„ &lt;strong&gt;Model Flexibility&lt;/strong&gt;: Compatible with any LLM (including Deepseek-R1, Grok, Gemini, etc.)&lt;/li&gt; 
 &lt;li&gt;ğŸ’° &lt;strong&gt;Cost-Effective&lt;/strong&gt;: Open-source alternative to Deep Research's $200/month subscription&lt;/li&gt; 
 &lt;li&gt;ğŸ¯ &lt;strong&gt;User-Friendly&lt;/strong&gt;: Easy-to-deploy CLI interface for seamless interaction&lt;/li&gt; 
 &lt;li&gt;ğŸ“ &lt;strong&gt;File Support&lt;/strong&gt;: Handles file uploads for enhanced data interaction&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align="center"&gt; 
 &lt;video width="80%" controls&gt; 
  &lt;source src="./assets/video_v1_compressed.mp4" type="video/mp4" /&gt; 
 &lt;/video&gt; 
 &lt;p&gt;&lt;em&gt;ğŸ¥ Deep Research (aka User Mode)&lt;/em&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;span id="agent-editor"&gt;&lt;/span&gt; 
&lt;h3&gt;2. &lt;code&gt;agent editor&lt;/code&gt; (Agent Creation without Workflow)&lt;/h3&gt; 
&lt;p&gt;The most distinctive feature of AutoAgent is its natural language customization capability. Unlike other agent frameworks, AutoAgent allows you to create tools, agents, and workflows using natural language alone. Simply choose &lt;code&gt;agent editor&lt;/code&gt; or &lt;code&gt;workflow editor&lt;/code&gt; mode to start your journey of building agents through conversations.&lt;/p&gt; 
&lt;p&gt;You can use &lt;code&gt;agent editor&lt;/code&gt; as shown in the following figure.&lt;/p&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr align="center"&gt; 
   &lt;td width="33%"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/agent_editor/1-requirement.png" alt="requirement" width="100%" /&gt; &lt;br /&gt; &lt;em&gt;Input what kind of agent you want to create.&lt;/em&gt; &lt;/td&gt; 
   &lt;td width="33%"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/agent_editor/2-profiling.png" alt="profiling" width="100%" /&gt; &lt;br /&gt; &lt;em&gt;Automated agent profiling.&lt;/em&gt; &lt;/td&gt; 
   &lt;td width="33%"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/agent_editor/3-profiles.png" alt="profiles" width="100%" /&gt; &lt;br /&gt; &lt;em&gt;Output the agent profiles.&lt;/em&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr align="center"&gt; 
   &lt;td width="33%"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/agent_editor/4-tools.png" alt="tools" width="100%" /&gt; &lt;br /&gt; &lt;em&gt;Create the desired tools.&lt;/em&gt; &lt;/td&gt; 
   &lt;td width="33%"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/agent_editor/5-task.png" alt="task" width="100%" /&gt; &lt;br /&gt; &lt;em&gt;Input what do you want to complete with the agent. (Optional)&lt;/em&gt; &lt;/td&gt; 
   &lt;td width="33%"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/agent_editor/6-output-next.png" alt="output" width="100%" /&gt; &lt;br /&gt; &lt;em&gt;Create the desired agent(s) and go to the next step.&lt;/em&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;span id="workflow-editor"&gt;&lt;/span&gt; 
&lt;h3&gt;3. &lt;code&gt;workflow editor&lt;/code&gt; (Agent Creation with Workflow)&lt;/h3&gt; 
&lt;p&gt;You can also create the agent workflows using natural language description with the &lt;code&gt;workflow editor&lt;/code&gt; mode, as shown in the following figure. (Tips: this mode does not support tool creation temporarily.)&lt;/p&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr align="center"&gt; 
   &lt;td width="33%"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/workflow_editor/1-requirement.png" alt="requirement" width="100%" /&gt; &lt;br /&gt; &lt;em&gt;Input what kind of workflow you want to create.&lt;/em&gt; &lt;/td&gt; 
   &lt;td width="33%"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/workflow_editor/2-profiling.png" alt="profiling" width="100%" /&gt; &lt;br /&gt; &lt;em&gt;Automated workflow profiling.&lt;/em&gt; &lt;/td&gt; 
   &lt;td width="33%"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/workflow_editor/3-profiles.png" alt="profiles" width="100%" /&gt; &lt;br /&gt; &lt;em&gt;Output the workflow profiles.&lt;/em&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr align="center"&gt; 
   &lt;td width="33%"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/workflow_editor/4-task.png" alt="task" width="66%" /&gt; &lt;br /&gt; &lt;em&gt;Input what do you want to complete with the workflow. (Optional)&lt;/em&gt; &lt;/td&gt; 
   &lt;td width="33%"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/workflow_editor/5-output-next.png" alt="output" width="66%" /&gt; &lt;br /&gt; &lt;em&gt;Create the desired workflow(s) and go to the next step.&lt;/em&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;span id="quick-start"&gt;&lt;/span&gt; 
&lt;h2&gt;âš¡ Quick Start&lt;/h2&gt; 
&lt;span id="installation"&gt;&lt;/span&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;h4&gt;AutoAgent Installation&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/HKUDS/AutoAgent.git
cd AutoAgent
pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Docker Installation&lt;/h4&gt; 
&lt;p&gt;We use Docker to containerize the agent-interactive environment. So please install &lt;a href="https://www.docker.com/"&gt;Docker&lt;/a&gt; first. You don't need to manually pull the pre-built image, because we have let Auto-Deep-Research &lt;strong&gt;automatically pull the pre-built image based on your architecture of your machine&lt;/strong&gt;.&lt;/p&gt; 
&lt;span id="api-keys-setup"&gt;&lt;/span&gt; 
&lt;h3&gt;API Keys Setup&lt;/h3&gt; 
&lt;p&gt;Create an environment variable file, just like &lt;code&gt;.env.template&lt;/code&gt;, and set the API keys for the LLMs you want to use. Not every LLM API Key is required, use what you need.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Required Github Tokens of your own
GITHUB_AI_TOKEN=

# Optional API Keys
OPENAI_API_KEY=
DEEPSEEK_API_KEY=
ANTHROPIC_API_KEY=
GEMINI_API_KEY=
HUGGINGFACE_API_KEY=
GROQ_API_KEY=
XAI_API_KEY=
&lt;/code&gt;&lt;/pre&gt; 
&lt;span id="start-with-cli-mode"&gt;&lt;/span&gt; 
&lt;h3&gt;Start with CLI Mode&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[ğŸš¨ &lt;strong&gt;News&lt;/strong&gt;: ] We have updated a more easy-to-use command to start the CLI mode and fix the bug of different LLM providers from issues. You can follow the following steps to start the CLI mode with different LLM providers with much less configuration.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Command Options:&lt;/h4&gt; 
&lt;p&gt;You can run &lt;code&gt;auto main&lt;/code&gt; to start full part of AutoAgent, including &lt;code&gt;user mode&lt;/code&gt;, &lt;code&gt;agent editor&lt;/code&gt; and &lt;code&gt;workflow editor&lt;/code&gt;. Btw, you can also run &lt;code&gt;auto deep-research&lt;/code&gt; to start more lightweight &lt;code&gt;user mode&lt;/code&gt;, just like the &lt;a href="https://github.com/HKUDS/Auto-Deep-Research"&gt;Auto-Deep-Research&lt;/a&gt; project. Some configuration of this command is shown below.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--container_name&lt;/code&gt;: Name of the Docker container (default: 'deepresearch')&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--port&lt;/code&gt;: Port for the container (default: 12346)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;COMPLETION_MODEL&lt;/code&gt;: Specify the LLM model to use, you should follow the name of &lt;a href="https://github.com/BerriAI/litellm"&gt;Litellm&lt;/a&gt; to set the model name. (Default: &lt;code&gt;claude-3-5-sonnet-20241022&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;DEBUG&lt;/code&gt;: Enable debug mode for detailed logs (default: False)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;API_BASE_URL&lt;/code&gt;: The base URL for the LLM provider (default: None)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;FN_CALL&lt;/code&gt;: Enable function calling (default: None). Most of time, you could ignore this option because we have already set the default value based on the model name.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;git_clone&lt;/code&gt;: Clone the AutoAgent repository to the local environment (only support with the &lt;code&gt;auto main&lt;/code&gt; command, default: True)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;test_pull_name&lt;/code&gt;: The name of the test pull. (only support with the &lt;code&gt;auto main&lt;/code&gt; command, default: 'autoagent_mirror')&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;More details about &lt;code&gt;git_clone&lt;/code&gt; and &lt;code&gt;test_pull_name&lt;/code&gt;]&lt;/h4&gt; 
&lt;p&gt;In the &lt;code&gt;agent editor&lt;/code&gt; and &lt;code&gt;workflow editor&lt;/code&gt; mode, we should clone a mirror of the AutoAgent repository to the local agent-interactive environment and let our &lt;strong&gt;AutoAgent&lt;/strong&gt; automatically update the AutoAgent itself, such as creating new tools, agents and workflows. So if you want to use the &lt;code&gt;agent editor&lt;/code&gt; and &lt;code&gt;workflow editor&lt;/code&gt; mode, you should set the &lt;code&gt;git_clone&lt;/code&gt; to True and set the &lt;code&gt;test_pull_name&lt;/code&gt; to 'autoagent_mirror' or other branches.&lt;/p&gt; 
&lt;h4&gt;&lt;code&gt;auto main&lt;/code&gt; with different LLM Providers&lt;/h4&gt; 
&lt;p&gt;Then I will show you how to use the full part of AutoAgent with the &lt;code&gt;auto main&lt;/code&gt; command and different LLM providers. If you want to use the &lt;code&gt;auto deep-research&lt;/code&gt; command, you can refer to the &lt;a href="https://github.com/HKUDS/Auto-Deep-Research"&gt;Auto-Deep-Research&lt;/a&gt; project for more details.&lt;/p&gt; 
&lt;h5&gt;Anthropic&lt;/h5&gt; 
&lt;ul&gt; 
 &lt;li&gt;set the &lt;code&gt;ANTHROPIC_API_KEY&lt;/code&gt; in the &lt;code&gt;.env&lt;/code&gt; file.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;ANTHROPIC_API_KEY=your_anthropic_api_key
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;run the following command to start Auto-Deep-Research.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;auto main # default model is claude-3-5-sonnet-20241022
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;OpenAI&lt;/h5&gt; 
&lt;ul&gt; 
 &lt;li&gt;set the &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; in the &lt;code&gt;.env&lt;/code&gt; file.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;OPENAI_API_KEY=your_openai_api_key
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;run the following command to start Auto-Deep-Research.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;COMPLETION_MODEL=gpt-4o auto main
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;Mistral&lt;/h5&gt; 
&lt;ul&gt; 
 &lt;li&gt;set the &lt;code&gt;MISTRAL_API_KEY&lt;/code&gt; in the &lt;code&gt;.env&lt;/code&gt; file.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;MISTRAL_API_KEY=your_mistral_api_key
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;run the following command to start Auto-Deep-Research.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;COMPLETION_MODEL=mistral/mistral-large-2407 auto main
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;Gemini - Google AI Studio&lt;/h5&gt; 
&lt;ul&gt; 
 &lt;li&gt;set the &lt;code&gt;GEMINI_API_KEY&lt;/code&gt; in the &lt;code&gt;.env&lt;/code&gt; file.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;GEMINI_API_KEY=your_gemini_api_key
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;run the following command to start Auto-Deep-Research.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;COMPLETION_MODEL=gemini/gemini-2.0-flash auto main
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;Huggingface&lt;/h5&gt; 
&lt;ul&gt; 
 &lt;li&gt;set the &lt;code&gt;HUGGINGFACE_API_KEY&lt;/code&gt; in the &lt;code&gt;.env&lt;/code&gt; file.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;HUGGINGFACE_API_KEY=your_huggingface_api_key
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;run the following command to start Auto-Deep-Research.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;COMPLETION_MODEL=huggingface/meta-llama/Llama-3.3-70B-Instruct auto main
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;Groq&lt;/h5&gt; 
&lt;ul&gt; 
 &lt;li&gt;set the &lt;code&gt;GROQ_API_KEY&lt;/code&gt; in the &lt;code&gt;.env&lt;/code&gt; file.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;GROQ_API_KEY=your_groq_api_key
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;run the following command to start Auto-Deep-Research.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;COMPLETION_MODEL=groq/deepseek-r1-distill-llama-70b auto main
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;OpenAI-Compatible Endpoints (e.g., Grok)&lt;/h5&gt; 
&lt;ul&gt; 
 &lt;li&gt;set the &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; in the &lt;code&gt;.env&lt;/code&gt; file.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;OPENAI_API_KEY=your_api_key_for_openai_compatible_endpoints
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;run the following command to start Auto-Deep-Research.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;COMPLETION_MODEL=openai/grok-2-latest API_BASE_URL=https://api.x.ai/v1 auto main
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;OpenRouter (e.g., DeepSeek-R1)&lt;/h5&gt; 
&lt;p&gt;We recommend using OpenRouter as LLM provider of DeepSeek-R1 temporarily. Because official API of DeepSeek-R1 can not be used efficiently.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;set the &lt;code&gt;OPENROUTER_API_KEY&lt;/code&gt; in the &lt;code&gt;.env&lt;/code&gt; file.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;OPENROUTER_API_KEY=your_openrouter_api_key
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;run the following command to start Auto-Deep-Research.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;COMPLETION_MODEL=openrouter/deepseek/deepseek-r1 auto main
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;DeepSeek&lt;/h5&gt; 
&lt;ul&gt; 
 &lt;li&gt;set the &lt;code&gt;DEEPSEEK_API_KEY&lt;/code&gt; in the &lt;code&gt;.env&lt;/code&gt; file.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;DEEPSEEK_API_KEY=your_deepseek_api_key
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;run the following command to start Auto-Deep-Research.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;COMPLETION_MODEL=deepseek/deepseek-chat auto main
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After the CLI mode is started, you can see the start page of AutoAgent:&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;!-- &lt;img src="./assets/AutoAgentnew-intro.pdf" alt="Logo" width="100%"&gt; --&gt; 
 &lt;figure&gt; 
  &lt;img src="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/assets/cover.png" alt="Logo" style="max-width: 100%; height: auto;" /&gt; 
  &lt;figcaption&gt;
   &lt;em&gt;Start Page of AutoAgent.&lt;/em&gt;
  &lt;/figcaption&gt; 
 &lt;/figure&gt; 
&lt;/div&gt; 
&lt;h3&gt;Tips&lt;/h3&gt; 
&lt;h4&gt;Import browser cookies to browser environment&lt;/h4&gt; 
&lt;p&gt;You can import the browser cookies to the browser environment to let the agent better access some specific websites. For more details, please refer to the &lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/AutoAgent/environment/cookie_json/README.md"&gt;cookies&lt;/a&gt; folder.&lt;/p&gt; 
&lt;h4&gt;Add your own API keys for third-party Tool Platforms&lt;/h4&gt; 
&lt;p&gt;If you want to create tools from the third-party tool platforms, such as RapidAPI, you should subscribe tools from the platform and add your own API keys by running &lt;a href="https://raw.githubusercontent.com/HKUDS/AutoAgent/main/process_tool_docs.py"&gt;process_tool_docs.py&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python process_tool_docs.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;More features coming soon! ğŸš€ &lt;strong&gt;Web GUI interface&lt;/strong&gt; under development.&lt;/p&gt; 
&lt;span id="todo"&gt;&lt;/span&gt; 
&lt;h2&gt;â˜‘ï¸ Todo List&lt;/h2&gt; 
&lt;p&gt;AutoAgent is continuously evolving! Here's what's coming:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ“Š &lt;strong&gt;More Benchmarks&lt;/strong&gt;: Expanding evaluations to &lt;strong&gt;SWE-bench&lt;/strong&gt;, &lt;strong&gt;WebArena&lt;/strong&gt;, and more&lt;/li&gt; 
 &lt;li&gt;ğŸ–¥ï¸ &lt;strong&gt;GUI Agent&lt;/strong&gt;: Supporting &lt;em&gt;Computer-Use&lt;/em&gt; agents with GUI interaction&lt;/li&gt; 
 &lt;li&gt;ğŸ”§ &lt;strong&gt;Tool Platforms&lt;/strong&gt;: Integration with more platforms like &lt;strong&gt;Composio&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸ—ï¸ &lt;strong&gt;Code Sandboxes&lt;/strong&gt;: Supporting additional environments like &lt;strong&gt;E2B&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸ¨ &lt;strong&gt;Web Interface&lt;/strong&gt;: Developing comprehensive GUI for better user experience&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Have ideas or suggestions? Feel free to open an issue! Stay tuned for more exciting updates! ğŸš€&lt;/p&gt; 
&lt;span id="reproduce"&gt;&lt;/span&gt; 
&lt;h2&gt;ğŸ”¬ How To Reproduce the Results in the Paper&lt;/h2&gt; 
&lt;h3&gt;GAIA Benchmark&lt;/h3&gt; 
&lt;p&gt;For the GAIA benchmark, you can run the following command to run the inference.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd path/to/AutoAgent &amp;amp;&amp;amp; sh evaluation/gaia/scripts/run_infer.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For the evaluation, you can run the following command.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd path/to/AutoAgent &amp;amp;&amp;amp; python evaluation/gaia/get_score.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Agentic-RAG&lt;/h3&gt; 
&lt;p&gt;For the Agentic-RAG task, you can run the following command to run the inference.&lt;/p&gt; 
&lt;p&gt;Step1. Turn to &lt;a href="https://huggingface.co/datasets/yixuantt/MultiHopRAG"&gt;this page&lt;/a&gt; and download it. Save them to your datapath.&lt;/p&gt; 
&lt;p&gt;Step2. Run the following command to run the inference.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd path/to/AutoAgent &amp;amp;&amp;amp; sh evaluation/multihoprag/scripts/run_rag.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Step3. The result will be saved in the &lt;code&gt;evaluation/multihoprag/result.json&lt;/code&gt;.&lt;/p&gt; 
&lt;span id="documentation"&gt;&lt;/span&gt; 
&lt;h2&gt;ğŸ“– Documentation&lt;/h2&gt; 
&lt;p&gt;A more detailed documentation is coming soon ğŸš€, and we will update in the &lt;a href="https://AutoAgent-ai.github.io/docs"&gt;Documentation&lt;/a&gt; page.&lt;/p&gt; 
&lt;span id="community"&gt;&lt;/span&gt; 
&lt;h2&gt;ğŸ¤ Join the Community&lt;/h2&gt; 
&lt;p&gt;We want to build a community for AutoAgent, and we welcome everyone to join us. You can join our community by:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://join.slack.com/t/AutoAgent-workspace/shared_invite/zt-2zibtmutw-v7xOJObBf9jE2w3x7nctFQ"&gt;Join our Slack workspace&lt;/a&gt; - Here we talk about research, architecture, and future development.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://discord.gg/z68KRvwB"&gt;Join our Discord server&lt;/a&gt; - This is a community-run server for general discussion, questions, and feedback.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/HKUDS/AutoAgent/issues"&gt;Read or post Github Issues&lt;/a&gt; - Check out the issues we're working on, or add your own ideas.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;span id="acknowledgements"&gt;&lt;/span&gt; 
&lt;h2&gt;Misc&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/HKUDS/AutoAgent/stargazers"&gt;&lt;img src="https://reporoster.com/stars/HKUDS/AutoAgent" alt="Stargazers repo roster for @HKUDS/AutoAgent" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/HKUDS/AutoAgent/network/members"&gt;&lt;img src="https://reporoster.com/forks/HKUDS/AutoAgent" alt="Forkers repo roster for @HKUDS/AutoAgent" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://star-history.com/#HKUDS/AutoAgent&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=HKUDS/AutoAgent&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸ™ Acknowledgements&lt;/h2&gt; 
&lt;p&gt;Rome wasn't built in a day. AutoAgent stands on the shoulders of giants, and we are deeply grateful for the outstanding work that came before us. Our framework architecture draws inspiration from &lt;a href="https://github.com/openai/swarm"&gt;OpenAI Swarm&lt;/a&gt;, while our user mode's three-agent design benefits from &lt;a href="https://github.com/microsoft/autogen/tree/main/python/packages/autogen-magentic-one"&gt;Magentic-one&lt;/a&gt;'s insights. We've also learned from &lt;a href="https://github.com/All-Hands-AI/OpenHands"&gt;OpenHands&lt;/a&gt; for documentation structure and many other excellent projects for agent-environment interaction design, among others. We express our sincere gratitude and respect to all these pioneering works that have been instrumental in shaping AutoAgent.&lt;/p&gt; 
&lt;span id="cite"&gt;&lt;/span&gt; 
&lt;h2&gt;ğŸŒŸ Cite&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-tex"&gt;@misc{AutoAgent,
      title={{AutoAgent: A Fully-Automated and Zero-Code Framework for LLM Agents}},
      author={Jiabin Tang, Tianyu Fan, Chao Huang},
      year={2025},
      eprint={202502.05957},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2502.05957},
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>Cinnamon/kotaemon</title>
      <link>https://github.com/Cinnamon/kotaemon</link>
      <description>&lt;p&gt;An open-source RAG-based tool for chatting with your documents.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h1&gt;kotaemon&lt;/h1&gt; 
 &lt;p&gt;An open-source clean &amp;amp; customizable RAG UI for chatting with your documents. Built with both end users and developers in mind.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Cinnamon/kotaemon/main/docs/images/preview-graph.png" alt="Preview" /&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/11607" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/11607" alt="Cinnamon%2Fkotaemon | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://huggingface.co/spaces/cin-model/kotaemon"&gt;Live Demo #1&lt;/a&gt; | &lt;a href="https://huggingface.co/spaces/cin-model/kotaemon-demo"&gt;Live Demo #2&lt;/a&gt; | &lt;a href="https://cinnamon.github.io/kotaemon/online_install/"&gt;Online Install&lt;/a&gt; | &lt;a href="https://colab.research.google.com/drive/1eTfieec_UOowNizTJA1NjawBJH9y_1nn"&gt;Colab Notebook (Local RAG)&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://cinnamon.github.io/kotaemon/"&gt;User Guide&lt;/a&gt; | &lt;a href="https://cinnamon.github.io/kotaemon/development/"&gt;Developer Guide&lt;/a&gt; | &lt;a href="https://github.com/Cinnamon/kotaemon/issues"&gt;Feedback&lt;/a&gt; | &lt;a href="mailto:kotaemon.support@cinnamon.is"&gt;Contact&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://www.python.org/downloads/release/python-31013/"&gt;&lt;img src="https://img.shields.io/badge/python-3.10+-blue.svg?sanitize=true" alt="Python 3.10+" /&gt;&lt;/a&gt; &lt;a href="https://github.com/psf/black"&gt;&lt;img src="https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true" alt="Code style: black" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Cinnamon/kotaemon/pkgs/container/kotaemon" target="_blank"&gt; &lt;img src="https://img.shields.io/badge/docker_pull-kotaemon:latest-brightgreen" alt="docker pull ghcr.io/cinnamon/kotaemon:latest" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/github/downloads/Cinnamon/kotaemon/total.svg?label=downloads&amp;amp;color=blue" alt="download" /&gt; &lt;a href="https://huggingface.co/spaces/cin-model/kotaemon-demo"&gt;&lt;img src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue" /&gt;&lt;/a&gt; &lt;a href="https://hellogithub.com/en/repository/d3141471a0244d5798bc654982b263eb" target="_blank"&gt;&lt;img src="https://abroad.hellogithub.com/v1/widgets/recommend.svg?rid=d3141471a0244d5798bc654982b263eb&amp;amp;claim_uid=RLiD9UZ1rEHNaMf&amp;amp;theme=small" alt="Featuredï½œHelloGitHub" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;!-- start-intro --&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;This project serves as a functional RAG UI for both end users who want to do QA on their documents and developers who want to build their own RAG pipeline. &lt;br /&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yml"&gt;+----------------------------------------------------------------------------+
| End users: Those who use apps built with `kotaemon`.                       |
| (You use an app like the one in the demo above)                            |
|     +----------------------------------------------------------------+     |
|     | Developers: Those who built with `kotaemon`.                   |     |
|     | (You have `import kotaemon` somewhere in your project)         |     |
|     |     +----------------------------------------------------+     |     |
|     |     | Contributors: Those who make `kotaemon` better.    |     |     |
|     |     | (You make PR to this repo)                         |     |     |
|     |     +----------------------------------------------------+     |     |
|     +----------------------------------------------------------------+     |
+----------------------------------------------------------------------------+
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;For end users&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Clean &amp;amp; Minimalistic UI&lt;/strong&gt;: A user-friendly interface for RAG-based QA.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Support for Various LLMs&lt;/strong&gt;: Compatible with LLM API providers (OpenAI, AzureOpenAI, Cohere, etc.) and local LLMs (via &lt;code&gt;ollama&lt;/code&gt; and &lt;code&gt;llama-cpp-python&lt;/code&gt;).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Easy Installation&lt;/strong&gt;: Simple scripts to get you started quickly.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;For developers&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Framework for RAG Pipelines&lt;/strong&gt;: Tools to build your own RAG-based document QA pipeline.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Customizable UI&lt;/strong&gt;: See your RAG pipeline in action with the provided UI, built with &lt;a href="https://github.com/gradio-app/gradio"&gt;Gradio &lt;img src="https://img.shields.io/github/stars/gradio-app/gradio" /&gt;&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Gradio Theme&lt;/strong&gt;: If you use Gradio for development, check out our theme here: &lt;a href="https://github.com/lone17/kotaemon-gradio-theme"&gt;kotaemon-gradio-theme&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Host your own document QA (RAG) web-UI&lt;/strong&gt;: Support multi-user login, organize your files in private/public collections, collaborate and share your favorite chat with others.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Organize your LLM &amp;amp; Embedding models&lt;/strong&gt;: Support both local LLMs &amp;amp; popular API providers (OpenAI, Azure, Ollama, Groq).&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Hybrid RAG pipeline&lt;/strong&gt;: Sane default RAG pipeline with hybrid (full-text &amp;amp; vector) retriever and re-ranking to ensure best retrieval quality.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Multi-modal QA support&lt;/strong&gt;: Perform Question Answering on multiple documents with figures and tables support. Support multi-modal document parsing (selectable options on UI).&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Advanced citations with document preview&lt;/strong&gt;: By default the system will provide detailed citations to ensure the correctness of LLM answers. View your citations (incl. relevant score) directly in the &lt;em&gt;in-browser PDF viewer&lt;/em&gt; with highlights. Warning when retrieval pipeline return low relevant articles.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Support complex reasoning methods&lt;/strong&gt;: Use question decomposition to answer your complex/multi-hop question. Support agent-based reasoning with &lt;code&gt;ReAct&lt;/code&gt;, &lt;code&gt;ReWOO&lt;/code&gt; and other agents.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Configurable settings UI&lt;/strong&gt;: You can adjust most important aspects of retrieval &amp;amp; generation process on the UI (incl. prompts).&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Extensible&lt;/strong&gt;: Being built on Gradio, you are free to customize or add any UI elements as you like. Also, we aim to support multiple strategies for document indexing &amp;amp; retrieval. &lt;code&gt;GraphRAG&lt;/code&gt; indexing pipeline is provided as an example.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Cinnamon/kotaemon/main/docs/images/preview.png" alt="Preview" /&gt;&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;If you are not a developer and just want to use the app, please check out our easy-to-follow &lt;a href="https://cinnamon.github.io/kotaemon/"&gt;User Guide&lt;/a&gt;. Download the &lt;code&gt;.zip&lt;/code&gt; file from the &lt;a href="https://github.com/Cinnamon/kotaemon/releases/latest"&gt;latest release&lt;/a&gt; to get all the newest features and bug fixes.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;System requirements&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://www.python.org/downloads/"&gt;Python&lt;/a&gt; &amp;gt;= 3.10&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.docker.com/"&gt;Docker&lt;/a&gt;: optional, if you &lt;a href="https://raw.githubusercontent.com/Cinnamon/kotaemon/main/#with-docker-recommended"&gt;install with Docker&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.unstructured.io/open-source/installation/full-installation#full-installation"&gt;Unstructured&lt;/a&gt; if you want to process files other than &lt;code&gt;.pdf&lt;/code&gt;, &lt;code&gt;.html&lt;/code&gt;, &lt;code&gt;.mhtml&lt;/code&gt;, and &lt;code&gt;.xlsx&lt;/code&gt; documents. Installation steps differ depending on your operating system. Please visit the link and follow the specific instructions provided there.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;With Docker (recommended)&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;We support both &lt;code&gt;lite&lt;/code&gt; &amp;amp; &lt;code&gt;full&lt;/code&gt; version of Docker images. With &lt;code&gt;full&lt;/code&gt; version, the extra packages of &lt;code&gt;unstructured&lt;/code&gt; will be installed, which can support additional file types (&lt;code&gt;.doc&lt;/code&gt;, &lt;code&gt;.docx&lt;/code&gt;, ...) but the cost is larger docker image size. For most users, the &lt;code&gt;lite&lt;/code&gt; image should work well in most cases.&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;To use the &lt;code&gt;full&lt;/code&gt; version.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;docker run \
-e GRADIO_SERVER_NAME=0.0.0.0 \
-e GRADIO_SERVER_PORT=7860 \
-v ./ktem_app_data:/app/ktem_app_data \
-p 7860:7860 -it --rm \
ghcr.io/cinnamon/kotaemon:main-full
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;To use the &lt;code&gt;full&lt;/code&gt; version with bundled &lt;strong&gt;Ollama&lt;/strong&gt; for &lt;em&gt;local / private RAG&lt;/em&gt;.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# change image name to
docker run &amp;lt;...&amp;gt; ghcr.io/cinnamon/kotaemon:main-ollama
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;To use the &lt;code&gt;lite&lt;/code&gt; version.&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;pre&gt;&lt;code class="language-bash"&gt; # change image name to
 docker run &amp;lt;...&amp;gt; ghcr.io/cinnamon/kotaemon:main-lite
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;We currently support and test two platforms: &lt;code&gt;linux/amd64&lt;/code&gt; and &lt;code&gt;linux/arm64&lt;/code&gt; (for newer Mac). You can specify the platform by passing &lt;code&gt;--platform&lt;/code&gt; in the &lt;code&gt;docker run&lt;/code&gt; command. For example:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# To run docker with platform linux/arm64
docker run \
-e GRADIO_SERVER_NAME=0.0.0.0 \
-e GRADIO_SERVER_PORT=7860 \
-v ./ktem_app_data:/app/ktem_app_data \
-p 7860:7860 -it --rm \
--platform linux/arm64 \
ghcr.io/cinnamon/kotaemon:main-lite
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Once everything is set up correctly, you can go to &lt;code&gt;http://localhost:7860/&lt;/code&gt; to access the WebUI.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;We use &lt;a href="https://docs.github.com/en/packages/working-with-a-github-packages-registry/working-with-the-container-registry"&gt;GHCR&lt;/a&gt; to store docker images, all images can be found &lt;a href="https://github.com/Cinnamon/kotaemon/pkgs/container/kotaemon"&gt;here.&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Without Docker&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Clone and install required packages on a fresh python environment.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;# optional (setup env)
conda create -n kotaemon python=3.10
conda activate kotaemon

# clone this repo
git clone https://github.com/Cinnamon/kotaemon
cd kotaemon

pip install -e "libs/kotaemon[all]"
pip install -e "libs/ktem"
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Create a &lt;code&gt;.env&lt;/code&gt; file in the root of this project. Use &lt;code&gt;.env.example&lt;/code&gt; as a template&lt;/p&gt; &lt;p&gt;The &lt;code&gt;.env&lt;/code&gt; file is there to serve use cases where users want to pre-config the models before starting up the app (e.g. deploy the app on HF hub). The file will only be used to populate the db once upon the first run, it will no longer be used in consequent runs.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;(Optional) To enable in-browser &lt;code&gt;PDF_JS&lt;/code&gt; viewer, download &lt;a href="https://github.com/mozilla/pdf.js/releases/download/v4.0.379/pdfjs-4.0.379-dist.zip"&gt;PDF_JS_DIST&lt;/a&gt; then extract it to &lt;code&gt;libs/ktem/ktem/assets/prebuilt&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;img src="https://raw.githubusercontent.com/Cinnamon/kotaemon/main/docs/images/pdf-viewer-setup.png" alt="pdf-setup" width="300" /&gt; 
&lt;ol start="4"&gt; 
 &lt;li&gt; &lt;p&gt;Start the web server:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;python app.py
&lt;/code&gt;&lt;/pre&gt; 
  &lt;ul&gt; 
   &lt;li&gt;The app will be automatically launched in your browser.&lt;/li&gt; 
   &lt;li&gt;Default username and password are both &lt;code&gt;admin&lt;/code&gt;. You can set up additional users directly through the UI.&lt;/li&gt; 
  &lt;/ul&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Cinnamon/kotaemon/main/docs/images/chat-tab.png" alt="Chat tab" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Check the &lt;code&gt;Resources&lt;/code&gt; tab and &lt;code&gt;LLMs and Embeddings&lt;/code&gt; and ensure that your &lt;code&gt;api_key&lt;/code&gt; value is set correctly from your &lt;code&gt;.env&lt;/code&gt; file. If it is not set, you can set it there.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Setup GraphRAG&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Official MS GraphRAG indexing only works with OpenAI or Ollama API. We recommend most users to use NanoGraphRAG implementation for straightforward integration with Kotaemon.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;details&gt; 
 &lt;summary&gt;Setup Nano GRAPHRAG&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Install nano-GraphRAG: &lt;code&gt;pip install nano-graphrag&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;nano-graphrag&lt;/code&gt; install might introduce version conflicts, see &lt;a href="https://github.com/Cinnamon/kotaemon/issues/440"&gt;this issue&lt;/a&gt; 
   &lt;ul&gt; 
    &lt;li&gt;To quickly fix: &lt;code&gt;pip uninstall hnswlib chroma-hnswlib &amp;amp;&amp;amp; pip install chroma-hnswlib&lt;/code&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;Launch Kotaemon with &lt;code&gt;USE_NANO_GRAPHRAG=true&lt;/code&gt; environment variable.&lt;/li&gt; 
  &lt;li&gt;Set your default LLM &amp;amp; Embedding models in Resources setting and it will be recognized automatically from NanoGraphRAG.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Setup LIGHTRAG&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Install LightRAG: &lt;code&gt;pip install git+https://github.com/HKUDS/LightRAG.git&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;LightRAG&lt;/code&gt; install might introduce version conflicts, see &lt;a href="https://github.com/Cinnamon/kotaemon/issues/440"&gt;this issue&lt;/a&gt; 
   &lt;ul&gt; 
    &lt;li&gt;To quickly fix: &lt;code&gt;pip uninstall hnswlib chroma-hnswlib &amp;amp;&amp;amp; pip install chroma-hnswlib&lt;/code&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;Launch Kotaemon with &lt;code&gt;USE_LIGHTRAG=true&lt;/code&gt; environment variable.&lt;/li&gt; 
  &lt;li&gt;Set your default LLM &amp;amp; Embedding models in Resources setting and it will be recognized automatically from LightRAG.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Setup MS GRAPHRAG&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Non-Docker Installation&lt;/strong&gt;: If you are not using Docker, install GraphRAG with the following command:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;pip install "graphrag&amp;lt;=0.3.6" future
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Setting Up API KEY&lt;/strong&gt;: To use the GraphRAG retriever feature, ensure you set the &lt;code&gt;GRAPHRAG_API_KEY&lt;/code&gt; environment variable. You can do this directly in your environment or by adding it to a &lt;code&gt;.env&lt;/code&gt; file.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Using Local Models and Custom Settings&lt;/strong&gt;: If you want to use GraphRAG with local models (like &lt;code&gt;Ollama&lt;/code&gt;) or customize the default LLM and other configurations, set the &lt;code&gt;USE_CUSTOMIZED_GRAPHRAG_SETTING&lt;/code&gt; environment variable to true. Then, adjust your settings in the &lt;code&gt;settings.yaml.example&lt;/code&gt; file.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;Setup Local Models (for local/private RAG)&lt;/h3&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/Cinnamon/kotaemon/main/docs/local_model.md"&gt;Local model setup&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Setup multimodal document parsing (OCR, table parsing, figure extraction)&lt;/h3&gt; 
&lt;p&gt;These options are available:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://azure.microsoft.com/en-us/products/ai-services/ai-document-intelligence"&gt;Azure Document Intelligence (API)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://developer.adobe.com/document-services/docs/overview/pdf-extract-api/"&gt;Adobe PDF Extract (API)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/DS4SD/docling"&gt;Docling (local, open-source)&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;To use Docling, first install required dependencies: &lt;code&gt;pip install docling&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Select corresponding loaders in &lt;code&gt;Settings -&amp;gt; Retrieval Settings -&amp;gt; File loader&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Customize your application&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;By default, all application data is stored in the &lt;code&gt;./ktem_app_data&lt;/code&gt; folder. You can back up or copy this folder to transfer your installation to a new machine.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;For advanced users or specific use cases, you can customize these files:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;flowsettings.py&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;.env&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;&lt;code&gt;flowsettings.py&lt;/code&gt;&lt;/h4&gt; 
&lt;p&gt;This file contains the configuration of your application. You can use the example &lt;a href="https://raw.githubusercontent.com/Cinnamon/kotaemon/main/flowsettings.py"&gt;here&lt;/a&gt; as the starting point.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Notable settings&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# setup your preferred document store (with full-text search capabilities)
KH_DOCSTORE=(Elasticsearch | LanceDB | SimpleFileDocumentStore)

# setup your preferred vectorstore (for vector-based search)
KH_VECTORSTORE=(ChromaDB | LanceDB | InMemory | Milvus | Qdrant)

# Enable / disable multimodal QA
KH_REASONINGS_USE_MULTIMODAL=True

# Setup your new reasoning pipeline or modify existing one.
KH_REASONINGS = [
    "ktem.reasoning.simple.FullQAPipeline",
    "ktem.reasoning.simple.FullDecomposeQAPipeline",
    "ktem.reasoning.react.ReactAgentPipeline",
    "ktem.reasoning.rewoo.RewooAgentPipeline",
]
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h4&gt;&lt;code&gt;.env&lt;/code&gt;&lt;/h4&gt; 
&lt;p&gt;This file provides another way to configure your models and credentials.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Configure model via the .env file&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;Alternatively, you can configure the models via the &lt;code&gt;.env&lt;/code&gt; file with the information needed to connect to the LLMs. This file is located in the folder of the application. If you don't see it, you can create one.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Currently, the following providers are supported:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt; &lt;p&gt;&lt;strong&gt;OpenAI&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;In the &lt;code&gt;.env&lt;/code&gt; file, set the &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; variable with your OpenAI API key in order to enable access to OpenAI's models. There are other variables that can be modified, please feel free to edit them to fit your case. Otherwise, the default parameter should work for most people.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;OPENAI_API_BASE=https://api.openai.com/v1
OPENAI_API_KEY=&amp;lt;your OpenAI API key here&amp;gt;
OPENAI_CHAT_MODEL=gpt-3.5-turbo
OPENAI_EMBEDDINGS_MODEL=text-embedding-ada-002
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;&lt;strong&gt;Azure OpenAI&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;For OpenAI models via Azure platform, you need to provide your Azure endpoint and API key. Your might also need to provide your developments' name for the chat model and the embedding model depending on how you set up Azure development.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;AZURE_OPENAI_ENDPOINT=
AZURE_OPENAI_API_KEY=
OPENAI_API_VERSION=2024-02-15-preview
AZURE_OPENAI_CHAT_DEPLOYMENT=gpt-35-turbo
AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT=text-embedding-ada-002
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;&lt;strong&gt;Local Models&lt;/strong&gt;&lt;/p&gt; 
     &lt;ul&gt; 
      &lt;li&gt; &lt;p&gt;Using &lt;code&gt;ollama&lt;/code&gt; OpenAI compatible server:&lt;/p&gt; 
       &lt;ul&gt; 
        &lt;li&gt; &lt;p&gt;Install &lt;a href="https://github.com/ollama/ollama"&gt;ollama&lt;/a&gt; and start the application.&lt;/p&gt; &lt;/li&gt; 
        &lt;li&gt; &lt;p&gt;Pull your model, for example:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;ollama pull llama3.1:8b
ollama pull nomic-embed-text
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
        &lt;li&gt; &lt;p&gt;Set the model names on web UI and make it as default:&lt;/p&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Cinnamon/kotaemon/main/docs/images/models.png" alt="Models" /&gt;&lt;/p&gt; &lt;/li&gt; 
       &lt;/ul&gt; &lt;/li&gt; 
      &lt;li&gt; &lt;p&gt;Using &lt;code&gt;GGUF&lt;/code&gt; with &lt;code&gt;llama-cpp-python&lt;/code&gt;&lt;/p&gt; &lt;p&gt;You can search and download a LLM to be ran locally from the &lt;a href="https://huggingface.co/models"&gt;Hugging Face Hub&lt;/a&gt;. Currently, these model formats are supported:&lt;/p&gt; 
       &lt;ul&gt; 
        &lt;li&gt; &lt;p&gt;GGUF&lt;/p&gt; &lt;p&gt;You should choose a model whose size is less than your device's memory and should leave about 2 GB. For example, if you have 16 GB of RAM in total, of which 12 GB is available, then you should choose a model that takes up at most 10 GB of RAM. Bigger models tend to give better generation but also take more processing time.&lt;/p&gt; &lt;p&gt;Here are some recommendations and their size in memory:&lt;/p&gt; &lt;/li&gt; 
        &lt;li&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen1.5-1.8B-Chat-GGUF/resolve/main/qwen1_5-1_8b-chat-q8_0.gguf?download=true"&gt;Qwen1.5-1.8B-Chat-GGUF&lt;/a&gt;: around 2 GB&lt;/p&gt; &lt;p&gt;Add a new LlamaCpp model with the provided model name on the web UI.&lt;/p&gt; &lt;/li&gt; 
       &lt;/ul&gt; &lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt;
 &lt;/ul&gt;
&lt;/details&gt;   
&lt;h3&gt;Adding your own RAG pipeline&lt;/h3&gt; 
&lt;h4&gt;Custom Reasoning Pipeline&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;Check the default pipeline implementation in &lt;a href="https://raw.githubusercontent.com/Cinnamon/kotaemon/main/libs/ktem/ktem/reasoning/simple.py"&gt;here&lt;/a&gt;. You can make quick adjustment to how the default QA pipeline work.&lt;/li&gt; 
 &lt;li&gt;Add new &lt;code&gt;.py&lt;/code&gt; implementation in &lt;code&gt;libs/ktem/ktem/reasoning/&lt;/code&gt; and later include it in &lt;code&gt;flowssettings&lt;/code&gt; to enable it on the UI.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Custom Indexing Pipeline&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Check sample implementation in &lt;code&gt;libs/ktem/ktem/index/file/graph&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;(more instruction WIP).&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;!-- end-intro --&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;Please cite this project as&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-BibTeX"&gt;@misc{kotaemon2024,
    title = {Kotaemon - An open-source RAG-based tool for chatting with any content.},
    author = {The Kotaemon Team},
    year = {2024},
    howpublished = {\url{https://github.com/Cinnamon/kotaemon}},
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;a href="https://star-history.com/#Cinnamon/kotaemon&amp;amp;Date"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=Cinnamon/kotaemon&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=Cinnamon/kotaemon&amp;amp;type=Date" /&gt; 
  &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=Cinnamon/kotaemon&amp;amp;type=Date" /&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;h2&gt;Contribution&lt;/h2&gt; 
&lt;p&gt;Since our project is actively being developed, we greatly value your feedback and contributions. Please see our &lt;a href="https://github.com/Cinnamon/kotaemon/raw/main/CONTRIBUTING.md"&gt;Contributing Guide&lt;/a&gt; to get started. Thank you to all our contributors!&lt;/p&gt; 
&lt;a href="https://github.com/Cinnamon/kotaemon/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=Cinnamon/kotaemon" /&gt; &lt;/a&gt;</description>
    </item>
    
    <item>
      <title>Vector-Wangel/XLeRobot</title>
      <link>https://github.com/Vector-Wangel/XLeRobot</link>
      <description>&lt;p&gt;XLeRobot: Practical Dual-Arm Mobile Home Robot for $660&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;a href="https://xlerobot.readthedocs.io/en/latest/index.html"&gt;XLeRobot ğŸ¤–&lt;/a&gt;&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/Vector-Wangel/XLeRobot/main/README.md"&gt;&lt;img src="https://img.shields.io/badge/lang-en-blue.svg?sanitize=true" alt="en" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Vector-Wangel/XLeRobot/main/README_CN.md"&gt;&lt;img src="https://img.shields.io/badge/lang-%E4%B8%AD%E6%96%87-brown.svg?sanitize=true" alt="ä¸­æ–‡" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;a href="https://xlerobot.readthedocs.io/en/latest/index.html"&gt; &lt;img width="1725" height="1140" alt="front" src="https://github.com/user-attachments/assets/f9c454ee-2c46-42b4-a5d7-88834a1c95ab" /&gt; &lt;/a&gt; 
&lt;h2&gt;&lt;a href="https://opensource.org/licenses/Apache-2.0"&gt;&lt;img src="https://img.shields.io/badge/License-Apache%202.0-blue.svg?sanitize=true" alt="Apache License" /&gt;&lt;/a&gt; &lt;a href="https://twitter.com/VectorWang2"&gt;&lt;img src="https://img.shields.io/twitter/follow/VectorWang?style=social" alt="Twitter/X" /&gt;&lt;/a&gt; &lt;a href="https://xlerobot.readthedocs.io/en/latest/"&gt;&lt;img src="https://img.shields.io/badge/docs-passing-brightgreen.svg?sanitize=true" alt="Docs status" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/bjZveEUh6F"&gt;&lt;img src="https://img.shields.io/badge/Discord-XLeRobot-7289da?style=flat&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;ğŸš€ Bringing Embodied AI to Everyone - Cheaper Than an iPhone! ğŸ“±&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;ğŸ’µ Starts from $660 cost and â° &amp;lt;4hrs total assembly time!!&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Built upon the giants: &lt;a href="https://github.com/huggingface/lerobot"&gt;LeRobot&lt;/a&gt;, &lt;a href="https://github.com/TheRobotStudio/SO-ARM100"&gt;SO-100/SO-101&lt;/a&gt;, &lt;a href="https://github.com/SIGRobotics-UIUC/LeKiwi"&gt;Lekiwi&lt;/a&gt;, &lt;a href="https://github.com/timqian/bambot"&gt;Bambot&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src="https://github.com/user-attachments/assets/17e31979-bd5e-4790-be70-566ea8bb181e" width="250" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/user-attachments/assets/96ff4a3e-3402-47a2-bc6b-b45137ee3fdd" width="250" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/user-attachments/assets/f6d52acc-bc8d-46f6-b3cd-8821f0306a7f" width="250" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src="https://github.com/user-attachments/assets/59086300-3e6f-4a3c-b5e0-db893eeabc0c" width="250" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/user-attachments/assets/4ddbc0ff-ca42-4ad0-94c6-4e0f4047fd01" width="250" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/user-attachments/assets/7abc890e-9c9c-4983-8b25-122573028de5" width="250" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src="https://github.com/user-attachments/assets/e74a602b-0146-49c4-953d-3fa3b038a7f7" width="250" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/user-attachments/assets/d8090b15-97f3-4abc-98c8-208ae79894d5" width="250" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/user-attachments/assets/8b54adc3-d61b-42a0-8985-ea28f2e8f64c" width="250" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h1&gt;ğŸ“° News&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;2025-09-09: &lt;strong&gt;Developer Assembly kit (excluding battery and IKEA cart) ready for purchase&lt;/strong&gt; in &lt;a href="https://e.tb.cn/h.SZFbBgZABZ8zRPe?tk=ba514rTBRjQ"&gt;China (Taobao) for &lt;strong&gt;3699ï¿¥&lt;/strong&gt;&lt;/a&gt; and &lt;a href="https://shop.wowrobo.com/products/xlerobot-dual-arm-mobile-household-robot-kit?variant=47297659961561"&gt;world-wide for &lt;strong&gt;579$&lt;/strong&gt;&lt;/a&gt;. &lt;em&gt;(In collaboration with &lt;strong&gt;Wowrobo&lt;/strong&gt;, one of the official collaborators with Huggingface SO101 arm, they have sold 5k+ SO101 arm worldwide with great customer feedbacks.)&lt;/em&gt; &lt;img width="1482" height="485" alt="image" src="https://github.com/user-attachments/assets/788836c1-966a-4d11-a911-5c37befc0b85" /&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Non-profit, I personally don't earn any from this. I also asked Wowrobo to set the price as low as possible.&lt;/li&gt; 
   &lt;li&gt;This is only the assembly kit for developers, please check documentation website and this repo for available codes and tutorials before you purchase.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025-09-09: Joined &lt;a href="https://www.seeedstudio.com/embodied-ai-worldwide-hackathon-home-robot.html"&gt;Embodied AI Home Robot Hackathon&lt;/a&gt; (Oct 25â€“26, Bay Area) held by &lt;strong&gt;SEEED x Nvidia x Huggingface&lt;/strong&gt; as mentor! &lt;a href="https://docs.google.com/forms/d/e/1FAIpQLSdYYDegdgIypxuGJNLcoc8kbdmU4jKgl49zg4X-107LAmBN4g/viewform"&gt;Register HERE&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;img width="2400" height="1256" alt="image" src="https://github.com/user-attachments/assets/4132c23b-5c86-4bb9-94b4-a6b12059685b" /&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025-08-30: XLeRobot 0.3.0 Release with final outfit touch up and household chores showcase demos.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025-07-30: &lt;a href="https://xlerobot.readthedocs.io/en/latest/software/index.html"&gt;Control XLeRobot in real life&lt;/a&gt; with &lt;strong&gt;keyboard/Xbox controller/Switch joycon&lt;/strong&gt; in the wild anywhere. All bluetooth, no wifi needed and zero latency.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/de8f50ad-a370-406c-97fb-fc01638d5624" alt="rea" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025-07-08: &lt;a href="https://xlerobot.readthedocs.io/en/latest/simulation/index.html"&gt;&lt;strong&gt;Simulation&lt;/strong&gt;&lt;/a&gt; with updated urdfs, control scripts (support Quest3 VR, keyboard, Xbox controller, switch joycon), support for new hardware and cameras, RL environment. Get started in 15 min.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/68b77bea-fdcf-4f42-9cf0-efcf1b188358" alt="vr" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025-07-01: &lt;a href="https://xlerobot.readthedocs.io/en/latest/index.html"&gt;&lt;strong&gt;Documentation&lt;/strong&gt; website&lt;/a&gt; out for more orgainized tutorials, demos and resources.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025-06-13: &lt;a href="https://xlerobot.readthedocs.io"&gt;&lt;strong&gt;XLeRobot 0.2.0&lt;/strong&gt;&lt;/a&gt; hardware setup, the 1st version fully capable for autonomous household tasks, starts from 660$.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ’µ Total Cost ğŸ’µ&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Cost excludes 3D printing, tools, shipping, and taxes.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Price (Buy all the parts yourself)&lt;/th&gt; 
   &lt;th&gt;US&lt;/th&gt; 
   &lt;th&gt;EU&lt;/th&gt; 
   &lt;th&gt;CN&lt;/th&gt; 
   &lt;th&gt;IN&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Basic&lt;/strong&gt; (use your laptop, single RGB head cam)&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;~$660&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;~â‚¬680&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;~Â¥3999&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;~â‚¹87000&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;â†‘ Stereo dual-eye RGB head cam&lt;/td&gt; 
   &lt;td&gt;+$30&lt;/td&gt; 
   &lt;td&gt;+â‚¬30&lt;/td&gt; 
   &lt;td&gt;+Â¥199&lt;/td&gt; 
   &lt;td&gt;+â‚¹6550&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;+ RasberryPi&lt;/td&gt; 
   &lt;td&gt;+$79&lt;/td&gt; 
   &lt;td&gt;+â‚¬79&lt;/td&gt; 
   &lt;td&gt;+Â¥399&lt;/td&gt; 
   &lt;td&gt;+â‚¹7999&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;â†‘ RealSense RGBD head cam&lt;/td&gt; 
   &lt;td&gt;+$220&lt;/td&gt; 
   &lt;td&gt;+â‚¬230&lt;/td&gt; 
   &lt;td&gt;+Â¥1499&lt;/td&gt; 
   &lt;td&gt;+â‚¹35726&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸš€ Get Started ğŸš€&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] If you are totally new to programming, please spend at least a day to get yourself familiar with basic Python, Ubuntu and Github (with the help of Google and AI). At least you should know how to setup ubuntu system, git clone, pip install, use intepreters (VS Code, Cursor, Pycharm, etc.) and directly run commands in the terminals.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ol&gt; 
 &lt;li&gt;ğŸ’µ &lt;strong&gt;Buy your parts&lt;/strong&gt;: &lt;a href="https://xlerobot.readthedocs.io/en/latest/hardware/getting_started/material.html"&gt;Bill of Materials&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸ–¨ï¸ &lt;strong&gt;Print your stuff&lt;/strong&gt;: &lt;a href="https://xlerobot.readthedocs.io/en/latest/hardware/getting_started/3d.html"&gt;3D printing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸ”¨ &lt;del&gt;Avengers&lt;/del&gt;: &lt;a href="https://xlerobot.readthedocs.io/en/latest/hardware/getting_started/assemble.html"&gt;&lt;strong&gt;Assemble&lt;/strong&gt;!&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸ’» &lt;strong&gt;Software&lt;/strong&gt;: &lt;a href="https://xlerobot.readthedocs.io/en/latest/software/index.html"&gt;Get your robot moving!&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Contribute&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;ğŸ‘‹ Want to contribute to XLeRobot?&lt;/strong&gt; Please refer to &lt;a href="https://raw.githubusercontent.com/Vector-Wangel/XLeRobot/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for guidance on how to get involved!&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Main Contributors&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://vector-wangel.github.io/"&gt;Gaotian/Vector Wang&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://lzhuoyi.github.io/Zhuoyi_Lu.github.io/"&gt;Zhuoyi Lu&lt;/a&gt;: RL sim2real deploy, teleop on real robot (Xbox, VR, Joycon)&lt;/li&gt; 
 &lt;li&gt;Yiyang Huang: RL &amp;amp; VLA implementation (ongoing)&lt;/li&gt; 
 &lt;li&gt;YCP: WebUI for remote control (ongoing)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Vector-Wangel/XLeRobot/main/lixingzhang.com"&gt;Lixing Zhang&lt;/a&gt;: Hardware design improvements&lt;/li&gt; 
 &lt;li&gt;Nicole Yue: Documentation website setup&lt;/li&gt; 
 &lt;li&gt;Yuesong Wang: Mujoco simulation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This is just a small brick in the pyramid, made possible by&amp;nbsp;&lt;a href="https://github.com/huggingface/lerobot"&gt;LeRobot&lt;/a&gt;,&amp;nbsp;&lt;a href="https://github.com/TheRobotStudio/SO-ARM100"&gt;SO-100&lt;/a&gt;,&amp;nbsp;&lt;a href="https://github.com/SIGRobotics-UIUC/LeKiwi"&gt;Lekiwi&lt;/a&gt;, and&amp;nbsp;&lt;a href="https://github.com/timqian/bambot"&gt;Bambot&lt;/a&gt;. Thanks to all the talented contributors behind these detailed and professional projects.&lt;/p&gt; 
&lt;p&gt;Looking forward to collaborating with anyone interested in contributing to this project!&lt;/p&gt; 
&lt;h2&gt;About me&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://vector-wangel.github.io/"&gt;Gaotian/Vector Wang&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;I am a CS graduate student at Rice University &lt;a href="https://robotpilab.github.io/"&gt;RobotPi Lab&lt;/a&gt;, focusing on robust object manipulation, where we propse virtual cages and funnels and physics-aware world models to close the Sim2real gap and achieve robust manipulation under uncertainties. One of my papers, Caging in Time, has recently been accepted by International Journal of Robotics Research (IJRR).&lt;/p&gt; 
&lt;p&gt;I built XLeRobot as a personal hobby to instantiate my research theory, also to provide a low-cost platform for people who are interested in robotics and embodied AI to work with.&lt;/p&gt; 
&lt;h2&gt;&lt;a href="https://star-history.com/#Vector-Wangel/XLeRobot&amp;amp;Timeline"&gt;&lt;img src="https://api.star-history.com/svg?repos=Vector-Wangel/XLeRobot&amp;amp;type=Timeline" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you want, you can cite this work with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{wang2025xlerobot,
    author = {Wang, Gaotian and Lu, Zhuoyi},
    title = {XLeRobot: A Practical Low-cost Household Dual-Arm Mobile Robot Design for General Manipulation},
    howpublished = "\url{https://github.com/Vector-Wangel/XLeRobot}",
    year = {2025}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;---&lt;img src="https://github.com/user-attachments/assets/682ef049-bb42-4b50-bf98-74d6311e774d" alt="Generated Image August 27, 2025 - 4_58PM" /&gt;&lt;/p&gt; 
&lt;h2&gt;ğŸª§ Disclaimer ğŸª§&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] If you build, buy, or develop a XLeRobot based on this repo, you will be fully responsible for all the physical and mental damages it does to you or others.&lt;/p&gt; 
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>hiroi-sora/Umi-OCR</title>
      <link>https://github.com/hiroi-sora/Umi-OCR</link>
      <description>&lt;p&gt;OCR software, free and offline. å¼€æºã€å…è´¹çš„ç¦»çº¿OCRè½¯ä»¶ã€‚æ”¯æŒæˆªå±/æ‰¹é‡å¯¼å…¥å›¾ç‰‡ï¼ŒPDFæ–‡æ¡£è¯†åˆ«ï¼Œæ’é™¤æ°´å°/é¡µçœ‰é¡µè„šï¼Œæ‰«æ/ç”ŸæˆäºŒç»´ç ã€‚å†…ç½®å¤šå›½è¯­è¨€åº“ã€‚&lt;/p&gt;&lt;hr&gt;&lt;p align="left"&gt; &lt;span&gt; &lt;b&gt;ä¸­æ–‡&lt;/b&gt; &lt;/span&gt; &lt;span&gt; â€¢ &lt;/span&gt; &lt;a href="https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/README_en.md"&gt; English &lt;/a&gt; &lt;span&gt; â€¢ &lt;/span&gt; &lt;a href="https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/README_ja.md"&gt; æ—¥æœ¬èª &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/hiroi-sora/Umi-OCR"&gt; &lt;img width="200" height="128" src="https://tupian.li/images/2022/10/27/icon---256.png" alt="Umi-OCR" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h1 align="center"&gt;Umi-OCR æ–‡å­—è¯†åˆ«å·¥å…·&lt;/h1&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/hiroi-sora/Umi-OCR/releases/latest"&gt; &lt;img src="https://img.shields.io/github/v/release/hiroi-sora/Umi-OCR?style=flat-square" alt="Umi-OCR" /&gt; &lt;/a&gt; &lt;a href="https://github.com/hiroi-sora/Umi-OCR/raw/main/LICENSE"&gt; &lt;img src="https://img.shields.io/github/license/hiroi-sora/Umi-OCR?style=flat-square" alt="LICENSE" /&gt; &lt;/a&gt; &lt;a href="#ä¸‹è½½å‘è¡Œç‰ˆ"&gt; &lt;img src="https://img.shields.io/github/downloads/hiroi-sora/Umi-OCR/total?style=flat-square" alt="forks" /&gt; &lt;/a&gt; &lt;a href="https://star-history.com/#hiroi-sora/Umi-OCR"&gt; &lt;img src="https://img.shields.io/github/stars/hiroi-sora/Umi-OCR?style=flat-square" alt="stars" /&gt; &lt;/a&gt; &lt;a href="https://github.com/hiroi-sora/Umi-OCR/forks"&gt; &lt;img src="https://img.shields.io/github/forks/hiroi-sora/Umi-OCR?style=flat-square" alt="forks" /&gt; &lt;/a&gt; &lt;a href="https://hosted.weblate.org/engage/umi-ocr/"&gt; &lt;img src="https://hosted.weblate.org/widget/umi-ocr/svg-badge.svg?sanitize=true" alt="ç¿»è¯‘çŠ¶æ€" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;h3&gt; &lt;a href="#ç›®å½•"&gt; ä½¿ç”¨è¯´æ˜ &lt;/a&gt; &lt;span&gt; â€¢ &lt;/span&gt; &lt;a href="#ä¸‹è½½å‘è¡Œç‰ˆ"&gt; ä¸‹è½½åœ°å€ &lt;/a&gt; &lt;span&gt; â€¢ &lt;/span&gt; &lt;a href="https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/CHANGE_LOG.md"&gt; æ›´æ–°æ—¥å¿— &lt;/a&gt; &lt;span&gt; â€¢ &lt;/span&gt; &lt;a href="https://github.com/hiroi-sora/Umi-OCR/issues"&gt; æäº¤Bug &lt;/a&gt; &lt;/h3&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt; 
 &lt;strong&gt;å…è´¹ï¼Œå¼€æºï¼Œå¯æ‰¹é‡çš„ç¦»çº¿OCRè½¯ä»¶&lt;/strong&gt;
 &lt;br /&gt; 
 &lt;sub&gt;é€‚ç”¨äº Windows7 x64 ã€Linux x64 &lt;/sub&gt;
&lt;/div&gt;
&lt;br /&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;å…è´¹&lt;/strong&gt;ï¼šæœ¬é¡¹ç›®æ‰€æœ‰ä»£ç å¼€æºï¼Œå®Œå…¨å…è´¹ã€‚&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;æ–¹ä¾¿&lt;/strong&gt;ï¼šè§£å‹å³ç”¨ï¼Œç¦»çº¿è¿è¡Œï¼Œæ— éœ€ç½‘ç»œã€‚&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;é«˜æ•ˆ&lt;/strong&gt;ï¼šè‡ªå¸¦é«˜æ•ˆç‡çš„ç¦»çº¿OCRå¼•æ“ï¼Œå†…ç½®å¤šç§è¯­è¨€è¯†åˆ«åº“ã€‚&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;çµæ´»&lt;/strong&gt;ï¼šæ”¯æŒå‘½ä»¤è¡Œã€HTTPæ¥å£ç­‰å¤–éƒ¨è°ƒç”¨æ–¹å¼ã€‚&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;åŠŸèƒ½&lt;/strong&gt;ï¼šæˆªå›¾OCR / æ‰¹é‡OCR / PDFè¯†åˆ« / äºŒç»´ç  / å…¬å¼è¯†åˆ«&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="center"&gt;&lt;img src="https://tupian.li/images/2023/11/19/65599097ab5f4.png" alt="1-æ ‡é¢˜-1.png" style="width: 80%;" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://tupian.li/images/2023/11/19/6559909fdeeba.png" alt="1-æ ‡é¢˜-2.png" /&gt;&lt;/p&gt; 
&lt;h2&gt;ç›®å½•&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/#%E6%88%AA%E5%9B%BEOCR"&gt;æˆªå›¾è¯†åˆ«&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/#%E6%96%87%E6%9C%AC%E5%90%8E%E5%A4%84%E7%90%86"&gt;æ’ç‰ˆè§£æ&lt;/a&gt; - è¯†åˆ«ä¸åŒæ’ç‰ˆï¼ŒæŒ‰æ­£ç¡®é¡ºåºè¾“å‡ºæ–‡å­—&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/#%E6%89%B9%E9%87%8FOCR"&gt;æ‰¹é‡è¯†åˆ«&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/#%E5%BF%BD%E7%95%A5%E5%8C%BA%E5%9F%9F"&gt;å¿½ç•¥åŒºåŸŸ&lt;/a&gt; - æ’é™¤æˆªå›¾æ°´å°å¤„çš„æ–‡å­—&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/#%E4%BA%8C%E7%BB%B4%E7%A0%81"&gt;äºŒç»´ç &lt;/a&gt; æ”¯æŒæ‰«ç æˆ–ç”ŸæˆäºŒç»´ç å›¾ç‰‡&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/#%E6%96%87%E6%A1%A3%E8%AF%86%E5%88%AB"&gt;æ–‡æ¡£è¯†åˆ«&lt;/a&gt; ä»PDFæ‰«æä»¶ä¸­æå–æ–‡æœ¬ï¼Œæˆ–è½¬ä¸ºåŒå±‚å¯æœç´¢PDF&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/#%E5%85%A8%E5%B1%80%E8%AE%BE%E7%BD%AE"&gt;å…¨å±€è®¾ç½®&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/docs/README_CLI.md"&gt;å‘½ä»¤è¡Œè°ƒç”¨&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/docs/http/README.md"&gt;HTTPæ¥å£&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/#%E6%9E%84%E5%BB%BA%E9%A1%B9%E7%9B%AE"&gt;æ„å»ºé¡¹ç›®ï¼ˆWindowsã€Linuxï¼‰&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ä½¿ç”¨æºç &lt;/h2&gt; 
&lt;p&gt;å¼€å‘è€…è¯·åŠ¡å¿…é˜…è¯» &lt;a href="https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/#%E6%9E%84%E5%BB%BA%E9%A1%B9%E7%9B%AE"&gt;æ„å»ºé¡¹ç›®&lt;/a&gt; ã€‚&lt;/p&gt; 
&lt;h2&gt;ä¸‹è½½å‘è¡Œç‰ˆ&lt;/h2&gt; 
&lt;p&gt;ä»¥ä¸‹å‘å¸ƒé“¾æ¥å‡é•¿æœŸç»´æŠ¤ï¼Œæä¾›ç¨³å®šç‰ˆæœ¬çš„ä¸‹è½½ã€‚&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;è“å¥äº‘&lt;/strong&gt; &lt;a href="https://hiroi-sora.lanzoul.com/s/umi-ocr"&gt;https://hiroi-sora.lanzoul.com/s/umi-ocr&lt;/a&gt; ï¼ˆå›½å†…æ¨èï¼Œå…æ³¨å†Œ/æ— é™é€Ÿï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;GitHub&lt;/strong&gt; &lt;a href="https://github.com/hiroi-sora/Umi-OCR/releases/latest"&gt;https://github.com/hiroi-sora/Umi-OCR/releases/latest&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Source Forge&lt;/strong&gt; &lt;a href="https://sourceforge.net/projects/umi-ocr"&gt;https://sourceforge.net/projects/umi-ocr&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;â€¢&amp;nbsp;&amp;nbsp;Scoop Installer&lt;/b&gt;ï¼ˆç‚¹å‡»å±•å¼€ï¼‰&lt;/summary&gt; 
 &lt;p&gt;&lt;a href="https://scoop.sh/"&gt;Scoop&lt;/a&gt; æ˜¯ä¸€æ¬¾Windowsä¸‹çš„å‘½ä»¤è¡Œå®‰è£…ç¨‹åºï¼Œå¯æ–¹ä¾¿åœ°ç®¡ç†å¤šä¸ªåº”ç”¨ã€‚æ‚¨å¯ä»¥å…ˆå®‰è£… Scoop ï¼Œå†ä½¿ç”¨ä»¥ä¸‹æŒ‡ä»¤å®‰è£… &lt;code&gt;Umi-OCR&lt;/code&gt; ï¼š&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;æ·»åŠ  &lt;code&gt;extras&lt;/code&gt; æ¡¶ï¼š&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;pre&gt;&lt;code&gt;scoop bucket add extras
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ul&gt; 
  &lt;li&gt;ï¼ˆå¯é€‰1ï¼‰å®‰è£… Umi-OCRï¼ˆè‡ªå¸¦ &lt;code&gt;Rapid-OCR&lt;/code&gt; å¼•æ“ï¼Œå…¼å®¹æ€§å¥½ï¼‰ï¼š&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;pre&gt;&lt;code&gt;scoop install extras/umi-ocr
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ul&gt; 
  &lt;li&gt;ï¼ˆå¯é€‰2ï¼‰å®‰è£… Umi-OCRï¼ˆè‡ªå¸¦ &lt;code&gt;Paddle-OCR&lt;/code&gt; å¼•æ“ï¼Œé€Ÿåº¦ç¨å¿«ï¼‰ï¼š&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;pre&gt;&lt;code&gt;scoop install extras/umi-ocr-paddle
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ul&gt; 
  &lt;li&gt;ä¸è¦åŒæ—¶å®‰è£…äºŒè€…ï¼Œå¿«æ·æ–¹å¼å¯èƒ½ä¼šè¢«è¦†ç›–ã€‚ä½†æ‚¨å¯ä»¥é¢å¤–å¯¼å…¥ &lt;a href="https://github.com/hiroi-sora/Umi-OCR_plugins"&gt;æ’ä»¶&lt;/a&gt; ï¼Œéšæ—¶åˆ‡æ¢ä¸åŒOCRå¼•æ“ã€‚&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;br /&gt; 
&lt;h2&gt;å¼€å§‹ä½¿ç”¨&lt;/h2&gt; 
&lt;p&gt;è½¯ä»¶å‘å¸ƒåŒ…ä¸‹è½½ä¸º &lt;code&gt;.7z&lt;/code&gt; å‹ç¼©åŒ…æˆ– &lt;code&gt;.7z.exe&lt;/code&gt; è‡ªè§£å‹åŒ…ã€‚è‡ªè§£å‹åŒ…å¯åœ¨æ²¡æœ‰å®‰è£…å‹ç¼©è½¯ä»¶çš„ç”µè„‘ä¸Šï¼Œè§£å‹æ–‡ä»¶ã€‚&lt;/p&gt; 
&lt;p&gt;æœ¬è½¯ä»¶æ— éœ€å®‰è£…ã€‚è§£å‹åï¼Œç‚¹å‡» &lt;code&gt;Umi-OCR.exe&lt;/code&gt; å³å¯å¯åŠ¨ç¨‹åºã€‚&lt;/p&gt; 
&lt;p&gt;é‡åˆ°ä»»ä½•é—®é¢˜ï¼Œè¯·æ &lt;a href="https://github.com/hiroi-sora/Umi-OCR/issues"&gt;Issue&lt;/a&gt; ï¼Œæˆ‘ä¼šå°½å¯èƒ½å¸®åŠ©ä½ ã€‚&lt;/p&gt; 
&lt;h2&gt;ç•Œé¢è¯­è¨€&lt;/h2&gt; 
&lt;p&gt;Umi-OCR æ”¯æŒçš„ç•Œé¢å¤šå›½è¯­è¨€ã€‚åœ¨ç¬¬ä¸€æ¬¡æ‰“å¼€è½¯ä»¶æ—¶ï¼Œå°†ä¼šæŒ‰ç…§ä½ çš„ç”µè„‘çš„ç³»ç»Ÿè®¾ç½®ï¼Œè‡ªåŠ¨åˆ‡æ¢è¯­è¨€ã€‚&lt;/p&gt; 
&lt;p&gt;å¦‚æœéœ€è¦æ‰‹åŠ¨åˆ‡æ¢è¯­è¨€ï¼Œè¯·å‚è€ƒä¸‹å›¾ï¼Œ&lt;code&gt;å…¨å±€è®¾ç½®&lt;/code&gt;â†’&lt;code&gt;è¯­è¨€/Language&lt;/code&gt; ã€‚&lt;/p&gt; 
&lt;p align="center"&gt;&lt;img src="https://tupian.li/images/2023/11/19/65599c3f9e600.png" alt="1-æ ‡é¢˜-1.png" style="width: 80%;" /&gt;&lt;/p&gt; 
&lt;h2&gt;æ ‡ç­¾é¡µ&lt;/h2&gt; 
&lt;p&gt;Umi-OCR v2 ç”±ä¸€ç³»åˆ—çµæ´»å¥½ç”¨çš„&lt;strong&gt;æ ‡ç­¾é¡µ&lt;/strong&gt;ç»„æˆã€‚æ‚¨å¯æŒ‰ç…§è‡ªå·±çš„å–œå¥½ï¼Œæ‰“å¼€éœ€è¦çš„æ ‡ç­¾é¡µã€‚&lt;/p&gt; 
&lt;p&gt;æ ‡ç­¾æ å·¦ä¸Šè§’å¯ä»¥åˆ‡æ¢&lt;strong&gt;çª—å£ç½®é¡¶&lt;/strong&gt;ã€‚å³ä¸Šè§’èƒ½å¤Ÿ&lt;strong&gt;é”å®šæ ‡ç­¾é¡µ&lt;/strong&gt;ï¼Œä»¥é˜²æ­¢æ—¥å¸¸ä½¿ç”¨ä¸­è¯¯è§¦å…³é—­æ ‡ç­¾é¡µã€‚&lt;/p&gt; 
&lt;h3&gt;æˆªå›¾OCR&lt;/h3&gt; 
&lt;p align="center"&gt;&lt;img src="https://tupian.li/images/2023/11/19/65599097aba8e.png" alt="2-æˆªå›¾-1.png" style="width: 80%;" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;æˆªå›¾OCR&lt;/strong&gt;ï¼šæ‰“å¼€è¿™ä¸€é¡µåï¼Œå°±å¯ä»¥ç”¨å¿«æ·é”®å”¤èµ·æˆªå›¾ï¼Œè¯†åˆ«å›¾ä¸­çš„æ–‡å­—ã€‚&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;å·¦ä¾§çš„å›¾ç‰‡é¢„è§ˆæ ï¼Œå¯ç›´æ¥ç”¨é¼ æ ‡åˆ’é€‰å¤åˆ¶ã€‚&lt;/li&gt; 
 &lt;li&gt;å³ä¾§çš„è¯†åˆ«è®°å½•æ ï¼Œå¯ä»¥ç¼–è¾‘æ–‡å­—ï¼Œå…è®¸åˆ’é€‰å¤šä¸ªè®°å½•å¤åˆ¶ã€‚&lt;/li&gt; 
 &lt;li&gt;ä¹Ÿæ”¯æŒåœ¨åˆ«å¤„å¤åˆ¶å›¾ç‰‡ï¼Œç²˜è´´åˆ°Umi-OCRè¿›è¡Œè¯†åˆ«ã€‚&lt;/li&gt; 
 &lt;li&gt;å…³äº &lt;a href="https://github.com/hiroi-sora/Umi-OCR/issues/254"&gt;å…¬å¼è¯†åˆ«&lt;/a&gt; åŠŸèƒ½&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;æ–‡æœ¬åå¤„ç†&lt;/h4&gt; 
&lt;p align="center"&gt;&lt;img src="https://tupian.li/images/2023/11/19/6559909f3e378.png" alt="2-æˆªå›¾-2.png" style="width: 80%;" /&gt;&lt;/p&gt; 
&lt;p&gt;å…³äº &lt;strong&gt;OCRæ–‡æœ¬åå¤„ç† - æ’ç‰ˆè§£ææ–¹æ¡ˆ&lt;/strong&gt;ï¼š å¯ä»¥æ•´ç†OCRç»“æœçš„æ’ç‰ˆå’Œé¡ºåºï¼Œä½¿æ–‡æœ¬æ›´é€‚åˆé˜…è¯»å’Œä½¿ç”¨ã€‚é¢„è®¾æ–¹æ¡ˆï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;å¤šæ -æŒ‰è‡ªç„¶æ®µæ¢è¡Œ&lt;/code&gt;ï¼šé€‚åˆå¤§éƒ¨åˆ†æƒ…æ™¯ï¼Œè‡ªåŠ¨è¯†åˆ«å¤šæ å¸ƒå±€ï¼ŒæŒ‰è‡ªç„¶æ®µè§„åˆ™è¿›è¡Œæ¢è¡Œã€‚&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;å¤šæ -æ€»æ˜¯æ¢è¡Œ&lt;/code&gt;ï¼šæ¯æ®µè¯­å¥éƒ½è¿›è¡Œæ¢è¡Œã€‚&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;å¤šæ -æ— æ¢è¡Œ&lt;/code&gt;ï¼šå¼ºåˆ¶å°†æ‰€æœ‰è¯­å¥åˆå¹¶åˆ°åŒä¸€è¡Œã€‚&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;å•æ -æŒ‰è‡ªç„¶æ®µæ¢è¡Œ&lt;/code&gt;/&lt;code&gt;æ€»æ˜¯æ¢è¡Œ&lt;/code&gt;/&lt;code&gt;æ— æ¢è¡Œ&lt;/code&gt;ï¼šä¸ä¸Šè¿°ç±»ä¼¼ï¼Œä¸è¿‡ ä¸åŒºåˆ†å¤šæ å¸ƒå±€ã€‚&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;å•æ -ä¿ç•™ç¼©è¿›&lt;/code&gt;ï¼šé€‚ç”¨äºè§£æä»£ç æˆªå›¾ï¼Œä¿ç•™è¡Œé¦–ç¼©è¿›å’Œè¡Œä¸­ç©ºæ ¼ã€‚&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ä¸åšå¤„ç†&lt;/code&gt;ï¼šOCRå¼•æ“çš„åŸå§‹è¾“å‡ºï¼Œé»˜è®¤æ¯æ®µè¯­å¥éƒ½è¿›è¡Œæ¢è¡Œã€‚&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;ä¸Šè¿°æ–¹æ¡ˆï¼Œå‡èƒ½è‡ªåŠ¨å¤„ç†æ¨ªæ’å’Œç«–æ’ï¼ˆä»å³åˆ°å·¦ï¼‰çš„æ’ç‰ˆã€‚ï¼ˆç«–æ’æ–‡å­—è¿˜éœ€è¦OCRå¼•æ“æœ¬èº«æ”¯æŒï¼‰&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;æ‰¹é‡OCR&lt;/h3&gt; 
&lt;p align="center"&gt;&lt;img src="https://tupian.li/images/2023/11/19/655990a2511e0.png" alt="3-æ‰¹é‡-1.png" style="width: 80%;" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;æ‰¹é‡OCR&lt;/strong&gt;ï¼šè¿™ä¸€é¡µç”¨äºæ‰¹é‡å¯¼å…¥æœ¬åœ°å›¾ç‰‡è¿›è¡Œè¯†åˆ«ã€‚&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;æ”¯æŒæ ¼å¼ï¼š&lt;code&gt;jpg, jpe, jpeg, jfif, png, webp, bmp, tif, tiff&lt;/code&gt;ã€‚&lt;/li&gt; 
 &lt;li&gt;ä¿å­˜è¯†åˆ«ç»“æœçš„æ”¯æŒæ ¼å¼ï¼š&lt;code&gt;txt, jsonl, md, csv(Excel)&lt;/code&gt;ã€‚&lt;/li&gt; 
 &lt;li&gt;ä¸æˆªå›¾OCRä¸€æ ·ï¼Œæ”¯æŒ&lt;code&gt;æ–‡æœ¬åå¤„ç†&lt;/code&gt;åŠŸèƒ½ï¼Œæ•´ç†OCRæ–‡æœ¬çš„æ’ç‰ˆå’Œé¡ºåºã€‚&lt;/li&gt; 
 &lt;li&gt;æ²¡æœ‰æ•°é‡ä¸Šé™ï¼Œå¯ä¸€æ¬¡æ€§å¯¼å…¥å‡ ç™¾å¼ å›¾ç‰‡è¿›è¡Œä»»åŠ¡ã€‚&lt;/li&gt; 
 &lt;li&gt;æ”¯æŒä»»åŠ¡å®Œæˆåè‡ªåŠ¨å…³æœº/å¾…æœºã€‚&lt;/li&gt; 
 &lt;li&gt;å¦‚æœè¦è¯†åˆ«åƒç´ è¶…å¤§çš„é•¿å›¾æˆ–å¤§å›¾ï¼Œè¯·è°ƒæ•´ï¼š&lt;strong&gt;é¡µé¢çš„è®¾ç½®â†’æ–‡å­—è¯†åˆ«â†’é™åˆ¶å›¾åƒè¾¹é•¿â†’ã€è°ƒé«˜æ•°å€¼ã€‘&lt;/strong&gt;ã€‚&lt;/li&gt; 
 &lt;li&gt;æ‹¥æœ‰ç‰¹æ®ŠåŠŸèƒ½ &lt;code&gt;å¿½ç•¥åŒºåŸŸ&lt;/code&gt; ã€‚&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;å¿½ç•¥åŒºåŸŸ&lt;/h4&gt; 
&lt;p align="center"&gt;&lt;img src="https://tupian.li/images/2023/11/19/6559911d28be7.png" alt="3-æ‰¹é‡-2.png" style="width: 80%;" /&gt;&lt;/p&gt; 
&lt;p&gt;å…³äº &lt;strong&gt;OCRæ–‡æœ¬åå¤„ç† - å¿½ç•¥åŒºåŸŸ&lt;/strong&gt;ï¼š æ‰¹é‡OCRä¸­çš„ä¸€ç§ç‰¹æ®ŠåŠŸèƒ½ï¼Œé€‚ç”¨äºæ’é™¤å›¾ç‰‡ä¸­çš„ä¸æƒ³è¦çš„æ–‡å­—ã€‚&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;åœ¨æ‰¹é‡è¯†åˆ«é¡µçš„å³æ è®¾ç½®ä¸­å¯è¿›å…¥å¿½ç•¥åŒºåŸŸç¼–è¾‘å™¨ã€‚&lt;/li&gt; 
 &lt;li&gt;å¦‚ä¸Šæ–¹æ ·ä¾‹ï¼Œå›¾ç‰‡é¡¶éƒ¨å’Œå³ä¸‹è§’å­˜åœ¨å¤šä¸ªæ°´å° / LOGOã€‚å¦‚æœæ‰¹é‡è¯†åˆ«è¿™ç±»å›¾ç‰‡ï¼Œæ°´å°ä¼šå¯¹è¯†åˆ«ç»“æœé€ æˆå¹²æ‰°ã€‚&lt;/li&gt; 
 &lt;li&gt;æŒ‰ä½å³é”®ï¼Œç»˜åˆ¶å¤šä¸ªçŸ©å½¢æ¡†ã€‚è¿™äº›åŒºåŸŸå†…çš„æ–‡å­—å°†åœ¨ä»»åŠ¡ä¸­è¢«å¿½ç•¥ã€‚&lt;/li&gt; 
 &lt;li&gt;è¯·å°½é‡å°†çŸ©å½¢æ¡†ç”»å¾—å¤§ä¸€äº›ï¼Œå®Œå…¨åŒ…è£¹ä½æ°´å°æ‰€æœ‰å¯èƒ½å‡ºç°çš„ä½ç½®ã€‚&lt;/li&gt; 
 &lt;li&gt;æ³¨æ„ï¼Œåªæœ‰å¤„äºå¿½ç•¥åŒºåŸŸæ¡†å†…éƒ¨çš„æ•´ä¸ªæ–‡æœ¬å—ï¼ˆè€Œä¸æ˜¯å•ä¸ªå­—ç¬¦ï¼‰ä¼šè¢«å¿½ç•¥ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œé»„è‰²è¾¹æ¡†çš„æ·±è‰²çŸ©å½¢æ˜¯ä¸€ä¸ªå¿½ç•¥åŒºåŸŸã€‚é‚£ä¹ˆåªæœ‰&lt;code&gt;key_mouse&lt;/code&gt;æ‰ä¼šè¢«å¿½ç•¥ã€‚&lt;code&gt;pubsub_connector.py&lt;/code&gt;ã€&lt;code&gt;pubsub_service.py&lt;/code&gt; è¿™ä¸¤ä¸ªæ–‡æœ¬å—å¾—ä»¥ä¿ç•™ã€‚&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="center"&gt;&lt;img src="https://tupian.li/images/2024/05/30/66587bf03ae15.png" alt="å¿½ç•¥åŒºåŸŸèŒƒå›´ç¤ºä¾‹.png" style="width: 80%;" /&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;æ–‡æ¡£è¯†åˆ«&lt;/h3&gt; 
&lt;p align="center"&gt;&lt;img src="https://github.com/hiroi-sora/Umi-OCR/assets/56373419/fc2266ee-b9b7-4079-8b10-6610e6da6cf5" alt="" style="width: 80%;" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;æ–‡æ¡£è¯†åˆ«&lt;/strong&gt;ï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;æ”¯æŒæ ¼å¼ï¼š&lt;code&gt;pdf, xps, epub, mobi, fb2, cbz&lt;/code&gt;ã€‚&lt;/li&gt; 
 &lt;li&gt;å¯¹æ‰«æä»¶è¿›è¡ŒOCRï¼Œæˆ–æå–åŸæœ‰æ–‡æœ¬ã€‚å¯è¾“å‡ºä¸º &lt;strong&gt;åŒå±‚å¯æœç´¢PDF&lt;/strong&gt; ã€‚&lt;/li&gt; 
 &lt;li&gt;æ”¯æŒè®¾å®š &lt;strong&gt;å¿½ç•¥åŒºåŸŸ&lt;/strong&gt; ï¼Œå¯ç”¨äºæ’é™¤é¡µçœ‰é¡µè„šçš„æ–‡å­—ã€‚&lt;/li&gt; 
 &lt;li&gt;å¯è®¾ç½®ä»»åŠ¡å®Œæˆå &lt;strong&gt;è‡ªåŠ¨å…³æœº/ä¼‘çœ &lt;/strong&gt; ã€‚&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h3&gt;äºŒç»´ç &lt;/h3&gt; 
&lt;p align="center"&gt;&lt;img src="https://tupian.li/images/2023/11/19/655991268d6b1.png" alt="4-äºŒç»´ç -1.png" style="width: 80%;" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;æ‰«ç &lt;/strong&gt;ï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;æˆªå›¾/ç²˜è´´/æ‹–å…¥æœ¬åœ°å›¾ç‰‡ï¼Œè¯»å–å…¶ä¸­çš„äºŒç»´ç ã€æ¡å½¢ç ã€‚&lt;/li&gt; 
 &lt;li&gt;æ”¯æŒä¸€å›¾å¤šç ã€‚&lt;/li&gt; 
 &lt;li&gt;æ”¯æŒ19ç§åè®®ï¼Œå¦‚ä¸‹ï¼š&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;code&gt;Aztec&lt;/code&gt;,&lt;code&gt;Codabar&lt;/code&gt;,&lt;code&gt;Code128&lt;/code&gt;,&lt;code&gt;Code39&lt;/code&gt;,&lt;code&gt;Code93&lt;/code&gt;,&lt;code&gt;DataBar&lt;/code&gt;,&lt;code&gt;DataBarExpanded&lt;/code&gt;,&lt;code&gt;DataMatrix&lt;/code&gt;,&lt;code&gt;EAN13&lt;/code&gt;,&lt;code&gt;EAN8&lt;/code&gt;,&lt;code&gt;ITF&lt;/code&gt;,&lt;code&gt;LinearCodes&lt;/code&gt;,&lt;code&gt;MatrixCodes&lt;/code&gt;,&lt;code&gt;MaxiCode&lt;/code&gt;,&lt;code&gt;MicroQRCode&lt;/code&gt;,&lt;code&gt;PDF417&lt;/code&gt;,&lt;code&gt;QRCode&lt;/code&gt;,&lt;code&gt;UPCA&lt;/code&gt;,&lt;code&gt;UPCE&lt;/code&gt;&lt;/p&gt; 
&lt;p align="center"&gt;&lt;img src="https://tupian.li/images/2023/11/19/6559911cda737.png" alt="4-äºŒç»´ç -2.png" style="width: 80%;" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;ç”Ÿæˆç &lt;/strong&gt;ï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;è¾“å…¥æ–‡æœ¬ï¼Œç”ŸæˆäºŒç»´ç å›¾ç‰‡ã€‚&lt;/li&gt; 
 &lt;li&gt;æ”¯æŒ19ç§åè®®å’Œ&lt;strong&gt;çº é”™ç­‰çº§&lt;/strong&gt;ç­‰å‚æ•°ã€‚&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h3&gt;å…¨å±€è®¾ç½®&lt;/h3&gt; 
&lt;p align="center"&gt;&lt;img src="https://tupian.li/images/2023/11/19/655991252e780.png" alt="5-å…¨å±€è®¾ç½®-1.png" style="width: 80%;" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;å…¨å±€è®¾ç½®&lt;/strong&gt;ï¼šåœ¨è¿™é‡Œå¯ä»¥è°ƒæ•´è½¯ä»¶çš„å…¨å±€å‚æ•°ã€‚å¸¸ç”¨åŠŸèƒ½å¦‚ä¸‹ï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ä¸€é”®æ·»åŠ å¿«æ·æ–¹å¼æˆ–è®¾ç½®å¼€æœºè‡ªå¯ã€‚&lt;/li&gt; 
 &lt;li&gt;æ›´æ”¹ç•Œé¢&lt;strong&gt;è¯­è¨€&lt;/strong&gt;ã€‚Umiæ”¯æŒç¹ä¸­ã€è‹±è¯­ã€æ—¥è¯­ç­‰è¯­è¨€ã€‚&lt;/li&gt; 
 &lt;li&gt;åˆ‡æ¢ç•Œé¢&lt;strong&gt;ä¸»é¢˜&lt;/strong&gt;ã€‚Umiæ‹¥æœ‰å¤šä¸ªäº®/æš—ä¸»é¢˜ã€‚&lt;/li&gt; 
 &lt;li&gt;è°ƒæ•´ç•Œé¢&lt;strong&gt;æ–‡å­—çš„å¤§å°&lt;/strong&gt;å’Œ&lt;strong&gt;å­—ä½“&lt;/strong&gt;ã€‚&lt;/li&gt; 
 &lt;li&gt;åˆ‡æ¢OCRæ’ä»¶ã€‚&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;æ¸²æŸ“å™¨&lt;/strong&gt;ï¼šè½¯ä»¶ç•Œé¢é»˜è®¤æ”¯æŒæ˜¾å¡åŠ é€Ÿæ¸²æŸ“ã€‚å¦‚æœåœ¨ä½ çš„æœºå™¨ä¸Šå‡ºç°æˆªå±é—ªçƒã€UIé”™ä½çš„æƒ…å†µï¼Œè¯·è°ƒæ•´&lt;code&gt;ç•Œé¢å’Œå¤–è§‚&lt;/code&gt; â†’ &lt;code&gt;æ¸²æŸ“å™¨&lt;/code&gt; ï¼Œå°è¯•åˆ‡æ¢åˆ°ä¸åŒæ¸²æŸ“æ–¹æ¡ˆï¼Œæˆ–å…³é—­ç¡¬ä»¶åŠ é€Ÿã€‚&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;è°ƒç”¨æ¥å£ï¼š&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/docs/README_CLI.md"&gt;å‘½ä»¤è¡Œæ‰‹å†Œ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/docs/http/README.md"&gt;HTTPæ¥å£æ‰‹å†Œ&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;å…³äºé¡¹ç›®ç»“æ„&lt;/h2&gt; 
&lt;h3&gt;å„ä»“åº“ï¼š&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/hiroi-sora/Umi-OCR"&gt;ä¸»ä»“åº“&lt;/a&gt; ğŸ‘ˆ&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/hiroi-sora/Umi-OCR_plugins"&gt;æ’ä»¶åº“&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/hiroi-sora/Umi-OCR_runtime_windows"&gt;Windows è¿è¡Œåº“&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/hiroi-sora/Umi-OCR_runtime_linux"&gt;Linux è¿è¡Œåº“&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;å·¥ç¨‹ç»“æ„ï¼š&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;**&lt;/code&gt; åç¼€è¡¨ç¤ºæœ¬ä»“åº“(&lt;code&gt;ä¸»ä»“åº“&lt;/code&gt;)åŒ…å«çš„å†…å®¹ã€‚&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Umi-OCR
â”œâ”€ Umi-OCR.exe
â”œâ”€ umi-ocr.sh
â””â”€ UmiOCR-data
   â”œâ”€ main.py **
   â”œâ”€ version.py **
   â”œâ”€ qt_res **
   â”‚  â””â”€ é¡¹ç›®qtèµ„æºï¼ŒåŒ…æ‹¬å›¾æ ‡å’Œqmlæºç 
   â”œâ”€ py_src **
   â”‚  â””â”€ é¡¹ç›®pythonæºç 
   â”œâ”€ plugins
   â”‚  â””â”€ æ’ä»¶
   â””â”€ i18n **
      â””â”€ ç¿»è¯‘æ–‡ä»¶
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;æ”¯æŒçš„ç¦»çº¿OCRå¼•æ“ï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/hiroi-sora/PaddleOCR-json"&gt;PaddleOCR-json&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/hiroi-sora/RapidOCR-json"&gt;RapidOCR-json&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;è¿è¡Œç¯å¢ƒæ¡†æ¶ï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/skywind3000/PyStand"&gt;PyStand&lt;/a&gt; å®šåˆ¶ç‰ˆ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;æ„å»ºé¡¹ç›®&lt;/h2&gt; 
&lt;p&gt;è¯·è·³è½¬ä¸‹è¿°ä»“åº“ï¼Œå®Œæˆå¯¹åº”å¹³å°çš„å¼€å‘/è¿è¡Œç¯å¢ƒéƒ¨ç½²ã€‚&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/hiroi-sora/Umi-OCR_runtime_windows"&gt;Windows&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/hiroi-sora/Umi-OCR_runtime_linux"&gt;Linux&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;è½¯ä»¶æœ¬åœ°åŒ–ç¿»è¯‘ï¼š&lt;/h2&gt; 
&lt;p&gt;æœ¬é¡¹ç›®ä½¿ç”¨ Weblate å¹³å°è¿›è¡ŒUIç•Œé¢çš„æœ¬åœ°åŒ–ç¿»è¯‘åä½œã€‚æˆ‘ä»¬æ¬¢è¿ä»»ä½•è¯‘è€…å‚ä¸ç¿»è¯‘å·¥ä½œï¼Œæ‚¨å¯è¿›å…¥æ­¤é“¾æ¥ &lt;a href="https://hosted.weblate.org/engage/umi-ocr/"&gt;Weblate: Umi-OCR&lt;/a&gt; ï¼Œåœ¨çº¿æ ¡å¯¹ã€è¡¥å……ç°æœ‰è¯­è¨€ï¼Œæˆ–æ·»åŠ æ–°è¯­è¨€ã€‚&lt;/p&gt; 
&lt;p&gt;æ„Ÿè°¢ä»¥ä¸‹è¯‘è€…ï¼Œä¸º Umi-OCR è´¡çŒ®äº†æœ¬åœ°åŒ–ç¿»è¯‘å·¥ä½œï¼š&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;è¯‘è€…&lt;/th&gt; 
   &lt;th&gt;è´¡çŒ®è¯­è¨€&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://hosted.weblate.org/user/q021"&gt;bob&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;English, ç¹é«”ä¸­æ–‡, æ—¥æœ¬èª&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/QZGao"&gt;Qingzheng Gao&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;English, ç¹é«”ä¸­æ–‡&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://hosted.weblate.org/user/ChiaLingWeng"&gt;Weng, Chia-Ling&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;English, ç¹é«”ä¸­æ–‡&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://hosted.weblate.org/user/linzow"&gt;linzow&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;English, ç¹é«”ä¸­æ–‡&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://hosted.weblate.org/user/ultramarkorj9"&gt;Marcos i&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;English, PortuguÃªs&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://hosted.weblate.org/user/qwedc001"&gt;Eric Guo&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;English&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://hosted.weblate.org/user/steven0081"&gt;steven0081&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;English&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://hosted.weblate.org/user/random4t4x14"&gt;Brandon Cagle&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;English&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://hosted.weblate.org/user/plum7x"&gt;plum7x&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ç¹é«”ä¸­æ–‡&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://hosted.weblate.org/user/hugoalh"&gt;hugoalh&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ç¹é«”ä¸­æ–‡&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://hosted.weblate.org/user/Anarkiisto"&gt;Anarkiisto&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ç¹é«”ä¸­æ–‡&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://hosted.weblate.org/user/umren190402"&gt;ãƒ‰ã‚³ãƒ¢å…‰&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;æ—¥æœ¬èª&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://hosted.weblate.org/user/ypf"&gt;æ¨é¹&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;PortuguÃªs&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://hosted.weblate.org/user/1969"&gt;Ğ’ÑÑ‡ĞµÑĞ»Ğ°Ğ² ĞĞ½Ğ°Ñ‚Ğ¾Ğ»ÑŒĞµĞ²Ğ¸Ñ‡ ĞœĞ°Ğ»Ñ‹ÑˆĞµĞ²&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Ğ ÑƒÑÑĞºĞ¸Ğ¹&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://hosted.weblate.org/user/muhammadyusuf.kurbonov2002"&gt;Muhammadyusuf Kurbonov&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Ğ ÑƒÑÑĞºĞ¸Ğ¹&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://hosted.weblate.org/user/TamilNeram/"&gt;à®¤à®®à®¿à®´à¯à®¨à¯‡à®°à®®à¯&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;à®¤à®®à®¿à®´à¯&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;å¦‚æœæœ‰ä¿¡æ¯é”™è¯¯æˆ–äººå‘˜ç¼ºæ¼ï¼Œè¯·åœ¨ &lt;a href="https://github.com/hiroi-sora/Umi-OCR/discussions/449"&gt;è¿™ä¸ªè®¨è®º&lt;/a&gt; ä¸­å›å¤ã€‚&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;èµåŠ©&lt;/h2&gt; 
&lt;p&gt;Umi-OCR é¡¹ç›®ä¸»è¦ç”±ä½œè€… &lt;a href="https://github.com/hiroi-sora"&gt;hiroi-sora&lt;/a&gt; ç”¨ä¸šä½™æ—¶é—´åœ¨å¼€å‘å’Œç»´æŠ¤ã€‚å¦‚æœæ‚¨å–œæ¬¢è¿™æ¬¾è½¯ä»¶ï¼Œæ¬¢è¿èµåŠ©ã€‚&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;å›½å†…ç”¨æˆ·å¯é€šè¿‡ &lt;a href="https://afdian.com/a/hiroi-sora"&gt;çˆ±å‘ç”µ&lt;/a&gt; èµåŠ©ä½œè€…ã€‚&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#hiroi-sora/Umi-OCR&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=hiroi-sora/Umi-OCR&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;&lt;a href="https://raw.githubusercontent.com/hiroi-sora/Umi-OCR/main/CHANGE_LOG.md"&gt;æ›´æ–°æ—¥å¿—&lt;/a&gt;&lt;/h2&gt; 
&lt;h2&gt;å¼€å‘è®¡åˆ’&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;å·²å®Œæˆçš„å·¥ä½œ&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;æ ‡ç­¾é¡µæ¡†æ¶ã€‚&lt;/li&gt; 
  &lt;li&gt;OCR APIæ§åˆ¶å™¨ã€‚&lt;/li&gt; 
  &lt;li&gt;OCR ä»»åŠ¡æ§åˆ¶å™¨ã€‚&lt;/li&gt; 
  &lt;li&gt;ä¸»é¢˜ç®¡ç†å™¨ï¼Œæ”¯æŒåˆ‡æ¢æµ…è‰²/æ·±è‰²ä¸»é¢˜ä¸»é¢˜ã€‚&lt;/li&gt; 
  &lt;li&gt;å®ç° &lt;strong&gt;æ‰¹é‡OCR&lt;/strong&gt;ã€‚&lt;/li&gt; 
  &lt;li&gt;å®ç° &lt;strong&gt;æˆªå›¾OCR&lt;/strong&gt;ã€‚&lt;/li&gt; 
  &lt;li&gt;å¿«æ·é”®æœºåˆ¶ã€‚&lt;/li&gt; 
  &lt;li&gt;ç³»ç»Ÿæ‰˜ç›˜èœå•ã€‚&lt;/li&gt; 
  &lt;li&gt;æ–‡æœ¬å—åå¤„ç†ï¼ˆæ’ç‰ˆä¼˜åŒ–ï¼‰ã€‚&lt;/li&gt; 
  &lt;li&gt;å¼•æ“å†…å­˜æ¸…ç†ã€‚&lt;/li&gt; 
  &lt;li&gt;è½¯ä»¶ç•Œé¢å¤šå›½è¯­è¨€ã€‚&lt;/li&gt; 
  &lt;li&gt;å‘½ä»¤è¡Œæ¨¡å¼ã€‚&lt;/li&gt; 
  &lt;li&gt;Win7å…¼å®¹ã€‚&lt;/li&gt; 
  &lt;li&gt;Excelï¼ˆcsvï¼‰è¾“å‡ºæ ¼å¼ã€‚&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;Esc&lt;/code&gt;ä¸­æ–­æˆªå›¾æ“ä½œ&lt;/li&gt; 
  &lt;li&gt;å¤–ç½®ä¸»é¢˜æ–‡ä»¶&lt;/li&gt; 
  &lt;li&gt;å­—ä½“åˆ‡æ¢&lt;/li&gt; 
  &lt;li&gt;åŠ è½½åŠ¨ç”»&lt;/li&gt; 
  &lt;li&gt;å¿½ç•¥åŒºåŸŸã€‚&lt;/li&gt; 
  &lt;li&gt;äºŒç»´ç è¯†åˆ«ã€‚&lt;/li&gt; 
  &lt;li&gt;æ‰¹é‡è¯†åˆ«é¡µé¢çš„å›¾ç‰‡é¢„è§ˆçª—å£ã€‚&lt;/li&gt; 
  &lt;li&gt;PDFè¯†åˆ«ã€‚&lt;/li&gt; 
  &lt;li&gt;è°ƒç”¨æœ¬åœ°å›¾ç‰‡æµè§ˆå™¨æ‰“å¼€å›¾ç‰‡ã€‚ &lt;a href="https://github.com/hiroi-sora/Umi-OCR/issues/335"&gt;#335&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;é‡å¤ä¸Šä¸€æ¬¡æˆªå›¾ã€‚ &lt;a href="https://github.com/hiroi-sora/Umi-OCR/issues/357"&gt;#357&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;ä¿®Bugï¼šæ–‡æ¡£è¯†åˆ«åœ¨Windows7ç³»ç»Ÿçš„å…¼å®¹æ€§é—®é¢˜ã€‚&lt;/li&gt; 
  &lt;li&gt;HTTP/å‘½ä»¤è¡Œæ¥å£æ·»åŠ äºŒç»´ç è¯†åˆ«/ç”ŸæˆåŠŸèƒ½ã€‚ (#423)&lt;/li&gt; 
  &lt;li&gt;äºŒç»´ç æ¥å£çš„æ–‡æ¡£ã€‚&lt;/li&gt; 
  &lt;li&gt;Linux å¹³å°ç§»æ¤ã€‚&lt;/li&gt; 
  &lt;li&gt;HTTP æ–‡æ¡£è¯†åˆ«æ¥å£ã€‚&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;!-- ##### æ­£åœ¨è¿›è¡Œçš„å·¥ä½œ --&gt; 
&lt;h5&gt;è¿œæœŸè®¡åˆ’&lt;/h5&gt; 
&lt;details&gt; 
 &lt;summary&gt;å±•å¼€&lt;/summary&gt; 
 &lt;p&gt;è¿™äº›æ˜¯é¢„æƒ³ä¸­çš„åŠŸèƒ½ï¼Œåœ¨å¼€å‘åˆæœŸå·²é¢„ç•™å¥½æ¥å£ï¼Œå°†åœ¨è¿œæœŸæ…¢æ…¢å®ç°ã€‚&lt;/p&gt; 
 &lt;p&gt;ä½†å¼€å‘é€”ä¸­å—é™äºå®é™…æƒ…å†µï¼Œå¯èƒ½æ›´æ”¹åŠŸèƒ½è®¾è®¡ã€æ–°å¢åŠå–æ¶ˆåŠŸèƒ½ã€‚&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;p&gt;é‡æ„åº•å±‚æ’ä»¶æœºåˆ¶ã€‚&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;p&gt;åœ¨çº¿ OCR API æ’ä»¶ã€‚&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;p&gt;ç‹¬ç«‹çš„æ•°å­¦å…¬å¼è¯†åˆ«æ’ä»¶ã€‚&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;p&gt;â€œæ•°å­¦å…¬å¼â€æ ‡ç­¾é¡µï¼Œæä¾›ç‹¬ç«‹çš„æ•°å­¦å…¬å¼è¯†åˆ«/Latexæ¸²æŸ“ã€‚&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;p&gt;æ£€æŸ¥æ›´æ–°æœºåˆ¶ã€‚&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;p&gt;æ’ç‰ˆè§£æä¹‹å¤–çš„æ–‡æœ¬åå¤„ç†æ¨¡å—ï¼ˆå¦‚ä¿ç•™æ•°å­—ã€åŠå…¨è§’å­—ç¬¦è½¬æ¢ã€æ–‡æœ¬çº é”™ï¼‰ã€‚&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;p&gt;å…³é”®æ¥å£å‡½æ•°æ·»åŠ äº‹ä»¶è§¦å‘æ–¹å¼ã€‚&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;åŸºäºGPUçš„ç¦»çº¿OCRã€‚&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;å›¾ç‰‡ç¿»è¯‘&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;ç¦»çº¿ç¿»è¯‘ã€‚&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;å›ºå®šåŒºåŸŸè¯†åˆ«ã€‚&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;è¯†åˆ«è¡¨æ ¼å›¾ç‰‡ï¼Œè¾“å‡ºä¸ºExcelã€‚&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;å†å²è®°å½•ç³»ç»Ÿã€‚&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;å…¼å®¹ MacOS / Ubuntu ç­‰å¹³å°ã€‚&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt;</description>
    </item>
    
    <item>
      <title>NEKOparapa/AiNiee</title>
      <link>https://github.com/NEKOparapa/AiNiee</link>
      <description>&lt;p&gt;ä¸€æ¬¾ä¸“æ³¨äºAiç¿»è¯‘çš„å·¥å…·ï¼Œä¸€é”®è‡ªåŠ¨ç¿»è¯‘RPG SLGæ¸¸æˆï¼ŒEpub TXTå°è¯´ï¼ŒSrt Vtt Lrcå­—å¹•ï¼ŒWord MDæ–‡æ¡£ç­‰ç­‰å¤æ‚é•¿æ–‡æœ¬ã€‚&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;a href="https://github.com/NEKOparapa/AiNiee-chatgpt"&gt; &lt;img src="https://github.com/NEKOparapa/AiNiee-chatgpt/raw/main/Example%20image/logo.png" width="60%" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://raw.githubusercontent.com/NEKOparapa/AiNiee/main/README_EN.md"&gt;English&lt;/a&gt; | ç®€ä½“ä¸­æ–‡ 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;è½¯ä»¶ä»‹ç»ğŸ§¾&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;strong&gt;AiNiee&lt;/strong&gt; æ˜¯ä¸€æ¬¾ä¸“æ³¨äº AI ç¿»è¯‘çš„å·¥å…·ï¼Œ
 &lt;br /&gt;ä¸€é”®è‡ªåŠ¨ç¿»è¯‘æ¸¸æˆã€ä¹¦ç±ã€å­—å¹•ã€æ–‡æ¡£ç­‰å¤æ‚é•¿æ–‡æœ¬å†…å®¹ã€‚ 
&lt;/div&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;æ ¼å¼å…¨èƒ½ï¼Œè¦†ç›–å¹¿æ³›&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;ğŸ® &lt;strong&gt;æ¸¸æˆç¿»è¯‘&lt;/strong&gt;ï¼šæ·±åº¦æ”¯æŒ Mtool, Renpy, Translator++, ParaTranzr, VNText, SExtractor ç­‰æ¸¸æˆæ–‡æœ¬å¯¼å‡ºå·¥å…·ã€‚&lt;/li&gt; 
   &lt;li&gt;ğŸ“š &lt;strong&gt;å¤šæ ·æ”¯æŒ&lt;/strong&gt;ï¼šè½»æ¾å¤„ç† I18Next æ•°æ®ã€Epub/TXT ç”µå­ä¹¦ã€Srt/Vtt/Lrc å­—å¹•ã€Word/PDF/MD æ–‡æ¡£ç­‰ã€‚&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;æ™ºèƒ½é«˜æ•ˆï¼Œçœæ—¶çœå¿ƒ&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;ğŸš€ &lt;strong&gt;ä¸€é”®æ“ä½œ&lt;/strong&gt;ï¼šä¸€æ‹–ä¸€ç‚¹ï¼Œè‡ªåŠ¨è¯†åˆ«æ–‡ä»¶ä¸è¯­è¨€ï¼Œæ— éœ€è®¾ç½®ã€‚&lt;/li&gt; 
   &lt;li&gt;â±ï¸ &lt;strong&gt;æé€Ÿç¿»è¯‘&lt;/strong&gt;ï¼šå–æ¯å¯ä¹çš„å·¥å¤«ï¼Œå°±èƒ½æ‹¿åˆ°è¯‘æ–‡ã€‚&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;é•¿æ–‡ä¼˜åŒ–ï¼Œè´¨é‡å‡ºä¼—&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;ğŸ¯ &lt;strong&gt;çªç ´å±€é™&lt;/strong&gt;ï¼šé‡‡ç”¨è½»ç›ˆç¿»è¯‘æ ¼å¼ã€æ€ç»´é“¾ç¿»è¯‘ã€AI æœ¯è¯­è¡¨ã€ä¸Šä¸‹æ–‡å…³è”ç­‰æŠ€æœ¯ï¼Œç¡®ä¿é•¿æ–‡æœ¬ç¿»è¯‘çš„è¿è´¯æ€§ä¸å‡†ç¡®æ€§ã€‚&lt;/li&gt; 
   &lt;li&gt;ğŸ’ &lt;strong&gt;è´¨é‡è¿½æ±‚&lt;/strong&gt;ï¼šæ”¯æŒ åŸºç¡€æç¤ºã€è§’è‰²ä»‹ç»ã€èƒŒæ™¯è®¾å®šã€ç¿»è¯‘é£æ ¼ ç­‰æç¤ºè¯è°ƒæ•´ï¼Œæ‹¥æœ‰ ä¸€é”®AIæ¶¦è‰²ã€ä¸€é”®AIæ’ç‰ˆã€ä¸€é”®æå–æœ¯è¯­ ç­‰åŠŸèƒ½ï¼Œæ»¡è¶³å¯¹ç¿»è¯‘è´¨é‡æœ‰æ›´é«˜è¦æ±‚çš„ç”¨æˆ·ã€‚&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;AiNieeä¸‰æ­¥èµ° ğŸ“¢&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ç¬¬ä¸€æ­¥ï¼šé…ç½®æ¥å£&lt;/strong&gt;&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;img src="https://raw.githubusercontent.com/NEKOparapa/AiNiee/main/Example%20image/ä¸‰æ­¥èµ°/ç¬¬ä¸€æ­¥.png" /&gt; 
  &lt;/blockquote&gt; 
  &lt;ul&gt; 
   &lt;li&gt;åœ¨çº¿æ¥å£ï¼šéœ€ä»˜è´¹ä½†æ€§ä»·æ¯”å¾ˆé«˜ï¼Œæ— æ˜¾å¡è¦æ±‚ï¼Œå…¨è¯­è¨€æ”¯æŒï¼Œ&lt;a href="https://github.com/NEKOparapa/AiNiee/wiki/QuickStartDeepSeek"&gt;æ¥å£è®¾ç½®è¯´æ˜ - DeepSeek&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;åœ¨çº¿æ¥å£ï¼šåŒä¸Šï¼Œå¦‚æœDeepseekå®˜ç½‘æ— æ³•æ­£å¸¸ä½¿ç”¨ï¼Œå¯æ¢è¯¥æ¥å£ï¼Œ&lt;a href="https://github.com/NEKOparapa/AiNiee/wiki/QuickStartHuo"&gt;æ¥å£è®¾ç½®è¯´æ˜ - ç«å±±å¼•æ“&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ç¬¬äºŒæ­¥ï¼šæ‹–å…¥æ–‡ä»¶å¤¹&lt;/strong&gt;&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;img src="https://raw.githubusercontent.com/NEKOparapa/AiNiee/main/Example%20image/ä¸‰æ­¥èµ°/ç¬¬äºŒæ­¥.png" /&gt; 
  &lt;/blockquote&gt; 
  &lt;ul&gt; 
   &lt;li&gt;è¾“å…¥æ–‡ä»¶å¤¹ï¼šå°†åŸæ–‡æ–‡ä»¶å•ç‹¬æ”¾ç½®æ–°çš„æ–‡ä»¶å¤¹ï¼Œå¹¶å°†è¯¥æ–‡ä»¶å¤¹æ‹–å…¥æ¡†å†…ã€‚å°è¯´ã€å­—å¹•ã€æ–‡æ¡£å¯ç›´æ¥è¿›è¡Œç¿»è¯‘ï¼Œæ¸¸æˆéœ€è¦æ–‡æœ¬æå–å·¥å…·è¿›è¡Œé…åˆã€‚&lt;br /&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ç¬¬ä¸‰æ­¥ï¼šå¼€å§‹ç¿»è¯‘&lt;/strong&gt;&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;img src="https://raw.githubusercontent.com/NEKOparapa/AiNiee/main/Example%20image/ä¸‰æ­¥èµ°/ç¬¬ä¸‰æ­¥.png" /&gt; 
  &lt;/blockquote&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;ç‚¹å‡»å¼€å§‹æŒ‰é’®ï¼Œå‰©ä¸‹ç­‰å¾…ä»»åŠ¡çš„å®Œæˆã€‚&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/NEKOparapa/AiNiee/releases"&gt;AiNieeä¸‹è½½åœ°å€&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;h2&gt;æ¸¸æˆç¿»è¯‘&lt;a href="https://raw.githubusercontent.com/NEKOparapa/AiNiee/main/#%E6%B8%B8%E6%88%8F%E7%BF%BB%E8%AF%91"&gt;&lt;img src="https://raw.githubusercontent.com/aregtech/areg-sdk/master/docs/img/pin.svg?sanitize=true" alt="" /&gt;&lt;/a&gt;&lt;/h2&gt; &lt;/summary&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;h3&gt;å·¥å…·å‡†å¤‡&lt;/h3&gt; &lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;ğŸ“–æ¸¸æˆæ–‡æœ¬æå–å·¥å…·&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt; 
    &lt;table&gt; 
     &lt;thead&gt; 
      &lt;tr&gt; 
       &lt;th align="center"&gt;å·¥å…·å&lt;/th&gt; 
       &lt;th align="center"&gt;ä»‹ç»&lt;/th&gt; 
       &lt;th align="center"&gt;é¡¹ç›®ç±»å‹&lt;/th&gt; 
      &lt;/tr&gt; 
     &lt;/thead&gt; 
     &lt;tbody&gt; 
      &lt;tr&gt; 
       &lt;td align="center"&gt;&lt;a href="https://afdian.com/p/d42dd1e234aa11eba42452540025c377"&gt;Mtool&lt;/a&gt;&lt;/td&gt; 
       &lt;td align="center"&gt;ä¸Šæ‰‹ç®€å•ï¼Œæ¨èæ–°äººä½¿ç”¨&lt;/td&gt; 
       &lt;td align="center"&gt;Mtoolå¯¼å‡ºæ–‡ä»¶&lt;/td&gt; 
      &lt;/tr&gt; 
      &lt;tr&gt; 
       &lt;td align="center"&gt;&lt;a href="https://dreamsavior.net/download/"&gt;Translator++&lt;/a&gt;&lt;/td&gt; 
       &lt;td align="center"&gt;ä¸Šæ‰‹å¤æ‚ï¼ŒåŠŸèƒ½å¼ºå¤§ï¼Œæ¨èå¤§ä½¬ä½¿ç”¨&lt;/td&gt; 
       &lt;td align="center"&gt;T++å¯¼å‡ºæ–‡ä»¶æˆ–Transå·¥ç¨‹æ–‡ä»¶&lt;/td&gt; 
      &lt;/tr&gt; 
      &lt;tr&gt; 
       &lt;td align="center"&gt;&lt;a href="https://paratranz.cn/projects"&gt;ParaTranzr&lt;/a&gt;&lt;/td&gt; 
       &lt;td align="center"&gt;ä¸Šæ‰‹ä¸­ç­‰ï¼ŒåŠŸèƒ½å¼ºå¤§ï¼Œæ¨èå¤§ä½¬ä½¿ç”¨&lt;/td&gt; 
       &lt;td align="center"&gt;ParaTranzrå¯¼å‡ºæ–‡ä»¶&lt;/td&gt; 
      &lt;/tr&gt; 
      &lt;tr&gt; 
       &lt;td align="center"&gt;&lt;a href="https://www.renpy.org/latest.html"&gt;RenPy SDK&lt;/a&gt;&lt;/td&gt; 
       &lt;td align="center"&gt;ä¸Šæ‰‹ä¸­ç­‰ï¼ŒåŠŸèƒ½å¼ºå¤§ï¼Œæ¨èå¤§ä½¬ä½¿ç”¨&lt;/td&gt; 
       &lt;td align="center"&gt;renpyå¯¼å‡ºæ–‡ä»¶&lt;/td&gt; 
      &lt;/tr&gt; 
     &lt;/tbody&gt; 
    &lt;/table&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;ğŸ§°æœ¬åœ°æ¨¡å‹è¿è¡Œå·¥å…·&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt; 
    &lt;table&gt; 
     &lt;thead&gt; 
      &lt;tr&gt; 
       &lt;th align="center"&gt;å·¥å…·å&lt;/th&gt; 
       &lt;th align="center"&gt;è¯´æ˜&lt;/th&gt; 
      &lt;/tr&gt; 
     &lt;/thead&gt; 
     &lt;tbody&gt; 
      &lt;tr&gt; 
       &lt;td align="center"&gt;&lt;a href="https://github.com/PiDanShouRouZhouXD/Sakura_Launcher_GUI"&gt;Sakura_Launcher_GUI&lt;/a&gt;&lt;/td&gt; 
       &lt;td align="center"&gt;Sakuraæ¨¡å‹çš„ä¸“å±GUIå¯åŠ¨å™¨&lt;/td&gt; 
      &lt;/tr&gt; 
      &lt;tr&gt; 
       &lt;td align="center"&gt;&lt;a href="https://lmstudio.ai/download"&gt;LM Studio&lt;/a&gt;&lt;/td&gt; 
       &lt;td align="center"&gt;ä¸€ä¸ªæœ¬åœ°éƒ¨ç½²å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¹³å°ï¼Œè‡´åŠ›äºç®€åŒ–LLMçš„ä½¿ç”¨å’Œç®¡ç†ã€‚&lt;/td&gt; 
      &lt;/tr&gt; 
      &lt;tr&gt; 
       &lt;td align="center"&gt;&lt;a href="https://ollama.com/"&gt;ollama&lt;/a&gt;&lt;/td&gt; 
       &lt;td align="center"&gt;å¼€æºè·¨å¹³å°å¤§æ¨¡å‹å·¥å…·&lt;/td&gt; 
      &lt;/tr&gt; 
     &lt;/tbody&gt; 
    &lt;/table&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;h3&gt;ç¿»è¯‘æ•™ç¨‹&lt;/h3&gt; &lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;ğŸ“ºæ¸¸æˆç¿»è¯‘è§†é¢‘æ•™ç¨‹&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt; 
    &lt;table&gt; 
     &lt;thead&gt; 
      &lt;tr&gt; 
       &lt;th align="center"&gt;è§†é¢‘é“¾æ¥&lt;/th&gt; 
       &lt;th align="center"&gt;è¯´æ˜&lt;/th&gt; 
      &lt;/tr&gt; 
     &lt;/thead&gt; 
     &lt;tbody&gt; 
      &lt;tr&gt; 
       &lt;td align="center"&gt;&lt;a href="https://www.bilibili.com/video/BV1h6421c7MA"&gt;Mtoolæ•™ç¨‹&lt;/a&gt;&lt;/td&gt; 
       &lt;td align="center"&gt;åˆæ¬¡ä½¿ç”¨æ¨èè§‚çœ‹&lt;/td&gt; 
      &lt;/tr&gt; 
      &lt;tr&gt; 
       &lt;td align="center"&gt;&lt;a href="https://www.bilibili.com/video/BV1LgfoYzEaX/?share_source=copy_web&amp;amp;vd_source=b0eede35fc5eaa5c382509c6040d6501"&gt;Translator++æ•™ç¨‹&lt;/a&gt;&lt;/td&gt; 
       &lt;td align="center"&gt;åˆæ¬¡ä½¿ç”¨æ¨èè§‚çœ‹&lt;/td&gt; 
      &lt;/tr&gt; 
      &lt;tr&gt; 
       &lt;td align="center"&gt;&lt;a href="https://www.bilibili.com/video/BV1SnXbYiEjQ/?share_source=copy_web&amp;amp;vd_source=b0eede35fc5eaa5c382509c6040d6501"&gt;Wolfæ¸¸æˆæ•™ç¨‹&lt;/a&gt;&lt;/td&gt; 
       &lt;td align="center"&gt;åˆæ¬¡ä½¿ç”¨æ¨èè§‚çœ‹&lt;/td&gt; 
      &lt;/tr&gt; 
      &lt;tr&gt; 
       &lt;td align="center"&gt;&lt;a href="https://www.bilibili.com/video/BV1j1VyzqERD/?share_source=copy_web&amp;amp;vd_source=b0eede35fc5eaa5c382509c6040d6501"&gt;äººåè¯»å–æ•™ç¨‹&lt;/a&gt;&lt;/td&gt; 
       &lt;td align="center"&gt;è¿›é˜¶ç¿»è¯‘æ¨èè§‚çœ‹&lt;/td&gt; 
      &lt;/tr&gt; 
     &lt;/tbody&gt; 
    &lt;/table&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;ğŸ«æ¸¸æˆç¿»è¯‘å›¾æ–‡æ•™ç¨‹&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt; 
    &lt;table&gt; 
     &lt;thead&gt; 
      &lt;tr&gt; 
       &lt;th align="center"&gt;è§†é¢‘é“¾æ¥&lt;/th&gt; 
       &lt;th align="center"&gt;è¯´æ˜&lt;/th&gt; 
      &lt;/tr&gt; 
     &lt;/thead&gt; 
     &lt;tbody&gt; 
      &lt;tr&gt; 
       &lt;td align="center"&gt;&lt;a href="https://github.com/NEKOparapa/AiNiee/wiki/%E6%B8%B8%E6%88%8F%E7%BF%BB%E8%AF%91%E2%80%90Mtool"&gt;Mtoolæ•™ç¨‹&lt;/a&gt;&lt;/td&gt; 
       &lt;td align="center"&gt;é€‚åˆæ–°äººï¼Œæ‡’äººç¿»è¯‘RPG,RenPY,Krkrç­‰æ¸¸æˆï¼Œè¿›è¡Œå¤–æŒ‚å¼ç¿»è¯‘&lt;/td&gt; 
      &lt;/tr&gt; 
      &lt;tr&gt; 
       &lt;td align="center"&gt;&lt;a href="https://github.com/NEKOparapa/AiNiee/wiki/%E6%B8%B8%E6%88%8F%E7%BF%BB%E8%AF%91%E2%80%90Translator--%EF%BC%88%E5%B7%A5%E7%A8%8B%E6%96%87%E4%BB%B6%E7%89%88%EF%BC%89"&gt;Translator++æ•™ç¨‹&lt;/a&gt;&lt;/td&gt; 
       &lt;td align="center"&gt;é€‚åˆç¿»è¯‘RPG,RenPY,Krkrç­‰ç­‰æ¸¸æˆï¼Œè¿›è¡Œå†…åµŒå¼ç¿»è¯‘&lt;/td&gt; 
      &lt;/tr&gt; 
      &lt;tr&gt; 
       &lt;td align="center"&gt;&lt;a href="https://github.com/NEKOparapa/AiNiee/wiki/%E6%B8%B8%E6%88%8F%E7%BF%BB%E8%AF%91%E2%80%90Paratranz"&gt;Paratranzæ•™ç¨‹&lt;/a&gt;&lt;/td&gt; 
       &lt;td align="center"&gt;é€‚åˆç¿»è¯‘å„ç±»å¤§å‹æ¸¸æˆçš„MOD&lt;/td&gt; 
      &lt;/tr&gt; 
      &lt;tr&gt; 
       &lt;td align="center"&gt;&lt;a href="https://github.com/NEKOparapa/AiNiee/wiki/%E6%B8%B8%E6%88%8F%E7%BF%BB%E8%AF%91%E2%80%90StevExtraction"&gt;StevExtractionæ•™ç¨‹&lt;/a&gt;&lt;/td&gt; 
       &lt;td align="center"&gt;é€‚åˆç¿»è¯‘RPGmakerMZ/MZæ¸¸æˆ&lt;/td&gt; 
      &lt;/tr&gt; 
      &lt;tr&gt; 
       &lt;td align="center"&gt;&lt;a href="https://zhuanlan.zhihu.com/p/1894065679927313655"&gt;Unityç¿»è¯‘æ•™ç¨‹&lt;/a&gt;&lt;/td&gt; 
       &lt;td align="center"&gt;é€‚åˆç¿»è¯‘unityæ¸¸æˆ&lt;/td&gt; 
      &lt;/tr&gt; 
      &lt;tr&gt; 
       &lt;td align="center"&gt;&lt;a href="https://www.notion.so/AI-1d43d31f89b280f6bd61e12580652ce5?pvs=4"&gt;ç»¼åˆæ¸¸æˆç¿»è¯‘è¶…è¯¦ç»†æ•™ç¨‹&lt;/a&gt;&lt;/td&gt; 
       &lt;td align="center"&gt;é€‚åˆç¿»è¯‘å„ç±»æ¸¸æˆï¼Œåˆ¶ä½œé«˜è´¨é‡çš„å†…åµŒè¡¥ä¸&lt;/td&gt; 
      &lt;/tr&gt; 
     &lt;/tbody&gt; 
    &lt;/table&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;h2&gt;åŠŸèƒ½è¯´æ˜&lt;a href="https://raw.githubusercontent.com/NEKOparapa/AiNiee/main/#%E5%8A%9F%E8%83%BD%E8%AF%B4%E6%98%8E"&gt;&lt;img src="https://raw.githubusercontent.com/aregtech/areg-sdk/master/docs/img/pin.svg?sanitize=true" alt="" /&gt;&lt;/a&gt;&lt;/h2&gt; &lt;/summary&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;h3&gt;è®¾ç½®è¯´æ˜&lt;/h3&gt; &lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://github.com/NEKOparapa/AiNiee/wiki/%E5%8A%9F%E8%83%BD%E2%80%90%E6%8E%A5%E5%8F%A3%E7%AE%A1%E7%90%86"&gt;åŠŸèƒ½ â€ æ¥å£ç®¡ç†&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;h3&gt;è¡¨æ ¼è¯´æ˜&lt;/h3&gt; &lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://github.com/NEKOparapa/AiNiee/wiki/%E8%A1%A8%E6%A0%BC%E2%80%90AI%E6%9C%AF%E8%AF%AD%E8%A1%A8%E4%BB%8B%E7%BB%8D"&gt;è¡¨æ ¼ - AIæœ¯è¯­è¡¨&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/NEKOparapa/AiNiee/wiki/%E8%A1%A8%E6%A0%BC%E2%80%90AI%E7%A6%81%E7%BF%BB%E8%A1%A8%E4%BB%8B%E7%BB%8D"&gt;è¡¨æ ¼ - AIç¦ç¿»è¡¨&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/NEKOparapa/AiNiee/wiki/%E8%A1%A8%E6%A0%BC%E2%80%90%E6%96%87%E6%9C%AC%E6%9B%BF%E6%8D%A2%E4%BB%8B%E7%BB%8D"&gt;è¡¨æ ¼ - æ–‡æœ¬æ›¿æ¢&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;h3&gt;æ’ä»¶è¯´æ˜&lt;/h3&gt; &lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://github.com/NEKOparapa/AiNiee/wiki/%E6%8F%92%E4%BB%B6%E2%80%90LanguageFilter"&gt;æ’ä»¶ - è¯­è¨€è¿‡æ»¤å™¨&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/NEKOparapa/AiNiee/wiki/%E6%8F%92%E4%BB%B6%E2%80%90TextNormalizer"&gt;æ’ä»¶ - æ–‡æœ¬è§„èŒƒå™¨&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt; &lt;h3&gt;å…¶ä»–è¯´æ˜&lt;/h3&gt; &lt;/summary&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;&lt;code&gt; å¤škeyè½®è¯¢&lt;/code&gt;&lt;/p&gt; 
    &lt;blockquote&gt; 
     &lt;p&gt;å¦‚æœæƒ³ä½¿ç”¨å¤šä¸ªkeyæ¥åˆ†æ‹…æ¶ˆè€—å‹åŠ›ï¼Œæ ¹æ®keyæ•°é‡è¿›è¡ŒåŠ é€Ÿç¿»è¯‘ï¼Œè¯·ä½¿ç”¨åŒç±»å‹è´¦å·çš„keyï¼Œè€Œä¸”è¾“å…¥æ—¶åœ¨æ¯ä¸ªkeyä¸­é—´åŠ ä¸Šè‹±æ–‡é€—å·ï¼Œä¸è¦æ¢è¡Œã€‚ä¾‹å¦‚ï¼škey1,key2,key3&lt;/p&gt; 
    &lt;/blockquote&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;code&gt; æ‰¹é‡æ–‡ä»¶ç¿»è¯‘&lt;/code&gt;&lt;/p&gt; 
    &lt;blockquote&gt; 
     &lt;p&gt;æŠŠæ‰€æœ‰éœ€è¦ç¿»è¯‘çš„æ–‡ä»¶æ”¾åœ¨è¾“å…¥æ–‡ä»¶å¤¹å³å¯ï¼Œä¹Ÿæ”¯æŒå¤šæ–‡ä»¶å¤¹ç»“æ„&lt;/p&gt; 
    &lt;/blockquote&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;code&gt; é…ç½®è¿ç§»&lt;/code&gt;&lt;/p&gt; 
    &lt;blockquote&gt; 
     &lt;p&gt;é…ç½®ä¿¡æ¯éƒ½ä¼šå­˜å‚¨åœ¨resourceçš„config.jsonä¸­ï¼Œä¸‹è½½æ–°ç‰ˆæœ¬å¯ä»¥æŠŠå®ƒå¤åˆ¶åˆ°æ–°ç‰ˆæœ¬çš„resourceä¸­ã€‚&lt;/p&gt; 
    &lt;/blockquote&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;h2&gt;è´¡çŒ®æŒ‡å—&lt;a href="https://raw.githubusercontent.com/NEKOparapa/AiNiee/main/#%E8%B4%A1%E7%8C%AE%E6%8C%87%E5%8D%97"&gt;&lt;img src="https://raw.githubusercontent.com/aregtech/areg-sdk/master/docs/img/pin.svg?sanitize=true" alt="" /&gt;&lt;/a&gt;&lt;/h2&gt; &lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;å¼€å‘å¢å¼ºæ’ä»¶&lt;/code&gt;&lt;/strong&gt;: è¯·æ ¹æ®&lt;a href="https://github.com/NEKOparapa/AiNiee/raw/main/PluginScripts/README.md"&gt;æ’ä»¶ç¼–å†™æŒ‡å—&lt;/a&gt;è¿›è¡Œå¼€å‘æ›´å¼ºåŠŸèƒ½æ’ä»¶&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;æ”¹è¿›æˆ–å¢åŠ æ”¯æŒæ–‡ä»¶&lt;/code&gt;&lt;/strong&gt;: éœ€è¦æœ‰ä¸€å®šçš„ä»£ç ç¼–ç¨‹èƒ½åŠ›ï¼Œæ‹‰å–æºç è¿›è¡Œæ”¹è¿›ã€‚æ–‡ä»¶å…·ä½“è¯»å–ä»£ç åœ¨ModuleFolders\FileReaderä¸FileOutputeræ–‡ä»¶å¤¹ä¸­ã€‚&lt;a href="https://github.com/NEKOparapa/AiNiee/raw/main/ModuleFolders/FileAccessor/README.md"&gt;è¯»å†™å™¨ç³»ç»Ÿç¼–å†™æŒ‡å—&lt;/a&gt;ã€‚UIæ”¯æŒåœ¨UserInterface\Settingçš„ProjectSettingsPageã€‚&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;å®Œå–„æ­£åˆ™åº“&lt;/code&gt;&lt;/strong&gt;: æ­£åˆ™åº“çš„å®Œå¤‡å°†æå¤§å¸®åŠ©æ¸¸æˆå†…åµŒå·¥ä½œçš„è¿›è¡Œï¼Œå¹¶åˆ©å¥½ä¸‹ä¸€æ¬¡æ¸¸æˆç¿»è¯‘å·¥ä½œå’Œé€ ç¦å…¶ä»–ç¿»è¯‘ç”¨æˆ·ï¼Œæ­£åˆ™åº“åœ¨&lt;a href="https://github.com/NEKOparapa/AiNiee/raw/main/Resource/Regex/regex.json"&gt;Resource\Regex&lt;/a&gt;æ–‡ä»¶å¤¹ä¸­&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;æ”¹è¿›ç•Œé¢ç¿»è¯‘&lt;/code&gt;&lt;/strong&gt;: å¤šè¯­è¨€ç•Œé¢çš„UIæ–‡æœ¬å¯èƒ½ç¿»è¯‘ä¸å¤Ÿå‡†ç¡®åˆé€‚ï¼Œå¯ä»¥æäº¤ä½ çš„ä¿®æ”¹æ„è§ï¼Œæˆ–è€…ç›´æ¥è¿›è¡Œä¿®æ”¹ã€‚æœ¬åœ°åŒ–æ–‡æœ¬åœ¨&lt;a href="https://github.com/NEKOparapa/AiNiee/tree/main/Resource/Localization"&gt;Resource\Localization&lt;/a&gt;æ–‡ä»¶å¤¹ä¸­&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ç‰¹åˆ«å£°æ˜&lt;a href="https://raw.githubusercontent.com/NEKOparapa/AiNiee/main/#%E7%89%B9%E5%88%AB%E5%A3%B0%E6%98%8E"&gt;&lt;img src="https://raw.githubusercontent.com/aregtech/areg-sdk/master/docs/img/pin.svg?sanitize=true" alt="" /&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;AiNieeèƒ½å¤Ÿä¸æ–­å‘å±•è¿­ä»£è‡³ä»Šï¼Œå…¶å…³é”®åŠŸèƒ½æ¡†æ¶å‡æºäºé¡¹ç›®åˆ›ç«‹ä»¥æ¥çš„æŒç»­ä¸ªäººç ”å‘ã€ç”¨æˆ·åé¦ˆå»ºè®®ä»¥åŠå¤§ä½¬ä»¬PRçš„å…±åŒåŠªåŠ›ä¸åˆ›é€ ã€‚ è¿™æ˜¯ä¸¤å¹´ä»¥æ¥ä¸€ä¸ªä¸æ–­æ‘¸ç´¢ã€æŒç»­æ”¹è¿›ã€å…±åŒæ„ç­‘çš„è¿‡ç¨‹ï¼Œæ‰å½¢æˆäº†AiNieeå¦‚ä»Šç›¸å¯¹æˆç†Ÿå’Œå®Œæ•´çš„AIç¿»è¯‘ä½“ç³»ã€‚ è¯·å¤§å®¶åœ¨ä½¿ç”¨å’Œå­¦ä¹ ä¹‹ä½™ï¼Œå°Šé‡å¼€æºç²¾ç¥ï¼Œç½²åæ¥æºé¡¹ç›®ï¼Œå¹¶ä¸å¿˜äº†ç»™é¡¹ç›®ç‚¹ä¸ªstarã€‚&lt;/p&gt; 
&lt;p&gt;è¯¥æ¬¾AIç¿»è¯‘å·¥å…·ä»…ä¾›ä¸ªäººåˆæ³•ç”¨é€”,ä»»ä½•ä½¿ç”¨è¯¥å·¥å…·è¿›è¡Œç›´æ¥æˆ–è€…é—´æ¥éæ³•ç›ˆåˆ©æ´»åŠ¨çš„è¡Œä¸º,å‡ä¸å±äºæˆæƒèŒƒå›´,ä¹Ÿä¸å—åˆ°ä»»ä½•æ”¯æŒå’Œè®¤å¯ã€‚&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;äº¤â™‚äº¤æµç¾¤&lt;/code&gt;&lt;/strong&gt;: QQäº¤æµç¾¤(ä¸»è¦æ´»è·ƒï¼Œç­”æ¡ˆï¼šgithub)ï¼š8216248ä¹é›¶ï¼Œå¤‡ç”¨TGç¾¤ï¼š&lt;a href="https://t.me/+JVHbDSGo8SI2Njhl"&gt;https://t.me/+JVHbDSGo8SI2Njhl&lt;/a&gt; ,&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;èµåŠ©ğŸ’–&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/NEKOparapa/AiNiee-chatgpt/main/Example%20image/Sponsor/%E8%B5%9E%E8%B5%8F%E7%A0%81.png"&gt;&lt;img src="https://raw.githubusercontent.com/NEKOparapa/AiNiee-chatgpt/main/Example%20image/Sponsor/%E5%BE%BD%E7%AB%A0.png" alt="xxxx" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>resemble-ai/chatterbox</title>
      <link>https://github.com/resemble-ai/chatterbox</link>
      <description>&lt;p&gt;SoTA open-source TTS&lt;/p&gt;&lt;hr&gt;&lt;img width="1200" height="600" alt="Chatterbox-Multilingual" src="https://www.resemble.ai/wp-content/uploads/2025/09/Chatterbox-Multilingual-1.png" /&gt; 
&lt;h1&gt;Chatterbox TTS&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://resemble-ai.github.io/chatterbox_demopage/"&gt;&lt;img src="https://img.shields.io/badge/listen-demo_samples-blue" alt="Alt Text" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/spaces/ResembleAI/Chatterbox"&gt;&lt;img src="https://huggingface.co/datasets/huggingface/badges/resolve/main/open-in-hf-spaces-sm.svg?sanitize=true" alt="Alt Text" /&gt;&lt;/a&gt; &lt;a href="https://podonos.com/resembleai/chatterbox"&gt;&lt;img src="https://static-public.podonos.com/badges/insight-on-pdns-sm-dark.svg?sanitize=true" alt="Alt Text" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/rJq9cRJBJ6"&gt;&lt;img src="https://img.shields.io/discord/1377773249798344776?label=join%20discord&amp;amp;logo=discord&amp;amp;style=flat" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;_Made with â™¥ï¸ by &lt;a href="https://resemble.ai" target="_blank"&gt;&lt;img width="100" alt="resemble-logo-horizontal" src="https://github.com/user-attachments/assets/35cf756b-3506-4943-9c72-c05ddfa4e525" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;We're excited to introduce &lt;strong&gt;Chatterbox Multilingual&lt;/strong&gt;, &lt;a href="https://resemble.ai"&gt;Resemble AI's&lt;/a&gt; first production-grade open source TTS model supporting &lt;strong&gt;23 languages&lt;/strong&gt; out of the box. Licensed under MIT, Chatterbox has been benchmarked against leading closed-source systems like ElevenLabs, and is consistently preferred in side-by-side evaluations.&lt;/p&gt; 
&lt;p&gt;Whether you're working on memes, videos, games, or AI agents, Chatterbox brings your content to life across languages. It's also the first open source TTS model to support &lt;strong&gt;emotion exaggeration control&lt;/strong&gt; with robust &lt;strong&gt;multilingual zero-shot voice cloning&lt;/strong&gt;. Try the english only version now on our &lt;a href="https://huggingface.co/spaces/ResembleAI/Chatterbox"&gt;English Hugging Face Gradio app.&lt;/a&gt;. Or try the multilingual version on our &lt;a href="https://huggingface.co/spaces/ResembleAI/Chatterbox-Multilingual-TTS"&gt;Multilingual Hugging Face Gradio app.&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you like the model but need to scale or tune it for higher accuracy, check out our competitively priced TTS service (&lt;a href="https://resemble.ai"&gt;link&lt;/a&gt;). It delivers reliable performance with ultra-low latency of sub 200msâ€”ideal for production use in agents, applications, or interactive media.&lt;/p&gt; 
&lt;h1&gt;Key Details&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;Multilingual, zero-shot TTS supporting 23 languages&lt;/li&gt; 
 &lt;li&gt;SoTA zeroshot English TTS&lt;/li&gt; 
 &lt;li&gt;0.5B Llama backbone&lt;/li&gt; 
 &lt;li&gt;Unique exaggeration/intensity control&lt;/li&gt; 
 &lt;li&gt;Ultra-stable with alignment-informed inference&lt;/li&gt; 
 &lt;li&gt;Trained on 0.5M hours of cleaned data&lt;/li&gt; 
 &lt;li&gt;Watermarked outputs&lt;/li&gt; 
 &lt;li&gt;Easy voice conversion script&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://podonos.com/resembleai/chatterbox"&gt;Outperforms ElevenLabs&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Supported Languages&lt;/h1&gt; 
&lt;p&gt;Arabic (ar) â€¢ Danish (da) â€¢ German (de) â€¢ Greek (el) â€¢ English (en) â€¢ Spanish (es) â€¢ Finnish (fi) â€¢ French (fr) â€¢ Hebrew (he) â€¢ Hindi (hi) â€¢ Italian (it) â€¢ Japanese (ja) â€¢ Korean (ko) â€¢ Malay (ms) â€¢ Dutch (nl) â€¢ Norwegian (no) â€¢ Polish (pl) â€¢ Portuguese (pt) â€¢ Russian (ru) â€¢ Swedish (sv) â€¢ Swahili (sw) â€¢ Turkish (tr) â€¢ Chinese (zh)&lt;/p&gt; 
&lt;h1&gt;Tips&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;General Use (TTS and Voice Agents):&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Ensure that the reference clip matches the specified language tag. Otherwise, language transfer outputs may inherit the accent of the reference clipâ€™s language. To mitigate this, set &lt;code&gt;cfg_weight&lt;/code&gt; to &lt;code&gt;0&lt;/code&gt;.&lt;/li&gt; 
   &lt;li&gt;The default settings (&lt;code&gt;exaggeration=0.5&lt;/code&gt;, &lt;code&gt;cfg_weight=0.5&lt;/code&gt;) work well for most prompts across all languages.&lt;/li&gt; 
   &lt;li&gt;If the reference speaker has a fast speaking style, lowering &lt;code&gt;cfg_weight&lt;/code&gt; to around &lt;code&gt;0.3&lt;/code&gt; can improve pacing.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Expressive or Dramatic Speech:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Try lower &lt;code&gt;cfg_weight&lt;/code&gt; values (e.g. &lt;code&gt;~0.3&lt;/code&gt;) and increase &lt;code&gt;exaggeration&lt;/code&gt; to around &lt;code&gt;0.7&lt;/code&gt; or higher.&lt;/li&gt; 
   &lt;li&gt;Higher &lt;code&gt;exaggeration&lt;/code&gt; tends to speed up speech; reducing &lt;code&gt;cfg_weight&lt;/code&gt; helps compensate with slower, more deliberate pacing.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Installation&lt;/h1&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip install chatterbox-tts
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, you can install from source:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# conda create -yn chatterbox python=3.11
# conda activate chatterbox

git clone https://github.com/resemble-ai/chatterbox.git
cd chatterbox
pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;We developed and tested Chatterbox on Python 3.11 on Debian 11 OS; the versions of the dependencies are pinned in &lt;code&gt;pyproject.toml&lt;/code&gt; to ensure consistency. You can modify the code or dependencies in this installation mode.&lt;/p&gt; 
&lt;h1&gt;Usage&lt;/h1&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import torchaudio as ta
from chatterbox.tts import ChatterboxTTS
from chatterbox.mtl_tts import ChatterboxMultilingualTTS

# English example
model = ChatterboxTTS.from_pretrained(device="cuda")

text = "Ezreal and Jinx teamed up with Ahri, Yasuo, and Teemo to take down the enemy's Nexus in an epic late-game pentakill."
wav = model.generate(text)
ta.save("test-english.wav", wav, model.sr)

# Multilingual examples
multilingual_model = ChatterboxMultilingualTTS.from_pretrained(device=device)

french_text = "Bonjour, comment Ã§a va? Ceci est le modÃ¨le de synthÃ¨se vocale multilingue Chatterbox, il prend en charge 23 langues."
wav_french = multilingual_model.generate(spanish_text, language_id="fr")
ta.save("test-french.wav", wav_french, model.sr)

chinese_text = "ä½ å¥½ï¼Œä»Šå¤©å¤©æ°”çœŸä¸é”™ï¼Œå¸Œæœ›ä½ æœ‰ä¸€ä¸ªæ„‰å¿«çš„å‘¨æœ«ã€‚"
wav_chinese = multilingual_model.generate(chinese_text, language_id="zh")
ta.save("test-chinese.wav", wav_chinese, model.sr)

# If you want to synthesize with a different voice, specify the audio prompt
AUDIO_PROMPT_PATH = "YOUR_FILE.wav"
wav = model.generate(text, audio_prompt_path=AUDIO_PROMPT_PATH)
ta.save("test-2.wav", wav, model.sr)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;code&gt;example_tts.py&lt;/code&gt; and &lt;code&gt;example_vc.py&lt;/code&gt; for more examples.&lt;/p&gt; 
&lt;h1&gt;Acknowledgements&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/FunAudioLLM/CosyVoice"&gt;Cosyvoice&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning"&gt;Real-Time-Voice-Cloning&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/yl4579/HiFTNet"&gt;HiFT-GAN&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/meta-llama/llama3"&gt;Llama 3&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/xingchensong/S3Tokenizer"&gt;S3Tokenizer&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Built-in PerTh Watermarking for Responsible AI&lt;/h1&gt; 
&lt;p&gt;Every audio file generated by Chatterbox includes &lt;a href="https://github.com/resemble-ai/perth"&gt;Resemble AI's Perth (Perceptual Threshold) Watermarker&lt;/a&gt; - imperceptible neural watermarks that survive MP3 compression, audio editing, and common manipulations while maintaining nearly 100% detection accuracy.&lt;/p&gt; 
&lt;h2&gt;Watermark extraction&lt;/h2&gt; 
&lt;p&gt;You can look for the watermark using the following script.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import perth
import librosa

AUDIO_PATH = "YOUR_FILE.wav"

# Load the watermarked audio
watermarked_audio, sr = librosa.load(AUDIO_PATH, sr=None)

# Initialize watermarker (same as used for embedding)
watermarker = perth.PerthImplicitWatermarker()

# Extract watermark
watermark = watermarker.get_watermark(watermarked_audio, sample_rate=sr)
print(f"Extracted watermark: {watermark}")
# Output: 0.0 (no watermark) or 1.0 (watermarked)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;Official Discord&lt;/h1&gt; 
&lt;p&gt;ğŸ‘‹ Join us on &lt;a href="https://discord.gg/rJq9cRJBJ6"&gt;Discord&lt;/a&gt; and let's build something awesome together!&lt;/p&gt; 
&lt;h1&gt;Citation&lt;/h1&gt; 
&lt;p&gt;If you find this model useful, please consider citing.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@misc{chatterboxtts2025,
  author       = {{Resemble AI}},
  title        = {{Chatterbox-TTS}},
  year         = {2025},
  howpublished = {\url{https://github.com/resemble-ai/chatterbox}},
  note         = {GitHub repository}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;Disclaimer&lt;/h1&gt; 
&lt;p&gt;Don't use this model to do bad things. Prompts are sourced from freely available data on the internet.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>awslabs/agent-squad</title>
      <link>https://github.com/awslabs/agent-squad</link>
      <description>&lt;p&gt;Flexible and powerful framework for managing multiple AI agents and handling complex conversations&lt;/p&gt;&lt;hr&gt;&lt;h2 align="center"&gt;Agent Squad&lt;/h2&gt; 
&lt;p align="center"&gt;Flexible, lightweight open-source framework for orchestrating multiple AI agents to handle complex conversations.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p align="center"&gt; &lt;strong&gt;ğŸ“¢ New Name Alert:&lt;/strong&gt; Multi-Agent Orchestrator is now &lt;strong&gt;Agent Squad!&lt;/strong&gt; ğŸ‰&lt;br /&gt; Same powerful functionalities, new catchy name. Embrace the squad! &lt;/p&gt; 
&lt;hr /&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/awslabs/agent-squad"&gt;&lt;img alt="GitHub Repo" src="https://img.shields.io/badge/GitHub-Repo-green.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://www.npmjs.com/package/agent-squad"&gt;&lt;img alt="npm" src="https://img.shields.io/npm/v/agent-squad.svg?style=flat-square" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/agent-squad/"&gt;&lt;img alt="PyPI" src="https://img.shields.io/pypi/v/agent-squad.svg?style=flat-square" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; 
 &lt;!-- GitHub Stats --&gt; &lt;img src="https://img.shields.io/github/stars/awslabs/agent-squad?style=social" alt="GitHub stars" /&gt; &lt;img src="https://img.shields.io/github/forks/awslabs/agent-squad?style=social" alt="GitHub forks" /&gt; &lt;img src="https://img.shields.io/github/watchers/awslabs/agent-squad?style=social" alt="GitHub watchers" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; 
 &lt;!-- Repository Info --&gt; &lt;img src="https://img.shields.io/github/last-commit/awslabs/agent-squad" alt="Last Commit" /&gt; &lt;img src="https://img.shields.io/github/issues/awslabs/agent-squad" alt="Issues" /&gt; &lt;img src="https://img.shields.io/github/issues-pr/awslabs/agent-squad" alt="Pull Requests" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://awslabs.github.io/agent-squad/" style="display: inline-block; background-color: #0066cc; color: white; padding: 10px 20px; text-decoration: none; border-radius: 5px; font-weight: bold; font-size: 15px; transition: background-color 0.3s;"&gt; ğŸ“š Explore Full Documentation &lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;ğŸ”– Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ§  &lt;strong&gt;Intelligent intent classification&lt;/strong&gt; â€” Dynamically route queries to the most suitable agent based on context and content.&lt;/li&gt; 
 &lt;li&gt;ğŸ”¤ &lt;strong&gt;Dual language support&lt;/strong&gt; â€” Fully implemented in both &lt;strong&gt;Python&lt;/strong&gt; and &lt;strong&gt;TypeScript&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;ğŸŒŠ &lt;strong&gt;Flexible agent responses&lt;/strong&gt; â€” Support for both streaming and non-streaming responses from different agents.&lt;/li&gt; 
 &lt;li&gt;ğŸ“š &lt;strong&gt;Context management&lt;/strong&gt; â€” Maintain and utilize conversation context across multiple agents for coherent interactions.&lt;/li&gt; 
 &lt;li&gt;ğŸ”§ &lt;strong&gt;Extensible architecture&lt;/strong&gt; â€” Easily integrate new agents or customize existing ones to fit your specific needs.&lt;/li&gt; 
 &lt;li&gt;ğŸŒ &lt;strong&gt;Universal deployment&lt;/strong&gt; â€” Run anywhere - from AWS Lambda to your local environment or any cloud platform.&lt;/li&gt; 
 &lt;li&gt;ğŸ“¦ &lt;strong&gt;Pre-built agents and classifiers&lt;/strong&gt; â€” A variety of ready-to-use agents and multiple classifier implementations available.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;What's the Agent Squad â“&lt;/h2&gt; 
&lt;p&gt;The Agent Squad is a flexible framework for managing multiple AI agents and handling complex conversations. It intelligently routes queries and maintains context across interactions.&lt;/p&gt; 
&lt;p&gt;The system offers pre-built components for quick deployment, while also allowing easy integration of custom agents and conversation messages storage solutions.&lt;/p&gt; 
&lt;p&gt;This adaptability makes it suitable for a wide range of applications, from simple chatbots to sophisticated AI systems, accommodating diverse requirements and scaling efficiently.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ—ï¸ High-level architecture flow diagram&lt;/h2&gt; 
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/awslabs/agent-squad/main/img/flow.jpg" alt="High-level architecture flow diagram" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;The process begins with user input, which is analyzed by a Classifier.&lt;/li&gt; 
 &lt;li&gt;The Classifier leverages both Agents' Characteristics and Agents' Conversation history to select the most appropriate agent for the task.&lt;/li&gt; 
 &lt;li&gt;Once an agent is selected, it processes the user input.&lt;/li&gt; 
 &lt;li&gt;The orchestrator then saves the conversation, updating the Agents' Conversation history, before delivering the response back to the user.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;&lt;img src="https://raw.githubusercontent.com/awslabs/agent-squad/main/img/new.png" alt="" /&gt; Introducing SupervisorAgent: Agents Coordination&lt;/h2&gt; 
&lt;p&gt;The Agent Squad now includes a powerful new SupervisorAgent that enables sophisticated team coordination between multiple specialized agents. This new component implements a "agent-as-tools" architecture, allowing a lead agent to coordinate a team of specialized agents in parallel, maintaining context and delivering coherent responses.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/awslabs/agent-squad/main/img/flow-supervisor.jpg" alt="SupervisorAgent flow diagram" /&gt;&lt;/p&gt; 
&lt;p&gt;Key capabilities:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ¤ &lt;strong&gt;Team Coordination&lt;/strong&gt; - Coordinate multiple specialized agents working together on complex tasks&lt;/li&gt; 
 &lt;li&gt;âš¡ &lt;strong&gt;Parallel Processing&lt;/strong&gt; - Execute multiple agent queries simultaneously&lt;/li&gt; 
 &lt;li&gt;ğŸ§  &lt;strong&gt;Smart Context Management&lt;/strong&gt; - Maintain conversation history across all team members&lt;/li&gt; 
 &lt;li&gt;ğŸ”„ &lt;strong&gt;Dynamic Delegation&lt;/strong&gt; - Intelligently distribute subtasks to appropriate team members&lt;/li&gt; 
 &lt;li&gt;ğŸ¤– &lt;strong&gt;Agent Compatibility&lt;/strong&gt; - Works with all agent types (Bedrock, Anthropic, Lex, etc.)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The SupervisorAgent can be used in two powerful ways:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Direct Usage&lt;/strong&gt; - Call it directly when you need dedicated team coordination for specific tasks&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Classifier Integration&lt;/strong&gt; - Add it as an agent within the classifier to build complex hierarchical systems with multiple specialized teams&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Here are just a few examples where this agent can be used:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Customer Support Teams with specialized sub-teams&lt;/li&gt; 
 &lt;li&gt;AI Movie Production Studios&lt;/li&gt; 
 &lt;li&gt;Travel Planning Services&lt;/li&gt; 
 &lt;li&gt;Product Development Teams&lt;/li&gt; 
 &lt;li&gt;Healthcare Coordination Systems&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://awslabs.github.io/agent-squad/agents/built-in/supervisor-agent"&gt;Learn more about SupervisorAgent â†’&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;ğŸ’¬ Demo App&lt;/h2&gt; 
&lt;p&gt;In the screen recording below, we demonstrate an extended version of the demo app that uses 6 specialized agents:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Travel Agent&lt;/strong&gt;: Powered by an Amazon Lex Bot&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Weather Agent&lt;/strong&gt;: Utilizes a Bedrock LLM Agent with a tool to query the open-meteo API&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Restaurant Agent&lt;/strong&gt;: Implemented as an Amazon Bedrock Agent&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Math Agent&lt;/strong&gt;: Utilizes a Bedrock LLM Agent with two tools for executing mathematical operations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Tech Agent&lt;/strong&gt;: A Bedrock LLM Agent designed to answer questions on technical topics&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Health Agent&lt;/strong&gt;: A Bedrock LLM Agent focused on addressing health-related queries&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Watch as the system seamlessly switches context between diverse topics, from booking flights to checking weather, solving math problems, and providing health information. Notice how the appropriate agent is selected for each query, maintaining coherence even with brief follow-up inputs.&lt;/p&gt; 
&lt;p&gt;The demo highlights the system's ability to handle complex, multi-turn conversations while preserving context and leveraging specialized agents across various domains.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/awslabs/agent-squad/main/img/demo-app.gif?raw=true" alt="" /&gt;&lt;/p&gt; 
&lt;h2&gt;ğŸ¯ Examples &amp;amp; Quick Start&lt;/h2&gt; 
&lt;p&gt;Get hands-on experience with the Agent Squad through our diverse set of examples:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Demo Applications&lt;/strong&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://github.com/awslabs/agent-squad/tree/main/examples/python"&gt;Streamlit Global Demo&lt;/a&gt;: A single Streamlit application showcasing multiple demos, including: 
    &lt;ul&gt; 
     &lt;li&gt;AI Movie Production Studio&lt;/li&gt; 
     &lt;li&gt;AI Travel Planner&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://awslabs.github.io/agent-squad/cookbook/examples/chat-demo-app/"&gt;Chat Demo App&lt;/a&gt;: 
    &lt;ul&gt; 
     &lt;li&gt;Explore multiple specialized agents handling various domains like travel, weather, math, and health&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://awslabs.github.io/agent-squad/cookbook/examples/ecommerce-support-simulator/"&gt;E-commerce Support Simulator&lt;/a&gt;: Experience AI-powered customer support with: 
    &lt;ul&gt; 
     &lt;li&gt;Automated response generation for common queries&lt;/li&gt; 
     &lt;li&gt;Intelligent routing of complex issues to human support&lt;/li&gt; 
     &lt;li&gt;Real-time chat and email-style communication&lt;/li&gt; 
     &lt;li&gt;Human-in-the-loop interactions for complex cases&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Sample Projects&lt;/strong&gt;: Explore our example implementations in the &lt;code&gt;examples&lt;/code&gt; folder: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://github.com/awslabs/agent-squad/tree/main/examples/chat-demo-app"&gt;&lt;code&gt;chat-demo-app&lt;/code&gt;&lt;/a&gt;: Web-based chat interface with multiple specialized agents&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/awslabs/agent-squad/tree/main/examples/ecommerce-support-simulator"&gt;&lt;code&gt;ecommerce-support-simulator&lt;/code&gt;&lt;/a&gt;: AI-powered customer support system&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/awslabs/agent-squad/tree/main/examples/chat-chainlit-app"&gt;&lt;code&gt;chat-chainlit-app&lt;/code&gt;&lt;/a&gt;: Chat application built with Chainlit&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/awslabs/agent-squad/tree/main/examples/fast-api-streaming"&gt;&lt;code&gt;fast-api-streaming&lt;/code&gt;&lt;/a&gt;: FastAPI implementation with streaming support&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/awslabs/agent-squad/tree/main/examples/text-2-structured-output"&gt;&lt;code&gt;text-2-structured-output&lt;/code&gt;&lt;/a&gt;: Natural Language to Structured Data&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/awslabs/agent-squad/tree/main/examples/bedrock-inline-agents"&gt;&lt;code&gt;bedrock-inline-agents&lt;/code&gt;&lt;/a&gt;: Bedrock Inline Agents sample&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/awslabs/agent-squad/tree/main/examples/bedrock-prompt-routing"&gt;&lt;code&gt;bedrock-prompt-routing&lt;/code&gt;&lt;/a&gt;: Bedrock Prompt Routing sample code&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Examples are available in both Python and TypeScript. Check out our &lt;a href="https://awslabs.github.io/agent-squad/"&gt;documentation&lt;/a&gt; for comprehensive guides on setting up and using the Agent Squad framework!&lt;/p&gt; 
&lt;h2&gt;ğŸ“š Deep Dives: Stories, Blogs &amp;amp; Podcasts&lt;/h2&gt; 
&lt;p&gt;Discover creative implementations and diverse applications of the Agent Squad:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://community.aws/content/2lCi8jEKydhDm8eE8QFIQ5K23pF/from-bonjour-to-boarding-pass-multilingual-ai-chatbot-for-flight-reservations"&gt;From 'Bonjour' to 'Boarding Pass': Multilingual AI Chatbot for Flight Reservations&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This article demonstrates how to build a multilingual chatbot using the Agent Squad framework. The article explains how to use an &lt;strong&gt;Amazon Lex&lt;/strong&gt; bot as an agent, along with 2 other new agents to make it work in many languages with just a few lines of code.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://community.aws/content/2lq6cYYwTYGc7S3Zmz28xZoQNQj/beyond-auto-replies-building-an-ai-powered-e-commerce-support-system"&gt;Beyond Auto-Replies: Building an AI-Powered E-commerce Support system&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This article demonstrates how to build an AI-driven multi-agent system for automated e-commerce customer email support. It covers the architecture and setup of specialized AI agents using the Agent Squad framework, integrating automated processing with human-in-the-loop oversight. The guide explores email ingestion, intelligent routing, automated response generation, and human verification, providing a comprehensive approach to balancing AI efficiency with human expertise in customer support.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://community.aws/content/2mt7CFG7xg4yw6GRHwH9akhg0oD/speak-up-ai-voicing-your-agents-with-amazon-connect-lex-and-bedrock"&gt;Speak Up, AI: Voicing Your Agents with Amazon Connect, Lex, and Bedrock&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This article demonstrates how to build an AI customer call center. It covers the architecture and setup of specialized AI agents using the Agent Squad framework interacting with voice via &lt;strong&gt;Amazon Connect&lt;/strong&gt; and &lt;strong&gt;Amazon Lex&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://community.aws/content/2pTsHrYPqvAbJBl9ht1XxPOSPjR/unlock-bedrock-invokeinlineagent-api-s-hidden-potential-with-agent-squad"&gt;Unlock Bedrock InvokeInlineAgent API's Hidden Potential&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Learn how to scale &lt;strong&gt;Amazon Bedrock Agents&lt;/strong&gt; beyond knowledge base limitations using the Agent Squad framework and &lt;strong&gt;InvokeInlineAgent API&lt;/strong&gt;. This article demonstrates dynamic agent creation and knowledge base selection for enterprise-scale AI applications.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://community.aws/content/2phMjQ0bqWMg4PBwejBs1uf4YQE/supercharging-amazon-bedrock-flows-with-aws-agent-squad"&gt;Supercharging Amazon Bedrock Flows&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Learn how to enhance &lt;strong&gt;Amazon Bedrock Flows&lt;/strong&gt; with conversation memory and multi-flow orchestration using the Agent Squad framework. This guide shows how to overcome Bedrock Flows' limitations to build more sophisticated AI workflows with persistent memory and intelligent routing between flows.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ™ï¸ Podcast Discussions&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ‡«ğŸ‡· Podcast (French)&lt;/strong&gt;: L'orchestrateur multi-agents : Un orchestrateur open source pour vos agents IA&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Platforms&lt;/strong&gt;: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://podcasts.apple.com/be/podcast/lorchestrateur-multi-agents/id1452118442?i=1000684332612"&gt;Apple Podcasts&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://open.spotify.com/episode/4RdMazSRhZUyW2pniG91Vf"&gt;Spotify&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ‡¬ğŸ‡§ Podcast (English)&lt;/strong&gt;: An Orchestrator for Your AI Agents&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Platforms&lt;/strong&gt;: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://podcasts.apple.com/us/podcast/an-orchestrator-for-your-ai-agents/id1574162669?i=1000677039579"&gt;Apple Podcasts&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://open.spotify.com/episode/2a9DBGZn2lVqVMBLWGipHU"&gt;Spotify&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;TypeScript Version&lt;/h3&gt; 
&lt;h4&gt;Installation&lt;/h4&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ğŸ”„ &lt;code&gt;multi-agent-orchestrator&lt;/code&gt; becomes &lt;code&gt;agent-squad&lt;/code&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npm install agent-squad
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Usage&lt;/h4&gt; 
&lt;p&gt;The following example demonstrates how to use the Agent Squad with two different types of agents: a Bedrock LLM Agent with Converse API support and a Lex Bot Agent. This showcases the flexibility of the system in integrating various AI services.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-typescript"&gt;import { AgentSquad, BedrockLLMAgent, LexBotAgent } from "agent-squad";

const orchestrator = new AgentSquad();

// Add a Bedrock LLM Agent with Converse API support
orchestrator.addAgent(
  new BedrockLLMAgent({
      name: "Tech Agent",
      description:
        "Specializes in technology areas including software development, hardware, AI, cybersecurity, blockchain, cloud computing, emerging tech innovations, and pricing/costs related to technology products and services.",
      streaming: true
  })
);

// Add a Lex Bot Agent for handling travel-related queries
orchestrator.addAgent(
  new LexBotAgent({
    name: "Travel Agent",
    description: "Helps users book and manage their flight reservations",
    botId: process.env.LEX_BOT_ID,
    botAliasId: process.env.LEX_BOT_ALIAS_ID,
    localeId: "en_US",
  })
);

// Example usage
const response = await orchestrator.routeRequest(
  "I want to book a flight",
  'user123',
  'session456'
);

// Handle the response (streaming or non-streaming)
if (response.streaming == true) {
    console.log("\n** RESPONSE STREAMING ** \n");
    // Send metadata immediately
    console.log(`&amp;gt; Agent ID: ${response.metadata.agentId}`);
    console.log(`&amp;gt; Agent Name: ${response.metadata.agentName}`);
    console.log(`&amp;gt; User Input: ${response.metadata.userInput}`);
    console.log(`&amp;gt; User ID: ${response.metadata.userId}`);
    console.log(`&amp;gt; Session ID: ${response.metadata.sessionId}`);
    console.log(
      `&amp;gt; Additional Parameters:`,
      response.metadata.additionalParams
    );
    console.log(`\n&amp;gt; Response: `);

    // Stream the content
    for await (const chunk of response.output) {
      if (typeof chunk === "string") {
        process.stdout.write(chunk);
      } else {
        console.error("Received unexpected chunk type:", typeof chunk);
      }
    }

} else {
    // Handle non-streaming response (AgentProcessingResult)
    console.log("\n** RESPONSE ** \n");
    console.log(`&amp;gt; Agent ID: ${response.metadata.agentId}`);
    console.log(`&amp;gt; Agent Name: ${response.metadata.agentName}`);
    console.log(`&amp;gt; User Input: ${response.metadata.userInput}`);
    console.log(`&amp;gt; User ID: ${response.metadata.userId}`);
    console.log(`&amp;gt; Session ID: ${response.metadata.sessionId}`);
    console.log(
      `&amp;gt; Additional Parameters:`,
      response.metadata.additionalParams
    );
    console.log(`\n&amp;gt; Response: ${response.output}`);
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Python Version&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ğŸ”„ &lt;code&gt;multi-agent-orchestrator&lt;/code&gt; becomes &lt;code&gt;agent-squad&lt;/code&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Optional: Set up a virtual environment
python -m venv venv
source venv/bin/activate  # On Windows use `venv\Scripts\activate`
pip install agent-squad[aws]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Default Usage&lt;/h4&gt; 
&lt;p&gt;Here's an equivalent Python example demonstrating the use of the Agent Squad with a Bedrock LLM Agent and a Lex Bot Agent:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import sys
import asyncio
from agent_squad.orchestrator import AgentSquad
from agent_squad.agents import BedrockLLMAgent, BedrockLLMAgentOptions, AgentStreamResponse

orchestrator = AgentSquad()

tech_agent = BedrockLLMAgent(BedrockLLMAgentOptions(
  name="Tech Agent",
  streaming=True,
  description="Specializes in technology areas including software development, hardware, AI, \
  cybersecurity, blockchain, cloud computing, emerging tech innovations, and pricing/costs \
  related to technology products and services.",
  model_id="anthropic.claude-3-sonnet-20240229-v1:0",
))
orchestrator.add_agent(tech_agent)


health_agent = BedrockLLMAgent(BedrockLLMAgentOptions(
  name="Health Agent",
  streaming=True,
  description="Specializes in health and well being",
))
orchestrator.add_agent(health_agent)

async def main():
    # Example usage
    response = await orchestrator.route_request(
        "What is AWS Lambda?",
        'user123',
        'session456',
        {},
        True
    )

    # Handle the response (streaming or non-streaming)
    if response.streaming:
        print("\n** RESPONSE STREAMING ** \n")
        # Send metadata immediately
        print(f"&amp;gt; Agent ID: {response.metadata.agent_id}")
        print(f"&amp;gt; Agent Name: {response.metadata.agent_name}")
        print(f"&amp;gt; User Input: {response.metadata.user_input}")
        print(f"&amp;gt; User ID: {response.metadata.user_id}")
        print(f"&amp;gt; Session ID: {response.metadata.session_id}")
        print(f"&amp;gt; Additional Parameters: {response.metadata.additional_params}")
        print("\n&amp;gt; Response: ")

        # Stream the content
        async for chunk in response.output:
            async for chunk in response.output:
              if isinstance(chunk, AgentStreamResponse):
                  print(chunk.text, end='', flush=True)
              else:
                  print(f"Received unexpected chunk type: {type(chunk)}", file=sys.stderr)

    else:
        # Handle non-streaming response (AgentProcessingResult)
        print("\n** RESPONSE ** \n")
        print(f"&amp;gt; Agent ID: {response.metadata.agent_id}")
        print(f"&amp;gt; Agent Name: {response.metadata.agent_name}")
        print(f"&amp;gt; User Input: {response.metadata.user_input}")
        print(f"&amp;gt; User ID: {response.metadata.user_id}")
        print(f"&amp;gt; Session ID: {response.metadata.session_id}")
        print(f"&amp;gt; Additional Parameters: {response.metadata.additional_params}")
        print(f"\n&amp;gt; Response: {response.output.content}")

if __name__ == "__main__":
  asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;These examples showcase:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;The use of a Bedrock LLM Agent with Converse API support, allowing for multi-turn conversations.&lt;/li&gt; 
 &lt;li&gt;Integration of a Lex Bot Agent for specialized tasks (in this case, travel-related queries).&lt;/li&gt; 
 &lt;li&gt;The orchestrator's ability to route requests to the most appropriate agent based on the input.&lt;/li&gt; 
 &lt;li&gt;Handling of both streaming and non-streaming responses from different types of agents.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Modular Installation Options&lt;/h3&gt; 
&lt;p&gt;The Agent Squad is designed with a modular architecture, allowing you to install only the components you need while ensuring you always get the core functionality.&lt;/p&gt; 
&lt;h4&gt;Installation Options&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;1. AWS Integration&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt; pip install "agent-squad[aws]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Includes core orchestration functionality with comprehensive AWS service integrations (&lt;code&gt;BedrockLLMAgent&lt;/code&gt;, &lt;code&gt;AmazonBedrockAgent&lt;/code&gt;, &lt;code&gt;LambdaAgent&lt;/code&gt;, etc.)&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;2. Anthropic Integration&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install "agent-squad[anthropic]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;3. OpenAI Integration&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install "agent-squad[openai]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Adds OpenAI's GPT models for agents and classification, along with core packages.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;4. Full Installation&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install "agent-squad[all]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Includes all optional dependencies for maximum flexibility.&lt;/p&gt; 
&lt;h3&gt;ğŸ™Œ &lt;strong&gt;We Want to Hear From You!&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;Have something to share, discuss, or brainstorm? Weâ€™d love to connect with you and hear about your journey with the &lt;strong&gt;Agent Squad framework&lt;/strong&gt;. Hereâ€™s how you can get involved:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ™Œ Show &amp;amp; Tell&lt;/strong&gt;: Got a success story, cool project, or creative implementation? Share it with us in the &lt;a href="https://github.com/awslabs/agent-squad/discussions/categories/show-and-tell"&gt;&lt;strong&gt;Show and Tell&lt;/strong&gt;&lt;/a&gt; section. Your work might inspire the entire community! ğŸ‰&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ’¬ General Discussion&lt;/strong&gt;: Have questions, feedback, or suggestions? Join the conversation in our &lt;a href="https://github.com/awslabs/agent-squad/discussions/categories/general"&gt;&lt;strong&gt;General Discussions&lt;/strong&gt;&lt;/a&gt; section. Itâ€™s the perfect place to connect with other users and contributors.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ğŸ’¡ Ideas&lt;/strong&gt;: Thinking of a new feature or improvement? Share your thoughts in the &lt;a href="https://github.com/awslabs/agent-squad/discussions/categories/ideas"&gt;&lt;strong&gt;Ideas&lt;/strong&gt;&lt;/a&gt; section. Weâ€™re always open to exploring innovative ways to make the orchestrator even better!&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Letâ€™s collaborate, learn from each other, and build something incredible together! ğŸš€&lt;/p&gt; 
&lt;h2&gt;ğŸ“ Pull Request Guidelines&lt;/h2&gt; 
&lt;h3&gt;Issue-First Policy&lt;/h3&gt; 
&lt;p&gt;This repository follows an &lt;strong&gt;Issue-First&lt;/strong&gt; policy:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Every pull request must be linked to an existing issue&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;If there isn't an issue for the changes you want to make, please create one first&lt;/li&gt; 
 &lt;li&gt;Use the issue to discuss proposed changes before investing time in implementation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;How to Link Pull Requests to Issues&lt;/h3&gt; 
&lt;p&gt;When creating a pull request, you must link it to an issue using one of these methods:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Include a reference in the PR description using keywords:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;Fixes #123&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;Resolves #123&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;Closes #123&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Manually link the PR to an issue through GitHub's UI:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;On the right sidebar of your PR, click "Development" and then "Link an issue"&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Automated Enforcement&lt;/h3&gt; 
&lt;p&gt;We use GitHub Actions to automatically verify that each PR is linked to an issue. PRs without linked issues will not pass required checks and cannot be merged.&lt;/p&gt; 
&lt;p&gt;This policy helps us:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Maintain clear documentation of changes and their purposes&lt;/li&gt; 
 &lt;li&gt;Ensure community discussion before implementation&lt;/li&gt; 
 &lt;li&gt;Keep a structured development process&lt;/li&gt; 
 &lt;li&gt;Make project history more traceable and understandable&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ¤ Contributing&lt;/h2&gt; 
&lt;p&gt;âš ï¸ Note: Our project has been renamed from &lt;strong&gt;Multi-Agent Orchestrator&lt;/strong&gt; to &lt;strong&gt;Agent Squad&lt;/strong&gt;. Please use the new name in your contributions and discussions.&lt;/p&gt; 
&lt;p&gt;âš ï¸ We value your contributions! Before submitting changes, please start a discussion by opening an issue to share your proposal.&lt;/p&gt; 
&lt;p&gt;Once your proposal is approved, here are the next steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;ğŸ“š Review our &lt;a href="https://raw.githubusercontent.com/awslabs/agent-squad/main/CONTRIBUTING.md"&gt;Contributing Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸ’¡ Create a &lt;a href="https://github.com/awslabs/agent-squad/issues"&gt;GitHub Issue&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸ”¨ Submit a pull request&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;âœ… Follow existing project structure and include documentation for new features.&lt;/p&gt; 
&lt;p&gt;ğŸŒŸ &lt;strong&gt;Stay Updated&lt;/strong&gt;: Star the repository to be notified about new features, improvements, and exciting developments in the Agent Squad framework!&lt;/p&gt; 
&lt;h1&gt;Authors&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.linkedin.com/in/corneliucroitoru/"&gt;Corneliu Croitoru&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.linkedin.com/in/anthonybernabeu/"&gt;Anthony Bernabeu&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;ğŸ‘¥ Contributors&lt;/h1&gt; 
&lt;p&gt;Big shout out to our awesome contributors! Thank you for making this project better! ğŸŒŸ â­ ğŸš€&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/awslabs/agent-squad/graphs/contributors"&gt;&lt;img src="https://contrib.rocks/image?repo=awslabs/agent-squad&amp;amp;max=2000" alt="contributors" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Please see our &lt;a href="https://raw.githubusercontent.com/awslabs/agent-squad/main/CONTRIBUTING.md"&gt;contributing guide&lt;/a&gt; for guidelines on how to propose bugfixes and improvements.&lt;/p&gt; 
&lt;h2&gt;ğŸ“„ LICENSE&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the Apache 2.0 licence - see the &lt;a href="https://raw.githubusercontent.com/awslabs/agent-squad/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;h2&gt;ğŸ“„ Font License&lt;/h2&gt; 
&lt;p&gt;This project uses the JetBrainsMono NF font, licensed under the SIL Open Font License 1.1. For full license details, see &lt;a href="https://github.com/JetBrains/JetBrainsMono/raw/master/OFL.txt"&gt;FONT-LICENSE.md&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>1Panel-dev/MaxKB</title>
      <link>https://github.com/1Panel-dev/MaxKB</link>
      <description>&lt;p&gt;ğŸ”¥ MaxKB is an open-source platform for building enterprise-grade agents. MaxKB æ˜¯å¼ºå¤§æ˜“ç”¨çš„å¼€æºä¼ä¸šçº§æ™ºèƒ½ä½“å¹³å°ã€‚&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt;&lt;img src="https://github.com/1Panel-dev/maxkb/assets/52996290/c0694996-0eed-40d8-b369-322bf2a380bf" alt="MaxKB" width="300" /&gt;&lt;/p&gt; 
&lt;h3 align="center"&gt;Open-source platform for building enterprise-grade agents&lt;/h3&gt; 
&lt;h3 align="center"&gt;å¼ºå¤§æ˜“ç”¨çš„ä¼ä¸šçº§æ™ºèƒ½ä½“å¹³å°&lt;/h3&gt; 
&lt;p align="center"&gt;&lt;a href="https://trendshift.io/repositories/9113" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/9113" alt="1Panel-dev%2FMaxKB | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://www.gnu.org/licenses/gpl-3.0.html#license-text"&gt;&lt;img src="https://img.shields.io/github/license/1Panel-dev/maxkb?color=%231890FF" alt="License: GPL v3" /&gt;&lt;/a&gt; &lt;a href="https://github.com/1Panel-dev/maxkb/releases/latest"&gt;&lt;img src="https://img.shields.io/github/v/release/1Panel-dev/maxkb" alt="Latest release" /&gt;&lt;/a&gt; &lt;a href="https://github.com/1Panel-dev/maxkb"&gt;&lt;img src="https://img.shields.io/github/stars/1Panel-dev/maxkb?color=%231890FF&amp;amp;style=flat-square" alt="Stars" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/1panel/maxkb"&gt;&lt;img src="https://img.shields.io/docker/pulls/1panel/maxkb?label=downloads" alt="Download" /&gt;&lt;/a&gt;&lt;br /&gt; [&lt;a href="https://raw.githubusercontent.com/1Panel-dev/MaxKB/v2/README_CN.md"&gt;ä¸­æ–‡(ç®€ä½“)&lt;/a&gt;] | [&lt;a href="https://raw.githubusercontent.com/1Panel-dev/MaxKB/v2/README.md"&gt;English&lt;/a&gt;] &lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;MaxKB = Max Knowledge Brain, it is an open-source platform for building enterprise-grade agents. MaxKB integrates Retrieval-Augmented Generation (RAG) pipelines, supports robust workflows, and provides advanced MCP tool-use capabilities. MaxKB is widely applied in scenarios such as intelligent customer service, corporate internal knowledge bases, academic research, and education.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;RAG Pipeline&lt;/strong&gt;: Supports direct uploading of documents / automatic crawling of online documents, with features for automatic text splitting, vectorization. This effectively reduces hallucinations in large models, providing a superior smart Q&amp;amp;A interaction experience.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Agentic Workflow&lt;/strong&gt;: Equipped with a powerful workflow engine, function library and MCP tool-use, enabling the orchestration of AI processes to meet the needs of complex business scenarios.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Seamless Integration&lt;/strong&gt;: Facilitates zero-coding rapid integration into third-party business systems, quickly equipping existing systems with intelligent Q&amp;amp;A capabilities to enhance user satisfaction.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Model-Agnostic&lt;/strong&gt;: Supports various large models, including private models (such as DeepSeek, Llama, Qwen, etc.) and public models (like OpenAI, Claude, Gemini, etc.).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi Modal&lt;/strong&gt;: Native support for input and output text, image, audio and video.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Quick start&lt;/h2&gt; 
&lt;p&gt;Execute the script below to start a MaxKB container using Docker:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run -d --name=maxkb --restart=always -p 8080:8080 -v ~/.maxkb:/opt/maxkb 1panel/maxkb
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Access MaxKB web interface at &lt;code&gt;http://your_server_ip:8080&lt;/code&gt; with default admin credentials:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;username: admin&lt;/li&gt; 
 &lt;li&gt;password: MaxKB@123..&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;ä¸­å›½ç”¨æˆ·å¦‚é‡åˆ° Docker é•œåƒ Pull å¤±è´¥é—®é¢˜ï¼Œè¯·å‚ç…§è¯¥ &lt;a href="https://maxkb.cn/docs/v2/installation/offline_installtion/"&gt;ç¦»çº¿å®‰è£…æ–‡æ¡£&lt;/a&gt; è¿›è¡Œå®‰è£…ã€‚&lt;/p&gt; 
&lt;h2&gt;Screenshots&lt;/h2&gt; 
&lt;table style="border-collapse: collapse; border: 1px solid black;"&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td style="padding: 5px;background-color:#fff;"&gt;&lt;img src="https://github.com/user-attachments/assets/eb285512-a66a-4752-8941-c65ed1592238" alt="MaxKB Demo1" /&gt;&lt;/td&gt; 
   &lt;td style="padding: 5px;background-color:#fff;"&gt;&lt;img src="https://github.com/user-attachments/assets/f732f1f5-472c-4fd2-93c1-a277eda83d04" alt="MaxKB Demo2" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td style="padding: 5px;background-color:#fff;"&gt;&lt;img src="https://github.com/user-attachments/assets/c927474a-9a23-4830-822f-5db26025c9b2" alt="MaxKB Demo3" /&gt;&lt;/td&gt; 
   &lt;td style="padding: 5px;background-color:#fff;"&gt;&lt;img src="https://github.com/user-attachments/assets/e6268996-a46d-4e58-9f30-31139df78ad2" alt="MaxKB Demo4" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;Technical stack&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Frontendï¼š&lt;a href="https://vuejs.org/"&gt;Vue.js&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Backendï¼š&lt;a href="https://www.djangoproject.com/"&gt;Python / Django&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;LLM Frameworkï¼š&lt;a href="https://www.langchain.com/"&gt;LangChain&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Databaseï¼š&lt;a href="https://www.postgresql.org/"&gt;PostgreSQL + pgvector&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#1Panel-dev/MaxKB&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=1Panel-dev/MaxKB&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Licensed under The GNU General Public License version 3 (GPLv3) (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.gnu.org/licenses/gpl-3.0.html"&gt;https://www.gnu.org/licenses/gpl-3.0.html&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>mxrch/GHunt</title>
      <link>https://github.com/mxrch/GHunt</link>
      <description>&lt;p&gt;ğŸ•µï¸â€â™‚ï¸ Offensive Google framework.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/mxrch/GHunt/master/assets/long_banner.png" alt="" /&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;h4&gt;ğŸŒ GHunt Online version : &lt;a href="https://osint.industries"&gt;https://osint.industries&lt;/a&gt;&lt;/h4&gt; 
&lt;h4&gt;ğŸ Now Python 3.13 compatible !&lt;/h4&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;img src="https://img.shields.io/badge/Python-3.10%2B-brightgreen" alt="Python minimum version" /&gt;&lt;/p&gt; 
&lt;h1&gt;ğŸ˜Š Description&lt;/h1&gt; 
&lt;p&gt;GHunt (v2) is an offensive Google framework, designed to evolve efficiently.&lt;br /&gt; It's currently focused on OSINT, but any use related with Google is possible.&lt;/p&gt; 
&lt;p&gt;Features :&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;CLI usage and modules&lt;/li&gt; 
 &lt;li&gt;Python library usage&lt;/li&gt; 
 &lt;li&gt;Fully async&lt;/li&gt; 
 &lt;li&gt;JSON export&lt;/li&gt; 
 &lt;li&gt;Browser extension to ease login&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;âœ”ï¸ Requirements&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python &amp;gt;= 3.10&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;âš™ï¸ Installation&lt;/h1&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;$ pip3 install pipx
$ pipx ensurepath
$ pipx install ghunt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;It will automatically use venvs to avoid dependency conflicts with other projects.&lt;/p&gt; 
&lt;h1&gt;ğŸ’ƒ Usage&lt;/h1&gt; 
&lt;h2&gt;Login&lt;/h2&gt; 
&lt;p&gt;First, launch the listener by doing &lt;code&gt;ghunt login&lt;/code&gt; and choose between 1 of the 2 first methods :&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;$ ghunt login

[1] (Companion) Put GHunt on listening mode (currently not compatible with docker)
[2] (Companion) Paste base64-encoded cookies
[3] Enter manually all cookies

Choice =&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then, use GHunt Companion to complete the login.&lt;/p&gt; 
&lt;p&gt;The extension is available on the following stores :&lt;br /&gt; &lt;br /&gt; &lt;a href="https://addons.mozilla.org/en-US/firefox/addon/ghunt-companion/"&gt;&lt;img src="https://files.catbox.moe/5g2ld5.png" alt="Firefox" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://chrome.google.com/webstore/detail/ghunt-companion/dpdcofblfbmmnikcbmmiakkclocadjab"&gt;&lt;img src="https://developer.chrome.com/static/docs/webstore/branding/image/206x58-chrome-web-bcb82d15b2486.png" alt="Chrome" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Modules&lt;/h2&gt; 
&lt;p&gt;Then, profit :&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;Usage: ghunt [-h] {login,email,gaia,drive,geolocate} ...

Positional Arguments:
  {login,email,gaia,drive,geolocate}
    login               Authenticate GHunt to Google.
    email               Get information on an email address.
    gaia                Get information on a Gaia ID.
    drive               Get information on a Drive file or folder.
    geolocate           Geolocate a BSSID.
    spiderdal           Find assets using Digital Assets Links.

Options:
  -h, --help            show this help message and exit
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;ğŸ“„ You can also use --json with email, gaia, drive and geolocate modules to export in JSON ! Example :&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;$ ghunt email &amp;lt;email_address&amp;gt; --json user_data.json
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Have fun ğŸ¥°ğŸ’&lt;/strong&gt;&lt;/p&gt; 
&lt;h1&gt;ğŸ§‘â€ğŸ’» Developers&lt;/h1&gt; 
&lt;p&gt;ğŸ“• I started writing some docs &lt;a href="https://github.com/mxrch/GHunt/wiki"&gt;here&lt;/a&gt; and examples &lt;a href="https://github.com/mxrch/GHunt/tree/master/examples"&gt;here&lt;/a&gt;, feel free to contribute !&lt;/p&gt; 
&lt;p&gt;To use GHunt as a lib, you can't use pipx because it uses a venv.&lt;br /&gt; So you should install GHunt with pip :&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;$ pip3 install ghunt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;And now, you should be able to &lt;code&gt;import ghunt&lt;/code&gt; in your projects !&lt;br /&gt; You can right now play with the &lt;a href="https://github.com/mxrch/GHunt/tree/master/examples"&gt;examples&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;ğŸ“® Details&lt;/h1&gt; 
&lt;h2&gt;Obvious disclaimer&lt;/h2&gt; 
&lt;p&gt;This tool is for educational purposes only, I am not responsible for its use.&lt;/p&gt; 
&lt;h2&gt;Less obvious disclaimer&lt;/h2&gt; 
&lt;p&gt;This project is under &lt;a href="https://choosealicense.com/licenses/agpl-3.0/"&gt;AGPL Licence&lt;/a&gt;, and you have to respect it.&lt;br /&gt; &lt;strong&gt;Use it only in personal, criminal investigations, pentesting, or open-source projects.&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;Thanks&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/novitae"&gt;novitae&lt;/a&gt; for being my Python colleague&lt;/li&gt; 
 &lt;li&gt;All the people on &lt;a href="https://discord.gg/sg2YcrC6x9"&gt;Malfrats Industries&lt;/a&gt; and elsewhere for the beta test !&lt;/li&gt; 
 &lt;li&gt;The HideAndSec team ğŸ’— (blog : &lt;a href="https://hideandsec.sh"&gt;https://hideandsec.sh&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://dribbble.com/jouiniamine"&gt;Med Amine Jouini&lt;/a&gt; for his beautiful rework of the Google logo, which I was inspired by &lt;em&gt;a lot&lt;/em&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Sponsors&lt;/h2&gt; 
&lt;p&gt;Thanks to these awesome people for supporting me !&lt;/p&gt; 
&lt;!-- sponsors --&gt;
&lt;a href="https://github.com/BlWasp"&gt;&lt;img src="https://github.com/BlWasp.png" width="50px" alt="BlWasp" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;
&lt;a href="https://github.com/gingeleski"&gt;&lt;img src="https://github.com/gingeleski.png" width="50px" alt="gingeleski" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;
&lt;a href="https://github.com/ADS-Fund"&gt;&lt;img src="https://github.com/ADS-Fund.png" width="50px" alt="ADS-Fund" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;
&lt;!-- sponsors --&gt; 
&lt;p&gt;&lt;br /&gt; You like my work ?&lt;br /&gt; &lt;a href="https://github.com/sponsors/mxrch"&gt;Sponsor me&lt;/a&gt; on GitHub ! ğŸ¤—&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Physical-Intelligence/openpi</title>
      <link>https://github.com/Physical-Intelligence/openpi</link>
      <description>&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;openpi&lt;/h1&gt; 
&lt;p&gt;openpi holds open-source models and packages for robotics, published by the &lt;a href="https://www.physicalintelligence.company/"&gt;Physical Intelligence team&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Currently, this repo contains three types of models:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;the &lt;a href="https://www.physicalintelligence.company/blog/pi0"&gt;Ï€â‚€ model&lt;/a&gt;, a flow-based vision-language-action model (VLA).&lt;/li&gt; 
 &lt;li&gt;the &lt;a href="https://www.physicalintelligence.company/research/fast"&gt;Ï€â‚€-FAST model&lt;/a&gt;, an autoregressive VLA, based on the FAST action tokenizer.&lt;/li&gt; 
 &lt;li&gt;the &lt;a href="https://www.physicalintelligence.company/blog/pi05"&gt;Ï€â‚€.â‚… model&lt;/a&gt;, an upgraded version of Ï€â‚€ with better open-world generalization trained with &lt;a href="https://www.physicalintelligence.company/research/knowledge_insulation"&gt;knowledge insulation&lt;/a&gt;. Note that, in this repository, we currently only support the flow matching head for both $\pi_{0.5}$ training and inference.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For all models, we provide &lt;em&gt;base model&lt;/em&gt; checkpoints, pre-trained on 10k+ hours of robot data, and examples for using them out of the box or fine-tuning them to your own datasets.&lt;/p&gt; 
&lt;p&gt;This is an experiment: $\pi_0$ was developed for our own robots, which differ from the widely used platforms such as &lt;a href="https://tonyzhaozh.github.io/aloha/"&gt;ALOHA&lt;/a&gt; and &lt;a href="https://droid-dataset.github.io/"&gt;DROID&lt;/a&gt;, and though we are optimistic that researchers and practitioners will be able to run creative new experiments adapting $\pi_0$ to their own platforms, we do not expect every such attempt to be successful. All this is to say: $\pi_0$ may or may not work for you, but you are welcome to try it and see!&lt;/p&gt; 
&lt;h2&gt;Updates&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;[Sept 2025] We released PyTorch support in openpi.&lt;/li&gt; 
 &lt;li&gt;[Sept 2025] We released pi05, an upgraded version of pi0 with better open-world generalization.&lt;/li&gt; 
 &lt;li&gt;[Sept 2025]: We have added an &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/droid/README_train.md#data-filtering"&gt;improved idle filter&lt;/a&gt; for DROID training.&lt;/li&gt; 
 &lt;li&gt;[Jun 2025]: We have added &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/droid/README_train.md"&gt;instructions&lt;/a&gt; for using &lt;code&gt;openpi&lt;/code&gt; to train VLAs on the full &lt;a href="https://droid-dataset.github.io/"&gt;DROID dataset&lt;/a&gt;. This is an approximate open-source implementation of the training pipeline used to train pi0-FAST-DROID.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;p&gt;To run the models in this repository, you will need an NVIDIA GPU with at least the following specifications. These estimations assume a single GPU, but you can also use multiple GPUs with model parallelism to reduce per-GPU memory requirements by configuring &lt;code&gt;fsdp_devices&lt;/code&gt; in the training config. Please also note that the current training script does not yet support multi-node training.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Mode&lt;/th&gt; 
   &lt;th&gt;Memory Required&lt;/th&gt; 
   &lt;th&gt;Example GPU&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Inference&lt;/td&gt; 
   &lt;td&gt;&amp;gt; 8 GB&lt;/td&gt; 
   &lt;td&gt;RTX 4090&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Fine-Tuning (LoRA)&lt;/td&gt; 
   &lt;td&gt;&amp;gt; 22.5 GB&lt;/td&gt; 
   &lt;td&gt;RTX 4090&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Fine-Tuning (Full)&lt;/td&gt; 
   &lt;td&gt;&amp;gt; 70 GB&lt;/td&gt; 
   &lt;td&gt;A100 (80GB) / H100&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;The repo has been tested with Ubuntu 22.04, we do not currently support other operating systems.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;When cloning this repo, make sure to update submodules:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone --recurse-submodules git@github.com:Physical-Intelligence/openpi.git

# Or if you already cloned the repo:
git submodule update --init --recursive
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;We use &lt;a href="https://docs.astral.sh/uv/"&gt;uv&lt;/a&gt; to manage Python dependencies. See the &lt;a href="https://docs.astral.sh/uv/getting-started/installation/"&gt;uv installation instructions&lt;/a&gt; to set it up. Once uv is installed, run the following to set up the environment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;GIT_LFS_SKIP_SMUDGE=1 uv sync
GIT_LFS_SKIP_SMUDGE=1 uv pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;NOTE: &lt;code&gt;GIT_LFS_SKIP_SMUDGE=1&lt;/code&gt; is needed to pull LeRobot as a dependency.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Docker&lt;/strong&gt;: As an alternative to uv installation, we provide instructions for installing openpi using Docker. If you encounter issues with your system setup, consider using Docker to simplify installation. See &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/docs/docker.md"&gt;Docker Setup&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;Model Checkpoints&lt;/h2&gt; 
&lt;h3&gt;Base Models&lt;/h3&gt; 
&lt;p&gt;We provide multiple base VLA model checkpoints. These checkpoints have been pre-trained on 10k+ hours of robot data, and can be used for fine-tuning.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Use Case&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Checkpoint Path&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_0$&lt;/td&gt; 
   &lt;td&gt;Fine-Tuning&lt;/td&gt; 
   &lt;td&gt;Base &lt;a href="https://www.physicalintelligence.company/blog/pi0"&gt;Ï€â‚€ model&lt;/a&gt; for fine-tuning&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi0_base&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_0$-FAST&lt;/td&gt; 
   &lt;td&gt;Fine-Tuning&lt;/td&gt; 
   &lt;td&gt;Base autoregressive &lt;a href="https://www.physicalintelligence.company/research/fast"&gt;Ï€â‚€-FAST model&lt;/a&gt; for fine-tuning&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi0_fast_base&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_{0.5}$&lt;/td&gt; 
   &lt;td&gt;Fine-Tuning&lt;/td&gt; 
   &lt;td&gt;Base &lt;a href="https://www.physicalintelligence.company/blog/pi05"&gt;Ï€â‚€.â‚… model&lt;/a&gt; for fine-tuning&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi05_base&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Fine-Tuned Models&lt;/h3&gt; 
&lt;p&gt;We also provide "expert" checkpoints for various robot platforms and tasks. These models are fine-tuned from the base models above and intended to run directly on the target robot. These may or may not work on your particular robot. Since these checkpoints were fine-tuned on relatively small datasets collected with more widely available robots, such as ALOHA and the DROID Franka setup, they might not generalize to your particular setup, though we found some of these, especially the DROID checkpoint, to generalize quite broadly in practice.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Use Case&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Checkpoint Path&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_0$-FAST-DROID&lt;/td&gt; 
   &lt;td&gt;Inference&lt;/td&gt; 
   &lt;td&gt;$\pi_0$-FAST model fine-tuned on the &lt;a href="https://droid-dataset.github.io/"&gt;DROID dataset&lt;/a&gt;: can perform a wide range of simple table-top manipulation tasks 0-shot in new scenes on the DROID robot platform&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi0_fast_droid&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_0$-DROID&lt;/td&gt; 
   &lt;td&gt;Fine-Tuning&lt;/td&gt; 
   &lt;td&gt;$\pi_0$ model fine-tuned on the &lt;a href="https://droid-dataset.github.io/"&gt;DROID dataset&lt;/a&gt;: faster inference than $\pi_0$-FAST-DROID, but may not follow language commands as well&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi0_droid&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_0$-ALOHA-towel&lt;/td&gt; 
   &lt;td&gt;Inference&lt;/td&gt; 
   &lt;td&gt;$\pi_0$ model fine-tuned on internal &lt;a href="https://tonyzhaozh.github.io/aloha/"&gt;ALOHA&lt;/a&gt; data: can fold diverse towels 0-shot on ALOHA robot platforms&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi0_aloha_towel&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_0$-ALOHA-tupperware&lt;/td&gt; 
   &lt;td&gt;Inference&lt;/td&gt; 
   &lt;td&gt;$\pi_0$ model fine-tuned on internal &lt;a href="https://tonyzhaozh.github.io/aloha/"&gt;ALOHA&lt;/a&gt; data: can unpack food from a tupperware container&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi0_aloha_tupperware&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_0$-ALOHA-pen-uncap&lt;/td&gt; 
   &lt;td&gt;Inference&lt;/td&gt; 
   &lt;td&gt;$\pi_0$ model fine-tuned on public &lt;a href="https://dit-policy.github.io/"&gt;ALOHA&lt;/a&gt; data: can uncap a pen&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi0_aloha_pen_uncap&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_{0.5}$-LIBERO&lt;/td&gt; 
   &lt;td&gt;Inference&lt;/td&gt; 
   &lt;td&gt;$\pi_{0.5}$ model fine-tuned for the &lt;a href="https://libero-project.github.io/datasets"&gt;LIBERO&lt;/a&gt; benchmark: gets state-of-the-art performance (see &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/libero/README.md"&gt;LIBERO README&lt;/a&gt;)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi05_libero&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_{0.5}$-DROID&lt;/td&gt; 
   &lt;td&gt;Inference / Fine-Tuning&lt;/td&gt; 
   &lt;td&gt;$\pi_{0.5}$ model fine-tuned on the &lt;a href="https://droid-dataset.github.io/"&gt;DROID dataset&lt;/a&gt; with &lt;a href="https://www.physicalintelligence.company/research/knowledge_insulation"&gt;knowledge insulation&lt;/a&gt;: fast inference and good language-following&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi05_droid&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;By default, checkpoints are automatically downloaded from &lt;code&gt;gs://openpi-assets&lt;/code&gt; and are cached in &lt;code&gt;~/.cache/openpi&lt;/code&gt; when needed. You can overwrite the download path by setting the &lt;code&gt;OPENPI_DATA_HOME&lt;/code&gt; environment variable.&lt;/p&gt; 
&lt;h2&gt;Running Inference for a Pre-Trained Model&lt;/h2&gt; 
&lt;p&gt;Our pre-trained model checkpoints can be run with a few lines of code (here our $\pi_0$-FAST-DROID model):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from openpi.training import config as _config
from openpi.policies import policy_config
from openpi.shared import download

config = _config.get_config("pi05_droid")
checkpoint_dir = download.maybe_download("gs://openpi-assets/checkpoints/pi05_droid")

# Create a trained policy.
policy = policy_config.create_trained_policy(config, checkpoint_dir)

# Run inference on a dummy example.
example = {
    "observation/exterior_image_1_left": ...,
    "observation/wrist_image_left": ...,
    ...
    "prompt": "pick up the fork"
}
action_chunk = policy.infer(example)["actions"]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also test this out in the &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/inference.ipynb"&gt;example notebook&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;We provide detailed step-by-step examples for running inference of our pre-trained checkpoints on &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/droid/README.md"&gt;DROID&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/aloha_real/README.md"&gt;ALOHA&lt;/a&gt; robots.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Remote Inference&lt;/strong&gt;: We provide &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/docs/remote_inference.md"&gt;examples and code&lt;/a&gt; for running inference of our models &lt;strong&gt;remotely&lt;/strong&gt;: the model can run on a different server and stream actions to the robot via a websocket connection. This makes it easy to use more powerful GPUs off-robot and keep robot and policy environments separate.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Test inference without a robot&lt;/strong&gt;: We provide a &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/simple_client/README.md"&gt;script&lt;/a&gt; for testing inference without a robot. This script will generate a random observation and run inference with the model. See &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/simple_client/README.md"&gt;here&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;Fine-Tuning Base Models on Your Own Data&lt;/h2&gt; 
&lt;p&gt;We will fine-tune the $\pi_{0.5}$ model on the &lt;a href="https://libero-project.github.io/datasets"&gt;LIBERO dataset&lt;/a&gt; as a running example for how to fine-tune a base model on your own data. We will explain three steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Convert your data to a LeRobot dataset (which we use for training)&lt;/li&gt; 
 &lt;li&gt;Defining training configs and running training&lt;/li&gt; 
 &lt;li&gt;Spinning up a policy server and running inference&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;1. Convert your data to a LeRobot dataset&lt;/h3&gt; 
&lt;p&gt;We provide a minimal example script for converting LIBERO data to a LeRobot dataset in &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/libero/convert_libero_data_to_lerobot.py"&gt;&lt;code&gt;examples/libero/convert_libero_data_to_lerobot.py&lt;/code&gt;&lt;/a&gt;. You can easily modify it to convert your own data! You can download the raw LIBERO dataset from &lt;a href="https://huggingface.co/datasets/openvla/modified_libero_rlds"&gt;here&lt;/a&gt;, and run the script with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run examples/libero/convert_libero_data_to_lerobot.py --data_dir /path/to/your/libero/data
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; If you just want to fine-tune on LIBERO, you can skip this step, because our LIBERO fine-tuning configs point to a pre-converted LIBERO dataset. This step is merely an example that you can adapt to your own data.&lt;/p&gt; 
&lt;h3&gt;2. Defining training configs and running training&lt;/h3&gt; 
&lt;p&gt;To fine-tune a base model on your own data, you need to define configs for data processing and training. We provide example configs with detailed comments for LIBERO below, which you can modify for your own dataset:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/src/openpi/policies/libero_policy.py"&gt;&lt;code&gt;LiberoInputs&lt;/code&gt; and &lt;code&gt;LiberoOutputs&lt;/code&gt;&lt;/a&gt;: Defines the data mapping from the LIBERO environment to the model and vice versa. Will be used for both, training and inference.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/src/openpi/training/config.py"&gt;&lt;code&gt;LeRobotLiberoDataConfig&lt;/code&gt;&lt;/a&gt;: Defines how to process raw LIBERO data from LeRobot dataset for training.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/src/openpi/training/config.py"&gt;&lt;code&gt;TrainConfig&lt;/code&gt;&lt;/a&gt;: Defines fine-tuning hyperparameters, data config, and weight loader.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We provide example fine-tuning configs for &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/src/openpi/training/config.py"&gt;Ï€â‚€&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/src/openpi/training/config.py"&gt;Ï€â‚€-FAST&lt;/a&gt;, and &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/src/openpi/training/config.py"&gt;Ï€â‚€.â‚…&lt;/a&gt; on LIBERO data.&lt;/p&gt; 
&lt;p&gt;Before we can run training, we need to compute the normalization statistics for the training data. Run the script below with the name of your training config:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run scripts/compute_norm_stats.py --config-name pi05_libero
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Now we can kick off training with the following command (the &lt;code&gt;--overwrite&lt;/code&gt; flag is used to overwrite existing checkpoints if you rerun fine-tuning with the same config):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;XLA_PYTHON_CLIENT_MEM_FRACTION=0.9 uv run scripts/train.py pi05_libero --exp-name=my_experiment --overwrite
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The command will log training progress to the console and save checkpoints to the &lt;code&gt;checkpoints&lt;/code&gt; directory. You can also monitor training progress on the Weights &amp;amp; Biases dashboard. For maximally using the GPU memory, set &lt;code&gt;XLA_PYTHON_CLIENT_MEM_FRACTION=0.9&lt;/code&gt; before running training -- this enables JAX to use up to 90% of the GPU memory (vs. the default of 75%).&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; We provide functionality for &lt;em&gt;reloading&lt;/em&gt; normalization statistics for state / action normalization from pre-training. This can be beneficial if you are fine-tuning to a new task on a robot that was part of our pre-training mixture. For more details on how to reload normalization statistics, see the &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/docs/norm_stats.md"&gt;norm_stats.md&lt;/a&gt; file.&lt;/p&gt; 
&lt;h3&gt;3. Spinning up a policy server and running inference&lt;/h3&gt; 
&lt;p&gt;Once training is complete, we can run inference by spinning up a policy server and then querying it from a LIBERO evaluation script. Launching a model server is easy (we use the checkpoint for iteration 20,000 for this example, modify as needed):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run scripts/serve_policy.py policy:checkpoint --policy.config=pi05_libero --policy.dir=checkpoints/pi05_libero/my_experiment/20000
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will spin up a server that listens on port 8000 and waits for observations to be sent to it. We can then run an evaluation script (or robot runtime) that queries the server.&lt;/p&gt; 
&lt;p&gt;For running the LIBERO eval in particular, we provide (and recommend using) a Dockerized workflow that handles both the policy server and the evaluation script together. See the &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/libero/README.md"&gt;LIBERO README&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;p&gt;If you want to embed a policy server call in your own robot runtime, we have a minimal example of how to do so in the &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/docs/remote_inference.md"&gt;remote inference docs&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;More Examples&lt;/h3&gt; 
&lt;p&gt;We provide more examples for how to fine-tune and run inference with our models on the ALOHA platform in the following READMEs:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/aloha_sim"&gt;ALOHA Simulator&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/aloha_real"&gt;ALOHA Real&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/ur5"&gt;UR5&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;PyTorch Support&lt;/h2&gt; 
&lt;p&gt;openpi now provides PyTorch implementations of Ï€â‚€ and Ï€â‚€.â‚… models alongside the original JAX versions! The PyTorch implementation has been validated on the LIBERO benchmark (both inference and finetuning). A few features are currently not supported (this may change in the future):&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;The Ï€â‚€-FAST model&lt;/li&gt; 
 &lt;li&gt;Mixed precision training&lt;/li&gt; 
 &lt;li&gt;FSDP (fully-sharded data parallelism) training&lt;/li&gt; 
 &lt;li&gt;LoRA (low-rank adaptation) training&lt;/li&gt; 
 &lt;li&gt;EMA (exponential moving average) weights during training&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Setup&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Make sure that you have the latest version of all dependencies installed: &lt;code&gt;uv sync&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Double check that you have transformers 4.53.2 installed: &lt;code&gt;uv pip show transformers&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Apply the transformers library patches:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cp -r ./src/openpi/models_pytorch/transformers_replace/* .venv/lib/python3.11/site-packages/transformers/
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;This overwrites several files in the transformers library with necessary model changes: 1) supporting AdaRMS, 2) correctly controlling the precision of activations, and 3) allowing the KV cache to be used without being updated.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;WARNING&lt;/strong&gt;: With the default uv link mode (hardlink), this will permanently affect the transformers library in your uv cache, meaning the changes will survive reinstallations of transformers and could even propagate to other projects that use transformers. To fully undo this operation, you must run &lt;code&gt;uv cache clean transformers&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Converting JAX Models to PyTorch&lt;/h3&gt; 
&lt;p&gt;To convert a JAX model checkpoint to PyTorch format:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run examples/convert_jax_model_to_pytorch.py \
    --checkpoint_dir /path/to/jax/checkpoint \
    --config_name &amp;lt;config name&amp;gt; \
    --output_path /path/to/converted/pytorch/checkpoint
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Running Inference with PyTorch&lt;/h3&gt; 
&lt;p&gt;The PyTorch implementation uses the same API as the JAX version - you only need to change the checkpoint path to point to the converted PyTorch model:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from openpi.training import config as _config
from openpi.policies import policy_config
from openpi.shared import download

config = _config.get_config("pi05_droid")
checkpoint_dir = "/path/to/converted/pytorch/checkpoint"

# Create a trained policy (automatically detects PyTorch format)
policy = policy_config.create_trained_policy(config, checkpoint_dir)

# Run inference (same API as JAX)
action_chunk = policy.infer(example)["actions"]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Policy Server with PyTorch&lt;/h3&gt; 
&lt;p&gt;The policy server works identically with PyTorch models - just point to the converted checkpoint directory:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run scripts/serve_policy.py policy:checkpoint \
    --policy.config=pi05_droid \
    --policy.dir=/path/to/converted/pytorch/checkpoint
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Finetuning with PyTorch&lt;/h3&gt; 
&lt;p&gt;To finetune a model in PyTorch:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Convert the JAX base model to PyTorch format:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;uv run examples/convert_jax_model_to_pytorch.py \
    --config_name &amp;lt;config name&amp;gt; \
    --checkpoint_dir /path/to/jax/base/model \
    --output_path /path/to/pytorch/base/model
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Specify the converted PyTorch model path in your config using &lt;code&gt;pytorch_weight_path&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Launch training using one of these modes:&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Single GPU training:
uv run scripts/train_pytorch.py &amp;lt;config_name&amp;gt; --exp_name &amp;lt;run_name&amp;gt; --save_interval &amp;lt;interval&amp;gt;

# Example:
uv run scripts/train_pytorch.py debug --exp_name pytorch_test
uv run scripts/train_pytorch.py debug --exp_name pytorch_test --resume  # Resume from latest checkpoint

# Multi-GPU training (single node):
uv run torchrun --standalone --nnodes=1 --nproc_per_node=&amp;lt;num_gpus&amp;gt; scripts/train_pytorch.py &amp;lt;config_name&amp;gt; --exp_name &amp;lt;run_name&amp;gt;

# Example:
uv run torchrun --standalone --nnodes=1 --nproc_per_node=2 scripts/train_pytorch.py pi0_aloha_sim --exp_name pytorch_ddp_test
uv run torchrun --standalone --nnodes=1 --nproc_per_node=2 scripts/train_pytorch.py pi0_aloha_sim --exp_name pytorch_ddp_test --resume

# Multi-Node Training:
uv run torchrun \
    --nnodes=&amp;lt;num_nodes&amp;gt; \
    --nproc_per_node=&amp;lt;gpus_per_node&amp;gt; \
    --node_rank=&amp;lt;rank_of_node&amp;gt; \
    --master_addr=&amp;lt;master_ip&amp;gt; \
    --master_port=&amp;lt;port&amp;gt; \
    scripts/train_pytorch.py &amp;lt;config_name&amp;gt; --exp_name=&amp;lt;run_name&amp;gt; --save_interval &amp;lt;interval&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Precision Settings&lt;/h3&gt; 
&lt;p&gt;JAX and PyTorch implementations handle precision as follows:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;JAX:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Inference: most weights and computations in bfloat16, with a few computations in float32 for stability&lt;/li&gt; 
 &lt;li&gt;Training: defaults to mixed precision: weights and gradients in float32, (most) activations and computations in bfloat16. You can change to full float32 training by setting &lt;code&gt;dtype&lt;/code&gt; to float32 in the config.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;PyTorch:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Inference: matches JAX -- most weights and computations in bfloat16, with a few weights converted to float32 for stability&lt;/li&gt; 
 &lt;li&gt;Training: supports either full bfloat16 (default) or full float32. You can change it by setting &lt;code&gt;pytorch_training_precision&lt;/code&gt; in the config. bfloat16 uses less memory but exhibits higher losses compared to float32. Mixed precision is not yet supported.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;With torch.compile, inference speed is comparable between JAX and PyTorch.&lt;/p&gt; 
&lt;h2&gt;Troubleshooting&lt;/h2&gt; 
&lt;p&gt;We will collect common issues and their solutions here. If you encounter an issue, please check here first. If you can't find a solution, please file an issue on the repo (see &lt;a href="https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/CONTRIBUTING.md"&gt;here&lt;/a&gt; for guidelines).&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Issue&lt;/th&gt; 
   &lt;th&gt;Resolution&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;uv sync&lt;/code&gt; fails with dependency conflicts&lt;/td&gt; 
   &lt;td&gt;Try removing the virtual environment directory (&lt;code&gt;rm -rf .venv&lt;/code&gt;) and running &lt;code&gt;uv sync&lt;/code&gt; again. If issues persist, check that you have the latest version of &lt;code&gt;uv&lt;/code&gt; installed (&lt;code&gt;uv self update&lt;/code&gt;).&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Training runs out of GPU memory&lt;/td&gt; 
   &lt;td&gt;Make sure you set &lt;code&gt;XLA_PYTHON_CLIENT_MEM_FRACTION=0.9&lt;/code&gt; (or higher) before running training to allow JAX to use more GPU memory. You can also use &lt;code&gt;--fsdp-devices &amp;lt;n&amp;gt;&lt;/code&gt; where &lt;code&gt;&amp;lt;n&amp;gt;&lt;/code&gt; is your number of GPUs, to enable &lt;a href="https://engineering.fb.com/2021/07/15/open-source/fsdp/"&gt;fully-sharded data parallelism&lt;/a&gt;, which reduces memory usage in exchange for slower training (the amount of slowdown depends on your particular setup). If you are still running out of memory, you may way to consider disabling EMA.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Policy server connection errors&lt;/td&gt; 
   &lt;td&gt;Check that the server is running and listening on the expected port. Verify network connectivity and firewall settings between client and server.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Missing norm stats error when training&lt;/td&gt; 
   &lt;td&gt;Run &lt;code&gt;scripts/compute_norm_stats.py&lt;/code&gt; with your config name before starting training.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Dataset download fails&lt;/td&gt; 
   &lt;td&gt;Check your internet connection. For HuggingFace datasets, ensure you're logged in (&lt;code&gt;huggingface-cli login&lt;/code&gt;).&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;CUDA/GPU errors&lt;/td&gt; 
   &lt;td&gt;Verify NVIDIA drivers are installed correctly. For Docker, ensure nvidia-container-toolkit is installed. Check GPU compatibility. You do NOT need CUDA libraries installed at a system level --- they will be installed via uv. You may even want to try &lt;em&gt;uninstalling&lt;/em&gt; system CUDA libraries if you run into CUDA issues, since system libraries can sometimes cause conflicts.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Import errors when running examples&lt;/td&gt; 
   &lt;td&gt;Make sure you've installed all dependencies with &lt;code&gt;uv sync&lt;/code&gt;. Some examples may have additional requirements listed in their READMEs.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Action dimensions mismatch&lt;/td&gt; 
   &lt;td&gt;Verify your data processing transforms match the expected input/output dimensions of your robot. Check the action space definitions in your policy classes.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Diverging training loss&lt;/td&gt; 
   &lt;td&gt;Check the &lt;code&gt;q01&lt;/code&gt;, &lt;code&gt;q99&lt;/code&gt;, and &lt;code&gt;std&lt;/code&gt; values in &lt;code&gt;norm_stats.json&lt;/code&gt; for your dataset. Certain dimensions that are rarely used can end up with very small &lt;code&gt;q01&lt;/code&gt;, &lt;code&gt;q99&lt;/code&gt;, or &lt;code&gt;std&lt;/code&gt; values, leading to huge states and actions after normalization. You can manually adjust the norm stats as a workaround.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt;</description>
    </item>
    
  </channel>
</rss>