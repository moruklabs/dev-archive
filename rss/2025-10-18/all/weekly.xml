<rss version="2.0">
  <channel>
    <title>GitHub All Languages Weekly Trending</title>
    <description>Weekly Trending of All Languages in GitHub</description>
    <pubDate>Fri, 17 Oct 2025 01:40:09 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>nitrojs/nitro</title>
      <link>https://github.com/nitrojs/nitro</link>
      <description>&lt;p&gt;Next Generation Server Toolkit. Create web servers with everything you need and deploy them wherever you prefer.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Nitro&lt;/h1&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] You‚Äôre viewing the &lt;strong&gt;v3 Alpha&lt;/strong&gt; branch. For the current stable release, see &lt;a href="https://github.com/nitrojs/nitro/tree/v2"&gt;Nitro v2&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;Nitro&lt;/strong&gt; extends your Vite app with a &lt;strong&gt;production-ready server&lt;/strong&gt;, designed to run &lt;strong&gt;anywhere&lt;/strong&gt;. Add server routes, deploy across multiple platforms, and enjoy a &lt;strong&gt;zero-config&lt;/strong&gt; experience.&lt;/p&gt; 
&lt;p&gt;üìò &lt;strong&gt;Docs (v3 Alpha):&lt;/strong&gt; &lt;a href="https://v3.nitro.build"&gt;https://v3.nitro.build&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;See Check out the &lt;a href="https://raw.githubusercontent.com/nitrojs/nitro/main/CONTRIBUTING.md"&gt;Contribution Guide&lt;/a&gt; to get started.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Released under the &lt;a href="https://raw.githubusercontent.com/nitrojs/nitro/main/LICENSE"&gt;MIT License&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>davila7/claude-code-templates</title>
      <link>https://github.com/davila7/claude-code-templates</link>
      <description>&lt;p&gt;CLI tool for configuring and monitoring Claude Code&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href="https://www.npmjs.com/package/claude-code-templates"&gt;&lt;img src="https://img.shields.io/npm/v/claude-code-templates.svg?sanitize=true" alt="npm version" /&gt;&lt;/a&gt; &lt;a href="https://www.npmjs.com/package/claude-code-templates"&gt;&lt;img src="https://img.shields.io/npm/dt/claude-code-templates.svg?sanitize=true" alt="npm downloads" /&gt;&lt;/a&gt; &lt;a href="https://opensource.org/licenses/MIT"&gt;&lt;img src="https://img.shields.io/badge/License-MIT-yellow.svg?sanitize=true" alt="License: MIT" /&gt;&lt;/a&gt; &lt;a href="https://opensource.org/"&gt;&lt;img src="https://badges.frapsoft.com/os/v1/open-source.svg?v=103" alt="Open Source" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/davila7/claude-code-templates/main/CONTRIBUTING.md"&gt;&lt;img src="https://img.shields.io/badge/PRs-welcome-brightgreen.svg?sanitize=true" alt="PRs Welcome" /&gt;&lt;/a&gt; &lt;a href="https://github.com/davila7/claude-code-templates"&gt;&lt;img src="https://img.shields.io/github/stars/davila7/claude-code-templates.svg?style=social&amp;amp;label=Star" alt="GitHub stars" /&gt;&lt;/a&gt; &lt;a href="https://github.com/hesreallyhim/awesome-claude-code"&gt;&lt;img src="https://awesome.re/mentioned-badge-flat.svg?sanitize=true" alt="Mentioned in Awesome Claude Code" /&gt;&lt;/a&gt; &lt;a href="https://buymeacoffee.com/daniavila"&gt;&lt;img src="https://img.shields.io/badge/%E2%98%95-Buy%20me%20a%20coffee-ffdd00?style=flat&amp;amp;logo=buy-me-a-coffee" alt="Buy Me a Coffee" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Claude Code Templates (aitmpl.com)&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;Ready-to-use configurations for Anthropic's Claude Code.&lt;/strong&gt; A comprehensive collection of AI agents, custom commands, settings, hooks, external integrations (MCPs), and project templates to enhance your development workflow.&lt;/p&gt; 
&lt;h2&gt;Browse &amp;amp; Install Components and Templates&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://aitmpl.com"&gt;Browse All Templates&lt;/a&gt;&lt;/strong&gt; - Interactive web interface to explore and install 100+ agents, commands, settings, hooks, and MCPs.&lt;/p&gt; 
&lt;img width="1049" height="855" alt="Screenshot 2025-08-19 at 08 09 24" src="https://github.com/user-attachments/assets/e3617410-9b1c-4731-87b7-a3858800b737" /&gt; 
&lt;h2&gt;üöÄ Quick Installation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install a complete development stack
npx claude-code-templates@latest --agent development-team/frontend-developer --command testing/generate-tests --mcp development/github-integration

# Browse and install interactively
npx claude-code-templates@latest

# Install specific components
npx claude-code-templates@latest --agent business-marketing/security-auditor
npx claude-code-templates@latest --command performance/optimize-bundle
npx claude-code-templates@latest --setting performance/mcp-timeouts
npx claude-code-templates@latest --hook git/pre-commit-validation
npx claude-code-templates@latest --mcp database/postgresql-integration
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;What You Get&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Component&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Examples&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ü§ñ Agents&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;AI specialists for specific domains&lt;/td&gt; 
   &lt;td&gt;Security auditor, React performance optimizer, database architect&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;‚ö° Commands&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Custom slash commands&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;/generate-tests&lt;/code&gt;, &lt;code&gt;/optimize-bundle&lt;/code&gt;, &lt;code&gt;/check-security&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üîå MCPs&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;External service integrations&lt;/td&gt; 
   &lt;td&gt;GitHub, PostgreSQL, Stripe, AWS, OpenAI&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;‚öôÔ∏è Settings&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Claude Code configurations&lt;/td&gt; 
   &lt;td&gt;Timeouts, memory settings, output styles&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ü™ù Hooks&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Automation triggers&lt;/td&gt; 
   &lt;td&gt;Pre-commit validation, post-completion actions&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üì¶ Templates&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Complete project configurations with CLAUDE.md, .claude/* files and .mcp.json&lt;/td&gt; 
   &lt;td&gt;Framework-specific setups, project best practices&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;üõ†Ô∏è Additional Tools&lt;/h2&gt; 
&lt;p&gt;Beyond the template catalog, Claude Code Templates includes powerful development tools:&lt;/p&gt; 
&lt;h3&gt;üìä Claude Code Analytics&lt;/h3&gt; 
&lt;p&gt;Monitor your AI-powered development sessions in real-time with live state detection and performance metrics.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npx claude-code-templates@latest --analytics
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;üí¨ Conversation Monitor&lt;/h3&gt; 
&lt;p&gt;Mobile-optimized interface to view Claude responses in real-time with secure remote access.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Local access
npx claude-code-templates@latest --chats

# Secure remote access via Cloudflare Tunnel
npx claude-code-templates@latest --chats --tunnel
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;üîç Health Check&lt;/h3&gt; 
&lt;p&gt;Comprehensive diagnostics to ensure your Claude Code installation is optimized.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npx claude-code-templates@latest --health-check
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;üîå Plugin Dashboard&lt;/h3&gt; 
&lt;p&gt;View marketplaces, installed plugins, and manage permissions from a unified interface.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npx claude-code-templates@latest --plugins
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;üìñ Documentation&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://docs.aitmpl.com/"&gt;üìö docs.aitmpl.com&lt;/a&gt;&lt;/strong&gt; - Complete guides, examples, and API reference for all components and tools.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions! &lt;strong&gt;&lt;a href="https://aitmpl.com"&gt;Browse existing templates&lt;/a&gt;&lt;/strong&gt; to see what's available, then check our &lt;a href="https://raw.githubusercontent.com/davila7/claude-code-templates/main/CONTRIBUTING.md"&gt;contributing guidelines&lt;/a&gt; to add your own agents, commands, MCPs, settings, or hooks.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Please read our &lt;a href="https://raw.githubusercontent.com/davila7/claude-code-templates/main/CODE_OF_CONDUCT.md"&gt;Code of Conduct&lt;/a&gt; before contributing.&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;Attribution&lt;/h2&gt; 
&lt;p&gt;This collection includes components from multiple sources:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Agents Collection:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;wshobson/agents Collection&lt;/strong&gt; by &lt;a href="https://github.com/wshobson/agents"&gt;wshobson&lt;/a&gt; - Licensed under MIT License (48 agents)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Commands Collection:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;awesome-claude-code Commands&lt;/strong&gt; by &lt;a href="https://github.com/hesreallyhim/awesome-claude-code"&gt;hesreallyhim&lt;/a&gt; - Licensed under CC0 1.0 Universal (21 commands)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üìÑ License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the MIT License - see the &lt;a href="https://raw.githubusercontent.com/davila7/claude-code-templates/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;h2&gt;üîó Links&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;üåê Browse Templates&lt;/strong&gt;: &lt;a href="https://aitmpl.com"&gt;aitmpl.com&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üìö Documentation&lt;/strong&gt;: &lt;a href="https://docs.aitmpl.com"&gt;docs.aitmpl.com&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üí¨ Community&lt;/strong&gt;: &lt;a href="https://github.com/davila7/claude-code-templates/discussions"&gt;GitHub Discussions&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üêõ Issues&lt;/strong&gt;: &lt;a href="https://github.com/davila7/claude-code-templates/issues"&gt;GitHub Issues&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;‚≠ê Star History&lt;/h2&gt; 
&lt;a href="https://star-history.com/#davila7/claude-code-templates&amp;amp;Date"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=davila7/claude-code-templates&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=davila7/claude-code-templates&amp;amp;type=Date" /&gt; 
  &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=davila7/claude-code-templates&amp;amp;type=Date" /&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;strong&gt;‚≠ê Found this useful? Give us a star to support the project!&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://buymeacoffee.com/daniavila"&gt;&lt;img src="https://img.buymeacoffee.com/button-api/?text=Buy%20me%20a%20coffee&amp;amp;slug=daniavila&amp;amp;button_colour=FFDD00&amp;amp;font_colour=000000&amp;amp;font_family=Cookie&amp;amp;outline_colour=000000&amp;amp;coffee_colour=ffffff" alt="Buy Me A Coffee" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>evershopcommerce/evershop</title>
      <link>https://github.com/evershopcommerce/evershop</link>
      <description>&lt;p&gt;üõçÔ∏è Typescript E-commerce Platform&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img width="60" height="68" alt="EverShop Logo" src="https://raw.githubusercontent.com/evershopcommerce/evershop/dev/.github/images/logo-green.png" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;/p&gt;
&lt;h1 align="center"&gt;EverShop&lt;/h1&gt; 
&lt;p&gt;&lt;/p&gt; 
&lt;h4 align="center"&gt; &lt;a href="https://evershop.io/docs/development/getting-started/introduction"&gt;Documentation&lt;/a&gt; | &lt;a href="https://demo.evershop.io/"&gt;Demo&lt;/a&gt; &lt;/h4&gt; 
&lt;p align="center"&gt; &lt;img src="https://github.com/evershopcommerce/evershop/actions/workflows/build_test.yml/badge.svg?sanitize=true" alt="Github Action" /&gt; &lt;a href="https://twitter.com/evershopjs"&gt; &lt;img alt="Twitter Follow" src="https://img.shields.io/twitter/follow/evershopjs?style=social" /&gt; &lt;/a&gt; &lt;a href="https://discord.gg/GSzt7dt7RM"&gt; &lt;img src="https://img.shields.io/discord/757179260417867879?label=discord" alt="Discord" /&gt; &lt;/a&gt; &lt;a href="https://opensource.org/licenses/GPL-3.0"&gt; &lt;img src="https://img.shields.io/badge/License-GPLv3-blue.svg?sanitize=true" alt="License" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;img alt="EverShop" width="950" src="https://raw.githubusercontent.com/evershopcommerce/evershop/dev/.github/images/banner.png" /&gt; &lt;/p&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;EverShop is a modern, TypeScript-first eCommerce platform built with GraphQL and React. Designed for developers, it offers essential commerce features in a modular, fully customizable architecture‚Äîperfect for building tailored shopping experiences with confidence and speed.&lt;/p&gt; 
&lt;h2&gt;Installation Using Docker&lt;/h2&gt; 
&lt;p&gt;You can get started with EverShop in minutes by using the Docker image. The Docker image is a great way to get started with EverShop without having to worry about installing dependencies or configuring your environment.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -sSL https://raw.githubusercontent.com/evershopcommerce/evershop/main/docker-compose.yml &amp;gt; docker-compose.yml
docker-compose up -d
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For the full installation guide, please refer to our &lt;a href="https://evershop.io/docs/development/getting-started/installation-guide"&gt;Installation guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://evershop.io/docs/development/getting-started/installation-guide"&gt;Installation guide&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://evershop.io/docs/development/module/create-your-first-extension"&gt;Extension development&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://evershop.io/docs/development/theme/theme-overview"&gt;Theme development&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Demo&lt;/h2&gt; 
&lt;p&gt;Explore our demo store.&lt;/p&gt; 
&lt;p align="left"&gt; &lt;a href="https://demo.evershop.io/admin" target="_blank"&gt; &lt;img alt="EverShop Admin Demo" height="35" src="https://raw.githubusercontent.com/evershopcommerce/evershop/dev/.github/images/evershop-demo-back.png" /&gt; &lt;/a&gt; &lt;a href="https://demo.evershop.io/" target="_blank"&gt; &lt;img alt="EverShop Store Demo" height="35" src="https://raw.githubusercontent.com/evershopcommerce/evershop/dev/.github/images/evershop-demo-front.png" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;b&gt;Demo user:&lt;/b&gt; 
&lt;p&gt;Email: &lt;a href="mailto:demo@evershop.io"&gt;demo@evershop.io&lt;/a&gt;&lt;br /&gt; Password: 123456&lt;/p&gt; 
&lt;h2&gt;Support&lt;/h2&gt; 
&lt;p&gt;If you like my work, feel free to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;‚≠ê this repository. It helps.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://twitter.com/intent/tweet?url=https%3A%2F%2Fgithub.com%2Fevershopcommerce%2Fevershop&amp;amp;text=Awesome%20React%20Ecommerce%20Project&amp;amp;hashtags=react,ecommerce,expressjs,graphql"&gt;&lt;img src="https://img.shields.io/twitter/url/http/shields.io.svg?style=social" alt="Tweet" /&gt;&lt;/a&gt; about EverShop. Thank you!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;EverShop is an open-source project. We are committed to a fully transparent development process and appreciate highly any contributions. Whether you are helping us fix bugs, proposing new features, improving our documentation or spreading the word - we would love to have you as part of the EverShop community.&lt;/p&gt; 
&lt;h3&gt;Ask a question about EverShop&lt;/h3&gt; 
&lt;p&gt;You can ask questions, and participate in discussions about EverShop-related topics in the EverShop Discord channel.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://discord.gg/GSzt7dt7RM"&gt;&lt;img src="https://raw.githubusercontent.com/evershopcommerce/evershop/dev/.github/images/discord_banner_github.svg?sanitize=true" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Create a bug report&lt;/h3&gt; 
&lt;p&gt;If you see an error message or run into an issue, please &lt;a href="https://github.com/evershopcommerce/evershop/issues/new"&gt;create bug report&lt;/a&gt;. This effort is valued and it will help all EverShop users.&lt;/p&gt; 
&lt;h3&gt;Submit a feature request&lt;/h3&gt; 
&lt;p&gt;If you have an idea, or you're missing a capability that would make development easier and more robust, please &lt;a href="https://github.com/evershopcommerce/evershop/issues/new"&gt;Submit feature request&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If a similar feature request already exists, don't forget to leave a "+1". If you add some more information such as your thoughts and vision about the feature, your comments will be embraced warmly :)&lt;/p&gt; 
&lt;p&gt;Please refer to our &lt;a href="https://raw.githubusercontent.com/evershopcommerce/evershop/dev/CONTRIBUTING.md"&gt;Contribution Guidelines&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/evershopcommerce/evershop/dev/CODE_OF_CONDUCT.md"&gt;Code of Conduct&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/evershopcommerce/evershop/raw/main/LICENSE"&gt;GPL-3.0 License&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>78/xiaozhi-esp32</title>
      <link>https://github.com/78/xiaozhi-esp32</link>
      <description>&lt;p&gt;An MCP-based chatbot | ‰∏Ä‰∏™Âü∫‰∫éMCPÁöÑËÅäÂ§©Êú∫Âô®‰∫∫&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;An MCP-based Chatbot&lt;/h1&gt; 
&lt;p&gt;Ôºà‰∏≠Êñá | &lt;a href="https://raw.githubusercontent.com/78/xiaozhi-esp32/main/README_en.md"&gt;English&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/78/xiaozhi-esp32/main/README_ja.md"&gt;Êó•Êú¨Ë™û&lt;/a&gt;Ôºâ&lt;/p&gt; 
&lt;h2&gt;‰ªãÁªç&lt;/h2&gt; 
&lt;p&gt;üëâ &lt;a href="https://www.bilibili.com/video/BV1bpjgzKEhd/"&gt;‰∫∫Á±ªÔºöÁªô AI Ë£ÖÊëÑÂÉèÂ§¥ vs AIÔºöÂΩìÂú∫ÂèëÁé∞‰∏ª‰∫∫‰∏âÂ§©Ê≤°Ê¥óÂ§¥„Äêbilibili„Äë&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;üëâ &lt;a href="https://www.bilibili.com/video/BV1XnmFYLEJN/"&gt;ÊâãÂ∑•ÊâìÈÄ†‰Ω†ÁöÑ AI Â•≥ÂèãÔºåÊñ∞ÊâãÂÖ•Èó®ÊïôÁ®ã„Äêbilibili„Äë&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Â∞èÊô∫ AI ËÅäÂ§©Êú∫Âô®‰∫∫‰Ωú‰∏∫‰∏Ä‰∏™ËØ≠Èü≥‰∫§‰∫íÂÖ•Âè£ÔºåÂà©Áî® Qwen / DeepSeek Á≠âÂ§ßÊ®°ÂûãÁöÑ AI ËÉΩÂäõÔºåÈÄöËøá MCP ÂçèËÆÆÂÆûÁé∞Â§öÁ´ØÊéßÂà∂„ÄÇ&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/78/xiaozhi-esp32/main/docs/mcp-based-graph.jpg" alt="ÈÄöËøáMCPÊéßÂà∂‰∏áÁâ©" width="320" /&gt; 
&lt;h3&gt;ÁâàÊú¨ËØ¥Êòé&lt;/h3&gt; 
&lt;p&gt;ÂΩìÂâç v2 ÁâàÊú¨‰∏é v1 ÁâàÊú¨ÂàÜÂå∫Ë°®‰∏çÂÖºÂÆπÔºåÊâÄ‰ª•Êó†Ê≥ï‰ªé v1 ÁâàÊú¨ÈÄöËøá OTA ÂçáÁ∫ßÂà∞ v2 ÁâàÊú¨„ÄÇÂàÜÂå∫Ë°®ËØ¥ÊòéÂèÇËßÅ &lt;a href="https://raw.githubusercontent.com/78/xiaozhi-esp32/main/partitions/v2/README.md"&gt;partitions/v2/README.md&lt;/a&gt;„ÄÇ&lt;/p&gt; 
&lt;p&gt;‰ΩøÁî® v1 ÁâàÊú¨ÁöÑÊâÄÊúâÁ°¨‰ª∂ÔºåÂèØ‰ª•ÈÄöËøáÊâãÂä®ÁÉßÂΩïÂõ∫‰ª∂Êù•ÂçáÁ∫ßÂà∞ v2 ÁâàÊú¨„ÄÇ&lt;/p&gt; 
&lt;p&gt;v1 ÁöÑÁ®≥ÂÆöÁâàÊú¨‰∏∫ 1.9.2ÔºåÂèØ‰ª•ÈÄöËøá &lt;code&gt;git checkout v1&lt;/code&gt; Êù•ÂàáÊç¢Âà∞ v1 ÁâàÊú¨ÔºåËØ•ÂàÜÊîØ‰ºöÊåÅÁª≠Áª¥Êä§Âà∞ 2026 Âπ¥ 2 Êúà„ÄÇ&lt;/p&gt; 
&lt;h3&gt;Â∑≤ÂÆûÁé∞ÂäüËÉΩ&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Wi-Fi / ML307 Cat.1 4G&lt;/li&gt; 
 &lt;li&gt;Á¶ªÁ∫øËØ≠Èü≥Âî§ÈÜí &lt;a href="https://github.com/espressif/esp-sr"&gt;ESP-SR&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ÊîØÊåÅ‰∏§ÁßçÈÄö‰ø°ÂçèËÆÆÔºà&lt;a href="https://raw.githubusercontent.com/78/xiaozhi-esp32/main/docs/websocket.md"&gt;Websocket&lt;/a&gt; Êàñ MQTT+UDPÔºâ&lt;/li&gt; 
 &lt;li&gt;ÈááÁî® OPUS Èü≥È¢ëÁºñËß£Á†Å&lt;/li&gt; 
 &lt;li&gt;Âü∫‰∫éÊµÅÂºè ASR + LLM + TTS Êû∂ÊûÑÁöÑËØ≠Èü≥‰∫§‰∫í&lt;/li&gt; 
 &lt;li&gt;Â£∞Á∫πËØÜÂà´ÔºåËØÜÂà´ÂΩìÂâçËØ¥ËØù‰∫∫ÁöÑË∫´‰ªΩ &lt;a href="https://github.com/modelscope/3D-Speaker"&gt;3D Speaker&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;OLED / LCD ÊòæÁ§∫Â±èÔºåÊîØÊåÅË°®ÊÉÖÊòæÁ§∫&lt;/li&gt; 
 &lt;li&gt;ÁîµÈáèÊòæÁ§∫‰∏éÁîµÊ∫êÁÆ°ÁêÜ&lt;/li&gt; 
 &lt;li&gt;ÊîØÊåÅÂ§öËØ≠Ë®ÄÔºà‰∏≠Êñá„ÄÅËã±Êñá„ÄÅÊó•ÊñáÔºâ&lt;/li&gt; 
 &lt;li&gt;ÊîØÊåÅ ESP32-C3„ÄÅESP32-S3„ÄÅESP32-P4 ËäØÁâáÂπ≥Âè∞&lt;/li&gt; 
 &lt;li&gt;ÈÄöËøáËÆæÂ§áÁ´Ø MCP ÂÆûÁé∞ËÆæÂ§áÊéßÂà∂ÔºàÈü≥Èáè„ÄÅÁÅØÂÖâ„ÄÅÁîµÊú∫„ÄÅGPIO Á≠âÔºâ&lt;/li&gt; 
 &lt;li&gt;ÈÄöËøá‰∫ëÁ´Ø MCP Êâ©Â±ïÂ§ßÊ®°ÂûãËÉΩÂäõÔºàÊô∫ËÉΩÂÆ∂Â±ÖÊéßÂà∂„ÄÅPCÊ°åÈù¢Êìç‰Ωú„ÄÅÁü•ËØÜÊêúÁ¥¢„ÄÅÈÇÆ‰ª∂Êî∂ÂèëÁ≠âÔºâ&lt;/li&gt; 
 &lt;li&gt;Ëá™ÂÆö‰πâÂî§ÈÜíËØç„ÄÅÂ≠ó‰Ωì„ÄÅË°®ÊÉÖ‰∏éËÅäÂ§©ËÉåÊôØÔºåÊîØÊåÅÁΩëÈ°µÁ´ØÂú®Á∫ø‰øÆÊîπ (&lt;a href="https://github.com/78/xiaozhi-assets-generator"&gt;Ëá™ÂÆö‰πâAssetsÁîüÊàêÂô®&lt;/a&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Á°¨‰ª∂&lt;/h2&gt; 
&lt;h3&gt;Èù¢ÂåÖÊùøÊâãÂ∑•Âà∂‰ΩúÂÆûË∑µ&lt;/h3&gt; 
&lt;p&gt;ËØ¶ËßÅÈ£û‰π¶ÊñáÊ°£ÊïôÁ®ãÔºö&lt;/p&gt; 
&lt;p&gt;üëâ &lt;a href="https://ccnphfhqs21z.feishu.cn/wiki/F5krwD16viZoF0kKkvDcrZNYnhb?from=from_copylink"&gt;„ÄäÂ∞èÊô∫ AI ËÅäÂ§©Êú∫Âô®‰∫∫ÁôæÁßëÂÖ®‰π¶„Äã&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Èù¢ÂåÖÊùøÊïàÊûúÂõæÂ¶Ç‰∏ãÔºö&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/78/xiaozhi-esp32/main/docs/v1/wiring2.jpg" alt="Èù¢ÂåÖÊùøÊïàÊûúÂõæ" /&gt;&lt;/p&gt; 
&lt;h3&gt;ÊîØÊåÅ 70 Â§ö‰∏™ÂºÄÊ∫êÁ°¨‰ª∂Ôºà‰ªÖÂ±ïÁ§∫ÈÉ®ÂàÜÔºâ&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://oshwhub.com/li-chuang-kai-fa-ban/li-chuang-shi-zhan-pai-esp32-s3-kai-fa-ban" target="_blank" title="Á´ãÂàõ¬∑ÂÆûÊàòÊ¥æ ESP32-S3 ÂºÄÂèëÊùø"&gt;Á´ãÂàõ¬∑ÂÆûÊàòÊ¥æ ESP32-S3 ÂºÄÂèëÊùø&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/espressif/esp-box" target="_blank" title="‰πêÈë´ ESP32-S3-BOX3"&gt;‰πêÈë´ ESP32-S3-BOX3&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.m5stack.com/zh_CN/core/CoreS3" target="_blank" title="M5Stack CoreS3"&gt;M5Stack CoreS3&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.m5stack.com/en/atom/Atomic%20Echo%20Base" target="_blank" title="AtomS3R + Echo Base"&gt;M5Stack AtomS3R + Echo Base&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://gf.bilibili.com/item/detail/1108782064" target="_blank" title="Á•ûÂ•áÊåâÈíÆ 2.4"&gt;Á•ûÂ•áÊåâÈíÆ 2.4&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.waveshare.net/shop/ESP32-S3-Touch-AMOLED-1.8.htm" target="_blank" title="ÂæÆÈõ™ÁîµÂ≠ê ESP32-S3-Touch-AMOLED-1.8"&gt;ÂæÆÈõ™ÁîµÂ≠ê ESP32-S3-Touch-AMOLED-1.8&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Xinyuan-LilyGO/T-Circle-S3" target="_blank" title="LILYGO T-Circle-S3"&gt;LILYGO T-Circle-S3&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://oshwhub.com/tenclass01/xmini_c3" target="_blank" title="ËôæÂì• Mini C3"&gt;ËôæÂì• Mini C3&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://oshwhub.com/movecall/cuican-ai-pendant-lights-up-y" target="_blank" title="Movecall CuiCan ESP32S3"&gt;ÁíÄÁí®¬∑AI ÂêäÂù†&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/WMnologo/xingzhi-ai" target="_blank" title="Êó†ÂêçÁßëÊäÄNologo-ÊòüÊô∫-1.54"&gt;Êó†ÂêçÁßëÊäÄ Nologo-ÊòüÊô∫-1.54TFT&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.seeedstudio.com/SenseCAP-Watcher-W1-A-p-5979.html" target="_blank" title="SenseCAP Watcher"&gt;SenseCAP Watcher&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.bilibili.com/video/BV1BHJtz6E2S/" target="_blank" title="ESP-HI Ë∂Ö‰ΩéÊàêÊú¨Êú∫Âô®Áãó"&gt;ESP-HI Ë∂Ö‰ΩéÊàêÊú¨Êú∫Âô®Áãó&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div style="display: flex; justify-content: space-between;"&gt; 
 &lt;a href="https://raw.githubusercontent.com/78/xiaozhi-esp32/main/docs/v1/lichuang-s3.jpg" target="_blank" title="Á´ãÂàõ¬∑ÂÆûÊàòÊ¥æ ESP32-S3 ÂºÄÂèëÊùø"&gt; &lt;img src="https://raw.githubusercontent.com/78/xiaozhi-esp32/main/docs/v1/lichuang-s3.jpg" width="240" /&gt; &lt;/a&gt; 
 &lt;a href="https://raw.githubusercontent.com/78/xiaozhi-esp32/main/docs/v1/espbox3.jpg" target="_blank" title="‰πêÈë´ ESP32-S3-BOX3"&gt; &lt;img src="https://raw.githubusercontent.com/78/xiaozhi-esp32/main/docs/v1/espbox3.jpg" width="240" /&gt; &lt;/a&gt; 
 &lt;a href="https://raw.githubusercontent.com/78/xiaozhi-esp32/main/docs/v1/m5cores3.jpg" target="_blank" title="M5Stack CoreS3"&gt; &lt;img src="https://raw.githubusercontent.com/78/xiaozhi-esp32/main/docs/v1/m5cores3.jpg" width="240" /&gt; &lt;/a&gt; 
 &lt;a href="https://raw.githubusercontent.com/78/xiaozhi-esp32/main/docs/v1/atoms3r.jpg" target="_blank" title="AtomS3R + Echo Base"&gt; &lt;img src="https://raw.githubusercontent.com/78/xiaozhi-esp32/main/docs/v1/atoms3r.jpg" width="240" /&gt; &lt;/a&gt; 
 &lt;a href="https://raw.githubusercontent.com/78/xiaozhi-esp32/main/docs/v1/magiclick.jpg" target="_blank" title="Á•ûÂ•áÊåâÈíÆ 2.4"&gt; &lt;img src="https://raw.githubusercontent.com/78/xiaozhi-esp32/main/docs/v1/magiclick.jpg" width="240" /&gt; &lt;/a&gt; 
 &lt;a href="https://raw.githubusercontent.com/78/xiaozhi-esp32/main/docs/v1/waveshare.jpg" target="_blank" title="ÂæÆÈõ™ÁîµÂ≠ê ESP32-S3-Touch-AMOLED-1.8"&gt; &lt;img src="https://raw.githubusercontent.com/78/xiaozhi-esp32/main/docs/v1/waveshare.jpg" width="240" /&gt; &lt;/a&gt; 
 &lt;a href="https://raw.githubusercontent.com/78/xiaozhi-esp32/main/docs/v1/lilygo-t-circle-s3.jpg" target="_blank" title="LILYGO T-Circle-S3"&gt; &lt;img src="https://raw.githubusercontent.com/78/xiaozhi-esp32/main/docs/v1/lilygo-t-circle-s3.jpg" width="240" /&gt; &lt;/a&gt; 
 &lt;a href="https://raw.githubusercontent.com/78/xiaozhi-esp32/main/docs/v1/xmini-c3.jpg" target="_blank" title="ËôæÂì• Mini C3"&gt; &lt;img src="https://raw.githubusercontent.com/78/xiaozhi-esp32/main/docs/v1/xmini-c3.jpg" width="240" /&gt; &lt;/a&gt; 
 &lt;a href="https://raw.githubusercontent.com/78/xiaozhi-esp32/main/docs/v1/movecall-cuican-esp32s3.jpg" target="_blank" title="CuiCan"&gt; &lt;img src="https://raw.githubusercontent.com/78/xiaozhi-esp32/main/docs/v1/movecall-cuican-esp32s3.jpg" width="240" /&gt; &lt;/a&gt; 
 &lt;a href="https://raw.githubusercontent.com/78/xiaozhi-esp32/main/docs/v1/wmnologo_xingzhi_1.54.jpg" target="_blank" title="Êó†ÂêçÁßëÊäÄNologo-ÊòüÊô∫-1.54"&gt; &lt;img src="https://raw.githubusercontent.com/78/xiaozhi-esp32/main/docs/v1/wmnologo_xingzhi_1.54.jpg" width="240" /&gt; &lt;/a&gt; 
 &lt;a href="https://raw.githubusercontent.com/78/xiaozhi-esp32/main/docs/v1/sensecap_watcher.jpg" target="_blank" title="SenseCAP Watcher"&gt; &lt;img src="https://raw.githubusercontent.com/78/xiaozhi-esp32/main/docs/v1/sensecap_watcher.jpg" width="240" /&gt; &lt;/a&gt; 
 &lt;a href="https://raw.githubusercontent.com/78/xiaozhi-esp32/main/docs/v1/esp-hi.jpg" target="_blank" title="ESP-HI Ë∂Ö‰ΩéÊàêÊú¨Êú∫Âô®Áãó"&gt; &lt;img src="https://raw.githubusercontent.com/78/xiaozhi-esp32/main/docs/v1/esp-hi.jpg" width="240" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;ËΩØ‰ª∂&lt;/h2&gt; 
&lt;h3&gt;Âõ∫‰ª∂ÁÉßÂΩï&lt;/h3&gt; 
&lt;p&gt;Êñ∞ÊâãÁ¨¨‰∏ÄÊ¨°Êìç‰ΩúÂª∫ËÆÆÂÖà‰∏çË¶ÅÊê≠Âª∫ÂºÄÂèëÁéØÂ¢ÉÔºåÁõ¥Êé•‰ΩøÁî®ÂÖçÂºÄÂèëÁéØÂ¢ÉÁÉßÂΩïÁöÑÂõ∫‰ª∂„ÄÇ&lt;/p&gt; 
&lt;p&gt;Âõ∫‰ª∂ÈªòËÆ§Êé•ÂÖ• &lt;a href="https://xiaozhi.me"&gt;xiaozhi.me&lt;/a&gt; ÂÆòÊñπÊúçÂä°Âô®Ôºå‰∏™‰∫∫Áî®Êà∑Ê≥®ÂÜåË¥¶Âè∑ÂèØ‰ª•ÂÖçË¥π‰ΩøÁî® Qwen ÂÆûÊó∂Ê®°Âûã„ÄÇ&lt;/p&gt; 
&lt;p&gt;üëâ &lt;a href="https://ccnphfhqs21z.feishu.cn/wiki/Zpz4wXBtdimBrLk25WdcXzxcnNS"&gt;Êñ∞ÊâãÁÉßÂΩïÂõ∫‰ª∂ÊïôÁ®ã&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;ÂºÄÂèëÁéØÂ¢É&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Cursor Êàñ VSCode&lt;/li&gt; 
 &lt;li&gt;ÂÆâË£Ö ESP-IDF Êèí‰ª∂ÔºåÈÄâÊã© SDK ÁâàÊú¨ 5.4 Êàñ‰ª•‰∏ä&lt;/li&gt; 
 &lt;li&gt;Linux ÊØî Windows Êõ¥Â•ΩÔºåÁºñËØëÈÄüÂ∫¶Âø´Ôºå‰πüÂÖçÂéªÈ©±Âä®ÈóÆÈ¢òÁöÑÂõ∞Êâ∞&lt;/li&gt; 
 &lt;li&gt;Êú¨È°πÁõÆ‰ΩøÁî® Google C++ ‰ª£Á†ÅÈ£éÊ†ºÔºåÊèê‰∫§‰ª£Á†ÅÊó∂ËØ∑Á°Æ‰øùÁ¨¶ÂêàËßÑËåÉ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ÂºÄÂèëËÄÖÊñáÊ°£&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/78/xiaozhi-esp32/main/docs/custom-board.md"&gt;Ëá™ÂÆö‰πâÂºÄÂèëÊùøÊåáÂçó&lt;/a&gt; - Â≠¶‰π†Â¶Ç‰Ωï‰∏∫Â∞èÊô∫ AI ÂàõÂª∫Ëá™ÂÆö‰πâÂºÄÂèëÊùø&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/78/xiaozhi-esp32/main/docs/mcp-usage.md"&gt;MCP ÂçèËÆÆÁâ©ËÅîÁΩëÊéßÂà∂Áî®Ê≥ïËØ¥Êòé&lt;/a&gt; - ‰∫ÜËß£Â¶Ç‰ΩïÈÄöËøá MCP ÂçèËÆÆÊéßÂà∂Áâ©ËÅîÁΩëËÆæÂ§á&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/78/xiaozhi-esp32/main/docs/mcp-protocol.md"&gt;MCP ÂçèËÆÆ‰∫§‰∫íÊµÅÁ®ã&lt;/a&gt; - ËÆæÂ§áÁ´Ø MCP ÂçèËÆÆÁöÑÂÆûÁé∞ÊñπÂºè&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/78/xiaozhi-esp32/main/docs/mqtt-udp.md"&gt;MQTT + UDP Ê∑∑ÂêàÈÄö‰ø°ÂçèËÆÆÊñáÊ°£&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/78/xiaozhi-esp32/main/docs/websocket.md"&gt;‰∏Ä‰ªΩËØ¶ÁªÜÁöÑ WebSocket ÈÄö‰ø°ÂçèËÆÆÊñáÊ°£&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Â§ßÊ®°ÂûãÈÖçÁΩÆ&lt;/h2&gt; 
&lt;p&gt;Â¶ÇÊûú‰Ω†Â∑≤ÁªèÊã•Êúâ‰∏Ä‰∏™Â∞èÊô∫ AI ËÅäÂ§©Êú∫Âô®‰∫∫ËÆæÂ§áÔºåÂπ∂‰∏îÂ∑≤Êé•ÂÖ•ÂÆòÊñπÊúçÂä°Âô®ÔºåÂèØ‰ª•ÁôªÂΩï &lt;a href="https://xiaozhi.me"&gt;xiaozhi.me&lt;/a&gt; ÊéßÂà∂Âè∞ËøõË°åÈÖçÁΩÆ„ÄÇ&lt;/p&gt; 
&lt;p&gt;üëâ &lt;a href="https://www.bilibili.com/video/BV1jUCUY2EKM/"&gt;ÂêéÂè∞Êìç‰ΩúËßÜÈ¢ëÊïôÁ®ãÔºàÊóßÁâàÁïåÈù¢Ôºâ&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Áõ∏ÂÖ≥ÂºÄÊ∫êÈ°πÁõÆ&lt;/h2&gt; 
&lt;p&gt;Âú®‰∏™‰∫∫ÁîµËÑë‰∏äÈÉ®ÁΩ≤ÊúçÂä°Âô®ÔºåÂèØ‰ª•ÂèÇËÄÉ‰ª•‰∏ãÁ¨¨‰∏âÊñπÂºÄÊ∫êÁöÑÈ°πÁõÆÔºö&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/xinnan-tech/xiaozhi-esp32-server"&gt;xinnan-tech/xiaozhi-esp32-server&lt;/a&gt; Python ÊúçÂä°Âô®&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/joey-zhou/xiaozhi-esp32-server-java"&gt;joey-zhou/xiaozhi-esp32-server-java&lt;/a&gt; Java ÊúçÂä°Âô®&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/AnimeAIChat/xiaozhi-server-go"&gt;AnimeAIChat/xiaozhi-server-go&lt;/a&gt; Golang ÊúçÂä°Âô®&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;‰ΩøÁî®Â∞èÊô∫ÈÄö‰ø°ÂçèËÆÆÁöÑÁ¨¨‰∏âÊñπÂÆ¢Êà∑Á´ØÈ°πÁõÆÔºö&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/huangjunsen0406/py-xiaozhi"&gt;huangjunsen0406/py-xiaozhi&lt;/a&gt; Python ÂÆ¢Êà∑Á´Ø&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/TOM88812/xiaozhi-android-client"&gt;TOM88812/xiaozhi-android-client&lt;/a&gt; Android ÂÆ¢Êà∑Á´Ø&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://github.com/100askTeam/xiaozhi-linux"&gt;100askTeam/xiaozhi-linux&lt;/a&gt; ÁôæÈóÆÁßëÊäÄÊèê‰æõÁöÑ Linux ÂÆ¢Êà∑Á´Ø&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/78/xiaozhi-sf32"&gt;78/xiaozhi-sf32&lt;/a&gt; ÊÄùÊæàÁßëÊäÄÁöÑËìùÁâôËäØÁâáÂõ∫‰ª∂&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/QuecPython/solution-xiaozhiAI"&gt;QuecPython/solution-xiaozhiAI&lt;/a&gt; ÁßªËøúÊèê‰æõÁöÑ QuecPython Âõ∫‰ª∂&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ÂÖ≥‰∫éÈ°πÁõÆ&lt;/h2&gt; 
&lt;p&gt;ËøôÊòØ‰∏Ä‰∏™Áî±ËôæÂì•ÂºÄÊ∫êÁöÑ ESP32 È°πÁõÆÔºå‰ª• MIT ËÆ∏ÂèØËØÅÂèëÂ∏ÉÔºåÂÖÅËÆ∏‰ªª‰Ωï‰∫∫ÂÖçË¥π‰ΩøÁî®Ôºå‰øÆÊîπÊàñÁî®‰∫éÂïÜ‰∏öÁî®ÈÄî„ÄÇ&lt;/p&gt; 
&lt;p&gt;Êàë‰ª¨Â∏åÊúõÈÄöËøáËøô‰∏™È°πÁõÆÔºåËÉΩÂ§üÂ∏ÆÂä©Â§ßÂÆ∂‰∫ÜËß£ AI Á°¨‰ª∂ÂºÄÂèëÔºåÂ∞ÜÂΩì‰∏ãÈ£ûÈÄüÂèëÂ±ïÁöÑÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂ∫îÁî®Âà∞ÂÆûÈôÖÁöÑÁ°¨‰ª∂ËÆæÂ§á‰∏≠„ÄÇ&lt;/p&gt; 
&lt;p&gt;Â¶ÇÊûú‰Ω†Êúâ‰ªª‰ΩïÊÉ≥Ê≥ïÊàñÂª∫ËÆÆÔºåËØ∑ÈöèÊó∂ÊèêÂá∫ Issues ÊàñÂä†ÂÖ• QQ Áæ§Ôºö1011329060&lt;/p&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;a href="https://star-history.com/#78/xiaozhi-esp32&amp;amp;Date"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=78/xiaozhi-esp32&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=78/xiaozhi-esp32&amp;amp;type=Date" /&gt; 
  &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=78/xiaozhi-esp32&amp;amp;type=Date" /&gt; 
 &lt;/picture&gt; &lt;/a&gt;</description>
    </item>
    
    <item>
      <title>MODSetter/SurfSense</title>
      <link>https://github.com/MODSetter/SurfSense</link>
      <description>&lt;p&gt;Open Source Alternative to NotebookLM / Perplexity, connected to external sources such as Search Engines, Slack, Linear, Jira, ClickUp, Confluence, Notion, YouTube, GitHub, Discord and more. Join our discord: https://discord.gg/ejRNvftDp9&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/e236b764-0ddc-42ff-a1f1-8fbb3d2e0e65" alt="new_header" /&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://discord.gg/ejRNvftDp9"&gt; &lt;img src="https://img.shields.io/discord/1359368468260192417" alt="Discord" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h1&gt;SurfSense&lt;/h1&gt; 
&lt;p&gt;While tools like NotebookLM and Perplexity are impressive and highly effective for conducting research on any topic/query, SurfSense elevates this capability by integrating with your personal knowledge base. It is a highly customizable AI research agent, connected to external sources such as Search Engines (SearxNG, Tavily, LinkUp), Slack, Linear, Jira, ClickUp, Confluence, Gmail, Notion, YouTube, GitHub, Discord, Airtable, Google Calendar, Luma and more to come.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://trendshift.io/repositories/13606" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/13606" alt="MODSetter%2FSurfSense | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;h1&gt;Video&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/d9221908-e0de-4b2f-ac3a-691cf4b202da"&gt;https://github.com/user-attachments/assets/d9221908-e0de-4b2f-ac3a-691cf4b202da&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Podcast Sample&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/a0a16566-6967-4374-ac51-9b3e07fbecd7"&gt;https://github.com/user-attachments/assets/a0a16566-6967-4374-ac51-9b3e07fbecd7&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;h3&gt;üí° &lt;strong&gt;Idea&lt;/strong&gt;:&lt;/h3&gt; 
&lt;p&gt;Have your own highly customizable private NotebookLM and Perplexity integrated with external sources.&lt;/p&gt; 
&lt;h3&gt;üìÅ &lt;strong&gt;Multiple File Format Uploading Support&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;Save content from your own personal files &lt;em&gt;(Documents, images, videos and supports &lt;strong&gt;50+ file extensions&lt;/strong&gt;)&lt;/em&gt; to your own personal knowledge base .&lt;/p&gt; 
&lt;h3&gt;üîç &lt;strong&gt;Powerful Search&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;Quickly research or find anything in your saved content .&lt;/p&gt; 
&lt;h3&gt;üí¨ &lt;strong&gt;Chat with your Saved Content&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;Interact in Natural Language and get cited answers.&lt;/p&gt; 
&lt;h3&gt;üìÑ &lt;strong&gt;Cited Answers&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;Get Cited answers just like Perplexity.&lt;/p&gt; 
&lt;h3&gt;üîî &lt;strong&gt;Privacy &amp;amp; Local LLM Support&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;Works Flawlessly with Ollama local LLMs.&lt;/p&gt; 
&lt;h3&gt;üè† &lt;strong&gt;Self Hostable&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;Open source and easy to deploy locally.&lt;/p&gt; 
&lt;h3&gt;üéôÔ∏è Podcasts&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Blazingly fast podcast generation agent. (Creates a 3-minute podcast in under 20 seconds.)&lt;/li&gt; 
 &lt;li&gt;Convert your chat conversations into engaging audio content&lt;/li&gt; 
 &lt;li&gt;Support for local TTS providers (Kokoro TTS)&lt;/li&gt; 
 &lt;li&gt;Support for multiple TTS providers (OpenAI, Azure, Google Vertex AI)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üìä &lt;strong&gt;Advanced RAG Techniques&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Supports 100+ LLM's&lt;/li&gt; 
 &lt;li&gt;Supports 6000+ Embedding Models.&lt;/li&gt; 
 &lt;li&gt;Supports all major Rerankers (Pinecode, Cohere, Flashrank etc)&lt;/li&gt; 
 &lt;li&gt;Uses Hierarchical Indices (2 tiered RAG setup).&lt;/li&gt; 
 &lt;li&gt;Utilizes Hybrid Search (Semantic + Full Text Search combined with Reciprocal Rank Fusion).&lt;/li&gt; 
 &lt;li&gt;RAG as a Service API Backend.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;‚ÑπÔ∏è &lt;strong&gt;External Sources&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Search Engines (Tavily, LinkUp)&lt;/li&gt; 
 &lt;li&gt;SearxNG (self-hosted instances)&lt;/li&gt; 
 &lt;li&gt;Slack&lt;/li&gt; 
 &lt;li&gt;Linear&lt;/li&gt; 
 &lt;li&gt;Jira&lt;/li&gt; 
 &lt;li&gt;ClickUp&lt;/li&gt; 
 &lt;li&gt;Confluence&lt;/li&gt; 
 &lt;li&gt;Notion&lt;/li&gt; 
 &lt;li&gt;Gmail&lt;/li&gt; 
 &lt;li&gt;Youtube Videos&lt;/li&gt; 
 &lt;li&gt;GitHub&lt;/li&gt; 
 &lt;li&gt;Discord&lt;/li&gt; 
 &lt;li&gt;Airtable&lt;/li&gt; 
 &lt;li&gt;Google Calendar&lt;/li&gt; 
 &lt;li&gt;Luma&lt;/li&gt; 
 &lt;li&gt;and more to come.....&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üìÑ &lt;strong&gt;Supported File Extensions&lt;/strong&gt;&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: File format support depends on your ETL service configuration. LlamaCloud supports 50+ formats, Unstructured supports 34+ core formats, and Docling (core formats, local processing, privacy-focused, no API key).&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Documents &amp;amp; Text&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;LlamaCloud&lt;/strong&gt;: &lt;code&gt;.pdf&lt;/code&gt;, &lt;code&gt;.doc&lt;/code&gt;, &lt;code&gt;.docx&lt;/code&gt;, &lt;code&gt;.docm&lt;/code&gt;, &lt;code&gt;.dot&lt;/code&gt;, &lt;code&gt;.dotm&lt;/code&gt;, &lt;code&gt;.rtf&lt;/code&gt;, &lt;code&gt;.txt&lt;/code&gt;, &lt;code&gt;.xml&lt;/code&gt;, &lt;code&gt;.epub&lt;/code&gt;, &lt;code&gt;.odt&lt;/code&gt;, &lt;code&gt;.wpd&lt;/code&gt;, &lt;code&gt;.pages&lt;/code&gt;, &lt;code&gt;.key&lt;/code&gt;, &lt;code&gt;.numbers&lt;/code&gt;, &lt;code&gt;.602&lt;/code&gt;, &lt;code&gt;.abw&lt;/code&gt;, &lt;code&gt;.cgm&lt;/code&gt;, &lt;code&gt;.cwk&lt;/code&gt;, &lt;code&gt;.hwp&lt;/code&gt;, &lt;code&gt;.lwp&lt;/code&gt;, &lt;code&gt;.mw&lt;/code&gt;, &lt;code&gt;.mcw&lt;/code&gt;, &lt;code&gt;.pbd&lt;/code&gt;, &lt;code&gt;.sda&lt;/code&gt;, &lt;code&gt;.sdd&lt;/code&gt;, &lt;code&gt;.sdp&lt;/code&gt;, &lt;code&gt;.sdw&lt;/code&gt;, &lt;code&gt;.sgl&lt;/code&gt;, &lt;code&gt;.sti&lt;/code&gt;, &lt;code&gt;.sxi&lt;/code&gt;, &lt;code&gt;.sxw&lt;/code&gt;, &lt;code&gt;.stw&lt;/code&gt;, &lt;code&gt;.sxg&lt;/code&gt;, &lt;code&gt;.uof&lt;/code&gt;, &lt;code&gt;.uop&lt;/code&gt;, &lt;code&gt;.uot&lt;/code&gt;, &lt;code&gt;.vor&lt;/code&gt;, &lt;code&gt;.wps&lt;/code&gt;, &lt;code&gt;.zabw&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Unstructured&lt;/strong&gt;: &lt;code&gt;.doc&lt;/code&gt;, &lt;code&gt;.docx&lt;/code&gt;, &lt;code&gt;.odt&lt;/code&gt;, &lt;code&gt;.rtf&lt;/code&gt;, &lt;code&gt;.pdf&lt;/code&gt;, &lt;code&gt;.xml&lt;/code&gt;, &lt;code&gt;.txt&lt;/code&gt;, &lt;code&gt;.md&lt;/code&gt;, &lt;code&gt;.markdown&lt;/code&gt;, &lt;code&gt;.rst&lt;/code&gt;, &lt;code&gt;.html&lt;/code&gt;, &lt;code&gt;.org&lt;/code&gt;, &lt;code&gt;.epub&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Docling&lt;/strong&gt;: &lt;code&gt;.pdf&lt;/code&gt;, &lt;code&gt;.docx&lt;/code&gt;, &lt;code&gt;.html&lt;/code&gt;, &lt;code&gt;.htm&lt;/code&gt;, &lt;code&gt;.xhtml&lt;/code&gt;, &lt;code&gt;.adoc&lt;/code&gt;, &lt;code&gt;.asciidoc&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Presentations&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;LlamaCloud&lt;/strong&gt;: &lt;code&gt;.ppt&lt;/code&gt;, &lt;code&gt;.pptx&lt;/code&gt;, &lt;code&gt;.pptm&lt;/code&gt;, &lt;code&gt;.pot&lt;/code&gt;, &lt;code&gt;.potm&lt;/code&gt;, &lt;code&gt;.potx&lt;/code&gt;, &lt;code&gt;.odp&lt;/code&gt;, &lt;code&gt;.key&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Unstructured&lt;/strong&gt;: &lt;code&gt;.ppt&lt;/code&gt;, &lt;code&gt;.pptx&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Docling&lt;/strong&gt;: &lt;code&gt;.pptx&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Spreadsheets &amp;amp; Data&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;LlamaCloud&lt;/strong&gt;: &lt;code&gt;.xlsx&lt;/code&gt;, &lt;code&gt;.xls&lt;/code&gt;, &lt;code&gt;.xlsm&lt;/code&gt;, &lt;code&gt;.xlsb&lt;/code&gt;, &lt;code&gt;.xlw&lt;/code&gt;, &lt;code&gt;.csv&lt;/code&gt;, &lt;code&gt;.tsv&lt;/code&gt;, &lt;code&gt;.ods&lt;/code&gt;, &lt;code&gt;.fods&lt;/code&gt;, &lt;code&gt;.numbers&lt;/code&gt;, &lt;code&gt;.dbf&lt;/code&gt;, &lt;code&gt;.123&lt;/code&gt;, &lt;code&gt;.dif&lt;/code&gt;, &lt;code&gt;.sylk&lt;/code&gt;, &lt;code&gt;.slk&lt;/code&gt;, &lt;code&gt;.prn&lt;/code&gt;, &lt;code&gt;.et&lt;/code&gt;, &lt;code&gt;.uos1&lt;/code&gt;, &lt;code&gt;.uos2&lt;/code&gt;, &lt;code&gt;.wk1&lt;/code&gt;, &lt;code&gt;.wk2&lt;/code&gt;, &lt;code&gt;.wk3&lt;/code&gt;, &lt;code&gt;.wk4&lt;/code&gt;, &lt;code&gt;.wks&lt;/code&gt;, &lt;code&gt;.wq1&lt;/code&gt;, &lt;code&gt;.wq2&lt;/code&gt;, &lt;code&gt;.wb1&lt;/code&gt;, &lt;code&gt;.wb2&lt;/code&gt;, &lt;code&gt;.wb3&lt;/code&gt;, &lt;code&gt;.qpw&lt;/code&gt;, &lt;code&gt;.xlr&lt;/code&gt;, &lt;code&gt;.eth&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Unstructured&lt;/strong&gt;: &lt;code&gt;.xls&lt;/code&gt;, &lt;code&gt;.xlsx&lt;/code&gt;, &lt;code&gt;.csv&lt;/code&gt;, &lt;code&gt;.tsv&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Docling&lt;/strong&gt;: &lt;code&gt;.xlsx&lt;/code&gt;, &lt;code&gt;.csv&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Images&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;LlamaCloud&lt;/strong&gt;: &lt;code&gt;.jpg&lt;/code&gt;, &lt;code&gt;.jpeg&lt;/code&gt;, &lt;code&gt;.png&lt;/code&gt;, &lt;code&gt;.gif&lt;/code&gt;, &lt;code&gt;.bmp&lt;/code&gt;, &lt;code&gt;.svg&lt;/code&gt;, &lt;code&gt;.tiff&lt;/code&gt;, &lt;code&gt;.webp&lt;/code&gt;, &lt;code&gt;.html&lt;/code&gt;, &lt;code&gt;.htm&lt;/code&gt;, &lt;code&gt;.web&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Unstructured&lt;/strong&gt;: &lt;code&gt;.jpg&lt;/code&gt;, &lt;code&gt;.jpeg&lt;/code&gt;, &lt;code&gt;.png&lt;/code&gt;, &lt;code&gt;.bmp&lt;/code&gt;, &lt;code&gt;.tiff&lt;/code&gt;, &lt;code&gt;.heic&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Docling&lt;/strong&gt;: &lt;code&gt;.jpg&lt;/code&gt;, &lt;code&gt;.jpeg&lt;/code&gt;, &lt;code&gt;.png&lt;/code&gt;, &lt;code&gt;.bmp&lt;/code&gt;, &lt;code&gt;.tiff&lt;/code&gt;, &lt;code&gt;.tif&lt;/code&gt;, &lt;code&gt;.webp&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Audio &amp;amp; Video &lt;em&gt;(Always Supported)&lt;/em&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;.mp3&lt;/code&gt;, &lt;code&gt;.mpga&lt;/code&gt;, &lt;code&gt;.m4a&lt;/code&gt;, &lt;code&gt;.wav&lt;/code&gt;, &lt;code&gt;.mp4&lt;/code&gt;, &lt;code&gt;.mpeg&lt;/code&gt;, &lt;code&gt;.webm&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Email &amp;amp; Communication&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Unstructured&lt;/strong&gt;: &lt;code&gt;.eml&lt;/code&gt;, &lt;code&gt;.msg&lt;/code&gt;, &lt;code&gt;.p7s&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;üîñ Cross Browser Extension&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;The SurfSense extension can be used to save any webpage you like.&lt;/li&gt; 
 &lt;li&gt;Its main usecase is to save any webpages protected beyond authentication.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;FEATURE REQUESTS AND FUTURE&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;SurfSense is actively being developed.&lt;/strong&gt; While it's not yet production-ready, you can help us speed up the process.&lt;/p&gt; 
&lt;p&gt;Join the &lt;a href="https://discord.gg/ejRNvftDp9"&gt;SurfSense Discord&lt;/a&gt; and help shape the future of SurfSense!&lt;/p&gt; 
&lt;h2&gt;üöÄ Roadmap&lt;/h2&gt; 
&lt;p&gt;Stay up to date with our development progress and upcoming features!&lt;br /&gt; Check out our public roadmap and contribute your ideas or feedback:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;View the Roadmap:&lt;/strong&gt; &lt;a href="https://github.com/users/MODSetter/projects/2"&gt;SurfSense Roadmap on GitHub Projects&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;How to get started?&lt;/h2&gt; 
&lt;h3&gt;Installation Options&lt;/h3&gt; 
&lt;p&gt;SurfSense provides two installation methods:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://www.surfsense.net/docs/docker-installation"&gt;Docker Installation&lt;/a&gt;&lt;/strong&gt; - The easiest way to get SurfSense up and running with all dependencies containerized.&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Includes pgAdmin for database management through a web UI&lt;/li&gt; 
   &lt;li&gt;Supports environment variable customization via &lt;code&gt;.env&lt;/code&gt; file&lt;/li&gt; 
   &lt;li&gt;Flexible deployment options (full stack or core services only)&lt;/li&gt; 
   &lt;li&gt;No need to manually edit configuration files between environments&lt;/li&gt; 
   &lt;li&gt;See &lt;a href="https://raw.githubusercontent.com/MODSetter/SurfSense/main/DOCKER_SETUP.md"&gt;Docker Setup Guide&lt;/a&gt; for detailed instructions&lt;/li&gt; 
   &lt;li&gt;For deployment scenarios and options, see &lt;a href="https://raw.githubusercontent.com/MODSetter/SurfSense/main/DEPLOYMENT_GUIDE.md"&gt;Deployment Guide&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://www.surfsense.net/docs/manual-installation"&gt;Manual Installation (Recommended)&lt;/a&gt;&lt;/strong&gt; - For users who prefer more control over their setup or need to customize their deployment.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Both installation guides include detailed OS-specific instructions for Windows, macOS, and Linux.&lt;/p&gt; 
&lt;p&gt;Before installation, make sure to complete the &lt;a href="https://www.surfsense.net/docs/"&gt;prerequisite setup steps&lt;/a&gt; including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;PGVector setup&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;File Processing ETL Service&lt;/strong&gt; (choose one): 
  &lt;ul&gt; 
   &lt;li&gt;Unstructured.io API key (supports 34+ formats)&lt;/li&gt; 
   &lt;li&gt;LlamaIndex API key (enhanced parsing, supports 50+ formats)&lt;/li&gt; 
   &lt;li&gt;Docling (local processing, no API key required, supports PDF, Office docs, images, HTML, CSV)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Other required API keys&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Screenshots&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Research Agent&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/e22c5d86-f511-4c72-8c50-feba0c1561b4" alt="updated_researcher" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Search Spaces&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/e254c38c-f937-44b6-9e9d-770db583d099" alt="search_spaces" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Manage Documents&lt;/strong&gt; &lt;img src="https://github.com/user-attachments/assets/7001e306-eb06-4009-89c6-8fadfdc3fc4d" alt="documents" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Podcast Agent&lt;/strong&gt; &lt;img src="https://github.com/user-attachments/assets/6cb82ffd-9e14-4172-bc79-67faf34c4c1c" alt="podcasts" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Agent Chat&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/bb352d52-1c6d-4020-926b-722d0b98b491" alt="git_chat" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Browser Extension&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/1f042b7a-6349-422b-94fb-d40d0df16c40" alt="ext1" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/a9b9f1aa-2677-404d-b0a0-c1b2dddf24a7" alt="ext2" /&gt;&lt;/p&gt; 
&lt;h2&gt;Tech Stack&lt;/h2&gt; 
&lt;h3&gt;&lt;strong&gt;BackEnd&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;FastAPI&lt;/strong&gt;: Modern, fast web framework for building APIs with Python&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;PostgreSQL with pgvector&lt;/strong&gt;: Database with vector search capabilities for similarity searches&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;SQLAlchemy&lt;/strong&gt;: SQL toolkit and ORM (Object-Relational Mapping) for database interactions&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Alembic&lt;/strong&gt;: A database migrations tool for SQLAlchemy.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;FastAPI Users&lt;/strong&gt;: Authentication and user management with JWT and OAuth support&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;LangGraph&lt;/strong&gt;: Framework for developing AI-agents.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;LangChain&lt;/strong&gt;: Framework for developing AI-powered applications.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;LLM Integration&lt;/strong&gt;: Integration with LLM models through LiteLLM&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Rerankers&lt;/strong&gt;: Advanced result ranking for improved search relevance&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Hybrid Search&lt;/strong&gt;: Combines vector similarity and full-text search for optimal results using Reciprocal Rank Fusion (RRF)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Vector Embeddings&lt;/strong&gt;: Document and text embeddings for semantic search&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;pgvector&lt;/strong&gt;: PostgreSQL extension for efficient vector similarity operations&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Chonkie&lt;/strong&gt;: Advanced document chunking and embedding library&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Uses &lt;code&gt;AutoEmbeddings&lt;/code&gt; for flexible embedding model selection&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;LateChunker&lt;/code&gt; for optimized document chunking based on embedding model's max sequence length&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h3&gt;&lt;strong&gt;FrontEnd&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Next.js 15.2.3&lt;/strong&gt;: React framework featuring App Router, server components, automatic code-splitting, and optimized rendering.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;React 19.0.0&lt;/strong&gt;: JavaScript library for building user interfaces.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;TypeScript&lt;/strong&gt;: Static type-checking for JavaScript, enhancing code quality and developer experience.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Vercel AI SDK Kit UI Stream Protocol&lt;/strong&gt;: To create scalable chat UI.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Tailwind CSS 4.x&lt;/strong&gt;: Utility-first CSS framework for building custom UI designs.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Shadcn&lt;/strong&gt;: Headless components library.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Lucide React&lt;/strong&gt;: Icon set implemented as React components.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Framer Motion&lt;/strong&gt;: Animation library for React.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Sonner&lt;/strong&gt;: Toast notification library.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Geist&lt;/strong&gt;: Font family from Vercel.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;React Hook Form&lt;/strong&gt;: Form state management and validation.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Zod&lt;/strong&gt;: TypeScript-first schema validation with static type inference.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;@hookform/resolvers&lt;/strong&gt;: Resolvers for using validation libraries with React Hook Form.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;@tanstack/react-table&lt;/strong&gt;: Headless UI for building powerful tables &amp;amp; datagrids.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;strong&gt;DevOps&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Docker&lt;/strong&gt;: Container platform for consistent deployment across environments&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Docker Compose&lt;/strong&gt;: Tool for defining and running multi-container Docker applications&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;pgAdmin&lt;/strong&gt;: Web-based PostgreSQL administration tool included in Docker setup&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;strong&gt;Extension&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;Manifest v3 on Plasmo&lt;/p&gt; 
&lt;h2&gt;Future Work&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Add More Connectors.&lt;/li&gt; 
 &lt;li&gt;Patch minor bugs.&lt;/li&gt; 
 &lt;li&gt;Document Podcasts&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contribute&lt;/h2&gt; 
&lt;p&gt;Contributions are very welcome! A contribution can be as small as a ‚≠ê or even finding and creating issues. Fine-tuning the Backend is always desired.&lt;/p&gt; 
&lt;p&gt;For detailed contribution guidelines, please see our &lt;a href="https://raw.githubusercontent.com/MODSetter/SurfSense/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; file.&lt;/p&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;a href="https://www.star-history.com/#MODSetter/SurfSense&amp;amp;Date"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=MODSetter/SurfSense&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=MODSetter/SurfSense&amp;amp;type=Date" /&gt; 
  &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=MODSetter/SurfSense&amp;amp;type=Date" /&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;hr /&gt; 
&lt;hr /&gt; 
&lt;p align="center"&gt; &lt;img src="https://github.com/user-attachments/assets/329c9bc2-6005-4aed-a629-700b5ae296b4" alt="Catalyst Project" width="200" /&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;hr /&gt;</description>
    </item>
    
    <item>
      <title>anthropics/claude-code</title>
      <link>https://github.com/anthropics/claude-code</link>
      <description>&lt;p&gt;Claude Code is an agentic coding tool that lives in your terminal, understands your codebase, and helps you code faster by executing routine tasks, explaining complex code, and handling git workflows - all through natural language commands.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Claude Code&lt;/h1&gt; 
&lt;p&gt;&lt;img src="https://img.shields.io/badge/Node.js-18%2B-brightgreen?style=flat-square" alt="" /&gt; &lt;a href="https://www.npmjs.com/package/@anthropic-ai/claude-code"&gt;&lt;img src="https://img.shields.io/npm/v/@anthropic-ai/claude-code.svg?style=flat-square" alt="npm" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Claude Code is an agentic coding tool that lives in your terminal, understands your codebase, and helps you code faster by executing routine tasks, explaining complex code, and handling git workflows -- all through natural language commands. Use it in your terminal, IDE, or tag @claude on Github.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Learn more in the &lt;a href="https://docs.anthropic.com/en/docs/claude-code/overview"&gt;official documentation&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/anthropics/claude-code/main/demo.gif" /&gt; 
&lt;h2&gt;Get started&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install Claude Code:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;npm install -g @anthropic-ai/claude-code
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Navigate to your project directory and run &lt;code&gt;claude&lt;/code&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Reporting Bugs&lt;/h2&gt; 
&lt;p&gt;We welcome your feedback. Use the &lt;code&gt;/bug&lt;/code&gt; command to report issues directly within Claude Code, or file a &lt;a href="https://github.com/anthropics/claude-code/issues"&gt;GitHub issue&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Connect on Discord&lt;/h2&gt; 
&lt;p&gt;Join the &lt;a href="https://anthropic.com/discord"&gt;Claude Developers Discord&lt;/a&gt; to connect with other developers using Claude Code. Get help, share feedback, and discuss your projects with the community.&lt;/p&gt; 
&lt;h2&gt;Data collection, usage, and retention&lt;/h2&gt; 
&lt;p&gt;When you use Claude Code, we collect feedback, which includes usage data (such as code acceptance or rejections), associated conversation data, and user feedback submitted via the &lt;code&gt;/bug&lt;/code&gt; command.&lt;/p&gt; 
&lt;h3&gt;How we use your data&lt;/h3&gt; 
&lt;p&gt;See our &lt;a href="https://docs.anthropic.com/en/docs/claude-code/data-usage"&gt;data usage policies&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Privacy safeguards&lt;/h3&gt; 
&lt;p&gt;We have implemented several safeguards to protect your data, including limited retention periods for sensitive information, restricted access to user session data, and clear policies against using feedback for model training.&lt;/p&gt; 
&lt;p&gt;For full details, please review our &lt;a href="https://www.anthropic.com/legal/commercial-terms"&gt;Commercial Terms of Service&lt;/a&gt; and &lt;a href="https://www.anthropic.com/legal/privacy"&gt;Privacy Policy&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>dair-ai/Prompt-Engineering-Guide</title>
      <link>https://github.com/dair-ai/Prompt-Engineering-Guide</link>
      <description>&lt;p&gt;üêô Guides, papers, lecture, notebooks and resources for prompt engineering&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Prompt Engineering Guide&lt;/h1&gt; 
&lt;h5 align="center"&gt; Sponsored by&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://serpapi.com/"&gt;&lt;img src="https://cdn.rawgit.com/standard/standard/master/docs/logos/serpapi.png" height="35" valign="middle" /&gt;&lt;/a&gt; &lt;/h5&gt; 
&lt;p&gt;Prompt engineering is a relatively new discipline for developing and optimizing prompts to efficiently use language models (LMs) for a wide variety of applications and research topics. Prompt engineering skills help to better understand the capabilities and limitations of large language models (LLMs). Researchers use prompt engineering to improve the capacity of LLMs on a wide range of common and complex tasks such as question answering and arithmetic reasoning. Developers use prompt engineering to design robust and effective prompting techniques that interface with LLMs and other tools.&lt;/p&gt; 
&lt;p&gt;Motivated by the high interest in developing with LLMs, we have created this new prompt engineering guide that contains all the latest papers, learning guides, lectures, references, and tools related to prompt engineering for LLMs.&lt;/p&gt; 
&lt;p&gt;üåê &lt;a href="https://www.promptingguide.ai/"&gt;Prompt Engineering Guide (Web Version)&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;üéâ We are excited to launch our new prompt engineering, RAG, and AI Agents courses under the DAIR.AI Academy. &lt;a href="https://dair-ai.thinkific.com/bundles/pro"&gt;Join Now&lt;/a&gt;!&lt;/p&gt; 
&lt;p&gt;The courses are meant to compliment this guide and provide a more hands-on approach to learning about prompt engineering, context engineering, and AI Agents.&lt;/p&gt; 
&lt;p&gt;Use code PROMPTING20 to get an extra 20% off.&lt;/p&gt; 
&lt;p&gt;Happy Prompting!&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Announcements / Updates&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;üéì We now offer self-paced prompt engineering courses under our DAIR.AI Academy. &lt;a href="https://dair-ai.thinkific.com/bundles/pro"&gt;Join Now&lt;/a&gt;!&lt;/li&gt; 
 &lt;li&gt;üéì New course on Prompt Engineering for LLMs announced! &lt;a href="https://maven.com/dair-ai/prompt-engineering-llms"&gt;Enroll here&lt;/a&gt;!&lt;/li&gt; 
 &lt;li&gt;üíº We now offer several &lt;a href="https://www.promptingguide.ai/services"&gt;services&lt;/a&gt; like corporate training, consulting, and talks.&lt;/li&gt; 
 &lt;li&gt;üåê We now support 13 languages! Welcoming more translations.&lt;/li&gt; 
 &lt;li&gt;üë©‚Äçüéì We crossed 3 million learners in January 2024!&lt;/li&gt; 
 &lt;li&gt;üéâ We have launched a new web version of the guide &lt;a href="https://www.promptingguide.ai/"&gt;here&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üî• We reached #1 on Hacker News on 21 Feb 2023&lt;/li&gt; 
 &lt;li&gt;üéâ The First Prompt Engineering Lecture went live &lt;a href="https://youtu.be/dOxUroR57xs"&gt;here&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://discord.com/invite/SKgkVT8BGJ"&gt;Join our Discord&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://twitter.com/dair_ai"&gt;Follow us on Twitter&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/channel/UCyna_OxOWL7IEuOwb7WhmxQ"&gt;Subscribe to our YouTube&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://nlpnews.substack.com/"&gt;Subscribe to our Newsletter&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Guides&lt;/h2&gt; 
&lt;p&gt;You can also find the most up-to-date guides on our new website &lt;a href="https://www.promptingguide.ai/"&gt;https://www.promptingguide.ai/&lt;/a&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.promptingguide.ai/introduction"&gt;Prompt Engineering - Introduction&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://www.promptingguide.ai/introduction/settings"&gt;Prompt Engineering - LLM Settings&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.promptingguide.ai/introduction/basics"&gt;Prompt Engineering - Basics of Prompting&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.promptingguide.ai/introduction/elements"&gt;Prompt Engineering - Prompt Elements&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.promptingguide.ai/introduction/tips"&gt;Prompt Engineering - General Tips for Designing Prompts&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.promptingguide.ai/introduction/examples"&gt;Prompt Engineering - Examples of Prompts&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.promptingguide.ai/techniques"&gt;Prompt Engineering - Techniques&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://www.promptingguide.ai/techniques/zeroshot"&gt;Prompt Engineering - Zero-Shot Prompting&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.promptingguide.ai/techniques/fewshot"&gt;Prompt Engineering - Few-Shot Prompting&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.promptingguide.ai/techniques/cot"&gt;Prompt Engineering - Chain-of-Thought Prompting&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.promptingguide.ai/techniques/consistency"&gt;Prompt Engineering - Self-Consistency&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.promptingguide.ai/techniques/knowledge"&gt;Prompt Engineering - Generate Knowledge Prompting&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.promptingguide.ai/techniques/prompt_chaining"&gt;Prompt Engineering - Prompt Chaining&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.promptingguide.ai/techniques/tot"&gt;Prompt Engineering - Tree of Thoughts (ToT)&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.promptingguide.ai/techniques/rag"&gt;Prompt Engineering - Retrieval Augmented Generation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.promptingguide.ai/techniques/art"&gt;Prompt Engineering - Automatic Reasoning and Tool-use (ART)&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.promptingguide.ai/techniques/ape"&gt;Prompt Engineering - Automatic Prompt Engineer&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.promptingguide.ai/techniques/activeprompt"&gt;Prompt Engineering - Active-Prompt&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.promptingguide.ai/techniques/dsp"&gt;Prompt Engineering - Directional Stimulus Prompting&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.promptingguide.ai/techniques/pal"&gt;Prompt Engineering - Program-Aided Language Models&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.promptingguide.ai/techniques/react"&gt;Prompt Engineering - ReAct Prompting&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.promptingguide.ai/techniques/multimodalcot"&gt;Prompt Engineering - Multimodal CoT Prompting&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.promptingguide.ai/techniques/graph"&gt;Prompt Engineering - Graph Prompting&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.promptingguide.ai/applications"&gt;Prompt Engineering - Applications&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://www.promptingguide.ai/applications/function_calling"&gt;Prompt Engineering - Function Calling&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.promptingguide.ai/applications/generating"&gt;Prompt Engineering - Generating Data&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.promptingguide.ai/applications/synthetic_rag"&gt;Prompt Engineering - Generating Synthetic Dataset for RAG&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.promptingguide.ai/applications/generating_textbooks"&gt;Prompt Engineering - Takling Generated Datasets Diversity&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.promptingguide.ai/applications/coding"&gt;Prompt Engineering - Generating Code&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.promptingguide.ai/applications/workplace_casestudy"&gt;Prompt Engineering - Graduate Job Classification Case Study&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.promptingguide.ai/prompts"&gt;Prompt Engineering - Prompt Hub&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://www.promptingguide.ai/prompts/classification"&gt;Prompt Engineering - Classification&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.promptingguide.ai/prompts/coding"&gt;Prompt Engineering - Coding&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.promptingguide.ai/prompts/creativity"&gt;Prompt Engineering - Creativity&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.promptingguide.ai/prompts/evaluation"&gt;Prompt Engineering - Evaluation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.promptingguide.ai/prompts/information-extraction"&gt;Prompt Engineering - Information Extraction&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.promptingguide.ai/prompts/image-generation"&gt;Prompt Engineering - Image Generation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.promptingguide.ai/prompts/mathematics"&gt;Prompt Engineering - Mathematics&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.promptingguide.ai/prompts/question-answering"&gt;Prompt Engineering - Question Answering&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.promptingguide.ai/prompts/reasoning"&gt;Prompt Engineering - Reasoning&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.promptingguide.ai/prompts/text-summarization"&gt;Prompt Engineering - Text Summarization&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.promptingguide.ai/prompts/truthfulness"&gt;Prompt Engineering - Truthfulness&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.promptingguide.ai/prompts/adversarial-prompting"&gt;Prompt Engineering - Adversarial Prompting&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.promptingguide.ai/models"&gt;Prompt Engineering - Models&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://www.promptingguide.ai/models/chatgpt"&gt;Prompt Engineering - ChatGPT&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.promptingguide.ai/models/code-llama"&gt;Prompt Engineering - Code Llama&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.promptingguide.ai/models/flan"&gt;Prompt Engineering - Flan&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.promptingguide.ai/models/gemini"&gt;Prompt Engineering - Gemini&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.promptingguide.ai/models/gpt-4"&gt;Prompt Engineering - GPT-4&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.promptingguide.ai/models/llama"&gt;Prompt Engineering - LLaMA&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.promptingguide.ai/models/mistral-7b"&gt;Prompt Engineering - Mistral 7B&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.promptingguide.ai/models/mixtral"&gt;Prompt Engineering - Mixtral&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.promptingguide.ai/models/olmo"&gt;Prompt Engineering - OLMo&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.promptingguide.ai/models/phi-2"&gt;Prompt Engineering - Phi-2&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.promptingguide.ai/models/collection"&gt;Prompt Engineering - Model Collection&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.promptingguide.ai/risks"&gt;Prompt Engineering - Risks and Misuses&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://www.promptingguide.ai/risks/adversarial"&gt;Prompt Engineering - Adversarial Prompting&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.promptingguide.ai/risks/factuality"&gt;Prompt Engineering - Factuality&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.promptingguide.ai/risks/biases"&gt;Prompt Engineering - Biases&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.promptingguide.ai/papers"&gt;Prompt Engineering - Papers&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://www.promptingguide.ai/papers#overviews"&gt;Prompt Engineering - Overviews&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.promptingguide.ai/papers#approaches"&gt;Prompt Engineering - Approaches&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.promptingguide.ai/papers#applications"&gt;Prompt Engineering - Applications&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.promptingguide.ai/papers#collections"&gt;Prompt Engineering - Collections&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.promptingguide.ai/tools"&gt;Prompt Engineering - Tools&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.promptingguide.ai/notebooks"&gt;Prompt Engineering - Notebooks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.promptingguide.ai/datasets"&gt;Prompt Engineering - Datasets&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.promptingguide.ai/readings"&gt;Prompt Engineering - Additional Readings&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Lecture&lt;/h2&gt; 
&lt;p&gt;We have published a 1 hour lecture that provides a comprehensive overview of prompting techniques, applications, and tools.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://youtu.be/dOxUroR57xs"&gt;Video Lecture&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/dair-ai/Prompt-Engineering-Guide/raw/main/notebooks/pe-lecture.ipynb"&gt;Notebook with code&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/dair-ai/Prompt-Engineering-Guide/raw/main/lecture/Prompt-Engineering-Lecture-Elvis.pdf"&gt;Slides&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Running the guide locally&lt;/h2&gt; 
&lt;p&gt;To run the guide locally, for example to check the correct implementation of a new translation, you will need to:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install Node &amp;gt;=18.0.0&lt;/li&gt; 
 &lt;li&gt;Install &lt;code&gt;pnpm&lt;/code&gt; if not present in your system. Check &lt;a href="https://pnpm.io/installation"&gt;here&lt;/a&gt; for detailed instructions.&lt;/li&gt; 
 &lt;li&gt;Install the dependencies: &lt;code&gt;pnpm i next react react-dom nextra nextra-theme-docs&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Boot the guide with &lt;code&gt;pnpm dev&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Browse the guide at &lt;code&gt;http://localhost:3000/&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Appearances&lt;/h2&gt; 
&lt;p&gt;Some places where we have been featured:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Wall Street Journal - &lt;a href="https://www.wsj.com/articles/chatgpt-ask-the-right-question-12d0f035"&gt;ChatGPT Can Give Great Answers. But Only If You Know How to Ask the Right Question&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Forbes - &lt;a href="https://www.forbes.com/sites/craigsmith/2023/04/05/mom-dad-i-want-to-be-a-prompt-engineer/?sh=7f1213159c8e"&gt;Mom, Dad, I Want To Be A Prompt Engineer&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Markettechpost - &lt;a href="https://www.marktechpost.com/2023/04/04/best-free-prompt-engineering-resources-2023/"&gt;Best Free Prompt Engineering Resources (2023)&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;p&gt;If you are using the guide for your work or research, please cite us as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@article{Saravia_Prompt_Engineering_Guide_2022,
author = {Saravia, Elvis},
journal = {https://github.com/dair-ai/Prompt-Engineering-Guide},
month = {12},
title = {{Prompt Engineering Guide}},
year = {2022}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/dair-ai/Prompt-Engineering-Guide/raw/main/LICENSE.md"&gt;MIT License&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Feel free to open a PR if you think something is missing here. Always welcome feedback and suggestions. Just open an issue!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>TapXWorld/ChinaTextbook</title>
      <link>https://github.com/TapXWorld/ChinaTextbook</link>
      <description>&lt;p&gt;ÊâÄÊúâÂ∞èÂàùÈ´ò„ÄÅÂ§ßÂ≠¶PDFÊïôÊùê„ÄÇ&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;È°πÁõÆÁöÑÁî±Êù•&lt;/h2&gt; 
&lt;p&gt;ËôΩÁÑ∂ÂõΩÂÜÖÊïôËÇ≤ÁΩëÁ´ôÂ∑≤Êèê‰æõÂÖçË¥πËµÑÊ∫êÔºå‰ΩÜÂ§ßÂ§öÊï∞ÊôÆÈÄö‰∫∫Ëé∑Âèñ‰ø°ÊÅØÁöÑÈÄîÂæÑ‰æùÁÑ∂ÂèóÈôê„ÄÇÊúâ‰∫õ‰∫∫Âà©Áî®Ëøô‰∏ÄÁÇπÔºåÂú®ÊüêÁ´ô‰∏äÈîÄÂîÆËøô‰∫õÂ∏¶ÊúâÁßÅ‰∫∫Ê∞¥Âç∞ÁöÑËµÑÊ∫ê„ÄÇ‰∏∫‰∫ÜÂ∫îÂØπËøôÁßçÊÉÖÂÜµÔºåÊàëËÆ°ÂàíÂ∞ÜËøô‰∫õËµÑÊ∫êÈõÜ‰∏≠Âπ∂ÂºÄÊ∫êÔºå‰ª•‰øÉËøõ‰πâÂä°ÊïôËÇ≤ÁöÑÊôÆÂèäÂíåÊ∂àÈô§Âú∞Âå∫Èó¥ÁöÑÊïôËÇ≤Ë¥´Âõ∞„ÄÇ&lt;/p&gt; 
&lt;p&gt;ËøòÊúâ‰∏Ä‰∏™ÊúÄÈáçË¶ÅÁöÑÂéüÂõ†ÊòØÔºåÂ∏åÊúõÊµ∑Â§ñÂçé‰∫∫ËÉΩÂ§üËÆ©Ëá™Â∑±ÁöÑÂ≠©Â≠êÁªßÁª≠‰∫ÜËß£ÂõΩÂÜÖÊïôËÇ≤„ÄÇ&lt;/p&gt; 
&lt;h2&gt;Â≠¶‰π†Êï∞Â≠¶&lt;/h2&gt; 
&lt;p&gt;Â∏åÊúõÊú™Êù•Âá∫Áé∞Êõ¥Â§ö‰∏çÊòØ‰∏∫‰∫ÜËÄÉÂ≠¶ËÄåËØª‰π¶ÁöÑ‰∫∫„ÄÇ&lt;/p&gt; 
&lt;h3&gt;Â∞èÂ≠¶Êï∞Â≠¶&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/TapXWorld/ChinaTextbook/raw/master/%E5%B0%8F%E5%AD%A6/%E6%95%B0%E5%AD%A6/%E4%BA%BA%E6%95%99%E7%89%88/%E4%B9%89%E5%8A%A1%E6%95%99%E8%82%B2%E6%95%99%E7%A7%91%E4%B9%A6%20%C2%B7%20%E6%95%B0%E5%AD%A6%E4%B8%80%E5%B9%B4%E7%BA%A7%E4%B8%8A%E5%86%8C.pdf"&gt;‰∏ÄÂπ¥Á∫ß‰∏äÂÜå&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/TapXWorld/ChinaTextbook/raw/master/%E5%B0%8F%E5%AD%A6/%E6%95%B0%E5%AD%A6/%E4%BA%BA%E6%95%99%E7%89%88/%E4%B9%89%E5%8A%A1%E6%95%99%E8%82%B2%E6%95%99%E7%A7%91%E4%B9%A6%C2%B7%E6%95%B0%E5%AD%A6%E4%B8%80%E5%B9%B4%E7%BA%A7%E4%B8%8B%E5%86%8C.pdf"&gt;‰∏ÄÂπ¥Á∫ß‰∏ãÂÜå&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/TapXWorld/ChinaTextbook/raw/master/%E5%B0%8F%E5%AD%A6/%E6%95%B0%E5%AD%A6/%E4%BA%BA%E6%95%99%E7%89%88/%E4%B9%89%E5%8A%A1%E6%95%99%E8%82%B2%E6%95%99%E7%A7%91%E4%B9%A6%20%C2%B7%20%E6%95%B0%E5%AD%A6%E4%BA%8C%E5%B9%B4%E7%BA%A7%E4%B8%8A%E5%86%8C.pdf"&gt;‰∫åÂπ¥Á∫ß‰∏äÂÜå&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/TapXWorld/ChinaTextbook/raw/master/%E5%B0%8F%E5%AD%A6/%E6%95%B0%E5%AD%A6/%E4%BA%BA%E6%95%99%E7%89%88/%E4%B9%89%E5%8A%A1%E6%95%99%E8%82%B2%E6%95%99%E7%A7%91%E4%B9%A6%C2%B7%E6%95%B0%E5%AD%A6%E4%BA%8C%E5%B9%B4%E7%BA%A7%E4%B8%8B%E5%86%8C.pdf"&gt;‰∫åÂπ¥Á∫ß‰∏ãÂÜå&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/TapXWorld/ChinaTextbook/raw/master/%E5%B0%8F%E5%AD%A6/%E6%95%B0%E5%AD%A6/%E4%BA%BA%E6%95%99%E7%89%88/%E4%B9%89%E5%8A%A1%E6%95%99%E8%82%B2%E6%95%99%E7%A7%91%E4%B9%A6%20%C2%B7%20%E6%95%B0%E5%AD%A6%E4%B8%89%E5%B9%B4%E7%BA%A7%E4%B8%8A%E5%86%8C.pdf"&gt;‰∏âÂπ¥Á∫ß‰∏äÂÜå&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/TapXWorld/ChinaTextbook/raw/master/%E5%B0%8F%E5%AD%A6/%E6%95%B0%E5%AD%A6/%E4%BA%BA%E6%95%99%E7%89%88/%E4%B9%89%E5%8A%A1%E6%95%99%E8%82%B2%E6%95%99%E7%A7%91%E4%B9%A6%C2%B7%E6%95%B0%E5%AD%A6%E4%B8%89%E5%B9%B4%E7%BA%A7%E4%B8%8B%E5%86%8C.pdf"&gt;‰∏âÂπ¥Á∫ß‰∏ãÂÜå&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/TapXWorld/ChinaTextbook/raw/master/%E5%B0%8F%E5%AD%A6/%E6%95%B0%E5%AD%A6/%E4%BA%BA%E6%95%99%E7%89%88/%E4%B9%89%E5%8A%A1%E6%95%99%E8%82%B2%E6%95%99%E7%A7%91%E4%B9%A6%20%C2%B7%20%E6%95%B0%E5%AD%A6%E5%9B%9B%E5%B9%B4%E7%BA%A7%E4%B8%8A%E5%86%8C.pdf"&gt;ÂõõÂπ¥Á∫ß‰∏äÂÜå&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/TapXWorld/ChinaTextbook/raw/master/%E5%B0%8F%E5%AD%A6/%E6%95%B0%E5%AD%A6/%E4%BA%BA%E6%95%99%E7%89%88/%E4%B9%89%E5%8A%A1%E6%95%99%E8%82%B2%E6%95%99%E7%A7%91%E4%B9%A6%C2%B7%E6%95%B0%E5%AD%A6%E5%9B%9B%E5%B9%B4%E7%BA%A7%E4%B8%8B%E5%86%8C.pdf"&gt;ÂõõÂπ¥Á∫ß‰∏ãÂÜå&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/TapXWorld/ChinaTextbook/raw/master/%E5%B0%8F%E5%AD%A6/%E6%95%B0%E5%AD%A6/%E4%BA%BA%E6%95%99%E7%89%88/%E4%B9%89%E5%8A%A1%E6%95%99%E8%82%B2%E6%95%99%E7%A7%91%E4%B9%A6%20%C2%B7%20%E6%95%B0%E5%AD%A6%E4%BA%94%E5%B9%B4%E7%BA%A7%E4%B8%8A%E5%86%8C.pdf"&gt;‰∫îÂπ¥Á∫ß‰∏äÂÜå&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/TapXWorld/ChinaTextbook/raw/master/%E5%B0%8F%E5%AD%A6/%E6%95%B0%E5%AD%A6/%E4%BA%BA%E6%95%99%E7%89%88/%E4%B9%89%E5%8A%A1%E6%95%99%E8%82%B2%E6%95%99%E7%A7%91%E4%B9%A6%C2%B7%E6%95%B0%E5%AD%A6%E4%BA%94%E5%B9%B4%E7%BA%A7%E4%B8%8B%E5%86%8C.pdf"&gt;‰∫îÂπ¥Á∫ß‰∏ãÂÜå&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/TapXWorld/ChinaTextbook/raw/master/%E5%B0%8F%E5%AD%A6/%E6%95%B0%E5%AD%A6/%E4%BA%BA%E6%95%99%E7%89%88/%E4%B9%89%E5%8A%A1%E6%95%99%E8%82%B2%E6%95%99%E7%A7%91%E4%B9%A6%20%C2%B7%20%E6%95%B0%E5%AD%A6%E5%85%AD%E5%B9%B4%E7%BA%A7%E4%B8%8A%E5%86%8C.pdf"&gt;ÂÖ≠Âπ¥Á∫ß‰∏äÂÜå&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/TapXWorld/ChinaTextbook/raw/master/%E5%B0%8F%E5%AD%A6/%E6%95%B0%E5%AD%A6/%E4%BA%BA%E6%95%99%E7%89%88/%E4%B9%89%E5%8A%A1%E6%95%99%E8%82%B2%E6%95%99%E7%A7%91%E4%B9%A6%C2%B7%E6%95%B0%E5%AD%A6%E5%85%AD%E5%B9%B4%E7%BA%A7%E4%B8%8B%E5%86%8C.pdf"&gt;ÂÖ≠Âπ¥Á∫ß‰∏ãÂÜå&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Âàù‰∏≠Êï∞Â≠¶&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/TapXWorld/ChinaTextbook/raw/master/%E5%88%9D%E4%B8%AD/%E6%95%B0%E5%AD%A6/%E4%BA%BA%E6%95%99%E7%89%88-%E4%BA%BA%E6%B0%91%E6%95%99%E8%82%B2%E5%87%BA%E7%89%88%E7%A4%BE/%E4%B8%83%E5%B9%B4%E7%BA%A7/%E4%B9%89%E5%8A%A1%E6%95%99%E8%82%B2%E6%95%99%E7%A7%91%E4%B9%A6%C2%B7%E6%95%B0%E5%AD%A6%E4%B8%83%E5%B9%B4%E7%BA%A7%E4%B8%8A%E5%86%8C.pdf"&gt;Âàù‰∏Ä‰∏äÂÜå&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/TapXWorld/ChinaTextbook/raw/master/%E5%88%9D%E4%B8%AD/%E6%95%B0%E5%AD%A6/%E4%BA%BA%E6%95%99%E7%89%88-%E4%BA%BA%E6%B0%91%E6%95%99%E8%82%B2%E5%87%BA%E7%89%88%E7%A4%BE/%E4%B8%83%E5%B9%B4%E7%BA%A7/%E4%B9%89%E5%8A%A1%E6%95%99%E8%82%B2%E6%95%99%E7%A7%91%E4%B9%A6%C2%B7%E6%95%B0%E5%AD%A6%E4%B8%83%E5%B9%B4%E7%BA%A7%E4%B8%8B%E5%86%8C.pdf"&gt;Âàù‰∏Ä‰∏ãÂÜå&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/TapXWorld/ChinaTextbook/raw/master/%E5%88%9D%E4%B8%AD/%E6%95%B0%E5%AD%A6/%E4%BA%BA%E6%95%99%E7%89%88-%E4%BA%BA%E6%B0%91%E6%95%99%E8%82%B2%E5%87%BA%E7%89%88%E7%A4%BE/%E5%85%AB%E5%B9%B4%E7%BA%A7/%E4%B9%89%E5%8A%A1%E6%95%99%E8%82%B2%E6%95%99%E7%A7%91%E4%B9%A6%C2%B7%E6%95%B0%E5%AD%A6%E5%85%AB%E5%B9%B4%E7%BA%A7%E4%B8%8A%E5%86%8C.pdf"&gt;Âàù‰∫å‰∏äÂÜå&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/TapXWorld/ChinaTextbook/raw/master/%E5%88%9D%E4%B8%AD/%E6%95%B0%E5%AD%A6/%E4%BA%BA%E6%95%99%E7%89%88-%E4%BA%BA%E6%B0%91%E6%95%99%E8%82%B2%E5%87%BA%E7%89%88%E7%A4%BE/%E5%85%AB%E5%B9%B4%E7%BA%A7/%E4%B9%89%E5%8A%A1%E6%95%99%E8%82%B2%E6%95%99%E7%A7%91%E4%B9%A6%C2%B7%E6%95%B0%E5%AD%A6%E5%85%AB%E5%B9%B4%E7%BA%A7%E4%B8%8B%E5%86%8C.pdf"&gt;Âàù‰∫å‰∏ãÂÜå&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/TapXWorld/ChinaTextbook/raw/master/%E5%88%9D%E4%B8%AD/%E6%95%B0%E5%AD%A6/%E4%BA%BA%E6%95%99%E7%89%88-%E4%BA%BA%E6%B0%91%E6%95%99%E8%82%B2%E5%87%BA%E7%89%88%E7%A4%BE/%E4%B9%9D%E5%B9%B4%E7%BA%A7/%E4%B9%89%E5%8A%A1%E6%95%99%E8%82%B2%E6%95%99%E7%A7%91%E4%B9%A6%C2%B7%E6%95%B0%E5%AD%A6%E4%B9%9D%E5%B9%B4%E7%BA%A7%E4%B8%8A%E5%86%8C.pdf"&gt;Âàù‰∏â‰∏äÂÜå&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/TapXWorld/ChinaTextbook/raw/master/%E5%88%9D%E4%B8%AD/%E6%95%B0%E5%AD%A6/%E4%BA%BA%E6%95%99%E7%89%88-%E4%BA%BA%E6%B0%91%E6%95%99%E8%82%B2%E5%87%BA%E7%89%88%E7%A4%BE/%E4%B9%9D%E5%B9%B4%E7%BA%A7/%E4%B9%89%E5%8A%A1%E6%95%99%E8%82%B2%E6%95%99%E7%A7%91%E4%B9%A6%C2%B7%E6%95%B0%E5%AD%A6%E4%B9%9D%E5%B9%B4%E7%BA%A7%E4%B8%8B%E5%86%8C.pdf"&gt;Âàù‰∏â‰∏ãÂÜå&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;È´ò‰∏≠Êï∞Â≠¶&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/TapXWorld/ChinaTextbook/tree/master/%E9%AB%98%E4%B8%AD/%E6%95%B0%E5%AD%A6/%E4%BA%BA%E6%95%99%E7%89%88%EF%BC%88A%E7%89%88%EF%BC%89%EF%BC%88%E4%B8%BB%E7%BC%96%EF%BC%9A%E7%AB%A0%E5%BB%BA%E8%B7%83%26%E6%9D%8E%E5%A2%9E%E6%B2%AA%EF%BC%89-%E4%BA%BA%E6%B0%91%E6%95%99%E8%82%B2%E5%87%BA%E7%89%88%E7%A4%BE"&gt;ÁõÆÂΩï&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Â§ßÂ≠¶Êï∞Â≠¶&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/TapXWorld/ChinaTextbook/tree/master/%E5%A4%A7%E5%AD%A6/%E9%AB%98%E7%AD%89%E6%95%B0%E5%AD%A6/%E5%90%8C%E6%B5%8E%E5%A4%A7%E5%AD%A6%E9%AB%98%E7%AD%89%E6%95%B0%E5%AD%A6%E7%AC%AC%E4%B8%83%E7%89%88"&gt;È´òÁ≠âÊï∞Â≠¶&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/TapXWorld/ChinaTextbook/tree/master/%E5%A4%A7%E5%AD%A6/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0"&gt;Á∫øÊÄß‰ª£Êï∞&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/TapXWorld/ChinaTextbook/tree/master/%E5%A4%A7%E5%AD%A6/%E7%A6%BB%E6%95%A3%E6%95%B0%E5%AD%A6"&gt;Á¶ªÊï£Êï∞Â≠¶&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/TapXWorld/ChinaTextbook/tree/master/%E5%A4%A7%E5%AD%A6/%E6%A6%82%E7%8E%87%E8%AE%BA"&gt;Ê¶ÇÁéáËÆ∫&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="http://www.dxsx.net/index.php"&gt;Êõ¥Â§öÊï∞Â≠¶ËµÑÊñô-(Â§ßÂ≠¶Êï∞Â≠¶ÁΩë)&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;/h2&gt; 
&lt;h2&gt;ÈóÆÈ¢òÔºöÂ¶Ç‰ΩïÂêàÂπ∂Ë¢´ÊãÜÂàÜÁöÑÊñá‰ª∂Ôºü&lt;/h2&gt; 
&lt;p&gt;Áî±‰∫é GitHub ÂØπÂçï‰∏™Êñá‰ª∂ÁöÑ‰∏ä‰º†ÊúâÊúÄÂ§ßÈôêÂà∂ÔºåË∂ÖËøá 100MB ÁöÑÊñá‰ª∂‰ºöË¢´ÊãíÁªù‰∏ä‰º†ÔºåË∂ÖËøá 50MB ÁöÑÊñá‰ª∂‰∏ä‰º†Êó∂‰ºöÊî∂Âà∞Ë≠¶Âëä„ÄÇÂõ†Ê≠§ÔºåÊñá‰ª∂Â§ßÂ∞èË∂ÖËøá 50MB ÁöÑÊñá‰ª∂‰ºöË¢´ÊãÜÂàÜÊàêÊØè‰∏™ 35MB ÁöÑÂ§ö‰∏™Êñá‰ª∂„ÄÇ&lt;/p&gt; 
&lt;h3&gt;Á§∫‰æã&lt;/h3&gt; 
&lt;p&gt;Êñá‰ª∂Ë¢´ÊãÜÂàÜÁöÑÁ§∫‰æãÔºö&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;‰πâÂä°ÊïôËÇ≤ÊïôÁßë‰π¶ ¬∑ Êï∞Â≠¶‰∏ÄÂπ¥Á∫ß‰∏äÂÜå.pdf.1&lt;/li&gt; 
 &lt;li&gt;‰πâÂä°ÊïôËÇ≤ÊïôÁßë‰π¶ ¬∑ Êï∞Â≠¶‰∏ÄÂπ¥Á∫ß‰∏äÂÜå.pdf.2&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Ëß£ÂÜ≥ÂäûÊ≥ï&lt;/h3&gt; 
&lt;p&gt;Ë¶ÅÂêàÂπ∂Ëøô‰∫õË¢´ÊãÜÂàÜÁöÑÊñá‰ª∂ÔºåÊÇ®Âè™ÈúÄÊâßË°å‰ª•‰∏ãÊ≠•È™§(ÂÖ∂‰ªñÊìç‰ΩúÁ≥ªÁªüÂêåÁêÜ)Ôºö&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Â∞ÜÂêàÂπ∂Á®ãÂ∫è &lt;code&gt;mergePDFs-windows-amd64.exe&lt;/code&gt; ‰∏ãËΩΩÂà∞ÂåÖÂê´ PDF Êñá‰ª∂ÁöÑÊñá‰ª∂Â§π‰∏≠„ÄÇ&lt;/li&gt; 
 &lt;li&gt;Á°Æ‰øù &lt;code&gt;mergePDFs-windows-amd64.exe&lt;/code&gt; ÂíåË¢´ÊãÜÂàÜÁöÑ PDF Êñá‰ª∂Âú®Âêå‰∏ÄÁõÆÂΩï‰∏ã„ÄÇ&lt;/li&gt; 
 &lt;li&gt;ÂèåÂáª &lt;code&gt;mergePDFs-windows-amd64.exe&lt;/code&gt; Á®ãÂ∫èÂç≥ÂèØËá™Âä®ÂÆåÊàêÊñá‰ª∂ÂêàÂπ∂„ÄÇ&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;‰∏ãËΩΩÊñπÂºè&lt;/h3&gt; 
&lt;p&gt;ÊÇ®ÂèØ‰ª•ÈÄöËøá‰ª•‰∏ãÈìæÊé•Ôºå‰∏ãËΩΩÊñá‰ª∂ÂêàÂπ∂Á®ãÂ∫èÔºö&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/TapXWorld/ChinaTextbook-tools/releases"&gt;‰∏ãËΩΩÊñá‰ª∂ÂêàÂπ∂Á®ãÂ∫è&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Êñá‰ª∂ÂíåÁ®ãÂ∫èÁ§∫‰æã&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;mergePDFs-windows-amd64.exe&lt;/li&gt; 
 &lt;li&gt;‰πâÂä°ÊïôËÇ≤ÊïôÁßë‰π¶ ¬∑ Êï∞Â≠¶‰∏ÄÂπ¥Á∫ß‰∏äÂÜå.pdf.1&lt;/li&gt; 
 &lt;li&gt;‰πâÂä°ÊïôËÇ≤ÊïôÁßë‰π¶ ¬∑ Êï∞Â≠¶‰∏ÄÂπ¥Á∫ß‰∏äÂÜå.pdf.2&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ÈáçÊñ∞‰∏ãËΩΩ&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Â¶ÇÊûúÊÇ®‰Ωç‰∫éÂÜÖÂú∞ÔºåÂπ∂‰∏îÁΩëÁªú‰∏çÈîôÔºåÊÉ≥ÈáçÊñ∞‰∏ãËΩΩÔºåÊÇ®ÂèØ‰ª•‰ΩøÁî® &lt;a href="https://github.com/happycola233/tchMaterial-parser"&gt;tchMaterial-parser&lt;/a&gt; È°πÁõÆÔºàÈºìÂä±ÂºÄÊ∫êÔºâÔºåËøõË°åÈáçÊñ∞‰∏ãËΩΩ„ÄÇ&lt;/li&gt; 
 &lt;li&gt;Â¶ÇÊûúÊÇ®‰Ωç‰∫éÂõΩÂ§ñÔºåÂíåÂÜÖÂú∞ÁΩëÁªúÈÄö‰ø°ÈÄüÂ∫¶ËæÉÊÖ¢ÔºåÂª∫ËÆÆ‰ΩøÁî®Êú¨Â≠òÂÇ®Â∫ìËøõË°åÁ≠æÂá∫„ÄÇ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ÊïôÊùêÊçêÁåÆ&lt;/h2&gt; 
&lt;p&gt;Â¶ÇÊûúËøô‰∏™È°πÁõÆÂ∏ÆÂä©ÊÇ®ÂÖçË¥πËé∑ÂèñÊïôËÇ≤ËµÑÊ∫êÔºåËØ∑ËÄÉËôëÊîØÊåÅÊàë‰ª¨Êé®ÂπøÂºÄÊîæÊïôËÇ≤ÁöÑÂä™ÂäõÔºÅÊÇ®ÁöÑÊçêÁåÆÂ∞ÜÂ∏ÆÂä©Êàë‰ª¨Áª¥Êä§ÂíåÊâ©Â±ïËøô‰∏™ËµÑÊ∫êÂ∫ì„ÄÇ&lt;/p&gt; 
&lt;p&gt;Âä†ÂÖ•Êàë‰ª¨ÁöÑ Telegram Á§æÂå∫ÔºåËé∑ÂèñÊúÄÊñ∞Âä®ÊÄÅÂπ∂ÂàÜ‰∫´ÊÇ®ÁöÑÊÉ≥Ê≥ïÔºö&lt;a href="https://t.me/+1V6WjEq8WEM4MDM1"&gt;https://t.me/+1V6WjEq8WEM4MDM1&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;ÊîØÊåÅÊàë&lt;/h2&gt; 
&lt;p&gt;Â¶ÇÊûúÊÇ®ËßâÂæóËøô‰∏™È°πÁõÆÂØπÊÇ®ÊúâÂ∏ÆÂä©ÔºåÊÇ®ÂèØ‰ª•Êâ´Êèè‰ª•‰∏ã‰∫åÁª¥Á†ÅËøõË°åÊçêËµ†Ôºö&lt;/p&gt; 
&lt;p align="left"&gt; &lt;img src="https://raw.githubusercontent.com/TapXWorld/ChinaTextbook/master/.cache/support-alipay.png" width="20%" /&gt; &lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Stremio/stremio-web</title>
      <link>https://github.com/Stremio/stremio-web</link>
      <description>&lt;p&gt;Stremio - Freedom to Stream&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Stremio - Freedom to Stream&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/Stremio/stremio-web/actions/workflows/build.yml"&gt;&lt;img src="https://github.com/Stremio/stremio-web/actions/workflows/build.yml/badge.svg?sanitize=true" alt="Build" /&gt;&lt;/a&gt; &lt;a href="https://stremio.github.io/stremio-web/development"&gt;&lt;img src="https://img.shields.io/website?label=Page&amp;amp;logo=github&amp;amp;up_message=online&amp;amp;down_message=offline&amp;amp;url=https%3A%2F%2Fstremio.github.io%2Fstremio-web%2F" alt="Github Page" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Stremio is a modern media center that's a one-stop solution for your video entertainment. You discover, watch and organize video content from easy to install addons.&lt;/p&gt; 
&lt;h2&gt;Build&lt;/h2&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Node.js 12 or higher&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pnpm.io/installation"&gt;pnpm&lt;/a&gt; 10 or higher&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Install dependencies&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pnpm install
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Start development server&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pnpm start
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Production build&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pnpm run build
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Run with Docker&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker build -t stremio-web .
docker run -p 8080:8080 stremio-web
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Screenshots&lt;/h2&gt; 
&lt;h3&gt;Board&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Stremio/stremio-web/development/screenshots/board.png" alt="Board" /&gt;&lt;/p&gt; 
&lt;h3&gt;Discover&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Stremio/stremio-web/development/screenshots/discover.png" alt="Discover" /&gt;&lt;/p&gt; 
&lt;h3&gt;Meta Details&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Stremio/stremio-web/development/screenshots/metadetails.png" alt="Meta Details" /&gt;&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Stremio is copyright 2017-2023 Smart code and available under GPLv2 license. See the &lt;a href="https://raw.githubusercontent.com/Stremio/stremio-web/development/LICENSE.md"&gt;LICENSE&lt;/a&gt; file in the project for more information.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>supermemoryai/supermemory</title>
      <link>https://github.com/supermemoryai/supermemory</link>
      <description>&lt;p&gt;Memory engine and app that is extremely fast, scalable. The Memory API for the AI era.&lt;/p&gt;&lt;hr&gt;&lt;div align="center" style="padding-bottom:20px;padding-top:20px"&gt; 
 &lt;img src="https://raw.githubusercontent.com/supermemoryai/supermemory/main/logo.svg?sanitize=true" alt="supermemory Logo" width="400" /&gt; 
&lt;/div&gt; 
&lt;div align="center" style="padding-bottom:10px;padding-top:10px"&gt; 
 &lt;img src="https://raw.githubusercontent.com/supermemoryai/supermemory/main/apps/web/public/landing-page.jpeg" alt="supermemory" width="100%" /&gt; 
&lt;/div&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;h3&gt;Core Functionality&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/supermemoryai/supermemory/main/#add-memory"&gt;Add Memories from Any Content&lt;/a&gt;&lt;/strong&gt;: Easily add memories from URLs, PDFs, and plain text‚Äîjust paste, upload, or link.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/supermemoryai/supermemory/main/#chat-memories"&gt;Chat with Your Memories&lt;/a&gt;&lt;/strong&gt;: Converse with your stored content using natural language chat.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/supermemoryai/supermemory/main/#mcp-integration"&gt;Supermemory MCP Integration&lt;/a&gt;&lt;/strong&gt;: Seamlessly connect with all major AI tools (Claude, Cursor, etc.) via Supermemory MCP.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;How do i use this?&lt;/h2&gt; 
&lt;p&gt;Go to &lt;a href="https://app.supermemory.ai"&gt;app.supermemory.ai&lt;/a&gt; and sign into with your account&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a id="add-memory"&gt;&lt;/a&gt;Start Adding Memory with your choice of format (Note, Link, File)&lt;/li&gt; 
&lt;/ol&gt; 
&lt;div align="center" style="padding-bottom:10px;padding-top:10px"&gt; 
 &lt;img src="https://raw.githubusercontent.com/supermemoryai/supermemory/main/apps/web/public/add-memory.png" alt="supermemory" width="100%" /&gt; 
&lt;/div&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;You can also Connect to your favourite services (Notion, Google Drive, OneDrive)&lt;/li&gt; 
&lt;/ol&gt; 
&lt;div align="center" style="padding-bottom:10px;padding-top:10px"&gt; 
 &lt;img src="https://raw.githubusercontent.com/supermemoryai/supermemory/main/apps/web/public/add-connections.png" alt="supermemory" width="100%" /&gt; 
&lt;/div&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;&lt;a id="chat-memories"&gt;&lt;/a&gt;Once Memories are added, you can chat with Supermemory by clicking on "Open Chat" and retrieve info from your saved memories&lt;/li&gt; 
&lt;/ol&gt; 
&lt;div align="center" style="padding-bottom:10px;padding-top:10px"&gt; 
 &lt;img src="https://raw.githubusercontent.com/supermemoryai/supermemory/main/apps/web/public/chat.png" alt="supermemory" width="100%" /&gt; 
&lt;/div&gt; 
&lt;ol start="4"&gt; 
 &lt;li&gt;&lt;a id="mcp-integration"&gt;&lt;/a&gt;Add MCP to your AI Tools (by clicking on "Connect to your AI" and select the AI tool you are trying to integrate)&lt;/li&gt; 
&lt;/ol&gt; 
&lt;div align="center" style="padding-bottom:10px;padding-top:10px"&gt; 
 &lt;img src="https://raw.githubusercontent.com/supermemoryai/supermemory/main/apps/web/public/mcp.png" alt="supermemory" width="100%" /&gt; 
&lt;/div&gt; 
&lt;h2&gt;Support&lt;/h2&gt; 
&lt;p&gt;Have questions or feedback? We're here to help:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Email: &lt;a href="mailto:dhravya@supermemory.com"&gt;dhravya@supermemory.com&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Documentation: &lt;a href="https://docs.supermemory.ai"&gt;docs.supermemory.ai&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions from developers of all skill levels! Whether you're fixing bugs, adding features, or improving documentation, your help makes supermemory better for everyone.&lt;/p&gt; 
&lt;h3&gt;Quick Start for Contributors&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Fork and clone&lt;/strong&gt; the repository&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Install dependencies&lt;/strong&gt; with &lt;code&gt;bun install&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Set up your environment&lt;/strong&gt; by copying &lt;code&gt;.env.example&lt;/code&gt; to &lt;code&gt;.env.local&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Start developing&lt;/strong&gt; with &lt;code&gt;bun run dev&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;For detailed guidelines, development setup, coding standards, and the complete contribution workflow, please see our &lt;a href="https://raw.githubusercontent.com/supermemoryai/supermemory/main/CONTRIBUTING.md"&gt;&lt;strong&gt;Contributing Guide&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Ways to Contribute&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;üêõ &lt;strong&gt;Bug fixes&lt;/strong&gt; - Help us squash those pesky issues&lt;/li&gt; 
 &lt;li&gt;‚ú® &lt;strong&gt;New features&lt;/strong&gt; - Add functionality that users will love&lt;/li&gt; 
 &lt;li&gt;üé® &lt;strong&gt;UI/UX improvements&lt;/strong&gt; - Make the interface more intuitive&lt;/li&gt; 
 &lt;li&gt;‚ö° &lt;strong&gt;Performance optimizations&lt;/strong&gt; - Help us make supermemory faster&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Check out our &lt;a href="https://github.com/supermemoryai/supermemory/issues"&gt;Issues&lt;/a&gt; page for &lt;code&gt;good first issue&lt;/code&gt; and &lt;code&gt;help wanted&lt;/code&gt; labels to get started!&lt;/p&gt; 
&lt;h2&gt;Updates &amp;amp; Roadmap&lt;/h2&gt; 
&lt;p&gt;Stay up to date with the latest improvements:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.supermemory.ai/changelog/overview"&gt;Changelog&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://x.com/supermemoryai"&gt;X&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>anthropics/prompt-eng-interactive-tutorial</title>
      <link>https://github.com/anthropics/prompt-eng-interactive-tutorial</link>
      <description>&lt;p&gt;Anthropic's Interactive Prompt Engineering Tutorial&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Welcome to Anthropic's Prompt Engineering Interactive Tutorial&lt;/h1&gt; 
&lt;h2&gt;Course introduction and goals&lt;/h2&gt; 
&lt;p&gt;This course is intended to provide you with a comprehensive step-by-step understanding of how to engineer optimal prompts within Claude.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;After completing this course, you will be able to&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Master the basic structure of a good prompt&lt;/li&gt; 
 &lt;li&gt;Recognize common failure modes and learn the '80/20' techniques to address them&lt;/li&gt; 
 &lt;li&gt;Understand Claude's strengths and weaknesses&lt;/li&gt; 
 &lt;li&gt;Build strong prompts from scratch for common use cases&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Course structure and content&lt;/h2&gt; 
&lt;p&gt;This course is structured to allow you many chances to practice writing and troubleshooting prompts yourself. The course is broken up into &lt;strong&gt;9 chapters with accompanying exercises&lt;/strong&gt;, as well as an appendix of even more advanced methods. It is intended for you to &lt;strong&gt;work through the course in chapter order&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Each lesson has an "Example Playground" area&lt;/strong&gt; at the bottom where you are free to experiment with the examples in the lesson and see for yourself how changing prompts can change Claude's responses. There is also an &lt;a href="https://docs.google.com/spreadsheets/d/1jIxjzUWG-6xBVIa2ay6yDpLyeuOh_hR_ZB75a47KX_E/edit?usp=sharing"&gt;answer key&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Note: This tutorial uses our smallest, fastest, and cheapest model, Claude 3 Haiku. Anthropic has &lt;a href="https://docs.anthropic.com/claude/docs/models-overview"&gt;two other models&lt;/a&gt;, Claude 3 Sonnet and Claude 3 Opus, which are more intelligent than Haiku, with Opus being the most intelligent.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;This tutorial also exists on &lt;a href="https://docs.google.com/spreadsheets/d/19jzLgRruG9kjUQNKtCg1ZjdD6l6weA6qRXG5zLIAhC8/edit?usp=sharing"&gt;Google Sheets using Anthropic's Claude for Sheets extension&lt;/a&gt;. We recommend using that version as it is more user friendly.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;When you are ready to begin, go to &lt;code&gt;01_Basic Prompt Structure&lt;/code&gt; to proceed.&lt;/p&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;p&gt;Each chapter consists of a lesson and a set of exercises.&lt;/p&gt; 
&lt;h3&gt;Beginner&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Chapter 1:&lt;/strong&gt; Basic Prompt Structure&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Chapter 2:&lt;/strong&gt; Being Clear and Direct&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Chapter 3:&lt;/strong&gt; Assigning Roles&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Intermediate&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Chapter 4:&lt;/strong&gt; Separating Data from Instructions&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Chapter 5:&lt;/strong&gt; Formatting Output &amp;amp; Speaking for Claude&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Chapter 6:&lt;/strong&gt; Precognition (Thinking Step by Step)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Chapter 7:&lt;/strong&gt; Using Examples&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Advanced&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Chapter 8:&lt;/strong&gt; Avoiding Hallucinations&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Chapter 9:&lt;/strong&gt; Building Complex Prompts (Industry Use Cases)&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Complex Prompts from Scratch - Chatbot&lt;/li&gt; 
   &lt;li&gt;Complex Prompts for Legal Services&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Exercise:&lt;/strong&gt; Complex Prompts for Financial Services&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Exercise:&lt;/strong&gt; Complex Prompts for Coding&lt;/li&gt; 
   &lt;li&gt;Congratulations &amp;amp; Next Steps&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Appendix:&lt;/strong&gt; Beyond Standard Prompting&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Chaining Prompts&lt;/li&gt; 
   &lt;li&gt;Tool Use&lt;/li&gt; 
   &lt;li&gt;Search &amp;amp; Retrieval&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>google/computer-use-preview</title>
      <link>https://github.com/google/computer-use-preview</link>
      <description>&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Computer Use Preview&lt;/h1&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;p&gt;This section will guide you through setting up and running the Computer Use Preview model. Follow these steps to get started.&lt;/p&gt; 
&lt;h3&gt;1. Installation&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Clone the Repository&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/google/computer-use-preview.git
cd computer-use-preview
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Set up Python Virtual Environment and Install Dependencies&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Install Playwright and Browser Dependencies&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install system dependencies required by Playwright for Chrome
playwright install-deps chrome

# Install the Chrome browser for Playwright
playwright install chrome
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. Configuration&lt;/h3&gt; 
&lt;p&gt;You can get started using either the Gemini Developer API or Vertex AI.&lt;/p&gt; 
&lt;h4&gt;A. If using the Gemini Developer API:&lt;/h4&gt; 
&lt;p&gt;You need a Gemini API key to use the agent:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export GEMINI_API_KEY="YOUR_GEMINI_API_KEY"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or to add this to your virtual environment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;echo 'export GEMINI_API_KEY="YOUR_GEMINI_API_KEY"' &amp;gt;&amp;gt; .venv/bin/activate
# After editing, you'll need to deactivate and reactivate your virtual
# environment if it's already active:
deactivate
source .venv/bin/activate
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Replace &lt;code&gt;YOUR_GEMINI_API_KEY&lt;/code&gt; with your actual key.&lt;/p&gt; 
&lt;h4&gt;B. If using the Vertex AI Client:&lt;/h4&gt; 
&lt;p&gt;You need to explicitly use Vertex AI, then provide project and location to use the agent:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export USE_VERTEXAI=true
export VERTEXAI_PROJECT="YOUR_PROJECT_ID"
export VERTEXAI_LOCATION="YOUR_LOCATION"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or to add this to your virtual environment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;echo 'export USE_VERTEXAI=true' &amp;gt;&amp;gt; .venv/bin/activate
echo 'export VERTEXAI_PROJECT="your-project-id"' &amp;gt;&amp;gt; .venv/bin/activate
echo 'export VERTEXAI_LOCATION="your-location"' &amp;gt;&amp;gt; .venv/bin/activate
# After editing, you'll need to deactivate and reactivate your virtual
# environment if it's already active:
deactivate
source .venv/bin/activate
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Replace &lt;code&gt;YOUR_PROJECT_ID&lt;/code&gt; and &lt;code&gt;YOUR_LOCATION&lt;/code&gt; with your actual project and location.&lt;/p&gt; 
&lt;h3&gt;3. Running the Tool&lt;/h3&gt; 
&lt;p&gt;The primary way to use the tool is via the &lt;code&gt;main.py&lt;/code&gt; script.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;General Command Structure:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python main.py --query "Go to Google and type 'Hello World' into the search bar"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Available Environments:&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;You can specify a particular environment with the &lt;code&gt;--env &amp;lt;environment&amp;gt;&lt;/code&gt; flag. Available options:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;playwright&lt;/code&gt;: Runs the browser locally using Playwright.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;browserbase&lt;/code&gt;: Connects to a Browserbase instance.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Local Playwright&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Runs the agent using a Chrome browser instance controlled locally by Playwright.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python main.py --query="Go to Google and type 'Hello World' into the search bar" --env="playwright"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also specify an initial URL for the Playwright environment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python main.py --query="Go to Google and type 'Hello World' into the search bar" --env="playwright" --initial_url="https://www.google.com/search?q=latest+AI+news"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Browserbase&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Runs the agent using Browserbase as the browser backend. Ensure the proper Browserbase environment variables are set:&lt;code&gt;BROWSERBASE_API_KEY&lt;/code&gt; and &lt;code&gt;BROWSERBASE_PROJECT_ID&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python main.py --query="Go to Google and type 'Hello World' into the search bar" --env="browserbase"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Agent CLI&lt;/h2&gt; 
&lt;p&gt;The &lt;code&gt;main.py&lt;/code&gt; script is the command-line interface (CLI) for running the browser agent.&lt;/p&gt; 
&lt;h3&gt;Command-Line Arguments&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Argument&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Required&lt;/th&gt; 
   &lt;th&gt;Default&lt;/th&gt; 
   &lt;th&gt;Supported Environment(s)&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--query&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;The natural language query for the browser agent to execute.&lt;/td&gt; 
   &lt;td&gt;Yes&lt;/td&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
   &lt;td&gt;All&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--env&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;The computer use environment to use. Must be one of the following: &lt;code&gt;playwright&lt;/code&gt;, or &lt;code&gt;browserbase&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;No&lt;/td&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
   &lt;td&gt;All&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--initial_url&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;The initial URL to load when the browser starts.&lt;/td&gt; 
   &lt;td&gt;No&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.google.com"&gt;https://www.google.com&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;All&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--highlight_mouse&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;If specified, the agent will attempt to highlight the mouse cursor's position in the screenshots. This is useful for visual debugging.&lt;/td&gt; 
   &lt;td&gt;No&lt;/td&gt; 
   &lt;td&gt;False (not highlighted)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;playwright&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Environment Variables&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Required&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;GEMINI_API_KEY&lt;/td&gt; 
   &lt;td&gt;Your API key for the Gemini model.&lt;/td&gt; 
   &lt;td&gt;Yes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BROWSERBASE_API_KEY&lt;/td&gt; 
   &lt;td&gt;Your API key for Browserbase.&lt;/td&gt; 
   &lt;td&gt;Yes (when using the browserbase environment)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BROWSERBASE_PROJECT_ID&lt;/td&gt; 
   &lt;td&gt;Your Project ID for Browserbase.&lt;/td&gt; 
   &lt;td&gt;Yes (when using the browserbase environment)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt;</description>
    </item>
    
    <item>
      <title>microsoft/RD-Agent</title>
      <link>https://github.com/microsoft/RD-Agent</link>
      <description>&lt;p&gt;Research and development (R&amp;D) is crucial for the enhancement of industrial productivity, especially in the AI era, where the core aspects of R&amp;D are mainly focused on data and models. We are committed to automating these high-value generic R&amp;D processes through R&amp;D-Agent, which lets AI drive data-driven AI. üîóhttps://aka.ms/RD-Agent-Tech-Report&lt;/p&gt;&lt;hr&gt;&lt;h4 align="center"&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/RD-Agent/main/docs/_static/logo.png" alt="RA-Agent logo" style="width:70%; " /&gt; &lt;p&gt;&lt;a href="https://rdagent.azurewebsites.net" target="_blank"&gt;üñ•Ô∏è Live Demo&lt;/a&gt; | &lt;a href="https://rdagent.azurewebsites.net/factor_loop" target="_blank"&gt;üé• Demo Video&lt;/a&gt; &lt;a href="https://www.youtube.com/watch?v=JJ4JYO3HscM&amp;amp;list=PLALmKB0_N3_i52fhUmPQiL4jsO354uopR" target="_blank"&gt;‚ñ∂Ô∏èYouTube&lt;/a&gt; | &lt;a href="https://rdagent.readthedocs.io/en/latest/index.html" target="_blank"&gt;üìñ Documentation&lt;/a&gt; | &lt;a href="https://aka.ms/RD-Agent-Tech-Report" target="_blank"&gt;üìÑ Tech Report&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/RD-Agent/main/#-paperwork-list"&gt; üìÉ Papers &lt;/a&gt;&lt;/p&gt; &lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://github.com/microsoft/RD-Agent/actions/workflows/ci.yml"&gt;&lt;img src="https://github.com/microsoft/RD-Agent/actions/workflows/ci.yml/badge.svg?sanitize=true" alt="CI" /&gt;&lt;/a&gt; &lt;a href="https://github.com/microsoft/RD-Agent/actions/workflows/github-code-scanning/codeql"&gt;&lt;img src="https://github.com/microsoft/RD-Agent/actions/workflows/github-code-scanning/codeql/badge.svg?sanitize=true" alt="CodeQL" /&gt;&lt;/a&gt; &lt;a href="https://github.com/microsoft/RD-Agent/actions/workflows/dependabot/dependabot-updates"&gt;&lt;img src="https://github.com/microsoft/RD-Agent/actions/workflows/dependabot/dependabot-updates/badge.svg?sanitize=true" alt="Dependabot Updates" /&gt;&lt;/a&gt; &lt;a href="https://github.com/microsoft/RD-Agent/actions/workflows/pr.yml"&gt;&lt;img src="https://github.com/microsoft/RD-Agent/actions/workflows/pr.yml/badge.svg?sanitize=true" alt="Lint PR Title" /&gt;&lt;/a&gt; &lt;a href="https://github.com/microsoft/RD-Agent/actions/workflows/release.yml"&gt;&lt;img src="https://github.com/microsoft/RD-Agent/actions/workflows/release.yml/badge.svg?sanitize=true" alt="Release.yml" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/rdagent/#files"&gt;&lt;img src="https://img.shields.io/badge/platform-Linux-blue" alt="Platform" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/rdagent/"&gt;&lt;img src="https://img.shields.io/pypi/v/rdagent" alt="PyPI" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/rdagent/"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/rdagent" alt="PyPI - Python Version" /&gt;&lt;/a&gt; &lt;a href="https://github.com/microsoft/RD-Agent/releases"&gt;&lt;img src="https://img.shields.io/github/v/release/microsoft/RD-Agent" alt="Release" /&gt;&lt;/a&gt; &lt;a href="https://github.com/microsoft/RD-Agent/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/microsoft/RD-Agent" alt="GitHub" /&gt;&lt;/a&gt; &lt;a href="https://github.com/pre-commit/pre-commit"&gt;&lt;img src="https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit" alt="pre-commit" /&gt;&lt;/a&gt; &lt;a href="http://mypy-lang.org/"&gt;&lt;img src="https://www.mypy-lang.org/static/mypy_badge.svg?sanitize=true" alt="Checked with mypy" /&gt;&lt;/a&gt; &lt;a href="https://github.com/astral-sh/ruff"&gt;&lt;img src="https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json" alt="Ruff" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/ybQ97B6Jjy"&gt;&lt;img src="https://img.shields.io/badge/chat-discord-blue" alt="Chat" /&gt;&lt;/a&gt; &lt;a href="https://rdagent.readthedocs.io/en/latest/?badge=latest"&gt;&lt;img src="https://readthedocs.org/projects/rdagent/badge/?version=latest" alt="Documentation Status" /&gt;&lt;/a&gt; &lt;a href="https://github.com/microsoft/RD-Agent/actions/workflows/readthedocs-preview.yml"&gt;&lt;img src="https://github.com/microsoft/RD-Agent/actions/workflows/readthedocs-preview.yml/badge.svg?sanitize=true" alt="Readthedocs Preview" /&gt;&lt;/a&gt; 
 &lt;!-- this badge is too long, please place it in the last one to make it pretty --&gt; &lt;a href="https://arxiv.org/abs/2505.14738"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2505.14738-00ff00.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;üì∞ News&lt;/h1&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;üóûÔ∏è News&lt;/th&gt; 
   &lt;th&gt;üìù Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;NeurIPS 2025 Acceptance&lt;/td&gt; 
   &lt;td&gt;We are thrilled to announce that our paper &lt;a href="https://arxiv.org/abs/2505.15155"&gt;R&amp;amp;D-Agent-Quant&lt;/a&gt; has been accepted to NeurIPS 2025&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/RD-Agent/main/#overall-technical-report"&gt;Technical Report Release&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Overall framework description and results on MLE-bench&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/RD-Agent/main/#deep-application-in-diverse-scenarios"&gt;R&amp;amp;D-Agent-Quant Release&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Apply R&amp;amp;D-Agent to quant trading&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MLE-Bench Results Released&lt;/td&gt; 
   &lt;td&gt;R&amp;amp;D-Agent currently leads as the &lt;a href="https://raw.githubusercontent.com/microsoft/RD-Agent/main/#-the-best-machine-learning-engineering-agent"&gt;top-performing machine learning engineering agent&lt;/a&gt; on MLE-bench&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Support LiteLLM Backend&lt;/td&gt; 
   &lt;td&gt;We now fully support &lt;strong&gt;&lt;a href="https://github.com/BerriAI/litellm"&gt;LiteLLM&lt;/a&gt;&lt;/strong&gt; as our default backend for integration with multiple LLM providers.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;General Data Science Agent&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://rdagent.readthedocs.io/en/latest/scens/data_science.html"&gt;Data Science Agent&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Kaggle Scenario release&lt;/td&gt; 
   &lt;td&gt;We release &lt;strong&gt;&lt;a href="https://rdagent.readthedocs.io/en/latest/scens/data_science.html"&gt;Kaggle Agent&lt;/a&gt;&lt;/strong&gt;, try the new features!&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Official WeChat group release&lt;/td&gt; 
   &lt;td&gt;We created a WeChat group, welcome to join! (üó™&lt;a href="https://github.com/microsoft/RD-Agent/issues/880"&gt;QR Code&lt;/a&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Official Discord release&lt;/td&gt; 
   &lt;td&gt;We launch our first chatting channel in Discord (üó™&lt;a href="https://discord.gg/ybQ97B6Jjy"&gt;&lt;img src="https://img.shields.io/badge/chat-discord-blue" alt="Chat" /&gt;&lt;/a&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;First release&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;R&amp;amp;D-Agent&lt;/strong&gt; is released on GitHub&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;üèÜ The Best Machine Learning Engineering Agent!&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/openai/mle-bench"&gt;MLE-bench&lt;/a&gt; is a comprehensive benchmark evaluating the performance of AI agents on machine learning engineering tasks. Utilizing datasets from 75 Kaggle competitions, MLE-bench provides robust assessments of AI systems' capabilities in real-world ML engineering scenarios.&lt;/p&gt; 
&lt;p&gt;R&amp;amp;D-Agent currently leads as the top-performing machine learning engineering agent on MLE-bench:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Agent&lt;/th&gt; 
   &lt;th&gt;Low == Lite (%)&lt;/th&gt; 
   &lt;th&gt;Medium (%)&lt;/th&gt; 
   &lt;th&gt;High (%)&lt;/th&gt; 
   &lt;th&gt;All (%)&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;R&amp;amp;D-Agent o3(R)+GPT-4.1(D)&lt;/td&gt; 
   &lt;td&gt;51.52 ¬± 6.9&lt;/td&gt; 
   &lt;td&gt;19.3 ¬± 5.5&lt;/td&gt; 
   &lt;td&gt;26.67 ¬± 0&lt;/td&gt; 
   &lt;td&gt;30.22 ¬± 1.5&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;R&amp;amp;D-Agent o1-preview&lt;/td&gt; 
   &lt;td&gt;48.18 ¬± 2.49&lt;/td&gt; 
   &lt;td&gt;8.95 ¬± 2.36&lt;/td&gt; 
   &lt;td&gt;18.67 ¬± 2.98&lt;/td&gt; 
   &lt;td&gt;22.4 ¬± 1.1&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;AIDE o1-preview&lt;/td&gt; 
   &lt;td&gt;34.3 ¬± 2.4&lt;/td&gt; 
   &lt;td&gt;8.8 ¬± 1.1&lt;/td&gt; 
   &lt;td&gt;10.0 ¬± 1.9&lt;/td&gt; 
   &lt;td&gt;16.9 ¬± 1.1&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Notes:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;O3(R)+GPT-4.1(D)&lt;/strong&gt;: This version is designed to both reduce average time per loop and leverage a cost-effective combination of backend LLMs by seamlessly integrating Research Agent (o3) with Development Agent (GPT-4.1).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;AIDE o1-preview&lt;/strong&gt;: Represents the previously best public result on MLE-bench as reported in the original MLE-bench paper.&lt;/li&gt; 
 &lt;li&gt;Average and standard deviation results for R&amp;amp;D-Agent o1-preview is based on a independent of 5 seeds and for R&amp;amp;D-Agent o3(R)+GPT-4.1(D) is based on 6 seeds.&lt;/li&gt; 
 &lt;li&gt;According to MLE-Bench, the 75 competitions are categorized into three levels of complexity: &lt;strong&gt;Low==Lite&lt;/strong&gt; if we estimate that an experienced ML engineer can produce a sensible solution in under 2 hours, excluding the time taken to train any models; &lt;strong&gt;Medium&lt;/strong&gt; if it takes between 2 and 10 hours; and &lt;strong&gt;High&lt;/strong&gt; if it takes more than 10 hours.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can inspect the detailed runs of the above results online.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://aka.ms/RD-Agent_MLE-Bench_O1-preview"&gt;R&amp;amp;D-Agent o1-preview detailed runs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aka.ms/RD-Agent_MLE-Bench_O3_GPT41"&gt;R&amp;amp;D-Agent o3(R)+GPT-4.1(D) detailed runs&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For running R&amp;amp;D-Agent on MLE-bench, refer to &lt;strong&gt;&lt;a href="https://rdagent.readthedocs.io/en/latest/scens/data_science.html"&gt;MLE-bench Guide: Running ML Engineering via MLE-bench&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h1&gt;ü•á The First Data-Centric Quant Multi-Agent Framework!&lt;/h1&gt; 
&lt;p&gt;R&amp;amp;D-Agent for Quantitative Finance, in short &lt;strong&gt;RD-Agent(Q)&lt;/strong&gt;, is the first data-centric, multi-agent framework designed to automate the full-stack research and development of quantitative strategies via coordinated factor-model co-optimization.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/3198bc10-47ba-4ee0-8a8e-46d5ce44f45d" alt="image" /&gt;&lt;/p&gt; 
&lt;p&gt;Extensive experiments in real stock markets show that, at a cost under $10, RD-Agent(Q) achieves approximately 2√ó higher ARR than benchmark factor libraries while using over 70% fewer factors. It also surpasses state-of-the-art deep time-series models under smaller resource budgets. Its alternating factor‚Äìmodel optimization further delivers excellent trade-off between predictive accuracy and strategy robustness.&lt;/p&gt; 
&lt;p&gt;You can learn more details about &lt;strong&gt;RD-Agent(Q)&lt;/strong&gt; through the &lt;a href="https://arxiv.org/abs/2505.15155"&gt;paper&lt;/a&gt; and reproduce it through the &lt;a href="https://rdagent.readthedocs.io/en/latest/scens/quant_agent_fin.html"&gt;documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Data Science Agent Preview&lt;/h1&gt; 
&lt;p&gt;Check out our demo video showcasing the current progress of our Data Science Agent under development:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/3eccbecb-34a4-4c81-bce4-d3f8862f7305"&gt;https://github.com/user-attachments/assets/3eccbecb-34a4-4c81-bce4-d3f8862f7305&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;üåü Introduction&lt;/h1&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/microsoft/RD-Agent/main/docs/_static/scen.png" alt="Our focused scenario" style="width:80%; " /&gt; 
&lt;/div&gt; 
&lt;p&gt;R&amp;amp;D-Agent aims to automate the most critical and valuable aspects of the industrial R&amp;amp;D process, and we begin with focusing on the data-driven scenarios to streamline the development of models and data. Methodologically, we have identified a framework with two key components: 'R' for proposing new ideas and 'D' for implementing them. We believe that the automatic evolution of R&amp;amp;D will lead to solutions of significant industrial value.&lt;/p&gt; 
&lt;!-- Tag Cloud --&gt; 
&lt;p&gt;R&amp;amp;D is a very general scenario. The advent of R&amp;amp;D-Agent can be your&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üí∞ &lt;strong&gt;Automatic Quant Factory&lt;/strong&gt; (&lt;a href="https://rdagent.azurewebsites.net/factor_loop"&gt;üé•Demo Video&lt;/a&gt;|&lt;a href="https://www.youtube.com/watch?v=X4DK2QZKaKY&amp;amp;t=6s"&gt;‚ñ∂Ô∏èYouTube&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;ü§ñ &lt;strong&gt;Data Mining Agent:&lt;/strong&gt; Iteratively proposing data &amp;amp; models (&lt;a href="https://rdagent.azurewebsites.net/model_loop"&gt;üé•Demo Video 1&lt;/a&gt;|&lt;a href="https://www.youtube.com/watch?v=dm0dWL49Bc0&amp;amp;t=104s"&gt;‚ñ∂Ô∏èYouTube&lt;/a&gt;) (&lt;a href="https://rdagent.azurewebsites.net/dmm"&gt;üé•Demo Video 2&lt;/a&gt;|&lt;a href="https://www.youtube.com/watch?v=VIaSTZuoZg4"&gt;‚ñ∂Ô∏èYouTube&lt;/a&gt;) and implementing them by gaining knowledge from data.&lt;/li&gt; 
 &lt;li&gt;ü¶æ &lt;strong&gt;Research Copilot:&lt;/strong&gt; Auto read research papers (&lt;a href="https://rdagent.azurewebsites.net/report_model"&gt;üé•Demo Video&lt;/a&gt;|&lt;a href="https://www.youtube.com/watch?v=BiA2SfdKQ7o"&gt;‚ñ∂Ô∏èYouTube&lt;/a&gt;) / financial reports (&lt;a href="https://rdagent.azurewebsites.net/report_factor"&gt;üé•Demo Video&lt;/a&gt;|&lt;a href="https://www.youtube.com/watch?v=ECLTXVcSx-c"&gt;‚ñ∂Ô∏èYouTube&lt;/a&gt;) and implement model structures or building datasets.&lt;/li&gt; 
 &lt;li&gt;ü§ñ &lt;strong&gt;Kaggle Agent:&lt;/strong&gt; Auto Model Tuning and Feature Engineering(&lt;a href=""&gt;üé•Demo Video Coming Soon...&lt;/a&gt;) and implementing them to achieve more in competitions.&lt;/li&gt; 
 &lt;li&gt;...&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can click the links above to view the demo. We're continuously adding more methods and scenarios to the project to enhance your R&amp;amp;D processes and boost productivity.&lt;/p&gt; 
&lt;p&gt;Additionally, you can take a closer look at the examples in our &lt;strong&gt;&lt;a href="https://rdagent.azurewebsites.net/"&gt;üñ•Ô∏è Live Demo&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://rdagent.azurewebsites.net/" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/RD-Agent/main/docs/_static/demo.png" alt="Watch the demo" width="80%" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h1&gt;‚ö° Quick start&lt;/h1&gt; 
&lt;h3&gt;RD-Agent currently only supports Linux.&lt;/h3&gt; 
&lt;p&gt;You can try above demos by running the following command:&lt;/p&gt; 
&lt;h3&gt;üê≥ Docker installation.&lt;/h3&gt; 
&lt;p&gt;Users must ensure Docker is installed before attempting most scenarios. Please refer to the &lt;a href="https://docs.docker.com/engine/install/"&gt;official üê≥Docker page&lt;/a&gt; for installation instructions. Ensure the current user can run Docker commands &lt;strong&gt;without using sudo&lt;/strong&gt;. You can verify this by executing &lt;code&gt;docker run hello-world&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;üêç Create a Conda Environment&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Create a new conda environment with Python (3.10 and 3.11 are well-tested in our CI): &lt;pre&gt;&lt;code class="language-sh"&gt;conda create -n rdagent python=3.10
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Activate the environment: &lt;pre&gt;&lt;code class="language-sh"&gt;conda activate rdagent
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üõ†Ô∏è Install the R&amp;amp;D-Agent&lt;/h3&gt; 
&lt;h4&gt;For Users&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;You can directly install the R&amp;amp;D-Agent package from PyPI: &lt;pre&gt;&lt;code class="language-sh"&gt;pip install rdagent
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;For Developers&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;If you want to try the latest version or contribute to RD-Agent, you can install it from the source and follow the development setup: &lt;pre&gt;&lt;code class="language-sh"&gt;git clone https://github.com/microsoft/RD-Agent
cd RD-Agent
make dev
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;More details can be found in the &lt;a href="https://rdagent.readthedocs.io/en/latest/development.html"&gt;development setup&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;üíä Health check&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;rdagent provides a health check that currently checks two things. 
  &lt;ul&gt; 
   &lt;li&gt;whether the docker installation was successful.&lt;/li&gt; 
   &lt;li&gt;whether the default port used by the &lt;a href="https://github.com/microsoft/RD-Agent?tab=readme-ov-file#%EF%B8%8F-monitor-the-application-results"&gt;rdagent ui&lt;/a&gt; is occupied.&lt;/li&gt; 
  &lt;/ul&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;rdagent health_check --no-check-env
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;‚öôÔ∏è Configuration&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;The demos requires following ability:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;ChatCompletion&lt;/li&gt; 
   &lt;li&gt;json_mode&lt;/li&gt; 
   &lt;li&gt;embedding query&lt;/li&gt; 
  &lt;/ul&gt; &lt;p&gt;You can set your Chat Model and Embedding Model in the following ways:&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;&lt;strong&gt;üî• Attention&lt;/strong&gt;: We now provide experimental support for &lt;strong&gt;DeepSeek&lt;/strong&gt; models! You can use DeepSeek's official API for cost-effective and high-performance inference. See the configuration example below for DeepSeek setup.&lt;/p&gt; 
  &lt;/blockquote&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Using LiteLLM (Default)&lt;/strong&gt;: We now support LiteLLM as a backend for integration with multiple LLM providers. You can configure in multiple ways:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Option 1: Unified API base for both models&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Configuration Example: &lt;code&gt;OpenAI&lt;/code&gt; Setup :&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cat &amp;lt;&amp;lt; EOF  &amp;gt; .env
# Set to any model supported by LiteLLM.
CHAT_MODEL=gpt-4o 
EMBEDDING_MODEL=text-embedding-3-small
# Configure unified API base
OPENAI_API_BASE=&amp;lt;your_unified_api_base&amp;gt;
OPENAI_API_KEY=&amp;lt;replace_with_your_openai_api_key&amp;gt;
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;Configuration Example: &lt;code&gt;Azure OpenAI&lt;/code&gt; Setup :&lt;/em&gt;&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;Before using this configuration, please confirm in advance that your &lt;code&gt;Azure OpenAI API key&lt;/code&gt; supports &lt;code&gt;embedded models&lt;/code&gt;.&lt;/p&gt; 
  &lt;/blockquote&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cat &amp;lt;&amp;lt; EOF  &amp;gt; .env
EMBEDDING_MODEL=azure/&amp;lt;Model deployment supporting embedding&amp;gt;
CHAT_MODEL=azure/&amp;lt;your deployment name&amp;gt;
AZURE_API_KEY=&amp;lt;replace_with_your_openai_api_key&amp;gt;
AZURE_API_BASE=&amp;lt;your_unified_api_base&amp;gt;
AZURE_API_VERSION=&amp;lt;azure api version&amp;gt;
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Option 2: Separate API bases for Chat and Embedding models&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cat &amp;lt;&amp;lt; EOF  &amp;gt; .env
# Set to any model supported by LiteLLM.
# Configure separate API bases for chat and embedding

# CHAT MODEL:
CHAT_MODEL=gpt-4o 
OPENAI_API_BASE=&amp;lt;your_chat_api_base&amp;gt;
OPENAI_API_KEY=&amp;lt;replace_with_your_openai_api_key&amp;gt;

# EMBEDDING MODEL:
# TAKE siliconflow as an example, you can use other providers.
# Note: embedding requires litellm_proxy prefix
EMBEDDING_MODEL=litellm_proxy/BAAI/bge-large-en-v1.5
LITELLM_PROXY_API_KEY=&amp;lt;replace_with_your_siliconflow_api_key&amp;gt;
LITELLM_PROXY_API_BASE=https://api.siliconflow.cn/v1
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;Configuration Example: &lt;code&gt;DeepSeek&lt;/code&gt; Setup :&lt;/em&gt;&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;Since many users encounter configuration errors when setting up DeepSeek. Here's a complete working example for DeepSeek Setup:&lt;/p&gt; 
  &lt;/blockquote&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cat &amp;lt;&amp;lt; EOF  &amp;gt; .env
# CHAT MODEL: Using DeepSeek Official API
CHAT_MODEL=deepseek/deepseek-chat 
DEEPSEEK_API_KEY=&amp;lt;replace_with_your_deepseek_api_key&amp;gt;

# EMBEDDING MODEL: Using SiliconFlow for embedding since deepseek has no embedding model.
# Note: embedding requires litellm_proxy prefix
EMBEDDING_MODEL=litellm_proxy/BAAI/bge-m3
LITELLM_PROXY_API_KEY=&amp;lt;replace_with_your_siliconflow_api_key&amp;gt;
LITELLM_PROXY_API_BASE=https://api.siliconflow.cn/v1
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Notice: If you are using reasoning models that include thought processes in their responses (such as &amp;lt;think&amp;gt; tags), you need to set the following environment variable:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;REASONING_THINK_RM=True
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can also use a deprecated backend if you only use &lt;code&gt;OpenAI API&lt;/code&gt; or &lt;code&gt;Azure OpenAI&lt;/code&gt; directly. For this deprecated setting and more configuration information, please refer to the &lt;a href="https://rdagent.readthedocs.io/en/latest/installation_and_configuration.html"&gt;documentation&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;If your environment configuration is complete, please execute the following commands to check if your configuration is valid. This step is necessary.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;rdagent health_check
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üöÄ Run the Application&lt;/h3&gt; 
&lt;p&gt;The &lt;strong&gt;&lt;a href="https://rdagent.azurewebsites.net/"&gt;üñ•Ô∏è Live Demo&lt;/a&gt;&lt;/strong&gt; is implemented by the following commands(each item represents one demo, you can select the one you prefer):&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Run the &lt;strong&gt;Automated Quantitative Trading &amp;amp; Iterative Factors Model Joint Evolution&lt;/strong&gt;: &lt;a href="http://github.com/microsoft/qlib"&gt;Qlib&lt;/a&gt; self-loop factor &amp;amp; model proposal and implementation application&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;rdagent fin_quant
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run the &lt;strong&gt;Automated Quantitative Trading &amp;amp; Iterative Factors Evolution&lt;/strong&gt;: &lt;a href="http://github.com/microsoft/qlib"&gt;Qlib&lt;/a&gt; self-loop factor proposal and implementation application&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;rdagent fin_factor
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run the &lt;strong&gt;Automated Quantitative Trading &amp;amp; Iterative Model Evolution&lt;/strong&gt;: &lt;a href="http://github.com/microsoft/qlib"&gt;Qlib&lt;/a&gt; self-loop model proposal and implementation application&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;rdagent fin_model
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run the &lt;strong&gt;Automated Quantitative Trading &amp;amp; Factors Extraction from Financial Reports&lt;/strong&gt;: Run the &lt;a href="http://github.com/microsoft/qlib"&gt;Qlib&lt;/a&gt; factor extraction and implementation application based on financial reports&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;# 1. Generally, you can run this scenario using the following command:
rdagent fin_factor_report --report-folder=&amp;lt;Your financial reports folder path&amp;gt;

# 2. Specifically, you need to prepare some financial reports first. You can follow this concrete example:
wget https://github.com/SunsetWolf/rdagent_resource/releases/download/reports/all_reports.zip
unzip all_reports.zip -d git_ignore_folder/reports
rdagent fin_factor_report --report-folder=git_ignore_folder/reports
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run the &lt;strong&gt;Automated Model Research &amp;amp; Development Copilot&lt;/strong&gt;: model extraction and implementation application&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;# 1. Generally, you can run your own papers/reports with the following command:
rdagent general_model &amp;lt;Your paper URL&amp;gt;

# 2. Specifically, you can do it like this. For more details and additional paper examples, use `rdagent general_model -h`:
rdagent general_model  "https://arxiv.org/pdf/2210.09789"
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run the &lt;strong&gt;Automated Medical Prediction Model Evolution&lt;/strong&gt;: Medical self-loop model proposal and implementation application&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# Generally, you can run the data science program with the following command:
rdagent data_science --competition &amp;lt;your competition name&amp;gt;

# Specifically, you need to create a folder for storing competition files (e.g., competition description file, competition datasets, etc.), and configure the path to the folder in your environment. In addition, you need to use chromedriver when you download the competition descriptors, which you can follow for this specific example:

# 1. Download the dataset, extract it to the target folder.
wget https://github.com/SunsetWolf/rdagent_resource/releases/download/ds_data/arf-12-hours-prediction-task.zip
unzip arf-12-hours-prediction-task.zip -d ./git_ignore_folder/ds_data/

# 2. Configure environment variables in the `.env` file
dotenv set DS_LOCAL_DATA_PATH "$(pwd)/git_ignore_folder/ds_data"
dotenv set DS_CODER_ON_WHOLE_PIPELINE True
dotenv set DS_IF_USING_MLE_DATA False
dotenv set DS_SAMPLE_DATA_BY_LLM False
dotenv set DS_SCEN rdagent.scenarios.data_science.scen.DataScienceScen

# 3. run the application
rdagent data_science --competition arf-12-hours-prediction-task
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; For more information about the dataset, please refer to the &lt;a href="https://rdagent.readthedocs.io/en/latest/scens/data_science.html"&gt;documentation&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run the &lt;strong&gt;Automated Kaggle Model Tuning &amp;amp; Feature Engineering&lt;/strong&gt;: self-loop model proposal and feature engineering implementation application &lt;br /&gt;&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;Using &lt;strong&gt;tabular-playground-series-dec-2021&lt;/strong&gt; as an example. &lt;br /&gt;&lt;/p&gt; 
   &lt;ol&gt; 
    &lt;li&gt;Register and login on the &lt;a href="https://www.kaggle.com/"&gt;Kaggle&lt;/a&gt; website. &lt;br /&gt;&lt;/li&gt; 
    &lt;li&gt;Configuring the Kaggle API. &lt;br /&gt; (1) Click on the avatar (usually in the top right corner of the page) -&amp;gt; &lt;code&gt;Settings&lt;/code&gt; -&amp;gt; &lt;code&gt;Create New Token&lt;/code&gt;, A file called &lt;code&gt;kaggle.json&lt;/code&gt; will be downloaded. &lt;br /&gt; (2) Move &lt;code&gt;kaggle.json&lt;/code&gt; to &lt;code&gt;~/.config/kaggle/&lt;/code&gt; &lt;br /&gt; (3) Modify the permissions of the kaggle.json file. Reference command: &lt;code&gt;chmod 600 ~/.config/kaggle/kaggle.json&lt;/code&gt; &lt;br /&gt;&lt;/li&gt; 
    &lt;li&gt;Join the competition: Click &lt;code&gt;Join the competition&lt;/code&gt; -&amp;gt; &lt;code&gt;I Understand and Accept&lt;/code&gt; at the bottom of the &lt;a href="https://www.kaggle.com/competitions/tabular-playground-series-dec-2021/data"&gt;competition details page&lt;/a&gt;.&lt;/li&gt; 
   &lt;/ol&gt; 
  &lt;/blockquote&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# Generally, you can run the Kaggle competition program with the following command:
rdagent data_science --competition &amp;lt;your competition name&amp;gt;

# 1. Configure environment variables in the `.env` file
mkdir -p ./git_ignore_folder/ds_data
dotenv set DS_LOCAL_DATA_PATH "$(pwd)/git_ignore_folder/ds_data"
dotenv set DS_CODER_ON_WHOLE_PIPELINE True
dotenv set DS_IF_USING_MLE_DATA True
dotenv set DS_SAMPLE_DATA_BY_LLM True
dotenv set DS_SCEN rdagent.scenarios.data_science.scen.KaggleScen

# 2. run the application
rdagent data_science --competition tabular-playground-series-dec-2021
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üñ•Ô∏è Monitor the Application Results&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;You can run the following command for our demo program to see the run logs.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;rdagent ui --port 19899 --log-dir &amp;lt;your log folder like "log/"&amp;gt; --data-science
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;About the &lt;code&gt;data_science&lt;/code&gt; parameter: If you want to see the logs of the data science scenario, set the &lt;code&gt;data_science&lt;/code&gt; parameter to &lt;code&gt;True&lt;/code&gt;; otherwise set it to &lt;code&gt;False&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Although port 19899 is not commonly used, but before you run this demo, you need to check if port 19899 is occupied. If it is, please change it to another port that is not occupied.&lt;/p&gt; &lt;p&gt;You can check if a port is occupied by running the following command.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;rdagent health_check --no-check-env --no-check-docker
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;üè≠ Scenarios&lt;/h1&gt; 
&lt;p&gt;We have applied R&amp;amp;D-Agent to multiple valuable data-driven industrial scenarios.&lt;/p&gt; 
&lt;h2&gt;üéØ Goal: Agent for Data-driven R&amp;amp;D&lt;/h2&gt; 
&lt;p&gt;In this project, we are aiming to build an Agent to automate Data-Driven R&amp;amp;D that can&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìÑ Read real-world material (reports, papers, etc.) and &lt;strong&gt;extract&lt;/strong&gt; key formulas, descriptions of interested &lt;strong&gt;features&lt;/strong&gt; and &lt;strong&gt;models&lt;/strong&gt;, which are the key components of data-driven R&amp;amp;D .&lt;/li&gt; 
 &lt;li&gt;üõ†Ô∏è &lt;strong&gt;Implement&lt;/strong&gt; the extracted formulas (e.g., features, factors, and models) in runnable codes. 
  &lt;ul&gt; 
   &lt;li&gt;Due to the limited ability of LLM in implementing at once, build an evolving process for the agent to improve performance by learning from feedback and knowledge.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;üí° Propose &lt;strong&gt;new ideas&lt;/strong&gt; based on current knowledge and observations.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- ![Data-Centric R&amp;D Overview](docs/_static/overview.png) --&gt; 
&lt;h2&gt;üìà Scenarios/Demos&lt;/h2&gt; 
&lt;p&gt;In the two key areas of data-driven scenarios, model implementation and data building, our system aims to serve two main roles: ü¶æCopilot and ü§ñAgent.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;The ü¶æCopilot follows human instructions to automate repetitive tasks.&lt;/li&gt; 
 &lt;li&gt;The ü§ñAgent, being more autonomous, actively proposes ideas for better results in the future.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The supported scenarios are listed below:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Scenario/Target&lt;/th&gt; 
   &lt;th&gt;Model Implementation&lt;/th&gt; 
   &lt;th&gt;Data Building&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üíπ Finance&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;ü§ñ &lt;a href="https://rdagent.azurewebsites.net/model_loop"&gt;Iteratively Proposing Ideas &amp;amp; Evolving&lt;/a&gt;&lt;a href="https://www.youtube.com/watch?v=dm0dWL49Bc0&amp;amp;t=104s"&gt;‚ñ∂Ô∏èYouTube&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ü§ñ &lt;a href="https://rdagent.azurewebsites.net/factor_loop"&gt;Iteratively Proposing Ideas &amp;amp; Evolving&lt;/a&gt; &lt;a href="https://www.youtube.com/watch?v=X4DK2QZKaKY&amp;amp;t=6s"&gt;‚ñ∂Ô∏èYouTube&lt;/a&gt; &lt;br /&gt; ü¶æ &lt;a href="https://rdagent.azurewebsites.net/report_factor"&gt;Auto reports reading &amp;amp; implementation&lt;/a&gt;&lt;a href="https://www.youtube.com/watch?v=ECLTXVcSx-c"&gt;‚ñ∂Ô∏èYouTube&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ü©∫ Medical&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;ü§ñ &lt;a href="https://rdagent.azurewebsites.net/dmm"&gt;Iteratively Proposing Ideas &amp;amp; Evolving&lt;/a&gt;&lt;a href="https://www.youtube.com/watch?v=VIaSTZuoZg4"&gt;‚ñ∂Ô∏èYouTube&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üè≠ General&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;ü¶æ &lt;a href="https://rdagent.azurewebsites.net/report_model"&gt;Auto paper reading &amp;amp; implementation&lt;/a&gt;&lt;a href="https://www.youtube.com/watch?v=BiA2SfdKQ7o"&gt;‚ñ∂Ô∏èYouTube&lt;/a&gt; &lt;br /&gt; ü§ñ Auto Kaggle Model Tuning&lt;/td&gt; 
   &lt;td&gt;ü§ñAuto Kaggle feature Engineering&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://rdagent.readthedocs.io/en/latest/scens/data_science.html#roadmap"&gt;RoadMap&lt;/a&gt;&lt;/strong&gt;: Currently, we are working hard to add new features to the Kaggle scenario.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Different scenarios vary in entrance and configuration. Please check the detailed setup tutorial in the scenarios documents.&lt;/p&gt; 
&lt;p&gt;Here is a gallery of &lt;a href="https://github.com/SunsetWolf/rdagent_resource/releases/download/demo_traces/demo_traces.zip"&gt;successful explorations&lt;/a&gt; (5 traces showed in &lt;strong&gt;&lt;a href="https://rdagent.azurewebsites.net/"&gt;üñ•Ô∏è Live Demo&lt;/a&gt;&lt;/strong&gt;). You can download and view the execution trace using &lt;a href="https://github.com/microsoft/RD-Agent?tab=readme-ov-file#%EF%B8%8F-monitor-the-application-results"&gt;this command&lt;/a&gt; from the documentation.&lt;/p&gt; 
&lt;p&gt;Please refer to &lt;strong&gt;&lt;a href="https://rdagent.readthedocs.io/en/latest/scens/catalog.html"&gt;üìñreadthedocs_scen&lt;/a&gt;&lt;/strong&gt; for more details of the scenarios.&lt;/p&gt; 
&lt;h1&gt;‚öôÔ∏è Framework&lt;/h1&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/microsoft/RD-Agent/main/docs/_static/Framework-RDAgent.png" alt="Framework-RDAgent" width="85%" /&gt; 
&lt;/div&gt; 
&lt;p&gt;Automating the R&amp;amp;D process in data science is a highly valuable yet underexplored area in industry. We propose a framework to push the boundaries of this important research field.&lt;/p&gt; 
&lt;p&gt;The research questions within this framework can be divided into three main categories:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Research Area&lt;/th&gt; 
   &lt;th&gt;Paper/Work List&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Benchmark the R&amp;amp;D abilities&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/RD-Agent/main/#benchmark"&gt;Benchmark&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Idea proposal:&lt;/strong&gt; Explore new ideas or refine existing ones&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/RD-Agent/main/#research"&gt;Research&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Ability to realize ideas:&lt;/strong&gt; Implement and execute ideas&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/RD-Agent/main/#development"&gt;Development&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;We believe that the key to delivering high-quality solutions lies in the ability to evolve R&amp;amp;D capabilities. Agents should learn like human experts, continuously improving their R&amp;amp;D skills.&lt;/p&gt; 
&lt;p&gt;More documents can be found in the &lt;strong&gt;&lt;a href="https://rdagent.readthedocs.io/"&gt;üìñ readthedocs&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; 
&lt;h1&gt;üìÉ Paper/Work list&lt;/h1&gt; 
&lt;h2&gt;Overall Technical Report&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/2505.14738"&gt;R&amp;amp;D-Agent: Automating Data-Driven AI Solution Building Through LLM-Powered Automated Research, Development, and Evolution&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-BibTeX"&gt;@misc{yang2024rdagent,
    title={R\&amp;amp;D-Agent: Automating Data-Driven AI Solution Building Through LLM-Powered Automated Research, Development, and Evolution},
    author={Xu Yang and Xiao Yang and Shikai Fang and Bowen Xian and Yuante Li and Jian Wang and Minrui Xu and Haoran Pan and Xinpeng Hong and Weiqing Liu and Yelong Shen and Weizhu Chen and Jiang Bian},
    year={2025},
    eprint={2505.14738},
    archivePrefix={arXiv},
    primaryClass={cs.AI},
    url={https://arxiv.org/abs/2505.14738}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/28b0488d-a546-4fef-8dc5-563ed64a9b4d" alt="image" /&gt;&lt;/p&gt; 
&lt;h2&gt;üìä Benchmark&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/2404.11276"&gt;Towards Data-Centric Automatic R&amp;amp;D&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-BibTeX"&gt;@misc{chen2024datacentric,
    title={Towards Data-Centric Automatic R&amp;amp;D},
    author={Haotian Chen and Xinjie Shen and Zeqi Ye and Wenjun Feng and Haoxue Wang and Xiao Yang and Xu Yang and Weiqing Liu and Jiang Bian},
    year={2024},
    eprint={2404.11276},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/494f55d3-de9e-4e73-ba3d-a787e8f9e841" alt="image" /&gt;&lt;/p&gt; 
&lt;h2&gt;üîç Research&lt;/h2&gt; 
&lt;p&gt;In a data mining expert's daily research and development process, they propose a hypothesis (e.g., a model structure like RNN can capture patterns in time-series data), design experiments (e.g., finance data contains time-series and we can verify the hypothesis in this scenario), implement the experiment as code (e.g., Pytorch model structure), and then execute the code to get feedback (e.g., metrics, loss curve, etc.). The experts learn from the feedback and improve in the next iteration.&lt;/p&gt; 
&lt;p&gt;Based on the principles above, we have established a basic method framework that continuously proposes hypotheses, verifies them, and gets feedback from the real-world practice. This is the first scientific research automation framework that supports linking with real-world verification.&lt;/p&gt; 
&lt;p&gt;For more detail, please refer to our &lt;strong&gt;&lt;a href="https://rdagent.azurewebsites.net"&gt;üñ•Ô∏è Live Demo page&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; 
&lt;h2&gt;üõ†Ô∏è Development&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/2407.18690"&gt;Collaborative Evolving Strategy for Automatic Data-Centric Development&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-BibTeX"&gt;@misc{yang2024collaborative,
    title={Collaborative Evolving Strategy for Automatic Data-Centric Development},
    author={Xu Yang and Haotian Chen and Wenjun Feng and Haoxue Wang and Zeqi Ye and Xinjie Shen and Xiao Yang and Shizhao Sun and Weiqing Liu and Jiang Bian},
    year={2024},
    eprint={2407.18690},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/75d9769b-0edd-4caf-9d45-57d1e577054b" alt="image" /&gt;&lt;/p&gt; 
&lt;h2&gt;Deep Application in Diverse Scenarios&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/2505.15155"&gt;R&amp;amp;D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-BibTeX"&gt;@misc{li2025rdagentquant,
    title={R\&amp;amp;D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization},
    author={Yuante Li and Xu Yang and Xiao Yang and Minrui Xu and Xisen Wang and Weiqing Liu and Jiang Bian},
    year={2025},
    eprint={2505.15155},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/3186f67a-c2f8-4b6b-8bb9-a9b959c13866" alt="image" /&gt;&lt;/p&gt; 
&lt;h1&gt;ü§ù Contributing&lt;/h1&gt; 
&lt;p&gt;We welcome contributions and suggestions to improve R&amp;amp;D-Agent. Please refer to the &lt;a href="https://raw.githubusercontent.com/microsoft/RD-Agent/main/CONTRIBUTING.md"&gt;Contributing Guide&lt;/a&gt; for more details on how to contribute.&lt;/p&gt; 
&lt;p&gt;Before submitting a pull request, ensure that your code passes the automatic CI checks.&lt;/p&gt; 
&lt;h2&gt;üìù Guidelines&lt;/h2&gt; 
&lt;p&gt;This project welcomes contributions and suggestions. Contributing to this project is straightforward and rewarding. Whether it's solving an issue, addressing a bug, enhancing documentation, or even correcting a typo, every contribution is valuable and helps improve R&amp;amp;D-Agent.&lt;/p&gt; 
&lt;p&gt;To get started, you can explore the issues list, or search for &lt;code&gt;TODO:&lt;/code&gt; comments in the codebase by running the command &lt;code&gt;grep -r "TODO:"&lt;/code&gt;.&lt;/p&gt; 
&lt;img src="https://img.shields.io/github/contributors-anon/microsoft/RD-Agent" /&gt; 
&lt;a href="https://github.com/microsoft/RD-Agent/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=microsoft/RD-Agent&amp;amp;max=100&amp;amp;columns=15" /&gt; &lt;/a&gt; 
&lt;p&gt;Before we released R&amp;amp;D-Agent as an open-source project on GitHub, it was an internal project within our group. Unfortunately, the internal commit history was not preserved when we removed some confidential code. As a result, some contributions from our group members, including Haotian Chen, Wenjun Feng, Haoxue Wang, Zeqi Ye, Xinjie Shen, and Jinhui Li, were not included in the public commits.&lt;/p&gt; 
&lt;h1&gt;‚öñÔ∏è Legal disclaimer&lt;/h1&gt; 
&lt;p style="line-height: 1; font-style: italic;"&gt;The RD-agent is provided ‚Äúas is‚Äù, without warranty of any kind, express or implied, including but not limited to the warranties of merchantability, fitness for a particular purpose and noninfringement. The RD-agent is aimed to facilitate research and development process in the financial industry and not ready-to-use for any financial investment or advice. Users shall independently assess and test the risks of the RD-agent in a specific use scenario, ensure the responsible use of AI technology, including but not limited to developing and integrating risk mitigation measures, and comply with all applicable laws and regulations in all applicable jurisdictions. The RD-agent does not provide financial opinions or reflect the opinions of Microsoft, nor is it designed to replace the role of qualified financial professionals in formulating, assessing, and approving finance products. The inputs and outputs of the RD-agent belong to the users and users shall assume all liability under any theory of liability, whether in contract, torts, regulatory, negligence, products liability, or otherwise, associated with use of the RD-agent and any inputs and outputs thereof.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>TibixDev/winboat</title>
      <link>https://github.com/TibixDev/winboat</link>
      <description>&lt;p&gt;Run Windows apps on üêß Linux with ‚ú® seamless integration&lt;/p&gt;&lt;hr&gt;&lt;div align="left"&gt; 
 &lt;table&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td&gt; &lt;img src="https://raw.githubusercontent.com/TibixDev/winboat/main/gh-assets/winboat_logo.svg?sanitize=true" alt="WinBoat Logo" width="150" /&gt; &lt;/td&gt; 
    &lt;td&gt; &lt;h1 style="color: #7C86FF; margin: 0; font-size: 32px;"&gt;WinBoat&lt;/h1&gt; &lt;p style="color: oklch(90% 0 0); font-size: 14px; margin: 5px 0;"&gt;Windows for Penguins.&lt;br /&gt; Run Windows apps on üêß Linux with ‚ú® seamless integration&lt;/p&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h2&gt;Screenshots&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/TibixDev/winboat/main/gh-assets/features/feat_dash.png" alt="WinBoat Dashboard" width="45%" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/TibixDev/winboat/main/gh-assets/features/feat_apps.png" alt="WinBoat Apps" width="45%" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/TibixDev/winboat/main/gh-assets/features/feat_native.png" alt="Native Windows" width="45%" /&gt; 
&lt;/div&gt; 
&lt;h2&gt;‚ö†Ô∏è Work in Progress ‚ö†Ô∏è&lt;/h2&gt; 
&lt;p&gt;WinBoat is currently in beta, so expect to occasionally run into hiccups and bugs. You should be comfortable with some level of troubleshooting if you decide to try it, however we encourage you to give it a shot anyway.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;üé® Elegant Interface&lt;/strong&gt;: Sleek and intuitive interface that seamlessly integrates Windows into your Linux desktop environment, making it feel like a native experience&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üì¶ Automated Installs&lt;/strong&gt;: Simple installation process through our interface - pick your preferences &amp;amp; specs and let us handle the rest&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üöÄ Run Any App&lt;/strong&gt;: If it runs on Windows, it can run on WinBoat. Enjoy the full range of Windows applications as native OS-level windows in your Linux environment&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üñ•Ô∏è Full Windows Desktop&lt;/strong&gt;: Access the complete Windows desktop experience when you need it, or run individual apps seamlessly integrated into your Linux workflow&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üìÅ Filesystem Integration&lt;/strong&gt;: Your home directory is mounted in Windows, allowing easy file sharing between the two systems without any hassle&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;‚ú® And many more&lt;/strong&gt;: Smartcard passthrough, resource monitoring, and more features being added regularly&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;How Does It Work?&lt;/h2&gt; 
&lt;p&gt;WinBoat is an Electron app which allows you to run Windows apps on Linux using a containerized approach. Windows runs as a VM inside a Docker container, we communicate with it using the &lt;a href="https://github.com/TibixDev/winboat/tree/main/guest_server"&gt;WinBoat Guest Server&lt;/a&gt; to retrieve data we need from Windows. For compositing applications as native OS-level windows, we use FreeRDP together with Windows's RemoteApp protocol.&lt;/p&gt; 
&lt;h2&gt;Prerequisites&lt;/h2&gt; 
&lt;p&gt;Before running WinBoat, ensure your system meets the following requirements:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;RAM&lt;/strong&gt;: At least 4 GB of RAM&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;CPU&lt;/strong&gt;: At least 2 CPU threads&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Storage&lt;/strong&gt;: At least 32 GB free space on the drive your selected install folder corresponds to&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Virtualization&lt;/strong&gt;: KVM enabled in BIOS/UEFI 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://duckduckgo.com/?t=h_&amp;amp;q=how+to+enable+virtualization+in+%3Cmotherboard+brand%3E+bios&amp;amp;ia=web"&gt;How to enable virtualization&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Docker&lt;/strong&gt;: Required for containerization 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://docs.docker.com/engine/install/"&gt;Installation Guide&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;‚ö†Ô∏è NOTE:&lt;/strong&gt; Docker Desktop is &lt;strong&gt;not&lt;/strong&gt; supported, you will run into issues if you use it&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Docker Compose v2&lt;/strong&gt;: Required for compatibility with docker-compose.yml files 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://docs.docker.com/compose/install/#plugin-linux-only"&gt;Installation Guide&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Docker User Group&lt;/strong&gt;: Add your user to the &lt;code&gt;docker&lt;/code&gt; group 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://docs.docker.com/engine/install/linux-postinstall/#manage-docker-as-a-non-root-user"&gt;Setup Instructions&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;FreeRDP&lt;/strong&gt;: Required for remote desktop connection (Please make sure you have &lt;strong&gt;Version 3.x.x&lt;/strong&gt; with sound support included) 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://github.com/FreeRDP/FreeRDP/wiki/PreBuilds"&gt;Installation Guide&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;[OPTIONAL] &lt;strong&gt;Kernel Modules&lt;/strong&gt;: The &lt;code&gt;iptables&lt;/code&gt; / &lt;code&gt;nftables&lt;/code&gt; and &lt;code&gt;iptable_nat&lt;/code&gt; kernel modules can be loaded for network autodiscovery and better shared filesystem performance, but this is not obligatory in newer versions of WinBoat 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://rentry.org/rmfq2e5e"&gt;Module loading instructions&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Downloading&lt;/h2&gt; 
&lt;p&gt;You can download the latest Linux builds under the &lt;a href="https://github.com/TibixDev/winboat/releases"&gt;Releases&lt;/a&gt; tab. We currently offer four variants:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;AppImage:&lt;/strong&gt; A popular &amp;amp; portable app format which should run fine on most distributions&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Unpacked:&lt;/strong&gt; The raw unpacked files, simply run the executable (&lt;code&gt;linux-unpacked/winboat&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;.deb:&lt;/strong&gt; The intended format for Debian based distributions&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;.rpm:&lt;/strong&gt; The intended format for Fedora based distributions&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Known Issues About Container Runtimes&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Podman is &lt;strong&gt;unsupported&lt;/strong&gt; for now&lt;/li&gt; 
 &lt;li&gt;Docker Desktop is &lt;strong&gt;unsupported&lt;/strong&gt; for now&lt;/li&gt; 
 &lt;li&gt;Distros that emulate Docker through a Podman socket are &lt;strong&gt;unsupported&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Any rootless containerization solution is currently &lt;strong&gt;unsupported&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Building WinBoat&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;For building you need to have NodeJS and Go installed on your system&lt;/li&gt; 
 &lt;li&gt;Clone the repo (&lt;code&gt;git clone https://github.com/TibixDev/WinBoat&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Install the dependencies (&lt;code&gt;npm i&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Build the app and the guest server using &lt;code&gt;npm run build:linux-gs&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;You can now find the built app under &lt;code&gt;dist&lt;/code&gt; with an AppImage and an Unpacked variant&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Running WinBoat in development mode&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Make sure you meet the &lt;a href="https://raw.githubusercontent.com/TibixDev/winboat/main/#prerequisites"&gt;prerequisites&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Additionally, for development you need to have NodeJS and Go installed on your system&lt;/li&gt; 
 &lt;li&gt;Clone the repo (&lt;code&gt;git clone https://github.com/TibixDev/WinBoat&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Install the dependencies (&lt;code&gt;npm i&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Build the guest server (&lt;code&gt;npm run build-guest-server&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Run the app (&lt;code&gt;npm run dev&lt;/code&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Contributions are welcome! Whether it's bug fixes, feature improvements, or documentation updates, we appreciate your help making WinBoat better.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Please note&lt;/strong&gt;: We maintain a focus on technical contributions only. Pull requests containing political/sexual content, or other sensitive/controversial topics will not be accepted. Let's keep things focused on making great software! üöÄ&lt;/p&gt; 
&lt;p&gt;Feel free to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Report bugs and issues&lt;/li&gt; 
 &lt;li&gt;Submit feature requests&lt;/li&gt; 
 &lt;li&gt;Contribute code improvements&lt;/li&gt; 
 &lt;li&gt;Help with documentation&lt;/li&gt; 
 &lt;li&gt;Share feedback and suggestions&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Check out our issues page to get started, or feel free to open a new issue if you've found something that needs attention.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;WinBoat is licensed under the &lt;a href="https://github.com/TibixDev/winboat/raw/main/LICENSE"&gt;MIT&lt;/a&gt; license&lt;/p&gt; 
&lt;h2&gt;Inspiration / Alternatives&lt;/h2&gt; 
&lt;p&gt;These past few years some cool projects have surfaced with similar concepts, some of which we've also taken inspirations from.&lt;br /&gt; They're awesome and you should check them out:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/winapps-org/winapps"&gt;WinApps&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/casualsnek/cassowary"&gt;Cassowary&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/dockur/windows"&gt;dockur/windows&lt;/a&gt; (üåü Also used in WinBoat)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Socials &amp;amp; Contact&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.winboat.app/"&gt;&lt;img src="https://img.shields.io/badge/Website-winboat.app-blue?style=flat&amp;amp;logo=googlechrome&amp;amp;logoColor=white" alt="Website" /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://x.com/winboat_app"&gt;&lt;img src="https://img.shields.io/badge/Twitter-@winboat__app-1DA1F2?style=flat&amp;amp;logo=x&amp;amp;logoColor=white" alt="Twitter" /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://fosstodon.org/@winboat"&gt;&lt;img src="https://img.shields.io/badge/Mastodon-@winboat-6364FF?style=flat&amp;amp;logo=mastodon&amp;amp;logoColor=white" alt="Mastodon" /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://bsky.app/profile/winboat.app"&gt;&lt;img src="https://img.shields.io/badge/Bluesky-winboat.app-00A8E8?style=flat&amp;amp;logo=bluesky&amp;amp;logoColor=white" alt="Bluesky" /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://discord.gg/MEwmpWm4tN"&gt;&lt;img src="https://img.shields.io/badge/Discord-Join_Community-5865F2?style=flat&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="mailto:staff@winboat.app"&gt;&lt;img src="https://img.shields.io/badge/Email-staff@winboat.app-D14836?style=flat&amp;amp;logo=gmail&amp;amp;logoColor=white" alt="Email" /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://deepwiki.com/TibixDev/winboat"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki" /&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;a href="https://www.star-history.com/#tibixdev/winboat&amp;amp;Date"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=tibixdev/winboat&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=tibixdev/winboat&amp;amp;type=Date" /&gt; 
  &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=tibixdev/winboat&amp;amp;type=Date" /&gt; 
 &lt;/picture&gt; &lt;/a&gt;</description>
    </item>
    
    <item>
      <title>karpathy/nanoGPT</title>
      <link>https://github.com/karpathy/nanoGPT</link>
      <description>&lt;p&gt;The simplest, fastest repository for training/finetuning medium-sized GPTs.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;nanoGPT&lt;/h1&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/karpathy/nanoGPT/master/assets/nanogpt.jpg" alt="nanoGPT" /&gt;&lt;/p&gt; 
&lt;p&gt;The simplest, fastest repository for training/finetuning medium-sized GPTs. It is a rewrite of &lt;a href="https://github.com/karpathy/minGPT"&gt;minGPT&lt;/a&gt; that prioritizes teeth over education. Still under active development, but currently the file &lt;code&gt;train.py&lt;/code&gt; reproduces GPT-2 (124M) on OpenWebText, running on a single 8XA100 40GB node in about 4 days of training. The code itself is plain and readable: &lt;code&gt;train.py&lt;/code&gt; is a ~300-line boilerplate training loop and &lt;code&gt;model.py&lt;/code&gt; a ~300-line GPT model definition, which can optionally load the GPT-2 weights from OpenAI. That's it.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/karpathy/nanoGPT/master/assets/gpt2_124M_loss.png" alt="repro124m" /&gt;&lt;/p&gt; 
&lt;p&gt;Because the code is so simple, it is very easy to hack to your needs, train new models from scratch, or finetune pretrained checkpoints (e.g. biggest one currently available as a starting point would be the GPT-2 1.3B model from OpenAI).&lt;/p&gt; 
&lt;h2&gt;install&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;pip install torch numpy transformers datasets tiktoken wandb tqdm
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Dependencies:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://pytorch.org"&gt;pytorch&lt;/a&gt; &amp;lt;3&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://numpy.org/install/"&gt;numpy&lt;/a&gt; &amp;lt;3&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;transformers&lt;/code&gt; for huggingface transformers &amp;lt;3 (to load GPT-2 checkpoints)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;datasets&lt;/code&gt; for huggingface datasets &amp;lt;3 (if you want to download + preprocess OpenWebText)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;tiktoken&lt;/code&gt; for OpenAI's fast BPE code &amp;lt;3&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;wandb&lt;/code&gt; for optional logging &amp;lt;3&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;tqdm&lt;/code&gt; for progress bars &amp;lt;3&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;quick start&lt;/h2&gt; 
&lt;p&gt;If you are not a deep learning professional and you just want to feel the magic and get your feet wet, the fastest way to get started is to train a character-level GPT on the works of Shakespeare. First, we download it as a single (1MB) file and turn it from raw text into one large stream of integers:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python data/shakespeare_char/prepare.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This creates a &lt;code&gt;train.bin&lt;/code&gt; and &lt;code&gt;val.bin&lt;/code&gt; in that data directory. Now it is time to train your GPT. The size of it very much depends on the computational resources of your system:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;I have a GPU&lt;/strong&gt;. Great, we can quickly train a baby GPT with the settings provided in the &lt;a href="https://raw.githubusercontent.com/karpathy/nanoGPT/master/config/train_shakespeare_char.py"&gt;config/train_shakespeare_char.py&lt;/a&gt; config file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python train.py config/train_shakespeare_char.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you peek inside it, you'll see that we're training a GPT with a context size of up to 256 characters, 384 feature channels, and it is a 6-layer Transformer with 6 heads in each layer. On one A100 GPU this training run takes about 3 minutes and the best validation loss is 1.4697. Based on the configuration, the model checkpoints are being written into the &lt;code&gt;--out_dir&lt;/code&gt; directory &lt;code&gt;out-shakespeare-char&lt;/code&gt;. So once the training finishes we can sample from the best model by pointing the sampling script at this directory:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python sample.py --out_dir=out-shakespeare-char
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This generates a few samples, for example:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;ANGELO:
And cowards it be strawn to my bed,
And thrust the gates of my threats,
Because he that ale away, and hang'd
An one with him.

DUKE VINCENTIO:
I thank your eyes against it.

DUKE VINCENTIO:
Then will answer him to save the malm:
And what have you tyrannous shall do this?

DUKE VINCENTIO:
If you have done evils of all disposition
To end his power, the day of thrust for a common men
That I leave, to fight with over-liking
Hasting in a roseman.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;lol &lt;code&gt;¬Ø\_(„ÉÑ)_/¬Ø&lt;/code&gt;. Not bad for a character-level model after 3 minutes of training on a GPU. Better results are quite likely obtainable by instead finetuning a pretrained GPT-2 model on this dataset (see finetuning section later).&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;I only have a macbook&lt;/strong&gt; (or other cheap computer). No worries, we can still train a GPT but we want to dial things down a notch. I recommend getting the bleeding edge PyTorch nightly (&lt;a href="https://pytorch.org/get-started/locally/"&gt;select it here&lt;/a&gt; when installing) as it is currently quite likely to make your code more efficient. But even without it, a simple train run could look as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python train.py config/train_shakespeare_char.py --device=cpu --compile=False --eval_iters=20 --log_interval=1 --block_size=64 --batch_size=12 --n_layer=4 --n_head=4 --n_embd=128 --max_iters=2000 --lr_decay_iters=2000 --dropout=0.0
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Here, since we are running on CPU instead of GPU we must set both &lt;code&gt;--device=cpu&lt;/code&gt; and also turn off PyTorch 2.0 compile with &lt;code&gt;--compile=False&lt;/code&gt;. Then when we evaluate we get a bit more noisy but faster estimate (&lt;code&gt;--eval_iters=20&lt;/code&gt;, down from 200), our context size is only 64 characters instead of 256, and the batch size only 12 examples per iteration, not 64. We'll also use a much smaller Transformer (4 layers, 4 heads, 128 embedding size), and decrease the number of iterations to 2000 (and correspondingly usually decay the learning rate to around max_iters with &lt;code&gt;--lr_decay_iters&lt;/code&gt;). Because our network is so small we also ease down on regularization (&lt;code&gt;--dropout=0.0&lt;/code&gt;). This still runs in about ~3 minutes, but gets us a loss of only 1.88 and therefore also worse samples, but it's still good fun:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python sample.py --out_dir=out-shakespeare-char --device=cpu
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Generates samples like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;GLEORKEN VINGHARD III:
Whell's the couse, the came light gacks,
And the for mought you in Aut fries the not high shee
bot thou the sought bechive in that to doth groan you,
No relving thee post mose the wear
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Not bad for ~3 minutes on a CPU, for a hint of the right character gestalt. If you're willing to wait longer, feel free to tune the hyperparameters, increase the size of the network, the context length (&lt;code&gt;--block_size&lt;/code&gt;), the length of training, etc.&lt;/p&gt; 
&lt;p&gt;Finally, on Apple Silicon Macbooks and with a recent PyTorch version make sure to add &lt;code&gt;--device=mps&lt;/code&gt; (short for "Metal Performance Shaders"); PyTorch then uses the on-chip GPU that can &lt;em&gt;significantly&lt;/em&gt; accelerate training (2-3X) and allow you to use larger networks. See &lt;a href="https://github.com/karpathy/nanoGPT/issues/28"&gt;Issue 28&lt;/a&gt; for more.&lt;/p&gt; 
&lt;h2&gt;reproducing GPT-2&lt;/h2&gt; 
&lt;p&gt;A more serious deep learning professional may be more interested in reproducing GPT-2 results. So here we go - we first tokenize the dataset, in this case the &lt;a href="https://openwebtext2.readthedocs.io/en/latest/"&gt;OpenWebText&lt;/a&gt;, an open reproduction of OpenAI's (private) WebText:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python data/openwebtext/prepare.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This downloads and tokenizes the &lt;a href="https://huggingface.co/datasets/openwebtext"&gt;OpenWebText&lt;/a&gt; dataset. It will create a &lt;code&gt;train.bin&lt;/code&gt; and &lt;code&gt;val.bin&lt;/code&gt; which holds the GPT2 BPE token ids in one sequence, stored as raw uint16 bytes. Then we're ready to kick off training. To reproduce GPT-2 (124M) you'll want at least an 8X A100 40GB node and run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;torchrun --standalone --nproc_per_node=8 train.py config/train_gpt2.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will run for about 4 days using PyTorch Distributed Data Parallel (DDP) and go down to loss of ~2.85. Now, a GPT-2 model just evaluated on OWT gets a val loss of about 3.11, but if you finetune it it will come down to ~2.85 territory (due to an apparent domain gap), making the two models ~match.&lt;/p&gt; 
&lt;p&gt;If you're in a cluster environment and you are blessed with multiple GPU nodes you can make GPU go brrrr e.g. across 2 nodes like:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Run on the first (master) node with example IP 123.456.123.456:
torchrun --nproc_per_node=8 --nnodes=2 --node_rank=0 --master_addr=123.456.123.456 --master_port=1234 train.py
# Run on the worker node:
torchrun --nproc_per_node=8 --nnodes=2 --node_rank=1 --master_addr=123.456.123.456 --master_port=1234 train.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;It is a good idea to benchmark your interconnect (e.g. iperf3). In particular, if you don't have Infiniband then also prepend &lt;code&gt;NCCL_IB_DISABLE=1&lt;/code&gt; to the above launches. Your multinode training will work, but most likely &lt;em&gt;crawl&lt;/em&gt;. By default checkpoints are periodically written to the &lt;code&gt;--out_dir&lt;/code&gt;. We can sample from the model by simply &lt;code&gt;python sample.py&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Finally, to train on a single GPU simply run the &lt;code&gt;python train.py&lt;/code&gt; script. Have a look at all of its args, the script tries to be very readable, hackable and transparent. You'll most likely want to tune a number of those variables depending on your needs.&lt;/p&gt; 
&lt;h2&gt;baselines&lt;/h2&gt; 
&lt;p&gt;OpenAI GPT-2 checkpoints allow us to get some baselines in place for openwebtext. We can get the numbers as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;$ python train.py config/eval_gpt2.py
$ python train.py config/eval_gpt2_medium.py
$ python train.py config/eval_gpt2_large.py
$ python train.py config/eval_gpt2_xl.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;and observe the following losses on train and val:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;model&lt;/th&gt; 
   &lt;th&gt;params&lt;/th&gt; 
   &lt;th&gt;train loss&lt;/th&gt; 
   &lt;th&gt;val loss&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;gpt2&lt;/td&gt; 
   &lt;td&gt;124M&lt;/td&gt; 
   &lt;td&gt;3.11&lt;/td&gt; 
   &lt;td&gt;3.12&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;gpt2-medium&lt;/td&gt; 
   &lt;td&gt;350M&lt;/td&gt; 
   &lt;td&gt;2.85&lt;/td&gt; 
   &lt;td&gt;2.84&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;gpt2-large&lt;/td&gt; 
   &lt;td&gt;774M&lt;/td&gt; 
   &lt;td&gt;2.66&lt;/td&gt; 
   &lt;td&gt;2.67&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;gpt2-xl&lt;/td&gt; 
   &lt;td&gt;1558M&lt;/td&gt; 
   &lt;td&gt;2.56&lt;/td&gt; 
   &lt;td&gt;2.54&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;However, we have to note that GPT-2 was trained on (closed, never released) WebText, while OpenWebText is just a best-effort open reproduction of this dataset. This means there is a dataset domain gap. Indeed, taking the GPT-2 (124M) checkpoint and finetuning on OWT directly for a while reaches loss down to ~2.85. This then becomes the more appropriate baseline w.r.t. reproduction.&lt;/p&gt; 
&lt;h2&gt;finetuning&lt;/h2&gt; 
&lt;p&gt;Finetuning is no different than training, we just make sure to initialize from a pretrained model and train with a smaller learning rate. For an example of how to finetune a GPT on new text go to &lt;code&gt;data/shakespeare&lt;/code&gt; and run &lt;code&gt;prepare.py&lt;/code&gt; to download the tiny shakespeare dataset and render it into a &lt;code&gt;train.bin&lt;/code&gt; and &lt;code&gt;val.bin&lt;/code&gt;, using the OpenAI BPE tokenizer from GPT-2. Unlike OpenWebText this will run in seconds. Finetuning can take very little time, e.g. on a single GPU just a few minutes. Run an example finetuning like:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python train.py config/finetune_shakespeare.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will load the config parameter overrides in &lt;code&gt;config/finetune_shakespeare.py&lt;/code&gt; (I didn't tune them much though). Basically, we initialize from a GPT2 checkpoint with &lt;code&gt;init_from&lt;/code&gt; and train as normal, except shorter and with a small learning rate. If you're running out of memory try decreasing the model size (they are &lt;code&gt;{'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}&lt;/code&gt;) or possibly decreasing the &lt;code&gt;block_size&lt;/code&gt; (context length). The best checkpoint (lowest validation loss) will be in the &lt;code&gt;out_dir&lt;/code&gt; directory, e.g. in &lt;code&gt;out-shakespeare&lt;/code&gt; by default, per the config file. You can then run the code in &lt;code&gt;sample.py --out_dir=out-shakespeare&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;THEODORE:
Thou shalt sell me to the highest bidder: if I die,
I sell thee to the first; if I go mad,
I sell thee to the second; if I
lie, I sell thee to the third; if I slay,
I sell thee to the fourth: so buy or sell,
I tell thee again, thou shalt not sell my
possession.

JULIET:
And if thou steal, thou shalt not sell thyself.

THEODORE:
I do not steal; I sell the stolen goods.

THEODORE:
Thou know'st not what thou sell'st; thou, a woman,
Thou art ever a victim, a thing of no worth:
Thou hast no right, no right, but to be sold.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Whoa there, GPT, entering some dark place over there. I didn't really tune the hyperparameters in the config too much, feel free to try!&lt;/p&gt; 
&lt;h2&gt;sampling / inference&lt;/h2&gt; 
&lt;p&gt;Use the script &lt;code&gt;sample.py&lt;/code&gt; to sample either from pre-trained GPT-2 models released by OpenAI, or from a model you trained yourself. For example, here is a way to sample from the largest available &lt;code&gt;gpt2-xl&lt;/code&gt; model:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python sample.py \
    --init_from=gpt2-xl \
    --start="What is the answer to life, the universe, and everything?" \
    --num_samples=5 --max_new_tokens=100
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you'd like to sample from a model you trained, use the &lt;code&gt;--out_dir&lt;/code&gt; to point the code appropriately. You can also prompt the model with some text from a file, e.g. &lt;code&gt;python sample.py --start=FILE:prompt.txt&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;efficiency notes&lt;/h2&gt; 
&lt;p&gt;For simple model benchmarking and profiling, &lt;code&gt;bench.py&lt;/code&gt; might be useful. It's identical to what happens in the meat of the training loop of &lt;code&gt;train.py&lt;/code&gt;, but omits much of the other complexities.&lt;/p&gt; 
&lt;p&gt;Note that the code by default uses &lt;a href="https://pytorch.org/get-started/pytorch-2.0/"&gt;PyTorch 2.0&lt;/a&gt;. At the time of writing (Dec 29, 2022) this makes &lt;code&gt;torch.compile()&lt;/code&gt; available in the nightly release. The improvement from the one line of code is noticeable, e.g. cutting down iteration time from ~250ms / iter to 135ms / iter. Nice work PyTorch team!&lt;/p&gt; 
&lt;h2&gt;todos&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Investigate and add FSDP instead of DDP&lt;/li&gt; 
 &lt;li&gt;Eval zero-shot perplexities on standard evals (e.g. LAMBADA? HELM? etc.)&lt;/li&gt; 
 &lt;li&gt;Finetune the finetuning script, I think the hyperparams are not great&lt;/li&gt; 
 &lt;li&gt;Schedule for linear batch size increase during training&lt;/li&gt; 
 &lt;li&gt;Incorporate other embeddings (rotary, alibi)&lt;/li&gt; 
 &lt;li&gt;Separate out the optim buffers from model params in checkpoints I think&lt;/li&gt; 
 &lt;li&gt;Additional logging around network health (e.g. gradient clip events, magnitudes)&lt;/li&gt; 
 &lt;li&gt;Few more investigations around better init etc.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;troubleshooting&lt;/h2&gt; 
&lt;p&gt;Note that by default this repo uses PyTorch 2.0 (i.e. &lt;code&gt;torch.compile&lt;/code&gt;). This is fairly new and experimental, and not yet available on all platforms (e.g. Windows). If you're running into related error messages try to disable this by adding &lt;code&gt;--compile=False&lt;/code&gt; flag. This will slow down the code but at least it will run.&lt;/p&gt; 
&lt;p&gt;For some context on this repository, GPT, and language modeling it might be helpful to watch my &lt;a href="https://karpathy.ai/zero-to-hero.html"&gt;Zero To Hero series&lt;/a&gt;. Specifically, the &lt;a href="https://www.youtube.com/watch?v=kCc8FmEb1nY"&gt;GPT video&lt;/a&gt; is popular if you have some prior language modeling context.&lt;/p&gt; 
&lt;p&gt;For more questions/discussions feel free to stop by &lt;strong&gt;#nanoGPT&lt;/strong&gt; on Discord:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://discord.gg/3zy8kqD9Cp"&gt;&lt;img src="https://dcbadge.vercel.app/api/server/3zy8kqD9Cp?compact=true&amp;amp;style=flat" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;acknowledgements&lt;/h2&gt; 
&lt;p&gt;All nanoGPT experiments are powered by GPUs on &lt;a href="https://lambdalabs.com"&gt;Lambda labs&lt;/a&gt;, my favorite Cloud GPU provider. Thank you Lambda labs for sponsoring nanoGPT!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>hsliuping/TradingAgents-CN</title>
      <link>https://github.com/hsliuping/TradingAgents-CN</link>
      <description>&lt;p&gt;Âü∫‰∫éÂ§öÊô∫ËÉΩ‰ΩìLLMÁöÑ‰∏≠ÊñáÈáëËûç‰∫§ÊòìÊ°ÜÊû∂ - TradingAgents‰∏≠ÊñáÂ¢ûÂº∫Áâà&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;TradingAgents ‰∏≠ÊñáÂ¢ûÂº∫Áâà&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://opensource.org/licenses/Apache-2.0"&gt;&lt;img src="https://img.shields.io/badge/License-Apache%202.0-blue.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://www.python.org/"&gt;&lt;img src="https://img.shields.io/badge/Python-3.10%2B-blue.svg?sanitize=true" alt="Python" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/VERSION"&gt;&lt;img src="https://img.shields.io/badge/Version-cn--0.1.15-green.svg?sanitize=true" alt="Version" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/"&gt;&lt;img src="https://img.shields.io/badge/docs-%E4%B8%AD%E6%96%87%E6%96%87%E6%A1%A3-green.svg?sanitize=true" alt="Documentation" /&gt;&lt;/a&gt; &lt;a href="https://github.com/TauricResearch/TradingAgents"&gt;&lt;img src="https://img.shields.io/badge/%E5%9F%BA%E4%BA%8E-TauricResearch/TradingAgents-orange.svg?sanitize=true" alt="Original" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;üöÄ &lt;strong&gt;ÊúÄÊñ∞ÁâàÊú¨ cn-0.1.15&lt;/strong&gt;: ÂºÄÂèëËÄÖ‰ΩìÈ™å‰∏éLLMÁîüÊÄÅÁ≥ªÁªüÂ§ßÂçáÁ∫ßÔºÅÊñ∞Â¢ûÂçÉÂ∏ÜÂ§ßÊ®°ÂûãÊîØÊåÅ„ÄÅÂÆåÊï¥ÂºÄÂèëÂ∑•ÂÖ∑Èìæ„ÄÅÂ≠¶ÊúØÁ†îÁ©∂ËµÑÊñô„ÄÅ‰ºÅ‰∏öÁ∫ßÂ∑•‰ΩúÊµÅËßÑËåÉÔºÅ&lt;/p&gt; 
 &lt;p&gt;üéØ &lt;strong&gt;Ê†∏ÂøÉÂäüËÉΩ&lt;/strong&gt;: ÂéüÁîüOpenAIÊîØÊåÅ | Google AIÂÖ®Èù¢ÈõÜÊàê | Ëá™ÂÆö‰πâÁ´ØÁÇπÈÖçÁΩÆ | Êô∫ËÉΩÊ®°ÂûãÈÄâÊã© | Â§öLLMÊèê‰æõÂïÜÊîØÊåÅ | Ê®°ÂûãÈÄâÊã©ÊåÅ‰πÖÂåñ | DockerÂÆπÂô®ÂåñÈÉ®ÁΩ≤ | ‰∏ì‰∏öÊä•ÂëäÂØºÂá∫ | ÂÆåÊï¥AËÇ°ÊîØÊåÅ | ‰∏≠ÊñáÊú¨Âú∞Âåñ&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Âü∫‰∫éÂ§öÊô∫ËÉΩ‰ΩìÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑ&lt;strong&gt;‰∏≠ÊñáÈáëËûç‰∫§ÊòìÂÜ≥Á≠ñÊ°ÜÊû∂&lt;/strong&gt;„ÄÇ‰∏ì‰∏∫‰∏≠ÊñáÁî®Êà∑‰ºòÂåñÔºåÊèê‰æõÂÆåÊï¥ÁöÑAËÇ°/Ê∏ØËÇ°/ÁæéËÇ°ÂàÜÊûêËÉΩÂäõ„ÄÇ&lt;/p&gt; 
&lt;h2&gt;üôè Ëá¥Êï¨Ê∫êÈ°πÁõÆ&lt;/h2&gt; 
&lt;p&gt;ÊÑüË∞¢ &lt;a href="https://github.com/TauricResearch"&gt;Tauric Research&lt;/a&gt; Âõ¢ÈòüÂàõÈÄ†ÁöÑÈù©ÂëΩÊÄßÂ§öÊô∫ËÉΩ‰Ωì‰∫§ÊòìÊ°ÜÊû∂ &lt;a href="https://github.com/TauricResearch/TradingAgents"&gt;TradingAgents&lt;/a&gt;ÔºÅ&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;üéØ Êàë‰ª¨ÁöÑ‰ΩøÂëΩ&lt;/strong&gt;: ‰∏∫‰∏≠ÂõΩÁî®Êà∑Êèê‰æõÂÆåÊï¥ÁöÑ‰∏≠ÊñáÂåñ‰ΩìÈ™åÔºåÊîØÊåÅAËÇ°/Ê∏ØËÇ°Â∏ÇÂú∫ÔºåÈõÜÊàêÂõΩ‰∫ßÂ§ßÊ®°ÂûãÔºåÊé®Âä®AIÈáëËûçÊäÄÊúØÂú®‰∏≠ÊñáÁ§æÂå∫ÁöÑÊôÆÂèäÂ∫îÁî®„ÄÇ&lt;/p&gt; 
&lt;h2&gt;üÜï v0.1.15 ÈáçÂ§ßÊõ¥Êñ∞&lt;/h2&gt; 
&lt;h3&gt;ü§ñ LLMÁîüÊÄÅÁ≥ªÁªüÂ§ßÂçáÁ∫ß&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ÂçÉÂ∏ÜÂ§ßÊ®°ÂûãÊîØÊåÅ&lt;/strong&gt;: Êñ∞Â¢ûÁôæÂ∫¶ÂçÉÂ∏Ü(ERNIE)Â§ßÊ®°ÂûãÂÆåÊï¥ÈõÜÊàê&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;LLMÈÄÇÈÖçÂô®ÈáçÊûÑ&lt;/strong&gt;: Áªü‰∏ÄÁöÑOpenAIÂÖºÂÆπÈÄÇÈÖçÂô®Êû∂ÊûÑ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Â§öÂéÇÂïÜÊîØÊåÅ&lt;/strong&gt;: ÊîØÊåÅÊõ¥Â§öÂõΩ‰∫ßÂ§ßÊ®°ÂûãÊèê‰æõÂïÜ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ÈõÜÊàêÊåáÂçó&lt;/strong&gt;: ÂÆåÊï¥ÁöÑLLMÈõÜÊàêÂºÄÂèëÊñáÊ°£ÂíåÊµãËØïÂ∑•ÂÖ∑&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üìö Â≠¶ÊúØÁ†îÁ©∂ÊîØÊåÅ&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;TradingAgentsËÆ∫Êñá&lt;/strong&gt;: ÂÆåÊï¥ÁöÑ‰∏≠ÊñáÁøªËØëÁâàÊú¨ÂíåÊ∑±Â∫¶Ëß£ËØª&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ÊäÄÊúØÂçöÂÆ¢&lt;/strong&gt;: ËØ¶ÁªÜÁöÑÊäÄÊúØÂàÜÊûêÂíåÂÆûÁé∞ÂéüÁêÜËß£ËØª&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Â≠¶ÊúØËµÑÊñô&lt;/strong&gt;: PDFËÆ∫ÊñáÂíåÁõ∏ÂÖ≥Á†îÁ©∂ËµÑÊñô&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ÂºïÁî®ÊîØÊåÅ&lt;/strong&gt;: Ê†áÂáÜÁöÑÂ≠¶ÊúØÂºïÁî®Ê†ºÂºèÂíåÂèÇËÄÉÊñáÁåÆ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üõ†Ô∏è ÂºÄÂèëËÄÖ‰ΩìÈ™åÂçáÁ∫ß&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ÂºÄÂèëÂ∑•‰ΩúÊµÅ&lt;/strong&gt;: Ê†áÂáÜÂåñÁöÑÂºÄÂèëÊµÅÁ®ãÂíåÂàÜÊîØÁÆ°ÁêÜËßÑËåÉ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ÂÆâË£ÖÈ™åËØÅ&lt;/strong&gt;: ÂÆåÊï¥ÁöÑÂÆâË£ÖÊµãËØïÂíåÈ™åËØÅËÑöÊú¨&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ÊñáÊ°£ÈáçÊûÑ&lt;/strong&gt;: ÁªìÊûÑÂåñÁöÑÊñáÊ°£Á≥ªÁªüÂíåÂø´ÈÄüÂºÄÂßãÊåáÂçó&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;PRÊ®°Êùø&lt;/strong&gt;: Ê†áÂáÜÂåñÁöÑPull RequestÊ®°ÊùøÂíå‰ª£Á†ÅÂÆ°Êü•ÊµÅÁ®ã&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üîß ‰ºÅ‰∏öÁ∫ßÂ∑•ÂÖ∑Èìæ&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ÂàÜÊîØ‰øùÊä§&lt;/strong&gt;: GitHubÂàÜÊîØ‰øùÊä§Á≠ñÁï•ÂíåÂÆâÂÖ®ËßÑÂàô&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Á¥ßÊÄ•Á®ãÂ∫è&lt;/strong&gt;: ÂÆåÊï¥ÁöÑÁ¥ßÊÄ•Â§ÑÁêÜÂíåÊïÖÈöúÊÅ¢Â§çÁ®ãÂ∫è&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ÊµãËØïÊ°ÜÊû∂&lt;/strong&gt;: Â¢ûÂº∫ÁöÑÊµãËØïË¶ÜÁõñÂíåÈ™åËØÅÂ∑•ÂÖ∑&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ÈÉ®ÁΩ≤ÊåáÂçó&lt;/strong&gt;: ‰ºÅ‰∏öÁ∫ßÈÉ®ÁΩ≤ÂíåÈÖçÁΩÆÁÆ°ÁêÜ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üìã v0.1.14 ÂäüËÉΩÂõûÈ°æ&lt;/h2&gt; 
&lt;h3&gt;üë• Áî®Êà∑ÊùÉÈôêÁÆ°ÁêÜÁ≥ªÁªü&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ÂÆåÊï¥Áî®Êà∑ÁÆ°ÁêÜ&lt;/strong&gt;: Êñ∞Â¢ûÁî®Êà∑Ê≥®ÂÜå„ÄÅÁôªÂΩï„ÄÅÊùÉÈôêÊéßÂà∂ÂäüËÉΩ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ËßíËâ≤ÊùÉÈôê&lt;/strong&gt;: ÊîØÊåÅÂ§öÁ∫ßÁî®Êà∑ËßíËâ≤ÂíåÊùÉÈôêÁÆ°ÁêÜ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;‰ºöËØùÁÆ°ÁêÜ&lt;/strong&gt;: ÂÆâÂÖ®ÁöÑÁî®Êà∑‰ºöËØùÂíåÁä∂ÊÄÅÁÆ°ÁêÜ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Áî®Êà∑Ê¥ªÂä®Êó•Âøó&lt;/strong&gt;: ÂÆåÊï¥ÁöÑÁî®Êà∑Êìç‰ΩúËÆ∞ÂΩïÂíåÂÆ°ËÆ°ÂäüËÉΩ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üîê WebÁî®Êà∑ËÆ§ËØÅÁ≥ªÁªü&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ÁôªÂΩïÁªÑ‰ª∂&lt;/strong&gt;: Áé∞‰ª£ÂåñÁöÑÁî®Êà∑ÁôªÂΩïÁïåÈù¢&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ËÆ§ËØÅÁÆ°ÁêÜÂô®&lt;/strong&gt;: Áªü‰∏ÄÁöÑÁî®Êà∑ËÆ§ËØÅÂíåÊéàÊùÉÁÆ°ÁêÜ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ÂÆâÂÖ®Â¢ûÂº∫&lt;/strong&gt;: ÂØÜÁ†ÅÂä†ÂØÜ„ÄÅ‰ºöËØùÂÆâÂÖ®Á≠âÂÆâÂÖ®Êú∫Âà∂&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Áî®Êà∑‰ª™Ë°®Êùø&lt;/strong&gt;: ‰∏™ÊÄßÂåñÁöÑÁî®Êà∑Ê¥ªÂä®‰ª™Ë°®Êùø&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üóÑÔ∏è Êï∞ÊçÆÁÆ°ÁêÜ‰ºòÂåñ&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;MongoDBÈõÜÊàêÂ¢ûÂº∫&lt;/strong&gt;: ÊîπËøõÁöÑMongoDBËøûÊé•ÂíåÊï∞ÊçÆÁÆ°ÁêÜ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Êï∞ÊçÆÁõÆÂΩïÈáçÁªÑ&lt;/strong&gt;: ‰ºòÂåñÁöÑÊï∞ÊçÆÂ≠òÂÇ®ÁªìÊûÑÂíåÁÆ°ÁêÜ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Êï∞ÊçÆËøÅÁßªËÑöÊú¨&lt;/strong&gt;: ÂÆåÊï¥ÁöÑÊï∞ÊçÆËøÅÁßªÂíåÂ§á‰ªΩÂ∑•ÂÖ∑&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ÁºìÂ≠ò‰ºòÂåñ&lt;/strong&gt;: ÊèêÂçáÊï∞ÊçÆÂä†ËΩΩÂíåÂàÜÊûêÁªìÊûúÁºìÂ≠òÊÄßËÉΩ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üß™ ÊµãËØïË¶ÜÁõñÂ¢ûÂº∫&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ÂäüËÉΩÊµãËØïËÑöÊú¨&lt;/strong&gt;: Êñ∞Â¢û6‰∏™‰∏ìÈ°πÂäüËÉΩÊµãËØïËÑöÊú¨&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Â∑•ÂÖ∑Â§ÑÁêÜÂô®ÊµãËØï&lt;/strong&gt;: GoogleÂ∑•ÂÖ∑Â§ÑÁêÜÂô®‰øÆÂ§çÈ™åËØÅ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ÂºïÂØºËá™Âä®ÈöêËóèÊµãËØï&lt;/strong&gt;: UI‰∫§‰∫íÂäüËÉΩÊµãËØï&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Âú®Á∫øÂ∑•ÂÖ∑ÈÖçÁΩÆÊµãËØï&lt;/strong&gt;: Â∑•ÂÖ∑ÈÖçÁΩÆÂíåÈÄâÊã©ÈÄªËæëÊµãËØï&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ÁúüÂÆûÂú∫ÊôØÊµãËØï&lt;/strong&gt;: ÂÆûÈôÖ‰ΩøÁî®Âú∫ÊôØÁöÑÁ´ØÂà∞Á´ØÊµãËØï&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ÁæéËÇ°Áã¨Á´ãÊÄßÊµãËØï&lt;/strong&gt;: ÁæéËÇ°ÂàÜÊûêÂäüËÉΩÁã¨Á´ãÊÄßÈ™åËØÅ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üÜï v0.1.13 ÈáçÂ§ßÊõ¥Êñ∞&lt;/h2&gt; 
&lt;h3&gt;ü§ñ ÂéüÁîüOpenAIÁ´ØÁÇπÊîØÊåÅ&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Ëá™ÂÆö‰πâOpenAIÁ´ØÁÇπ&lt;/strong&gt;: ÊîØÊåÅÈÖçÁΩÆ‰ªªÊÑèOpenAIÂÖºÂÆπÁöÑAPIÁ´ØÁÇπ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ÁÅµÊ¥ªÊ®°ÂûãÈÄâÊã©&lt;/strong&gt;: ÂèØ‰ª•‰ΩøÁî®‰ªª‰ΩïOpenAIÊ†ºÂºèÁöÑÊ®°ÂûãÔºå‰∏çÈôê‰∫éÂÆòÊñπÊ®°Âûã&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Êô∫ËÉΩÈÄÇÈÖçÂô®&lt;/strong&gt;: Êñ∞Â¢ûÂéüÁîüOpenAIÈÄÇÈÖçÂô®ÔºåÊèê‰æõÊõ¥Â•ΩÁöÑÂÖºÂÆπÊÄßÂíåÊÄßËÉΩ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ÈÖçÁΩÆÁÆ°ÁêÜ&lt;/strong&gt;: Áªü‰∏ÄÁöÑÁ´ØÁÇπÂíåÊ®°ÂûãÈÖçÁΩÆÁÆ°ÁêÜÁ≥ªÁªü&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üß† Google AIÁîüÊÄÅÁ≥ªÁªüÂÖ®Èù¢ÈõÜÊàê&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;‰∏âÂ§ßGoogle AIÂåÖÊîØÊåÅ&lt;/strong&gt;: langchain-google-genai„ÄÅgoogle-generativeai„ÄÅgoogle-genai&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;9‰∏™È™åËØÅÊ®°Âûã&lt;/strong&gt;: gemini-2.5-pro, gemini-2.5-flash, gemini-2.0-flashÁ≠âÊúÄÊñ∞Ê®°Âûã&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;GoogleÂ∑•ÂÖ∑Â§ÑÁêÜÂô®&lt;/strong&gt;: ‰∏ìÈó®ÁöÑGoogle AIÂ∑•ÂÖ∑Ë∞ÉÁî®Â§ÑÁêÜÂô®&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Êô∫ËÉΩÈôçÁ∫ßÊú∫Âà∂&lt;/strong&gt;: È´òÁ∫ßÂäüËÉΩÂ§±Ë¥•Êó∂Ëá™Âä®ÈôçÁ∫ßÂà∞Âü∫Á°ÄÂäüËÉΩ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üîß LLMÈÄÇÈÖçÂô®Êû∂ÊûÑ‰ºòÂåñ&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;GoogleOpenAIAdapter&lt;/strong&gt;: Êñ∞Â¢ûGoogle AIÁöÑOpenAIÂÖºÂÆπÈÄÇÈÖçÂô®&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Áªü‰∏ÄÊé•Âè£&lt;/strong&gt;: ÊâÄÊúâLLMÊèê‰æõÂïÜ‰ΩøÁî®Áªü‰∏ÄÁöÑË∞ÉÁî®Êé•Âè£&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ÈîôËØØÂ§ÑÁêÜÂ¢ûÂº∫&lt;/strong&gt;: ÊîπËøõÁöÑÂºÇÂ∏∏Â§ÑÁêÜÂíåËá™Âä®ÈáçËØïÊú∫Âà∂&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ÊÄßËÉΩÁõëÊéß&lt;/strong&gt;: Ê∑ªÂä†LLMË∞ÉÁî®ÊÄßËÉΩÁõëÊéßÂíåÁªüËÆ°&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üé® WebÁïåÈù¢Êô∫ËÉΩ‰ºòÂåñ&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Êô∫ËÉΩÊ®°ÂûãÈÄâÊã©&lt;/strong&gt;: Ê†πÊçÆÂèØÁî®ÊÄßËá™Âä®ÈÄâÊã©ÊúÄ‰Ω≥Ê®°Âûã&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;KeyError‰øÆÂ§ç&lt;/strong&gt;: ÂΩªÂ∫ïËß£ÂÜ≥Ê®°ÂûãÈÄâÊã©‰∏≠ÁöÑKeyErrorÈóÆÈ¢ò&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;UIÂìçÂ∫î‰ºòÂåñ&lt;/strong&gt;: ÊîπËøõÊ®°ÂûãÂàáÊç¢ÁöÑÂìçÂ∫îÈÄüÂ∫¶ÂíåÁî®Êà∑‰ΩìÈ™å&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ÈîôËØØÊèêÁ§∫&lt;/strong&gt;: Êõ¥ÂèãÂ•ΩÁöÑÈîôËØØÊèêÁ§∫ÂíåËß£ÂÜ≥Âª∫ËÆÆ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üÜï v0.1.12 ÈáçÂ§ßÊõ¥Êñ∞&lt;/h2&gt; 
&lt;h3&gt;üß† Êô∫ËÉΩÊñ∞ÈóªÂàÜÊûêÊ®°Âùó&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Êô∫ËÉΩÊñ∞ÈóªËøáÊª§Âô®&lt;/strong&gt;: Âü∫‰∫éAIÁöÑÊñ∞ÈóªÁõ∏ÂÖ≥ÊÄßËØÑÂàÜÂíåË¥®ÈáèËØÑ‰º∞&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Â§öÂ±ÇÊ¨°ËøáÊª§Êú∫Âà∂&lt;/strong&gt;: Âü∫Á°ÄËøáÊª§„ÄÅÂ¢ûÂº∫ËøáÊª§„ÄÅÈõÜÊàêËøáÊª§‰∏âÁ∫ßÂ§ÑÁêÜ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Êñ∞ÈóªË¥®ÈáèËØÑ‰º∞&lt;/strong&gt;: Ëá™Âä®ËØÜÂà´ÂíåËøáÊª§‰ΩéË¥®Èáè„ÄÅÈáçÂ§ç„ÄÅÊó†ÂÖ≥Êñ∞Èóª&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Áªü‰∏ÄÊñ∞ÈóªÂ∑•ÂÖ∑&lt;/strong&gt;: Êï¥ÂêàÂ§ö‰∏™Êñ∞ÈóªÊ∫êÔºåÊèê‰æõÁªü‰∏ÄÁöÑÊñ∞ÈóªËé∑ÂèñÊé•Âè£&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üîß ÊäÄÊúØ‰øÆÂ§çÂíå‰ºòÂåñ&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;DashScopeÈÄÇÈÖçÂô®‰øÆÂ§ç&lt;/strong&gt;: Ëß£ÂÜ≥Â∑•ÂÖ∑Ë∞ÉÁî®ÂÖºÂÆπÊÄßÈóÆÈ¢ò&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;DeepSeekÊ≠ªÂæ™ÁéØ‰øÆÂ§ç&lt;/strong&gt;: ‰øÆÂ§çÊñ∞ÈóªÂàÜÊûêÂ∏àÁöÑÊó†ÈôêÂæ™ÁéØÈóÆÈ¢ò&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;LLMÂ∑•ÂÖ∑Ë∞ÉÁî®Â¢ûÂº∫&lt;/strong&gt;: ÊèêÂçáÂ∑•ÂÖ∑Ë∞ÉÁî®ÁöÑÂèØÈù†ÊÄßÂíåÁ®≥ÂÆöÊÄß&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Êñ∞ÈóªÊ£ÄÁ¥¢Âô®‰ºòÂåñ&lt;/strong&gt;: Â¢ûÂº∫Êñ∞ÈóªÊï∞ÊçÆËé∑ÂèñÂíåÂ§ÑÁêÜËÉΩÂäõ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üìö ÂÆåÂñÑÊµãËØïÂíåÊñáÊ°£&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ÂÖ®Èù¢ÊµãËØïË¶ÜÁõñ&lt;/strong&gt;: Êñ∞Â¢û15+‰∏™ÊµãËØïÊñá‰ª∂ÔºåË¶ÜÁõñÊâÄÊúâÊñ∞ÂäüËÉΩ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ËØ¶ÁªÜÊäÄÊúØÊñáÊ°£&lt;/strong&gt;: Êñ∞Â¢û8‰∏™ÊäÄÊúØÂàÜÊûêÊä•ÂëäÂíå‰øÆÂ§çÊñáÊ°£&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Áî®Êà∑ÊåáÂçóÂÆåÂñÑ&lt;/strong&gt;: Êñ∞Â¢ûÊñ∞ÈóªËøáÊª§‰ΩøÁî®ÊåáÂçóÂíåÊúÄ‰Ω≥ÂÆûË∑µ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ÊºîÁ§∫ËÑöÊú¨&lt;/strong&gt;: Êèê‰æõÂÆåÊï¥ÁöÑÊñ∞ÈóªËøáÊª§ÂäüËÉΩÊºîÁ§∫&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üóÇÔ∏è È°πÁõÆÁªìÊûÑ‰ºòÂåñ&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ÊñáÊ°£ÂàÜÁ±ªÊï¥ÁêÜ&lt;/strong&gt;: ÊåâÂäüËÉΩÂ∞ÜÊñáÊ°£ÂàÜÁ±ªÂà∞docsÂ≠êÁõÆÂΩï&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Á§∫‰æã‰ª£Á†ÅÂΩí‰Ωç&lt;/strong&gt;: ÊºîÁ§∫ËÑöÊú¨Áªü‰∏ÄÂà∞examplesÁõÆÂΩï&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Ê†πÁõÆÂΩïÊï¥Ê¥Å&lt;/strong&gt;: ‰øùÊåÅÊ†πÁõÆÂΩïÁÆÄÊ¥ÅÔºåÊèêÂçáÈ°πÁõÆ‰∏ì‰∏öÂ∫¶&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üéØ Ê†∏ÂøÉÁâπÊÄß&lt;/h2&gt; 
&lt;h3&gt;ü§ñ Â§öÊô∫ËÉΩ‰ΩìÂçè‰ΩúÊû∂ÊûÑ&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;‰∏ì‰∏öÂàÜÂ∑•&lt;/strong&gt;: Âü∫Êú¨Èù¢„ÄÅÊäÄÊúØÈù¢„ÄÅÊñ∞ÈóªÈù¢„ÄÅÁ§æ‰∫§Â™í‰ΩìÂõõÂ§ßÂàÜÊûêÂ∏à&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ÁªìÊûÑÂåñËæ©ËÆ∫&lt;/strong&gt;: ÁúãÊ∂®/ÁúãË∑åÁ†îÁ©∂ÂëòËøõË°åÊ∑±Â∫¶ÂàÜÊûê&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Êô∫ËÉΩÂÜ≥Á≠ñ&lt;/strong&gt;: ‰∫§ÊòìÂëòÂü∫‰∫éÊâÄÊúâËæìÂÖ•ÂÅöÂá∫ÊúÄÁªàÊäïËµÑÂª∫ËÆÆ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;È£éÈô©ÁÆ°ÁêÜ&lt;/strong&gt;: Â§öÂ±ÇÊ¨°È£éÈô©ËØÑ‰º∞ÂíåÁÆ°ÁêÜÊú∫Âà∂&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üñ•Ô∏è WebÁïåÈù¢Â±ïÁ§∫&lt;/h2&gt; 
&lt;h3&gt;üì∏ ÁïåÈù¢Êà™Âõæ&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;üé® &lt;strong&gt;Áé∞‰ª£ÂåñWebÁïåÈù¢&lt;/strong&gt;: Âü∫‰∫éStreamlitÊûÑÂª∫ÁöÑÂìçÂ∫îÂºèWebÂ∫îÁî®ÔºåÊèê‰æõÁõ¥ËßÇÁöÑËÇ°Á•®ÂàÜÊûê‰ΩìÈ™å&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;üè† ‰∏ªÁïåÈù¢ - ÂàÜÊûêÈÖçÁΩÆ&lt;/h4&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/images/README/1755003162925.png" alt="1755003162925" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/images/README/1755002619976.png" alt="1755002619976" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Êô∫ËÉΩÈÖçÁΩÆÈù¢ÊùøÔºåÊîØÊåÅÂ§öÂ∏ÇÂú∫ËÇ°Á•®ÂàÜÊûêÔºå5Á∫ßÁ†îÁ©∂Ê∑±Â∫¶ÈÄâÊã©&lt;/em&gt;&lt;/p&gt; 
&lt;h4&gt;üìä ÂÆûÊó∂ÂàÜÊûêËøõÂ∫¶&lt;/h4&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/images/README/1755002731483.png" alt="1755002731483" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;ÂÆûÊó∂ËøõÂ∫¶Ë∑üË∏™ÔºåÂèØËßÜÂåñÂàÜÊûêËøáÁ®ãÔºåÊô∫ËÉΩÊó∂Èó¥È¢Ñ‰º∞&lt;/em&gt;&lt;/p&gt; 
&lt;h4&gt;üìà ÂàÜÊûêÁªìÊûúÂ±ïÁ§∫&lt;/h4&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/images/README/1755002901204.png" alt="1755002901204" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/images/README/1755002924844.png" alt="1755002924844" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/images/README/1755002939905.png" alt="1755002939905" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/images/README/1755002968608.png" alt="1755002968608" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/images/README/1755002985903.png" alt="1755002985903" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/images/README/1755003004403.png" alt="1755003004403" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/images/README/1755003019759.png" alt="1755003019759" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/images/README/1755003033939.png" alt="1755003033939" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/images/README/1755003048242.png" alt="1755003048242" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/images/README/1755003064598.png" alt="1755003064598" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/images/README/1755003090603.png" alt="1755003090603" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;‰∏ì‰∏öÊäïËµÑÊä•ÂëäÔºåÂ§öÁª¥Â∫¶ÂàÜÊûêÁªìÊûúÔºå‰∏ÄÈîÆÂØºÂá∫ÂäüËÉΩ&lt;/em&gt;&lt;/p&gt; 
&lt;h3&gt;üéØ Ê†∏ÂøÉÂäüËÉΩÁâπËâ≤&lt;/h3&gt; 
&lt;h4&gt;üìã &lt;strong&gt;Êô∫ËÉΩÂàÜÊûêÈÖçÁΩÆ&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;üåç Â§öÂ∏ÇÂú∫ÊîØÊåÅ&lt;/strong&gt;: ÁæéËÇ°„ÄÅAËÇ°„ÄÅÊ∏ØËÇ°‰∏ÄÁ´ôÂºèÂàÜÊûê&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üéØ 5Á∫ßÁ†îÁ©∂Ê∑±Â∫¶&lt;/strong&gt;: ‰ªé2ÂàÜÈíüÂø´ÈÄüÂàÜÊûêÂà∞25ÂàÜÈíüÂÖ®Èù¢Á†îÁ©∂&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ü§ñ Êô∫ËÉΩ‰ΩìÈÄâÊã©&lt;/strong&gt;: Â∏ÇÂú∫ÊäÄÊúØ„ÄÅÂü∫Êú¨Èù¢„ÄÅÊñ∞Èóª„ÄÅÁ§æ‰∫§Â™í‰ΩìÂàÜÊûêÂ∏à&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üìÖ ÁÅµÊ¥ªÊó∂Èó¥ËÆæÁΩÆ&lt;/strong&gt;: ÊîØÊåÅÂéÜÂè≤‰ªªÊÑèÊó∂Èó¥ÁÇπÂàÜÊûê&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;üöÄ &lt;strong&gt;ÂÆûÊó∂ËøõÂ∫¶Ë∑üË∏™&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;üìä ÂèØËßÜÂåñËøõÂ∫¶&lt;/strong&gt;: ÂÆûÊó∂ÊòæÁ§∫ÂàÜÊûêËøõÂ±ïÂíåÂâ©‰ΩôÊó∂Èó¥&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üîÑ Êô∫ËÉΩÊ≠•È™§ËØÜÂà´&lt;/strong&gt;: Ëá™Âä®ËØÜÂà´ÂΩìÂâçÂàÜÊûêÈò∂ÊÆµ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;‚è±Ô∏è ÂáÜÁ°ÆÊó∂Èó¥È¢Ñ‰º∞&lt;/strong&gt;: Âü∫‰∫éÂéÜÂè≤Êï∞ÊçÆÁöÑÊô∫ËÉΩÊó∂Èó¥ËÆ°ÁÆó&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üíæ Áä∂ÊÄÅÊåÅ‰πÖÂåñ&lt;/strong&gt;: È°µÈù¢Âà∑Êñ∞‰∏ç‰∏¢Â§±ÂàÜÊûêËøõÂ∫¶&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;üìà &lt;strong&gt;‰∏ì‰∏öÁªìÊûúÂ±ïÁ§∫&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;üéØ ÊäïËµÑÂÜ≥Á≠ñ&lt;/strong&gt;: ÊòéÁ°ÆÁöÑ‰π∞ÂÖ•/ÊåÅÊúâ/ÂçñÂá∫Âª∫ËÆÆ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üìä Â§öÁª¥ÂàÜÊûê&lt;/strong&gt;: ÊäÄÊúØÈù¢„ÄÅÂü∫Êú¨Èù¢„ÄÅÊñ∞ÈóªÈù¢ÁªºÂêàËØÑ‰º∞&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üî¢ ÈáèÂåñÊåáÊ†á&lt;/strong&gt;: ÁΩÆ‰ø°Â∫¶„ÄÅÈ£éÈô©ËØÑÂàÜ„ÄÅÁõÆÊ†á‰ª∑‰Ωç&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üìÑ ‰∏ì‰∏öÊä•Âëä&lt;/strong&gt;: ÊîØÊåÅMarkdown/Word/PDFÊ†ºÂºèÂØºÂá∫&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;ü§ñ &lt;strong&gt;Â§öLLMÊ®°ÂûãÁÆ°ÁêÜ&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;üåê 4Â§ßÊèê‰æõÂïÜ&lt;/strong&gt;: DashScope„ÄÅDeepSeek„ÄÅGoogle AI„ÄÅOpenRouter&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üéØ 60+Ê®°ÂûãÈÄâÊã©&lt;/strong&gt;: ‰ªéÁªèÊµéÂûãÂà∞ÊóóËà∞Á∫ßÊ®°ÂûãÂÖ®Ë¶ÜÁõñ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üíæ ÈÖçÁΩÆÊåÅ‰πÖÂåñ&lt;/strong&gt;: URLÂèÇÊï∞Â≠òÂÇ®ÔºåÂà∑Êñ∞‰øùÊåÅËÆæÁΩÆ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;‚ö° Âø´ÈÄüÂàáÊç¢&lt;/strong&gt;: 5‰∏™ÁÉ≠Èó®Ê®°Âûã‰∏ÄÈîÆÈÄâÊã©ÊåâÈíÆ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üéÆ WebÁïåÈù¢Êìç‰ΩúÊåáÂçó&lt;/h3&gt; 
&lt;h4&gt;üöÄ &lt;strong&gt;Âø´ÈÄüÂºÄÂßãÊµÅÁ®ã&lt;/strong&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;ÂêØÂä®Â∫îÁî®&lt;/strong&gt;: &lt;code&gt;python start_web.py&lt;/code&gt; Êàñ &lt;code&gt;docker-compose up -d&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ËÆøÈóÆÁïåÈù¢&lt;/strong&gt;: ÊµèËßàÂô®ÊâìÂºÄ &lt;code&gt;http://localhost:8501&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ÈÖçÁΩÆÊ®°Âûã&lt;/strong&gt;: ‰æßËæπÊ†èÈÄâÊã©LLMÊèê‰æõÂïÜÂíåÊ®°Âûã&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ËæìÂÖ•ËÇ°Á•®&lt;/strong&gt;: ËæìÂÖ•ËÇ°Á•®‰ª£Á†ÅÔºàÂ¶Ç AAPL„ÄÅ000001„ÄÅ0700.HKÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ÈÄâÊã©Ê∑±Â∫¶&lt;/strong&gt;: Ê†πÊçÆÈúÄÊ±ÇÈÄâÊã©1-5Á∫ßÁ†îÁ©∂Ê∑±Â∫¶&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ÂºÄÂßãÂàÜÊûê&lt;/strong&gt;: ÁÇπÂáª"üöÄ ÂºÄÂßãÂàÜÊûê"ÊåâÈíÆ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Êü•ÁúãÁªìÊûú&lt;/strong&gt;: ÂÆûÊó∂Ë∑üË∏™ËøõÂ∫¶ÔºåÊü•ÁúãÂàÜÊûêÊä•Âëä&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ÂØºÂá∫Êä•Âëä&lt;/strong&gt;: ‰∏ÄÈîÆÂØºÂá∫‰∏ì‰∏öÊ†ºÂºèÊä•Âëä&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;üìä &lt;strong&gt;ÊîØÊåÅÁöÑËÇ°Á•®‰ª£Á†ÅÊ†ºÂºè&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;üá∫üá∏ ÁæéËÇ°&lt;/strong&gt;: &lt;code&gt;AAPL&lt;/code&gt;, &lt;code&gt;TSLA&lt;/code&gt;, &lt;code&gt;MSFT&lt;/code&gt;, &lt;code&gt;NVDA&lt;/code&gt;, &lt;code&gt;GOOGL&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üá®üá≥ AËÇ°&lt;/strong&gt;: &lt;code&gt;000001&lt;/code&gt;, &lt;code&gt;600519&lt;/code&gt;, &lt;code&gt;300750&lt;/code&gt;, &lt;code&gt;002415&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üá≠üá∞ Ê∏ØËÇ°&lt;/strong&gt;: &lt;code&gt;0700.HK&lt;/code&gt;, &lt;code&gt;9988.HK&lt;/code&gt;, &lt;code&gt;3690.HK&lt;/code&gt;, &lt;code&gt;1810.HK&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;üéØ &lt;strong&gt;Á†îÁ©∂Ê∑±Â∫¶ËØ¥Êòé&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;1Á∫ß (2-4ÂàÜÈíü)&lt;/strong&gt;: Âø´ÈÄüÊ¶ÇËßàÔºåÂü∫Á°ÄÊäÄÊúØÊåáÊ†á&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;2Á∫ß (4-6ÂàÜÈíü)&lt;/strong&gt;: Ê†áÂáÜÂàÜÊûêÔºåÊäÄÊúØ+Âü∫Êú¨Èù¢&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;3Á∫ß (6-10ÂàÜÈíü)&lt;/strong&gt;: Ê∑±Â∫¶ÂàÜÊûêÔºåÂä†ÂÖ•Êñ∞ÈóªÊÉÖÁª™ ‚≠ê &lt;strong&gt;Êé®Ëçê&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;4Á∫ß (10-15ÂàÜÈíü)&lt;/strong&gt;: ÂÖ®Èù¢ÂàÜÊûêÔºåÂ§öËΩÆÊô∫ËÉΩ‰ΩìËæ©ËÆ∫&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;5Á∫ß (15-25ÂàÜÈíü)&lt;/strong&gt;: ÊúÄÊ∑±Â∫¶ÂàÜÊûêÔºåÂÆåÊï¥Á†îÁ©∂Êä•Âëä&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;üí° &lt;strong&gt;‰ΩøÁî®ÊäÄÂ∑ß&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;üîÑ ÂÆûÊó∂Âà∑Êñ∞&lt;/strong&gt;: ÂàÜÊûêËøáÁ®ã‰∏≠ÂèØÈöèÊó∂Âà∑Êñ∞È°µÈù¢ÔºåËøõÂ∫¶‰∏ç‰∏¢Â§±&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üì± ÁßªÂä®ÈÄÇÈÖç&lt;/strong&gt;: ÊîØÊåÅÊâãÊú∫ÂíåÂπ≥ÊùøËÆæÂ§áËÆøÈóÆ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üé® Ê∑±Ëâ≤Ê®°Âºè&lt;/strong&gt;: Ëá™Âä®ÈÄÇÈÖçÁ≥ªÁªü‰∏ªÈ¢òËÆæÁΩÆ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;‚å®Ô∏è Âø´Êç∑ÈîÆ&lt;/strong&gt;: ÊîØÊåÅEnterÈîÆÂø´ÈÄüÊèê‰∫§ÂàÜÊûê&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üìã ÂéÜÂè≤ËÆ∞ÂΩï&lt;/strong&gt;: Ëá™Âä®‰øùÂ≠òÊúÄËøëÁöÑÂàÜÊûêÈÖçÁΩÆ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;üìñ &lt;strong&gt;ËØ¶ÁªÜÊåáÂçó&lt;/strong&gt;: ÂÆåÊï¥ÁöÑWebÁïåÈù¢‰ΩøÁî®ËØ¥ÊòéËØ∑ÂèÇËÄÉ &lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/usage/web-interface-detailed-guide.md"&gt;üñ•Ô∏è WebÁïåÈù¢ËØ¶ÁªÜ‰ΩøÁî®ÊåáÂçó&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;üéØ ÂäüËÉΩÁâπÊÄß&lt;/h2&gt; 
&lt;h3&gt;üöÄ Êô∫ËÉΩÊñ∞ÈóªÂàÜÊûê‚ú® &lt;strong&gt;v0.1.12ÈáçÂ§ßÂçáÁ∫ß&lt;/strong&gt;&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;ÂäüËÉΩÁâπÊÄß&lt;/th&gt; 
   &lt;th&gt;Áä∂ÊÄÅ&lt;/th&gt; 
   &lt;th&gt;ËØ¶ÁªÜËØ¥Êòé&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üß† Êô∫ËÉΩÊñ∞ÈóªÂàÜÊûê&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;üÜï v0.1.12&lt;/td&gt; 
   &lt;td&gt;AIÊñ∞ÈóªËøáÊª§ÔºåË¥®ÈáèËØÑ‰º∞ÔºåÁõ∏ÂÖ≥ÊÄßÂàÜÊûê&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üîß Êñ∞ÈóªËøáÊª§Âô®&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;üÜï v0.1.12&lt;/td&gt; 
   &lt;td&gt;Â§öÂ±ÇÊ¨°ËøáÊª§ÔºåÂü∫Á°Ä/Â¢ûÂº∫/ÈõÜÊàê‰∏âÁ∫ßÂ§ÑÁêÜ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üì∞ Áªü‰∏ÄÊñ∞ÈóªÂ∑•ÂÖ∑&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;üÜï v0.1.12&lt;/td&gt; 
   &lt;td&gt;Êï¥ÂêàÂ§öÊ∫êÊñ∞ÈóªÔºåÁªü‰∏ÄÊé•Âè£ÔºåÊô∫ËÉΩÊ£ÄÁ¥¢&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ü§ñ Â§öLLMÊèê‰æõÂïÜ&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;üÜï v0.1.11&lt;/td&gt; 
   &lt;td&gt;4Â§ßÊèê‰æõÂïÜÔºå60+Ê®°ÂûãÔºåÊô∫ËÉΩÂàÜÁ±ªÁÆ°ÁêÜ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üíæ Ê®°ÂûãÈÄâÊã©ÊåÅ‰πÖÂåñ&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;üÜï v0.1.11&lt;/td&gt; 
   &lt;td&gt;URLÂèÇÊï∞Â≠òÂÇ®ÔºåÂà∑Êñ∞‰øùÊåÅÔºåÈÖçÁΩÆÂàÜ‰∫´&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üéØ Âø´ÈÄüÈÄâÊã©ÊåâÈíÆ&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;üÜï v0.1.11&lt;/td&gt; 
   &lt;td&gt;‰∏ÄÈîÆÂàáÊç¢ÁÉ≠Èó®Ê®°ÂûãÔºåÊèêÂçáÊìç‰ΩúÊïàÁéá&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üìä ÂÆûÊó∂ËøõÂ∫¶ÊòæÁ§∫&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ v0.1.10&lt;/td&gt; 
   &lt;td&gt;ÂºÇÊ≠•ËøõÂ∫¶Ë∑üË∏™ÔºåÊô∫ËÉΩÊ≠•È™§ËØÜÂà´ÔºåÂáÜÁ°ÆÊó∂Èó¥ËÆ°ÁÆó&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üíæ Êô∫ËÉΩ‰ºöËØùÁÆ°ÁêÜ&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ v0.1.10&lt;/td&gt; 
   &lt;td&gt;Áä∂ÊÄÅÊåÅ‰πÖÂåñÔºåËá™Âä®ÈôçÁ∫ßÔºåË∑®È°µÈù¢ÊÅ¢Â§ç&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üéØ ‰∏ÄÈîÆÊü•ÁúãÊä•Âëä&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ v0.1.10&lt;/td&gt; 
   &lt;td&gt;ÂàÜÊûêÂÆåÊàêÂêé‰∏ÄÈîÆÊü•ÁúãÔºåÊô∫ËÉΩÁªìÊûúÊÅ¢Â§ç&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üñ•Ô∏è StreamlitÁïåÈù¢&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ ÂÆåÊï¥ÊîØÊåÅ&lt;/td&gt; 
   &lt;td&gt;Áé∞‰ª£ÂåñÂìçÂ∫îÂºèÁïåÈù¢ÔºåÂÆûÊó∂‰∫§‰∫íÂíåÊï∞ÊçÆÂèØËßÜÂåñ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;‚öôÔ∏è ÈÖçÁΩÆÁÆ°ÁêÜ&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ ÂÆåÊï¥ÊîØÊåÅ&lt;/td&gt; 
   &lt;td&gt;WebÁ´ØAPIÂØÜÈí•ÁÆ°ÁêÜÔºåÊ®°ÂûãÈÄâÊã©ÔºåÂèÇÊï∞ÈÖçÁΩÆ&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;üé® CLIÁî®Êà∑‰ΩìÈ™å ‚ú® &lt;strong&gt;v0.1.9‰ºòÂåñ&lt;/strong&gt;&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;ÂäüËÉΩÁâπÊÄß&lt;/th&gt; 
   &lt;th&gt;Áä∂ÊÄÅ&lt;/th&gt; 
   &lt;th&gt;ËØ¶ÁªÜËØ¥Êòé&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üñ•Ô∏è ÁïåÈù¢‰∏éÊó•ÂøóÂàÜÁ¶ª&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ ÂÆåÊï¥ÊîØÊåÅ&lt;/td&gt; 
   &lt;td&gt;Áî®Êà∑ÁïåÈù¢Ê∏ÖÁàΩÁæéËßÇÔºåÊäÄÊúØÊó•ÂøóÁã¨Á´ãÁÆ°ÁêÜ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üîÑ Êô∫ËÉΩËøõÂ∫¶ÊòæÁ§∫&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ ÂÆåÊï¥ÊîØÊåÅ&lt;/td&gt; 
   &lt;td&gt;Â§öÈò∂ÊÆµËøõÂ∫¶Ë∑üË∏™ÔºåÈò≤Ê≠¢ÈáçÂ§çÊèêÁ§∫&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;‚è±Ô∏è Êó∂Èó¥È¢Ñ‰º∞ÂäüËÉΩ&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ ÂÆåÊï¥ÊîØÊåÅ&lt;/td&gt; 
   &lt;td&gt;Êô∫ËÉΩÂàÜÊûêÈò∂ÊÆµÊòæÁ§∫È¢ÑËÆ°ËÄóÊó∂&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üåà RichÂΩ©Ëâ≤ËæìÂá∫&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ ÂÆåÊï¥ÊîØÊåÅ&lt;/td&gt; 
   &lt;td&gt;ÂΩ©Ëâ≤ËøõÂ∫¶ÊåáÁ§∫ÔºåÁä∂ÊÄÅÂõæÊ†áÔºåËßÜËßâÊïàÊûúÊèêÂçá&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;üß† LLMÊ®°ÂûãÊîØÊåÅ ‚ú® &lt;strong&gt;v0.1.13ÂÖ®Èù¢ÂçáÁ∫ß&lt;/strong&gt;&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Ê®°ÂûãÊèê‰æõÂïÜ&lt;/th&gt; 
   &lt;th&gt;ÊîØÊåÅÊ®°Âûã&lt;/th&gt; 
   &lt;th&gt;ÁâπËâ≤ÂäüËÉΩ&lt;/th&gt; 
   &lt;th&gt;Êñ∞Â¢ûÂäüËÉΩ&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üá®üá≥ ÈòøÈáåÁôæÁÇº&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;qwen-turbo/plus/max&lt;/td&gt; 
   &lt;td&gt;‰∏≠Êñá‰ºòÂåñÔºåÊàêÊú¨ÊïàÁõäÈ´ò&lt;/td&gt; 
   &lt;td&gt;‚úÖ ÈõÜÊàê&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üá®üá≥ DeepSeek&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;deepseek-chat&lt;/td&gt; 
   &lt;td&gt;Â∑•ÂÖ∑Ë∞ÉÁî®ÔºåÊÄß‰ª∑ÊØîÊûÅÈ´ò&lt;/td&gt; 
   &lt;td&gt;‚úÖ ÈõÜÊàê&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üåç Google AI&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;9‰∏™È™åËØÅÊ®°Âûã&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;ÊúÄÊñ∞Gemini 2.5Á≥ªÂàó&lt;/td&gt; 
   &lt;td&gt;üÜï ÂçáÁ∫ß&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;‚îú‚îÄ&lt;strong&gt;ÊúÄÊñ∞ÊóóËà∞&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;gemini-2.5-pro/flash&lt;/td&gt; 
   &lt;td&gt;ÊúÄÊñ∞ÊóóËà∞ÔºåË∂ÖÂø´ÂìçÂ∫î&lt;/td&gt; 
   &lt;td&gt;üÜï Êñ∞Â¢û&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;‚îú‚îÄ&lt;strong&gt;Á®≥ÂÆöÊé®Ëçê&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;gemini-2.0-flash&lt;/td&gt; 
   &lt;td&gt;Êé®Ëçê‰ΩøÁî®ÔºåÂπ≥Ë°°ÊÄßËÉΩ&lt;/td&gt; 
   &lt;td&gt;üÜï Êñ∞Â¢û&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;‚îú‚îÄ&lt;strong&gt;ÁªèÂÖ∏Âº∫Â§ß&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;gemini-1.5-pro/flash&lt;/td&gt; 
   &lt;td&gt;ÁªèÂÖ∏Á®≥ÂÆöÔºåÈ´òË¥®ÈáèÂàÜÊûê&lt;/td&gt; 
   &lt;td&gt;‚úÖ ÈõÜÊàê&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;‚îî‚îÄ&lt;strong&gt;ËΩªÈáèÂø´ÈÄü&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;gemini-2.5-flash-lite&lt;/td&gt; 
   &lt;td&gt;ËΩªÈáèÁ∫ß‰ªªÂä°ÔºåÂø´ÈÄüÂìçÂ∫î&lt;/td&gt; 
   &lt;td&gt;üÜï Êñ∞Â¢û&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üåê ÂéüÁîüOpenAI&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Ëá™ÂÆö‰πâÁ´ØÁÇπÊîØÊåÅ&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;‰ªªÊÑèOpenAIÂÖºÂÆπÁ´ØÁÇπ&lt;/td&gt; 
   &lt;td&gt;üÜï Êñ∞Â¢û&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üåê OpenRouter&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;60+Ê®°ÂûãËÅöÂêàÂπ≥Âè∞&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;‰∏Ä‰∏™APIËÆøÈóÆÊâÄÊúâ‰∏ªÊµÅÊ®°Âûã&lt;/td&gt; 
   &lt;td&gt;‚úÖ ÈõÜÊàê&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;‚îú‚îÄ&lt;strong&gt;OpenAI&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;o4-mini-high, o3-pro, GPT-4o&lt;/td&gt; 
   &lt;td&gt;ÊúÄÊñ∞oÁ≥ªÂàóÔºåÊé®ÁêÜ‰∏ì‰∏öÁâà&lt;/td&gt; 
   &lt;td&gt;‚úÖ ÈõÜÊàê&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;‚îú‚îÄ&lt;strong&gt;Anthropic&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Claude 4 Opus/Sonnet/Haiku&lt;/td&gt; 
   &lt;td&gt;È°∂Á∫ßÊÄßËÉΩÔºåÂπ≥Ë°°ÁâàÊú¨&lt;/td&gt; 
   &lt;td&gt;‚úÖ ÈõÜÊàê&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;‚îú‚îÄ&lt;strong&gt;Meta&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Llama 4 Maverick/Scout&lt;/td&gt; 
   &lt;td&gt;ÊúÄÊñ∞Llama 4Á≥ªÂàó&lt;/td&gt; 
   &lt;td&gt;‚úÖ ÈõÜÊàê&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;‚îî‚îÄ&lt;strong&gt;Ëá™ÂÆö‰πâ&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;‰ªªÊÑèOpenRouterÊ®°ÂûãID&lt;/td&gt; 
   &lt;td&gt;Êó†ÈôêÊâ©Â±ïÔºå‰∏™ÊÄßÂåñÈÄâÊã©&lt;/td&gt; 
   &lt;td&gt;‚úÖ ÈõÜÊàê&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;üéØ Âø´ÈÄüÈÄâÊã©&lt;/strong&gt;: 5‰∏™ÁÉ≠Èó®Ê®°ÂûãÂø´ÈÄüÊåâÈíÆ | &lt;strong&gt;üíæ ÊåÅ‰πÖÂåñ&lt;/strong&gt;: URLÂèÇÊï∞Â≠òÂÇ®ÔºåÂà∑Êñ∞‰øùÊåÅ | &lt;strong&gt;üîÑ Êô∫ËÉΩÂàáÊç¢&lt;/strong&gt;: ‰∏ÄÈîÆÂàáÊç¢‰∏çÂêåÊèê‰æõÂïÜ&lt;/p&gt; 
&lt;h3&gt;üìä Êï∞ÊçÆÊ∫ê‰∏éÂ∏ÇÂú∫&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Â∏ÇÂú∫Á±ªÂûã&lt;/th&gt; 
   &lt;th&gt;Êï∞ÊçÆÊ∫ê&lt;/th&gt; 
   &lt;th&gt;Ë¶ÜÁõñËåÉÂõ¥&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üá®üá≥ AËÇ°&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Tushare, AkShare, ÈÄöËææ‰ø°&lt;/td&gt; 
   &lt;td&gt;Ê≤™Ê∑±‰∏§Â∏ÇÔºåÂÆûÊó∂Ë°åÊÉÖÔºåË¥¢Êä•Êï∞ÊçÆ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üá≠üá∞ Ê∏ØËÇ°&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;AkShare, Yahoo Finance&lt;/td&gt; 
   &lt;td&gt;Ê∏Ø‰∫§ÊâÄÔºåÂÆûÊó∂Ë°åÊÉÖÔºåÂü∫Êú¨Èù¢&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üá∫üá∏ ÁæéËÇ°&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;FinnHub, Yahoo Finance&lt;/td&gt; 
   &lt;td&gt;NYSE, NASDAQÔºåÂÆûÊó∂Êï∞ÊçÆ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üì∞ Êñ∞Èóª&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Google News&lt;/td&gt; 
   &lt;td&gt;ÂÆûÊó∂Êñ∞ÈóªÔºåÂ§öËØ≠Ë®ÄÊîØÊåÅ&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;ü§ñ Êô∫ËÉΩ‰ΩìÂõ¢Èòü&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;ÂàÜÊûêÂ∏àÂõ¢Èòü&lt;/strong&gt;: üìàÂ∏ÇÂú∫ÂàÜÊûê | üí∞Âü∫Êú¨Èù¢ÂàÜÊûê | üì∞Êñ∞ÈóªÂàÜÊûê | üí¨ÊÉÖÁª™ÂàÜÊûê &lt;strong&gt;Á†îÁ©∂Âõ¢Èòü&lt;/strong&gt;: üêÇÁúãÊ∂®Á†îÁ©∂Âëò | üêªÁúãË∑åÁ†îÁ©∂Âëò | üéØ‰∫§ÊòìÂÜ≥Á≠ñÂëò &lt;strong&gt;ÁÆ°ÁêÜÂ±Ç&lt;/strong&gt;: üõ°Ô∏èÈ£éÈô©ÁÆ°ÁêÜÂëò | üëîÁ†îÁ©∂‰∏ªÁÆ°&lt;/p&gt; 
&lt;h2&gt;üöÄ Âø´ÈÄüÂºÄÂßã&lt;/h2&gt; 
&lt;h3&gt;üê≥ DockerÈÉ®ÁΩ≤ (Êé®Ëçê)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 1. ÂÖãÈöÜÈ°πÁõÆ
git clone https://github.com/hsliuping/TradingAgents-CN.git
cd TradingAgents-CN

# 2. ÈÖçÁΩÆÁéØÂ¢ÉÂèòÈáè
cp .env.example .env
# ÁºñËæë .env Êñá‰ª∂ÔºåÂ°´ÂÖ•APIÂØÜÈí•

# 3. ÂêØÂä®ÊúçÂä°
# È¶ñÊ¨°ÂêØÂä®Êàñ‰ª£Á†ÅÂèòÊõ¥Êó∂ÔºàÈúÄË¶ÅÊûÑÂª∫ÈïúÂÉèÔºâ
docker-compose up -d --build

# Êó•Â∏∏ÂêØÂä®ÔºàÈïúÂÉèÂ∑≤Â≠òÂú®ÔºåÊó†‰ª£Á†ÅÂèòÊõ¥Ôºâ
docker-compose up -d

# Êô∫ËÉΩÂêØÂä®ÔºàËá™Âä®Âà§Êñ≠ÊòØÂê¶ÈúÄË¶ÅÊûÑÂª∫Ôºâ
# WindowsÁéØÂ¢É
powershell -ExecutionPolicy Bypass -File scripts\smart_start.ps1

# Linux/MacÁéØÂ¢É
chmod +x scripts/smart_start.sh &amp;amp;&amp;amp; ./scripts/smart_start.sh

# 4. ËÆøÈóÆÂ∫îÁî®
# WebÁïåÈù¢: http://localhost:8501
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;üíª Êú¨Âú∞ÈÉ®ÁΩ≤&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 1. ÂçáÁ∫ßpip (ÈáçË¶ÅÔºÅÈÅøÂÖçÂÆâË£ÖÈîôËØØ)
python -m pip install --upgrade pip

# 2. ÂÆâË£Ö‰æùËµñ
pip install -e .

# 3. ÂêØÂä®Â∫îÁî®
python start_web.py

# 4. ËÆøÈóÆ http://localhost:8501
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;üìä ÂºÄÂßãÂàÜÊûê&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;ÈÄâÊã©Ê®°Âûã&lt;/strong&gt;: DeepSeek V3 / ÈÄö‰πâÂçÉÈóÆ / Gemini&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ËæìÂÖ•ËÇ°Á•®&lt;/strong&gt;: &lt;code&gt;000001&lt;/code&gt; (AËÇ°) / &lt;code&gt;AAPL&lt;/code&gt; (ÁæéËÇ°) / &lt;code&gt;0700.HK&lt;/code&gt; (Ê∏ØËÇ°)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ÂºÄÂßãÂàÜÊûê&lt;/strong&gt;: ÁÇπÂáª"üöÄ ÂºÄÂßãÂàÜÊûê"ÊåâÈíÆ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ÂÆûÊó∂Ë∑üË∏™&lt;/strong&gt;: ËßÇÂØüÂÆûÊó∂ËøõÂ∫¶ÂíåÂàÜÊûêÊ≠•È™§&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Êü•ÁúãÊä•Âëä&lt;/strong&gt;: ÁÇπÂáª"üìä Êü•ÁúãÂàÜÊûêÊä•Âëä"ÊåâÈíÆ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ÂØºÂá∫Êä•Âëä&lt;/strong&gt;: ÊîØÊåÅWord/PDF/MarkdownÊ†ºÂºè&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;üîê Áî®Êà∑ÊùÉÈôêÁÆ°ÁêÜ&lt;/h2&gt; 
&lt;h3&gt;üîë ÈªòËÆ§Ë¥¶Âè∑‰ø°ÊÅØ&lt;/h3&gt; 
&lt;p&gt;Á≥ªÁªüÊèê‰æõ‰ª•‰∏ãÈªòËÆ§Ë¥¶Âè∑ÔºåÈ¶ñÊ¨°ÂêØÂä®Êó∂Ëá™Âä®ÂàõÂª∫Ôºö&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Áî®Êà∑Âêç&lt;/th&gt; 
   &lt;th&gt;ÂØÜÁ†Å&lt;/th&gt; 
   &lt;th&gt;ËßíËâ≤&lt;/th&gt; 
   &lt;th&gt;ÊùÉÈôêËØ¥Êòé&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;admin&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;admin123&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;ÁÆ°ÁêÜÂëò&lt;/td&gt; 
   &lt;td&gt;ÂÆåÊï¥Á≥ªÁªüÊùÉÈôêÔºåÁî®Êà∑ÁÆ°ÁêÜÔºåÁ≥ªÁªüÈÖçÁΩÆ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;user&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;user123&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;ÊôÆÈÄöÁî®Êà∑&lt;/td&gt; 
   &lt;td&gt;ËÇ°Á•®ÂàÜÊûêÔºåÊä•ÂëäÊü•ÁúãÔºåÂü∫Á°ÄÂäüËÉΩ&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;‚ö†Ô∏è &lt;strong&gt;ÂÆâÂÖ®ÊèêÈÜí&lt;/strong&gt;: È¶ñÊ¨°ÁôªÂΩïÂêéËØ∑Á´ãÂç≥‰øÆÊîπÈªòËÆ§ÂØÜÁ†ÅÔºÅ&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;üõ°Ô∏è ÊùÉÈôêÊéßÂà∂‰ΩìÁ≥ª&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;üîê ÁôªÂΩïËÆ§ËØÅ&lt;/strong&gt;: Âü∫‰∫éÁî®Êà∑ÂêçÂØÜÁ†ÅÁöÑÂÆâÂÖ®ËÆ§ËØÅ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üë• ËßíËâ≤ÁÆ°ÁêÜ&lt;/strong&gt;: ÁÆ°ÁêÜÂëò„ÄÅÊôÆÈÄöÁî®Êà∑Á≠âÂ§öÁ∫ßÊùÉÈôê&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;‚è∞ ‰ºöËØùÁÆ°ÁêÜ&lt;/strong&gt;: Ëá™Âä®Ë∂ÖÊó∂‰øùÊä§ÔºåÂÆâÂÖ®ÁôªÂá∫&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üìä Êìç‰ΩúÊó•Âøó&lt;/strong&gt;: ÂÆåÊï¥ÁöÑÁî®Êà∑Ê¥ªÂä®ËÆ∞ÂΩï&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üõ†Ô∏è Áî®Êà∑ÁÆ°ÁêÜÂ∑•ÂÖ∑&lt;/h3&gt; 
&lt;p&gt;Á≥ªÁªüÊèê‰æõÂÆåÊï¥ÁöÑÂëΩ‰ª§Ë°åÁî®Êà∑ÁÆ°ÁêÜÂ∑•ÂÖ∑Ôºö&lt;/p&gt; 
&lt;h4&gt;Windows Áî®Êà∑&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-powershell"&gt;# ‰ΩøÁî® PowerShell ËÑöÊú¨
.\scripts\user_manager.ps1 list                    # ÂàóÂá∫ÊâÄÊúâÁî®Êà∑
.\scripts\user_manager.ps1 change-password admin   # ‰øÆÊîπÂØÜÁ†Å
.\scripts\user_manager.ps1 create newuser trader  # ÂàõÂª∫Êñ∞Áî®Êà∑
.\scripts\user_manager.ps1 delete olduser         # Âà†Èô§Áî®Êà∑

# Êàñ‰ΩøÁî®ÊâπÂ§ÑÁêÜÊñá‰ª∂
.\scripts\user_manager.bat list
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Python ËÑöÊú¨ÔºàË∑®Âπ≥Âè∞Ôºâ&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Áõ¥Êé•‰ΩøÁî® Python ËÑöÊú¨
python scripts/user_password_manager.py list
python scripts/user_password_manager.py change-password admin
python scripts/user_password_manager.py create newuser --role trader
python scripts/user_password_manager.py delete olduser
python scripts/user_password_manager.py reset  # ÈáçÁΩÆ‰∏∫ÈªòËÆ§ÈÖçÁΩÆ
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;üìã ÊîØÊåÅÁöÑÁî®Êà∑Êìç‰Ωú&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;üìù ÂàóÂá∫Áî®Êà∑&lt;/strong&gt;: Êü•ÁúãÊâÄÊúâÁî®Êà∑ÂèäÂÖ∂ËßíËâ≤ÊùÉÈôê&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üîë ‰øÆÊîπÂØÜÁ†Å&lt;/strong&gt;: ÂÆâÂÖ®ÁöÑÂØÜÁ†ÅÊõ¥Êñ∞Êú∫Âà∂&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üë§ ÂàõÂª∫Áî®Êà∑&lt;/strong&gt;: ÊîØÊåÅËá™ÂÆö‰πâËßíËâ≤ÂíåÊùÉÈôê&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üóëÔ∏è Âà†Èô§Áî®Êà∑&lt;/strong&gt;: ÂÆâÂÖ®ÁöÑÁî®Êà∑Âà†Èô§ÂäüËÉΩ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üîÑ ÈáçÁΩÆÈÖçÁΩÆ&lt;/strong&gt;: ÊÅ¢Â§çÈªòËÆ§Áî®Êà∑ËÆæÁΩÆ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üìÅ ÈÖçÁΩÆÊñá‰ª∂‰ΩçÁΩÆ&lt;/h3&gt; 
&lt;p&gt;Áî®Êà∑ÈÖçÁΩÆÂ≠òÂÇ®Âú®Ôºö&lt;code&gt;web/config/users.json&lt;/code&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;üìö &lt;strong&gt;ËØ¶ÁªÜÊñáÊ°£&lt;/strong&gt;: ÂÆåÊï¥ÁöÑÁî®Êà∑ÁÆ°ÁêÜÊåáÂçóËØ∑ÂèÇËÄÉ &lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/scripts/USER_MANAGEMENT.md"&gt;scripts/USER_MANAGEMENT.md&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;üöß ÂΩìÂâçÁâàÊú¨ÈôêÂà∂&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;‚ùå ÊöÇ‰∏çÊîØÊåÅÂú®Á∫øÁî®Êà∑Ê≥®ÂÜå&lt;/li&gt; 
 &lt;li&gt;‚ùå ÊöÇ‰∏çÊîØÊåÅWebÁïåÈù¢ÁöÑËßíËâ≤ÁÆ°ÁêÜ&lt;/li&gt; 
 &lt;li&gt;‚úÖ ÊîØÊåÅÂÆåÊï¥ÁöÑÂëΩ‰ª§Ë°åÁî®Êà∑ÁÆ°ÁêÜ&lt;/li&gt; 
 &lt;li&gt;‚úÖ ÊîØÊåÅÂÆåÊï¥ÁöÑÊùÉÈôêÊéßÂà∂Ê°ÜÊû∂&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üéØ Ê†∏ÂøÉ‰ºòÂäø&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;üß† Êô∫ËÉΩÊñ∞ÈóªÂàÜÊûê&lt;/strong&gt;: v0.1.12Êñ∞Â¢ûAIÈ©±Âä®ÁöÑÊñ∞ÈóªËøáÊª§ÂíåË¥®ÈáèËØÑ‰º∞Á≥ªÁªü&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üîß Â§öÂ±ÇÊ¨°ËøáÊª§&lt;/strong&gt;: Âü∫Á°Ä„ÄÅÂ¢ûÂº∫„ÄÅÈõÜÊàê‰∏âÁ∫ßÊñ∞ÈóªËøáÊª§Êú∫Âà∂&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üì∞ Áªü‰∏ÄÊñ∞ÈóªÂ∑•ÂÖ∑&lt;/strong&gt;: Êï¥ÂêàÂ§öÊ∫êÊñ∞ÈóªÔºåÊèê‰æõÁªü‰∏ÄÁöÑÊô∫ËÉΩÊ£ÄÁ¥¢Êé•Âè£&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üÜï Â§öLLMÈõÜÊàê&lt;/strong&gt;: v0.1.11Êñ∞Â¢û4Â§ßÊèê‰æõÂïÜÔºå60+Ê®°ÂûãÔºå‰∏ÄÁ´ôÂºèAI‰ΩìÈ™å&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üíæ ÈÖçÁΩÆÊåÅ‰πÖÂåñ&lt;/strong&gt;: Ê®°ÂûãÈÄâÊã©ÁúüÊ≠£ÊåÅ‰πÖÂåñÔºåURLÂèÇÊï∞Â≠òÂÇ®ÔºåÂà∑Êñ∞‰øùÊåÅ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üéØ Âø´ÈÄüÂàáÊç¢&lt;/strong&gt;: 5‰∏™ÁÉ≠Èó®Ê®°ÂûãÂø´ÈÄüÊåâÈíÆÔºå‰∏ÄÈîÆÂàáÊç¢‰∏çÂêåAI&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üÜï ÂÆûÊó∂ËøõÂ∫¶&lt;/strong&gt;: v0.1.10ÂºÇÊ≠•ËøõÂ∫¶Ë∑üË∏™ÔºåÂëäÂà´ÈªëÁõíÁ≠âÂæÖ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üíæ Êô∫ËÉΩ‰ºöËØù&lt;/strong&gt;: Áä∂ÊÄÅÊåÅ‰πÖÂåñÔºåÈ°µÈù¢Âà∑Êñ∞‰∏ç‰∏¢Â§±ÂàÜÊûêÁªìÊûú&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üîê Áî®Êà∑ÊùÉÈôê&lt;/strong&gt;: v0.1.14Êñ∞Â¢ûÂÆåÊï¥ÁöÑÁî®Êà∑ËÆ§ËØÅÂíåÊùÉÈôêÁÆ°ÁêÜ‰ΩìÁ≥ª&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üá®üá≥ ‰∏≠ÂõΩ‰ºòÂåñ&lt;/strong&gt;: AËÇ°/Ê∏ØËÇ°Êï∞ÊçÆ + ÂõΩ‰∫ßLLM + ‰∏≠ÊñáÁïåÈù¢&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üê≥ ÂÆπÂô®Âåñ&lt;/strong&gt;: Docker‰∏ÄÈîÆÈÉ®ÁΩ≤ÔºåÁéØÂ¢ÉÈöîÁ¶ªÔºåÂø´ÈÄüÊâ©Â±ï&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üìÑ ‰∏ì‰∏öÊä•Âëä&lt;/strong&gt;: Â§öÊ†ºÂºèÂØºÂá∫ÔºåËá™Âä®ÁîüÊàêÊäïËµÑÂª∫ËÆÆ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üõ°Ô∏è Á®≥ÂÆöÂèØÈù†&lt;/strong&gt;: Â§öÂ±ÇÊï∞ÊçÆÊ∫êÔºåÊô∫ËÉΩÈôçÁ∫ßÔºåÈîôËØØÊÅ¢Â§ç&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üîß ÊäÄÊúØÊû∂ÊûÑ&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Ê†∏ÂøÉÊäÄÊúØ&lt;/strong&gt;: Python 3.10+ | LangChain | Streamlit | MongoDB | Redis &lt;strong&gt;AIÊ®°Âûã&lt;/strong&gt;: DeepSeek V3 | ÈòøÈáåÁôæÁÇº | Google AI | OpenRouter(60+Ê®°Âûã) | OpenAI &lt;strong&gt;Êï∞ÊçÆÊ∫ê&lt;/strong&gt;: Tushare | AkShare | FinnHub | Yahoo Finance &lt;strong&gt;ÈÉ®ÁΩ≤&lt;/strong&gt;: Docker | Docker Compose | Êú¨Âú∞ÈÉ®ÁΩ≤&lt;/p&gt; 
&lt;h2&gt;üìö ÊñáÊ°£ÂíåÊîØÊåÅ&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;üìñ ÂÆåÊï¥ÊñáÊ°£&lt;/strong&gt;: &lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/"&gt;docs/&lt;/a&gt; - ÂÆâË£ÖÊåáÂçó„ÄÅ‰ΩøÁî®ÊïôÁ®ã„ÄÅAPIÊñáÊ°£&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üö® ÊïÖÈöúÊéíÈô§&lt;/strong&gt;: &lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/troubleshooting/"&gt;troubleshooting/&lt;/a&gt; - Â∏∏ËßÅÈóÆÈ¢òËß£ÂÜ≥ÊñπÊ°à&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üîÑ Êõ¥Êñ∞Êó•Âøó&lt;/strong&gt;: &lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/releases/CHANGELOG.md"&gt;CHANGELOG.md&lt;/a&gt; - ËØ¶ÁªÜÁâàÊú¨ÂéÜÂè≤&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üöÄ Âø´ÈÄüÂºÄÂßã&lt;/strong&gt;: &lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/QUICKSTART.md"&gt;QUICKSTART.md&lt;/a&gt; - 5ÂàÜÈíüÂø´ÈÄüÈÉ®ÁΩ≤ÊåáÂçó&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üÜö ‰∏≠ÊñáÂ¢ûÂº∫ÁâπËâ≤&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Áõ∏ÊØîÂéüÁâàÊñ∞Â¢û&lt;/strong&gt;: Êô∫ËÉΩÊñ∞ÈóªÂàÜÊûê | Â§öÂ±ÇÊ¨°Êñ∞ÈóªËøáÊª§ | Êñ∞ÈóªË¥®ÈáèËØÑ‰º∞ | Áªü‰∏ÄÊñ∞ÈóªÂ∑•ÂÖ∑ | Â§öLLMÊèê‰æõÂïÜÈõÜÊàê | Ê®°ÂûãÈÄâÊã©ÊåÅ‰πÖÂåñ | Âø´ÈÄüÂàáÊç¢ÊåâÈíÆ | | ÂÆûÊó∂ËøõÂ∫¶ÊòæÁ§∫ | Êô∫ËÉΩ‰ºöËØùÁÆ°ÁêÜ | ‰∏≠ÊñáÁïåÈù¢ | AËÇ°Êï∞ÊçÆ | ÂõΩ‰∫ßLLM | DockerÈÉ®ÁΩ≤ | ‰∏ì‰∏öÊä•ÂëäÂØºÂá∫ | Áªü‰∏ÄÊó•ÂøóÁÆ°ÁêÜ | WebÈÖçÁΩÆÁïåÈù¢ | ÊàêÊú¨‰ºòÂåñ&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;DockerÈÉ®ÁΩ≤ÂåÖÂê´ÁöÑÊúçÂä°&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üåê &lt;strong&gt;WebÂ∫îÁî®&lt;/strong&gt;: TradingAgents-CN‰∏ªÁ®ãÂ∫è&lt;/li&gt; 
 &lt;li&gt;üóÑÔ∏è &lt;strong&gt;MongoDB&lt;/strong&gt;: Êï∞ÊçÆÊåÅ‰πÖÂåñÂ≠òÂÇ®&lt;/li&gt; 
 &lt;li&gt;‚ö° &lt;strong&gt;Redis&lt;/strong&gt;: È´òÈÄüÁºìÂ≠ò&lt;/li&gt; 
 &lt;li&gt;üìä &lt;strong&gt;MongoDB Express&lt;/strong&gt;: Êï∞ÊçÆÂ∫ìÁÆ°ÁêÜÁïåÈù¢&lt;/li&gt; 
 &lt;li&gt;üéõÔ∏è &lt;strong&gt;Redis Commander&lt;/strong&gt;: ÁºìÂ≠òÁÆ°ÁêÜÁïåÈù¢&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;üíª ÊñπÂºè‰∫åÔºöÊú¨Âú∞ÈÉ®ÁΩ≤&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;ÈÄÇÁî®Âú∫ÊôØ&lt;/strong&gt;: ÂºÄÂèëÁéØÂ¢É„ÄÅËá™ÂÆö‰πâÈÖçÁΩÆ„ÄÅÁ¶ªÁ∫ø‰ΩøÁî®&lt;/p&gt; 
&lt;h3&gt;ÁéØÂ¢ÉË¶ÅÊ±Ç&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.10+ (Êé®Ëçê 3.11)&lt;/li&gt; 
 &lt;li&gt;4GB+ RAM (Êé®Ëçê 8GB+)&lt;/li&gt; 
 &lt;li&gt;Á®≥ÂÆöÁöÑÁΩëÁªúËøûÊé•&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ÂÆâË£ÖÊ≠•È™§&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 1. ÂÖãÈöÜÈ°πÁõÆ
git clone https://github.com/hsliuping/TradingAgents-CN.git
cd TradingAgents-CN

# 2. ÂàõÂª∫ËôöÊãüÁéØÂ¢É
python -m venv env
# Windows
env\Scripts\activate
# Linux/macOS
source env/bin/activate

# 3. ÂçáÁ∫ßpip
python -m pip install --upgrade pip

# 4. ÂÆâË£ÖÊâÄÊúâ‰æùËµñ
pip install -r requirements.txt
#ÊàñËÄÖ‰ΩøÁî®pip install -e .
pip install -e .

# Ê≥®ÊÑèÔºörequirements.txtÂ∑≤ÂåÖÂê´ÊâÄÊúâÂøÖÈúÄ‰æùËµñÔºö
# - Êï∞ÊçÆÂ∫ìÊîØÊåÅ (MongoDB + Redis)
# - Â§öÂ∏ÇÂú∫Êï∞ÊçÆÊ∫ê (Tushare, AKShare, FinnHubÁ≠â)
# - WebÁïåÈù¢ÂíåÊä•ÂëäÂØºÂá∫ÂäüËÉΩ
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;ÈÖçÁΩÆAPIÂØÜÈí•&lt;/h3&gt; 
&lt;h4&gt;üá®üá≥ Êé®ËçêÔºö‰ΩøÁî®ÈòøÈáåÁôæÁÇºÔºàÂõΩ‰∫ßÂ§ßÊ®°ÂûãÔºâ&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Â§çÂà∂ÈÖçÁΩÆÊ®°Êùø
cp .env.example .env

# ÁºñËæë .env Êñá‰ª∂ÔºåÈÖçÁΩÆ‰ª•‰∏ãÂøÖÈúÄÁöÑAPIÂØÜÈí•Ôºö
DASHSCOPE_API_KEY=your_dashscope_api_key_here
FINNHUB_API_KEY=your_finnhub_api_key_here

# Êé®ËçêÔºöTushare APIÔºà‰∏ì‰∏öAËÇ°Êï∞ÊçÆÔºâ
TUSHARE_TOKEN=your_tushare_token_here
TUSHARE_ENABLED=true

# ÂèØÈÄâÔºöÂÖ∂‰ªñAIÊ®°ÂûãAPI
GOOGLE_API_KEY=your_google_api_key_here
DEEPSEEK_API_KEY=your_deepseek_api_key_here

# Êï∞ÊçÆÂ∫ìÈÖçÁΩÆÔºàÂèØÈÄâÔºåÊèêÂçáÊÄßËÉΩÔºâ
# Êú¨Âú∞ÈÉ®ÁΩ≤‰ΩøÁî®Ê†áÂáÜÁ´ØÂè£
MONGODB_ENABLED=false  # ËÆæ‰∏∫trueÂêØÁî®MongoDB
REDIS_ENABLED=false    # ËÆæ‰∏∫trueÂêØÁî®Redis
MONGODB_HOST=localhost
MONGODB_PORT=27017     # Ê†áÂáÜMongoDBÁ´ØÂè£
REDIS_HOST=localhost
REDIS_PORT=6379        # Ê†áÂáÜRedisÁ´ØÂè£

# DockerÈÉ®ÁΩ≤Êó∂ÈúÄË¶Å‰øÆÊîπ‰∏ªÊú∫Âêç
# MONGODB_HOST=mongodb
# REDIS_HOST=redis
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;üìã ÈÉ®ÁΩ≤Ê®°ÂºèÈÖçÁΩÆËØ¥Êòé&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;Êú¨Âú∞ÈÉ®ÁΩ≤Ê®°Âºè&lt;/strong&gt;Ôºö&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Êï∞ÊçÆÂ∫ìÈÖçÁΩÆÔºàÊú¨Âú∞ÈÉ®ÁΩ≤Ôºâ
MONGODB_ENABLED=true
REDIS_ENABLED=true
MONGODB_HOST=localhost      # Êú¨Âú∞‰∏ªÊú∫
MONGODB_PORT=27017         # Ê†áÂáÜÁ´ØÂè£
REDIS_HOST=localhost       # Êú¨Âú∞‰∏ªÊú∫
REDIS_PORT=6379           # Ê†áÂáÜÁ´ØÂè£
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;DockerÈÉ®ÁΩ≤Ê®°Âºè&lt;/strong&gt;Ôºö&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Êï∞ÊçÆÂ∫ìÈÖçÁΩÆÔºàDockerÈÉ®ÁΩ≤Ôºâ
MONGODB_ENABLED=true
REDIS_ENABLED=true
MONGODB_HOST=mongodb       # DockerÂÆπÂô®ÊúçÂä°Âêç
MONGODB_PORT=27017        # Ê†áÂáÜÁ´ØÂè£
REDIS_HOST=redis          # DockerÂÆπÂô®ÊúçÂä°Âêç
REDIS_PORT=6379          # Ê†áÂáÜÁ´ØÂè£
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;üí° &lt;strong&gt;ÈÖçÁΩÆÊèêÁ§∫&lt;/strong&gt;Ôºö&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Êú¨Âú∞ÈÉ®ÁΩ≤ÔºöÈúÄË¶ÅÊâãÂä®ÂêØÂä®MongoDBÂíåRedisÊúçÂä°&lt;/li&gt; 
  &lt;li&gt;DockerÈÉ®ÁΩ≤ÔºöÊï∞ÊçÆÂ∫ìÊúçÂä°ÈÄöËøádocker-composeËá™Âä®ÂêØÂä®&lt;/li&gt; 
  &lt;li&gt;Á´ØÂè£ÂÜ≤Á™ÅÔºöÂ¶ÇÊûúÊú¨Âú∞Â∑≤ÊúâÊï∞ÊçÆÂ∫ìÊúçÂä°ÔºåÂèØ‰øÆÊîπdocker-compose.yml‰∏≠ÁöÑÁ´ØÂè£Êò†Â∞Ñ&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;üåç ÂèØÈÄâÔºö‰ΩøÁî®ÂõΩÂ§ñÊ®°Âûã&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# OpenAI (ÈúÄË¶ÅÁßëÂ≠¶‰∏äÁΩë)
OPENAI_API_KEY=your_openai_api_key

# Anthropic (ÈúÄË¶ÅÁßëÂ≠¶‰∏äÁΩë)
ANTHROPIC_API_KEY=your_anthropic_api_key
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;üóÑÔ∏è Êï∞ÊçÆÂ∫ìÈÖçÁΩÆÔºàMongoDB + RedisÔºâ&lt;/h3&gt; 
&lt;h4&gt;È´òÊÄßËÉΩÊï∞ÊçÆÂ≠òÂÇ®ÊîØÊåÅ&lt;/h4&gt; 
&lt;p&gt;Êú¨È°πÁõÆÊîØÊåÅ &lt;strong&gt;MongoDB&lt;/strong&gt; Âíå &lt;strong&gt;Redis&lt;/strong&gt; Êï∞ÊçÆÂ∫ìÔºåÊèê‰æõÔºö&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;üìä ËÇ°Á•®Êï∞ÊçÆÁºìÂ≠ò&lt;/strong&gt;: ÂáèÂ∞ëAPIË∞ÉÁî®ÔºåÊèêÂçáÂìçÂ∫îÈÄüÂ∫¶&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üîÑ Êô∫ËÉΩÈôçÁ∫ßÊú∫Âà∂&lt;/strong&gt;: MongoDB ‚Üí API ‚Üí Êú¨Âú∞ÁºìÂ≠òÁöÑÂ§öÂ±ÇÊï∞ÊçÆÊ∫ê&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;‚ö° È´òÊÄßËÉΩÁºìÂ≠ò&lt;/strong&gt;: RedisÁºìÂ≠òÁÉ≠ÁÇπÊï∞ÊçÆÔºåÊØ´ÁßíÁ∫ßÂìçÂ∫î&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üõ°Ô∏è Êï∞ÊçÆÊåÅ‰πÖÂåñ&lt;/strong&gt;: MongoDBÂ≠òÂÇ®ÂéÜÂè≤Êï∞ÊçÆÔºåÊîØÊåÅÁ¶ªÁ∫øÂàÜÊûê&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Êï∞ÊçÆÂ∫ìÈÉ®ÁΩ≤ÊñπÂºè&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;üê≥ DockerÈÉ®ÁΩ≤ÔºàÊé®ËçêÔºâ&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Â¶ÇÊûúÊÇ®‰ΩøÁî®DockerÈÉ®ÁΩ≤ÔºåÊï∞ÊçÆÂ∫ìÂ∑≤Ëá™Âä®ÂåÖÂê´Âú®ÂÜÖÔºö&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# DockerÈÉ®ÁΩ≤‰ºöËá™Âä®ÂêØÂä®ÊâÄÊúâÊúçÂä°ÔºåÂåÖÊã¨Ôºö
docker-compose up -d --build
# - WebÂ∫îÁî® (Á´ØÂè£8501)
# - MongoDB (Á´ØÂè£27017)
# - Redis (Á´ØÂè£6379)
# - Êï∞ÊçÆÂ∫ìÁÆ°ÁêÜÁïåÈù¢ (Á´ØÂè£8081, 8082)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;üíª Êú¨Âú∞ÈÉ®ÁΩ≤ - Êï∞ÊçÆÂ∫ìÈÖçÁΩÆ&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Â¶ÇÊûúÊÇ®‰ΩøÁî®Êú¨Âú∞ÈÉ®ÁΩ≤ÔºåÂèØ‰ª•ÈÄâÊã©‰ª•‰∏ãÊñπÂºèÔºö&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;ÊñπÂºè‰∏ÄÔºö‰ªÖÂêØÂä®Êï∞ÊçÆÂ∫ìÊúçÂä°&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# ‰ªÖÂêØÂä® MongoDB + Redis ÊúçÂä°Ôºà‰∏çÂêØÂä®WebÂ∫îÁî®Ôºâ
docker-compose up -d mongodb redis mongo-express redis-commander

# Êü•ÁúãÊúçÂä°Áä∂ÊÄÅ
docker-compose ps

# ÂÅúÊ≠¢ÊúçÂä°
docker-compose down
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;ÊñπÂºè‰∫åÔºöÂÆåÂÖ®Êú¨Âú∞ÂÆâË£Ö&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Êï∞ÊçÆÂ∫ì‰æùËµñÂ∑≤ÂåÖÂê´Âú®requirements.txt‰∏≠ÔºåÊó†ÈúÄÈ¢ùÂ§ñÂÆâË£Ö

# ÂêØÂä® MongoDB (ÈªòËÆ§Á´ØÂè£ 27017)
mongod --dbpath ./data/mongodb

# ÂêØÂä® Redis (ÈªòËÆ§Á´ØÂè£ 6379)
redis-server
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;‚ö†Ô∏è &lt;strong&gt;ÈáçË¶ÅËØ¥Êòé&lt;/strong&gt;:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;üê≥ DockerÈÉ®ÁΩ≤&lt;/strong&gt;: Êï∞ÊçÆÂ∫ìËá™Âä®ÂåÖÂê´ÔºåÊó†ÈúÄÈ¢ùÂ§ñÈÖçÁΩÆ&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;üíª Êú¨Âú∞ÈÉ®ÁΩ≤&lt;/strong&gt;: ÂèØÈÄâÊã©‰ªÖÂêØÂä®Êï∞ÊçÆÂ∫ìÊúçÂä°ÊàñÂÆåÂÖ®Êú¨Âú∞ÂÆâË£Ö&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;üìã Êé®Ëçê&lt;/strong&gt;: ‰ΩøÁî®DockerÈÉ®ÁΩ≤‰ª•Ëé∑ÂæóÊúÄ‰Ω≥‰ΩìÈ™åÂíå‰∏ÄËá¥ÊÄß&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Êï∞ÊçÆÂ∫ìÈÖçÁΩÆÈÄâÈ°π&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;ÁéØÂ¢ÉÂèòÈáèÈÖçÁΩÆ&lt;/strong&gt;ÔºàÊé®ËçêÔºâÔºö&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# MongoDB ÈÖçÁΩÆ
MONGODB_HOST=localhost
MONGODB_PORT=27017
MONGODB_DATABASE=trading_agents
MONGODB_USERNAME=admin
MONGODB_PASSWORD=your_password

# Redis ÈÖçÁΩÆ
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_PASSWORD=your_redis_password
REDIS_DB=0
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;ÈÖçÁΩÆÊñá‰ª∂ÊñπÂºè&lt;/strong&gt;Ôºö&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# config/database_config.py
DATABASE_CONFIG = {
    'mongodb': {
        'host': 'localhost',
        'port': 27017,
        'database': 'trading_agents',
        'username': 'admin',
        'password': 'your_password'
    },
    'redis': {
        'host': 'localhost',
        'port': 6379,
        'password': 'your_redis_password',
        'db': 0
    }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Êï∞ÊçÆÂ∫ìÂäüËÉΩÁâπÊÄß&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;MongoDB ÂäüËÉΩ&lt;/strong&gt;Ôºö&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;‚úÖ ËÇ°Á•®Âü∫Á°Ä‰ø°ÊÅØÂ≠òÂÇ®&lt;/li&gt; 
 &lt;li&gt;‚úÖ ÂéÜÂè≤‰ª∑Ê†ºÊï∞ÊçÆÁºìÂ≠ò&lt;/li&gt; 
 &lt;li&gt;‚úÖ ÂàÜÊûêÁªìÊûúÊåÅ‰πÖÂåñ&lt;/li&gt; 
 &lt;li&gt;‚úÖ Áî®Êà∑ÈÖçÁΩÆÁÆ°ÁêÜ&lt;/li&gt; 
 &lt;li&gt;‚úÖ Ëá™Âä®Êï∞ÊçÆÂêåÊ≠•&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Redis ÂäüËÉΩ&lt;/strong&gt;Ôºö&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;‚ö° ÂÆûÊó∂‰ª∑Ê†ºÊï∞ÊçÆÁºìÂ≠ò&lt;/li&gt; 
 &lt;li&gt;‚ö° APIÂìçÂ∫îÁªìÊûúÁºìÂ≠ò&lt;/li&gt; 
 &lt;li&gt;‚ö° ‰ºöËØùÁä∂ÊÄÅÁÆ°ÁêÜ&lt;/li&gt; 
 &lt;li&gt;‚ö° ÁÉ≠ÁÇπÊï∞ÊçÆÈ¢ÑÂä†ËΩΩ&lt;/li&gt; 
 &lt;li&gt;‚ö° ÂàÜÂ∏ÉÂºèÈîÅÊîØÊåÅ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Êô∫ËÉΩÈôçÁ∫ßÊú∫Âà∂&lt;/h4&gt; 
&lt;p&gt;Á≥ªÁªüÈááÁî®Â§öÂ±ÇÊï∞ÊçÆÊ∫êÈôçÁ∫ßÁ≠ñÁï•ÔºåÁ°Æ‰øùÈ´òÂèØÁî®ÊÄßÔºö&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;üìä Êï∞ÊçÆËé∑ÂèñÊµÅÁ®ãÔºö
1. üîç Ê£ÄÊü• Redis ÁºìÂ≠ò (ÊØ´ÁßíÁ∫ß)
2. üìö Êü•ËØ¢ MongoDB Â≠òÂÇ® (ÁßíÁ∫ß)
3. üåê Ë∞ÉÁî®ÈÄöËææ‰ø°API (ÁßíÁ∫ß)
4. üíæ Êú¨Âú∞Êñá‰ª∂ÁºìÂ≠ò (Â§áÁî®)
5. ‚ùå ËøîÂõûÈîôËØØ‰ø°ÊÅØ
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;ÈÖçÁΩÆÈôçÁ∫ßÁ≠ñÁï•&lt;/strong&gt;Ôºö&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Âú® .env Êñá‰ª∂‰∏≠ÈÖçÁΩÆ
ENABLE_MONGODB=true
ENABLE_REDIS=true
ENABLE_FALLBACK=true

# ÁºìÂ≠òËøáÊúüÊó∂Èó¥ÔºàÁßíÔºâ
REDIS_CACHE_TTL=300
MONGODB_CACHE_TTL=3600
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;ÊÄßËÉΩ‰ºòÂåñÂª∫ËÆÆ&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;Áîü‰∫ßÁéØÂ¢ÉÈÖçÁΩÆ&lt;/strong&gt;Ôºö&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# MongoDB ‰ºòÂåñ
MONGODB_MAX_POOL_SIZE=50
MONGODB_MIN_POOL_SIZE=5
MONGODB_MAX_IDLE_TIME=30000

# Redis ‰ºòÂåñ
REDIS_MAX_CONNECTIONS=20
REDIS_CONNECTION_POOL_SIZE=10
REDIS_SOCKET_TIMEOUT=5
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Êï∞ÊçÆÂ∫ìÁÆ°ÁêÜÂ∑•ÂÖ∑&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# ÂàùÂßãÂåñÊï∞ÊçÆÂ∫ì
python scripts/setup/init_database.py

# Á≥ªÁªüÁä∂ÊÄÅÊ£ÄÊü•
python scripts/validation/check_system_status.py

# Ê∏ÖÁêÜÁºìÂ≠òÂ∑•ÂÖ∑
python scripts/maintenance/cleanup_cache.py --days 7
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;ÊïÖÈöúÊéíÈô§&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;Â∏∏ËßÅÈóÆÈ¢òËß£ÂÜ≥&lt;/strong&gt;Ôºö&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ü™ü Windows 10 ChromaDBÂÖºÂÆπÊÄßÈóÆÈ¢ò&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;ÈóÆÈ¢òÁé∞Ë±°&lt;/strong&gt;ÔºöÂú®Windows 10‰∏äÂá∫Áé∞ &lt;code&gt;Configuration error: An instance of Chroma already exists for ephemeral with different settings&lt;/code&gt; ÈîôËØØÔºåËÄåWindows 11Ê≠£Â∏∏„ÄÇ&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Âø´ÈÄüËß£ÂÜ≥ÊñπÊ°à&lt;/strong&gt;Ôºö&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# ÊñπÊ°à1ÔºöÁ¶ÅÁî®ÂÜÖÂ≠òÂäüËÉΩÔºàÊé®ËçêÔºâ
# Âú® .env Êñá‰ª∂‰∏≠Ê∑ªÂä†Ôºö
MEMORY_ENABLED=false

# ÊñπÊ°à2Ôºö‰ΩøÁî®‰∏ìÁî®‰øÆÂ§çËÑöÊú¨
powershell -ExecutionPolicy Bypass -File scripts\fix_chromadb_win10.ps1

# ÊñπÊ°à3ÔºöÁÆ°ÁêÜÂëòÊùÉÈôêËøêË°å
# Âè≥ÈîÆPowerShell -&amp;gt; "‰ª•ÁÆ°ÁêÜÂëòË∫´‰ªΩËøêË°å"
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;ËØ¶ÁªÜËß£ÂÜ≥ÊñπÊ°à&lt;/strong&gt;ÔºöÂèÇËÄÉ &lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/troubleshooting/windows10-chromadb-fix.md"&gt;Windows 10ÂÖºÂÆπÊÄßÊåáÂçó&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;MongoDBËøûÊé•Â§±Ë¥•&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;DockerÈÉ®ÁΩ≤&lt;/strong&gt;Ôºö&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# Ê£ÄÊü•ÊúçÂä°Áä∂ÊÄÅ
docker-compose logs mongodb

# ÈáçÂêØÊúçÂä°
docker-compose restart mongodb
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Êú¨Âú∞ÈÉ®ÁΩ≤&lt;/strong&gt;Ôºö&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# Ê£ÄÊü•MongoDBËøõÁ®ã
ps aux | grep mongod

# ÈáçÂêØMongoDB
sudo systemctl restart mongod  # Linux
brew services restart mongodb  # macOS
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;RedisËøûÊé•Ë∂ÖÊó∂&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# Ê£ÄÊü•RedisÁä∂ÊÄÅ
redis-cli ping

# Ê∏ÖÁêÜRedisÁºìÂ≠ò
redis-cli flushdb
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ÁºìÂ≠òÈóÆÈ¢ò&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# Ê£ÄÊü•Á≥ªÁªüÁä∂ÊÄÅÂíåÁºìÂ≠ò
python scripts/validation/check_system_status.py

# Ê∏ÖÁêÜËøáÊúüÁºìÂ≠ò
python scripts/maintenance/cleanup_cache.py --days 7
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;üí° &lt;strong&gt;ÊèêÁ§∫&lt;/strong&gt;: Âç≥‰Ωø‰∏çÈÖçÁΩÆÊï∞ÊçÆÂ∫ìÔºåÁ≥ªÁªü‰ªçÂèØÊ≠£Â∏∏ËøêË°åÔºå‰ºöËá™Âä®ÈôçÁ∫ßÂà∞APIÁõ¥Êé•Ë∞ÉÁî®Ê®°Âºè„ÄÇÊï∞ÊçÆÂ∫ìÈÖçÁΩÆÊòØÂèØÈÄâÁöÑÊÄßËÉΩ‰ºòÂåñÂäüËÉΩ„ÄÇ&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;üìö &lt;strong&gt;ËØ¶ÁªÜÊñáÊ°£&lt;/strong&gt;: Êõ¥Â§öÊï∞ÊçÆÂ∫ìÈÖçÁΩÆ‰ø°ÊÅØËØ∑ÂèÇËÄÉ &lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/architecture/database-architecture.md"&gt;Êï∞ÊçÆÂ∫ìÊû∂ÊûÑÊñáÊ°£&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;üì§ Êä•ÂëäÂØºÂá∫ÂäüËÉΩ&lt;/h3&gt; 
&lt;h4&gt;Êñ∞Â¢ûÂäüËÉΩÔºö‰∏ì‰∏öÂàÜÊûêÊä•ÂëäÂØºÂá∫&lt;/h4&gt; 
&lt;p&gt;Êú¨È°πÁõÆÁé∞Â∑≤ÊîØÊåÅÂ∞ÜËÇ°Á•®ÂàÜÊûêÁªìÊûúÂØºÂá∫‰∏∫Â§öÁßç‰∏ì‰∏öÊ†ºÂºèÔºö&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;ÊîØÊåÅÁöÑÂØºÂá∫Ê†ºÂºè&lt;/strong&gt;Ôºö&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;üìÑ Markdown (.md)&lt;/strong&gt; - ËΩªÈáèÁ∫ßÊ†áËÆ∞ËØ≠Ë®ÄÔºåÈÄÇÂêàÊäÄÊúØÁî®Êà∑ÂíåÁâàÊú¨ÊéßÂà∂&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üìù Word (.docx)&lt;/strong&gt; - Microsoft WordÊñáÊ°£ÔºåÈÄÇÂêàÂïÜÂä°Êä•ÂëäÂíåËøõ‰∏ÄÊ≠•ÁºñËæë&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üìä PDF (.pdf)&lt;/strong&gt; - ‰æøÊê∫ÂºèÊñáÊ°£Ê†ºÂºèÔºåÈÄÇÂêàÊ≠£ÂºèÂàÜ‰∫´ÂíåÊâìÂç∞&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Êä•ÂëäÂÜÖÂÆπÁªìÊûÑ&lt;/strong&gt;Ôºö&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üéØ &lt;strong&gt;ÊäïËµÑÂÜ≥Á≠ñÊëòË¶Å&lt;/strong&gt; - ‰π∞ÂÖ•/ÊåÅÊúâ/ÂçñÂá∫Âª∫ËÆÆÔºåÁΩÆ‰ø°Â∫¶ÔºåÈ£éÈô©ËØÑÂàÜ&lt;/li&gt; 
 &lt;li&gt;üìä &lt;strong&gt;ËØ¶ÁªÜÂàÜÊûêÊä•Âëä&lt;/strong&gt; - ÊäÄÊúØÂàÜÊûêÔºåÂü∫Êú¨Èù¢ÂàÜÊûêÔºåÂ∏ÇÂú∫ÊÉÖÁª™ÔºåÊñ∞Èóª‰∫ã‰ª∂&lt;/li&gt; 
 &lt;li&gt;‚ö†Ô∏è &lt;strong&gt;È£éÈô©ÊèêÁ§∫&lt;/strong&gt; - ÂÆåÊï¥ÁöÑÊäïËµÑÈ£éÈô©Â£∞ÊòéÂíåÂÖçË¥£Êù°Ê¨æ&lt;/li&gt; 
 &lt;li&gt;üìã &lt;strong&gt;ÈÖçÁΩÆ‰ø°ÊÅØ&lt;/strong&gt; - ÂàÜÊûêÂèÇÊï∞ÔºåÊ®°Âûã‰ø°ÊÅØÔºåÁîüÊàêÊó∂Èó¥&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;‰ΩøÁî®ÊñπÊ≥ï&lt;/strong&gt;Ôºö&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;ÂÆåÊàêËÇ°Á•®ÂàÜÊûêÂêéÔºåÂú®ÁªìÊûúÈ°µÈù¢Â∫ïÈÉ®ÊâæÂà∞"üì§ ÂØºÂá∫Êä•Âëä"ÈÉ®ÂàÜ&lt;/li&gt; 
 &lt;li&gt;ÈÄâÊã©ÈúÄË¶ÅÁöÑÊ†ºÂºèÔºöMarkdown„ÄÅWordÊàñPDF&lt;/li&gt; 
 &lt;li&gt;ÁÇπÂáªÂØºÂá∫ÊåâÈíÆÔºåÁ≥ªÁªüËá™Âä®ÁîüÊàêÂπ∂Êèê‰æõ‰∏ãËΩΩ&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;ÂÆâË£ÖÂØºÂá∫‰æùËµñ&lt;/strong&gt;Ôºö&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# ÂÆâË£ÖPython‰æùËµñ
pip install markdown pypandoc

# ÂÆâË£ÖÁ≥ªÁªüÂ∑•ÂÖ∑ÔºàÁî®‰∫éPDFÂØºÂá∫Ôºâ
# Windows: choco install pandoc wkhtmltopdf
# macOS: brew install pandoc wkhtmltopdf
# Linux: sudo apt-get install pandoc wkhtmltopdf
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;üìö &lt;strong&gt;ËØ¶ÁªÜÊñáÊ°£&lt;/strong&gt;: ÂÆåÊï¥ÁöÑÂØºÂá∫ÂäüËÉΩ‰ΩøÁî®ÊåáÂçóËØ∑ÂèÇËÄÉ &lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/EXPORT_GUIDE.md"&gt;ÂØºÂá∫ÂäüËÉΩÊåáÂçó&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;üöÄ ÂêØÂä®Â∫îÁî®&lt;/h3&gt; 
&lt;h4&gt;üê≥ DockerÂêØÂä®ÔºàÊé®ËçêÔºâ&lt;/h4&gt; 
&lt;p&gt;Â¶ÇÊûúÊÇ®‰ΩøÁî®DockerÈÉ®ÁΩ≤ÔºåÂ∫îÁî®Â∑≤ÁªèËá™Âä®ÂêØÂä®Ôºö&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Â∫îÁî®Â∑≤Âú®Docker‰∏≠ËøêË°åÔºåÁõ¥Êé•ËÆøÈóÆÔºö
# WebÁïåÈù¢: http://localhost:8501
# Êï∞ÊçÆÂ∫ìÁÆ°ÁêÜ: http://localhost:8081
# ÁºìÂ≠òÁÆ°ÁêÜ: http://localhost:8082

# Êü•ÁúãËøêË°åÁä∂ÊÄÅ
docker-compose ps

# Êü•ÁúãÊó•Âøó
docker-compose logs -f web
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;üíª Êú¨Âú∞ÂêØÂä®&lt;/h4&gt; 
&lt;p&gt;Â¶ÇÊûúÊÇ®‰ΩøÁî®Êú¨Âú∞ÈÉ®ÁΩ≤Ôºö&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 1. ÊøÄÊ¥ªËôöÊãüÁéØÂ¢É
# Windows
.\env\Scripts\activate
# Linux/macOS
source env/bin/activate

# 2. ÂÆâË£ÖÈ°πÁõÆÂà∞ËôöÊãüÁéØÂ¢ÉÔºàÈáçË¶ÅÔºÅÔºâ
pip install -e .

# 3. ÂêØÂä®WebÁÆ°ÁêÜÁïåÈù¢
# ÊñπÊ≥ï1Ôºö‰ΩøÁî®È°πÁõÆÂêØÂä®ËÑöÊú¨ÔºàÊé®ËçêÔºâ
python start_web.py

# ÊñπÊ≥ï2Ôºö‰ΩøÁî®ÂéüÂßãÂêØÂä®ËÑöÊú¨
python web/run_web.py

# ÊñπÊ≥ï3ÔºöÁõ¥Êé•‰ΩøÁî®streamlitÔºàÈúÄË¶ÅÂÖàÂÆâË£ÖÈ°πÁõÆÔºâ
streamlit run web/app.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;ÁÑ∂ÂêéÂú®ÊµèËßàÂô®‰∏≠ËÆøÈóÆ &lt;code&gt;http://localhost:8501&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;WebÁïåÈù¢ÁâπËâ≤ÂäüËÉΩ&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üá∫üá∏ &lt;strong&gt;ÁæéËÇ°ÂàÜÊûê&lt;/strong&gt;: ÊîØÊåÅ AAPL, TSLA, NVDA Á≠âÁæéËÇ°‰ª£Á†Å&lt;/li&gt; 
 &lt;li&gt;üá®üá≥ &lt;strong&gt;AËÇ°ÂàÜÊûê&lt;/strong&gt;: ÊîØÊåÅ 000001, 600519, 300750 Á≠âAËÇ°‰ª£Á†Å&lt;/li&gt; 
 &lt;li&gt;üìä &lt;strong&gt;ÂÆûÊó∂Êï∞ÊçÆ&lt;/strong&gt;: ÈÄöËææ‰ø°APIÊèê‰æõAËÇ°ÂÆûÊó∂Ë°åÊÉÖÊï∞ÊçÆ&lt;/li&gt; 
 &lt;li&gt;ü§ñ &lt;strong&gt;Êô∫ËÉΩ‰ΩìÈÄâÊã©&lt;/strong&gt;: ÂèØÈÄâÊã©‰∏çÂêåÁöÑÂàÜÊûêÂ∏àÁªÑÂêà&lt;/li&gt; 
 &lt;li&gt;üì§ &lt;strong&gt;Êä•ÂëäÂØºÂá∫&lt;/strong&gt;: ‰∏ÄÈîÆÂØºÂá∫Markdown/Word/PDFÊ†ºÂºè‰∏ì‰∏öÂàÜÊûêÊä•Âëä&lt;/li&gt; 
 &lt;li&gt;üéØ &lt;strong&gt;5Á∫ßÁ†îÁ©∂Ê∑±Â∫¶&lt;/strong&gt;: ‰ªéÂø´ÈÄüÂàÜÊûê(2-4ÂàÜÈíü)Âà∞ÂÖ®Èù¢ÂàÜÊûê(15-25ÂàÜÈíü)&lt;/li&gt; 
 &lt;li&gt;üìä &lt;strong&gt;Êô∫ËÉΩÂàÜÊûêÂ∏àÈÄâÊã©&lt;/strong&gt;: Â∏ÇÂú∫ÊäÄÊúØ„ÄÅÂü∫Êú¨Èù¢„ÄÅÊñ∞Èóª„ÄÅÁ§æ‰∫§Â™í‰ΩìÂàÜÊûêÂ∏à&lt;/li&gt; 
 &lt;li&gt;üîÑ &lt;strong&gt;ÂÆûÊó∂ËøõÂ∫¶ÊòæÁ§∫&lt;/strong&gt;: ÂèØËßÜÂåñÂàÜÊûêËøáÁ®ãÔºåÈÅøÂÖçÁ≠âÂæÖÁÑ¶Ëôë&lt;/li&gt; 
 &lt;li&gt;üìà &lt;strong&gt;ÁªìÊûÑÂåñÁªìÊûú&lt;/strong&gt;: ÊäïËµÑÂª∫ËÆÆ„ÄÅÁõÆÊ†á‰ª∑‰Ωç„ÄÅÁΩÆ‰ø°Â∫¶„ÄÅÈ£éÈô©ËØÑ‰º∞&lt;/li&gt; 
 &lt;li&gt;üá®üá≥ &lt;strong&gt;ÂÆåÂÖ®‰∏≠ÊñáÂåñ&lt;/strong&gt;: ÁïåÈù¢ÂíåÂàÜÊûêÁªìÊûúÂÖ®‰∏≠ÊñáÊòæÁ§∫&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Á†îÁ©∂Ê∑±Â∫¶Á∫ßÂà´ËØ¥Êòé&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;1Á∫ß - Âø´ÈÄüÂàÜÊûê&lt;/strong&gt; (2-4ÂàÜÈíü): Êó•Â∏∏ÁõëÊéßÔºåÂü∫Á°ÄÂÜ≥Á≠ñ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;2Á∫ß - Âü∫Á°ÄÂàÜÊûê&lt;/strong&gt; (4-6ÂàÜÈíü): Â∏∏ËßÑÊäïËµÑÔºåÂπ≥Ë°°ÈÄüÂ∫¶&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;3Á∫ß - Ê†áÂáÜÂàÜÊûê&lt;/strong&gt; (6-10ÂàÜÈíü): ÈáçË¶ÅÂÜ≥Á≠ñÔºåÊé®ËçêÈªòËÆ§&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;4Á∫ß - Ê∑±Â∫¶ÂàÜÊûê&lt;/strong&gt; (10-15ÂàÜÈíü): ÈáçÂ§ßÊäïËµÑÔºåËØ¶ÁªÜÁ†îÁ©∂&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;5Á∫ß - ÂÖ®Èù¢ÂàÜÊûê&lt;/strong&gt; (15-25ÂàÜÈíü): ÊúÄÈáçË¶ÅÂÜ≥Á≠ñÔºåÊúÄÂÖ®Èù¢ÂàÜÊûê&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;üíª ‰ª£Á†ÅË∞ÉÁî®ÔºàÈÄÇÂêàÂºÄÂèëËÄÖÔºâ&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from tradingagents.graph.trading_graph import TradingAgentsGraph
from tradingagents.default_config import DEFAULT_CONFIG

# ÈÖçÁΩÆÈòøÈáåÁôæÁÇº
config = DEFAULT_CONFIG.copy()
config["llm_provider"] = "dashscope"
config["deep_think_llm"] = "qwen-plus"      # Ê∑±Â∫¶ÂàÜÊûê
config["quick_think_llm"] = "qwen-turbo"    # Âø´ÈÄü‰ªªÂä°

# ÂàõÂª∫‰∫§ÊòìÊô∫ËÉΩ‰Ωì
ta = TradingAgentsGraph(debug=True, config=config)

# ÂàÜÊûêËÇ°Á•® (‰ª•ËãπÊûúÂÖ¨Âè∏‰∏∫‰æã)
state, decision = ta.propagate("AAPL", "2024-01-15")

# ËæìÂá∫ÂàÜÊûêÁªìÊûú
print(f"Êé®ËçêÂä®‰Ωú: {decision['action']}")
print(f"ÁΩÆ‰ø°Â∫¶: {decision['confidence']:.1%}")
print(f"È£éÈô©ËØÑÂàÜ: {decision['risk_score']:.1%}")
print(f"Êé®ÁêÜËøáÁ®ã: {decision['reasoning']}")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Âø´ÈÄüÂêØÂä®ËÑöÊú¨&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# ÈòøÈáåÁôæÁÇºÊºîÁ§∫ÔºàÊé®Ëçê‰∏≠ÊñáÁî®Êà∑Ôºâ
python examples/dashscope/demo_dashscope_chinese.py

# ÈòøÈáåÁôæÁÇºÂÆåÊï¥ÊºîÁ§∫
python examples/dashscope/demo_dashscope.py

# ÈòøÈáåÁôæÁÇºÁÆÄÂåñÊµãËØï
python examples/dashscope/demo_dashscope_simple.py

# OpenAIÊºîÁ§∫ÔºàÈúÄË¶ÅÂõΩÂ§ñAPIÔºâ
python examples/openai/demo_openai.py

# ÈõÜÊàêÊµãËØï
python tests/integration/test_dashscope_integration.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;üìÅ Êï∞ÊçÆÁõÆÂΩïÈÖçÁΩÆ&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;Êñ∞ÂäüËÉΩ&lt;/strong&gt;: ÁÅµÊ¥ªÈÖçÁΩÆÊï∞ÊçÆÂ≠òÂÇ®Ë∑ØÂæÑÔºåÊîØÊåÅÂ§öÁßçÈÖçÁΩÆÊñπÂºèÔºö&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Êü•ÁúãÂΩìÂâçÊï∞ÊçÆÁõÆÂΩïÈÖçÁΩÆ
python -m cli.main data-config --show

# ËÆæÁΩÆËá™ÂÆö‰πâÊï∞ÊçÆÁõÆÂΩï
python -m cli.main data-config --set /path/to/your/data

# ÈáçÁΩÆ‰∏∫ÈªòËÆ§ÈÖçÁΩÆ
python -m cli.main data-config --reset
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;ÁéØÂ¢ÉÂèòÈáèÈÖçÁΩÆ&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Windows
set TRADING_AGENTS_DATA_DIR=C:\MyTradingData

# Linux/macOS
export TRADING_AGENTS_DATA_DIR=/home/user/trading_data
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Á®ãÂ∫èÂåñÈÖçÁΩÆ&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from tradingagents.config_manager import ConfigManager

# ËÆæÁΩÆÊï∞ÊçÆÁõÆÂΩï
config_manager = ConfigManager()
config_manager.set_data_directory("/path/to/data")

# Ëé∑ÂèñÈÖçÁΩÆ
data_dir = config_manager.get_data_directory()
print(f"Êï∞ÊçÆÁõÆÂΩï: {data_dir}")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;ÈÖçÁΩÆ‰ºòÂÖàÁ∫ß&lt;/strong&gt;: Á®ãÂ∫èËÆæÁΩÆ &amp;gt; ÁéØÂ¢ÉÂèòÈáè &amp;gt; ÈÖçÁΩÆÊñá‰ª∂ &amp;gt; ÈªòËÆ§ÂÄº&lt;/p&gt; 
&lt;p&gt;ËØ¶ÁªÜËØ¥ÊòéËØ∑ÂèÇËÄÉ: &lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/configuration/data-directory-configuration.md"&gt;üìÅ Êï∞ÊçÆÁõÆÂΩïÈÖçÁΩÆÊåáÂçó&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;‰∫§‰∫íÂºèÂàÜÊûê&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# ÂêØÂä®‰∫§‰∫íÂºèÂëΩ‰ª§Ë°åÁïåÈù¢
python -m cli.main
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;üéØ &lt;strong&gt;Âø´ÈÄüÂØºËà™&lt;/strong&gt; - ÊâæÂà∞ÊÇ®ÈúÄË¶ÅÁöÑÂÜÖÂÆπ&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;üéØ&lt;strong&gt;ÊàëÊÉ≥Ë¶Å...&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;üìñ&lt;strong&gt;Êé®ËçêÊñáÊ°£&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;‚è±Ô∏è&lt;strong&gt;ÈòÖËØªÊó∂Èó¥&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Âø´ÈÄü‰∏äÊâã&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/overview/quick-start.md"&gt;üöÄ Âø´ÈÄüÂºÄÂßã&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;10ÂàÜÈíü&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;‰∫ÜËß£Êû∂ÊûÑ&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/architecture/system-architecture.md"&gt;üèõÔ∏è Á≥ªÁªüÊû∂ÊûÑ&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;15ÂàÜÈíü&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Áúã‰ª£Á†ÅÁ§∫‰æã&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/examples/basic-examples.md"&gt;üìö Âü∫Á°ÄÁ§∫‰æã&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;20ÂàÜÈíü&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Ëß£ÂÜ≥ÈóÆÈ¢ò&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/faq/faq.md"&gt;üÜò Â∏∏ËßÅÈóÆÈ¢ò&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;5ÂàÜÈíü&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Ê∑±Â∫¶Â≠¶‰π†&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/#-%E8%AF%A6%E7%BB%86%E6%96%87%E6%A1%A3%E7%9B%AE%E5%BD%95"&gt;üìÅ ÂÆåÊï¥ÊñáÊ°£ÁõÆÂΩï&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2Â∞èÊó∂+&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;üí° &lt;strong&gt;ÊèêÁ§∫&lt;/strong&gt;: Êàë‰ª¨ÁöÑ &lt;code&gt;docs/&lt;/code&gt; ÁõÆÂΩïÂåÖÂê´‰∫Ü &lt;strong&gt;50,000+Â≠ó&lt;/strong&gt; ÁöÑËØ¶ÁªÜ‰∏≠ÊñáÊñáÊ°£ÔºåËøôÊòØ‰∏éÂéüÁâàÊúÄÂ§ßÁöÑÂå∫Âà´ÔºÅ&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;üìö ÂÆåÊï¥ÊñáÊ°£‰ΩìÁ≥ª - Ê†∏ÂøÉ‰∫ÆÁÇπ&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;üåü ËøôÊòØÊú¨È°πÁõÆ‰∏éÂéüÁâàÊúÄÂ§ßÁöÑÂå∫Âà´ÔºÅ&lt;/strong&gt; Êàë‰ª¨ÊûÑÂª∫‰∫Ü‰∏öÁïåÊúÄÂÆåÊï¥ÁöÑ‰∏≠ÊñáÈáëËûçAIÊ°ÜÊû∂ÊñáÊ°£‰ΩìÁ≥ªÔºåÂåÖÂê´Ë∂ÖËøá &lt;strong&gt;50,000Â≠ó&lt;/strong&gt; ÁöÑËØ¶ÁªÜÊäÄÊúØÊñáÊ°£Ôºå&lt;strong&gt;20+&lt;/strong&gt; ‰∏™‰∏ì‰∏öÊñáÊ°£Êñá‰ª∂Ôºå&lt;strong&gt;100+&lt;/strong&gt; ‰∏™‰ª£Á†ÅÁ§∫‰æã„ÄÇ&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;üéØ ‰∏∫‰ªÄ‰πàÈÄâÊã©Êàë‰ª¨ÁöÑÊñáÊ°£Ôºü&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;ÂØπÊØîÁª¥Â∫¶&lt;/th&gt; 
   &lt;th&gt;ÂéüÁâà TradingAgents&lt;/th&gt; 
   &lt;th&gt;üöÄ&lt;strong&gt;‰∏≠ÊñáÂ¢ûÂº∫Áâà&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ÊñáÊ°£ËØ≠Ë®Ä&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Ëã±ÊñáÂü∫Á°ÄËØ¥Êòé&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;ÂÆåÊï¥‰∏≠Êñá‰ΩìÁ≥ª&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ÊñáÊ°£Ê∑±Â∫¶&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;ÁÆÄÂçï‰ªãÁªç&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Ê∑±Â∫¶ÊäÄÊúØÂâñÊûê&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Êû∂ÊûÑËØ¥Êòé&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Ê¶ÇÂøµÊÄßÊèèËø∞&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;ËØ¶ÁªÜËÆæËÆ°ÊñáÊ°£ + Êû∂ÊûÑÂõæ&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;‰ΩøÁî®ÊåáÂçó&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Âü∫Á°ÄÁ§∫‰æã&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;‰ªéÂÖ•Èó®Âà∞‰∏ìÂÆ∂ÁöÑÂÆåÊï¥Ë∑ØÂæÑ&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ÊïÖÈöúÊéíÈô§&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Êó†&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;ËØ¶ÁªÜFAQ + Ëß£ÂÜ≥ÊñπÊ°à&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;‰ª£Á†ÅÁ§∫‰æã&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Â∞ëÈáèÁ§∫‰æã&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;100+ ÂÆûÁî®Á§∫‰æã&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;üìñ ÊñáÊ°£ÂØºËà™ - ÊåâÂ≠¶‰π†Ë∑ØÂæÑÁªÑÁªá&lt;/h3&gt; 
&lt;h4&gt;üöÄ &lt;strong&gt;Êñ∞ÊâãÂÖ•Èó®Ë∑ØÂæÑ&lt;/strong&gt; (Êé®Ëçê‰ªéËøôÈáåÂºÄÂßã)&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/overview/project-overview.md"&gt;üìã È°πÁõÆÊ¶ÇËø∞&lt;/a&gt; - &lt;strong&gt;‰∫ÜËß£È°πÁõÆËÉåÊôØÂíåÊ†∏ÂøÉ‰ª∑ÂÄº&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/overview/installation.md"&gt;‚öôÔ∏è ËØ¶ÁªÜÂÆâË£Ö&lt;/a&gt; - &lt;strong&gt;ÂêÑÂπ≥Âè∞ËØ¶ÁªÜÂÆâË£ÖÊåáÂçó&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/overview/quick-start.md"&gt;üöÄ Âø´ÈÄüÂºÄÂßã&lt;/a&gt; - &lt;strong&gt;10ÂàÜÈíü‰∏äÊâãÊåáÂçó&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/examples/basic-examples.md"&gt;üìö Âü∫Á°ÄÁ§∫‰æã&lt;/a&gt; - &lt;strong&gt;8‰∏™ÂÆûÁî®ÁöÑÂÖ•Èó®Á§∫‰æã&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;üèóÔ∏è &lt;strong&gt;Êû∂ÊûÑÁêÜËß£Ë∑ØÂæÑ&lt;/strong&gt; (Ê∑±ÂÖ•‰∫ÜËß£Á≥ªÁªüËÆæËÆ°)&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/architecture/system-architecture.md"&gt;üèõÔ∏è Á≥ªÁªüÊû∂ÊûÑ&lt;/a&gt; - &lt;strong&gt;ÂÆåÊï¥ÁöÑÁ≥ªÁªüÊû∂ÊûÑËÆæËÆ°&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/architecture/agent-architecture.md"&gt;ü§ñ Êô∫ËÉΩ‰ΩìÊû∂ÊûÑ&lt;/a&gt; - &lt;strong&gt;Â§öÊô∫ËÉΩ‰ΩìÂçè‰ΩúÊú∫Âà∂&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/architecture/data-flow-architecture.md"&gt;üìä Êï∞ÊçÆÊµÅÊû∂ÊûÑ&lt;/a&gt; - &lt;strong&gt;Êï∞ÊçÆÂ§ÑÁêÜÂÖ®ÊµÅÁ®ã&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/architecture/graph-structure.md"&gt;üîÑ ÂõæÁªìÊûÑËÆæËÆ°&lt;/a&gt; - &lt;strong&gt;LangGraphÂ∑•‰ΩúÊµÅÁ®ã&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;ü§ñ &lt;strong&gt;Êô∫ËÉΩ‰ΩìÊ∑±Â∫¶Ëß£Êûê&lt;/strong&gt; (‰∫ÜËß£ÊØè‰∏™Êô∫ËÉΩ‰ΩìÁöÑËÆæËÆ°)&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/agents/analysts.md"&gt;üìà ÂàÜÊûêÂ∏àÂõ¢Èòü&lt;/a&gt; - &lt;strong&gt;ÂõõÁ±ª‰∏ì‰∏öÂàÜÊûêÂ∏àËØ¶Ëß£&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/agents/researchers.md"&gt;üî¨ Á†îÁ©∂ÂëòÂõ¢Èòü&lt;/a&gt; - &lt;strong&gt;ÁúãÊ∂®/ÁúãË∑åËæ©ËÆ∫Êú∫Âà∂&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/agents/trader.md"&gt;üíº ‰∫§ÊòìÂëòÊô∫ËÉΩ‰Ωì&lt;/a&gt; - &lt;strong&gt;‰∫§ÊòìÂÜ≥Á≠ñÂà∂ÂÆöÊµÅÁ®ã&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/agents/risk-management.md"&gt;üõ°Ô∏è È£éÈô©ÁÆ°ÁêÜ&lt;/a&gt; - &lt;strong&gt;Â§öÂ±ÇÊ¨°È£éÈô©ËØÑ‰º∞&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/agents/managers.md"&gt;üëî ÁÆ°ÁêÜÂ±ÇÊô∫ËÉΩ‰Ωì&lt;/a&gt; - &lt;strong&gt;ÂçèË∞ÉÂíåÂÜ≥Á≠ñÁÆ°ÁêÜ&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;üìä &lt;strong&gt;Êï∞ÊçÆÂ§ÑÁêÜ‰∏ìÈ¢ò&lt;/strong&gt; (ÊéåÊè°Êï∞ÊçÆÂ§ÑÁêÜÊäÄÊúØ)&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/data/data-sources.md"&gt;üîå Êï∞ÊçÆÊ∫êÈõÜÊàê&lt;/a&gt; - &lt;strong&gt;Â§öÊï∞ÊçÆÊ∫êAPIÈõÜÊàê&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/data/data-processing.md"&gt;‚öôÔ∏è Êï∞ÊçÆÂ§ÑÁêÜÊµÅÁ®ã&lt;/a&gt; - &lt;strong&gt;Êï∞ÊçÆÊ∏ÖÊ¥óÂíåËΩ¨Êç¢&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/data/caching.md"&gt;üíæ ÁºìÂ≠òÁ≠ñÁï•&lt;/a&gt; - &lt;strong&gt;Â§öÂ±ÇÁºìÂ≠ò‰ºòÂåñÊÄßËÉΩ&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;‚öôÔ∏è &lt;strong&gt;ÈÖçÁΩÆÂíå‰ºòÂåñ&lt;/strong&gt; (ÊÄßËÉΩË∞É‰ºòÂíåÂÆöÂà∂)&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/configuration/config-guide.md"&gt;üìù ÈÖçÁΩÆÊåáÂçó&lt;/a&gt; - &lt;strong&gt;ËØ¶ÁªÜÈÖçÁΩÆÈÄâÈ°πËØ¥Êòé&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/configuration/llm-config.md"&gt;üß† LLMÈÖçÁΩÆ&lt;/a&gt; - &lt;strong&gt;Â§ßËØ≠Ë®ÄÊ®°Âûã‰ºòÂåñ&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;üí° &lt;strong&gt;È´òÁ∫ßÂ∫îÁî®&lt;/strong&gt; (Êâ©Â±ïÂºÄÂèëÂíåÂÆûÊàò)&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/examples/basic-examples.md"&gt;üìö Âü∫Á°ÄÁ§∫‰æã&lt;/a&gt; - &lt;strong&gt;8‰∏™ÂÆûÁî®Âü∫Á°ÄÁ§∫‰æã&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/examples/advanced-examples.md"&gt;üöÄ È´òÁ∫ßÁ§∫‰æã&lt;/a&gt; - &lt;strong&gt;Â§çÊùÇÂú∫ÊôØÂíåÊâ©Â±ïÂºÄÂèë&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;‚ùì &lt;strong&gt;ÈóÆÈ¢òËß£ÂÜ≥&lt;/strong&gt; (ÈÅáÂà∞ÈóÆÈ¢òÊó∂Êü•Áúã)&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/faq/faq.md"&gt;üÜò Â∏∏ËßÅÈóÆÈ¢ò&lt;/a&gt; - &lt;strong&gt;ËØ¶ÁªÜFAQÂíåËß£ÂÜ≥ÊñπÊ°à&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;üìä ÊñáÊ°£ÁªüËÆ°Êï∞ÊçÆ&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìÑ &lt;strong&gt;ÊñáÊ°£Êñá‰ª∂Êï∞&lt;/strong&gt;: 20+ ‰∏™‰∏ì‰∏öÊñáÊ°£&lt;/li&gt; 
 &lt;li&gt;üìù &lt;strong&gt;ÊÄªÂ≠óÊï∞&lt;/strong&gt;: 50,000+ Â≠óËØ¶ÁªÜÂÜÖÂÆπ&lt;/li&gt; 
 &lt;li&gt;üíª &lt;strong&gt;‰ª£Á†ÅÁ§∫‰æã&lt;/strong&gt;: 100+ ‰∏™ÂÆûÁî®Á§∫‰æã&lt;/li&gt; 
 &lt;li&gt;üìà &lt;strong&gt;Êû∂ÊûÑÂõæË°®&lt;/strong&gt;: 10+ ‰∏™‰∏ì‰∏öÂõæË°®&lt;/li&gt; 
 &lt;li&gt;üéØ &lt;strong&gt;Ë¶ÜÁõñËåÉÂõ¥&lt;/strong&gt;: ‰ªéÂÖ•Èó®Âà∞‰∏ìÂÆ∂ÁöÑÂÆåÊï¥Ë∑ØÂæÑ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üé® ÊñáÊ°£ÁâπËâ≤&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;üá®üá≥ ÂÆåÂÖ®‰∏≠ÊñáÂåñ&lt;/strong&gt;: ‰∏ì‰∏∫‰∏≠ÊñáÁî®Êà∑‰ºòÂåñÁöÑË°®ËææÊñπÂºè&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üìä ÂõæÊñáÂπ∂ËåÇ&lt;/strong&gt;: ‰∏∞ÂØåÁöÑÊû∂ÊûÑÂõæÂíåÊµÅÁ®ãÂõæ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üíª ‰ª£Á†Å‰∏∞ÂØå&lt;/strong&gt;: ÊØè‰∏™Ê¶ÇÂøµÈÉΩÊúâÂØπÂ∫îÁöÑ‰ª£Á†ÅÁ§∫‰æã&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üîç Ê∑±Â∫¶ÂâñÊûê&lt;/strong&gt;: ‰∏ç‰ªÖÂëäËØâ‰Ω†ÊÄé‰πàÂÅöÔºåËøòÂëäËØâ‰Ω†‰∏∫‰ªÄ‰πàËøôÊ†∑ÂÅö&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üõ†Ô∏è ÂÆûÁî®ÂØºÂêë&lt;/strong&gt;: ÊâÄÊúâÊñáÊ°£ÈÉΩÈù¢ÂêëÂÆûÈôÖÂ∫îÁî®Âú∫ÊôØ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üìö ËØ¶ÁªÜÊñáÊ°£ÁõÆÂΩï&lt;/h2&gt; 
&lt;h3&gt;üìÅ &lt;strong&gt;docs/ ÁõÆÂΩïÁªìÊûÑ&lt;/strong&gt; - ÂÆåÊï¥ÁöÑÁü•ËØÜ‰ΩìÁ≥ª&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;docs/
‚îú‚îÄ‚îÄ üìñ overview/              # È°πÁõÆÊ¶ÇËßà - Êñ∞ÊâãÂøÖËØª
‚îÇ   ‚îú‚îÄ‚îÄ project-overview.md   # üìã È°πÁõÆËØ¶ÁªÜ‰ªãÁªç
‚îÇ   ‚îú‚îÄ‚îÄ quick-start.md        # üöÄ 10ÂàÜÈíüÂø´ÈÄü‰∏äÊâã
‚îÇ   ‚îî‚îÄ‚îÄ installation.md       # ‚öôÔ∏è ËØ¶ÁªÜÂÆâË£ÖÊåáÂçó
‚îÇ
‚îú‚îÄ‚îÄ üèóÔ∏è architecture/          # Á≥ªÁªüÊû∂ÊûÑ - Ê∑±Â∫¶ÁêÜËß£
‚îÇ   ‚îú‚îÄ‚îÄ system-architecture.md    # üèõÔ∏è Êï¥‰ΩìÊû∂ÊûÑËÆæËÆ°
‚îÇ   ‚îú‚îÄ‚îÄ agent-architecture.md     # ü§ñ Êô∫ËÉΩ‰ΩìÂçè‰ΩúÊú∫Âà∂
‚îÇ   ‚îú‚îÄ‚îÄ data-flow-architecture.md # üìä Êï∞ÊçÆÊµÅÂ§ÑÁêÜÊû∂ÊûÑ
‚îÇ   ‚îî‚îÄ‚îÄ graph-structure.md        # üîÑ LangGraphÂ∑•‰ΩúÊµÅ
‚îÇ
‚îú‚îÄ‚îÄ ü§ñ agents/               # Êô∫ËÉΩ‰ΩìËØ¶Ëß£ - Ê†∏ÂøÉÁªÑ‰ª∂
‚îÇ   ‚îú‚îÄ‚îÄ analysts.md          # üìà ÂõõÁ±ª‰∏ì‰∏öÂàÜÊûêÂ∏à
‚îÇ   ‚îú‚îÄ‚îÄ researchers.md       # üî¨ ÁúãÊ∂®/ÁúãË∑åËæ©ËÆ∫Êú∫Âà∂
‚îÇ   ‚îú‚îÄ‚îÄ trader.md           # üíº ‰∫§ÊòìÂÜ≥Á≠ñÂà∂ÂÆö
‚îÇ   ‚îú‚îÄ‚îÄ risk-management.md  # üõ°Ô∏è Â§öÂ±ÇÈ£éÈô©ËØÑ‰º∞
‚îÇ   ‚îî‚îÄ‚îÄ managers.md         # üëî ÁÆ°ÁêÜÂ±ÇÂçèË∞É
‚îÇ
‚îú‚îÄ‚îÄ üìä data/                 # Êï∞ÊçÆÂ§ÑÁêÜ - ÊäÄÊúØÊ†∏ÂøÉ
‚îÇ   ‚îú‚îÄ‚îÄ data-sources.md      # üîå Â§öÊï∞ÊçÆÊ∫êÈõÜÊàê
‚îÇ   ‚îú‚îÄ‚îÄ data-processing.md   # ‚öôÔ∏è Êï∞ÊçÆÂ§ÑÁêÜÊµÅÁ®ã
‚îÇ   ‚îî‚îÄ‚îÄ caching.md          # üíæ ÁºìÂ≠ò‰ºòÂåñÁ≠ñÁï•
‚îÇ
‚îú‚îÄ‚îÄ ‚öôÔ∏è configuration/        # ÈÖçÁΩÆ‰ºòÂåñ - ÊÄßËÉΩË∞É‰ºò
‚îÇ   ‚îú‚îÄ‚îÄ config-guide.md      # üìù ËØ¶ÁªÜÈÖçÁΩÆËØ¥Êòé
‚îÇ   ‚îî‚îÄ‚îÄ llm-config.md       # üß† LLMÊ®°Âûã‰ºòÂåñ
‚îÇ
‚îú‚îÄ‚îÄ üí° examples/             # Á§∫‰æãÊïôÁ®ã - ÂÆûÊàòÂ∫îÁî®
‚îÇ   ‚îú‚îÄ‚îÄ basic-examples.md    # üìö 8‰∏™Âü∫Á°ÄÁ§∫‰æã
‚îÇ   ‚îî‚îÄ‚îÄ advanced-examples.md # üöÄ È´òÁ∫ßÂºÄÂèëÁ§∫‰æã
‚îÇ
‚îî‚îÄ‚îÄ ‚ùì faq/                  # ÈóÆÈ¢òËß£ÂÜ≥ - ÁñëÈöæËß£Á≠î
    ‚îî‚îÄ‚îÄ faq.md              # üÜò Â∏∏ËßÅÈóÆÈ¢òFAQ
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;üéØ &lt;strong&gt;ÈáçÁÇπÊé®ËçêÊñáÊ°£&lt;/strong&gt; (ÂøÖËØªÁ≤æÈÄâ)&lt;/h3&gt; 
&lt;h4&gt;üî• &lt;strong&gt;ÊúÄÂèóÊ¨¢ËøéÁöÑÊñáÊ°£&lt;/strong&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/overview/project-overview.md"&gt;üìã È°πÁõÆÊ¶ÇËø∞&lt;/a&gt;&lt;/strong&gt; - ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;‰∫ÜËß£È°πÁõÆÁöÑÊ†∏ÂøÉ‰ª∑ÂÄºÂíåÊäÄÊúØÁâπËâ≤Ôºå5ÂàÜÈíüËØªÊáÇÊï¥‰∏™Ê°ÜÊû∂&lt;/p&gt; 
  &lt;/blockquote&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/architecture/system-architecture.md"&gt;üèõÔ∏è Á≥ªÁªüÊû∂ÊûÑ&lt;/a&gt;&lt;/strong&gt; - ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;Ê∑±Â∫¶Ëß£ÊûêÂ§öÊô∫ËÉΩ‰ΩìÂçè‰ΩúÊú∫Âà∂ÔºåÂåÖÂê´ËØ¶ÁªÜÊû∂ÊûÑÂõæ&lt;/p&gt; 
  &lt;/blockquote&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/examples/basic-examples.md"&gt;üìö Âü∫Á°ÄÁ§∫‰æã&lt;/a&gt;&lt;/strong&gt; - ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;8‰∏™ÂÆûÁî®Á§∫‰æãÔºå‰ªéËÇ°Á•®ÂàÜÊûêÂà∞ÊäïËµÑÁªÑÂêà‰ºòÂåñ&lt;/p&gt; 
  &lt;/blockquote&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;üöÄ &lt;strong&gt;ÊäÄÊúØÊ∑±Â∫¶ÊñáÊ°£&lt;/strong&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/architecture/agent-architecture.md"&gt;ü§ñ Êô∫ËÉΩ‰ΩìÊû∂ÊûÑ&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;Â§öÊô∫ËÉΩ‰ΩìËÆæËÆ°Ê®°ÂºèÂíåÂçè‰ΩúÊú∫Âà∂ËØ¶Ëß£&lt;/p&gt; 
  &lt;/blockquote&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/architecture/data-flow-architecture.md"&gt;üìä Êï∞ÊçÆÊµÅÊû∂ÊûÑ&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;Êï∞ÊçÆËé∑Âèñ„ÄÅÂ§ÑÁêÜ„ÄÅÁºìÂ≠òÁöÑÂÆåÊï¥ÊµÅÁ®ã&lt;/p&gt; 
  &lt;/blockquote&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/agents/researchers.md"&gt;üî¨ Á†îÁ©∂ÂëòÂõ¢Èòü&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;ÁúãÊ∂®/ÁúãË∑åÁ†îÁ©∂ÂëòËæ©ËÆ∫Êú∫Âà∂ÁöÑÂàõÊñ∞ËÆæËÆ°&lt;/p&gt; 
  &lt;/blockquote&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;üíº &lt;strong&gt;ÂÆûÁî®Â∑•ÂÖ∑ÊñáÊ°£&lt;/strong&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/usage/web-interface-guide.md"&gt;üåê WebÁïåÈù¢ÊåáÂçó&lt;/a&gt;&lt;/strong&gt; - ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;ÂÆåÊï¥ÁöÑWebÁïåÈù¢‰ΩøÁî®ÊïôÁ®ãÔºåÂåÖÂê´5Á∫ßÁ†îÁ©∂Ê∑±Â∫¶ËØ¶ÁªÜËØ¥Êòé&lt;/p&gt; 
  &lt;/blockquote&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/usage/investment_analysis_guide.md"&gt;üí∞ ÊäïËµÑÂàÜÊûêÊåáÂçó&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;‰ªéÂü∫Á°ÄÂà∞È´òÁ∫ßÁöÑÂÆåÊï¥ÊäïËµÑÂàÜÊûêÊïôÁ®ã&lt;/p&gt; 
  &lt;/blockquote&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/configuration/llm-config.md"&gt;üß† LLMÈÖçÁΩÆ&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;Â§öLLMÊ®°ÂûãÈÖçÁΩÆÂíåÊàêÊú¨‰ºòÂåñÁ≠ñÁï•&lt;/p&gt; 
  &lt;/blockquote&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/data/caching.md"&gt;üíæ ÁºìÂ≠òÁ≠ñÁï•&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;Â§öÂ±ÇÁºìÂ≠òËÆæËÆ°ÔºåÊòæËëóÈôç‰ΩéAPIË∞ÉÁî®ÊàêÊú¨&lt;/p&gt; 
  &lt;/blockquote&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/faq/faq.md"&gt;üÜò Â∏∏ËßÅÈóÆÈ¢ò&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;ËØ¶ÁªÜÁöÑFAQÂíåÊïÖÈöúÊéíÈô§ÊåáÂçó&lt;/p&gt; 
  &lt;/blockquote&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;üìñ &lt;strong&gt;ÊåâÊ®°ÂùóÊµèËßàÊñáÊ°£&lt;/strong&gt;&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìñ Ê¶ÇËßàÊñáÊ°£&lt;/strong&gt; - È°πÁõÆÂÖ•Èó®ÂøÖËØª&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/overview/project-overview.md"&gt;üìã È°πÁõÆÊ¶ÇËø∞&lt;/a&gt; - ËØ¶ÁªÜÁöÑÈ°πÁõÆËÉåÊôØÂíåÁâπÊÄß‰ªãÁªç&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/overview/quick-start.md"&gt;üöÄ Âø´ÈÄüÂºÄÂßã&lt;/a&gt; - ‰ªéÂÆâË£ÖÂà∞Á¨¨‰∏ÄÊ¨°ËøêË°åÁöÑÂÆåÊï¥ÊåáÂçó&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/overview/installation.md"&gt;‚öôÔ∏è ËØ¶ÁªÜÂÆâË£Ö&lt;/a&gt; - ÂêÑÂπ≥Âè∞ËØ¶ÁªÜÂÆâË£ÖËØ¥Êòé&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üèóÔ∏è Êû∂ÊûÑÊñáÊ°£&lt;/strong&gt; - Ê∑±Â∫¶ÁêÜËß£Á≥ªÁªüËÆæËÆ°&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/architecture/system-architecture.md"&gt;üèõÔ∏è Á≥ªÁªüÊû∂ÊûÑ&lt;/a&gt; - ÂÆåÊï¥ÁöÑÁ≥ªÁªüÊû∂ÊûÑËÆæËÆ°&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/architecture/agent-architecture.md"&gt;ü§ñ Êô∫ËÉΩ‰ΩìÊû∂ÊûÑ&lt;/a&gt; - Êô∫ËÉΩ‰ΩìËÆæËÆ°Ê®°ÂºèÂíåÂçè‰ΩúÊú∫Âà∂&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/architecture/data-flow-architecture.md"&gt;üìä Êï∞ÊçÆÊµÅÊû∂ÊûÑ&lt;/a&gt; - Êï∞ÊçÆËé∑Âèñ„ÄÅÂ§ÑÁêÜÂíåÂàÜÂèëÊµÅÁ®ã&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/architecture/graph-structure.md"&gt;üîÑ ÂõæÁªìÊûÑËÆæËÆ°&lt;/a&gt; - LangGraphÂ∑•‰ΩúÊµÅÁ®ãËÆæËÆ°&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;ü§ñ Êô∫ËÉΩ‰ΩìÊñáÊ°£&lt;/strong&gt; - Ê†∏ÂøÉÁªÑ‰ª∂ËØ¶Ëß£&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/agents/analysts.md"&gt;üìà ÂàÜÊûêÂ∏àÂõ¢Èòü&lt;/a&gt; - ÂõõÁ±ª‰∏ì‰∏öÂàÜÊûêÂ∏àËØ¶Ëß£&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/agents/researchers.md"&gt;üî¨ Á†îÁ©∂ÂëòÂõ¢Èòü&lt;/a&gt; - ÁúãÊ∂®/ÁúãË∑åÁ†îÁ©∂ÂëòÂíåËæ©ËÆ∫Êú∫Âà∂&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/agents/trader.md"&gt;üíº ‰∫§ÊòìÂëòÊô∫ËÉΩ‰Ωì&lt;/a&gt; - ‰∫§ÊòìÂÜ≥Á≠ñÂà∂ÂÆöÊµÅÁ®ã&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/agents/risk-management.md"&gt;üõ°Ô∏è È£éÈô©ÁÆ°ÁêÜ&lt;/a&gt; - Â§öÂ±ÇÊ¨°È£éÈô©ËØÑ‰º∞‰ΩìÁ≥ª&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/agents/managers.md"&gt;üëî ÁÆ°ÁêÜÂ±ÇÊô∫ËÉΩ‰Ωì&lt;/a&gt; - ÂçèË∞ÉÂíåÂÜ≥Á≠ñÁÆ°ÁêÜ&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìä Êï∞ÊçÆÂ§ÑÁêÜ&lt;/strong&gt; - ÊäÄÊúØÊ†∏ÂøÉÂÆûÁé∞&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/data/data-sources.md"&gt;üîå Êï∞ÊçÆÊ∫êÈõÜÊàê&lt;/a&gt; - ÊîØÊåÅÁöÑÊï∞ÊçÆÊ∫êÂíåAPIÈõÜÊàê&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/data/data-processing.md"&gt;‚öôÔ∏è Êï∞ÊçÆÂ§ÑÁêÜÊµÅÁ®ã&lt;/a&gt; - Êï∞ÊçÆÊ∏ÖÊ¥ó„ÄÅËΩ¨Êç¢ÂíåÈ™åËØÅ&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/data/caching.md"&gt;üíæ ÁºìÂ≠òÁ≠ñÁï•&lt;/a&gt; - Â§öÂ±ÇÁºìÂ≠ò‰ºòÂåñÊÄßËÉΩ&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;‚öôÔ∏è ÈÖçÁΩÆ‰∏éÈÉ®ÁΩ≤&lt;/strong&gt; - ÊÄßËÉΩË∞É‰ºòÊåáÂçó&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/configuration/config-guide.md"&gt;üìù ÈÖçÁΩÆÊåáÂçó&lt;/a&gt; - ËØ¶ÁªÜÁöÑÈÖçÁΩÆÈÄâÈ°πËØ¥Êòé&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/configuration/llm-config.md"&gt;üß† LLMÈÖçÁΩÆ&lt;/a&gt; - Â§ßËØ≠Ë®ÄÊ®°ÂûãÈÖçÁΩÆ‰ºòÂåñ&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üí° Á§∫‰æãÂíåÊïôÁ®ã&lt;/strong&gt; - ÂÆûÊàòÂ∫îÁî®ÊåáÂçó&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/examples/basic-examples.md"&gt;üìö Âü∫Á°ÄÁ§∫‰æã&lt;/a&gt; - 8‰∏™ÂÆûÁî®ÁöÑÂü∫Á°ÄÁ§∫‰æã&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/examples/advanced-examples.md"&gt;üöÄ È´òÁ∫ßÁ§∫‰æã&lt;/a&gt; - Â§çÊùÇÂú∫ÊôØÂíåÊâ©Â±ïÂºÄÂèë&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;‚ùì Â∏ÆÂä©ÊñáÊ°£&lt;/strong&gt; - ÈóÆÈ¢òËß£ÂÜ≥ÊñπÊ°à&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/faq/faq.md"&gt;üÜò Â∏∏ËßÅÈóÆÈ¢ò&lt;/a&gt; - ËØ¶ÁªÜÁöÑFAQÂíåËß£ÂÜ≥ÊñπÊ°à&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;üí∞ ÊàêÊú¨ÊéßÂà∂&lt;/h2&gt; 
&lt;h3&gt;ÂÖ∏Âûã‰ΩøÁî®ÊàêÊú¨&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ÁªèÊµéÊ®°Âºè&lt;/strong&gt;: $0.01-0.05/Ê¨°ÂàÜÊûê (‰ΩøÁî® gpt-4o-mini)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Ê†áÂáÜÊ®°Âºè&lt;/strong&gt;: $0.05-0.15/Ê¨°ÂàÜÊûê (‰ΩøÁî® gpt-4o)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;È´òÁ≤æÂ∫¶Ê®°Âºè&lt;/strong&gt;: $0.10-0.30/Ê¨°ÂàÜÊûê (‰ΩøÁî® gpt-4o + Â§öËΩÆËæ©ËÆ∫)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ÊàêÊú¨‰ºòÂåñÂª∫ËÆÆ&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# ‰ΩéÊàêÊú¨ÈÖçÁΩÆÁ§∫‰æã
cost_optimized_config = {
    "deep_think_llm": "gpt-4o-mini",
    "quick_think_llm": "gpt-4o-mini", 
    "max_debate_rounds": 1,
    "online_tools": False  # ‰ΩøÁî®ÁºìÂ≠òÊï∞ÊçÆ
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ü§ù Ë¥°ÁåÆÊåáÂçó&lt;/h2&gt; 
&lt;p&gt;Êàë‰ª¨Ê¨¢ËøéÂêÑÁßçÂΩ¢ÂºèÁöÑË¥°ÁåÆÔºö&lt;/p&gt; 
&lt;h3&gt;Ë¥°ÁåÆÁ±ªÂûã&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;üêõ &lt;strong&gt;Bug‰øÆÂ§ç&lt;/strong&gt; - ÂèëÁé∞Âπ∂‰øÆÂ§çÈóÆÈ¢ò&lt;/li&gt; 
 &lt;li&gt;‚ú® &lt;strong&gt;Êñ∞ÂäüËÉΩ&lt;/strong&gt; - Ê∑ªÂä†Êñ∞ÁöÑÂäüËÉΩÁâπÊÄß&lt;/li&gt; 
 &lt;li&gt;üìö &lt;strong&gt;ÊñáÊ°£ÊîπËøõ&lt;/strong&gt; - ÂÆåÂñÑÊñáÊ°£ÂíåÊïôÁ®ã&lt;/li&gt; 
 &lt;li&gt;üåê &lt;strong&gt;Êú¨Âú∞Âåñ&lt;/strong&gt; - ÁøªËØëÂíåÊú¨Âú∞ÂåñÂ∑•‰Ωú&lt;/li&gt; 
 &lt;li&gt;üé® &lt;strong&gt;‰ª£Á†Å‰ºòÂåñ&lt;/strong&gt; - ÊÄßËÉΩ‰ºòÂåñÂíå‰ª£Á†ÅÈáçÊûÑ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Ë¥°ÁåÆÊµÅÁ®ã&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Fork Êú¨‰ªìÂ∫ì&lt;/li&gt; 
 &lt;li&gt;ÂàõÂª∫ÁâπÊÄßÂàÜÊîØ (&lt;code&gt;git checkout -b feature/AmazingFeature&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Êèê‰∫§Êõ¥Êîπ (&lt;code&gt;git commit -m 'Add some AmazingFeature'&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Êé®ÈÄÅÂà∞ÂàÜÊîØ (&lt;code&gt;git push origin feature/AmazingFeature&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;ÂàõÂª∫ Pull Request&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;üìã Êü•ÁúãË¥°ÁåÆËÄÖ&lt;/h3&gt; 
&lt;p&gt;Êü•ÁúãÊâÄÊúâË¥°ÁåÆËÄÖÂíåËØ¶ÁªÜË¥°ÁåÆÂÜÖÂÆπÔºö&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/CONTRIBUTORS.md"&gt;ü§ù Ë¥°ÁåÆËÄÖÂêçÂçï&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;üìÑ ËÆ∏ÂèØËØÅ&lt;/h2&gt; 
&lt;p&gt;Êú¨È°πÁõÆÂü∫‰∫é Apache 2.0 ËÆ∏ÂèØËØÅÂºÄÊ∫ê„ÄÇËØ¶ËßÅ &lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/LICENSE"&gt;LICENSE&lt;/a&gt; Êñá‰ª∂„ÄÇ&lt;/p&gt; 
&lt;h3&gt;ËÆ∏ÂèØËØÅËØ¥Êòé&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;‚úÖ ÂïÜ‰∏ö‰ΩøÁî®&lt;/li&gt; 
 &lt;li&gt;‚úÖ ‰øÆÊîπÂíåÂàÜÂèë&lt;/li&gt; 
 &lt;li&gt;‚úÖ ÁßÅ‰∫∫‰ΩøÁî®&lt;/li&gt; 
 &lt;li&gt;‚úÖ ‰∏ìÂà©‰ΩøÁî®&lt;/li&gt; 
 &lt;li&gt;‚ùó ÈúÄË¶Å‰øùÁïôÁâàÊùÉÂ£∞Êòé&lt;/li&gt; 
 &lt;li&gt;‚ùó ÈúÄË¶ÅÂåÖÂê´ËÆ∏ÂèØËØÅÂâØÊú¨&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üôè Ëá¥Ë∞¢‰∏éÊÑüÊÅ©&lt;/h2&gt; 
&lt;h3&gt;üåü ÂêëÊ∫êÈ°πÁõÆÂºÄÂèëËÄÖËá¥Êï¨&lt;/h3&gt; 
&lt;p&gt;Êàë‰ª¨Âêë &lt;a href="https://github.com/TauricResearch"&gt;Tauric Research&lt;/a&gt; Âõ¢ÈòüË°®ËææÊúÄÊ∑±ÁöÑÊï¨ÊÑèÂíåÊÑüË∞¢Ôºö&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;üéØ ÊÑøÊôØÈ¢ÜÂØºËÄÖ&lt;/strong&gt;: ÊÑüË∞¢ÊÇ®‰ª¨Âú®AIÈáëËûçÈ¢ÜÂüüÁöÑÂâçÁûªÊÄßÊÄùËÄÉÂíåÂàõÊñ∞ÂÆûË∑µ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üíé ÁèçË¥µÊ∫êÁ†Å&lt;/strong&gt;: ÊÑüË∞¢ÊÇ®‰ª¨ÂºÄÊ∫êÁöÑÊØè‰∏ÄË°å‰ª£Á†ÅÔºåÂÆÉ‰ª¨ÂáùËÅöÁùÄÊó†Êï∞ÁöÑÊô∫ÊÖßÂíåÂøÉË°Ä&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üèóÔ∏è Êû∂ÊûÑÂ§ßÂ∏à&lt;/strong&gt;: ÊÑüË∞¢ÊÇ®‰ª¨ËÆæËÆ°‰∫ÜÂ¶ÇÊ≠§‰ºòÈõÖ„ÄÅÂèØÊâ©Â±ïÁöÑÂ§öÊô∫ËÉΩ‰ΩìÊ°ÜÊû∂&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üí° ÊäÄÊúØÂÖàÈ©±&lt;/strong&gt;: ÊÑüË∞¢ÊÇ®‰ª¨Â∞ÜÂâçÊ≤øAIÊäÄÊúØ‰∏éÈáëËûçÂÆûÂä°ÂÆåÁæéÁªìÂêà&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üîÑ ÊåÅÁª≠Ë¥°ÁåÆ&lt;/strong&gt;: ÊÑüË∞¢ÊÇ®‰ª¨ÊåÅÁª≠ÁöÑÁª¥Êä§„ÄÅÊõ¥Êñ∞ÂíåÊîπËøõÂ∑•‰Ωú&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ü§ù Á§æÂå∫Ë¥°ÁåÆËÄÖËá¥Ë∞¢&lt;/h3&gt; 
&lt;p&gt;ÊÑüË∞¢ÊâÄÊúâ‰∏∫TradingAgents-CNÈ°πÁõÆÂÅöÂá∫Ë¥°ÁåÆÁöÑÂºÄÂèëËÄÖÂíåÁî®Êà∑ÔºÅ&lt;/p&gt; 
&lt;p&gt;ËØ¶ÁªÜÁöÑË¥°ÁåÆËÄÖÂêçÂçïÂíåË¥°ÁåÆÂÜÖÂÆπËØ∑Êü•ÁúãÔºö&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/CONTRIBUTORS.md"&gt;üìã Ë¥°ÁåÆËÄÖÂêçÂçï&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;ÂåÖÊã¨‰ΩÜ‰∏çÈôê‰∫éÔºö&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üê≥ &lt;strong&gt;DockerÂÆπÂô®Âåñ&lt;/strong&gt; - ÈÉ®ÁΩ≤ÊñπÊ°à‰ºòÂåñ&lt;/li&gt; 
 &lt;li&gt;üìÑ &lt;strong&gt;Êä•ÂëäÂØºÂá∫ÂäüËÉΩ&lt;/strong&gt; - Â§öÊ†ºÂºèËæìÂá∫ÊîØÊåÅ&lt;/li&gt; 
 &lt;li&gt;üêõ &lt;strong&gt;Bug‰øÆÂ§ç&lt;/strong&gt; - Á≥ªÁªüÁ®≥ÂÆöÊÄßÊèêÂçá&lt;/li&gt; 
 &lt;li&gt;üîß &lt;strong&gt;‰ª£Á†Å‰ºòÂåñ&lt;/strong&gt; - Áî®Êà∑‰ΩìÈ™åÊîπËøõ&lt;/li&gt; 
 &lt;li&gt;üìù &lt;strong&gt;ÊñáÊ°£ÂÆåÂñÑ&lt;/strong&gt; - ‰ΩøÁî®ÊåáÂçóÂíåÊïôÁ®ã&lt;/li&gt; 
 &lt;li&gt;üåç &lt;strong&gt;Á§æÂå∫Âª∫ËÆæ&lt;/strong&gt; - ÈóÆÈ¢òÂèçÈ¶àÂíåÊé®Âπø&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üåç ÂºÄÊ∫êË¥°ÁåÆ&lt;/strong&gt;: ÊÑüË∞¢ÊÇ®‰ª¨ÈÄâÊã©Apache 2.0ÂçèËÆÆÔºåÁªô‰∫àÂºÄÂèëËÄÖÊúÄÂ§ßÁöÑËá™Áî±&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üìö Áü•ËØÜÂàÜ‰∫´&lt;/strong&gt;: ÊÑüË∞¢ÊÇ®‰ª¨Êèê‰æõÁöÑËØ¶ÁªÜÊñáÊ°£ÂíåÊúÄ‰Ω≥ÂÆûË∑µÊåáÂØº&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;ÁâπÂà´ÊÑüË∞¢&lt;/strong&gt;Ôºö&lt;a href="https://github.com/TauricResearch/TradingAgents"&gt;TradingAgents&lt;/a&gt; È°πÁõÆ‰∏∫Êàë‰ª¨Êèê‰æõ‰∫ÜÂùöÂÆûÁöÑÊäÄÊúØÂü∫Á°Ä„ÄÇËôΩÁÑ∂Apache 2.0ÂçèËÆÆËµã‰∫à‰∫ÜÊàë‰ª¨‰ΩøÁî®Ê∫êÁ†ÅÁöÑÊùÉÂà©Ôºå‰ΩÜÊàë‰ª¨Ê∑±Áü•ÊØè‰∏ÄË°å‰ª£Á†ÅÁöÑÁèçË¥µ‰ª∑ÂÄºÔºåÂ∞ÜÊ∞∏ËøúÈì≠ËÆ∞Âπ∂ÊÑüË∞¢ÊÇ®‰ª¨ÁöÑÊó†ÁßÅË¥°ÁåÆ„ÄÇ&lt;/p&gt; 
&lt;h3&gt;üá®üá≥ Êé®Âπø‰ΩøÂëΩÁöÑÂàùÂøÉ&lt;/h3&gt; 
&lt;p&gt;ÂàõÂª∫Ëøô‰∏™‰∏≠ÊñáÂ¢ûÂº∫ÁâàÊú¨ÔºåÊàë‰ª¨ÊÄÄÁùÄ‰ª•‰∏ãÂàùÂøÉÔºö&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;üåâ ÊäÄÊúØ‰º†Êí≠&lt;/strong&gt;: ËÆ©‰ºòÁßÄÁöÑTradingAgentsÊäÄÊúØÂú®‰∏≠ÂõΩÂæóÂà∞Êõ¥ÂπøÊ≥õÁöÑÂ∫îÁî®&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üéì ÊïôËÇ≤ÊôÆÂèä&lt;/strong&gt;: ‰∏∫‰∏≠ÂõΩÁöÑAIÈáëËûçÊïôËÇ≤Êèê‰æõÊõ¥Â•ΩÁöÑÂ∑•ÂÖ∑ÂíåËµÑÊ∫ê&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ü§ù ÊñáÂåñÊ°•Ê¢Å&lt;/strong&gt;: Âú®‰∏≠Ë•øÊñπÊäÄÊúØÁ§æÂå∫‰πãÈó¥Êê≠Âª∫‰∫§ÊµÅÂêà‰ΩúÁöÑÊ°•Ê¢Å&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üöÄ ÂàõÊñ∞Êé®Âä®&lt;/strong&gt;: Êé®Âä®‰∏≠ÂõΩÈáëËûçÁßëÊäÄÈ¢ÜÂüüÁöÑAIÊäÄÊúØÂàõÊñ∞ÂíåÂ∫îÁî®&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üåç ÂºÄÊ∫êÁ§æÂå∫&lt;/h3&gt; 
&lt;p&gt;ÊÑüË∞¢ÊâÄÊúâ‰∏∫Êú¨È°πÁõÆË¥°ÁåÆ‰ª£Á†Å„ÄÅÊñáÊ°£„ÄÅÂª∫ËÆÆÂíåÂèçÈ¶àÁöÑÂºÄÂèëËÄÖÂíåÁî®Êà∑„ÄÇÊ≠£ÊòØÂõ†‰∏∫Êúâ‰∫ÜÂ§ßÂÆ∂ÁöÑÊîØÊåÅÔºåÊàë‰ª¨ÊâçËÉΩÊõ¥Â•ΩÂú∞ÊúçÂä°‰∏≠ÊñáÁî®Êà∑Á§æÂå∫„ÄÇ&lt;/p&gt; 
&lt;h3&gt;ü§ù Âêà‰ΩúÂÖ±Ëµ¢&lt;/h3&gt; 
&lt;p&gt;Êàë‰ª¨ÊâøËØ∫Ôºö&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Â∞äÈáçÂéüÂàõ&lt;/strong&gt;: ÂßãÁªàÂ∞äÈáçÊ∫êÈ°πÁõÆÁöÑÁü•ËØÜ‰∫ßÊùÉÂíåÂºÄÊ∫êÂçèËÆÆ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ÂèçÈ¶àË¥°ÁåÆ&lt;/strong&gt;: Â∞ÜÊúâ‰ª∑ÂÄºÁöÑÊîπËøõÂíåÂàõÊñ∞ÂèçÈ¶àÁªôÊ∫êÈ°πÁõÆÂíåÂºÄÊ∫êÁ§æÂå∫&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ÊåÅÁª≠ÊîπËøõ&lt;/strong&gt;: ‰∏çÊñ≠ÂÆåÂñÑ‰∏≠ÊñáÂ¢ûÂº∫ÁâàÊú¨ÔºåÊèê‰æõÊõ¥Â•ΩÁöÑÁî®Êà∑‰ΩìÈ™å&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ÂºÄÊîæÂêà‰Ωú&lt;/strong&gt;: Ê¨¢Ëøé‰∏éÊ∫êÈ°πÁõÆÂõ¢ÈòüÂíåÂÖ®ÁêÉÂºÄÂèëËÄÖËøõË°åÊäÄÊúØ‰∫§ÊµÅ‰∏éÂêà‰Ωú&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üìà ÁâàÊú¨ÂéÜÂè≤&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;v0.1.13&lt;/strong&gt; (2025-08-02): ü§ñ ÂéüÁîüOpenAIÊîØÊåÅ‰∏éGoogle AIÁîüÊÄÅÁ≥ªÁªüÂÖ®Èù¢ÈõÜÊàê ‚ú® &lt;strong&gt;ÊúÄÊñ∞ÁâàÊú¨&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;v0.1.12&lt;/strong&gt; (2025-07-29): üß† Êô∫ËÉΩÊñ∞ÈóªÂàÜÊûêÊ®°Âùó‰∏éÈ°πÁõÆÁªìÊûÑ‰ºòÂåñ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;v0.1.11&lt;/strong&gt; (2025-07-27): ü§ñ Â§öLLMÊèê‰æõÂïÜÈõÜÊàê‰∏éÊ®°ÂûãÈÄâÊã©ÊåÅ‰πÖÂåñ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;v0.1.10&lt;/strong&gt; (2025-07-18): üöÄ WebÁïåÈù¢ÂÆûÊó∂ËøõÂ∫¶ÊòæÁ§∫‰∏éÊô∫ËÉΩ‰ºöËØùÁÆ°ÁêÜ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;v0.1.9&lt;/strong&gt; (2025-07-16): üéØ CLIÁî®Êà∑‰ΩìÈ™åÈáçÂ§ß‰ºòÂåñ‰∏éÁªü‰∏ÄÊó•ÂøóÁÆ°ÁêÜ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;v0.1.8&lt;/strong&gt; (2025-07-15): üé® WebÁïåÈù¢ÂÖ®Èù¢‰ºòÂåñ‰∏éÁî®Êà∑‰ΩìÈ™åÊèêÂçá&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;v0.1.7&lt;/strong&gt; (2025-07-13): üê≥ ÂÆπÂô®ÂåñÈÉ®ÁΩ≤‰∏é‰∏ì‰∏öÊä•ÂëäÂØºÂá∫&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;v0.1.6&lt;/strong&gt; (2025-07-11): üîß ÈòøÈáåÁôæÁÇº‰øÆÂ§ç‰∏éÊï∞ÊçÆÊ∫êÂçáÁ∫ß&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;v0.1.5&lt;/strong&gt; (2025-07-08): üìä Ê∑ªÂä†DeepseekÊ®°ÂûãÊîØÊåÅ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;v0.1.4&lt;/strong&gt; (2025-07-05): üèóÔ∏è Êû∂ÊûÑ‰ºòÂåñ‰∏éÈÖçÁΩÆÁÆ°ÁêÜÈáçÊûÑ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;v0.1.3&lt;/strong&gt; (2025-06-28): üá®üá≥ AËÇ°Â∏ÇÂú∫ÂÆåÊï¥ÊîØÊåÅ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;v0.1.2&lt;/strong&gt; (2025-06-15): üåê WebÁïåÈù¢ÂíåÈÖçÁΩÆÁÆ°ÁêÜ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;v0.1.1&lt;/strong&gt; (2025-06-01): üß† ÂõΩ‰∫ßLLMÈõÜÊàê&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;üìã &lt;strong&gt;ËØ¶ÁªÜÊõ¥Êñ∞Êó•Âøó&lt;/strong&gt;: &lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/releases/CHANGELOG.md"&gt;CHANGELOG.md&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;üìû ËÅîÁ≥ªÊñπÂºè&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;GitHub Issues&lt;/strong&gt;: &lt;a href="https://github.com/hsliuping/TradingAgents-CN/issues"&gt;Êèê‰∫§ÈóÆÈ¢òÂíåÂª∫ËÆÆ&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ÈÇÆÁÆ±&lt;/strong&gt;: &lt;a href="mailto:hsliup@163.com"&gt;hsliup@163.com&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;È°πÁõÆÔº±Ôº±Áæ§Ôºö187537480&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;È°πÁõÆÂæÆ‰ø°ÂÖ¨‰ºóÂè∑ÔºöTradingAgents-CN&lt;/p&gt; &lt;img src="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/assets/weixin.png" alt="ÂæÆ‰ø°ÂÖ¨‰ºóÂè∑" width="200" /&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ÂéüÈ°πÁõÆ&lt;/strong&gt;: &lt;a href="https://github.com/TauricResearch/TradingAgents"&gt;TauricResearch/TradingAgents&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ÊñáÊ°£&lt;/strong&gt;: &lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/"&gt;ÂÆåÊï¥ÊñáÊ°£ÁõÆÂΩï&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;‚ö†Ô∏è È£éÈô©ÊèêÁ§∫&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;ÈáçË¶ÅÂ£∞Êòé&lt;/strong&gt;: Êú¨Ê°ÜÊû∂‰ªÖÁî®‰∫éÁ†îÁ©∂ÂíåÊïôËÇ≤ÁõÆÁöÑÔºå‰∏çÊûÑÊàêÊäïËµÑÂª∫ËÆÆ„ÄÇ&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìä ‰∫§ÊòìË°®Áé∞ÂèØËÉΩÂõ†Â§öÁßçÂõ†Á¥†ËÄåÂºÇ&lt;/li&gt; 
 &lt;li&gt;ü§ñ AIÊ®°ÂûãÁöÑÈ¢ÑÊµãÂ≠òÂú®‰∏çÁ°ÆÂÆöÊÄß&lt;/li&gt; 
 &lt;li&gt;üí∞ ÊäïËµÑÊúâÈ£éÈô©ÔºåÂÜ≥Á≠ñÈúÄË∞®ÊÖé&lt;/li&gt; 
 &lt;li&gt;üë®‚Äçüíº Âª∫ËÆÆÂí®ËØ¢‰∏ì‰∏öË¥¢Âä°È°æÈóÆ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;strong&gt;üåü Â¶ÇÊûúËøô‰∏™È°πÁõÆÂØπÊÇ®ÊúâÂ∏ÆÂä©ÔºåËØ∑ÁªôÊàë‰ª¨‰∏Ä‰∏™ StarÔºÅ&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/hsliuping/TradingAgents-CN"&gt;‚≠ê Star this repo&lt;/a&gt; | &lt;a href="https://github.com/hsliuping/TradingAgents-CN/fork"&gt;üç¥ Fork this repo&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/"&gt;üìñ Read the docs&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>QwenLM/Qwen3-VL</title>
      <link>https://github.com/QwenLM/Qwen3-VL</link>
      <description>&lt;p&gt;Qwen3-VL is the multimodal large language model series developed by Qwen team, Alibaba Cloud.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Qwen3-VL&lt;/h1&gt; 
&lt;p align="center"&gt; &lt;img src="https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/qwen3vllogo.png" width="400" /&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p align="center"&gt; üíú &lt;a href="https://chat.qwenlm.ai/"&gt;&lt;b&gt;Qwen Chat&lt;/b&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ü§ó &lt;a href="https://huggingface.co/collections/Qwen/qwen3-vl-68d2a7c1b8a8afce4ebd2dbe"&gt;Hugging Face&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ü§ñ &lt;a href="https://modelscope.cn/collections/Qwen3-VL-5c7a94c8cb144b"&gt;ModelScope&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;üìë &lt;a href="https://qwen.ai/blog?id=99f0335c4ad9ff6153e517418d48535ab6d8afef&amp;amp;from=research.latest-advancements-list"&gt;Blog&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;üìö &lt;a href="https://github.com/QwenLM/Qwen3-VL/tree/main/cookbooks"&gt;Cookbooks&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;üìë Paper is coming&amp;nbsp;&amp;nbsp; &lt;br /&gt; üñ•Ô∏è &lt;a href="https://huggingface.co/spaces/Qwen/Qwen3-VL-Demo"&gt;Demo&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;üí¨ &lt;a href="https://github.com/QwenLM/Qwen/raw/main/assets/wechat.png"&gt;WeChat (ÂæÆ‰ø°)&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ü´® &lt;a href="https://discord.gg/CV4E9rpNSD"&gt;Discord&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;üìë &lt;a href="https://help.aliyun.com/zh/model-studio/developer-reference/qwen-vl-api"&gt;API&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;üñ•Ô∏è &lt;a href="https://gallery.pai-ml.com/#/preview/deepLearning/cv/qwen2.5-vl"&gt;PAI-DSW&lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;Meet Qwen3-VL ‚Äî the most powerful vision-language model in the Qwen series to date.&lt;/p&gt; 
&lt;p&gt;This generation delivers comprehensive upgrades across the board: superior text understanding &amp;amp; generation, deeper visual perception &amp;amp; reasoning, extended context length, enhanced spatial and video dynamics comprehension, and stronger agent interaction capabilities.&lt;/p&gt; 
&lt;p&gt;Available in Dense and MoE architectures that scale from edge to cloud, with Instruct and reasoning‚Äëenhanced Thinking editions for flexible, on‚Äëdemand deployment.&lt;/p&gt; 
&lt;h4&gt;Key Enhancements:&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Visual Agent&lt;/strong&gt;: Operates PC/mobile GUIs‚Äîrecognizes elements, understands functions, invokes tools, completes tasks.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Visual Coding Boost&lt;/strong&gt;: Generates Draw.io/HTML/CSS/JS from images/videos.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Advanced Spatial Perception&lt;/strong&gt;: Judges object positions, viewpoints, and occlusions; provides stronger 2D grounding and enables 3D grounding for spatial reasoning and embodied AI.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Long Context &amp;amp; Video Understanding&lt;/strong&gt;: Native 256K context, expandable to 1M; handles books and hours-long video with full recall and second-level indexing.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Enhanced Multimodal Reasoning&lt;/strong&gt;: Excels in STEM/Math‚Äîcausal analysis and logical, evidence-based answers.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Upgraded Visual Recognition&lt;/strong&gt;: Broader, higher-quality pretraining is able to ‚Äúrecognize everything‚Äù‚Äîcelebrities, anime, products, landmarks, flora/fauna, etc.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Expanded OCR&lt;/strong&gt;: Supports 32 languages (up from 10); robust in low light, blur, and tilt; better with rare/ancient characters and jargon; improved long-document structure parsing.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Text Understanding on par with pure LLMs&lt;/strong&gt;: Seamless text‚Äìvision fusion for lossless, unified comprehension.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Model Architecture Updates:&lt;/h4&gt; 
&lt;p align="center"&gt; &lt;img src="https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/qwen3vl_arc.jpg" width="80%" /&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Interleaved-MRoPE&lt;/strong&gt;: Full‚Äëfrequency allocation over time, width, and height via robust positional embeddings, enhancing long‚Äëhorizon video reasoning.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;DeepStack&lt;/strong&gt;: Fuses multi‚Äëlevel ViT features to capture fine‚Äëgrained details and sharpen image‚Äìtext alignment.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Text‚ÄìTimestamp Alignment:&lt;/strong&gt; Moves beyond T‚ÄëRoPE to precise, timestamp‚Äëgrounded event localization for stronger video temporal modeling.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;2025.10.15: We have released the &lt;strong&gt;Qwen3-VL-4B&lt;/strong&gt; (&lt;a href="https://huggingface.co/Qwen/Qwen3-VL-4B-Instruct"&gt;Instruct&lt;/a&gt;/&lt;a href="https://huggingface.co/Qwen/Qwen3-VL-4B-Thinking"&gt;Thinking&lt;/a&gt;) and &lt;strong&gt;Qwen3-VL-8B&lt;/strong&gt; (&lt;a href="https://huggingface.co/Qwen/Qwen3-VL-8B-Instruct"&gt;Instruct&lt;/a&gt;/&lt;a href="https://huggingface.co/Qwen/Qwen3-VL-8B-Thinking"&gt;Thinking&lt;/a&gt;). Enjoy it!&lt;/li&gt; 
 &lt;li&gt;2025.10.4: We have released the &lt;a href="https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Instruct"&gt;Qwen3-VL-30B-A3B-Instruct&lt;/a&gt; and &lt;a href="https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Thinking"&gt;Qwen3-VL-30B-A3B-Thinking&lt;/a&gt;. We have also released the FP8 version of the Qwen3-VL models ‚Äî available in our &lt;a href="https://huggingface.co/collections/Qwen/qwen3-vl-68d2a7c1b8a8afce4ebd2dbe"&gt;HuggingFace collection&lt;/a&gt; and &lt;a href="https://modelscope.cn/collections/Qwen3-VL-5c7a94c8cb144b"&gt;ModelScope collection&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;2025.09.23: We have released the &lt;a href="https://huggingface.co/Qwen/Qwen3-VL-235B-A22B-Instruct"&gt;Qwen3-VL-235B-A22B-Instruct&lt;/a&gt; and &lt;a href="https://huggingface.co/Qwen/Qwen3-VL-235B-A22B-Thinking"&gt;Qwen3-VL-235B-A22B-Thinking&lt;/a&gt;. For more details, please check our &lt;a href="https://qwen.ai/blog?id=99f0335c4ad9ff6153e517418d48535ab6d8afef&amp;amp;from=research.latest-advancements-list"&gt;blog&lt;/a&gt;!&lt;/li&gt; 
 &lt;li&gt;2025.04.08: We provide the &lt;a href="https://github.com/QwenLM/Qwen2.5-VL/tree/main/qwen-vl-finetune"&gt;code&lt;/a&gt; for fine-tuning Qwen2-VL and Qwen2.5-VL.&lt;/li&gt; 
 &lt;li&gt;2025.03.25: We have released the &lt;a href="https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct"&gt;Qwen2.5-VL-32B&lt;/a&gt;. It is smarter and its responses align more closely with human preferences. For more details, please check our &lt;a href="https://qwenlm.github.io/blog/qwen2.5-vl-32b/"&gt;blog&lt;/a&gt;!&lt;/li&gt; 
 &lt;li&gt;2025.02.20: we have released the &lt;a href="https://arxiv.org/abs/2502.13923"&gt;Qwen2.5-VL Technical Report&lt;/a&gt;. Alongside the report, we have also released AWQ-quantized models for Qwen2.5-VL in three different sizes: &lt;a href="https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct-AWQ"&gt;3B&lt;/a&gt;, &lt;a href="https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct-AWQ"&gt;7B&lt;/a&gt; , and &lt;a href="https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct-AWQ"&gt;72B&lt;/a&gt; parameters.&lt;/li&gt; 
 &lt;li&gt;2025.01.28: We have released the &lt;a href="https://huggingface.co/Qwen"&gt;Qwen2.5-VL series&lt;/a&gt;. For more details, please check our &lt;a href="https://qwenlm.github.io/blog/qwen2.5-vl/"&gt;blog&lt;/a&gt;!&lt;/li&gt; 
 &lt;li&gt;2024.12.25: We have released the &lt;a href="https://huggingface.co/Qwen/QVQ-72B-Preview"&gt;QvQ-72B-Preview&lt;/a&gt;. QvQ-72B-Preview is an experimental research model, focusing on enhancing visual reasoning capabilities. For more details, please check our &lt;a href="https://qwenlm.github.io/blog/qvq-72b-preview/"&gt;blog&lt;/a&gt;!&lt;/li&gt; 
 &lt;li&gt;2024.09.19: The instruction-tuned &lt;a href="https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct"&gt;Qwen2-VL-72B model&lt;/a&gt; and its quantized version [&lt;a href="https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct-AWQ"&gt;AWQ&lt;/a&gt;, &lt;a href="https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct-GPTQ-Int4"&gt;GPTQ-Int4&lt;/a&gt;, &lt;a href="https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct-GPTQ-Int8"&gt;GPTQ-Int8&lt;/a&gt;] are now available. We have also released the &lt;a href="https://arxiv.org/pdf/2409.12191"&gt;Qwen2-VL paper&lt;/a&gt; simultaneously.&lt;/li&gt; 
 &lt;li&gt;2024.08.30: We have released the &lt;a href="https://huggingface.co/collections/Qwen/qwen2-vl-66cee7455501d7126940800d"&gt;Qwen2-VL series&lt;/a&gt;. The 2B and 7B models are now available, and the 72B model for open source is coming soon. For more details, please check our &lt;a href="https://qwenlm.github.io/blog/qwen2-vl/"&gt;blog&lt;/a&gt;!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Performance&lt;/h2&gt; 
&lt;h3&gt;Visual Tasks&lt;/h3&gt; 
&lt;div style="display: flex; justify-content: center; gap: 16px; flex-wrap: wrap;"&gt; 
 &lt;img src="https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/table_nothinking_vl.jpg" width="24%" /&gt; 
 &lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-VL/table_thinking_vl_.jpg" width="24%" /&gt; 
 &lt;img src="https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/table_nothinking_vl-30a3.jpg" width="26%" /&gt; 
 &lt;img src="https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/table_thinking_vl_30A3.jpg" width="22.5%" /&gt; 
&lt;/div&gt; 
&lt;div style="display: flex; justify-content: center; gap: 16px; flex-wrap: wrap;"&gt; 
 &lt;img src="https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/qwen3vl_4b_8b_vl_instruct.jpg" width="30%" /&gt; 
 &lt;img src="https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/qwen3vl_4b_8b_vl_thinking.jpg" width="24%" /&gt; 
&lt;/div&gt; 
&lt;h3&gt;Text-Centric Tasks&lt;/h3&gt; 
&lt;div style="display: flex; justify-content: center; gap: 16px; flex-wrap: wrap;"&gt; 
 &lt;img src="https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/table_nothinking_text.jpg" width="30%" /&gt; 
 &lt;img src="https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/table_thinking_text.jpg" width="32%" /&gt; 
 &lt;img src="https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/table_nothinking_text-30a3.jpg" width="30%" /&gt; 
&lt;/div&gt; 
&lt;div style="display: flex; justify-content: center; gap: 16px; flex-wrap: wrap;"&gt; 
 &lt;img src="https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/qwen3vl_4b_8b_text_instruct.jpg" width="33%" /&gt; 
 &lt;img src="https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/qwen3vl_4b_8b_text_thinking.jpg" width="28%" /&gt; 
&lt;/div&gt; 
&lt;h2&gt;Cookbooks&lt;/h2&gt; 
&lt;p&gt;We are preparing &lt;a href="https://github.com/QwenLM/Qwen3-VL/tree/main/cookbooks"&gt;cookbooks&lt;/a&gt; for many capabilities, including recognition, localization, document parsing, video understanding, key information extraction, and more. Welcome to learn more!&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Cookbook&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Open&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/omni_recognition.ipynb"&gt;Omni Recognition&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Not only identify animals, plants, people, and scenic spots but also recognize various objects such as cars and merchandise.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/omni_recognition.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/document_parsing.ipynb"&gt;Powerful Document Parsing Capabilities&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;The parsing of documents has reached a higher level, including not only text but also layout position information and our Qwen HTML format.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/document_parsing.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/2d_grounding.ipynb"&gt;Precise Object Grounding Across Formats&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Using relative position coordinates, it supports both boxes and points, allowing for diverse combinations of positioning and labeling tasks.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/2d_grounding.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/ocr.ipynb"&gt;General OCR and Key Information Extraction&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Stronger text recognition capabilities in natural scenes and multiple languages, supporting diverse key information extraction needs.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/ocr.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/video_understanding.ipynb"&gt;Video Understanding&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Better video OCR, long video understanding, and video grounding.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/video_understanding.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/mobile_agent.ipynb"&gt;Mobile Agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Locate and think for mobile phone control.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/mobile_agent.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/computer_use.ipynb"&gt;Computer-Use Agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Locate and think for controlling computers and Web.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/computer_use.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/3d_grounding.ipynb"&gt;3D Grounding&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Provide accurate 3D bounding boxes for both indoor and outdoor objects.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/3d_grounding.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/think_with_images.ipynb"&gt;Thinking with Images&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Utilize image_zoom_in_tool and search_tool to facilitate the model‚Äôs precise comprehension of fine-grained visual details within images.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/think_with_images.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/mmcode.ipynb"&gt;MultiModal Coding&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Generate accurate code based on rigorous comprehension of multimodal information.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/mmcode.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/long_document_understanding.ipynb"&gt;Long Document Understanding&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Achieve rigorous semantic comprehension of ultra-long documents.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/long_document_understanding.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/spatial_understanding.ipynb"&gt;Spatial Understanding&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;See, understand and reason about the spatial information&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/spatial_understanding.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Quickstart&lt;/h2&gt; 
&lt;p&gt;Below, we provide simple examples to show how to use Qwen3-VL with ü§ñ ModelScope and ü§ó Transformers.&lt;/p&gt; 
&lt;p&gt;The code of Qwen3-VL has been in the latest Hugging face transformers and we advise you to build from source with command:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install git+https://github.com/huggingface/transformers
# pip install transformers==4.57.0 # currently, V4.57.0 is not released
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;ü§ñ ModelScope&lt;/h3&gt; 
&lt;p&gt;We strongly advise users especially those in mainland China to use ModelScope. &lt;code&gt;snapshot_download&lt;/code&gt; can help you solve issues concerning downloading checkpoints.&lt;/p&gt; 
&lt;h3&gt;Using ü§ó Transformers to Chat&lt;/h3&gt; 
&lt;p&gt;Here we show a code snippet to show you how to use the chat model with &lt;code&gt;transformers&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from transformers import AutoModelForImageTextToText, AutoProcessor

# default: Load the model on the available device(s)
model = AutoModelForImageTextToText.from_pretrained(
    "Qwen/Qwen3-VL-235B-A22B-Instruct", dtype="auto", device_map="auto"
)

# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.
# model = AutoModelForImageTextToText.from_pretrained(
#     "Qwen/Qwen3-VL-235B-A22B-Instruct",
#     dtype=torch.bfloat16,
#     attn_implementation="flash_attention_2",
#     device_map="auto",
# )

processor = AutoProcessor.from_pretrained("Qwen/Qwen3-VL-235B-A22B-Instruct")

messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "image",
                "image": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg",
            },
            {"type": "text", "text": "Describe this image."},
        ],
    }
]

# Preparation for inference
inputs = processor.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,
    return_dict=True,
    return_tensors="pt"
)
inputs = inputs.to(model.device)

# Inference: Generation of the output
generated_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids_trimmed = [
    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
print(output_text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;!-- &lt;details&gt;
&lt;summary&gt;Minimum VRAM requirements&lt;/summary&gt;

| Precision | Qwen2.5-VL-3B | Qwen2.5-VL-7B | Qwen2.5-VL-72B |
|-----------|------------| --------- | -------- |
| FP32      | 11.5 GB    | 26.34 GB  | 266.21 GB |
| BF16      | 5.75 GB    | 13.17 GB  | 133.11 GB |
| INT8      | 2.87 GB    | 6.59 GB   | 66.5 GB |
| INT4      | 1.44 GB    | 3.29 GB   | 33.28 GB |

Note: The table above presents the theoretical minimum video memory requirements for inference with `transformers`; however, in practice, the actual memory usage is typically at least 1.2 times higher. For more information, see the linked resource [here](https://huggingface.co/docs/accelerate/main/en/usage_guides/model_size_estimator).
&lt;/details&gt; --&gt; 
&lt;details&gt; 
 &lt;summary&gt;Multi image inference&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Messages containing multiple images and a text query
messages = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "file:///path/to/image1.jpg"},
            {"type": "image", "image": "file:///path/to/image2.jpg"},
            {"type": "text", "text": "Identify the similarities between these images."},
        ],
    }
]

# Preparation for inference
inputs = processor.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,
    return_dict=True,
    return_tensors="pt"
)
inputs = inputs.to(model.device)

# Inference: Generation of the output
generated_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids_trimmed = [
    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
print(output_text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Video inference&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Messages containing a video url(or a local path) and a text query
messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "video",
                "video": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/space_woaudio.mp4",
            },
            {"type": "text", "text": "Describe this video."},
        ],
    }
]

# Preparation for inference
inputs = processor.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,
    return_dict=True,
    return_tensors="pt"
)
inputs = inputs.to(model.device)

# Inference: Generation of the output
generated_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids_trimmed = [
    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
print(output_text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Batch inference&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# for batch generation, padding_side should be set to left!
processor.tokenizer.padding_side = 'left'

# Sample messages for batch inference
messages1 = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "file:///path/to/image1.jpg"},
            {"type": "image", "image": "file:///path/to/image2.jpg"},
            {"type": "text", "text": "What are the common elements in these pictures?"},
        ],
    }
]
messages2 = [
    {"role": "system", "content": [{"type": "text", "text": "You are a helpful assistant."}]},
    {"role": "user", "content": [{"type": "text", "text": "Who are you?"}]},
]
# Combine messages for batch processing
messages = [messages1, messages2]

# Preparation for inference
inputs = processor.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,
    return_dict=True,
    return_tensors="pt",
    padding=True # padding should be set for batch generation!
)
inputs = inputs.to(model.device)

# Inference: Generation of the output
generated_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids_trimmed = [
    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
print(output_text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Pixel Control via Official Processor&lt;/summary&gt; 
 &lt;p&gt;Using the official HF processor, we can conveniently control the budget of visual tokens. Since the Qwen3-VL processor separates image and video processing, we can independently configure the pixel budget for each modality.&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;For the image processor&lt;/strong&gt;:&lt;br /&gt; The parameter &lt;code&gt;size['longest_edge']&lt;/code&gt; originally corresponds to &lt;code&gt;max_pixels&lt;/code&gt;, which defines the maximum number of pixels allowed for an image (i.e., for an image of height H and width W, H √ó W must not exceed &lt;code&gt;max_pixels&lt;/code&gt;; image channels are ignored for simplicity).&lt;br /&gt; Similarly, &lt;code&gt;size['shortest_edge']&lt;/code&gt; corresponds to &lt;code&gt;min_pixels&lt;/code&gt;, specifying the minimum allowable pixel count for an image.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;For the video processor&lt;/strong&gt;:&lt;br /&gt; The interpretation differs slightly. &lt;code&gt;size['longest_edge']&lt;/code&gt; represents the maximum total number of pixels across all frames in a video ‚Äî for a video of shape T√óH√óW, the product T√óH√óW must not exceed &lt;code&gt;size['longest_edge']&lt;/code&gt;.&lt;br /&gt; Similarly, &lt;code&gt;size['shortest_edge']&lt;/code&gt; sets the minimum total pixel budget for the video.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;processor = AutoProcessor.from_pretrained("Qwen/Qwen3-VL-235B-A22B-Instruct")

# budget for image processor, since the compression ratio is 32 for Qwen3-VL, we can set the number of visual tokens of a single image to 256-1280
processor.image_processor.size = {"longest_edge": 1280*32*32, "shortest_edge": 256*32*32}

# budget for video processor, we can set the number of visual tokens of a single video to 256-16384
processor.video_processor.size = {"longest_edge": 16384*32*32, "shortest_edge": 256*32*32}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ul&gt; 
  &lt;li&gt;You can further control the &lt;strong&gt;sample fps&lt;/strong&gt; or &lt;strong&gt;sample frames&lt;/strong&gt; of video, as shown below.&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "video",
                "video": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/space_woaudio.mp4",
            },
            {"type": "text", "text": "Describe this video."},
        ],
    }
]

# for video input, we can further control the fps or num_frames. \
# defaultly, fps is set to 2

# set fps = 4
inputs = processor.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,
    return_dict=True,
    return_tensors="pt",
    fps=4
)
inputs = inputs.to(model.device)

# set num_frames = 128 and overwrite the fps to None!
# inputs = processor.apply_chat_template(
#     messages,
#     tokenize=True,
#     add_generation_prompt=True,
#     return_dict=True,
#     return_tensors="pt",
#     num_frames=128,
#     fps=None,
# )
# inputs = inputs.to(model.device)

# Inference: Generation of the output
generated_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids_trimmed = [
    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
print(output_text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;New &lt;code&gt;qwen-vl-utils&lt;/code&gt; Usage&lt;/h3&gt; 
&lt;p&gt;With the latest &lt;code&gt;qwen-vl-utils&lt;/code&gt; toolkit (backward compatible with Qwen2.5-VL), you can control pixel constraints per visual input.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install qwen-vl-utils==0.0.14
# It's highly recommended to use `[decord]` feature for faster video loading.
# pip install qwen-vl-utils[decord]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Compared to previous version, the new &lt;code&gt;qwen-vl-utils&lt;/code&gt; introduces:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;"image_patch_size": &lt;code&gt;14&lt;/code&gt; for Qwen2.5-VL and &lt;code&gt;16&lt;/code&gt; for Qwen3-VL. Default set to &lt;code&gt;14&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;"return_video_metadata"(Qwen3-VL only): Due to the new video processor, if True, each video returns as (video_tensor, video_metadata). Default set to &lt;code&gt;False&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# for Qwen2.5VL, you can simply call 
images, videos, video_kwargs = process_vision_info(messages, return_video_kwargs=True)

# For Qwen3VL series, you should call 
images, videos, video_kwargs = process_vision_info(messages, image_patch_size=16, return_video_kwargs=True, return_video_metadata=True)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;üìå Note: Since &lt;code&gt;qwen-vl-utils&lt;/code&gt; already resizes images/videos, pass &lt;code&gt;do_resize=False&lt;/code&gt; to the processor to avoid duplicate resizing.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Process Images&lt;/summary&gt; 
 &lt;p&gt;For input images, we support local files, base64, and URLs.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# You can directly insert a local file path, a URL, or a base64-encoded image into the position where you want in the text.
## Local file path
messages = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "file:///path/to/your/image.jpg"},
            {"type": "text", "text": "Describe this image."},
        ],
    }
]
## Image URL
messages = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "http://path/to/your/image.jpg"},
            {"type": "text", "text": "Describe this image."},
        ],
    }
]
## Base64 encoded image
messages = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "data:image;base64,/9j/..."},
            {"type": "text", "text": "Describe this image."},
        ],
    }
]
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;We provide two methods for fine-grained control over the image size input to the model:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;Specify exact dimensions: Directly set resized_height and resized_width. These values will be rounded to the nearest multiple of 32 (32 for Qwen3VL, 28 for Qwen2.5VL).&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Define min_pixels and max_pixels: Images will be resized to maintain their aspect ratio within the range of min_pixels and max_pixels&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from transformers import AutoModelForImageTextToText, AutoProcessor
from qwen_vl_utils import process_vision_info

model = AutoModelForImageTextToText.from_pretrained(
    "Qwen/Qwen3-VL-235B-A22B-Instruct", dtype="auto", device_map="auto"
)

processor = AutoProcessor.from_pretrained("Qwen/Qwen3-VL-235B-A22B-Instruct")

# resized_height and resized_width
messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "image",
                "image": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg",
                "resized_height": 280,
                "resized_width": 420,
            },
            {"type": "text", "text": "Describe this image."},
        ],
    }
]

# min_pixels and max_pixels
messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "image",
                "image": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg",
                "min_pixels": 50176,
                "max_pixels": 50176,

            },
            {"type": "text", "text": "Describe this image."},
        ],
    }
]

# Preparation for inference with qwen-vl-utils
text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
images, videos = process_vision_info(messages, image_patch_size=16)

# since qwen-vl-utils has resize the images/videos, \
# we should pass do_resize=False to avoid duplicate operation in processor!
inputs = processor(text=text, images=images, videos=videos, do_resize=False, return_tensors="pt")
inputs = inputs.to(model.device)

# Inference: Generation of the output
generated_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids_trimmed = [
    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
print(output_text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Process Videos&lt;/summary&gt; 
 &lt;p&gt;For input videos, we support images lists, local path and url.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Messages containing a images list as a video and a text query
messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "video",
                "video": [
                    "file:///path/to/frame1.jpg",
                    "file:///path/to/frame2.jpg",
                    "file:///path/to/frame3.jpg",
                    "file:///path/to/frame4.jpg",
                ],
                'sample_fps':'1', # sample_fps: frame sampling rate (frames per second), used to determine timestamps for each frame
            },
            {"type": "text", "text": "Describe this video."},
        ],
    }
]

# Messages containing a local video path and a text query
messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "video",
                "video": "file:///path/to/video1.mp4",
                "max_pixels": 360 * 420,
                "fps": 1.0,
            },
            {"type": "text", "text": "Describe this video."},
        ],
    }
]

# Messages containing a video url and a text query
messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "video",
                "video": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/space_woaudio.mp4",
                "min_pixels": 4 * 32 * 32,
                "max_pixels": 256 * 32 * 32,
                "total_pixels": 20480 * 32 * 32,
            },
            {"type": "text", "text": "Describe this video."},
        ],
    }
]

&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;We recommend setting appropriate values for the &lt;code&gt;min_pixels&lt;/code&gt; and &lt;code&gt;max_pixels&lt;/code&gt; parameters based on available GPU memory and the specific application scenario to restrict the resolution of individual frames in the video.&lt;/p&gt; 
 &lt;p&gt;Alternatively, you can use the &lt;code&gt;total_pixels&lt;/code&gt; parameter to limit the total number of tokens in the video (it is recommended to set this value below 24576 * 32 * 32 to avoid excessively long input sequences). For more details on parameter usage and processing logic, please refer to the &lt;code&gt;fetch_video&lt;/code&gt; function in &lt;code&gt;qwen_vl_utils/vision_process.py&lt;/code&gt;.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from transformers import AutoModelForImageTextToText, AutoProcessor
from qwen_vl_utils import process_vision_info

model = AutoModelForImageTextToText.from_pretrained(
    "Qwen/Qwen3-VL-235B-A22B-Instruct", dtype="auto", device_map="auto"
)

processor = AutoProcessor.from_pretrained("Qwen/Qwen3-VL-235B-A22B-Instruct")

messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "video",
                "video": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/space_woaudio.mp4",
                "min_pixels": 4 * 32 * 32,
                "max_pixels": 256 * 32 * 32,
                "total_pixels": 20480 * 32 * 32,
            },
            {"type": "text", "text": "Describe this video."},
        ],
    }
]

text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
images, videos, video_kwargs = process_vision_info(messages, image_patch_size=16, return_video_kwargs=True, return_video_metadata=True)

# split the videos and according metadatas
if videos is not None:
    videos, video_metadatas = zip(*videos)
    videos, video_metadatas = list(videos), list(video_metadatas)
else:
    video_metadatas = None

# since qwen-vl-utils has resize the images/videos, \
# we should pass do_resize=False to avoid duplicate operation in processor!
inputs = processor(text=text, images=images, videos=videos, video_metadata=video_metadatas, return_tensors="pt", do_resize=False, **video_kwargs)
inputs = inputs.to(model.device)

# Inference: Generation of the output
generated_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids_trimmed = [
    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
print(output_text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Video Backends and URL Compatibility&lt;/summary&gt; 
 &lt;p&gt;Currently, &lt;code&gt;qwen-vl-utils&lt;/code&gt; supports three video decoding backends: &lt;code&gt;torchvision&lt;/code&gt;, &lt;code&gt;decord&lt;/code&gt;, and &lt;code&gt;torchcodec&lt;/code&gt;. While &lt;code&gt;decord&lt;/code&gt; and &lt;code&gt;torchcodec&lt;/code&gt; generally offer significantly faster decoding speeds compared to &lt;code&gt;torchvision&lt;/code&gt;, we recommend using &lt;code&gt;torchcodec&lt;/code&gt;. This is because &lt;code&gt;decord&lt;/code&gt; has known issues, such as decoding hangs, and its project is no longer actively maintained.&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;For &lt;code&gt;decord&lt;/code&gt;, if you are not using Linux, you might not be able to install &lt;code&gt;decord&lt;/code&gt; from PyPI. In that case, you can use &lt;code&gt;pip install qwen-vl-utils&lt;/code&gt; which will fall back to using torchvision for video processing. However, you can still &lt;a href="https://github.com/dmlc/decord?tab=readme-ov-file#install-from-source"&gt;install decord from source&lt;/a&gt; to get decord used when loading video.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;To use &lt;code&gt;torchcodec&lt;/code&gt; as the backend for video decoding, follow the installation instructions provided in the official &lt;a href="https://github.com/pytorch/torchcodec/tree/main?tab=readme-ov-file#installing-torchcodec"&gt;torchcodec repository&lt;/a&gt; and install it manually. Note that &lt;code&gt;torchcodec&lt;/code&gt; depends on FFmpeg for decoding functionality.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;Video URL compatibility is primarily determined by the version of the third-party library being used. For more details, refer to the table below. If you prefer not to use the default backend, you can switch it by setting &lt;code&gt;FORCE_QWENVL_VIDEO_READER&lt;/code&gt; to &lt;code&gt;torchvision&lt;/code&gt;, &lt;code&gt;decord&lt;/code&gt;, or &lt;code&gt;torchcodec&lt;/code&gt;.&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Backend&lt;/th&gt; 
    &lt;th&gt;HTTP&lt;/th&gt; 
    &lt;th&gt;HTTPS&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;torchvision &amp;gt;= 0.19.0&lt;/td&gt; 
    &lt;td&gt;‚úÖ&lt;/td&gt; 
    &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;torchvision &amp;lt; 0.19.0&lt;/td&gt; 
    &lt;td&gt;‚ùå&lt;/td&gt; 
    &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;decord&lt;/td&gt; 
    &lt;td&gt;‚úÖ&lt;/td&gt; 
    &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;torchcodec&lt;/td&gt; 
    &lt;td&gt;‚úÖ&lt;/td&gt; 
    &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h3&gt;More Usage Tips&lt;/h3&gt; 
&lt;h4&gt;Add ids for Multiple Visual Inputs&lt;/h4&gt; 
&lt;p&gt;By default, images and video content are directly included in the conversation. When handling multiple images, it's helpful to add labels to the images and videos for better reference. Users can control this behavior with the following settings:&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Add vision ids&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;conversation = [
    {
        "role": "user",
        "content": [{"type": "image"}, {"type": "text", "text": "Hello, how are you?"}],
    },
    {
        "role": "assistant",
        "content": "I'm doing well, thank you for asking. How can I assist you today?",
    },
    {
        "role": "user",
        "content": [
            {"type": "text", "text": "Can you describe these images and video?"},
            {"type": "image"},
            {"type": "image"},
            {"type": "video"},
            {"type": "text", "text": "These are from my vacation."},
        ],
    },
    {
        "role": "assistant",
        "content": "I'd be happy to describe the images and video for you. Could you please provide more context about your vacation?",
    },
    {
        "role": "user",
        "content": "It was a trip to the mountains. Can you see the details in the images and video?",
    },
]

# default:
prompt_without_id = processor.apply_chat_template(
    conversation, add_generation_prompt=True
)
# Excepted output: '&amp;lt;|im_start|&amp;gt;system\nYou are a helpful assistant.&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;user\n&amp;lt;|vision_start|&amp;gt;&amp;lt;|image_pad|&amp;gt;&amp;lt;|vision_end|&amp;gt;Hello, how are you?&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;assistant\nI'm doing well, thank you for asking. How can I assist you today?&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;user\nCan you describe these images and video?&amp;lt;|vision_start|&amp;gt;&amp;lt;|image_pad|&amp;gt;&amp;lt;|vision_end|&amp;gt;&amp;lt;|vision_start|&amp;gt;&amp;lt;|image_pad|&amp;gt;&amp;lt;|vision_end|&amp;gt;&amp;lt;|vision_start|&amp;gt;&amp;lt;|video_pad|&amp;gt;&amp;lt;|vision_end|&amp;gt;These are from my vacation.&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;assistant\nI'd be happy to describe the images and video for you. Could you please provide more context about your vacation?&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;user\nIt was a trip to the mountains. Can you see the details in the images and video?&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;assistant\n'


# add ids
prompt_with_id = processor.apply_chat_template(
    conversation, add_generation_prompt=True, add_vision_id=True
)
# Excepted output: '&amp;lt;|im_start|&amp;gt;system\nYou are a helpful assistant.&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;user\nPicture 1: &amp;lt;|vision_start|&amp;gt;&amp;lt;|image_pad|&amp;gt;&amp;lt;|vision_end|&amp;gt;Hello, how are you?&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;assistant\nI'm doing well, thank you for asking. How can I assist you today?&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;user\nCan you describe these images and video?Picture 2: &amp;lt;|vision_start|&amp;gt;&amp;lt;|image_pad|&amp;gt;&amp;lt;|vision_end|&amp;gt;Picture 3: &amp;lt;|vision_start|&amp;gt;&amp;lt;|image_pad|&amp;gt;&amp;lt;|vision_end|&amp;gt;Video 1: &amp;lt;|vision_start|&amp;gt;&amp;lt;|video_pad|&amp;gt;&amp;lt;|vision_end|&amp;gt;These are from my vacation.&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;assistant\nI'd be happy to describe the images and video for you. Could you please provide more context about your vacation?&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;user\nIt was a trip to the mountains. Can you see the details in the images and video?&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;assistant\n'
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h4&gt;Flash-Attention 2 to speed up generation&lt;/h4&gt; 
&lt;p&gt;First, make sure to install the latest version of Flash Attention 2:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -U flash-attn --no-build-isolation
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Also, you should have a hardware that is compatible with Flash-Attention 2. Read more about it in the official documentation of the &lt;a href="https://github.com/Dao-AILab/flash-attention"&gt;flash attention repository&lt;/a&gt;. FlashAttention-2 can only be used when a model is loaded in &lt;code&gt;torch.float16&lt;/code&gt; or &lt;code&gt;torch.bfloat16&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;To load and run a model using Flash Attention-2, simply add &lt;code&gt;attn_implementation="flash_attention_2"&lt;/code&gt; when loading the model as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import torch
from transformers import AutoModelForImageTextToText

model = AutoModelForImageTextToText.from_pretrained(
    "Qwen/Qwen3-VL-235B-A22B-Instruct", 
    torch_dtype=torch.bfloat16, 
    attn_implementation="flash_attention_2",
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Processing Long Texts&lt;/h4&gt; 
&lt;p&gt;The current &lt;code&gt;config.json&lt;/code&gt; is set for context length up to 256K tokens. To handle extensive inputs exceeding 256K tokens, we utilize &lt;a href="https://arxiv.org/abs/2309.00071"&gt;YaRN&lt;/a&gt;, a technique for enhancing model length extrapolation, ensuring optimal performance on lengthy texts.&lt;/p&gt; 
&lt;p&gt;For supported frameworks (currently transformers and vLLM), you could modify &lt;code&gt;max_position_embeddings&lt;/code&gt; and &lt;code&gt;rope_scaling&lt;/code&gt; in &lt;code&gt;config.json&lt;/code&gt; to enable YaRN:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;{
    "max_position_embeddings": 1000000,
	...,
    "rope_scaling": {
        "rope_type": "yarn",
        "mrope_section": [
            24,
            20,
            20
        ],
        "mrope_interleaved": true,
        "factor": 3.0,
        "original_max_position_embeddings": 262144
    },
    ...
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;When using vLLM for serving, you can also enable YaRN by adding the additional arguments &lt;code&gt;--rope-scaling&lt;/code&gt; and &lt;code&gt;--max-model-len&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;vllm serve Qwen/Qwen3-VL-235B-A22B-Instruct --rope-scaling '{"rope_type":"yarn","factor":3.0,"original_max_position_embeddings": 262144,"mrope_section":[24,20,20],"mrope_interleaved": true}' --max-model-len 1000000
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Because Interleaved-MRoPE‚Äôs position IDs grow more slowly than vanilla RoPE, use a &lt;strong&gt;smaller scaling factor&lt;/strong&gt;. For example, to support 1M context with 256K context length, set factor=2 or 3 ‚Äî not 4.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Try Qwen3-VL-235B-A22 with API!&lt;/h3&gt; 
&lt;p&gt;To explore Qwen3-VL-235B-A22, a more fascinating multimodal model, we encourage you to test our cutting-edge API service. Let's start the exciting journey right now!&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from openai import OpenAI

# set your DASHSCOPE_API_KEY here
DASHSCOPE_API_KEY = ""

client = OpenAI(
    api_key=DASHSCOPE_API_KEY,
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
)

completion = client.chat.completions.create(
    model="qwen3-vl-235b-a22b-instruct",
    messages=[{"role": "user", "content": [
        {"type": "image_url",
         "image_url": {"url": "https://dashscope.oss-cn-beijing.aliyuncs.com/images/dog_and_girl.jpeg"}},
        {"type": "text", "text": "ËøôÊòØ‰ªÄ‰πà"},
    ]}]
)
print(completion.model_dump_json())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more usage, please refer to the tutorial at &lt;a href="https://help.aliyun.com/zh/model-studio/developer-reference/qwen-vl-api"&gt;aliyun&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Web UI Example&lt;/h3&gt; 
&lt;p&gt;In this section, we provide instructions for users to build a web-based user interface (UI) demo. This UI demo allows users to interact with a predefined model or application through a web browser. Follow the steps below to get started.&lt;/p&gt; 
&lt;p&gt;Install the required dependencies by running the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r requirements_web_demo.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Launch a browser-based UI to interact with the model:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python web_demo_mm.py -c /your/path/to/qwen3vl/weight
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After running the command, you‚Äôll see a link generated in the terminal similar to this:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Running on local: http://127.0.0.1:7860/
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Open the link in your browser to interact with the model ‚Äî try text, images, or other features. For a quick start, you can also use our pre-built Docker image:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;cd docker &amp;amp;&amp;amp; bash run_web_demo.sh -c /your/path/to/qwen3vl/weight --port 8881
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Deployment&lt;/h2&gt; 
&lt;p&gt;We recommend using vLLM for fast Qwen3-VL deployment and inference. You need to install &lt;code&gt;vllm&amp;gt;=0.11.0&lt;/code&gt; to enable Qwen3-VL support. You can also use our &lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen3-VL/main/#-docker"&gt;official docker image&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Please check &lt;a href="https://docs.vllm.ai/en/latest/serving/multimodal_inputs.html"&gt;vLLM official documentation&lt;/a&gt; for more details about online serving and offline inference for multimodal models.&lt;/p&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install accelerate
pip install qwen-vl-utils==0.0.14
# Install the latest version of vLLM 'vllm&amp;gt;=0.11.0'
uv pip install -U vllm
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Online Serving&lt;/h3&gt; 
&lt;p&gt;You can start either a vLLM or SGLang server to serve LLMs efficiently, and then access it using an OpenAI-style API.&lt;/p&gt; 
&lt;p&gt;The following launch command is applicable to H100/H200; for more efficient deployment or deployment on other GPUs, please refer to the &lt;a href="https://docs.vllm.ai/projects/recipes/en/latest/Qwen/Qwen3-VL.html"&gt;vLLM community guide&lt;/a&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;vLLM server&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# Efficient inference with FP8 checkpoint
# Requires NVIDIA H100+ and CUDA 12+
vllm serve Qwen/Qwen3-VL-235B-A22B-Instruct-FP8 \
  --tensor-parallel-size 8 \
  --mm-encoder-tp-mode data \
  --enable-expert-parallel \
  --async-scheduling \
  --host 0.0.0.0 \
  --port 22002
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;SGLang server&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;python -m sglang.launch_server \
   --model-path Qwen/Qwen3-VL-235B-A22B-Instruct \
   --host 0.0.0.0 \
   --port 22002 \
   --tp 8
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Image Request Example&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import time
from openai import OpenAI

client = OpenAI(
    api_key="EMPTY",
    base_url="http://127.0.0.1:22002/v1",
    timeout=3600
)

messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "image_url",
                "image_url": {
                    "url": "https://ofasys-multimodal-wlcb-3-toshanghai.oss-accelerate.aliyuncs.com/wpf272043/keepme/image/receipt.png"
                }
            },
            {
                "type": "text",
                "text": "Read all the text in the image."
            }
        ]
    }
]

start = time.time()
response = client.chat.completions.create(
    model="Qwen/Qwen3-VL-235B-A22B-Instruct-FP8",
    messages=messages,
    max_tokens=2048
)
print(f"Response costs: {time.time() - start:.2f}s")
print(f"Generated text: {response.choices[0].message.content}")
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Video Request Example&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import time
from openai import OpenAI

client = OpenAI(
    api_key="EMPTY",
    base_url="http://127.0.0.1:22002/v1",
    timeout=3600
)

messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "video_url",
                "video_url": {
                    "url": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/space_woaudio.mp4"
                }
            },
            {
                "type": "text",
                "text": "How long is this video?"
            }
        ]
    }
]

start = time.time()
response = client.chat.completions.create(
    model="Qwen/Qwen3-VL-235B-A22B-Instruct-FP8",
    messages=messages,
    max_tokens=2048
)

print(f"Response costs: {time.time() - start:.2f}s")
print(f"Generated text: {response.choices[0].message.content}")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Offline Inference&lt;/h3&gt; 
&lt;p&gt;You can also use vLLM or SGLang to inference Qwen3-VL locally:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;vLLM Examples&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# -*- coding: utf-8 -*-
import torch
from qwen_vl_utils import process_vision_info
from transformers import AutoProcessor
from vllm import LLM, SamplingParams

import os
os.environ['VLLM_WORKER_MULTIPROC_METHOD'] = 'spawn'

def prepare_inputs_for_vllm(messages, processor):
    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    # qwen_vl_utils 0.0.14+ reqired
    image_inputs, video_inputs, video_kwargs = process_vision_info(
        messages,
        image_patch_size=processor.image_processor.patch_size,
        return_video_kwargs=True,
        return_video_metadata=True
    )
    print(f"video_kwargs: {video_kwargs}")

    mm_data = {}
    if image_inputs is not None:
        mm_data['image'] = image_inputs
    if video_inputs is not None:
        mm_data['video'] = video_inputs

    return {
        'prompt': text,
        'multi_modal_data': mm_data,
        'mm_processor_kwargs': video_kwargs
    }


if __name__ == '__main__':
    # messages = [
    #     {
    #         "role": "user",
    #         "content": [
    #             {
    #                 "type": "video",
    #                 "video": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/space_woaudio.mp4",
    #             },
    #             {"type": "text", "text": "ËøôÊÆµËßÜÈ¢ëÊúâÂ§öÈïø"},
    #         ],
    #     }
    # ]

    messages = [
        {
            "role": "user",
            "content": [
              {
                  "type": "image",
                  "image": "https://ofasys-multimodal-wlcb-3-toshanghai.oss-accelerate.aliyuncs.com/wpf272043/keepme/image/receipt.png",
              },
              {"type": "text", "text": "Read all the text in the image."},
            ],
        }
    ]

    # TODO: change to your own checkpoint path
    checkpoint_path = "Qwen/Qwen3-VL-235B-A22B-Instruct-FP8"
    processor = AutoProcessor.from_pretrained(checkpoint_path)
    inputs = [prepare_inputs_for_vllm(message, processor) for message in [messages]]

    llm = LLM(
        model=checkpoint_path,
        mm_encoder_tp_mode="data",
        enable_expert_parallel=True,
        tensor_parallel_size=torch.cuda.device_count(),
        seed=0
    )

    sampling_params = SamplingParams(
        temperature=0,
        max_tokens=1024,
        top_k=-1,
        stop_token_ids=[],
    )

    for i, input_ in enumerate(inputs):
        print()
        print('=' * 40)
        print(f"Inputs[{i}]: {input_['prompt']=!r}")
    print('\n' + '&amp;gt;' * 40)

    outputs = llm.generate(inputs, sampling_params=sampling_params)
    for i, output in enumerate(outputs):
        generated_text = output.outputs[0].text
        print()
        print('=' * 40)
        print(f"Generated text: {generated_text!r}")
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;SGLang Examples&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import time
from PIL import Image
from sglang import Engine
from qwen_vl_utils import process_vision_info
from transformers import AutoProcessor, AutoConfig


if __name__ == "__main__":
    # TODO: change to your own checkpoint path
    checkpoint_path = "Qwen/Qwen3-VL-235B-A22B-Instruct"
    processor = AutoProcessor.from_pretrained(checkpoint_path)

    messages = [
        {
            "role": "user",
            "content": [
              {
                  "type": "image",
                  "image": "https://ofasys-multimodal-wlcb-3-toshanghai.oss-accelerate.aliyuncs.com/wpf272043/keepme/image/receipt.png",
              },
              {"type": "text", "text": "Read all the text in the image."},
            ],
        }
    ]

    text = processor.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    image_inputs, _ = process_vision_info(messages, image_patch_size=processor.image_processor.patch_size)

    llm = Engine(
        model_path=checkpoint_path,
        enable_multimodal=True,
        mem_fraction_static=0.8,
        tp_size=4,
        attention_backend="fa3",
        context_length=10240,
        disable_cuda_graph=True,
    )

    start = time.time()
    sampling_params = {"max_new_tokens": 1024}
    response = llm.generate(prompt=text, image_data=image_inputs, sampling_params=sampling_params)
    print(f"Response costs: {time.time() - start:.2f}s")
    print(f"Generated text: {response['text']}")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Evaluation Reproduction&lt;/h2&gt; 
&lt;p&gt;To facilitate faithful reproduction of our reported results, we summarize our official evaluation settings below.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Inference runtime: &lt;a href="https://github.com/vllm-project/vllm"&gt;vLLM&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Evaluation frameworks: &lt;a href="https://github.com/open-compass/VLMEvalKit"&gt;VLMEvalKit&lt;/a&gt;, &lt;a href="https://github.com/EvolvingLMMs-Lab/lmms-eval"&gt;lmms-eval&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Notes: 
  &lt;ul&gt; 
   &lt;li&gt;For a few benchmarks, we slightly modified the evaluation prompts; detailed changes will be documented in the upcoming technical report.&lt;/li&gt; 
   &lt;li&gt;A small number of benchmarks are internally constructed; we plan to release the code and reproduction assets afterwards.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Generation Hyperparameters&lt;/h3&gt; 
&lt;h4&gt;Instruct models&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export greedy='false'
export seed=3407
export top_p=0.8
export top_k=20
export temperature=0.7
export repetition_penalty=1.0
export presence_penalty=1.5
export out_seq_length=32768
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Thinking models&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export greedy='false'
export seed=1234
export top_p=0.95
export top_k=20
export repetition_penalty=1.0
export presence_penalty=0.0
export temperature=0.6
export out_seq_length=40960
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;üê≥ Docker&lt;/h2&gt; 
&lt;p&gt;To simplify the deploy process, we provide docker images with pre-build environments: &lt;a href="https://hub.docker.com/r/qwenllm/qwenvl"&gt;qwenllm/qwenvl&lt;/a&gt;. You only need to install the driver and download model files to launch demos.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run --gpus all --ipc=host --network=host --rm --name qwen3vl -it qwenllm/qwenvl:qwen3vl-cu128 bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you find our paper and code useful in your research, please consider giving a star &lt;span&gt;‚≠ê&lt;/span&gt; and citation &lt;span&gt;üìù&lt;/span&gt; :)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-BibTeX"&gt;
@article{Qwen2.5-VL,
  title={Qwen2.5-VL Technical Report},
  author={Bai, Shuai and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Song, Sibo and Dang, Kai and Wang, Peng and Wang, Shijie and Tang, Jun and Zhong, Humen and Zhu, Yuanzhi and Yang, Mingkun and Li, Zhaohai and Wan, Jianqiang and Wang, Pengfei and Ding, Wei and Fu, Zheren and Xu, Yiheng and Ye, Jiabo and Zhang, Xi and Xie, Tianbao and Cheng, Zesen and Zhang, Hang and Yang, Zhibo and Xu, Haiyang and Lin, Junyang},
  journal={arXiv preprint arXiv:2502.13923},
  year={2025}
}

@article{Qwen2-VL,
  title={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution},
  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang},
  journal={arXiv preprint arXiv:2409.12191},
  year={2024}
}

@article{Qwen-VL,
  title={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},
  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2308.12966},
  year={2023}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;br /&gt;</description>
    </item>
    
  </channel>
</rss>