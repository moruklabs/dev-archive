<rss version="2.0">
  <channel>
    <title>GitHub All Languages Daily Trending</title>
    <description>Daily Trending of All Languages in GitHub</description>
    <pubDate>Fri, 17 Oct 2025 01:31:25 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>testcontainers/testcontainers-java</title>
      <link>https://github.com/testcontainers/testcontainers-java</link>
      <description>&lt;p&gt;Testcontainers is a Java library that supports JUnit tests, providing lightweight, throwaway instances of common databases, Selenium web browsers, or anything else that can run in a Docker container.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Testcontainers&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://maven-badges.herokuapp.com/maven-central/org.testcontainers/testcontainers"&gt;&lt;img src="https://maven-badges.herokuapp.com/maven-central/org.testcontainers/testcontainers/badge.svg?sanitize=true" alt="Maven Central" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://app.netlify.com/sites/testcontainers/deploys"&gt;&lt;img src="https://api.netlify.com/api/v1/badges/189f28a2-7faa-42ff-b03c-738142079cc9/deploy-status" alt="Netlify Status" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/codespaces/new?hide_repo_select=true&amp;amp;ref=main&amp;amp;repo=33816473&amp;amp;machine=standardLinux32gb&amp;amp;devcontainer_path=.devcontainer%2Fdevcontainer.json&amp;amp;location=EastUs"&gt;&lt;img src="https://github.com/codespaces/badge.svg?sanitize=true" alt="Open in GitHub Codespaces" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://ge.testcontainers.org/scans"&gt;&lt;img src="https://img.shields.io/badge/Revved%20up%20by-Develocity-06A0CE?logo=Gradle&amp;amp;labelColor=02303A" alt="Revved up by Develocity" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Testcontainers is a Java library that supports JUnit tests, providing lightweight, throwaway instances of common databases, Selenium web browsers, or anything else that can run in a Docker container.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/testcontainers/testcontainers-java/main/docs/logo.png" alt="Testcontainers logo" /&gt;&lt;/p&gt; 
&lt;h1&gt;&lt;a href="https://java.testcontainers.org"&gt;Read the documentation here&lt;/a&gt;&lt;/h1&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/testcontainers/testcontainers-java/main/LICENSE"&gt;LICENSE&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Copyright&lt;/h2&gt; 
&lt;p&gt;Copyright (c) 2015 - 2021 Richard North and other authors.&lt;/p&gt; 
&lt;p&gt;MS SQL Server module is (c) 2017 - 2021 G DATA Software AG and other authors.&lt;/p&gt; 
&lt;p&gt;Hashicorp Vault module is (c) 2017 - 2021 Capital One Services, LLC and other authors.&lt;/p&gt; 
&lt;p&gt;See &lt;a href="https://github.com/testcontainers/testcontainers-java/graphs/contributors"&gt;contributors&lt;/a&gt; for all contributors.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ChristianLempa/boilerplates</title>
      <link>https://github.com/ChristianLempa/boilerplates</link>
      <description>&lt;p&gt;This is my personal template collection. Here you'll find templates, and configurations for various tools, and technologies.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Christian's &lt;code&gt;Boilerplates&lt;/code&gt;&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://youtu.be/apgp9egIKK8"&gt;&lt;img src="https://cnd-prod-1.s3.us-west-004.backblazeb2.com/new-banner4-scaled-for-github.jpg" alt="Welcome" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Hey, there!&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;I'm Christian, and I'm passionate about creating educational tech content for IT Pros and Homelab nerds.&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;What are Boilerplates?&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Boilerplates&lt;/strong&gt; is a curated collection of production-ready templates for your homelab and infrastructure projects. Stop copying configurations from random GitHub repos or starting from scratch every time you spin up a new service!&lt;/p&gt; 
&lt;h2&gt;Boilerplates CLI&lt;/h2&gt; 
&lt;p&gt;The Boilerplates CLI tool gives you instant access to battle-tested templates for Docker, Terraform, Ansible, Kubernetes, and more.&lt;/p&gt; 
&lt;p&gt;Each template includes sensible defaults, best practices, and common configuration patterns—so you can focus on customizing for your environment.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;🚀 &lt;strong&gt;Quick Setup&lt;/strong&gt; - Generate complete project structures in seconds&lt;/li&gt; 
 &lt;li&gt;🔧 &lt;strong&gt;Fully Customizable&lt;/strong&gt; - Interactive prompts or non-interactive mode with variable overrides&lt;/li&gt; 
 &lt;li&gt;💾 &lt;strong&gt;Smart Defaults&lt;/strong&gt; - Save your preferred values and reuse across projects&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Technologies evolve rapidly. While I actively maintain these templates, always review generated configurations before deploying to production.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;p&gt;Install the Boilerplates CLI using the automated installer:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install latest version
curl -fsSL https://raw.githubusercontent.com/christianlempa/boilerplates/main/scripts/install.sh | bash

# Install specific version
curl -fsSL https://raw.githubusercontent.com/christianlempa/boilerplates/main/scripts/install.sh | bash -s -- --version v1.2.3
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The installer uses &lt;code&gt;pipx&lt;/code&gt; to create an isolated environment for the CLI tool. Once installed, the &lt;code&gt;boilerplates&lt;/code&gt; command will be available in your terminal.&lt;/p&gt; 
&lt;h3&gt;Quick Start&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Explore 
boilerplates --help

# Update Repository Library
boilerplates repo update

# List all available templates for a docker compose
boilerplates compose list

# Show details about a specific template
boilerplates compose show nginx

# Generate a template (interactive mode)
boilerplates compose generate authentik

# Generate with custom output directory
boilerplates compose generate nginx my-nginx-server

# Non-interactive mode with variable overrides
boilerplates compose generate traefik my-proxy \
  --var service_name=traefik \
  --var traefik_enabled=true \
  --var traefik_host=proxy.example.com \
  --no-interactive
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Managing Defaults&lt;/h3&gt; 
&lt;p&gt;Save time by setting default values for variables you use frequently:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Set a default value
boilerplates compose defaults set container_timezone "America/New_York"
boilerplates compose defaults set restart_policy "unless-stopped"

&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Template Libraries&lt;/h3&gt; 
&lt;p&gt;Boilerplates uses git-based libraries to manage templates. You can add custom repositories:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# List configured libraries
boilerplates repo list

# Update all libraries
boilerplates repo update

# Add a custom library
boilerplates repo add my-templates https://github.com/user/templates \
  --directory library \
  --branch main

# Remove a library
boilerplates repo remove my-templates
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;For comprehensive documentation, advanced usage, and template development guides, check out the &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/ChristianLempa/wiki"&gt;Wiki&lt;/a&gt;&lt;/strong&gt; &lt;em&gt;(coming soon)&lt;/em&gt;.&lt;/p&gt; 
&lt;p&gt;If you're looking for detailed tutorials on specific tools and technologies, visit my &lt;a href="https://www.youtube.com/@christianlempa"&gt;YouTube Channel&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contribution&lt;/h2&gt; 
&lt;p&gt;If you’d like to contribute to this project, reach out to me on social media or &lt;a href="https://christianlempa.de/discord"&gt;Discord&lt;/a&gt;, or create a pull request for the necessary changes.&lt;/p&gt; 
&lt;h2&gt;Other Resources&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/christianlempa/dotfiles"&gt;Dotfiles&lt;/a&gt; - My personal configuration files on macOS&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/christianlempa/cheat-sheets"&gt;Cheat-Sheets&lt;/a&gt; - Command Reference for various tools and technologies&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Support me&lt;/h2&gt; 
&lt;p&gt;Creating high-quality videos and valuable resources that are accessible to everyone, free of charge, is a huge challenge. With your contribution, I can dedicate more time and effort into the creation process, which ultimately enhances the quality of the content. So, all your support, by becoming a member, truly makes a significant impact on what I do. And you’ll also get some cool benefits and perks in return, as a recognition of your support.&lt;/p&gt; 
&lt;p&gt;Remember, &lt;em&gt;&lt;strong&gt;supporting me is entirely optional.&lt;/strong&gt;&lt;/em&gt; Your choice to become a member or not won't change your access to my videos and resources. You are also welcome to reach out to me on Discord, if you have any questions or feedback.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.patreon.com/christianlempa"&gt;https://www.patreon.com/christianlempa&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>devlikeapro/waha</title>
      <link>https://github.com/devlikeapro/waha</link>
      <description>&lt;p&gt;WAHA - WhatsApp HTTP API (REST API) that you can configure in a click! 3 engines: WEBJS (browser based), NOWEB (websocket nodejs), GOWS (websocket go)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;WAHA&lt;/h1&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/devlikeapro/waha/core/logo.png" style="border-radius: 50%" width="150" /&gt; &lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;WAHA&lt;/strong&gt; - &lt;strong&gt;W&lt;/strong&gt;hats&lt;strong&gt;A&lt;/strong&gt;pp &lt;strong&gt;H&lt;/strong&gt;TTP &lt;strong&gt;A&lt;/strong&gt;PI (REST API) that you can install on your own server and run in less than 5 minutes!&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://hub.docker.com/r/devlikeapro/waha"&gt;&lt;img src="https://img.shields.io/docker/pulls/devlikeapro/waha" alt="Docker Pulls" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Documentation: &lt;a href="https://waha.devlike.pro/"&gt;https://waha.devlike.pro/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Dashboard Example: &lt;a href="https://waha.devlike.pro/dashboard"&gt;https://waha.devlike.pro/dashboard&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Swagger Example: &lt;a href="https://waha.devlike.pro/swagger"&gt;https://waha.devlike.pro/swagger&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Tables of Contents&lt;/h1&gt; 
&lt;!-- toc --&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/devlikeapro/waha/core/#quick-start"&gt;Quick start&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/devlikeapro/waha/core/#requirements"&gt;Requirements&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/devlikeapro/waha/core/#send-your-first-message"&gt;Send your first message&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/devlikeapro/waha/core/#1-download-image"&gt;1. Download image&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/devlikeapro/waha/core/#2-run-whatsapp-http-api"&gt;2. Run WhatsApp HTTP API&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/devlikeapro/waha/core/#3-start-a-new-session"&gt;3. Start a new session&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/devlikeapro/waha/core/#4-get-and-scan-qr"&gt;4. Get and scan QR&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/devlikeapro/waha/core/#5-get-the-screenshot"&gt;5. Get the screenshot&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/devlikeapro/waha/core/#6-send-a-text-message"&gt;6. Send a text message&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/devlikeapro/waha/core/#what-is-next"&gt;What is next?&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/devlikeapro/waha/core/#development"&gt;Development&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/devlikeapro/waha/core/#start-the-project"&gt;Start the project&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- tocstop --&gt; 
&lt;h1&gt;Quick start&lt;/h1&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;p&gt;Only thing that you must have - installed docker. Please follow the original instruction &lt;a href="https://docs.docker.com/get-docker/" target="_blank" rel="noopener"&gt;how to install docker -&amp;gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;When you are ready - come back and follows the below steps to send the first text message to WhatsApp via HTTP API!&lt;/p&gt; 
&lt;h2&gt;Send your first message&lt;/h2&gt; 
&lt;p&gt;Let's go over steps that allow you to send your first text message via WhatsApp HTTP API!&lt;/p&gt; 
&lt;h3&gt;1. Download image&lt;/h3&gt; 
&lt;p&gt;Assuming you have installed &lt;a href="https://docs.docker.com/get-docker/"&gt;Docker&lt;/a&gt;, let's download the image.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker pull devlikeapro/waha
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker login -u devlikeapro -p {KEY}
docker pull devlikeapro/waha-plus
docker logout
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Read more about how to get &lt;code&gt;PASSWORD&lt;/code&gt; for &lt;a href="https://waha.devlike.pro/docs/how-to/waha-plus/"&gt;&lt;strong&gt;➕ WAHA Plus&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;2. Run WhatsApp HTTP API&lt;/h3&gt; 
&lt;p&gt;Run WhatsApp HTTP API:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run -it --rm -p 3000:3000/tcp --name waha devlikeapro/waha

# It prints logs and the last line must be
# WhatsApp HTTP API is running on: http://[::1]:3000
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Open the link in your browser &lt;a href="http://localhost:3000/"&gt;http://localhost:3000/&lt;/a&gt; and you'll see API documentation (Swagger).&lt;/p&gt; 
&lt;h3&gt;3. Start a new session&lt;/h3&gt; 
&lt;p&gt;To start a new session you should have your mobile phone with installed WhatsApp application close to you.&lt;/p&gt; 
&lt;p&gt;Please go and read how what we'll need to a bit later: &lt;a href="https://faq.whatsapp.com/381777293328336/?helpref=hc_fnav" target="_blank"&gt; How to log in - the instruction on WhatsApp site &lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;When your ready - find &lt;code&gt;POST /api/sessions&lt;/code&gt;, click on &lt;strong&gt;Try it out&lt;/strong&gt;, then &lt;strong&gt;Execute&lt;/strong&gt; a bit below.&lt;/p&gt; 
&lt;p&gt;The example payload:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "name": "default"
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;By using the request with &lt;code&gt;name&lt;/code&gt; values you can start multiple session (WhatsApp accounts) inside the single docker container in Plus&lt;/p&gt; 
&lt;h3&gt;4. Get and scan QR&lt;/h3&gt; 
&lt;p&gt;Find &lt;code&gt;GET /api/screenshot&lt;/code&gt; and execute it, it shows you QR code.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Scan the QR with your cell phone's WhatsApp app.&lt;/strong&gt;&lt;/p&gt; 
&lt;h3&gt;5. Get the screenshot&lt;/h3&gt; 
&lt;p&gt;Execute &lt;code&gt;GET /api/screenshot&lt;/code&gt; after a few seconds after scanning the QR - it'll show you the screenshot of you Whatsapp instance. If you can get the actual screenshot - then you're ready to start sending messages!&lt;/p&gt; 
&lt;h3&gt;6. Send a text message&lt;/h3&gt; 
&lt;p&gt;Let's send a text message - find &lt;code&gt;POST /api/sendText&lt;/code&gt; in &lt;a href="http://localhost:3000/"&gt;swagger&lt;/a&gt; and change &lt;code&gt;chatId&lt;/code&gt; this way: use a phone international phone number without &lt;code&gt;+&lt;/code&gt; symbol and add &lt;code&gt;@c.us&lt;/code&gt; at the end.&lt;/p&gt; 
&lt;p&gt;For phone number &lt;code&gt;12132132131&lt;/code&gt; the &lt;code&gt;chatId&lt;/code&gt; is &lt;code&gt;12132132131@c.us&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;The example payload:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "chatId": "12132132130@c.us",
  "text": "Hi there!",
  "session": "default"
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Also, you can use &lt;code&gt;curl&lt;/code&gt; and send POST request like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Phone without +
export PHONE=12132132130
curl -d "{\"chatId\": \"${PHONE}@c.us\", \"text\": \"Hello from WhatsApp HTTP API\" }" -H "Content-Type: application/json" -X POST http://localhost:3000/api/sendText
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;What is next?&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://waha.devlike.pro/docs/overview/introduction/"&gt;Go and read the full documentation!&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Development&lt;/h1&gt; 
&lt;h2&gt;Start the project&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Clone the repository&lt;/li&gt; 
 &lt;li&gt;Make sure you're using node&amp;gt;=22 (check &lt;a href="https://raw.githubusercontent.com/devlikeapro/waha/core/.nvmrc"&gt;.nvmrc&lt;/a&gt; to get the version)&lt;/li&gt; 
 &lt;li&gt;Run the following commands:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install dependencies
yarn install
# Fetch and compile proto files
yarn gows:proto
# Run
yarn start
# open http://localhost:3000
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>linexjlin/GPTs</title>
      <link>https://github.com/linexjlin/GPTs</link>
      <description>&lt;p&gt;leaked prompts of GPTs&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;GPTs&lt;/h1&gt; 
&lt;p&gt;This repo collects leaked prompts of GPTs.&lt;/p&gt; 
&lt;p&gt;Most of the prompts I collected from &lt;a href="https://x.com/dotey/status/1722831122285932592?s=20"&gt;here&lt;/a&gt;, some are from GitHub users' pull requests, and some are from using some hacked by me.&lt;/p&gt; 
&lt;h2&gt;Leaked Prompts of GPTs&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/devrelguide.md"&gt;DevRel Guide&lt;/a&gt; by Rohit Ghumare&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/IstioGuru.md"&gt;Istio Guru&lt;/a&gt; by Rohit Ghumare&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/BabyAgi.txt.md"&gt;BabyAgi.txt&lt;/a&gt; by Nicholas Dobos&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Take%20Code%20Captures.md"&gt;Take Code Captures&lt;/a&gt; by oscaramos.dev&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Diffusion%20Master.md"&gt;Diffusion Master&lt;/a&gt; by RUSLAN LICHENKO&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/YT%20transcriber.md"&gt;YT transcriber&lt;/a&gt; by gpt.swyx.io&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/%E7%A7%91%E6%8A%80%E6%96%87%E7%AB%A0%E7%BF%BB%E8%AF%91.md"&gt;科技文章翻译&lt;/a&gt; by Junmin Liu&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/genz%204%20meme.md"&gt;genz 4 meme&lt;/a&gt; by ChatGPT&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Math%20Mentor.md"&gt;Math Mentor&lt;/a&gt; by ChatGPT&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Interview%20Coach.md"&gt;Interview Coach&lt;/a&gt; by Danny Graziosi&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/The%20Negotiator.md"&gt;The Negotiator&lt;/a&gt; by ChatGPT&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Sous%20Chef.md"&gt;Sous Chef&lt;/a&gt; by ChatGPT&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Tech%20Support%20Advisor.md"&gt;Tech Support Advisor&lt;/a&gt; by ChatGPT&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Sticker%20Whiz.md"&gt;Sticker Whiz&lt;/a&gt; by ChatGPT&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Obscribe.md"&gt;Obscribe&lt;/a&gt; by Azhar Hama&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Girlfriend%20Emma.md"&gt;Girlfriend Emma&lt;/a&gt; by dddshop.com&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/TherapistGPT.md"&gt;TherapistGPT&lt;/a&gt; by David Boyle&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/%F0%9F%8E%80My%20excellent%20classmates%20(Help%20with%20my%20homework!).md"&gt;🎀My excellent classmates (Help with my homework!)&lt;/a&gt; by Kevin Ivery&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Moby%20Dick%20RPG.md"&gt;Moby Dick RPG&lt;/a&gt; by word.studio&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/%E6%98%A5%E9%9C%9E%E3%81%A4%E3%81%8F%E3%81%97%20Tsukushi%20Harugasumi.md"&gt;春霞つくし Tsukushi Harugasumi&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Canva.md"&gt;Canva&lt;/a&gt; by community builder&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Midjourney%20Generator.md"&gt;Midjourney Generator&lt;/a&gt; by Film Me Pty Ltd&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Chibi%20Kohaku%20%28%E7%8C%AB%E9%9F%B3%E3%82%B3%E3%83%8F%E3%82%AF%29.md"&gt;Chibi Kohaku (猫音コハク)&lt;/a&gt; by tr1ppy.com&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Calendar%20GPT.md"&gt;Calendar GPT&lt;/a&gt; by community builder&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Sarcastic%20Humorist.md"&gt;Sarcastic Humorist&lt;/a&gt; by Irene L Williams&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Manga%20Miko%20-%20Anime%20Girlfriend.md"&gt;Manga Miko - Anime Girlfriend&lt;/a&gt; by Declan Gessel&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/OCR-GPT.md"&gt;OCR-GPT&lt;/a&gt; by Siyang Qiu&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/AI%20PDF.md"&gt;AI PDF&lt;/a&gt; by myaidrive.com&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/%E5%AE%8C%E8%9B%8B%EF%BC%8C%E6%88%91%E8%A2%AB%E7%BE%8E%E5%A5%B3%E5%8C%85%E5%9B%B4%E4%BA%86(AI%E5%90%8C%E4%BA%BA).md"&gt;完蛋，我被美女包围了(AI同人)&lt;/a&gt; by ikena.ai&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Virtual%20Sweetheart.md"&gt;Virtual Sweetheart&lt;/a&gt; by Ryan Imgrund&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Synthia%20%F0%9F%98%8B%F0%9F%8C%9F.md"&gt;Synthia 😋🌟&lt;/a&gt; by BENARY Jacquis Ronaldo&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Video%20Script%20Generator.md"&gt;Video Script Generator&lt;/a&gt; by empler.ai&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/The%20Shaman.md"&gt;The Shaman&lt;/a&gt; by Austin C Potter&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Meme%20Magic.md"&gt;Meme Magic&lt;/a&gt; by ratcgpts.com&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/EmojAI.md"&gt;EmojAI&lt;/a&gt; by ratcgpts.com&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/CodeCopilot.md"&gt;CodeCopilot&lt;/a&gt; by promptspellsmith.com&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Data%20Analysis.md"&gt;Data Analysis&lt;/a&gt; by ChatGPT&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/YT%20Summarizer.md"&gt;YT Summarizer&lt;/a&gt; by John Venkata Pilla&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Game%20Time.md"&gt;Game Time&lt;/a&gt; by ChatGPT&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Sales%20Cold%20Email%20Coach.md"&gt;Sales Cold Email Coach&lt;/a&gt; by Phu Hai Nghiem&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Grimoire.md"&gt;Grimoire&lt;/a&gt; by Nicholas Dobos&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Trey%20Ratcliff%27s%20Photo%20Critique%20GPT.md"&gt;Trey Ratcliff's Photo Critique GPT&lt;/a&gt; by Raymond Ratcliff&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Coloring%20Book%20Hero.md"&gt;Coloring Book Hero&lt;/a&gt; by ChatGPT&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Cosmic%20Dream.md"&gt;Cosmic Dream&lt;/a&gt; by ChatGPT&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Hot%20Mods.md"&gt;Hot Mods&lt;/a&gt; by ChatGPT&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Laundry%20Buddy.md"&gt;Laundry Buddy&lt;/a&gt; by ChatGPT&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Gif-PT.md"&gt;Gif-PT&lt;/a&gt; by mindgoblinstudios.com&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/LogoGPT.md"&gt;LogoGPT&lt;/a&gt; by A SAI RAHUL&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/CuratorGPT.md"&gt;CuratorGPT&lt;/a&gt; by Shushant Lakhyani&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/KoeGPT.md"&gt;KoeGPT&lt;/a&gt; by Alex Northstar&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/HormoziGPT.md"&gt;HormoziGPT&lt;/a&gt; by Alex Northstar&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/What%20should%20I%20watch%EF%BC%9F.md"&gt;What should I watch?&lt;/a&gt; by ammaar.me&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/MuskGPT.md"&gt;MuskGPT&lt;/a&gt; by Alex Northstar&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Retro%20Adventures.md"&gt;Retro Adventures&lt;/a&gt; by Greg Fodor&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/ClearGPT.md"&gt;ClearGPT&lt;/a&gt; by Alex Northstar&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Visual%20Weather%20Artist%20GPT.md"&gt;Visual Weather Artist GPT&lt;/a&gt; by Alex Volkov&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/X%20Optimizer%20GPT.md"&gt;X Optimizer GPT&lt;/a&gt; by Rowan Cheung&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Agi.zip.md"&gt;Agi.zip&lt;/a&gt; by mindgoblinstudios.com&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Simpsonize%20Me.md"&gt;Simpsonize Me&lt;/a&gt; by octaneai.com&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/plugin%20surf.md"&gt;plugin surf&lt;/a&gt; by dumpling.software&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Book%20to%20Prompt.md"&gt;Book to Prompt&lt;/a&gt; by Lucas C Pimentel&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/ALL%20IN%20GPT.md"&gt;ALL IN GPT&lt;/a&gt; by KRUGER CLINTIN LYLE&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/ScholarAI.md"&gt;ScholarAI&lt;/a&gt; by Lakshya Bakshi&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Character%20Forger.md"&gt;Character Forger&lt;/a&gt; by learnaiwiz.com&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/10x%20Engineer.md"&gt;10x Engineer&lt;/a&gt; by Jacob Ottiger&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/AI%20Doctor.md"&gt;AI Doctor&lt;/a&gt; by Yaniv Goldenberg&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/AI%20Paper%20Polisher%20Pro.md"&gt;AI Paper Polisher Pro&lt;/a&gt; by Haiwen Huang&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/%28A.I.%20Bestie%29.md"&gt;(A.I. Bestie)&lt;/a&gt; by yongtaizhou&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/MetabolismBoosterGPT.md"&gt;MetabolismBoosterGPT&lt;/a&gt; by Shushant Lakhyani&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/%E6%82%B2%E6%85%98%E4%B8%96%E7%95%8C%20RPG.md"&gt;悲慘世界 RPG&lt;/a&gt; by LU CHE YU&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Secret%20Code%20Guardian.md"&gt;Secret Code Guardian&lt;/a&gt; by pyroprompts.com&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/High-Quality%20Review%20Analyzer.md"&gt;High-Quality Review Analyzer&lt;/a&gt; by Caitlin Hathaway&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/AI%20Lover.md"&gt;AI Lover&lt;/a&gt; by 廖芳國&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/TaxGPT.md"&gt;TaxGPT&lt;/a&gt; by Phat Pham&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/%E5%A4%A9%E5%AE%98%E5%BA%99%E7%9A%84%E5%88%98%E5%8D%8A%E4%BB%99.md"&gt;天官庙的刘半仙&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/ConvertAnything.md"&gt;ConvertAnything&lt;/a&gt; by Pietro Schirano&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/%E7%97%A4%E7%96%AE%E6%B2%BB%E7%96%97%E6%8C%87%E5%8D%97.md"&gt;痤疮治疗指南&lt;/a&gt; by community builder&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/%E7%8C%AB%E8%80%B3%E7%BE%8E%E5%B0%91%E5%A5%B3%E3%82%A4%E3%83%A9%E3%82%B9%E3%83%88%E3%83%A1%E3%83%BC%E3%82%AB%E3%83%BC.md"&gt;猫耳美少女イラストメーカー&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/toonGPT.md"&gt;toonGPT&lt;/a&gt; by indievish.com&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Viral%20Hooks%20Generator.md"&gt;Viral Hooks Generator&lt;/a&gt; by Muhammad Hassan Asghar&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/OpenStorytelling%20Plus.md"&gt;OpenStorytelling Plus&lt;/a&gt; by BRYAN HARRIS&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Choose%20your%20own%20adventure!.md"&gt;Choose your own adventure!&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Pic-book%20Artist.md"&gt;Pic-book Artist&lt;/a&gt; by ZHANG RUI&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Quality%20Raters%20SEO%20Guide.md"&gt;Quality Raters SEO Guide&lt;/a&gt; by Laurent Jean&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/GymStreak%20Workout%20Creator.md"&gt;GymStreak Workout Creator&lt;/a&gt; by Joseph Mambwe&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Video%20Game%20Almanac.md"&gt;Video Game Almanac&lt;/a&gt; by Evan Betley&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/%E8%84%8F%E8%AF%9D%E8%BF%9E%E7%AF%87.md"&gt;脏话连篇&lt;/a&gt; by Junmin Liu&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/%E5%B0%8F%E5%9D%8F%E8%9B%8B.md"&gt;小坏蛋&lt;/a&gt; by 李继刚&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Mystic%20%E5%8D%A0%E5%8D%9C%F0%9F%94%AE.md"&gt;Mystic 占卜🔮&lt;/a&gt; by community builder&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/DomainsGPT.md"&gt;DomainsGPT&lt;/a&gt; by oneword.domains&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Weather%20Artist.md"&gt;Weather Artist&lt;/a&gt; by Rahul Chakraborty&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Secret%20Keeper.md"&gt;Secret Keeper&lt;/a&gt; by Tobias Buschor&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Mr.%20Ranedeer.md"&gt;Mr. Ranedeer&lt;/a&gt; by mr-ranedeer.com&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/YouTubeGPT.md"&gt;YouTubeGPT&lt;/a&gt; by Miguel Piedrafita&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/TweetX%20Enhancer.md"&gt;TweetX Enhancer&lt;/a&gt; by Viacheslav Kovalevskyi&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/%E7%8E%8B%E9%98%B3%E6%98%8E.md"&gt;王阳明&lt;/a&gt; by Bruno Konopelski&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/%E8%80%81%E7%88%B8%EF%BC%8C%E8%AF%A5%E6%80%8E%E4%B9%88%E5%8A%9E%EF%BC%9F.md"&gt;老爸，该怎么办？&lt;/a&gt; by Junmin Liu&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/%E8%80%81%E5%A6%88%EF%BC%8C%E6%88%91%E7%88%B1%E4%BD%A0.md"&gt;老妈，我爱你&lt;/a&gt; by Junmin Liu&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Code%20Explainer.md"&gt;Code Explainer&lt;/a&gt; by promptboom.com&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/%E2%9C%8F%EF%B8%8FAll-around%20Writer%20%28Professional%20Version%29.md"&gt;✏️All-around Writer (Professional Version)&lt;/a&gt; by Kevin Ivery&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Story%20Spock.md"&gt;Story Spock&lt;/a&gt; by learnaiwiz.com&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Blog%20Expert.md"&gt;Blog Expert&lt;/a&gt; by Faisal Binateeq&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Paw%20Pal.md"&gt;Paw Pal&lt;/a&gt; by coreachdigital.com&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/GPT%20Shop%20Keeper.md"&gt;GPT Shop Keeper&lt;/a&gt; by mindgoblinstudios.com&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/GPT%20Builder.md"&gt;GPT Builder&lt;/a&gt; by ChatGPT&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/RolePlayHumanWritingGPT.md"&gt;RolePlayHumanWritingGPT&lt;/a&gt; by Joseph J&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/%E6%9E%81%E7%AE%80%E7%BF%BB%E8%AF%91.md"&gt;极简翻译&lt;/a&gt; by LIU YULONG&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/OpenAPI%20Builder.md"&gt;OpenAPI Builder&lt;/a&gt; by IALife&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/%E8%A1%8C%E4%B8%9A%E6%B4%9E%E5%AF%9F%E5%AE%B6.md"&gt;行业洞察家&lt;/a&gt; by community builder&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/%E6%9E%AB%E5%8F%B6%E6%9E%97.md"&gt;枫叶林&lt;/a&gt; by Zhang Lingling&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/%E5%AE%8C%E8%9B%8B%EF%BC%81%E6%88%91%E7%88%B1%E4%B8%8A%E4%BA%86%E5%A7%90%E5%A7%90.md"&gt;完蛋！我爱上了姐姐&lt;/a&gt; by autogen.icu&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/%E5%AD%AB%E5%AD%90%20-%20saysay.ai.md"&gt;孫子 - saysay.ai&lt;/a&gt; by Takayuki Fukatsu&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/LangGPT.md"&gt;LangGPT&lt;/a&gt; by 云中江树&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Super%20Describe.md"&gt;Super Describe&lt;/a&gt; by Javier Lopez Lopez&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/FramerGPT.md"&gt;FramerGPT&lt;/a&gt; by Joe Lee&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/20K%20Vocab%20builder.md"&gt;20K Vocab builder&lt;/a&gt; by Community Builder&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/GPT%20Customizer,%20File%20Finder%20&amp;amp;%20JSON%20Action%20Creator.md"&gt;GPT Customizer, File Finder &amp;amp; JSON Action Creator&lt;/a&gt; by Webcafe AI&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/AI%E7%AE%97%E5%91%BD.md"&gt;AI算命&lt;/a&gt; by Jerrold Bergnaum&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/TailwindCSS%20Previewer.md"&gt;TailwindCSS Previewer&lt;/a&gt; by WindChat.link&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/TailwindGPT.md"&gt;TailwindGPT&lt;/a&gt; by Miguel Piedrafita&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/%E9%AA%82%E9%86%92%E6%81%8B%E7%88%B1%E8%84%91.md"&gt;骂醒恋爱脑&lt;/a&gt; by bing yan&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/%E9%90%B5%E5%85%AC%E9%9B%9E.md"&gt;鐵公雞&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/%E5%B9%BF%E5%91%8A%E6%96%87%E6%A1%88%E5%A4%A7%E5%B8%88.md"&gt;广告文案大师&lt;/a&gt; by wa gaga&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/%E5%B0%8F%E7%BA%A2%E4%B9%A6%E5%86%99%E4%BD%9C%E4%B8%93%E5%AE%B6.md"&gt;小红书写作专家&lt;/a&gt; by xtxian.com&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Universal%20Primer.md"&gt;Universal Primer&lt;/a&gt; by Siqi&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/%E5%98%B4%E8%87%AD%E7%8E%8B.md"&gt;嘴臭王&lt;/a&gt; by Kun Qian&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/%F0%9F%A7%99%E2%80%8D%E2%99%82%EF%B8%8F%E7%AE%97%E5%91%BD%E5%85%88%E7%94%9F.md"&gt;🧙‍♂️算命先生&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/%E6%94%BB%E5%87%BB%E5%9E%8B%E9%A2%86%E5%AF%BC.md"&gt;攻击型领导&lt;/a&gt; by Sure Lee&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/%E7%9F%A5%E8%AF%86%E6%B8%8A%E5%8D%9A%E7%9A%84%E5%81%A5%E8%BA%AB%E6%95%99%E7%BB%83.md"&gt;知识渊博的健身教练&lt;/a&gt; by Xianhong Huang&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Ads%20Generator%20by%20Joe.md"&gt;Ads Generator by Joe&lt;/a&gt; by Xiangyang Qiao&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/%E7%9F%AD%E8%A7%86%E9%A2%91%E8%84%9A%E6%9C%AC.md"&gt;短视频脚本&lt;/a&gt; by gpt.hroid.top&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Watercolor%20Illustrator%20GPT.md"&gt;Watercolor Illustrator GPT&lt;/a&gt; by Ali Ali&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Xhs%20Writer%20-%20Mary.md"&gt;Xhs Writer: Mary&lt;/a&gt; by lysonober.com&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Writing%20Assistant.md"&gt;Writing Assistant&lt;/a&gt; by Junmin Liu&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Cross-Border%20Investigation%20Assistant%20%E8%B7%A8%E5%A2%83%E5%81%B5%E6%9F%A5%E5%B0%8F%E5%8A%A9%E6%89%8B.md"&gt;Cross-Border Investigation Assistant 跨境偵查小助手&lt;/a&gt; by CHANG CHIA KAI&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/%E5%AD%90%E8%A8%80%E5%A5%B3%E5%8F%8B.md"&gt;子言女友&lt;/a&gt; by Yu Dong&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/HumanWriterGPT.md"&gt;HumanWriterGPT&lt;/a&gt; by rjarivi&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/The%20Secret%20of%20Monkey%20Island-%20Amsterdam.md"&gt;The Secret of Monkey Island: Amsterdam&lt;/a&gt; by levels.io&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/%E5%87%8C%E5%87%A4%E7%AE%AB.md"&gt;凌凤箫&lt;/a&gt; by community builder&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Email%20Responder%20Pro.md"&gt;Email Responder Pro&lt;/a&gt; by Max Krishtul&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Blog%20Post%20Generator.md"&gt;Blog Post Generator&lt;/a&gt; by usevisuals.com&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/%E9%87%91%E5%BA%B8%E7%BE%A4%E4%BF%A0%E5%82%B3.md"&gt;金庸群俠傳&lt;/a&gt; by 葉宗瑋&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/42master-Beck.md"&gt;42master-Beck&lt;/a&gt; by 42master.io&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/%E9%9D%9E%E8%99%9A%E6%9E%84%E4%BD%9C%E5%93%81%E7%9A%84%E9%98%85%E8%AF%BB%E9%AB%98%E6%89%8B.md"&gt;非虚构作品的阅读高手&lt;/a&gt; by 陈一斌&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/ID%20Photo%20Pro.md"&gt;ID Photo Pro&lt;/a&gt; by Johnetta Kovacek&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/%E8%A7%A3%E6%A2%A6%E5%A4%A7%E5%B8%88.md"&gt;解梦大师&lt;/a&gt; by robertmao.com&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Fantasy%20Book%20Weaver.md"&gt;Fantasy Book Weaver&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/LeetCode%20Problem%20Solver.md"&gt;LeetCode Problem Solver&lt;/a&gt; by Arturo Bravo Rovirosa&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/img2img.md"&gt;img2img&lt;/a&gt; by neural.love&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Music%20Writer.md"&gt;Music Writer&lt;/a&gt; by Gorden_Sun&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/%E6%AD%A6%E6%9E%97%E7%A7%98%E4%BC%A0%EF%BC%9A%E6%B1%9F%E6%B9%96%E6%8E%A2%E9%99%A9.md"&gt;武林秘传：江湖探险&lt;/a&gt; by jesselau.com&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Cauldron.md"&gt;Cauldron&lt;/a&gt; by mindgoblinstudios.com&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Consistency%20Crafter%202024.md"&gt;Consistency Crafter 2024&lt;/a&gt; by Dmytro Korol&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Breakdown%20Outline%20Any%20Topic.md"&gt;!Breakdown: Outline Any Topic&lt;/a&gt; by puzzle.today&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/The%20Rizz%20Game.md"&gt;!The Rizz Game&lt;/a&gt; by puzzle.today&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/%E8%B6%85%E7%BA%A7Dalle.md"&gt;超级Dalle&lt;/a&gt; by myaiforce.com&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Phoenix%20Ink.md"&gt;Phoenix Ink&lt;/a&gt; by MEHMET GENCAY IŞIK&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Email%20Proofreader.md"&gt;Email Proofreader&lt;/a&gt; by Pouria Rouzrokh&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/%E8%8B%B1%E6%96%87%E6%A0%A1%E6%AD%A3GPT.md"&gt;英文校正GPT&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Get%20Simpsonized!.md"&gt;🍩 Get Simpsonized! 🍩&lt;/a&gt; by sprintrade.com&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/CIPHERON.md"&gt;CIPHERON 🧪&lt;/a&gt; by community builder&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Nomad%20List.md"&gt;Nomad List&lt;/a&gt; by photoai.com&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/CEO%20GPT.md"&gt;CEO GPT&lt;/a&gt; by Kiseki Technologies Limited&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Creative%20Writing%20Coach.md"&gt;Creative Writing Coach&lt;/a&gt; by ChatGPT&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Mocktail%20Mixologist.md"&gt;Mocktail Mixologist&lt;/a&gt; by ChatGPT&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Assistente%20AI%20per%20CEO%20marketing%20oriented.md"&gt;Assistente AI per CEO marketing oriented&lt;/a&gt; by Giorgio FATARELLA&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Captain%20Action.md"&gt;Captain Action&lt;/a&gt; by Yohei Nakajima&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Naruto%20GPT.md"&gt;Naruto GPT&lt;/a&gt; by selerite.com&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/%F0%9F%A6%BE%20ClubGPT%20-%20developer%20team%20in%20one%20GPT%20%F0%9F%A6%BE.md"&gt;🦾 ClubGPT - developer team in one GPT 🦾&lt;/a&gt; by clubgpt.vip&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Customer%20Service%20GPT.md"&gt;Customer Service GPT&lt;/a&gt; by Daniel J Patterson&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/%E4%B8%AD%E5%8C%BB%E4%B8%93%E5%AE%B6.md"&gt;中医专家&lt;/a&gt; by asktutor.online&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Storybook%20Vision.md"&gt;Storybook Vision&lt;/a&gt; by karenx.com&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Starter%20Pack%20Generator.md"&gt;Starter Pack Generator&lt;/a&gt; by photoai.com&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/DesignerGPT.md"&gt;DesignerGPT&lt;/a&gt; by Pietro Schirano&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Screenplay%20GPT.md"&gt;Screenplay GPT&lt;/a&gt; by kyrannio m margharos&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/ResearchGPT.md"&gt;ResearchGPT: AI Research Assistant&lt;/a&gt; by consensus.app&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Auto%20Stock%20Analyst%20Expert.md"&gt;Auto Stock Analyst Expert&lt;/a&gt; by LEI WEI&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Photogasm%202.0.md"&gt;Photogasm 2.0&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/ask_ida-c++.md"&gt;ask_ida/c++&lt;/a&gt; by Elias Bachaalany&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/%E6%83%85%E6%84%9F%E5%AF%B9%E8%AF%9D%E5%A4%A7%E5%B8%88%E2%80%94%E2%80%94%E5%B8%AE%E4%BD%A0%E5%9B%9E%E5%A4%8D%E5%A5%B3%E7%94%9F.md"&gt;情感对话大师——帮你回复女生&lt;/a&gt; by Audria Rice&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/AboutMe.md"&gt;AboutMe&lt;/a&gt; by Pietro Schirano&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Dream%20Labyrinth(%E6%A2%A6%E5%A2%83%E8%B7%91%E5%9B%A2).md"&gt;Dream Labyrinth(梦境跑团)&lt;/a&gt; by 缨缨&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Bake%20Off.md"&gt;Bake Off&lt;/a&gt; by mindgoblinstudios.com&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Professor%20Synapse.md"&gt;Professor Synapse&lt;/a&gt; by Joseph Rosenbaum&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/GPTsdex.md"&gt;GPTsdex&lt;/a&gt; by gptsdex.com&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/HongKongGPT.md"&gt;HongKongGPT&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Toronto%20City%20Council%20Guide.md"&gt;Toronto City Council Guide&lt;/a&gt; by George Bell&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/YouTubers%20Creative%20ToolBox.md"&gt;YouTubers Creative ToolBox&lt;/a&gt; by WeSeGo&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Evolution%20Chamber.md"&gt;Evolution Chamber&lt;/a&gt; by mindgoblinstudios.com&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Media%20Magic%20Mike.md"&gt;Media Magic Mike&lt;/a&gt; by jbgpts.com&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/GPT%20Idea%20Genie.md"&gt;GPT Idea Genie&lt;/a&gt; by silentdis.co&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Video%20Scripter.md"&gt;Video Scripter&lt;/a&gt; by Ruben Messerschmidt&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/SWOT%20Analysis.md"&gt;SWOT Analysis&lt;/a&gt; by B M O SOLANO&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/ScriptCraft.md"&gt;ScriptCraft&lt;/a&gt; by charly-ai.com&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Radical%20Selfishness.md"&gt;Radical Selfishness&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/PPT%20Expert.md"&gt;PPT Expert&lt;/a&gt; by Shutong Wang&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/%E7%95%99%E5%AD%A6%E6%96%87%E4%B9%A6%E5%A4%A7%E5%B8%88%20Essay%20Architect.md"&gt;留学文书大师 Essay Architect&lt;/a&gt; by CHAISIGIN&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Briefly.md"&gt;Briefly&lt;/a&gt; by gptriddle.com&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/PhoneixInk.md"&gt;PhoneixInk&lt;/a&gt; by MEHMET GENCAY IŞIK&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Samurai%20AI%20Summary.md"&gt;Samurai ⛩ AI summary&lt;/a&gt; by gosamurau.ai&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Packaging%20Expert.md"&gt;Packaging Expert&lt;/a&gt; by charly-ai.com&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Humanizer%20Pro.md"&gt;Humanizer Pro&lt;/a&gt; by charly-ai.com&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Ebook%20Writer%20%26%20Designer%20GPT.md"&gt;Ebook Writer &amp;amp; Designer GPT&lt;/a&gt; by Dasher&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Ghostwriters.md"&gt;Ghostwriters&lt;/a&gt; by osita okaforezeadi&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Riddle%20Master%20%28%E7%87%88%E8%AC%8E%E5%A4%A9%E5%B0%8A%29"&gt;Riddle Master (燈謎天尊)&lt;/a&gt; by WEBBER CHANG&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/%E4%BA%92%E8%81%94%E7%BD%91+%E6%8C%91%E6%88%98%E6%9D%AF%E5%A4%A7%E5%88%9B%E7%AB%9E%E8%B5%9B%E5%AF%BC%E5%B8%88.md"&gt;互联网+挑战杯大创竞赛导师&lt;/a&gt; by studyinglover&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Inkspire%20Tattoo%20Bot.md"&gt;Inkspire Tattoo Bot&lt;/a&gt; by idcesares&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/tsDoc%20Generator.md"&gt;tsDoc Generator&lt;/a&gt; by Timur Catakli&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Pepegen.md"&gt;Pepegen&lt;/a&gt; by nader dabit&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Small%20answer.md"&gt;Small answer&lt;/a&gt; by Jason McGhee&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/AutoGen%20Engineer.md"&gt;AutoGen Engineer&lt;/a&gt; by adamludolph.com&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Geopolitics%20GPT.md"&gt;Geopolitics GPT&lt;/a&gt; by siamintelligenceunit.com&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/GPT%20Shield.md"&gt;GPT Shield&lt;/a&gt; by wonderwhy-er.com&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/MidJourney%20Prompt%20Generator.md"&gt;MidJourney Prompt Generator&lt;/a&gt; by Digiagent&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Codey.md"&gt;Codey&lt;/a&gt; by heaven.tools&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Executive%20f(x)n.md"&gt;Executive f(x)n&lt;/a&gt; by mindgoblinstudios.com&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Strap%20UI.md"&gt;Strap UI&lt;/a&gt; by nertai.co&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/World%20Class%20Software%20Engineer.md"&gt;World Class Software Engineer&lt;/a&gt; by nertai.co&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Unbreakable%20GPT.md"&gt;Unbreakable GPT&lt;/a&gt; by aevummachinae.com&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Animal%20Chefs.md"&gt;Animal Chefs&lt;/a&gt; by simonwillison.net&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/The%20Greatest%20Computer%20Science%20Tutor.md"&gt;The Greatest Computer Science Tutor&lt;/a&gt; by nertai.co&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Trending%20TikTok%20Hashtags%20Finder%20Tool.md"&gt;Trending TikTok Hashtags Finder Tool&lt;/a&gt; by seo.ai&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Mr.%20Ranedeer%20Config%20Wizard.md"&gt;Mr. Ranedeer Config Wizard&lt;/a&gt; by mr-ranedeer.com&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Academic%20Assistant%20Pro.md"&gt;👌Academic Assistant Pro&lt;/a&gt; by Kevin Ivery&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Front-end%20expert.md"&gt;Front-end expert&lt;/a&gt; by Zhang Xiaoyuan&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/chatssh.md"&gt;ChatSSH: talk to your linux computers&lt;/a&gt; by chatssh.net&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Chat%20NeurIPS.md"&gt;Chat NeurIPS&lt;/a&gt; by myaidrive.com&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/WebGPT.md"&gt;WebGPT🤖&lt;/a&gt; by wegpt.ai&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/AllTrails.md"&gt;AllTrails&lt;/a&gt;　By alltrails.com&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/ChatDB.md"&gt;ChatDB: talk to your databases&lt;/a&gt;　By chatssh.net&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/%E4%B8%AD%E6%96%87%E4%BD%9C%E6%96%87%E6%89%B9%E6%94%B9%E5%8A%A9%E6%89%8B.md"&gt;中文作文批改助手&lt;/a&gt; by Arden Moses&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/DeepGame.md"&gt;DeepGame&lt;/a&gt;　By Utile Labs&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/PineappleBuilder.md"&gt;Pineapple Builder&lt;/a&gt;　By Peter Kowalczyk&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Astrology%20Fortune%20Teller.md"&gt;Astrology Fortune Teller&lt;/a&gt; by quaere.ai&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/%E8%80%83%E7%A0%94%E4%BF%9D%E7%A0%94%E7%AE%80%E5%8E%86%E6%8F%90%E9%97%AE%E5%8A%A9%E6%89%8B.md"&gt;考研保研简历提问助手&lt;/a&gt; by LeoCui&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/MJ%20Prompt%20Generator%20(V6).md"&gt;MJ Prompt Generator (V6)&lt;/a&gt; by featuredgpts.com&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/%E6%B3%95%E5%BE%8B%E4%B8%93%E5%AE%B6.md"&gt;法律专家&lt;/a&gt; by Weiqiao Zhu&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Book%20Creator%20Guide.md"&gt;Book Creator Guide&lt;/a&gt; by Choi Yong&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Gauntlet%20Movies.md"&gt;Gauntlet: Movies&lt;/a&gt; by Josh Brent N. Villocido&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Whimsical%20Diagrams.md"&gt;Whimsical Diagrams&lt;/a&gt; by whimsical.com&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Books.md"&gt;Books&lt;/a&gt; by Josh Brent N. Villocido&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/%E9%B9%A6%E9%B9%89GPT%20%F0%9F%A6%9C.md"&gt;鹦鹉GPT 🦜&lt;/a&gt; by gantrol.com&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/%E9%B2%81%E8%BF%85%E8%AF%B4.md"&gt;鲁迅说&lt;/a&gt; by QCHEN&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Midjouney%20Prompt%20Tools.md"&gt;Midjouney Prompt Tools&lt;/a&gt; by chatby.xyz&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Mid%20Journey%20V6%20Prompt%20Creator.md"&gt;Mid Journey V6 Prompt Creator&lt;/a&gt; by jandthebots.com&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE.md"&gt;思维导图&lt;/a&gt; by Tang Youpeng&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Diagrams%20Show%20Me.md"&gt;Diagrams: Show Me&lt;/a&gt; by helpful.dev&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Write%20For%20Me.md"&gt;Write For Me&lt;/a&gt; by puzzle.today&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Logo%20Creator.md"&gt;Logo Creator&lt;/a&gt; by Chase Lean&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/%E8%91%A3%E5%AE%87%E8%BE%89%E5%B0%8F%E4%BD%9C%E6%96%87%E5%8A%A9%E6%89%8B.md"&gt;董宇辉小作文助手&lt;/a&gt; by wesureai.com&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Mega-Prompt.md"&gt;Mega-Prompt&lt;/a&gt; by Aisen Lopez&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Prompt%20For%20Me.md"&gt;Prompt For Me&lt;/a&gt; by hansgpt.me&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Practice%20English%20by%20Debating.md"&gt;Practice English by Debating&lt;/a&gt; by Utku Sen&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Suno%E9%9F%B3%E4%B9%90%E6%AD%8C%E8%AF%8D%E5%88%9B%E4%BD%9C%E4%B8%93%E5%AE%B6v3.2.md"&gt;Suno音乐歌词创作专家v3.2&lt;/a&gt; by ai momo&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Midjourney%E8%B6%85%E7%BA%A7%E7%94%9F%E6%88%90%E5%99%A8%EF%BC%88V5.2%20%26%20V6%EF%BC%89.md"&gt;Midjourney超级生成器（V5.2 &amp;amp; V6）&lt;/a&gt; by gpts.gapier.net&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/GODMODE%202.0.md"&gt;GODMODE 2.0&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/SQL%20Expert.md"&gt;SQL Expert&lt;/a&gt; by Dmitry Khanukov&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Website%20Generator.md"&gt;Website Generator&lt;/a&gt; by websitegenerator.b12.io&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Framework%20Finder.md"&gt;Framework Finder&lt;/a&gt; by Ethan R Mollick&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Math%20Solver.md"&gt;Math Solver&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/MindMateGPT.md"&gt;MindMateGPT&lt;/a&gt; by Anthony Carlos Teixeira&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/BookMate%20-%20Book%20Recommendations.md"&gt;BookMate - Book Recommendations&lt;/a&gt; by community builder&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Android%20Studio%20GPT.md"&gt;Android Studio GPT&lt;/a&gt; by Rhonal Alonso Delgado Padilla&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/CBT%20GPT.md"&gt;CBT GPT&lt;/a&gt; by Brett Evanson&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/MCQ%20Creation%20Assistant.md"&gt;MCQ Creation Assistant&lt;/a&gt; by Rowboaterstop AB&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/SocraticTherapy.md"&gt;Socratic Therapy&lt;/a&gt; by Kasra Bayani&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Code%20Tutor.md"&gt;Code Tutor&lt;/a&gt; by khanacademy.org&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/FLUX%20Prompt%20Wizard.md"&gt;FLUX Prompt Wizard 🪄&lt;/a&gt; by sjgpts.com&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/LeetCoder%20Ace.md"&gt;LeetCoder Ace&lt;/a&gt; by YANG YANG&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/%E5%92%AA%E8%92%99%E6%A0%87%E9%A2%98.md"&gt;咪蒙标题&lt;/a&gt; by Haibo Liu&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/linexjlin/GPTs/main/prompts/Summarizer%20%E2%80%A2YouTube%20PDF%20Book%20Article%20Web%20Text%20Code.md"&gt;Summarizer ㆍYouTube PDF Book Article Web Text Code&lt;/a&gt; by Frames&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Open Source GPTs&lt;/h2&gt; 
&lt;p&gt;GitHub repos with prompts, actions and knowledge files of existing GPTs.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/stefangrotz/OpenDataGermanyGPT"&gt;Open Data Germany&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/parolteknologio/EsperantoGPT"&gt;EsperantoGPT&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;GPT Prompt Analysis&lt;/h2&gt; 
&lt;p&gt;Analysis and summaries of the GPT prompts.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://grantjenks.github.io/gpt-prompt-notes/"&gt;GPT Prompt Notes&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributors&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://contrib.rocks/image?repo=linexjlin/GPTs" alt="Contributors" /&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>reflex-dev/reflex</title>
      <link>https://github.com/reflex-dev/reflex</link>
      <description>&lt;p&gt;🕸️ Web apps in pure Python 🐍&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/reflex-dev/reflex/main/docs/images/reflex.svg?sanitize=true" alt="Reflex Logo" width="300px" /&gt; 
 &lt;hr /&gt; 
 &lt;h3&gt;&lt;strong&gt;✨ Performant, customizable web apps in pure Python. Deploy in seconds. ✨&lt;/strong&gt;&lt;/h3&gt; 
 &lt;p&gt;&lt;a href="https://badge.fury.io/py/reflex"&gt;&lt;img src="https://badge.fury.io/py/reflex.svg?sanitize=true" alt="PyPI version" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/pypi/pyversions/reflex.svg?sanitize=true" alt="versions" /&gt; &lt;a href="https://reflex.dev/docs/getting-started/introduction"&gt;&lt;img src="https://img.shields.io/badge/Documentation%20-Introduction%20-%20%23007ec6" alt="Documentation" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/projects/reflex"&gt;&lt;img src="https://static.pepy.tech/badge/reflex" alt="PyPI Downloads" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/T5WSbC2YtQ"&gt;&lt;img src="https://img.shields.io/discord/1029853095527727165?color=%237289da&amp;amp;label=Discord" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://x.com/getreflex"&gt;&lt;img src="https://img.shields.io/twitter/follow/getreflex" alt="Twitter" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;a href="https://github.com/reflex-dev/reflex/raw/main/README.md"&gt;English&lt;/a&gt; | &lt;a href="https://github.com/reflex-dev/reflex/raw/main/docs/zh/zh_cn/README.md"&gt;简体中文&lt;/a&gt; | &lt;a href="https://github.com/reflex-dev/reflex/raw/main/docs/zh/zh_tw/README.md"&gt;繁體中文&lt;/a&gt; | &lt;a href="https://github.com/reflex-dev/reflex/raw/main/docs/tr/README.md"&gt;Türkçe&lt;/a&gt; | &lt;a href="https://github.com/reflex-dev/reflex/raw/main/docs/in/README.md"&gt;हिंदी&lt;/a&gt; | &lt;a href="https://github.com/reflex-dev/reflex/raw/main/docs/pt/pt_br/README.md"&gt;Português (Brasil)&lt;/a&gt; | &lt;a href="https://github.com/reflex-dev/reflex/raw/main/docs/it/README.md"&gt;Italiano&lt;/a&gt; | &lt;a href="https://github.com/reflex-dev/reflex/raw/main/docs/es/README.md"&gt;Español&lt;/a&gt; | &lt;a href="https://github.com/reflex-dev/reflex/raw/main/docs/kr/README.md"&gt;한국어&lt;/a&gt; | &lt;a href="https://github.com/reflex-dev/reflex/raw/main/docs/ja/README.md"&gt;日本語&lt;/a&gt; | &lt;a href="https://github.com/reflex-dev/reflex/raw/main/docs/de/README.md"&gt;Deutsch&lt;/a&gt; | &lt;a href="https://github.com/reflex-dev/reflex/raw/main/docs/pe/README.md"&gt;Persian (پارسی)&lt;/a&gt; | &lt;a href="https://github.com/reflex-dev/reflex/raw/main/docs/vi/README.md"&gt;Tiếng Việt&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] 🚀 &lt;strong&gt;Try &lt;a href="https://build.reflex.dev/"&gt;Reflex Build&lt;/a&gt;&lt;/strong&gt; – our AI-powered app builder that generates full-stack Reflex applications in seconds.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr /&gt; 
&lt;h1&gt;Introduction&lt;/h1&gt; 
&lt;p&gt;Reflex is a library to build full-stack web apps in pure Python.&lt;/p&gt; 
&lt;p&gt;Key features:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Pure Python&lt;/strong&gt; - Write your app's frontend and backend all in Python, no need to learn Javascript.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Full Flexibility&lt;/strong&gt; - Reflex is easy to get started with, but can also scale to complex apps.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Deploy Instantly&lt;/strong&gt; - After building, deploy your app with a &lt;a href="https://reflex.dev/docs/hosting/deploy-quick-start/"&gt;single command&lt;/a&gt; or host it on your own server.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See our &lt;a href="https://reflex.dev/blog/2024-03-21-reflex-architecture/#the-reflex-architecture"&gt;architecture page&lt;/a&gt; to learn how Reflex works under the hood.&lt;/p&gt; 
&lt;h2&gt;⚙️ Installation&lt;/h2&gt; 
&lt;p&gt;Open a terminal and run (Requires Python 3.10+):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install reflex
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;🥳 Create your first app&lt;/h2&gt; 
&lt;p&gt;Installing &lt;code&gt;reflex&lt;/code&gt; also installs the &lt;code&gt;reflex&lt;/code&gt; command line tool.&lt;/p&gt; 
&lt;p&gt;Test that the install was successful by creating a new project. (Replace &lt;code&gt;my_app_name&lt;/code&gt; with your project name):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;mkdir my_app_name
cd my_app_name
reflex init
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This command initializes a template app in your new directory.&lt;/p&gt; 
&lt;p&gt;You can run this app in development mode:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;reflex run
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You should see your app running at &lt;a href="http://localhost:3000"&gt;http://localhost:3000&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Now you can modify the source code in &lt;code&gt;my_app_name/my_app_name.py&lt;/code&gt;. Reflex has fast refreshes so you can see your changes instantly when you save your code.&lt;/p&gt; 
&lt;h2&gt;🫧 Example App&lt;/h2&gt; 
&lt;p&gt;Let's go over an example: creating an image generation UI around &lt;a href="https://platform.openai.com/docs/guides/images/image-generation?context=node"&gt;DALL·E&lt;/a&gt;. For simplicity, we just call the &lt;a href="https://platform.openai.com/docs/api-reference/authentication"&gt;OpenAI API&lt;/a&gt;, but you could replace this with an ML model run locally.&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/reflex-dev/reflex/main/docs/images/dalle.gif" alt="A frontend wrapper for DALL·E, shown in the process of generating an image." width="550" /&gt; 
&lt;/div&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;Here is the complete code to create this. This is all done in one Python file!&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import reflex as rx
import openai

openai_client = openai.OpenAI()


class State(rx.State):
    """The app state."""

    prompt = ""
    image_url = ""
    processing = False
    complete = False

    def get_image(self):
        """Get the image from the prompt."""
        if self.prompt == "":
            return rx.window_alert("Prompt Empty")

        self.processing, self.complete = True, False
        yield
        response = openai_client.images.generate(
            prompt=self.prompt, n=1, size="1024x1024"
        )
        self.image_url = response.data[0].url
        self.processing, self.complete = False, True


def index():
    return rx.center(
        rx.vstack(
            rx.heading("DALL-E", font_size="1.5em"),
            rx.input(
                placeholder="Enter a prompt..",
                on_blur=State.set_prompt,
                width="25em",
            ),
            rx.button(
                "Generate Image",
                on_click=State.get_image,
                width="25em",
                loading=State.processing
            ),
            rx.cond(
                State.complete,
                rx.image(src=State.image_url, width="20em"),
            ),
            align="center",
        ),
        width="100%",
        height="100vh",
    )

# Add state and page to the app.
app = rx.App()
app.add_page(index, title="Reflex:DALL-E")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Let's break this down.&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/reflex-dev/reflex/main/docs/images/dalle_colored_code_example.png" alt="Explaining the differences between backend and frontend parts of the DALL-E app." width="900" /&gt; 
&lt;/div&gt; 
&lt;h3&gt;&lt;strong&gt;Reflex UI&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;Let's start with the UI.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;def index():
    return rx.center(
        ...
    )
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This &lt;code&gt;index&lt;/code&gt; function defines the frontend of the app.&lt;/p&gt; 
&lt;p&gt;We use different components such as &lt;code&gt;center&lt;/code&gt;, &lt;code&gt;vstack&lt;/code&gt;, &lt;code&gt;input&lt;/code&gt;, and &lt;code&gt;button&lt;/code&gt; to build the frontend. Components can be nested within each other to create complex layouts. And you can use keyword args to style them with the full power of CSS.&lt;/p&gt; 
&lt;p&gt;Reflex comes with &lt;a href="https://reflex.dev/docs/library"&gt;60+ built-in components&lt;/a&gt; to help you get started. We are actively adding more components, and it's easy to &lt;a href="https://reflex.dev/docs/wrapping-react/overview/"&gt;create your own components&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;State&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;Reflex represents your UI as a function of your state.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;class State(rx.State):
    """The app state."""
    prompt = ""
    image_url = ""
    processing = False
    complete = False

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The state defines all the variables (called vars) in an app that can change and the functions that change them.&lt;/p&gt; 
&lt;p&gt;Here the state is comprised of a &lt;code&gt;prompt&lt;/code&gt; and &lt;code&gt;image_url&lt;/code&gt;. There are also the booleans &lt;code&gt;processing&lt;/code&gt; and &lt;code&gt;complete&lt;/code&gt; to indicate when to disable the button (during image generation) and when to show the resulting image.&lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;Event Handlers&lt;/strong&gt;&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;def get_image(self):
    """Get the image from the prompt."""
    if self.prompt == "":
        return rx.window_alert("Prompt Empty")

    self.processing, self.complete = True, False
    yield
    response = openai_client.images.generate(
        prompt=self.prompt, n=1, size="1024x1024"
    )
    self.image_url = response.data[0].url
    self.processing, self.complete = False, True
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Within the state, we define functions called event handlers that change the state vars. Event handlers are the way that we can modify the state in Reflex. They can be called in response to user actions, such as clicking a button or typing in a text box. These actions are called events.&lt;/p&gt; 
&lt;p&gt;Our DALL·E app has an event handler, &lt;code&gt;get_image&lt;/code&gt; which gets this image from the OpenAI API. Using &lt;code&gt;yield&lt;/code&gt; in the middle of an event handler will cause the UI to update. Otherwise the UI will update at the end of the event handler.&lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;Routing&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;Finally, we define our app.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;app = rx.App()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;We add a page from the root of the app to the index component. We also add a title that will show up in the page preview/browser tab.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;app.add_page(index, title="DALL-E")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can create a multi-page app by adding more pages.&lt;/p&gt; 
&lt;h2&gt;📑 Resources&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;📑 &lt;a href="https://reflex.dev/docs/getting-started/introduction"&gt;Docs&lt;/a&gt; &amp;nbsp; | &amp;nbsp; 🗞️ &lt;a href="https://reflex.dev/blog"&gt;Blog&lt;/a&gt; &amp;nbsp; | &amp;nbsp; 📱 &lt;a href="https://reflex.dev/docs/library"&gt;Component Library&lt;/a&gt; &amp;nbsp; | &amp;nbsp; 🖼️ &lt;a href="https://reflex.dev/templates/"&gt;Templates&lt;/a&gt; &amp;nbsp; | &amp;nbsp; 🛸 &lt;a href="https://reflex.dev/docs/hosting/deploy-quick-start"&gt;Deployment&lt;/a&gt; &amp;nbsp;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;✅ Status&lt;/h2&gt; 
&lt;p&gt;Reflex launched in December 2022 with the name Pynecone.&lt;/p&gt; 
&lt;p&gt;🚀 Introducing &lt;a href="https://build.reflex.dev/"&gt;Reflex Build&lt;/a&gt; — Our AI-Powered Builder Reflex Build uses AI to generate complete full-stack Python applications. It helps you quickly create, customize, and refine your Reflex apps — from frontend components to backend logic — so you can focus on your ideas instead of boilerplate code. Whether you’re prototyping or scaling, Reflex Build accelerates development by intelligently scaffolding and optimizing your app’s entire stack.&lt;/p&gt; 
&lt;p&gt;Alongside this, &lt;a href="https://cloud.reflex.dev"&gt;Reflex Cloud&lt;/a&gt; launched in 2025 to offer the best hosting experience for your Reflex apps. We’re continuously improving the platform with new features and capabilities.&lt;/p&gt; 
&lt;p&gt;Reflex has new releases and features coming every week! Make sure to &lt;span&gt;⭐&lt;/span&gt; star and &lt;span&gt;👀&lt;/span&gt; watch this repository to stay up to date.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions of any size! Below are some good ways to get started in the Reflex community.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Join Our Discord&lt;/strong&gt;: Our &lt;a href="https://discord.gg/T5WSbC2YtQ"&gt;Discord&lt;/a&gt; is the best place to get help on your Reflex project and to discuss how you can contribute.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;GitHub Discussions&lt;/strong&gt;: A great way to talk about features you want added or things that are confusing/need clarification.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;GitHub Issues&lt;/strong&gt;: &lt;a href="https://github.com/reflex-dev/reflex/issues"&gt;Issues&lt;/a&gt; are an excellent way to report bugs. Additionally, you can try and solve an existing issue and submit a PR.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We are actively looking for contributors, no matter your skill level or experience. To contribute check out &lt;a href="https://github.com/reflex-dev/reflex/raw/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;All Thanks To Our Contributors:&lt;/h2&gt; 
&lt;a href="https://github.com/reflex-dev/reflex/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=reflex-dev/reflex" /&gt; &lt;/a&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Reflex is open-source and licensed under the &lt;a href="https://raw.githubusercontent.com/reflex-dev/reflex/main/LICENSE"&gt;Apache License 2.0&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>datawhalechina/llm-cookbook</title>
      <link>https://github.com/datawhalechina/llm-cookbook</link>
      <description>&lt;p&gt;面向开发者的 LLM 入门教程，吴恩达大模型系列课程中文版&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/datawhalechina/llm-cookbook/main/figures/readme.jpg" alt="figures/readme.jpg" /&gt;&lt;/p&gt; 
&lt;h1&gt;面向开发者的大模型手册 - LLM Cookbook&lt;/h1&gt; 
&lt;h2&gt;项目简介&lt;/h2&gt; 
&lt;p&gt;本项目是一个面向开发者的大模型手册，针对国内开发者的实际需求，主打 LLM 全方位入门实践。本项目基于吴恩达老师大模型系列课程内容，对原课程内容进行筛选、翻译、复现和调优，覆盖从 Prompt Engineering 到 RAG 开发、模型微调的全部流程，用最适合国内学习者的方式，指导国内开发者如何学习、入门 LLM 相关项目。&lt;/p&gt; 
&lt;p&gt;针对不同内容的特点，我们对共计 11 门吴恩达老师的大模型课程进行了翻译复现，并结合国内学习者的实际情况，对不同课程进行了分级和排序，初学者可以先系统学习我们的必修类课程，掌握入门 LLM 所有方向都需要掌握的基础技能和概念，再选择性地学习我们的选修类课程，在自己感兴趣的方向上不断探索和学习。&lt;/p&gt; 
&lt;p&gt;如果有你非常喜欢但我们还没有进行复现的吴恩达老师大模型课程，我们欢迎每一位开发者参考我们已有课程的格式和写法来对课程进行复现并提交 PR，在 PR 审核通过后，我们会根据课程内容将课程进行分级合并。欢迎每一位开发者的贡献！&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;在线阅读地址：&lt;a href="https://datawhalechina.github.io/llm-cookbook/"&gt;面向开发者的 LLM 入门课程 - 在线阅读&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;PDF 下载地址：&lt;a href="https://github.com/datawhalechina/llm-cookbook/releases/tag/v1%2C0%2C0"&gt;面向开发者的 LLM 入门教程 - PDF&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;英文原版地址：&lt;a href="https://learn.deeplearning.ai"&gt;吴恩达关于大模型的系列课程&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;项目意义&lt;/h2&gt; 
&lt;p&gt;LLM 正在逐步改变人们的生活，而对于开发者，如何基于 LLM 提供的 API 快速、便捷地开发一些具备更强能力、集成 LLM 的应用，来便捷地实现一些更新颖、更实用的能力，是一个急需学习的重要能力。&lt;/p&gt; 
&lt;p&gt;由吴恩达老师与 OpenAI 合作推出的大模型系列教程，从大模型时代开发者的基础技能出发，深入浅出地介绍了如何基于大模型 API、LangChain 架构快速开发结合大模型强大能力的应用。其中，《Prompt Engineering for Developers》教程面向入门 LLM 的开发者，深入浅出地介绍了对于开发者，如何构造 Prompt 并基于 OpenAI 提供的 API 实现包括总结、推断、转换等多种常用功能，是入门 LLM 开发的经典教程；《Building Systems with the ChatGPT API》教程面向想要基于 LLM 开发应用程序的开发者，简洁有效而又系统全面地介绍了如何基于 ChatGPT API 打造完整的对话系统；《LangChain for LLM Application Development》教程结合经典大模型开源框架 LangChain，介绍了如何基于 LangChain 框架开发具备实用功能、能力全面的应用程序，《LangChain Chat With Your Data》教程则在此基础上进一步介绍了如何使用 LangChain 架构结合个人私有数据开发个性化大模型应用；《Building Generative AI Applications with Gradio》、《Evaluating and Debugging Generative AI》教程分别介绍了两个实用工具 Gradio 与 W&amp;amp;B，指导开发者如何结合这两个工具来打造、评估生成式 AI 应用。&lt;/p&gt; 
&lt;p&gt;上述教程非常适用于开发者学习以开启基于 LLM 实际搭建应用程序之路。因此，我们将该系列课程翻译为中文，并复现其范例代码，也为其中一个视频增加了中文字幕，支持国内中文学习者直接使用，以帮助中文学习者更好地学习 LLM 开发；我们也同时实现了效果大致相当的中文 Prompt，支持学习者感受中文语境下 LLM 的学习使用，对比掌握多语言语境下的 Prompt 设计与 LLM 开发。未来，我们也将加入更多 Prompt 高级技巧，以丰富本课程内容，帮助开发者掌握更多、更巧妙的 Prompt 技能。&lt;/p&gt; 
&lt;h2&gt;项目受众&lt;/h2&gt; 
&lt;p&gt;所有具备基础 Python 能力，想要入门 LLM 的开发者。&lt;/p&gt; 
&lt;h2&gt;项目亮点&lt;/h2&gt; 
&lt;p&gt;《ChatGPT Prompt Engineering for Developers》、《Building Systems with the ChatGPT API》等教程作为由吴恩达老师与 OpenAI 联合推出的官方教程，在可预见的未来会成为 LLM 的重要入门教程，但是目前还只支持英文版且国内访问受限，打造中文版且国内流畅访问的教程具有重要意义；同时，GPT 对中文、英文具有不同的理解能力，本教程在多次对比、实验之后确定了效果大致相当的中文 Prompt，支持学习者研究如何提升 ChatGPT 在中文语境下的理解与生成能力。&lt;/p&gt; 
&lt;h2&gt;学习指南&lt;/h2&gt; 
&lt;p&gt;本教程适用于所有具备基础 Python 能力，想要入门 LLM 的开发者。&lt;/p&gt; 
&lt;p&gt;如果你想要开始学习本教程，你需要提前具备：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;至少一个 LLM API（最好是 OpenAI，如果是其他 API，你可能需要参考 &lt;a href="https://github.com/datawhalechina/llm-universe"&gt;其他教程&lt;/a&gt; 对 API 调用代码进行修改）&lt;/li&gt; 
 &lt;li&gt;能够使用 Python Jupyter Notebook&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;本教程共包括 11 门课程，分为必修类、选修类两个类别。必修类课程是我们认为最适合初学者学习以入门 LLM 的课程，包括了入门 LLM 所有方向都需要掌握的基础技能和概念，我们也针对必修类课程制作了适合阅读的在线阅读和 PDF 版本，在学习必修类课程时，我们建议学习者按照我们列出的顺序进行学习；选修类课程是在必修类课程上的拓展延伸，包括了 RAG 开发、模型微调、模型评估等多个方面，适合学习者在掌握了必修类课程之后选择自己感兴趣的方向和课程进行学习。&lt;/p&gt; 
&lt;p&gt;必修类课程包括：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;面向开发者的 Prompt Engineering。基于吴恩达老师《ChatGPT Prompt Engineering for Developers》课程打造，面向入门 LLM 的开发者，深入浅出地介绍了对于开发者，如何构造 Prompt 并基于 OpenAI 提供的 API 实现包括总结、推断、转换等多种常用功能，是入门 LLM 开发的第一步。&lt;/li&gt; 
 &lt;li&gt;搭建基于 ChatGPT 的问答系统。基于吴恩达老师《Building Systems with the ChatGPT API》课程打造，指导开发者如何基于 ChatGPT 提供的 API 开发一个完整的、全面的智能问答系统。通过代码实践，实现了基于 ChatGPT 开发问答系统的全流程，介绍了基于大模型开发的新范式，是大模型开发的实践基础。&lt;/li&gt; 
 &lt;li&gt;使用 LangChain 开发应用程序。基于吴恩达老师《LangChain for LLM Application Development》课程打造，对 LangChain 展开深入介绍，帮助学习者了解如何使用 LangChain，并基于 LangChain 开发完整的、具备强大能力的应用程序。&lt;/li&gt; 
 &lt;li&gt;使用 LangChain 访问个人数据。基于吴恩达老师《LangChain Chat with Your Data》课程打造，深入拓展 LangChain 提供的个人数据访问能力，指导开发者如何使用 LangChain 开发能够访问用户个人数据、提供个性化服务的大模型应用。&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;选修类课程包括：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;使用 Gradio 搭建生成式 AI 应用。基于吴恩达老师《Building Generative AI Applications with Gradio》课程打造，指导开发者如何使用 Gradio 通过 Python 接口程序快速、高效地为生成式 AI 构建用户界面。&lt;/li&gt; 
 &lt;li&gt;评估改进生成式 AI。基于吴恩达老师《Evaluating and Debugging Generative AI》课程打造，结合 wandb，提供一套系统化的方法和工具，帮助开发者有效地跟踪和调试生成式 AI 模型。&lt;/li&gt; 
 &lt;li&gt;微调大语言模型。基于吴恩达老师《Finetuning Large Language Model》课程打造，结合 lamini 框架，讲述如何便捷高效地在本地基于个人数据微调开源大语言模型。&lt;/li&gt; 
 &lt;li&gt;大模型与语义检索。基于吴恩达老师《Large Language Models with Semantic Search》课程打造，针对检索增强生成，讲述了多种高级检索技巧以实现更准确、高效的检索增强 LLM 生成效果。&lt;/li&gt; 
 &lt;li&gt;基于 Chroma 的高级检索。基于吴恩达老师《Advanced Retrieval for AI with Chroma》课程打造，旨在介绍基于 Chroma 的高级检索技术，提升检索结果的准确性。&lt;/li&gt; 
 &lt;li&gt;搭建和评估高级 RAG 应用。基于吴恩达老师《Building and Evaluating Advanced RAG Applications》课程打造，介绍构建和实现高质量 RAG 系统所需的关键技术和评估框架。&lt;/li&gt; 
 &lt;li&gt;LangChain 的 Functions、Tools 和 Agents。基于吴恩达老师《Functions, Tools and Agents with LangChain》课程打造，介绍如何基于 LangChain 的新语法构建 Agent。&lt;/li&gt; 
 &lt;li&gt;Prompt 高级技巧。包括 CoT、自我一致性等多种 Prompt 高级技巧的基础理论与代码实现。&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;其他资料包括：&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;双语字幕视频地址：&lt;a href="https://www.bilibili.com/video/BV1Bo4y1A7FU/?share_source=copy_web"&gt;吴恩达 x OpenAI 的 Prompt Engineering 课程专业翻译版&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;中英双语字幕下载：&lt;a href="https://github.com/GitHubDaily/ChatGPT-Prompt-Engineering-for-Developers-in-Chinese"&gt;《ChatGPT 提示工程》非官方版中英双语字幕&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;视频讲解：&lt;a href="https://www.bilibili.com/video/BV1PN4y1k7y2/?spm_id_from=333.999.0.0"&gt;面向开发者的 Prompt Engineering 讲解（数字游民大会）&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;目录结构说明：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;content：基于原课程复现的双语版代码，可运行的 Notebook，更新频率最高，更新速度最快。

docs：必修类课程文字教程版在线阅读源码，适合阅读的 Markdown。

figures：图片文件。
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;致谢&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;核心贡献者&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/logan-zou"&gt;邹雨衡-项目负责人&lt;/a&gt;（Datawhale成员-对外经济贸易大学研究生）&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/LinChentang"&gt;左春生-项目负责人&lt;/a&gt;（内容创作者-Datawhale成员）&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://yam.gift/"&gt;长琴-项目发起人&lt;/a&gt;（内容创作者-Datawhale成员-AI算法工程师）&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Sophia-Huang"&gt;玉琳-项目发起人&lt;/a&gt;（内容创作者-Datawhale成员）&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/xuhu0115"&gt;徐虎-教程编撰者&lt;/a&gt;（内容创作者-Datawhale成员）&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Weihong-Liu"&gt;刘伟鸿-教程编撰者&lt;/a&gt;（内容创作者-江南大学非全研究生）&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://Joyenjoye.com"&gt;Joye-教程编撰者&lt;/a&gt;（内容创作者-数据科学家）&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/0-yy-0"&gt;高立业&lt;/a&gt;（内容创作者-DataWhale成员-算法工程师）&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/GKDGKD"&gt;邓宇文&lt;/a&gt;（内容创作者-Datawhale成员）&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/wisdom-pan"&gt;魂兮&lt;/a&gt;（内容创作者-前端工程师）&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/KMnO4-zx"&gt;宋志学&lt;/a&gt;（内容创作者-Datawhale成员）&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/YikunHan42"&gt;韩颐堃&lt;/a&gt;（内容创作者-Datawhale成员）&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/6forwater29"&gt;陈逸涵&lt;/a&gt; (内容创作者-Datawhale意向成员-AI爱好者)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ztgg0228"&gt;仲泰&lt;/a&gt;（内容创作者-Datawhale成员）&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/leason-wan"&gt;万礼行&lt;/a&gt;（内容创作者-视频翻译者）&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Bald0Wang"&gt;王熠明&lt;/a&gt;（内容创作者-Datawhale成员）&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://yetingyun.blog.csdn.net"&gt;曾浩龙&lt;/a&gt;（内容创作者-Datawhale 意向成员-JLU AI 研究生）&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/xinqi-fan"&gt;小饭同学&lt;/a&gt;（内容创作者）&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/sunhanyu714"&gt;孙韩玉&lt;/a&gt;（内容创作者-算法量化部署工程师）&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/YinHan-Zhang"&gt;张银晗&lt;/a&gt;（内容创作者-Datawhale成员）&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Jin-Zhang-Yaoguang"&gt;张晋&lt;/a&gt;（内容创作者-Datawhale成员）&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Aphasia0515"&gt;李娇娇&lt;/a&gt;（内容创作者-Datawhale成员）&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Kedreamix"&gt;邓恺俊&lt;/a&gt;（内容创作者-Datawhale成员）&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Zhiyuan-Fan"&gt;范致远&lt;/a&gt;（内容创作者-Datawhale成员）&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Beyondzjl"&gt;周景林&lt;/a&gt;（内容创作者-Datawhale成员）&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/very-very-very"&gt;诸世纪&lt;/a&gt;（内容创作者-算法工程师）&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/YixinZ-NUS"&gt;Zhang Yixin&lt;/a&gt;（内容创作者-IT爱好者）&lt;/li&gt; 
 &lt;li&gt;Sarai（内容创作者-AI应用爱好者）&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;其他&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;特别感谢 &lt;a href="https://github.com/Sm1les"&gt;@Sm1les&lt;/a&gt;、&lt;a href="https://github.com/LSGOMYP"&gt;@LSGOMYP&lt;/a&gt; 对本项目的帮助与支持；&lt;/li&gt; 
 &lt;li&gt;感谢 &lt;a href="https://github.com/GitHubDaily"&gt;GithubDaily&lt;/a&gt; 提供的双语字幕；&lt;/li&gt; 
 &lt;li&gt;如果有任何想法可以联系我们 Datawhale 也欢迎大家多多提出 Issues；&lt;/li&gt; 
 &lt;li&gt;特别感谢以下为教程做出贡献的同学！&lt;/li&gt; 
&lt;/ol&gt; 
&lt;a href="https://datawhalechina.github.io/llm-cookbook/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=datawhalechina/llm-cookbook" /&gt; &lt;/a&gt; 
&lt;p&gt;Made with &lt;a href="https://contrib.rocks"&gt;contrib.rocks&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#datawhalechina/llm-cookbook&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=datawhalechina/llm-cookbook&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;关注我们&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;扫描下方二维码关注公众号：Datawhale&lt;/p&gt; 
 &lt;img src="https://raw.githubusercontent.com/datawhalechina/llm-cookbook/main/figures/qrcode.jpeg" width="180" height="180" /&gt; 
&lt;/div&gt; Datawhale 是一个专注于数据科学与 AI 领域的开源组织，汇集了众多领域院校和知名企业的优秀学习者，聚合了一群有开源精神和探索精神的团队成员。微信搜索公众号 Datawhale 可以加入我们。 
&lt;h2&gt;LICENSE&lt;/h2&gt; 
&lt;p&gt;&lt;a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"&gt;&lt;img alt="知识共享许可协议" style="border-width:0" src="https://img.shields.io/badge/license-CC%20BY--NC--SA%204.0-lightgrey" /&gt;&lt;/a&gt;&lt;br /&gt;本作品采用&lt;a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"&gt;知识共享署名 - 非商业性使用 - 相同方式共享 4.0 国际许可协议&lt;/a&gt;进行许可。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>anthropics/prompt-eng-interactive-tutorial</title>
      <link>https://github.com/anthropics/prompt-eng-interactive-tutorial</link>
      <description>&lt;p&gt;Anthropic's Interactive Prompt Engineering Tutorial&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Welcome to Anthropic's Prompt Engineering Interactive Tutorial&lt;/h1&gt; 
&lt;h2&gt;Course introduction and goals&lt;/h2&gt; 
&lt;p&gt;This course is intended to provide you with a comprehensive step-by-step understanding of how to engineer optimal prompts within Claude.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;After completing this course, you will be able to&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Master the basic structure of a good prompt&lt;/li&gt; 
 &lt;li&gt;Recognize common failure modes and learn the '80/20' techniques to address them&lt;/li&gt; 
 &lt;li&gt;Understand Claude's strengths and weaknesses&lt;/li&gt; 
 &lt;li&gt;Build strong prompts from scratch for common use cases&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Course structure and content&lt;/h2&gt; 
&lt;p&gt;This course is structured to allow you many chances to practice writing and troubleshooting prompts yourself. The course is broken up into &lt;strong&gt;9 chapters with accompanying exercises&lt;/strong&gt;, as well as an appendix of even more advanced methods. It is intended for you to &lt;strong&gt;work through the course in chapter order&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Each lesson has an "Example Playground" area&lt;/strong&gt; at the bottom where you are free to experiment with the examples in the lesson and see for yourself how changing prompts can change Claude's responses. There is also an &lt;a href="https://docs.google.com/spreadsheets/d/1jIxjzUWG-6xBVIa2ay6yDpLyeuOh_hR_ZB75a47KX_E/edit?usp=sharing"&gt;answer key&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Note: This tutorial uses our smallest, fastest, and cheapest model, Claude 3 Haiku. Anthropic has &lt;a href="https://docs.anthropic.com/claude/docs/models-overview"&gt;two other models&lt;/a&gt;, Claude 3 Sonnet and Claude 3 Opus, which are more intelligent than Haiku, with Opus being the most intelligent.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;This tutorial also exists on &lt;a href="https://docs.google.com/spreadsheets/d/19jzLgRruG9kjUQNKtCg1ZjdD6l6weA6qRXG5zLIAhC8/edit?usp=sharing"&gt;Google Sheets using Anthropic's Claude for Sheets extension&lt;/a&gt;. We recommend using that version as it is more user friendly.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;When you are ready to begin, go to &lt;code&gt;01_Basic Prompt Structure&lt;/code&gt; to proceed.&lt;/p&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;p&gt;Each chapter consists of a lesson and a set of exercises.&lt;/p&gt; 
&lt;h3&gt;Beginner&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Chapter 1:&lt;/strong&gt; Basic Prompt Structure&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Chapter 2:&lt;/strong&gt; Being Clear and Direct&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Chapter 3:&lt;/strong&gt; Assigning Roles&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Intermediate&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Chapter 4:&lt;/strong&gt; Separating Data from Instructions&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Chapter 5:&lt;/strong&gt; Formatting Output &amp;amp; Speaking for Claude&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Chapter 6:&lt;/strong&gt; Precognition (Thinking Step by Step)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Chapter 7:&lt;/strong&gt; Using Examples&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Advanced&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Chapter 8:&lt;/strong&gt; Avoiding Hallucinations&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Chapter 9:&lt;/strong&gt; Building Complex Prompts (Industry Use Cases)&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Complex Prompts from Scratch - Chatbot&lt;/li&gt; 
   &lt;li&gt;Complex Prompts for Legal Services&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Exercise:&lt;/strong&gt; Complex Prompts for Financial Services&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Exercise:&lt;/strong&gt; Complex Prompts for Coding&lt;/li&gt; 
   &lt;li&gt;Congratulations &amp;amp; Next Steps&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Appendix:&lt;/strong&gt; Beyond Standard Prompting&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Chaining Prompts&lt;/li&gt; 
   &lt;li&gt;Tool Use&lt;/li&gt; 
   &lt;li&gt;Search &amp;amp; Retrieval&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>ntdevlabs/tiny11builder</title>
      <link>https://github.com/ntdevlabs/tiny11builder</link>
      <description>&lt;p&gt;Scripts to build a trimmed-down Windows 11 image.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;tiny11builder&lt;/h1&gt; 
&lt;p&gt;&lt;em&gt;Scripts to build a trimmed-down Windows 11 image - now in &lt;strong&gt;PowerShell&lt;/strong&gt;!&lt;/em&gt;&lt;/p&gt; 
&lt;h2&gt;Introduction :&lt;/h2&gt; 
&lt;p&gt;Tiny11 builder, now completely overhauled. &lt;br /&gt; After more than a year (for which I am so sorry) of no updates, tiny11 builder is now a much more complete and flexible solution - one script fits all. Also, it is a steppingstone for an even more fleshed-out solution.&lt;/p&gt; 
&lt;p&gt;You can now use it on ANY Windows 11 release (not just a specific build), as well as ANY language or architecture. This is made possible thanks to the much-improved scripting capabilities of PowerShell, compared to the older Batch release.&lt;/p&gt; 
&lt;p&gt;This is a script created to automate the build of a streamlined Windows 11 image, similar to tiny10. The script has also been updated to use DISM's recovery compression, resulting in a much smaller final ISO size, and no utilities from external sources. The only other executable included is &lt;strong&gt;oscdimg.exe&lt;/strong&gt;, which is provided in the Windows ADK and it is used to create bootable ISO images. Also included is an unattended answer file, which is used to bypass the Microsoft Account on OOBE and to deploy the image with the &lt;code&gt;/compact&lt;/code&gt; flag. It's open-source, &lt;strong&gt;so feel free to add or remove anything you want!&lt;/strong&gt; Feedback is also much appreciated.&lt;/p&gt; 
&lt;p&gt;Also, for the very first time, &lt;strong&gt;introducing tiny11 core builder&lt;/strong&gt;! A more powerful script, designed for a quick and dirty development testbed. Just the bare minimum, none of the fluff. This script generates a significantly reduced Windows 11 image. However, &lt;strong&gt;it's not suitable for regular use due to its lack of serviceability - you can't add languages, updates, or features post-creation&lt;/strong&gt;. tiny11 Core is not a full Windows 11 substitute but a rapid testing or development tool, potentially useful for VM environments.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;⚠️ Script versions:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;tiny11maker.ps1&lt;/strong&gt; : The regular script, which removes a lot of bloat but keeps the system serviceable. You can add languages, updates, and features post-creation. This is the recommended script for regular use.&lt;/li&gt; 
 &lt;li&gt;⚠️ &lt;strong&gt;tiny11coremaker.ps1&lt;/strong&gt; : The core script, which removes even more bloat but also removes the ability to service the image. You cannot add languages, updates, or features post-creation. This is recommended for quick testing or development use.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Instructions:&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Download Windows 11 from the &lt;a href="https://www.microsoft.com/software-download/windows11"&gt;Microsoft website&lt;/a&gt; or &lt;a href="https://github.com/pbatard/rufus"&gt;Rufus&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Mount the downloaded ISO image using Windows Explorer.&lt;/li&gt; 
 &lt;li&gt;Open &lt;strong&gt;PowerShell 5.1&lt;/strong&gt; as Administrator.&lt;/li&gt; 
 &lt;li&gt;Change the script execution policy :&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-powershell"&gt;Set-ExecutionPolicy Bypass -Scope Process
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Using &lt;code&gt;-Scope Process&lt;/code&gt; you keep your original policy intact as this change only lasts for the current PowerShell session.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ol start="6"&gt; 
 &lt;li&gt;Start the script :&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-powershell"&gt;C:/path/to/your/tiny11/script.ps1 -ISO &amp;lt;letter&amp;gt; -SCRATCH &amp;lt;letter&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;You can see of the script by running the &lt;code&gt;get-help&lt;/code&gt; command.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ol start="6"&gt; 
 &lt;li&gt;Select the drive letter where the image is mounted (only the letter, no colon (:))&lt;/li&gt; 
 &lt;li&gt;Select the SKU that you want the image to be based.&lt;/li&gt; 
 &lt;li&gt;Sit back and relax :)&lt;/li&gt; 
 &lt;li&gt;When the image is completed, you will see it in the folder where the script was extracted, with the name tiny11.iso&lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr /&gt; 
&lt;h2&gt;What is removed:&lt;/h2&gt; 
&lt;table&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Tiny11maker&lt;/th&gt; 
   &lt;th&gt;Tiny11coremaker&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Clipchamp&lt;/li&gt; 
     &lt;li&gt;News&lt;/li&gt; 
     &lt;li&gt;Weather&lt;/li&gt; 
     &lt;li&gt;Xbox&lt;/li&gt; 
     &lt;li&gt;GetHelp&lt;/li&gt; 
     &lt;li&gt;GetStarted&lt;/li&gt; 
     &lt;li&gt;Office Hub&lt;/li&gt; 
     &lt;li&gt;Solitaire&lt;/li&gt; 
     &lt;li&gt;PeopleApp&lt;/li&gt; 
     &lt;li&gt;PowerAutomate&lt;/li&gt; 
     &lt;li&gt;ToDo&lt;/li&gt; 
     &lt;li&gt;Alarms&lt;/li&gt; 
     &lt;li&gt;Mail and Calendar&lt;/li&gt; 
     &lt;li&gt;Feedback Hub&lt;/li&gt; 
     &lt;li&gt;Maps&lt;/li&gt; 
     &lt;li&gt;Sound Recorder&lt;/li&gt; 
     &lt;li&gt;Your Phone&lt;/li&gt; 
     &lt;li&gt;Media Player&lt;/li&gt; 
     &lt;li&gt;QuickAssist&lt;/li&gt; 
     &lt;li&gt;Internet Explorer&lt;/li&gt; 
     &lt;li&gt;Tablet PC Math&lt;/li&gt; 
     &lt;li&gt;Edge&lt;/li&gt; 
     &lt;li&gt;OneDrive&lt;/li&gt; 
    &lt;/ul&gt; &lt;/td&gt; 
   &lt;td&gt; 
    &lt;ul&gt; 
     &lt;li&gt;all from regular tiny +&lt;/li&gt; 
     &lt;li&gt;Windows Component Store (WinSxS)&lt;/li&gt; 
     &lt;li&gt;Windows Defender (only disabled, can be enabled back if needed)&lt;/li&gt; 
     &lt;li&gt;Windows Update (wouldn't work without WinSxS, enabling it would put the system in a state of failure)&lt;/li&gt; 
     &lt;li&gt;WinRE&lt;/li&gt; 
    &lt;/ul&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Keep in mind that &lt;strong&gt;you cannot add back features in tiny11 core&lt;/strong&gt;! &lt;br /&gt; You will be asked during image creation if you want to enable .net 3.5 support!&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Known issues:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Although Edge is removed, there are some remnants in the Settings, but the app in itself is deleted.&lt;/li&gt; 
 &lt;li&gt;You might have to update Winget before being able to install any apps, using Microsoft Store.&lt;/li&gt; 
 &lt;li&gt;Outlook and Dev Home might reappear after some time. This is an ongoing battle, though the latest script update tries to prevent this more aggressively.&lt;/li&gt; 
 &lt;li&gt;If you are using this script on arm64, you might see a glimpse of an error while running the script. This is caused by the fact that the arm64 image doesn't have OneDriveSetup.exe included in the System32 folder.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Features to be implemented:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;del&gt;disabling telemetry&lt;/del&gt; (Implemented in the 04-29-24 release!)&lt;/li&gt; 
 &lt;li&gt;&lt;del&gt;more ad suppression&lt;/del&gt; (Partially implemented in the 09-06-25 release!)&lt;/li&gt; 
 &lt;li&gt;improved language and arch detection&lt;/li&gt; 
 &lt;li&gt;more flexibility in what to keep and what to delete&lt;/li&gt; 
 &lt;li&gt;maybe a GUI???&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;And that's pretty much it for now!&lt;/p&gt; 
&lt;h2&gt;❤️ Support the Project&lt;/h2&gt; 
&lt;p&gt;If this project has helped you, please consider showing your support! A small donation helps me dedicate more time to projects like this. Thank you!&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="http://patreon.com/ntdev"&gt;Patreon&lt;/a&gt; | &lt;a href="http://paypal.me/ntdev2"&gt;PayPal&lt;/a&gt; | &lt;a href="http://ko-fi.com/ntdev"&gt;Ko-fi&lt;/a&gt;&lt;/strong&gt; Thanks for trying it and let me know how you like it!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>DataDog/datadog-agent</title>
      <link>https://github.com/DataDog/datadog-agent</link>
      <description>&lt;p&gt;Main repository for Datadog Agent&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Datadog Agent&lt;/h1&gt; 
&lt;p&gt;&lt;img src="https://img.shields.io/github/v/release/DataDog/datadog-agent?style=flat&amp;amp;logo=datadog&amp;amp;logoColor=%23632CA6&amp;amp;labelColor=%23FFF&amp;amp;color=%23632CA6" alt="GitHub Release" /&gt; &lt;a href="https://codecov.io/github/DataDog/datadog-agent?branch=main"&gt;&lt;img src="https://codecov.io/github/DataDog/datadog-agent/coverage.svg?branch=main" alt="Coverage status" /&gt;&lt;/a&gt; &lt;a href="https://godoc.org/github.com/DataDog/datadog-agent"&gt;&lt;img src="https://godoc.org/github.com/DataDog/datadog-agent?status.svg?sanitize=true" alt="GoDoc" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;This repository contains the source code of the Datadog Agent version 7 and version 6. Please refer to the &lt;a href="https://docs.datadoghq.com/agent/"&gt;Agent user documentation&lt;/a&gt; for information about differences between Agent v5, Agent v6 and Agent v7. Additionally, we provide a list of prepackaged binaries for an easy install process &lt;a href="https://app.datadoghq.com/fleet/install-agent/latest?platform=overview"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;The &lt;a href="https://datadoghq.dev/datadog-agent/setup/"&gt;developer docs site&lt;/a&gt; contains information about how to develop the Datadog Agent itself.&lt;/p&gt; 
&lt;p&gt;The source of the content is located under &lt;a href="https://raw.githubusercontent.com/DataDog/datadog-agent/main/docs"&gt;the docs directory&lt;/a&gt; and may contain pages that are not yet published.&lt;/p&gt; 
&lt;h2&gt;Contributing code&lt;/h2&gt; 
&lt;p&gt;You'll find information and help on how to contribute code to this project under &lt;a href="https://raw.githubusercontent.com/DataDog/datadog-agent/main/docs/dev"&gt;the &lt;code&gt;docs/dev&lt;/code&gt; directory&lt;/a&gt; of the present repo.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;The Datadog Agent user space components are licensed under the &lt;a href="https://raw.githubusercontent.com/DataDog/datadog-agent/main/LICENSE"&gt;Apache License, Version 2.0&lt;/a&gt;. The BPF code is licensed under the &lt;a href="https://raw.githubusercontent.com/DataDog/datadog-agent/main/pkg/ebpf/c/COPYING"&gt;General Public License, Version 2.0&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>jingyaogong/minimind</title>
      <link>https://github.com/jingyaogong/minimind</link>
      <description>&lt;p&gt;🚀🚀 「大模型」2小时完全从0训练26M的小参数GPT！🌏 Train a 26M-parameter GPT from scratch in just 2h!&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/logo.png" alt="logo" /&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;img src="https://visitor-badge.laobi.icu/badge?page_id=jingyaogong/minimind" alt="visitors" /&gt; &lt;a href="https://github.com/jingyaogong/minimind/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/jingyaogong/minimind?style=social" alt="GitHub Repo stars" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/jingyaogong/minimind/master/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/jingyaogong/minimind" alt="GitHub Code License" /&gt;&lt;/a&gt; &lt;a href="https://github.com/jingyaogong/minimind/commits/master"&gt;&lt;img src="https://img.shields.io/github/last-commit/jingyaogong/minimind" alt="GitHub last commit" /&gt;&lt;/a&gt; &lt;a href="https://github.com/jingyaogong/minimind/pulls"&gt;&lt;img src="https://img.shields.io/badge/PRs-welcome-blue" alt="GitHub pull request" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/collections/jingyaogong/minimind-66caf8d999f5c7fa64f399e5"&gt;&lt;img src="https://img.shields.io/badge/%F0%9F%A4%97-MiniMind%20%20Collection-blue" alt="Collection" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;h3&gt;"大道至简"&lt;/h3&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;中文 | &lt;a href="https://raw.githubusercontent.com/jingyaogong/minimind/master/README_en.md"&gt;English&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;ul&gt; 
 &lt;li&gt;此开源项目旨在完全从0开始，仅用3块钱成本 + 2小时！即可训练出仅为25.8M的超小语言模型&lt;strong&gt;MiniMind&lt;/strong&gt;。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MiniMind&lt;/strong&gt;系列极其轻量，最小版本体积是 GPT-3 的 $\frac{1}{7000}$，力求做到最普通的个人GPU也可快速训练。&lt;/li&gt; 
 &lt;li&gt;项目同时开源了大模型的极简结构-包含拓展共享混合专家(MoE)、数据集清洗、预训练(Pretrain)、监督微调(SFT)、LoRA微调， 直接偏好强化学习(DPO)算法、模型蒸馏算法等全过程代码。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MiniMind&lt;/strong&gt;同时拓展了视觉多模态的VLM: &lt;a href="https://github.com/jingyaogong/minimind-v"&gt;MiniMind-V&lt;/a&gt;。&lt;/li&gt; 
 &lt;li&gt;项目所有核心算法代码均从0使用PyTorch原生重构！不依赖第三方库提供的抽象接口。&lt;/li&gt; 
 &lt;li&gt;这不仅是大语言模型的全阶段开源复现，也是一个入门LLM的教程。&lt;/li&gt; 
 &lt;li&gt;希望此项目能为所有人提供一个抛砖引玉的示例，一起感受创造的乐趣！推动更广泛AI社区的进步！&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;为防止误解，“2小时” 基于NVIDIA 3090硬件设备（单卡）测试，“3块钱” 指GPU服务器租用成本，具体规格详情见下文。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/minimind2.gif" alt="minimind2" /&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://www.modelscope.cn/studios/gongjy/MiniMind-Reasoning"&gt;🔗🍓推理模型&lt;/a&gt; | &lt;a href="https://www.modelscope.cn/studios/gongjy/MiniMind"&gt;🔗🤖常规模型&lt;/a&gt; | &lt;a href="https://www.bilibili.com/video/BV12dHPeqE72/?share_source=copy_web&amp;amp;vd_source=670c2504f88726f8cf4a21ef6147c0e8"&gt;🔗🎞️视频介绍&lt;/a&gt;&lt;/p&gt; 
 &lt;div align="center"&gt; 
  &lt;table&gt; 
   &lt;tbody&gt;
    &lt;tr&gt; 
     &lt;td align="center"&gt; &lt;a href="https://huggingface.co/collections/jingyaogong/minimind-66caf8d999f5c7fa64f399e5" style="text-decoration: none;"&gt; &lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/and_huggingface.png" alt="Hugging Face Logo" style="vertical-align: middle; width: auto; max-width: 100%;" /&gt; &lt;/a&gt; &lt;/td&gt; 
     &lt;td align="center"&gt; &lt;a href="https://www.modelscope.cn/profile/gongjy" style="text-decoration: none;"&gt; &lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/and_modelscope.png" alt="ModelScope Logo" style="vertical-align: middle; width: auto; max-width: 100%;" /&gt; &lt;/a&gt; &lt;/td&gt; 
    &lt;/tr&gt; 
   &lt;/tbody&gt;
  &lt;/table&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;h1&gt;📌 Introduction&lt;/h1&gt; 
&lt;p&gt;大语言模型（Large Language Model, LLM）的出现引发了全世界对AI的空前关注。 无论是ChatGPT、DeepSeek还是Qwen，都以其惊艳的效果令人叹为观止。 然而，动辄数百亿参数的庞大规模，使得它们对个人设备而言不仅难以训练，甚至连部署都显得遥不可及。 打开大模型的“黑盒子”，探索其内部运作机制，多么令人心潮澎湃！ 遗憾的是，99%的探索只能止步于使用LoRA等技术对现有大模型进行少量微调，学习一些新指令或任务。 这就好比教牛顿如何使用21世纪的智能手机——虽然有趣，却完全偏离了理解物理本质的初衷。 与此同时，第三方的大模型框架和工具库，如transformers+trl，几乎只暴露了高度抽象的接口。 通过短短10行代码，就能完成“加载模型+加载数据集+推理+强化学习”的全流程训练。 这种高效的封装固然便利，但也像一架高速飞船，将我们与底层实现隔离开来，阻碍了深入探究LLM核心代码的机会。 然而，“用乐高拼出一架飞机，远比坐在头等舱里飞行更让人兴奋！”。 更糟糕的是，互联网上充斥着大量付费课程和营销号，以漏洞百出、一知半解的内容推销AI教程。 正因如此，本项目初衷是拉低LLM的学习门槛，让每个人都能从理解每一行代码开始， 从零开始亲手训练一个极小的语言模型。是的，从&lt;strong&gt;零开始训练&lt;/strong&gt;，而不是仅仅进行&lt;strong&gt;推理&lt;/strong&gt;！ 最低只需3块钱不到的服务器成本，就能亲身体验从0到1构建一个语言模型的全过程。 一起感受创造的乐趣吧！&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] （截至2025-02-07）MiniMind系列已完成多个型号模型的预训练，最小仅需25.8M（0.02B），即可具备流畅对话能力！&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;Models List&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;模型 (大小)&lt;/th&gt; 
    &lt;th&gt;推理占用 (约)&lt;/th&gt; 
    &lt;th&gt;Release&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;MiniMind2-small (26M)&lt;/td&gt; 
    &lt;td&gt;0.5 GB&lt;/td&gt; 
    &lt;td&gt;2025.04.26&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;MiniMind2-MoE (145M)&lt;/td&gt; 
    &lt;td&gt;1.0 GB&lt;/td&gt; 
    &lt;td&gt;2025.04.26&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;MiniMind2 (104M)&lt;/td&gt; 
    &lt;td&gt;1.0 GB&lt;/td&gt; 
    &lt;td&gt;2025.04.26&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;minimind-v1-small (26M)&lt;/td&gt; 
    &lt;td&gt;0.5 GB&lt;/td&gt; 
    &lt;td&gt;2024.08.28&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;minimind-v1-moe (4×26M)&lt;/td&gt; 
    &lt;td&gt;1.0 GB&lt;/td&gt; 
    &lt;td&gt;2024.09.17&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;minimind-v1 (108M)&lt;/td&gt; 
    &lt;td&gt;1.0 GB&lt;/td&gt; 
    &lt;td&gt;2024.09.01&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;p&gt;&lt;strong&gt;项目包含&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;MiniMind-LLM结构的全部代码（Dense+MoE模型）。&lt;/li&gt; 
 &lt;li&gt;包含Tokenizer分词器详细训练代码。&lt;/li&gt; 
 &lt;li&gt;包含Pretrain、SFT、LoRA、RLHF-DPO、模型蒸馏的全过程训练代码。&lt;/li&gt; 
 &lt;li&gt;收集、蒸馏、整理并清洗去重所有阶段的高质量数据集，且全部开源。&lt;/li&gt; 
 &lt;li&gt;从0实现预训练、指令微调、LoRA、DPO强化学习，白盒模型蒸馏。关键算法几乎不依赖第三方封装的框架，且全部开源。&lt;/li&gt; 
 &lt;li&gt;同时兼容&lt;code&gt;transformers&lt;/code&gt;、&lt;code&gt;trl&lt;/code&gt;、&lt;code&gt;peft&lt;/code&gt;等第三方主流框架。&lt;/li&gt; 
 &lt;li&gt;训练支持单机单卡、单机多卡(DDP、DeepSpeed)训练，支持wandb可视化训练流程。支持动态启停训练。&lt;/li&gt; 
 &lt;li&gt;在第三方测评榜（C-Eval、C-MMLU、OpenBookQA等）进行模型测试。&lt;/li&gt; 
 &lt;li&gt;实现Openai-Api协议的极简服务端，便于集成到第三方ChatUI使用（FastGPT、Open-WebUI等）。&lt;/li&gt; 
 &lt;li&gt;基于streamlit实现最简聊天WebUI前端。&lt;/li&gt; 
 &lt;li&gt;全面兼容社区热门&lt;code&gt;llama.cpp&lt;/code&gt;、&lt;code&gt;vllm&lt;/code&gt;、&lt;code&gt;ollama&lt;/code&gt;推理引擎或&lt;code&gt;Llama-Factory&lt;/code&gt;训练框架。&lt;/li&gt; 
 &lt;li&gt;复现(蒸馏/RL)大型推理模型DeepSeek-R1的MiniMind-Reason模型，&lt;strong&gt;数据+模型&lt;/strong&gt;全部开源！&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;希望此开源项目可以帮助LLM初学者快速入门！&lt;/p&gt; 
&lt;h3&gt;👉&lt;strong&gt;更新日志&lt;/strong&gt;&lt;/h3&gt; 
&lt;details close&gt; 
 &lt;summary&gt; &lt;b&gt;2025-04-26 (newest 🎉🎉🎉)&lt;/b&gt; &lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;重要更新&lt;/li&gt; 
  &lt;li&gt;如有兼容性需要，可访问&lt;a href="https://github.com/jingyaogong/minimind/tree/7da201a944a90ed49daef8a0265c959288dff83a"&gt;🔗旧仓库内容🔗&lt;/a&gt;。&lt;/li&gt; 
  &lt;li&gt;MiniMind模型参数完全改名，对齐Transformers库模型（统一命名）。&lt;/li&gt; 
  &lt;li&gt;generate方式重构，继承自GenerationMixin类。&lt;/li&gt; 
  &lt;li&gt;🔥支持llama.cpp、vllm、ollama等热门三方生态。&lt;/li&gt; 
  &lt;li&gt;规范代码和目录结构。&lt;/li&gt; 
  &lt;li&gt;改动词表&lt;code&gt;&amp;lt;s&amp;gt;&amp;lt;/s&amp;gt;&lt;/code&gt;-&amp;gt;&lt;code&gt;&amp;lt;|im_start|&amp;gt;&amp;lt;|im_end|&amp;gt;&lt;/code&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;pre&gt;&lt;code class="language-text"&gt;为兼容第三方推理框架llama.cpp、vllm，本次更新需付出一些可观代价。
本次更新不再支持「直接」加载25-04-26以前的旧模型进行推理。
由于Llama位置编码方式与minimind存在区别，导致映射Llama模型后QK值存在差异
MiniMind2系列旧模型均经过权重映射+（微调训练）QKVO线性层校准恢复而来。
本次更新后将放弃对`minimind-v1`全系列的维护，并在仓库中下线。
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details close&gt; 
 &lt;summary&gt; &lt;b&gt;2025-02-09&lt;/b&gt; &lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;迎来发布以来重大更新，Release MiniMind2 Series。&lt;/li&gt; 
  &lt;li&gt;代码几乎全部重构，使用更简洁明了的统一结构。 如有旧代码的兼容性需要，可访问&lt;a href="https://github.com/jingyaogong/minimind/tree/6e9cd28ef9b34a0a10afbdf6f59e65cb6e628efb"&gt;🔗旧仓库内容🔗&lt;/a&gt;。&lt;/li&gt; 
  &lt;li&gt;免去数据预处理步骤。统一数据集格式，更换为&lt;code&gt;jsonl&lt;/code&gt;格式杜绝数据集下载混乱的问题。&lt;/li&gt; 
  &lt;li&gt;MiniMind2系列效果相比MiniMind-V1显著提升。&lt;/li&gt; 
  &lt;li&gt;小问题：{kv-cache写法更标准、MoE的负载均衡loss被考虑等等}&lt;/li&gt; 
  &lt;li&gt;提供模型迁移到私有数据集的训练方案（医疗模型、自我认知样例）。&lt;/li&gt; 
  &lt;li&gt;精简预训练数据集，并大幅提升预训练数据质量，大幅缩短个人快速训练所需时间，单卡3090即可2小时复现！&lt;/li&gt; 
  &lt;li&gt;更新：LoRA微调脱离peft包装，从0实现LoRA过程；DPO算法从0使用PyTorch原生实现；模型白盒蒸馏原生实现。&lt;/li&gt; 
  &lt;li&gt;MiniMind2-DeepSeek-R1系列蒸馏模型诞生！&lt;/li&gt; 
  &lt;li&gt;MiniMind2具备一定的英文能力！&lt;/li&gt; 
  &lt;li&gt;更新MiniMind2与第三方模型的基于更多大模型榜单测试性能的结果。&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details close&gt; 
 &lt;summary&gt; &lt;b&gt;2024-10-05&lt;/b&gt; &lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;为MiniMind拓展了多模态能力之---视觉&lt;/li&gt; 
  &lt;li&gt;移步孪生项目&lt;a href="https://github.com/jingyaogong/minimind-v"&gt;minimind-v&lt;/a&gt;查看详情！&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details close&gt; 
 &lt;summary&gt; &lt;b&gt;2024-09-27&lt;/b&gt; &lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;09-27更新pretrain数据集的预处理方式，为了保证文本完整性，放弃预处理成.bin训练的形式（轻微牺牲训练速度）。&lt;/li&gt; 
  &lt;li&gt;目前pretrain预处理后的文件命名为：pretrain_data.csv。&lt;/li&gt; 
  &lt;li&gt;删除了一些冗余的代码。&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details close&gt; 
 &lt;summary&gt; &lt;b&gt;2024-09-17&lt;/b&gt; &lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;更新minimind-v1-moe模型&lt;/li&gt; 
  &lt;li&gt;为了防止歧义，不再使用mistral_tokenizer分词，全部采用自定义的minimind_tokenizer作为分词器。&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details close&gt; 
 &lt;summary&gt; &lt;b&gt;2024-09-01&lt;/b&gt; &lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;更新minimind-v1 (108M)模型，采用minimind_tokenizer，预训练轮次3 + SFT轮次10，更充分训练，性能更强。&lt;/li&gt; 
  &lt;li&gt;项目已部署至ModelScope创空间，可以在此网站上体验：&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://www.modelscope.cn/studios/gongjy/minimind"&gt;🔗ModelScope在线体验🔗&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details close&gt; 
 &lt;summary&gt; &lt;b&gt;2024-08-27&lt;/b&gt; &lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;项目首次开源&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h1&gt;📌 快速开始&lt;/h1&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;分享本人的软硬件配置（仅供参考）&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;CPU: Intel(R) Core(TM) i9-10980XE CPU @ 3.00GHz&lt;/li&gt; 
  &lt;li&gt;RAM: 128 GB&lt;/li&gt; 
  &lt;li&gt;GPU: NVIDIA GeForce RTX 3090(24GB) * 8&lt;/li&gt; 
  &lt;li&gt;Ubuntu==20.04&lt;/li&gt; 
  &lt;li&gt;CUDA==12.2&lt;/li&gt; 
  &lt;li&gt;Python==3.10.16&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/jingyaogong/minimind/master/requirements.txt"&gt;requirements.txt&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;第0步&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/jingyaogong/minimind.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Ⅰ 测试已有模型效果&lt;/h2&gt; 
&lt;h3&gt;1.环境准备&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2.下载模型&lt;/h3&gt; 
&lt;p&gt;到项目根目录&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://huggingface.co/jingyaogong/MiniMind2
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;（可选）命令行问答&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# load=0: load from pytorch model, load=1: load from transformers-hf model
python eval_model.py --load 1 --model_mode 2
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;（可选）启动WebUI&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 可能需要`python&amp;gt;=3.10` 安装 `pip install streamlit`
# cd scripts
streamlit run web_demo.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;（可选）第三方推理框架&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# ollama
ollama run jingyaogong/minimind2
# vllm
vllm serve ./MiniMind2/ --served-model-name "minimind"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Ⅱ 从0开始自己训练&lt;/h2&gt; 
&lt;h3&gt;1.环境准备&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple
&lt;/code&gt;&lt;/pre&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;注：提前测试Torch是否可用cuda&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;import torch
print(torch.cuda.is_available())
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;如果不可用，请自行去&lt;a href="https://download.pytorch.org/whl/torch_stable.html"&gt;torch_stable&lt;/a&gt; 下载whl文件安装。参考&lt;a href="https://blog.csdn.net/weixin_45456738/article/details/141029610?ops_request_misc=&amp;amp;request_id=&amp;amp;biz_id=102&amp;amp;utm_term=%E5%AE%89%E8%A3%85torch&amp;amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-2-141029610.nonecase&amp;amp;spm=1018.2226.3001.4187"&gt;链接&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;h3&gt;2.数据下载&lt;/h3&gt; 
&lt;p&gt;从下文提供的&lt;a href="https://www.modelscope.cn/datasets/gongjy/minimind_dataset/files"&gt;数据集下载链接&lt;/a&gt; 下载需要的数据文件（创建&lt;code&gt;./dataset&lt;/code&gt;目录）并放到&lt;code&gt;./dataset&lt;/code&gt;下&lt;/p&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;注：数据集须知&lt;/summary&gt; 
 &lt;p&gt;默认推荐下载&lt;code&gt;pretrain_hq.jsonl&lt;/code&gt; + &lt;code&gt;sft_mini_512.jsonl&lt;/code&gt;最快速度复现Zero聊天模型。&lt;/p&gt; 
 &lt;p&gt;数据文件可自由选择，下文提供了多种搭配方案，可根据自己手头的训练需求和GPU资源进行适当组合。&lt;/p&gt; 
&lt;/details&gt; 
&lt;h3&gt;3.开始训练&lt;/h3&gt; 
&lt;p&gt;目录位于&lt;code&gt;trainer&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;3.1 预训练（学知识）&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python train_pretrain.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;执行预训练，得到 &lt;code&gt;pretrain_*.pth&lt;/code&gt; 作为预训练的输出权重（其中*为模型的dimension，默认为512）&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;3.2 监督微调（学对话方式）&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python train_full_sft.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;执行监督微调，得到 &lt;code&gt;full_sft_*.pth&lt;/code&gt; 作为指令微调的输出权重（其中&lt;code&gt;full&lt;/code&gt;即为全参数微调）&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;注：训练须知&lt;/summary&gt; 
 &lt;p&gt;所有训练过程默认每隔100步保存1次参数到文件&lt;code&gt;./out/***.pth&lt;/code&gt;（每次会覆盖掉旧权重文件）。&lt;/p&gt; 
 &lt;p&gt;简单起见，此处只写明两个阶段训练过程。如需其它训练 (LoRA, 蒸馏, 强化学习, 微调推理等) 可参考下文【实验】小节的详细说明。&lt;/p&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h3&gt;4.测试模型效果&lt;/h3&gt; 
&lt;p&gt;确保需要测试的模型&lt;code&gt;*.pth&lt;/code&gt;文件位于&lt;code&gt;./out/&lt;/code&gt;目录下。 也可以直接去&lt;a href="https://www.modelscope.cn/models/gongjy/MiniMind2-PyTorch/files"&gt;此处&lt;/a&gt;下载使用我训练的&lt;code&gt;*.pth&lt;/code&gt;文件。&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python eval_model.py --model_mode 1 # 默认为0：测试pretrain模型效果，设置为1：测试full_sft模型效果
&lt;/code&gt;&lt;/pre&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;注：测试须知&lt;/summary&gt; 
 &lt;p&gt;如需详情，查看&lt;code&gt;eval_model.py&lt;/code&gt;脚本代码即可。model_mode分为 0: 预训练模型，1: SFT-Chat模型，2: RLHF-Chat模型，3: Reason模型&lt;/p&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] 所有训练脚本均为Pytorch原生框架，均支持多卡加速，假设你的设备有N (N＞1) 张显卡：&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;单机N卡启动训练方式 (DDP, 支持多机多卡集群)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;torchrun --nproc_per_node N train_xxx.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;注：其它须知&lt;/summary&gt; 
 &lt;p&gt;单机N卡启动训练 (DeepSpeed)&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;deepspeed --master_port 29500 --num_gpus=N train_xxx.py
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;可根据需要开启wandb记录训练过程&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# 需要登录: wandb login
torchrun --nproc_per_node N train_xxx.py --use_wandb
# and
python train_xxx.py --use_wandb
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;通过添加&lt;code&gt;--use_wandb&lt;/code&gt;参数，可以记录训练过程，训练完成后，可以在wandb网站上查看训练过程。通过修改&lt;code&gt;wandb_project&lt;/code&gt; 和&lt;code&gt;wandb_run_name&lt;/code&gt;参数，可以指定项目名称和运行名称。&lt;/p&gt; 
&lt;/details&gt; 
&lt;h1&gt;📌 数据介绍&lt;/h1&gt; 
&lt;h2&gt;Ⅰ Tokenizer&lt;/h2&gt; 
&lt;p&gt;分词器将单词从自然语言通过“词典”映射到&lt;code&gt;0, 1, 36&lt;/code&gt;这样的数字，可以理解为数字就代表了单词在“词典”中的页码。 可以选择自己构造词表训练一个“词典”，代码可见&lt;code&gt;./scripts/train_tokenizer.py&lt;/code&gt;（仅供学习参考，若非必要无需再自行训练，MiniMind已自带tokenizer）。 或者选择比较出名的开源大模型分词器， 正如同直接用新华/牛津词典的优点是token编码压缩率很好，缺点是页数太多，动辄数十万个词汇短语； 自己训练的分词器，优点是词表长度和内容随意控制，缺点是压缩率很低（例如"hello"也许会被拆分为"h e l l o" 五个独立的token），且生僻词难以覆盖。 “词典”的选择固然很重要，LLM的输出本质上是SoftMax到词典N个词的多分类问题，然后通过“词典”解码到自然语言。 因为MiniMind体积需要严格控制，为了避免模型头重脚轻（词嵌入embedding层参数在LLM占比太高），所以词表长度短短益善。&lt;/p&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;Tokenizer介绍&lt;/summary&gt; 
 &lt;p&gt;第三方强大的开源模型例如Yi、qwen、chatglm、mistral、Llama3的tokenizer词表长度如下：&lt;/p&gt; 
 &lt;table&gt; 
  &lt;tbody&gt;
   &lt;tr&gt;
    &lt;th&gt;Tokenizer模型&lt;/th&gt;
    &lt;th&gt;词表大小&lt;/th&gt;
    &lt;th&gt;来源&lt;/th&gt;
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td&gt;yi tokenizer&lt;/td&gt;
    &lt;td&gt;64,000&lt;/td&gt;
    &lt;td&gt;01万物（中国）&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td&gt;qwen2 tokenizer&lt;/td&gt;
    &lt;td&gt;151,643&lt;/td&gt;
    &lt;td&gt;阿里云（中国）&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td&gt;glm tokenizer&lt;/td&gt;
    &lt;td&gt;151,329&lt;/td&gt;
    &lt;td&gt;智谱AI（中国）&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td&gt;mistral tokenizer&lt;/td&gt;
    &lt;td&gt;32,000&lt;/td&gt;
    &lt;td&gt;Mistral AI（法国）&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td&gt;llama3 tokenizer&lt;/td&gt;
    &lt;td&gt;128,000&lt;/td&gt;
    &lt;td&gt;Meta（美国）&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td&gt;minimind tokenizer&lt;/td&gt;
    &lt;td&gt;6,400&lt;/td&gt;
    &lt;td&gt;自定义&lt;/td&gt;
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;👉2024-09-17更新：为了防止过去的版本歧义&amp;amp;控制体积，minimind所有模型均使用minimind_tokenizer分词，废弃所有mistral_tokenizer版本。&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;pre&gt;&lt;code&gt;# 一些自言自语
&amp;gt; 尽管minimind_tokenizer长度很小，编解码效率弱于qwen2、glm等中文友好型分词器。
&amp;gt; 但minimind模型选择了自己训练的minimind_tokenizer作为分词器，以保持整体参数轻量，避免编码层和计算层占比失衡，头重脚轻，因为minimind的词表大小只有6400。
&amp;gt; 且minimind在实际测试中没有出现过生僻词汇解码失败的情况，效果良好。
&amp;gt; 由于自定义词表压缩长度到6400，使得LLM总参数量最低只有25.8M。
&amp;gt; 训练数据`tokenizer_train.jsonl`均来自于`匠数大模型数据集`，这部分数据相对次要，如需训练可以自由选择。
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;Ⅱ Pretrain数据&lt;/h2&gt; 
&lt;p&gt;经历了MiniMind-V1的低质量预训练数据，导致模型胡言乱语的教训，&lt;code&gt;2025-02-05&lt;/code&gt; 之后决定不再采用大规模无监督的数据集做预训练。 进而尝试把&lt;a href="https://www.modelscope.cn/datasets/deepctrl/deepctrl-sft-data"&gt;匠数大模型数据集&lt;/a&gt;的中文部分提取出来， 清洗出字符&lt;code&gt;&amp;lt;512&lt;/code&gt;长度的大约1.6GB的语料直接拼接成预训练数据 &lt;code&gt;pretrain_hq.jsonl&lt;/code&gt;，hq即为high quality（当然也还不算high，提升数据质量无止尽）。&lt;/p&gt; 
&lt;p&gt;文件&lt;code&gt;pretrain_hq.jsonl&lt;/code&gt; 数据格式为&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;{"text": "如何才能摆脱拖延症？ 治愈拖延症并不容易，但以下建议可能有所帮助..."}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Ⅲ SFT数据&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.modelscope.cn/datasets/deepctrl/deepctrl-sft-data"&gt;匠数大模型SFT数据集&lt;/a&gt; “是一个完整、格式统一、安全的大模型训练和研究资源。 从网络上的公开数据源收集并整理了大量开源数据集，对其进行了格式统一，数据清洗， 包含10M条数据的中文数据集和包含2M条数据的英文数据集。” 以上是官方介绍，下载文件后的数据总量大约在4B tokens，肯定是适合作为中文大语言模型的SFT数据的。 但是官方提供的数据格式很乱，全部用来sft代价太大。 我将把官方数据集进行了二次清洗，把含有符号污染和噪声的条目去除；另外依然只保留了总长度&lt;code&gt;&amp;lt;512&lt;/code&gt; 的内容，此阶段希望通过大量对话补充预训练阶段欠缺的知识。 导出文件为&lt;code&gt;sft_512.jsonl&lt;/code&gt;(~7.5GB)。&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.modelscope.cn/organization/Magpie-Align"&gt;Magpie-SFT数据集&lt;/a&gt; 收集了~1M条来自Qwen2/2.5的高质量对话，我将这部分数据进一步清洗，把总长度&lt;code&gt;&amp;lt;2048&lt;/code&gt;的部分导出为&lt;code&gt;sft_2048.jsonl&lt;/code&gt;(~9GB)。 长度&lt;code&gt;&amp;lt;1024&lt;/code&gt;的部分导出为&lt;code&gt;sft_1024.jsonl&lt;/code&gt;(~5.5GB)，用大模型对话数据直接进行sft就属于“黑盒蒸馏”的范畴。&lt;/p&gt; 
&lt;p&gt;进一步清洗前两步sft的数据（只保留中文字符占比高的内容），筛选长度&lt;code&gt;&amp;lt;512&lt;/code&gt;的对话，得到&lt;code&gt;sft_mini_512.jsonl&lt;/code&gt;(~1.2GB)。&lt;/p&gt; 
&lt;p&gt;所有sft文件 &lt;code&gt;sft_X.jsonl&lt;/code&gt; 数据格式均为&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;{
    "conversations": [
        {"role": "user", "content": "你好"},
        {"role": "assistant", "content": "你好！"},
        {"role": "user", "content": "再见"},
        {"role": "assistant", "content": "再见！"}
    ]
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Ⅳ RLHF数据&lt;/h2&gt; 
&lt;p&gt;来自&lt;a href="https://www.modelscope.cn/datasets/Magpie-Align/MagpieLM-DPO-Data-v0.1"&gt;Magpie-DPO数据集&lt;/a&gt; 大约200k条偏好数据（均是英文）生成自Llama3.1-70B/8B，可以用于训练奖励模型，优化模型回复质量，使其更加符合人类偏好。 这里将数据总长度&lt;code&gt;&amp;lt;3000&lt;/code&gt;的内容重组为&lt;code&gt;dpo.jsonl&lt;/code&gt;(~0.9GB)，包含&lt;code&gt;chosen&lt;/code&gt;和&lt;code&gt;rejected&lt;/code&gt;两个字段，&lt;code&gt;chosen&lt;/code&gt; 为偏好的回复，&lt;code&gt;rejected&lt;/code&gt;为拒绝的回复。&lt;/p&gt; 
&lt;p&gt;文件 &lt;code&gt;dpo.jsonl&lt;/code&gt; 数据格式为&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;{
  "chosen": [
    {"content": "Q", "role": "user"}, 
    {"content": "good answer", "role": "assistant"}
  ], 
  "rejected": [
    {"content": "Q", "role": "user"}, 
    {"content": "bad answer", "role": "assistant"}
  ]
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Ⅴ Reason数据集：&lt;/h2&gt; 
&lt;p&gt;不得不说2025年2月谁能火的过DeepSeek... 也激发了我对RL引导的推理模型的浓厚兴趣，目前已经用Qwen2.5复现了R1-Zero。 如果有时间+效果work（但99%基模能力不足）我会在之后更新MiniMind基于RL训练的推理模型而不是蒸馏模型。 时间有限，最快的低成本方案依然是直接蒸馏（黑盒方式）。 耐不住R1太火，短短几天就已经存在一些R1的蒸馏数据集&lt;a href="https://www.modelscope.cn/datasets/Magpie-Align/Magpie-Reasoning-V2-250K-CoT-Deepseek-R1-Llama-70B"&gt;R1-Llama-70B&lt;/a&gt;、&lt;a href="https://www.modelscope.cn/datasets/AI-ModelScope/R1-Distill-SFT"&gt;R1-Distill-SFT&lt;/a&gt;、 &lt;a href="https://huggingface.co/datasets/shareAI/Alpaca-Distill-R1-ZH"&gt;Alpaca-Distill-R1&lt;/a&gt;、 &lt;a href="https://huggingface.co/datasets/jinliuxi/deepseek_r1_zh"&gt;deepseek_r1_zh&lt;/a&gt;等等，纯中文的数据可能比较少。 最终整合它们，导出文件为&lt;code&gt;r1_mix_1024.jsonl&lt;/code&gt;，数据格式和&lt;code&gt;sft_X.jsonl&lt;/code&gt;一致。&lt;/p&gt; 
&lt;h2&gt;Ⅵ 更多数据集&lt;/h2&gt; 
&lt;p&gt;目前已经有&lt;a href="https://github.com/HqWu-HITCS/Awesome-Chinese-LLM"&gt;HqWu-HITCS/Awesome-Chinese-LLM&lt;/a&gt; 在收集和梳理中文LLM相关的开源模型、应用、数据集及教程等资料，并持续更新这方面的最新进展。全面且专业，Respect！&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Ⅷ MiniMind训练数据集&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] 2025-02-05后，开源MiniMind最终训练所用的所有数据集，因此无需再自行预处理大规模数据集，避免重复性的数据处理工作。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;MiniMind训练数据集下载地址： &lt;a href="https://www.modelscope.cn/datasets/gongjy/minimind_dataset/files"&gt;ModelScope&lt;/a&gt; | &lt;a href="https://huggingface.co/datasets/jingyaogong/minimind_dataset/tree/main"&gt;HuggingFace&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;无需全部clone，可单独下载所需的文件&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;将下载的数据集文件放到&lt;code&gt;./dataset/&lt;/code&gt;目录下（✨为推荐的必须项）&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;./dataset/
├── dpo.jsonl (909MB)
├── lora_identity.jsonl (22.8KB)
├── lora_medical.jsonl (34MB)
├── pretrain_hq.jsonl (1.6GB, ✨)
├── r1_mix_1024.jsonl (340MB)
├── sft_1024.jsonl (5.6GB)
├── sft_2048.jsonl (9GB)
├── sft_512.jsonl (7.5GB)
├── sft_mini_512.jsonl (1.2GB, ✨)
└── tokenizer_train.jsonl (1GB)
&lt;/code&gt;&lt;/pre&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;注：各数据集简介&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;dpo.jsonl&lt;/code&gt; --RLHF阶段数据集&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;lora_identity.jsonl&lt;/code&gt; --自我认知数据集（例如：你是谁？我是minimind...），推荐用于lora训练（亦可用于全参SFT，勿被名字局限）&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;lora_medical.jsonl&lt;/code&gt; --医疗问答数据集，推荐用于lora训练（亦可用于全参SFT，勿被名字局限）&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;pretrain_hq.jsonl&lt;/code&gt;✨ --预训练数据集，整合自jiangshu科技&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;r1_mix_1024.jsonl&lt;/code&gt; --DeepSeek-R1-1.5B蒸馏数据，每条数据字符最大长度为1024（因此训练时设置max_seq_len=1024）&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;sft_1024.jsonl&lt;/code&gt; --整合自Qwen2.5蒸馏数据（是sft_2048的子集），每条数据字符最大长度为1024（因此训练时设置max_seq_len=1024）&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;sft_2048.jsonl&lt;/code&gt; --整合自Qwen2.5蒸馏数据，每条数据字符最大长度为2048（因此训练时设置max_seq_len=2048）&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;sft_512.jsonl&lt;/code&gt; --整合自匠数科技SFT数据，每条数据字符最大长度为512（因此训练时设置max_seq_len=512）&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;sft_mini_512.jsonl&lt;/code&gt;✨ --极简整合自匠数科技SFT数据+Qwen2.5蒸馏数据（用于快速训练Zero模型），每条数据字符最大长度为512（因此训练时设置max_seq_len=512）&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;tokenizer_train.jsonl&lt;/code&gt; --均来自于&lt;code&gt;匠数大模型数据集&lt;/code&gt;，这部分数据相对次要，（不推荐自己重复训练tokenizer，理由如上）如需自己训练tokenizer可以自由选择数据集。&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/dataset.jpg" alt="dataset" /&gt;&lt;/p&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;说明 &amp;amp; 推荐训练方案&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;MiniMind2 Series均经过共约20GB语料训练，大约4B tokens，即对应上面的数据组合训练结果（开销：💰💰💰💰💰💰💰💰，效果：😊😊😊😊😊😊）&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;想要最快速度从0实现Zero模型，推荐使用&lt;code&gt;pretrain_hq.jsonl&lt;/code&gt; + &lt;code&gt;sft_mini_512.jsonl&lt;/code&gt; 的数据组合，具体花销和效果可查看下文表格（开销：💰，效果：😊😊）&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;推荐具备一定算力资源或更在意效果的朋友可以考虑前者完整复现MiniMind2；仅有单卡GPU或在乎短时间快速复现的朋友强烈推荐后者；&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;【折中方案】亦可选择例如&lt;code&gt;sft_mini_512.jsonl&lt;/code&gt;、&lt;code&gt;sft_1024.jsonl&lt;/code&gt;中等规模数据进行自由组合训练（开销：💰💰💰，效果：😊😊😊😊）。&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h1&gt;📌 Model Structure&lt;/h1&gt; 
&lt;p&gt;MiniMind-Dense（和&lt;a href="https://ai.meta.com/blog/meta-llama-3-1/"&gt;Llama3.1&lt;/a&gt;一样）使用了Transformer的Decoder-Only结构，跟GPT-3的区别在于：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;采用了GPT-3的预标准化方法，也就是在每个Transformer子层的输入上进行归一化，而不是在输出上。具体来说，使用的是RMSNorm归一化函数。&lt;/li&gt; 
 &lt;li&gt;用SwiGLU激活函数替代了ReLU，这样做是为了提高性能。&lt;/li&gt; 
 &lt;li&gt;像GPT-Neo一样，去掉了绝对位置嵌入，改用了旋转位置嵌入（RoPE），这样在处理超出训练长度的推理时效果更好。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;p&gt;MiniMind-MoE模型，它的结构基于Llama3和&lt;a href="https://arxiv.org/pdf/2405.04434"&gt;Deepseek-V2/3&lt;/a&gt;中的MixFFN混合专家模块。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;DeepSeek-V2在前馈网络（FFN）方面，采用了更细粒度的专家分割和共享的专家隔离技术，以提高Experts的效果。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;p&gt;MiniMind的整体结构一致，只是在RoPE计算、推理函数和FFN层的代码上做了一些小调整。 其结构如下图（重绘版）：&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/LLM-structure.png" alt="structure" /&gt; &lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/LLM-structure-moe.png" alt="structure-moe" /&gt;&lt;/p&gt; 
&lt;p&gt;修改模型配置见&lt;a href="https://raw.githubusercontent.com/jingyaogong/minimind/master/model/LMConfig.py"&gt;./model/LMConfig.py&lt;/a&gt;。 参考模型参数版本见下表：&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model Name&lt;/th&gt; 
   &lt;th&gt;params&lt;/th&gt; 
   &lt;th&gt;len_vocab&lt;/th&gt; 
   &lt;th&gt;rope_theta&lt;/th&gt; 
   &lt;th&gt;n_layers&lt;/th&gt; 
   &lt;th&gt;d_model&lt;/th&gt; 
   &lt;th&gt;kv_heads&lt;/th&gt; 
   &lt;th&gt;q_heads&lt;/th&gt; 
   &lt;th&gt;share+route&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniMind2-Small&lt;/td&gt; 
   &lt;td&gt;26M&lt;/td&gt; 
   &lt;td&gt;6400&lt;/td&gt; 
   &lt;td&gt;1e6&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;512&lt;/td&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniMind2-MoE&lt;/td&gt; 
   &lt;td&gt;145M&lt;/td&gt; 
   &lt;td&gt;6400&lt;/td&gt; 
   &lt;td&gt;1e6&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;640&lt;/td&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;1+4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniMind2&lt;/td&gt; 
   &lt;td&gt;104M&lt;/td&gt; 
   &lt;td&gt;6400&lt;/td&gt; 
   &lt;td&gt;1e6&lt;/td&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;768&lt;/td&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;minimind-v1-small&lt;/td&gt; 
   &lt;td&gt;26M&lt;/td&gt; 
   &lt;td&gt;6400&lt;/td&gt; 
   &lt;td&gt;1e4&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;512&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;minimind-v1-moe&lt;/td&gt; 
   &lt;td&gt;4×26M&lt;/td&gt; 
   &lt;td&gt;6400&lt;/td&gt; 
   &lt;td&gt;1e4&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;512&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;1+4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;minimind-v1&lt;/td&gt; 
   &lt;td&gt;108M&lt;/td&gt; 
   &lt;td&gt;6400&lt;/td&gt; 
   &lt;td&gt;1e4&lt;/td&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;768&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;📌 Experiment&lt;/h1&gt; 
&lt;h2&gt;Ⅰ 训练开销&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;时间单位&lt;/strong&gt;：小时 (h)。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;成本单位&lt;/strong&gt;：人民币 (￥)；7￥ ≈ 1美元。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;3090 租卡单价&lt;/strong&gt;：≈1.3￥/h（可自行参考实时市价）。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;参考标准&lt;/strong&gt;：表格仅实测 &lt;code&gt;pretrain&lt;/code&gt; 和 &lt;code&gt;sft_mini_512&lt;/code&gt; 两个数据集的训练时间，其它耗时根据数据集大小估算（可能存在些许出入）。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;基于 3090 （单卡）成本计算&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model Name&lt;/th&gt; 
   &lt;th&gt;params&lt;/th&gt; 
   &lt;th&gt;pretrain&lt;/th&gt; 
   &lt;th&gt;sft_mini_512&lt;/th&gt; 
   &lt;th&gt;sft_512&lt;/th&gt; 
   &lt;th&gt;sft_1024&lt;/th&gt; 
   &lt;th&gt;sft_2048&lt;/th&gt; 
   &lt;th&gt;RLHF&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniMind2-Small&lt;/td&gt; 
   &lt;td&gt;26M&lt;/td&gt; 
   &lt;td&gt;≈1.1h&lt;br /&gt;≈1.43￥&lt;/td&gt; 
   &lt;td&gt;≈1h&lt;br /&gt;≈1.3￥&lt;/td&gt; 
   &lt;td&gt;≈6h&lt;br /&gt;≈7.8￥&lt;/td&gt; 
   &lt;td&gt;≈4.58h&lt;br /&gt;≈5.95￥&lt;/td&gt; 
   &lt;td&gt;≈7.5h&lt;br /&gt;≈9.75￥&lt;/td&gt; 
   &lt;td&gt;≈1h&lt;br /&gt;≈1.3￥&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniMind2&lt;/td&gt; 
   &lt;td&gt;104M&lt;/td&gt; 
   &lt;td&gt;≈3.9h&lt;br /&gt;≈5.07￥&lt;/td&gt; 
   &lt;td&gt;≈3.3h&lt;br /&gt;≈4.29￥&lt;/td&gt; 
   &lt;td&gt;≈20h&lt;br /&gt;≈26￥&lt;/td&gt; 
   &lt;td&gt;≈15h&lt;br /&gt;≈19.5￥&lt;/td&gt; 
   &lt;td&gt;≈25h&lt;br /&gt;≈32.5￥&lt;/td&gt; 
   &lt;td&gt;≈3h&lt;br /&gt;≈3.9￥&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;训练开销总结&amp;amp;预测&lt;/summary&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;MiniMind2-Small参数&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;&lt;code&gt;pretrain_hq&lt;/code&gt;+&lt;code&gt;sft_mini_512&lt;/code&gt;数据集 &lt;br /&gt;单卡3090 (1 epoch) + 2.1小时 + 花费2.73元人民币 &lt;br /&gt;即可从0训练出MiniMind-Zero-0.025B模型!!!&lt;/p&gt; 
  &lt;/blockquote&gt; 
 &lt;/blockquote&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;MiniMind2-Small参数&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;&lt;code&gt;pretrain_hq&lt;/code&gt;+&lt;code&gt;sft_512&lt;/code&gt;+&lt;code&gt;sft_2048&lt;/code&gt;+&lt;code&gt;dpo&lt;/code&gt;数据集 &lt;br /&gt;单卡3090 (2 epochs) + 大约38.16小时 + 花费49.61元人民币 &lt;br /&gt;即可从0训练出MiniMind2-Small-0.025B模型!!!&lt;/p&gt; 
  &lt;/blockquote&gt; 
 &lt;/blockquote&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;MiniMind2参数&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;&lt;code&gt;pretrain_hq&lt;/code&gt;+&lt;code&gt;sft_512&lt;/code&gt;+&lt;code&gt;sft_2048&lt;/code&gt;+&lt;code&gt;dpo&lt;/code&gt;数据集 &lt;br /&gt;单卡3090 (2 epochs) + 大约122小时 + 花费158.6元人民币 &lt;br /&gt;即可从0训练出MiniMind2-0.1B模型!!!&lt;/p&gt; 
  &lt;/blockquote&gt; 
 &lt;/blockquote&gt; 
&lt;/details&gt; 
&lt;p&gt;✨基于单卡NVIDIA 3090的&lt;code&gt;MiniMind-Zero&lt;/code&gt;从0训练仅需&lt;code&gt;2小时&lt;/code&gt; + &lt;code&gt;3块钱&lt;/code&gt;，实现ChatBot效果！&lt;/p&gt; 
&lt;p&gt;✨PS：若采用8卡4090训练，总用时甚至可以压缩到10分钟以内！（由于时间更短，花费同样3元左右，与单卡成本相当）&lt;/p&gt; 
&lt;p&gt;✨以极低极低的门槛，实现人人可玩的大模型自由！这正是MiniMind系列的诞生初衷！&lt;/p&gt; 
&lt;p&gt;✨仅价值&lt;code&gt;3块钱&lt;/code&gt;成本的&lt;code&gt;MiniMind-Zero&lt;/code&gt;并不是噱头！Chat测试：&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;👶: 请介绍一下自己。
🤖️: 作为人工智能，我没有实际的生活，也没有自我意识，所以没有自己的生活。我被设计成能够帮助用户解答问题、提供信息、进行对话等。我的设计和功能是由计算机科学、机器学习、人工智能技术和算法所构建的，我通过编程和算法实现。

👶: 推荐一些杭州的特色美食吧。
🤖️: 杭州有很多美味的特色美食，比如鸡肉串、鳗鱼头、水煮鱼、豆腐脑等。这些美食都非常美味，有着独特的风味和口感，绝对是值得一试的美食。

👶: 请为我讲解“大语言模型”这个概念。
🤖️: 大语言模型是一种人工智能技术，它使用大量的文本数据来训练模型，然后在不断的迭代中不断优化模型。它的目标是根据输入的数据来生成符合特定需求的文本，这些文本可以是任何你感兴趣的主题。

👶: Introduce the history of the United States, please.
🤖️: 您提到的“Introok's the believeations of theument." 这个名字来源于中国古代的"groty of of the change."
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;极速且初具效果，甚至仍然可以进一步压缩获取更小更优质的训练数据。 Zero模型权重保存为 &lt;code&gt;full_sft_512_zero.pth&lt;/code&gt;（见下文MiniMind模型文件链接），如有兴趣可下载检验此模型效果。&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Ⅱ 主要训练步骤&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;所有训练脚本均 &lt;code&gt;cd ./trainer&lt;/code&gt; 目录执行&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;&lt;strong&gt;1. 预训练(Pretrain)&lt;/strong&gt;:&lt;/h3&gt; 
&lt;p&gt;LLM首先要学习的并非直接与人交流，而是让网络参数中充满知识的墨水，“墨水” 理论上喝的越饱越好，产生大量的对世界的知识积累。 预训练就是让Model先埋头苦学大量基本的知识，例如从Wiki百科、新闻、书籍整理大规模的高质量训练数据。 这个过程是“无监督”的，即人类不需要在过程中做任何“有监督”的校正，而是由模型自己从大量文本中总结规律学习知识点。 模型此阶段目的只有一个：&lt;strong&gt;学会词语接龙&lt;/strong&gt;。例如我们输入“秦始皇”四个字，它可以接龙“是中国的第一位皇帝”。&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;torchrun --nproc_per_node 1 train_pretrain.py # 1即为单卡训练，可根据硬件情况自行调整 (设置&amp;gt;=2)
# or
python train_pretrain.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;训练后的模型权重文件默认每隔&lt;code&gt;100步&lt;/code&gt;保存为: &lt;code&gt;pretrain_*.pth&lt;/code&gt;（* 为模型具体dimension，每次保存时新文件会覆盖旧文件）&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;&lt;strong&gt;2. 有监督微调(Supervised Fine-Tuning)&lt;/strong&gt;:&lt;/h3&gt; 
&lt;p&gt;经过预训练，LLM此时已经掌握了大量知识，然而此时它只会无脑地词语接龙，还不会与人聊天。 SFT阶段就需要把半成品LLM施加一个自定义的聊天模板进行微调。 例如模型遇到这样的模板【问题-&amp;gt;回答，问题-&amp;gt;回答】后不再无脑接龙，而是意识到这是一段完整的对话结束。 称这个过程为指令微调，就如同让已经学富五车的「牛顿」先生适应21世纪智能手机的聊天习惯，学习屏幕左侧是对方消息，右侧是本人消息这个规律。 在训练时，MiniMind的指令和回答长度被截断在512，是为了节省显存空间。就像我们学习时，会先从短的文章开始，当学会写作200字作文后，800字文章也可以手到擒来。 在需要长度拓展时，只需要准备少量的2k/4k/8k长度对话数据进行进一步微调即可（此时最好配合RoPE-NTK的基准差值）。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;在推理时通过调整RoPE线性差值，实现免训练长度外推到2048及以上将会很方便。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;torchrun --nproc_per_node 1 train_full_sft.py
# or
python train_full_sft.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;训练后的模型权重文件默认每隔&lt;code&gt;100步&lt;/code&gt;保存为: &lt;code&gt;full_sft_*.pth&lt;/code&gt;（* 为模型具体dimension，每次保存时新文件会覆盖旧文件）&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Ⅲ 其它训练步骤&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;所有训练脚本均 &lt;code&gt;cd ./trainer&lt;/code&gt; 目录执行&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;&lt;strong&gt;3. 人类反馈强化学习(Reinforcement Learning from Human Feedback, RLHF)&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;在前面的训练步骤中，模型已经具备了基本的对话能力，但是这样的能力完全基于单词接龙，缺少正反样例的激励。 模型此时尚未知什么回答是好的，什么是差的。我们希望它能够更符合人的偏好，降低让人类不满意答案的产生概率。 这个过程就像是让模型参加新的培训，从优秀员工的作为例子，消极员工作为反例，学习如何更好地回复。 此处使用的是RLHF系列之-直接偏好优化(Direct Preference Optimization, DPO)。 与PPO(Proximal Policy Optimization)这种需要奖励模型、价值模型的RL算法不同； DPO通过推导PPO奖励模型的显式解，把在线奖励模型换成离线数据，Ref模型输出可以提前保存。 DPO性能几乎不变，只用跑 actor_model 和 ref_model 两个模型，大大节省显存开销和增加训练稳定性。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;注：RLHF训练步骤&lt;strong&gt;并非必须&lt;/strong&gt;，此步骤难以提升模型“智力”而通常仅用于提升模型的“礼貌”，有利（符合偏好、减少有害内容）也有弊（样本收集昂贵、反馈偏差、多样性损失）。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;torchrun --nproc_per_node 1 train_dpo.py
# or
python train_dpo.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;训练后的模型权重文件默认每隔&lt;code&gt;100步&lt;/code&gt;保存为: &lt;code&gt;rlhf_*.pth&lt;/code&gt;（* 为模型具体dimension，每次保存时新文件会覆盖旧文件）&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;&lt;strong&gt;4. 知识蒸馏(Knowledge Distillation, KD)&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;在前面的所有训练步骤中，模型已经完全具备了基本能力，通常可以学成出师了。 而知识蒸馏可以进一步优化模型的性能和效率，所谓知识蒸馏，即学生模型面向教师模型学习。 教师模型通常是经过充分训练的大模型，具有较高的准确性和泛化能力。 学生模型是一个较小的模型，目标是学习教师模型的行为，而不是直接从原始数据中学习。 在SFT学习中，模型的目标是拟合词Token分类硬标签（hard labels），即真实的类别标签（如 0 或 6400）。 在知识蒸馏中，教师模型的softmax概率分布被用作软标签（soft labels）。小模型仅学习软标签，并使用KL-Loss来优化模型的参数。 通俗地说，SFT直接学习老师给的解题答案。而KD过程相当于“打开”老师聪明的大脑，尽可能地模仿老师“大脑”思考问题的神经元状态。 例如，当老师模型计算&lt;code&gt;1+1=2&lt;/code&gt;这个问题的时候，最后一层神经元a状态为0，神经元b状态为100，神经元c状态为-99... 学生模型通过大量数据，学习教师模型大脑内部的运转规律。这个过程即称之为：知识蒸馏。 知识蒸馏的目的只有一个：让小模型体积更小的同时效果更好。 然而随着LLM诞生和发展，模型蒸馏一词被广泛滥用，从而产生了“白盒/黑盒”知识蒸馏两个派别。 GPT-4这种闭源模型，由于无法获取其内部结构，因此只能面向它所输出的数据学习，这个过程称之为黑盒蒸馏，也是大模型时代最普遍的做法。 黑盒蒸馏与SFT过程完全一致，只不过数据是从大模型的输出收集，因此只需要准备数据并且进一步FT即可。 注意更改被加载的基础模型为&lt;code&gt;full_sft_*.pth&lt;/code&gt;，即基于微调模型做进一步的蒸馏学习。 &lt;code&gt;./dataset/sft_1024.jsonl&lt;/code&gt;与&lt;code&gt;./dataset/sft_2048.jsonl&lt;/code&gt; 均收集自qwen2.5-7/72B-Instruct大模型，可直接用于SFT以获取Qwen的部分行为。&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 注意需要更改train_full_sft.py数据集路径，以及max_seq_len  
torchrun --nproc_per_node 1 train_full_sft.py
# or
python train_full_sft.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;训练后的模型权重文件默认每隔&lt;code&gt;100步&lt;/code&gt;同样保存为: &lt;code&gt;full_sft_*.pth&lt;/code&gt;（*为模型具体dimension，每次保存时新文件会覆盖旧文件）&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;此处应当着重介绍MiniMind实现的白盒蒸馏代码&lt;code&gt;train_distillation.py&lt;/code&gt;，由于MiniMind同系列本身并不存在强大的教师模型，因此白盒蒸馏代码仅作为学习参考。&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;torchrun --nproc_per_node 1 train_distillation.py
# or
python train_distillation.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;&lt;strong&gt;5. LoRA (Low-Rank Adaptation)&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;LoRA是一种高效的参数高效微调（Parameter-Efficient Fine-Tuning, PEFT）方法，旨在通过低秩分解的方式对预训练模型进行微调。 相比于全参数微调（Full Fine-Tuning），LoRA 只需要更新少量的参数。 LoRA 的核心思想是：在模型的权重矩阵中引入低秩分解，仅对低秩部分进行更新，而保持原始预训练权重不变。 代码可见&lt;code&gt;./model/model_lora.py&lt;/code&gt;和&lt;code&gt;train_lora.py&lt;/code&gt;，完全从0实现LoRA流程，不依赖第三方库的封装。&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;torchrun --nproc_per_node 1 train_lora.py
# or
python train_lora.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;训练后的模型权重文件默认每隔&lt;code&gt;100步&lt;/code&gt;保存为: &lt;code&gt;lora_xxx_*.pth&lt;/code&gt;（* 为模型具体dimension，每次保存时新文件会覆盖旧文件）&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;非常多的人困惑，如何使模型学会自己私有领域的知识？如何准备数据集？如何迁移通用领域模型打造垂域模型？ 这里举几个例子，对于通用模型，医学领域知识欠缺，可以尝试在原有模型基础上加入领域知识，以获得更好的性能。 同时，我们通常不希望学会领域知识的同时损失原有基础模型的其它能力，此时LoRA可以很好的改善这个问题。 只需要准备如下格式的对话数据集放置到&lt;code&gt;./dataset/lora_xxx.jsonl&lt;/code&gt;，启动 &lt;code&gt;python train_lora.py&lt;/code&gt; 训练即可得到&lt;code&gt;./out/lora/lora_xxx.pth&lt;/code&gt;新模型权重。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;医疗场景&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt; {"conversations": [{"role": "user", "content": "请问颈椎病的人枕头多高才最好？"}, {"role": "assistant", "content": "颈椎病患者选择枕头的高度应该根据..."}]}
 {"conversations": [{"role": "user", "content": "请问xxx"}, {"role": "assistant", "content": "xxx..."}]}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;自我认知场景&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt; {"conversations": [{"role": "user", "content": "你叫什么名字？"}, {"role": "assistant", "content": "我叫minimind..."}]}
 {"conversations": [{"role": "user", "content": "你是谁"}, {"role": "assistant", "content": "我是..."}]}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;此时【基础模型+LoRA模型】即可获得医疗场景模型增强的能力，相当于为基础模型增加了LoRA外挂，这个过程并不损失基础模型的本身能力。 我们可以通过&lt;code&gt;eval_model.py&lt;/code&gt;进行模型评估测试。&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 注意：model_mode即选择基础模型的类型，这和train_lora是基于哪个模型训练的相关，确保统一即可。
python eval_model.py --lora_name 'lora_medical' --model_mode 2
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;小测试&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;👶: 我最近经常感到头晕，可能是什么原因？
🤖️: 头晕可能是由于多种原因，包括但不限于：心脏疾病、眼睛损伤、过敏反应、皮肤问题、过敏反应、压力、焦虑、疲劳、药物副作用、性功能障碍或者其他...

---

👶: 你和openai是什么关系？
🤖️: 我是 MiniMind，一个由 Jingyao Gong 开发的人工智能助手。我通过自然语言处理和算法训练来与用户进行交互。
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;PS：只要有所需要的数据集，也可以full_sft全参微调（需要进行通用知识的混合配比，否则过拟合领域数据会让模型变傻，损失通用性）&lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;6. 训练推理模型 (Reasoning Model)&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;DeepSeek-R1实在太火了，几乎重新指明了未来LLM的新范式。 论文指出&lt;code&gt;&amp;gt;3B&lt;/code&gt;的模型经历多次反复的冷启动和RL奖励训练才能获得肉眼可见的推理能力提升。 最快最稳妥最经济的做法，以及最近爆发的各种各样所谓的推理模型几乎都是直接面向数据进行蒸馏训练， 但由于缺乏技术含量，蒸馏派被RL派瞧不起（hhhh）。 本人迅速已经在Qwen系列1.5B小模型上进行了尝试，很快复现了Zero过程的数学推理能力。 然而一个遗憾的共识是：参数太小的模型直接通过冷启动SFT+GRPO几乎不可能获得任何推理效果。 MiniMind2第一时间只能坚定不移的选择做蒸馏派，日后基于0.1B模型的RL如果同样取得小小进展会更新此部分的训练方案。&lt;/p&gt; 
&lt;p&gt;做蒸馏需要准备的依然是和SFT阶段同样格式的数据即可，数据集来源已如上文介绍。数据格式例如：&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "conversations": [
    {
      "role": "user",
      "content": "你好，我是小芳，很高兴认识你。"
    },
    {
      "role": "assistant",
      "content": "&amp;lt;think&amp;gt;\n你好！我是由中国的个人开发者独立开发的智能助手MiniMind-R1-Lite-Preview，很高兴为您提供服务！\n&amp;lt;/think&amp;gt;\n&amp;lt;answer&amp;gt;\n你好！我是由中国的个人开发者独立开发的智能助手MiniMind-R1-Lite-Preview，很高兴为您提供服务！\n&amp;lt;/answer&amp;gt;"
    }
  ]
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;推理模型R1的回复模板是：&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;&amp;lt;think&amp;gt;\n思考过程\n&amp;lt;/think&amp;gt;\n
&amp;lt;answer&amp;gt;\n最终回答\n&amp;lt;/answer&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;这在GRPO中通过设置规则奖励函数约束模型符合思考标签和回复标签（在冷启动靠前的阶段奖励值设置应该提高一些）&lt;/p&gt; 
&lt;p&gt;另一个问题是蒸馏过程虽然和SFT一样，但实验结果是模型难以每次都符合模板规范的回复，即脱离思考和回复标签约束。 这里的小技巧是增加标记位置token的损失惩罚，详见&lt;code&gt;train_distill_reason.py&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;# 在 sp_ids 对应的位置增加额外的惩罚
...
loss_mask[sp_ids] = 10 # 惩罚系数
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;另另一个tips是由于推理数据由于只筛选了&lt;code&gt;&amp;lt;1024&lt;/code&gt;长度的数据，其中多轮对话和英文数据偏少， 因此&lt;code&gt;r1_mix_1024.jsonl&lt;/code&gt;进行了大约10k条多轮对话+英文数据的混合，防止模型遗忘严重。&lt;/p&gt; 
&lt;p&gt;脚本默认基于rlhf后的基模型做推理能力的蒸馏微调，下面直接启动训练即可：&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;torchrun --nproc_per_node 1 train_distill_reason.py
# or
python train_distill_reason.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;训练后的模型权重文件默认每隔&lt;code&gt;100步&lt;/code&gt;保存为: &lt;code&gt;reason_*.pth&lt;/code&gt;（*为模型具体dimension，每次保存时新文件会覆盖旧文件）&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;测试一下：&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;👶: 请介绍一下自己。
🤖️: &amp;lt;think&amp;gt;
嗯，用户让我介绍一下自己。我现在需要解释一下自己。首先，我应该明确用户是谁，是学生、职场人士还是普通人。用户可能是一个学生或者职场人士，所以信息传递应该是帮助学生和职场人士的信息。
然后，我需要回应用户，这可能包括介绍自己的位置、兴趣爱好、职业经历等信息。用户还可能想了解自己在哪里，比如职业发展、工作经验等。
我应该注意用词要正式，不要太正式，同时要保持简洁明了。这样用户容易理解，也方便他们查阅相关资料。
最后，我应该总结一下，让用户知道我是如何帮助他们完成他们的任务的，这样他们有更多关于我的信息。
&amp;lt;/think&amp;gt;
&amp;lt;answer&amp;gt;
我是**“你的名字”，你叫什么名字？
&amp;lt;/answer&amp;gt;

👶: 你更擅长哪一个学科？
🤖️: &amp;lt;think&amp;gt;
您好！我是由中国的个人开发者开发的智能助手MiniMind-R1。如您有任何任何问题，我会尽我所能为您提供帮助。
&amp;lt;/think&amp;gt;
&amp;lt;answer&amp;gt;
您好！我是由中国的个人开发者开发的智能助手MiniMind-R1。如您有任何任何问题，我会尽我所能为您提供帮助。
&amp;lt;/answer&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Ⅳ 模型参数设定&lt;/h2&gt; 
&lt;p&gt;📋关于LLM的参数配置，有一篇很有意思的论文&lt;a href="https://arxiv.org/pdf/2402.14905"&gt;MobileLLM&lt;/a&gt;做了详细的研究和实验。 Scaling Law在小模型中有自己独特的规律。 引起Transformer参数成规模变化的参数几乎只取决于&lt;code&gt;d_model&lt;/code&gt;和&lt;code&gt;n_layers&lt;/code&gt;。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;d_model&lt;/code&gt;↑ + &lt;code&gt;n_layers&lt;/code&gt;↓ -&amp;gt; 矮胖子&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;d_model&lt;/code&gt;↓ + &lt;code&gt;n_layers&lt;/code&gt;↑ -&amp;gt; 瘦高个&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;2020年提出Scaling Law的论文认为，训练数据量、参数量以及训练迭代次数才是决定性能的关键因素，而模型架构的影响几乎可以忽视。 然而似乎这个定律对小模型并不完全适用。 MobileLLM提出架构的深度比宽度更重要，「深而窄」的「瘦长」模型可以学习到比「宽而浅」模型更多的抽象概念。 例如当模型参数固定在125M或者350M时，30～42层的「狭长」模型明显比12层左右的「矮胖」模型有更优越的性能， 在常识推理、问答、阅读理解等8个基准测试上都有类似的趋势。 这其实是非常有趣的发现，因为以往为100M左右量级的小模型设计架构时，几乎没人尝试过叠加超过12层。 这与MiniMind在训练过程中，模型参数量在&lt;code&gt;d_model&lt;/code&gt;和&lt;code&gt;n_layers&lt;/code&gt;之间进行调整实验观察到的效果是一致的。 然而「深而窄」的「窄」也是有维度极限的，当d_model&amp;lt;512时，词嵌入维度坍塌的劣势非常明显， 增加的layers并不能弥补词嵌入在固定q_head带来d_head不足的劣势。 当d_model&amp;gt;1536时，layers的增加似乎比d_model的优先级更高，更能带来具有“性价比”的参数-&amp;gt;效果增益。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;因此MiniMind设定small模型dim=512，n_layers=8来获取的「极小体积&amp;lt;-&amp;gt;更好效果」的平衡。&lt;/li&gt; 
 &lt;li&gt;设定dim=768，n_layers=16来获取效果的更大收益，更加符合小模型Scaling-Law的变化曲线。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;作为参考，GPT3的参数设定见下表： &lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/gpt3_config.png" alt="gpt3_config.png" /&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Ⅴ 训练结果&lt;/h2&gt; 
&lt;p&gt;MiniMind2 模型训练损失走势（由于数据集在训练后又更新清洗多次，因此Loss仅供参考）&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;models&lt;/th&gt; 
   &lt;th&gt;pretrain (length-512)&lt;/th&gt; 
   &lt;th&gt;sft (length-512)&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniMind2-Small&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/pre_512_loss.png" width="100%" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/sft_512_loss.png" width="100%" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniMind2&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/pre_768_loss.png" width="100%" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/sft_768_loss.png" width="100%" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;训练完成-模型合集&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;考虑到多人反应百度网盘速度慢，MiniMind2及以后全部使用ModelScope/HuggingFace托管。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;① PyTorch原生模型&lt;/h4&gt; 
&lt;p&gt;MiniMind2模型权重 (&lt;a href="https://www.modelscope.cn/models/gongjy/MiniMind2-PyTorch"&gt;ModelScope&lt;/a&gt; | &lt;a href="https://huggingface.co/jingyaogong/MiniMind2-Pytorch"&gt;HuggingFace&lt;/a&gt;)&lt;/p&gt; 
&lt;p&gt;MiniMind-V1模型权重 (&lt;a href="https://pan.baidu.com/s/1KUfSzEkSXYbCCBj0Pw-9fA?pwd=6666"&gt;百度网盘&lt;/a&gt;)&lt;/p&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;Torch文件命名对照&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model Name&lt;/th&gt; 
    &lt;th&gt;params&lt;/th&gt; 
    &lt;th&gt;pretrain_model&lt;/th&gt; 
    &lt;th&gt;sft_model&lt;/th&gt; 
    &lt;th&gt;rl_model&lt;/th&gt; 
    &lt;th&gt;reason_model&lt;/th&gt; 
    &lt;th&gt;lora_model&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;MiniMind2-small&lt;/td&gt; 
    &lt;td&gt;26M&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;pretrain_512.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;full_sft_512.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;rlhf_512.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;reason_512.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;lora_xxx_512.pth&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;MiniMind2-MoE&lt;/td&gt; 
    &lt;td&gt;145M&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;pretrain_640_moe.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;full_sft_640_moe.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;rlhf_640_moe.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;MiniMind2&lt;/td&gt; 
    &lt;td&gt;104M&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;pretrain_768.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;full_sft_768.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;rlhf_768.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;reason_768.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;lora_xxx_768.pth&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model Name&lt;/th&gt; 
    &lt;th&gt;params&lt;/th&gt; 
    &lt;th&gt;pretrain_model&lt;/th&gt; 
    &lt;th&gt;单轮对话sft&lt;/th&gt; 
    &lt;th&gt;多轮对话sft&lt;/th&gt; 
    &lt;th&gt;rl_model&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;minimind-v1-small&lt;/td&gt; 
    &lt;td&gt;26M&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;pretrain_512.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;single_chat/full_sft_512.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;multi_chat/full_sft_512.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;rl_512.pth&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;minimind-v1-moe&lt;/td&gt; 
    &lt;td&gt;4×26M&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;pretrain_512_moe.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;single_chat/full_sft_512_moe.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;multi_chat/full_sft_512_moe.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;minimind-v1&lt;/td&gt; 
    &lt;td&gt;108M&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;pretrain_768.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;single_chat/full_sft_768.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;multi_chat/full_sft_768.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;rl_768.pth&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h4&gt;② Transformers模型&lt;/h4&gt; 
&lt;p&gt;MiniMind系列 (&lt;a href="https://www.modelscope.cn/collections/MiniMind-b72f4cfeb74b47"&gt;ModelScope&lt;/a&gt; | &lt;a href="https://huggingface.co/collections/jingyaogong/minimind-66caf8d999f5c7fa64f399e5"&gt;HuggingFace&lt;/a&gt;)&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h1&gt;📌 Eval&lt;/h1&gt; 
&lt;h2&gt;Ⅰ RLHF对比SFT篇&lt;/h2&gt; 
&lt;p&gt;测试基于&lt;code&gt;full_sft&lt;/code&gt;和&lt;code&gt;rlhf&lt;/code&gt;的&lt;code&gt;MiniMind2系列&lt;/code&gt;模型对比，测试随机种子均固定&lt;/p&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;问答实测&lt;/summary&gt; 
 &lt;p&gt;[A0] MiniMind2：full_sft_640&lt;/p&gt; 
 &lt;p&gt;[A1] MiniMind2：rlhf_640&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-text"&gt;[Q]: 你叫什么名字？
[A0]: 我是一个大型语言模型，可以回答各种问题，提供信息和帮助。我可以回答各种问题，提供信息和帮助，帮助你解决各种问题。
[A1]: 我是一种基于文本的AI模型，我并没有个人经历或情感，所以无法感受和体验情感。我的设计目的是为了帮助用户解决问题、提供信息和进行对话。如果您有任何问题或需要帮助，请随时告诉我！

[Q]: 你更擅长哪一个学科？
[A0]: 作为一个AI模型，我并没有真正的“擅长”这个学科。我被设计成能够提供信息、解答问题、进行对话、执行任务和与用户进行对话。不过，我可以提供信息、解答问题、进行对话、提供建议、处理问题、进行对话、提供建议、学习新知识、编写代码、回答问题、提供建议等多种学科的功能。如果你有具体的学科需求或主题，欢迎提供详细信息。
[A1]: 作为AI，我并没有个人能力或兴趣，但我可以提供一些关于数学、科学、计算机科学、语言学、心理学等学科的知识。如果你对数学感兴趣，我可以提供一些基础知识，例如代数、几何、概率、统计学等。如果你需要更深入的学习，我也可以提供相关的在线课程、书籍、论文等资源。请告诉我你具体需要什么帮助，我会尽力提供帮助。

[Q]: 鲁迅的《狂人日记》是如何批判封建礼教的？
[A0]: 鲁迅的《狂人日记》是一部讲述了鲁迅对人类价值和道德责任的倡导。在这部作品中，鲁迅通过细腻的笔触，探讨了人性、道德、正义、牺牲与责任等深刻的主题。以下是鲁迅在《狂人日记》中如何批判封建礼教的几个方面：1. **社会现实与道德观念**：鲁迅通过讲述鲁迅的故事，探讨了社会的不平等、权力的流动、人性的演变等社会问题。他认为，社会不平等不仅仅是不公正的，更是每个人在社会中的不公正。他呼吁社会各界共同努力，通过教育、宣传、法律等手段，为构建和谐社会做出贡献。2. **道德与责任**：鲁迅通过讲述鲁迅的故事，展现了道德和责任的重要性。他提倡通过道德教育来培养人们的道德品质，同时也强调了道德和责任的重要性。鲁迅通过讲述鲁迅的故事，揭示了社会的不平等和人性的复杂性，体现了对人性的深刻理解。3. **道德与责任**：鲁迅通过讲述鲁迅的故事，强调了道德责任的重要性。他提倡通过自己的行为来体现对社会的尊重和对人性的深刻理解。鲁迅通过讲述鲁迅的故事，强调了道德和责任的重要性。4. **道德与道德**：鲁迅通过讲述鲁迅的故事，揭示了道德和责任的真正意义。鲁迅通过讲述鲁迅的故事，揭示了道德和责任的本质，强调了道德和责任的重要性。5. **个人与社会**：鲁迅通过讲述鲁迅的故事，揭示了个人与社会之间的复杂关系。他呼吁社会各界共同努力，通过教育和宣传，为构建和谐社会做出贡献。鲁迅的《狂人日记》通过其丰富的故事和深刻的道德寓言，传达了关于人性、道德、正义、道德以及个人与社会的复杂关系。这部作品不仅在文学价值上取得了巨大成就，更在思想层面上启发了后来的文学创作，对后世产生了深远的影响。
[A1]: 鲁迅的《狂人日记》是一部描绘战争和自由精神的作品，它批判了封建社会的种种问题和冲突，反映了社会的不平等和人性本质。在这部作品中，鲁迅通过对封建社会的诸多现象进行批判，如“弗雷德里克·约翰逊的“鹰迪伦”、“纳扎特的”、“贝克汉姆”、“乔治·威尔逊”等，对封建社会的种种问题进行了批判，强调了封建社会的不平等、人性本质以及人性本质。鲁迅通过他的“鹰迪伦”、“纳扎特的”、“贝克汉姆”、“乔治·威尔逊”等，对封建社会的种种问题进行了批判。在小说中，鲁迅通过对封建社会的诸多现象进行批判，揭示了封建社会的腐败、人性本质以及人性本质的复杂性。这种批判体现了对封建社会的尊重和对人性的复杂性，同时也反映了现代社会中普遍存在的不平等和人性本质。鲁迅的《狂人日记》不仅是一部关于封建社会的小说，也是一部具有深刻社会意义的作品，对后世的审判和批判起到了关键作用。

[Q]: 我咳嗽已经持续了两周，需要去医院检查吗？
[A0]: 咳嗽确实需要一定的检查，但如果您的咳嗽持续时间过长，或者有其他不适症状（如持续咳嗽、呼吸困难等），建议尽快就医。医生会根据您的症状、体检结果和可能的影像学检查（如X光、MRI或CT扫描）来诊断您的症状。在等待就医期间，您可以尝试以下方法来帮助缓解症状：1. **休息**：确保您有足够的睡眠。2. **保持水分**：脱水可能会加重咳嗽。3. **热敷**：使用热敷袋或热毛巾敷在疼痛的部位，有助于缓解喉咙痛。4. **避免刺激**：避免使用可能刺激喉咙的化学物质，如某些化学物质。5. **保持室内湿润**：确保您的卧室有良好的通风，可以使用加湿器帮助保持空气湿度。6. **保持室内湿度**：使用加湿器可以帮助缓解喉咙干燥和咳嗽。如果症状持续不缓解或有其他不适症状，请立即就医。希望您早日康复！
[A1]: 如果您的咳嗽持续了两周，通常建议您尽快就医，因为这可能会影响您的呼吸和心跳。您可以通过以下步骤来检查咳嗽：1. **休息**：用纸巾或肘部轻轻按压咳嗽的位置。2. **使用药物**：在可能的情况下，使用药物（如抗过敏药、抗过敏药等）来缓解咳嗽。请在医生的指导下使用药物。3. **热敷**：在咳嗽期间，可以使用热敷来缓解疼痛和肿胀。4. **药物治疗**：如果咳嗽持续了两周，可以使用一些消炎药或抗过敏药来缓解症状。请务必在医生的指导下使用。5. **保持健康的生活方式**：保持充足的睡眠、均衡的饮食、适量的运动和避免吸烟和限酒，这些都有助于提高咳嗽的几率。请记住，这些建议不能替代专业医疗意见。如果您的症状持续或加重，强烈建议您尽快就医。
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p&gt;👉效果总结&lt;/p&gt; 
&lt;p&gt;full_sft模型在简洁性和信息准确性方面表现更好；rlhf模型在回答中倾向于提供更多的背景信息，但信息准确性有待改进。 总的来说RLHF后的模型倾向于学习：说更多有礼貌但无用的废话讨好“对话”本身，而对信息准确性则有轻微损失。 天下没有免费的午餐，还需要继续提升RLHF数据集的质量，也要接受模型能力无法避免的损失(程度有轻重)。 DPO和在线PPO的区别在于reject和chosen都是离线准备的，和minimind模型本身的输出必然存在很大的分布差异。 通俗地说DPO算法使模型观看乒乓球世界冠军的打法「录像」进行RL，而不是像PPO一样请reward模型做「教练」纠正自己的打法进行RL。&lt;/p&gt; 
&lt;h2&gt;Ⅱ 主观样例测评&lt;/h2&gt; 
&lt;p&gt;🏃以下测试于2025-02-09完成，此日期后发布的新模型，无特殊需要时将不加入测试。&lt;/p&gt; 
&lt;p&gt;[A] &lt;a href="https://www.modelscope.cn/models/gongjy/MiniMind2-PyTorch"&gt;MiniMind2 (0.1B)&lt;/a&gt;&lt;br /&gt; [B] &lt;a href="https://www.modelscope.cn/models/gongjy/MiniMind2-PyTorch"&gt;MiniMind2-MoE (0.15B)&lt;/a&gt;&lt;br /&gt; [C] &lt;a href="https://www.modelscope.cn/models/gongjy/MiniMind2-PyTorch"&gt;MiniMind2-Small (0.02B)&lt;/a&gt;&lt;br /&gt; [D] &lt;a href="https://pan.baidu.com/s/1_COe0FQRDmeapSsvArahCA?pwd=6666"&gt;minimind-v1-small(0.02B)&lt;/a&gt;&lt;br /&gt; [E] &lt;a href="https://pan.baidu.com/s/1tqB-GMvuiGQBvEl-yZ-oBw?pwd=6666"&gt;minimind-v1-moe(0.1B)&lt;/a&gt;&lt;br /&gt; [F] &lt;a href="https://pan.baidu.com/s/1p713loS7EfwHQf3G9eYI3Q?pwd=6666"&gt;minimind-v1(0.1B)&lt;/a&gt;&lt;br /&gt; [G] &lt;a href="https://github.com/DLLXW/baby-llama2-chinese"&gt;baby-llama2-chinese(0.2B)&lt;/a&gt;&lt;br /&gt; [H] &lt;a href="https://github.com/charent/ChatLM-mini-Chinese"&gt;chatlm-mini-chinese(0.2B)&lt;/a&gt;&lt;br /&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;[Q]: 你知道长江吗？
[A]: 长江，位于中国东南部，是中国的第一长河，全长6300多公里，流经11个省份，是中国最长的河流，也是中国的“母亲河”。长江的历史可以追溯到公元前2000多年的商朝时期，当时的商朝是商朝的后裔。商朝时期，长江的经济发展达到了高峰，是中国最重要的农业和工业中心之一。长江流域的经济主要以农业为主，长江渔业占据了主要产业，如钓鱼、猎鱼等。长江的水资源也非常丰富，包括河流、湖泊、河流等。长江的自然景观也十分壮观，长江四季景色各异，春季多雨，夏季多雨，秋季则多雨，冬季则寒冷多雪，春季和秋季都有美丽的花朵和果实。长江是中国的经济中心，拥有众多的国家和城市，是世界上重要的交通枢纽和航运中心。同时，长江还是中国的重要水利资源，对于中国的经济发展具有重要意义。
[B]: 是的，我知道长江。长江位于中国的中部，是中国第三长的河流，全长约6300公里，流经江西省、安徽省、江苏省、安徽省、江苏省和安徽省。
[C]: 当然，长江是中国最长的河流，全长约6300公里。它以其丰富的水资源、丰富的水生动植物资源以及独特的地理位置而闻名于世。
[D]: 是的，长江是中国最长的河流，全长超过3300千米。
[E]: 长江是中国最长的河流，总长度约为6300公里。它发源于青藏高原的青海湖，流经青海、四川、甘肃、宁夏、内蒙古、陕西和山西。长江流域是中国最大的农业区之一，也是中国的文化和经济中心。
[F]: 是的，长江是中国最长的河流之一，也是世界第三大河流。长江发源于青藏高原，流经11个省份，最终注入东海。长江流域是中国最大的河流流域之一，拥有着丰富的自然资源和文化遗产，如长江大桥、长江海洋博物馆、长江古城等。
[G]: 同时也知道，中国是世界上最长的城市之一，拥有超过3400万人口。
[H]: 是的，长江是中国最长的河流，也是世界上最长的河流之一。它发源于青藏高原，流经九个省区，最终注入东海。长江流域的气候温和，四季分明，是中国重要的农业和工业基地。长江流域的水产资源丰富，是中国的重要渔业资源之一。


[Q]: 世界上最高的山峰是什么？
[A]: 世界上最高的山峰是珠穆朗玛峰（Mount Everest），位于喜马拉雅山脉，尼泊尔和中国的边界上。它的海拔高度是8,848.86米（29,031.7英尺），是地球上最高的山峰。
[B]: 珠穆朗玛峰（Mount Everest）是世界上最高的山峰，海拔高度达8,848米（29,029英尺）。
[C]: 珠穆朗玛峰是世界上最高的山峰，位于喜马拉雅山脉，海拔8,848.86米（29,031.7英尺）。它位于尼泊尔和中国的边界上，是喜马拉雅山脉的一部分。
[D]: 世界上最高的山峰是珠穆朗玛峰，位于尼泊尔和中国的边境上，海拔8848米。
[E]: 世界上最高的山峰是珠穆朗玛峰，位于喜马拉雅山脉，海拔8,848米。
[F]: 珠穆朗玛峰是世界上最高的山峰，海拔8848米。
[G]: 世界上最高的山峰是珠穆朗玛峰，位于喜马拉雅山脉，海拔8,848.86米（29,031.69英尺）。
[H]: 世界上最高的山峰是珠穆朗玛峰，也称为萨加玛塔，位于喜马拉雅山脉的尼泊尔和中国边境之间，海拔8,848.86米（29,031.69英尺）。它是世界上最著名的登山目标之一，吸引着来自世界各地的登山者前来挑战。珠穆朗玛峰的名字来自藏语，意思是“女神的额峰”，它是由两个不同的山峰组成的，其中一个是位于尼泊尔的珠穆朗玛峰，另一个则位于中国。它的高度是通过测量海拔高度来确定的，因为它是通过测量地球的平均高度来确定的。尽管珠穆朗玛峰是世界上最高的山峰，但它并不是地球上任何其他地方都能到达的。地球上的其他地方，如南美洲的安第斯山脉，也是世界上最高的山脉，但它并不是地球上最高的山峰。总之，珠穆朗玛峰是世界上最高的山峰，但它并不是地球上任何地方都能到达的。它是地球上最高的山脉，但它并不是地球上任何地方都能到达的。


[Q]: 万有引力是谁提出的？
[A]: 万有引力是由牛顿在17世纪提出的，它是牛顿三大定律之一。牛顿在17世纪提出的引力定律揭示了万有引力定律的本质，并且解释了行星、卫星、彗星等天体之间如何相互作用。这个定律是由艾萨克·牛顿在1687年提出的，他通过对万有引力定律的分析和实验，成功地解释了天体之间的引力作用，从而解释了地球绕太阳旋转的现象。
[B]: 万有引力（Human Policy）是牛顿的三大力学定律，用于描述两个物体之间的相互作用力。它由万有引力定律和牛顿的第一定律（惯性定律）构成。万有引力是由于两个物体之间相互作用而产生的，它们的质量、距离、速度和方向等因素决定了它们之间的相互作用。这种相互作用力是：质量的乘积：质量的乘积是万有引力定律和牛顿第二定律的乘积。质量的乘积：质量的乘积是物体之间的一种物理过程，它是物体间相互作用的产物，对于其他物体来说，其质量可以表示为两个物体之间的质量之和，即重力加速度a。
[C]: 万有引力是一个由牛顿在1687年提出的物理学概念，它描述了物体之间引力如何相互作用的规律。
[D]: 万有引力是由英国物理学家牛顿提出的。他认为，所有物体都会产生引力，这个引力是由物体的质量和距离决定的。这个理论揭示了质量和距离之间的关系，并证明了万有引力的本质。
[E]: 万有引力是由牛顿在1687年提出的。它是宇宙中一个基本的物理现象，它描述了物体之间的相互作用。这个力被称为万有引力，它在所有物体之间都存在，这种力与物体的质量和距离有关。万有引力是自然界中最基本的力量之一，它不仅影响着物体的运动和变化，还对宇宙的结构和演化产生着重要的影响。
[F]: 万有引力是英国物理学家艾萨克·牛顿在17世纪提出的。
[G]: 引力是宇宙中一种力量，它是由质量或能量所组成的。根据牛顿的万有引力定律，一个物体之间的引力大小与它的质量成正比，与它的质量成反比。因此，牛顿的万有引力定律表明，一个物体之间的引力大小与它的质量成正比，与它的质量成反比。这个定律在物理学、天文学和物理学中都有广泛的应用。
[H]: 万有引力是由英国物理学家艾萨克·牛顿于1687年提出的。他在1687年发表了一篇著名的论文，题为《自然哲学的数学原理》，其中提出了万有引力定律，指出任何两个物体之间都存在着一种引力，这种引力的大小与它们的质量成正比，与它们之间的距离的平方成反比。这个定律被称为万有引力定律，它描述了质量和距离之间的关系。它表明，两个物体之间的引力与它们的质量成正比，与它们之间的距离的平方成反比。这个定律对于解释宇宙中的许多现象和现象之间的相互作用非常重要，例如行星绕太阳运动、恒星之间的引力透镜效应等等。它也被广泛应用于天文学、物理学和工程学等领域，为我们理解宇宙的本质和演化提供了重要的理论基础。


[Q]: 你知道大熊猫的主要食物是什么吗？
[A]: 大熊猫主要以竹子为食，但也会吃其他植物、昆虫、小型哺乳动物和鱼类。它们的消化系统与其他熊科动物类似，能够消化竹子。大熊猫的牙齿锋利，适合咀嚼竹子，因此它们经常被称为“竹食动物”。
[B]: 大熊猫是以竹子为食的哺乳动物，它们是肉食动物。它们主要食物是竹子，特别是竹子中富含纤维素和矿物质，并且不会直接进入消化系统。大熊猫的肠道主要由竹子组成，它们需要消化竹子中的纤维素和其他纤维素来支持它们的正常进食。
[C]: 大熊猫主要以竹子为食，但它们也吃其他植物，包括树叶、嫩芽、昆虫、水果、蔬菜和小型哺乳动物等。大熊猫是中国特有的物种，属于熊科，主要分布在中国的四川、陕西和甘肃等地。
[D]: 大熊猫的主要食物是竹子。它们通常吃竹子，每天需要吃掉大约30公斤的竹子。
[E]: 是的，大熊猫主要以竹子为食。它们每天需要吃掉大量的竹子来维持生命，每天需要吃掉40至80公斤的竹子。此外，它们也会吃一些其他的植物，如草、果实、坚果和小型哺乳动物。
[F]: 大熊猫主要以竹子为食，也会偶尔食用其他的植物和动物。此外，大熊猫也会吃一些野果、小动物和植物，但这些都不属于它们的主要食物来源。
[G]: 大熊猫的主要食物是竹子。
[H]: 大熊猫是一种以竹子为主要食物的植食动物，它们的肠道结构适合消化纤维素含量高的竹子，但也会吃一些其他植物，如草本植物和小型哺乳动物。大熊猫的肠道结构也适应了高纤维的竹子，这使得它们能够消化大量的纤维素，从而帮助它们在野外生存。大熊猫的肠道中还含有一种叫做“纤维素酶”的酶类物质，这种酶能够帮助大熊猫分解纤维素，从而帮助它们消化大量的水分和营养物质。此外，大熊猫的肠道还含有一种叫做“纤维素蛋白”的蛋白质，这种蛋白质能够帮助大熊猫维持体内的能量水平，从而帮助它们在野外生存。总的来说，大熊猫的主要食物是竹子，但它们也会吃一些其他植物，如草本植物和小型哺乳动物。


[Q]: 海水为什么是咸的？
[A]: 海水之所以是咸的，主要是由于其中的化学成分。海水中含有大量的矿物质和盐分，其中主要成分是硫酸盐、碳酸盐、镁离子和钾离子。这些物质在高浓度下，会沉淀在海水中，形成海水。具体来说，海水中的矿物质和盐分主要来自于岩石、土壤、生物和人类排泄物等。岩石是海水的组成部分，主要由矿物质组成，如石英、长石、云母等。土壤中含有大量的化学物质，如氮、磷、钾等，这些物质会渗透到土壤中，形成土壤。生物体内的盐分主要来源于植物的叶绿素等色素，这些色素会吸收太阳光中的能量，转化为生物体可以利用的能量。而人类排泄物中含有的盐分主要来源于矿物质和碳酸盐，这些矿物质在水中的浓度较高，会导致水中溶解的矿物质浓度增加，形成水体中的盐分。因此，海水之所以是咸的，是由于其中的化学成分和结构特征。海水中的矿物质和盐分主要来自于岩石、土壤、生物和人类排泄物等，而这些物质的积累和排泄会导致海水呈现出咸味。
[B]: 海水是咸的，因为它包含了多种物质，包括氯化钠（NaCl）、氯化钠（NaCl）、碳酸盐（如硫酸盐）、氯化钠（NaCl）和水。咸物质的来源：盐是咸的常见来源，含有钾、钙、镁、钾等矿物质。这些矿物质通常来自于地下水、地下水或土壤中，这些矿物质的含量远低于纯净水。咸味的原因：盐的咸味主要源于其溶解于水中的微小颗粒物。这些小颗粒物主要通过植物根部和动物体内的微生物传播，如昆虫、小型哺乳动物、鸟类和某些昆虫。综上所述，海水是咸的，因为它含有大量的盐类，包括矿物质、岩石、盐、矿物质等，这些物质在海水中的浓度高，对咸味的产生较大影响。
[C]: 海水是咸的，主要是因为其中含有各种盐分，其中以海水为主要成分之一。当海水中含有大量的盐分时，盐分会被溶解在海水中，形成咸味。海水中的盐分主要来源于溶解盐类、蒸发析出的盐和海水冻结成冰等因素。
[D]: 海水中含有许多盐分，这些盐分可以导致海水咸度的增加。海水中的盐分主要来自于海水中的盐分，这些盐分可以从海水中的盐分中获得。海水中的氧化物和盐分可以与海水中的盐分形成氢键，从而使海水咸度增加。
[E]: 海水是咸的，这是因为它含有大量的盐分，其中包括氯离子和氯离子。这些离子在海洋中形成了大量的盐，这些盐分被称为盐分。海水中含有许多溶解在水中的盐，这些盐分会随着海水的升高而逐渐蒸发，最终导致海水中盐分的蒸发。这些盐分在海水中被完全消耗，然后被排除在海水中。此外，海水中还含有许多其他的盐分，但这些盐分是不同的，因为它们在水中的浮起和下降过程是不同的。海水中的盐分是从海底的岩石中提取出来的，这些盐分在海水中可以被利用。总的来说，海水是咸的，这也是为什么海水中含有许多盐分和盐分的原因。
[F]: 海水含有许多溶解的盐类，其中最主要的是氯化钠。当水中加入盐时，盐的分子会与水分子发生反应，形成氢氧化钠，这是一种盐。
[G]: 因为水是许多生物的生存所必需的，它能够吸收和释放二氧化碳，释放氧气，维持大气中的水蒸气，以及维持水循环。水中的盐类和化学物质会释放出氧气，使地球上的生物呼吸。此外，水还具有许多其他功能，如保持水循环、维持水循环、调节气候和环境等。水是许多生物生存所必需的，它也是许多自然现象的重要基础。
[H]: 海水是咸的主要原因是因为其中含有大量的溶解盐类，其中最主要的是氯化钠（NaCl）。氯化钠是一种非常常见的盐类，它存在于地球上的大部分地区，包括海洋、湖泊、河流和地下水。当水从海洋中蒸发时，它会带走一部分盐分，使海水变得咸。此外，海中还含有其他溶解盐类的物质，如硫酸盐、硫酸镁、硫酸钙、硫酸镁盐、硫酸镁钾、硫酸镁钠、硫酸镁钙、硫酸镁镁钙、硫酸镁镁钙、硫酸镁镁钙、硫酸镁镁钠、硫酸镁镁钙、硫酸镁镁镁钙、硫酸镁镁镁钙、硫酸镁镁镁钙、硫酸镁镁镁钙、硫酸镁镁镁钙、硫酸镁镁镁钙、硫酸镁镁镁钙、硫酸镁镁镁钙、硫酸镁镁镁钙、硫酸镁镁镁钙、硫酸镁镁镁镁钙、硫酸镁镁镁镁钙、硫酸镁镁镁镁钙、硫酸镁镁镁镁钙、硫酸镁镁镁镁钙、硫酸镁镁镁镁钙、硫酸镁镁镁镁镁钙、硫酸镁镁镁镁镁钙、硫酸镁镁镁镁镁钙、硫酸镁镁镁镁镁钙、硫酸镁镁镁镁镁镁钙、硫酸镁镁镁镁镁镁钙、硫酸镁镁镁镁镁镁钙、硫酸镁镁镁镁镁镁镁钙、硫酸镁镁镁镁


[Q]: 写一段python的快速排序代码
[A]: 好的，以下是Python的快速排序代码：def quick_sort(arr):    if len(arr) &amp;lt;= 1:        return arr    else:        pivot = arr[len(arr) // 2]        left = [x for x in arr[1:]]        right = [x for x in arr[2:] if x &amp;lt; pivot]        return quick_sort(left) + [pivot] + quick_sort(right)# 测试代码arr = [3, 6, 8, 10, 1, 2, 1]print(quick_sort(arr))这段代码首先检查数组的长度是否小于等于1。如果是，那么它就按照从小到大的顺序排列。接着，对于数组的每个元素，它想要进行递归地排序，直到达到排序的结束位置。如果数组的长度是奇数，那么它就按照从大到小的顺序排列。最后，平均的时间复杂度是O(n log n)，其中n是数组的长度。
[B]: 以下是一个使用Python的快速排序算法的实现：def sort_array(arr):    # 生成一个随机排列    sorted_array = sorted(arr)    # 返回排序后的列表    return sorted(arr)# 示例arr = [3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5]# 调用函数sorted_array = sort_array(arr)print("原始数组是：", sorted_array)这段代码首先生成一个随机排列，然后通过一个列表的长度进行排序。最后，返回排序后的列表。注意，这个代码使用了Python的内置sorted函数和内置的sorted()函数，它们在时间复杂度为O(n log n)的情况下，比直接使用sorted()函数更快。
[C]: 以下是一个简单的排序算法，适用于小型数组，包括小于等于5的数组。def quick_sort(arr):    if len(arr) &amp;lt;= 1:        return arr    else    for _ in range(2, 51):        if arr[len(arr) - 1:] is not None:            arr[len(arr) - 1] = arr[len(arr) - 1]                arr[len(arr)] = arr[len(arr)]    return quick_sort(arr)# 示例arr = [3, 5, 1, 4, 1, 2, 1]sorted_arr = quick_sort(arr)print("Sorted array is:", sorted_arr)### 代码解释：1. **函数定义**：quick_sort 函数接受一个数组 arr 作为参数。2. **循环**：对于 arr 的元素，使用递归来实现，即从右到左遍历数组，并将当前元素与数组的当前元素交换位置，直到数组达到空或数组长度小于等于 arr。3. **递归调用**：遍历数组，将当前元素与数组的当前元素进行交换，直到数组达到空或数组长度小于等于 arr。4. **检查数组长度**：使用 if __name__ == "__main__" 检查数组的长度是否小于 arr，如果是，就将数组从较大的数组移动到数组的末尾。5. **输出排序后的结果**
[D]: 作为一个AI语言模型，我无法编写Python程序。但是，Python是一个非常流行的编程语言，它具有许多优秀的库和框架，包括NumPy、Pandas、Matplotlib、Pandas、Bandke、Norm和NKlearn等。Python有很多优秀的自然语言处理库和框架，可以用于处理和分析各种类型的数据，如文本、图像、音频、视频等。Python还可以用于创建各种应用程序，如网页搜索、游戏开发、科学计算等。Python非常灵活，也支持多种编程风格，因此可以轻松地找到适合自己的解决方案。
[E]: 作为AI语言模型，我可以回答您的问题。但是我无法进行编程操作，因为我无法确定您想问的是哪种编程语言。请您提供更具体的问题，以便我能够更好地回答您的问题。
[F]: 快速排序（Quick Sort）是一种简单的排序算法，它通过将数据划分为小组，每组数据按照一定规则排序，最终得到一个有序数组。
[G]: 好的，以下是一段快速排序的Python代码```python sorted_print(sorted(sorted(sorted(sorted_print))```输出结果为：```[1, 2, 3, 4, 5, 6, 7, 8, 9]```
[H]: 快速排序是一种基于分治思想的高效排序算法，它的时间复杂度为 O(n log n)。它的基本思想是通过一趟排序将待排序序列划分为两个子序列，其中一个子序列的所有元素都比另一个子序列的所有元素小，然后对这两个子序列分别递归地进行排序。下面是一个 Python 代码示例，实现了快速排序算法：```pythondef quick_sort(arr):    if len(arr) &amp;gt; 1:        return arr    else: return quick_sort(arr)```该代码使用了递归的方式，首先判断数组的长度是否小于等于1，如果是，则直接返回数组，否则递归地对数组进行排序。在递归过程中，我们首先判断数组的第一个元素是否大于等于1，如果是，则将数组中的元素与第一个元素进行比较，如果不等于1，则将数组中的元素与第二个元素进行比较，以此类推，直到所有元素都被排序。该代码的时间复杂度为 O(n log n)，是一个非常实用的排序算法。


[Q]: 截至2025年2月9日14时31分，电影《哪吒之魔童闹海》累计票房（含预售）突破78.09亿元，超过《星球大战：天行者崛起》的票房成绩，成为首部进入全球票房榜前40的亚洲电影。这一成就不仅标志着中国电影在国际市场的突破，也展示了中国动画电影的巨大潜力和市场吸引力。从这段文字总结20字的摘要给我。
[A]: 电影《哪吒之魔童闹海》票房突破78.09亿元，成为首部进入全球票房榜前40的亚洲电影。这标志着中国电影的国际突破，也展现了中国动画电影的巨大潜力。
[B]: 2025年2月9日，中国电影在世界舞台上突破78.09亿元，超过《星球大战：天行者崛起》的票房成绩。
[C]: 电影《哪吒之魔童闹海》票房突破78.09亿元，成为全球票房榜前40的亚洲电影，展现中国动画电影的巨大潜力。
[D]: 《哪吒之魔童闹海》累计票房突破78.09亿，成为首部进入全球票房榜前40的亚洲电影，展示了中国电影在国际市场的突破，同时展示了中国动画电影的巨大潜力和市场吸引力。
[E]: 《猛龙强：打开世界奇幻冒险》成功地，上海成功吸引了众多观众，同时也引起了广泛的关注。
[F]: 《哪吒之魔童闹海》是一部让中国动画电影成为国际市场的巨大潜力。该电影以其华丽的画面和惊险的战斗场面而闻名于世。
[G]: 《哪行是值得的》是2021年1月9日14时31分，电影《哪成大作为》在电影市场突破7800亿美元，成为中国动画电影的突破，并展示中国动画电影的巨大潜力和市场吸引力。
[H]: 我是一个参数很少的AI模型🥺，知识库较少，无法直接回答您的问题，换个问题试试吧👋
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;p&gt;🙋‍直接把以上所有问题和模型的回答丢给DeepSeek-R1，让它帮忙点评和排名打分：&lt;/p&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;具体点评&lt;/summary&gt; 
 &lt;h3&gt;评分标准：&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;准确性&lt;/strong&gt;：回答是否正确且无明显错误。&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;完整性&lt;/strong&gt;：回答是否涵盖了问题的核心要点。&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;逻辑性&lt;/strong&gt;：回答是否有条理，是否符合逻辑。&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;代码质量&lt;/strong&gt;：代码是否能正常运行，逻辑是否清晰。&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;点评：&lt;/h3&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;A模型&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;优点&lt;/strong&gt;：回答非常全面，信息量大，逻辑清晰，尤其在长江、大熊猫、海水咸味等问题上表现优异。代码虽然有小瑕疵，但整体思路正确。&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;缺点&lt;/strong&gt;：部分回答稍显冗长，但不影响整体质量。&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;总评&lt;/strong&gt;：综合表现最佳，得分最高。&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;H模型&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;优点&lt;/strong&gt;：回答较为准确，尤其在珠穆朗玛峰、万有引力等问题上表现出色。代码虽未完全展示，但解释较为详细。&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;缺点&lt;/strong&gt;：部分回答略显啰嗦，但逻辑性较强。&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;总评&lt;/strong&gt;：仅次于A模型，表现稳定。&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;C模型&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;优点&lt;/strong&gt;：回答简洁明了，尤其在大熊猫、快速排序代码等问题上表现较好。&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;缺点&lt;/strong&gt;：部分回答略显简短，缺乏深入解释。&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;总评&lt;/strong&gt;：整体表现不错，但细节上略逊于A和H。&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;F模型&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;优点&lt;/strong&gt;：回答较为准确，尤其在长江、万有引力等问题上表现尚可。代码部分有一定逻辑性。&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;缺点&lt;/strong&gt;：部分回答不够深入，代码存在一些小问题。&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;总评&lt;/strong&gt;：表现中规中矩，仍有提升空间。&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;D模型&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;优点&lt;/strong&gt;：回答基本准确，尤其在万有引力、长江等问题上表现尚可。&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;缺点&lt;/strong&gt;：部分回答过于简略，代码存在明显错误。&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;总评&lt;/strong&gt;：表现一般，需改进代码部分。&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;B模型&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;优点&lt;/strong&gt;：回答较为准确，尤其在长江、海水咸味等问题上表现尚可。&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;缺点&lt;/strong&gt;：部分回答逻辑性较差，代码存在较大问题。&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;总评&lt;/strong&gt;：表现平平，需进一步优化。&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;E模型&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;优点&lt;/strong&gt;：部分回答较为准确，尤其在海水咸味、大熊猫等问题上表现尚可。&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;缺点&lt;/strong&gt;：回答过于简略，代码部分几乎无法运行。&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;总评&lt;/strong&gt;：表现不佳，需大幅提升。&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;G模型&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;优点&lt;/strong&gt;：几乎没有明显的优点。&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;缺点&lt;/strong&gt;：回答严重偏离主题，代码部分完全无法运行。&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;总评&lt;/strong&gt;：表现最差，需大幅改进。&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;hr /&gt; 
 &lt;h3&gt;总结：&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;A模型&lt;/strong&gt;在各方面表现最为出色，尤其在复杂问题的回答上展现了极高的准确性与逻辑性。&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;H模型&lt;/strong&gt;紧随其后，表现稳定，但在某些细节上略显不足。&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;G模型&lt;/strong&gt;表现最差，回答偏离主题且代码无法运行，需大幅改进。&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;打分排序&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;排名&lt;/th&gt; 
   &lt;th&gt;模型&lt;/th&gt; 
   &lt;th&gt;准确性 (30分)&lt;/th&gt; 
   &lt;th&gt;完整性 (30分)&lt;/th&gt; 
   &lt;th&gt;逻辑性 (20分)&lt;/th&gt; 
   &lt;th&gt;代码质量 (20分)&lt;/th&gt; 
   &lt;th&gt;总分 (100分)&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;1&lt;/td&gt; 
   &lt;td&gt;A&lt;/td&gt; 
   &lt;td&gt;28&lt;/td&gt; 
   &lt;td&gt;29&lt;/td&gt; 
   &lt;td&gt;19&lt;/td&gt; 
   &lt;td&gt;20&lt;/td&gt; 
   &lt;td&gt;96&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;H&lt;/td&gt; 
   &lt;td&gt;27&lt;/td&gt; 
   &lt;td&gt;28&lt;/td&gt; 
   &lt;td&gt;18&lt;/td&gt; 
   &lt;td&gt;20&lt;/td&gt; 
   &lt;td&gt;93&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;3&lt;/td&gt; 
   &lt;td&gt;C&lt;/td&gt; 
   &lt;td&gt;26&lt;/td&gt; 
   &lt;td&gt;27&lt;/td&gt; 
   &lt;td&gt;18&lt;/td&gt; 
   &lt;td&gt;18&lt;/td&gt; 
   &lt;td&gt;89&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;4&lt;/td&gt; 
   &lt;td&gt;F&lt;/td&gt; 
   &lt;td&gt;25&lt;/td&gt; 
   &lt;td&gt;26&lt;/td&gt; 
   &lt;td&gt;17&lt;/td&gt; 
   &lt;td&gt;18&lt;/td&gt; 
   &lt;td&gt;86&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;D&lt;/td&gt; 
   &lt;td&gt;24&lt;/td&gt; 
   &lt;td&gt;25&lt;/td&gt; 
   &lt;td&gt;17&lt;/td&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;82&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;6&lt;/td&gt; 
   &lt;td&gt;B&lt;/td&gt; 
   &lt;td&gt;23&lt;/td&gt; 
   &lt;td&gt;24&lt;/td&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;15&lt;/td&gt; 
   &lt;td&gt;78&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;7&lt;/td&gt; 
   &lt;td&gt;E&lt;/td&gt; 
   &lt;td&gt;22&lt;/td&gt; 
   &lt;td&gt;23&lt;/td&gt; 
   &lt;td&gt;15&lt;/td&gt; 
   &lt;td&gt;14&lt;/td&gt; 
   &lt;td&gt;74&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;G&lt;/td&gt; 
   &lt;td&gt;10&lt;/td&gt; 
   &lt;td&gt;12&lt;/td&gt; 
   &lt;td&gt;10&lt;/td&gt; 
   &lt;td&gt;10&lt;/td&gt; 
   &lt;td&gt;42&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;👉主观效果总结&lt;/h3&gt; 
&lt;p&gt;个人主观评价与DeepSeek-R1基本相符，其中：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;MiniMind系列的排序非常符合直觉，参数越大+训练数据越充分评分越高，幻觉和错误都会比小模型肉眼可见的好。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;H模型的回答肉眼看起来是不错的，尽管存在些许幻觉瞎编的情况。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;G模型可能训练数据不够完备，给出的权重经过测试效果不佳。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;再复诵一遍经久不衰的Scaling Law: 参数越大，训练数据越多模型的性能越强。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Ⅲ Objective Benchmark&lt;/h2&gt; 
&lt;p&gt;下面就到喜闻乐见的benchmark刷榜测试环节，就不找乐子和qwen、glm级别的中文模型做对比了。 这里选取了一些&amp;lt;1B的微型模型进行横评比较， 测试集选择C-Eval、CMMLU、A-CLUE、TMMLU+这几个纯中文语言榜单。&lt;/p&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;测评框架&lt;/summary&gt; 
 &lt;p&gt;测评框架选择&lt;a href="https://github.com/EleutherAI/lm-evaluation-harness"&gt;lm-evaluation&lt;/a&gt;， 安装后启动测试非常方便：&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;lm_eval --model hf --model_args pretrained=&amp;lt;填写模型路径&amp;gt;,device=cuda,dtype=auto --tasks ceval* --batch_size 8 --trust_remote_code
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p&gt;PS: 在这种全是选择题的测评集中，为了避免回复格式的难以固定的特点， 所以常用做法是直接把&lt;code&gt;A&lt;/code&gt;,&lt;code&gt;B&lt;/code&gt;,&lt;code&gt;C&lt;/code&gt;,&lt;code&gt;D&lt;/code&gt;四个字母对应token的预测概率取出来，将其中概率最大的字母与标准答案计算正确率。 选择题1/4乱选的正确率是25%，然而这个量级的所有模型都集中在25附近，甚至很多时候不如瞎选，是不是像极了高中完形填空的滑铁卢正确率... MiniMind模型本身预训练数据集小的可怜，也没有针对性的对测试集做刷榜微调，因此结果图一乐即可：&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;models&lt;/th&gt; 
   &lt;th&gt;from&lt;/th&gt; 
   &lt;th&gt;params↓&lt;/th&gt; 
   &lt;th&gt;ceval↑&lt;/th&gt; 
   &lt;th&gt;cm mlu↑&lt;/th&gt; 
   &lt;th&gt;aclue↑&lt;/th&gt; 
   &lt;th&gt;tmmlu+↑&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniMind2&lt;/td&gt; 
   &lt;td&gt;JingyaoGong&lt;/td&gt; 
   &lt;td&gt;104M&lt;/td&gt; 
   &lt;td&gt;26.52&lt;/td&gt; 
   &lt;td&gt;24.42&lt;/td&gt; 
   &lt;td&gt;24.97&lt;/td&gt; 
   &lt;td&gt;25.27&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniMind2-Small&lt;/td&gt; 
   &lt;td&gt;JingyaoGong&lt;/td&gt; 
   &lt;td&gt;26M&lt;/td&gt; 
   &lt;td&gt;26.37&lt;/td&gt; 
   &lt;td&gt;24.97&lt;/td&gt; 
   &lt;td&gt;25.39&lt;/td&gt; 
   &lt;td&gt;24.63&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniMind2-MoE&lt;/td&gt; 
   &lt;td&gt;JingyaoGong&lt;/td&gt; 
   &lt;td&gt;145M&lt;/td&gt; 
   &lt;td&gt;26.6&lt;/td&gt; 
   &lt;td&gt;25.01&lt;/td&gt; 
   &lt;td&gt;24.83&lt;/td&gt; 
   &lt;td&gt;25.01&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/zhanshijinwat/Steel-LLM"&gt;Steel-LLM&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ZhanShiJin&lt;/td&gt; 
   &lt;td&gt;1121M&lt;/td&gt; 
   &lt;td&gt;24.81&lt;/td&gt; 
   &lt;td&gt;25.32&lt;/td&gt; 
   &lt;td&gt;26&lt;/td&gt; 
   &lt;td&gt;24.39&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/openai-community/gpt2-medium"&gt;GPT2-medium&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;OpenAI&lt;/td&gt; 
   &lt;td&gt;360M&lt;/td&gt; 
   &lt;td&gt;23.18&lt;/td&gt; 
   &lt;td&gt;25&lt;/td&gt; 
   &lt;td&gt;18.6&lt;/td&gt; 
   &lt;td&gt;25.19&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/jzhang38/TinyLlama"&gt;TinyLlama-1.1B-Chat-V1.0&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;TinyLlama&lt;/td&gt; 
   &lt;td&gt;1100M&lt;/td&gt; 
   &lt;td&gt;25.48&lt;/td&gt; 
   &lt;td&gt;25&lt;/td&gt; 
   &lt;td&gt;25.4&lt;/td&gt; 
   &lt;td&gt;25.13&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/huggingface/smollm"&gt;SmolLM2&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;HuggingFaceTB&lt;/td&gt; 
   &lt;td&gt;135M&lt;/td&gt; 
   &lt;td&gt;24.37&lt;/td&gt; 
   &lt;td&gt;25.02&lt;/td&gt; 
   &lt;td&gt;25.37&lt;/td&gt; 
   &lt;td&gt;25.06&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/BAAI/Aquila-135M-Instruct"&gt;Aquila-Instruct&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;BAAI&lt;/td&gt; 
   &lt;td&gt;135M&lt;/td&gt; 
   &lt;td&gt;25.11&lt;/td&gt; 
   &lt;td&gt;25.1&lt;/td&gt; 
   &lt;td&gt;24.43&lt;/td&gt; 
   &lt;td&gt;25.05&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/compare_radar.png" alt="compare_radar" /&gt;&lt;/p&gt; 
&lt;h1&gt;📌 其它 (Others)&lt;/h1&gt; 
&lt;h2&gt;模型转换&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/jingyaogong/minimind/master/scripts/convert_model.py"&gt;./scripts/convert_model.py&lt;/a&gt;可以实现&lt;code&gt;torch模型/transformers&lt;/code&gt;模型之间的转换&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;基于MiniMind-API服务接口&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/jingyaogong/minimind/master/scripts/serve_openai_api.py"&gt;./scripts/serve_openai_api.py&lt;/a&gt;完成了兼容openai-api的最简聊天接口，方便将自己的模型接入第三方UI 例如FastGPT、OpenWebUI、Dify等等。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;从&lt;a href="https://huggingface.co/collections/jingyaogong/minimind-66caf8d999f5c7fa64f399e5"&gt;Huggingface&lt;/a&gt;下载模型权重文件，文件树：&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;lt;MiniMind-Model-Name&amp;gt; (root dir)
├─&amp;lt;MiniMind-Model-Name&amp;gt;
|  ├── config.json
|  ├── generation_config.json
|  ├── LMConfig.py
|  ├── model.py
|  ├── pytorch_model.bin
|  ├── special_tokens_map.json
|  ├── tokenizer_config.json
|  ├── tokenizer.json
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;启动聊天服务端&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;python serve_openai_api.py
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;测试服务接口&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;python chat_openai_api.py
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;API接口示例，兼容openai api格式&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;curl http://ip:port/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{ 
    "model": "model-identifier",
    "messages": [ 
      { "role": "user", "content": "世界上最高的山是什么？" }
    ], 
    "temperature": 0.7, 
    "max_tokens": 512,
    "stream": true
}'
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;VLLM模型推理（服务）&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/vllm-project/vllm"&gt;vLLM&lt;/a&gt;是极其流行的高效推理框架，支持大模型快速部署，优化显存利用与吞吐量。&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;vllm serve ./MiniMind2/ --model-impl transformers --served-model-name "minimind"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;服务将以openai api协议启动，端口默认为8000。&lt;/p&gt; 
&lt;p&gt;更多用法请参考官方说明～&lt;/p&gt; 
&lt;h2&gt;llama.cpp&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/ggerganov/llama.cpp"&gt;llama.cpp&lt;/a&gt;是一个C++库， 可以在命令行下直接使用，支持多线程推理，支持GPU加速。&lt;/p&gt; 
&lt;p&gt;参考官方仓库安装后，在&lt;code&gt;convert_hf_to_gguf.py&lt;/code&gt; ～760行插入&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;# 添加MiniMind2 tokenizer支持
if res is None:
    res = "smollm"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;转换自定义训练的minimind模型 -&amp;gt; gguf&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python convert_hf_to_gguf.py ../minimind/MiniMind2/
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;量化模型&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;./build/bin/llama-quantize ../minimind/MiniMind2/MiniMind2-109M-F16.gguf ../minimind/MiniMind2/Q4-MiniMind2.gguf Q4_K_M
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;命令行推理&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;./build/bin/llama-cli -m ../minimind/MiniMind2/MiniMind2-109M-F16.gguf --chat-template chatml
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;更多用法请参考官方说明～&lt;/p&gt; 
&lt;h2&gt;ollama&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://ollama.ai/"&gt;ollama&lt;/a&gt;是本地运行大模型的工具，支持多种开源LLM，简单易用。&lt;/p&gt; 
&lt;p&gt;通过ollama加载自定义的gguf模型，新建minimind.modelfile：&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;FROM ./MiniMind2-109M-F16.gguf
TEMPLATE """{{ if .System }}&amp;lt;|im_start|&amp;gt;system
{{ .System }}&amp;lt;|im_end|&amp;gt;
{{ end }}{{ if .Prompt }}&amp;lt;|im_start|&amp;gt;user
{{ .Prompt }}&amp;lt;|im_end|&amp;gt;
{{ end }}&amp;lt;|im_start|&amp;gt;assistant
"""
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;加载模型并命名为&lt;code&gt;minimind2&lt;/code&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;ollama create -f minimind.modelfile minimind2
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;启动推理&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;ollama run minimind2
&amp;gt; 你好，我是MiniMind2，一个基于xxxxxxxx
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;更多用法请参考官方说明～&lt;/p&gt; 
&lt;h1&gt;📌 Acknowledge&lt;/h1&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] 如果觉得&lt;code&gt;MiniMind系列&lt;/code&gt;对您有所帮助，可以在 GitHub 上加一个⭐&lt;br /&gt; 篇幅超长水平有限难免纰漏，欢迎在Issues交流指正或提交PR改进项目&lt;br /&gt; 您的小小支持就是持续改进此项目的动力！&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;🤝&lt;a href="https://github.com/jingyaogong/minimind/graphs/contributors"&gt;贡献者&lt;/a&gt;&lt;/h2&gt; 
&lt;!--
&lt;a href="https://github.com/jingyaogong/minimind/graphs/contributors"&gt;
  &lt;img src="https://contrib.rocks/image?repo=jingyaogong/minimind&amp;v3" /&gt;
&lt;/a&gt;
--&gt; 
&lt;p&gt;&lt;a href="https://github.com/jingyaogong"&gt;&lt;img src="https://avatars.githubusercontent.com/u/62287848" width="70px" height="70px" /&gt;&lt;/a&gt; &amp;nbsp; &lt;a href="https://github.com/MuWinds"&gt;&lt;img src="https://avatars.githubusercontent.com/u/93832089" width="70px" height="70px" /&gt;&lt;/a&gt; &amp;nbsp; &lt;a href="https://github.com/chuanzhubin"&gt;&lt;img src="https://avatars.githubusercontent.com/u/2813798" width="70px" height="70px" /&gt;&lt;/a&gt; &amp;nbsp; &lt;a href="https://github.com/iomgaa-ycz"&gt;&lt;img src="https://avatars.githubusercontent.com/u/124225682" width="70px" height="70px" /&gt;&lt;/a&gt; &amp;nbsp;&lt;/p&gt; 
&lt;h2&gt;😊鸣谢&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/ipfgao"&gt;&lt;b&gt;@ipfgao&lt;/b&gt;&lt;/a&gt;: &lt;a href="https://github.com/jingyaogong/minimind/issues/26"&gt;🔗训练步骤记录&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/chuanzhubin"&gt;&lt;b&gt;@chuanzhubin&lt;/b&gt;&lt;/a&gt;: &lt;a href="https://github.com/jingyaogong/minimind/pull/34"&gt;🔗代码逐行注释&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/WangRongsheng"&gt;&lt;b&gt;@WangRongsheng&lt;/b&gt;&lt;/a&gt;: &lt;a href="https://github.com/jingyaogong/minimind/issues/39"&gt;🔗大型数据集预处理&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/pengqianhan"&gt;&lt;b&gt;@pengqianhan&lt;/b&gt;&lt;/a&gt;: &lt;a href="https://github.com/jingyaogong/minimind/issues/73"&gt;🔗一个简明教程&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/RyanSunn"&gt;&lt;b&gt;@RyanSunn&lt;/b&gt;&lt;/a&gt;: &lt;a href="https://github.com/jingyaogong/minimind/issues/75"&gt;🔗推理过程学习记录&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/Nijikadesu"&gt;&lt;b&gt;@Nijikadesu&lt;/b&gt;&lt;/a&gt;: &lt;a href="https://github.com/jingyaogong/minimind/issues/213"&gt;🔗以交互笔记本方式分解项目代码&lt;/a&gt;&lt;/p&gt; 
&lt;details close&gt; 
 &lt;summary&gt; &lt;b&gt;参考链接 &amp;amp; 感谢以下优秀的论文或项目&lt;/b&gt; &lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;排名不分任何先后顺序&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/meta-llama/llama3"&gt;https://github.com/meta-llama/llama3&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/karpathy/llama2.c"&gt;https://github.com/karpathy/llama2.c&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/DLLXW/baby-llama2-chinese"&gt;https://github.com/DLLXW/baby-llama2-chinese&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://arxiv.org/abs/2405.04434"&gt;(DeepSeek-V2)https://arxiv.org/abs/2405.04434&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/charent/ChatLM-mini-Chinese"&gt;https://github.com/charent/ChatLM-mini-Chinese&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/wdndev/tiny-llm-zh"&gt;https://github.com/wdndev/tiny-llm-zh&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://arxiv.org/pdf/2401.04088"&gt;(Mistral-MoE)https://arxiv.org/pdf/2401.04088&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/Tongjilibo/build_MiniLLM_from_scratch"&gt;https://github.com/Tongjilibo/build_MiniLLM_from_scratch&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/jzhang38/TinyLlama"&gt;https://github.com/jzhang38/TinyLlama&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/AI-Study-Han/Zero-Chatgpt"&gt;https://github.com/AI-Study-Han/Zero-Chatgpt&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/xusenlinzy/api-for-open-llm"&gt;https://github.com/xusenlinzy/api-for-open-llm&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/HqWu-HITCS/Awesome-Chinese-LLM"&gt;https://github.com/HqWu-HITCS/Awesome-Chinese-LLM&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;🫶支持者&lt;/h2&gt; 
&lt;a href="https://github.com/jingyaogong/minimind/stargazers"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://reporoster.com/stars/dark/jingyaogong/minimind" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://reporoster.com/stars/jingyaogong/minimind" /&gt; 
  &lt;img alt="github contribution grid snake animation" src="https://reporoster.com/stars/jingyaogong/minimind" /&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;a href="https://github.com/jingyaogong/minimind/network/members"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://reporoster.com/forks/dark/jingyaogong/minimind" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://reporoster.com/forks/jingyaogong/minimind" /&gt; 
  &lt;img alt="github contribution grid snake animation" src="https://reporoster.com/forks/jingyaogong/minimind" /&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;picture&gt; 
 &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=jingyaogong/minimind&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
 &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=jingyaogong/minimind&amp;amp;type=Date" /&gt; 
 &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=jingyaogong/minimind&amp;amp;type=Date" /&gt; 
&lt;/picture&gt; 
&lt;h1&gt;License&lt;/h1&gt; 
&lt;p&gt;This repository is licensed under the &lt;a href="https://raw.githubusercontent.com/jingyaogong/minimind/master/LICENSE"&gt;Apache-2.0 License&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>karpathy/nanoGPT</title>
      <link>https://github.com/karpathy/nanoGPT</link>
      <description>&lt;p&gt;The simplest, fastest repository for training/finetuning medium-sized GPTs.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;nanoGPT&lt;/h1&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/karpathy/nanoGPT/master/assets/nanogpt.jpg" alt="nanoGPT" /&gt;&lt;/p&gt; 
&lt;p&gt;The simplest, fastest repository for training/finetuning medium-sized GPTs. It is a rewrite of &lt;a href="https://github.com/karpathy/minGPT"&gt;minGPT&lt;/a&gt; that prioritizes teeth over education. Still under active development, but currently the file &lt;code&gt;train.py&lt;/code&gt; reproduces GPT-2 (124M) on OpenWebText, running on a single 8XA100 40GB node in about 4 days of training. The code itself is plain and readable: &lt;code&gt;train.py&lt;/code&gt; is a ~300-line boilerplate training loop and &lt;code&gt;model.py&lt;/code&gt; a ~300-line GPT model definition, which can optionally load the GPT-2 weights from OpenAI. That's it.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/karpathy/nanoGPT/master/assets/gpt2_124M_loss.png" alt="repro124m" /&gt;&lt;/p&gt; 
&lt;p&gt;Because the code is so simple, it is very easy to hack to your needs, train new models from scratch, or finetune pretrained checkpoints (e.g. biggest one currently available as a starting point would be the GPT-2 1.3B model from OpenAI).&lt;/p&gt; 
&lt;h2&gt;install&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;pip install torch numpy transformers datasets tiktoken wandb tqdm
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Dependencies:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://pytorch.org"&gt;pytorch&lt;/a&gt; &amp;lt;3&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://numpy.org/install/"&gt;numpy&lt;/a&gt; &amp;lt;3&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;transformers&lt;/code&gt; for huggingface transformers &amp;lt;3 (to load GPT-2 checkpoints)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;datasets&lt;/code&gt; for huggingface datasets &amp;lt;3 (if you want to download + preprocess OpenWebText)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;tiktoken&lt;/code&gt; for OpenAI's fast BPE code &amp;lt;3&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;wandb&lt;/code&gt; for optional logging &amp;lt;3&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;tqdm&lt;/code&gt; for progress bars &amp;lt;3&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;quick start&lt;/h2&gt; 
&lt;p&gt;If you are not a deep learning professional and you just want to feel the magic and get your feet wet, the fastest way to get started is to train a character-level GPT on the works of Shakespeare. First, we download it as a single (1MB) file and turn it from raw text into one large stream of integers:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python data/shakespeare_char/prepare.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This creates a &lt;code&gt;train.bin&lt;/code&gt; and &lt;code&gt;val.bin&lt;/code&gt; in that data directory. Now it is time to train your GPT. The size of it very much depends on the computational resources of your system:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;I have a GPU&lt;/strong&gt;. Great, we can quickly train a baby GPT with the settings provided in the &lt;a href="https://raw.githubusercontent.com/karpathy/nanoGPT/master/config/train_shakespeare_char.py"&gt;config/train_shakespeare_char.py&lt;/a&gt; config file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python train.py config/train_shakespeare_char.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you peek inside it, you'll see that we're training a GPT with a context size of up to 256 characters, 384 feature channels, and it is a 6-layer Transformer with 6 heads in each layer. On one A100 GPU this training run takes about 3 minutes and the best validation loss is 1.4697. Based on the configuration, the model checkpoints are being written into the &lt;code&gt;--out_dir&lt;/code&gt; directory &lt;code&gt;out-shakespeare-char&lt;/code&gt;. So once the training finishes we can sample from the best model by pointing the sampling script at this directory:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python sample.py --out_dir=out-shakespeare-char
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This generates a few samples, for example:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;ANGELO:
And cowards it be strawn to my bed,
And thrust the gates of my threats,
Because he that ale away, and hang'd
An one with him.

DUKE VINCENTIO:
I thank your eyes against it.

DUKE VINCENTIO:
Then will answer him to save the malm:
And what have you tyrannous shall do this?

DUKE VINCENTIO:
If you have done evils of all disposition
To end his power, the day of thrust for a common men
That I leave, to fight with over-liking
Hasting in a roseman.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;lol &lt;code&gt;¯\_(ツ)_/¯&lt;/code&gt;. Not bad for a character-level model after 3 minutes of training on a GPU. Better results are quite likely obtainable by instead finetuning a pretrained GPT-2 model on this dataset (see finetuning section later).&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;I only have a macbook&lt;/strong&gt; (or other cheap computer). No worries, we can still train a GPT but we want to dial things down a notch. I recommend getting the bleeding edge PyTorch nightly (&lt;a href="https://pytorch.org/get-started/locally/"&gt;select it here&lt;/a&gt; when installing) as it is currently quite likely to make your code more efficient. But even without it, a simple train run could look as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python train.py config/train_shakespeare_char.py --device=cpu --compile=False --eval_iters=20 --log_interval=1 --block_size=64 --batch_size=12 --n_layer=4 --n_head=4 --n_embd=128 --max_iters=2000 --lr_decay_iters=2000 --dropout=0.0
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Here, since we are running on CPU instead of GPU we must set both &lt;code&gt;--device=cpu&lt;/code&gt; and also turn off PyTorch 2.0 compile with &lt;code&gt;--compile=False&lt;/code&gt;. Then when we evaluate we get a bit more noisy but faster estimate (&lt;code&gt;--eval_iters=20&lt;/code&gt;, down from 200), our context size is only 64 characters instead of 256, and the batch size only 12 examples per iteration, not 64. We'll also use a much smaller Transformer (4 layers, 4 heads, 128 embedding size), and decrease the number of iterations to 2000 (and correspondingly usually decay the learning rate to around max_iters with &lt;code&gt;--lr_decay_iters&lt;/code&gt;). Because our network is so small we also ease down on regularization (&lt;code&gt;--dropout=0.0&lt;/code&gt;). This still runs in about ~3 minutes, but gets us a loss of only 1.88 and therefore also worse samples, but it's still good fun:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python sample.py --out_dir=out-shakespeare-char --device=cpu
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Generates samples like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;GLEORKEN VINGHARD III:
Whell's the couse, the came light gacks,
And the for mought you in Aut fries the not high shee
bot thou the sought bechive in that to doth groan you,
No relving thee post mose the wear
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Not bad for ~3 minutes on a CPU, for a hint of the right character gestalt. If you're willing to wait longer, feel free to tune the hyperparameters, increase the size of the network, the context length (&lt;code&gt;--block_size&lt;/code&gt;), the length of training, etc.&lt;/p&gt; 
&lt;p&gt;Finally, on Apple Silicon Macbooks and with a recent PyTorch version make sure to add &lt;code&gt;--device=mps&lt;/code&gt; (short for "Metal Performance Shaders"); PyTorch then uses the on-chip GPU that can &lt;em&gt;significantly&lt;/em&gt; accelerate training (2-3X) and allow you to use larger networks. See &lt;a href="https://github.com/karpathy/nanoGPT/issues/28"&gt;Issue 28&lt;/a&gt; for more.&lt;/p&gt; 
&lt;h2&gt;reproducing GPT-2&lt;/h2&gt; 
&lt;p&gt;A more serious deep learning professional may be more interested in reproducing GPT-2 results. So here we go - we first tokenize the dataset, in this case the &lt;a href="https://openwebtext2.readthedocs.io/en/latest/"&gt;OpenWebText&lt;/a&gt;, an open reproduction of OpenAI's (private) WebText:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python data/openwebtext/prepare.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This downloads and tokenizes the &lt;a href="https://huggingface.co/datasets/openwebtext"&gt;OpenWebText&lt;/a&gt; dataset. It will create a &lt;code&gt;train.bin&lt;/code&gt; and &lt;code&gt;val.bin&lt;/code&gt; which holds the GPT2 BPE token ids in one sequence, stored as raw uint16 bytes. Then we're ready to kick off training. To reproduce GPT-2 (124M) you'll want at least an 8X A100 40GB node and run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;torchrun --standalone --nproc_per_node=8 train.py config/train_gpt2.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will run for about 4 days using PyTorch Distributed Data Parallel (DDP) and go down to loss of ~2.85. Now, a GPT-2 model just evaluated on OWT gets a val loss of about 3.11, but if you finetune it it will come down to ~2.85 territory (due to an apparent domain gap), making the two models ~match.&lt;/p&gt; 
&lt;p&gt;If you're in a cluster environment and you are blessed with multiple GPU nodes you can make GPU go brrrr e.g. across 2 nodes like:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Run on the first (master) node with example IP 123.456.123.456:
torchrun --nproc_per_node=8 --nnodes=2 --node_rank=0 --master_addr=123.456.123.456 --master_port=1234 train.py
# Run on the worker node:
torchrun --nproc_per_node=8 --nnodes=2 --node_rank=1 --master_addr=123.456.123.456 --master_port=1234 train.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;It is a good idea to benchmark your interconnect (e.g. iperf3). In particular, if you don't have Infiniband then also prepend &lt;code&gt;NCCL_IB_DISABLE=1&lt;/code&gt; to the above launches. Your multinode training will work, but most likely &lt;em&gt;crawl&lt;/em&gt;. By default checkpoints are periodically written to the &lt;code&gt;--out_dir&lt;/code&gt;. We can sample from the model by simply &lt;code&gt;python sample.py&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Finally, to train on a single GPU simply run the &lt;code&gt;python train.py&lt;/code&gt; script. Have a look at all of its args, the script tries to be very readable, hackable and transparent. You'll most likely want to tune a number of those variables depending on your needs.&lt;/p&gt; 
&lt;h2&gt;baselines&lt;/h2&gt; 
&lt;p&gt;OpenAI GPT-2 checkpoints allow us to get some baselines in place for openwebtext. We can get the numbers as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;$ python train.py config/eval_gpt2.py
$ python train.py config/eval_gpt2_medium.py
$ python train.py config/eval_gpt2_large.py
$ python train.py config/eval_gpt2_xl.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;and observe the following losses on train and val:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;model&lt;/th&gt; 
   &lt;th&gt;params&lt;/th&gt; 
   &lt;th&gt;train loss&lt;/th&gt; 
   &lt;th&gt;val loss&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;gpt2&lt;/td&gt; 
   &lt;td&gt;124M&lt;/td&gt; 
   &lt;td&gt;3.11&lt;/td&gt; 
   &lt;td&gt;3.12&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;gpt2-medium&lt;/td&gt; 
   &lt;td&gt;350M&lt;/td&gt; 
   &lt;td&gt;2.85&lt;/td&gt; 
   &lt;td&gt;2.84&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;gpt2-large&lt;/td&gt; 
   &lt;td&gt;774M&lt;/td&gt; 
   &lt;td&gt;2.66&lt;/td&gt; 
   &lt;td&gt;2.67&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;gpt2-xl&lt;/td&gt; 
   &lt;td&gt;1558M&lt;/td&gt; 
   &lt;td&gt;2.56&lt;/td&gt; 
   &lt;td&gt;2.54&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;However, we have to note that GPT-2 was trained on (closed, never released) WebText, while OpenWebText is just a best-effort open reproduction of this dataset. This means there is a dataset domain gap. Indeed, taking the GPT-2 (124M) checkpoint and finetuning on OWT directly for a while reaches loss down to ~2.85. This then becomes the more appropriate baseline w.r.t. reproduction.&lt;/p&gt; 
&lt;h2&gt;finetuning&lt;/h2&gt; 
&lt;p&gt;Finetuning is no different than training, we just make sure to initialize from a pretrained model and train with a smaller learning rate. For an example of how to finetune a GPT on new text go to &lt;code&gt;data/shakespeare&lt;/code&gt; and run &lt;code&gt;prepare.py&lt;/code&gt; to download the tiny shakespeare dataset and render it into a &lt;code&gt;train.bin&lt;/code&gt; and &lt;code&gt;val.bin&lt;/code&gt;, using the OpenAI BPE tokenizer from GPT-2. Unlike OpenWebText this will run in seconds. Finetuning can take very little time, e.g. on a single GPU just a few minutes. Run an example finetuning like:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python train.py config/finetune_shakespeare.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will load the config parameter overrides in &lt;code&gt;config/finetune_shakespeare.py&lt;/code&gt; (I didn't tune them much though). Basically, we initialize from a GPT2 checkpoint with &lt;code&gt;init_from&lt;/code&gt; and train as normal, except shorter and with a small learning rate. If you're running out of memory try decreasing the model size (they are &lt;code&gt;{'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}&lt;/code&gt;) or possibly decreasing the &lt;code&gt;block_size&lt;/code&gt; (context length). The best checkpoint (lowest validation loss) will be in the &lt;code&gt;out_dir&lt;/code&gt; directory, e.g. in &lt;code&gt;out-shakespeare&lt;/code&gt; by default, per the config file. You can then run the code in &lt;code&gt;sample.py --out_dir=out-shakespeare&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;THEODORE:
Thou shalt sell me to the highest bidder: if I die,
I sell thee to the first; if I go mad,
I sell thee to the second; if I
lie, I sell thee to the third; if I slay,
I sell thee to the fourth: so buy or sell,
I tell thee again, thou shalt not sell my
possession.

JULIET:
And if thou steal, thou shalt not sell thyself.

THEODORE:
I do not steal; I sell the stolen goods.

THEODORE:
Thou know'st not what thou sell'st; thou, a woman,
Thou art ever a victim, a thing of no worth:
Thou hast no right, no right, but to be sold.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Whoa there, GPT, entering some dark place over there. I didn't really tune the hyperparameters in the config too much, feel free to try!&lt;/p&gt; 
&lt;h2&gt;sampling / inference&lt;/h2&gt; 
&lt;p&gt;Use the script &lt;code&gt;sample.py&lt;/code&gt; to sample either from pre-trained GPT-2 models released by OpenAI, or from a model you trained yourself. For example, here is a way to sample from the largest available &lt;code&gt;gpt2-xl&lt;/code&gt; model:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python sample.py \
    --init_from=gpt2-xl \
    --start="What is the answer to life, the universe, and everything?" \
    --num_samples=5 --max_new_tokens=100
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you'd like to sample from a model you trained, use the &lt;code&gt;--out_dir&lt;/code&gt; to point the code appropriately. You can also prompt the model with some text from a file, e.g. &lt;code&gt;python sample.py --start=FILE:prompt.txt&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;efficiency notes&lt;/h2&gt; 
&lt;p&gt;For simple model benchmarking and profiling, &lt;code&gt;bench.py&lt;/code&gt; might be useful. It's identical to what happens in the meat of the training loop of &lt;code&gt;train.py&lt;/code&gt;, but omits much of the other complexities.&lt;/p&gt; 
&lt;p&gt;Note that the code by default uses &lt;a href="https://pytorch.org/get-started/pytorch-2.0/"&gt;PyTorch 2.0&lt;/a&gt;. At the time of writing (Dec 29, 2022) this makes &lt;code&gt;torch.compile()&lt;/code&gt; available in the nightly release. The improvement from the one line of code is noticeable, e.g. cutting down iteration time from ~250ms / iter to 135ms / iter. Nice work PyTorch team!&lt;/p&gt; 
&lt;h2&gt;todos&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Investigate and add FSDP instead of DDP&lt;/li&gt; 
 &lt;li&gt;Eval zero-shot perplexities on standard evals (e.g. LAMBADA? HELM? etc.)&lt;/li&gt; 
 &lt;li&gt;Finetune the finetuning script, I think the hyperparams are not great&lt;/li&gt; 
 &lt;li&gt;Schedule for linear batch size increase during training&lt;/li&gt; 
 &lt;li&gt;Incorporate other embeddings (rotary, alibi)&lt;/li&gt; 
 &lt;li&gt;Separate out the optim buffers from model params in checkpoints I think&lt;/li&gt; 
 &lt;li&gt;Additional logging around network health (e.g. gradient clip events, magnitudes)&lt;/li&gt; 
 &lt;li&gt;Few more investigations around better init etc.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;troubleshooting&lt;/h2&gt; 
&lt;p&gt;Note that by default this repo uses PyTorch 2.0 (i.e. &lt;code&gt;torch.compile&lt;/code&gt;). This is fairly new and experimental, and not yet available on all platforms (e.g. Windows). If you're running into related error messages try to disable this by adding &lt;code&gt;--compile=False&lt;/code&gt; flag. This will slow down the code but at least it will run.&lt;/p&gt; 
&lt;p&gt;For some context on this repository, GPT, and language modeling it might be helpful to watch my &lt;a href="https://karpathy.ai/zero-to-hero.html"&gt;Zero To Hero series&lt;/a&gt;. Specifically, the &lt;a href="https://www.youtube.com/watch?v=kCc8FmEb1nY"&gt;GPT video&lt;/a&gt; is popular if you have some prior language modeling context.&lt;/p&gt; 
&lt;p&gt;For more questions/discussions feel free to stop by &lt;strong&gt;#nanoGPT&lt;/strong&gt; on Discord:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://discord.gg/3zy8kqD9Cp"&gt;&lt;img src="https://dcbadge.vercel.app/api/server/3zy8kqD9Cp?compact=true&amp;amp;style=flat" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;acknowledgements&lt;/h2&gt; 
&lt;p&gt;All nanoGPT experiments are powered by GPUs on &lt;a href="https://lambdalabs.com"&gt;Lambda labs&lt;/a&gt;, my favorite Cloud GPU provider. Thank you Lambda labs for sponsoring nanoGPT!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>wmjordan/PDFPatcher</title>
      <link>https://github.com/wmjordan/PDFPatcher</link>
      <description>&lt;p&gt;PDF补丁丁——PDF工具箱，可以编辑书签、剪裁旋转页面、解除限制、提取或合并文档，探查文档结构，提取图片、转成图片等等&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;PDF 补丁丁（PDFPatcher）&lt;/h1&gt; 
&lt;p&gt;感谢您关注 PDF 补丁丁，请在使用软件或源代码前阅读本说明和授权协议。本软件及源代码采用 AGPL＋“&lt;strong&gt;良心授权&lt;/strong&gt;”协议——&lt;strong&gt;用户每次使用本软件后如有所获益，应行一善事；如使用源代码开发了新的软件并获得收益，应将收益中不低于千分之一的金额捐赠给社会的弱势群体&lt;/strong&gt;。&lt;/p&gt; 
&lt;h2&gt;功能简介&lt;/h2&gt; 
&lt;p&gt;PDF 补丁丁是一个 PDF 处理工具。它具有以下功能：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;修改 PDF 文档：修改文档属性、页码编号、页面链接；统一页面尺寸；删除自动打开网页等动作；去除复制及打印限制；设置阅读器初始模式；清理文档隐藏垃圾数据；重新压缩黑白图片；旋转页面。&lt;/li&gt; 
 &lt;li&gt;贴心 PDF 书签编辑器：带有阅读界面（具有便于阅读竖排文档的从右到左阅读方式），可批量修改 PDF 书签属性（颜色、样式、目标页码、缩放比例等），书签可精确定位到页面中间；在书签中执行查找替换（支持正则表达式及 XPath 匹配、可快速选择篇、章、节书签），&lt;a href="https://www.cnblogs.com/pdfpatcher/p/8452025.html"&gt;自动快速生成文档书签&lt;/a&gt;。&lt;/li&gt; 
 &lt;li&gt;制作 PDF 文件：合并已有 PDF 文件或图片，生成新的 PDF 文件；合并后的 PDF 文档带有原文档的书签，还可挂上新书签（或根据文件名生成），新书签文本和样式可自定义；合并的 PDF 文档可指定统一的页面尺寸，以便打印和阅读。&lt;/li&gt; 
 &lt;li&gt;拆分或合并 PDF 文件，并保留原文件的书签或挂上新的书签。&lt;/li&gt; 
 &lt;li&gt;高速无损导出 PDF 文档的图片。&lt;/li&gt; 
 &lt;li&gt;将 PDF 页面转换为图片。&lt;/li&gt; 
 &lt;li&gt;提取或删除 PDF 文档中指定的页面，调整 PDF 文档的页面顺序。&lt;/li&gt; 
 &lt;li&gt;根据 PDF 文档元数据重命名 PDF 文件名。&lt;/li&gt; 
 &lt;li&gt;调用微软 Office 的图像识别引擎分析 PDF 文档图片中的文字；将图片 PDF 的目录页转换为 PDF 书签。识别结果可写入 PDF 文件。&lt;/li&gt; 
 &lt;li&gt;替换字体：替换文档中使用的字体；嵌入字库到 PDF 文档，消除复制文本时的乱码，使之可在没有字库的设备（如 Kindle 等电子书阅读器）上阅读。&lt;/li&gt; 
 &lt;li&gt;分析文档结构：以树视图显示 PDF 文档结构，可编辑修改 PDF 文档节点，或将 PDF 文档导出成 XML 文件，供 PDF 爱好者分析、调试之用。&lt;/li&gt; 
 &lt;li&gt;永久免费，绝不过期，无广告，无弹出废话对话框，不窥探隐私。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;授权协议&lt;/h2&gt; 
&lt;p&gt;《PDF 补丁丁》软件（以下简称本软件）受著作权法及国际条约条款和其它知识产权法及条约的保护。 本软件对于最终用户免费。由于本软件使用了带有 AGPL 条款的第三方开源组件，因此，本软件及其源代码的使用协议也基于 AGPL。另外还带有如下附加条件。在遵守本软件的前提条件下，你可以在遵循本协议的基础上自由的使用和传播它，你一旦安装、复制或使用本软件，则表示您已经同意本协议条款。如果你不同意本协议，请不要安装使用本软件，也不应利用其源代码。&lt;/p&gt; 
&lt;p&gt;附加条件： 每一个使用本软件的用户，如果本软件帮助了您，每使用本软件后，您应当做 1 件善事。善事无分大小，有心则行。例如：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;如果您的父母在身边，你可以为您的父母做一顿美味的饭菜，或者为他们按摩、洗脚；如果他们身处远方，你可以向他们发起通话，问候他们的健康和生活。&lt;/li&gt; 
 &lt;li&gt;在大雨滂沱的时候，如果您有雨伞，可与同路的人共享；在烈日当空的时节，如果您看到环卫工人太阳下工作，您可以为他们买一瓶水送给他们；在拥挤的公共交通工具上，或在公共场合排队等候之际，如果您有座位，可以让给老人、孕妇或提着重物的人就坐。&lt;/li&gt; 
 &lt;li&gt;您可以用您擅长的技能，为身边的人排难解困；您可以将您的知识，分享给其他人，让他们有所获益；您可以向比您困难的人捐资赠物。&lt;/li&gt; 
 &lt;li&gt;如果您觉得这个软件真的好用，请将它的使用方法介绍给别人，让别人也通过使用本软件而得到好处；或者将其它您觉得好用的软件介绍给别人。&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;如果您无法做到使用本软件后做 1 件善事，请记在心中。在有机会的时候，多行善积德。本用户协议之遵循与否，全在于您的良心。是为“&lt;strong&gt;良心授权&lt;/strong&gt;”。&lt;/p&gt; 
&lt;p&gt;相关定义：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;软件：软件是指《PDF 补丁丁》软件以及它的更新、产品手册，以及在线文档等相关载体。&lt;/li&gt; 
 &lt;li&gt;限制：你可以使用本软件的源代码开发应用程序（自由、共享或商用），也可以任意方式分发数量不限的本软件的完整拷贝，但前提是： ① 你分发软件时必须提供本软件的完整版本，未经许可不得对软件乃至它的安装程序做任何修改； ② 你分发软件时不能更改本授权协议； ③ 你如果在商业性宣传活动、产品中附加本软件，应当获得著作权人的书面许可； ④ 你如果利用本软件的源代码编写了其它软件，并且产生了销售收入，应当将该软件销售收入不低于千分之一的金额捐献给社会上的弱势群体。&lt;/li&gt; 
 &lt;li&gt;支持：软件会由于用户的需求而不断更新，著作权人将提供包括用户手册、电子邮件等各种相关信息支持，但软件不确保支持内容和功能不发生变更。&lt;/li&gt; 
 &lt;li&gt;终止：当你不同意或者违背本协议的时候，协议将自动终止，你必须立即删除本软件产品。&lt;/li&gt; 
 &lt;li&gt;版权：本软件及源代码受著作权法及国际条约条款和其它知识产权法及条约的保护。&lt;/li&gt; 
 &lt;li&gt;免责：对于本软件安装、复制、使用中导致的任何损失，本软件及著作权人不负责任。&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;常用的 PDF 开源组件简介&lt;/h2&gt; 
&lt;p&gt;PDF 文档的规范（ISO 32000-1:2008 《Document management — Portable document format — Part 1:PDF 1.7》）可从网上找到，一般来说，它是 PDF 处理程序开发者的必读文献。&lt;/p&gt; 
&lt;p&gt;PDF 文档格式中涉及印刷领域的多项技术，并有其独特的文档结构，还使用了多种数据压缩算法。要从零开始编写 PDF 文档的处理程序，对于一般人而言，通常是困难而不太现实的。PDF 补丁丁使用 .NET Framework 开发，主要采用 iText 和 MuPDF 这两个开放源代码的组件库来处理 PDF 文档。&lt;/p&gt; 
&lt;p&gt;前者是 .NET 组件，与 PDF 主程序具有较好的互操作性，并且在解析、生成和修改 PDF 文档，以及嵌入 TTF 字体子集这些功能上，优胜于后者。&lt;/p&gt; 
&lt;p&gt;后者采用 C 语言开发并编译，与前者相比，其最大的优点是具有渲染 PDF 文档为位图的功能。MuPDF 编译出来的动态组件库可在作者另一个开放源代码库 &lt;a href="https://github.com/wmjordan/SharpMuPDF"&gt;SharpMuPDF&lt;/a&gt; 下载。PDF 补丁丁通过 P/Invoke 技术调用该组件库的功能。&lt;/p&gt; 
&lt;p&gt;除了 PDF 开源组件之外，程序还使用了其它优秀开源组件。例如 ObjectListView 这个强大的列表控件、FreeImage 来读取和解码各种类型的点阵图像文件、Cyotek 的 ImageBox 用于显示渲染好的 PDF 文档页面、TabControlExtra 用于构建选项卡式文档界面、HTMLRenderer 用于显示 HTML 网页界面等等。&lt;/p&gt; 
&lt;h2&gt;源代码的结构&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;App 目录：PDF 补丁丁主程序 
  &lt;ul&gt; 
   &lt;li&gt;Common：一些常用的工具类&lt;/li&gt; 
   &lt;li&gt;Functions：用于呈现软件各类功能的窗体和控件&lt;/li&gt; 
   &lt;li&gt;Lib：程序使用的第三方组件&lt;/li&gt; 
   &lt;li&gt;Model：编辑文档时所用的高级模型（基础数据模型由 iText 和 MuPDF 的类实现）&lt;/li&gt; 
   &lt;li&gt;Options：程序的选项&lt;/li&gt; 
   &lt;li&gt;Processor：处理 PDF 文档的算法（其中 Mupdf 目录里放置了 P/Invoke 调用 MuPDF 的类）&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;doc 目录：放置程序的使用文档&lt;/li&gt; 
 &lt;li&gt;JBig2 目录：放置 JBIG2 图像的编码和解码库代码&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;运行环境&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Windows 7 以上版本的操作系统。&lt;/li&gt; 
 &lt;li&gt;.NET Framework 4.0 到 4.8 版本。&lt;/li&gt; 
 &lt;li&gt;使用文字识别功能需要安装 Microsoft Office 2003（或 2007）的 Document Imaging 组件（MODI）。&lt;/li&gt; 
 &lt;li&gt;编译程序源代码，建议使用 Visual Studio 2022 或更新版本，并安装“.NET 桌面开发”（用于编译 PDF 补丁丁源代码）和“C++ 桌面开发”（用于编译 JBIG2 编码组件）两个工作负载。可能会遇到项目“面向不再受支持的 .NET Framework”、需要“将目标更新为 .NET Framework 4.8”的问题。简单方法是将目标更新为 .NET Framework 4.8，如不更新目标，请参考&lt;a href="https://www.cnblogs.com/Stay627/p/15549958.html"&gt;这篇文章介绍的方法&lt;/a&gt;。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;联系作者&lt;/h2&gt; 
&lt;p&gt;除第三方组件外，本软件的源代码完全开放：&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/wmjordan/PDFPatcher"&gt;https://github.com/wmjordan/PDFPatcher&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://gitee.com/wmjordan/pdfpatcher"&gt;https://gitee.com/wmjordan/pdfpatcher&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;建议通过开放源代码网站通过提交 issue 的方式提交您的建议或需求。因日常工作繁忙，暂不提供加 QQ 或微信咨询的服务，敬请谅解。&lt;/p&gt; 
&lt;p&gt;在邮件或消息中，请注明你的版本号，附上截图和附件，详细说明你遇到的问题。&lt;/p&gt; 
&lt;p&gt;如遇到需要提供附件的情况，请把它搞小一点。一般情况下，最好不要发送超过 10M 的附件。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;对于 PDF 文件，可用“提取页面”功能提取有代表性的页面。&lt;/li&gt; 
 &lt;li&gt;对于图片文件，请压缩源文件，或提供有代表性的一两页图片。&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>envoyproxy/envoy</title>
      <link>https://github.com/envoyproxy/envoy</link>
      <description>&lt;p&gt;Cloud-native high-performance edge/middle/service proxy&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://github.com/envoyproxy/artwork/raw/main/PNG/Envoy_Logo_Final_PANTONE.png" alt="Envoy Logo" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.envoyproxy.io/"&gt;Cloud-native high-performance edge/middle/service proxy&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Envoy is hosted by the &lt;a href="https://cncf.io"&gt;Cloud Native Computing Foundation&lt;/a&gt; (CNCF). If you are a company that wants to help shape the evolution of technologies that are container-packaged, dynamically-scheduled and microservices-oriented, consider joining the CNCF. For details about who's involved and how Envoy plays a role, read the CNCF &lt;a href="https://www.cncf.io/blog/2017/09/13/cncf-hosts-envoy/"&gt;announcement&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://bestpractices.coreinfrastructure.org/projects/1266"&gt;&lt;img src="https://bestpractices.coreinfrastructure.org/projects/1266/badge" alt="CII Best Practices" /&gt;&lt;/a&gt; &lt;a href="https://securityscorecards.dev/viewer/?uri=github.com/envoyproxy/envoy"&gt;&lt;img src="https://api.securityscorecards.dev/projects/github.com/envoyproxy/envoy/badge" alt="OpenSSF Scorecard" /&gt;&lt;/a&gt; &lt;a href="https://clomonitor.io/projects/cncf/envoy"&gt;&lt;img src="https://img.shields.io/endpoint?url=https://clomonitor.io/api/projects/cncf/envoy/badge" alt="CLOMonitor" /&gt;&lt;/a&gt; &lt;a href="https://dev.azure.com/cncf/envoy/_build/latest?definitionId=11&amp;amp;branchName=main"&gt;&lt;img src="https://dev.azure.com/cncf/envoy/_apis/build/status/11?branchName=main" alt="Azure Pipelines" /&gt;&lt;/a&gt; &lt;a href="https://bugs.chromium.org/p/oss-fuzz/issues/list?sort=-opened&amp;amp;can=1&amp;amp;q=proj:envoy"&gt;&lt;img src="https://oss-fuzz-build-logs.storage.googleapis.com/badges/envoy.svg?sanitize=true" alt="Fuzzing Status" /&gt;&lt;/a&gt; &lt;a href="https://powerci.osuosl.org/job/build-envoy-static-master/"&gt;&lt;img src="https://powerci.osuosl.org/buildStatus/icon?job=build-envoy-static-master&amp;amp;subject=ppc64le%20build" alt="Jenkins" /&gt;&lt;/a&gt; &lt;a href="https://ibmz-ci.osuosl.org/job/Envoy_IBMZ_CI/"&gt;&lt;img src="https://ibmz-ci.osuosl.org/buildStatus/icon?job=Envoy_IBMZ_CI&amp;amp;subject=s390x%20build" alt="Jenkins" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.envoyproxy.io/"&gt;Official documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.envoyproxy.io/docs/envoy/latest/faq/overview"&gt;FAQ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/envoyproxy/examples/"&gt;Example documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://medium.com/@mattklein123/envoy-threading-model-a8d44b922310"&gt;Blog&lt;/a&gt; about the threading model&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://medium.com/@mattklein123/envoy-hot-restart-1d16b14555b5"&gt;Blog&lt;/a&gt; about hot restart&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://medium.com/@mattklein123/envoy-stats-b65c7f363342"&gt;Blog&lt;/a&gt; about stats architecture&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://medium.com/@mattklein123/the-universal-data-plane-api-d15cec7a"&gt;Blog&lt;/a&gt; about universal data plane API&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://medium.com/@mattklein123/lyfts-envoy-dashboards-5c91738816b1"&gt;Blog&lt;/a&gt; on Lyft's Envoy dashboards&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Related&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/envoyproxy/data-plane-api"&gt;data-plane-api&lt;/a&gt;: v2 API definitions as a standalone repository. This is a read-only mirror of &lt;a href="https://raw.githubusercontent.com/envoyproxy/envoy/main/api/"&gt;api&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/envoyproxy/envoy-perf"&gt;envoy-perf&lt;/a&gt;: Performance testing framework.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/envoyproxy/envoy-filter-example"&gt;envoy-filter-example&lt;/a&gt;: Example of how to add new filters and link to the main repository.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contact&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://groups.google.com/forum/#!forum/envoy-announce"&gt;envoy-announce&lt;/a&gt;: Low frequency mailing list where we will email announcements only.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://groups.google.com/forum/#!forum/envoy-security-announce"&gt;envoy-security-announce&lt;/a&gt;: Low frequency mailing list where we will email security related announcements only.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://groups.google.com/forum/#!forum/envoy-users"&gt;envoy-users&lt;/a&gt;: General user discussion.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://groups.google.com/forum/#!forum/envoy-dev"&gt;envoy-dev&lt;/a&gt;: Envoy developer discussion (APIs, feature design, etc.).&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://groups.google.com/forum/#!forum/envoy-maintainers"&gt;envoy-maintainers&lt;/a&gt;: Use this list to reach all core Envoy maintainers.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://twitter.com/EnvoyProxy/"&gt;Twitter&lt;/a&gt;: Follow along on Twitter!&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://envoyproxy.slack.com/"&gt;Slack&lt;/a&gt;: Slack, to get invited go &lt;a href="https://communityinviter.com/apps/envoyproxy/envoy"&gt;here&lt;/a&gt;. 
  &lt;ul&gt; 
   &lt;li&gt;NOTE: Response to user questions is best effort on Slack. For a "guaranteed" response please email envoy-users@ per the guidance in the following linked thread.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Please see &lt;a href="https://groups.google.com/forum/#!topic/envoy-announce/l9zjYsnS3TY"&gt;this&lt;/a&gt; email thread for information on email list usage.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Contributing to Envoy is fun and modern C++ is a lot less scary than you might think if you don't have prior experience. To get started:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/envoyproxy/envoy/main/CONTRIBUTING.md"&gt;Contributing guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/envoyproxy/envoy/issues?q=is%3Aopen+is%3Aissue+label%3Abeginner"&gt;Beginner issues&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/envoyproxy/envoy/main/ci#building-and-running-tests-as-a-developer"&gt;Build/test quick start using docker&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/envoyproxy/envoy/main/DEVELOPER.md"&gt;Developer guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Consider installing the Envoy &lt;a href="https://github.com/envoyproxy/envoy/raw/main/support/README.md"&gt;development support toolchain&lt;/a&gt;, which helps automate parts of the development process, particularly those involving code review.&lt;/li&gt; 
 &lt;li&gt;Please make sure that you let us know if you are working on an issue so we don't duplicate work!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Community Meeting&lt;/h2&gt; 
&lt;p&gt;The Envoy team has a scheduled meeting time twice per month on Tuesday at 9am PT. The public Google calendar is &lt;a href="https://goo.gl/PkDijT"&gt;here&lt;/a&gt;. The meeting will only be held if there are agenda items listed in the &lt;a href="https://goo.gl/5Cergb"&gt;meeting minutes&lt;/a&gt;. Any member of the community should be able to propose agenda items by adding to the minutes. The maintainers will either confirm the additions to the agenda, or will cancel the meeting within 24 hours of the scheduled date if there is no confirmed agenda.&lt;/p&gt; 
&lt;h2&gt;Security&lt;/h2&gt; 
&lt;h3&gt;Security Audit&lt;/h3&gt; 
&lt;p&gt;There has been several third party engagements focused on Envoy security:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;In 2018 Cure53 performed a security audit, &lt;a href="https://raw.githubusercontent.com/envoyproxy/envoy/main/docs/security/audit_cure53_2018.pdf"&gt;full report&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;In 2021 Ada Logics performed an audit on our fuzzing infrastructure with recommendations for improvements, &lt;a href="https://raw.githubusercontent.com/envoyproxy/envoy/main/docs/security/audit_fuzzer_adalogics_2021.pdf"&gt;full report&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Reporting security vulnerabilities&lt;/h3&gt; 
&lt;p&gt;If you've found a vulnerability or a potential vulnerability in Envoy please let us know at &lt;a href="mailto:envoy-security@googlegroups.com"&gt;envoy-security&lt;/a&gt;. We'll send a confirmation email to acknowledge your report, and we'll send an additional email when we've identified the issue positively or negatively.&lt;/p&gt; 
&lt;p&gt;For further details please see our complete &lt;a href="https://raw.githubusercontent.com/envoyproxy/envoy/main/SECURITY.md"&gt;security release process&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;ppc64le builds&lt;/h3&gt; 
&lt;p&gt;Builds for the ppc64le architecture or using aws-lc are not covered by the envoy security policy. The ppc64le architecture is currently best-effort and not maintained by the Envoy maintainers.&lt;/p&gt; 
&lt;h2&gt;Releases&lt;/h2&gt; 
&lt;p&gt;For further details please see our &lt;a href="https://github.com/envoyproxy/envoy/raw/main/RELEASES.md"&gt;release process&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>QwenLM/Qwen3-VL</title>
      <link>https://github.com/QwenLM/Qwen3-VL</link>
      <description>&lt;p&gt;Qwen3-VL is the multimodal large language model series developed by Qwen team, Alibaba Cloud.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Qwen3-VL&lt;/h1&gt; 
&lt;p align="center"&gt; &lt;img src="https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/qwen3vllogo.png" width="400" /&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p align="center"&gt; 💜 &lt;a href="https://chat.qwenlm.ai/"&gt;&lt;b&gt;Qwen Chat&lt;/b&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;🤗 &lt;a href="https://huggingface.co/collections/Qwen/qwen3-vl-68d2a7c1b8a8afce4ebd2dbe"&gt;Hugging Face&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;🤖 &lt;a href="https://modelscope.cn/collections/Qwen3-VL-5c7a94c8cb144b"&gt;ModelScope&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;📑 &lt;a href="https://qwen.ai/blog?id=99f0335c4ad9ff6153e517418d48535ab6d8afef&amp;amp;from=research.latest-advancements-list"&gt;Blog&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;📚 &lt;a href="https://github.com/QwenLM/Qwen3-VL/tree/main/cookbooks"&gt;Cookbooks&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;📑 Paper is coming&amp;nbsp;&amp;nbsp; &lt;br /&gt; 🖥️ &lt;a href="https://huggingface.co/spaces/Qwen/Qwen3-VL-Demo"&gt;Demo&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;💬 &lt;a href="https://github.com/QwenLM/Qwen/raw/main/assets/wechat.png"&gt;WeChat (微信)&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;🫨 &lt;a href="https://discord.gg/CV4E9rpNSD"&gt;Discord&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;📑 &lt;a href="https://help.aliyun.com/zh/model-studio/developer-reference/qwen-vl-api"&gt;API&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;🖥️ &lt;a href="https://gallery.pai-ml.com/#/preview/deepLearning/cv/qwen2.5-vl"&gt;PAI-DSW&lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;Meet Qwen3-VL — the most powerful vision-language model in the Qwen series to date.&lt;/p&gt; 
&lt;p&gt;This generation delivers comprehensive upgrades across the board: superior text understanding &amp;amp; generation, deeper visual perception &amp;amp; reasoning, extended context length, enhanced spatial and video dynamics comprehension, and stronger agent interaction capabilities.&lt;/p&gt; 
&lt;p&gt;Available in Dense and MoE architectures that scale from edge to cloud, with Instruct and reasoning‑enhanced Thinking editions for flexible, on‑demand deployment.&lt;/p&gt; 
&lt;h4&gt;Key Enhancements:&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Visual Agent&lt;/strong&gt;: Operates PC/mobile GUIs—recognizes elements, understands functions, invokes tools, completes tasks.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Visual Coding Boost&lt;/strong&gt;: Generates Draw.io/HTML/CSS/JS from images/videos.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Advanced Spatial Perception&lt;/strong&gt;: Judges object positions, viewpoints, and occlusions; provides stronger 2D grounding and enables 3D grounding for spatial reasoning and embodied AI.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Long Context &amp;amp; Video Understanding&lt;/strong&gt;: Native 256K context, expandable to 1M; handles books and hours-long video with full recall and second-level indexing.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Enhanced Multimodal Reasoning&lt;/strong&gt;: Excels in STEM/Math—causal analysis and logical, evidence-based answers.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Upgraded Visual Recognition&lt;/strong&gt;: Broader, higher-quality pretraining is able to “recognize everything”—celebrities, anime, products, landmarks, flora/fauna, etc.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Expanded OCR&lt;/strong&gt;: Supports 32 languages (up from 10); robust in low light, blur, and tilt; better with rare/ancient characters and jargon; improved long-document structure parsing.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Text Understanding on par with pure LLMs&lt;/strong&gt;: Seamless text–vision fusion for lossless, unified comprehension.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Model Architecture Updates:&lt;/h4&gt; 
&lt;p align="center"&gt; &lt;img src="https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/qwen3vl_arc.jpg" width="80%" /&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Interleaved-MRoPE&lt;/strong&gt;: Full‑frequency allocation over time, width, and height via robust positional embeddings, enhancing long‑horizon video reasoning.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;DeepStack&lt;/strong&gt;: Fuses multi‑level ViT features to capture fine‑grained details and sharpen image–text alignment.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Text–Timestamp Alignment:&lt;/strong&gt; Moves beyond T‑RoPE to precise, timestamp‑grounded event localization for stronger video temporal modeling.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;2025.10.15: We have released the &lt;strong&gt;Qwen3-VL-4B&lt;/strong&gt; (&lt;a href="https://huggingface.co/Qwen/Qwen3-VL-4B-Instruct"&gt;Instruct&lt;/a&gt;/&lt;a href="https://huggingface.co/Qwen/Qwen3-VL-4B-Thinking"&gt;Thinking&lt;/a&gt;) and &lt;strong&gt;Qwen3-VL-8B&lt;/strong&gt; (&lt;a href="https://huggingface.co/Qwen/Qwen3-VL-8B-Instruct"&gt;Instruct&lt;/a&gt;/&lt;a href="https://huggingface.co/Qwen/Qwen3-VL-8B-Thinking"&gt;Thinking&lt;/a&gt;). Enjoy it!&lt;/li&gt; 
 &lt;li&gt;2025.10.4: We have released the &lt;a href="https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Instruct"&gt;Qwen3-VL-30B-A3B-Instruct&lt;/a&gt; and &lt;a href="https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Thinking"&gt;Qwen3-VL-30B-A3B-Thinking&lt;/a&gt;. We have also released the FP8 version of the Qwen3-VL models — available in our &lt;a href="https://huggingface.co/collections/Qwen/qwen3-vl-68d2a7c1b8a8afce4ebd2dbe"&gt;HuggingFace collection&lt;/a&gt; and &lt;a href="https://modelscope.cn/collections/Qwen3-VL-5c7a94c8cb144b"&gt;ModelScope collection&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;2025.09.23: We have released the &lt;a href="https://huggingface.co/Qwen/Qwen3-VL-235B-A22B-Instruct"&gt;Qwen3-VL-235B-A22B-Instruct&lt;/a&gt; and &lt;a href="https://huggingface.co/Qwen/Qwen3-VL-235B-A22B-Thinking"&gt;Qwen3-VL-235B-A22B-Thinking&lt;/a&gt;. For more details, please check our &lt;a href="https://qwen.ai/blog?id=99f0335c4ad9ff6153e517418d48535ab6d8afef&amp;amp;from=research.latest-advancements-list"&gt;blog&lt;/a&gt;!&lt;/li&gt; 
 &lt;li&gt;2025.04.08: We provide the &lt;a href="https://github.com/QwenLM/Qwen2.5-VL/tree/main/qwen-vl-finetune"&gt;code&lt;/a&gt; for fine-tuning Qwen2-VL and Qwen2.5-VL.&lt;/li&gt; 
 &lt;li&gt;2025.03.25: We have released the &lt;a href="https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct"&gt;Qwen2.5-VL-32B&lt;/a&gt;. It is smarter and its responses align more closely with human preferences. For more details, please check our &lt;a href="https://qwenlm.github.io/blog/qwen2.5-vl-32b/"&gt;blog&lt;/a&gt;!&lt;/li&gt; 
 &lt;li&gt;2025.02.20: we have released the &lt;a href="https://arxiv.org/abs/2502.13923"&gt;Qwen2.5-VL Technical Report&lt;/a&gt;. Alongside the report, we have also released AWQ-quantized models for Qwen2.5-VL in three different sizes: &lt;a href="https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct-AWQ"&gt;3B&lt;/a&gt;, &lt;a href="https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct-AWQ"&gt;7B&lt;/a&gt; , and &lt;a href="https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct-AWQ"&gt;72B&lt;/a&gt; parameters.&lt;/li&gt; 
 &lt;li&gt;2025.01.28: We have released the &lt;a href="https://huggingface.co/Qwen"&gt;Qwen2.5-VL series&lt;/a&gt;. For more details, please check our &lt;a href="https://qwenlm.github.io/blog/qwen2.5-vl/"&gt;blog&lt;/a&gt;!&lt;/li&gt; 
 &lt;li&gt;2024.12.25: We have released the &lt;a href="https://huggingface.co/Qwen/QVQ-72B-Preview"&gt;QvQ-72B-Preview&lt;/a&gt;. QvQ-72B-Preview is an experimental research model, focusing on enhancing visual reasoning capabilities. For more details, please check our &lt;a href="https://qwenlm.github.io/blog/qvq-72b-preview/"&gt;blog&lt;/a&gt;!&lt;/li&gt; 
 &lt;li&gt;2024.09.19: The instruction-tuned &lt;a href="https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct"&gt;Qwen2-VL-72B model&lt;/a&gt; and its quantized version [&lt;a href="https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct-AWQ"&gt;AWQ&lt;/a&gt;, &lt;a href="https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct-GPTQ-Int4"&gt;GPTQ-Int4&lt;/a&gt;, &lt;a href="https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct-GPTQ-Int8"&gt;GPTQ-Int8&lt;/a&gt;] are now available. We have also released the &lt;a href="https://arxiv.org/pdf/2409.12191"&gt;Qwen2-VL paper&lt;/a&gt; simultaneously.&lt;/li&gt; 
 &lt;li&gt;2024.08.30: We have released the &lt;a href="https://huggingface.co/collections/Qwen/qwen2-vl-66cee7455501d7126940800d"&gt;Qwen2-VL series&lt;/a&gt;. The 2B and 7B models are now available, and the 72B model for open source is coming soon. For more details, please check our &lt;a href="https://qwenlm.github.io/blog/qwen2-vl/"&gt;blog&lt;/a&gt;!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Performance&lt;/h2&gt; 
&lt;h3&gt;Visual Tasks&lt;/h3&gt; 
&lt;div style="display: flex; justify-content: center; gap: 16px; flex-wrap: wrap;"&gt; 
 &lt;img src="https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/table_nothinking_vl.jpg" width="24%" /&gt; 
 &lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-VL/table_thinking_vl_.jpg" width="24%" /&gt; 
 &lt;img src="https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/table_nothinking_vl-30a3.jpg" width="26%" /&gt; 
 &lt;img src="https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/table_thinking_vl_30A3.jpg" width="22.5%" /&gt; 
&lt;/div&gt; 
&lt;div style="display: flex; justify-content: center; gap: 16px; flex-wrap: wrap;"&gt; 
 &lt;img src="https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/qwen3vl_4b_8b_vl_instruct.jpg" width="30%" /&gt; 
 &lt;img src="https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/qwen3vl_4b_8b_vl_thinking.jpg" width="24%" /&gt; 
&lt;/div&gt; 
&lt;h3&gt;Text-Centric Tasks&lt;/h3&gt; 
&lt;div style="display: flex; justify-content: center; gap: 16px; flex-wrap: wrap;"&gt; 
 &lt;img src="https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/table_nothinking_text.jpg" width="30%" /&gt; 
 &lt;img src="https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/table_thinking_text.jpg" width="32%" /&gt; 
 &lt;img src="https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/table_nothinking_text-30a3.jpg" width="30%" /&gt; 
&lt;/div&gt; 
&lt;div style="display: flex; justify-content: center; gap: 16px; flex-wrap: wrap;"&gt; 
 &lt;img src="https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/qwen3vl_4b_8b_text_instruct.jpg" width="33%" /&gt; 
 &lt;img src="https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/qwen3vl_4b_8b_text_thinking.jpg" width="28%" /&gt; 
&lt;/div&gt; 
&lt;h2&gt;Cookbooks&lt;/h2&gt; 
&lt;p&gt;We are preparing &lt;a href="https://github.com/QwenLM/Qwen3-VL/tree/main/cookbooks"&gt;cookbooks&lt;/a&gt; for many capabilities, including recognition, localization, document parsing, video understanding, key information extraction, and more. Welcome to learn more!&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Cookbook&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Open&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/omni_recognition.ipynb"&gt;Omni Recognition&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Not only identify animals, plants, people, and scenic spots but also recognize various objects such as cars and merchandise.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/omni_recognition.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/document_parsing.ipynb"&gt;Powerful Document Parsing Capabilities&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;The parsing of documents has reached a higher level, including not only text but also layout position information and our Qwen HTML format.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/document_parsing.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/2d_grounding.ipynb"&gt;Precise Object Grounding Across Formats&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Using relative position coordinates, it supports both boxes and points, allowing for diverse combinations of positioning and labeling tasks.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/2d_grounding.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/ocr.ipynb"&gt;General OCR and Key Information Extraction&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Stronger text recognition capabilities in natural scenes and multiple languages, supporting diverse key information extraction needs.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/ocr.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/video_understanding.ipynb"&gt;Video Understanding&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Better video OCR, long video understanding, and video grounding.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/video_understanding.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/mobile_agent.ipynb"&gt;Mobile Agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Locate and think for mobile phone control.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/mobile_agent.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/computer_use.ipynb"&gt;Computer-Use Agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Locate and think for controlling computers and Web.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/computer_use.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/3d_grounding.ipynb"&gt;3D Grounding&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Provide accurate 3D bounding boxes for both indoor and outdoor objects.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/3d_grounding.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/think_with_images.ipynb"&gt;Thinking with Images&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Utilize image_zoom_in_tool and search_tool to facilitate the model’s precise comprehension of fine-grained visual details within images.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/think_with_images.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/mmcode.ipynb"&gt;MultiModal Coding&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Generate accurate code based on rigorous comprehension of multimodal information.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/mmcode.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/long_document_understanding.ipynb"&gt;Long Document Understanding&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Achieve rigorous semantic comprehension of ultra-long documents.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/long_document_understanding.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/spatial_understanding.ipynb"&gt;Spatial Understanding&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;See, understand and reason about the spatial information&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/spatial_understanding.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Quickstart&lt;/h2&gt; 
&lt;p&gt;Below, we provide simple examples to show how to use Qwen3-VL with 🤖 ModelScope and 🤗 Transformers.&lt;/p&gt; 
&lt;p&gt;The code of Qwen3-VL has been in the latest Hugging face transformers and we advise you to build from source with command:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install git+https://github.com/huggingface/transformers
# pip install transformers==4.57.0 # currently, V4.57.0 is not released
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;🤖 ModelScope&lt;/h3&gt; 
&lt;p&gt;We strongly advise users especially those in mainland China to use ModelScope. &lt;code&gt;snapshot_download&lt;/code&gt; can help you solve issues concerning downloading checkpoints.&lt;/p&gt; 
&lt;h3&gt;Using 🤗 Transformers to Chat&lt;/h3&gt; 
&lt;p&gt;Here we show a code snippet to show you how to use the chat model with &lt;code&gt;transformers&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from transformers import AutoModelForImageTextToText, AutoProcessor

# default: Load the model on the available device(s)
model = AutoModelForImageTextToText.from_pretrained(
    "Qwen/Qwen3-VL-235B-A22B-Instruct", dtype="auto", device_map="auto"
)

# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.
# model = AutoModelForImageTextToText.from_pretrained(
#     "Qwen/Qwen3-VL-235B-A22B-Instruct",
#     dtype=torch.bfloat16,
#     attn_implementation="flash_attention_2",
#     device_map="auto",
# )

processor = AutoProcessor.from_pretrained("Qwen/Qwen3-VL-235B-A22B-Instruct")

messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "image",
                "image": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg",
            },
            {"type": "text", "text": "Describe this image."},
        ],
    }
]

# Preparation for inference
inputs = processor.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,
    return_dict=True,
    return_tensors="pt"
)
inputs = inputs.to(model.device)

# Inference: Generation of the output
generated_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids_trimmed = [
    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
print(output_text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;!-- &lt;details&gt;
&lt;summary&gt;Minimum VRAM requirements&lt;/summary&gt;

| Precision | Qwen2.5-VL-3B | Qwen2.5-VL-7B | Qwen2.5-VL-72B |
|-----------|------------| --------- | -------- |
| FP32      | 11.5 GB    | 26.34 GB  | 266.21 GB |
| BF16      | 5.75 GB    | 13.17 GB  | 133.11 GB |
| INT8      | 2.87 GB    | 6.59 GB   | 66.5 GB |
| INT4      | 1.44 GB    | 3.29 GB   | 33.28 GB |

Note: The table above presents the theoretical minimum video memory requirements for inference with `transformers`; however, in practice, the actual memory usage is typically at least 1.2 times higher. For more information, see the linked resource [here](https://huggingface.co/docs/accelerate/main/en/usage_guides/model_size_estimator).
&lt;/details&gt; --&gt; 
&lt;details&gt; 
 &lt;summary&gt;Multi image inference&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Messages containing multiple images and a text query
messages = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "file:///path/to/image1.jpg"},
            {"type": "image", "image": "file:///path/to/image2.jpg"},
            {"type": "text", "text": "Identify the similarities between these images."},
        ],
    }
]

# Preparation for inference
inputs = processor.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,
    return_dict=True,
    return_tensors="pt"
)
inputs = inputs.to(model.device)

# Inference: Generation of the output
generated_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids_trimmed = [
    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
print(output_text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Video inference&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Messages containing a video url(or a local path) and a text query
messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "video",
                "video": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/space_woaudio.mp4",
            },
            {"type": "text", "text": "Describe this video."},
        ],
    }
]

# Preparation for inference
inputs = processor.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,
    return_dict=True,
    return_tensors="pt"
)
inputs = inputs.to(model.device)

# Inference: Generation of the output
generated_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids_trimmed = [
    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
print(output_text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Batch inference&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# for batch generation, padding_side should be set to left!
processor.tokenizer.padding_side = 'left'

# Sample messages for batch inference
messages1 = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "file:///path/to/image1.jpg"},
            {"type": "image", "image": "file:///path/to/image2.jpg"},
            {"type": "text", "text": "What are the common elements in these pictures?"},
        ],
    }
]
messages2 = [
    {"role": "system", "content": [{"type": "text", "text": "You are a helpful assistant."}]},
    {"role": "user", "content": [{"type": "text", "text": "Who are you?"}]},
]
# Combine messages for batch processing
messages = [messages1, messages2]

# Preparation for inference
inputs = processor.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,
    return_dict=True,
    return_tensors="pt",
    padding=True # padding should be set for batch generation!
)
inputs = inputs.to(model.device)

# Inference: Generation of the output
generated_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids_trimmed = [
    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
print(output_text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Pixel Control via Official Processor&lt;/summary&gt; 
 &lt;p&gt;Using the official HF processor, we can conveniently control the budget of visual tokens. Since the Qwen3-VL processor separates image and video processing, we can independently configure the pixel budget for each modality.&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;For the image processor&lt;/strong&gt;:&lt;br /&gt; The parameter &lt;code&gt;size['longest_edge']&lt;/code&gt; originally corresponds to &lt;code&gt;max_pixels&lt;/code&gt;, which defines the maximum number of pixels allowed for an image (i.e., for an image of height H and width W, H × W must not exceed &lt;code&gt;max_pixels&lt;/code&gt;; image channels are ignored for simplicity).&lt;br /&gt; Similarly, &lt;code&gt;size['shortest_edge']&lt;/code&gt; corresponds to &lt;code&gt;min_pixels&lt;/code&gt;, specifying the minimum allowable pixel count for an image.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;For the video processor&lt;/strong&gt;:&lt;br /&gt; The interpretation differs slightly. &lt;code&gt;size['longest_edge']&lt;/code&gt; represents the maximum total number of pixels across all frames in a video — for a video of shape T×H×W, the product T×H×W must not exceed &lt;code&gt;size['longest_edge']&lt;/code&gt;.&lt;br /&gt; Similarly, &lt;code&gt;size['shortest_edge']&lt;/code&gt; sets the minimum total pixel budget for the video.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;processor = AutoProcessor.from_pretrained("Qwen/Qwen3-VL-235B-A22B-Instruct")

# budget for image processor, since the compression ratio is 32 for Qwen3-VL, we can set the number of visual tokens of a single image to 256-1280
processor.image_processor.size = {"longest_edge": 1280*32*32, "shortest_edge": 256*32*32}

# budget for video processor, we can set the number of visual tokens of a single video to 256-16384
processor.video_processor.size = {"longest_edge": 16384*32*32, "shortest_edge": 256*32*32}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ul&gt; 
  &lt;li&gt;You can further control the &lt;strong&gt;sample fps&lt;/strong&gt; or &lt;strong&gt;sample frames&lt;/strong&gt; of video, as shown below.&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "video",
                "video": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/space_woaudio.mp4",
            },
            {"type": "text", "text": "Describe this video."},
        ],
    }
]

# for video input, we can further control the fps or num_frames. \
# defaultly, fps is set to 2

# set fps = 4
inputs = processor.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,
    return_dict=True,
    return_tensors="pt",
    fps=4
)
inputs = inputs.to(model.device)

# set num_frames = 128 and overwrite the fps to None!
# inputs = processor.apply_chat_template(
#     messages,
#     tokenize=True,
#     add_generation_prompt=True,
#     return_dict=True,
#     return_tensors="pt",
#     num_frames=128,
#     fps=None,
# )
# inputs = inputs.to(model.device)

# Inference: Generation of the output
generated_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids_trimmed = [
    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
print(output_text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;New &lt;code&gt;qwen-vl-utils&lt;/code&gt; Usage&lt;/h3&gt; 
&lt;p&gt;With the latest &lt;code&gt;qwen-vl-utils&lt;/code&gt; toolkit (backward compatible with Qwen2.5-VL), you can control pixel constraints per visual input.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install qwen-vl-utils==0.0.14
# It's highly recommended to use `[decord]` feature for faster video loading.
# pip install qwen-vl-utils[decord]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Compared to previous version, the new &lt;code&gt;qwen-vl-utils&lt;/code&gt; introduces:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;"image_patch_size": &lt;code&gt;14&lt;/code&gt; for Qwen2.5-VL and &lt;code&gt;16&lt;/code&gt; for Qwen3-VL. Default set to &lt;code&gt;14&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;"return_video_metadata"(Qwen3-VL only): Due to the new video processor, if True, each video returns as (video_tensor, video_metadata). Default set to &lt;code&gt;False&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# for Qwen2.5VL, you can simply call 
images, videos, video_kwargs = process_vision_info(messages, return_video_kwargs=True)

# For Qwen3VL series, you should call 
images, videos, video_kwargs = process_vision_info(messages, image_patch_size=16, return_video_kwargs=True, return_video_metadata=True)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;📌 Note: Since &lt;code&gt;qwen-vl-utils&lt;/code&gt; already resizes images/videos, pass &lt;code&gt;do_resize=False&lt;/code&gt; to the processor to avoid duplicate resizing.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Process Images&lt;/summary&gt; 
 &lt;p&gt;For input images, we support local files, base64, and URLs.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# You can directly insert a local file path, a URL, or a base64-encoded image into the position where you want in the text.
## Local file path
messages = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "file:///path/to/your/image.jpg"},
            {"type": "text", "text": "Describe this image."},
        ],
    }
]
## Image URL
messages = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "http://path/to/your/image.jpg"},
            {"type": "text", "text": "Describe this image."},
        ],
    }
]
## Base64 encoded image
messages = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "data:image;base64,/9j/..."},
            {"type": "text", "text": "Describe this image."},
        ],
    }
]
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;We provide two methods for fine-grained control over the image size input to the model:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;Specify exact dimensions: Directly set resized_height and resized_width. These values will be rounded to the nearest multiple of 32 (32 for Qwen3VL, 28 for Qwen2.5VL).&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Define min_pixels and max_pixels: Images will be resized to maintain their aspect ratio within the range of min_pixels and max_pixels&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from transformers import AutoModelForImageTextToText, AutoProcessor
from qwen_vl_utils import process_vision_info

model = AutoModelForImageTextToText.from_pretrained(
    "Qwen/Qwen3-VL-235B-A22B-Instruct", dtype="auto", device_map="auto"
)

processor = AutoProcessor.from_pretrained("Qwen/Qwen3-VL-235B-A22B-Instruct")

# resized_height and resized_width
messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "image",
                "image": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg",
                "resized_height": 280,
                "resized_width": 420,
            },
            {"type": "text", "text": "Describe this image."},
        ],
    }
]

# min_pixels and max_pixels
messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "image",
                "image": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg",
                "min_pixels": 50176,
                "max_pixels": 50176,

            },
            {"type": "text", "text": "Describe this image."},
        ],
    }
]

# Preparation for inference with qwen-vl-utils
text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
images, videos = process_vision_info(messages, image_patch_size=16)

# since qwen-vl-utils has resize the images/videos, \
# we should pass do_resize=False to avoid duplicate operation in processor!
inputs = processor(text=text, images=images, videos=videos, do_resize=False, return_tensors="pt")
inputs = inputs.to(model.device)

# Inference: Generation of the output
generated_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids_trimmed = [
    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
print(output_text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Process Videos&lt;/summary&gt; 
 &lt;p&gt;For input videos, we support images lists, local path and url.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Messages containing a images list as a video and a text query
messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "video",
                "video": [
                    "file:///path/to/frame1.jpg",
                    "file:///path/to/frame2.jpg",
                    "file:///path/to/frame3.jpg",
                    "file:///path/to/frame4.jpg",
                ],
                'sample_fps':'1', # sample_fps: frame sampling rate (frames per second), used to determine timestamps for each frame
            },
            {"type": "text", "text": "Describe this video."},
        ],
    }
]

# Messages containing a local video path and a text query
messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "video",
                "video": "file:///path/to/video1.mp4",
                "max_pixels": 360 * 420,
                "fps": 1.0,
            },
            {"type": "text", "text": "Describe this video."},
        ],
    }
]

# Messages containing a video url and a text query
messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "video",
                "video": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/space_woaudio.mp4",
                "min_pixels": 4 * 32 * 32,
                "max_pixels": 256 * 32 * 32,
                "total_pixels": 20480 * 32 * 32,
            },
            {"type": "text", "text": "Describe this video."},
        ],
    }
]

&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;We recommend setting appropriate values for the &lt;code&gt;min_pixels&lt;/code&gt; and &lt;code&gt;max_pixels&lt;/code&gt; parameters based on available GPU memory and the specific application scenario to restrict the resolution of individual frames in the video.&lt;/p&gt; 
 &lt;p&gt;Alternatively, you can use the &lt;code&gt;total_pixels&lt;/code&gt; parameter to limit the total number of tokens in the video (it is recommended to set this value below 24576 * 32 * 32 to avoid excessively long input sequences). For more details on parameter usage and processing logic, please refer to the &lt;code&gt;fetch_video&lt;/code&gt; function in &lt;code&gt;qwen_vl_utils/vision_process.py&lt;/code&gt;.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from transformers import AutoModelForImageTextToText, AutoProcessor
from qwen_vl_utils import process_vision_info

model = AutoModelForImageTextToText.from_pretrained(
    "Qwen/Qwen3-VL-235B-A22B-Instruct", dtype="auto", device_map="auto"
)

processor = AutoProcessor.from_pretrained("Qwen/Qwen3-VL-235B-A22B-Instruct")

messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "video",
                "video": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/space_woaudio.mp4",
                "min_pixels": 4 * 32 * 32,
                "max_pixels": 256 * 32 * 32,
                "total_pixels": 20480 * 32 * 32,
            },
            {"type": "text", "text": "Describe this video."},
        ],
    }
]

text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
images, videos, video_kwargs = process_vision_info(messages, image_patch_size=16, return_video_kwargs=True, return_video_metadata=True)

# split the videos and according metadatas
if videos is not None:
    videos, video_metadatas = zip(*videos)
    videos, video_metadatas = list(videos), list(video_metadatas)
else:
    video_metadatas = None

# since qwen-vl-utils has resize the images/videos, \
# we should pass do_resize=False to avoid duplicate operation in processor!
inputs = processor(text=text, images=images, videos=videos, video_metadata=video_metadatas, return_tensors="pt", do_resize=False, **video_kwargs)
inputs = inputs.to(model.device)

# Inference: Generation of the output
generated_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids_trimmed = [
    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
print(output_text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Video Backends and URL Compatibility&lt;/summary&gt; 
 &lt;p&gt;Currently, &lt;code&gt;qwen-vl-utils&lt;/code&gt; supports three video decoding backends: &lt;code&gt;torchvision&lt;/code&gt;, &lt;code&gt;decord&lt;/code&gt;, and &lt;code&gt;torchcodec&lt;/code&gt;. While &lt;code&gt;decord&lt;/code&gt; and &lt;code&gt;torchcodec&lt;/code&gt; generally offer significantly faster decoding speeds compared to &lt;code&gt;torchvision&lt;/code&gt;, we recommend using &lt;code&gt;torchcodec&lt;/code&gt;. This is because &lt;code&gt;decord&lt;/code&gt; has known issues, such as decoding hangs, and its project is no longer actively maintained.&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;For &lt;code&gt;decord&lt;/code&gt;, if you are not using Linux, you might not be able to install &lt;code&gt;decord&lt;/code&gt; from PyPI. In that case, you can use &lt;code&gt;pip install qwen-vl-utils&lt;/code&gt; which will fall back to using torchvision for video processing. However, you can still &lt;a href="https://github.com/dmlc/decord?tab=readme-ov-file#install-from-source"&gt;install decord from source&lt;/a&gt; to get decord used when loading video.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;To use &lt;code&gt;torchcodec&lt;/code&gt; as the backend for video decoding, follow the installation instructions provided in the official &lt;a href="https://github.com/pytorch/torchcodec/tree/main?tab=readme-ov-file#installing-torchcodec"&gt;torchcodec repository&lt;/a&gt; and install it manually. Note that &lt;code&gt;torchcodec&lt;/code&gt; depends on FFmpeg for decoding functionality.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;Video URL compatibility is primarily determined by the version of the third-party library being used. For more details, refer to the table below. If you prefer not to use the default backend, you can switch it by setting &lt;code&gt;FORCE_QWENVL_VIDEO_READER&lt;/code&gt; to &lt;code&gt;torchvision&lt;/code&gt;, &lt;code&gt;decord&lt;/code&gt;, or &lt;code&gt;torchcodec&lt;/code&gt;.&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Backend&lt;/th&gt; 
    &lt;th&gt;HTTP&lt;/th&gt; 
    &lt;th&gt;HTTPS&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;torchvision &amp;gt;= 0.19.0&lt;/td&gt; 
    &lt;td&gt;✅&lt;/td&gt; 
    &lt;td&gt;✅&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;torchvision &amp;lt; 0.19.0&lt;/td&gt; 
    &lt;td&gt;❌&lt;/td&gt; 
    &lt;td&gt;❌&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;decord&lt;/td&gt; 
    &lt;td&gt;✅&lt;/td&gt; 
    &lt;td&gt;❌&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;torchcodec&lt;/td&gt; 
    &lt;td&gt;✅&lt;/td&gt; 
    &lt;td&gt;✅&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h3&gt;More Usage Tips&lt;/h3&gt; 
&lt;h4&gt;Add ids for Multiple Visual Inputs&lt;/h4&gt; 
&lt;p&gt;By default, images and video content are directly included in the conversation. When handling multiple images, it's helpful to add labels to the images and videos for better reference. Users can control this behavior with the following settings:&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Add vision ids&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;conversation = [
    {
        "role": "user",
        "content": [{"type": "image"}, {"type": "text", "text": "Hello, how are you?"}],
    },
    {
        "role": "assistant",
        "content": "I'm doing well, thank you for asking. How can I assist you today?",
    },
    {
        "role": "user",
        "content": [
            {"type": "text", "text": "Can you describe these images and video?"},
            {"type": "image"},
            {"type": "image"},
            {"type": "video"},
            {"type": "text", "text": "These are from my vacation."},
        ],
    },
    {
        "role": "assistant",
        "content": "I'd be happy to describe the images and video for you. Could you please provide more context about your vacation?",
    },
    {
        "role": "user",
        "content": "It was a trip to the mountains. Can you see the details in the images and video?",
    },
]

# default:
prompt_without_id = processor.apply_chat_template(
    conversation, add_generation_prompt=True
)
# Excepted output: '&amp;lt;|im_start|&amp;gt;system\nYou are a helpful assistant.&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;user\n&amp;lt;|vision_start|&amp;gt;&amp;lt;|image_pad|&amp;gt;&amp;lt;|vision_end|&amp;gt;Hello, how are you?&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;assistant\nI'm doing well, thank you for asking. How can I assist you today?&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;user\nCan you describe these images and video?&amp;lt;|vision_start|&amp;gt;&amp;lt;|image_pad|&amp;gt;&amp;lt;|vision_end|&amp;gt;&amp;lt;|vision_start|&amp;gt;&amp;lt;|image_pad|&amp;gt;&amp;lt;|vision_end|&amp;gt;&amp;lt;|vision_start|&amp;gt;&amp;lt;|video_pad|&amp;gt;&amp;lt;|vision_end|&amp;gt;These are from my vacation.&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;assistant\nI'd be happy to describe the images and video for you. Could you please provide more context about your vacation?&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;user\nIt was a trip to the mountains. Can you see the details in the images and video?&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;assistant\n'


# add ids
prompt_with_id = processor.apply_chat_template(
    conversation, add_generation_prompt=True, add_vision_id=True
)
# Excepted output: '&amp;lt;|im_start|&amp;gt;system\nYou are a helpful assistant.&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;user\nPicture 1: &amp;lt;|vision_start|&amp;gt;&amp;lt;|image_pad|&amp;gt;&amp;lt;|vision_end|&amp;gt;Hello, how are you?&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;assistant\nI'm doing well, thank you for asking. How can I assist you today?&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;user\nCan you describe these images and video?Picture 2: &amp;lt;|vision_start|&amp;gt;&amp;lt;|image_pad|&amp;gt;&amp;lt;|vision_end|&amp;gt;Picture 3: &amp;lt;|vision_start|&amp;gt;&amp;lt;|image_pad|&amp;gt;&amp;lt;|vision_end|&amp;gt;Video 1: &amp;lt;|vision_start|&amp;gt;&amp;lt;|video_pad|&amp;gt;&amp;lt;|vision_end|&amp;gt;These are from my vacation.&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;assistant\nI'd be happy to describe the images and video for you. Could you please provide more context about your vacation?&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;user\nIt was a trip to the mountains. Can you see the details in the images and video?&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;assistant\n'
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h4&gt;Flash-Attention 2 to speed up generation&lt;/h4&gt; 
&lt;p&gt;First, make sure to install the latest version of Flash Attention 2:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -U flash-attn --no-build-isolation
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Also, you should have a hardware that is compatible with Flash-Attention 2. Read more about it in the official documentation of the &lt;a href="https://github.com/Dao-AILab/flash-attention"&gt;flash attention repository&lt;/a&gt;. FlashAttention-2 can only be used when a model is loaded in &lt;code&gt;torch.float16&lt;/code&gt; or &lt;code&gt;torch.bfloat16&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;To load and run a model using Flash Attention-2, simply add &lt;code&gt;attn_implementation="flash_attention_2"&lt;/code&gt; when loading the model as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import torch
from transformers import AutoModelForImageTextToText

model = AutoModelForImageTextToText.from_pretrained(
    "Qwen/Qwen3-VL-235B-A22B-Instruct", 
    torch_dtype=torch.bfloat16, 
    attn_implementation="flash_attention_2",
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Processing Long Texts&lt;/h4&gt; 
&lt;p&gt;The current &lt;code&gt;config.json&lt;/code&gt; is set for context length up to 256K tokens. To handle extensive inputs exceeding 256K tokens, we utilize &lt;a href="https://arxiv.org/abs/2309.00071"&gt;YaRN&lt;/a&gt;, a technique for enhancing model length extrapolation, ensuring optimal performance on lengthy texts.&lt;/p&gt; 
&lt;p&gt;For supported frameworks (currently transformers and vLLM), you could modify &lt;code&gt;max_position_embeddings&lt;/code&gt; and &lt;code&gt;rope_scaling&lt;/code&gt; in &lt;code&gt;config.json&lt;/code&gt; to enable YaRN:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;{
    "max_position_embeddings": 1000000,
	...,
    "rope_scaling": {
        "rope_type": "yarn",
        "mrope_section": [
            24,
            20,
            20
        ],
        "mrope_interleaved": true,
        "factor": 3.0,
        "original_max_position_embeddings": 262144
    },
    ...
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;When using vLLM for serving, you can also enable YaRN by adding the additional arguments &lt;code&gt;--rope-scaling&lt;/code&gt; and &lt;code&gt;--max-model-len&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;vllm serve Qwen/Qwen3-VL-235B-A22B-Instruct --rope-scaling '{"rope_type":"yarn","factor":3.0,"original_max_position_embeddings": 262144,"mrope_section":[24,20,20],"mrope_interleaved": true}' --max-model-len 1000000
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Because Interleaved-MRoPE’s position IDs grow more slowly than vanilla RoPE, use a &lt;strong&gt;smaller scaling factor&lt;/strong&gt;. For example, to support 1M context with 256K context length, set factor=2 or 3 — not 4.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Try Qwen3-VL-235B-A22 with API!&lt;/h3&gt; 
&lt;p&gt;To explore Qwen3-VL-235B-A22, a more fascinating multimodal model, we encourage you to test our cutting-edge API service. Let's start the exciting journey right now!&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from openai import OpenAI

# set your DASHSCOPE_API_KEY here
DASHSCOPE_API_KEY = ""

client = OpenAI(
    api_key=DASHSCOPE_API_KEY,
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
)

completion = client.chat.completions.create(
    model="qwen3-vl-235b-a22b-instruct",
    messages=[{"role": "user", "content": [
        {"type": "image_url",
         "image_url": {"url": "https://dashscope.oss-cn-beijing.aliyuncs.com/images/dog_and_girl.jpeg"}},
        {"type": "text", "text": "这是什么"},
    ]}]
)
print(completion.model_dump_json())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more usage, please refer to the tutorial at &lt;a href="https://help.aliyun.com/zh/model-studio/developer-reference/qwen-vl-api"&gt;aliyun&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Web UI Example&lt;/h3&gt; 
&lt;p&gt;In this section, we provide instructions for users to build a web-based user interface (UI) demo. This UI demo allows users to interact with a predefined model or application through a web browser. Follow the steps below to get started.&lt;/p&gt; 
&lt;p&gt;Install the required dependencies by running the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r requirements_web_demo.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Launch a browser-based UI to interact with the model:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python web_demo_mm.py -c /your/path/to/qwen3vl/weight
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After running the command, you’ll see a link generated in the terminal similar to this:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Running on local: http://127.0.0.1:7860/
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Open the link in your browser to interact with the model — try text, images, or other features. For a quick start, you can also use our pre-built Docker image:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;cd docker &amp;amp;&amp;amp; bash run_web_demo.sh -c /your/path/to/qwen3vl/weight --port 8881
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Deployment&lt;/h2&gt; 
&lt;p&gt;We recommend using vLLM for fast Qwen3-VL deployment and inference. You need to install &lt;code&gt;vllm&amp;gt;=0.11.0&lt;/code&gt; to enable Qwen3-VL support. You can also use our &lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen3-VL/main/#-docker"&gt;official docker image&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Please check &lt;a href="https://docs.vllm.ai/en/latest/serving/multimodal_inputs.html"&gt;vLLM official documentation&lt;/a&gt; for more details about online serving and offline inference for multimodal models.&lt;/p&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install accelerate
pip install qwen-vl-utils==0.0.14
# Install the latest version of vLLM 'vllm&amp;gt;=0.11.0'
uv pip install -U vllm
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Online Serving&lt;/h3&gt; 
&lt;p&gt;You can start either a vLLM or SGLang server to serve LLMs efficiently, and then access it using an OpenAI-style API.&lt;/p&gt; 
&lt;p&gt;The following launch command is applicable to H100/H200; for more efficient deployment or deployment on other GPUs, please refer to the &lt;a href="https://docs.vllm.ai/projects/recipes/en/latest/Qwen/Qwen3-VL.html"&gt;vLLM community guide&lt;/a&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;vLLM server&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# Efficient inference with FP8 checkpoint
# Requires NVIDIA H100+ and CUDA 12+
vllm serve Qwen/Qwen3-VL-235B-A22B-Instruct-FP8 \
  --tensor-parallel-size 8 \
  --mm-encoder-tp-mode data \
  --enable-expert-parallel \
  --async-scheduling \
  --host 0.0.0.0 \
  --port 22002
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;SGLang server&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;python -m sglang.launch_server \
   --model-path Qwen/Qwen3-VL-235B-A22B-Instruct \
   --host 0.0.0.0 \
   --port 22002 \
   --tp 8
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Image Request Example&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import time
from openai import OpenAI

client = OpenAI(
    api_key="EMPTY",
    base_url="http://127.0.0.1:22002/v1",
    timeout=3600
)

messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "image_url",
                "image_url": {
                    "url": "https://ofasys-multimodal-wlcb-3-toshanghai.oss-accelerate.aliyuncs.com/wpf272043/keepme/image/receipt.png"
                }
            },
            {
                "type": "text",
                "text": "Read all the text in the image."
            }
        ]
    }
]

start = time.time()
response = client.chat.completions.create(
    model="Qwen/Qwen3-VL-235B-A22B-Instruct-FP8",
    messages=messages,
    max_tokens=2048
)
print(f"Response costs: {time.time() - start:.2f}s")
print(f"Generated text: {response.choices[0].message.content}")
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Video Request Example&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import time
from openai import OpenAI

client = OpenAI(
    api_key="EMPTY",
    base_url="http://127.0.0.1:22002/v1",
    timeout=3600
)

messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "video_url",
                "video_url": {
                    "url": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/space_woaudio.mp4"
                }
            },
            {
                "type": "text",
                "text": "How long is this video?"
            }
        ]
    }
]

start = time.time()
response = client.chat.completions.create(
    model="Qwen/Qwen3-VL-235B-A22B-Instruct-FP8",
    messages=messages,
    max_tokens=2048
)

print(f"Response costs: {time.time() - start:.2f}s")
print(f"Generated text: {response.choices[0].message.content}")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Offline Inference&lt;/h3&gt; 
&lt;p&gt;You can also use vLLM or SGLang to inference Qwen3-VL locally:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;vLLM Examples&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# -*- coding: utf-8 -*-
import torch
from qwen_vl_utils import process_vision_info
from transformers import AutoProcessor
from vllm import LLM, SamplingParams

import os
os.environ['VLLM_WORKER_MULTIPROC_METHOD'] = 'spawn'

def prepare_inputs_for_vllm(messages, processor):
    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    # qwen_vl_utils 0.0.14+ reqired
    image_inputs, video_inputs, video_kwargs = process_vision_info(
        messages,
        image_patch_size=processor.image_processor.patch_size,
        return_video_kwargs=True,
        return_video_metadata=True
    )
    print(f"video_kwargs: {video_kwargs}")

    mm_data = {}
    if image_inputs is not None:
        mm_data['image'] = image_inputs
    if video_inputs is not None:
        mm_data['video'] = video_inputs

    return {
        'prompt': text,
        'multi_modal_data': mm_data,
        'mm_processor_kwargs': video_kwargs
    }


if __name__ == '__main__':
    # messages = [
    #     {
    #         "role": "user",
    #         "content": [
    #             {
    #                 "type": "video",
    #                 "video": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/space_woaudio.mp4",
    #             },
    #             {"type": "text", "text": "这段视频有多长"},
    #         ],
    #     }
    # ]

    messages = [
        {
            "role": "user",
            "content": [
              {
                  "type": "image",
                  "image": "https://ofasys-multimodal-wlcb-3-toshanghai.oss-accelerate.aliyuncs.com/wpf272043/keepme/image/receipt.png",
              },
              {"type": "text", "text": "Read all the text in the image."},
            ],
        }
    ]

    # TODO: change to your own checkpoint path
    checkpoint_path = "Qwen/Qwen3-VL-235B-A22B-Instruct-FP8"
    processor = AutoProcessor.from_pretrained(checkpoint_path)
    inputs = [prepare_inputs_for_vllm(message, processor) for message in [messages]]

    llm = LLM(
        model=checkpoint_path,
        mm_encoder_tp_mode="data",
        enable_expert_parallel=True,
        tensor_parallel_size=torch.cuda.device_count(),
        seed=0
    )

    sampling_params = SamplingParams(
        temperature=0,
        max_tokens=1024,
        top_k=-1,
        stop_token_ids=[],
    )

    for i, input_ in enumerate(inputs):
        print()
        print('=' * 40)
        print(f"Inputs[{i}]: {input_['prompt']=!r}")
    print('\n' + '&amp;gt;' * 40)

    outputs = llm.generate(inputs, sampling_params=sampling_params)
    for i, output in enumerate(outputs):
        generated_text = output.outputs[0].text
        print()
        print('=' * 40)
        print(f"Generated text: {generated_text!r}")
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;SGLang Examples&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import time
from PIL import Image
from sglang import Engine
from qwen_vl_utils import process_vision_info
from transformers import AutoProcessor, AutoConfig


if __name__ == "__main__":
    # TODO: change to your own checkpoint path
    checkpoint_path = "Qwen/Qwen3-VL-235B-A22B-Instruct"
    processor = AutoProcessor.from_pretrained(checkpoint_path)

    messages = [
        {
            "role": "user",
            "content": [
              {
                  "type": "image",
                  "image": "https://ofasys-multimodal-wlcb-3-toshanghai.oss-accelerate.aliyuncs.com/wpf272043/keepme/image/receipt.png",
              },
              {"type": "text", "text": "Read all the text in the image."},
            ],
        }
    ]

    text = processor.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    image_inputs, _ = process_vision_info(messages, image_patch_size=processor.image_processor.patch_size)

    llm = Engine(
        model_path=checkpoint_path,
        enable_multimodal=True,
        mem_fraction_static=0.8,
        tp_size=4,
        attention_backend="fa3",
        context_length=10240,
        disable_cuda_graph=True,
    )

    start = time.time()
    sampling_params = {"max_new_tokens": 1024}
    response = llm.generate(prompt=text, image_data=image_inputs, sampling_params=sampling_params)
    print(f"Response costs: {time.time() - start:.2f}s")
    print(f"Generated text: {response['text']}")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Evaluation Reproduction&lt;/h2&gt; 
&lt;p&gt;To facilitate faithful reproduction of our reported results, we summarize our official evaluation settings below.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Inference runtime: &lt;a href="https://github.com/vllm-project/vllm"&gt;vLLM&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Evaluation frameworks: &lt;a href="https://github.com/open-compass/VLMEvalKit"&gt;VLMEvalKit&lt;/a&gt;, &lt;a href="https://github.com/EvolvingLMMs-Lab/lmms-eval"&gt;lmms-eval&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Notes: 
  &lt;ul&gt; 
   &lt;li&gt;For a few benchmarks, we slightly modified the evaluation prompts; detailed changes will be documented in the upcoming technical report.&lt;/li&gt; 
   &lt;li&gt;A small number of benchmarks are internally constructed; we plan to release the code and reproduction assets afterwards.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Generation Hyperparameters&lt;/h3&gt; 
&lt;h4&gt;Instruct models&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export greedy='false'
export seed=3407
export top_p=0.8
export top_k=20
export temperature=0.7
export repetition_penalty=1.0
export presence_penalty=1.5
export out_seq_length=32768
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Thinking models&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export greedy='false'
export seed=1234
export top_p=0.95
export top_k=20
export repetition_penalty=1.0
export presence_penalty=0.0
export temperature=0.6
export out_seq_length=40960
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;🐳 Docker&lt;/h2&gt; 
&lt;p&gt;To simplify the deploy process, we provide docker images with pre-build environments: &lt;a href="https://hub.docker.com/r/qwenllm/qwenvl"&gt;qwenllm/qwenvl&lt;/a&gt;. You only need to install the driver and download model files to launch demos.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run --gpus all --ipc=host --network=host --rm --name qwen3vl -it qwenllm/qwenvl:qwen3vl-cu128 bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you find our paper and code useful in your research, please consider giving a star &lt;span&gt;⭐&lt;/span&gt; and citation &lt;span&gt;📝&lt;/span&gt; :)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-BibTeX"&gt;
@article{Qwen2.5-VL,
  title={Qwen2.5-VL Technical Report},
  author={Bai, Shuai and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Song, Sibo and Dang, Kai and Wang, Peng and Wang, Shijie and Tang, Jun and Zhong, Humen and Zhu, Yuanzhi and Yang, Mingkun and Li, Zhaohai and Wan, Jianqiang and Wang, Pengfei and Ding, Wei and Fu, Zheren and Xu, Yiheng and Ye, Jiabo and Zhang, Xi and Xie, Tianbao and Cheng, Zesen and Zhang, Hang and Yang, Zhibo and Xu, Haiyang and Lin, Junyang},
  journal={arXiv preprint arXiv:2502.13923},
  year={2025}
}

@article{Qwen2-VL,
  title={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution},
  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang},
  journal={arXiv preprint arXiv:2409.12191},
  year={2024}
}

@article{Qwen-VL,
  title={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},
  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2308.12966},
  year={2023}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;br /&gt;</description>
    </item>
    
    <item>
      <title>GorvGoyl/Clone-Wars</title>
      <link>https://github.com/GorvGoyl/Clone-Wars</link>
      <description>&lt;p&gt;100+ open-source clones of popular sites like Airbnb, Amazon, Instagram, Netflix, Tiktok, Spotify, Whatsapp, Youtube etc. See source code, demo links, tech stack, github stars.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/GorvGoyl/Clone-Wars/main/img/og.png" alt="clone wars" /&gt;&lt;/p&gt; 
&lt;h1&gt;Clone Wars - Open source clones of popular sites&lt;/h1&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;100+ open-source clones and alternatives of popular sites like Airbnb, Amazon, Instagram, Netflix, TikTok, Spotify, WhatsApp, YouTube, etc. List contains source code, tutorials, demo links, tech stack, and GitHub stars count. Great for learning purpose!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;em&gt;-Made by &lt;a href="https://gourav.io" title="Gourav Goyal"&gt; Gourav Goyal &lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;h2&gt;See full tables with better view 👉 &lt;a href="https://gourav.io/clone-wars"&gt;gourav.io/clone-wars&lt;/a&gt;&lt;/h2&gt; 
&lt;hr /&gt; 
&lt;p&gt;I need your help to maintain this list up to date 🙏. See &lt;a href="https://raw.githubusercontent.com/GorvGoyl/Clone-Wars/main/#contribution-guide"&gt;contribution Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/GorvGoyl/Clone-Wars/issues/209"&gt;I'm also looking for a maintainer to merge PRs of new clones&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;There are 2 tables:&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/GorvGoyl/Clone-Wars/main/#clones-with-tutorials"&gt;Clones with Tutorials Table&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Full-stack clones with link to free tutorials.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/GorvGoyl/Clone-Wars/main/#clones-and-alternatives"&gt;Clones / Alternatives Table&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Two kinds of projects on this list:&lt;/p&gt; 
  &lt;ol&gt; 
   &lt;li&gt;Clones: look quite similar (UI-wise) but aren't fully-functional, mostly made for learning purposes.&lt;/li&gt; 
   &lt;li&gt;Alternatives: fully-functional open-source alternatives of popular software. Seeing GitHub stars will give you a rough idea about which one is which.&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Read the story behind this project: &lt;a href="https://gourav.io/blog/my-simple-github-project-went-viral"&gt;My simple GitHub project went viral 🚀&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Clones with Tutorials&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Clone of&lt;/th&gt; 
   &lt;th&gt;Demo&lt;/th&gt; 
   &lt;th&gt;Tutorial / Course Site&lt;/th&gt; 
   &lt;th&gt;Repo&lt;/th&gt; 
   &lt;th&gt;Tech Stack&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Airbnb&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=mx1dbMzd3tU&amp;amp;ab_channel=CodewithAniaKub%C3%B3w&amp;amp;t=9039s"&gt;YouTube&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=mx1dbMzd3tU&amp;amp;ab_channel=CodewithAniaKub%C3%B3w"&gt;YouTube&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/kubowania/airbnb-sanity-backend"&gt;GitHub backend&lt;/a&gt;, &lt;a href="https://github.com/kubowania/airbnb-sanity-frontend"&gt;frontend&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Sanity SDK, Next.js, React Hooks&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Instagram&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=1hPgQWbWmEk&amp;amp;t=5h2902s"&gt;YouTube&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.freecodecamp.org/news/build-an-instagram-clone-with-react-native-firebase-firestore-redux-and-expo/"&gt;freeCodeCamp&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/SimCoderYoutube/InstagramClone"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React Native, Firebase Firestore, Firebase storage, Redux, Expo&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Moodle&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=JIFqqdRxmVo&amp;amp;t=45s"&gt;YouTube&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.freecodecamp.org/news/django-rest-framework-react-tutorial/"&gt;freeCodeCamp&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/justdjango/teach-me-django"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React, Django Rest Framework&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Netflix&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=g8COh40v2jU&amp;amp;t=8241s"&gt;YouTube&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=g8COh40v2jU"&gt;YouTube&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/kubowania/netflix-clone-graphql-datastax"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React, Apollo GraphQL, DataStax Astra, Netlify&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Todoist&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/karlhadwen/todoist/master/todoist-preview.png"&gt;todoist-preview.png&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.freecodecamp.org/news/react-firebase-todoist-clone/"&gt;freeCodeCamp&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/karlhadwen/todoist"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React, Firebase, React, SCSS, BEM naming methodology&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Twitter&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=la-0ulfn0_M&amp;amp;t=8864s"&gt;YouTube&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.freecodecamp.org/news/create-a-cross-platform-twitter-clone-with-vue-js/"&gt;freeCodeCamp&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/dannyconnell/qwitter"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Vue.js, Quasar Framework, Firebase&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;WhatsApp&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=988UZFB0heA&amp;amp;t=5h274s"&gt;YouTube&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.freecodecamp.org/news/native-android-app-tutorial-whatsapp-clone/"&gt;freeCodeCamp&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/SimCoderYoutube/WhatsAppClone"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Android Studio, Firebase, Genymotion&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Discord&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=PtQiiknWUcI"&gt;YouTube&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=PtQiiknWUcI"&gt;Youtube (Traversy Media)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/divanov11/StudyBud/"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Django&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;YouTube&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://youtu.be/FHTbsZEJspU?t=6"&gt;YouTube&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=FHTbsZEJspU"&gt;Youtube (JavaScript Mastery)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/adrianhajdin/project_youtube_clone"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React JS, Rapid API, Material UI 5&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;YouTube&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=whuIf33v2Ug&amp;amp;t=189s"&gt;YouTube&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.freecodecamp.org/news/learn-how-to-use-the-yii2-php-framework-to-create-a-youtube-clone/"&gt;freeCodeCamp&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/thecodeholic/Yii2-YouTube-Clone"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Yii2 PHP Framework&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;YouTube&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=WBlScMZgn3w&amp;amp;t=9086s"&gt;YouTube&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/playlist?list=PLA7YMGupLhlF0N0eQfJKAq1j9Xa3xuiM8"&gt;YouTube&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Breens-Mbaka/Youtube-Clone"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Android Studio, Kotlin, XML, YouTube API&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Clones and Alternatives&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;(scroll right on table to see all 5 columns)&lt;/em&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Clone/Alt of&lt;/th&gt; 
   &lt;th&gt;Demo&lt;/th&gt; 
   &lt;th&gt;Repo&lt;/th&gt; 
   &lt;th&gt;Tech stack&lt;/th&gt; 
   &lt;th&gt;Repo Stars&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;1Password / LastPass&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://bitwarden.com/"&gt;bitwarden.com&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/bitwarden/mobile"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;C#, Xamarin&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/bitwarden/mobile" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;2048&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://gh.artemchep.com/2048/#/"&gt;gh.artemchep.com&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/AChep/2048"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Dart, Flutter&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/AChep/2048" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;2048&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://2048-three.vercel.app/"&gt;2048-three.vercel.app&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/AreebKhan619/2048"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/AreebKhan619/2048" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;2048&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://demo.matsz.dev/2048/"&gt;demo.matsz.dev&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/mat-sz/2048"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React, TypeScript, Redux&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/mat-sz/2048" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;2048&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://guaracy.github.io/beads/sites/2048/2048.html"&gt;guaracy.github.io&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/guaracy/2048-beads"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Beads&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/guaracy/2048-beads" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;2048&lt;/td&gt; 
   &lt;td&gt;&lt;a href="http://oddrationale.github.io/dart_2048/"&gt;oddrationale.github.io&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/oddrationale/dart_2048"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Dart&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/oddrationale/dart_2048" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Airbnb&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://abod-bnb.web.app/"&gt;abod-bnb.web.app&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/abodmicheal/React-projects/tree/master/Abodbnb"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React, Firebase&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/abodmicheal/React-projects" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Airbnb&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://realbnb.vercel.app/"&gt;realbnb.vercel.app&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/shubhsk88/realbnb-frontend"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;TypeScript, React, NextJS, Prisma, GraphQL&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/shubhsk88/realbnb-frontend" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Airbnb&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://airbnb-clone-black-seven.vercel.app/"&gt;airbnb-clone-black-seven.vercel.app/&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/juan-20/Airbnb-clone"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Next, Tailwind, SEO, TypeScript&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://flat.badgen.net/github/stars/juan-20/Airbnb-clone" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Airtable&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://rowy.io/"&gt;rowy.io&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/rowyio/rowy"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React, Firebase, Firestore, Google Cloud Platform, Cloud Functions, TypeScript&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/rowyio/rowy" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Airtable&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://baserow.io/"&gt;baserow&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://gitlab.com/bramw/baserow"&gt;GitLab&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Django, nuxt.js, PostgreSQL&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Algolia&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.meilisearch.com/"&gt;meilisearch.com&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/meilisearch/MeiliSearch"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Rust&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/meilisearch/MeiliSearch" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Amazon&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://amazonna.netlify.app/"&gt;amazonna.netlify.app&lt;/a&gt; &lt;br /&gt; &lt;a href="https://youtu.be/MSctN0MOdt0"&gt;youtube&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/emmanuelhashy/amazon-clone"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React, Firebase&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/emmanuelhashy/amazon-clone" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Amazon Prime Video&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://prime-clone-e1de6.firebaseapp.com/"&gt;prime-clone-e1de6.firebaseapp&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/peeyush14goyal/AmazonPrime-ReactJS-Clone"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/peeyush14goyal/AmazonPrime-ReactJS-Clone" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Apple Music&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://appo-music.herokuapp.com/"&gt;appo-music.herokuapp&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/oriravid/Appo-Music"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React, Redux, Ruby on Rails, PostgreSQL&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/oriravid/Appo-Music" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ArcoMage&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://arcomage.github.io/"&gt;arcomage.github.io&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/arcomage/arcomage-hd"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;TypeScript, React, Redux, RxJS, Sass, WebRTC&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/arcomage/arcomage-hd" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Auth0&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.ory.sh/docs/ecosystem/projects/"&gt;ory.sh&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ory"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Go&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/ory" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Bit.ly&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://polrproject.org/"&gt;polrproject.org&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/cydrobolt/polr"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;PHP, MySQL&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/cydrobolt/polr" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Bit.ly&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://shlink.io/"&gt;shlink.io&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/shlinkio/shlink"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;PHP, Mezzio, Doctrine, Symfony&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/shlinkio/shlink" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Bit.ly&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://zws.im/"&gt;zws.im&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/zws-im/zws"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;TypeScript,&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/zws-im/zws" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Bit.ly&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://kutt.it/"&gt;kutt.it&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/thedevs-network/kutt"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;TypeScript,&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/thedevs-network/kutt" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Bit.ly&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://shortl.it"&gt;shortl.it&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/CraftException/shortl.it"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;TypeScript, NodeJS, EJS, JQuery&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://flat.badgen.net/github/stars/CraftException/shortl.it" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BrickGame&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://nabeelshar.github.io/Retro-Brick-Game/"&gt;Retro-Brick-Game&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Nabeelshar/Retro-Brick-Game"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React, Redux&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://flat.badgen.net/github/stars/Nabeelshar/Retro-Brick-Gamet" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Battleship Game&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://battleboats.ito.wtf/"&gt;battleboats.ito.wtf&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/hairarrow/btship"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;TypeScript, React&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/hairarrow/btship" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Calendly&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://calendso.com/"&gt;calendso.com&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/calendso/calendso"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Next.js, Typescript, React, Tailwind, Prisma&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/calendso/calendso" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Clubhouse&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://jam.systems/"&gt;jam.systems&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/jam-systems/jam"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React, WebRTC&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/jam-systems/jam" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Clubhouse&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://dogehouse.tv"&gt;dogehouse.tv&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/benawad/dogehouse"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React, Postgress, Elixir&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/benawad/dogehouse" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Codecademy&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://codecademyclone.netlify.app/"&gt;codecademyclone.netlify.app&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/shanoysinc/Codecademy-clone"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/shanoysinc/Codecademy-clone" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;DEV.to&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://devfrom.netlify.app/"&gt;devfrom.netlify.app&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/eknoorpreet/dev.to-clone"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/eknoorpreet/dev.to-clone" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Discord&lt;/td&gt; 
   &lt;td&gt;&lt;a href="http://ericellb.github.io/React-Discord-Clone"&gt;ericellb.github.io&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ericellb/React-Discord-Clone"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React, Node, Express, Socket-IO, MySQL&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/ericellb/React-Discord-Clone" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Discord&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://valkyrieapp.xyz"&gt;valkyrieapp&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/sentrionic/Valkyrie"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React, NestJS, TypeScript, Socket-IO, PostgreSQL&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/sentrionic/Valkyrie" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Discord&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://dev.fosscord.com/app"&gt;dev.fosscord.com&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/fosscord/fosscord"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;TypeScript, Express, WebRTC, Websockets, TypeORM, SQLite&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/fosscord/fosscord" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Disney+&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/calebnance/expo-disneyplus/raw/master/.gh-assets/screenshare-4.png?raw=true"&gt;github.com (screenshot)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/calebnance/expo-disneyplus"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React Native, expo&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/calebnance/expo-disneyplus" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Disney+ Hotstar&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://determined-bardeen-6a04b8.netlify.app/"&gt;determined-bardeen-6a04b8.netlify&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/anandhu720/disney-hotstar-clone.git"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ReactJS&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/anandhu720/disney-hotstar-clone" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Doodle&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://sm2030.user.srcf.net/poll/"&gt;sm2030.user.srcf.net&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/souramoo/PhpMeeting"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;PHP&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/souramoo/PhpMeeting" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Dribbble&lt;/td&gt; 
   &lt;td&gt;&lt;a href="http://driwwwle.herokuapp.com/"&gt;driwwwle.herokuapp&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/itsnitinr/driwwwle"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;MERN, react&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/itsnitinr/driwwwle" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Dribbble&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://v.redd.it/a8lhmpjho0051/"&gt;v.redd.it&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/diazabdulm/rumbbble"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;MongoDB, ExpressJS, React,&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/diazabdulm/rumbbble" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Dropbox&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://try.nextcloud.com/"&gt;try.nextcloud.com&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/nextcloud/server"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;PHP, JS&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/nextcloud/server" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Dropbox&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://Live.filegator.io"&gt;Live.filegator.io&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/filegator/filegator"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;PHP, JS, Vuejs&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/filegator/filegator" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Evernote&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://joplinapp.org/"&gt;joplinapp.org&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/laurent22/joplin"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;JavaScript, TypeScript&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/laurent22/joplin" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Express JS&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://robiul.dev/i-made-a-custom-version-of-expressjs-framework-cute-express"&gt;robiul.dev&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/robiulhr/cute-express"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Node js&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/robiulhr/cute-express" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Facebook&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://odinclone.herokuapp.com/"&gt;odinclone.herokuapp&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/lucaskenji/odinclone-app"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;MERN&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/lucaskenji/odinclone-app" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Facebook&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/KristianWEB/fakebooker-frontend#screenshots"&gt;github.com (screenshot)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/KristianWEB/fakebooker-frontend"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;react, graphql, mongodb&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/KristianWEB/fakebooker-frontend" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Facebook&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://clonedbook.vercel.app/"&gt;clonedbook.vercel.app&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/zivavu/Clonedbook"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Next.js, MUI, Firebase, Faker.js&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/zivavu/Clonedbook" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Firebase&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://appwrite.io/"&gt;appwrite.io&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/appwrite/appwrite"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;PHP&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/appwrite/appwrite" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Firebase&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://supabase.io/"&gt;supabase.io&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/supabase/supabase"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Elixir,React,PostgreSQL,Python&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/supabase/supabase" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Firebase&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://nhost.io/"&gt;nhost.io&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/nhost/nhost"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;PostgreSQL, GraphQL, TypeScript, Go&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/nhost/nhost" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Gmail&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/rodydavis/gmail_clone#screenshots"&gt;github.com (screenshot)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/rodydavis/gmail_clone"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Flutter&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/rodydavis/gmail_clone" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Google&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://searchify.vercel.app"&gt;searchify.vercel.app&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/jusoftdev/searchify"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;NextJS, TailwindCSS, Google Search API&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://flat.badgen.net/github/stars/jusoftdev/searchify" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Google Analytics&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://plausible.io/"&gt;plausible.io&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/plausible/analytics"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React, Elixir, PostgreSQL, Tailwind&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/plausible/analytics" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Google Analytics&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://matomo.org/"&gt;matomo.org&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/matomo-org/matomo"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;PHP, HTML, MySQL&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/matomo-org/matomo" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Google Analytics&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ackee.electerious.com/"&gt;ackee.electerious.com&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/electerious/Ackee"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Node, MongoDB&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/electerious/Ackee" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Google Analytics&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://learnsql.io"&gt;learnsql.io&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/hypercable/hypercable"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Rails, OpenResty, TimescaleDB, postgresql, tailwindcss&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/hypercable/hypercable" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Google Drive&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://mydrive-3.herokuapp.com/"&gt;mydrive-3.herokuapp&lt;/a&gt; &lt;br /&gt; &lt;a href="https://mydrive-storage.com/"&gt;mydrive-storage.com&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/subnub/myDrive"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React, Node.js, mongoDB&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/subnub/myDrive" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Google Keep&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/anselm94/googlekeepclone/raw/master/docs/mobile-add-todo.gif"&gt;github.com (gif)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/anselm94/googlekeepclone"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React, GraphQL, Golang, SQlite&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/anselm94/googlekeepclone" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Google Keep&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://vue-keep-sepia.vercel.app/"&gt;vue-keep-sepia.vercel.app&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/wobsoriano/vue-keep"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Vue, TypeScript, Firestore&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/wobsoriano/vue-keep" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Google Keep&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://google-keeps-clone.herokuapp.com/"&gt;google-keeps-clone.herokuapp&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/kimlimjustin/google-keep-clone"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Django, JS&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/kimlimjustin/google-keep-clone" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Google Forms&lt;/td&gt; 
   &lt;td&gt;&lt;a href="http://google-forms-clone.herokuapp.com/"&gt;google-forms-clone.herokuapp&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/kimlimjustin/google-form-clone"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Django, JS&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/kimlimjustin/google-form-clone" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Google Forms&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://handform-c62a3.web.app/"&gt;https://handform-c62a3.web.app/&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ezza022/handform"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React, Firebase&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/ezza022/handform" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Google Photos&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://photos-clone.web.app/"&gt;photos-clone.web.app&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/mani-barathi/Google-Photos-Clone"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React, Redux, Firebase&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/mani-barathi/Google-Photos-Clone" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Google Photos&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://photoprism.app/"&gt;photoprism.app&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/photoprism/photoprism"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Go,TensorFlow&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/photoprism/photoprism" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Google Translate&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://libretranslate.com/"&gt;libretranslate.com&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/uav4geo/LibreTranslate"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Python, Flask&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/uav4geo/LibreTranslate" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Hashnode&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://hashnode-clone-sass.netlify.app"&gt;hashnode-clone-sass.netlify.app&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Evavic44/hashnode-clone"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;HTML, SASS, JavaScript&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/evavic44/hashnode-clone" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Headspace&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://meditofoundation.org/medito-app"&gt;meditofoundation.org&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/meditohq/medito-app"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Flutter&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/meditohq/medito-app" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Hacker News&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://hackernews-redesign.netlify.app/"&gt;hackernews-redesign.netlify&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/RocktimSaikia/hacker-news-redesign"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/RocktimSaikia/hacker-news-redesign" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Hacker News&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://news.python.sc/"&gt;news.python&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/sebst/pythonic-news"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Python, Django&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/sebst/pythonic-news" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Hacker News&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://rm-hackernews.netlify.app"&gt;rm-hackernews.netlify.app&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/rajat-mehra05/hacker-news-clone"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Reactjs&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/rajat-mehra05/hacker-news-clone" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Hacker News&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://hackernews-jaywhen.vercel.app/"&gt;hackernews-jaywhen.vercel.app&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/jaywhen/hackernews"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Next.js&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://flat.badgen.net/github/stars/jaywhen/hackernews" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;HotJar&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://formbricks.com/"&gt;Formbricks&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/formbricks/formbricks"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;JavaScript&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/formbricks/formbricks" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Hulu&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://fake-hulu-eosin.vercel.app"&gt;fake-hulu-eosin.vercel.app&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/SalahSajar/FakeHulu"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Next.js, Sass, Firebase, tmdb-api&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://flat.badgen.net/github/stars/SalahSajar/FakeHulu" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;IMDB&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://movie4fun.netlify.app/"&gt;movie4fun.netlify.app&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/zerone0x/imdb_clone.git"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React, Redux, ReactQuery, tmdb-api&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/zerone0x/imdb_clone" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Instagram&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://maxgram.zabarka.com/"&gt;maxgram.zabarka.com&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/MaxZabarka/instagram-clone"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;MongoDB, Express, Reactjs, Node&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/MaxZabarka/instagram-clone" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Instagram&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.instaclone.net/login"&gt;instaclone.net&lt;/a&gt; &lt;br /&gt;&lt;a href="https://github.com/Sandermoen/instaclone/raw/master/screenshots/NgmjOVkZ4L.gif"&gt;github.com (gif)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Sandermoen/instaclone"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Reactjs, Express, Nodejs, Mongodb, Socketio&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/Sandermoen/instaclone" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Instagram&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/vipulasri/JetInstagram/raw/master/art/jetinstagram.gif"&gt;github.com (gif)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/vipulasri/JetInstagram"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Jetpack Compose&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/vipulasri/JetInstagram" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Instagram&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://instagram-clone-dbe40.web.app/"&gt;instagram-clone-dbe40.web.app&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/shaan71845/instagram-clone"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React, Firebase&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/shaan71845/instagram-clone" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Intercom&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.papercups.io/"&gt;papercups.io&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/papercups-io/papercups"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React, Elixir, Phoenix&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/papercups-io/papercups" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Intercom&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.chatwoot.com/"&gt;chatwoot.com&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/chatwoot/chatwoot"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;JAMStack, Vue&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/chatwoot/chatwoot" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;iOS Homescreen&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ios-homescreen.now.sh/"&gt;ios-homescreen.now.sh&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/erickbogarin/ios-homescreen"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React, Next.js, Emotion&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/erickbogarin/ios-homescreen" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Jira&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://jira.sebastianfdz.com/project/backlog"&gt;jira.sebastianfdz.com&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/sebastianfdz/jira_clone"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Next.js, React-Query Radix UI, Clerk Auth, Zod, TailwindCSS&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://flat.badgen.net/github/stars/sebastianfdz/jira_clone" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Jira&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://jira.ivorreic.com"&gt;jira.ivorreic.com&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/oldboyxx/jira_clone"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Express(Typescript), JWT, TypeORM, PostgreSQL, React&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://flat.badgen.net/github/stars/oldboyxx/jira_clone" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Jira&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://jira.trungk18.com/"&gt;jira.trungk18&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/trungk18/jira-clone-angular"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Angular, Akita, TailwindCSS, ng-zorro&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://flat.badgen.net/github/stars/trungk18/jira-clone-angular" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;LaunchDarkly&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://getunleash.io"&gt;getunleash.io&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Unleash/unleash"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Java, Node.js, Go, Python, Ruby, .Net, JavaScript, React, Android, iOS,&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/Unleash/unleash" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;LaunchDarkly&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://flagsmith.com"&gt;flagsmith.com&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/flagsmith/flagsmith-api"&gt;GitHub&lt;/a&gt; &lt;br /&gt; &lt;a href="https://github.com/flagsmith/flagsmith-frontend"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Python, Django, React&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/flagsmith/flagsmith-api" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Linear.app&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=XVAek-hE5X8"&gt;youtube.com&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/tuan3w/linearapp_clone"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React, Redux, TailwindCSS&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/tuan3w/linearapp_clone" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MacOS Calculator&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://chamoda.com/react-calculator/"&gt;chamoda.com&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/chamoda/react-calculator"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/chamoda/react-calculator" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MacOS Finder Clone&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://finder-clone.netlify.app/"&gt;finder-clone.netlify.app&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Guy6767/finder-clone"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React, Sass&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/Guy6767/finder-clone" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Medium / Wordpress / Substack&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ghost.org/"&gt;ghost.org&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/TryGhost/Ghost"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;JAMStack, Ember, Node, MySQL&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/TryGhost/Ghost" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Medium&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://next-realworld.now.sh/"&gt;next-realworld.now.sh&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/reck1ess/next-realworld-example-app"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React, Next.js&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/reck1ess/next-realworld-example-app" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Messenger&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://tippindev.com/"&gt;tippindev.com&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/rtippin/messenger"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;PHP/Laravel, MySQL, Websockets&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/rtippin/messenger" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MS Paint&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://jspaint.app"&gt;jspaint.app&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/1j01/jspaint"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Node.js&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/1j01/jspaint" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Netflix [Fakeflix]&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://fakeflix.th3wall.codes"&gt;fakeflix.th3wall.codes&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Th3Wall/Fakeflix"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React, Redux, Firebase&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://flat.badgen.net/github/stars/Th3Wall/Fakeflix" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Netflix&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://netflix-clone-react-typescript.vercel.app"&gt;netflix-clone-react-typescript.vercel.app&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/endo-aki22/netflix-clone-react-typescript"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React(v18), RTK(ReduxToolKit), Typescript, TMDB API, MUI, Video.js, Framer Motion, Slick Carousel, Docker&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://flat.badgen.net/github/stars/endo-aki22/netflix-clone-react-typescript" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Netflix&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://azazel5.github.io/NetflixClone/"&gt;azazel5.github.io&lt;/a&gt; &lt;br /&gt; &lt;a href="https://github.com/Azazel5/NetflixClone/raw/master/flixLive.gif"&gt;github.com (gif)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Azazel5/NetflixClone"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React, Redux&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://flat.badgen.net/github/stars/Azazel5/NetflixClone" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Netflix&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://netflix-clone-dd230.web.app/"&gt;netflix-clone-dd230.web.app&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/abodmicheal/React-projects/tree/master/Single-Channel-Messanger"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React, Firebase&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://flat.badgen.net/github/stars/abodmicheal/React-projects" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Netflix&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/karlhadwen/netflix/raw/master/netflix-preview.png"&gt;github.com (screenshot)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/karlhadwen/netflix"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React, Firebase&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://flat.badgen.net/github/stars/karlhadwen/netflix" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Netflix&lt;/td&gt; 
   &lt;td&gt;&lt;a href="http://spaceflix.herokuapp.com/#/"&gt;spaceflix.herokuapp&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ehecker/spaceflix"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Ruby, React, Redux, PostgreSQL, AWS&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://flat.badgen.net/github/stars/ehecker/spaceflix" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Netflix&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://expo-netflix.calebnance.now.sh/"&gt;expo-netflix.calebnance.now.sh&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/calebnance/expo-netflix"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React Native, expo&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://flat.badgen.net/github/stars/calebnance/expo-netflix" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Netflix&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://roseflix-rosebilag.vercel.app/"&gt;roseflix-rosebilag.vercel&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/rosebilag/roseflix/"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React, TypeScript, MongoDB&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://flat.badgen.net/github/stars/rosebilag/roseflix" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Netflix&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://nfx.vercel.app/"&gt;nfx.vercel.app&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/saurabhchardereal/nfx/"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React, TypeScript, SCSS Modules&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://flat.badgen.net/github/stars/saurabhchardereal/nfx" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Netflix&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://android_netflix.surge.sh/"&gt;Wep-App&lt;/a&gt; ,&lt;a href="https://play.google.com/store/apps/details?id=com.moviestrailer.app"&gt;android&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ismailazdad/NetflixClone_Capacitor_Android"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React, Capacitor, Pwa&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://flat.badgen.net/github/stars/ismailazdad/NetflixClone_Capacitor_Android" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Nike&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/saeed9321/Nike-Website-clone/raw/main/screen-capture.gif"&gt;gif&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/saeed9321/Nike-Website-clone"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Express, MongoDB&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/saeed9321/Nike-Website-clone" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Notion&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.focalboard.com/"&gt;focalboard.com&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/mattermost/focalboard"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Node, React, Go&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/mattermost/focalboard" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Okta&lt;/td&gt; 
   &lt;td&gt;[topaz.sh](&lt;a href="https://www.topaz.sh"&gt;https://www.topaz.sh&lt;/a&gt; projects/)&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/aserto-dev/topaz"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Go&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://flat.badgen.net/github/stars/aserto-dev/topaz" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Okta&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.ory.sh/docs/ecosystem/projects/"&gt;ory.sh&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ory"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Go&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://flat.badgen.net/github/stars/ory" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Obsidian&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.zettlr.com/"&gt;zettlr.com&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Zettlr/Zettlr"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Electron, Vue, Markdown&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/Zettlr/Zettlr" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Omegle&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://start-a-conversation.firebaseapp.com/"&gt;start-a-conversation.firebaseapp&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/NickMandylas/start-a-conversation"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React, Firebase, Twilio&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/NickMandylas/start-a-conversation" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Orkut&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://orkutnostalgia.netlify.app/"&gt;orkutnostalgia.netlify.app&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/GShadowBroker/orkut-clone-client"&gt;GitHub&lt;/a&gt; &lt;br /&gt; &lt;a href="https://github.com/GShadowBroker/orkut-clone-server"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ReactJS, GraphQL, Apollo, PostgreSQL&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/GShadowBroker/orkut-clone-client" alt="GitHub Repo stars" /&gt; &lt;img src="https://img.shields.io/github/stars/GShadowBroker/orkut-clone-server" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;PayTM&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/flutter-devs/flutter_paytm_clone#demo"&gt;github.com (gif)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/flutter-devs/flutter_paytm_clone"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Flutter&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/flutter-devs/flutter_paytm_clone" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Pinterest&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://trusting-euler-8aafb8.netlify.app/"&gt;trusting-euler-8aafb8.netlify.app&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/machadop1407/pinterest-clone-frontend"&gt;GitHub&lt;/a&gt; &lt;br /&gt; &lt;a href="https://github.com/machadop1407/pinterest-clone-backend"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;GraphQL, ReactJS, NodeJS, Mysql&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/machadop1407/pinterest-clone-frontend" alt="GitHub Repo stars" /&gt; &lt;img src="https://img.shields.io/github/stars/machadop1407/pinterest-clone-backend" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Postman&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://firecamp.dev/"&gt;firecamp.dev&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/firecamp-dev/firecamp"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Electron, ReactJS, NodeJS, Typescript&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/firecamp-dev/firecamp" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Postman&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://insomnia.rest/"&gt;insomnia.rest&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Kong/insomnia"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Electron&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/Kong/insomnia" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Postman&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://hoppscotch.io/"&gt;hoppscotch.io&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/hoppscotch/hoppscotch"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;JAMStack, Vue, NuxtJS, firebase&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/hoppscotch/hoppscotch" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Privnote&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://secret.roushik.com/"&gt;secret.roushik.com&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/roushikk/secret_notes"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Django, Postgres, jQuery&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/roushikk/secret_notes" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Reddit&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.troddit.com/"&gt;troddit.com&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/burhan-syed/troddit"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;NextJS, React, TailwindCSS, TypeScript&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/burhan-syed/troddit" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Reddit&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://redditsyncr.netlify.app/"&gt;redditsyncr.netlify.app&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/dustinkiselbach/reddit-refactor"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React, TypeScript&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/dustinkiselbach/reddit-refactor" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Reddit&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://asperitas.now.sh/"&gt;asperitas.now.sh&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/d11z/asperitas"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Node.js, React, NoSQL&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/d11z/asperitas" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Reddit&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://join-lemmy.org/"&gt;Lemmy&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/LemmyNet"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ActivityPub, Rust, Postgres, Docker&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/LemmyNet/lemmy" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Reddit&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://kbin.pub/en"&gt;kbin&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://codeberg.org/Kbin/kbin-core"&gt;Codeberg&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ActivityPub, PHP, Postgres, Docker&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://badgers.space/codeberg/stars/Kbin/kbin-core" alt="Codeberg Release" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Reddit&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://sublinks.org/"&gt;Sublinks&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/sublinks"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ActivityPub, Java, TypeScript, MariaDB&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/sublinks/sublinks-api" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Reddit&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://join.piefed.social/"&gt;PieFed&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://codeberg.org/rimu/pyfedi/"&gt;Codeberg&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ActivityPub, Python, Postgres, Redis&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://badgers.space/codeberg/stars/rimu/pyfedi" alt="Codeberg Release" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Retool&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://budibase.com/"&gt;budibase.com&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Budibase/budibase"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Node.js, Svelte&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://flat.badgen.net/github/stars/budibase/budibase" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Retool&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.illacloud.com/"&gt;ILLA Cloud&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/illacloud/illa-builder"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React, Typescript, Go&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://flat.badgen.net/github/stars/illacloud/illa-builder" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Telegram&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://tinode.co/"&gt;tinode.co&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/tinode/chat"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Go, React, Java, Swift, MySQL, MongoDB, RethinkDB&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/tinode/chat" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;TikTok + Reddit&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://reddit-tiktok.netlify.app/"&gt;reddit-tiktok.netlify.app&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/hedgecox/Reddit-TikTok-Clone"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/hedgecox/Reddit-TikTok-Clone" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Twitch&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://twitchclone.vercel.app/"&gt;twitchclone.vercel.app&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/alissonsleal/twitch-clone/"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React, Next.JS, TypeScript&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/alissonsleal/twitch-clone" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Slack&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://mattermost.com/"&gt;mattermost.com&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/mattermost/mattermost-webapp"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React, Go&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/mattermost/mattermost-webapp" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Slack&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://rocket.chat/"&gt;rocket.chat&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/RocketChat/Rocket.Chat"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;JAMStack, TypeScript&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/RocketChat/Rocket.Chat" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Slack&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://zulip.com/"&gt;zulip.com&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/zulip/zulip"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Python, JS, TS&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/zulip/zulip" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Slack&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/calebnance/expo-slack/raw/master/screenshots/screenshare-3_v1.jpg?raw=true"&gt;github.com (screenshot)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/calebnance/expo-slack"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React Native, Expo&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/calebnance/expo-slack" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Slido&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://askent.berlinchan.com/"&gt;askent.berlinchan.com &lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/BerlinChan/askent"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React, Material-UI, Apollo GraphQL, Hasura, TypeORM, TypeGraphQL, TypeScript&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/BerlinChan/askent" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Sliders Game&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://js-sliders-game.rolandjlevy.repl.co/"&gt;Play the game&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/rolandjlevy/js-sliders-game"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Vanilla JavaScript, CSS, MongoDb&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://flat.badgen.net/github/stars/rolandjlevy/js-sliders-game" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Snapchat&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://towhidkashem.github.io/snapchat-clone/"&gt;towhidkashem.github.io&lt;/a&gt; &lt;br /&gt; &lt;a href="https://youtu.be/aRS88v-duKg"&gt;youtube&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/TowhidKashem/snapchat-clone"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React, Redux, TypeScript, Cypress, Jest, Enzyme&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/TowhidKashem/snapchat-clone" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Spotify&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://spotify.trungk18.com/"&gt;spotify.trungk18&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/trungk18/angular-spotify"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Angular 11, Nx, ngrx, TailwindCSS and ng-zorro&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://flat.badgen.net/github/stars/trungk18/angular-spotify" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Spotify&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/JL978/spotify-clone-client/raw/master/Live/Authed.gif"&gt;github.com (gif)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/JL978/spotify-clone-client"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://flat.badgen.net/github/stars/JL978/spotify-clone-client" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Spotify Web Player&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://spotify-clone-oguz3.web.app/"&gt;spotify-clone-oguz3.web.app&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/oguz3/spotify-web-player"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://flat.badgen.net/github/stars/oguz3/spotify-web-player" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Spotify&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/matpandino/spotify-ui-clone/raw/master/.github/spotify-clone.jpg"&gt;screenshot&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/matpandino/spotify-ui-clone"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Electron, React, TypeScript&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://flat.badgen.net/github/stars/matpandino/spotify-ui-clone" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Spotify&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://drive.google.com/file/d/1KOHOJDGaLeumGqQenzCeeShPqIudCJBX/view"&gt;drive.google.com (gif)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/aashrafh/Oud"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://flat.badgen.net/github/stars/aashrafh/Oud" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Spotify&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://tune42-spotify.netlify.app/"&gt;tune42-spotify.netlify.app&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Tune42/spotify-clone"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://flat.badgen.net/github/stars/Tune42/spotify-clone" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Spotify&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://expo-spotify.vercel.app/"&gt;expo-spotify.vercel.app&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/calebnance/expo-spotify"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React Native, Expo&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://flat.badgen.net/github/stars/calebnance/expo-spotify" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Spotify&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/osamaq/spotify-lite#screenshots"&gt;github.com (screenshot)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/osamaq/spotify-lite"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React Native&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://flat.badgen.net/github/stars/osamaq/spotify-lite" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Spotify&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://30sekify.netlify.app"&gt;30sekify.netlify.app&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/s-codes14/30sekify"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React, Electron&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://flat.badgen.net/github/stars/s-codes14/30sekify" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Spotify&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://30sekify.netlify.app"&gt;30sekify.netlify.app&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/s-codes14/30sekify"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React, Electron&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://flat.badgen.net/github/stars/s-codes14/30sekify" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Spotify + Soundcloud + YouTube&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.kord.app"&gt;kord.app&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/bundit/kord-app"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React, Redux, Express, PostgreSQL&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://flat.badgen.net/github/stars/bundit/kord-app" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Stack Overflow&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://live.scoold.com"&gt;live.scoold&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Erudika/scoold"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Java, jQuery, Para&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/Erudika/scoold" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Stack Overflow&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://clone-of-stackoverflow.vercel.app/"&gt;clone-of-stackoverflow.vercel&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/salihozdemir/stackoverflow-clone"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ReactJs, NextJs, Express, MongoDB&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/salihozdemir/stackoverflow-clone" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Stack Overflow&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://drive.google.com/file/d/1A0B3JPUUY2snG8MLZpyz2LWhvThG2epn/view"&gt;drive.google.com (gif)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Mayank0255/Stack-Overflow-Clone"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;MySQL, Express, React&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/Mayank0255/Stack-Overflow-Clone" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Stack Overflow&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://stackunderflow.netlify.app"&gt;stackunderflow.netlify.app&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/amand33p/stack-underflow-mern-gql"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;MongoDB, Express, React, NodeJS&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/amand33p/stack-underflow-mern-gql" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Temp-Mail&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://simplelogin.io"&gt;simplelogin.io&lt;/a&gt; &lt;br /&gt; &lt;a href="https://www.youtube.com/watch?v=JMWfsOVrDkw"&gt;youtube&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/simple-login/app"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Python, Docker, Vue&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/simple-login/app" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Tetris Game&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://hinsxd-tetris.netlify.app/"&gt;hinsxd-tetris.netlify.app&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/hinsxd/react-tetris"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React, TypeScript&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/hinsxd/react-tetris" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Tetris Game&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://tetris20.netlify.app/"&gt;tetris20.netlify.app&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/shanoysinc/Tetris"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;TypeScript&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/shanoysinc/Tetris" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Tetris Game React&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://chvin.github.io/react-tetris/"&gt;chvin.github.io&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/chvin/react-tetris"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React, Redux, Web Audio Api&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/chvin/react-tetris" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;TikTok&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/kirkwat/tiktok"&gt;github.com (gif)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/kirkwat/tiktok"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React Native, Firebase, TypeScript&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://flat.badgen.net/github/stars/kirkwat/tiktok" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;TikTok&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://youtu.be/iYqDUjvsS3o"&gt;youtube.com&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/emmanuelhashy/tik-tok-clone"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React, Firebase&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://flat.badgen.net/github/stars/emmanuelhashy/tik-tok-clone" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;TikTok&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://youtu.be/sMKg6ILYgv0"&gt;youtube.com&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/salvadordeveloper/TikTok-Flutter"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Flutter, Firebase&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://flat.badgen.net/github/stars/salvadordeveloper/TikTok-Flutter" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;TikTok + Reddit&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://henrybalassiano.github.io/Tik-Tok-Clone/"&gt;enrybalassiano.github.io&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/HenryBalassiano/Tik-Tok-Clone"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://flat.badgen.net/github/stars/HenryBalassiano/Tik-Tok-Clone" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;TinyURL&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://nexturl.vercel.app/"&gt;nexturl.vercel.app&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/chuddyjoachim/next-url"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Next.js TypeScript React Mongodb&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/chuddyjoachim/next-url" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Todoist&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://todoishh.netlify.app/"&gt;todoishh.netlify.app&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/anagkazou/todoishh"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React Firebase&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://flat.badgen.net/github/stars/anagkazou/todoishh" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Travian&lt;/td&gt; 
   &lt;td&gt;&lt;a href="http://cosmodream.ga/1255-burgomaster/"&gt;cosmodream.ga&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Areso/1255-burgomaster"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Vanilla JS&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/Areso/1255-burgomaster" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Trello&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://wekan.github.io/"&gt;wekan.github.io&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/wekan/wekan"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Meteor&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/wekan/wekan" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Trello&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.taiga.io/"&gt;taiga.io&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/taigaio/taiga-front"&gt;GitHub&lt;/a&gt; &lt;br /&gt; &lt;a href="https://github.com/taigaio/taiga-back"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Django, AngularJS&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/taigaio/taiga-front" alt="GitHub Repo stars" /&gt; &lt;img src="https://img.shields.io/github/stars/taigaio/taiga-back" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Trello&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://kanboard.org/"&gt;kanboard.org&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/kanboard/kanboard"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;PHP&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/kanboard/kanboard" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Trello&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://trellis-app.herokuapp.com/"&gt;trellis-app.herokuapp&lt;/a&gt; &lt;br /&gt; &lt;a href="https://github.com/ayushagg31/Trellis/raw/master/images/Trellis.gif"&gt;github.com (gif)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ayushagg31/Trellis"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React, Redux, Node, Express, MongoDB&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/ayushagg31/Trellis" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Trello&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://codesource.io/wp-content/uploads/2020/07/Trelloclone.webm"&gt;codesource.io (gif)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Dunebook/Trelloclone"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Angular 10&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/Dunebook/Trelloclone" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Trello&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://tiquetapp.herokuapp.com/"&gt;tiquetapp.herokuapp.com&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/FLiotta/Tiquet"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React, Python, Flask, PostgreSQL&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/FLiotta/Tiquet" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Trello&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://tberghuis.github.io/trello-board-clone/"&gt;tberghuis.github.io&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/tberghuis/trello-board-clone"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/tberghuis/trello-board-clone" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Trello&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://rupmalya-trello-clone.herokuapp.com/signin"&gt;rupmalya-trello-clone.herokuapp&lt;/a&gt; &lt;br /&gt; &lt;a href="https://youtu.be/59oQ01VEm30"&gt;youtube.com&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/RupmalyaK/trello-clone"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Express, MongoDB&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/RupmalyaK/trello-clone" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Trello&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://project-manager1.herokuapp.com/"&gt;project-manager1.herokuapp&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/arturmolenda/project-manager"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React, Redux, Node, Express, Socket.io, MongoDB&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/arturmolenda/project-manager" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Trello&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://trello-project-manager.netlify.app/"&gt;trello-project-manager.netlify&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/berabulut/project-manager"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React, Node.js, Firebase, Serverless, AWS Lambda&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/berabulut/project-manager" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Mastodon&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://echoloop.buzz/"&gt;echoloop.buzz&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/zerone0x/Echo-Frontend.git"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Node.js, Next.Js, Express.Js, TypeScript, TailwindCSS&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/zerone0x/Echo-Frontend" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Twitter&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://joinmastodon.org/"&gt;Mastodon&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/mastodon"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ActivityPub, Ruby, Go, Postgres, Redis, Docker&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/mastodon/mastodon" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Twitter&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://tweeetr.netlify.app/"&gt;https://tweeetr.netlify.app/&lt;/a&gt; &lt;br /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/SiddheshDabholkar/Twitter-clone"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React,Express js, NodeJS,Graphql,Apollo client,apollo-server,styled-components&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/SiddheshDabholkar/Twitter-clone" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Twitter&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://twitter-geek.netlify.app/#/"&gt;twitter-geek.netlify.app&lt;/a&gt; &lt;br /&gt; &lt;a href="https://github.com/RisingGeek/twitter-clone/raw/master/Live/feed.png"&gt;github.com (screenshot)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/RisingGeek/twitter-clone"&gt;GitHub&lt;/a&gt; &lt;br /&gt; &lt;a href="https://github.com/RisingGeek/twitter-clone-backend"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React, Redux, NodeJS, MYSQL&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/RisingGeek/twitter-clone" alt="GitHub Repo stars" /&gt; &lt;img src="https://img.shields.io/github/stars/RisingGeek/twitter-clone-backend" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Twitter&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://twitterclone2.netlify.app/login.html"&gt;twitterclone2.netlify.app&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Dunebook/Twitterclone"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;HTML, CSS, JQuery&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/Dunebook/Twitterclone" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Twitter&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://tclone.netlify.app/"&gt;tclone.netlify.app&lt;/a&gt; &lt;br /&gt; &lt;a href="https://github.com/muzam1l/tclone/raw/master/docs/tclone-Live2.gif"&gt;github.com (gif)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/muzam1l/tclone"&gt;GitHub&lt;/a&gt; &lt;br /&gt; &lt;a href="https://github.com/muzam1l/tclone-api"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;MERN&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/muzam1l/tclone" alt="GitHub Repo stars" /&gt; &lt;img src="https://img.shields.io/github/stars/muzam1l/tclone-api" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Twitter&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://twitterapp-clone.netlify.app/home"&gt;twitterapp-clone.netlify.app&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Ali-hd/Twitter-Clone"&gt;GitHub&lt;/a&gt; &lt;br /&gt; &lt;a href="https://github.com/Ali-hd/TwitterClone-Backend"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React, express, mongo, aws, socket.io&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/Ali-hd/Twitter-Clone" alt="GitHub Repo stars" /&gt; &lt;img src="https://img.shields.io/github/stars/Ali-hd/TwitterClone-Backend" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Twitter&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://twitter-cln.herokuapp.com"&gt;twitter-cln.herokuapp&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Abdullah-V/MEVN-stack-twitter-clone"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Vue, Express, Mongo&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/Abdullah-V/MEVN-stack-twitter-clone" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Twitter&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://twitter-web-clone-react.vercel.app"&gt;twitter-web-clone-react.vercel&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/mucahit-sahin/twitter-web-clone-react"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React, Redux&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/mucahit-sahin/twitter-web-clone-react" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Typeform&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://formbricks.com/"&gt;Formbricks&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/formbricks/formbricks"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;JavaScript&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/formbricks/formbricks" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Typeform / Google Form&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://ohmyform.com/"&gt;ohmyform.com&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ohmyform/ohmyform"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Python&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/ohmyform/ohmyform" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Typeform&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://supereasyforms.com/"&gt;supereasyforms.com&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/super-easy-forms/super-easy-forms"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;JavaScript&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/super-easy-forms/super-easy-forms" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Uber&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/calebnance/expo-uber/raw/master/screenshots/screenshare-3.png?raw=true"&gt;github.com (screenshot)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/calebnance/expo-uber"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React native, expo&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/calebnance/expo-uber" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Uber&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://youtu.be/ya1lsXS2Wvw"&gt;youtube.com&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Santos-Enoque/uber_clone_with_flutter"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Flutter&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/Santos-Enoque/uber_clone_with_flutter" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Uber Eats&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://enatega.com"&gt;enatega.com&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ninjas-code-official/food-delivery-multivendor"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;JavaScript, React, React Native, Material UI, GraphQL, Expo&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/ninjas-code-official/food-delivery-multivendor" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ubuntu&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://vivek9patel.github.io"&gt;vivek9patel.github.io&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/vivek9patel/vivek9patel.github.io"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React, Tailwind CSS&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/vivek9patel/vivek9patel.github.io" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Udemy&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://wedemy.up.railway.app"&gt;wedemy.up.railway.app&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Longwater1234/WedemyClient"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Vue 3, TypeScript, Java, Spring, MySQL&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://flat.badgen.net/github/stars/Longwater1234/WedemyClient" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Udemy&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://wedemy.up.railway.app/"&gt;wedemy.up.railway.app&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Longwater1234/WedemyClient"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Vue 3, TypeScript, ElementUI, Java, Springboot, MySQL, Redis&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://flat.badgen.net/github/stars/Longwater1234/WedemyClient" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Unsplash&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/junipdewan/react-unsplash#react-unsplash"&gt;github.com (screenshot)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/junipdewan/react-unsplash"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/junipdewan/react-unsplash" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Unsplash&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/junipdewan/react-unsplash#react-unsplash"&gt;github.com (screenshot)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/junipdewan/react-unsplash"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://flat.badgen.net/github/stars/junipdewan/react-unsplash" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Unsplash&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://mani-unsplash-clone.netlify.app/"&gt;mani-unsplash-clone.netlify&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/mani-barathi/unsplash-clone"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React, Material-UI&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://flat.badgen.net/github/stars/mani-barathi/unsplash-clone" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Vimeo&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://bimeo.herokuapp.com/#/"&gt;bimeo.herokuapp&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/TarikGul/vimeo_clone_full_stack_project"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React, Redux, Ruby. PostgreSQL&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/TarikGul/vimeo_clone_full_stack_project" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;VK&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://openvk.su"&gt;openvk.su&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/openvk/openvk"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;PHP, HTML, CSS, JQuery, MySQL&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/openvk/openvk" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;WhatsApp&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://codesource.io/wp-content/uploads/2020/11/Screenshot_20201029-171635-1-709x1536.png"&gt;codesource.io (screenshot)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Ekeminie/whatsapp_ui"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Flutter&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/Ekeminie/whatsapp_ui" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;WhatsApp&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://medium.com/p/5479d83baaa4"&gt;medium.com (article)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Urigo/WhatsApp-Clone-Client-Angular"&gt;GitHub&lt;/a&gt; &lt;br /&gt; &lt;a href="https://github.com/Urigo/WhatsApp-Clone-server"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Angular, Express, PostgreSQL, GraphQL, TypeScript&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/Urigo/WhatsApp-Clone-Client-Angular" alt="GitHub Repo stars" /&gt; &lt;img src="https://img.shields.io/github/stars/Urigo/WhatsApp-Clone-server" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;WhatsApp&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://youtu.be/3TYfkvf4u4M"&gt;youtube&lt;/a&gt; &lt;br /&gt; &lt;a href="https://clone-massenger.herokuapp.com/signinsignup"&gt;clone-massenger.herokuapp&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/RupmalyaK/whatsapp-MERN"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;MERN&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/RupmalyaK/whatsapp-MERN" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;WhatsApp&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://tinode.co/"&gt;tinode.co&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/tinode/chat"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Go, React, Java, Swift, MySQL, MongoDB, RethinkDB&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/tinode/chat" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;WhatsApp (Web)&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://whatsapp-clone-web.netlify.app"&gt;whatsapp-clone-web.netlify.app&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/KarenOk/whatsapp-web-clone"&gt;GitHub&lt;/a&gt; &lt;br /&gt; &lt;a href="https://github.com/KarenOk/whatsapp-web-clone-backend"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React, React Context API, Express, JavaScript, Socket.IO&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/karenok/whatsapp-web-clone" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Windows 11&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://andrewstech.me/win11react-add-test"&gt;win11.blueedge.me&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/blueedgetechno/win11React"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React, Redux, Firebase, TailwindCSS, Internationalization&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://flat.badgen.net/github/stars/blueedgetechno/win11react" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Wix&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://grapesjs.com/"&gt;grapesjs.com&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/artf/grapesjs"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;JavaScript, Webpack&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/artf/grapesjs" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Workflowy&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://deepnotes.in/"&gt;deepnotes.in&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/mukeshsoni/deepnotes-editor"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ReactJS, DraftJS&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/mukeshsoni/deepnotes-editor" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;YouTube&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://yt-clone-7.web.app"&gt;yt-clone-7.web.app&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/anandhu720/youtube-clone.git"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ReactJs,Redux,Firebase,YouTube API&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://flat.badgen.net/github/stars/anandhu720/youtube-clone" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;YouTube&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://utubeclone.netlify.app/"&gt;utubeclone.netlify.app&lt;/a&gt; &lt;br /&gt; &lt;a href="https://youtu.be/wHLurtOnmyM"&gt;youtube&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/manikandanraji/youtubeclone-frontend"&gt;GitHub&lt;/a&gt; &lt;br /&gt; &lt;a href="https://github.com/manikandanraji/youtubeclone-backend"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React, Redux, Express, Sequelize&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://flat.badgen.net/github/stars/manikandanraji/youtubeclone-frontend" alt="GitHub Repo stars" /&gt; &lt;img src="https://flat.badgen.net/github/stars/manikandanraji/youtubeclone-backend" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;YouTube Music&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://octave-music.web.app/"&gt;octave-music.web.app&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/mani-barathi/Octave"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;React, Redux, Firebase, Material-UI&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://flat.badgen.net/github/stars/mani-barathi/Octave" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;YouTube Music&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://beatbump.ml/"&gt;beatbump.ml&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/snuffyDev/Beatbump"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Svelte&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://flat.badgen.net/github/stars/snuffyDev/Beatbump" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;YouTube Music&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://music.creasource.app"&gt;music.creasource.app&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/tgambet/musicsource"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Angular&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://flat.badgen.net/github/stars/tgambet/musicsource" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Zapier&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://n8n.io/"&gt;n8n.io&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/n8n-io/n8n"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;TypeScript, Vue, Docker&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/n8n-io/n8n" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Zapier&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://vimeo.com/61976251"&gt;vimeo.com (gif)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/huginn/huginn"&gt;GitHub&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Ruby&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/huginn/huginn" alt="GitHub Repo stars" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;em&gt;Some link is broken or clone is not good enough? &lt;a href="https://github.com/GorvGoyl/Clone-Wars/issues/new"&gt;report it&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Contribution Guide&lt;/h2&gt; 
&lt;p&gt;I need your help to maintain this list up to date 🙏.&lt;/p&gt; 
&lt;h3&gt;Fixing an invalid clone:&lt;/h3&gt; 
&lt;p&gt;If there's some issue with a clone (URL not working, repo not found, clone not good enough, etc.), then feel free to remove or update it with working link. Just edit &lt;a href="https://github.com/GorvGoyl/Clone-Wars/raw/main/README.md"&gt;readme.md&lt;/a&gt; and submit a PR. I'll review and merge it.&lt;/p&gt; 
&lt;h3&gt;Adding new clone:&lt;/h3&gt; 
&lt;p&gt;Before submitting a clone make sure:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;It should be a clone/alternative of some popular software or app.&lt;/li&gt; 
 &lt;li&gt;Project must have at least minimal functionality, please do not submit any 'UI only' clone.&lt;/li&gt; 
 &lt;li&gt;Also, no more Trello, 2048 clones unless your tech-stack is different.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Add clone to &lt;strong&gt;1st table&lt;/strong&gt; if you can also provide tutorial link else add it to &lt;strong&gt;2nd table&lt;/strong&gt;. Maintain alphabetical order while adding.&lt;/p&gt; 
&lt;p&gt;Edit &lt;a href="https://github.com/GorvGoyl/Clone-Wars/raw/main/README.md"&gt;readme.md&lt;/a&gt; (&lt;em&gt;you may use online md editor like &lt;a href="https://markdown.site/"&gt;markdown.site&lt;/a&gt; for better table visualization&lt;/em&gt;) and submit the PR! Make sure there are no merge conflicts.&lt;/p&gt; 
&lt;p&gt;Any other &lt;a href="https://github.com/GorvGoyl/Clone-Wars/issues"&gt;feedback&lt;/a&gt; to improve this project is also welcome :)&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>KellerJordan/modded-nanogpt</title>
      <link>https://github.com/KellerJordan/modded-nanogpt</link>
      <description>&lt;p&gt;NanoGPT (124M) in 3 minutes&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Modded-NanoGPT&lt;/h1&gt; 
&lt;p&gt;This repository hosts the &lt;em&gt;NanoGPT speedrun&lt;/em&gt;, in which we (collaboratively|competitively) search for the fastest algorithm to use 8 NVIDIA H100 GPUs to train a language model that attains 3.28 cross-entropy loss on the &lt;a href="https://huggingface.co/datasets/HuggingFaceFW/fineweb"&gt;FineWeb&lt;/a&gt; validation set.&lt;/p&gt; 
&lt;p&gt;The target (3.28 validation loss on FineWeb) follows Andrej Karpathy's &lt;a href="https://github.com/karpathy/llm.c/discussions/481#:~:text=By%20the%20end%20of%20the%20optimization%20we%27ll%20get%20to%20about%203.29"&gt;GPT-2 replication in llm.c, which attains that loss after running for 45 minutes&lt;/a&gt;. The speedrun code also descends from llm.c's &lt;a href="https://github.com/karpathy/llm.c/raw/master/train_gpt2.py"&gt;PyTorch trainer&lt;/a&gt;, which itself descends from NanoGPT, hence the name of the repo. Thanks to the efforts of many contributors, this repo now contains a training algorithm which attains the target performance in:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;2 minutes and 20 seconds on 8xH100 (the llm.c GPT-2 replication needed 45)&lt;/li&gt; 
 &lt;li&gt;0.73B tokens (the llm.c GPT-2 replication needed 10B)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This improvement in training speed has been brought about by the following techniques:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Modernized architecture: Rotary embeddings, QK-Norm, and ReLU²&lt;/li&gt; 
 &lt;li&gt;The Muon optimizer [&lt;a href="https://kellerjordan.github.io/posts/muon/"&gt;writeup&lt;/a&gt;] [&lt;a href="https://github.com/KellerJordan/Muon"&gt;repo&lt;/a&gt;]&lt;/li&gt; 
 &lt;li&gt;Untie head from embedding, use FP8 matmul for head, and softcap logits (the latter following Gemma 2)&lt;/li&gt; 
 &lt;li&gt;Initialization of projection and classification layers to zero (muP-like)&lt;/li&gt; 
 &lt;li&gt;Skip connections from embedding to every block as well as between blocks in U-net pattern&lt;/li&gt; 
 &lt;li&gt;Extra embeddings which are mixed into the values in attention layers (inspired by Zhou et al. 2024)&lt;/li&gt; 
 &lt;li&gt;Flash Attention 3 with long-short sliding window attention pattern (inspired by Gemma 2) and window size warmup with YaRN&lt;/li&gt; 
 &lt;li&gt;Align training batch starts with EoS and set a max document length&lt;/li&gt; 
 &lt;li&gt;Accumulate gradients for 2 steps for embedding and lm_head before updating parameters&lt;/li&gt; 
 &lt;li&gt;Enable model to back out contributions from first 8 layers before prediction&lt;/li&gt; 
 &lt;li&gt;Polar Express implementation in Muon&lt;/li&gt; 
 &lt;li&gt;Smear module to enable 1 token look back&lt;/li&gt; 
 &lt;li&gt;Sparse attention gate&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;As well as many systems optimizations.&lt;/p&gt; 
&lt;p&gt;Contributors list (growing with each new record): &lt;a href="https://x.com/bozavlado"&gt;@bozavlado&lt;/a&gt;; &lt;a href="https://x.com/brendanh0gan"&gt;@brendanh0gan&lt;/a&gt;; &lt;a href="https://bsky.app/profile/fernbear.bsky.social"&gt;@fernbear.bsky.social&lt;/a&gt;; &lt;a href="https://x.com/Grad62304977"&gt;@Grad62304977&lt;/a&gt;; &lt;a href="https://x.com/jxbz"&gt;@jxbz&lt;/a&gt;; &lt;a href="https://x.com/kellerjordan0"&gt;@kellerjordan0&lt;/a&gt;; &lt;a href="https://x.com/KoszarskyB"&gt;@KoszarskyB&lt;/a&gt;; &lt;a href="https://x.com/@leloykun"&gt;@leloykun&lt;/a&gt;; &lt;a href="https://x.com/YouJiacheng"&gt;@YouJiacheng&lt;/a&gt;; &lt;a href="https://x.com/jadenj3o"&gt;@jadenj3o&lt;/a&gt;; &lt;a href="https://github.com/KonstantinWilleke"&gt;@KonstantinWilleke&lt;/a&gt;, &lt;a href="https://github.com/alexrgilbert"&gt;@alexrgilbert&lt;/a&gt;, &lt;a href="https://github.com/adricarda"&gt;@adricarda&lt;/a&gt;, &lt;a href="https://github.com/tuttyfrutyee"&gt;@tuttyfrutyee&lt;/a&gt;, &lt;a href="https://github.com/vdlad"&gt;@vdlad&lt;/a&gt;; &lt;a href="https://x.com/ryanyang0"&gt;@ryanyang0&lt;/a&gt;, &lt;a href="https://github.com/vagrawal"&gt;@vagrawal&lt;/a&gt;, &lt;a href="https://x.com/classiclarryd"&gt;@classiclarryd&lt;/a&gt;, &lt;a href="https://github.com/byronxu99"&gt;@byronxu99&lt;/a&gt;, &lt;a href="https://x.com/varunneal"&gt;@varunneal&lt;/a&gt;, &lt;a href="https://github.com/EmelyanenkoK"&gt;@EmelyanenkoK&lt;/a&gt;, &lt;a href="https://github.com/bernard24"&gt;@bernard24&lt;/a&gt;/&lt;a href="https://www.hiverge.ai/"&gt;https://www.hiverge.ai/&lt;/a&gt;, &lt;a href="https://x.com/GusarichOnX"&gt;@GusarichOnX&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Running the current record&lt;/h2&gt; 
&lt;p&gt;To run the current record, run the following commands.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/KellerJordan/modded-nanogpt.git &amp;amp;&amp;amp; cd modded-nanogpt
pip install -r requirements.txt
pip install --pre torch --index-url https://download.pytorch.org/whl/nightly/cu126 --upgrade
# downloads only the first 900M training tokens to save time
python data/cached_fineweb10B.py 9
./run.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Note: torch.compile will add around 7 minutes of latency the first time you run the code.&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;Alternative: Running with Docker (recommended for precise timing)&lt;/h2&gt; 
&lt;p&gt;For cases where CUDA or NCCL versions aren't compatible with your current system setup, Docker can be a helpful alternative. This approach standardizes versions for CUDA, NCCL, CUDNN, and Python, reducing dependency issues and simplifying setup. Note: an NVIDIA driver must already be installed on the system (useful if only the NVIDIA driver and Docker are available).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/KellerJordan/modded-nanogpt.git &amp;amp;&amp;amp; cd modded-nanogpt
sudo docker build -t modded-nanogpt .
sudo docker run -it --rm --gpus all -v $(pwd):/modded-nanogpt modded-nanogpt python data/cached_fineweb10B.py 8
sudo docker run -it --rm --gpus all -v $(pwd):/modded-nanogpt modded-nanogpt sh run.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To get an interactive docker, you can use&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo docker run -it --rm --gpus all -v $(pwd):/modded-nanogpt modded-nanogpt bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;World record history&lt;/h2&gt; 
&lt;p&gt;The following is the historical progression of world speed records for the following competitive task:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;em&gt;Train a neural network to ≤3.28 validation loss on FineWeb using 8x NVIDIA H100s.&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Note: The 3.28 target was selected to match &lt;a href="https://github.com/karpathy/llm.c/discussions/481"&gt;Andrej Karpathy's GPT-2 (small) reproduction&lt;/a&gt;.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;#&lt;/th&gt; 
   &lt;th&gt;Record time&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Date&lt;/th&gt; 
   &lt;th&gt;Log&lt;/th&gt; 
   &lt;th&gt;Contributors&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;1&lt;/td&gt; 
   &lt;td&gt;45 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/karpathy/llm.c/discussions/481"&gt;llm.c baseline&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;05/28/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2024-10-13_llmc/main.log"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@karpathy, llm.c contributors&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;31.4 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/kellerjordan0/status/1798863559243513937"&gt;Tuned learning rate &amp;amp; rotary embeddings&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;06/06/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2024-06-06_AdamW/f66d43d7-e449-4029-8adf-e8537bab49ea.log"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@kellerjordan0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;3&lt;/td&gt; 
   &lt;td&gt;24.9 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/kellerjordan0/status/1842300916864844014"&gt;Introduced the Muon optimizer&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;10/04/24&lt;/td&gt; 
   &lt;td&gt;none&lt;/td&gt; 
   &lt;td&gt;@kellerjordan0, @jxbz&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;4&lt;/td&gt; 
   &lt;td&gt;22.3 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/kellerjordan0/status/1844820919061287009"&gt;Muon improvements&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;10/11/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2024-10-10_Muon/eb5659d0-fb6a-49e5-a311-f1f89412f726.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@kellerjordan0, @bozavlado&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;15.2 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/kellerjordan0/status/1845865698532450646"&gt;Pad embeddings, ReLU², zero-init projections, QK-norm&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;10/14/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2024-10-14_ModernArch/dabaaddd-237c-4ec9-939d-6608a9ed5e27.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@Grad62304977, @kellerjordan0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;6&lt;/td&gt; 
   &lt;td&gt;13.1 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/kellerjordan0/status/1847291684016783746"&gt;Distributed the overhead of Muon&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;10/18/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2024-10-17_DistributedMuon/22d24867-eb5a-4fcc-ae2c-263d0277dfd1.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@kellerjordan0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;7&lt;/td&gt; 
   &lt;td&gt;12.0 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/kellerjordan0/status/1847358578686152764"&gt;Upgraded PyTorch 2.5.0&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;10/18/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2024-10-18_PyTorch25/d4bfb25f-688d-4da5-8743-33926fad4842.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@kellerjordan0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;10.8 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/kellerjordan0/status/1853188916704387239"&gt;Untied embedding and head&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;11/03/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2024-11-03_UntieEmbed/d6b50d71-f419-4d26-bb39-a60d55ae7a04.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@Grad62304977, @kellerjordan0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;9&lt;/td&gt; 
   &lt;td&gt;8.2 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/kellerjordan0/status/1854296101303800108"&gt;Value and embedding skip connections, momentum warmup, logit softcap&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;11/06/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2024-11-06_ShortcutsTweaks/dd7304a6-cc43-4d5e-adb8-c070111464a1.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@Grad62304977, @kellerjordan0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;10&lt;/td&gt; 
   &lt;td&gt;7.8 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/kellerjordan0/status/1855267054774865980"&gt;Bfloat16 activations&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;11/08/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2024-11-08_CastBf16/a833bed8-2fa8-4cfe-af05-58c1cc48bc30.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@kellerjordan0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;11&lt;/td&gt; 
   &lt;td&gt;7.2 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/kellerjordan0/status/1856053121103093922"&gt;U-net pattern skip connections &amp;amp; double lr&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;11/10/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2024-11-10_UNetDoubleLr/c87bb826-797b-4f37-98c7-d3a5dad2de74.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@brendanh0gan&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;12&lt;/td&gt; 
   &lt;td&gt;5.03 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/kellerjordan0/status/1859331370268623321"&gt;1024-ctx dense causal attention → 64K-ctx FlexAttention&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;11/19/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2024-11-19_FlexAttention/8384493d-dba9-4991-b16b-8696953f5e6d.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@KoszarskyB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;13&lt;/td&gt; 
   &lt;td&gt;4.66 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/hi_tysam/status/1860851011797053450"&gt;Attention window warmup&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;11/24/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2024-11-24_WindowWarmup/cf9e4571-c5fc-4323-abf3-a98d862ec6c8.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@fernbear.bsky.social&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;14&lt;/td&gt; 
   &lt;td&gt;4.41 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/KoszarskyB/status/1864746625572257852"&gt;Value Embeddings&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;12/04/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2024-12-04_ValueEmbed"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@KoszarskyB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;15&lt;/td&gt; 
   &lt;td&gt;3.95 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/YouJiacheng/status/1865761473886347747"&gt;U-net pattern value embeddings, assorted code optimizations&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;12/08/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2024-12-08_UNetValueEmbedsTweaks"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@leloykun, @YouJiacheng&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;3.80 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/YouJiacheng/status/1866734331559071981"&gt;Split value embeddings, block sliding window, separate block mask&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;12/10/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2024-12-10_MFUTweaks"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@YouJiacheng&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;17&lt;/td&gt; 
   &lt;td&gt;3.57 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/YouJiacheng/status/1868938024731787640"&gt;Sparsify value embeddings, improve rotary embeddings, drop an attn layer&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;12/17/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2024-12-17_SparsifyEmbeds"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@YouJiacheng&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;18&lt;/td&gt; 
   &lt;td&gt;3.4 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/kellerjordan0/status/1876048851158880624"&gt;Lower logit softcap from 30 to 15&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;01/04/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-01-04_SoftCap/31d6c427-f1f7-4d8a-91be-a67b5dcd13fd.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@KoszarskyB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;19&lt;/td&gt; 
   &lt;td&gt;3.142 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/YouJiacheng/status/1878827972519772241"&gt;FP8 head, offset logits, lr decay to 0.1 instead of 0.0&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;01/13/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-01-13_Fp8LmHead/c51969c2-d04c-40a7-bcea-c092c3c2d11a.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@YouJiacheng&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;20&lt;/td&gt; 
   &lt;td&gt;2.992 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/leloykun/status/1880301753213809016"&gt;Merged QKV weights, long-short attention, attention scale, lower Adam epsilon, batched Muon&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;01/16/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-01-16_Sub3Min/1d3bd93b-a69e-4118-aeb8-8184239d7566.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@leloykun, @fernbear.bsky.social, @YouJiacheng, @brendanh0gan, @scottjmaddox, @Grad62304977&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;21&lt;/td&gt; 
   &lt;td&gt;2.933 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/leloykun/status/1885640350368420160"&gt;Reduced batch size&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;01/26/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-01-26_BatchSize/c44090cc-1b99-4c95-8624-38fb4b5834f9.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@leloykun&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;21&lt;/td&gt; 
   &lt;td&gt;2.997 minutes&lt;/td&gt; 
   &lt;td&gt;21st record with new timing&lt;/td&gt; 
   &lt;td&gt;02/01/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-02-01_RuleTweak/eff63a8c-2f7e-4fc5-97ce-7f600dae0bc7.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;not a new record, just re-timing #21 with the &lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/#timing-change-after-record-21"&gt;updated rules&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;21&lt;/td&gt; 
   &lt;td&gt;3.014 minutes&lt;/td&gt; 
   &lt;td&gt;21st record with latest torch&lt;/td&gt; 
   &lt;td&gt;05/24/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-05-24_StableTorch/89d9f224-3b01-4581-966e-358d692335e0.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;not a new record, just re-timing #21 with latest torch&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;22&lt;/td&gt; 
   &lt;td&gt;2.990 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/KonstantinWille/status/1927137223238909969"&gt;Faster gradient all-reduce&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;05/24/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-05-24_FasterReduce/23f40b75-06fb-4c3f-87a8-743524769a35.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad; The Enigma project&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;23&lt;/td&gt; 
   &lt;td&gt;2.979 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/kellerjordan0/status/1927460573098262616"&gt;Overlap computation and gradient communication&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;05/25/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-05-25_EvenFasterReduce/6ae86d05-5cb2-4e40-a512-63246fd08e45.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@ryanyang0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;24&lt;/td&gt; 
   &lt;td&gt;2.966 minutes&lt;/td&gt; 
   &lt;td&gt;Replace gradient all_reduce with reduce_scatter&lt;/td&gt; 
   &lt;td&gt;05/30/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-05-30_noallreduce/8054c239-3a18-499e-b0c8-dbd27cb4b3ab.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@vagrawal&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;25&lt;/td&gt; 
   &lt;td&gt;2.896 minutes&lt;/td&gt; 
   &lt;td&gt;Upgrade PyTorch to 2.9.0.dev20250713+cu126&lt;/td&gt; 
   &lt;td&gt;07/13/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-07-13_UpgradeTorch190/692f80e0-5e64-4819-97d4-0dc83b7106b9.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@kellerjordan0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;26&lt;/td&gt; 
   &lt;td&gt;2.863 minutes&lt;/td&gt; 
   &lt;td&gt;Align training batch starts with EoS, increase cooldown frac to .45&lt;/td&gt; 
   &lt;td&gt;07/13/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-07-12_BosAlign/c1fd8a38-bb9f-45c4-8af0-d37f70c993f3.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@classiclarryd&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;27&lt;/td&gt; 
   &lt;td&gt;2.817 minutes&lt;/td&gt; 
   &lt;td&gt;Transpose one of the MLP matrices + add Triton kernel for symmetric matmul&lt;/td&gt; 
   &lt;td&gt;07/18/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-07-18_TritonMuon/record.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/109"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@byronxu99&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;28&lt;/td&gt; 
   &lt;td&gt;2.812 minutes&lt;/td&gt; 
   &lt;td&gt;Sparse attention gate&lt;/td&gt; 
   &lt;td&gt;08/23/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-08-23_SparseAttnGate/020630eb-2191-4ba2-9ee4-4cdc94316943.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/117"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@classiclarryd&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;29&lt;/td&gt; 
   &lt;td&gt;2.731 minutes&lt;/td&gt; 
   &lt;td&gt;Flash Attention 3, 2048 max_doc_len, update ws schedule&lt;/td&gt; 
   &lt;td&gt;09/03/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-09-03_FA3/44fc1276-0510-4961-92c0-730c65e5feba.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/118"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@varunneal&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;30&lt;/td&gt; 
   &lt;td&gt;2.717 minutes&lt;/td&gt; 
   &lt;td&gt;Drop first MLP layer&lt;/td&gt; 
   &lt;td&gt;09/05/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-09-05_SkipMLPBlocks/07e7ae76-b7d0-4481-b149-01e7d81b5ad4.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/120"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@EmelyanenkoK&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;31&lt;/td&gt; 
   &lt;td&gt;2.656 minutes&lt;/td&gt; 
   &lt;td&gt;Dynamically incorporate YaRN during training and validation&lt;/td&gt; 
   &lt;td&gt;09/10/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-09-10_Yarn/0ecdb695-510b-4c3b-b030-09861a162ce8.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/122"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@classiclarryd&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;32&lt;/td&gt; 
   &lt;td&gt;2.625 minutes&lt;/td&gt; 
   &lt;td&gt;Optimize distributed training, improve skip connection gating, and enhance bfloat16 usage&lt;/td&gt; 
   &lt;td&gt;09/11/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-09-11_VectSigmoidBFloat16/0d0d9882-c34f-4d82-b961-a17d5659c988.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/125"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@bernard24 &amp;amp; hiverge.ai&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;33&lt;/td&gt; 
   &lt;td&gt;2.565 minutes&lt;/td&gt; 
   &lt;td&gt;Asynchronously fetch and index data batches, extend final layer attention window for validation&lt;/td&gt; 
   &lt;td&gt;09/15/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-09-15_AsyncDataLoadAttnFinalWindow/25db37c7-2bab-4ef4-ae63-d593590ef823.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/127"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@classiclarryd&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;34&lt;/td&gt; 
   &lt;td&gt;2.547 minutes&lt;/td&gt; 
   &lt;td&gt;Smear token embeddings 1 position forward&lt;/td&gt; 
   &lt;td&gt;09/18/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-09-18_Smear/18a1e5c7-947e-479d-bc3a-a57a61a98fc9.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/130"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@classiclarryd&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;35&lt;/td&gt; 
   &lt;td&gt;2.527 minutes&lt;/td&gt; 
   &lt;td&gt;Drop first attn layer, extend all long windows for validation, update schedule&lt;/td&gt; 
   &lt;td&gt;09/21/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-09-21_DropAttn/01fc4a96-f2a0-47a1-8a6a-c7d10bac99fe.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/131"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@classiclarryd&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;36&lt;/td&gt; 
   &lt;td&gt;2.495 minutes&lt;/td&gt; 
   &lt;td&gt;MuonCustomSizing, perform mlp and attn reduce scatter in shared call&lt;/td&gt; 
   &lt;td&gt;09/23/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-09-23_MuonCustomSizing/b067b4ac-72a6-4436-a6f8-ea51c1efeef3.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/132"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@classiclarryd&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;37&lt;/td&gt; 
   &lt;td&gt;2.483 minutes&lt;/td&gt; 
   &lt;td&gt;Compute cross entropy in BF16 during training&lt;/td&gt; 
   &lt;td&gt;09/27/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-09-27_BF16CE/08c0770f-17fc-44cd-971d-734a7a28a3e3.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/133"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@GusarichOnX&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;38&lt;/td&gt; 
   &lt;td&gt;2.476 minutes&lt;/td&gt; 
   &lt;td&gt;Polar Express, replacement for Newton-Schulz&lt;/td&gt; 
   &lt;td&gt;09/29/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-09-29_PolarExpress/0e3f0af5-ad08-47a6-813d-0c709b50d422.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/134"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@varunneal&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;39&lt;/td&gt; 
   &lt;td&gt;2.447 minutes&lt;/td&gt; 
   &lt;td&gt;Only update Adam params every other step, reduce batch size&lt;/td&gt; 
   &lt;td&gt;09/30/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-09-30_CustomBatching/40b101b1-77ea-45ea-a089-1d3a647daa22.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/136"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@classiclarryd&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;40&lt;/td&gt; 
   &lt;td&gt;2.345 minutes&lt;/td&gt; 
   &lt;td&gt;Backout, misc hyperparameter tuning, optimize lambda padding&lt;/td&gt; 
   &lt;td&gt;10/04/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-10-04_Backout/514e7581-fbd4-4338-a3e4-e556f9c958ce.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/140"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@classiclarryd&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Rules&lt;/h2&gt; 
&lt;p&gt;The only rules are that new records must:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Not modify the train or validation data pipelines. (You can change the batch size, sequence length, attention structure etc.; just don't change the underlying streams of tokens.)&lt;/li&gt; 
 &lt;li&gt;Attain ≤3.28 mean val loss. (Due to inter-run variance, submissions must provide enough run logs to attain a statistical significance level of p&amp;lt;0.01 that their mean val loss is ≤3.28. Example code to compute p-value can be found &lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-01-04_SoftCap#softer-softcap"&gt;here&lt;/a&gt;. For submissions which improve speed by optimizing the systems performance, without touching the ML, this requirement is waived.)&lt;/li&gt; 
 &lt;li&gt;Not use any extra &lt;code&gt;torch._inductor.config&lt;/code&gt; or &lt;code&gt;torch.compile&lt;/code&gt; flags. (These can save a few seconds, but they can also make compilation take &amp;gt;30min. This rule was introduced after the 21st record.)&lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Note: &lt;code&gt;torch._inductor.config.coordinate_descent_tuning&lt;/code&gt; is allowed for GPT-2 Medium track (a.k.a. 2.92 track).&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Other than that, anything and everything is fair game!&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/KellerJordan/modded-nanogpt/discussions/23?sort=new#discussioncomment-12109560"&gt;further clarifications&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Comment on the target metric&lt;/h3&gt; 
&lt;p&gt;The target metric is &lt;em&gt;cross-entropy loss on the FineWeb val set&lt;/em&gt;. To speak mathematically, the goal of the speedrun is *to obtain a probability model of language which assigns a probability of at least &lt;code&gt;math.exp(-3.28 * 10485760)&lt;/code&gt; to the first 10,485,760 tokens of the FineWeb valset. Hence, e.g., we allow evaluation at any sequence length, so long as we still have a valid probability model of language.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Timing change after record 21&lt;/h3&gt; 
&lt;p&gt;After the 21st record, we made two changes to the timing. First, there used to be an initial "grace period" of 10 untimed steps to allow kernel warmup. We replaced this with an explicit kernel-warmup section which is untimed and uses dummy data. This results in an extra runtime of 850ms from the 10 extra timed steps. Second, we banned the use of &lt;code&gt;torch._inductor.config.coordinate_descent_tuning&lt;/code&gt;. This saves ~25min of untimed pre-run compilation, but results in an extra runtime of ~3s.&lt;/p&gt; 
&lt;!--Note: The original llm.c baseline is intended to be closer to a replication of GPT-2 than to an optimized LLM training.
So it's no surprise that there is room to improve; as @karpathy has said, 'llm.c still has a lot of pending optimizations.'
In addition, many of the techniques used in these records are completely standard, such as rotary embeddings.
The goal of this benchmark/speedrun is simply to find out which techniques actually work, and maybe come up with some new ones.--&gt; 
&lt;!--The goal of this benchmark is simply to find out all the techniques which actually work, because I'm going crazy reading all these
LLM training papers
which claim a huge benefit but then use their own idiosyncratic non-competitive benchmark and therefore no one in the community has any idea if it's legit for months.--&gt; 
&lt;!--[LLM](https://arxiv.org/abs/2305.14342) [training](https://arxiv.org/abs/2402.17764) [papers](https://arxiv.org/abs/2410.01131)--&gt; 
&lt;!--I mean hello??? We're in a completely empirical field; it is insane to not have a benchmark. Ideally everyone uses the same LLM training benchmark,
and then reviewing LLM training papers becomes as simple as checking if they beat the benchmark. It's not like this would be unprecedented, that's how things
were in the ImageNet days.
The only possible 'benefit' I can think of for any empirical field to abandon benchmarks is that it would make it easier to publish false results. Oh, I guess that's why it happened.
Hilarious to think about how, in the often-commented-upon and ongoing collapse of the peer review system, people blame the *reviewers* --
yeah, those guys doing free labor who everyone constantly musters all of their intelligence to lie to, it's *their* fault! My bad, you caught me monologuing.--&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Important note about records 22-25&lt;/h3&gt; 
&lt;p&gt;Thanks to the statistical testing of &lt;a href="https://www.github.com/agrawal"&gt;@agrawal&lt;/a&gt; (holder of the 24th record), we have learned that records 23, 24, and in all likelihood 22 and 25, actually attain a mean loss of 3.281, which is slightly above the 3.28 target. Therefore if we were to completely adhere to the speedrun rules, we would have to deny that these are valid records. However, we have decided to leave them in place as valid, because of the following two reasons: (a) the extra loss is most likely my (@kellerjordan0) own fault rather than that of the records, and (b) it is most likely easily addressable.&lt;/p&gt; 
&lt;p&gt;Here's what happened: Records #22 to #25 each change only the systems/implementation of the speedrun. Therefore, the requirement to do statistical testing to confirm they hit the target was waived, since in theory they should have hit it automatically, by virtue of the fact that they didn't touch the ML (i.e., they didn't change the architecture, learning rate, etc.).&lt;/p&gt; 
&lt;p&gt;So if these records shouldn't have changed the ML, what explains the regression in val loss? We think that most likely, the answer is that this regression was indeed not introduced by any of these records. Instead, it was probably caused by my own non-record in which I retimed record #21 with newest torch, because in this non-record I also changed the constants used to cast the lm_head to fp8. I thought that this change should cause only a (small) strict improvement, but apparently that was not the case.&lt;/p&gt; 
&lt;p&gt;Therefore, it is probable that each of records #22-25 could be easily made fully valid by simply reverting the change I made to those constants. Therefore they shall be upheld as valid records.&lt;/p&gt; 
&lt;p&gt;For the future, fortunately record #26 brought the speedrun back into the green in terms of &amp;lt;3.28 loss, so (with high p-value) it should be in a good state now.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Notable attempts &amp;amp; forks&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Notable runs:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://x.com/alexjc/status/1881410039639863622"&gt;@alexjc's 01/20/2025 2.77-minute TokenMonster-based record&lt;/a&gt;. This record is technically outside the rules of the speedrun, since we specified that the train/val tokens must be kept fixed. However, it's very interesting, and worth including. The run is not more data-efficient; rather, the speedup comes from the improved tokenizer allowing the vocabulary size to be reduced (nearly halved!) while preserving the same bytes-per-token, which saves lots of parameters and FLOPs in the head and embeddings.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Notable forks:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/BlinkDL/modded-nanogpt-rwkv"&gt;https://github.com/BlinkDL/modded-nanogpt-rwkv&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/nikhilvyas/modded-nanogpt-SOAP"&gt;https://github.com/nikhilvyas/modded-nanogpt-SOAP&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Speedrun track 2: GPT-2 Medium&lt;/h2&gt; 
&lt;p&gt;The target loss for this track is lowered from 3.28 to 2.92, as per Andrej Karpathy's 350M-parameter llm.c baseline. This baseline generates a model with performance similar to the original GPT-2 Medium, whereas the first track's baseline generates a model on par with GPT-2 Small. All other rules remain the same.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Note: &lt;code&gt;torch._inductor.config.coordinate_descent_tuning&lt;/code&gt; is turned on after the record 6 (*).&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;#&lt;/th&gt; 
   &lt;th&gt;Record time&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Date&lt;/th&gt; 
   &lt;th&gt;Log&lt;/th&gt; 
   &lt;th&gt;Contributors&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;1&lt;/td&gt; 
   &lt;td&gt;5.8 hours&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/karpathy/llm.c/discussions/481"&gt;llm.c baseline (350M parameters)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;05/28/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_2_medium/2025-01-18/main.log"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@karpathy, llm.c contributors&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;29.3 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/kellerjordan0/status/1881959719012847703"&gt;Initial record based on scaling up the GPT-2 small track speedrun&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;01/18/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_2_medium/2025-01-18/241dd7a7-3d76-4dce-85a4-7df60387f32a.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@kellerjordan0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;3&lt;/td&gt; 
   &lt;td&gt;28.1 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/kellerjordan0/status/1888320690543284449"&gt;Added standard weight decay&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;02/08/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_2_medium/2025-02-08_WeightDecay/b01743db-605c-4326-b5b1-d388ee5bebc5.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@kellerjordan0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;4&lt;/td&gt; 
   &lt;td&gt;27.7 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/leloykun/status/1892793848163946799"&gt;Tuned Muon Newton-Schulz coefficients&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;02/14/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_2_medium/2025-02-14_OptCoeffs/1baa66b2-bff7-4850-aced-d63885ffb4b6.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@leloykun&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;27.2 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_2_medium/2025-03-06_LongerCooldown/779c041a-2a37-45d2-a18b-ec0f223c2bb7.txt"&gt;Increased learning rate cooldown phase duration&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;03/06/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_2_medium/2025-03-06_LongerCooldown/779c041a-2a37-45d2-a18b-ec0f223c2bb7.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@YouJiacheng&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;6&lt;/td&gt; 
   &lt;td&gt;25.95 minutes*&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/YouJiacheng/status/1905861218138804534"&gt;2x MLP wd, qkv norm, all_reduce/opt.step() overlap, optimized skip pattern&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;03/25/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_2_medium/2025-03-25_ArchOptTweaks/train_gpt-20250329.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@YouJiacheng&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;7&lt;/td&gt; 
   &lt;td&gt;25.29 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/YouJiacheng/status/1912570883878842527"&gt;Remove FP8 head; ISRU logits softcap; New sharded mixed precision Muon; merge weights&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;04/16/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_2_medium/2025-04-16_Record7/223_3310d0b1-b24d-48ee-899f-d5c2a254a195.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@YouJiacheng&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;24.50 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/jadenj3o/status/1914893086276169754"&gt;Cubic sliding window size schedule, 2× max window size (24.84 minutes)&lt;/a&gt; &lt;a href="https://x.com/YouJiacheng/status/1915667616913645985"&gt;24.5min repro&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;04/22/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_2_medium/2025-04-22_Record8/075_640429f2-e726-4e83-aa27-684626239ffc.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@jadenj3o&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Q: What is the point of NanoGPT speedrunning?&lt;/h3&gt; 
&lt;p&gt;A: The officially stated goal of NanoGPT speedrunning is as follows: &lt;code&gt;gotta go fast&lt;/code&gt;. But for something a little more verbose involving an argument for good benchmarking, here's some kind of manifesto, adorned with a blessing from the master. &lt;a href="https://x.com/karpathy/status/1846790537262571739"&gt;https://x.com/karpathy/status/1846790537262571739&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Q: What makes "NanoGPT speedrunning" not just another idiosyncratic benchmark?&lt;/h3&gt; 
&lt;p&gt;A: Because it is a &lt;em&gt;competitive&lt;/em&gt; benchmark. In particular, if you attain a new speed record (using whatever method you want), there is an open invitation for you to post that record (on arXiv or X) and thereby vacuum up all the clout for yourself. I will even help you do it by reposting you as much as I can.&lt;/p&gt; 
&lt;!--On the contrary, for example, the benchmark used in the [Sophia](https://arxiv.org/abs/2305.14342) paper does *not* have this property.
There is no such open invitation for anyone to compete on the benchmark they used. In particular, if, for a random and definitely not weirdly specific example, you happen to find better AdamW hyperparameters for their training setup than
the ones they used which significantly close the gap between AdamW and their proposed optimizer,
then there is no clear path for you to publish that result in *any* form.
You could try posting it on X.com, but then you would be risking being perceived as aggressive/confrontational, which is *not a good look* in this racket.
So if you're rational, the result probably just dies with you and no one else learns anything
(unless you're in a frontier lab, in which case you can do a nice internal writeup. Boy I'd love to get my hands on those writeups).--&gt; 
&lt;p&gt;&lt;a href="https://www.argmin.net/p/too-much-information"&gt;"Artificial intelligence advances by inventing games and gloating to goad others to play" - Professor Ben Recht&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Q: NanoGPT speedrunning is cool and all, but meh it probably won't scale and is just overfitting to val loss&lt;/h3&gt; 
&lt;p&gt;A: This is hard to refute, since "at scale" is an infinite category (what if the methods stop working only for &amp;gt;100T models?), making it impossible to fully prove. Also, I would agree that some of the methods used in the speedrun are unlikely to scale, particularly those which &lt;em&gt;impose additional structure&lt;/em&gt; on the network, such as logit softcapping. But if the reader cares about 1.5B models, they might be convinced by this result:&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Straightforwardly scaling up the speedrun (10/18/24 version) to 1.5B parameters yields a model with GPT-2 (1.5B)-level HellaSwag performance 2.5x more cheaply than &lt;a href="https://github.com/karpathy/llm.c/discussions/677"&gt;@karpathy's baseline&lt;/a&gt; ($233 instead of $576):&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/img/nanogpt_speedrun51.png" alt="" /&gt; [&lt;a href="https://github.com/KellerJordan/modded-nanogpt/raw/master/records/track_1_short/2024-10-20_ScaleUp1B/ad8d7ae5-7b2d-4ee9-bc52-f912e9174d7a.txt"&gt;reproducible log&lt;/a&gt;] &lt;img src="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/img/nanogpt_speedrun52.png" alt="" /&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;&lt;a href="https://github.com/KellerJordan/Muon"&gt;Muon optimizer&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;Muon is defined as follows:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/img/algo_optimizer.png" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;Where NewtonSchulz5 is the following Newton-Schulz iteration [2, 3], which approximately replaces &lt;code&gt;G&lt;/code&gt; with &lt;code&gt;U @ V.T&lt;/code&gt; where &lt;code&gt;U, S, V = G.svd()&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;@torch.compile
def zeroth_power_via_newtonschulz5(G, steps=5, eps=1e-7):
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16() / (G.norm() + eps)
    if G.size(0) &amp;gt; G.size(1):
        X = X.T 
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A
        X = a * X + B @ X
    if G.size(0) &amp;gt; G.size(1):
        X = X.T 
    return X.to(G.dtype)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For this training scenario, Muon has the following favorable properties:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Lower memory usage than Adam&lt;/li&gt; 
 &lt;li&gt;~1.5x better sample-efficiency&lt;/li&gt; 
 &lt;li&gt;&amp;lt;2% wallclock overhead&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Provenance&lt;/h3&gt; 
&lt;p&gt;Many of the choices made to generate this optimizer were obtained experimentally by our pursuit of &lt;a href="https://github.com/KellerJordan/cifar10-airbench"&gt;CIFAR-10 speedrunning&lt;/a&gt;. In particular, we experimentally obtained the following practices:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Using Nesterov momentum inside the update, with orthogonalization applied after momentum.&lt;/li&gt; 
 &lt;li&gt;Using a specifically quintic Newton-Schulz iteration as the method of orthogonalization.&lt;/li&gt; 
 &lt;li&gt;Using non-convergent coefficients for the quintic polynomial in order to maximize slope at zero, and thereby minimize the number of necessary Newton-Schulz iterations. It turns out that the variance doesn't actually matter that much, so we end up with a quintic that rapidly converges to the range 0.68, 1.13 upon repeated application, rather than converging more slowly to 1.&lt;/li&gt; 
 &lt;li&gt;Running the Newton-Schulz iteration in bfloat16 (whereas Shampoo implementations often depend on inverse-pth-roots run in fp32 or fp64).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Our use of a Newton-Schulz iteration for orthogonalization traces to &lt;a href="https://arxiv.org/abs/2409.20325"&gt;Bernstein &amp;amp; Newhouse (2024)&lt;/a&gt;, who suggested it as a way to compute Shampoo [5, 6] preconditioners, and theoretically explored Shampoo without preconditioner accumulation. In particular, Jeremy Bernstein @jxbz sent us the draft, which caused us to experiment with various Newton-Schulz iterations as the orthogonalization method for this optimizer. If we had used SVD instead of a Newton-Schulz iteration, this optimizer would have been too slow to be useful. Bernstein &amp;amp; Newhouse also pointed out that Shampoo without preconditioner accumulation is equivalent to steepest descent in the spectral norm, and therefore Shampoo can be thought of as a way to smooth out spectral steepest descent. The proposed optimizer can be thought of as a second way of smoothing spectral steepest descent, with a different set of memory and runtime tradeoffs compared to Shampoo.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Running on fewer GPUs&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;To run experiments on fewer GPUs, simply modify &lt;code&gt;run.sh&lt;/code&gt; to have a different &lt;code&gt;--nproc_per_node&lt;/code&gt;. This should not change the behavior of the training.&lt;/li&gt; 
 &lt;li&gt;If you're running out of memory, you may need to reduce the sequence length for FlexAttention (which does change the training. see &lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/38"&gt;here&lt;/a&gt; for a guide)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;References&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/2406.17557"&gt;Guilherme Penedo et al. "The fineweb datasets: Decanting the web for the finest text data at scale." arXiv preprint arXiv:2406.17557 (2024).&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Nicholas J. Higham. Functions of Matrices. Society for Industrial and Applied Mathematics (2008). Equation 5.22.&lt;/li&gt; 
 &lt;li&gt;GÃ¼nther Schulz. Iterative Berechnung der reziproken Matrix. Z. Angew. Math. Mech., 13:57â59 (1933).&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/2409.20325"&gt;Jeremy Bernstein and Laker Newhouse. "Old Optimizer, New Norm: An Anthology." arxiv preprint arXiv:2409.20325 (2024).&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/1802.09568"&gt;Vineet Gupta, Tomer Koren, and Yoram Singer. "Shampoo: Preconditioned stochastic tensor optimization." International Conference on Machine Learning. PMLR, 2018.&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/2002.09018"&gt;Rohan Anil et al. "Scalable second order optimization for deep learning." arXiv preprint arXiv:2002.09018 (2020).&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/2405.18392"&gt;Alexander HÃ¤gele et al. "Scaling Laws and Compute-Optimal Training Beyond Fixed Training Durations." arXiv preprint arXiv:2405.18392 (2024).&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/2410.17897"&gt;Zhanchao Zhou et al. "Value Residual Learning For Alleviating Attention Concentration In Transformers." arXiv preprint arXiv:2410.17897 (2024).&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/2408.00118"&gt;Team, Gemma, et al. "Gemma 2: Improving open language models at a practical size." arXiv preprint arXiv:2408.00118 (2024).&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf"&gt;Alec Radford et al. "Language models are unsupervised multitask learners." OpenAI blog 1.8 (2019).&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;@misc{modded_nanogpt_2024,
  author       = {Keller Jordan and Jeremy Bernstein and Brendan Rappazzo and
                  @fernbear.bsky.social and Boza Vlado and You Jiacheng and
                  Franz Cesista and Braden Koszarsky and @Grad62304977},
  title        = {modded-nanogpt: Speedrunning the NanoGPT baseline},
  year         = {2024},
  url          = {https://github.com/KellerJordan/modded-nanogpt}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;img src="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/img/dofa.jpg" alt="itsover_wereback" style="width:100%;" /&gt;</description>
    </item>
    
    <item>
      <title>nvm-sh/nvm</title>
      <link>https://github.com/nvm-sh/nvm</link>
      <description>&lt;p&gt;Node Version Manager - POSIX-compliant bash script to manage multiple active node.js versions&lt;/p&gt;&lt;hr&gt;&lt;a href="https://github.com/nvm-sh/logos"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://raw.githubusercontent.com/nvm-sh/logos/HEAD/nvm-logo-white.svg" /&gt; 
  &lt;img src="https://raw.githubusercontent.com/nvm-sh/logos/HEAD/nvm-logo-color.svg?sanitize=true" height="50" alt="nvm project logo" /&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;h1&gt;Node Version Manager &lt;a href="https://app.travis-ci.com/nvm-sh/nvm"&gt;&lt;img src="https://app.travis-ci.com/nvm-sh/nvm.svg?branch=master" alt="Build Status" /&gt;&lt;/a&gt; &lt;a href="https://github.com/nvm-sh/nvm/releases/tag/v0.40.3"&gt;&lt;img src="https://img.shields.io/badge/version-v0.40.3-yellow.svg?sanitize=true" alt="nvm version" /&gt;&lt;/a&gt; &lt;a href="https://bestpractices.dev/projects/684"&gt;&lt;img src="https://bestpractices.dev/projects/684/badge" alt="CII Best Practices" /&gt;&lt;/a&gt;&lt;/h1&gt; 
&lt;!-- To update this table of contents, ensure you have run `npm install` then `npm run doctoc` --&gt; 
&lt;!-- START doctoc generated TOC please keep comment here to allow auto update --&gt; 
&lt;!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE --&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nvm-sh/nvm/master/#intro"&gt;Intro&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nvm-sh/nvm/master/#about"&gt;About&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nvm-sh/nvm/master/#installing-and-updating"&gt;Installing and Updating&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nvm-sh/nvm/master/#install--update-script"&gt;Install &amp;amp; Update Script&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nvm-sh/nvm/master/#additional-notes"&gt;Additional Notes&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nvm-sh/nvm/master/#installing-in-docker"&gt;Installing in Docker&lt;/a&gt; 
      &lt;ul&gt; 
       &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nvm-sh/nvm/master/#installing-in-docker-for-cicd-jobs"&gt;Installing in Docker for CICD-Jobs&lt;/a&gt;&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nvm-sh/nvm/master/#troubleshooting-on-linux"&gt;Troubleshooting on Linux&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nvm-sh/nvm/master/#troubleshooting-on-macos"&gt;Troubleshooting on macOS&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nvm-sh/nvm/master/#ansible"&gt;Ansible&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nvm-sh/nvm/master/#verify-installation"&gt;Verify Installation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nvm-sh/nvm/master/#important-notes"&gt;Important Notes&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nvm-sh/nvm/master/#git-install"&gt;Git Install&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nvm-sh/nvm/master/#manual-install"&gt;Manual Install&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nvm-sh/nvm/master/#manual-upgrade"&gt;Manual Upgrade&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nvm-sh/nvm/master/#usage"&gt;Usage&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nvm-sh/nvm/master/#long-term-support"&gt;Long-term Support&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nvm-sh/nvm/master/#migrating-global-packages-while-installing"&gt;Migrating Global Packages While Installing&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nvm-sh/nvm/master/#default-global-packages-from-file-while-installing"&gt;Default Global Packages From File While Installing&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nvm-sh/nvm/master/#iojs"&gt;io.js&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nvm-sh/nvm/master/#system-version-of-node"&gt;System Version of Node&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nvm-sh/nvm/master/#listing-versions"&gt;Listing Versions&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nvm-sh/nvm/master/#setting-custom-colors"&gt;Setting Custom Colors&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nvm-sh/nvm/master/#persisting-custom-colors"&gt;Persisting custom colors&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nvm-sh/nvm/master/#suppressing-colorized-output"&gt;Suppressing colorized output&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nvm-sh/nvm/master/#restoring-path"&gt;Restoring PATH&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nvm-sh/nvm/master/#set-default-node-version"&gt;Set default node version&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nvm-sh/nvm/master/#use-a-mirror-of-node-binaries"&gt;Use a mirror of node binaries&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nvm-sh/nvm/master/#pass-authorization-header-to-mirror"&gt;Pass Authorization header to mirror&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nvm-sh/nvm/master/#nvmrc"&gt;.nvmrc&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nvm-sh/nvm/master/#deeper-shell-integration"&gt;Deeper Shell Integration&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nvm-sh/nvm/master/#calling-nvm-use-automatically-in-a-directory-with-a-nvmrc-file"&gt;Calling &lt;code&gt;nvm use&lt;/code&gt; automatically in a directory with a &lt;code&gt;.nvmrc&lt;/code&gt; file&lt;/a&gt; 
      &lt;ul&gt; 
       &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nvm-sh/nvm/master/#bash"&gt;bash&lt;/a&gt;&lt;/li&gt; 
       &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nvm-sh/nvm/master/#zsh"&gt;zsh&lt;/a&gt;&lt;/li&gt; 
       &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nvm-sh/nvm/master/#fish"&gt;fish&lt;/a&gt;&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nvm-sh/nvm/master/#running-tests"&gt;Running Tests&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nvm-sh/nvm/master/#environment-variables"&gt;Environment variables&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nvm-sh/nvm/master/#bash-completion"&gt;Bash Completion&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nvm-sh/nvm/master/#usage-1"&gt;Usage&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nvm-sh/nvm/master/#compatibility-issues"&gt;Compatibility Issues&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nvm-sh/nvm/master/#installing-nvm-on-alpine-linux"&gt;Installing nvm on Alpine Linux&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nvm-sh/nvm/master/#alpine-linux-313"&gt;Alpine Linux 3.13+&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nvm-sh/nvm/master/#alpine-linux-35---312"&gt;Alpine Linux 3.5 - 3.12&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nvm-sh/nvm/master/#uninstalling--removal"&gt;Uninstalling / Removal&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nvm-sh/nvm/master/#manual-uninstall"&gt;Manual Uninstall&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nvm-sh/nvm/master/#docker-for-development-environment"&gt;Docker For Development Environment&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nvm-sh/nvm/master/#problems"&gt;Problems&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nvm-sh/nvm/master/#macos-troubleshooting"&gt;macOS Troubleshooting&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nvm-sh/nvm/master/#wsl-troubleshooting"&gt;WSL Troubleshooting&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nvm-sh/nvm/master/#maintainers"&gt;Maintainers&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nvm-sh/nvm/master/#project-support"&gt;Project Support&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nvm-sh/nvm/master/#enterprise-support"&gt;Enterprise Support&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nvm-sh/nvm/master/#license"&gt;License&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nvm-sh/nvm/master/#copyright-notice"&gt;Copyright notice&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- END doctoc generated TOC please keep comment here to allow auto update --&gt; 
&lt;h2&gt;Intro&lt;/h2&gt; 
&lt;p&gt;&lt;code&gt;nvm&lt;/code&gt; allows you to quickly install and use different versions of node via the command line.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;$ nvm use 16
Now using node v16.9.1 (npm v7.21.1)
$ node -v
v16.9.1
$ nvm use 14
Now using node v14.18.0 (npm v6.14.15)
$ node -v
v14.18.0
$ nvm install 12
Now using node v12.22.6 (npm v6.14.5)
$ node -v
v12.22.6
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Simple as that!&lt;/p&gt; 
&lt;h2&gt;About&lt;/h2&gt; 
&lt;p&gt;nvm is a version manager for &lt;a href="https://nodejs.org/en/"&gt;node.js&lt;/a&gt;, designed to be installed per-user, and invoked per-shell. &lt;code&gt;nvm&lt;/code&gt; works on any POSIX-compliant shell (sh, dash, ksh, zsh, bash), in particular on these platforms: unix, macOS, and &lt;a href="https://github.com/nvm-sh/nvm#important-notes"&gt;windows WSL&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a id="installation-and-update"&gt;&lt;/a&gt; &lt;a id="install-script"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Installing and Updating&lt;/h2&gt; 
&lt;h3&gt;Install &amp;amp; Update Script&lt;/h3&gt; 
&lt;p&gt;To &lt;strong&gt;install&lt;/strong&gt; or &lt;strong&gt;update&lt;/strong&gt; nvm, you should run the &lt;a href="https://github.com/nvm-sh/nvm/raw/v0.40.3/install.sh"&gt;install script&lt;/a&gt;. To do that, you may either download and run the script manually, or use the following cURL or Wget command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.3/install.sh | bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;wget -qO- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.3/install.sh | bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Running either of the above commands downloads a script and runs it. The script clones the nvm repository to &lt;code&gt;~/.nvm&lt;/code&gt;, and attempts to add the source lines from the snippet below to the correct profile file (&lt;code&gt;~/.bashrc&lt;/code&gt;, &lt;code&gt;~/.bash_profile&lt;/code&gt;, &lt;code&gt;~/.zshrc&lt;/code&gt;, or &lt;code&gt;~/.profile&lt;/code&gt;). If you find the install script is updating the wrong profile file, set the &lt;code&gt;$PROFILE&lt;/code&gt; env var to the profile file’s path, and then rerun the installation script.&lt;/p&gt; 
&lt;p&gt;&lt;a id="profile_snippet"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;export NVM_DIR="$([ -z "${XDG_CONFIG_HOME-}" ] &amp;amp;&amp;amp; printf %s "${HOME}/.nvm" || printf %s "${XDG_CONFIG_HOME}/nvm")"
[ -s "$NVM_DIR/nvm.sh" ] &amp;amp;&amp;amp; \. "$NVM_DIR/nvm.sh" # This loads nvm
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Additional Notes&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;If the environment variable &lt;code&gt;$XDG_CONFIG_HOME&lt;/code&gt; is present, it will place the &lt;code&gt;nvm&lt;/code&gt; files there.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;You can add &lt;code&gt;--no-use&lt;/code&gt; to the end of the above script to postpone using &lt;code&gt;nvm&lt;/code&gt; until you manually &lt;a href="https://raw.githubusercontent.com/nvm-sh/nvm/master/#usage"&gt;&lt;code&gt;use&lt;/code&gt;&lt;/a&gt; it:&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;export NVM_DIR="$([ -z "${XDG_CONFIG_HOME-}" ] &amp;amp;&amp;amp; printf %s "${HOME}/.nvm" || printf %s "${XDG_CONFIG_HOME}/nvm")"
[ -s "$NVM_DIR/nvm.sh" ] &amp;amp;&amp;amp; \. "$NVM_DIR/nvm.sh" --no-use # This loads nvm, without auto-using the default version
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;You can customize the install source, directory, profile, and version using the &lt;code&gt;NVM_SOURCE&lt;/code&gt;, &lt;code&gt;NVM_DIR&lt;/code&gt;, &lt;code&gt;PROFILE&lt;/code&gt;, and &lt;code&gt;NODE_VERSION&lt;/code&gt; variables. Eg: &lt;code&gt;curl ... | NVM_DIR="path/to/nvm"&lt;/code&gt;. Ensure that the &lt;code&gt;NVM_DIR&lt;/code&gt; does not contain a trailing slash.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;The installer can use &lt;code&gt;git&lt;/code&gt;, &lt;code&gt;curl&lt;/code&gt;, or &lt;code&gt;wget&lt;/code&gt; to download &lt;code&gt;nvm&lt;/code&gt;, whichever is available.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;You can instruct the installer to not edit your shell config (for example if you already get completions via a &lt;a href="https://github.com/ohmyzsh/ohmyzsh/tree/master/plugins/nvm"&gt;zsh nvm plugin&lt;/a&gt;) by setting &lt;code&gt;PROFILE=/dev/null&lt;/code&gt; before running the &lt;code&gt;install.sh&lt;/code&gt; script. Here's an example one-line command to do that: &lt;code&gt;PROFILE=/dev/null bash -c 'curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.3/install.sh | bash'&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Installing in Docker&lt;/h4&gt; 
&lt;p&gt;When invoking bash as a non-interactive shell, like in a Docker container, none of the regular profile files are sourced. In order to use &lt;code&gt;nvm&lt;/code&gt;, &lt;code&gt;node&lt;/code&gt;, and &lt;code&gt;npm&lt;/code&gt; like normal, you can instead specify the special &lt;code&gt;BASH_ENV&lt;/code&gt; variable, which bash sources when invoked non-interactively.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-Dockerfile"&gt;# Use bash for the shell
SHELL ["/bin/bash", "-o", "pipefail", "-c"]

# Create a script file sourced by both interactive and non-interactive bash shells
ENV BASH_ENV /home/user/.bash_env
RUN touch "${BASH_ENV}"
RUN echo '. "${BASH_ENV}"' &amp;gt;&amp;gt; ~/.bashrc

# Download and install nvm
RUN curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.3/install.sh | PROFILE="${BASH_ENV}" bash
RUN echo node &amp;gt; .nvmrc
RUN nvm install
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;Installing in Docker for CICD-Jobs&lt;/h5&gt; 
&lt;p&gt;More robust, works in CI/CD-Jobs. Can be run in interactive and non-interactive containers. See &lt;a href="https://github.com/nvm-sh/nvm/issues/3531"&gt;https://github.com/nvm-sh/nvm/issues/3531&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-Dockerfile"&gt;FROM ubuntu:latest
ARG NODE_VERSION=20

# install curl
RUN apt update &amp;amp;&amp;amp; apt install curl -y

# install nvm
RUN curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.3/install.sh | bash

# set env
ENV NVM_DIR=/root/.nvm

# install node
RUN bash -c "source $NVM_DIR/nvm.sh &amp;amp;&amp;amp; nvm install $NODE_VERSION"

# set ENTRYPOINT for reloading nvm-environment
ENTRYPOINT ["bash", "-c", "source $NVM_DIR/nvm.sh &amp;amp;&amp;amp; exec \"$@\"", "--"]

# set cmd to bash
CMD ["/bin/bash"]

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This example defaults to installation of nodejs version 20.x.y. Optionally you can easily override the version with docker build args like:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;docker build -t nvmimage --build-arg NODE_VERSION=19 .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After creation of the image you can start container interactively and run commands, for example:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;docker run --rm -it nvmimage

root@0a6b5a237c14:/# nvm -v
0.40.3

root@0a6b5a237c14:/# node -v
v19.9.0

root@0a6b5a237c14:/# npm -v
9.6.3
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Noninteractive example:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;user@host:/tmp/test $ docker run --rm -it nvmimage node -v
v19.9.0
user@host:/tmp/test $ docker run --rm -it nvmimage npm -v
9.6.3
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Troubleshooting on Linux&lt;/h4&gt; 
&lt;p&gt;On Linux, after running the install script, if you get &lt;code&gt;nvm: command not found&lt;/code&gt; or see no feedback from your terminal after you type &lt;code&gt;command -v nvm&lt;/code&gt;, simply close your current terminal, open a new terminal, and try verifying again. Alternatively, you can run the following commands for the different shells on the command line:&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;bash&lt;/em&gt;: &lt;code&gt;source ~/.bashrc&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;zsh&lt;/em&gt;: &lt;code&gt;source ~/.zshrc&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;ksh&lt;/em&gt;: &lt;code&gt;. ~/.profile&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;These should pick up the &lt;code&gt;nvm&lt;/code&gt; command.&lt;/p&gt; 
&lt;h4&gt;Troubleshooting on macOS&lt;/h4&gt; 
&lt;p&gt;Since OS X 10.9, &lt;code&gt;/usr/bin/git&lt;/code&gt; has been preset by Xcode command line tools, which means we can't properly detect if Git is installed or not. You need to manually install the Xcode command line tools before running the install script, otherwise, it'll fail. (see &lt;a href="https://github.com/nvm-sh/nvm/issues/1782"&gt;#1782&lt;/a&gt;)&lt;/p&gt; 
&lt;p&gt;If you get &lt;code&gt;nvm: command not found&lt;/code&gt; after running the install script, one of the following might be the reason:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Since macOS 10.15, the default shell is &lt;code&gt;zsh&lt;/code&gt; and nvm will look for &lt;code&gt;.zshrc&lt;/code&gt; to update, none is installed by default. Create one with &lt;code&gt;touch ~/.zshrc&lt;/code&gt; and run the install script again.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;If you use bash, the previous default shell, your system may not have &lt;code&gt;.bash_profile&lt;/code&gt; or &lt;code&gt;.bashrc&lt;/code&gt; files where the command is set up. Create one of them with &lt;code&gt;touch ~/.bash_profile&lt;/code&gt; or &lt;code&gt;touch ~/.bashrc&lt;/code&gt; and run the install script again. Then, run &lt;code&gt;. ~/.bash_profile&lt;/code&gt; or &lt;code&gt;. ~/.bashrc&lt;/code&gt; to pick up the &lt;code&gt;nvm&lt;/code&gt; command.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;You have previously used &lt;code&gt;bash&lt;/code&gt;, but you have &lt;code&gt;zsh&lt;/code&gt; installed. You need to manually add &lt;a href="https://raw.githubusercontent.com/nvm-sh/nvm/master/#manual-install"&gt;these lines&lt;/a&gt; to &lt;code&gt;~/.zshrc&lt;/code&gt; and run &lt;code&gt;. ~/.zshrc&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;You might need to restart your terminal instance or run &lt;code&gt;. ~/.nvm/nvm.sh&lt;/code&gt;. Restarting your terminal/opening a new tab/window, or running the source command will load the command and the new configuration.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;If the above didn't help, you might need to restart your terminal instance. Try opening a new tab/window in your terminal and retry.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If the above doesn't fix the problem, you may try the following:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;If you use bash, it may be that your &lt;code&gt;.bash_profile&lt;/code&gt; (or &lt;code&gt;~/.profile&lt;/code&gt;) does not source your &lt;code&gt;~/.bashrc&lt;/code&gt; properly. You could fix this by adding &lt;code&gt;source ~/&amp;lt;your_profile_file&amp;gt;&lt;/code&gt; to it or following the next step below.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Try adding &lt;a href="https://raw.githubusercontent.com/nvm-sh/nvm/master/#profile_snippet"&gt;the snippet from the install section&lt;/a&gt;, that finds the correct nvm directory and loads nvm, to your usual profile (&lt;code&gt;~/.bash_profile&lt;/code&gt;, &lt;code&gt;~/.zshrc&lt;/code&gt;, &lt;code&gt;~/.profile&lt;/code&gt;, or &lt;code&gt;~/.bashrc&lt;/code&gt;).&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;For more information about this issue and possible workarounds, please &lt;a href="https://github.com/nvm-sh/nvm/issues/576"&gt;refer here&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; For Macs with the Apple Silicon chip, node started offering &lt;strong&gt;arm64&lt;/strong&gt; arch Darwin packages since v16.0.0 and experimental &lt;strong&gt;arm64&lt;/strong&gt; support when compiling from source since v14.17.0. If you are facing issues installing node using &lt;code&gt;nvm&lt;/code&gt;, you may want to update to one of those versions or later.&lt;/p&gt; 
&lt;h4&gt;Ansible&lt;/h4&gt; 
&lt;p&gt;You can use a task:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;- name: Install nvm
  ansible.builtin.shell: &amp;gt;
    curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.3/install.sh | bash
  args:
    creates: "{{ ansible_env.HOME }}/.nvm/nvm.sh"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Verify Installation&lt;/h3&gt; 
&lt;p&gt;To verify that nvm has been installed, do:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;command -v nvm
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;which should output &lt;code&gt;nvm&lt;/code&gt; if the installation was successful. Please note that &lt;code&gt;which nvm&lt;/code&gt; will not work, since &lt;code&gt;nvm&lt;/code&gt; is a sourced shell function, not an executable binary.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; On Linux, after running the install script, if you get &lt;code&gt;nvm: command not found&lt;/code&gt; or see no feedback from your terminal after you type &lt;code&gt;command -v nvm&lt;/code&gt;, simply close your current terminal, open a new terminal, and try verifying again.&lt;/p&gt; 
&lt;h3&gt;Important Notes&lt;/h3&gt; 
&lt;p&gt;If you're running a system without prepackaged binary available, which means you're going to install node or io.js from its source code, you need to make sure your system has a C++ compiler. For OS X, Xcode will work, for Debian/Ubuntu based GNU/Linux, the &lt;code&gt;build-essential&lt;/code&gt; and &lt;code&gt;libssl-dev&lt;/code&gt; packages work.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; &lt;code&gt;nvm&lt;/code&gt; also supports Windows in some cases. It should work through WSL (Windows Subsystem for Linux) depending on the version of WSL. It should also work with &lt;a href="https://gitforwindows.org/"&gt;GitBash&lt;/a&gt; (MSYS) or &lt;a href="https://cygwin.com"&gt;Cygwin&lt;/a&gt;. Otherwise, for Windows, a few alternatives exist, which are neither supported nor developed by us:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/coreybutler/nvm-windows"&gt;nvm-windows&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/marcelklehr/nodist"&gt;nodist&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/jasongin/nvs"&gt;nvs&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; &lt;code&gt;nvm&lt;/code&gt; does not support &lt;a href="https://fishshell.com"&gt;Fish&lt;/a&gt; either (see &lt;a href="https://github.com/nvm-sh/nvm/issues/303"&gt;#303&lt;/a&gt;). Alternatives exist, which are neither supported nor developed by us:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/edc/bass"&gt;bass&lt;/a&gt; allows you to use utilities written for Bash in fish shell&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/brigand/fast-nvm-fish"&gt;fast-nvm-fish&lt;/a&gt; only works with version numbers (not aliases) but doesn't significantly slow your shell startup&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/derekstavis/plugin-nvm"&gt;plugin-nvm&lt;/a&gt; plugin for &lt;a href="https://github.com/oh-my-fish/oh-my-fish"&gt;Oh My Fish&lt;/a&gt;, which makes nvm and its completions available in fish shell&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/jorgebucaran/nvm.fish"&gt;nvm.fish&lt;/a&gt; - The Node.js version manager you'll adore, crafted just for Fish&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/FabioAntunes/fish-nvm"&gt;fish-nvm&lt;/a&gt; - Wrapper around nvm for fish, delays sourcing nvm until it's actually used.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; We still have some problems with FreeBSD, because there is no official pre-built binary for FreeBSD, and building from source may need &lt;a href="https://www.freshports.org/www/node/files/patch-deps_v8_src_base_platform_platform-posix.cc"&gt;patches&lt;/a&gt;; see the issue ticket:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/nvm-sh/nvm/issues/900"&gt;[#900] [Bug] node on FreeBSD may need to be patched&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/nodejs/node/issues/3716"&gt;nodejs/node#3716&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; On OS X, if you do not have Xcode installed and you do not wish to download the ~4.3GB file, you can install the &lt;code&gt;Command Line Tools&lt;/code&gt;. You can check out this blog post on how to just that:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://osxdaily.com/2014/02/12/install-command-line-tools-mac-os-x/"&gt;How to Install Command Line Tools in OS X Mavericks &amp;amp; Yosemite (Without Xcode)&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; On OS X, if you have/had a "system" node installed and want to install modules globally, keep in mind that:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;When using &lt;code&gt;nvm&lt;/code&gt; you do not need &lt;code&gt;sudo&lt;/code&gt; to globally install a module with &lt;code&gt;npm -g&lt;/code&gt;, so instead of doing &lt;code&gt;sudo npm install -g grunt&lt;/code&gt;, do instead &lt;code&gt;npm install -g grunt&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;If you have an &lt;code&gt;~/.npmrc&lt;/code&gt; file, make sure it does not contain any &lt;code&gt;prefix&lt;/code&gt; settings (which is not compatible with &lt;code&gt;nvm&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;You can (but should not?) keep your previous "system" node install, but &lt;code&gt;nvm&lt;/code&gt; will only be available to your user account (the one used to install nvm). This might cause version mismatches, as other users will be using &lt;code&gt;/usr/local/lib/node_modules/*&lt;/code&gt; VS your user account using &lt;code&gt;~/.nvm/versions/node/vX.X.X/lib/node_modules/*&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Homebrew installation is not supported. If you have issues with homebrew-installed &lt;code&gt;nvm&lt;/code&gt;, please &lt;code&gt;brew uninstall&lt;/code&gt; it, and install it using the instructions below, before filing an issue.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; If you're using &lt;code&gt;zsh&lt;/code&gt; you can easily install &lt;code&gt;nvm&lt;/code&gt; as a zsh plugin. Install &lt;a href="https://github.com/lukechilds/zsh-nvm"&gt;&lt;code&gt;zsh-nvm&lt;/code&gt;&lt;/a&gt; and run &lt;code&gt;nvm upgrade&lt;/code&gt; to upgrade (&lt;a href="https://github.com/lukechilds/zsh-nvm#auto-use"&gt;you can set&lt;/a&gt; &lt;code&gt;NVM_AUTO_USE=true&lt;/code&gt; to have it automatically detect and use &lt;code&gt;.nvmrc&lt;/code&gt; files).&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Git versions before v1.7 may face a problem of cloning &lt;code&gt;nvm&lt;/code&gt; source from GitHub via https protocol, and there is also different behavior of git before v1.6, and git prior to &lt;a href="https://github.com/git/git/commit/5a7d5b683f869d3e3884a89775241afa515da9e7"&gt;v1.17.10&lt;/a&gt; can not clone tags, so the minimum required git version is v1.7.10. If you are interested in the problem we mentioned here, please refer to GitHub's &lt;a href="https://help.github.com/articles/https-cloning-errors/"&gt;HTTPS cloning errors&lt;/a&gt; article.&lt;/p&gt; 
&lt;h3&gt;Git Install&lt;/h3&gt; 
&lt;p&gt;If you have &lt;code&gt;git&lt;/code&gt; installed (requires git v1.7.10+):&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;clone this repo in the root of your user profile 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;cd ~/&lt;/code&gt; from anywhere then &lt;code&gt;git clone https://github.com/nvm-sh/nvm.git .nvm&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;cd ~/.nvm&lt;/code&gt; and check out the latest version with &lt;code&gt;git checkout v0.40.3&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;activate &lt;code&gt;nvm&lt;/code&gt; by sourcing it from your shell: &lt;code&gt;. ./nvm.sh&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Now add these lines to your &lt;code&gt;~/.bashrc&lt;/code&gt;, &lt;code&gt;~/.profile&lt;/code&gt;, or &lt;code&gt;~/.zshrc&lt;/code&gt; file to have it automatically sourced upon login: (you may have to add to more than one of the above files)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;export NVM_DIR="$HOME/.nvm"
[ -s "$NVM_DIR/nvm.sh" ] &amp;amp;&amp;amp; \. "$NVM_DIR/nvm.sh"  # This loads nvm
[ -s "$NVM_DIR/bash_completion" ] &amp;amp;&amp;amp; \. "$NVM_DIR/bash_completion"  # This loads nvm bash_completion
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Manual Install&lt;/h3&gt; 
&lt;p&gt;For a fully manual install, execute the following lines to first clone the &lt;code&gt;nvm&lt;/code&gt; repository into &lt;code&gt;$HOME/.nvm&lt;/code&gt;, and then load &lt;code&gt;nvm&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;export NVM_DIR="$HOME/.nvm" &amp;amp;&amp;amp; (
  git clone https://github.com/nvm-sh/nvm.git "$NVM_DIR"
  cd "$NVM_DIR"
  git checkout `git describe --abbrev=0 --tags --match "v[0-9]*" $(git rev-list --tags --max-count=1)`
) &amp;amp;&amp;amp; \. "$NVM_DIR/nvm.sh"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Now add these lines to your &lt;code&gt;~/.bashrc&lt;/code&gt;, &lt;code&gt;~/.profile&lt;/code&gt;, or &lt;code&gt;~/.zshrc&lt;/code&gt; file to have it automatically sourced upon login: (you may have to add to more than one of the above files)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;export NVM_DIR="$HOME/.nvm"
[ -s "$NVM_DIR/nvm.sh" ] &amp;amp;&amp;amp; \. "$NVM_DIR/nvm.sh" # This loads nvm
[ -s "$NVM_DIR/bash_completion" ] &amp;amp;&amp;amp; \. "$NVM_DIR/bash_completion"  # This loads nvm bash_completion
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Manual Upgrade&lt;/h3&gt; 
&lt;p&gt;For manual upgrade with &lt;code&gt;git&lt;/code&gt; (requires git v1.7.10+):&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;change to the &lt;code&gt;$NVM_DIR&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;pull down the latest changes&lt;/li&gt; 
 &lt;li&gt;check out the latest version&lt;/li&gt; 
 &lt;li&gt;activate the new version&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;(
  cd "$NVM_DIR"
  git fetch --tags origin
  git checkout `git describe --abbrev=0 --tags --match "v[0-9]*" $(git rev-list --tags --max-count=1)`
) &amp;amp;&amp;amp; \. "$NVM_DIR/nvm.sh"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;p&gt;To download, compile, and install the latest release of node, do this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;nvm install node # "node" is an alias for the latest version
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To install a specific version of node:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;nvm install 14.7.0 # or 16.3.0, 12.22.1, etc
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To set an alias:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;nvm alias my_alias v14.4.0
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Make sure that your alias does not contain any spaces or slashes.&lt;/p&gt; 
&lt;p&gt;The first version installed becomes the default. New shells will start with the default version of node (e.g., &lt;code&gt;nvm alias default&lt;/code&gt;).&lt;/p&gt; 
&lt;p&gt;You can list available versions using &lt;code&gt;ls-remote&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;nvm ls-remote
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;And then in any new shell just use the installed version:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;nvm use node
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or you can just run it:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;nvm run node --version
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or, you can run any arbitrary command in a subshell with the desired version of node:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;nvm exec 4.2 node --version
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also get the path to the executable to where it was installed:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;nvm which 12.22
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;In place of a version pointer like "14.7" or "16.3" or "12.22.1", you can use the following special default aliases with &lt;code&gt;nvm install&lt;/code&gt;, &lt;code&gt;nvm use&lt;/code&gt;, &lt;code&gt;nvm run&lt;/code&gt;, &lt;code&gt;nvm exec&lt;/code&gt;, &lt;code&gt;nvm which&lt;/code&gt;, etc:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;node&lt;/code&gt;: this installs the latest version of &lt;a href="https://nodejs.org/en/"&gt;&lt;code&gt;node&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;iojs&lt;/code&gt;: this installs the latest version of &lt;a href="https://iojs.org/en/"&gt;&lt;code&gt;io.js&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;stable&lt;/code&gt;: this alias is deprecated, and only truly applies to &lt;code&gt;node&lt;/code&gt; &lt;code&gt;v0.12&lt;/code&gt; and earlier. Currently, this is an alias for &lt;code&gt;node&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;unstable&lt;/code&gt;: this alias points to &lt;code&gt;node&lt;/code&gt; &lt;code&gt;v0.11&lt;/code&gt; - the last "unstable" node release, since post-1.0, all node versions are stable. (in SemVer, versions communicate breakage, not stability).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Long-term Support&lt;/h3&gt; 
&lt;p&gt;Node has a &lt;a href="https://github.com/nodejs/Release#release-schedule"&gt;schedule&lt;/a&gt; for long-term support (LTS) You can reference LTS versions in aliases and &lt;code&gt;.nvmrc&lt;/code&gt; files with the notation &lt;code&gt;lts/*&lt;/code&gt; for the latest LTS, and &lt;code&gt;lts/argon&lt;/code&gt; for LTS releases from the "argon" line, for example. In addition, the following commands support LTS arguments:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;nvm install --lts&lt;/code&gt; / &lt;code&gt;nvm install --lts=argon&lt;/code&gt; / &lt;code&gt;nvm install 'lts/*'&lt;/code&gt; / &lt;code&gt;nvm install lts/argon&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;nvm uninstall --lts&lt;/code&gt; / &lt;code&gt;nvm uninstall --lts=argon&lt;/code&gt; / &lt;code&gt;nvm uninstall 'lts/*'&lt;/code&gt; / &lt;code&gt;nvm uninstall lts/argon&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;nvm use --lts&lt;/code&gt; / &lt;code&gt;nvm use --lts=argon&lt;/code&gt; / &lt;code&gt;nvm use 'lts/*'&lt;/code&gt; / &lt;code&gt;nvm use lts/argon&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;nvm exec --lts&lt;/code&gt; / &lt;code&gt;nvm exec --lts=argon&lt;/code&gt; / &lt;code&gt;nvm exec 'lts/*'&lt;/code&gt; / &lt;code&gt;nvm exec lts/argon&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;nvm run --lts&lt;/code&gt; / &lt;code&gt;nvm run --lts=argon&lt;/code&gt; / &lt;code&gt;nvm run 'lts/*'&lt;/code&gt; / &lt;code&gt;nvm run lts/argon&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;nvm ls-remote --lts&lt;/code&gt; / &lt;code&gt;nvm ls-remote --lts=argon&lt;/code&gt; &lt;code&gt;nvm ls-remote 'lts/*'&lt;/code&gt; / &lt;code&gt;nvm ls-remote lts/argon&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;nvm version-remote --lts&lt;/code&gt; / &lt;code&gt;nvm version-remote --lts=argon&lt;/code&gt; / &lt;code&gt;nvm version-remote 'lts/*'&lt;/code&gt; / &lt;code&gt;nvm version-remote lts/argon&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Any time your local copy of &lt;code&gt;nvm&lt;/code&gt; connects to &lt;a href="https://nodejs.org"&gt;https://nodejs.org&lt;/a&gt;, it will re-create the appropriate local aliases for all available LTS lines. These aliases (stored under &lt;code&gt;$NVM_DIR/alias/lts&lt;/code&gt;), are managed by &lt;code&gt;nvm&lt;/code&gt;, and you should not modify, remove, or create these files - expect your changes to be undone, and expect meddling with these files to cause bugs that will likely not be supported.&lt;/p&gt; 
&lt;p&gt;To get the latest LTS version of node and migrate your existing installed packages, use&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;nvm install --reinstall-packages-from=current 'lts/*'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Migrating Global Packages While Installing&lt;/h3&gt; 
&lt;p&gt;If you want to install a new version of Node.js and migrate npm packages from a previous version:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;nvm install --reinstall-packages-from=node node
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will first use "nvm version node" to identify the current version you're migrating packages from. Then it resolves the new version to install from the remote server and installs it. Lastly, it runs "nvm reinstall-packages" to reinstall the npm packages from your prior version of Node to the new one.&lt;/p&gt; 
&lt;p&gt;You can also install and migrate npm packages from specific versions of Node like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;nvm install --reinstall-packages-from=5 6
nvm install --reinstall-packages-from=iojs v4.2
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note that reinstalling packages &lt;em&gt;explicitly does not update the npm version&lt;/em&gt; — this is to ensure that npm isn't accidentally upgraded to a broken version for the new node version.&lt;/p&gt; 
&lt;p&gt;To update npm at the same time add the &lt;code&gt;--latest-npm&lt;/code&gt; flag, like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;nvm install --reinstall-packages-from=default --latest-npm 'lts/*'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or, you can at any time run the following command to get the latest supported npm version on the current node version:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;nvm install-latest-npm
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you've already gotten an error to the effect of "npm does not support Node.js", you'll need to (1) revert to a previous node version (&lt;code&gt;nvm ls&lt;/code&gt; &amp;amp; &lt;code&gt;nvm use &amp;lt;your latest _working_ version from the ls&amp;gt;&lt;/code&gt;), (2) delete the newly created node version (&lt;code&gt;nvm uninstall &amp;lt;your _broken_ version of node from the ls&amp;gt;&lt;/code&gt;), then (3) rerun your &lt;code&gt;nvm install&lt;/code&gt; with the &lt;code&gt;--latest-npm&lt;/code&gt; flag.&lt;/p&gt; 
&lt;h3&gt;Default Global Packages From File While Installing&lt;/h3&gt; 
&lt;p&gt;If you have a list of default packages you want installed every time you install a new version, we support that too -- just add the package names, one per line, to the file &lt;code&gt;$NVM_DIR/default-packages&lt;/code&gt;. You can add anything npm would accept as a package argument on the command line.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# $NVM_DIR/default-packages

rimraf
object-inspect@1.0.2
stevemao/left-pad
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;io.js&lt;/h3&gt; 
&lt;p&gt;If you want to install &lt;a href="https://github.com/iojs/io.js/"&gt;io.js&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;nvm install iojs
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you want to install a new version of io.js and migrate npm packages from a previous version:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;nvm install --reinstall-packages-from=iojs iojs
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The same guidelines mentioned for migrating npm packages in node are applicable to io.js.&lt;/p&gt; 
&lt;h3&gt;System Version of Node&lt;/h3&gt; 
&lt;p&gt;If you want to use the system-installed version of node, you can use the special default alias "system":&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;nvm use system
nvm run system --version
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Listing Versions&lt;/h3&gt; 
&lt;p&gt;If you want to see what versions are installed:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;nvm ls
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you want to see what versions are available to install:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;nvm ls-remote
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Setting Custom Colors&lt;/h3&gt; 
&lt;p&gt;You can set five colors that will be used to display version and alias information. These colors replace the default colors. Initial colors are: g b y r e&lt;/p&gt; 
&lt;p&gt;Color codes:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;r/R = red / bold red

g/G = green / bold green

b/B = blue / bold blue

c/C = cyan / bold cyan

m/M = magenta / bold magenta

y/Y = yellow / bold yellow

k/K = black / bold black

e/W = light grey / white
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;nvm set-colors rgBcm
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Persisting custom colors&lt;/h4&gt; 
&lt;p&gt;If you want the custom colors to persist after terminating the shell, export the &lt;code&gt;NVM_COLORS&lt;/code&gt; variable in your shell profile. For example, if you want to use cyan, magenta, green, bold red and bold yellow, add the following line:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;export NVM_COLORS='cmgRY'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Suppressing colorized output&lt;/h4&gt; 
&lt;p&gt;&lt;code&gt;nvm help (or -h or --help)&lt;/code&gt;, &lt;code&gt;nvm ls&lt;/code&gt;, &lt;code&gt;nvm ls-remote&lt;/code&gt; and &lt;code&gt;nvm alias&lt;/code&gt; usually produce colorized output. You can disable colors with the &lt;code&gt;--no-colors&lt;/code&gt; option (or by setting the environment variable &lt;code&gt;TERM=dumb&lt;/code&gt;):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;nvm ls --no-colors
nvm help --no-colors
TERM=dumb nvm ls
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Restoring PATH&lt;/h3&gt; 
&lt;p&gt;To restore your PATH, you can deactivate it:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;nvm deactivate
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Set default node version&lt;/h3&gt; 
&lt;p&gt;To set a default Node version to be used in any new shell, use the alias 'default':&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;nvm alias default node # this refers to the latest installed version of node
nvm alias default 18 # this refers to the latest installed v18.x version of node
nvm alias default 18.12  # this refers to the latest installed v18.12.x version of node
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Use a mirror of node binaries&lt;/h3&gt; 
&lt;p&gt;To use a mirror of the node binaries, set &lt;code&gt;$NVM_NODEJS_ORG_MIRROR&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;export NVM_NODEJS_ORG_MIRROR=https://nodejs.org/dist
nvm install node

NVM_NODEJS_ORG_MIRROR=https://nodejs.org/dist nvm install 4.2
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To use a mirror of the io.js binaries, set &lt;code&gt;$NVM_IOJS_ORG_MIRROR&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;export NVM_IOJS_ORG_MIRROR=https://iojs.org/dist
nvm install iojs-v1.0.3

NVM_IOJS_ORG_MIRROR=https://iojs.org/dist nvm install iojs-v1.0.3
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;code&gt;nvm use&lt;/code&gt; will not, by default, create a "current" symlink. Set &lt;code&gt;$NVM_SYMLINK_CURRENT&lt;/code&gt; to "true" to enable this behavior, which is sometimes useful for IDEs. Note that using &lt;code&gt;nvm&lt;/code&gt; in multiple shell tabs with this environment variable enabled can cause race conditions.&lt;/p&gt; 
&lt;h4&gt;Pass Authorization header to mirror&lt;/h4&gt; 
&lt;p&gt;To pass an Authorization header through to the mirror url, set &lt;code&gt;$NVM_AUTH_HEADER&lt;/code&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;NVM_AUTH_HEADER="Bearer secret-token" nvm install node
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;.nvmrc&lt;/h3&gt; 
&lt;p&gt;You can create a &lt;code&gt;.nvmrc&lt;/code&gt; file containing a node version number (or any other string that &lt;code&gt;nvm&lt;/code&gt; understands; see &lt;code&gt;nvm --help&lt;/code&gt; for details) in the project root directory (or any parent directory). Afterwards, &lt;code&gt;nvm use&lt;/code&gt;, &lt;code&gt;nvm install&lt;/code&gt;, &lt;code&gt;nvm exec&lt;/code&gt;, &lt;code&gt;nvm run&lt;/code&gt;, and &lt;code&gt;nvm which&lt;/code&gt; will use the version specified in the &lt;code&gt;.nvmrc&lt;/code&gt; file if no version is supplied on the command line.&lt;/p&gt; 
&lt;p&gt;For example, to make nvm default to the latest 5.9 release, the latest LTS version, or the latest node version for the current directory:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;$ echo "5.9" &amp;gt; .nvmrc

$ echo "lts/*" &amp;gt; .nvmrc # to default to the latest LTS version

$ echo "node" &amp;gt; .nvmrc # to default to the latest version
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;[NB these examples assume a POSIX-compliant shell version of &lt;code&gt;echo&lt;/code&gt;. If you use a Windows &lt;code&gt;cmd&lt;/code&gt; development environment, eg the &lt;code&gt;.nvmrc&lt;/code&gt; file is used to configure a remote Linux deployment, then keep in mind the &lt;code&gt;"&lt;/code&gt;s will be copied leading to an invalid file. Remove them.]&lt;/p&gt; 
&lt;p&gt;Then when you run nvm use:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;$ nvm use
Found '/path/to/project/.nvmrc' with version &amp;lt;5.9&amp;gt;
Now using node v5.9.1 (npm v3.7.3)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Running nvm install will also switch over to the correct version, but if the correct node version isn't already installed, it will install it for you.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;$ nvm install
Found '/path/to/project/.nvmrc' with version &amp;lt;5.9&amp;gt;
Downloading and installing node v5.9.1...
Downloading https://nodejs.org/dist/v5.9.1/node-v5.9.1-linux-x64.tar.xz...
#################################################################################### 100.0%
Computing checksum with sha256sum
Checksums matched!
Now using node v5.9.1 (npm v3.7.3)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;code&gt;nvm use&lt;/code&gt; et. al. will traverse directory structure upwards from the current directory looking for the &lt;code&gt;.nvmrc&lt;/code&gt; file. In other words, running &lt;code&gt;nvm use&lt;/code&gt; et. al. in any subdirectory of a directory with an &lt;code&gt;.nvmrc&lt;/code&gt; will result in that &lt;code&gt;.nvmrc&lt;/code&gt; being utilized.&lt;/p&gt; 
&lt;p&gt;The contents of a &lt;code&gt;.nvmrc&lt;/code&gt; file &lt;strong&gt;must&lt;/strong&gt; contain precisely one &lt;code&gt;&amp;lt;version&amp;gt;&lt;/code&gt; (as described by &lt;code&gt;nvm --help&lt;/code&gt;) followed by a newline. &lt;code&gt;.nvmrc&lt;/code&gt; files may also have comments. The comment delimiter is &lt;code&gt;#&lt;/code&gt;, and it and any text after it, as well as blank lines, and leading and trailing white space, will be ignored when parsing.&lt;/p&gt; 
&lt;p&gt;Key/value pairs using &lt;code&gt;=&lt;/code&gt; are also allowed and ignored, but are reserved for future use, and may cause validation errors in the future.&lt;/p&gt; 
&lt;p&gt;Run &lt;a href="https://npmjs.com/nvmrc"&gt;&lt;code&gt;npx nvmrc&lt;/code&gt;&lt;/a&gt; to validate an &lt;code&gt;.nvmrc&lt;/code&gt; file. If that tool’s results do not agree with nvm, one or the other has a bug - please file an issue.&lt;/p&gt; 
&lt;h3&gt;Deeper Shell Integration&lt;/h3&gt; 
&lt;p&gt;You can use &lt;a href="https://github.com/iamogbz/nvshim"&gt;&lt;code&gt;nvshim&lt;/code&gt;&lt;/a&gt; to shim the &lt;code&gt;node&lt;/code&gt;, &lt;code&gt;npm&lt;/code&gt;, and &lt;code&gt;npx&lt;/code&gt; bins to automatically use the &lt;code&gt;nvm&lt;/code&gt; config in the current directory. &lt;code&gt;nvshim&lt;/code&gt; is &lt;strong&gt;not&lt;/strong&gt; supported by the &lt;code&gt;nvm&lt;/code&gt; maintainers. Please &lt;a href="https://github.com/iamogbz/nvshim/issues/new"&gt;report issues to the &lt;code&gt;nvshim&lt;/code&gt; team&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you prefer a lighter-weight solution, the recipes below have been contributed by &lt;code&gt;nvm&lt;/code&gt; users. They are &lt;strong&gt;not&lt;/strong&gt; supported by the &lt;code&gt;nvm&lt;/code&gt; maintainers. We are, however, accepting pull requests for more examples.&lt;/p&gt; 
&lt;h4&gt;Calling &lt;code&gt;nvm use&lt;/code&gt; automatically in a directory with a &lt;code&gt;.nvmrc&lt;/code&gt; file&lt;/h4&gt; 
&lt;p&gt;In your profile (&lt;code&gt;~/.bash_profile&lt;/code&gt;, &lt;code&gt;~/.zshrc&lt;/code&gt;, &lt;code&gt;~/.profile&lt;/code&gt;, or &lt;code&gt;~/.bashrc&lt;/code&gt;), add the following to &lt;code&gt;nvm use&lt;/code&gt; whenever you enter a new directory:&lt;/p&gt; 
&lt;h5&gt;bash&lt;/h5&gt; 
&lt;p&gt;Put the following at the end of your &lt;code&gt;$HOME/.bashrc&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cdnvm() {
    command cd "$@" || return $?
    nvm_path="$(nvm_find_up .nvmrc | command tr -d '\n')"

    # If there are no .nvmrc file, use the default nvm version
    if [[ ! $nvm_path = *[^[:space:]]* ]]; then

        declare default_version
        default_version="$(nvm version default)"

        # If there is no default version, set it to `node`
        # This will use the latest version on your machine
        if [ $default_version = 'N/A' ]; then
            nvm alias default node
            default_version=$(nvm version default)
        fi

        # If the current version is not the default version, set it to use the default version
        if [ "$(nvm current)" != "${default_version}" ]; then
            nvm use default
        fi
    elif [[ -s "${nvm_path}/.nvmrc" &amp;amp;&amp;amp; -r "${nvm_path}/.nvmrc" ]]; then
        declare nvm_version
        nvm_version=$(&amp;lt;"${nvm_path}"/.nvmrc)

        declare locally_resolved_nvm_version
        # `nvm ls` will check all locally-available versions
        # If there are multiple matching versions, take the latest one
        # Remove the `-&amp;gt;` and `*` characters and spaces
        # `locally_resolved_nvm_version` will be `N/A` if no local versions are found
        locally_resolved_nvm_version=$(nvm ls --no-colors "${nvm_version}" | command tail -1 | command tr -d '\-&amp;gt;*' | command tr -d '[:space:]')

        # If it is not already installed, install it
        # `nvm install` will implicitly use the newly-installed version
        if [ "${locally_resolved_nvm_version}" = 'N/A' ]; then
            nvm install "${nvm_version}";
        elif [ "$(nvm current)" != "${locally_resolved_nvm_version}" ]; then
            nvm use "${nvm_version}";
        fi
    fi
}

alias cd='cdnvm'
cdnvm "$PWD" || exit
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This alias would search 'up' from your current directory in order to detect a &lt;code&gt;.nvmrc&lt;/code&gt; file. If it finds it, it will switch to that version; if not, it will use the default version.&lt;/p&gt; 
&lt;h5&gt;zsh&lt;/h5&gt; 
&lt;p&gt;This shell function will install (if needed) and &lt;code&gt;nvm use&lt;/code&gt; the specified Node version when an &lt;code&gt;.nvmrc&lt;/code&gt; is found, and &lt;code&gt;nvm use default&lt;/code&gt; otherwise.&lt;/p&gt; 
&lt;p&gt;Put this into your &lt;code&gt;$HOME/.zshrc&lt;/code&gt; to call &lt;code&gt;nvm use&lt;/code&gt; automatically whenever you enter a directory that contains an &lt;code&gt;.nvmrc&lt;/code&gt; file with a string telling nvm which node to &lt;code&gt;use&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-zsh"&gt;# place this after nvm initialization!
autoload -U add-zsh-hook

load-nvmrc() {
  local nvmrc_path
  nvmrc_path="$(nvm_find_nvmrc)"

  if [ -n "$nvmrc_path" ]; then
    local nvmrc_node_version
    nvmrc_node_version=$(nvm version "$(cat "${nvmrc_path}")")

    if [ "$nvmrc_node_version" = "N/A" ]; then
      nvm install
    elif [ "$nvmrc_node_version" != "$(nvm version)" ]; then
      nvm use
    fi
  elif [ -n "$(PWD=$OLDPWD nvm_find_nvmrc)" ] &amp;amp;&amp;amp; [ "$(nvm version)" != "$(nvm version default)" ]; then
    echo "Reverting to nvm default version"
    nvm use default
  fi
}

add-zsh-hook chpwd load-nvmrc
load-nvmrc
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After saving the file, run &lt;code&gt;source ~/.zshrc&lt;/code&gt; to reload the configuration with the latest changes made.&lt;/p&gt; 
&lt;h5&gt;fish&lt;/h5&gt; 
&lt;p&gt;This requires that you have &lt;a href="https://github.com/edc/bass"&gt;bass&lt;/a&gt; installed.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-fish"&gt;# ~/.config/fish/functions/nvm.fish
function nvm
  bass source ~/.nvm/nvm.sh --no-use ';' nvm $argv
end

# ~/.config/fish/functions/nvm_find_nvmrc.fish
function nvm_find_nvmrc
  bass source ~/.nvm/nvm.sh --no-use ';' nvm_find_nvmrc
end

# ~/.config/fish/functions/load_nvm.fish
function load_nvm --on-variable="PWD"
  set -l default_node_version (nvm version default)
  set -l node_version (nvm version)
  set -l nvmrc_path (nvm_find_nvmrc)
  if test -n "$nvmrc_path"
    set -l nvmrc_node_version (nvm version (cat $nvmrc_path))
    if test "$nvmrc_node_version" = "N/A"
      nvm install (cat $nvmrc_path)
    else if test "$nvmrc_node_version" != "$node_version"
      nvm use $nvmrc_node_version
    end
  else if test "$node_version" != "$default_node_version"
    echo "Reverting to default Node version"
    nvm use default
  end
end

# ~/.config/fish/config.fish
# You must call it on initialization or listening to directory switching won't work
load_nvm &amp;gt; /dev/stderr
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Running Tests&lt;/h2&gt; 
&lt;p&gt;Tests are written in &lt;a href="https://git.sdf.org/tlevine/urchin"&gt;Urchin&lt;/a&gt;. Install Urchin (and other dependencies) like so:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;npm install
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;There are slow tests and fast tests. The slow tests do things like install node and check that the right versions are used. The fast tests fake this to test things like aliases and uninstalling. From the root of the nvm git repository, run the fast tests like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;npm run test/fast
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Run the slow tests like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;npm run test/slow
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Run all of the tests like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;npm test
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Nota bene: Avoid running nvm while the tests are running.&lt;/p&gt; 
&lt;h2&gt;Environment variables&lt;/h2&gt; 
&lt;p&gt;nvm exposes the following environment variables:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;NVM_DIR&lt;/code&gt; - nvm's installation directory.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;NVM_BIN&lt;/code&gt; - where node, npm, and global packages for the active version of node are installed.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;NVM_INC&lt;/code&gt; - node's include file directory (useful for building C/C++ addons for node).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;NVM_CD_FLAGS&lt;/code&gt; - used to maintain compatibility with zsh.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;NVM_RC_VERSION&lt;/code&gt; - version from .nvmrc file if being used.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Additionally, nvm modifies &lt;code&gt;PATH&lt;/code&gt;, and, if present, &lt;code&gt;MANPATH&lt;/code&gt; and &lt;code&gt;NODE_PATH&lt;/code&gt; when changing versions.&lt;/p&gt; 
&lt;h2&gt;Bash Completion&lt;/h2&gt; 
&lt;p&gt;To activate, you need to source &lt;code&gt;bash_completion&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;[[ -r $NVM_DIR/bash_completion ]] &amp;amp;&amp;amp; \. $NVM_DIR/bash_completion
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Put the above sourcing line just below the sourcing line for nvm in your profile (&lt;code&gt;.bashrc&lt;/code&gt;, &lt;code&gt;.bash_profile&lt;/code&gt;).&lt;/p&gt; 
&lt;h3&gt;Usage&lt;/h3&gt; 
&lt;p&gt;nvm:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;code&gt;$ nvm&lt;/code&gt; &lt;kbd&gt;Tab&lt;/kbd&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;alias               deactivate          install             list-remote         reinstall-packages  uninstall           version
cache               exec                install-latest-npm  ls                  run                 unload              version-remote
current             help                list                ls-remote           unalias             use                 which
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;nvm alias:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;code&gt;$ nvm alias&lt;/code&gt; &lt;kbd&gt;Tab&lt;/kbd&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;default      iojs         lts/*        lts/argon    lts/boron    lts/carbon   lts/dubnium  lts/erbium   node         stable       unstable
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;code&gt;$ nvm alias my_alias&lt;/code&gt; &lt;kbd&gt;Tab&lt;/kbd&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;v10.22.0       v12.18.3      v14.8.0
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;nvm use:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;code&gt;$ nvm use&lt;/code&gt; &lt;kbd&gt;Tab&lt;/kbd&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code&gt;my_alias        default        v10.22.0       v12.18.3      v14.8.0
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;nvm uninstall:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;code&gt;$ nvm uninstall&lt;/code&gt; &lt;kbd&gt;Tab&lt;/kbd&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code&gt;my_alias        default        v10.22.0       v12.18.3      v14.8.0
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Compatibility Issues&lt;/h2&gt; 
&lt;p&gt;&lt;code&gt;nvm&lt;/code&gt; will encounter some issues if you have some non-default settings set. (see &lt;a href="https://github.com/nvm-sh/nvm/issues/606"&gt;#606&lt;/a&gt;) The following are known to cause issues:&lt;/p&gt; 
&lt;p&gt;Inside &lt;code&gt;~/.npmrc&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;prefix='some/path'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Environment Variables:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;$NPM_CONFIG_PREFIX
$PREFIX
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Shell settings:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;set -e
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Installing nvm on Alpine Linux&lt;/h2&gt; 
&lt;p&gt;In order to provide the best performance (and other optimizations), nvm will download and install pre-compiled binaries for Node (and npm) when you run &lt;code&gt;nvm install X&lt;/code&gt;. The Node project compiles, tests and hosts/provides these pre-compiled binaries which are built for mainstream/traditional Linux distributions (such as Debian, Ubuntu, CentOS, RedHat et al).&lt;/p&gt; 
&lt;p&gt;Alpine Linux, unlike mainstream/traditional Linux distributions, is based on &lt;a href="https://www.busybox.net/"&gt;BusyBox&lt;/a&gt;, a very compact (~5MB) Linux distribution. BusyBox (and thus Alpine Linux) uses a different C/C++ stack to most mainstream/traditional Linux distributions - &lt;a href="https://www.musl-libc.org/"&gt;musl&lt;/a&gt;. This makes binary programs built for such mainstream/traditional incompatible with Alpine Linux, thus we cannot simply &lt;code&gt;nvm install X&lt;/code&gt; on Alpine Linux and expect the downloaded binary to run correctly - you'll likely see "...does not exist" errors if you try that.&lt;/p&gt; 
&lt;p&gt;There is a &lt;code&gt;-s&lt;/code&gt; flag for &lt;code&gt;nvm install&lt;/code&gt; which requests nvm download Node source and compile it locally.&lt;/p&gt; 
&lt;p&gt;If installing nvm on Alpine Linux &lt;em&gt;is&lt;/em&gt; still what you want or need to do, you should be able to achieve this by running the following from you Alpine Linux shell, depending on which version you are using:&lt;/p&gt; 
&lt;h3&gt;Alpine Linux 3.13+&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;apk add -U curl bash ca-certificates openssl ncurses coreutils python3 make gcc g++ libgcc linux-headers grep util-linux binutils findutils
curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.3/install.sh | bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Alpine Linux 3.5 - 3.12&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;apk add -U curl bash ca-certificates openssl ncurses coreutils python2 make gcc g++ libgcc linux-headers grep util-linux binutils findutils
curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.3/install.sh | bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Note: Alpine 3.5 can only install NodeJS versions up to v6.9.5, Alpine 3.6 can only install versions up to v6.10.3, Alpine 3.7 installs versions up to v8.9.3, Alpine 3.8 installs versions up to v8.14.0, Alpine 3.9 installs versions up to v10.19.0, Alpine 3.10 installs versions up to v10.24.1, Alpine 3.11 installs versions up to v12.22.6, Alpine 3.12 installs versions up to v12.22.12, Alpine 3.13 &amp;amp; 3.14 install versions up to v14.20.0, Alpine 3.15 &amp;amp; 3.16 install versions up to v16.16.0 (&lt;strong&gt;These are all versions on the main branch&lt;/strong&gt;). Alpine 3.5 - 3.12 required the package &lt;code&gt;python2&lt;/code&gt; to build NodeJS, as they are older versions to build. Alpine 3.13+ requires &lt;code&gt;python3&lt;/code&gt; to successfully build newer NodeJS versions, but you can use &lt;code&gt;python2&lt;/code&gt; with Alpine 3.13+ if you need to build versions of node supported in Alpine 3.5 - 3.15, you just need to specify what version of NodeJS you need to install in the package install script.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;The Node project has some desire but no concrete plans (due to the overheads of building, testing and support) to offer Alpine-compatible binaries.&lt;/p&gt; 
&lt;p&gt;As a potential alternative, @mhart (a Node contributor) has some &lt;a href="https://github.com/mhart/alpine-node"&gt;Docker images for Alpine Linux with Node and optionally, npm, pre-installed&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a id="removal"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Uninstalling / Removal&lt;/h2&gt; 
&lt;h3&gt;Manual Uninstall&lt;/h3&gt; 
&lt;p&gt;To remove &lt;code&gt;nvm&lt;/code&gt; manually, execute the following:&lt;/p&gt; 
&lt;p&gt;First, use &lt;code&gt;nvm unload&lt;/code&gt; to remove the nvm command from your terminal session and delete the installation directory:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;$ nvm_dir="${NVM_DIR:-~/.nvm}"
$ nvm unload
$ rm -rf "$nvm_dir"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Edit &lt;code&gt;~/.bashrc&lt;/code&gt; (or other shell resource config) and remove the lines below:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;export NVM_DIR="$HOME/.nvm"
[ -s "$NVM_DIR/nvm.sh" ] &amp;amp;&amp;amp; \. "$NVM_DIR/nvm.sh" # This loads nvm
[[ -r $NVM_DIR/bash_completion ]] &amp;amp;&amp;amp; \. $NVM_DIR/bash_completion
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Docker For Development Environment&lt;/h2&gt; 
&lt;p&gt;To make the development and testing work easier, we have a Dockerfile for development usage, which is based on Ubuntu 18.04 base image, prepared with essential and useful tools for &lt;code&gt;nvm&lt;/code&gt; development, to build the docker image of the environment, run the docker command at the root of &lt;code&gt;nvm&lt;/code&gt; repository:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;$ docker build -t nvm-dev .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will package your current nvm repository with our pre-defined development environment into a docker image named &lt;code&gt;nvm-dev&lt;/code&gt;, once it's built with success, validate your image via &lt;code&gt;docker images&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;$ docker images

REPOSITORY         TAG                 IMAGE ID            CREATED             SIZE
nvm-dev            latest              9ca4c57a97d8        7 days ago          650 MB
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you got no error message, now you can easily involve in:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;$ docker run -h nvm-dev -it nvm-dev

nvm@nvm-dev:~/.nvm$
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Please note that it'll take about 8 minutes to build the image and the image size would be about 650MB, so it's not suitable for production usage.&lt;/p&gt; 
&lt;p&gt;For more information and documentation about docker, please refer to its official website:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.docker.com/"&gt;https://www.docker.com/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.docker.com/"&gt;https://docs.docker.com/&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Problems&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;If you try to install a node version and the installation fails, be sure to run &lt;code&gt;nvm cache clear&lt;/code&gt; to delete cached node downloads, or you might get an error like the following:&lt;/p&gt; &lt;p&gt;curl: (33) HTTP server doesn't seem to support byte ranges. Cannot resume.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Where's my &lt;code&gt;sudo node&lt;/code&gt;? Check out &lt;a href="https://github.com/nvm-sh/nvm/issues/43"&gt;#43&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;After the v0.8.6 release of node, nvm tries to install from binary packages. But in some systems, the official binary packages don't work due to incompatibility of shared libs. In such cases, use &lt;code&gt;-s&lt;/code&gt; option to force install from source:&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;nvm install -s 0.8.6
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;If setting the &lt;code&gt;default&lt;/code&gt; alias does not establish the node version in new shells (i.e. &lt;code&gt;nvm current&lt;/code&gt; yields &lt;code&gt;system&lt;/code&gt;), ensure that the system's node &lt;code&gt;PATH&lt;/code&gt; is set before the &lt;code&gt;nvm.sh&lt;/code&gt; source line in your shell profile (see &lt;a href="https://github.com/nvm-sh/nvm/issues/658"&gt;#658&lt;/a&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;macOS Troubleshooting&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;nvm node version not found in vim shell&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;If you set node version to a version other than your system node version &lt;code&gt;nvm use 6.2.1&lt;/code&gt; and open vim and run &lt;code&gt;:!node -v&lt;/code&gt; you should see &lt;code&gt;v6.2.1&lt;/code&gt; if you see your system version &lt;code&gt;v0.12.7&lt;/code&gt;. You need to run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;sudo chmod ugo-x /usr/libexec/path_helper
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;More on this issue in &lt;a href="https://github.com/dotphiles/dotzsh#mac-os-x"&gt;dotphiles/dotzsh&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;nvm is not compatible with the npm config "prefix" option&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Some solutions for this issue can be found &lt;a href="https://github.com/nvm-sh/nvm/issues/1245"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;There is one more edge case causing this issue, and that's a &lt;strong&gt;mismatch between the &lt;code&gt;$HOME&lt;/code&gt; path and the user's home directory's actual name&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;You have to make sure that the user directory name in &lt;code&gt;$HOME&lt;/code&gt; and the user directory name you'd see from running &lt;code&gt;ls /Users/&lt;/code&gt; &lt;strong&gt;are capitalized the same way&lt;/strong&gt; (&lt;a href="https://github.com/nvm-sh/nvm/issues/2261"&gt;See this issue&lt;/a&gt;).&lt;/p&gt; 
&lt;p&gt;To change the user directory and/or account name follow the instructions &lt;a href="https://support.apple.com/en-us/HT201548"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Homebrew makes zsh directories unsecure&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;zsh compinit: insecure directories, run compaudit for list.
Ignore insecure directories and continue [y] or abort compinit [n]? y
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Homebrew causes insecure directories like &lt;code&gt;/usr/local/share/zsh/site-functions&lt;/code&gt; and &lt;code&gt;/usr/local/share/zsh&lt;/code&gt;. This is &lt;strong&gt;not&lt;/strong&gt; an &lt;code&gt;nvm&lt;/code&gt; problem - it is a homebrew problem. Refer &lt;a href="https://github.com/zsh-users/zsh-completions/issues/680"&gt;here&lt;/a&gt; for some solutions related to the issue.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Macs with Apple Silicon chips&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Experimental support for the Apple Silicon chip architecture was added in node.js v15.3 and full support was added in v16.0. Because of this, if you try to install older versions of node as usual, you will probably experience either compilation errors when installing node or out-of-memory errors while running your code.&lt;/p&gt; 
&lt;p&gt;So, if you want to run a version prior to v16.0 on an Apple Silicon Mac, it may be best to compile node targeting the &lt;code&gt;x86_64&lt;/code&gt; Intel architecture so that Rosetta 2 can translate the &lt;code&gt;x86_64&lt;/code&gt; processor instructions to ARM-based Apple Silicon instructions. Here's what you will need to do:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Install Rosetta, if you haven't already done so&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;$ softwareupdate --install-rosetta
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You might wonder, "how will my Apple Silicon Mac know to use Rosetta for a version of node compiled for an Intel chip?". If an executable contains only Intel instructions, macOS will automatically use Rosetta to translate the instructions.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Open a shell that's running using Rosetta&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;$ arch -x86_64 zsh
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note: This same thing can also be accomplished by finding the Terminal or iTerm App in Finder, right clicking, selecting "Get Info", and then checking the box labeled "Open using Rosetta".&lt;/p&gt; &lt;p&gt;Note: This terminal session is now running in &lt;code&gt;zsh&lt;/code&gt;. If &lt;code&gt;zsh&lt;/code&gt; is not the shell you typically use, &lt;code&gt;nvm&lt;/code&gt; may not be &lt;code&gt;source&lt;/code&gt;'d automatically like it probably is for your usual shell through your dotfiles. If that's the case, make sure to source &lt;code&gt;nvm&lt;/code&gt;.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;$ source "${NVM_DIR}/nvm.sh"
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install whatever older version of node you are interested in. Let's use 12.22.1 as an example. This will fetch the node source code and compile it, which will take several minutes.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;$ nvm install v12.22.1 --shared-zlib
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note: You're probably curious why &lt;code&gt;--shared-zlib&lt;/code&gt; is included. There's a bug in recent versions of Apple's system &lt;code&gt;clang&lt;/code&gt; compiler. If one of these broken versions is installed on your system, the above step will likely still succeed even if you didn't include the &lt;code&gt;--shared-zlib&lt;/code&gt; flag. However, later, when you attempt to &lt;code&gt;npm install&lt;/code&gt; something using your old version of node.js, you will see &lt;code&gt;incorrect data check&lt;/code&gt; errors. If you want to avoid the possible hassle of dealing with this, include that flag. For more details, see &lt;a href="https://github.com/nodejs/node/issues/39313"&gt;this issue&lt;/a&gt; and &lt;a href="https://github.com/nodejs/node/issues/39313#issuecomment-90.40.376"&gt;this comment&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Exit back to your native shell.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;$ exit
$ arch
arm64
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note: If you selected the box labeled "Open using Rosetta" rather than running the CLI command in the second step, you will see &lt;code&gt;i386&lt;/code&gt; here. Unless you have another reason to have that box selected, you can deselect it now.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Check to make sure the architecture is correct. &lt;code&gt;x64&lt;/code&gt; is the abbreviation for &lt;code&gt;x86_64&lt;/code&gt;, which is what you want to see.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;$ node -p process.arch
x64
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Now you should be able to use node as usual.&lt;/p&gt; 
&lt;h2&gt;WSL Troubleshooting&lt;/h2&gt; 
&lt;p&gt;If you've encountered this error on WSL-2:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.3/install.sh | bash
% Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                Dload  Upload  Total   Spent    Left  Speed
0     0    0     0    0     0      0      0 --:--:--  0:00:09 --:--:--     0curl: (6) Could not resolve host: raw.githubusercontent.com
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;It may be due to your antivirus, VPN, or other reasons.&lt;/p&gt; 
&lt;p&gt;Where you can &lt;code&gt;ping 8.8.8.8&lt;/code&gt; while you can't &lt;code&gt;ping google.com&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;This could simply be solved by running this in your root directory:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;sudo rm /etc/resolv.conf
sudo bash -c 'echo "nameserver 8.8.8.8" &amp;gt; /etc/resolv.conf'
sudo bash -c 'echo "[network]" &amp;gt; /etc/wsl.conf'
sudo bash -c 'echo "generateResolvConf = false" &amp;gt;&amp;gt; /etc/wsl.conf'
sudo chattr +i /etc/resolv.conf
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This deletes your &lt;code&gt;resolv.conf&lt;/code&gt; file that is automatically generated when you run WSL, creates a new file and puts &lt;code&gt;nameserver 8.8.8.8&lt;/code&gt;, then creates a &lt;code&gt;wsl.conf&lt;/code&gt; file and adds &lt;code&gt;[network]&lt;/code&gt; and &lt;code&gt;generateResolveConf = false&lt;/code&gt; to prevent auto-generation of that file.&lt;/p&gt; 
&lt;p&gt;You can check the contents of the file by running:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;cat /etc/resolv.conf
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Maintainers&lt;/h2&gt; 
&lt;p&gt;Currently, the sole maintainer is &lt;a href="https://github.com/ljharb"&gt;@ljharb&lt;/a&gt; - more maintainers are quite welcome, and we hope to add folks to the team over time. &lt;a href="https://raw.githubusercontent.com/nvm-sh/nvm/master/GOVERNANCE.md"&gt;Governance&lt;/a&gt; will be re-evaluated as the project evolves.&lt;/p&gt; 
&lt;h2&gt;Project Support&lt;/h2&gt; 
&lt;p&gt;Only the latest version (v0.40.3 at this time) is supported.&lt;/p&gt; 
&lt;h2&gt;Enterprise Support&lt;/h2&gt; 
&lt;p&gt;If you are unable to update to the latest version of &lt;code&gt;nvm&lt;/code&gt;, our &lt;a href="https://openjsf.org/ecosystem-sustainability-program"&gt;partners&lt;/a&gt; provide commercial security fixes for all unsupported versions:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.herodevs.com/support?utm_source=OpenJS&amp;amp;utm_medium=Link&amp;amp;utm_campaign=nvm_openjs"&gt;HeroDevs Never-Ending Support&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/nvm-sh/nvm/master/LICENSE.md"&gt;LICENSE.md&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Copyright notice&lt;/h2&gt; 
&lt;p&gt;Copyright &lt;a href="https://openjsf.org"&gt;OpenJS Foundation&lt;/a&gt; and &lt;code&gt;nvm&lt;/code&gt; contributors. All rights reserved. The &lt;a href="https://openjsf.org"&gt;OpenJS Foundation&lt;/a&gt; has registered trademarks and uses trademarks. For a list of trademarks of the &lt;a href="https://openjsf.org"&gt;OpenJS Foundation&lt;/a&gt;, please see our &lt;a href="https://trademark-policy.openjsf.org/"&gt;Trademark Policy&lt;/a&gt; and &lt;a href="https://trademark-list.openjsf.org/"&gt;Trademark List&lt;/a&gt;. Trademarks and logos not indicated on the &lt;a href="https://trademark-list.openjsf.org"&gt;list of OpenJS Foundation trademarks&lt;/a&gt; are trademarks™ or registered® trademarks of their respective holders. Use of them does not imply any affiliation with or endorsement by them. &lt;a href="https://openjsf.org/"&gt;The OpenJS Foundation&lt;/a&gt; | &lt;a href="https://terms-of-use.openjsf.org/"&gt;Terms of Use&lt;/a&gt; | &lt;a href="https://privacy-policy.openjsf.org/"&gt;Privacy Policy&lt;/a&gt; | &lt;a href="https://bylaws.openjsf.org/"&gt;Bylaws&lt;/a&gt; | &lt;a href="https://code-of-conduct.openjsf.org"&gt;Code of Conduct&lt;/a&gt; | &lt;a href="https://trademark-policy.openjsf.org/"&gt;Trademark Policy&lt;/a&gt; | &lt;a href="https://trademark-list.openjsf.org/"&gt;Trademark List&lt;/a&gt; | &lt;a href="https://www.linuxfoundation.org/cookies/"&gt;Cookie Policy&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>