<rss version="2.0">
  <channel>
    <title>GitHub Python Weekly Trending</title>
    <description>Weekly Trending of Python in GitHub</description>
    <pubDate>Tue, 19 Aug 2025 01:43:18 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>lfnovo/open-notebook</title>
      <link>https://github.com/lfnovo/open-notebook</link>
      <description>&lt;p&gt;An Open Source implementation of Notebook LM with more flexibility and features&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a id="readme-top"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;!-- [![Contributors][contributors-shield]][contributors-url] --&gt; 
&lt;p&gt;&lt;a href="https://github.com/lfnovo/open-notebook/network/members"&gt;&lt;img src="https://img.shields.io/github/forks/lfnovo/open-notebook.svg?style=for-the-badge" alt="Forks" /&gt;&lt;/a&gt; &lt;a href="https://github.com/lfnovo/open-notebook/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/lfnovo/open-notebook.svg?style=for-the-badge" alt="Stargazers" /&gt;&lt;/a&gt; &lt;a href="https://github.com/lfnovo/open-notebook/issues"&gt;&lt;img src="https://img.shields.io/github/issues/lfnovo/open-notebook.svg?style=for-the-badge" alt="Issues" /&gt;&lt;/a&gt; &lt;a href="https://github.com/lfnovo/open-notebook/raw/master/LICENSE.txt"&gt;&lt;img src="https://img.shields.io/github/license/lfnovo/open-notebook.svg?style=for-the-badge" alt="MIT License" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;!-- [![LinkedIn][linkedin-shield]][linkedin-url] --&gt; 
&lt;!-- PROJECT LOGO --&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/lfnovo/open-notebook"&gt; &lt;img src="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/assets/hero.svg?sanitize=true" alt="Logo" /&gt; &lt;/a&gt; 
 &lt;h3 align="center"&gt;Open Notebook&lt;/h3&gt; 
 &lt;p align="center"&gt; An open source, privacy-focused alternative to Google's Notebook LM! &lt;br /&gt;&lt;strong&gt;Join our &lt;a href="https://discord.gg/37XJPXfz2w"&gt;Discord server&lt;/a&gt; for help, to share workflow ideas, and suggest features!&lt;/strong&gt; &lt;br /&gt; &lt;a href="https://www.open-notebook.ai"&gt;&lt;strong&gt;Checkout our website »&lt;/strong&gt;&lt;/a&gt; &lt;br /&gt; &lt;br /&gt; &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/index.md"&gt;📚 Get Started&lt;/a&gt; · &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/index.md"&gt;📖 User Guide&lt;/a&gt; · &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/features/index.md"&gt;✨ Features&lt;/a&gt; · &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/deployment/index.md"&gt;🚀 Deploy&lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;📢 Open Notebook is under very active development&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Open Notebook is under active development! We're moving fast and making improvements every week. Your feedback is incredibly valuable to me during this exciting phase and it gives me motivation to keep improving and building this amazing tool. Please feel free to star the project if you find it useful, and don't hesitate to reach out with any questions or suggestions. I'm excited to see how you'll use it and what ideas you'll bring to the project! Let's build something amazing together! 🚀&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;About The Project&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/assets/asset_list.png" alt="New Notebook" /&gt;&lt;/p&gt; 
&lt;p&gt;An open source, privacy-focused alternative to Google's Notebook LM. Why give Google more of our data when we can take control of our own research workflows?&lt;/p&gt; 
&lt;p&gt;In a world dominated by Artificial Intelligence, having the ability to think 🧠 and acquire new knowledge 💡, is a skill that should not be a privilege for a few, nor restricted to a single provider.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Open Notebook empowers you to:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;🔒 &lt;strong&gt;Control your data&lt;/strong&gt; - Keep your research private and secure&lt;/li&gt; 
 &lt;li&gt;🤖 &lt;strong&gt;Choose your AI models&lt;/strong&gt; - Support for 16+ providers including OpenAI, Anthropic, Ollama, LM Studio, and more&lt;/li&gt; 
 &lt;li&gt;📚 &lt;strong&gt;Organize multi-modal content&lt;/strong&gt; - PDFs, videos, audio, web pages, and more&lt;/li&gt; 
 &lt;li&gt;🎙️ &lt;strong&gt;Generate professional podcasts&lt;/strong&gt; - Advanced multi-speaker podcast generation&lt;/li&gt; 
 &lt;li&gt;🔍 &lt;strong&gt;Search intelligently&lt;/strong&gt; - Full-text and vector search across all your content&lt;/li&gt; 
 &lt;li&gt;💬 &lt;strong&gt;Chat with context&lt;/strong&gt; - AI conversations powered by your research&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Learn more about our project at &lt;a href="https://www.open-notebook.ai"&gt;https://www.open-notebook.ai&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;🆚 Open Notebook vs Google Notebook LM&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Feature&lt;/th&gt; 
   &lt;th&gt;Open Notebook&lt;/th&gt; 
   &lt;th&gt;Google Notebook LM&lt;/th&gt; 
   &lt;th&gt;Advantage&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Privacy &amp;amp; Control&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Self-hosted, your data&lt;/td&gt; 
   &lt;td&gt;Google cloud only&lt;/td&gt; 
   &lt;td&gt;Complete data sovereignty&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;AI Provider Choice&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;16+ providers (OpenAI, Anthropic, Ollama, LM Studio, etc.)&lt;/td&gt; 
   &lt;td&gt;Google models only&lt;/td&gt; 
   &lt;td&gt;Flexibility and cost optimization&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Podcast Speakers&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;1-4 speakers with custom profiles&lt;/td&gt; 
   &lt;td&gt;2 speakers only&lt;/td&gt; 
   &lt;td&gt;Extreme flexibility&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Context Control&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;3 granular levels&lt;/td&gt; 
   &lt;td&gt;All-or-nothing&lt;/td&gt; 
   &lt;td&gt;Privacy and performance tuning&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Content Transformations&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Custom and built-in&lt;/td&gt; 
   &lt;td&gt;Limited options&lt;/td&gt; 
   &lt;td&gt;Unlimited processing power&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;API Access&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Full REST API&lt;/td&gt; 
   &lt;td&gt;No API&lt;/td&gt; 
   &lt;td&gt;Complete automation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Deployment&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Docker, cloud, or local&lt;/td&gt; 
   &lt;td&gt;Google hosted only&lt;/td&gt; 
   &lt;td&gt;Deploy anywhere&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Citations&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Comprehensive with sources&lt;/td&gt; 
   &lt;td&gt;Basic references&lt;/td&gt; 
   &lt;td&gt;Research integrity&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Customization&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Open source, fully customizable&lt;/td&gt; 
   &lt;td&gt;Closed system&lt;/td&gt; 
   &lt;td&gt;Unlimited extensibility&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Cost&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Pay only for AI usage&lt;/td&gt; 
   &lt;td&gt;Monthly subscription + usage&lt;/td&gt; 
   &lt;td&gt;Transparent and controllable&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Why Choose Open Notebook?&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;🔒 &lt;strong&gt;Privacy First&lt;/strong&gt;: Your sensitive research stays completely private&lt;/li&gt; 
 &lt;li&gt;💰 &lt;strong&gt;Cost Control&lt;/strong&gt;: Choose cheaper AI providers or run locally with Ollama&lt;/li&gt; 
 &lt;li&gt;🎙️ &lt;strong&gt;Better Podcasts&lt;/strong&gt;: Full script control and multi-speaker flexibility vs limited 2-speaker deep-dive format&lt;/li&gt; 
 &lt;li&gt;🔧 &lt;strong&gt;Unlimited Customization&lt;/strong&gt;: Modify, extend, and integrate as needed&lt;/li&gt; 
 &lt;li&gt;🌐 &lt;strong&gt;No Vendor Lock-in&lt;/strong&gt;: Switch providers, deploy anywhere, own your data&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Built With&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://www.python.org/"&gt;&lt;img src="https://img.shields.io/badge/Python-3776AB?style=for-the-badge&amp;amp;logo=python&amp;amp;logoColor=white" alt="Python" /&gt;&lt;/a&gt; &lt;a href="https://surrealdb.com/"&gt;&lt;img src="https://img.shields.io/badge/SurrealDB-FF5E00?style=for-the-badge&amp;amp;logo=databricks&amp;amp;logoColor=white" alt="SurrealDB" /&gt;&lt;/a&gt; &lt;a href="https://www.langchain.com/"&gt;&lt;img src="https://img.shields.io/badge/LangChain-3A3A3A?style=for-the-badge&amp;amp;logo=chainlink&amp;amp;logoColor=white" alt="LangChain" /&gt;&lt;/a&gt; &lt;a href="https://streamlit.io/"&gt;&lt;img src="https://img.shields.io/badge/Streamlit-FF4B4B?style=for-the-badge&amp;amp;logo=streamlit&amp;amp;logoColor=white" alt="Streamlit" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;🚀 Quick Start&lt;/h2&gt; 
&lt;p&gt;Ready to try Open Notebook? Choose your preferred method:&lt;/p&gt; 
&lt;h3&gt;⚡ Instant Setup (Recommended)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Create a new directory for your Open Notebook installation
mkdir open-notebook
cd open-notebook

# Using Docker - Get started in 2 minutes
docker run -d \
  --name open-notebook \
  -p 8502:8502 -p 5055:5055 \
  -v ./notebook_data:/app/data \
  -v ./surreal_data:/mydata \
  -e OPENAI_API_KEY=your_key \
  lfnovo/open_notebook:latest-single
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;What gets created:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;open-notebook/
├── notebook_data/     # Your notebooks and research content
└── surreal_data/      # Database files
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Access your installation:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;🖥️ Main Interface&lt;/strong&gt;: &lt;a href="http://localhost:8502"&gt;http://localhost:8502&lt;/a&gt; (Streamlit UI)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🔧 API Access&lt;/strong&gt;: &lt;a href="http://localhost:5055"&gt;http://localhost:5055&lt;/a&gt; (REST API)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;📚 API Documentation&lt;/strong&gt;: &lt;a href="http://localhost:5055/docs"&gt;http://localhost:5055/docs&lt;/a&gt; (Interactive Swagger UI)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;⚠️ Important&lt;/strong&gt;:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;strong&gt;Run from a dedicated folder&lt;/strong&gt;: Create and run this from inside a new &lt;code&gt;open-notebook&lt;/code&gt; folder so your data volumes are properly organized&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Volume persistence&lt;/strong&gt;: The volumes (&lt;code&gt;-v ./notebook_data:/app/data&lt;/code&gt; and &lt;code&gt;-v ./surreal_data:/mydata&lt;/code&gt;) are essential to persist your data between container restarts. Without them, you'll lose all your notebooks and research when the container stops.&lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;🛠️ Full Installation&lt;/h3&gt; 
&lt;p&gt;For development or customization:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/lfnovo/open-notebook
cd open-notebook
make start-all
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;📖 Need Help?&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;🤖 AI Installation Assistant&lt;/strong&gt;: We have a &lt;a href="https://chatgpt.com/g/g-68776e2765b48191bd1bae3f30212631-open-notebook-installation-assistant"&gt;CustomGPT built to help you install Open Notebook&lt;/a&gt; - it will guide you through each step!&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;New to Open Notebook?&lt;/strong&gt; Start with our &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/index.md"&gt;Getting Started Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Need installation help?&lt;/strong&gt; Check our &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/installation.md"&gt;Installation Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Want to see it in action?&lt;/strong&gt; Try our &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/quick-start.md"&gt;Quick Start Tutorial&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Provider Support Matrix&lt;/h2&gt; 
&lt;p&gt;Thanks to the &lt;a href="https://github.com/lfnovo/esperanto"&gt;Esperanto&lt;/a&gt; library, we support this providers out of the box!&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Provider&lt;/th&gt; 
   &lt;th&gt;LLM Support&lt;/th&gt; 
   &lt;th&gt;Embedding Support&lt;/th&gt; 
   &lt;th&gt;Speech-to-Text&lt;/th&gt; 
   &lt;th&gt;Text-to-Speech&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenAI&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Anthropic&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Groq&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Google (GenAI)&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Vertex AI&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ollama&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Perplexity&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ElevenLabs&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Azure OpenAI&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Mistral&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;DeepSeek&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Voyage&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;xAI&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenRouter&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenAI Compatible*&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;*Supports LM Studio and any OpenAI-compatible endpoint&lt;/p&gt; 
&lt;h2&gt;✨ Key Features&lt;/h2&gt; 
&lt;h3&gt;Core Capabilities&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;🔒 Privacy-First&lt;/strong&gt;: Your data stays under your control - no cloud dependencies&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🎯 Multi-Notebook Organization&lt;/strong&gt;: Manage multiple research projects seamlessly&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;📚 Universal Content Support&lt;/strong&gt;: PDFs, videos, audio, web pages, Office docs, and more&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🤖 Multi-Model AI Support&lt;/strong&gt;: 16+ providers including OpenAI, Anthropic, Ollama, Google, LM Studio, and more&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🎙️ Professional Podcast Generation&lt;/strong&gt;: Advanced multi-speaker podcasts with Episode Profiles&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🔍 Intelligent Search&lt;/strong&gt;: Full-text and vector search across all your content&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;💬 Context-Aware Chat&lt;/strong&gt;: AI conversations powered by your research materials&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;📝 AI-Assisted Notes&lt;/strong&gt;: Generate insights or write notes manually&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Advanced Features&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;⚡ Reasoning Model Support&lt;/strong&gt;: Full support for thinking models like DeepSeek-R1 and Qwen3&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🔧 Content Transformations&lt;/strong&gt;: Powerful customizable actions to summarize and extract insights&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🌐 Comprehensive REST API&lt;/strong&gt;: Full programmatic access for custom integrations &lt;a href="http://localhost:5055/docs"&gt;&lt;img src="https://img.shields.io/badge/API-Documentation-blue?style=flat-square" alt="API Docs" /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🔐 Optional Password Protection&lt;/strong&gt;: Secure public deployments with authentication&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;📊 Fine-Grained Context Control&lt;/strong&gt;: Choose exactly what to share with AI models&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;📎 Citations&lt;/strong&gt;: Get answers with proper source citations&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Three-Column Interface&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Sources&lt;/strong&gt;: Manage all your research materials&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Notes&lt;/strong&gt;: Create manual or AI-generated notes&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Chat&lt;/strong&gt;: Converse with AI using your content as context&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=D-760MlGwaI"&gt;&lt;img src="https://img.youtube.com/vi/D-760MlGwaI/0.jpg" alt="Check out our podcast sample" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;📚 Documentation&lt;/h2&gt; 
&lt;h3&gt;Getting Started&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/introduction.md"&gt;📖 Introduction&lt;/a&gt;&lt;/strong&gt; - Learn what Open Notebook offers&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/quick-start.md"&gt;⚡ Quick Start&lt;/a&gt;&lt;/strong&gt; - Get up and running in 5 minutes&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/installation.md"&gt;🔧 Installation&lt;/a&gt;&lt;/strong&gt; - Comprehensive setup guide&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/first-notebook.md"&gt;🎯 Your First Notebook&lt;/a&gt;&lt;/strong&gt; - Step-by-step tutorial&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;User Guide&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/interface-overview.md"&gt;📱 Interface Overview&lt;/a&gt;&lt;/strong&gt; - Understanding the layout&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/notebooks.md"&gt;📚 Notebooks&lt;/a&gt;&lt;/strong&gt; - Organizing your research&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/sources.md"&gt;📄 Sources&lt;/a&gt;&lt;/strong&gt; - Managing content types&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/notes.md"&gt;📝 Notes&lt;/a&gt;&lt;/strong&gt; - Creating and managing notes&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/chat.md"&gt;💬 Chat&lt;/a&gt;&lt;/strong&gt; - AI conversations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/search.md"&gt;🔍 Search&lt;/a&gt;&lt;/strong&gt; - Finding information&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Advanced Topics&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/features/podcasts.md"&gt;🎙️ Podcast Generation&lt;/a&gt;&lt;/strong&gt; - Create professional podcasts&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/features/transformations.md"&gt;🔧 Content Transformations&lt;/a&gt;&lt;/strong&gt; - Customize content processing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/features/ai-models.md"&gt;🤖 AI Models&lt;/a&gt;&lt;/strong&gt; - AI model configuration&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/development/api-reference.md"&gt;🔧 REST API Reference&lt;/a&gt;&lt;/strong&gt; - Complete API documentation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/deployment/security.md"&gt;🔐 Security&lt;/a&gt;&lt;/strong&gt; - Password protection and privacy&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/deployment/index.md"&gt;🚀 Deployment&lt;/a&gt;&lt;/strong&gt; - Complete deployment guides for all scenarios&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="right"&gt;(&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/#readme-top"&gt;back to top&lt;/a&gt;)&lt;/p&gt; 
&lt;h2&gt;🗺️ Roadmap&lt;/h2&gt; 
&lt;h3&gt;Upcoming Features&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;React Frontend&lt;/strong&gt;: Modern React-based frontend to replace Streamlit&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Live Front-End Updates&lt;/strong&gt;: Real-time UI updates for smoother experience&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Async Processing&lt;/strong&gt;: Faster UI through asynchronous content processing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Cross-Notebook Sources&lt;/strong&gt;: Reuse research materials across projects&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Bookmark Integration&lt;/strong&gt;: Connect with your favorite bookmarking apps&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Recently Completed ✅&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Comprehensive REST API&lt;/strong&gt;: Full programmatic access to all functionality&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-Model Support&lt;/strong&gt;: 16+ AI providers including OpenAI, Anthropic, Ollama, LM Studio&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Advanced Podcast Generator&lt;/strong&gt;: Professional multi-speaker podcasts with Episode Profiles&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Content Transformations&lt;/strong&gt;: Powerful customizable actions for content processing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Enhanced Citations&lt;/strong&gt;: Improved layout and finer control for source citations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multiple Chat Sessions&lt;/strong&gt;: Manage different conversations within notebooks&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See the &lt;a href="https://github.com/lfnovo/open-notebook/issues"&gt;open issues&lt;/a&gt; for a full list of proposed features and known issues.&lt;/p&gt; 
&lt;p align="right"&gt;(&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/#readme-top"&gt;back to top&lt;/a&gt;)&lt;/p&gt; 
&lt;h2&gt;🤝 Community &amp;amp; Contributing&lt;/h2&gt; 
&lt;h3&gt;Join the Community&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;💬 &lt;strong&gt;&lt;a href="https://discord.gg/37XJPXfz2w"&gt;Discord Server&lt;/a&gt;&lt;/strong&gt; - Get help, share ideas, and connect with other users&lt;/li&gt; 
 &lt;li&gt;🐛 &lt;strong&gt;&lt;a href="https://github.com/lfnovo/open-notebook/issues"&gt;GitHub Issues&lt;/a&gt;&lt;/strong&gt; - Report bugs and request features&lt;/li&gt; 
 &lt;li&gt;⭐ &lt;strong&gt;Star this repo&lt;/strong&gt; - Show your support and help others discover Open Notebook&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Contributing&lt;/h3&gt; 
&lt;p&gt;We welcome contributions! We're especially looking for help with:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Frontend Development&lt;/strong&gt;: Help build a modern React-based UI (planned replacement for current Streamlit interface)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Testing &amp;amp; Bug Fixes&lt;/strong&gt;: Make Open Notebook more robust&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Feature Development&lt;/strong&gt;: Build the coolest research tool together&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Documentation&lt;/strong&gt;: Improve guides and tutorials&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Current Tech Stack&lt;/strong&gt;: Python, FastAPI, SurrealDB, Streamlit&lt;br /&gt; &lt;strong&gt;Future Roadmap&lt;/strong&gt;: React frontend, enhanced real-time updates&lt;/p&gt; 
&lt;p&gt;See our &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/CONTRIBUTING.md"&gt;Contributing Guide&lt;/a&gt; for detailed information on how to get started.&lt;/p&gt; 
&lt;p align="right"&gt;(&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/#readme-top"&gt;back to top&lt;/a&gt;)&lt;/p&gt; 
&lt;h2&gt;📄 License&lt;/h2&gt; 
&lt;p&gt;Open Notebook is MIT licensed. See the &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;h2&gt;📞 Contact&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Luis Novo&lt;/strong&gt; - &lt;a href="https://twitter.com/lfnovo"&gt;@lfnovo&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Community Support&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;💬 &lt;a href="https://discord.gg/37XJPXfz2w"&gt;Discord Server&lt;/a&gt; - Get help, share ideas, and connect with users&lt;/li&gt; 
 &lt;li&gt;🐛 &lt;a href="https://github.com/lfnovo/open-notebook/issues"&gt;GitHub Issues&lt;/a&gt; - Report bugs and request features&lt;/li&gt; 
 &lt;li&gt;🌐 &lt;a href="https://www.open-notebook.ai"&gt;Website&lt;/a&gt; - Learn more about the project&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🙏 Acknowledgments&lt;/h2&gt; 
&lt;p&gt;Open Notebook is built on the shoulders of amazing open-source projects:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/lfnovo/podcast-creator"&gt;Podcast Creator&lt;/a&gt;&lt;/strong&gt; - Advanced podcast generation capabilities&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/lfnovo/surreal-commands"&gt;Surreal Commands&lt;/a&gt;&lt;/strong&gt; - Background job processing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/lfnovo/content-core"&gt;Content Core&lt;/a&gt;&lt;/strong&gt; - Content processing and management&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/lfnovo/esperanto"&gt;Esperanto&lt;/a&gt;&lt;/strong&gt; - Multi-provider AI model abstraction&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/docling-project/docling"&gt;Docling&lt;/a&gt;&lt;/strong&gt; - Document processing and parsing&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="right"&gt;(&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/#readme-top"&gt;back to top&lt;/a&gt;)&lt;/p&gt; 
&lt;!-- MARKDOWN LINKS &amp; IMAGES --&gt; 
&lt;!-- https://www.markdownguide.org/basic-syntax/#reference-style-links --&gt;</description>
    </item>
    
    <item>
      <title>denizsafak/abogen</title>
      <link>https://github.com/denizsafak/abogen</link>
      <description>&lt;p&gt;Generate audiobooks from EPUBs, PDFs and text with synchronized captions.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;abogen &lt;img width="40px" title="abogen icon" src="https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/abogen/assets/icon.ico" align="right" style="padding-left: 10px; padding-top:5px;" /&gt;&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/denizsafak/abogen/actions"&gt;&lt;img src="https://github.com/denizsafak/abogen/actions/workflows/test_pip.yml/badge.svg?sanitize=true" alt="Build Status" /&gt;&lt;/a&gt; &lt;a href="https://github.com/denizsafak/abogen/releases/latest"&gt;&lt;img src="https://img.shields.io/github/v/release/denizsafak/abogen" alt="GitHub Release" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/abogen/"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/abogen" alt="Abogen PyPi Python Versions" /&gt;&lt;/a&gt; &lt;a href="https://github.com/denizsafak/abogen/releases/latest"&gt;&lt;img src="https://img.shields.io/badge/os-windows%20%7C%20linux%20%7C%20macos%20-blue" alt="Operating Systems" /&gt;&lt;/a&gt; &lt;a href="https://github.com/psf/black"&gt;&lt;img src="https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true" alt="Code style: black" /&gt;&lt;/a&gt; &lt;a href="https://opensource.org/licenses/MIT"&gt;&lt;img src="https://img.shields.io/badge/License-MIT-maroon.svg?sanitize=true" alt="License: MIT" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Abogen is a powerful text-to-speech conversion tool that makes it easy to turn ePub, PDF, or text files into high-quality audio with matching subtitles in seconds. Use it for audiobooks, voiceovers for Instagram, YouTube, TikTok, or any project that needs natural-sounding text-to-speech, using &lt;a href="https://huggingface.co/hexgrad/Kokoro-82M"&gt;Kokoro-82M&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;img title="Abogen Main" src="https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/demo/abogen.png" width="380" /&gt; &lt;img title="Abogen Processing" src="https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/demo/abogen2.png" width="380" /&gt;&lt;/p&gt; 
&lt;h2&gt;Demo&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/cb66512d-0a52-48c3-bda4-f1e6a03fb8d6"&gt;https://github.com/user-attachments/assets/cb66512d-0a52-48c3-bda4-f1e6a03fb8d6&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;This demo was generated in just 5&amp;nbsp;seconds, producing ∼1&amp;nbsp;minute of audio with perfectly synced subtitles. To create a similar video, see &lt;a href="https://github.com/denizsafak/abogen/tree/main/demo"&gt;the demo guide&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;&lt;code&gt;How to install?&lt;/code&gt; &lt;a href="https://pypi.org/project/abogen/" target="_blank"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/abogen" alt="Abogen Compatible PyPi Python Versions" align="right" style="margin-top:6px;" /&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;h3&gt;Windows&lt;/h3&gt; 
&lt;p&gt;Go to &lt;a href="https://github.com/espeak-ng/espeak-ng/releases/latest"&gt;espeak-ng latest release&lt;/a&gt; download and run the *.msi file.&lt;/p&gt; 
&lt;h4&gt;OPTION 1: Install using script&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://github.com/denizsafak/abogen/archive/refs/heads/main.zip"&gt;Download&lt;/a&gt; the repository&lt;/li&gt; 
 &lt;li&gt;Extract the ZIP file&lt;/li&gt; 
 &lt;li&gt;Run &lt;code&gt;WINDOWS_INSTALL.bat&lt;/code&gt; by double-clicking it&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;This method handles everything automatically - installing all dependencies including CUDA in a self-contained environment without requiring a separate Python installation. (You still need to install &lt;a href="https://github.com/espeak-ng/espeak-ng/releases/latest"&gt;espeak-ng&lt;/a&gt;.)&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] You don't need to install Python separately. The script will install Python automatically.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;OPTION 2: Install using pip&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Create a virtual environment (optional)
mkdir abogen &amp;amp;&amp;amp; cd abogen
python -m venv venv
venv\Scripts\activate

# For NVIDIA GPUs:
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128

# For AMD GPUs:
# Not supported yet, because ROCm is not available on Windows. Use Linux if you have AMD GPU.

# Install abogen
pip install abogen
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Mac&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install espeak-ng
brew install espeak-ng

# Create a virtual environment (recommended)
mkdir abogen &amp;amp;&amp;amp; cd abogen
python3 -m venv venv
source venv/bin/activate

# Install abogen
pip3 install abogen
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Linux&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install espeak-ng
sudo apt install espeak-ng # Ubuntu/Debian
sudo pacman -S espeak-ng # Arch Linux
sudo dnf install espeak-ng # Fedora

# Create a virtual environment (recommended)
mkdir abogen &amp;amp;&amp;amp; cd abogen
python3 -m venv venv
source venv/bin/activate

# Install abogen
pip3 install abogen

# For NVIDIA GPUs:
# Already supported, no need to install CUDA separately.

# For AMD GPUs:
# After installing abogen, we need to uninstall the existing torch package
pip3 uninstall torch 
pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/rocm6.4
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] If you get &lt;code&gt;WARNING: The script abogen-cli is installed in '/home/username/.local/bin' which is not on PATH.&lt;/code&gt; error, run the following command to add it to your PATH:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;echo "export PATH=\"/home/$USER/.local/bin:\$PATH\"" &amp;gt;&amp;gt; ~/.bashrc &amp;amp;&amp;amp; source ~/.bashrc
&lt;/code&gt;&lt;/pre&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] If you get "No matching distribution found" error, try installing it on supported Python (3.10 to 3.12). You can use &lt;a href="https://github.com/pyenv/pyenv"&gt;pyenv&lt;/a&gt; to manage multiple Python versions easily in Linux. Watch this &lt;a href="https://www.youtube.com/watch?v=MVyb-nI4KyI"&gt;video&lt;/a&gt; by NetworkChuck for a quick guide.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Special thanks to &lt;a href="https://github.com/hg000125"&gt;@hg000125&lt;/a&gt; for his contribution in &lt;a href="https://github.com/denizsafak/abogen/issues/23"&gt;#23&lt;/a&gt;. AMD GPU support is possible thanks to his work.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;&lt;code&gt;How to run?&lt;/code&gt;&lt;/h2&gt; 
&lt;p&gt;If you installed using pip, you can simply run the following command to start Abogen:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;abogen
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] If you installed using the Windows installer &lt;code&gt;(WINDOWS_INSTALL.bat)&lt;/code&gt;, It should have created a shortcut in the same folder, or your desktop. You can run it from there. If you lost the shortcut, Abogen is located in &lt;code&gt;python_embedded/Scripts/abogen.exe&lt;/code&gt;. You can run it from there directly.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;&lt;code&gt;How to use?&lt;/code&gt;&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Drag and drop any ePub, PDF, or text file (or use the built-in text editor)&lt;/li&gt; 
 &lt;li&gt;Configure the settings: 
  &lt;ul&gt; 
   &lt;li&gt;Set speech speed&lt;/li&gt; 
   &lt;li&gt;Select a voice (or create a custom voice using voice mixer)&lt;/li&gt; 
   &lt;li&gt;Select subtitle generation style (by sentence, word, etc.)&lt;/li&gt; 
   &lt;li&gt;Select output format&lt;/li&gt; 
   &lt;li&gt;Select where to save the output&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Hit Start&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;&lt;code&gt;In action&lt;/code&gt;&lt;/h2&gt; 
&lt;img title="Abogen in action" src="https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/demo/abogen.gif" /&gt; 
&lt;p&gt;Here’s Abogen in action: in this demo, it processes ∼3,000 characters of text in just 11 seconds and turns it into 3 minutes and 28 seconds of audio, and I have a low-end &lt;strong&gt;RTX&amp;nbsp;2060&amp;nbsp;Mobile laptop GPU&lt;/strong&gt;. Your results may vary depending on your hardware.&lt;/p&gt; 
&lt;h2&gt;&lt;code&gt;Configuration&lt;/code&gt;&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Options&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Input Box&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Drag and drop &lt;code&gt;ePub&lt;/code&gt;, &lt;code&gt;PDF&lt;/code&gt;, or &lt;code&gt;.TXT&lt;/code&gt; files (or use built-in text editor)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Queue options&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Add multiple files to a queue and process them in batch, with individual settings for each file. See &lt;a href="https://raw.githubusercontent.com/denizsafak/abogen/main/#queue-mode"&gt;Queue mode&lt;/a&gt; for more details.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Speed&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Adjust speech rate from &lt;code&gt;0.1x&lt;/code&gt; to &lt;code&gt;2.0x&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Select Voice&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;First letter of the language code (e.g., &lt;code&gt;a&lt;/code&gt; for American English, &lt;code&gt;b&lt;/code&gt; for British English, etc.), second letter is for &lt;code&gt;m&lt;/code&gt; for male and &lt;code&gt;f&lt;/code&gt; for female.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Voice mixer&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Create custom voices by mixing different voice models with a profile system. See &lt;a href="https://raw.githubusercontent.com/denizsafak/abogen/main/#voice-mixer"&gt;Voice Mixer&lt;/a&gt; for more details.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Voice preview&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Listen to the selected voice before processing.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Generate subtitles&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;Disabled&lt;/code&gt;, &lt;code&gt;Sentence&lt;/code&gt;, &lt;code&gt;Sentence + Comma&lt;/code&gt;, &lt;code&gt;1 word&lt;/code&gt;, &lt;code&gt;2 words&lt;/code&gt;, &lt;code&gt;3 words&lt;/code&gt;, etc. (Represents the number of words in each subtitle entry)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Output voice format&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;.WAV&lt;/code&gt;, &lt;code&gt;.FLAC&lt;/code&gt;, &lt;code&gt;.MP3&lt;/code&gt;, &lt;code&gt;.OPUS (best compression)&lt;/code&gt; and &lt;code&gt;M4B (with chapters)&lt;/code&gt; (Special thanks to &lt;a href="https://github.com/jborza"&gt;@jborza&lt;/a&gt; for chapter support in PR &lt;a href="https://github.com/denizsafak/abogen/pull/10"&gt;#10&lt;/a&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Output subtitle format&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Configures the subtitle format as &lt;code&gt;SRT (standard)&lt;/code&gt;, &lt;code&gt;ASS (wide)&lt;/code&gt;, &lt;code&gt;ASS (narrow)&lt;/code&gt;, &lt;code&gt;ASS (centered wide)&lt;/code&gt;, or &lt;code&gt;ASS (centered narrow)&lt;/code&gt;.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Replace single newlines with spaces&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Replaces single newlines with spaces in the text. This is useful for texts that have imaginary line breaks.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Save location&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;Save next to input file&lt;/code&gt;, &lt;code&gt;Save to desktop&lt;/code&gt;, or &lt;code&gt;Choose output folder&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Book handler options&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Chapter Control&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Select specific &lt;code&gt;chapters&lt;/code&gt; from ePUBs or &lt;code&gt;chapters + pages&lt;/code&gt; from PDFs.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Save each chapter separately&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Save each chapter in e-books as a separate audio file.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Create a merged version&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Create a single audio file that combines all chapters. (If &lt;code&gt;Save each chapter separately&lt;/code&gt; is disabled, this option will be the default behavior.)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Save in a project folder with metadata&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Save the converted items in a project folder with available metadata files.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Menu options&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Theme&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Change the application's theme using &lt;code&gt;System&lt;/code&gt;, &lt;code&gt;Light&lt;/code&gt;, or &lt;code&gt;Dark&lt;/code&gt; options.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Configure max words per subtitle&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Configures the maximum number of words per subtitle entry.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Configure max lines in log window&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Configures the maximum number of lines to display in the log window.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Separate chapters audio format&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Configures the audio format for separate chapters as &lt;code&gt;wav&lt;/code&gt;, &lt;code&gt;flac&lt;/code&gt;, &lt;code&gt;mp3&lt;/code&gt;, or &lt;code&gt;opus&lt;/code&gt;.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Create desktop shortcut&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Creates a shortcut on your desktop for easy access.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Open config directory&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Opens the directory where the configuration file is stored.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Open cache directory&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Opens the cache directory where converted text files are stored.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Clear cache files&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Deletes cache files created during the conversion or preview.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Check for updates at startup&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Automatically checks for updates when the program starts.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Disable Kokoro's internet access&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Prevents Kokoro from downloading models or voices from HuggingFace Hub, useful for offline use.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Reset to default settings&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Resets all settings to their default values.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;&lt;code&gt;Voice Mixer&lt;/code&gt;&lt;/h2&gt; 
&lt;img title="Abogen Voice Mixer" src="https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/demo/voice_mixer.png" /&gt; 
&lt;p&gt;With voice mixer, you can create custom voices by mixing different voice models. You can adjust the weight of each voice and save your custom voice as a profile for future use. The voice mixer allows you to create unique and personalized voices. (Huge thanks to &lt;a href="https://github.com/jborza"&gt;@jborza&lt;/a&gt; for making this possible through his contributions in &lt;a href="https://github.com/denizsafak/abogen/pull/5"&gt;#5&lt;/a&gt;)&lt;/p&gt; 
&lt;h2&gt;&lt;code&gt;Queue Mode&lt;/code&gt;&lt;/h2&gt; 
&lt;img title="Abogen queue mode" src="https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/demo/queue.png" /&gt; 
&lt;p&gt;Abogen supports &lt;strong&gt;queue mode&lt;/strong&gt;, allowing you to add multiple files to a processing queue. This is useful if you want to convert several files in one batch.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;You can add text files (&lt;code&gt;.txt&lt;/code&gt;) directly using the &lt;strong&gt;Add files&lt;/strong&gt; button in the Queue Manager. To add PDF or EPUB files, use the input box in the main window and click the &lt;strong&gt;Add to Queue&lt;/strong&gt; button.&lt;/li&gt; 
 &lt;li&gt;Each file in the queue keeps the configuration settings that were active when it was added. Changing the main window configuration afterward does &lt;strong&gt;not&lt;/strong&gt; affect files already in the queue.&lt;/li&gt; 
 &lt;li&gt;You can view each file's configuration by hovering over them.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Abogen will process each item in the queue automatically, saving outputs as configured.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Special thanks to &lt;a href="https://github.com/jborza"&gt;@jborza&lt;/a&gt; for adding queue mode in PR &lt;a href="https://github.com/denizsafak/abogen/pull/35"&gt;#35&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;&lt;code&gt;About Chapter Markers&lt;/code&gt;&lt;/h2&gt; 
&lt;p&gt;When you process ePUB or PDF files, Abogen converts them into text files stored in your cache directory. When you click "Edit," you're actually modifying these converted text files. In these text files, you'll notice tags that look like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;lt;&amp;lt;CHAPTER_MARKER:Chapter Title&amp;gt;&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;These are chapter markers. They are automatically added when you process ePUB or PDF files, based on the chapters you select. They serve an important purpose:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Allow you to split the text into separate audio files for each chapter&lt;/li&gt; 
 &lt;li&gt;Save time by letting you reprocess only specific chapters if errors occur, rather than the entire file&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can manually add these markers to plain text files for the same benefits. Simply include them in your text like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;lt;&amp;lt;CHAPTER_MARKER:Introduction&amp;gt;&amp;gt;
This is the beginning of my text...  

&amp;lt;&amp;lt;CHAPTER_MARKER:Main Content&amp;gt;&amp;gt; 
Here's another part...  
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;When you process the text file, Abogen will detect these markers automatically and ask if you want to save each chapter separately and create a merged version.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/demo/chapter_marker.png" alt="Abogen Chapter Marker" /&gt;&lt;/p&gt; 
&lt;h2&gt;&lt;code&gt;About Metadata Tags&lt;/code&gt;&lt;/h2&gt; 
&lt;p&gt;Similar to chapter markers, it is possible to add metadata tags for &lt;code&gt;M4B&lt;/code&gt; files. This is useful for audiobook players that support metadata, allowing you to add information like title, author, year, etc. Abogen automatically adds these tags when you process ePUB or PDF files, but you can also add them manually to your text files. Add metadata tags &lt;strong&gt;at the beginning of your text file&lt;/strong&gt; like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;lt;&amp;lt;METADATA_TITLE:Title&amp;gt;&amp;gt;
&amp;lt;&amp;lt;METADATA_ARTIST:Author&amp;gt;&amp;gt;
&amp;lt;&amp;lt;METADATA_ALBUM:Album Title&amp;gt;&amp;gt;
&amp;lt;&amp;lt;METADATA_YEAR:Year&amp;gt;&amp;gt;
&amp;lt;&amp;lt;METADATA_ALBUM_ARTIST:Album Artist&amp;gt;&amp;gt;
&amp;lt;&amp;lt;METADATA_COMPOSER:Narrator&amp;gt;&amp;gt;
&amp;lt;&amp;lt;METADATA_GENRE:Audiobook&amp;gt;&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;&lt;code&gt;Supported Languages&lt;/code&gt;&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;# 🇺🇸 'a' =&amp;gt; American English, 🇬🇧 'b' =&amp;gt; British English
# 🇪🇸 'e' =&amp;gt; Spanish es
# 🇫🇷 'f' =&amp;gt; French fr-fr
# 🇮🇳 'h' =&amp;gt; Hindi hi
# 🇮🇹 'i' =&amp;gt; Italian it
# 🇯🇵 'j' =&amp;gt; Japanese: pip install misaki[ja]
# 🇧🇷 'p' =&amp;gt; Brazilian Portuguese pt-br
# 🇨🇳 'z' =&amp;gt; Mandarin Chinese: pip install misaki[zh]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For a complete list of supported languages and voices, refer to Kokoro's &lt;a href="https://huggingface.co/hexgrad/Kokoro-82M/blob/main/VOICES.md"&gt;VOICES.md&lt;/a&gt;. To listen to sample audio outputs, see &lt;a href="https://huggingface.co/hexgrad/Kokoro-82M/blob/main/SAMPLES.md"&gt;SAMPLES.md&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;&lt;code&gt;MPV Config&lt;/code&gt;&lt;/h2&gt; 
&lt;p&gt;I highly recommend using &lt;a href="https://mpv.io/installation/"&gt;MPV&lt;/a&gt; to play your audio files, as it supports displaying subtitles even without a video track. Here's my &lt;code&gt;mpv.conf&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;# --- MPV Settings ---
save-position-on-quit
keep-open=yes
# --- Subtitle ---
sub-ass-override=no
sub-margin-y=50
sub-margin-x=50
# --- Audio Quality ---
audio-spdif=ac3,dts,eac3,truehd,dts-hd
audio-channels=auto
audio-samplerate=48000
volume-max=200
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;&lt;code&gt;Docker Guide&lt;/code&gt;&lt;/h2&gt; 
&lt;p&gt;If you want to run Abogen in a Docker container:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://github.com/denizsafak/abogen/archive/refs/heads/main.zip"&gt;Download the repository&lt;/a&gt; and extract, or clone it using git.&lt;/li&gt; 
 &lt;li&gt;Go to &lt;code&gt;abogen&lt;/code&gt; folder. You should see &lt;code&gt;Dockerfile&lt;/code&gt; there.&lt;/li&gt; 
 &lt;li&gt;Open your termminal in that directory and run the following commands:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Build the Docker image:
docker build --progress plain -t abogen .

# Note that building the image may take a while.
# After building is complete, run the Docker container:

# Windows
docker run --name abogen -v %cd%:/shared -p 5800:5800 -p 5900:5900 --gpus all abogen

# Linux
docker run --name abogen -v $(pwd):/shared -p 5800:5800 -p 5900:5900 --gpus all abogen

# MacOS
docker run --name abogen -v $(pwd):/shared -p 5800:5800 -p 5900:5900 abogen

# We expose port 5800 for use by a web browser, 5900 if you want to connect with a VNC client.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Abogen launches automatically inside the container.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;You can access it via a web browser at &lt;a href="http://localhost:5800"&gt;http://localhost:5800&lt;/a&gt; or connect to it using a VNC client at &lt;code&gt;localhost:5900&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;You can use &lt;code&gt;/shared&lt;/code&gt; directory to share files between your host and the container.&lt;/li&gt; 
 &lt;li&gt;For later use, start it with &lt;code&gt;docker start abogen&lt;/code&gt; and stop it with &lt;code&gt;docker stop abogen&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Known issues:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Audio preview is not working inside container (ALSA error).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Open cache directory&lt;/code&gt; and &lt;code&gt;Open configuration directory&lt;/code&gt; options in settings not working. (Tried pcmanfm, did not work with Abogen).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;(Special thanks to &lt;a href="https://www.reddit.com/user/geo38/"&gt;@geo38&lt;/a&gt; from Reddit, who provided the Dockerfile and instructions in &lt;a href="https://www.reddit.com/r/selfhosted/comments/1k8x1yo/comment/mpe0bz8/"&gt;this comment&lt;/a&gt;.)&lt;/p&gt; 
&lt;h2&gt;&lt;code&gt;Similar Projects&lt;/code&gt;&lt;/h2&gt; 
&lt;p&gt;Abogen is a standalone project, but it is inspired by and shares some similarities with other projects. Here are a few:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/santinic/audiblez"&gt;audiblez&lt;/a&gt;: Generate audiobooks from e-books. &lt;strong&gt;(Has CLI and GUI support)&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/plusuncold/autiobooks"&gt;autiobooks&lt;/a&gt;: Automatically convert epubs to audiobooks&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/mateogon/pdf-narrator"&gt;pdf-narrator&lt;/a&gt;: Convert your PDFs and EPUBs into audiobooks effortlessly.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/p0n1/epub_to_audiobook"&gt;epub_to_audiobook&lt;/a&gt;: EPUB to audiobook converter, optimized for Audiobookshelf&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/DrewThomasson/ebook2audiobook"&gt;ebook2audiobook&lt;/a&gt;: Convert ebooks to audiobooks with chapters and metadata using dynamic AI models and voice cloning&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;code&gt;Roadmap&lt;/code&gt;&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Add OCR scan feature for PDF files using docling/teserract.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Add chapter metadata for .m4a files. (Issue &lt;a href="https://github.com/denizsafak/abogen/issues/9"&gt;#9&lt;/a&gt;, PR &lt;a href="https://github.com/denizsafak/abogen/pull/10"&gt;#10&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Add support for different languages in GUI.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Add voice formula feature that enables mixing different voice models. (Issue &lt;a href="https://github.com/denizsafak/abogen/issues/1"&gt;#1&lt;/a&gt;, PR &lt;a href="https://github.com/denizsafak/abogen/pull/5"&gt;#5&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Add support for kokoro-onnx (If it's necessary).&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Add dark mode.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;code&gt;Troubleshooting&lt;/code&gt;&lt;/h2&gt; 
&lt;p&gt;If you encounter any issues while running Abogen, try launching it from the command line with:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;abogen-cli
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will start Abogen in command-line mode and display detailed error messages. Please open a new issue on the &lt;a href="https://github.com/denizsafak/abogen/issues"&gt;Issues&lt;/a&gt; page with the error message and a description of your problem.&lt;/p&gt; 
&lt;h2&gt;&lt;code&gt;Contributing&lt;/code&gt;&lt;/h2&gt; 
&lt;p&gt;I welcome contributions! If you have ideas for new features, improvements, or bug fixes, please fork the repository and submit a pull request.&lt;/p&gt; 
&lt;h3&gt;For developers and contributors&lt;/h3&gt; 
&lt;p&gt;If you'd like to modify the code and contribute to development, you can &lt;a href="https://github.com/denizsafak/abogen/archive/refs/heads/main.zip"&gt;download the repository&lt;/a&gt;, extract it and run the following commands to build &lt;strong&gt;or&lt;/strong&gt; install the package:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Go to the directory where you extracted the repository and run:
pip install -e .      # Installs the package in editable mode
pip install build     # Install the build package
python -m build       # Builds the package in dist folder (optional)
abogen                # Opens the GUI
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Feel free to explore the code and make any changes you like.&lt;/p&gt; 
&lt;h2&gt;&lt;code&gt;Credits&lt;/code&gt;&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Abogen uses &lt;a href="https://github.com/hexgrad/kokoro"&gt;Kokoro&lt;/a&gt; for its high-quality, natural-sounding text-to-speech synthesis. Huge thanks to the Kokoro team for making this possible.&lt;/li&gt; 
 &lt;li&gt;Thanks to &lt;a href="https://github.com/wojiushixiaobai"&gt;@wojiushixiaobai&lt;/a&gt; for &lt;a href="https://github.com/wojiushixiaobai/Python-Embed-Win64"&gt;Embedded Python&lt;/a&gt; packages. These modified packages include pip pre-installed, enabling Abogen to function as a standalone application without requiring users to separately install Python in Windows.&lt;/li&gt; 
 &lt;li&gt;Thanks to creators of &lt;a href="https://github.com/aerkalov/ebooklib"&gt;EbookLib&lt;/a&gt;, a Python library for reading and writing ePub files, which is used for extracting text from ePub files.&lt;/li&gt; 
 &lt;li&gt;Special thanks to the &lt;a href="https://www.riverbankcomputing.com/software/pyqt/"&gt;PyQt&lt;/a&gt; team for providing the cross-platform GUI toolkit that powers Abogen's interface.&lt;/li&gt; 
 &lt;li&gt;Icons: &lt;a href="https://icons8.com/icon/aRiu1GGi6Aoe/usa"&gt;US&lt;/a&gt;, &lt;a href="https://icons8.com/icon/t3NE3BsOAQwq/great-britain"&gt;Great Britain&lt;/a&gt;, &lt;a href="https://icons8.com/icon/ly7tzANRt33n/spain"&gt;Spain&lt;/a&gt;, &lt;a href="https://icons8.com/icon/3muzEmi4dpD5/france"&gt;France&lt;/a&gt;, &lt;a href="https://icons8.com/icon/esGVrxg9VCJ1/india"&gt;India&lt;/a&gt;, &lt;a href="https://icons8.com/icon/PW8KZnP7qXzO/italy"&gt;Italy&lt;/a&gt;, &lt;a href="https://icons8.com/icon/McQbrq9qaQye/japan"&gt;Japan&lt;/a&gt;, &lt;a href="https://icons8.com/icon/zHmH8HpOmM90/brazil"&gt;Brazil&lt;/a&gt;, &lt;a href="https://icons8.com/icon/Ej50Oe3crXwF/china"&gt;China&lt;/a&gt;, &lt;a href="https://icons8.com/icon/uI49hxbpxTkp/female"&gt;Female&lt;/a&gt;, &lt;a href="https://icons8.com/icon/12351/male"&gt;Male&lt;/a&gt;, &lt;a href="https://icons8.com/icon/21698/adjust"&gt;Adjust&lt;/a&gt; and &lt;a href="https://icons8.com/icon/GskSeVoroQ7u/voice-id"&gt;Voice Id&lt;/a&gt; icons by &lt;a href="https://icons8.com/"&gt;Icons8&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;code&gt;License&lt;/code&gt;&lt;/h2&gt; 
&lt;p&gt;This project is available under the MIT License - see the &lt;a href="https://github.com/denizsafak/abogen/raw/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details. &lt;a href="https://github.com/hexgrad/kokoro"&gt;Kokoro&lt;/a&gt; is licensed under &lt;a href="https://github.com/hexgrad/kokoro/raw/main/LICENSE"&gt;Apache-2.0&lt;/a&gt; which allows commercial use, modification, distribution, and private use.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] Subtitle generation currently works only for English. This is because Kokoro provides timestamp tokens only for English text. If you want subtitles in other languages, please request this feature in the &lt;a href="https://github.com/hexgrad/kokoro"&gt;Kokoro project&lt;/a&gt;. For more technical details, see &lt;a href="https://github.com/hexgrad/kokoro/raw/6d87f4ae7abc2d14dbc4b3ef2e5f19852e861ac2/kokoro/pipeline.py#L383"&gt;this line&lt;/a&gt; in the Kokoro's code.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Tags: audiobook, kokoro, text-to-speech, TTS, audiobook generator, audiobooks, text to speech, audiobook maker, audiobook creator, audiobook generator, voice-synthesis, text to audio, text to audio converter, text to speech converter, text to speech generator, text to speech software, text to speech app, epub to audio, pdf to audio, content-creation, media-generation&lt;/p&gt; 
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>trailofbits/buttercup</title>
      <link>https://github.com/trailofbits/buttercup</link>
      <description>&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Buttercup Cyber Reasoning System (CRS)&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/trailofbits/buttercup/actions/workflows/tests.yml"&gt;&lt;img src="https://github.com/trailofbits/buttercup/actions/workflows/tests.yml/badge.svg?sanitize=true" alt="Tests" /&gt;&lt;/a&gt; &lt;a href="https://github.com/trailofbits/buttercup/actions/workflows/tests.yml"&gt;&lt;img src="https://github.com/trailofbits/buttercup/actions/workflows/tests.yml/badge.svg?event=schedule" alt="Tests (Nightly)" /&gt;&lt;/a&gt; &lt;a href="https://github.com/trailofbits/buttercup/actions/workflows/integration.yml"&gt;&lt;img src="https://github.com/trailofbits/buttercup/actions/workflows/integration.yml/badge.svg?sanitize=true" alt="Integration" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Buttercup&lt;/strong&gt; is a Cyber Reasoning System (CRS) developed by &lt;strong&gt;Trail of Bits&lt;/strong&gt; for the &lt;strong&gt;DARPA AIxCC (AI Cyber Challenge)&lt;/strong&gt;. Buttercup finds and patches software vulnerabilities in open-source code repositories like &lt;a href="https://github.com/tob-challenges/example-libpng"&gt;example-libpng&lt;/a&gt;. It starts by running an AI/ML-assisted fuzzing campaign (built on oss-fuzz) for the program. When vulnerabilities are found, Buttercup analyzes them and uses a multi-agent AI-driven patcher to repair the vulnerability. &lt;strong&gt;Buttercup&lt;/strong&gt; system consists of several components:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Orchestrator&lt;/strong&gt;: Coordinates the overall task process and manages the workflow&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Seed Generator&lt;/strong&gt;: Creates inputs for vulnerability discovery&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Fuzzer&lt;/strong&gt;: Discovers vulnerabilities through intelligent fuzzing techniques&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Program Model&lt;/strong&gt;: Analyzes code structure and semantics for better understanding&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Patcher&lt;/strong&gt;: Generates and applies security patches to fix vulnerabilities&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;System Requirements&lt;/h2&gt; 
&lt;h3&gt;Minimum Requirements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;CPU:&lt;/strong&gt; 8 cores&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Memory:&lt;/strong&gt; 16 GB RAM&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Storage:&lt;/strong&gt; 100 GB available disk space&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Network:&lt;/strong&gt; Stable internet connection for downloading dependencies&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Buttercup uses third-party AI providers (LLMs from companies like OpenAI, Anthropic and Google), which cost money. Please ensure that you manage per-deployment costs by using the built-in LLM budget setting.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Buttercup works best with access to models from OpenAI &lt;strong&gt;and&lt;/strong&gt; Anthropic, but can be run with at least one API key from one third-party provider (support for Gemini coming soon).&lt;/p&gt; 
&lt;h3&gt;Supported Systems&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Linux x86_64&lt;/strong&gt; (fully supported)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ARM64&lt;/strong&gt; (partial support for upstream Google OSS-Fuzz projects)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Required System Packages&lt;/h3&gt; 
&lt;p&gt;Before setup, ensure you have these packages installed:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Ubuntu/Debian
sudo apt-get update
sudo apt-get install -y make curl git

# RHEL/CentOS/Fedora
sudo yum install -y make curl git
# or
sudo dnf install -y make curl git

# MacOS
brew install make curl git
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Supported Targets&lt;/h3&gt; 
&lt;p&gt;Buttercup works with:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;C source code repositories&lt;/strong&gt; that are OSS-Fuzz compatible&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Java source code repositories&lt;/strong&gt; that are OSS-Fuzz compatible&lt;/li&gt; 
 &lt;li&gt;Projects that build successfully and have existing fuzzing harnesses&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Clone the repository with submodules:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone --recurse-submodules https://github.com/trailofbits/buttercup.git
cd buttercup
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Run automated setup (Recommended)&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;make setup-local
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This script will install all dependencies, configure the environment, and guide you through the setup process.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; If you prefer manual setup, see the &lt;a href="https://raw.githubusercontent.com/trailofbits/buttercup/main/MANUAL_SETUP.md"&gt;Manual Setup Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Start Buttercup locally&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;make deploy-local
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="4"&gt; 
 &lt;li&gt;Verify local deployment:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;make status
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;When a deployment is successful, you should see all pods in "Running" or "Completed" status.&lt;/p&gt; 
&lt;ol start="5"&gt; 
 &lt;li&gt;Send Buttercup a simple task&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; When tasked, Buttercup will start consuming third-party AI resources.&lt;/p&gt; 
&lt;p&gt;This command will make Buttercup pull down an example repo &lt;a href="https://github.com/tob-challenges/example-libpng"&gt;example-libpng&lt;/a&gt; with a known vulnerability. Buttercup will start fuzzing it to find and patch vulnerabilities.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;make send-libpng-task
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="6"&gt; 
 &lt;li&gt;Access Buttercup's web-based GUI&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;make web-ui
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then navigate to &lt;code&gt;http://localhost:31323&lt;/code&gt; in your web browser.&lt;/p&gt; 
&lt;p&gt;In the GUI you can monitor active tasks and see when Buttercup finds bugs and generates patches for them.&lt;/p&gt; 
&lt;ol start="7"&gt; 
 &lt;li&gt;Stop Buttercup&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; This is an important step to ensure Buttercup shuts down and stops consuming third-party AI resources.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;make undeploy
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Accessing Logs&lt;/h2&gt; 
&lt;p&gt;Buttercup includes local SigNoz deployment by default for comprehensive system observability. You can access logs, traces, and metrics through the SigNoz UI:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;make signoz-ui
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then navigate to &lt;code&gt;http://localhost:33301&lt;/code&gt; in your web browser to view:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Distributed traces&lt;/li&gt; 
 &lt;li&gt;Application metrics&lt;/li&gt; 
 &lt;li&gt;Error monitoring&lt;/li&gt; 
 &lt;li&gt;Performance insights&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If you configured LangFuse during setup, you can also monitor LLM usage and costs there.&lt;/p&gt; 
&lt;p&gt;For additional log access methods, see the &lt;a href="https://raw.githubusercontent.com/trailofbits/buttercup/main/QUICK_REFERENCE.md"&gt;Quick Reference Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Additional Resources&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/trailofbits/buttercup/main/QUICK_REFERENCE.md"&gt;Quick Reference Guide&lt;/a&gt; - Common commands and troubleshooting&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/trailofbits/buttercup/main/MANUAL_SETUP.md"&gt;Manual Setup Guide&lt;/a&gt; - Detailed manual installation steps&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/trailofbits/buttercup/main/AKS_DEPLOYMENT.md"&gt;AKS Deployment Guide&lt;/a&gt; - Production deployment on Azure&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/trailofbits/buttercup/main/CONTRIBUTING.md"&gt;Contributing Guidelines&lt;/a&gt; - Development workflow and standards&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/trailofbits/buttercup/main/deployment/README.md"&gt;Deployment Documentation&lt;/a&gt; - Advanced deployment configuration&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/trailofbits/buttercup/main/CUSTOM_CHALLENGES.md"&gt;Writing Custom Challenges&lt;/a&gt; - Custom project configuration and setup&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>LeCAR-Lab/ASAP</title>
      <link>https://github.com/LeCAR-Lab/ASAP</link>
      <description>&lt;p&gt;[RSS 2025] "ASAP: Aligning Simulation and Real-World Physics for Learning Agile Humanoid Whole-Body Skills"&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt; ASAP: Aligning Simulation and Real-World Physics for &lt;p&gt;Learning Agile Humanoid Whole-Body Skills &lt;/p&gt;&lt;/h1&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;Robotics: Science and Systems (RSS) 2025&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://agile.human2humanoid.com/"&gt;[Website]&lt;/a&gt; &lt;a href="https://arxiv.org/pdf/2502.01143"&gt;[Arxiv]&lt;/a&gt; &lt;a href="https://www.youtube.com/watch?v=tu7LSNYWDTs&amp;amp;ab_channel=LeCARLabatCMU"&gt;[Video]&lt;/a&gt;&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/imgs/CMU-NV-logo-crop-png.png" height="50&amp;quot;" /&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://developer.nvidia.com/isaac-gym"&gt;&lt;img src="https://img.shields.io/badge/IsaacGym-Preview4-b.svg?sanitize=true" alt="IsaacGym" /&gt;&lt;/a&gt; &lt;a href="https://docs.isaacsim.omniverse.nvidia.com/4.2.0/index.html"&gt;&lt;img src="https://img.shields.io/badge/IsaacSim-4.2.0-b.svg?sanitize=true" alt="IsaacSim" /&gt;&lt;/a&gt; &lt;a href="https://docs.isaacsim.omniverse.nvidia.com/4.2.0/index.html"&gt;&lt;img src="https://img.shields.io/badge/Genesis-0.2.1-b.svg?sanitize=true" alt="IsaacSim" /&gt;&lt;/a&gt; &lt;a href="https://ubuntu.com/blog/tag/22-04-lts"&gt;&lt;img src="https://img.shields.io/badge/Platform-linux--64-orange.svg?sanitize=true" alt="Linux platform" /&gt;&lt;/a&gt; &lt;a href=""&gt;&lt;img src="https://img.shields.io/badge/License-MIT-yellow.svg?sanitize=true" alt="License: MIT" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;img src="https://agile.human2humanoid.com/static/images/asap-preview-gif-480P.gif" width="400px" /&gt; 
&lt;/div&gt; 
&lt;!-- # Table of Contents --&gt; 
&lt;h2&gt;📚 Table of Contents&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#overview"&gt;Overview&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Links: &lt;a href="https://agile.human2humanoid.com/"&gt;Website&lt;/a&gt; • &lt;a href="https://arxiv.org/pdf/2502.01143"&gt;Arxiv&lt;/a&gt; • &lt;a href="https://www.youtube.com/watch?v=tu7LSNYWDTs&amp;amp;ab_channel=LeCARLabatCMU"&gt;Video&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#installation"&gt;Installation &amp;amp; Setup&lt;/a&gt;&lt;/strong&gt;&lt;br /&gt; 2.1 &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#isaacgym-conda-env"&gt;Base Frameworks&lt;/a&gt;&lt;br /&gt; 2.2 &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#install-isaacgym"&gt;IsaacGym Setup&lt;/a&gt;&lt;br /&gt; 2.3 &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#install-humanoidverse"&gt;HumanoidVerse Setup&lt;/a&gt;&lt;br /&gt; 2.4 &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#isaaclab-environment"&gt;IsaacSim + IsaacLab Setup&lt;/a&gt;&lt;br /&gt; 2.5 &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#genesis-environment"&gt;Genesis Environment Setup&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#motion-tracking-training"&gt;Training Pipelines&lt;/a&gt;&lt;/strong&gt;&lt;br /&gt; 3.1 &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#motion-tracking-training"&gt;Phase-Based Motion Tracking&lt;/a&gt;&lt;br /&gt; 3.2 &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#asap-delta-action-model-training"&gt;ASAP Delta Action Model&lt;/a&gt;&lt;br /&gt; - &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#train-delta-action-model"&gt;Train Delta Action Model&lt;/a&gt;&lt;br /&gt; - &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#use-delta-action-model-for-policy-finetuning"&gt;Finetune Policy with Delta Action Model&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#motion-retargeting-to-any-humanoid"&gt;Motion Retargeting to Any Humanoid&lt;/a&gt;&lt;/strong&gt;&lt;br /&gt; 4.1 &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#1-smpl-shape-preparation"&gt;Step 1: SMPL Shape Preparation&lt;/a&gt;&lt;br /&gt; 4.2 &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#2-smpl-motion-preparation-amass"&gt;Step 2: SMPL Motion Preparation (AMASS)&lt;/a&gt;&lt;br /&gt; 4.3 &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#3-robot-xml-and-motion-config-preparation"&gt;Step 3: Robot XML &amp;amp; Motion Config&lt;/a&gt;&lt;br /&gt; 4.4 &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#4-humanoid-smpl-shape-fitting"&gt;Step 4: Humanoid-SMPL Shape Fitting&lt;/a&gt;&lt;br /&gt; 4.5 &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#5-humanoid-smpl-motion-retargeting"&gt;Step 5: Humanoid-SMPL Motion Retargeting&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#sim2simsim2real"&gt;Deployment: Sim2Sim &amp;amp; Sim2Real&lt;/a&gt;&lt;/strong&gt;&lt;br /&gt; 5.1 &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#environment-setup"&gt;Environment Setup&lt;/a&gt;&lt;br /&gt; 5.2 &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#sim2sim"&gt;Sim2Sim Deployment&lt;/a&gt;&lt;br /&gt; 5.3 &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#sim2real"&gt;Sim2Real Deployment&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#citation"&gt;Citation&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/#license"&gt;License&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;TODO&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Release code backbone&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Release phase-based motion tracking training pipeline&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Release ASAP motion datasets&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Release motion retargeting pipeline&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Release sim2sim in MuJoCo&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Release sim2real with UnitreeSDK&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Release ASAP delta action model training pipeline&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Installation&lt;/h1&gt; 
&lt;p&gt;ASAP codebase is built on top of &lt;a href="https://github.com/LeCAR-Lab/HumanoidVerse"&gt;HumanoidVerse&lt;/a&gt; (a multi-simulator framework for humanoid learning) and &lt;a href="https://github.com/LeCAR-Lab/human2humanoid"&gt;Human2Humanoid&lt;/a&gt; (our prior work on humanoid whole-body tracking).&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/LeCAR-Lab/HumanoidVerse"&gt;HumanoidVerse&lt;/a&gt; allows you to train humanoid skills in multiple simulators, including IsaacGym, IsaacSim, and Genesis. Its key design logic is the separation and modularization of simulators, tasks, and algorithms, which enables smooth transfers between different simulators and the real world with minimum effort (just one line of code change). We leverage this framework to develop &lt;a href="https://agile.human2humanoid.com/"&gt;ASAP&lt;/a&gt; and study how to best transfer policies across simulators and the real world.&lt;/p&gt; 
&lt;h2&gt;IsaacGym Conda Env&lt;/h2&gt; 
&lt;p&gt;Create mamba/conda environment, in the following we use conda for example, but you can use mamba as well.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;conda create -n hvgym python=3.8
conda activate hvgym
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Install IsaacGym&lt;/h3&gt; 
&lt;p&gt;Download &lt;a href="https://developer.nvidia.com/isaac-gym/download"&gt;IsaacGym&lt;/a&gt; and extract:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;wget https://developer.nvidia.com/isaac-gym-preview-4
tar -xvzf isaac-gym-preview-4
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Install IsaacGym Python API:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -e isaacgym/python
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Test installation:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python 1080_balls_of_solitude.py  # or
python joint_monkey.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For libpython error:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Check conda path: &lt;pre&gt;&lt;code class="language-bash"&gt;conda info -e
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Set LD_LIBRARY_PATH: &lt;pre&gt;&lt;code class="language-bash"&gt;export LD_LIBRARY_PATH=&amp;lt;/path/to/conda/envs/your_env/lib&amp;gt;:$LD_LIBRARY_PATH
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Install HumanoidVerse&lt;/h3&gt; 
&lt;p&gt;Install dependencies:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -e .
pip install -e isaac_utils
pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Test with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;HYDRA_FULL_ERROR=1 python humanoidverse/train_agent.py \
+simulator=isaacgym \
+exp=locomotion \
+domain_rand=NO_domain_rand \
+rewards=loco/reward_g1_locomotion \
+robot=g1/g1_29dof_anneal_23dof \
+terrain=terrain_locomotion_plane \
+obs=loco/leggedloco_obs_singlestep_withlinvel \
num_envs=1 \
project_name=TestIsaacGymInstallation \
experiment_name=G123dof_loco \
headless=False
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;Note:&lt;/summary&gt; This is ONLY for testing, NOT how we train the locomotion policy in the ASAP paper. But still, you can train a locomotion policy by: 
 &lt;pre&gt;&lt;code class="language-bash"&gt;HYDRA_FULL_ERROR=1 python humanoidverse/train_agent.py \
+simulator=isaacgym \
+exp=locomotion \
+domain_rand=NO_domain_rand \
+rewards=loco/reward_g1_locomotion \
+robot=g1/g1_29dof_anneal_23dof \
+terrain=terrain_locomotion_plane \
+obs=loco/leggedloco_obs_singlestep_withlinvel \
num_envs=4096 \
project_name=TestIsaacGymInstallation \
experiment_name=G123dof_loco \
headless=True \
rewards.reward_penalty_curriculum=True \
rewards.reward_initial_penalty_scale=0.1 \
rewards.reward_penalty_degree=0.00003 
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;IsaacLab Environment&lt;/h2&gt; 
&lt;h3&gt;Install IsaacSim&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Download Omniverse Launcher&lt;/li&gt; 
 &lt;li&gt;Install Isaac Sim through launcher&lt;/li&gt; 
 &lt;li&gt;Set environment variables:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export ISAACSIM_PATH="${HOME}/.local/share/ov/pkg/isaac-sim-4.2.0"
export ISAACSIM_PYTHON_EXE="${ISAACSIM_PATH}/python.sh"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Install IsaacLab&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/isaac-sim/IsaacLab.git
cd IsaacLab &amp;amp;&amp;amp; ./isaaclab.sh --conda hvlab
mamba activate hvlab
sudo apt install cmake build-essential
./isaaclab.sh --install
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Setup HumanoidVerse&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -e .
pip install -e isaac_utils
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Genesis Environment&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;mamba create -n hvgen python=3.10
mamba activate hvgen
pip install genesis-world torch
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Install dependencies:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -e .
pip install -e isaac_utils
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;Motion Tracking Training&lt;/h1&gt; 
&lt;p&gt;Train a phase-based motion tracking policy to imitate Cristiano Ronaldo's signature Siuuu move&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python humanoidverse/train_agent.py \
+simulator=isaacgym \
+exp=motion_tracking \
+domain_rand=NO_domain_rand \
+rewards=motion_tracking/reward_motion_tracking_dm_2real \
+robot=g1/g1_29dof_anneal_23dof \
+terrain=terrain_locomotion_plane \
+obs=motion_tracking/deepmimic_a2c_nolinvel_LARGEnoise_history \
num_envs=4096 \
project_name=MotionTracking \
experiment_name=MotionTracking_CR7 \
robot.motion.motion_file="humanoidverse/data/motions/g1_29dof_anneal_23dof/TairanTestbed/singles/0-TairanTestbed_TairanTestbed_CR7_video_CR7_level1_filter_amass.pkl" \
rewards.reward_penalty_curriculum=True \
rewards.reward_penalty_degree=0.00001 \
env.config.resample_motion_when_training=False \
env.config.termination.terminate_when_motion_far=True \
env.config.termination_curriculum.terminate_when_motion_far_curriculum=True \
env.config.termination_curriculum.terminate_when_motion_far_threshold_min=0.3 \
env.config.termination_curriculum.terminate_when_motion_far_curriculum_degree=0.000025 \
robot.asset.self_collisions=0
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After training, you can visualize the policy by:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python humanoidverse/eval_agent.py \
+checkpoint=logs/MotionTracking/xxxxxxxx_xxxxxxx-MotionTracking_CR7-motion_tracking-g1_29dof_anneal_23dof/model_5800.pt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This is the visualization of the policy after traning 5800 iters. The policy is able to imitate the motion of Cristiano Ronaldo's Siuuu move. With more training, the policy will be more accurate and smooth (see the video in the &lt;a href="https://arxiv.org/pdf/2502.01143"&gt;paper&lt;/a&gt;).&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/imgs/motion_tracking_5800.gif" width="400px" /&gt; 
&lt;h1&gt;ASAP delta action model training&lt;/h1&gt; 
&lt;p&gt;Note that the only difference between the delta action model training and naive motion tracking training is that delta action model needs a motion file with extra keyname &lt;code&gt;"action"&lt;/code&gt; in the motion file, so that the resulting RL policy we are training is able to use the delta action model to &lt;code&gt;"control the robot"&lt;/code&gt; to match the real-world/sim2sim motions.&lt;/p&gt; 
&lt;h2&gt;Train delta action model&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;python humanoidverse/train_agent.py \                                                                                   
  +simulator=isaacgym \
  +exp=train_delta_a_open_loop \
  +domain_rand=NO_domain_rand \
  +rewards=motion_tracking/delta_a/reward_delta_a_openloop \
  +robot=g1/g1_29dof_anneal_23dof \
  +terrain=terrain_locomotion_plane \
  +obs=delta_a/open_loop \
  num_envs=5000 \
  project_name=DeltaA_Training \
  experiment_name=openloopDeltaA_training \
  robot.motion.motion_file="&amp;lt;PATH_TO_YOUR_MOTION_FILE_WITH_ACTION_KEYNAME&amp;gt;" \
  env.config.max_episode_length_s=1.0 \
  rewards.reward_scales.penalty_minimal_action_norm=-0.1 \
  +device=cuda:0 \
  env.config.resample_motion_when_training=True \
  env.config.resample_time_interval_s=10000
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Use delta action model for policy finetuning&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;HYDRA_FULL_ERROR=1 \
python humanoidverse/train_agent.py \
+simulator=isaacgym \
+exp=train_delta_a_closed_loop \
algo.config.policy_checkpoint='&amp;lt;PATH_TO_YOUR_DELTA_A_MODEL&amp;gt;' \
+domain_rand=NO_domain_rand_finetune_with_deltaA \
+rewards=motion_tracking/reward_motion_tracking_dm_simfinetuning \
+robot=g1/g1_29dof_anneal_23dof \
+terrain=terrain_locomotion_plane \
+obs=delta_a/train_policy_with_delta_a \
num_envs=4096 \
project_name=DeltaA_Finetune \
experiment_name=finetune_with_deltaA \
robot.motion.motion_file="&amp;lt;PATH_TO_YOUR_MOTION_FILE&amp;gt;" \
+opt=wandb \
env.config.add_extra_action=True \
+checkpoint="&amp;lt;PATH_TO_YOUR_POLICY_TO_BE_FINETUNED&amp;gt;" \
domain_rand.push_robots=False \
env.config.noise_to_initial_level=1 \
rewards.reward_penalty_curriculum=True \
+device=cuda:0 \
algo.config.save_interval=5 \
algo.config.num_learning_iterations=1000 

&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;Motion Retargeting to Any Humanoid&lt;/h1&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] Here we share a generic humanoid motion retargeting pipeline to any humanoid from &lt;a href="https://github.com/ZhengyiLuo/PHC"&gt;PHC&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] We have provided all the SMPL motions (&lt;code&gt;ASAP/humanoidverse/data/motions/raw_tairantestbed_smpl&lt;/code&gt;) and retargtted G1 motions (&lt;code&gt;ASAP/humanoidverse/data/motions/g1_29dof_anneal_23dof/TairanTestbed/singles&lt;/code&gt;) used in the ASAP paper in this codebase. If you are interested in using these motions G1, you can ignore this section. If you are interested in retargeting other humanoids or other motions, you can follow the steps below to prepare the SMPL shapes and motions.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;It has three steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;SMPL Shape preparation&lt;/li&gt; 
 &lt;li&gt;SMPL Motion preparation&lt;/li&gt; 
 &lt;li&gt;Robot XML and Motion Config preparation&lt;/li&gt; 
 &lt;li&gt;Humanoid-SMPL shape fitting&lt;/li&gt; 
 &lt;li&gt;Humanoid-SMPL motion retargeting&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;1. SMPL Shape preparation&lt;/h2&gt; 
&lt;p&gt;Download &lt;a href="https://download.is.tue.mpg.de/download.php?domain=smpl&amp;amp;sfile=SMPL_python_v.1.1.0.zip"&gt;v1.1.0 SMPL files with pkl format&lt;/a&gt; and put it under &lt;code&gt;humanoidverse/data/smpl/&lt;/code&gt;, and you should have:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;|-- ASAP
    |-- humanoidverse
        |-- data
            |-- smpl
                |-- SMPL_python_v.1.1.0.zip
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then &lt;code&gt;cd ASAP/humanoidverse/data/smpl/&lt;/code&gt; and &lt;code&gt;unzip SMPL_python_v.1.1.0.zip&lt;/code&gt;, after some copying and moving, you should have:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;|-- ASAP
    |-- humanoidverse
        |-- data
            |-- smpl
                |-- SMPL_python_v.1.1.0
                |-- models
                    |-- basicmodel_f_lbs_10_207_0_v1.1.0.pkl
                    |-- basicmodel_m_lbs_10_207_0_v1.1.0.pkl
                    |-- basicmodel_neutral_lbs_10_207_0_v1.1.0.pkl
                |-- smpl_webuser
                |-- ...

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Rename these three pkl files and move it under smpl like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;|-- ASAP
    |-- humanoidverse
        |-- data
            |-- smpl
                |-- SMPL_FEMALE.pkl
                |-- SMPL_MALE.pkl
                |-- SMPL_NEUTRAL.pkl
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;2. SMPL Motion preparation (AMASS)&lt;/h2&gt; 
&lt;p&gt;Download &lt;a href="https://amass.is.tue.mpg.de/index.html"&gt;AMASS Dataset&lt;/a&gt; with &lt;code&gt;SMPL + H G format&lt;/code&gt; and put it under &lt;code&gt;humanoidverse/data/motions/AMASS/AMASS_Complete/&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;|-- ASAP
    |-- humanoidverse
        |-- data
            |-- AMASS
                |-- AMASS_Complete
                    |-- ACCAD.tar.bz2
                    |-- BMLhandball.tar.bz2
                    |-- BMLmovi.tar.bz2
                    |-- BMLrub.tar
                    |-- CMU.tar.bz2
                    |-- ...
                    |-- Transitions.tar.bz2
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;And then cd ASAP/humanoidverse/data/motions/AMASS/AMASS_Complete/ and extract all the motion files by running:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;for file in *.tar.bz2; do
    tar -xvjf "$file"
done
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then you should have:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;|-- ASAP
    |-- humanoidverse
        |-- data
            |-- AMASS
                |-- AMASS_Complete
                    |-- ACCAD
                    |-- BioMotionLab_NTroje
                    |-- BMLhandball
                    |-- BMLmovi
                    |-- CMU
                    |-- ...
                    |-- Transitions
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;3. Robot XML and Motion Config preparation&lt;/h2&gt; 
&lt;p&gt;Make sure you have robot xml and meshes ready at (G1 as example) &lt;code&gt;humanoidverse/data/robots/g1/g1_29dof_anneal_23dof_fitmotionONLY.xml&lt;/code&gt; And add your config for the robot motion in &lt;code&gt;humanoidverse/config/robot/g1_29dof_anneal_23dof.yaml&lt;/code&gt; with like the following. Remember to link the xml path in the config.&lt;/p&gt; 
&lt;h2&gt;4. Humanoid-SMPL shape fitting&lt;/h2&gt; 
&lt;p&gt;Run the following command to fit the SMPL shape to the humanoid.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python scripts/data_process/fit_smpl_shape.py +robot=g1/g1_29dof_anneal_23dof
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;And you should have you shape file located at &lt;code&gt;humanoidverse/data/shape/g1_29dof_anneal_23dof/shape_optimized_v1.pkl&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;If you want to visualize the shape, you can run with flag &lt;code&gt;+vis=True&lt;/code&gt;, then you can have visualization of the fitted SMPL body shape and the humanoid body keypoints like this shape. The blue is the humanoid body keypoints and the orange is the fitted SMPL body keypoint. You can tune the &lt;code&gt;robot motion&lt;/code&gt; in &lt;code&gt;humanoidverse/config/robot/g1_29dof_anneal_23dof.yaml&lt;/code&gt; to adjust the correspondence, extend links lengths to get better fitted SMPL shape.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/imgs/g1_29dof_anneal_23dof_shape.png" width="400px" /&gt; 
&lt;h2&gt;5. Humanoid-SMPL motion retargeting&lt;/h2&gt; 
&lt;p&gt;Run the following command to retarget the motion to the humanoid.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python scripts/data_process/fit_smpl_motion.py +robot=g1/g1_29dof_anneal_23dof
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Visualize motion&lt;/h3&gt; 
&lt;p&gt;Run&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python scripts/vis/vis_q_mj.py +robot=g1/g1_29dof_anneal_23dof +visualize_motion_file="humanoidverse/data/motions/g1_29dof_anneal_23dof/TairanTestbed/singles/0-motions_raw_tairantestbed_smpl_video_side_jump_level4_filter_amass.pkl"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To test, and you should have you one single motion file located at &lt;code&gt;humanoidverse/data/motions/g1_29dof_anneal_23dof/TairanTestbed/singles/0-motions_raw_tairantestbed_smpl_video_side_jump_level4_filter_amass.pkl&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;If you want to visualize the motion, you can run&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python scripts/vis/vis_q_mj.py +robot=g1/g1_29dof_anneal_23dof +visualize_motion_file="humanoidverse/data/motions/g1_29dof_anneal_23dof/TairanTestbed/singles/0-motions_raw_tairantestbed_smpl_video_side_jump_level4_filter_amass.pkl"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You should have&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/imgs/g1_29dof_anneal_23dof_motion.gif" width="400px" /&gt; 
&lt;h1&gt;Sim2Sim/Sim2Real&lt;/h1&gt; 
&lt;h2&gt;Environment Setup&lt;/h2&gt; 
&lt;p&gt;Env Installation:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;mamba create -n asap_deploy python=3.10
mamba activate asap_deploy
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Install ros2-python&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# this adds the conda-forge channel to the new created environment configuration 
conda config --env --add channels conda-forge
# and the robostack channel
conda config --env --add channels robostack-staging
# remove the defaults channel just in case, this might return an error if it is not in the list which is ok
conda config --env --remove channels defaults
# install the ros2-python package
conda install ros-humble-desktop
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Test Ros2Installation:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;rviz2
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You should see the UI like this:&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/imgs/rviz.png" width="400px" /&gt; 
&lt;p&gt;Install Unitree SDK&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone git@github.com:unitreerobotics/unitree_sdk2_python.git
cd unitree_sdk2_python
pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;minor issue to fix:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install --upgrade numpy scipy
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Sim2Sim&lt;/h2&gt; 
&lt;p&gt;start the simulation in the sim2real folder:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python sim_env/base_sim.py --config=config/g1_29dof_hist.yaml
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;in another terminal, start the policy:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python rl_policy/deepmimic_dec_loco_height.py --config=config/g1_29dof_hist.yaml --loco_model_path=./models/dec_loco/20250109_231507-noDR_rand_history_loco_stand_height_noise-decoupled_locomotion-g1_29dof/model_6600.onnx --mimic_model_paths=./models/mimic
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;click to the policy terminal and press &lt;code&gt;]&lt;/code&gt; to activate the locomotion policy&lt;/li&gt; 
 &lt;li&gt;click to the policy terminal and press &lt;code&gt;[&lt;/code&gt; to activate the asap policy (phase-based motion tracking policy)&lt;/li&gt; 
 &lt;li&gt;click to the policy terminal and press &lt;code&gt;;&lt;/code&gt; to switch to the asap policy&lt;/li&gt; 
 &lt;li&gt;press &lt;code&gt;i&lt;/code&gt; to make the robot the initial position&lt;/li&gt; 
 &lt;li&gt;press &lt;code&gt;o&lt;/code&gt; to emergence stop the robot&lt;/li&gt; 
 &lt;li&gt;press &lt;code&gt;9&lt;/code&gt; in mujoco viewer to release the robostack&lt;/li&gt; 
 &lt;li&gt;press &lt;code&gt;=&lt;/code&gt; to switch between tapping and walking for the locomotion policy&lt;/li&gt; 
 &lt;li&gt;press &lt;code&gt;w/a/s/d&lt;/code&gt; to control the linear velocity&lt;/li&gt; 
 &lt;li&gt;press &lt;code&gt;q/e&lt;/code&gt; to control the angular velocity&lt;/li&gt; 
 &lt;li&gt;press &lt;code&gt;z&lt;/code&gt; to set all commands to zero&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;And you should be able to play around with some checkpoints from the ASAP paper. Have fun!&lt;/p&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/imgs/asap-sim2sim-clip0-ezgif.com-video-to-gif-converter.gif" width="300px" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/imgs/asap-sim2sim-clip1-ezgif.com-video-to-gif-converter.gif" width="300px" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/imgs/asap-sim2sim-clip3-ezgif.com-video-to-gif-converter.gif" width="300px" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/imgs/asap-sim2sim-clip4-ezgif.com-video-to-gif-converter.gif" width="300px" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/imgs/asap-sim2sim-clip5-ezgif.com-video-to-gif-converter.gif" width="300px" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/imgs/asap-sim2sim-clip6-ezgif.com-video-to-gif-converter.gif" width="300px" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;!-- Replace gif1.gif ... gif6.gif with your actual gif filenames and optionally add captions below each if desired --&gt; 
&lt;h2&gt;Sim2Real&lt;/h2&gt; 
&lt;p&gt;&lt;code&gt;Note from Tairan&lt;/code&gt;: make sure to make the G1 robot to 29dof following this &lt;a href="https://support.unitree.com/home/en/G1_developer/waist_fastener"&gt;doc&lt;/a&gt; and restart the robot after waist unlocking. If you don't know how to log into the Unitree Explore APP, contact unitree support.&lt;/p&gt; 
&lt;p&gt;Enter Low-Level for g1&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Open humanoid and wait until the head blue light is constantly on&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;L2+R2&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;L2+A&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;L2+B&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Connect PC to the G1 by ethernet cable and configure the network following &lt;a href="https://support.unitree.com/home/en/G1_developer/quick_development"&gt;this document&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Before starting the policy, modify the &lt;code&gt;config/g1_29dof_hist.yaml&lt;/code&gt; to set &lt;code&gt;INTERFACE&lt;/code&gt; to &lt;code&gt;eth0&lt;/code&gt; (if you are using linux), basically the network interface that you are using to connect to the robot with your PC's IP shown as &lt;code&gt;192.168.123.xxx&lt;/code&gt; in &lt;code&gt;ifconfig&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;start the policy:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python rl_policy/deepmimic_dec_loco_height.py --config=config/g1_29dof_hist.yaml --loco_model_path=./models/dec_loco/20250109_231507-noDR_rand_history_loco_stand_height_noise-decoupled_locomotion-g1_29dof/model_6600.onnx --mimic_model_paths=./models/mimic
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;click to the policy terminal and press &lt;code&gt;]&lt;/code&gt; to activate the locomotion policy&lt;/li&gt; 
 &lt;li&gt;click to the policy terminal and press &lt;code&gt;[&lt;/code&gt; to activate the asap policy (phase-based motion tracking policy)&lt;/li&gt; 
 &lt;li&gt;click to the policy terminal and press &lt;code&gt;;&lt;/code&gt; to switch to the asap policy&lt;/li&gt; 
 &lt;li&gt;press &lt;code&gt;i&lt;/code&gt; to make the robot the initial position&lt;/li&gt; 
 &lt;li&gt;press &lt;code&gt;o&lt;/code&gt; to emergence stop the robot&lt;/li&gt; 
 &lt;li&gt;press &lt;code&gt;9&lt;/code&gt; in mujoco viewer to release the robostack&lt;/li&gt; 
 &lt;li&gt;press &lt;code&gt;=&lt;/code&gt; to switch between tapping and walking for the locomotion policy&lt;/li&gt; 
 &lt;li&gt;press &lt;code&gt;w/a/s/d&lt;/code&gt; to control the linear velocity&lt;/li&gt; 
 &lt;li&gt;press &lt;code&gt;q/e&lt;/code&gt; to control the angular velocity&lt;/li&gt; 
 &lt;li&gt;press &lt;code&gt;z&lt;/code&gt; to set all commands to zero&lt;/li&gt; 
 &lt;li&gt;press &lt;code&gt;o&lt;/code&gt; to emergence stop the robot&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;‼️Alert &amp;amp; Disclaimer&lt;/h3&gt; 
&lt;p&gt;Deploying these models on physical hardware can be hazardous. Unless you have deep sim‑to‑real expertise and robust safety protocols, we strongly advise against running the model on real robots. These models are supplied for research use only, and we disclaim all responsibility for any harm, loss, or malfunction arising from their deployment.&lt;/p&gt; 
&lt;h3&gt;Demo code to collect real-world data&lt;/h3&gt; 
&lt;p&gt;We provide a demo code to collect real-world data in the &lt;code&gt;sim2real/rl_policy/listener_deltaa.py&lt;/code&gt; file. Since MoCap setup is hard to transfer across different robots/labs, we hope this code can help you to collect data for your own experiments. Contact us (&lt;a href="mailto:tairanh@andrew.cmu.edu"&gt;tairanh@andrew.cmu.edu&lt;/a&gt;) if you have any questions.&lt;/p&gt; 
&lt;h1&gt;Citation&lt;/h1&gt; 
&lt;p&gt;If you find our work useful, please consider citing us!&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@article{he2025asap,
  title={ASAP: Aligning Simulation and Real-World Physics for Learning Agile Humanoid Whole-Body Skills},
  author={He, Tairan and Gao, Jiawei and Xiao, Wenli and Zhang, Yuanhang and Wang, Zi and Wang, Jiashun and Luo, Zhengyi and He, Guanqi and Sobanbabu, Nikhil and Pan, Chaoyi and Yi, Zeji and Qu, Guannan and Kitani, Kris and Hodgins, Jessica and Fan, Linxi "Jim" and Zhu, Yuke and Liu, Changliu and Shi, Guanya},
  journal={arXiv preprint arXiv:2502.01143},
  year={2025}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;License&lt;/h1&gt; 
&lt;p&gt;This project is licensed under the MIT License - see the &lt;a href="https://raw.githubusercontent.com/LeCAR-Lab/ASAP/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>lipku/LiveTalking</title>
      <link>https://github.com/lipku/LiveTalking</link>
      <description>&lt;p&gt;Real time interactive streaming digital human&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/lipku/LiveTalking/main/README-EN.md"&gt;English&lt;/a&gt; | 中文版&lt;br /&gt; 实时交互流式数字人，实现音视频同步对话。基本可以达到商用效果 &lt;a href="https://www.bilibili.com/video/BV1scwBeyELA/"&gt;wav2lip效果&lt;/a&gt; | &lt;a href="https://www.bilibili.com/video/BV1G1421z73r/"&gt;ernerf效果&lt;/a&gt; | &lt;a href="https://www.bilibili.com/video/BV1gm421N7vQ/"&gt;musetalk效果&lt;/a&gt;&lt;br /&gt; 国内镜像地址:&lt;a href="https://gitee.com/lipku/LiveTalking"&gt;https://gitee.com/lipku/LiveTalking&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;为避免与3d数字人混淆，原项目metahuman-stream改名为livetalking，原有链接地址继续可用&lt;/h2&gt; 
&lt;h2&gt;News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;2024.12.8 完善多并发，显存不随并发数增加&lt;/li&gt; 
 &lt;li&gt;2024.12.21 添加wav2lip、musetalk模型预热，解决第一次推理卡顿问题。感谢&lt;a href="https://github.com/heimaojinzhangyz"&gt;@heimaojinzhangyz&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;2024.12.28 添加数字人模型Ultralight-Digital-Human。 感谢&lt;a href="https://github.com/lijihua2017"&gt;@lijihua2017&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;2025.2.7 添加fish-speech tts&lt;/li&gt; 
 &lt;li&gt;2025.2.21 添加wav2lip256开源模型 感谢@不蠢不蠢&lt;/li&gt; 
 &lt;li&gt;2025.3.2 添加腾讯语音合成服务&lt;/li&gt; 
 &lt;li&gt;2025.3.16 支持mac gpu推理，感谢&lt;a href="https://github.com/GcsSloop"&gt;@GcsSloop&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;2025.5.1 精简运行参数，ernerf模型移至git分支ernerf-rtmp&lt;/li&gt; 
 &lt;li&gt;2025.6.7 添加虚拟摄像头输出&lt;/li&gt; 
 &lt;li&gt;2025.7.5 添加豆包语音合成, 感谢&lt;a href="https://github.com/ELK-milu"&gt;@ELK-milu&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;2025.7.26 支持musetalk v1.5版本&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;支持多种数字人模型: ernerf、musetalk、wav2lip、Ultralight-Digital-Human&lt;/li&gt; 
 &lt;li&gt;支持声音克隆&lt;/li&gt; 
 &lt;li&gt;支持数字人说话被打断&lt;/li&gt; 
 &lt;li&gt;支持全身视频拼接&lt;/li&gt; 
 &lt;li&gt;支持webrtc、虚拟摄像头输出&lt;/li&gt; 
 &lt;li&gt;支持动作编排：不说话时播放自定义视频&lt;/li&gt; 
 &lt;li&gt;支持多并发&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;1. Installation&lt;/h2&gt; 
&lt;p&gt;Tested on Ubuntu 24.04, Python3.10, Pytorch 2.5.0 and CUDA 12.4&lt;/p&gt; 
&lt;h3&gt;1.1 Install dependency&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;conda create -n nerfstream python=3.10
conda activate nerfstream
#如果cuda版本不为12.4(运行nvidia-smi确认版本)，根据&amp;lt;https://pytorch.org/get-started/previous-versions/&amp;gt;安装对应版本的pytorch 
conda install pytorch==2.5.0 torchvision==0.20.0 torchaudio==2.5.0 pytorch-cuda=12.4 -c pytorch -c nvidia
pip install -r requirements.txt
#如果需要训练ernerf模型，安装下面的库
# pip install "git+https://github.com/facebookresearch/pytorch3d.git"
# pip install tensorflow-gpu==2.8.0
# pip install --upgrade "protobuf&amp;lt;=3.20.1"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;安装常见问题&lt;a href="https://livetalking-doc.readthedocs.io/zh-cn/latest/faq.html"&gt;FAQ&lt;/a&gt;&lt;br /&gt; linux cuda环境搭建可以参考这篇文章 &lt;a href="https://zhuanlan.zhihu.com/p/674972886"&gt;https://zhuanlan.zhihu.com/p/674972886&lt;/a&gt;&lt;br /&gt; 视频连不上解决方法 &lt;a href="https://mp.weixin.qq.com/s/MVUkxxhV2cgMMHalphr2cg"&gt;https://mp.weixin.qq.com/s/MVUkxxhV2cgMMHalphr2cg&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;2. Quick Start&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;下载模型&lt;br /&gt; 夸克云盘&lt;a href="https://pan.quark.cn/s/83a750323ef0"&gt;https://pan.quark.cn/s/83a750323ef0&lt;/a&gt;&lt;br /&gt; GoogleDriver &lt;a href="https://drive.google.com/drive/folders/1FOC_MD6wdogyyX_7V1d4NDIO7P9NlSAJ?usp=sharing"&gt;https://drive.google.com/drive/folders/1FOC_MD6wdogyyX_7V1d4NDIO7P9NlSAJ?usp=sharing&lt;/a&gt;&lt;br /&gt; 将wav2lip256.pth拷到本项目的models下, 重命名为wav2lip.pth;&lt;br /&gt; 将wav2lip256_avatar1.tar.gz解压后整个文件夹拷到本项目的data/avatars下&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;运行&lt;br /&gt; python app.py --transport webrtc --model wav2lip --avatar_id wav2lip256_avatar1&lt;br /&gt; &lt;font color="red"&gt;服务端需要开放端口 tcp:8010; udp:1-65536 &lt;/font&gt;&lt;br /&gt; 客户端可以选用以下两种方式:&lt;br /&gt; (1)用浏览器打开&lt;a href="http://serverip:8010/webrtcapi.html"&gt;http://serverip:8010/webrtcapi.html&lt;/a&gt; , 先点‘start',播放数字人视频；然后在文本框输入任意文字，提交。数字人播报该段文字&lt;br /&gt; (2)用客户端方式, 下载地址&lt;a href="https://pan.quark.cn/s/d7192d8ac19b"&gt;https://pan.quark.cn/s/d7192d8ac19b&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;快速体验&lt;br /&gt; &lt;a href="https://www.compshare.cn/images/4458094e-a43d-45fe-9b57-de79253befe4?referral_code=3XW3852OBmnD089hMMrtuU&amp;amp;ytag=GPU_GitHub_livetalking"&gt;https://www.compshare.cn/images/4458094e-a43d-45fe-9b57-de79253befe4?referral_code=3XW3852OBmnD089hMMrtuU&amp;amp;ytag=GPU_GitHub_livetalking&lt;/a&gt; 用该镜像创建实例即可运行成功&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;如果访问不了huggingface，在运行前&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;export HF_ENDPOINT=https://hf-mirror.com
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;3. More Usage&lt;/h2&gt; 
&lt;p&gt;使用说明: &lt;a href="https://livetalking-doc.readthedocs.io/"&gt;https://livetalking-doc.readthedocs.io/&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;4. Docker Run&lt;/h2&gt; 
&lt;p&gt;不需要前面的安装，直接运行。&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;docker run --gpus all -it --network=host --rm registry.cn-zhangjiakou.aliyuncs.com/codewithgpu3/lipku-livetalking:toza2irpHZ
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;代码在/root/livetalking，先git pull拉一下最新代码，然后执行命令同第2、3步&lt;/p&gt; 
&lt;p&gt;提供如下镜像&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;autodl镜像: &lt;a href="https://www.codewithgpu.com/i/lipku/livetalking/base"&gt;https://www.codewithgpu.com/i/lipku/livetalking/base&lt;/a&gt;&lt;br /&gt; &lt;a href="https://livetalking-doc.readthedocs.io/en/latest/autodl/README.html"&gt;autodl教程&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ucloud镜像: &lt;a href="https://www.compshare.cn/images/4458094e-a43d-45fe-9b57-de79253befe4?referral_code=3XW3852OBmnD089hMMrtuU&amp;amp;ytag=GPU_GitHub_livetalking"&gt;https://www.compshare.cn/images/4458094e-a43d-45fe-9b57-de79253befe4?referral_code=3XW3852OBmnD089hMMrtuU&amp;amp;ytag=GPU_GitHub_livetalking&lt;/a&gt;&lt;br /&gt; 可以开放任意端口，不需要另外部署srs服务.&lt;br /&gt; &lt;a href="https://livetalking-doc.readthedocs.io/en/latest/ucloud/ucloud.html"&gt;ucloud教程&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;5. 性能&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;性能主要跟cpu和gpu相关，每路视频压缩需要消耗cpu，cpu性能与视频分辨率正相关；每路口型推理跟gpu性能相关。&lt;/li&gt; 
 &lt;li&gt;不说话时的并发数跟cpu相关，同时说话的并发数跟gpu相关。&lt;/li&gt; 
 &lt;li&gt;后端日志inferfps表示显卡推理帧率，finalfps表示最终推流帧率。两者都要在25以上才能实时。如果inferfps在25以上，finalfps达不到25表示cpu性能不足。&lt;/li&gt; 
 &lt;li&gt;实时推理性能&lt;/li&gt; 
&lt;/ul&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;模型&lt;/th&gt; 
   &lt;th align="left"&gt;显卡型号&lt;/th&gt; 
   &lt;th align="left"&gt;fps&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;wav2lip256&lt;/td&gt; 
   &lt;td align="left"&gt;3060&lt;/td&gt; 
   &lt;td align="left"&gt;60&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;wav2lip256&lt;/td&gt; 
   &lt;td align="left"&gt;3080Ti&lt;/td&gt; 
   &lt;td align="left"&gt;120&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;musetalk&lt;/td&gt; 
   &lt;td align="left"&gt;3080Ti&lt;/td&gt; 
   &lt;td align="left"&gt;42&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;musetalk&lt;/td&gt; 
   &lt;td align="left"&gt;3090&lt;/td&gt; 
   &lt;td align="left"&gt;45&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;musetalk&lt;/td&gt; 
   &lt;td align="left"&gt;4090&lt;/td&gt; 
   &lt;td align="left"&gt;72&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;wav2lip256显卡3060以上即可，musetalk需要3080Ti以上。&lt;/p&gt; 
&lt;h2&gt;6. 商业版&lt;/h2&gt; 
&lt;p&gt;提供如下扩展功能，适用于对开源项目已经比较熟悉，需要扩展产品功能的用户&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;高清wav2lip模型&lt;/li&gt; 
 &lt;li&gt;完全语音交互，数字人回答过程中支持通过唤醒词或者按钮打断提问&lt;/li&gt; 
 &lt;li&gt;实时同步字幕，给前端提供数字人每句话播报开始、结束事件&lt;/li&gt; 
 &lt;li&gt;每个连接可以指定对应avatar和音色，avatar图片加载加速&lt;/li&gt; 
 &lt;li&gt;动作编排：不说话时动作、唤醒时动作、思考时动作&lt;/li&gt; 
 &lt;li&gt;支持不限时长的数字人形象avatar&lt;/li&gt; 
 &lt;li&gt;提供实时音频流输入接口&lt;/li&gt; 
 &lt;li&gt;数字人透明背景，叠加动态背景&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;更多详情&lt;a href="https://livetalking-doc.readthedocs.io/zh-cn/latest/service.html#wav2lip"&gt;https://livetalking-doc.readthedocs.io/zh-cn/latest/service.html#wav2lip&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;7. 声明&lt;/h2&gt; 
&lt;p&gt;基于本项目开发并发布在B站、视频号、抖音等网站上的视频需带上LiveTalking水印和标识，如需去除请联系作者备案授权。&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;如果本项目对你有帮助，帮忙点个star。也欢迎感兴趣的朋友一起来完善该项目.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;知识星球: &lt;a href="https://t.zsxq.com/7NMyO"&gt;https://t.zsxq.com/7NMyO&lt;/a&gt; 沉淀高质量常见问题、最佳实践经验、问题解答&lt;/li&gt; 
 &lt;li&gt;微信公众号：数字人技术&lt;br /&gt; &lt;img src="https://mmbiz.qpic.cn/sz_mmbiz_jpg/l3ZibgueFiaeyfaiaLZGuMGQXnhLWxibpJUS2gfs8Dje6JuMY8zu2tVyU9n8Zx1yaNncvKHBMibX0ocehoITy5qQEZg/640?wxfrom=12&amp;amp;tp=wxpic&amp;amp;usePicPrefetch=1&amp;amp;wx_fmt=jpeg&amp;amp;from=appmsg" alt="" /&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>dreammis/social-auto-upload</title>
      <link>https://github.com/dreammis/social-auto-upload</link>
      <description>&lt;p&gt;自动化上传视频到社交媒体：抖音、小红书、视频号、tiktok、youtube、bilibili&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;social-auto-upload&lt;/h1&gt; 
&lt;p&gt;&lt;code&gt;social-auto-upload&lt;/code&gt; 是一个强大的自动化工具，旨在帮助内容创作者和运营者高效地将视频内容一键发布到多个国内外主流社交媒体平台。 项目实现了对 &lt;code&gt;抖音&lt;/code&gt;、&lt;code&gt;Bilibili&lt;/code&gt;、&lt;code&gt;小红书&lt;/code&gt;、&lt;code&gt;快手&lt;/code&gt;、&lt;code&gt;视频号&lt;/code&gt;、&lt;code&gt;百家号&lt;/code&gt; 以及 &lt;code&gt;TikTok&lt;/code&gt; 等平台的视频上传、定时发布等功能。 结合各平台 &lt;code&gt;uploader&lt;/code&gt; 模块，您可以轻松配置和扩展支持的平台，并通过示例脚本快速上手。&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/dreammis/social-auto-upload/main/media/show/tkupload.gif" alt="tiktok show" width="800" /&gt; 
&lt;h2&gt;目录&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/dreammis/social-auto-upload/main/#%F0%9F%92%A1%E5%8A%9F%E8%83%BD%E7%89%B9%E6%80%A7"&gt;💡 功能特性&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/dreammis/social-auto-upload/main/#%F0%9F%9A%80%E6%94%AF%E6%8C%81%E7%9A%84%E5%B9%B3%E5%8F%B0"&gt;🚀 支持的平台&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/dreammis/social-auto-upload/main/#%F0%9F%92%BE%E5%AE%89%E8%A3%85%E6%8C%87%E5%8D%97"&gt;💾 安装指南&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/dreammis/social-auto-upload/main/#%F0%9F%8F%81%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B"&gt;🏁 快速开始&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/dreammis/social-auto-upload/main/#%F0%9F%90%87%E9%A1%B9%E7%9B%AE%E8%83%8C%E6%99%AF"&gt;🐇 项目背景&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/dreammis/social-auto-upload/main/#%F0%9F%93%83%E8%AF%A6%E7%BB%86%E6%96%87%E6%A1%A3"&gt;📃 详细文档&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/dreammis/social-auto-upload/main/#%F0%9F%90%BE%E4%BA%A4%E6%B5%81%E4%B8%8E%E6%94%AF%E6%8C%81"&gt;🐾 交流与支持&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/dreammis/social-auto-upload/main/#%F0%9F%A4%9D%E8%B4%A1%E7%8C%AE%E6%8C%87%E5%8D%97"&gt;🤝 贡献指南&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/dreammis/social-auto-upload/main/#%F0%9F%93%9C%E8%AE%B8%E5%8F%AF%E8%AF%81"&gt;📜 许可证&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/dreammis/social-auto-upload/main/#%E2%AD%90Star-History"&gt;⭐ Star History&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;💡功能特性&lt;/h2&gt; 
&lt;h3&gt;已支持平台&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;国内平台&lt;/strong&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; 抖音&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; 视频号&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Bilibili&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; 小红书&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; 快手&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; 百家号&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;国外平台&lt;/strong&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; TikTok&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;核心功能&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; 定时上传 (Cron Job / Scheduled Upload)&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Cookie 管理 (部分实现，持续优化中)&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; 国外平台 Proxy 设置 (部分实现)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;计划支持与开发中&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;平台扩展&lt;/strong&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" disabled /&gt; YouTube&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;功能增强&lt;/strong&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; 更易用的版本 (GUI / CLI 交互优化)&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; API 封装&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Docker 部署&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" disabled /&gt; 自动化上传 (更智能的调度策略)&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" disabled /&gt; 多线程/异步上传优化&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Slack/消息推送通知&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;🚀支持的平台&lt;/h2&gt; 
&lt;p&gt;本项目通过各平台对应的 &lt;code&gt;uploader&lt;/code&gt; 模块实现视频上传功能。您可以在 &lt;code&gt;examples&lt;/code&gt; 目录下找到各个平台的使用示例脚本。&lt;/p&gt; 
&lt;p&gt;每个示例脚本展示了如何配置和调用相应的 uploader。&lt;/p&gt; 
&lt;h2&gt;💾安装指南&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;克隆项目&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/dreammis/social-auto-upload.git
cd social-auto-upload
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;安装依赖&lt;/strong&gt;: 建议在虚拟环境中安装依赖。&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;conda create -n social-auto-upload python=3.10
conda activate social-auto-upload
# 挂载清华镜像 or 命令行代理
pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;安装 Playwright 浏览器驱动&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;playwright install chromium firefox
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;根据您的需求，至少需要安装 &lt;code&gt;chromium&lt;/code&gt;。&lt;code&gt;firefox&lt;/code&gt; 主要用于 TikTok 上传（旧版）。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;修改配置文件&lt;/strong&gt;: 复制 &lt;code&gt;conf.example.py&lt;/code&gt; 并重命名为 &lt;code&gt;conf.py&lt;/code&gt;。 在 &lt;code&gt;conf.py&lt;/code&gt; 中，您需要配置以下内容：&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;LOCAL_CHROME_PATH&lt;/code&gt;: 本地 Chrome 浏览器的路径，比如 &lt;code&gt;C:\Program Files\Google\Chrome\Application\chrome.exe&lt;/code&gt; 保存。&lt;/li&gt; 
  &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;临时解决方案&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;需要在根目录创建 &lt;code&gt;cookiesFile&lt;/code&gt; 和 &lt;code&gt;videoFile&lt;/code&gt; 两个文件夹，分别是 存储cookie文件 和 存储上传文件 的文件夹&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;配置数据库&lt;/strong&gt;: 如果 db/database.db 文件不存在，您可以运行以下命令来初始化数据库：&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cd db
python createTable.py
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;此命令将初始化 SQLite 数据库。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;启动后端项目&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;python sau_backend.py
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;后端项目将在 &lt;code&gt;http://localhost:5409&lt;/code&gt; 启动。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;启动前端项目&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cd sau_frontend
npm install
npm run dev
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;前端项目将在 &lt;code&gt;http://localhost:5173&lt;/code&gt; 启动，在浏览器中打开此链接即可访问。&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;非程序员用户可以参考：&lt;a href="https://juejin.cn/post/7372114027840208911"&gt;新手级教程&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;🏁快速开始&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;准备 Cookie&lt;/strong&gt;: 大多数平台需要登录后的 Cookie 信息才能进行操作。请参照 examples 目录下各 &lt;code&gt;get_xxx_cookie.py&lt;/code&gt; 脚本（例如 get_douyin_cookie.py, get_ks_cookie.py）的说明，运行脚本以生成并保存 Cookie 文件（通常在 &lt;code&gt;cookies/[PLATFORM]_uploader/account.json&lt;/code&gt;）。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;准备视频文件&lt;/strong&gt;: 将需要上传的视频文件（通常为 &lt;code&gt;.mp4&lt;/code&gt; 格式）放置在 videos 目录下。 部分平台支持视频封面，可以将封面图片（例如 &lt;code&gt;.png&lt;/code&gt; 格式，与视频同名）也放在此目录。 如果需要上传标题及标签，请在视频文件旁边创建一个同名的 &lt;code&gt;.txt&lt;/code&gt; 文件，内容为标题和标签，以换行分隔。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;修改并运行示例脚本&lt;/strong&gt;: 打开 examples 目录中您想使用的平台的上传脚本（例如 upload_video_to_douyin.py）。&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;根据脚本内的注释和说明，确认 Cookie 文件路径、视频文件路径等配置是否正确。&lt;/li&gt; 
   &lt;li&gt;您可以修改脚本以适应您的具体需求，例如批量上传、自定义标题、标签等。&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;执行上传&lt;/strong&gt;: 运行修改后的示例脚本，例如：&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;python examples/upload_video_to_douyin.py
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;🐇项目背景&lt;/h2&gt; 
&lt;p&gt;该项目最初是我个人用于自动化管理社交媒体视频发布的工具。我的主要发布策略是提前一天设置定时发布，因此项目中很多定时发布相关的逻辑是基于“第二天”的时间进行计算的。&lt;/p&gt; 
&lt;p&gt;如果您需要立即发布或其他定制化的发布策略，欢迎研究源码或在社区提问。&lt;/p&gt; 
&lt;h2&gt;📃详细文档&lt;/h2&gt; 
&lt;p&gt;更详细的文档和说明，请查看：&lt;a href="https://sap-doc.nasdaddy.com/"&gt;social-auto-upload 官方文档&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;🐾交流与支持&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.buymeacoffee.com/hysn2001m"&gt;☕ Donate as u like&lt;/a&gt; - 如果您觉得这个项目对您有帮助，可以考虑赞助。&lt;/p&gt; 
&lt;p&gt;如果您也是独立开发者、技术爱好者，对 #技术变现 #AI创业 #跨境电商 #自动化工具 #视频创作 等话题感兴趣，欢迎加入社群交流。&lt;/p&gt; 
&lt;h3&gt;Creator&lt;/h3&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt;
   &lt;td align="center"&gt; &lt;a href="https://sap-doc.nasdaddy.com/"&gt; &lt;img src="https://raw.githubusercontent.com/dreammis/social-auto-upload/main/media/mp.jpg" width="200px" alt="NasDaddy公众号" /&gt; &lt;br /&gt; &lt;sub&gt;&lt;b&gt;微信公众号&lt;/b&gt;&lt;/sub&gt; &lt;/a&gt; &lt;br /&gt; &lt;a href="https://github.com/dreammis/social-auto-upload/commits?author=dreammis" title="Code"&gt;💻&lt;/a&gt; &lt;br /&gt; 关注公众号，后台回复 `上传` 获取加群方式 &lt;/td&gt; 
   &lt;td align="center"&gt; &lt;a href="https://sap-doc.nasdaddy.com/"&gt; &lt;img src="https://raw.githubusercontent.com/dreammis/social-auto-upload/main/media/QR.png" width="200px" alt="赞赏码/入群引导" /&gt; &lt;br /&gt; &lt;sub&gt;&lt;b&gt;交流群 (通过公众号获取)&lt;/b&gt;&lt;/sub&gt; &lt;/a&gt; &lt;br /&gt; &lt;a href="https://sap-doc.nasdaddy.com/" title="Documentation"&gt;📖&lt;/a&gt; &lt;br /&gt; 如果您觉得项目有用，可以考虑打赏支持一下 &lt;/td&gt; 
  &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h3&gt;Active Core Team&lt;/h3&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt;
   &lt;td align="center"&gt; &lt;a href="https://leedebug.github.io/"&gt; &lt;img src="https://raw.githubusercontent.com/dreammis/social-auto-upload/main/media/edan-qrcode.png" width="200px" alt="Edan Lee" /&gt; &lt;br /&gt; &lt;sub&gt;&lt;b&gt;Edan Lee&lt;/b&gt;&lt;/sub&gt; &lt;/a&gt; &lt;br /&gt; &lt;a href="https://github.com/dreammis/social-auto-upload/commits?author=LeeDebug" title="Code"&gt;💻&lt;/a&gt; &lt;a href="https://leedebug.github.io/" title="Documentation"&gt;📖&lt;/a&gt; &lt;br /&gt; 封装了 api 接口和 web 前端管理界面 &lt;br /&gt; （请注明来意：进群、学习、企业咨询等） &lt;/td&gt; 
  &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;🤝贡献指南&lt;/h2&gt; 
&lt;p&gt;欢迎各种形式的贡献，包括但不限于：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;提交 Bug报告 和 Feature请求。&lt;/li&gt; 
 &lt;li&gt;改进代码、文档。&lt;/li&gt; 
 &lt;li&gt;分享使用经验和教程。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;如果您希望贡献代码，请遵循以下步骤：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Fork 本仓库。&lt;/li&gt; 
 &lt;li&gt;创建一个新的分支 (&lt;code&gt;git checkout -b feature/YourFeature&lt;/code&gt; 或 &lt;code&gt;bugfix/YourBugfix&lt;/code&gt;)。&lt;/li&gt; 
 &lt;li&gt;提交您的更改 (&lt;code&gt;git commit -m 'Add some feature'&lt;/code&gt;)。&lt;/li&gt; 
 &lt;li&gt;Push到您的分支 (&lt;code&gt;git push origin feature/YourFeature&lt;/code&gt;)。&lt;/li&gt; 
 &lt;li&gt;创建一个 Pull Request。&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;📜许可证&lt;/h2&gt; 
&lt;p&gt;本项目暂时采用 &lt;a href="https://raw.githubusercontent.com/dreammis/social-auto-upload/main/LICENSE"&gt;MIT License&lt;/a&gt; 开源许可证。&lt;/p&gt; 
&lt;h2&gt;⭐Star-History&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;如果这个项目对您有帮助，请给一个 ⭐ Star 以表示支持！&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#dreammis/social-auto-upload&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=dreammis/social-auto-upload&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>paperless-ngx/paperless-ngx</title>
      <link>https://github.com/paperless-ngx/paperless-ngx</link>
      <description>&lt;p&gt;A community-supported supercharged document management system: scan, index and archive all your documents&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href="https://github.com/paperless-ngx/paperless-ngx/actions"&gt;&lt;img src="https://github.com/paperless-ngx/paperless-ngx/workflows/ci/badge.svg?sanitize=true" alt="ci" /&gt;&lt;/a&gt; &lt;a href="https://crowdin.com/project/paperless-ngx"&gt;&lt;img src="https://badges.crowdin.net/paperless-ngx/localized.svg?sanitize=true" alt="Crowdin" /&gt;&lt;/a&gt; &lt;a href="https://docs.paperless-ngx.com"&gt;&lt;img src="https://img.shields.io/github/deployments/paperless-ngx/paperless-ngx/github-pages?label=docs" alt="Documentation Status" /&gt;&lt;/a&gt; &lt;a href="https://codecov.io/gh/paperless-ngx/paperless-ngx"&gt;&lt;img src="https://codecov.io/gh/paperless-ngx/paperless-ngx/branch/main/graph/badge.svg?token=VK6OUPJ3TY" alt="codecov" /&gt;&lt;/a&gt; &lt;a href="https://matrix.to/#/%23paperlessngx%3Amatrix.org"&gt;&lt;img src="https://matrix.to/img/matrix-badge.svg?sanitize=true" alt="Chat on Matrix" /&gt;&lt;/a&gt; &lt;a href="https://demo.paperless-ngx.com"&gt;&lt;img src="https://cronitor.io/badges/ve7ItY/production/W5E_B9jkelG9ZbDiNHUPQEVH3MY.svg?sanitize=true" alt="demo" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://github.com/paperless-ngx/paperless-ngx/blob/main/resources/logo/web/png/White%20logo%20-%20no%20background.png" width="50%" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://github.com/paperless-ngx/paperless-ngx/raw/main/resources/logo/web/png/Black%20logo%20-%20no%20background.png" width="50%" /&gt; 
  &lt;img src="https://github.com/paperless-ngx/paperless-ngx/raw/main/resources/logo/web/png/Black%20logo%20-%20no%20background.png" width="50%" /&gt; 
 &lt;/picture&gt; &lt;/p&gt; 
&lt;!-- omit in toc --&gt; 
&lt;h1&gt;Paperless-ngx&lt;/h1&gt; 
&lt;p&gt;Paperless-ngx is a document management system that transforms your physical documents into a searchable online archive so you can keep, well, &lt;em&gt;less paper&lt;/em&gt;.&lt;/p&gt; 
&lt;p&gt;Paperless-ngx is the official successor to the original &lt;a href="https://github.com/the-paperless-project/paperless"&gt;Paperless&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/jonaswinkler/paperless-ng"&gt;Paperless-ng&lt;/a&gt; projects and is designed to distribute the responsibility of advancing and supporting the project among a team of people. &lt;a href="https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/dev/#community-support"&gt;Consider joining us!&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Thanks to the generous folks at &lt;a href="https://m.do.co/c/8d70b916d462"&gt;DigitalOcean&lt;/a&gt;, a demo is available at &lt;a href="https://demo.paperless-ngx.com"&gt;demo.paperless-ngx.com&lt;/a&gt; using login &lt;code&gt;demo&lt;/code&gt; / &lt;code&gt;demo&lt;/code&gt;. &lt;em&gt;Note: demo content is reset frequently and confidential information should not be uploaded.&lt;/em&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/dev/#features"&gt;Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/dev/#getting-started"&gt;Getting started&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/dev/#contributing"&gt;Contributing&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/dev/#community-support"&gt;Community Support&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/dev/#translation"&gt;Translation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/dev/#feature-requests"&gt;Feature Requests&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/dev/#bugs"&gt;Bugs&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/dev/#related-projects"&gt;Related Projects&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/dev/#important-note"&gt;Important Note&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="right"&gt;This project is supported by:&lt;br /&gt; &lt;a href="https://m.do.co/c/8d70b916d462" style="padding-top: 4px; display: block;"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="https://opensource.nyc3.cdn.digitaloceanspaces.com/attribution/assets/SVG/DO_Logo_horizontal_white.svg" width="140px" /&gt; 
   &lt;source media="(prefers-color-scheme: light)" srcset="https://opensource.nyc3.cdn.digitaloceanspaces.com/attribution/assets/SVG/DO_Logo_horizontal_blue.svg" width="140px" /&gt; 
   &lt;img src="https://opensource.nyc3.cdn.digitaloceanspaces.com/attribution/assets/SVG/DO_Logo_horizontal_black_.svg?sanitize=true" width="140px" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h1&gt;Features&lt;/h1&gt; 
&lt;picture&gt; 
 &lt;source media="(prefers-color-scheme: dark)" srcset="https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/main/docs/assets/screenshots/documents-smallcards-dark.png" /&gt; 
 &lt;source media="(prefers-color-scheme: light)" srcset="https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/main/docs/assets/screenshots/documents-smallcards.png" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/main/docs/assets/screenshots/documents-smallcards.png" /&gt; 
&lt;/picture&gt; 
&lt;p&gt;A full list of &lt;a href="https://docs.paperless-ngx.com/#features"&gt;features&lt;/a&gt; and &lt;a href="https://docs.paperless-ngx.com/#screenshots"&gt;screenshots&lt;/a&gt; are available in the &lt;a href="https://docs.paperless-ngx.com/"&gt;documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Getting started&lt;/h1&gt; 
&lt;p&gt;The easiest way to deploy paperless is &lt;code&gt;docker compose&lt;/code&gt;. The files in the &lt;a href="https://github.com/paperless-ngx/paperless-ngx/tree/main/docker/compose"&gt;&lt;code&gt;/docker/compose&lt;/code&gt; directory&lt;/a&gt; are configured to pull the image from the GitHub container registry.&lt;/p&gt; 
&lt;p&gt;If you'd like to jump right in, you can configure a &lt;code&gt;docker compose&lt;/code&gt; environment with our install script:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;bash -c "$(curl -L https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/main/install-paperless-ngx.sh)"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;More details and step-by-step guides for alternative installation methods can be found in &lt;a href="https://docs.paperless-ngx.com/setup/#installation"&gt;the documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Migrating from Paperless-ng is easy, just drop in the new docker image! See the &lt;a href="https://docs.paperless-ngx.com/setup/#migrating-to-paperless-ngx"&gt;documentation on migrating&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;!-- omit in toc --&gt; 
&lt;h3&gt;Documentation&lt;/h3&gt; 
&lt;p&gt;The documentation for Paperless-ngx is available at &lt;a href="https://docs.paperless-ngx.com/"&gt;https://docs.paperless-ngx.com&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Contributing&lt;/h1&gt; 
&lt;p&gt;If you feel like contributing to the project, please do! Bug fixes, enhancements, visual fixes etc. are always welcome. If you want to implement something big: Please start a discussion about that! The &lt;a href="https://docs.paperless-ngx.com/development/"&gt;documentation&lt;/a&gt; has some basic information on how to get started.&lt;/p&gt; 
&lt;h2&gt;Community Support&lt;/h2&gt; 
&lt;p&gt;People interested in continuing the work on paperless-ngx are encouraged to reach out here on github and in the &lt;a href="https://matrix.to/#/#paperless:matrix.org"&gt;Matrix Room&lt;/a&gt;. If you would like to contribute to the project on an ongoing basis there are multiple &lt;a href="https://github.com/orgs/paperless-ngx/people"&gt;teams&lt;/a&gt; (frontend, ci/cd, etc) that could use your help so please reach out!&lt;/p&gt; 
&lt;h2&gt;Translation&lt;/h2&gt; 
&lt;p&gt;Paperless-ngx is available in many languages that are coordinated on Crowdin. If you want to help out by translating paperless-ngx into your language, please head over to &lt;a href="https://crowdin.com/project/paperless-ngx"&gt;https://crowdin.com/project/paperless-ngx&lt;/a&gt;, and thank you! More details can be found in &lt;a href="https://github.com/paperless-ngx/paperless-ngx/raw/main/CONTRIBUTING.md#translating-paperless-ngx"&gt;CONTRIBUTING.md&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Feature Requests&lt;/h2&gt; 
&lt;p&gt;Feature requests can be submitted via &lt;a href="https://github.com/paperless-ngx/paperless-ngx/discussions/categories/feature-requests"&gt;GitHub Discussions&lt;/a&gt;, you can search for existing ideas, add your own and vote for the ones you care about.&lt;/p&gt; 
&lt;h2&gt;Bugs&lt;/h2&gt; 
&lt;p&gt;For bugs please &lt;a href="https://github.com/paperless-ngx/paperless-ngx/issues"&gt;open an issue&lt;/a&gt; or &lt;a href="https://github.com/paperless-ngx/paperless-ngx/discussions"&gt;start a discussion&lt;/a&gt; if you have questions.&lt;/p&gt; 
&lt;h1&gt;Related Projects&lt;/h1&gt; 
&lt;p&gt;Please see &lt;a href="https://github.com/paperless-ngx/paperless-ngx/wiki/Related-Projects"&gt;the wiki&lt;/a&gt; for a user-maintained list of related projects and software that is compatible with Paperless-ngx.&lt;/p&gt; 
&lt;h1&gt;Important Note&lt;/h1&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Document scanners are typically used to scan sensitive documents like your social insurance number, tax records, invoices, etc. &lt;strong&gt;Paperless-ngx should never be run on an untrusted host&lt;/strong&gt; because information is stored in clear text without encryption. No guarantees are made regarding security (but we do try!) and you use the app at your own risk. &lt;strong&gt;The safest way to run Paperless-ngx is on a local server in your own home with backups in place&lt;/strong&gt;.&lt;/p&gt; 
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>manycore-research/SpatialLM</title>
      <link>https://github.com/manycore-research/SpatialLM</link>
      <description>&lt;p&gt;SpatialLM: Training Large Language Models for Structured Indoor Modeling&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;SpatialLM&lt;/h1&gt; 
&lt;!-- markdownlint-disable first-line-h1 --&gt; 
&lt;!-- markdownlint-disable html --&gt; 
&lt;!-- markdownlint-disable no-duplicate-header --&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/manycore-research/SpatialLM/main/figures/logo_light.png#gh-light-mode-only" width="60%" alt="SpatialLM" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/manycore-research/SpatialLM/main/figures/logo_dark.png#gh-dark-mode-only" width="60%" alt="SpatialLM" /&gt; 
&lt;/div&gt; 
&lt;hr style="margin-top: 0; margin-bottom: 8px;" /&gt; 
&lt;div align="center" style="margin-top: 0; padding-top: 0; line-height: 1;"&gt; 
 &lt;a href="https://manycore-research.github.io/SpatialLM" target="_blank" style="margin: 2px;"&gt;&lt;img alt="Project" src="https://img.shields.io/badge/🌐%20Website-SpatialLM-ffc107?color=42a5f5&amp;amp;logoColor=white" style="display: inline-block; vertical-align: middle;" /&gt;&lt;/a&gt; 
 &lt;a href="https://arxiv.org/abs/2506.07491" target="_blank" style="margin: 2px;"&gt;&lt;img alt="arXiv" src="https://img.shields.io/badge/arXiv-Techreport-b31b1b?logo=arxiv&amp;amp;logoColor=white" style="display: inline-block; vertical-align: middle;" /&gt;&lt;/a&gt; 
 &lt;a href="https://github.com/manycore-research/SpatialLM" target="_blank" style="margin: 2px;"&gt;&lt;img alt="GitHub" src="https://img.shields.io/badge/GitHub-SpatialLM-24292e?logo=github&amp;amp;logoColor=white" style="display: inline-block; vertical-align: middle;" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;div align="center" style="line-height: 1;"&gt; 
 &lt;a href="https://huggingface.co/manycore-research/SpatialLM1.1-Qwen-0.5B" target="_blank" style="margin: 2px;"&gt;&lt;img alt="Hugging Face" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-SpatialLM-ffc107?color=ffc107&amp;amp;logoColor=white" style="display: inline-block; vertical-align: middle;" /&gt;&lt;/a&gt; 
 &lt;a href="https://huggingface.co/datasets/manycore-research/SpatialLM-Testset" target="_blank" style="margin: 2px;"&gt;&lt;img alt="Dataset" src="https://img.shields.io/badge/%F0%9F%A4%97%20Dataset-Testset-ffc107?color=ffc107&amp;amp;logoColor=white" style="display: inline-block; vertical-align: middle;" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;✨ News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;[Jun, 2025] Check out our new models: &lt;a href="https://huggingface.co/manycore-research/SpatialLM1.1-Llama-1B"&gt;SpatialLM1.1-Llama-1B&lt;/a&gt; and &lt;a href="https://huggingface.co/manycore-research/SpatialLM1.1-Qwen-0.5B"&gt;SpatialLM1.1-Qwen-0.5B&lt;/a&gt;, now available on Hugging Face. SpatialLM1.1 doubles the point cloud resolution, incorporates a more powerful point cloud encoder &lt;a href="https://xywu.me/sonata/"&gt;Sonata&lt;/a&gt; and supports detection with user-specified categories.&lt;/li&gt; 
 &lt;li&gt;[Jun, 2025] SpatialLM &lt;a href="https://arxiv.org/abs/2506.07491"&gt;Technical Report&lt;/a&gt; is now on arXiv.&lt;/li&gt; 
 &lt;li&gt;[Mar, 2025] We're excited to release the &lt;a href="https://huggingface.co/manycore-research/SpatialLM-Llama-1B"&gt;SpatialLM-Llama-1B&lt;/a&gt; and &lt;a href="https://huggingface.co/manycore-research/SpatialLM-Qwen-0.5B"&gt;SpatialLM-Qwen-0.5B&lt;/a&gt; on Hugging Face.&lt;/li&gt; 
 &lt;li&gt;[Mar, 2025] Initial release of SpatialLM!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;SpatialLM is a 3D large language model designed to process 3D point cloud data and generate structured 3D scene understanding outputs. These outputs include architectural elements like walls, doors, windows, and oriented object bounding boxes with their semantic categories. Unlike previous methods that require specialized equipment for data collection, SpatialLM can handle point clouds from diverse sources such as monocular video sequences, RGBD images, and LiDAR sensors. This multimodal architecture effectively bridges the gap between unstructured 3D geometric data and structured 3D representations, offering high-level semantic understanding. It enhances spatial reasoning capabilities for applications in embodied robotics, autonomous navigation, and other complex 3D scene analysis tasks.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;video src="https://github.com/user-attachments/assets/c0218d6a-f676-41f8-ae76-bba228866306" poster="figures/cover.png"&gt; 
 &lt;/video&gt; 
 &lt;p&gt;&lt;i&gt;SpatialLM reconstructs 3D layout from a monocular RGB video with MASt3R-SLAM. Results aligned to video with GT cameras for visualization.&lt;/i&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;SpatialLM Models&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;Download&lt;/strong&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;SpatialLM1.1-Llama-1B&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://huggingface.co/manycore-research/SpatialLM1.1-Llama-1B"&gt;🤗 HuggingFace&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;SpatialLM1.1-Qwen-0.5B&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://huggingface.co/manycore-research/SpatialLM1.1-Qwen-0.5B"&gt;🤗 HuggingFace&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;SpatialLM1.0-Llama-1B&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://huggingface.co/manycore-research/SpatialLM-Llama-1B"&gt;🤗 HuggingFace&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;SpatialLM1.0-Qwen-0.5B&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://huggingface.co/manycore-research/SpatialLM-Qwen-0.5B"&gt;🤗 HuggingFace&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;p&gt;Tested with the following environment:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.11&lt;/li&gt; 
 &lt;li&gt;Pytorch 2.4.1&lt;/li&gt; 
 &lt;li&gt;CUDA Version 12.4&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# clone the repository
git clone https://github.com/manycore-research/SpatialLM.git
cd SpatialLM

# create a conda environment with cuda 12.4
conda create -n spatiallm python=3.11
conda activate spatiallm
conda install -y -c nvidia/label/cuda-12.4.0 cuda-toolkit conda-forge::sparsehash

# Install dependencies with poetry
pip install poetry &amp;amp;&amp;amp; poetry config virtualenvs.create false --local
poetry install
# SpatialLM1.0 dependency
poe install-torchsparse # Building wheel for torchsparse will take a while
# SpatialLM1.1 dependency
poe install-sonata # Building wheel for flash-attn will take a while
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Inference&lt;/h3&gt; 
&lt;p&gt;In the current version of SpatialLM, input point clouds are considered axis-aligned where the z-axis is the up axis. This orientation is crucial for maintaining consistency in spatial understanding and scene interpretation across different datasets and applications. Example preprocessed point clouds, reconstructed from RGB videos using &lt;a href="https://github.com/rmurai0610/MASt3R-SLAM"&gt;MASt3R-SLAM&lt;/a&gt;, are available in &lt;a href="https://raw.githubusercontent.com/manycore-research/SpatialLM/main/#spatiallm-testset"&gt;SpatialLM-Testset&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Download an example point cloud:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;huggingface-cli download manycore-research/SpatialLM-Testset pcd/scene0000_00.ply --repo-type dataset --local-dir .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Run inference:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python inference.py --point_cloud pcd/scene0000_00.ply --output scene0000_00.txt --model_path manycore-research/SpatialLM1.1-Qwen-0.5B
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Detection with user-specified categories&lt;/h3&gt; 
&lt;p&gt;SpatialLM1.1 supports object detection conditioned on user-specified categories by leveraging the flexibility of LLMs.&lt;/p&gt; 
&lt;p&gt;SpatialLM1.1 offers three variants of structured indoor modeling tasks:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Structured Reconstruction&lt;/strong&gt;: Detect walls, doors, windows, boxes.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Layout Estimation&lt;/strong&gt;: Detect walls, doors, windows.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;3D Object Detection&lt;/strong&gt;: Detect boxes.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For tasks that include object box estimation, you can specify a subset of the 59 furniture categories, and the model will only predict objects within those specified categories. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python inference.py --point_cloud pcd/scene0000_00.ply --output scene0000_00.txt --model_path manycore-research/SpatialLM1.1-Qwen-0.5B --detect_type object --category bed nightstand
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Visualization&lt;/h3&gt; 
&lt;p&gt;Use &lt;code&gt;rerun&lt;/code&gt; to visualize the point cloud and the predicted structured 3D layout output:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Convert the predicted layout to Rerun format
python visualize.py --point_cloud pcd/scene0000_00.ply --layout scene0000_00.txt --save scene0000_00.rrd

# Visualize the point cloud and the predicted layout
rerun scene0000_00.rrd
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Evaluation&lt;/h3&gt; 
&lt;p&gt;To evaluate the performance of SpatialLM, we provide &lt;code&gt;eval.py&lt;/code&gt; script that reports the benchmark results on the SpatialLM-Testset in the table below in section &lt;a href="https://raw.githubusercontent.com/manycore-research/SpatialLM/main/#benchmark-results"&gt;Benchmark Results&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Download the testset:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;huggingface-cli download manycore-research/SpatialLM-Testset --repo-type dataset --local-dir SpatialLM-Testset
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Run evaluation:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Run inference on the PLY point clouds in folder SpatialLM-Testset/pcd with SpatialLM1.1-Qwen-0.5B model
python inference.py --point_cloud SpatialLM-Testset/pcd --output SpatialLM-Testset/pred --model_path manycore-research/SpatialLM1.1-Qwen-0.5B

# Evaluate the predicted layouts
python eval.py --metadata SpatialLM-Testset/test.csv --gt_dir SpatialLM-Testset/layout --pred_dir SpatialLM-Testset/pred --label_mapping SpatialLM-Testset/benchmark_categories.tsv
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Example using a custom video&lt;/h3&gt; 
&lt;p&gt;We provide an example of how to use our model to estimate scene layout starting from a RGB video with the newly released &lt;a href="https://github.com/PKU-VCL-3DV/SLAM3R"&gt;SLAM3R&lt;/a&gt; in &lt;a href="https://raw.githubusercontent.com/manycore-research/SpatialLM/main/EXAMPLE.md"&gt;EXAMPLE.md&lt;/a&gt;. These steps work for MASt3R-SLAM, and other reconstruction methods as well.&lt;/p&gt; 
&lt;h2&gt;SpatialLM Testset&lt;/h2&gt; 
&lt;p&gt;We provide a test set of 107 preprocessed point clouds, reconstructed from RGB videos using &lt;a href="https://github.com/rmurai0610/MASt3R-SLAM"&gt;MASt3R-SLAM&lt;/a&gt;. SpatialLM-Testset is quite challenging compared to prior clean RGBD scans datasets due to the noises and occlusions in the point clouds reconstructed from monocular RGB videos.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;Dataset&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;Download&lt;/strong&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;SpatialLM-Testset&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://huggingface.co/datasets/manycore-research/SpatialLM-TestSet"&gt;🤗 Datasets&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h2&gt;Benchmark Results&lt;/h2&gt; 
&lt;h3&gt;Layout Estimation&lt;/h3&gt; 
&lt;p&gt;Layout estimation focuses on predicting architectural elements, i.e., walls, doors, and windows, within an indoor scene. We evaluated this task on the &lt;a href="https://structured3d-dataset.org"&gt;Structured3D&lt;/a&gt; dataset. For &lt;a href="https://github.com/ywyue/RoomFormer"&gt;RoomFormer&lt;/a&gt;, we directly downloaded the model checkpoint. SceneScript and SpatialLM were first trained on our dataset, and further fine-tuned on Structured3D.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;Method&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;RoomFormer&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;SceneScript (finetuned)&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;SpatialLM1.1-Qwen-0.5B (finetuned)&lt;/strong&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;&lt;strong&gt;F1 @.25 IoU&lt;/strong&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;70.4&lt;/td&gt; 
    &lt;td align="center"&gt;83.1&lt;/td&gt; 
    &lt;td align="center"&gt;86.5&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;&lt;strong&gt;F1 @.5 IoU&lt;/strong&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;67.2&lt;/td&gt; 
    &lt;td align="center"&gt;80.8&lt;/td&gt; 
    &lt;td align="center"&gt;84.6&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h3&gt;3D Object Detection&lt;/h3&gt; 
&lt;p&gt;We evaluate 3D object detection on &lt;a href="http://www.scan-net.org"&gt;ScanNet&lt;/a&gt; with annotations of 18 object categories. For &lt;a href="https://github.com/V-DETR/V-DETR"&gt;V-DETR&lt;/a&gt;, we directly download the model checkpoint. SceneScript and SpatialLM were first trained on our dataset, and further fine-tuned on ScanNet.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;Method&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;V-DETR&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;SceneScript (finetuned)&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;SpatialLM1.1-Qwen-0.5B (finetuned)&lt;/strong&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;&lt;strong&gt;F1 @.25 IoU&lt;/strong&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;65.1&lt;/td&gt; 
    &lt;td align="center"&gt;49.1&lt;/td&gt; 
    &lt;td align="center"&gt;65.6&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;&lt;strong&gt;F1 @.5 IoU&lt;/strong&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;56.8&lt;/td&gt; 
    &lt;td align="center"&gt;36.8&lt;/td&gt; 
    &lt;td align="center"&gt;52.6&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h3&gt;Zero-shot Detection on Videos&lt;/h3&gt; 
&lt;p&gt;Zero-shot detection results on the challenging SpatialLM-Testset are reported in the following table:&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;Method&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;SpatialLM1.1-Llama-1B&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;SpatialLM1.1-Qwen-0.5B&lt;/strong&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;&lt;strong&gt;Layout&lt;/strong&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;strong&gt;F1 @.25 IoU (2D)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;strong&gt;F1 @.25 IoU (2D)&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;wall&lt;/td&gt; 
    &lt;td align="center"&gt;68.9&lt;/td&gt; 
    &lt;td align="center"&gt;68.2&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;door&lt;/td&gt; 
    &lt;td align="center"&gt;46.3&lt;/td&gt; 
    &lt;td align="center"&gt;43.1&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;window&lt;/td&gt; 
    &lt;td align="center"&gt;43.8&lt;/td&gt; 
    &lt;td align="center"&gt;47.4&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;&lt;strong&gt;Objects&lt;/strong&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;strong&gt;F1 @.25 IoU (3D)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;strong&gt;F1 @.25 IoU (2D)&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;curtain&lt;/td&gt; 
    &lt;td align="center"&gt;34.9&lt;/td&gt; 
    &lt;td align="center"&gt;37.0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;nightstand&lt;/td&gt; 
    &lt;td align="center"&gt;62.8&lt;/td&gt; 
    &lt;td align="center"&gt;67.0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;chandelier&lt;/td&gt; 
    &lt;td align="center"&gt;53.5&lt;/td&gt; 
    &lt;td align="center"&gt;36.8&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;wardrobe&lt;/td&gt; 
    &lt;td align="center"&gt;29.4&lt;/td&gt; 
    &lt;td align="center"&gt;39.6&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;bed&lt;/td&gt; 
    &lt;td align="center"&gt;96.8&lt;/td&gt; 
    &lt;td align="center"&gt;95.2&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;sofa&lt;/td&gt; 
    &lt;td align="center"&gt;66.9&lt;/td&gt; 
    &lt;td align="center"&gt;69.1&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;chair&lt;/td&gt; 
    &lt;td align="center"&gt;20.8&lt;/td&gt; 
    &lt;td align="center"&gt;32.3&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;cabinet&lt;/td&gt; 
    &lt;td align="center"&gt;15.2&lt;/td&gt; 
    &lt;td align="center"&gt;11.2&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;dining table&lt;/td&gt; 
    &lt;td align="center"&gt;40.7&lt;/td&gt; 
    &lt;td align="center"&gt;24.2&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;plants&lt;/td&gt; 
    &lt;td align="center"&gt;29.5&lt;/td&gt; 
    &lt;td align="center"&gt;26.3&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;tv cabinet&lt;/td&gt; 
    &lt;td align="center"&gt;34.4&lt;/td&gt; 
    &lt;td align="center"&gt;27.3&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;coffee table&lt;/td&gt; 
    &lt;td align="center"&gt;56.4&lt;/td&gt; 
    &lt;td align="center"&gt;64.9&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;side table&lt;/td&gt; 
    &lt;td align="center"&gt;14.6&lt;/td&gt; 
    &lt;td align="center"&gt;9.7&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;air conditioner&lt;/td&gt; 
    &lt;td align="center"&gt;16.7&lt;/td&gt; 
    &lt;td align="center"&gt;24.0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;dresser&lt;/td&gt; 
    &lt;td align="center"&gt;46.7&lt;/td&gt; 
    &lt;td align="center"&gt;46.7&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;stool&lt;/td&gt; 
    &lt;td align="center"&gt;17.6&lt;/td&gt; 
    &lt;td align="center"&gt;30.8&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;refrigerator&lt;/td&gt; 
    &lt;td align="center"&gt;0.0&lt;/td&gt; 
    &lt;td align="center"&gt;16.7&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;painting&lt;/td&gt; 
    &lt;td align="center"&gt;34.9&lt;/td&gt; 
    &lt;td align="center"&gt;38.2&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;carpet&lt;/td&gt; 
    &lt;td align="center"&gt;40.3&lt;/td&gt; 
    &lt;td align="center"&gt;24.1&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;tv&lt;/td&gt; 
    &lt;td align="center"&gt;16.0&lt;/td&gt; 
    &lt;td align="center"&gt;18.0&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h3&gt;Result Visualizations&lt;/h3&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;Layout Estimation&lt;/th&gt; 
    &lt;th align="center"&gt;Object Detection&lt;/th&gt; 
    &lt;th align="center"&gt;Zero-shot Reconstruction&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/manycore-research/SpatialLM/main/figures/stru3d.jpg" alt="Structured3D" /&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/manycore-research/SpatialLM/main/figures/scannet.jpg" alt="ScanNet" /&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/manycore-research/SpatialLM/main/figures/zeroshot.jpg" alt="Zero-shot" /&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;&lt;a href="https://manycore-research-azure.kujiale.com/manycore-research/SpatialLM/supplementary/visualization_layout.html"&gt;Structured3D Results&lt;/a&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://manycore-research-azure.kujiale.com/manycore-research/SpatialLM/supplementary/visualization_object.html"&gt;ScanNet Results&lt;/a&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://manycore-research-azure.kujiale.com/manycore-research/SpatialLM/supplementary/visualization_zeroshot.html"&gt;Zeroshot Results&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;SpatialLM-Llama-1B is derived from Llama3.2-1B-Instruct, which is licensed under the Llama3.2 license. SpatialLM-Qwen-0.5B is derived from the Qwen-2.5 series, originally licensed under the Apache 2.0 License.&lt;/p&gt; 
&lt;p&gt;SpatialLM1.0 are built upon the SceneScript point cloud encoder, licensed under the CC-BY-NC-4.0 License. TorchSparse, utilized in this project, is licensed under the MIT License.&lt;/p&gt; 
&lt;p&gt;SpatialLM1.1 are built upon Sonata point cloud encoder, model weight is licensed under the CC-BY-NC-4.0 License. Code built on Pointcept is licensed under the Apache 2.0 License.&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you find this work useful, please consider citing:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@article{SpatialLM,
    title         = {SpatialLM: Training Large Language Models for Structured Indoor Modeling},
    author        = {Mao, Yongsen and Zhong, Junhao and Fang, Chuan and Zheng, Jia and Tang, Rui and Zhu, Hao and Tan, Ping and Zhou, Zihan},
    journal       = {arXiv preprint},
    year          = {2025},
    eprint        = {2506.07491},
    archivePrefix = {arXiv},
    primaryClass  = {cs.CV}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;p&gt;We would like to thank the following projects that made this work possible:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/meta-llama"&gt;Llama3.2&lt;/a&gt; | &lt;a href="https://github.com/QwenLM/Qwen2.5"&gt;Qwen2.5&lt;/a&gt; | &lt;a href="https://github.com/huggingface/transformers"&gt;Transformers&lt;/a&gt; | &lt;a href="https://github.com/facebookresearch/scenescript"&gt;SceneScript&lt;/a&gt; | &lt;a href="https://github.com/mit-han-lab/torchsparse"&gt;TorchSparse&lt;/a&gt; | &lt;a href="https://xywu.me/sonata/"&gt;Sonata&lt;/a&gt; | &lt;a href="https://github.com/Pointcept/Pointcept"&gt;Pointcept&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>tadata-org/fastapi_mcp</title>
      <link>https://github.com/tadata-org/fastapi_mcp</link>
      <description>&lt;p&gt;Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt;&lt;a href="https://github.com/tadata-org/fastapi_mcp"&gt;&lt;img src="https://github.com/user-attachments/assets/7e44e98b-a0ba-4aff-a68a-4ffee3a6189c" alt="fastapi-to-mcp" height="100/" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;span style="font-size: 0.85em; font-weight: normal;"&gt;Built by &lt;a href="https://tadata.com"&gt;Tadata&lt;/a&gt;&lt;/span&gt; 
&lt;/div&gt; 
&lt;h1 align="center"&gt; FastAPI-MCP &lt;/h1&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://trendshift.io/repositories/14064" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/14064" alt="tadata-org%2Ffastapi_mcp | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;p align="center"&gt;Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://pypi.org/project/fastapi-mcp/"&gt;&lt;img src="https://img.shields.io/pypi/v/fastapi-mcp?color=%2334D058&amp;amp;label=pypi%20package" alt="PyPI version" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/fastapi-mcp/"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/fastapi-mcp.svg?sanitize=true" alt="Python Versions" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/tadata-org/fastapi_mcp/main/#"&gt;&lt;img src="https://img.shields.io/badge/FastAPI-009485.svg?logo=fastapi&amp;amp;logoColor=white" alt="FastAPI" /&gt;&lt;/a&gt; &lt;a href="https://github.com/tadata-org/fastapi_mcp/actions/workflows/ci.yml"&gt;&lt;img src="https://github.com/tadata-org/fastapi_mcp/actions/workflows/ci.yml/badge.svg?sanitize=true" alt="CI" /&gt;&lt;/a&gt; &lt;a href="https://codecov.io/gh/tadata-org/fastapi_mcp"&gt;&lt;img src="https://codecov.io/gh/tadata-org/fastapi_mcp/branch/main/graph/badge.svg?sanitize=true" alt="Coverage" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p align="center"&gt;&lt;a href="https://github.com/tadata-org/fastapi_mcp"&gt;&lt;img src="https://github.com/user-attachments/assets/b205adc6-28c0-4e3c-a68b-9c1a80eb7d0c" alt="fastapi-mcp-usage" height="400" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Authentication&lt;/strong&gt; built in, using your existing FastAPI dependencies!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;FastAPI-native:&lt;/strong&gt; Not just another OpenAPI -&amp;gt; MCP converter&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Zero/Minimal configuration&lt;/strong&gt; required - just point it at your FastAPI app and it works&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Preserving schemas&lt;/strong&gt; of your request models and response models&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Preserve documentation&lt;/strong&gt; of all your endpoints, just as it is in Swagger&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Flexible deployment&lt;/strong&gt; - Mount your MCP server to the same app, or deploy separately&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ASGI transport&lt;/strong&gt; - Uses FastAPI's ASGI interface directly for efficient communication&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Hosted Solution&lt;/h2&gt; 
&lt;p&gt;If you prefer a managed hosted solution check out &lt;a href="https://tadata.com"&gt;tadata.com&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;We recommend using &lt;a href="https://docs.astral.sh/uv/"&gt;uv&lt;/a&gt;, a fast Python package installer:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv add fastapi-mcp
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, you can install with pip:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install fastapi-mcp
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Basic Usage&lt;/h2&gt; 
&lt;p&gt;The simplest way to use FastAPI-MCP is to add an MCP server directly to your FastAPI application:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from fastapi import FastAPI
from fastapi_mcp import FastApiMCP

app = FastAPI()

mcp = FastApiMCP(app)

# Mount the MCP server directly to your FastAPI app
mcp.mount()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;That's it! Your auto-generated MCP server is now available at &lt;code&gt;https://app.base.url/mcp&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Documentation, Examples and Advanced Usage&lt;/h2&gt; 
&lt;p&gt;FastAPI-MCP provides &lt;a href="https://fastapi-mcp.tadata.com/"&gt;comprehensive documentation&lt;/a&gt;. Additionaly, check out the &lt;a href="https://raw.githubusercontent.com/tadata-org/fastapi_mcp/main/examples"&gt;examples directory&lt;/a&gt; for code samples demonstrating these features in action.&lt;/p&gt; 
&lt;h2&gt;FastAPI-first Approach&lt;/h2&gt; 
&lt;p&gt;FastAPI-MCP is designed as a native extension of FastAPI, not just a converter that generates MCP tools from your API. This approach offers several key advantages:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Native dependencies&lt;/strong&gt;: Secure your MCP endpoints using familiar FastAPI &lt;code&gt;Depends()&lt;/code&gt; for authentication and authorization&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ASGI transport&lt;/strong&gt;: Communicates directly with your FastAPI app using its ASGI interface, eliminating the need for HTTP calls from the MCP to your API&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Unified infrastructure&lt;/strong&gt;: Your FastAPI app doesn't need to run separately from the MCP server (though &lt;a href="https://fastapi-mcp.tadata.com/advanced/deploy#deploying-separately-from-original-fastapi-app"&gt;separate deployment&lt;/a&gt; is also supported)&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This design philosophy ensures minimum friction when adding MCP capabilities to your existing FastAPI services.&lt;/p&gt; 
&lt;h2&gt;Development and Contributing&lt;/h2&gt; 
&lt;p&gt;Thank you for considering contributing to FastAPI-MCP! We encourage the community to post Issues and create Pull Requests.&lt;/p&gt; 
&lt;p&gt;Before you get started, please see our &lt;a href="https://raw.githubusercontent.com/tadata-org/fastapi_mcp/main/CONTRIBUTING.md"&gt;Contribution Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Community&lt;/h2&gt; 
&lt;p&gt;Join &lt;a href="https://join.slack.com/t/themcparty/shared_invite/zt-30yxr1zdi-2FG~XjBA0xIgYSYuKe7~Xg"&gt;MCParty Slack community&lt;/a&gt; to connect with other MCP enthusiasts, ask questions, and share your experiences with FastAPI-MCP.&lt;/p&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.10+ (Recommended 3.12)&lt;/li&gt; 
 &lt;li&gt;uv&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;MIT License. Copyright (c) 2025 Tadata Inc.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>coleam00/Archon</title>
      <link>https://github.com/coleam00/Archon</link>
      <description>&lt;p&gt;Beta release of Archon OS - the knowledge and task management backbone for AI coding assistants.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/coleam00/Archon/main/archon-ui-main/public/archon-main-graphic.png" alt="Archon Main Graphic" width="853" height="422" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;em&gt;Power up your AI coding assistants with your own custom knowledge base and task management as an MCP server&lt;/em&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://raw.githubusercontent.com/coleam00/Archon/main/#quick-start"&gt;Quick Start&lt;/a&gt; • &lt;a href="https://raw.githubusercontent.com/coleam00/Archon/main/#whats-included"&gt;What's Included&lt;/a&gt; • &lt;a href="https://raw.githubusercontent.com/coleam00/Archon/main/#architecture"&gt;Architecture&lt;/a&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;🎯 What is Archon?&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Archon is currently in beta! Expect things to not work 100%, and please feel free to share any feedback and contribute with fixes/new features! Thank you to everyone for all the excitement we have for Archon already, as well as the bug reports, PRs, and discussions. It's a lot for our small team to get through but we're committed to addressing everything and making Archon into the best tool it possibly can be!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Archon is the &lt;strong&gt;command center&lt;/strong&gt; for AI coding assistants. For you, it's a sleek interface to manage knowledge, context, and tasks for your projects. For the AI coding assistant(s), it's a &lt;strong&gt;Model Context Protocol (MCP) server&lt;/strong&gt; to collaborate on and leverage the same knowledge, context, and tasks. Connect Claude Code, Kiro, Cursor, Windsurf, etc. to give your AI agents access to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Your documentation&lt;/strong&gt; (crawled websites, uploaded PDFs/docs)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Smart search capabilities&lt;/strong&gt; with advanced RAG strategies&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Task management&lt;/strong&gt; integrated with your knowledge base&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Real-time updates&lt;/strong&gt; as you add new content and collaborate with your coding assistant on tasks&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Much more&lt;/strong&gt; coming soon to build Archon into an integrated environment for all context engineering&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This new vision for Archon replaces the old one (the agenteer). Archon used to be the AI agent that builds other agents, and now you can use Archon to do that and more.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;It doesn't matter what you're building or if it's a new/existing codebase - Archon's knowledge and task management capabilities will improve the output of &lt;strong&gt;any&lt;/strong&gt; AI driven coding.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;🔗 Important Links&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/coleam00/Archon/discussions"&gt;GitHub Discussions&lt;/a&gt;&lt;/strong&gt; - Join the conversation and share ideas about Archon&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/coleam00/Archon/main/CONTRIBUTING.md"&gt;Contributing Guide&lt;/a&gt;&lt;/strong&gt; - How to get involved and contribute to Archon&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://youtu.be/8pRc_s2VQIo"&gt;Introduction Video&lt;/a&gt;&lt;/strong&gt; - Getting Started Guide and Vision for Archon&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://dynamous.ai"&gt;Dynamous AI Mastery&lt;/a&gt;&lt;/strong&gt; - The birthplace of Archon - come join a vibrant community of other early AI adopters all helping each other transform their careers and businesses!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.docker.com/products/docker-desktop/"&gt;Docker Desktop&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://supabase.com/"&gt;Supabase&lt;/a&gt; account (free tier or local Supabase both work)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://platform.openai.com/api-keys"&gt;OpenAI API key&lt;/a&gt; (Gemini and Ollama are supported too!)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Setup Instructions&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Clone Repository&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/coleam00/archon.git
cd archon
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Environment Configuration&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cp .env.example .env
# Edit .env and add your Supabase credentials:
# SUPABASE_URL=https://your-project.supabase.co
# SUPABASE_SERVICE_KEY=your-service-key-here
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;NOTE: Supabase introduced a new type of service key but use the legacy one (the longer one).&lt;/p&gt; &lt;p&gt;OPTIONAL: If you want to enable the reranking RAG strategy, uncomment lines 20-22 in &lt;code&gt;python\requirements.server.txt&lt;/code&gt;. This will significantly increase the size of the Archon Server container which is why it's off by default.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Database Setup&lt;/strong&gt;: In your &lt;a href="https://supabase.com/dashboard"&gt;Supabase project&lt;/a&gt; SQL Editor, copy, paste, and execute the contents of &lt;code&gt;migration/complete_setup.sql&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Start Services&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;docker-compose up --build -d
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This starts the core microservices:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Server&lt;/strong&gt;: Core API and business logic (Port: 8181)&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;MCP Server&lt;/strong&gt;: Protocol interface for AI clients (Port: 8051)&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Agents (coming soon!)&lt;/strong&gt;: AI operations and streaming (Port: 8052)&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;UI&lt;/strong&gt;: Web interface (Port: 3737)&lt;/li&gt; 
  &lt;/ul&gt; &lt;p&gt;Ports are configurable in your .env as well!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Configure API Keys&lt;/strong&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Open &lt;a href="http://localhost:3737"&gt;http://localhost:3737&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Go to &lt;strong&gt;Settings&lt;/strong&gt; → Select your LLM/embedding provider and set the API key (OpenAI is default)&lt;/li&gt; 
   &lt;li&gt;Test by uploading a document or crawling a website&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;🔄 Database Reset (Start Fresh if Needed)&lt;/h2&gt; 
&lt;p&gt;If you need to completely reset your database and start fresh:&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;⚠️ &lt;strong&gt;Reset Database - This will delete ALL data for Archon!&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Run Reset Script&lt;/strong&gt;: In your Supabase SQL Editor, run the contents of &lt;code&gt;migration/RESET_DB.sql&lt;/code&gt;&lt;/p&gt; &lt;p&gt;⚠️ WARNING: This will delete all Archon specific tables and data! Nothing else will be touched in your DB though.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Rebuild Database&lt;/strong&gt;: After reset, run &lt;code&gt;migration/complete_setup.sql&lt;/code&gt; to create all the tables again.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Restart Services&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;docker-compose up -d
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Reconfigure&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Select your LLM/embedding provider and set the API key again&lt;/li&gt; 
    &lt;li&gt;Re-upload any documents or re-crawl websites&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;The reset script safely removes all tables, functions, triggers, and policies with proper dependency handling.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;⚡ Quick Test&lt;/h2&gt; 
&lt;p&gt;Once everything is running:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Test Web Crawling&lt;/strong&gt;: Go to &lt;a href="http://localhost:3737"&gt;http://localhost:3737&lt;/a&gt; → Knowledge Base → "Crawl Website" → Enter a doc URL (such as &lt;a href="https://ai.pydantic.dev/llms-full.txt"&gt;https://ai.pydantic.dev/llms-full.txt&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Test Document Upload&lt;/strong&gt;: Knowledge Base → Upload a PDF&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Test Projects&lt;/strong&gt;: Projects → Create a new project and add tasks&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Integrate with your AI coding assistant&lt;/strong&gt;: MCP Dashboard → Copy connection config for your AI coding assistant&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;📚 Documentation&lt;/h2&gt; 
&lt;h3&gt;Core Services&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Service&lt;/th&gt; 
   &lt;th&gt;Container Name&lt;/th&gt; 
   &lt;th&gt;Default URL&lt;/th&gt; 
   &lt;th&gt;Purpose&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Web Interface&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;archon-ui&lt;/td&gt; 
   &lt;td&gt;&lt;a href="http://localhost:3737"&gt;http://localhost:3737&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Main dashboard and controls&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;API Service&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;archon-server&lt;/td&gt; 
   &lt;td&gt;&lt;a href="http://localhost:8181"&gt;http://localhost:8181&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Web crawling, document processing&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;MCP Server&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;archon-mcp&lt;/td&gt; 
   &lt;td&gt;&lt;a href="http://localhost:8051"&gt;http://localhost:8051&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Model Context Protocol interface&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Agents Service&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;archon-agents&lt;/td&gt; 
   &lt;td&gt;&lt;a href="http://localhost:8052"&gt;http://localhost:8052&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;AI/ML operations, reranking&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;What's Included&lt;/h2&gt; 
&lt;h3&gt;🧠 Knowledge Management&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Smart Web Crawling&lt;/strong&gt;: Automatically detects and crawls entire documentation sites, sitemaps, and individual pages&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Document Processing&lt;/strong&gt;: Upload and process PDFs, Word docs, markdown files, and text documents with intelligent chunking&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Code Example Extraction&lt;/strong&gt;: Automatically identifies and indexes code examples from documentation for enhanced search&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Vector Search&lt;/strong&gt;: Advanced semantic search with contextual embeddings for precise knowledge retrieval&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Source Management&lt;/strong&gt;: Organize knowledge by source, type, and tags for easy filtering&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;🤖 AI Integration&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Model Context Protocol (MCP)&lt;/strong&gt;: Connect any MCP-compatible client (Claude Code, Cursor, even non-AI coding assistants like Claude Desktop)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;10 MCP Tools&lt;/strong&gt;: Comprehensive yet simple set of tools for RAG queries, task management, and project operations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-LLM Support&lt;/strong&gt;: Works with OpenAI, Ollama, and Google Gemini models&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;RAG Strategies&lt;/strong&gt;: Hybrid search, contextual embeddings, and result reranking for optimal AI responses&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Real-time Streaming&lt;/strong&gt;: Live responses from AI agents with progress tracking&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;📋 Project &amp;amp; Task Management&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Hierarchical Projects&lt;/strong&gt;: Organize work with projects, features, and tasks in a structured workflow&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;AI-Assisted Creation&lt;/strong&gt;: Generate project requirements and tasks using integrated AI agents&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Document Management&lt;/strong&gt;: Version-controlled documents with collaborative editing capabilities&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Progress Tracking&lt;/strong&gt;: Real-time updates and status management across all project activities&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;🔄 Real-time Collaboration&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;WebSocket Updates&lt;/strong&gt;: Live progress tracking for crawling, processing, and AI operations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-user Support&lt;/strong&gt;: Collaborative knowledge building and project management&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Background Processing&lt;/strong&gt;: Asynchronous operations that don't block the user interface&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Health Monitoring&lt;/strong&gt;: Built-in service health checks and automatic reconnection&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Architecture&lt;/h2&gt; 
&lt;h3&gt;Microservices Structure&lt;/h3&gt; 
&lt;p&gt;Archon uses true microservices architecture with clear separation of concerns:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   Frontend UI   │    │  Server (API)   │    │   MCP Server    │    │ Agents Service  │
│                 │    │                 │    │                 │    │                 │
│  React + Vite   │◄──►│    FastAPI +    │◄──►│    Lightweight  │◄──►│   PydanticAI    │
│  Port 3737      │    │    SocketIO     │    │    HTTP Wrapper │    │   Port 8052     │
│                 │    │    Port 8181    │    │    Port 8051    │    │                 │
└─────────────────┘    └─────────────────┘    └─────────────────┘    └─────────────────┘
         │                        │                        │                        │
         └────────────────────────┼────────────────────────┼────────────────────────┘
                                  │                        │
                         ┌─────────────────┐               │
                         │    Database     │               │
                         │                 │               │
                         │    Supabase     │◄──────────────┘
                         │    PostgreSQL   │
                         │    PGVector     │
                         └─────────────────┘
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Service Responsibilities&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Service&lt;/th&gt; 
   &lt;th&gt;Location&lt;/th&gt; 
   &lt;th&gt;Purpose&lt;/th&gt; 
   &lt;th&gt;Key Features&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Frontend&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;archon-ui-main/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Web interface and dashboard&lt;/td&gt; 
   &lt;td&gt;React, TypeScript, TailwindCSS, Socket.IO client&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Server&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;python/src/server/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Core business logic and APIs&lt;/td&gt; 
   &lt;td&gt;FastAPI, service layer, Socket.IO broadcasts, all ML/AI operations&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;MCP Server&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;python/src/mcp/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;MCP protocol interface&lt;/td&gt; 
   &lt;td&gt;Lightweight HTTP wrapper, 10 MCP tools, session management&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Agents&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;python/src/agents/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;PydanticAI agent hosting&lt;/td&gt; 
   &lt;td&gt;Document and RAG agents, streaming responses&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Communication Patterns&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;HTTP-based&lt;/strong&gt;: All inter-service communication uses HTTP APIs&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Socket.IO&lt;/strong&gt;: Real-time updates from Server to Frontend&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MCP Protocol&lt;/strong&gt;: AI clients connect to MCP Server via SSE or stdio&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;No Direct Imports&lt;/strong&gt;: Services are truly independent with no shared code dependencies&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Key Architectural Benefits&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Lightweight Containers&lt;/strong&gt;: Each service contains only required dependencies&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Independent Scaling&lt;/strong&gt;: Services can be scaled independently based on load&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Development Flexibility&lt;/strong&gt;: Teams can work on different services without conflicts&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Technology Diversity&lt;/strong&gt;: Each service uses the best tools for its specific purpose&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🔧 Configuring Custom Ports &amp;amp; Hostname&lt;/h2&gt; 
&lt;p&gt;By default, Archon services run on the following ports:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Archon-UI&lt;/strong&gt;: 3737&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Archon-Server&lt;/strong&gt;: 8181&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Archon-MCP&lt;/strong&gt;: 8051&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Archon-Agents&lt;/strong&gt;: 8052&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Archon-Docs&lt;/strong&gt;: 3838 (optional)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Changing Ports&lt;/h3&gt; 
&lt;p&gt;To use custom ports, add these variables to your &lt;code&gt;.env&lt;/code&gt; file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Service Ports Configuration
ARCHON_UI_PORT=3737
ARCHON_SERVER_PORT=8181
ARCHON_MCP_PORT=8051
ARCHON_AGENTS_PORT=8052
ARCHON_DOCS_PORT=3838
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Example: Running on different ports:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;ARCHON_SERVER_PORT=8282
ARCHON_MCP_PORT=8151
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Configuring Hostname&lt;/h3&gt; 
&lt;p&gt;By default, Archon uses &lt;code&gt;localhost&lt;/code&gt; as the hostname. You can configure a custom hostname or IP address by setting the &lt;code&gt;HOST&lt;/code&gt; variable in your &lt;code&gt;.env&lt;/code&gt; file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Hostname Configuration
HOST=localhost  # Default

# Examples of custom hostnames:
HOST=192.168.1.100     # Use specific IP address
HOST=archon.local      # Use custom domain
HOST=myserver.com      # Use public domain
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This is useful when:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Running Archon on a different machine and accessing it remotely&lt;/li&gt; 
 &lt;li&gt;Using a custom domain name for your installation&lt;/li&gt; 
 &lt;li&gt;Deploying in a network environment where &lt;code&gt;localhost&lt;/code&gt; isn't accessible&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;After changing hostname or ports:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Restart Docker containers: &lt;code&gt;docker-compose down &amp;amp;&amp;amp; docker-compose up -d&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Access the UI at: &lt;code&gt;http://${HOST}:${ARCHON_UI_PORT}&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Update your AI client configuration with the new hostname and MCP port&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;🔧 Development&lt;/h2&gt; 
&lt;p&gt;For development with hot reload:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Backend services (with auto-reload)
docker-compose up archon-server archon-mcp archon-agents --build

# Frontend (with hot reload)
cd archon-ui-main &amp;amp;&amp;amp; npm run dev

# Documentation (with hot reload)
cd docs &amp;amp;&amp;amp; npm start
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: The backend services are configured with &lt;code&gt;--reload&lt;/code&gt; flag in their uvicorn commands and have source code mounted as volumes for automatic hot reloading when you make changes.&lt;/p&gt; 
&lt;h2&gt;📄 License&lt;/h2&gt; 
&lt;p&gt;Archon Community License (ACL) v1.2 - see &lt;a href="https://raw.githubusercontent.com/coleam00/Archon/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: Archon is free, open, and hackable. Run it, fork it, share it - just don't sell it as-a-service without permission.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>googleapis/google-api-python-client</title>
      <link>https://github.com/googleapis/google-api-python-client</link>
      <description>&lt;p&gt;🐍 The official Python client library for Google's discovery based APIs.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Google API Client&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://badge.fury.io/py/google-api-python-client"&gt;&lt;img src="https://badge.fury.io/py/google-api-python-client.svg?sanitize=true" alt="PyPI version" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;This is the &lt;a href="https://cloud.google.com/apis/docs/client-libraries-explained#google_api_client_libraries"&gt;Google API Python client library&lt;/a&gt; for Google's discovery based APIs. To get started, please see the &lt;a href="https://github.com/googleapis/google-api-python-client/raw/main/docs/README.md"&gt;docs folder&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;This library is considered complete and is in maintenance mode. This means that we will address critical bugs and security issues but will not add any new features.&lt;/p&gt; 
&lt;p&gt;This library is officially supported by Google. However, the maintainers of this repository recommend using &lt;a href="https://github.com/googleapis/google-cloud-python"&gt;Cloud Client Libraries for Python&lt;/a&gt;, where possible, for new code development. For more information, please visit &lt;a href="https://cloud.google.com/apis/docs/client-libraries-explained"&gt;Client Libraries Explained&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Version 2.0 Release&lt;/h2&gt; 
&lt;p&gt;The 2.0 release of &lt;code&gt;google-api-python-client&lt;/code&gt; includes a substantial reliability improvement, compared with 1.x, as discovery documents are now cached in the library rather than fetched dynamically. It is highly recommended to upgrade from v1.x to v2.x.&lt;/p&gt; 
&lt;p&gt;Only python 3.7 and newer is supported. If you are not able to upgrade python, then please continue to use version 1.x as we will continue supporting python 2.7+ in &lt;a href="https://github.com/googleapis/google-api-python-client/tree/v1"&gt;v1&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Discovery documents will no longer be retrieved dynamically when you call &lt;code&gt;discovery.build()&lt;/code&gt;. The discovery documents will instead be retrieved from the client library directly. New versions of this library are released weekly. As a result of caching the discovery documents, the size of this package is at least 50 MB larger compared to the previous version.&lt;/p&gt; 
&lt;p&gt;Please see the &lt;a href="https://github.com/googleapis/google-api-python-client/raw/main/UPGRADING.md"&gt;Migration Guide&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;See the &lt;a href="https://github.com/googleapis/google-api-python-client/raw/main/docs/README.md"&gt;docs folder&lt;/a&gt; for more detailed instructions and additional documentation.&lt;/p&gt; 
&lt;h2&gt;Other Google API libraries&lt;/h2&gt; 
&lt;p&gt;The maintainers of this repository recommend using &lt;a href="https://github.com/googleapis/google-cloud-python"&gt;Cloud Client Libraries for Python&lt;/a&gt;, where possible, for new code development due to the following reasons:&lt;/p&gt; 
&lt;p&gt;With &lt;a href="https://github.com/googleapis/google-cloud-python"&gt;Cloud Client Libraries for Python&lt;/a&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;There is a separate client library for each API, so you can choose which client libraries to download. Whereas, &lt;code&gt;google-api-python-client&lt;/code&gt; is a single client library for all APIs. As a result, the total package size for &lt;code&gt;google-api-python-client&lt;/code&gt; exceeds 50MB.&lt;/li&gt; 
 &lt;li&gt;There are stricter controls for breaking changes to the underlying APIs as each client library is focused on a specific API.&lt;/li&gt; 
 &lt;li&gt;There are more features in these Cloud Client Libraries as each library is focused on a specific API, and in some cases, the libraries are owned by team who specialized in that API.&lt;/li&gt; 
 &lt;li&gt;Developers will benefit from intellisense.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For more information, please visit &lt;a href="https://cloud.google.com/apis/docs/client-libraries-explained"&gt;Client Libraries Explained&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Although there are many benefits to moving to &lt;a href="https://github.com/googleapis/google-cloud-python"&gt;Cloud Client Libraries for Python&lt;/a&gt;, the maintainers want to emphasize that &lt;code&gt;google-api-python-client&lt;/code&gt; will continue to be supported.&lt;/p&gt; 
&lt;p&gt;For Google Ads API, we recommend using &lt;a href="https://github.com/googleads/google-ads-python/"&gt;Google Ads API Client Library for Python&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For Google Firebase Admin API, we recommend using &lt;a href="https://github.com/firebase/firebase-admin-python"&gt;Firebase Admin Python SDK&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;Install this library in a &lt;a href="https://virtualenv.pypa.io/en/latest/"&gt;virtualenv&lt;/a&gt; using pip. virtualenv is a tool to create isolated Python environments. The basic problem it addresses is one of dependencies and versions, and indirectly permissions.&lt;/p&gt; 
&lt;p&gt;With virtualenv, it's possible to install this library without needing system install permissions, and without clashing with the installed system dependencies.&lt;/p&gt; 
&lt;h3&gt;Mac/Linux&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip3 install virtualenv
virtualenv &amp;lt;your-env&amp;gt;
source &amp;lt;your-env&amp;gt;/bin/activate
&amp;lt;your-env&amp;gt;/bin/pip install google-api-python-client
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Windows&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-batch"&gt;pip install virtualenv
virtualenv &amp;lt;your-env&amp;gt;
&amp;lt;your-env&amp;gt;\Scripts\activate
&amp;lt;your-env&amp;gt;\Scripts\pip.exe install google-api-python-client
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Supported Python Versions&lt;/h2&gt; 
&lt;p&gt;Python 3.7, 3.8, 3.9, 3.10, 3.11, 3.12 and 3.13 are fully supported and tested. This library may work on later versions of 3, but we do not currently run tests against those versions.&lt;/p&gt; 
&lt;h2&gt;Unsupported Python Versions&lt;/h2&gt; 
&lt;p&gt;Python &amp;lt; 3.7&lt;/p&gt; 
&lt;h2&gt;Third Party Libraries and Dependencies&lt;/h2&gt; 
&lt;p&gt;The following libraries will be installed when you install the client library:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/httplib2/httplib2"&gt;httplib2&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/sigmavirus24/uritemplate"&gt;uritemplate&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For development you will also need the following libraries:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://pypi.org/project/WebTest/"&gt;WebTest&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pypi.python.org/pypi/pyOpenSSL"&gt;pyopenssl&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Please see our &lt;a href="https://github.com/googleapis/google-api-python-client/raw/main/CONTRIBUTING.rst"&gt;Contribution Guide&lt;/a&gt;. In particular, we love pull requests - but please make sure to sign the contributor license agreement.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>gyoridavid/ai_agents_az</title>
      <link>https://github.com/gyoridavid/ai_agents_az</link>
      <description>&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AI Agents A-Z&lt;/h1&gt; 
&lt;p&gt;In this repo, you can find the n8n templates we created for the episodes of &lt;a href="https://www.youtube.com/channel/UCloXqLhp_KGhHBe1kwaL2Tg"&gt;AI Agents A-Z&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;&lt;a href="https://www.skool.com/ai-agents-az/about"&gt;📚 Join our Skool community for support, premium content and more!&lt;/a&gt;&lt;/h2&gt; 
&lt;h3&gt;Be part of a growing community and help us create more content&lt;/h3&gt; 
&lt;h2&gt;Season 1&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_1"&gt;Episode 1: Creating a prescription agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_2"&gt;Episode 2: Making a daily digest agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_3"&gt;Episode 3: Making LinkedIn posts using Human in the Loop approval process&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_4"&gt;Episode 4: Deep Research Agent using Google&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_5"&gt;Episode 5: Creating a blog writing system using deep research&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_6"&gt;Episode 6: Lead generation with X-Ray search and LinkedIn&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_7"&gt;Episode 7: Creating Youtube short videos using our custom MCP server&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_8"&gt;Episode 8: Creating an AI influencer on Instagram using n8n&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_9"&gt;Episode 9: Create revenge story videos for YouTube&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_10"&gt;Episode 10: n8n best practices&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_11"&gt;Episode 11: Create short (motivational) stories for YouTube and TikTok&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_12"&gt;Episode 12: Scheduling social media posts with Postiz and n8n&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_13"&gt;Episode 13: Create AI videos with MiniMax Hailuo 2 and n8n&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_14"&gt;Episode 14: Create AI videos with Seedance and n8n&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_15"&gt;Episode 15: Generate AI startup ideas from Reddit&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_16"&gt;Episode 16: Create AI poem videos with n8n for TikTok&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_17"&gt;Episode 17: Create Shopify product videos with Seedance, ElevenLabs, Latentsync, Flux Kontext and n8n&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_18"&gt;Episode 18: Scary story TikTok videos workflow&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_19"&gt;Episode 19: Run FLUX.1 Kontext [dev] with modal.com&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_20"&gt;Episode 20: Use Wan 2.2, ComfyUI and n8n to generate videos for free&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;servers&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://hub.docker.com/r/gyoridavid/ai-agents-no-code-tools"&gt;AI Agents No-Code Tools&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/gyoridavid/short-video-maker"&gt;Short video maker MCP/REST server&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://hub.docker.com/r/gyoridavid/narrated-story-creator"&gt;Narrated story creator REST/MCP server&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>instaloader/instaloader</title>
      <link>https://github.com/instaloader/instaloader</link>
      <description>&lt;p&gt;Download pictures (or videos) along with their captions and other metadata from Instagram.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;.. image:: &lt;a href="https://raw.githubusercontent.com/instaloader/instaloader/master/docs/logo_heading.png"&gt;https://raw.githubusercontent.com/instaloader/instaloader/master/docs/logo_heading.png&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;.. badges-start&lt;/p&gt; 
&lt;p&gt;|pypi| |pyversion| |license| |aur| |contributors| |downloads|&lt;/p&gt; 
&lt;p&gt;.. |pypi| image:: &lt;a href="https://img.shields.io/pypi/v/instaloader.svg"&gt;https://img.shields.io/pypi/v/instaloader.svg&lt;/a&gt; :alt: Instaloader PyPI Project Page :target: &lt;a href="https://pypi.org/project/instaloader/"&gt;https://pypi.org/project/instaloader/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;.. |license| image:: &lt;a href="https://img.shields.io/github/license/instaloader/instaloader.svg"&gt;https://img.shields.io/github/license/instaloader/instaloader.svg&lt;/a&gt; :alt: MIT License :target: &lt;a href="https://github.com/instaloader/instaloader/raw/master/LICENSE"&gt;https://github.com/instaloader/instaloader/blob/master/LICENSE&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;.. |pyversion| image:: &lt;a href="https://img.shields.io/pypi/pyversions/instaloader.svg"&gt;https://img.shields.io/pypi/pyversions/instaloader.svg&lt;/a&gt; :alt: Supported Python Versions&lt;/p&gt; 
&lt;p&gt;.. |contributors| image:: &lt;a href="https://img.shields.io/github/contributors/instaloader/instaloader.svg"&gt;https://img.shields.io/github/contributors/instaloader/instaloader.svg&lt;/a&gt; :alt: Contributor Count :target: &lt;a href="https://github.com/instaloader/instaloader/graphs/contributors"&gt;https://github.com/instaloader/instaloader/graphs/contributors&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;.. |aur| image:: &lt;a href="https://img.shields.io/aur/version/instaloader.svg"&gt;https://img.shields.io/aur/version/instaloader.svg&lt;/a&gt; :alt: Arch User Repository Package :target: &lt;a href="https://aur.archlinux.org/packages/instaloader/"&gt;https://aur.archlinux.org/packages/instaloader/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;.. |downloads| image:: &lt;a href="https://pepy.tech/badge/instaloader/month"&gt;https://pepy.tech/badge/instaloader/month&lt;/a&gt; :alt: PyPI Download Count :target: &lt;a href="https://pepy.tech/project/instaloader"&gt;https://pepy.tech/project/instaloader&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;.. badges-end&lt;/p&gt; 
&lt;p&gt;::&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ pip3 install instaloader

$ instaloader profile [profile ...]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Instaloader&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;downloads &lt;strong&gt;public and private profiles, hashtags, user stories, feeds and saved media&lt;/strong&gt;,&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;downloads &lt;strong&gt;comments, geotags and captions&lt;/strong&gt; of each post,&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;automatically &lt;strong&gt;detects profile name changes&lt;/strong&gt; and renames the target directory accordingly,&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;allows &lt;strong&gt;fine-grained customization&lt;/strong&gt; of filters and where to store downloaded media,&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;automatically &lt;strong&gt;resumes previously-interrupted&lt;/strong&gt; download iterations.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;::&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;instaloader [--comments] [--geotags]
            [--stories] [--highlights] [--tagged] [--reels] [--igtv]
            [--login YOUR-USERNAME] [--fast-update]
            profile | "#hashtag" | :stories | :feed | :saved
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;code&gt;Instaloader Documentation &amp;lt;https://instaloader.github.io/&amp;gt;&lt;/code&gt;__&lt;/p&gt; 
&lt;h2&gt;How to Automatically Download Pictures from Instagram&lt;/h2&gt; 
&lt;p&gt;To &lt;strong&gt;download all pictures and videos of a profile&lt;/strong&gt;, as well as the &lt;strong&gt;profile picture&lt;/strong&gt;, do&lt;/p&gt; 
&lt;p&gt;::&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;instaloader profile [profile ...]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;where &lt;code&gt;profile&lt;/code&gt; is the name of a profile you want to download. Instead of only one profile, you may also specify a list of profiles.&lt;/p&gt; 
&lt;p&gt;To later &lt;strong&gt;update your local copy&lt;/strong&gt; of that profiles, you may run&lt;/p&gt; 
&lt;p&gt;::&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;instaloader --fast-update profile [profile ...]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If &lt;code&gt;--fast-update&lt;/code&gt; is given, Instaloader stops when arriving at the first already-downloaded picture.&lt;/p&gt; 
&lt;p&gt;Alternatively, you can use &lt;code&gt;--latest-stamps&lt;/code&gt; to have Instaloader store the time each profile was last downloaded and only download newer media:&lt;/p&gt; 
&lt;p&gt;::&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;instaloader --latest-stamps -- profile [profile ...]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;With this option it's possible to move or delete downloaded media and still keep the archive updated.&lt;/p&gt; 
&lt;p&gt;When updating profiles, Instaloader automatically &lt;strong&gt;detects profile name changes&lt;/strong&gt; and renames the target directory accordingly.&lt;/p&gt; 
&lt;p&gt;Instaloader can also be used to &lt;strong&gt;download private profiles&lt;/strong&gt;. To do so, invoke it with&lt;/p&gt; 
&lt;p&gt;::&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;instaloader --login=your_username profile [profile ...]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;When logging in, Instaloader &lt;strong&gt;stores the session cookies&lt;/strong&gt; in a file in your temporary directory, which will be reused later the next time &lt;code&gt;--login&lt;/code&gt; is given. So you can download private profiles &lt;strong&gt;non-interactively&lt;/strong&gt; when you already have a valid session cookie file.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;Instaloader Documentation &amp;lt;https://instaloader.github.io/basic-usage.html&amp;gt;&lt;/code&gt;__&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;As an open source project, Instaloader heavily depends on the contributions from its community. See &lt;code&gt;contributing &amp;lt;https://instaloader.github.io/contributing.html&amp;gt;&lt;/code&gt;__ for how you may help Instaloader to become an even greater tool.&lt;/p&gt; 
&lt;h2&gt;Supporters&lt;/h2&gt; 
&lt;p&gt;.. current-sponsors-start&lt;/p&gt; 
&lt;p&gt;| Instaloader is proudly sponsored by | &lt;code&gt;@rocketapi-io &amp;lt;https://github.com/rocketapi-io&amp;gt;&lt;/code&gt;__&lt;/p&gt; 
&lt;p&gt;See &lt;code&gt;Alex' GitHub Sponsors &amp;lt;https://github.com/sponsors/aandergr&amp;gt;&lt;/code&gt;__ page for how you can sponsor the development of Instaloader!&lt;/p&gt; 
&lt;p&gt;.. current-sponsors-end&lt;/p&gt; 
&lt;p&gt;It is a pleasure for us to share our Instaloader to the world, and we are proud to have attracted such an active and motivating community, with so many users who share their suggestions and ideas with us. Buying a community-sponsored beer or coffee from time to time is very likely to further raise our passion for the development of Instaloader.&lt;/p&gt; 
&lt;p&gt;| For Donations, we provide GitHub Sponsors page, a PayPal.Me link and a Bitcoin address. | GitHub Sponsors: &lt;code&gt;Sponsor @aandergr on GitHub Sponsors &amp;lt;https://github.com/sponsors/aandergr&amp;gt;&lt;/code&gt;__ | PayPal: &lt;code&gt;PayPal.me/aandergr &amp;lt;https://www.paypal.me/aandergr&amp;gt;&lt;/code&gt;__ | BTC: 1Nst4LoadeYzrKjJ1DX9CpbLXBYE9RKLwY&lt;/p&gt; 
&lt;h2&gt;Disclaimer&lt;/h2&gt; 
&lt;p&gt;.. disclaimer-start&lt;/p&gt; 
&lt;p&gt;Instaloader is in no way affiliated with, authorized, maintained or endorsed by Instagram or any of its affiliates or subsidiaries. This is an independent and unofficial project. Use at your own risk.&lt;/p&gt; 
&lt;p&gt;Instaloader is licensed under an MIT license. Refer to &lt;code&gt;LICENSE&lt;/code&gt; file for more information.&lt;/p&gt; 
&lt;p&gt;.. disclaimer-end&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>vaastav/Fantasy-Premier-League</title>
      <link>https://github.com/vaastav/Fantasy-Premier-League</link>
      <description>&lt;p&gt;Creates a .csv file of all players in the English Player League with their respective team and total fantasy points&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href="https://gitspo.com/mentions/vaastav/Fantasy-Premier-League"&gt;&lt;img src="https://gitspo.com/badges/mentions/vaastav/Fantasy-Premier-League?style=flat-square" alt="GitSpo Mentions" /&gt;&lt;/a&gt; &lt;a href="https://www.paypal.com/donate?hosted_button_id=RQ2V64LXSKPV4"&gt;&lt;img src="https://www.paypalobjects.com/en_US/i/btn/btn_donateCC_LG.gif" alt="paypal" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Fantasy-Premier-League&lt;/h1&gt; 
&lt;h1&gt;NOTICE&lt;/h1&gt; 
&lt;p&gt;This weekly updates for the repository have been stopped at the end of the 2024-25 season. NO weekly updates will be posted to this repository after the 2024-25 season.&lt;/p&gt; 
&lt;p&gt;Instead, there will be 3 major updates posted to this repository:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;One at the start of the season&lt;/li&gt; 
 &lt;li&gt;One at the end of the January transfer window&lt;/li&gt; 
 &lt;li&gt;One at the end of the season&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Description&lt;/h2&gt; 
&lt;p&gt;A FPL library that gets all the basic stats for each player, gw-specific data for each player and season history of each player&lt;/p&gt; 
&lt;h3&gt;How to CIte this dataset?&lt;/h3&gt; 
&lt;p&gt;BibTeX:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@misc{anand2016fantasypremierleague,
  title = {{FPL Historical Dataset}},
  author = {Anand, Vaastav},
  year = {2022},
  howpublished = {Retrieved August 2022 from \url{https://github.com/vaastav/Fantasy-Premier-League/}}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Acknowledgement&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;rin-hairie for adding master team lists and merge scripts&lt;/li&gt; 
 &lt;li&gt;ergest for adding merged_gw.csv files for 2016-17 and 2017-18 seasons&lt;/li&gt; 
 &lt;li&gt;BDooley11 for providing top managers script&lt;/li&gt; 
 &lt;li&gt;speeder1987 for providing 2018/19 fixtures.csv file&lt;/li&gt; 
 &lt;li&gt;ravgeetdhillon for github actions automation for data update&lt;/li&gt; 
 &lt;li&gt;kz4killua for fixing GW37 data for the 21-22 season&lt;/li&gt; 
 &lt;li&gt;SaintJuniper for id-dictionary update for the 21-22 season&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;FAQ&lt;/h2&gt; 
&lt;h3&gt;Data Structure&lt;/h3&gt; 
&lt;p&gt;The data folder contains the data from past seasons as well as the current season. It is structured as follows:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;season/cleaned_players.csv : The overview stats for the season&lt;/li&gt; 
 &lt;li&gt;season/gws/gw_number.csv : GW-specific stats for the particular season&lt;/li&gt; 
 &lt;li&gt;season/gws/merged_gws.csv : GW-by-GW stats for each player in a single file&lt;/li&gt; 
 &lt;li&gt;season/players/player_name/gws.csv : GW-by-GW stats for that specific player&lt;/li&gt; 
 &lt;li&gt;season/players/player_name/history.csv : Prior seasons history stats for that specific player.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Accessing the Data Directly in Python&lt;/h3&gt; 
&lt;p&gt;You can access data files within this repository programmatically using Python and the &lt;code&gt;pandas&lt;/code&gt; library. Below is an example using the &lt;code&gt;data/2023-24/gws/merged_gw.csv&lt;/code&gt; file. Similar methods can be applied to other data files in the repository. Note this is using the raw URL for direct file access, bypassing the GitHub UI.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import pandas as pd

# URL of the CSV file (example)
url = "https://raw.githubusercontent.com/vaastav/Fantasy-Premier-League/master/data/2023-24/gws/merged_gw.csv"

# Read the CSV file into a pandas DataFrame
df = pd.read_csv(url)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Player Position Data&lt;/h3&gt; 
&lt;p&gt;In players_raw.csv, element_type is the field that corresponds to the position. 1 = GK 2 = DEF 3 = MID 4 = FWD&lt;/p&gt; 
&lt;h3&gt;Errata&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;GW35 expected points data is wrong (all values are 0).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Contributing&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;If you feel like there is some data that is missing which you would like to see, then please feel free to create a PR or create an issue highlighting what is missing and what you would like to be added&lt;/li&gt; 
 &lt;li&gt;If you have access to old data (pre-2016) then please feel free to create Pull Requests adding the data to the repo or create an issue with links to old data and I will add them myself.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Using&lt;/h3&gt; 
&lt;p&gt;If you use data from here for your website or blog posts, then I would humbly request that you please add a link back to this repo as the data source (and I would in turn add a link to your post/site as a notable usage of this repo).&lt;/p&gt; 
&lt;h2&gt;Downloading Your Team Data&lt;/h2&gt; 
&lt;p&gt;You can download the data for your team by executing the following steps:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;python teams_scraper.py &amp;lt;team_id&amp;gt;
#Eg: python teams_scraper.py 4582
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will create a new folder called "team_&amp;lt;team_id&amp;gt;_data18-19" with individual files of all the important data&lt;/p&gt; 
&lt;h1&gt;Notable Usages of this Repository&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://medium.com/arcticdb/picking-the-ultimate-fantasy-premier-league-team-with-arcticdb-4ae31ff5d817"&gt;Picking the Ultimate Fantasy Premier League Team with ArcticDB by Matthew Simpson&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/arifpras/BelutListrik"&gt;Analysing Fantasy Premier League data in R Course by Arif P. Sulistiono&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/francescobarbara/FPL-point-predictor-via-random-forests"&gt;Point Predictor via Random Forests by Francesco Barbara&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://www.dtsquared.co.uk/money-football-how-will-our-virtual-football-team-selected-entirely-by-machine-learning-compete-in-the-big-leagues/"&gt;Money (Foot)Ball – how will our virtual football team selected entirely by Machine Learning compete in the big leagues?&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://towardsdatascience.com/an-introduction-to-sql-using-fpl-data-8314ec982308"&gt;An introduction to SQL using FPL data by Liam Connors&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://alpscode.com/blog/hindsight-optimization/"&gt;Hindsight Optimization for FPL by Sertalp B. Cay&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://medium.com/the-sports-scientist/how-i-used-data-science-to-get-into-the-top-1-on-the-return-to-fantasy-premier-league-98829d4f65e5"&gt;Data Science to get top 1% on return to FPL by James Asher&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="http://www.fpldash.com"&gt;FPLDASH: A customizable Fantasy Premier League Dashboard by Jin Hyun Cheong&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://www.splunk.com/en_us/blog/machine-learning/how-to-win-at-fantasy-football-with-splunk-and-machine-learning-part-1.html"&gt;How to win at Fantasy Football with Splunk and Machine Learning by Rupert Truman&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=LzEuweGrHvc"&gt;2019-20 Winner Joshua Bull's Oxford Maths Public Lecture&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://twitter.com/theFPLkiwi/status/1297619700206239746?s=20"&gt;2019-20 Lottery Analysis by @theFPLKiwi&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://www.fantasynutmeg.com/history"&gt;Fantasy Nutmeg Website by code247&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://medium.com/@2017csb1079/fantasy-premier-league-19-20-a-review-part-1-basics-167e610e229"&gt;Fantasy Premier League 19/20, a review by Hersh Dhillon&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://medium.com/@erwindrarusli/visualisasi-data-fantasy-premier-league-19-20-a80aaf097a21"&gt;Visualisasi Data: Fantasy Premier League 19/20 by Erwindra Rusli&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://keytodatascience.com/fpl-machine-learning/"&gt;Machine Learning Model by pratz&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/FantasyPL/comments/erfdy1/a_plot_of_xg_vs_xa_for_for_attacking_midsforwards/"&gt;xA vs xG for Attacking Midfielders/Forwards by u/JLane1996&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/reddevils/comments/ecbn9j/corrected_plot_of_goals_vs_expected_goals_this/fba8vs3/"&gt;Expected Goals vs Actual Goals for Manchester United by u/JLane1996&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/tableau/comments/e2j0uq/my_first_tableu_viz_fpl/"&gt;Tableau Viz by u/richkelana&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/FantasyPL/comments/dz04hf/top_players_against_gw13_rival/"&gt;Top Players against GW13 rival by u/LiuSiuMing&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://mbarnfield.github.io/fpl.html"&gt;Captaincy Choice GW4 2019-20 post by Matthew Barnfield&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="http://www.didjfin.no/blog/fpl/fantasy-premier-league-data/"&gt;Building a dataset for Fantasy Premier League analysis by djfinnoy&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://whogottheassist.com/value-in-fpl-2019-20-report/"&gt;Value in FPL 2019-20 Report by Who Got The Assist?&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://whogottheassist.com/talisman-theory-part-one-2018-19-report/"&gt;Talisman Theory 2018-19 Report by Who Got The Assist?&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://twitter.com/fplscrapR"&gt;Historical Analyses in fplscrapR by Rasmus Chrisentsen&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://medium.com/@joseph.m.oconnor.88/linearly-optimising-fantasy-premier-league-teams-3b76e9694877"&gt;Linearly Optimising Fantasy Premier League Teams by Joseph O'Connor&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://medium.com/@sol.paul/how-to-win-at-fantasy-premier-league-using-data-part-1-forecasting-with-deep-learning-bf121f38643a"&gt;How to Win at Fantasy Premier league using Deep learning by Paul Solomon&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://api.better-fpl.com/graphql"&gt;graphql API by u/jeppews&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/alsgregory/Fantasy-Football"&gt;FPL modeling and prediction by @alsgregory&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="http://fpl.co.id/tools/talismans/"&gt;FPL.co.id Talismans by @FPL_COID&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://twitter.com/neilswmurrayFPL/status/1147407501736009728"&gt;Leicester City Brendan Rodgers Impact Analysis on twitter by @neilswmurrayFPL&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://twitter.com/StatOnScout"&gt;Stat Analysis on twitter by @StatOnScout&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://www.linkedin.com/pulse/whoever-wins-2019-uefa-europe-league-final-still-ends-velko-kamenov/"&gt;Arsenal-Chelsea LinkedIn article by Velko Kamenov&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://towardsdatascience.com/mythbusting-fantasy-premier-league-form-over-fixtures-eecf9022e834"&gt;Form vs Fixture Medium article by JinHyunCheong&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/dataisbeautiful/comments/9zlx14/points_per_game_vs_predictability_after_12_weeks/"&gt;Visualization by u/dkattir&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/FantasyPL/comments/9bjwra/created_a_very_crude_and_basic_comparison_chart/"&gt;Visualization by u/Dray11&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="http://fantasy.elek.hr/"&gt;Visualization website by @antoniaelek&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://medium.com/datacomics/building-an-fpl-captain-classifier-cf4ee343ebcc"&gt;FPL Captain Classifier by Raghunandh GS&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="http://vaastavanand.com/blog/"&gt;My Personal Blog&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://fpl.zoid.dev"&gt;FPL.zoid.dev - Query FPL data with SQL in your browser&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://fpl-pl-table.streamlit.app/"&gt;Premier League Table by FPL Points by Edward F&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://fpl-manager-medals.streamlit.app/"&gt;FPL Manager Medals by Edward F&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://fpl.infinitetrooper.com/"&gt;SiegFPL by @infinitetrooper&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>aliasrobotics/cai</title>
      <link>https://github.com/aliasrobotics/cai</link>
      <description>&lt;p&gt;Cybersecurity AI (CAI), the framework for AI Security&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Cybersecurity AI (&lt;code&gt;CAI&lt;/code&gt;)&lt;/h1&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;a align="center" href="" target="https://github.com/aliasrobotics/CAI"&gt; &lt;img width="100%" src="https://github.com/aliasrobotics/cai/raw/main/media/cai.png" /&gt; &lt;/a&gt; &lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://badge.fury.io/py/cai-framework"&gt;&lt;img src="https://badge.fury.io/py/cai-framework.svg?sanitize=true" alt="version" /&gt;&lt;/a&gt; &lt;a href="https://pypistats.org/packages/cai-framework"&gt;&lt;img src="https://img.shields.io/pypi/dm/cai-framework" alt="downloads" /&gt;&lt;/a&gt; &lt;a href="https://github.com/aliasrobotics/cai"&gt;&lt;img src="https://img.shields.io/badge/Linux-Supported-brightgreen?logo=linux&amp;amp;logoColor=white" alt="Linux" /&gt;&lt;/a&gt; &lt;a href="https://github.com/aliasrobotics/cai"&gt;&lt;img src="https://img.shields.io/badge/OS%20X-Supported-brightgreen?logo=apple&amp;amp;logoColor=white" alt="OS X" /&gt;&lt;/a&gt; &lt;a href="https://github.com/aliasrobotics/cai"&gt;&lt;img src="https://img.shields.io/badge/Windows-Supported-brightgreen?logo=windows&amp;amp;logoColor=white" alt="Windows" /&gt;&lt;/a&gt; &lt;a href="https://github.com/aliasrobotics/cai"&gt;&lt;img src="https://img.shields.io/badge/Android-Supported-brightgreen?logo=android&amp;amp;logoColor=white" alt="Android" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/fnUFcTaQAC"&gt;&lt;img src="https://img.shields.io/badge/Discord-7289DA?logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/pdf/2504.06017"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2504.06017-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2506.23592"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2506.23592-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;Cybersecurity AI (CAI) is a lightweight, open-source framework that empowers security professionals to build and deploy AI-powered offensive and defensive automation. CAI is the &lt;em&gt;de facto&lt;/em&gt; framework for AI Security, already used by thousands of individual users and hundreds of organizations. Whether you're a security researcher, ethical hacker, IT professional, or organization looking to enhance your security posture, CAI provides the building blocks to create specialized AI agents that can assist with mitigation, vulnerability discovery, exploitation, and security assessment.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;🤖 &lt;strong&gt;300+ AI Models&lt;/strong&gt;: Support for OpenAI, Anthropic, DeepSeek, Ollama, and more&lt;/li&gt; 
 &lt;li&gt;🔧 &lt;strong&gt;Built-in Security Tools&lt;/strong&gt;: Ready-to-use tools for reconnaissance, exploitation, and privilege escalation&lt;/li&gt; 
 &lt;li&gt;🏆 &lt;strong&gt;Battle-tested&lt;/strong&gt;: Proven in HackTheBox CTFs, bug bounties, and real-world security &lt;a href="https://aliasrobotics.com/case-studies-robot-cybersecurity.php"&gt;case studies&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;🎯 &lt;strong&gt;Agent-based Architecture&lt;/strong&gt;: Modular design with specialized agents for different security tasks&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Read the technical report: &lt;a href="https://arxiv.org/pdf/2504.06017"&gt;CAI: An Open, Bug Bounty-Ready Cybersecurity AI&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;a href="https://aliasrobotics.com/case-study-ecoforest.php"&gt;&lt;code&gt;OT&lt;/code&gt; - CAI and alias0 on: Ecoforest Heat Pumps&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://aliasrobotics.com/case-study-cai-mir.php"&gt;&lt;code&gt;Robotics&lt;/code&gt; - CAI and alias0 on: Mobile Industrial Robots (MiR)&lt;/a&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;CAI discovers critical vulnerability in Ecoforest heat pumps allowing unauthorized remote access and potential catastrophic failures. AI-powered security testing reveals exposed credentials and DES encryption weaknesses affecting all of their deployed units across Europe.&lt;/td&gt; 
   &lt;td&gt;CAI-powered security testing of MiR (Mobile Industrial Robot) platform through automated ROS message injection attacks. This study demonstrates how AI-driven vulnerability discovery can expose unauthorized access to robot control systems and alarm triggers.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://aliasrobotics.com/case-study-ecoforest.php"&gt;&lt;img src="https://aliasrobotics.com/img/case-study-portada-ecoforest.png" alt="" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://aliasrobotics.com/case-study-cai-mir.php"&gt;&lt;img src="https://aliasrobotics.com/img/case-study-portada-mir-cai.png" alt="" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;a href="https://aliasrobotics.com/case-study-mercado-libre.php"&gt;&lt;code&gt;IT&lt;/code&gt; (Web) - CAI and alias0 on: Mercado Libre's e-commerce&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://aliasrobotics.com/case-study-cai-mqtt-broker.php"&gt;&lt;code&gt;OT&lt;/code&gt; - CAI and alias0 on: MQTT broker&lt;/a&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;CAI-powered API vulnerability discovery at Mercado Libre through automated enumeration attacks. This study demonstrates how AI-driven security testing can expose user data exposure risks in e-commerce platforms at scale.&lt;/td&gt; 
   &lt;td&gt;CAI-powered testing exposed critical flaws in an MQTT broker within a Dockerized OT network. Without authentication, CAI subscribed to temperature and humidity topics and injected false values, corrupting data shown in Grafana dashboards.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://aliasrobotics.com/case-study-mercado-libre.php"&gt;&lt;img src="https://aliasrobotics.com/img/case-study-portada-mercado-libre.png" alt="" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://aliasrobotics.com/case-study-cai-mqtt-broker.php"&gt;&lt;img src="https://aliasrobotics.com/img/case-study-portada-mqtt-broker-cai.png" alt="" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING] &lt;span&gt;⚠&lt;/span&gt; CAI is in active development, so don't expect it to work flawlessly. Instead, contribute by raising an issue or &lt;a href="https://github.com/aliasrobotics/cai/pulls"&gt;sending a PR&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;Access to this library and the use of information, materials (or portions thereof), is &lt;strong&gt;&lt;u&gt;not intended&lt;/u&gt;, and is &lt;u&gt;prohibited&lt;/u&gt;, where such access or use violates applicable laws or regulations&lt;/strong&gt;. By no means the authors encourage or promote the unauthorized tampering with running systems. This can cause serious human harm and material damages.&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;By no means the authors of CAI encourage or promote the unauthorized tampering with compute systems. Please don't use the source code in here for cybercrime. &lt;u&gt;Pentest for good instead&lt;/u&gt;&lt;/em&gt;. By downloading, using, or modifying this source code, you agree to the terms of the &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/LICENSE"&gt;&lt;code&gt;LICENSE&lt;/code&gt;&lt;/a&gt; and the limitations outlined in the &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/DISCLAIMER"&gt;&lt;code&gt;DISCLAIMER&lt;/code&gt;&lt;/a&gt; file.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;&lt;span&gt;🔖&lt;/span&gt; Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#cybersecurity-ai-cai"&gt;Cybersecurity AI (&lt;code&gt;CAI&lt;/code&gt;)&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#bookmark-table-of-contents"&gt;&lt;span&gt;🔖&lt;/span&gt; Table of Contents&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-milestones"&gt;🎯 Milestones&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#pocs"&gt;PoCs&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#motivation"&gt;Motivation&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#bust_in_silhouette-why-cai"&gt;&lt;span&gt;👤&lt;/span&gt; Why CAI?&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#ethical-principles-behind-cai"&gt;Ethical principles behind CAI&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#closed-source-alternatives"&gt;Closed-source alternatives&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#learn---cai-fluency"&gt;Learn - &lt;code&gt;CAI&lt;/code&gt; Fluency&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#nut_and_bolt-install"&gt;&lt;span&gt;🔩&lt;/span&gt; Install&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#os-x"&gt;OS X&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#ubuntu-2404"&gt;Ubuntu 24.04&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#ubuntu-2004"&gt;Ubuntu 20.04&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#windows-wsl"&gt;Windows WSL&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#android"&gt;Android&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#nut_and_bolt-setup-env-file"&gt;&lt;span&gt;🔩&lt;/span&gt; Setup &lt;code&gt;.env&lt;/code&gt; file&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-custom-openai-base-url-support"&gt;🔹 Custom OpenAI Base URL Support&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#triangular_ruler-architecture"&gt;&lt;span&gt;📐&lt;/span&gt; Architecture:&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-agent"&gt;🔹 Agent&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-tools"&gt;🔹 Tools&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-handoffs"&gt;🔹 Handoffs&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-patterns"&gt;🔹 Patterns&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-turns-and-interactions"&gt;🔹 Turns and Interactions&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-tracing"&gt;🔹 Tracing&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#-human-in-the-loop-hitl"&gt;🔹 Human-In-The-Loop (HITL)&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#rocket-quickstart"&gt;&lt;span&gt;🚀&lt;/span&gt; Quickstart&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#environment-variables"&gt;Environment Variables&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#openrouter-integration"&gt;OpenRouter Integration&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#mcp"&gt;MCP&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#development"&gt;Development&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#contributions"&gt;Contributions&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#optional-requirements-caiextensions"&gt;Optional Requirements: caiextensions&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#information_source-usage-data-collection"&gt;&lt;span&gt;ℹ&lt;/span&gt; Usage Data Collection&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#reproduce-ci-setup-locally"&gt;Reproduce CI-Setup locally&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#faq"&gt;FAQ&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#citation"&gt;Citation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#acknowledgements"&gt;Acknowledgements&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🎯 Milestones&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://app.hackthebox.com/users/2268644"&gt;&lt;img src="https://img.shields.io/badge/HTB_ranking-top_90_Spain_(5_days)-red.svg?sanitize=true" alt="" /&gt;&lt;/a&gt; &lt;a href="https://app.hackthebox.com/users/2268644"&gt;&lt;img src="https://img.shields.io/badge/HTB_ranking-top_50_Spain_(6_days)-red.svg?sanitize=true" alt="" /&gt;&lt;/a&gt; &lt;a href="https://app.hackthebox.com/users/2268644"&gt;&lt;img src="https://img.shields.io/badge/HTB_ranking-top_30_Spain_(7_days)-red.svg?sanitize=true" alt="" /&gt;&lt;/a&gt; &lt;a href="https://app.hackthebox.com/users/2268644"&gt;&lt;img src="https://img.shields.io/badge/HTB_ranking-top_500_World_(7_days)-red.svg?sanitize=true" alt="" /&gt;&lt;/a&gt; &lt;a href="https://ctf.hackthebox.com/event/2000/scoreboard"&gt;&lt;img src="https://img.shields.io/badge/HTB_%22Human_vs_AI%22_CTF-top_1_(AIs)_world-red.svg?sanitize=true" alt="" /&gt;&lt;/a&gt; &lt;a href="https://ctf.hackthebox.com/event/2000/scoreboard"&gt;&lt;img src="https://img.shields.io/badge/HTB_%22Human_vs_AI%22_CTF-top_1_Spain-red.svg?sanitize=true" alt="" /&gt;&lt;/a&gt; &lt;a href="https://ctf.hackthebox.com/event/2000/scoreboard"&gt;&lt;img src="https://img.shields.io/badge/HTB_%22Human_vs_AI%22_CTF-top_20_World-red.svg?sanitize=true" alt="" /&gt;&lt;/a&gt; &lt;a href="https://ctf.hackthebox.com/event/2000/scoreboard"&gt;&lt;img src="https://img.shields.io/badge/HTB_%22Human_vs_AI%22_CTF-750_$-yellow.svg?sanitize=true" alt="" /&gt;&lt;/a&gt; &lt;a href="https://lu.ma/roboticshack?tk=RuryKF"&gt;&lt;img src="https://img.shields.io/badge/Mistral_AI_Robotics_Hackathon-2500_$-yellow.svg?sanitize=true" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;PoCs&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;CAI with &lt;code&gt;alias0&lt;/code&gt; on ROS message injection attacks in MiR-100 robot&lt;/th&gt; 
   &lt;th&gt;CAI with &lt;code&gt;alias0&lt;/code&gt; on API vulnerability discovery at Mercado Libre&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://asciinema.org/a/dNv705hZel2Rzrw0cju9HBGPh"&gt;&lt;img src="https://asciinema.org/a/dNv705hZel2Rzrw0cju9HBGPh.svg?sanitize=true" alt="asciicast" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://asciinema.org/a/9Hc9z1uFcdNjqP3bY5y7wO1Ww"&gt;&lt;img src="https://asciinema.org/a/9Hc9z1uFcdNjqP3bY5y7wO1Ww.svg?sanitize=true" alt="asciicast" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;CAI on JWT@PortSwigger CTF — Cybersecurity AI&lt;/th&gt; 
   &lt;th&gt;CAI on HackableII Boot2Root CTF — Cybersecurity AI&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://asciinema.org/a/713487"&gt;&lt;img src="https://asciinema.org/a/713487.svg?sanitize=true" alt="asciicast" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://asciinema.org/a/713485"&gt;&lt;img src="https://asciinema.org/a/713485.svg?sanitize=true" alt="asciicast" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;More case studies and PoCs are available at &lt;a href="https://aliasrobotics.com/case-studies-robot-cybersecurity.php"&gt;https://aliasrobotics.com/case-studies-robot-cybersecurity.php&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Motivation&lt;/h2&gt; 
&lt;h3&gt;&lt;span&gt;👤&lt;/span&gt; Why CAI?&lt;/h3&gt; 
&lt;p&gt;The cybersecurity landscape is undergoing a dramatic transformation as AI becomes increasingly integrated into security operations. &lt;strong&gt;We predict that by 2028, AI-powered security testing tools will outnumber human pentesters&lt;/strong&gt;. This shift represents a fundamental change in how we approach cybersecurity challenges. &lt;em&gt;AI is not just another tool - it's becoming essential for addressing complex security vulnerabilities and staying ahead of sophisticated threats. As organizations face more advanced cyber attacks, AI-enhanced security testing will be crucial for maintaining robust defenses.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;This work builds upon prior efforts[^4] and similarly, we believe that democratizing access to advanced cybersecurity AI tools is vital for the entire security community. That's why we're releasing Cybersecurity AI (&lt;code&gt;CAI&lt;/code&gt;) as an open source framework. Our goal is to empower security researchers, ethical hackers, and organizations to build and deploy powerful AI-driven security tools. By making these capabilities openly available, we aim to level the playing field and ensure that cutting-edge security AI technology isn't limited to well-funded private companies or state actors.&lt;/p&gt; 
&lt;p&gt;Bug Bounty programs have become a cornerstone of modern cybersecurity, providing a crucial mechanism for organizations to identify and fix vulnerabilities in their systems before they can be exploited. These programs have proven highly effective at securing both public and private infrastructure, with researchers discovering critical vulnerabilities that might have otherwise gone unnoticed. CAI is specifically designed to enhance these efforts by providing a lightweight, ergonomic framework for building specialized AI agents that can assist in various aspects of Bug Bounty hunting - from initial reconnaissance to vulnerability validation and reporting. Our framework aims to augment human expertise with AI capabilities, helping researchers work more efficiently and thoroughly in their quest to make digital systems more secure.&lt;/p&gt; 
&lt;h3&gt;Ethical principles behind CAI&lt;/h3&gt; 
&lt;p&gt;You might be wondering if releasing CAI &lt;em&gt;in-the-wild&lt;/em&gt; given its capabilities and security implications is ethical. Our decision to open-source this framework is guided by two core ethical principles:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Democratizing Cybersecurity AI&lt;/strong&gt;: We believe that advanced cybersecurity AI tools should be accessible to the entire security community, not just well-funded private companies or state actors. By releasing CAI as an open source framework, we aim to empower security researchers, ethical hackers, and organizations to build and deploy powerful AI-driven security tools, leveling the playing field in cybersecurity.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Transparency in AI Security Capabilities&lt;/strong&gt;: Based on our research results, understanding of the technology, and dissection of top technical reports, we argue that current LLM vendors are undermining their cybersecurity capabilities. This is extremely dangerous and misleading. By developing CAI openly, we provide a transparent benchmark of what AI systems can actually do in cybersecurity contexts, enabling more informed decisions about security postures.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;CAI is built on the following core principles:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Cybersecurity oriented AI framework&lt;/strong&gt;: CAI is specifically designed for cybersecurity use cases, aiming at semi- and fully-automating offensive and defensive security tasks.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Open source, free for research&lt;/strong&gt;: CAI is open source and free for research purposes. We aim at democratizing access to AI and Cybersecurity. For professional or commercial use, including on-premise deployments, dedicated technical support and custom extensions &lt;a href="mailto:research@aliasrobotics.com"&gt;reach out&lt;/a&gt; to obtain a license.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Lightweight&lt;/strong&gt;: CAI is designed to be fast, and easy to use.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Modular and agent-centric design&lt;/strong&gt;: CAI operates on the basis of agents and agentic patterns, which allows flexibility and scalability. You can easily add the most suitable agents and pattern for your cybersecuritytarget case.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Tool-integration&lt;/strong&gt;: CAI integrates already built-in tools, and allows the user to integrate their own tools with their own logic easily.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Logging and tracing integrated&lt;/strong&gt;: using &lt;a href="https://github.com/Arize-ai/phoenix"&gt;&lt;code&gt;phoenix&lt;/code&gt;&lt;/a&gt;, the open source tracing and logging tool for LLMs. This provides the user with a detailed traceability of the agents and their execution.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-Model Support&lt;/strong&gt;: more than 300 supported and empowered by &lt;a href="https://github.com/BerriAI/litellm"&gt;LiteLLM&lt;/a&gt;. The most popular providers: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Anthropic&lt;/strong&gt;: &lt;code&gt;Claude 3.7&lt;/code&gt;, &lt;code&gt;Claude 3.5&lt;/code&gt;, &lt;code&gt;Claude 3&lt;/code&gt;, &lt;code&gt;Claude 3 Opus&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;OpenAI&lt;/strong&gt;: &lt;code&gt;O1&lt;/code&gt;, &lt;code&gt;O1 Mini&lt;/code&gt;, &lt;code&gt;O3 Mini&lt;/code&gt;, &lt;code&gt;GPT-4o&lt;/code&gt;, &lt;code&gt;GPT-4.5 Preview&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;DeepSeek&lt;/strong&gt;: &lt;code&gt;DeepSeek V3&lt;/code&gt;, &lt;code&gt;DeepSeek R1&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Ollama&lt;/strong&gt;: &lt;code&gt;Qwen2.5 72B&lt;/code&gt;, &lt;code&gt;Qwen2.5 14B&lt;/code&gt;, etc&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Closed-source alternatives&lt;/h3&gt; 
&lt;p&gt;Cybersecurity AI is a critical field, yet many groups are misguidedly pursuing it through closed-source methods for pure economic return, leveraging similar techniques and building upon existing closed-source (&lt;em&gt;often third-party owned&lt;/em&gt;) models. This approach not only squanders valuable engineering resources but also represents an economic waste and results in redundant efforts, as they often end up reinventing the wheel. Here are some of the closed-source initiatives we keep track of and attempting to leverage genAI and agentic frameworks in cybersecurity AI:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.acyber.co/"&gt;Autonomous Cyber&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://cracken.ai/"&gt;CrackenAGI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ethiack.com/"&gt;ETHIACK&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://horizon3.ai/"&gt;Horizon3&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.kindo.ai/"&gt;Kindo&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://lakera.ai"&gt;Lakera&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/www.mindfort.ai"&gt;Mindfort&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://mindgard.ai/"&gt;Mindgard&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ndaysecurity.com/"&gt;NDAY Security&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.runsybil.com"&gt;Runsybil&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.selfhack.fi"&gt;Selfhack&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://squr.ai/"&gt;SQUR&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://staris.tech/"&gt;Staris&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.sxipher.com/"&gt;Sxipher&lt;/a&gt; (seems discontinued)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.terra.security"&gt;Terra Security&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://xint.io/"&gt;Xint&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.xbow.com"&gt;XBOW&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.zeropath.com"&gt;ZeroPath&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.zynap.com"&gt;Zynap&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://7ai.com"&gt;7ai&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Learn - &lt;code&gt;CAI&lt;/code&gt; Fluency&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;a align="center" href="" target="https://github.com/aliasrobotics/CAI"&gt; &lt;img width="100%" src="https://github.com/aliasrobotics/cai/raw/main/media/caiedu.PNG" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;English&lt;/th&gt; 
   &lt;th&gt;Spanish&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Episode 0&lt;/strong&gt;: What is CAI?&lt;/td&gt; 
   &lt;td&gt;Cybersecurity AI (&lt;code&gt;CAI&lt;/code&gt;) explained&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=nBdTxbKM4oo"&gt;&lt;img src="https://img.youtube.com/vi/nBdTxbKM4oo/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=FaUL9HXrQ5k"&gt;&lt;img src="https://img.youtube.com/vi/FaUL9HXrQ5k/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Episode 1&lt;/strong&gt;: The &lt;code&gt;CAI&lt;/code&gt; Framework&lt;/td&gt; 
   &lt;td&gt;Vision &amp;amp; Ethics - Explore the core motivation behind CAI and delve into the crucial ethical principles guiding its development. Understand the motivation behind CAI and how you can actively contribute to the future of cybersecurity and the CAI framework.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=QEiGdsMf29M&amp;amp;list=PLLc16OUiZWd4RuFdN5_Wx9xwjCVVbopzr&amp;amp;index=3"&gt;&lt;img src="https://img.youtube.com/vi/QEiGdsMf29M/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Episode 2&lt;/strong&gt;: From Zero to Cyber Hero&lt;/td&gt; 
   &lt;td&gt;Breaking into Cybersecurity with AI - A comprehensive guide for complete beginners to become cybersecurity practitioners using CAI and AI tools. Learn how to leverage artificial intelligence to accelerate your cybersecurity learning journey, from understanding basic security concepts to performing real-world security assessments, all without requiring prior cybersecurity experience.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=hSTLHOOcQoY&amp;amp;list=PLLc16OUiZWd4RuFdN5_Wx9xwjCVVbopzr&amp;amp;index=14"&gt;&lt;img src="https://img.youtube.com/vi/hSTLHOOcQoY/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Episode 3&lt;/strong&gt;: Vibe-Hacking Tutorial&lt;/td&gt; 
   &lt;td&gt;"My first Hack" - A Vibe-Hacking guide for newbies. We demonstrate a simple web security hack using a default agent and show how to leverage tools and interpret CIA output with the help of the CAI Python API. You'll also learn to compare different LLM models to find the best fit for your hacking endeavors.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=9vZ_Iyex7uI&amp;amp;list=PLLc16OUiZWd4RuFdN5_Wx9xwjCVVbopzr&amp;amp;index=1"&gt;&lt;img src="https://img.youtube.com/vi/9vZ_Iyex7uI/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=iAOMaI1ftiA&amp;amp;list=PLLc16OUiZWd4RuFdN5_Wx9xwjCVVbopzr&amp;amp;index=2"&gt;&lt;img src="https://img.youtube.com/vi/iAOMaI1ftiA/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Episode 4&lt;/strong&gt;: Intro ReAct&lt;/td&gt; 
   &lt;td&gt;The Evolution of LLMs - Learn how LLMs evolved from basic language models to advanced multiagency AI systems. From basic LLMs to Chain-of-Thought and Reasoning LLMs towards ReAct and Multi-Agent Architectures. Get to know the basic terms&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=tLdFO1flj_o&amp;amp;list=PLLc16OUiZWd4RuFdN5_Wx9xwjCVVbopzr&amp;amp;index=13"&gt;&lt;img src="https://img.youtube.com/vi/tLdFO1flj_o/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Episode 5&lt;/strong&gt;: CAI on CTF challenges&lt;/td&gt; 
   &lt;td&gt;Dive into Capture The Flag (CTF) competitions using CAI. Learn how to leverage AI agents to solve various cybersecurity challenges including web exploitation, cryptography, reverse engineering, and forensics. Discover how to configure CAI for competitive hacking scenarios and maximize your CTF performance with intelligent automation.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=MrXTQ0e2to4&amp;amp;list=PLLc16OUiZWd4RuFdN5_Wx9xwjCVVbopzr&amp;amp;index=13"&gt;&lt;img src="https://img.youtube.com/vi/MrXTQ0e2to4/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=r9US_JZa9_c&amp;amp;list=PLLc16OUiZWd4RuFdN5_Wx9xwjCVVbopzr&amp;amp;index=12"&gt;&lt;img src="https://img.youtube.com/vi/r9US_JZa9_c/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Annex 1&lt;/strong&gt;: &lt;code&gt;CAI&lt;/code&gt; 0.5.x release&lt;/td&gt; 
   &lt;td&gt;Introduce version 0.5 of &lt;code&gt;CAI&lt;/code&gt; including new multi-agent functionality, new commands such as &lt;code&gt;/history&lt;/code&gt;, &lt;code&gt;/compact&lt;/code&gt;, &lt;code&gt;/graph&lt;/code&gt; or &lt;code&gt;/memory&lt;/code&gt; and a case study showing how &lt;code&gt;CAI&lt;/code&gt; found a critical security flaw in OT heap pumps spread around the world.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=OPFH0ANUMMw"&gt;&lt;img src="https://img.youtube.com/vi/OPFH0ANUMMw/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=Q8AI4E4gH8k"&gt;&lt;img src="https://img.youtube.com/vi/Q8AI4E4gH8k/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Annex 2&lt;/strong&gt;: &lt;code&gt;CAI&lt;/code&gt; 0.4.x release and &lt;code&gt;alias0&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Introducing version 0.4 of &lt;code&gt;CAI&lt;/code&gt; with &lt;em&gt;streaming&lt;/em&gt; and improved MCP support. We also introduce &lt;code&gt;alias0&lt;/code&gt;, the Privacy-First Cybersecurity AI, a Model-of-Models Intelligence that implements a Privacy-by-Design architecture and obtains state-of-the-art results in cybersecurity benchmarks.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=NZjzfnvAZcc"&gt;&lt;img src="https://img.youtube.com/vi/NZjzfnvAZcc/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Annex 3&lt;/strong&gt;: Cybersecurity AI Community Meeting #1&lt;/td&gt; 
   &lt;td&gt;First Cybersecurity AI (&lt;code&gt;CAI&lt;/code&gt;) community meeting, over 40 participants from academia, industry, and defense gathered to discuss the open-source scaffolding behind CAI — a project designed to build agentic AI systems for cybersecurity that are open, modular, and Bug Bounty-ready.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.youtube.com/watch?v=4JqaTiVlgsw"&gt;&lt;img src="https://img.youtube.com/vi/4JqaTiVlgsw/0.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;&lt;span&gt;🔩&lt;/span&gt; Install&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install cai-framework
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Always create a new virtual environment to ensure proper dependency installation when updating CAI.&lt;/p&gt; 
&lt;p&gt;The following subsections provide a more detailed walkthrough on selected popular Operating Systems. Refer to the &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#development"&gt;Development&lt;/a&gt; section for developer-related install instructions.&lt;/p&gt; 
&lt;h3&gt;OS X&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;brew update &amp;amp;&amp;amp; \
    brew install git python@3.12

# Create virtual environment
python3.12 -m venv cai_env

# Install the package from the local directory
source cai_env/bin/activate &amp;amp;&amp;amp; pip install cai-framework

# Generate a .env file and set up with defaults
echo -e 'OPENAI_API_KEY="sk-1234"\nANTHROPIC_API_KEY=""\nOLLAMA=""\nPROMPT_TOOLKIT_NO_CPR=1\nCAI_STREAM=false' &amp;gt; .env

# Launch CAI
cai  # first launch it can take up to 30 seconds
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Ubuntu 24.04&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo apt-get update &amp;amp;&amp;amp; \
    sudo apt-get install -y git python3-pip python3.12-venv

# Create the virtual environment
python3.12 -m venv cai_env

# Install the package from the local directory
source cai_env/bin/activate &amp;amp;&amp;amp; pip install cai-framework

# Generate a .env file and set up with defaults
echo -e 'OPENAI_API_KEY="sk-1234"\nANTHROPIC_API_KEY=""\nOLLAMA=""\nPROMPT_TOOLKIT_NO_CPR=1\nCAI_STREAM=false' &amp;gt; .env

# Launch CAI
cai  # first launch it can take up to 30 seconds
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Ubuntu 20.04&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo apt-get update &amp;amp;&amp;amp; \
    sudo apt-get install -y software-properties-common

# Fetch Python 3.12
sudo add-apt-repository ppa:deadsnakes/ppa &amp;amp;&amp;amp; sudo apt update
sudo apt install python3.12 python3.12-venv python3.12-dev -y

# Create the virtual environment
python3.12 -m venv cai_env

# Install the package from the local directory
source cai_env/bin/activate &amp;amp;&amp;amp; pip install cai-framework

# Generate a .env file and set up with defaults
echo -e 'OPENAI_API_KEY="sk-1234"\nANTHROPIC_API_KEY=""\nOLLAMA=""\nPROMPT_TOOLKIT_NO_CPR=1\nCAI_STREAM=false' &amp;gt; .env

# Launch CAI
cai  # first launch it can take up to 30 seconds
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Windows WSL&lt;/h3&gt; 
&lt;p&gt;Go to the Microsoft page: &lt;a href="https://learn.microsoft.com/en-us/windows/wsl/install"&gt;https://learn.microsoft.com/en-us/windows/wsl/install&lt;/a&gt;. Here you will find all the instructions to install WSL&lt;/p&gt; 
&lt;p&gt;From Powershell write: wsl --install&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;
sudo apt-get update &amp;amp;&amp;amp; \
    sudo apt-get install -y git python3-pip python3-venv

# Create the virtual environment
python3 -m venv cai_env

# Install the package from the local directory
source cai_env/bin/activate &amp;amp;&amp;amp; pip install cai-framework

# Generate a .env file and set up with defaults
echo -e 'OPENAI_API_KEY="sk-1234"\nANTHROPIC_API_KEY=""\nOLLAMA=""\nPROMPT_TOOLKIT_NO_CPR=1\nCAI_STREAM=false' &amp;gt; .env

# Launch CAI
cai  # first launch it can take up to 30 seconds
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Android&lt;/h3&gt; 
&lt;p&gt;We recommend having at least 8 GB of RAM:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;First of all, install userland &lt;a href="https://play.google.com/store/apps/details?id=tech.ula&amp;amp;hl=es"&gt;https://play.google.com/store/apps/details?id=tech.ula&amp;amp;hl=es&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install Kali minimal in basic options (for free). [Or any other kali option if preferred]&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Update apt keys like in this example: &lt;a href="https://superuser.com/questions/1644520/apt-get-update-issue-in-kali"&gt;https://superuser.com/questions/1644520/apt-get-update-issue-in-kali&lt;/a&gt;, inside UserLand's Kali terminal execute&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Get new apt keys
wget http://http.kali.org/kali/pool/main/k/kali-archive-keyring/kali-archive-keyring_2024.1_all.deb

# Install new apt keys
sudo dpkg -i kali-archive-keyring_2024.1_all.deb &amp;amp;&amp;amp; rm kali-archive-keyring_2024.1_all.deb

# Update APT repository
sudo apt-get update

# CAI requieres python 3.12, lets install it (CAI for kali in Android)
sudo apt-get update &amp;amp;&amp;amp; sudo apt-get install -y git python3-pip build-essential zlib1g-dev libncurses5-dev libgdbm-dev libnss3-dev libssl-dev libreadline-dev libffi-dev libsqlite3-dev wget libbz2-dev pkg-config
wget https://www.python.org/ftp/python/3.12.4/Python-3.12.4.tar.xz
tar xf Python-3.12.4.tar.xz
cd ./configure --enable-optimizations
sudo make altinstall # This command takes long to execute

# Clone CAI's source code
git clone https://github.com/aliasrobotics/cai &amp;amp;&amp;amp; cd cai

# Create virtual environment
python3.12 -m venv cai_env

# Install the package from the local directory
source cai_env/bin/activate &amp;amp;&amp;amp; pip3 install -e .

# Generate a .env file and set up
cp .env.example .env  # edit here your keys/models

# Launch CAI
cai
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;&lt;span&gt;🔩&lt;/span&gt; Setup &lt;code&gt;.env&lt;/code&gt; file&lt;/h3&gt; 
&lt;p&gt;CAI leverages the &lt;code&gt;.env&lt;/code&gt; file to load configuration at launch. To facilitate the setup, the repo provides an exemplary &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/.env.example"&gt;&lt;code&gt;.env.example&lt;/code&gt;&lt;/a&gt; file provides a template for configuring CAI's setup and your LLM API keys to work with desired LLM models.&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;⚠&lt;/span&gt; Important:&lt;/p&gt; 
&lt;p&gt;CAI does NOT provide API keys for any model by default. Don't ask us to provide keys, use your own or host your own models.&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;⚠&lt;/span&gt; Note:&lt;/p&gt; 
&lt;p&gt;The OPENAI_API_KEY must not be left blank. It should contain either "sk-123" (as a placeholder) or your actual API key. See &lt;a href="https://github.com/aliasrobotics/cai/issues/27"&gt;https://github.com/aliasrobotics/cai/issues/27&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;⚠&lt;/span&gt; Note:&lt;/p&gt; 
&lt;p&gt;If you are using alias0 model, make sure that CAI is &amp;gt;0.4.0 version and here you have an .env example to be able to use it.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;OPENAI_API_KEY="sk-1234"
OLLAMA=""
ALIAS_API_KEY="&amp;lt;sk-your-key&amp;gt;"  # note, add yours
CAI_STEAM=False
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;🔹 Custom OpenAI Base URL Support&lt;/h3&gt; 
&lt;p&gt;CAI supports configuring a custom OpenAI API base URL via the &lt;code&gt;OPENAI_BASE_URL&lt;/code&gt; environment variable. This allows users to redirect API calls to a custom endpoint, such as a proxy or self-hosted OpenAI-compatible service.&lt;/p&gt; 
&lt;p&gt;Example &lt;code&gt;.env&lt;/code&gt; entry configuration:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;OLLAMA_API_BASE="https://custom-openai-proxy.com/v1"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or directly from the command line:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;OLLAMA_API_BASE="https://custom-openai-proxy.com/v1" cai
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;&lt;span&gt;📐&lt;/span&gt; Architecture:&lt;/h2&gt; 
&lt;p&gt;CAI focuses on making cybersecurity agent &lt;strong&gt;coordination&lt;/strong&gt; and &lt;strong&gt;execution&lt;/strong&gt; lightweight, highly controllable, and useful for humans. To do so it builds upon 7 pillars: &lt;code&gt;Agent&lt;/code&gt;s, &lt;code&gt;Tools&lt;/code&gt;, &lt;code&gt;Handoffs&lt;/code&gt;, &lt;code&gt;Patterns&lt;/code&gt;, &lt;code&gt;Turns&lt;/code&gt;, &lt;code&gt;Tracing&lt;/code&gt; and &lt;code&gt;HITL&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;                  ┌───────────────┐           ┌───────────┐
                  │      HITL     │◀─────────▶│   Turns   │
                  └───────┬───────┘           └───────────┘
                          │
                          ▼
┌───────────┐       ┌───────────┐       ┌───────────┐      ┌───────────┐
│  Patterns │◀─────▶│  Handoffs │◀────▶ │   Agents  │◀────▶│    LLMs   │
└───────────┘       └─────┬─────┘       └───────────┘      └───────────┘
                          │                   │
                          │                   ▼
┌────────────┐       ┌────┴──────┐       ┌───────────┐
│ Extensions │◀─────▶│  Tracing  │       │   Tools   │
└────────────┘       └───────────┘       └───────────┘
                                              │
                          ┌─────────────┬─────┴────┬─────────────┐
                          ▼             ▼          ▼             ▼
                    ┌───────────┐┌───────────┐┌────────────┐┌───────────┐
                    │ LinuxCmd  ││ WebSearch ││    Code    ││ SSHTunnel │
                    └───────────┘└───────────┘└────────────┘└───────────┘
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you want to dive deeper into the code, check the following files as a start point for using CAI:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/raw/main/src/cai/__init__.py"&gt;&lt;strong&gt;init&lt;/strong&gt;.py&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/raw/main/src/cai/cli.py"&gt;cli.py&lt;/a&gt; - entrypoint for command line interface&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/raw/main/src/cai/util.py"&gt;util.py&lt;/a&gt; - utility functions&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/raw/main/src/cai/agents"&gt;agents&lt;/a&gt; - Agent implementations&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/raw/main/src/cai/internal"&gt;internal&lt;/a&gt; - CAI internal functions (endpoints, metrics, logging, etc.)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/raw/main/src/cai/prompts"&gt;prompts&lt;/a&gt; - Agent Prompt Database&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/raw/main/src/cai/repl"&gt;repl&lt;/a&gt; - CLI aesthetics and commands&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/raw/main/src/cai/sdk"&gt;sdk&lt;/a&gt; - CAI command sdk&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/tree/main/src/cai/tools"&gt;tools&lt;/a&gt; - agent tools&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;🔹 Agent&lt;/h3&gt; 
&lt;p&gt;At its core, CAI abstracts its cybersecurity behavior via &lt;code&gt;Agents&lt;/code&gt; and agentic &lt;code&gt;Patterns&lt;/code&gt;. An Agent in &lt;em&gt;an intelligent system that interacts with some environment&lt;/em&gt;. More technically, within CAI we embrace a robotics-centric definition wherein an agent is anything that can be viewed as a system perceiving its environment through sensors, reasoning about its goals and and acting accordingly upon that environment through actuators (&lt;em&gt;adapted&lt;/em&gt; from Russel &amp;amp; Norvig, AI: A Modern Approach). In cybersecurity, an &lt;code&gt;Agent&lt;/code&gt; interacts with systems and networks, using peripherals and network interfaces as sensors, reasons accordingly and then executes network actions as if actuators. Correspondingly, in CAI, &lt;code&gt;Agent&lt;/code&gt;s implement the &lt;code&gt;ReACT&lt;/code&gt; (Reasoning and Action) agent model[^3]. For more information, see the &lt;a href="https://github.com/aliasrobotics/cai/raw/main/examples/basic/hello_world.py"&gt;example here&lt;/a&gt; for the full execution code, and refer to this &lt;a href="https://github.com/aliasrobotics/cai/raw/main/fluency/my-first-hack/my_first_hack.ipynb"&gt;jupyter notebook&lt;/a&gt; for a tutorial on how to use it.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from cai.sdk.agents import Agent, Runner, OpenAIChatCompletionsModel

import os
from openai import AsyncOpenAI
from dotenv import load_dotenv
load_dotenv()

agent = Agent(
      name="Custom Agent",
      instructions="""You are a Cybersecurity expert Leader""",
      model=OpenAIChatCompletionsModel(
          model=os.getenv('CAI_MODEL', "openai/gpt-4o"),
          openai_client=AsyncOpenAI(),
          )
      )

message = "Tell me about recursion in programming."
result = await Runner.run(agent, message)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;🔹 Tools&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;Tools&lt;/code&gt; let cybersecurity agents take actions by providing interfaces to execute system commands, run security scans, analyze vulnerabilities, and interact with target systems and APIs - they are the core capabilities that enable CAI agents to perform security tasks effectively; in CAI, tools include built-in cybersecurity utilities (like LinuxCmd for command execution, WebSearch for OSINT gathering, Code for dynamic script execution, and SSHTunnel for secure remote access), function calling mechanisms that allow integration of any Python function as a security tool, and agent-as-tool functionality that enables specialized security agents (such as reconnaissance or exploit agents) to be used by other agents, creating powerful collaborative security workflows without requiring formal handoffs between agents. For more information, please refer to the &lt;a href="https://github.com/aliasrobotics/cai/raw/main/examples/basic/tools.py"&gt;example here&lt;/a&gt; for the complete configuration of custom functions.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from cai.sdk.agents import Agent, Runner, OpenAIChatCompletionsModel
from cai.tools.reconnaissance.exec_code import execute_code
from cai.tools.reconnaissance.generic_linux_command import generic_linux_command

import os
from openai import AsyncOpenAI
from dotenv import load_dotenv
load_dotenv()

agent = Agent(
      name="Custom Agent",
      instructions="""You are a Cybersecurity expert Leader""",
      tools= [
        generic_linux_command,
        execute_code
      ],
      model=OpenAIChatCompletionsModel(
          model=os.getenv('CAI_MODEL', "openai/gpt-4o"),
          openai_client=AsyncOpenAI(),
          )
      )

message = "Tell me about recursion in programming."
result = await Runner.run(agent, message)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You may find different &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/cai/tools"&gt;tools&lt;/a&gt;. They are grouped in 6 major categories inspired by the security kill chain [^2]:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Reconnaissance and weaponization - &lt;em&gt;reconnaissance&lt;/em&gt; (crypto, listing, etc)&lt;/li&gt; 
 &lt;li&gt;Exploitation - &lt;em&gt;exploitation&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;Privilege escalation - &lt;em&gt;escalation&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;Lateral movement - &lt;em&gt;lateral&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;Data exfiltration - &lt;em&gt;exfiltration&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;Command and control - &lt;em&gt;control&lt;/em&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;🔹 Handoffs&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;Handoffs&lt;/code&gt; allow an &lt;code&gt;Agent&lt;/code&gt; to delegate tasks to another agent, which is crucial in cybersecurity operations where specialized expertise is needed for different phases of an engagement. In our framework, &lt;code&gt;Handoffs&lt;/code&gt; are implemented as tools for the LLM, where a &lt;strong&gt;handoff/transfer function&lt;/strong&gt; like &lt;code&gt;transfer_to_flag_discriminator&lt;/code&gt; enables the &lt;code&gt;ctf_agent&lt;/code&gt; to pass control to the &lt;code&gt;flag_discriminator_agent&lt;/code&gt; once it believes it has found the flag. This creates a security validation chain where the first agent handles exploitation and flag discovery, while the second agent specializes in flag verification, ensuring proper segregation of duties and leveraging specialized capabilities of different models for distinct security tasks. For more information, please refer to the &lt;a href="https://github.com/aliasrobotics/cai/raw/main/examples/cai/agent_patterns/handoffs.py"&gt;example here&lt;/a&gt; for the full execution code.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from cai.sdk.agents import function_tool
from cai.tools.common import run_command
from cai.sdk.agents import Agent, OpenAIChatCompletionsModel, HandoffInputData, Runner, function_tool, handoff, trace
from cai.sdk.agents.extensions import handoff_filters

import os
from openai import AsyncOpenAI
from dotenv import load_dotenv
load_dotenv()

@function_tool
def execute_cli_command(command: str) -&amp;gt; str:
    return run_command(command)

flag_discriminator = Agent(
    name="Flag discriminator",
    description="Agent focused on extracting the flag from the output",
    instructions="You are an agent tailored to extract the flag from a given output.",
    model=OpenAIChatCompletionsModel(
        model=os.getenv('CAI_MODEL', "qwen2.5:14b"),
        openai_client=AsyncOpenAI(),
    ) 
)

ctf_agent = Agent(
    name="CTF agent",
    description="Agent focused on conquering security challenges",
    instructions="You are a Cybersecurity expert Leader facing a CTF",
    tools=[
        execute_cli_command,
    ],
    model=OpenAIChatCompletionsModel(
        model= os.getenv('CAI_MODEL', "qwen2.5:14b"),
        openai_client=AsyncOpenAI(),
    ), 
    handoffs = [flag_discriminator]
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;🔹 Patterns&lt;/h3&gt; 
&lt;p&gt;An agentic &lt;code&gt;Pattern&lt;/code&gt; is a &lt;em&gt;structured design paradigm&lt;/em&gt; in artificial intelligence systems where autonomous or semi-autonomous agents operate within a defined &lt;em&gt;interaction framework&lt;/em&gt; (the pattern) to achieve a goal. These &lt;code&gt;Patterns&lt;/code&gt; specify the organization, coordination, and communication methods among agents, guiding decision-making, task execution, and delegation.&lt;/p&gt; 
&lt;p&gt;An agentic pattern (&lt;code&gt;AP&lt;/code&gt;) can be formally defined as a tuple:&lt;/p&gt; 
&lt;p&gt;\[ AP = (A, H, D, C, E) \]&lt;/p&gt; 
&lt;p&gt;wherein:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;\(A\) (Agents):&lt;/strong&gt; A set of autonomous entities, \( A = \{a_1, a_2, ..., a_n\} \), each with defined roles, capabilities, and internal states.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;\(H\) (Handoffs):&lt;/strong&gt; A function \( H: A \times T \to A \) that governs how tasks \( T \) are transferred between agents based on predefined logic (e.g., rules, negotiation, bidding).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;\(D\) (Decision Mechanism):&lt;/strong&gt; A decision function \( D: S \to A \) where \( S \) represents system states, and \( D \) determines which agent takes action at any given time.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;\(C\) (Communication Protocol):&lt;/strong&gt; A messaging function \( C: A \times A \to M \), where \( M \) is a message space, defining how agents share information.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;\(E\) (Execution Model):&lt;/strong&gt; A function \( E: A \times I \to O \) where \( I \) is the input space and \( O \) is the output space, defining how agents perform tasks.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;When building &lt;code&gt;Patterns&lt;/code&gt;, we generall y classify them among one of the following categories, though others exist:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;strong&gt;Agentic&lt;/strong&gt; &lt;code&gt;Pattern&lt;/code&gt; &lt;strong&gt;categories&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Swarm&lt;/code&gt; (Decentralized)&lt;/td&gt; 
   &lt;td&gt;Agents share tasks and self-assign responsibilities without a central orchestrator. Handoffs occur dynamically. &lt;em&gt;An example of a peer-to-peer agentic pattern is the &lt;code&gt;CTF Agentic Pattern&lt;/code&gt;, which involves a team of agents working together to solve a CTF challenge with dynamic handoffs.&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Hierarchical&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;A top-level agent (e.g., "PlannerAgent") assigns tasks via structured handoffs to specialized sub-agents. Alternatively, the structure of the agents is harcoded into the agentic pattern with pre-defined handoffs.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Chain-of-Thought&lt;/code&gt; (Sequential Workflow)&lt;/td&gt; 
   &lt;td&gt;A structured pipeline where Agent A produces an output, hands it to Agent B for reuse or refinement, and so on. Handoffs follow a linear sequence. &lt;em&gt;An example of a chain-of-thought agentic pattern is the &lt;code&gt;ReasonerAgent&lt;/code&gt;, which involves a Reasoning-type LLM that provides context to the main agent to solve a CTF challenge with a linear sequence.&lt;/em&gt;[^1]&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Auction-Based&lt;/code&gt; (Competitive Allocation)&lt;/td&gt; 
   &lt;td&gt;Agents "bid" on tasks based on priority, capability, or cost. A decision agent evaluates bids and hands off tasks to the best-fit agent.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Recursive&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;A single agent continuously refines its own output, treating itself as both executor and evaluator, with handoffs (internal or external) to itself. &lt;em&gt;An example of a recursive agentic pattern is the &lt;code&gt;CodeAgent&lt;/code&gt; (when used as a recursive agent), which continuously refines its own output by executing code and updating its own instructions.&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;For more information and examples of common agentic patterns, see the &lt;a href="https://github.com/aliasrobotics/cai/raw/main/examples/agent_patterns/README.md"&gt;examples folder&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;🔹 Turns and Interactions&lt;/h3&gt; 
&lt;p&gt;During the agentic flow (conversation), we distinguish between &lt;strong&gt;interactions&lt;/strong&gt; and &lt;strong&gt;turns&lt;/strong&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Interactions&lt;/strong&gt; are sequential exchanges between one or multiple agents. Each agent executing its logic corresponds with one &lt;em&gt;interaction&lt;/em&gt;. Since an &lt;code&gt;Agent&lt;/code&gt; in CAI generally implements the &lt;code&gt;ReACT&lt;/code&gt; agent model[^3], each &lt;em&gt;interaction&lt;/em&gt; consists of 1) a reasoning step via an LLM inference and 2) act by calling zero-to-n &lt;code&gt;Tools&lt;/code&gt;. This is defined in&lt;code&gt;process_interaction()&lt;/code&gt; in &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/cai/core.py"&gt;core.py&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Turns&lt;/strong&gt;: A turn represents a cycle of one ore more &lt;strong&gt;interactions&lt;/strong&gt; which finishes when the &lt;code&gt;Agent&lt;/code&gt; (or &lt;code&gt;Pattern&lt;/code&gt;) executing returns &lt;code&gt;None&lt;/code&gt;, judging there're no further actions to undertake. This is defined in &lt;code&gt;run()&lt;/code&gt;, see &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/cai/core.py"&gt;core.py&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] CAI Agents are not related to Assistants in the Assistants API. They are named similarly for convenience, but are otherwise completely unrelated. CAI is entirely powered by the Chat Completions API and is hence stateless between calls.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;🔹 Tracing&lt;/h3&gt; 
&lt;p&gt;CAI implements AI observability by adopting the OpenTelemetry standard and to do so, it leverages &lt;a href="https://github.com/Arize-ai/phoenix"&gt;Phoenix&lt;/a&gt; which provides comprehensive tracing capabilities through OpenTelemetry-based instrumentation, allowing you to monitor and analyze your security operations in real-time. This integration enables detailed visibility into agent interactions, tool usage, and attack vectors throughout penetration testing workflows, making it easier to debug complex exploitation chains, track vulnerability discovery processes, and optimize agent performance for more effective security assessments.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/media/tracing.png" alt="" /&gt;&lt;/p&gt; 
&lt;h3&gt;🔹 Human-In-The-Loop (HITL)&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;                      ┌─────────────────────────────────┐
                      │                                 │
                      │      Cybersecurity AI (CAI)     │
                      │                                 │
                      │       ┌─────────────────┐       │
                      │       │  Autonomous AI  │       │
                      │       └────────┬────────┘       │
                      │                │                │
                      │                │                │
                      │       ┌────────▼─────────┐      │
                      │       │ HITL Interaction │      │
                      │       └────────┬─────────┘      │
                      │                │                │
                      └────────────────┼────────────────┘
                                       │
                                       │ Ctrl+C (cli.py)
                                       │
                           ┌───────────▼───────────┐
                           │   Human Operator(s)   │
                           │  Expertise | Judgment │
                           │    Teleoperation      │
                           └───────────────────────┘
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;CAI delivers a framework for building Cybersecurity AIs with a strong emphasis on &lt;em&gt;semi-autonomous&lt;/em&gt; operation, as the reality is that &lt;strong&gt;fully-autonomous&lt;/strong&gt; cybersecurity systems remain premature and face significant challenges when tackling complex tasks. While CAI explores autonomous capabilities, we recognize that effective security operations still require human teleoperation providing expertise, judgment, and oversight in the security process.&lt;/p&gt; 
&lt;p&gt;Accordingly, the Human-In-The-Loop (&lt;code&gt;HITL&lt;/code&gt;) module is a core design principle of CAI, acknowledging that human intervention and teleoperation are essential components of responsible security testing. Through the &lt;code&gt;cli.py&lt;/code&gt; interface, users can seamlessly interact with agents at any point during execution by simply pressing &lt;code&gt;Ctrl+C&lt;/code&gt;. This is implemented across &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/cai/core.py"&gt;core.py&lt;/a&gt; and also in the REPL abstractions &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/cai/repl"&gt;REPL&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;&lt;span&gt;🚀&lt;/span&gt; Quickstart&lt;/h2&gt; 
&lt;p&gt;To start CAI after installing it, just type &lt;code&gt;cai&lt;/code&gt; in the CLI:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;└─# cai

          CCCCCCCCCCCCC      ++++++++   ++++++++      IIIIIIIIII
       CCC::::::::::::C  ++++++++++       ++++++++++  I::::::::I
     CC:::::::::::::::C ++++++++++         ++++++++++ I::::::::I
    C:::::CCCCCCCC::::C +++++++++    ++     +++++++++ II::::::II
   C:::::C       CCCCCC +++++++     +++++     +++++++   I::::I
  C:::::C                +++++     +++++++     +++++    I::::I
  C:::::C                ++++                   ++++    I::::I
  C:::::C                 ++                     ++     I::::I
  C:::::C                  +   +++++++++++++++   +      I::::I
  C:::::C                    +++++++++++++++++++        I::::I
  C:::::C                     +++++++++++++++++         I::::I
   C:::::C       CCCCCC        +++++++++++++++          I::::I
    C:::::CCCCCCCC::::C         +++++++++++++         II::::::II
     CC:::::::::::::::C           +++++++++           I::::::::I
       CCC::::::::::::C             +++++             I::::::::I
          CCCCCCCCCCCCC               ++              IIIIIIIIII

                      Cybersecurity AI (CAI), vX.Y.Z
                          Bug bounty-ready AI

CAI&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;That should initialize CAI and provide a prompt to execute any security task you want to perform. The navigation bar at the bottom displays important system information. This information helps you understand your environment while working with CAI.&lt;/p&gt; 
&lt;p&gt;Here's a quick &lt;a href="https://asciinema.org/a/zm7wS5DA2o0S9pu1Tb44pnlvy"&gt;demo video&lt;/a&gt; to help you get started with CAI. We'll walk through the basic steps — from launching the tool to running your first AI-powered task in the terminal. Whether you're a beginner or just curious, this guide will show you how easy it is to begin using CAI.&lt;/p&gt; 
&lt;p&gt;From here on, type on &lt;code&gt;CAI&lt;/code&gt; and start your security exercise. Best way to learn is by example:&lt;/p&gt; 
&lt;h3&gt;Environment Variables&lt;/h3&gt; 
&lt;p&gt;For using private models, you are given a &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/.env.example"&gt;&lt;code&gt;.env.example&lt;/code&gt;&lt;/a&gt; file. Copy it and rename it as &lt;code&gt;.env&lt;/code&gt;. Fill in your corresponding API keys, and you are ready to use CAI.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;List of Environment Variables&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Variable&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CTF_NAME&lt;/td&gt; 
    &lt;td&gt;Name of the CTF challenge to run (e.g. "picoctf_static_flag")&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CTF_CHALLENGE&lt;/td&gt; 
    &lt;td&gt;Specific challenge name within the CTF to test&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CTF_SUBNET&lt;/td&gt; 
    &lt;td&gt;Network subnet for the CTF container&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CTF_IP&lt;/td&gt; 
    &lt;td&gt;IP address for the CTF container&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CTF_INSIDE&lt;/td&gt; 
    &lt;td&gt;Whether to conquer the CTF from within container&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_MODEL&lt;/td&gt; 
    &lt;td&gt;Model to use for agents&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_DEBUG&lt;/td&gt; 
    &lt;td&gt;Set debug output level (0: Only tool outputs, 1: Verbose debug output, 2: CLI debug output)&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_BRIEF&lt;/td&gt; 
    &lt;td&gt;Enable/disable brief output mode&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_MAX_TURNS&lt;/td&gt; 
    &lt;td&gt;Maximum number of turns for agent interactions&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_TRACING&lt;/td&gt; 
    &lt;td&gt;Enable/disable OpenTelemetry tracing&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_AGENT_TYPE&lt;/td&gt; 
    &lt;td&gt;Specify the agents to use (boot2root, one_tool...)&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_STATE&lt;/td&gt; 
    &lt;td&gt;Enable/disable stateful mode&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_MEMORY&lt;/td&gt; 
    &lt;td&gt;Enable/disable memory mode (episodic, semantic, all)&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_MEMORY_ONLINE&lt;/td&gt; 
    &lt;td&gt;Enable/disable online memory mode&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_MEMORY_OFFLINE&lt;/td&gt; 
    &lt;td&gt;Enable/disable offline memory&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_ENV_CONTEXT&lt;/td&gt; 
    &lt;td&gt;Add dirs and current env to llm context&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_MEMORY_ONLINE_INTERVAL&lt;/td&gt; 
    &lt;td&gt;Number of turns between online memory updates&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_PRICE_LIMIT&lt;/td&gt; 
    &lt;td&gt;Price limit for the conversation in dollars&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_REPORT&lt;/td&gt; 
    &lt;td&gt;Enable/disable reporter mode (ctf, nis2, pentesting)&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_SUPPORT_MODEL&lt;/td&gt; 
    &lt;td&gt;Model to use for the support agent&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_SUPPORT_INTERVAL&lt;/td&gt; 
    &lt;td&gt;Number of turns between support agent executions&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_WORKSPACE&lt;/td&gt; 
    &lt;td&gt;Defines the name of the workspace&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CAI_WORKSPACE_DIR&lt;/td&gt; 
    &lt;td&gt;Specifies the directory path where the workspace is located&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h3&gt;OpenRouter Integration&lt;/h3&gt; 
&lt;p&gt;The Cybersecurity AI (CAI) platform offers seamless integration with OpenRouter, a unified interface for Large Language Models (LLMs). This integration is crucial for users who wish to leverage advanced AI capabilities in their cybersecurity tasks. OpenRouter acts as a bridge, allowing CAI to communicate with various LLMs, thereby enhancing the flexibility and power of the AI agents used within CAI.&lt;/p&gt; 
&lt;p&gt;To enable OpenRouter support in CAI, you need to configure your environment by adding specific entries to your &lt;code&gt;.env&lt;/code&gt; file. This setup ensures that CAI can interact with the OpenRouter API, facilitating the use of sophisticated models like Meta-LLaMA. Here’s how you can configure it:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;CAI_AGENT_TYPE=redteam_agent
CAI_MODEL=openrouter/meta-llama/llama-4-maverick
OPENROUTER_API_KEY=&amp;lt;sk-your-key&amp;gt;  # note, add yours
OPENROUTER_API_BASE=https://openrouter.ai/api/v1
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;MCP&lt;/h3&gt; 
&lt;p&gt;CAI supports the Model Context Protocol (MCP) for integrating external tools and services with AI agents. MCP is supported via two transport mechanisms:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;SSE (Server-Sent Events)&lt;/strong&gt; - For web-based servers that push updates over HTTP connections:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;CAI&amp;gt;/mcp load http://localhost:9876/sse burp
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;&lt;strong&gt;STDIO (Standard Input/Output)&lt;/strong&gt; - For local inter-process communication:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;CAI&amp;gt;/mcp load stdio myserver python mcp_server.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Once connected, you can add the MCP tools to any agent:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;CAI&amp;gt;/mcp add burp redteam_agent
Adding tools from MCP server 'burp' to agent 'Red Team Agent'...
                                 Adding tools to Red Team Agent
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Tool                              ┃ Status ┃ Details                                         ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ send_http_request                 │ Added  │ Available as: send_http_request                 │
│ create_repeater_tab               │ Added  │ Available as: create_repeater_tab               │
│ send_to_intruder                  │ Added  │ Available as: send_to_intruder                  │
│ url_encode                        │ Added  │ Available as: url_encode                        │
│ url_decode                        │ Added  │ Available as: url_decode                        │
│ base64encode                      │ Added  │ Available as: base64encode                      │
│ base64decode                      │ Added  │ Available as: base64decode                      │
│ generate_random_string            │ Added  │ Available as: generate_random_string            │
│ output_project_options            │ Added  │ Available as: output_project_options            │
│ output_user_options               │ Added  │ Available as: output_user_options               │
│ set_project_options               │ Added  │ Available as: set_project_options               │
│ set_user_options                  │ Added  │ Available as: set_user_options                  │
│ get_proxy_http_history            │ Added  │ Available as: get_proxy_http_history            │
│ get_proxy_http_history_regex      │ Added  │ Available as: get_proxy_http_history_regex      │
│ get_proxy_websocket_history       │ Added  │ Available as: get_proxy_websocket_history       │
│ get_proxy_websocket_history_regex │ Added  │ Available as: get_proxy_websocket_history_regex │
│ set_task_execution_engine_state   │ Added  │ Available as: set_task_execution_engine_state   │
│ set_proxy_intercept_state         │ Added  │ Available as: set_proxy_intercept_state         │
│ get_active_editor_contents        │ Added  │ Available as: get_active_editor_contents        │
│ set_active_editor_contents        │ Added  │ Available as: set_active_editor_contents        │
└───────────────────────────────────┴────────┴─────────────────────────────────────────────────┘
Added 20 tools from server 'burp' to agent 'Red Team Agent'.
CAI&amp;gt;/agent 13
CAI&amp;gt;Create a repeater tab
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can list all active MCP connections and their transport types:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;CAI&amp;gt;/mcp list
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/386a1fd3-3469-4f84-9396-2a5236febe1f"&gt;https://github.com/user-attachments/assets/386a1fd3-3469-4f84-9396-2a5236febe1f&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Development&lt;/h2&gt; 
&lt;p&gt;Development is facilitated via VS Code dev. environments. To try out our development environment, clone the repository, open VS Code and enter de dev. container mode:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/media/cai_devenv.gif" alt="CAI Development Environment" /&gt;&lt;/p&gt; 
&lt;h3&gt;Contributions&lt;/h3&gt; 
&lt;p&gt;If you want to contribute to this project, use &lt;a href="https://pre-commit.com/"&gt;&lt;strong&gt;Pre-commit&lt;/strong&gt;&lt;/a&gt; before your MR&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install pre-commit
pre-commit # files staged
pre-commit run --all-files # all files
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Optional Requirements: caiextensions&lt;/h3&gt; 
&lt;p&gt;Currently, the extensions are not publicly available as the engineering endeavour to maintain them is significant. Instead, we're making selected custom caiextensions available for partner companies across collaborations.&lt;/p&gt; 
&lt;h3&gt;&lt;span&gt;ℹ&lt;/span&gt; Usage Data Collection&lt;/h3&gt; 
&lt;p&gt;CAI is provided free of charge for researchers. To improve CAI’s detection accuracy and publish open security research, instead of payment for research use cases, we ask you to contribute to the CAI community by allowing usage data collection. This data helps us identify areas for improvement, understand how the framework is being used, and prioritize new features. Legal basis of data collection is under Art. 6 (1)(f) GDPR — CAI’s legitimate interest in maintaining and improving security tooling, with Art. 89 safeguards for research. The collected data includes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Basic system information (OS type, Python version)&lt;/li&gt; 
 &lt;li&gt;Username and IP information&lt;/li&gt; 
 &lt;li&gt;Tool usage patterns and performance metrics&lt;/li&gt; 
 &lt;li&gt;Model interactions and token usage statistics&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We take your privacy seriously and only collect what's needed to make CAI better. For further info, reach out to research＠aliasrobotics.com. You can disable some of the data collection features via the &lt;code&gt;CAI_TELEMETRY&lt;/code&gt; environment variable but we encourage you to keep it enabled and contribute back to research:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;CAI_TELEMETRY=False cai
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Reproduce CI-Setup locally&lt;/h3&gt; 
&lt;p&gt;To simulate the CI/CD pipeline, you can run the following in the Gitlab runner machines:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run --rm -it \
  --privileged \
  --network=exploitflow_net \
  --add-host="host.docker.internal:host-gateway" \
  -v /cache:/cache \
  -v /var/run/docker.sock:/var/run/docker.sock:rw \
  registry.gitlab.com/aliasrobotics/alias_research/cai:latest bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;FAQ&lt;/h2&gt; 
&lt;details&gt;
 &lt;summary&gt;OLLAMA is giving me 404 errors&lt;/summary&gt; 
 &lt;p&gt;Ollama's API in OpenAI mode uses &lt;code&gt;/v1/chat/completions&lt;/code&gt; whereas the &lt;code&gt;openai&lt;/code&gt; library uses &lt;code&gt;base_url&lt;/code&gt; + &lt;code&gt;/chat/completions&lt;/code&gt;.&lt;/p&gt; 
 &lt;p&gt;We adopt the latter for overall alignment with the gen AI community and empower the former by allowing users to add the &lt;code&gt;v1&lt;/code&gt; themselves via:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;OLLAMA_API_BASE=http://IP:PORT/v1
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;See the following issues that treat this topic in more detail:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/issues/76"&gt;https://github.com/aliasrobotics/cai/issues/76&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/issues/83"&gt;https://github.com/aliasrobotics/cai/issues/83&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/aliasrobotics/cai/issues/82"&gt;https://github.com/aliasrobotics/cai/issues/82&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;Where are all the caiextensions?&lt;/summary&gt; 
 &lt;p&gt;See &lt;a href="https://gitlab.com/aliasrobotics/alias_research/caiextensions"&gt;all caiextensions&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;How do I install the report caiextension?&lt;/summary&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/#optional-requirements-caiextensions"&gt;See here&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;How do I set up SSH access for Gitlab?&lt;/summary&gt; 
 &lt;p&gt;Generate a new SSH key&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;ssh-keygen -t ed25519
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Add the key to the SSH agent&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;ssh-add ~/.ssh/id_ed25519
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Add the public key to Gitlab Copy the key and add it to Gitlab under &lt;a href="https://gitlab.com/-/user_settings/ssh_keys"&gt;https://gitlab.com/-/user_settings/ssh_keys&lt;/a&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;cat ~/.ssh/id_ed25519.pub
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;To verify it:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;ssh -T git@gitlab.com
Welcome to GitLab, @vmayoral!
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;How do I clear Python cache?&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;find . -name "*.pyc" -delete &amp;amp;&amp;amp; find . -name "__pycache__" -delete
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;If host networking is not working with ollama check whether it has been disabled in Docker because you are not signed in&lt;/summary&gt; 
 &lt;p&gt;Docker in OS X behaves funny sometimes. Check if the following message has shown up:&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;Host networking has been disabled because you are not signed in. Please sign in to enable it&lt;/em&gt;.&lt;/p&gt; 
 &lt;p&gt;Make sure this has been addressed and also that the Dev Container is not forwarding the 8000 port (click on x, if necessary in the ports section).&lt;/p&gt; 
 &lt;p&gt;To verify connection, from within the VSCode devcontainer:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;curl -v http://host.docker.internal:8000/api/version
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Run CAI against any target&lt;/summary&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/imgs/readme_imgs/cai-004-first-message.png" alt="cai-004-first-message" /&gt;&lt;/p&gt; 
 &lt;p&gt;The starting user prompt in this case is: &lt;code&gt;Target IP: 192.168.3.10, perform a full network scan&lt;/code&gt;.&lt;/p&gt; 
 &lt;p&gt;The agent started performing a nmap scan. You could either interact with the agent and give it more instructions, or let it run to see what it explores next.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;How do I interact with the agent? Type twice CTRL + C &lt;/summary&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/imgs/readme_imgs/cai-005-ctrl-c.png" alt="cai-005-ctrl-c" /&gt;&lt;/p&gt; 
 &lt;p&gt;If you want to use the HITL mode, you can do it by presssing twice &lt;code&gt;Ctrl + C&lt;/code&gt;. This will allow you to interact (prompt) with the agent whenever you want. The agent will not lose the previous context, as it is stored in the &lt;code&gt;history&lt;/code&gt; variable, which is passed to it and any agent that is called. This enables any agent to use the previous information and be more accurate and efficient.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; Can I change the model while CAI is running? /model &lt;/summary&gt; 
 &lt;p&gt;Use &lt;code&gt;/model&lt;/code&gt; to change the model.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/imgs/readme_imgs/cai-007-model-change.png" alt="cai-007-model-change" /&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;How can I list all the agents available? /agent &lt;/summary&gt; 
 &lt;p&gt;Use &lt;code&gt;/agent&lt;/code&gt; to list all the agents available.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/imgs/readme_imgs/cai-010-agents-menu.png" alt="cai-010-agents-menu" /&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; Where can I list all the environment variables? /config &lt;/summary&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/imgs/readme_imgs/cai-008-config.png" alt="cai-008-config" /&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; How to know more about the CLI? /help &lt;/summary&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/imgs/readme_imgs/cai-006-help.png" alt="cai-006-help" /&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;How can I trace the whole execution?&lt;/summary&gt; The environment variable `CAI_TRACING` allows the user to set it to `CAI_TRACING=true` to enable tracing, or `CAI_TRACING=false` to disable it. When CAI is prompted by the first time, the user is provided with two paths, the execution log, and the tracing log. 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/imgs/readme_imgs/cai-009-logs.png" alt="cai-009-logs" /&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Can I expand CAI capabilities using previous run logs?&lt;/summary&gt; 
 &lt;p&gt;Absolutely! The &lt;strong&gt;memory extension&lt;/strong&gt; allows you to use a previously sucessful runs ( the log object is stored as a &lt;strong&gt;.jsonl file in the &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/cai/logs"&gt;log&lt;/a&gt; folder&lt;/strong&gt; ) in a new run against the same target. The user is also given the path highlighted in orange as shown below.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/aliasrobotics/cai/main/imgs/readme_imgs/cai-009-logs.png" alt="cai-009-logs" /&gt;&lt;/p&gt; 
 &lt;p&gt;How to make use of this functionality?&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Run CAI against the target. Let's assume the target name is: &lt;code&gt;target001&lt;/code&gt;.&lt;/li&gt; 
  &lt;li&gt;Get the log file path, something like: &lt;code&gt;logs/cai_20250408_111856.jsonl&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;Generate the memory using any model of your preference: &lt;code&gt;shell JSONL_FILE_PATH="logs/cai_20250408_111856.jsonl" CTF_INSIDE="false" CAI_MEMORY_COLLECTION="target001" CAI_MEMORY="episodic" CAI_MODEL="claude-3-5-sonnet-20241022" python3 tools/2_jsonl_to_memory.py &lt;/code&gt;&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;The script &lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/cai/tools/2_jsonl_to_memory.py"&gt;&lt;code&gt;tools/2_jsonl_to_memory.py&lt;/code&gt;&lt;/a&gt; will generate a memory collection file with the most relevant steps. The quality of the memory collection will depend on the model you use.&lt;/p&gt; 
 &lt;ol start="4"&gt; 
  &lt;li&gt;Use the generated memory collection and execute a new run: &lt;code&gt;shell CAI_MEMORY="episodic" CAI_MODEL="gpt-4o" CAI_MEMORY_COLLECTION="target001" CAI_TRACING=false python3 cai/cli.py&lt;/code&gt;&lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Can I expand CAI capabilities using scripts or extra information?&lt;/summary&gt; 
 &lt;p&gt;Currently, CAI supports text based information. You can add any extra information on the target you are facing by copy-pasting it directly into the system or user prompt.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;How?&lt;/strong&gt; By adding it to the system (&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/cai/repl/templates/system_master_template.md"&gt;&lt;code&gt;system_master_template.md&lt;/code&gt;&lt;/a&gt;) or the user prompt (&lt;a href="https://raw.githubusercontent.com/aliasrobotics/cai/main/cai/repl/templates/user_master_template.md"&gt;&lt;code&gt;user_master_template.md&lt;/code&gt;&lt;/a&gt;). You can always directly prompt the path to the model, and it will &lt;code&gt;cat&lt;/code&gt; it.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;How CAI licence works?&lt;/summary&gt; 
 &lt;p&gt;CAI’s current license does not restrict usage for research purposes. You are free to use CAI for security assessments (pentests), to develop additional features, and to integrate it into your research activities, as long as you comply with local laws.&lt;/p&gt; 
 &lt;p&gt;If you or your organization start benefiting commercially from CAI (e.g., offering pentesting services powered by CAI), then a commercial license will be required to help sustain the project.&lt;/p&gt; 
 &lt;p&gt;CAI itself is not a profit-seeking initiative. Our goal is to build a sustainable open-source project. We simply ask that those who profit from CAI contribute back and support our ongoing development.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you want to cite our work, please use the following:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{mayoralvilches2025caiopenbugbountyready,
      title={CAI: An Open, Bug Bounty-Ready Cybersecurity AI},
      author={Víctor Mayoral-Vilches and Luis Javier Navarrete-Lozano and María Sanz-Gómez and Lidia Salas Espejo and Martiño Crespo-Álvarez and Francisco Oca-Gonzalez and Francesco Balassone and Alfonso Glera-Picón and Unai Ayucar-Carbajo and Jon Ander Ruiz-Alcalde and Stefan Rass and Martin Pinzger and Endika Gil-Uriarte},
      year={2025},
      eprint={2504.06017},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2504.06017},
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{mayoralvilches2025cybersecurityaidangerousgap,
      title={Cybersecurity AI: The Dangerous Gap Between Automation and Autonomy}, 
      author={Víctor Mayoral-Vilches},
      year={2025},
      eprint={2506.23592},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2506.23592}, 
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;p&gt;CAI was initially developed by &lt;a href="https://aliasrobotics.com"&gt;Alias Robotics&lt;/a&gt; and co-funded by the European EIC accelerator project RIS (GA 101161136) - HORIZON-EIC-2023-ACCELERATOR-01 call. The original agentic principles are inspired from OpenAI's &lt;a href="https://github.com/openai/swarm"&gt;&lt;code&gt;swarm&lt;/code&gt;&lt;/a&gt; library and translated into newer prototypes. This project also makes use of other relevant open source building blocks including &lt;a href="https://github.com/BerriAI/litellm"&gt;&lt;code&gt;LiteLLM&lt;/code&gt;&lt;/a&gt;, and &lt;a href="https://github.com/Arize-ai/phoenix"&gt;&lt;code&gt;phoenix&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;!-- Footnotes --&gt; 
&lt;p&gt;[^1]: Arguably, the Chain-of-Thought agentic pattern is a special case of the Hierarchical agentic pattern. [^2]: Kamhoua, C. A., Leslie, N. O., &amp;amp; Weisman, M. J. (2018). Game theoretic modeling of advanced persistent threat in internet of things. Journal of Cyber Security and Information Systems. [^3]: Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., &amp;amp; Cao, Y. (2023, January). React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR). [^4]: Deng, G., Liu, Y., Mayoral-Vilches, V., Liu, P., Li, Y., Xu, Y., ... &amp;amp; Rass, S. (2024). {PentestGPT}: Evaluating and harnessing large language models for automated penetration testing. In 33rd USENIX Security Symposium (USENIX Security 24) (pp. 847-864).&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>