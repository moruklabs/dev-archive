<rss version="2.0">
  <channel>
    <title>GitHub Python Daily Trending</title>
    <description>Daily Trending of Python in GitHub</description>
    <pubDate>Thu, 15 Jan 2026 01:38:27 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>UKGovernmentBEIS/inspect_ai</title>
      <link>https://github.com/UKGovernmentBEIS/inspect_ai</link>
      <description>&lt;p&gt;Inspect: A framework for large language model evaluations&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href="https://aisi.gov.uk/"&gt;&lt;img width="295" src="https://inspect.aisi.org.uk/images/aisi-logo.svg?sanitize=true" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Welcome to Inspect, a framework for large language model evaluations created by the &lt;a href="https://aisi.gov.uk/"&gt;UK AI Security Institute&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Inspect provides many built-in components, including facilities for prompt engineering, tool usage, multi-turn dialog, and model graded evaluations. Extensions to Inspect (e.g.&amp;nbsp;to support new elicitation and scoring techniques) can be provided by other Python packages.&lt;/p&gt; 
&lt;p&gt;To get started with Inspect, please see the documentation at &lt;a href="https://inspect.aisi.org.uk/"&gt;https://inspect.aisi.org.uk/&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Inspect also includes a collection of over 100 pre-built evaluations ready to run on any model (learn more at &lt;a href="https://ukgovernmentbeis.github.io/inspect_evals/"&gt;Inspect Evals&lt;/a&gt;)&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;To work on development of Inspect, clone the repository and install with the &lt;code&gt;-e&lt;/code&gt; flag and &lt;code&gt;[dev]&lt;/code&gt; optional dependencies:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/UKGovernmentBEIS/inspect_ai.git
cd inspect_ai
pip install -e ".[dev]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Optionally install pre-commit hooks via&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;make hooks
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Run linting, formatting, and tests via&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;make check
make test
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you use VS Code, you should be sure to have installed the recommended extensions (Python, Ruff, and MyPy). Note that you'll be prompted to install these when you open the project in VS Code.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;To work on the Inspect documentation, install the optional &lt;code&gt;[doc]&lt;/code&gt; dependencies with the &lt;code&gt;-e&lt;/code&gt; flag and build the docs:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install -e ".[doc]"
cd docs
quarto render # or 'quarto preview'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you intend to work on the docs iteratively, you'll want to install the Quarto extension in VS Code.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>NanmiCoder/MediaCrawler</title>
      <link>https://github.com/NanmiCoder/MediaCrawler</link>
      <description>&lt;p&gt;å°çº¢ä¹¦ç¬”è®° | è¯„è®ºçˆ¬è™«ã€æŠ–éŸ³è§†é¢‘ | è¯„è®ºçˆ¬è™«ã€å¿«æ‰‹è§†é¢‘ | è¯„è®ºçˆ¬è™«ã€B ç«™è§†é¢‘ ï½œ è¯„è®ºçˆ¬è™«ã€å¾®åšå¸–å­ ï½œ è¯„è®ºçˆ¬è™«ã€ç™¾åº¦è´´å§å¸–å­ ï½œ ç™¾åº¦è´´å§è¯„è®ºå›å¤çˆ¬è™« | çŸ¥ä¹é—®ç­”æ–‡ç« ï½œè¯„è®ºçˆ¬è™«&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ğŸ”¥ MediaCrawler - è‡ªåª’ä½“å¹³å°çˆ¬è™« ğŸ•·ï¸&lt;/h1&gt; 
&lt;div align="center" markdown="1"&gt; 
 &lt;sup&gt;Special thanks to:&lt;/sup&gt; 
 &lt;br /&gt; 
 &lt;br /&gt; 
 &lt;a href="https://go.warp.dev/MediaCrawler"&gt; &lt;img alt="Warp sponsorship" width="400" src="https://github.com/warpdotdev/brand-assets/raw/main/Github/Sponsor/Warp-Github-LG-02.png?raw=true" /&gt; &lt;/a&gt; 
 &lt;h3&gt;&lt;a href="https://go.warp.dev/MediaCrawler"&gt;Warp is built for coding with multiple AI agents&lt;/a&gt;&lt;/h3&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://trendshift.io/repositories/8291" target="_blank"&gt; &lt;img src="https://trendshift.io/api/badge/repositories/8291" alt="NanmiCoder%2FMediaCrawler | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt; &lt;/a&gt; 
 &lt;p&gt;&lt;a href="https://github.com/NanmiCoder/MediaCrawler/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/NanmiCoder/MediaCrawler?style=social" alt="GitHub Stars" /&gt;&lt;/a&gt; &lt;a href="https://github.com/NanmiCoder/MediaCrawler/network/members"&gt;&lt;img src="https://img.shields.io/github/forks/NanmiCoder/MediaCrawler?style=social" alt="GitHub Forks" /&gt;&lt;/a&gt; &lt;a href="https://github.com/NanmiCoder/MediaCrawler/issues"&gt;&lt;img src="https://img.shields.io/github/issues/NanmiCoder/MediaCrawler" alt="GitHub Issues" /&gt;&lt;/a&gt; &lt;a href="https://github.com/NanmiCoder/MediaCrawler/pulls"&gt;&lt;img src="https://img.shields.io/github/issues-pr/NanmiCoder/MediaCrawler" alt="GitHub Pull Requests" /&gt;&lt;/a&gt; &lt;a href="https://github.com/NanmiCoder/MediaCrawler/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/NanmiCoder/MediaCrawler" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/NanmiCoder/MediaCrawler/main/README.md"&gt;&lt;img src="https://img.shields.io/badge/%F0%9F%87%A8%F0%9F%87%B3_%E4%B8%AD%E6%96%87-%E5%BD%93%E5%89%8D-blue" alt="ä¸­æ–‡" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/NanmiCoder/MediaCrawler/main/README_en.md"&gt;&lt;img src="https://img.shields.io/badge/%F0%9F%87%BA%F0%9F%87%B8_English-Available-green" alt="English" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/NanmiCoder/MediaCrawler/main/README_es.md"&gt;&lt;img src="https://img.shields.io/badge/%F0%9F%87%AA%F0%9F%87%B8_Espa%C3%B1ol-Available-green" alt="EspaÃ±ol" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;å…è´£å£°æ˜ï¼š&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;å¤§å®¶è¯·ä»¥å­¦ä¹ ä¸ºç›®çš„ä½¿ç”¨æœ¬ä»“åº“âš ï¸âš ï¸âš ï¸âš ï¸ï¼Œ&lt;a href="https://github.com/HiddenStrawberry/Crawler_Illegal_Cases_In_China"&gt;çˆ¬è™«è¿æ³•è¿è§„çš„æ¡ˆä»¶&lt;/a&gt; &lt;br /&gt;&lt;/p&gt; 
 &lt;p&gt;æœ¬ä»“åº“çš„æ‰€æœ‰å†…å®¹ä»…ä¾›å­¦ä¹ å’Œå‚è€ƒä¹‹ç”¨ï¼Œç¦æ­¢ç”¨äºå•†ä¸šç”¨é€”ã€‚ä»»ä½•äººæˆ–ç»„ç»‡ä¸å¾—å°†æœ¬ä»“åº“çš„å†…å®¹ç”¨äºéæ³•ç”¨é€”æˆ–ä¾µçŠ¯ä»–äººåˆæ³•æƒç›Šã€‚æœ¬ä»“åº“æ‰€æ¶‰åŠçš„çˆ¬è™«æŠ€æœ¯ä»…ç”¨äºå­¦ä¹ å’Œç ”ç©¶ï¼Œä¸å¾—ç”¨äºå¯¹å…¶ä»–å¹³å°è¿›è¡Œå¤§è§„æ¨¡çˆ¬è™«æˆ–å…¶ä»–éæ³•è¡Œä¸ºã€‚å¯¹äºå› ä½¿ç”¨æœ¬ä»“åº“å†…å®¹è€Œå¼•èµ·çš„ä»»ä½•æ³•å¾‹è´£ä»»ï¼Œæœ¬ä»“åº“ä¸æ‰¿æ‹…ä»»ä½•è´£ä»»ã€‚ä½¿ç”¨æœ¬ä»“åº“çš„å†…å®¹å³è¡¨ç¤ºæ‚¨åŒæ„æœ¬å…è´£å£°æ˜çš„æ‰€æœ‰æ¡æ¬¾å’Œæ¡ä»¶ã€‚&lt;/p&gt; 
 &lt;p&gt;ç‚¹å‡»æŸ¥çœ‹æ›´ä¸ºè¯¦ç»†çš„å…è´£å£°æ˜ã€‚&lt;a href="https://raw.githubusercontent.com/NanmiCoder/MediaCrawler/main/#disclaimer"&gt;ç‚¹å‡»è·³è½¬&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;ğŸ“– é¡¹ç›®ç®€ä»‹&lt;/h2&gt; 
&lt;p&gt;ä¸€ä¸ªåŠŸèƒ½å¼ºå¤§çš„&lt;strong&gt;å¤šå¹³å°è‡ªåª’ä½“æ•°æ®é‡‡é›†å·¥å…·&lt;/strong&gt;ï¼Œæ”¯æŒå°çº¢ä¹¦ã€æŠ–éŸ³ã€å¿«æ‰‹ã€Bç«™ã€å¾®åšã€è´´å§ã€çŸ¥ä¹ç­‰ä¸»æµå¹³å°çš„å…¬å¼€ä¿¡æ¯æŠ“å–ã€‚&lt;/p&gt; 
&lt;h3&gt;ğŸ”§ æŠ€æœ¯åŸç†&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;æ ¸å¿ƒæŠ€æœ¯&lt;/strong&gt;ï¼šåŸºäº &lt;a href="https://playwright.dev/"&gt;Playwright&lt;/a&gt; æµè§ˆå™¨è‡ªåŠ¨åŒ–æ¡†æ¶ç™»å½•ä¿å­˜ç™»å½•æ€&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;æ— éœ€JSé€†å‘&lt;/strong&gt;ï¼šåˆ©ç”¨ä¿ç•™ç™»å½•æ€çš„æµè§ˆå™¨ä¸Šä¸‹æ–‡ç¯å¢ƒï¼Œé€šè¿‡ JS è¡¨è¾¾å¼è·å–ç­¾åå‚æ•°&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ä¼˜åŠ¿ç‰¹ç‚¹&lt;/strong&gt;ï¼šæ— éœ€é€†å‘å¤æ‚çš„åŠ å¯†ç®—æ³•ï¼Œå¤§å¹…é™ä½æŠ€æœ¯é—¨æ§›&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;âœ¨ åŠŸèƒ½ç‰¹æ€§&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;å¹³å°&lt;/th&gt; 
   &lt;th&gt;å…³é”®è¯æœç´¢&lt;/th&gt; 
   &lt;th&gt;æŒ‡å®šå¸–å­IDçˆ¬å–&lt;/th&gt; 
   &lt;th&gt;äºŒçº§è¯„è®º&lt;/th&gt; 
   &lt;th&gt;æŒ‡å®šåˆ›ä½œè€…ä¸»é¡µ&lt;/th&gt; 
   &lt;th&gt;ç™»å½•æ€ç¼“å­˜&lt;/th&gt; 
   &lt;th&gt;IPä»£ç†æ± &lt;/th&gt; 
   &lt;th&gt;ç”Ÿæˆè¯„è®ºè¯äº‘å›¾&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;å°çº¢ä¹¦&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;æŠ–éŸ³&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;å¿«æ‰‹&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;B ç«™&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;å¾®åš&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;è´´å§&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;çŸ¥ä¹&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;details&gt; 
 &lt;summary&gt;ğŸš€ &lt;strong&gt;MediaCrawlerPro é‡ç£…å‘å¸ƒï¼å¼€æºä¸æ˜“ï¼Œæ¬¢è¿è®¢é˜…æ”¯æŒ&lt;/strong&gt;&lt;/summary&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;ä¸“æ³¨äºå­¦ä¹ æˆç†Ÿé¡¹ç›®çš„æ¶æ„è®¾è®¡ï¼Œä¸ä»…ä»…æ˜¯çˆ¬è™«æŠ€æœ¯ï¼ŒPro ç‰ˆæœ¬çš„ä»£ç è®¾è®¡æ€è·¯åŒæ ·å€¼å¾—æ·±å…¥å­¦ä¹ ï¼&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;p&gt;&lt;a href="https://github.com/MediaCrawlerPro"&gt;MediaCrawlerPro&lt;/a&gt; ç›¸è¾ƒäºå¼€æºç‰ˆæœ¬çš„æ ¸å¿ƒä¼˜åŠ¿ï¼š&lt;/p&gt; 
 &lt;h4&gt;ğŸ¯ æ ¸å¿ƒåŠŸèƒ½å‡çº§&lt;/h4&gt; 
 &lt;ul&gt; 
  &lt;li&gt;âœ… &lt;strong&gt;æ–­ç‚¹ç»­çˆ¬åŠŸèƒ½&lt;/strong&gt;ï¼ˆé‡ç‚¹ç‰¹æ€§ï¼‰&lt;/li&gt; 
  &lt;li&gt;âœ… &lt;strong&gt;å¤šè´¦å· + IPä»£ç†æ± æ”¯æŒ&lt;/strong&gt;ï¼ˆé‡ç‚¹ç‰¹æ€§ï¼‰&lt;/li&gt; 
  &lt;li&gt;âœ… &lt;strong&gt;å»é™¤ Playwright ä¾èµ–&lt;/strong&gt;ï¼Œä½¿ç”¨æ›´ç®€å•&lt;/li&gt; 
  &lt;li&gt;âœ… &lt;strong&gt;å®Œæ•´ Linux ç¯å¢ƒæ”¯æŒ&lt;/strong&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h4&gt;ğŸ—ï¸ æ¶æ„è®¾è®¡ä¼˜åŒ–&lt;/h4&gt; 
 &lt;ul&gt; 
  &lt;li&gt;âœ… &lt;strong&gt;ä»£ç é‡æ„ä¼˜åŒ–&lt;/strong&gt;ï¼Œæ›´æ˜“è¯»æ˜“ç»´æŠ¤ï¼ˆè§£è€¦ JS ç­¾åé€»è¾‘ï¼‰&lt;/li&gt; 
  &lt;li&gt;âœ… &lt;strong&gt;ä¼ä¸šçº§ä»£ç è´¨é‡&lt;/strong&gt;ï¼Œé€‚åˆæ„å»ºå¤§å‹çˆ¬è™«é¡¹ç›®&lt;/li&gt; 
  &lt;li&gt;âœ… &lt;strong&gt;å®Œç¾æ¶æ„è®¾è®¡&lt;/strong&gt;ï¼Œé«˜æ‰©å±•æ€§ï¼Œæºç å­¦ä¹ ä»·å€¼æ›´å¤§&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h4&gt;ğŸ é¢å¤–åŠŸèƒ½&lt;/h4&gt; 
 &lt;ul&gt; 
  &lt;li&gt;âœ… &lt;strong&gt;è‡ªåª’ä½“è§†é¢‘ä¸‹è½½å™¨æ¡Œé¢ç«¯&lt;/strong&gt;ï¼ˆé€‚åˆå­¦ä¹ å…¨æ ˆå¼€å‘ï¼‰&lt;/li&gt; 
  &lt;li&gt;âœ… &lt;strong&gt;å¤šå¹³å°é¦–é¡µä¿¡æ¯æµæ¨è&lt;/strong&gt;ï¼ˆHomeFeedï¼‰&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;strong&gt;åŸºäºè‡ªåª’ä½“å¹³å°çš„AI Agentæ­£åœ¨å¼€å‘ä¸­ ğŸš€ğŸš€&lt;/strong&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;ç‚¹å‡»æŸ¥çœ‹ï¼š&lt;a href="https://github.com/MediaCrawlerPro"&gt;MediaCrawlerPro é¡¹ç›®ä¸»é¡µ&lt;/a&gt; æ›´å¤šä»‹ç»&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;ğŸš€ å¿«é€Ÿå¼€å§‹&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ğŸ’¡ &lt;strong&gt;å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œè¯·ç»™ä¸ª â­ Star æ”¯æŒä¸€ä¸‹ï¼&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;ğŸ“‹ å‰ç½®ä¾èµ–&lt;/h2&gt; 
&lt;h3&gt;ğŸš€ uv å®‰è£…ï¼ˆæ¨èï¼‰&lt;/h3&gt; 
&lt;p&gt;åœ¨è¿›è¡Œä¸‹ä¸€æ­¥æ“ä½œä¹‹å‰ï¼Œè¯·ç¡®ä¿ç”µè„‘ä¸Šå·²ç»å®‰è£…äº† uvï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;å®‰è£…åœ°å€&lt;/strong&gt;ï¼š&lt;a href="https://docs.astral.sh/uv/getting-started/installation"&gt;uv å®˜æ–¹å®‰è£…æŒ‡å—&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;éªŒè¯å®‰è£…&lt;/strong&gt;ï¼šç»ˆç«¯è¾“å…¥å‘½ä»¤ &lt;code&gt;uv --version&lt;/code&gt;ï¼Œå¦‚æœæ­£å¸¸æ˜¾ç¤ºç‰ˆæœ¬å·ï¼Œè¯æ˜å·²ç»å®‰è£…æˆåŠŸ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;æ¨èç†ç”±&lt;/strong&gt;ï¼šuv æ˜¯ç›®å‰æœ€å¼ºçš„ Python åŒ…ç®¡ç†å·¥å…·ï¼Œé€Ÿåº¦å¿«ã€ä¾èµ–è§£æå‡†ç¡®&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸŸ¢ Node.js å®‰è£…&lt;/h3&gt; 
&lt;p&gt;é¡¹ç›®ä¾èµ– Node.jsï¼Œè¯·å‰å¾€å®˜ç½‘ä¸‹è½½å®‰è£…ï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ä¸‹è½½åœ°å€&lt;/strong&gt;ï¼š&lt;a href="https://nodejs.org/en/download/"&gt;https://nodejs.org/en/download/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ç‰ˆæœ¬è¦æ±‚&lt;/strong&gt;ï¼š&amp;gt;= 16.0.0&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ“¦ Python åŒ…å®‰è£…&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# è¿›å…¥é¡¹ç›®ç›®å½•
cd MediaCrawler

# ä½¿ç”¨ uv sync å‘½ä»¤æ¥ä¿è¯ python ç‰ˆæœ¬å’Œç›¸å…³ä¾èµ–åŒ…çš„ä¸€è‡´æ€§
uv sync
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;ğŸŒ æµè§ˆå™¨é©±åŠ¨å®‰è£…&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# å®‰è£…æµè§ˆå™¨é©±åŠ¨
uv run playwright install
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ğŸš€ è¿è¡Œçˆ¬è™«ç¨‹åº&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# åœ¨ config/base_config.py æŸ¥çœ‹é…ç½®é¡¹ç›®åŠŸèƒ½ï¼Œå†™çš„æœ‰ä¸­æ–‡æ³¨é‡Š

# ä»é…ç½®æ–‡ä»¶ä¸­è¯»å–å…³é”®è¯æœç´¢ç›¸å…³çš„å¸–å­å¹¶çˆ¬å–å¸–å­ä¿¡æ¯ä¸è¯„è®º
uv run main.py --platform xhs --lt qrcode --type search

# ä»é…ç½®æ–‡ä»¶ä¸­è¯»å–æŒ‡å®šçš„å¸–å­IDåˆ—è¡¨è·å–æŒ‡å®šå¸–å­çš„ä¿¡æ¯ä¸è¯„è®ºä¿¡æ¯
uv run main.py --platform xhs --lt qrcode --type detail

# æ‰“å¼€å¯¹åº”APPæ‰«äºŒç»´ç ç™»å½•

# å…¶ä»–å¹³å°çˆ¬è™«ä½¿ç”¨ç¤ºä¾‹ï¼Œæ‰§è¡Œä¸‹é¢çš„å‘½ä»¤æŸ¥çœ‹
uv run main.py --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;WebUIæ”¯æŒ&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;ğŸ–¥ï¸ &lt;strong&gt;WebUI å¯è§†åŒ–æ“ä½œç•Œé¢&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;MediaCrawler æä¾›äº†åŸºäº Web çš„å¯è§†åŒ–æ“ä½œç•Œé¢ï¼Œæ— éœ€å‘½ä»¤è¡Œä¹Ÿèƒ½è½»æ¾ä½¿ç”¨çˆ¬è™«åŠŸèƒ½ã€‚&lt;/p&gt; 
 &lt;h4&gt;å¯åŠ¨ WebUI æœåŠ¡&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;# å¯åŠ¨ API æœåŠ¡å™¨ï¼ˆé»˜è®¤ç«¯å£ 8080ï¼‰
uv run uvicorn api.main:app --port 8080 --reload

# æˆ–è€…ä½¿ç”¨æ¨¡å—æ–¹å¼å¯åŠ¨
uv run python -m api.main
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;å¯åŠ¨æˆåŠŸåï¼Œè®¿é—® &lt;code&gt;http://localhost:8080&lt;/code&gt; å³å¯æ‰“å¼€ WebUI ç•Œé¢ã€‚&lt;/p&gt; 
 &lt;h4&gt;WebUI åŠŸèƒ½ç‰¹æ€§&lt;/h4&gt; 
 &lt;ul&gt; 
  &lt;li&gt;å¯è§†åŒ–é…ç½®çˆ¬è™«å‚æ•°ï¼ˆå¹³å°ã€ç™»å½•æ–¹å¼ã€çˆ¬å–ç±»å‹ç­‰ï¼‰&lt;/li&gt; 
  &lt;li&gt;å®æ—¶æŸ¥çœ‹çˆ¬è™«è¿è¡ŒçŠ¶æ€å’Œæ—¥å¿—&lt;/li&gt; 
  &lt;li&gt;æ•°æ®é¢„è§ˆå’Œå¯¼å‡º&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h4&gt;ç•Œé¢é¢„è§ˆ&lt;/h4&gt; 
 &lt;img src="https://raw.githubusercontent.com/NanmiCoder/MediaCrawler/main/docs/static/images/img_8.png" alt="WebUI ç•Œé¢é¢„è§ˆ" /&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;ğŸ”— &lt;strong&gt;ä½¿ç”¨ Python åŸç”Ÿ venv ç®¡ç†ç¯å¢ƒï¼ˆä¸æ¨èï¼‰&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h4&gt;åˆ›å»ºå¹¶æ¿€æ´» Python è™šæ‹Ÿç¯å¢ƒ&lt;/h4&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;å¦‚æœæ˜¯çˆ¬å–æŠ–éŸ³å’ŒçŸ¥ä¹ï¼Œéœ€è¦æå‰å®‰è£… nodejs ç¯å¢ƒï¼Œç‰ˆæœ¬å¤§äºç­‰äºï¼š&lt;code&gt;16&lt;/code&gt; å³å¯&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;# è¿›å…¥é¡¹ç›®æ ¹ç›®å½•
cd MediaCrawler

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
# æˆ‘çš„ python ç‰ˆæœ¬æ˜¯ï¼š3.11 requirements.txt ä¸­çš„åº“æ˜¯åŸºäºè¿™ä¸ªç‰ˆæœ¬çš„
# å¦‚æœæ˜¯å…¶ä»– python ç‰ˆæœ¬ï¼Œå¯èƒ½ requirements.txt ä¸­çš„åº“ä¸å…¼å®¹ï¼Œéœ€è‡ªè¡Œè§£å†³
python -m venv venv

# macOS &amp;amp; Linux æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
source venv/bin/activate

# Windows æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
venv\Scripts\activate
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;å®‰è£…ä¾èµ–åº“&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;å®‰è£… playwright æµè§ˆå™¨é©±åŠ¨&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;playwright install
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;è¿è¡Œçˆ¬è™«ç¨‹åºï¼ˆåŸç”Ÿç¯å¢ƒï¼‰&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;# é¡¹ç›®é»˜è®¤æ˜¯æ²¡æœ‰å¼€å¯è¯„è®ºçˆ¬å–æ¨¡å¼ï¼Œå¦‚éœ€è¯„è®ºè¯·åœ¨ config/base_config.py ä¸­çš„ ENABLE_GET_COMMENTS å˜é‡ä¿®æ”¹
# ä¸€äº›å…¶ä»–æ”¯æŒé¡¹ï¼Œä¹Ÿå¯ä»¥åœ¨ config/base_config.py æŸ¥çœ‹åŠŸèƒ½ï¼Œå†™çš„æœ‰ä¸­æ–‡æ³¨é‡Š

# ä»é…ç½®æ–‡ä»¶ä¸­è¯»å–å…³é”®è¯æœç´¢ç›¸å…³çš„å¸–å­å¹¶çˆ¬å–å¸–å­ä¿¡æ¯ä¸è¯„è®º
python main.py --platform xhs --lt qrcode --type search

# ä»é…ç½®æ–‡ä»¶ä¸­è¯»å–æŒ‡å®šçš„å¸–å­IDåˆ—è¡¨è·å–æŒ‡å®šå¸–å­çš„ä¿¡æ¯ä¸è¯„è®ºä¿¡æ¯
python main.py --platform xhs --lt qrcode --type detail

# æ‰“å¼€å¯¹åº”APPæ‰«äºŒç»´ç ç™»å½•

# å…¶ä»–å¹³å°çˆ¬è™«ä½¿ç”¨ç¤ºä¾‹ï¼Œæ‰§è¡Œä¸‹é¢çš„å‘½ä»¤æŸ¥çœ‹
python main.py --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;ğŸ’¾ æ•°æ®ä¿å­˜&lt;/h2&gt; 
&lt;p&gt;MediaCrawler æ”¯æŒå¤šç§æ•°æ®å­˜å‚¨æ–¹å¼ï¼ŒåŒ…æ‹¬ CSVã€JSONã€Excelã€SQLite å’Œ MySQL æ•°æ®åº“ã€‚&lt;/p&gt; 
&lt;p&gt;ğŸ“– &lt;strong&gt;è¯¦ç»†ä½¿ç”¨è¯´æ˜è¯·æŸ¥çœ‹ï¼š&lt;a href="https://raw.githubusercontent.com/NanmiCoder/MediaCrawler/main/docs/data_storage_guide.md"&gt;æ•°æ®å­˜å‚¨æŒ‡å—&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/MediaCrawlerPro"&gt;ğŸš€ MediaCrawlerPro é‡ç£…å‘å¸ƒ ğŸš€ï¼æ›´å¤šçš„åŠŸèƒ½ï¼Œæ›´å¥½çš„æ¶æ„è®¾è®¡ï¼å¼€æºä¸æ˜“ï¼Œæ¬¢è¿è®¢é˜…æ”¯æŒï¼&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;ğŸ’¬ äº¤æµç¾¤ç»„&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;å¾®ä¿¡äº¤æµç¾¤&lt;/strong&gt;ï¼š&lt;a href="https://nanmicoder.github.io/MediaCrawler/%E5%BE%AE%E4%BF%A1%E4%BA%A4%E6%B5%81%E7%BE%A4.html"&gt;ç‚¹å‡»åŠ å…¥&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Bç«™è´¦å·&lt;/strong&gt;ï¼š&lt;a href="https://space.bilibili.com/434377496"&gt;å…³æ³¨æˆ‘&lt;/a&gt;ï¼Œåˆ†äº«AIä¸çˆ¬è™«æŠ€æœ¯çŸ¥è¯†&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ’° èµåŠ©å•†å±•ç¤º&lt;/h3&gt; 
&lt;a href="https://h.wandouip.com"&gt; &lt;img src="https://raw.githubusercontent.com/NanmiCoder/MediaCrawler/main/docs/static/images/img_8.jpg" /&gt; &lt;br /&gt; è±Œè±†HTTPè‡ªè¥åƒä¸‡çº§IPèµ„æºæ± ï¼ŒIPçº¯å‡€åº¦â‰¥99.8%ï¼Œæ¯æ—¥ä¿æŒIPé«˜é¢‘æ›´æ–°ï¼Œå¿«é€Ÿå“åº”ï¼Œç¨³å®šè¿æ¥,æ»¡è¶³å¤šç§ä¸šåŠ¡åœºæ™¯ï¼Œæ”¯æŒæŒ‰éœ€å®šåˆ¶ï¼Œæ³¨å†Œå…è´¹æå–10000ipã€‚ &lt;/a&gt; 
&lt;hr /&gt; 
&lt;a href="https://tikhub.io/?utm_source=github.com/NanmiCoder/MediaCrawler&amp;amp;utm_medium=marketing_social&amp;amp;utm_campaign=retargeting&amp;amp;utm_content=carousel_ad"&gt; &lt;img width="500" src="https://raw.githubusercontent.com/NanmiCoder/MediaCrawler/main/docs/static/images/tikhub_banner_zh.png" /&gt; &lt;br /&gt; TikHub.io æä¾› 900+ é«˜ç¨³å®šæ€§æ•°æ®æ¥å£ï¼Œè¦†ç›– TKã€DYã€XHSã€Y2Bã€Insã€X ç­‰ 14+ æµ·å†…å¤–ä¸»æµå¹³å°ï¼Œæ”¯æŒç”¨æˆ·ã€å†…å®¹ã€å•†å“ã€è¯„è®ºç­‰å¤šç»´åº¦å…¬å¼€æ•°æ® APIï¼Œå¹¶é…å¥— 4000 ä¸‡+ å·²æ¸…æ´—ç»“æ„åŒ–æ•°æ®é›†ï¼Œä½¿ç”¨é‚€è¯·ç  &lt;code&gt;cfzyejV9&lt;/code&gt; æ³¨å†Œå¹¶å……å€¼ï¼Œå³å¯é¢å¤–è·å¾— $2 èµ é€é¢åº¦ã€‚ &lt;/a&gt; 
&lt;hr /&gt; 
&lt;a href="https://www.thordata.com/?ls=github&amp;amp;lk=mediacrawler"&gt; &lt;img width="500" src="https://raw.githubusercontent.com/NanmiCoder/MediaCrawler/main/docs/static/images/Thordata.png" /&gt; &lt;br /&gt; Thordataï¼šå¯é ä¸”ç»æµé«˜æ•ˆçš„ä»£ç†æœåŠ¡æä¾›å•†ã€‚ä¸ºä¼ä¸šå’Œå¼€å‘è€…æä¾›ç¨³å®šã€é«˜æ•ˆä¸”åˆè§„çš„å…¨çƒä»£ç† IP æœåŠ¡ã€‚ç«‹å³æ³¨å†Œï¼Œèµ é€1GBä½å®…ä»£ç†å…è´¹è¯•ç”¨å’Œ2000æ¬¡serp-apiè°ƒç”¨ã€‚ &lt;/a&gt; 
&lt;br /&gt; 
&lt;a href="https://www.thordata.com/products/residential-proxies/?ls=github&amp;amp;lk=mediacrawler"&gt;ã€ä½å®…ä»£ç†ã€‘&lt;/a&gt; | 
&lt;a href="https://www.thordata.com/products/web-scraper/?ls=github&amp;amp;lk=mediacrawler"&gt;ã€serp-apiã€‘&lt;/a&gt; 
&lt;h3&gt;ğŸ¤ æˆä¸ºèµåŠ©è€…&lt;/h3&gt; 
&lt;p&gt;æˆä¸ºèµåŠ©è€…ï¼Œå¯ä»¥å°†æ‚¨çš„äº§å“å±•ç¤ºåœ¨è¿™é‡Œï¼Œæ¯å¤©è·å¾—å¤§é‡æ›å…‰ï¼&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;è”ç³»æ–¹å¼&lt;/strong&gt;ï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;å¾®ä¿¡ï¼š&lt;code&gt;relakkes&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;é‚®ç®±ï¼š&lt;code&gt;relakkes@gmail.com&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h3&gt;ğŸ“š å…¶ä»–&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;å¸¸è§é—®é¢˜&lt;/strong&gt;ï¼š&lt;a href="https://nanmicoder.github.io/MediaCrawler/"&gt;MediaCrawler å®Œæ•´æ–‡æ¡£&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;çˆ¬è™«å…¥é—¨æ•™ç¨‹&lt;/strong&gt;ï¼š&lt;a href="https://github.com/NanmiCoder/CrawlerTutorial"&gt;CrawlerTutorial å…è´¹æ•™ç¨‹&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;æ–°é—»çˆ¬è™«å¼€æºé¡¹ç›®&lt;/strong&gt;ï¼š&lt;a href="https://github.com/NanmiCoder/NewsCrawlerCollection"&gt;NewsCrawlerCollection&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;â­ Star è¶‹åŠ¿å›¾&lt;/h2&gt; 
&lt;p&gt;å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œè¯·ç»™ä¸ª â­ Star æ”¯æŒä¸€ä¸‹ï¼Œè®©æ›´å¤šçš„äººçœ‹åˆ° MediaCrawlerï¼&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#NanmiCoder/MediaCrawler&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=NanmiCoder/MediaCrawler&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;ğŸ“š å‚è€ƒ&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;å°çº¢ä¹¦ç­¾åä»“åº“&lt;/strong&gt;ï¼š&lt;a href="https://github.com/Cloxl/xhshow"&gt;Cloxl çš„ xhs ç­¾åä»“åº“&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;å°çº¢ä¹¦å®¢æˆ·ç«¯&lt;/strong&gt;ï¼š&lt;a href="https://github.com/ReaJason/xhs"&gt;ReaJason çš„ xhs ä»“åº“&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;çŸ­ä¿¡è½¬å‘&lt;/strong&gt;ï¼š&lt;a href="https://github.com/pppscn/SmsForwarder"&gt;SmsForwarder å‚è€ƒä»“åº“&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;å†…ç½‘ç©¿é€å·¥å…·&lt;/strong&gt;ï¼š&lt;a href="https://ngrok.com/docs/"&gt;ngrok å®˜æ–¹æ–‡æ¡£&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;å…è´£å£°æ˜&lt;/h1&gt; 
&lt;div id="disclaimer"&gt; 
 &lt;h2&gt;1. é¡¹ç›®ç›®çš„ä¸æ€§è´¨&lt;/h2&gt; 
 &lt;p&gt;æœ¬é¡¹ç›®ï¼ˆä»¥ä¸‹ç®€ç§°â€œæœ¬é¡¹ç›®â€ï¼‰æ˜¯ä½œä¸ºä¸€ä¸ªæŠ€æœ¯ç ”ç©¶ä¸å­¦ä¹ å·¥å…·è€Œåˆ›å»ºçš„ï¼Œæ—¨åœ¨æ¢ç´¢å’Œå­¦ä¹ ç½‘ç»œæ•°æ®é‡‡é›†æŠ€æœ¯ã€‚æœ¬é¡¹ç›®ä¸“æ³¨äºè‡ªåª’ä½“å¹³å°çš„æ•°æ®çˆ¬å–æŠ€æœ¯ç ”ç©¶ï¼Œæ—¨åœ¨æä¾›ç»™å­¦ä¹ è€…å’Œç ”ç©¶è€…ä½œä¸ºæŠ€æœ¯äº¤æµä¹‹ç”¨ã€‚&lt;/p&gt; 
 &lt;h2&gt;2. æ³•å¾‹åˆè§„æ€§å£°æ˜&lt;/h2&gt; 
 &lt;p&gt;æœ¬é¡¹ç›®å¼€å‘è€…ï¼ˆä»¥ä¸‹ç®€ç§°â€œå¼€å‘è€…â€ï¼‰éƒ‘é‡æé†’ç”¨æˆ·åœ¨ä¸‹è½½ã€å®‰è£…å’Œä½¿ç”¨æœ¬é¡¹ç›®æ—¶ï¼Œä¸¥æ ¼éµå®ˆä¸­åäººæ°‘å…±å’Œå›½ç›¸å…³æ³•å¾‹æ³•è§„ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºã€Šä¸­åäººæ°‘å…±å’Œå›½ç½‘ç»œå®‰å…¨æ³•ã€‹ã€ã€Šä¸­åäººæ°‘å…±å’Œå›½åé—´è°æ³•ã€‹ç­‰æ‰€æœ‰é€‚ç”¨çš„å›½å®¶æ³•å¾‹å’Œæ”¿ç­–ã€‚ç”¨æˆ·åº”è‡ªè¡Œæ‰¿æ‹…ä¸€åˆ‡å› ä½¿ç”¨æœ¬é¡¹ç›®è€Œå¯èƒ½å¼•èµ·çš„æ³•å¾‹è´£ä»»ã€‚&lt;/p&gt; 
 &lt;h2&gt;3. ä½¿ç”¨ç›®çš„é™åˆ¶&lt;/h2&gt; 
 &lt;p&gt;æœ¬é¡¹ç›®ä¸¥ç¦ç”¨äºä»»ä½•éæ³•ç›®çš„æˆ–éå­¦ä¹ ã€éç ”ç©¶çš„å•†ä¸šè¡Œä¸ºã€‚æœ¬é¡¹ç›®ä¸å¾—ç”¨äºä»»ä½•å½¢å¼çš„éæ³•ä¾µå…¥ä»–äººè®¡ç®—æœºç³»ç»Ÿï¼Œä¸å¾—ç”¨äºä»»ä½•ä¾µçŠ¯ä»–äººçŸ¥è¯†äº§æƒæˆ–å…¶ä»–åˆæ³•æƒç›Šçš„è¡Œä¸ºã€‚ç”¨æˆ·åº”ä¿è¯å…¶ä½¿ç”¨æœ¬é¡¹ç›®çš„ç›®çš„çº¯å±ä¸ªäººå­¦ä¹ å’ŒæŠ€æœ¯ç ”ç©¶ï¼Œä¸å¾—ç”¨äºä»»ä½•å½¢å¼çš„éæ³•æ´»åŠ¨ã€‚&lt;/p&gt; 
 &lt;h2&gt;4. å…è´£å£°æ˜&lt;/h2&gt; 
 &lt;p&gt;å¼€å‘è€…å·²å°½æœ€å¤§åŠªåŠ›ç¡®ä¿æœ¬é¡¹ç›®çš„æ­£å½“æ€§åŠå®‰å…¨æ€§ï¼Œä½†ä¸å¯¹ç”¨æˆ·ä½¿ç”¨æœ¬é¡¹ç›®å¯èƒ½å¼•èµ·çš„ä»»ä½•å½¢å¼çš„ç›´æ¥æˆ–é—´æ¥æŸå¤±æ‰¿æ‹…è´£ä»»ã€‚åŒ…æ‹¬ä½†ä¸é™äºç”±äºä½¿ç”¨æœ¬é¡¹ç›®è€Œå¯¼è‡´çš„ä»»ä½•æ•°æ®ä¸¢å¤±ã€è®¾å¤‡æŸåã€æ³•å¾‹è¯‰è®¼ç­‰ã€‚&lt;/p&gt; 
 &lt;h2&gt;5. çŸ¥è¯†äº§æƒå£°æ˜&lt;/h2&gt; 
 &lt;p&gt;æœ¬é¡¹ç›®çš„çŸ¥è¯†äº§æƒå½’å¼€å‘è€…æ‰€æœ‰ã€‚æœ¬é¡¹ç›®å—åˆ°è‘—ä½œæƒæ³•å’Œå›½é™…è‘—ä½œæƒæ¡çº¦ä»¥åŠå…¶ä»–çŸ¥è¯†äº§æƒæ³•å¾‹å’Œæ¡çº¦çš„ä¿æŠ¤ã€‚ç”¨æˆ·åœ¨éµå®ˆæœ¬å£°æ˜åŠç›¸å…³æ³•å¾‹æ³•è§„çš„å‰æä¸‹ï¼Œå¯ä»¥ä¸‹è½½å’Œä½¿ç”¨æœ¬é¡¹ç›®ã€‚&lt;/p&gt; 
 &lt;h2&gt;6. æœ€ç»ˆè§£é‡Šæƒ&lt;/h2&gt; 
 &lt;p&gt;å…³äºæœ¬é¡¹ç›®çš„æœ€ç»ˆè§£é‡Šæƒå½’å¼€å‘è€…æ‰€æœ‰ã€‚å¼€å‘è€…ä¿ç•™éšæ—¶æ›´æ”¹æˆ–æ›´æ–°æœ¬å…è´£å£°æ˜çš„æƒåˆ©ï¼Œæ•ä¸å¦è¡Œé€šçŸ¥ã€‚&lt;/p&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>datalab-to/chandra</title>
      <link>https://github.com/datalab-to/chandra</link>
      <description>&lt;p&gt;OCR model that handles complex tables, forms, handwriting with full layout.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/datalab-to/chandra/master/assets/datalab-logo.png" alt="Datalab Logo" width="150" /&gt; &lt;/p&gt; 
&lt;h1 align="center"&gt;Datalab&lt;/h1&gt; 
&lt;p align="center"&gt; &lt;strong&gt;State of the Art models for Document Intelligence&lt;/strong&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://opensource.org/licenses/Apache-2.0"&gt;&lt;img src="https://img.shields.io/badge/Code%20License-Apache_2.0-green.svg?sanitize=true" alt="Code License" /&gt;&lt;/a&gt; &lt;a href="https://www.datalab.to/pricing"&gt;&lt;img src="https://img.shields.io/badge/Model%20License-OpenRAIL--M-blue.svg?sanitize=true" alt="Model License" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/KuZwXNGnfH"&gt;&lt;img src="https://img.shields.io/badge/Discord-Join%20us-5865F2?logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;h1&gt;Chandra&lt;/h1&gt; 
&lt;p&gt;An OCR model for complex documents â€” handwriting, tables, math equations, and messy forms.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/datalab-to/chandra/master/assets/examples/forms/handwritten_form.png" width="700px" /&gt; 
&lt;h2&gt;Benchmarks&lt;/h2&gt; 
&lt;p&gt;Overall scores on the &lt;a href="https://github.com/allenai/olmocr"&gt;olmocr bench&lt;/a&gt;:&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/datalab-to/chandra/master/assets/benchmarks/bench.png" width="600px" /&gt; 
&lt;h2&gt;Hosted API&lt;/h2&gt; 
&lt;p&gt;A hosted API with additional accuracy improvements is available at &lt;a href="https://www.datalab.to/"&gt;datalab.to&lt;/a&gt;. Try the &lt;a href="https://www.datalab.to/playground"&gt;free playground&lt;/a&gt; without installing.&lt;/p&gt; 
&lt;h2&gt;Community&lt;/h2&gt; 
&lt;p&gt;Join &lt;a href="https://discord.gg//KuZwXNGnfH"&gt;Discord&lt;/a&gt; to discuss development and get help.&lt;/p&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip install chandra-ocr

# Start vLLM server, then run OCR
chandra_vllm
chandra input.pdf ./output

# Or use HuggingFace locally
chandra input.pdf ./output --method hf

# Interactive web app
chandra_app
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Python:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from chandra.model import InferenceManager
from chandra.input import load_pdf_images

manager = InferenceManager(method="hf")
images = load_pdf_images("document.pdf")
results = manager.generate(images)
print(results[0].markdown)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;How it Works.&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Two inference modes&lt;/strong&gt;: Run locally via HuggingFace Transformers, or deploy a vLLM server for production throughput&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Layout-aware output&lt;/strong&gt;: Every text block, table, and image comes with bounding box coordinates&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Structured formats&lt;/strong&gt;: Output as Markdown, HTML, or JSON with full layout metadata&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;40+ languages&lt;/strong&gt; supported&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;What It Handles&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Handwriting&lt;/strong&gt; â€” Doctor notes, filled forms, homework. Chandra reads cursive and messy print that trips up traditional OCR.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Tables&lt;/strong&gt; â€” Preserves structure including merged cells (colspan/rowspan). Works on financial filings, invoices, and data tables.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Math&lt;/strong&gt; â€” Inline and block equations rendered as LaTeX. Handles textbooks, worksheets, and research papers.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Forms&lt;/strong&gt; â€” Reconstructs checkboxes, radio buttons, and form fields with their values.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Complex Layouts&lt;/strong&gt; â€” Multi-column documents, newspapers, textbooks with figures and captions.&lt;/p&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/datalab-to/chandra/master/assets/examples/handwriting/doctor_note.png" width="350px" /&gt;&lt;br /&gt;&lt;strong&gt;Handwriting&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/datalab-to/chandra/master/assets/examples/tables/water_damage.png" width="350px" /&gt;&lt;br /&gt;&lt;strong&gt;Tables&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/datalab-to/chandra/master/assets/examples/math/ega.png" width="350px" /&gt;&lt;br /&gt;&lt;strong&gt;Math&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/datalab-to/chandra/master/assets/examples/newspapers/nyt.png" width="350px" /&gt;&lt;br /&gt;&lt;strong&gt;Newspapers&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;details&gt; 
 &lt;summary&gt;More examples&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Type&lt;/th&gt; 
    &lt;th&gt;Name&lt;/th&gt; 
    &lt;th&gt;Link&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Tables&lt;/td&gt; 
    &lt;td&gt;10K Filing&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/datalab-to/chandra/raw/master/assets/examples/tables/10k.png"&gt;View&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Forms&lt;/td&gt; 
    &lt;td&gt;Lease Agreement&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/datalab-to/chandra/raw/master/assets/examples/forms/lease.png"&gt;View&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Handwriting&lt;/td&gt; 
    &lt;td&gt;Math Homework&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/datalab-to/chandra/raw/master/assets/examples/handwriting/math_hw.png"&gt;View&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Books&lt;/td&gt; 
    &lt;td&gt;Geography Textbook&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/datalab-to/chandra/raw/master/assets/examples/books/geo_textbook_page.png"&gt;View&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Books&lt;/td&gt; 
    &lt;td&gt;Exercise Problems&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/datalab-to/chandra/raw/master/assets/examples/books/exercises.png"&gt;View&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Math&lt;/td&gt; 
    &lt;td&gt;Attention Diagram&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/datalab-to/chandra/raw/master/assets/examples/math/attn_all.png"&gt;View&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Math&lt;/td&gt; 
    &lt;td&gt;Worksheet&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/datalab-to/chandra/raw/master/assets/examples/math/worksheet.png"&gt;View&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Newspapers&lt;/td&gt; 
    &lt;td&gt;LA Times&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/datalab-to/chandra/raw/master/assets/examples/newspapers/la_times.png"&gt;View&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Other&lt;/td&gt; 
    &lt;td&gt;Transcript&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/datalab-to/chandra/raw/master/assets/examples/other/transcript.png"&gt;View&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Other&lt;/td&gt; 
    &lt;td&gt;Flowchart&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/datalab-to/chandra/raw/master/assets/examples/other/flowchart.png"&gt;View&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install chandra-ocr
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For HuggingFace inference, we recommend installing &lt;a href="https://github.com/Dao-AILab/flash-attention"&gt;flash attention&lt;/a&gt; for better performance.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;From source:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/datalab-to/chandra.git
cd chandra
uv sync
source .venv/bin/activate
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;h3&gt;CLI&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Single file with vLLM server
chandra input.pdf ./output --method vllm

# Directory with local model
chandra ./documents ./output --method hf
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Options:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--method [hf|vllm]&lt;/code&gt;: Inference method (default: vllm)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--page-range TEXT&lt;/code&gt;: Page range for PDFs (e.g., "1-5,7,9-12")&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--max-output-tokens INTEGER&lt;/code&gt;: Max tokens per page&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--max-workers INTEGER&lt;/code&gt;: Parallel workers for vLLM&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--include-images/--no-images&lt;/code&gt;: Extract and save images (default: include)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--include-headers-footers/--no-headers-footers&lt;/code&gt;: Include page headers/footers (default: exclude)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--batch-size INTEGER&lt;/code&gt;: Pages per batch (default: 1)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Output structure:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;output/
â””â”€â”€ filename/
    â”œâ”€â”€ filename.md           # Markdown
    â”œâ”€â”€ filename.html         # HTML with bounding boxes
    â”œâ”€â”€ filename_metadata.json
    â””â”€â”€ images/               # Extracted images
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;vLLM Server&lt;/h3&gt; 
&lt;p&gt;For production or batch processing:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;chandra_vllm
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Launches a Docker container with optimized inference. Configure via environment:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;VLLM_API_BASE&lt;/code&gt;: Server URL (default: &lt;code&gt;http://localhost:8000/v1&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;VLLM_MODEL_NAME&lt;/code&gt;: Model name (default: &lt;code&gt;chandra&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;VLLM_GPUS&lt;/code&gt;: GPU device IDs (default: &lt;code&gt;0&lt;/code&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Configuration&lt;/h3&gt; 
&lt;p&gt;Settings via environment variables or &lt;code&gt;local.env&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;MODEL_CHECKPOINT=datalab-to/chandra
MAX_OUTPUT_TOKENS=8192
VLLM_API_BASE=http://localhost:8000/v1
VLLM_GPUS=0
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Commercial Usage&lt;/h2&gt; 
&lt;p&gt;Code is Apache 2.0. Model weights use a modified OpenRAIL-M license: free for research, personal use, and startups under $2M funding/revenue. Cannot be used competitively with our API. For broader commercial licensing, see &lt;a href="https://www.datalab.to/pricing?utm_source=gh-chandra"&gt;pricing&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Credits&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/huggingface/transformers"&gt;Huggingface Transformers&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/vllm-project/vllm"&gt;vLLM&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/allenai/olmocr"&gt;olmocr&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/QwenLM/Qwen3"&gt;Qwen3 VL&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Support Datalab&lt;/h2&gt; 
&lt;p&gt;If you find this repository helpful, please consider giving it a star â­&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>robert-mcdermott/ai-knowledge-graph</title>
      <link>https://github.com/robert-mcdermott/ai-knowledge-graph</link>
      <description>&lt;p&gt;AI Powered Knowledge Graph Generator&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://github.com/robert-mcdermott/ai-knowledge-graph/raw/main/data/ai-knowledge-graph-example.png" alt="ai-knowledge-graph-example" /&gt;&lt;/p&gt; 
&lt;h1&gt;AI Powered Knowledge Graph Generator&lt;/h1&gt; 
&lt;p&gt;This system takes an unstructured text document, and uses an LLM of your choice to extract knowledge in the form of Subject-Predicate-Object (SPO) triplets, and visualizes the relationships as an interactive knowledge graph. A demo of a knowlege graph created with this project can be found here: &lt;a href="https://robert-mcdermott.github.io/ai-knowledge-graph/"&gt;Industrial-Revolution Knowledge Graph&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Text Chunking&lt;/strong&gt;: Automatically splits large documents into manageable chunks for processing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Knowledge Extraction&lt;/strong&gt;: Uses AI to identify entities and their relationships&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Entity Standardization&lt;/strong&gt;: Ensures consistent entity naming across document chunks&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Relationship Inference&lt;/strong&gt;: Discovers additional relationships between disconnected parts of the graph&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Interactive Visualization&lt;/strong&gt;: Creates an interactive graph visualization&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Works with Any OpenAI Compatible API Endpoint&lt;/strong&gt;: Ollama, LM Studio, OpenAI, vLLM, LiteLLM (provides access to AWS Bedrock, Azure OpenAI, Anthropic and many other LLM services)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.11+&lt;/li&gt; 
 &lt;li&gt;Required packages (install using &lt;code&gt;pip install -r requirements.txt&lt;/code&gt; or &lt;code&gt;uv sync&lt;/code&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Clone this repository&lt;/li&gt; 
 &lt;li&gt;Install dependencies: &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Configure your settings in &lt;code&gt;config.toml&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Run the system:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python generate-graph.py --input your_text_file.txt --output knowledge_graph.html
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or with UV:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run generate-graph.py --input your_text_file.txt --output knowledge_graph.html
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or installing and using as a module:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install --upgrade -e .
generate-graph --input your_text_file.txt --output knowledge_graph.html
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Configuration&lt;/h2&gt; 
&lt;p&gt;The system can be configured using the &lt;code&gt;config.toml&lt;/code&gt; file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-toml"&gt;[llm]
model = "gemma3"  # Google open weight model
api_key = "sk-1234"
base_url = "http://localhost:11434/v1/chat/completions" # Local Ollama instance running locally (but can be any OpenAI compatible endpoint)
max_tokens = 8192
temperature = 0.2

[chunking]
chunk_size = 200  # Number of words per chunk
overlap = 20      # Number of words to overlap between chunks

[standardization]
enabled = true            # Enable entity standardization
use_llm_for_entities = true  # Use LLM for additional entity resolution

[inference]
enabled = true             # Enable relationship inference
use_llm_for_inference = true  # Use LLM for relationship inference
apply_transitive = true    # Apply transitive inference rules
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Command Line Options&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--input FILE&lt;/code&gt;: Input text file to process&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--output FILE&lt;/code&gt;: Output HTML file for visualization (default: knowledge_graph.html)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--config FILE&lt;/code&gt;: Path to config file (default: config.toml)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--debug&lt;/code&gt;: Enable debug output with raw LLM responses&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--no-standardize&lt;/code&gt;: Disable entity standardization&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--no-inference&lt;/code&gt;: Disable relationship inference&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--test&lt;/code&gt;: Generate sample visualization using test data&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Usage message (--help)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;generate-graph --help
usage: generate-graph [-h] [--test] [--config CONFIG] [--output OUTPUT] [--input INPUT] [--debug] [--no-standardize] [--no-inference]

Knowledge Graph Generator and Visualizer

options:
  -h, --help        show this help message and exit
  --test            Generate a test visualization with sample data
  --config CONFIG   Path to configuration file
  --output OUTPUT   Output HTML file path
  --input INPUT     Path to input text file (required unless --test is used)
  --debug           Enable debug output (raw LLM responses and extracted JSON)
  --no-standardize  Disable entity standardization
  --no-inference    Disable relationship inference
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Example Run&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Command:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;generate-graph --input data/industrial-revolution.txt --output industrial-revolution-kg.html
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Console Output:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-markdown"&gt;Using input text from file: data/industrial-revolution.txt
==================================================
PHASE 1: INITIAL TRIPLE EXTRACTION
==================================================
Processing text in 13 chunks (size: 100 words, overlap: 20 words)
Processing chunk 1/13 (100 words)
Processing chunk 2/13 (100 words)
Processing chunk 3/13 (100 words)
Processing chunk 4/13 (100 words)
Processing chunk 5/13 (100 words)
Processing chunk 6/13 (100 words)
Processing chunk 7/13 (100 words)
Processing chunk 8/13 (100 words)
Processing chunk 9/13 (100 words)
Processing chunk 10/13 (100 words)
Processing chunk 11/13 (100 words)
Processing chunk 12/13 (86 words)
Processing chunk 13/13 (20 words)

Extracted a total of 216 triples from all chunks

==================================================
PHASE 2: ENTITY STANDARDIZATION
==================================================
Starting with 216 triples and 201 unique entities
Standardizing entity names across all triples...
Applied LLM-based entity standardization for 15 entity groups
Standardized 201 entities into 181 standard forms
After standardization: 216 triples and 160 unique entities

==================================================
PHASE 3: RELATIONSHIP INFERENCE
==================================================
Starting with 216 triples
Top 5 relationship types before inference:
  - enables: 20 occurrences
  - impacts: 15 occurrences
  - enabled: 12 occurrences
  - pioneered: 10 occurrences
  - invented: 9 occurrences
Inferring additional relationships between entities...
Identified 9 disconnected communities in the graph
Inferred 3 new relationships between communities
Inferred 3 new relationships between communities
Inferred 3 new relationships between communities
Inferred 3 new relationships between communities
Inferred 3 new relationships between communities
Inferred 3 new relationships between communities
Inferred 3 new relationships between communities
Inferred 3 new relationships between communities
Inferred 3 new relationships between communities
Inferred 3 new relationships between communities
Inferred 9 new relationships within communities
Inferred 2 new relationships within communities
Inferred 88 relationships based on lexical similarity
Added -22 inferred relationships

Top 5 relationship types after inference:
  - related to: 65 occurrences
  - advances via Artificial Intelligence: 36 occurrences
  - pioneered via computing: 26 occurrences
  - enables via computing: 24 occurrences
  - enables: 21 occurrences

Added 370 inferred relationships
Final knowledge graph: 564 triples
Saved raw knowledge graph data to /mnt/c/Users/rmcdermo/Documents/industrial-revolution-kg.json
Processing 564 triples for visualization
Found 161 unique nodes
Found 355 inferred relationships
Detected 9 communities using Louvain method
Nodes in NetworkX graph: 161
Edges in NetworkX graph: 537
Knowledge graph visualization saved to /mnt/c/Users/rmcdermo/Documents/industrial-revolution-kg.html
Graph Statistics: {
  "nodes": 161,
  "edges": 564,
  "original_edges": 209,
  "inferred_edges": 355,
  "communities": 9
}

Knowledge Graph Statistics:
Nodes: 161
Edges: 564
Communities: 9

To view the visualization, open the following file in your browser:
file:///mnt/c/Users/rmcdermo/Documents/industrial-revolution-kg.html
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;How It Works&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Chunking&lt;/strong&gt;: The document is split into overlapping chunks to fit within the LLM's context window&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;First Pass - SPO Extraction&lt;/strong&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;Each chunk is processed by the LLM to extract Subject-Predicate-Object triplets&lt;/li&gt; 
   &lt;li&gt;Implemented in the &lt;code&gt;process_with_llm&lt;/code&gt; function&lt;/li&gt; 
   &lt;li&gt;The LLM identifies entities and their relationships within each text segment&lt;/li&gt; 
   &lt;li&gt;Results are collected across all chunks to form the initial knowledge graph&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Second Pass - Entity Standardization&lt;/strong&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;Basic standardization through text normalization&lt;/li&gt; 
   &lt;li&gt;Optional LLM-assisted entity alignment (controlled by &lt;code&gt;standardization.use_llm_for_entities&lt;/code&gt; config)&lt;/li&gt; 
   &lt;li&gt;When enabled, the LLM reviews all unique entities from the graph and identifies groups that refer to the same concept&lt;/li&gt; 
   &lt;li&gt;This resolves cases where the same entity appears differently across chunks (e.g., "AI", "artificial intelligence", "AI system")&lt;/li&gt; 
   &lt;li&gt;Standardization helps create a more coherent and navigable knowledge graph&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Third Pass - Relationship Inference&lt;/strong&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;Automatic inference of transitive relationships&lt;/li&gt; 
   &lt;li&gt;Optional LLM-assisted inference between disconnected graph components (controlled by &lt;code&gt;inference.use_llm_for_inference&lt;/code&gt; config)&lt;/li&gt; 
   &lt;li&gt;When enabled, the LLM analyzes representative entities from disconnected communities and infers plausible relationships&lt;/li&gt; 
   &lt;li&gt;This reduces graph fragmentation by adding logical connections not explicitly stated in the text&lt;/li&gt; 
   &lt;li&gt;Both rule-based and LLM-based inference methods work together to create a more comprehensive graph&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Visualization&lt;/strong&gt;: An interactive HTML visualization is generated using the PyVis library&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Both the second and third passes are optional and can be disabled in the configuration to minimize LLM usage or control these processes manually.&lt;/p&gt; 
&lt;h2&gt;Visualization Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Color-coded Communities&lt;/strong&gt;: Node colors represent different communities&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Node Size&lt;/strong&gt;: Nodes sized by importance (degree, betweenness, eigenvector centrality)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Relationship Types&lt;/strong&gt;: Original relationships shown as solid lines, inferred relationships as dashed lines&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Interactive Controls&lt;/strong&gt;: Zoom, pan, hover for details, filtering and physics controls&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Light (default) and Dark mode themes&lt;/strong&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Project Layout&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;.
â”œâ”€â”€ config.toml                     # Main configuration file for the system
â”œâ”€â”€ generate-graph.py               # Entry point when run directly as a script
â”œâ”€â”€ pyproject.toml                  # Python project metadata and build configuration
â”œâ”€â”€ requirements.txt                # Python dependencies for 'pip' users
â”œâ”€â”€ uv.lock                         # Python dependencies for 'uv' users
â””â”€â”€ src/                            # Source code
    â”œâ”€â”€ generate_graph.py           # Main entry point script when run as a module
    â””â”€â”€ knowledge_graph/            # Core package
        â”œâ”€â”€ __init__.py             # Package initialization
        â”œâ”€â”€ config.py               # Configuration loading and validation
        â”œâ”€â”€ entity_standardization.py # Entity standardization algorithms
        â”œâ”€â”€ llm.py                  # LLM interaction and response processing
        â”œâ”€â”€ main.py                 # Main program flow and orchestration
        â”œâ”€â”€ prompts.py              # Centralized collection of LLM prompts
        â”œâ”€â”€ text_utils.py           # Text processing and chunking utilities
        â”œâ”€â”€ visualization.py        # Knowledge graph visualization generator
        â””â”€â”€ templates/              # HTML templates for visualization
            â””â”€â”€ graph_template.html # Base template for interactive graph
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Program Flow&lt;/h2&gt; 
&lt;p&gt;This diagram illustrates the program flow.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-mermaid"&gt;flowchart TD
    %% Main entry points
    A[main.py - Entry Point] --&amp;gt; B{Parse Arguments}
    
    %% Test mode branch
    B --&amp;gt;|--test flag| C[sample_data_visualization]
    C --&amp;gt; D[visualize_knowledge_graph]
    
    %% Normal processing branch
    B --&amp;gt;|normal processing| E[load_config]
    E --&amp;gt; F[process_text_in_chunks]
    
    %% Text processing
    F --&amp;gt; G[chunk_text]
    G --&amp;gt; H[process_with_llm]
    
    %% LLM processing
    H --&amp;gt; I[call_llm]
    I --&amp;gt; J[extract_json_from_text]
    
    %% Entity standardization phase
    F --&amp;gt; K{standardization enabled?}
    K --&amp;gt;|yes| L[standardize_entities]
    K --&amp;gt;|no| M{inference enabled?}
    L --&amp;gt; M
    
    %% Relationship inference phase
    M --&amp;gt;|yes| N[infer_relationships]
    M --&amp;gt;|no| O[visualize_knowledge_graph]
    N --&amp;gt; O
    
    %% Visualization components
    O --&amp;gt; P[_calculate_centrality_metrics]
    O --&amp;gt; Q[_detect_communities]
    O --&amp;gt; R[_calculate_node_sizes]
    O --&amp;gt; S[_add_nodes_and_edges_to_network]
    O --&amp;gt; T[_get_visualization_options]
    O --&amp;gt; U[_save_and_modify_html]
    
    %% Subprocesses
    L --&amp;gt; L1[_resolve_entities_with_llm]
    N --&amp;gt; N1[_identify_communities]
    N --&amp;gt; N2[_infer_relationships_with_llm]
    N --&amp;gt; N3[_infer_within_community_relationships]
    N --&amp;gt; N4[_apply_transitive_inference]
    N --&amp;gt; N5[_infer_relationships_by_lexical_similarity]
    N --&amp;gt; N6[_deduplicate_triples]
    
    %% File outputs
    U --&amp;gt; V[HTML Visualization]
    F --&amp;gt; W[JSON Data Export]
    
    %% Prompts usage
    Y[prompts.py] --&amp;gt; H
    Y --&amp;gt; L1
    Y --&amp;gt; N2
    Y --&amp;gt; N3
    
    %% Module dependencies
    subgraph Modules
        main.py
        config.py
        text_utils.py
        llm.py
        entity_standardization.py
        visualization.py
        prompts.py
    end
    
    %% Phases
    subgraph Phase 1: Triple Extraction
        G
        H
        I
        J
    end
    
    subgraph Phase 2: Entity Standardization
        L
        L1
    end
    
    subgraph Phase 3: Relationship Inference
        N
        N1
        N2
        N3
        N4
        N5
        N6
    end
    
    subgraph Phase 4: Visualization
        O
        P
        Q
        R
        S
        T
        U
    end
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Program Flow Description&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Entry Point&lt;/strong&gt;: The program starts in &lt;code&gt;main.py&lt;/code&gt; which parses command-line arguments.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Mode Selection&lt;/strong&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;If &lt;code&gt;--test&lt;/code&gt; flag is provided, it generates a sample visualization&lt;/li&gt; 
   &lt;li&gt;Otherwise, it processes the input text file&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Configuration&lt;/strong&gt;: Loads settings from &lt;code&gt;config.toml&lt;/code&gt; using &lt;code&gt;config.py&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Text Processing&lt;/strong&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Breaks text into chunks with overlap using &lt;code&gt;text_utils.py&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Processes each chunk with the LLM to extract triples&lt;/li&gt; 
   &lt;li&gt;Uses prompts from &lt;code&gt;prompts.py&lt;/code&gt; to guide the LLM's extraction process&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Entity Standardization&lt;/strong&gt; (optional):&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Standardizes entity names across all triples&lt;/li&gt; 
   &lt;li&gt;May use LLM for entity resolution in ambiguous cases&lt;/li&gt; 
   &lt;li&gt;Uses specialized prompts from &lt;code&gt;prompts.py&lt;/code&gt; for entity resolution&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Relationship Inference&lt;/strong&gt; (optional):&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Identifies communities in the graph&lt;/li&gt; 
   &lt;li&gt;Infers relationships between disconnected communities&lt;/li&gt; 
   &lt;li&gt;Applies transitive inference and lexical similarity rules&lt;/li&gt; 
   &lt;li&gt;Uses specialized prompts from &lt;code&gt;prompts.py&lt;/code&gt; for relationship inference&lt;/li&gt; 
   &lt;li&gt;Deduplicates triples&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Visualization&lt;/strong&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Calculates centrality metrics and community detection&lt;/li&gt; 
   &lt;li&gt;Determines node sizes and colors based on importance&lt;/li&gt; 
   &lt;li&gt;Creates an interactive HTML visualization using PyVis&lt;/li&gt; 
   &lt;li&gt;Customizes the HTML with templates&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Output&lt;/strong&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Saves the knowledge graph as both HTML and JSON&lt;/li&gt; 
   &lt;li&gt;Displays statistics about nodes, edges, and communities&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt;</description>
    </item>
    
    <item>
      <title>sierra-research/tau2-bench</title>
      <link>https://github.com/sierra-research/tau2-bench</link>
      <description>&lt;p&gt;Ï„Â²-Bench: Evaluating Conversational Agents in a Dual-Control Environment&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;$\tau^2$-Bench: Evaluating Conversational Agents in a Dual-Control Environment&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://www.python.org"&gt;&lt;img src="https://img.shields.io/badge/Python-3.10%2B-blue.svg?style=flat&amp;amp;logo=python&amp;amp;logoColor=white" alt="python" /&gt;&lt;/a&gt; &lt;a href="https://github.com/astral-sh/ruff"&gt;&lt;img src="https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json" alt="Ruff" /&gt;&lt;/a&gt; &lt;a href="https://github.com/psf/black"&gt;&lt;img src="https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true" alt="Code style: black" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2506.07982"&gt;&lt;img src="http://img.shields.io/badge/cs.AI-arXiv%3A2506.07982-B31B1B.svg?logo=arxiv&amp;amp;logoColor=red" alt="arXiv" /&gt;&lt;/a&gt; &lt;a href="https://sierra.ai/blog/benchmarking-agents-in-collaborative-real-world-scenarios"&gt;&lt;img src="https://img.shields.io/badge/blog-tau2--bench-green" alt="blog" /&gt;&lt;/a&gt; &lt;a href="https://x.com/SierraPlatform/status/1932464265207889974"&gt;&lt;img src="https://img.shields.io/twitter/url/https/twitter.com/sierra.svg?style=social&amp;amp;label=Follow%20%40SierraPlatform" alt="Twitter" /&gt;&lt;/a&gt; &lt;a href="https://www.linkedin.com/posts/sierra_last-year-we-introduced-%F0%9D%9C%8F-bench-a-benchmark-activity-7338229693898231809-F8L4?utm_source=share&amp;amp;utm_medium=member_desktop&amp;amp;rcm=ACoAAAdc8goBmhEsiEo1_t_XSJbAnY4_zMfAWcE"&gt;&lt;img src="https://img.shields.io/badge/LinkedIn-0077B5?logo=linkedin&amp;amp;logoColor=white" alt="LinkedIn" /&gt;&lt;/a&gt; &lt;a href="https://taubench.com"&gt;&lt;img src="https://img.shields.io/badge/%F0%9F%8F%86_Live_Leaderboard-taubench.com-brightgreen?style=flat" alt="Leaderboard" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/sierra-research/tau2-bench/main/figs/overview.png" width="95%" alt="System Overview" /&gt;
 &lt;br /&gt; 
 &lt;em&gt;Figure 1: Ï„Â²-bench allows users to interact with the agent and the environment&lt;/em&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/sierra-research/tau2-bench/main/figs/traj.png" width="95%" alt="Trajectory" /&gt;
 &lt;br /&gt; 
 &lt;em&gt;Figure 2: Trajectory of a conversation between an agent and a user&lt;/em&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸ†• What's New&lt;/h2&gt; 
&lt;h3&gt;ğŸ¤– Reinforcement Learning Support (New!)&lt;/h3&gt; 
&lt;p&gt;Ï„Â²-bench now supports RL training with a Gymnasium-compatible interface:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ‹ï¸ Train RL Agents&lt;/strong&gt;: Use the gym interface to train agents with popular RL frameworks.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ® Play as Agent or User&lt;/strong&gt;: Interactive mode lets you control either the agent or the user in conversations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ“Š Train/Test Splits&lt;/strong&gt;: To help support experiments around training Agents and evaluating them, all domains include standardized task splits for proper train/test evaluation.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;âš ï¸ IMPORTANT FOR BACKWARD COMPATIBILITY&lt;/strong&gt;: If you are just evaluating an agent (not training), you &lt;strong&gt;MUST&lt;/strong&gt; use the &lt;code&gt;base&lt;/code&gt; task split to evaluate on the complete task set that matches the original Ï„Â²-bench structure. This ensures your results are comparable to previous evaluations and maintains consistency with the established benchmark. (If you don't specify a task split, it will default to &lt;code&gt;base&lt;/code&gt;.)&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ”§ Gymnasium Compatible&lt;/strong&gt;: Standard gym interface works with existing RL tools and libraries&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/sierra-research/tau2-bench/main/src/tau2/gym/README.md"&gt;&lt;strong&gt;â†’ See Gym Documentation&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/sierra-research/tau2-bench/main/#interactive-play-mode"&gt;&lt;strong&gt;â†’ Try CLI Play Mode&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;ğŸ† Live Leaderboard (v0.2.0)&lt;/h3&gt; 
&lt;p&gt;The Ï„Â²-bench leaderboard is now live at &lt;strong&gt;&lt;a href="https://taubench.com"&gt;taubench.com&lt;/a&gt;&lt;/strong&gt;!&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ“Š Interactive Rankings&lt;/strong&gt;: Compare model performance across all domains&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ“± Mobile-Friendly&lt;/strong&gt;: View results on any device&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ” Detailed Analysis&lt;/strong&gt;: Explore trajectories and conversation flows&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ğŸ“¥ Easy Submission&lt;/strong&gt;: Submit your results directly through the interface&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://taubench.com"&gt;&lt;strong&gt;â†’ Visit the Leaderboard&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/sierra-research/tau2-bench/main/#leaderboard-submission"&gt;&lt;strong&gt;â†’ Submit Your Results&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Overview&lt;/h2&gt; 
&lt;p&gt;$\tau^2$-bench implements a simulation framework for evaluating customer service agents across various domains.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;$\tau^2$-bench is the new iteration of the original $\tau$-bench&lt;/strong&gt;, featuring code fixes and an additional telecom domain.&lt;/p&gt; 
&lt;p&gt;Each domain specifies:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;a policy that the agent must follow&lt;/li&gt; 
 &lt;li&gt;a set of tools that the agent can use&lt;/li&gt; 
 &lt;li&gt;a set of tasks to evaluate the agent's performance&lt;/li&gt; 
 &lt;li&gt;Optionally: A set of tools that the user simulator can use&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Domains are:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;mock&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;airline&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;retail&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;telecom&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;All the information that an agent developer needs to build an agent for a domain can be accessed through the domain's API docs. See &lt;a href="https://raw.githubusercontent.com/sierra-research/tau2-bench/main/#view-domain-documentation"&gt;View domain documentation&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Clone the repository:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/sierra-research/tau2-bench
cd tau2-bench
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Create a new environment (optional)&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;$\tau^2$-bench requires Python 3.10 or higher. You may create and activate a new environment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m venv .venv
source .venv/bin/activate
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Install tau2&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will enable you to run the &lt;code&gt;tau2&lt;/code&gt; command.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; If you use &lt;code&gt;pip install .&lt;/code&gt; (without &lt;code&gt;-e&lt;/code&gt;), you'll need to set the &lt;code&gt;TAU2_DATA_DIR&lt;/code&gt; environment variable to point to your data directory:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export TAU2_DATA_DIR=/path/to/your/tau2-bench/data
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Check your data directory setup:&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;After installation, you can verify that your data directory is correctly configured by running:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;tau2 check-data
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This command will check if the data directory exists and print instructions if it is missing.&lt;/p&gt; 
&lt;p&gt;To remove all the generated files and the virtual environment, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;make clean
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;h3&gt;Setup LLM API keys&lt;/h3&gt; 
&lt;p&gt;We use &lt;a href="https://github.com/BerriAI/litellm"&gt;LiteLLM&lt;/a&gt; to manage LLM APIs, so you can use any LLM provider supported by LiteLLM.&lt;/p&gt; 
&lt;p&gt;To provide your API keys, copy &lt;code&gt;.env.example&lt;/code&gt; as &lt;code&gt;.env&lt;/code&gt; and edit it to include your API keys.&lt;/p&gt; 
&lt;h3&gt;Run agent evaluation&lt;/h3&gt; 
&lt;p&gt;To run a test evaluation on only 5 tasks with 1 trial per task, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;tau2 run \ 
--domain airline \
--agent-llm gpt-4.1 \
--user-llm gpt-4.1 \
--num-trials 1 \
--num-tasks 5
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Results will be saved in &lt;code&gt;data/tau2/simulations/&lt;/code&gt;.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;ğŸ’¡ Tip&lt;/strong&gt;: For full agent evaluation that matches the original Ï„Â²-bench methodology, remove &lt;code&gt;--num-tasks&lt;/code&gt; and use &lt;code&gt;--task-split base&lt;/code&gt; to evaluate on the complete task set.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Command Line Interface&lt;/h2&gt; 
&lt;p&gt;The &lt;code&gt;tau2&lt;/code&gt; command provides a unified interface for all functionality:&lt;/p&gt; 
&lt;h3&gt;Running Benchmark&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;tau2 run \
  --domain &amp;lt;domain&amp;gt; \
  --agent-llm &amp;lt;llm_name&amp;gt; \
  --user-llm &amp;lt;llm_name&amp;gt; \
  --num-trials &amp;lt;trial_count&amp;gt; \
  --task-ids &amp;lt;task_ids&amp;gt; \
  --max-concurrency &amp;lt;concurrent_sims&amp;gt; \
  ...
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Interactive Play Mode&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;tau2 play
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Experience Ï„Â²-bench from either perspective! The play mode allows you to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Play as Agent&lt;/strong&gt;: Manually control the agent's responses and tool calls&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Play as User&lt;/strong&gt;: Control the user while an LLM agent handles requests (available in domains with user tools like telecom)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Understand tasks&lt;/strong&gt; by walking through scenarios step-by-step&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Test strategies&lt;/strong&gt; before implementing them in code&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Choose task splits&lt;/strong&gt; to practice on training data or test on held-out tasks&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This is perfect for:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Getting familiar with domain policies and tools from both perspectives&lt;/li&gt; 
 &lt;li&gt;Debugging task scenarios and conversation flows&lt;/li&gt; 
 &lt;li&gt;Developing intuition for agent strategies&lt;/li&gt; 
 &lt;li&gt;Testing user behavior and agent responses&lt;/li&gt; 
 &lt;li&gt;Training yourself before training your model!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/sierra-research/tau2-bench/main/src/tau2/gym/README.md"&gt;Gym Documentation&lt;/a&gt; for more details on using the gymnasium interface programmatically, including the &lt;code&gt;AgentGymEnv&lt;/code&gt; (play as agent) and &lt;code&gt;UserGymEnv&lt;/code&gt; (play as user).&lt;/p&gt; 
&lt;h3&gt;Viewing Results&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;tau2 view
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This tool allows you to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Browse simulation files (in &lt;code&gt;data/tau2/simulations/&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;View agent performance metrics&lt;/li&gt; 
 &lt;li&gt;View a particular simulation&lt;/li&gt; 
 &lt;li&gt;View task details&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;View domain documentation&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;tau2 domain &amp;lt;domain&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Visit &lt;a href="http://127.0.0.1:8004/redoc"&gt;http://127.0.0.1:8004/redoc&lt;/a&gt; to see the domain policy and API documentation.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/sierra-research/tau2-bench/main/figs/domain_viewer.png" alt="domain_viewer1" /&gt;&lt;/p&gt; 
&lt;h3&gt;Check data configuration&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;tau2 check-data
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This command checks if your data directory is properly configured and all required files are present.&lt;/p&gt; 
&lt;h2&gt;Leaderboard Submission&lt;/h2&gt; 
&lt;p&gt;To submit your agent results to the Ï„Â²-bench leaderboard, you need to prepare a valid submission package that meets specific requirements.&lt;/p&gt; 
&lt;h3&gt;Requirements for Valid Submissions&lt;/h3&gt; 
&lt;p&gt;Your trajectory runs must follow these constraints:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Complete domain coverage&lt;/strong&gt;: Include results for all three domains:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;retail&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;airline&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;telecom&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Consistent model configuration&lt;/strong&gt;: All trajectory files must use:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;The same agent LLM with identical arguments across all domains&lt;/li&gt; 
   &lt;li&gt;The same user simulator LLM with identical arguments across all domains&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;One result per domain&lt;/strong&gt;: Each domain should appear exactly once in your submission&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;All tasks completed&lt;/strong&gt;: Run evaluation on all tasks within each domain (don't use &lt;code&gt;--task-ids&lt;/code&gt; or &lt;code&gt;--num-tasks&lt;/code&gt; filters)&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;ğŸ“ Note&lt;/strong&gt;: For consistency with the original Ï„Â²-bench evaluation methodology, use the &lt;code&gt;base&lt;/code&gt; task split when evaluating your agent to ensure you're testing on the complete, standard task set.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Preparing Your Submission&lt;/h3&gt; 
&lt;h4&gt;Step 1: Run Evaluations&lt;/h4&gt; 
&lt;p&gt;First, run your agent evaluation on all domains with consistent settings:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Example: Run complete evaluation for all domains
tau2 run --domain retail --agent-llm gpt-4.1 --user-llm gpt-4.1 --num-trials 4 --save-to my_model_retail
tau2 run --domain airline --agent-llm gpt-4.1 --user-llm gpt-4.1 --num-trials 4 --save-to my_model_airline  
tau2 run --domain telecom --agent-llm gpt-4.1 --user-llm gpt-4.1 --num-trials 4 --save-to my_model_telecom
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: Use identical &lt;code&gt;--agent-llm&lt;/code&gt;, &lt;code&gt;--user-llm&lt;/code&gt;, and their arguments across all runs.&lt;/p&gt; 
&lt;h4&gt;Step 2: Prepare Submission Package&lt;/h4&gt; 
&lt;p&gt;Use the submission preparation tool to create your leaderboard submission:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;tau2 submit prepare data/tau2/simulations/my_model_*.json --output ./my_submission
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This command will:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Verify all trajectory files are valid&lt;/li&gt; 
 &lt;li&gt;Check that submission requirements are met&lt;/li&gt; 
 &lt;li&gt;Compute performance metrics (Pass^k rates)&lt;/li&gt; 
 &lt;li&gt;Prompt for required metadata (model name, organization, contact email)&lt;/li&gt; 
 &lt;li&gt;Create a structured submission directory with: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;submission.json&lt;/code&gt;: Metadata and metrics&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;trajectories/&lt;/code&gt;: Your trajectory files&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Step 3: Validate Your Submission&lt;/h4&gt; 
&lt;p&gt;Before submitting, validate your submission package:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;tau2 submit validate ./my_submission
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will verify:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;All required files are present&lt;/li&gt; 
 &lt;li&gt;Trajectory files are valid&lt;/li&gt; 
 &lt;li&gt;Domain coverage is complete&lt;/li&gt; 
 &lt;li&gt;Model configurations are consistent&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Additional Options&lt;/h3&gt; 
&lt;h4&gt;Skip Verification (if needed)&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;tau2 submit prepare data/tau2/simulations/my_model_*.json --output ./my_submission --no-verify
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Verify Individual Trajectory Files&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;tau2 submit verify-trajs data/tau2/simulations/my_model_*.json
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Submitting to the Leaderboard&lt;/h3&gt; 
&lt;p&gt;Once your submission package is prepared and validated:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Review the generated &lt;code&gt;submission.json&lt;/code&gt; file&lt;/li&gt; 
 &lt;li&gt;Follow the submission guidelines in &lt;a href="https://raw.githubusercontent.com/sierra-research/tau2-bench/main/web/leaderboard/public/submissions/README.md"&gt;web/leaderboard/public/submissions/README.md&lt;/a&gt; to create a Pull Request&lt;/li&gt; 
 &lt;li&gt;Keep your &lt;code&gt;trajectories/&lt;/code&gt; directory for reference&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;The leaderboard will display your model's Pass^k success rates (k=1,2,3,4) across all domains.&lt;/p&gt; 
&lt;h2&gt;Experiments&lt;/h2&gt; 
&lt;h3&gt;Experimental Code Directory&lt;/h3&gt; 
&lt;p&gt;The &lt;code&gt;@experiments/&lt;/code&gt; directory contains experimental features and research code that extends beyond the core tau2 benchmark. This directory is designed for community contributions of innovative approaches, prototypes, and new features that are not part of the core evaluation framework.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Purpose&lt;/strong&gt;: Research code and experimental features&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Location&lt;/strong&gt;: &lt;code&gt;src/experiments/&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Usage&lt;/strong&gt;: Each experimental component has its own README with documentation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Status&lt;/strong&gt;: Experimental code is provided as-is and may not be fully tested or supported&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For more details, see the &lt;a href="https://raw.githubusercontent.com/sierra-research/tau2-bench/main/src/experiments/README.md"&gt;experiments README&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Running Ablation Studies (No User, or Agent with Oracle Plan)&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;telecom&lt;/code&gt; domain enables running ablation studies.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Running an LLM in &lt;code&gt;no-user&lt;/code&gt; mode. In this mode, the LLM is given all the tools and the information upfront. Just choose &lt;code&gt;llm_agent_solo&lt;/code&gt; as the agent and &lt;code&gt;dummy_user&lt;/code&gt; as the user.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;tau2 run \
  --domain telecom \
  --agent llm_agent_solo \
  --agent-llm gpt-4.1 \
  --user dummy_user \
  ...
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Running an LLM in &lt;code&gt;oracle-plan&lt;/code&gt; mode. In this mode, the LLM is given an oracle plan ahead of time alleviating the need for action planning. Just choose &lt;code&gt;llm_agent_gt&lt;/code&gt; as the agent.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;tau2 run \
  --domain telecom \
  --agent llm_agent_gt \
  --agent-llm gpt-4.1 \
  --user-llm gpt-4.1 \
  ...
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Running Telecom Domain with Workflow Policy&lt;/h3&gt; 
&lt;p&gt;To test the impact of policy format, we provide an additional "workflow" policy for the telecom domain. To run using this policy, use the &lt;code&gt;telecom-workflow&lt;/code&gt; domain.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;tau2 run \
  --domain telecom-workflow \
  --agent-llm gpt-4.1 \
  --user-llm gpt-4.1 \
  ...
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Domains&lt;/h2&gt; 
&lt;p&gt;For all the details see the domains &lt;a href="https://raw.githubusercontent.com/sierra-research/tau2-bench/main/src/tau2/domains/README.md"&gt;README&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Basics&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Code is located in &lt;code&gt;src/tau2/domains/&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Data is located in &lt;code&gt;data/tau2/domains/&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Each domain has its own configuration and task definitions&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;View domain-specific policy and API docs:&lt;/h4&gt; 
&lt;p&gt;Run the following command to see the domain policy and API documentation.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;tau2 env &amp;lt;domain&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then visit &lt;a href="http://127.0.0.1:8004/redoc"&gt;http://127.0.0.1:8004/redoc&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Environment CLI (beta)&lt;/h3&gt; 
&lt;p&gt;An interactive command-line interface for directly querying and testing domain environments. Features:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Interactive query interface with domain-specific tools&lt;/li&gt; 
 &lt;li&gt;Support for multiple domains (airline, mock, etc.)&lt;/li&gt; 
 &lt;li&gt;Session management with history&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To use:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;make env-cli
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Available commands:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;:q&lt;/code&gt; - quit the program&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;:d&lt;/code&gt; - change domain&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;:n&lt;/code&gt; - start new session (clears history)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Example usage:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;$ make env-cli

Welcome to the Environment CLI!
Connected to airline domain.

Query (:n new session, :d change domain, :q quit)&amp;gt; What flights are available from SF to LA tomorrow?
Assistant: Let me check the flight availability for you...
[Flight details will appear here]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The Environment CLI is useful for:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Testing domain tools and queries&lt;/li&gt; 
 &lt;li&gt;Debugging environment responses&lt;/li&gt; 
 &lt;li&gt;Exploring available domain functionality&lt;/li&gt; 
 &lt;li&gt;Quick domain interaction without starting the full server stack&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Run tests&lt;/h2&gt; 
&lt;p&gt;To run the test suite use the command&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;make test
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Config&lt;/h2&gt; 
&lt;p&gt;To configure the framework, see the &lt;a href="https://raw.githubusercontent.com/sierra-research/tau2-bench/main/src/tau2/config.py"&gt;config&lt;/a&gt; file.&lt;/p&gt; 
&lt;h3&gt;LLM Calls caching&lt;/h3&gt; 
&lt;p&gt;LLM call caching is disabled by default.&lt;/p&gt; 
&lt;p&gt;To enable LLM calls caching: - Make sure &lt;code&gt;redis&lt;/code&gt; is running. - Update the redis config in &lt;code&gt;config.py&lt;/code&gt; if necessary. - Set &lt;code&gt;LLM_CACHE_ENABLED&lt;/code&gt; to &lt;code&gt;True&lt;/code&gt; in &lt;code&gt;config.py&lt;/code&gt;&lt;/p&gt; 
&lt;h2&gt;Evaluate Your Own Agent&lt;/h2&gt; 
&lt;p&gt;For local or remote agent evaluation, see our &lt;a href="https://raw.githubusercontent.com/sierra-research/tau2-bench/main/src/tau2/agent/README.md"&gt;agent developer guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions to Ï„Â²-bench! Whether you're fixing bugs, adding new features, creating new domains, or contributing experimental research code, please see our &lt;a href="https://raw.githubusercontent.com/sierra-research/tau2-bench/main/CONTRIBUTING.md"&gt;Contributing Guide&lt;/a&gt; for detailed guidelines on:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Opening issues&lt;/strong&gt; before starting work&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Branch naming conventions&lt;/strong&gt; and development workflow&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Code quality standards&lt;/strong&gt; and testing requirements&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Pull request guidelines&lt;/strong&gt; for clean, reviewable contributions&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Domain and experimental contributions&lt;/strong&gt; specific guidelines&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For experimental features and research code, check out the &lt;a href="https://raw.githubusercontent.com/sierra-research/tau2-bench/main/src/experiments/"&gt;&lt;code&gt;@experiments/&lt;/code&gt;&lt;/a&gt; directory.&lt;/p&gt; 
&lt;h2&gt;Orchestration Sequence Diagram&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-mermaid"&gt;sequenceDiagram
    participant O as Orchestrator
    participant A as Agent
    participant U as UserSimulator
    participant E as Environment

    Note over O: Initialize(task)
    rect rgb(100, 150, 150)
        O-&amp;gt;&amp;gt;A: get_init_state_info(message_history)
        A-&amp;gt;&amp;gt;O: agent_state_info
        O-&amp;gt;&amp;gt;U: get_init_state_info(message_history)
        U-&amp;gt;&amp;gt;O: user_state_info
        O-&amp;gt;&amp;gt;E: set_state(initialization_data, initialization_actions, message_history)
    end
    Note over O: Start simulation
    loop Pass messages between Agent, User, and Environment

        alt Agent/Env to User
            rect rgb(200, 150, 150)
            O-&amp;gt;&amp;gt;U: generate_next_message(msg, user_state_info)
            U--&amp;gt;&amp;gt;O: (user_msg, user_state_info)
            end
            Note over O: Check if user_msg is STOP
        else User/Env to Agent
            rect rgb(100, 200, 100)
            O-&amp;gt;&amp;gt;A: generate_next_message(msg, agent_state_info)
            A--&amp;gt;&amp;gt;O: (assistant_msg, agent_state_info)
            Note over O: Check if too many errors
            end
        else User/Agent to Environment
            rect rgb(150, 150, 200)
            O-&amp;gt;&amp;gt;E: get_response(tool_call)
            E--&amp;gt;&amp;gt;O: tool_message
            end
        end
        Note over O: Check if max turns reached.
    end
    Note over O: Return simulation run
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{barres2025tau2,
      title={$\tau^2$-Bench: Evaluating Conversational Agents in a Dual-Control Environment}, 
      author={Victor Barres and Honghua Dong and Soham Ray and Xujie Si and Karthik Narasimhan},
      year={2025},
      eprint={2506.07982},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2506.07982}, 
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>lvgalvao/data-engineering-roadmap</title>
      <link>https://github.com/lvgalvao/data-engineering-roadmap</link>
      <description>&lt;p&gt;FormaÃ§Ã£o Profissional em Engenharia de Dados e IA&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;FormaÃ§Ã£o Profissional em Engenharia de Dados e InteligÃªncia Artificial&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;(ExtensÃ£o UniversitÃ¡ria)&lt;/strong&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://suajornadadedados.com.br/"&gt;&lt;img src="https://raw.githubusercontent.com/lvgalvao/data-engineering-roadmap/main/pics/logo.png" alt="Jornada de Dados" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;em&gt;Nossa missÃ£o Ã© fornecer o melhor ensino em engenharia de dados&lt;/em&gt; &lt;/p&gt; 
&lt;h2&gt;ğŸ“‹ Sobre&lt;/h2&gt; 
&lt;p&gt;Este Ã© o &lt;strong&gt;repositÃ³rio oficial da FormaÃ§Ã£o Profissional em Engenharia de Dados e InteligÃªncia Artificial (ExtensÃ£o UniversitÃ¡ria)&lt;/strong&gt; da &lt;strong&gt;Jornada de Dados&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Esse Ã© o roadmap para se especializar em engenharia de dados&lt;/strong&gt;, baseado em fundamentos, principais tecnologias de mercado e projetos prÃ¡ticos do mundo real. Este repositÃ³rio contÃ©m todo o conteÃºdo prÃ¡tico, projetos, exercÃ­cios e materiais de apoio utilizados durante a formaÃ§Ã£o.&lt;/p&gt; 
&lt;h3&gt;ğŸ“ Por que "FormaÃ§Ã£o Profissional"?&lt;/h3&gt; 
&lt;p&gt;Este nÃ£o Ã© apenas um curso ou bootcamp. Ã‰ uma &lt;strong&gt;formaÃ§Ã£o completa&lt;/strong&gt; que:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;âœ… &lt;strong&gt;Eleva o nÃ­vel profissional&lt;/strong&gt;: ConteÃºdo estruturado para profissionais que buscam especializaÃ§Ã£o&lt;/li&gt; 
 &lt;li&gt;âœ… &lt;strong&gt;Reconhecimento institucional&lt;/strong&gt;: ExtensÃ£o UniversitÃ¡ria com validade acadÃªmica&lt;/li&gt; 
 &lt;li&gt;âœ… &lt;strong&gt;Foco em mercado&lt;/strong&gt;: Baseado em tecnologias e prÃ¡ticas reais do mercado de trabalho&lt;/li&gt; 
 &lt;li&gt;âœ… &lt;strong&gt;PreparaÃ§Ã£o completa&lt;/strong&gt;: Do zero atÃ© projetos avanÃ§ados de produÃ§Ã£o&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ› ï¸ Engenharia de Dados como Eixo Central&lt;/h3&gt; 
&lt;p&gt;A formaÃ§Ã£o tem como nÃºcleo a &lt;strong&gt;Engenharia de Dados&lt;/strong&gt;, cobrindo:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Pipelines de dados&lt;/strong&gt;: ETL/ELT, processamento em batch e streaming&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Infraestrutura&lt;/strong&gt;: Cloud, containers, orquestraÃ§Ã£o&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Qualidade e observabilidade&lt;/strong&gt;: ValidaÃ§Ã£o, monitoramento, testes&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ProduÃ§Ã£o&lt;/strong&gt;: Deploy, escalabilidade, manutenÃ§Ã£o&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ¤– InteligÃªncia Artificial como Complemento EstratÃ©gico&lt;/h3&gt; 
&lt;p&gt;A &lt;strong&gt;IA&lt;/strong&gt; entra de forma estratÃ©gica e prÃ¡tica:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Agentes de IA&lt;/strong&gt;: RAG, Vector Search, LangChain&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;AplicaÃ§Ãµes reais&lt;/strong&gt;: Chatbots, anÃ¡lise de dados com LLMs&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;IntegraÃ§Ã£o com dados&lt;/strong&gt;: Databricks + IA, pipelines inteligentes&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;PreparaÃ§Ã£o para o futuro&lt;/strong&gt;: ConteÃºdo alinhado com 2026+&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ“ ExtensÃ£o UniversitÃ¡ria&lt;/h3&gt; 
&lt;p&gt;A formaÃ§Ã£o possui reconhecimento como &lt;strong&gt;ExtensÃ£o UniversitÃ¡ria&lt;/strong&gt;, oferecendo:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;âœ… &lt;strong&gt;Horas complementares&lt;/strong&gt;: VÃ¡lidas para graduaÃ§Ã£o&lt;/li&gt; 
 &lt;li&gt;âœ… &lt;strong&gt;DiferenciaÃ§Ã£o no currÃ­culo&lt;/strong&gt;: CertificaÃ§Ã£o com validade acadÃªmica&lt;/li&gt; 
 &lt;li&gt;âœ… &lt;strong&gt;Legitimidade institucional&lt;/strong&gt;: Reconhecimento pelo MEC&lt;/li&gt; 
 &lt;li&gt;âœ… &lt;strong&gt;Valor profissional&lt;/strong&gt;: Diferencial competitivo no mercado&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Estrutura do RepositÃ³rio:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;01-projetos/&lt;/code&gt;&lt;/strong&gt;: Projetos prÃ¡ticos completos que demonstram conceitos avanÃ§ados de engenharia de dados&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;02-fundamentos-dados/&lt;/code&gt;&lt;/strong&gt;: Fundamentos essenciais (Git, GitHub, Deploy, WSL)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;03-python-avancado-para-dados/&lt;/code&gt;&lt;/strong&gt;: ConteÃºdo avanÃ§ado de Python aplicado a dados&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;04-sql-analytics-dbt-core/&lt;/code&gt;&lt;/strong&gt;: SQL avanÃ§ado e Analytics Engineering com dbt&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;04-workflow-orchestration-deploy-airflow/&lt;/code&gt;&lt;/strong&gt;: OrquestraÃ§Ã£o de workflows com Airflow&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;05-engenharia-de-dados-e-ia/&lt;/code&gt;&lt;/strong&gt;: Projetos avanÃ§ados (APIs, Kafka, Streamlit, Terraform)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;06-cloud-aws-para-dados/&lt;/code&gt;&lt;/strong&gt;: ConteÃºdo prÃ¡tico de Cloud AWS para dados&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ¯ Objetivos da FormaÃ§Ã£o&lt;/h2&gt; 
&lt;p&gt;Esta &lt;strong&gt;FormaÃ§Ã£o Profissional&lt;/strong&gt; visa capacitar profissionais para:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Construir pipelines de dados robustos e escalÃ¡veis&lt;/strong&gt; para ambientes de produÃ§Ã£o&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Dominar ferramentas modernas&lt;/strong&gt; de engenharia de dados (Python, SQL, Airflow, dbt, Cloud, Databricks)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Aplicar boas prÃ¡ticas&lt;/strong&gt; de desenvolvimento, arquitetura de dados e engenharia de software&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Implementar soluÃ§Ãµes de dados em produÃ§Ã£o&lt;/strong&gt; com qualidade e observabilidade&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Trabalhar com dados em grande escala&lt;/strong&gt; (Big Data) e processamento distribuÃ­do&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Integrar InteligÃªncia Artificial&lt;/strong&gt; em pipelines e aplicaÃ§Ãµes de dados&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Preparar-se para o mercado&lt;/strong&gt; com habilidades alinhadas Ã s demandas reais das empresas&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸš€ Como Usar Este RepositÃ³rio&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Navegue pelas pastas&lt;/strong&gt; seguindo a ordem sugerida ou conforme seu nÃ­vel de conhecimento&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Cada projeto/mÃ³dulo possui seu prÃ³prio README&lt;/strong&gt; com instruÃ§Ãµes detalhadas&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Clone o repositÃ³rio&lt;/strong&gt; para ter acesso local aos cÃ³digos: &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/lvgalvao/data-engineering-roadmap.git
cd data-engineering-roadmap
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Siga os prÃ©-requisitos&lt;/strong&gt; indicados em cada projeto antes de comeÃ§ar&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;ğŸ“š ConteÃºdo DisponÃ­vel&lt;/h2&gt; 
&lt;h3&gt;Projetos PrÃ¡ticos (&lt;code&gt;01-projetos/&lt;/code&gt;)&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Data Project Foundations&lt;/strong&gt;: EstruturaÃ§Ã£o de projetos de dados com boas prÃ¡ticas&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Python Big Data Processing&lt;/strong&gt;: Processamento de grandes volumes de dados (1 bilhÃ£o de linhas)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;CRUD API Data Application&lt;/strong&gt;: API REST completa com FastAPI, PostgreSQL e Streamlit&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Data Quality Engineering&lt;/strong&gt;: Engenharia de qualidade de dados com DuckDB&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;SQL Advanced Analytics&lt;/strong&gt;: AnÃ¡lises avanÃ§adas com SQL (banco Northwind)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Web Scraping NoSQL Pipelines&lt;/strong&gt;: Web scraping com Redis e MongoDB&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;PDF Data Extraction&lt;/strong&gt;: ExtraÃ§Ã£o de dados de PDFs com S3 e SQS&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Databricks Data Modeling&lt;/strong&gt;: Modelagem de dados no Databricks (Bronze-Silver-Gold)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Databricks AI Project&lt;/strong&gt;: Agentes de IA com LangChain e Vector Search&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Fundamentos (&lt;code&gt;02-fundamentos-dados/&lt;/code&gt;)&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Git e GitHub&lt;/li&gt; 
 &lt;li&gt;Deploy de aplicaÃ§Ãµes de dados&lt;/li&gt; 
 &lt;li&gt;ConfiguraÃ§Ã£o de ambiente WSL&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Python AvanÃ§ado (&lt;code&gt;03-python-avancado-para-dados/&lt;/code&gt;)&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;20 aulas cobrindo desde fundamentos atÃ© APIs e projetos completos&lt;/li&gt; 
 &lt;li&gt;ProgramaÃ§Ã£o Orientada a Objetos&lt;/li&gt; 
 &lt;li&gt;ETL pipelines&lt;/li&gt; 
 &lt;li&gt;Logging e tratamento de erros&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;SQL e Analytics (&lt;code&gt;04-sql-analytics-dbt-core/&lt;/code&gt;)&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;SQL avanÃ§ado para Analytics&lt;/li&gt; 
 &lt;li&gt;dbt-core para transformaÃ§Ã£o de dados&lt;/li&gt; 
 &lt;li&gt;13 aulas prÃ¡ticas + conteÃºdo Databricks&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;OrquestraÃ§Ã£o (&lt;code&gt;04-workflow-orchestration-deploy-airflow/&lt;/code&gt;)&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Airflow do bÃ¡sico ao avanÃ§ado&lt;/li&gt; 
 &lt;li&gt;Deploy de workflows&lt;/li&gt; 
 &lt;li&gt;7 exemplos prÃ¡ticos&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Engenharia de Dados e IA (&lt;code&gt;05-engenharia-de-dados-e-ia/&lt;/code&gt;)&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;REST APIs com FastAPI para aplicaÃ§Ãµes de dados&lt;/li&gt; 
 &lt;li&gt;Kafka e Pub/Sub para streaming de dados em tempo real&lt;/li&gt; 
 &lt;li&gt;Dashboards em tempo real com Streamlit&lt;/li&gt; 
 &lt;li&gt;Infrastructure as Code com Terraform&lt;/li&gt; 
 &lt;li&gt;IntegraÃ§Ã£o de IA em pipelines de dados&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Cloud AWS (&lt;code&gt;06-cloud-aws-para-dados/&lt;/code&gt;)&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;15 aulas prÃ¡ticas sobre AWS para dados + projetos integrados&lt;/li&gt; 
 &lt;li&gt;S3, EC2, RDS, Lambda, VPC, IAM, SQS, SNS, API Gateway, DynamoDB, Amplify e mais&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ”— Links Importantes&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Site Oficial&lt;/strong&gt;: &lt;a href="https://suajornadadedados.com.br/"&gt;Jornada de Dados&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Plataforma de Ensino&lt;/strong&gt;: &lt;a href="https://jornadadedados.alpaclass.com/"&gt;Alpaclass&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Canal YouTube&lt;/strong&gt;: &lt;a href="https://www.youtube.com/@JornadadeDados"&gt;Workshops ao vivo e tutoriais&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Se vocÃª gostou do conteÃºdo e quer se inscrever na &lt;strong&gt;FormaÃ§Ã£o Profissional em Engenharia de Dados e InteligÃªncia Artificial (ExtensÃ£o UniversitÃ¡ria)&lt;/strong&gt;, acesse: &lt;a href="https://suajornadadedados.com.br/"&gt;Jornada de Dados&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt;</description>
    </item>
    
    <item>
      <title>chidiwilliams/buzz</title>
      <link>https://github.com/chidiwilliams/buzz</link>
      <description>&lt;p&gt;Buzz transcribes and translates audio offline on your personal computer. Powered by OpenAI's Whisper.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;[&lt;a href="https://raw.githubusercontent.com/chidiwilliams/buzz/main/readme/README.zh_CN.md"&gt;ç®€ä½“ä¸­æ–‡&lt;/a&gt;] &amp;lt;- ç‚¹å‡»æŸ¥çœ‹ä¸­æ–‡é¡µé¢ã€‚&lt;/p&gt; 
&lt;h1&gt;Buzz&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://chidiwilliams.github.io/buzz/"&gt;Documentation&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Transcribe and translate audio offline on your personal computer. Powered by OpenAI's &lt;a href="https://github.com/openai/whisper"&gt;Whisper&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://img.shields.io/badge/license-MIT-green" alt="MIT License" /&gt; &lt;a href="https://github.com/chidiwilliams/buzz/actions/workflows/ci.yml"&gt;&lt;img src="https://github.com/chidiwilliams/buzz/actions/workflows/ci.yml/badge.svg?sanitize=true" alt="CI" /&gt;&lt;/a&gt; &lt;a href="https://codecov.io/github/chidiwilliams/buzz"&gt;&lt;img src="https://codecov.io/github/chidiwilliams/buzz/branch/main/graph/badge.svg?token=YJSB8S2VEP" alt="codecov" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/github/v/release/chidiwilliams/buzz" alt="GitHub release (latest by date)" /&gt; &lt;a href="https://GitHub.com/chidiwilliams/buzz/releases/"&gt;&lt;img src="https://img.shields.io/github/downloads/chidiwilliams/buzz/total.svg?sanitize=true" alt="Github all releases" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/chidiwilliams/buzz/main/buzz/assets/buzz-banner.jpg" alt="Buzz" /&gt;&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Transcribe audio and video files or Youtube links&lt;/li&gt; 
 &lt;li&gt;Live realtime audio transcription from microphone 
  &lt;ul&gt; 
   &lt;li&gt;Presentation window for easy accessibility during events and presentations&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Speech separation before transcription for better accuracy on noisy audio&lt;/li&gt; 
 &lt;li&gt;Speaker identification in transcribed media&lt;/li&gt; 
 &lt;li&gt;Multiple whisper backend support 
  &lt;ul&gt; 
   &lt;li&gt;CUDA acceleration support for Nvidia GPUs&lt;/li&gt; 
   &lt;li&gt;Apple Silicon support for Macs&lt;/li&gt; 
   &lt;li&gt;Vulkan acceleration support for Whisper.cpp on most GPUs, including integrated GPUs&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Export transcripts to TXT, SRT, and VTT&lt;/li&gt; 
 &lt;li&gt;Advanced Transcription Viewer with search, playback controls, and speed adjustment&lt;/li&gt; 
 &lt;li&gt;Keyboard shortcuts for efficient navigation&lt;/li&gt; 
 &lt;li&gt;Watch folder for automatic transcription of new files&lt;/li&gt; 
 &lt;li&gt;Command-Line Interface for scripting and automation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;macOS&lt;/h3&gt; 
&lt;p&gt;Download the &lt;code&gt;.dmg&lt;/code&gt; from the &lt;a href="https://sourceforge.net/projects/buzz-captions/files/"&gt;SourceForge&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Windows&lt;/h3&gt; 
&lt;p&gt;Get the installation files from the &lt;a href="https://sourceforge.net/projects/buzz-captions/files/"&gt;SourceForge&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;App is not signed, you will get a warning when you install it. Select &lt;code&gt;More info&lt;/code&gt; -&amp;gt; &lt;code&gt;Run anyway&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Alternatively, install with &lt;a href="https://learn.microsoft.com/en-us/windows/package-manager/winget/"&gt;winget&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;winget install ChidiWilliams.Buzz
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Linux&lt;/h3&gt; 
&lt;p&gt;Buzz is available as a &lt;a href="https://flathub.org/apps/io.github.chidiwilliams.Buzz"&gt;Flatpak&lt;/a&gt; or a &lt;a href="https://snapcraft.io/buzz"&gt;Snap&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;To install flatpak, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;flatpak install flathub io.github.chidiwilliams.Buzz
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href="https://flathub.org/en/apps/io.github.chidiwilliams.Buzz"&gt;&lt;img src="https://flathub.org/api/badge?svg&amp;amp;locale=en" alt="Download on Flathub" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;To install snap, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;sudo apt-get install libportaudio2 libcanberra-gtk-module libcanberra-gtk3-module
sudo snap install buzz
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href="https://snapcraft.io/buzz"&gt;&lt;img src="https://snapcraft.io/static/images/badges/en/snap-store-black.svg?sanitize=true" alt="Get it from the Snap Store" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;PyPI&lt;/h3&gt; 
&lt;p&gt;Install &lt;a href="https://www.ffmpeg.org/download.html"&gt;ffmpeg&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Ensure you use Python 3.12 environment.&lt;/p&gt; 
&lt;p&gt;Install Buzz&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip install buzz-captions
python -m buzz
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;GPU support for PyPI&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;To have GPU support for Nvidia GPUS on Windows, for PyPI installed version ensure, CUDA support for &lt;a href="https://pytorch.org/get-started/locally/"&gt;torch&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip3 install -U torch==2.8.0+cu129 torchaudio==2.8.0+cu129 --index-url https://download.pytorch.org/whl/cu129
pip3 install nvidia-cublas-cu12==12.9.1.4 nvidia-cuda-cupti-cu12==12.9.79 nvidia-cuda-runtime-cu12==12.9.79 --extra-index-url https://pypi.ngc.nvidia.com
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Latest development version&lt;/h3&gt; 
&lt;p&gt;For info on how to get latest development version with latest features and bug fixes see &lt;a href="https://chidiwilliams.github.io/buzz/docs/faq#9-where-can-i-get-latest-development-version"&gt;FAQ&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Screenshots&lt;/h3&gt; 
&lt;div style="display: flex; flex-wrap: wrap;"&gt; 
 &lt;img alt="File import" src="https://raw.githubusercontent.com/chidiwilliams/buzz/main/share/screenshots/buzz-1-import.png" style="max-width: 18%; margin-right: 1%;" /&gt; 
 &lt;img alt="Main screen" src="https://raw.githubusercontent.com/chidiwilliams/buzz/main/share/screenshots/buzz-2-main_screen.png" style="max-width: 18%; margin-right: 1%; height:auto;" /&gt; 
 &lt;img alt="Preferences" src="https://raw.githubusercontent.com/chidiwilliams/buzz/main/share/screenshots/buzz-3-preferences.png" style="max-width: 18%; margin-right: 1%; height:auto;" /&gt; 
 &lt;img alt="Model preferences" src="https://raw.githubusercontent.com/chidiwilliams/buzz/main/share/screenshots/buzz-3.2-model-preferences.png" style="max-width: 18%; margin-right: 1%; height:auto;" /&gt; 
 &lt;img alt="Transcript" src="https://raw.githubusercontent.com/chidiwilliams/buzz/main/share/screenshots/buzz-4-transcript.png" style="max-width: 18%; margin-right: 1%; height:auto;" /&gt; 
 &lt;img alt="Live recording" src="https://raw.githubusercontent.com/chidiwilliams/buzz/main/share/screenshots/buzz-5-live_recording.png" style="max-width: 18%; margin-right: 1%; height:auto;" /&gt; 
 &lt;img alt="Resize" src="https://raw.githubusercontent.com/chidiwilliams/buzz/main/share/screenshots/buzz-6-resize.png" style="max-width: 18%;" /&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>OpenPipe/ART</title>
      <link>https://github.com/OpenPipe/ART</link>
      <description>&lt;p&gt;Agent Reinforcement Trainer: train multi-step agents for real-world tasks using GRPO. Give your agents on-the-job training. Reinforcement learning for Qwen2.5, Qwen3, Llama, and more!&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://art.openpipe.ai"&gt;
   &lt;picture&gt; 
    &lt;img alt="ART logo" src="https://github.com/openpipe/art/raw/main/assets/ART_logo.png" width="160px" /&gt; 
   &lt;/picture&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;/p&gt;
 &lt;h1&gt;Agent Reinforcement Trainer&lt;/h1&gt; 
 &lt;p&gt;&lt;/p&gt; 
 &lt;p&gt; Train multi-step agents for real-world tasks using GRPO. &lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/openpipe/art/raw/main/CONTRIBUTING.md"&gt;&lt;img src="https://img.shields.io/badge/PRs-welcome-blue.svg?sanitize=true" alt="PRs-Welcome" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/openpipe-art/"&gt;&lt;img src="https://img.shields.io/pypi/v/openpipe-art?color=364fc7" alt="PyPI version" /&gt;&lt;/a&gt; &lt;a href="https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/2048/2048.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Train Agent" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://discord.gg/EceeVdhpxD"&gt;&lt;img src="https://img.shields.io/badge/Join%20Discord-5865F2?style=plastic&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Join Discord" /&gt;&lt;/a&gt; &lt;a href="https://art.openpipe.ai"&gt;&lt;img src="https://img.shields.io/badge/Documentation-orange?style=plastic&amp;amp;logo=gitbook&amp;amp;logoColor=white" alt="Documentation" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸš€ W&amp;amp;B Training: Serverless RL&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;W&amp;amp;B Training (Serverless RL)&lt;/strong&gt; is the first publicly available service for flexibly training models with reinforcement learning. It manages your training and inference infrastructure automatically, letting you focus on defining your data, environment and reward functionâ€”leading to faster feedback cycles, lower costs, and far less DevOps.&lt;/p&gt; 
&lt;p&gt;âœ¨ &lt;strong&gt;Key Benefits:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;40% lower cost&lt;/strong&gt; - Multiplexing on shared production-grade inference cluster&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;28% faster training&lt;/strong&gt; - Scale to 2000+ concurrent requests across many GPUs&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Zero infra headaches&lt;/strong&gt; - Fully managed infrastructure that stays healthy&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Instant deployment&lt;/strong&gt; - Every checkpoint instantly available via W&amp;amp;B Inference&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Before: Hours of GPU setup and infra management
# RuntimeError: CUDA error: out of memory ğŸ˜¢

# After: Serverless RL with instant feedback
from art.serverless.backend import ServerlessBackend

model = art.TrainableModel(
  project="voice-agent",
  name="agent-001",
  base_model="OpenPipe/Qwen3-14B-Instruct"
)

backend = ServerlessBackend(
    api_key="your_wandb_api_key"
)
model.register(backend)
# Edit and iterate in minutes, not hours!
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href="https://docs.wandb.ai/guides/training"&gt;ğŸ“– Learn more about W&amp;amp;B Training â†’&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;ART Overview&lt;/h2&gt; 
&lt;p&gt;ART is an open-source RL framework that improves agent reliability by allowing LLMs to &lt;strong&gt;learn from experience&lt;/strong&gt;. ART provides an ergonomic harness for integrating GRPO into any python application. For a quick hands-on introduction, run one of the notebooks below. When you're ready to learn more, check out the &lt;a href="https://art.openpipe.ai"&gt;docs&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;ğŸ“’ Notebooks&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Agent Task&lt;/th&gt; 
   &lt;th&gt;Example Notebook&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Comparative Performance&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ARTâ€¢E [Serverless]&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/art-e.ipynb"&gt;ğŸ‹ï¸ Train agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Qwen3 14B learns to search emails using RULER&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/openpipe/art/raw/main/assets/benchmarks/email_agent/accuracy-training-progress.svg?sanitize=true" height="72" /&gt; &lt;a href="https://raw.githubusercontent.com/OpenPipe/ART/main/dev/art-e/art_e/evaluate/display_benchmarks.ipynb"&gt;benchmarks&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;2048 [Serverless]&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/2048/2048.ipynb"&gt;ğŸ‹ï¸ Train agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Qwen3 14B learns to play 2048&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/openpipe/art/raw/main/assets/benchmarks/2048/accuracy-training-progress.svg?sanitize=true" height="72" /&gt; &lt;a href="https://raw.githubusercontent.com/OpenPipe/ART/main/examples/2048/display_benchmarks.ipynb"&gt;benchmarks&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ARTâ€¢E LangGraph&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/langgraph/art-e-langgraph.ipynb"&gt;ğŸ‹ï¸ Train agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Qwen 2.5 7B learns to search emails using LangGraph&lt;/td&gt; 
   &lt;td&gt;[Link coming soon]&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;MCPâ€¢RL&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/mcp-rl/mcp-rl.ipynb"&gt;ğŸ‹ï¸ Train agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Qwen 2.5 3B masters the NWS MCP server&lt;/td&gt; 
   &lt;td&gt;[Link coming soon]&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Temporal Clue&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/temporal_clue/temporal-clue.ipynb"&gt;ğŸ‹ï¸ Train agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Qwen 2.5 7B learns to solve Temporal Clue&lt;/td&gt; 
   &lt;td&gt;[Link coming soon]&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Tic Tac Toe&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/tic_tac_toe/tic-tac-toe.ipynb"&gt;ğŸ‹ï¸ Train agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Qwen 2.5 3B learns to play Tic Tac Toe&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/openpipe/art/raw/main/assets/benchmarks/tic-tac-toe-local/accuracy-training-progress.svg?sanitize=true" height="72" /&gt; &lt;a href="https://raw.githubusercontent.com/OpenPipe/ART/main/examples/tic_tac_toe/display-benchmarks.ipynb"&gt;benchmarks&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Codenames&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/codenames/Codenames_RL.ipynb"&gt;ğŸ‹ï¸ Train agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Qwen 2.5 3B learns to play Codenames&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/openpipe/art/raw/main/assets/benchmarks/codenames/win_rate_over_time.png" height="72" /&gt; &lt;a href="https://github.com/OpenPipe/art-notebooks/raw/main/examples/codenames/Codenames_RL.ipynb"&gt;benchmarks&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;AutoRL [RULER]&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/openpipe/art-notebooks/blob/main/examples/auto_rl.ipynb"&gt;ğŸ‹ï¸ Train agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Train Qwen 2.5 7B to master any task&lt;/td&gt; 
   &lt;td&gt;[Link coming soon]&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;ğŸ“° ART News&lt;/h2&gt; 
&lt;p&gt;Explore our latest research and updates on building SOTA agents.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ—ï¸ &lt;strong&gt;&lt;a href="https://art.openpipe.ai/integrations/langgraph-integration"&gt;ART now integrates seamlessly with LangGraph&lt;/a&gt;&lt;/strong&gt; - Train your LangGraph agents with reinforcement learning for smarter multi-step reasoning and improved tool usage.&lt;/li&gt; 
 &lt;li&gt;ğŸ—ï¸ &lt;strong&gt;&lt;a href="https://x.com/corbtt/status/1953171838382817625"&gt;MCPâ€¢RL: Teach Your Model to Master Any MCP Server&lt;/a&gt;&lt;/strong&gt; - Automatically train models to effectively use MCP server tools through reinforcement learning.&lt;/li&gt; 
 &lt;li&gt;ğŸ—ï¸ &lt;strong&gt;&lt;a href="https://x.com/mattshumer_/status/1950572449025650733"&gt;AutoRL: Zero-Data Training for Any Task&lt;/a&gt;&lt;/strong&gt; - Train custom AI models without labeled data using automatic input generation and RULER evaluation.&lt;/li&gt; 
 &lt;li&gt;ğŸ—ï¸ &lt;strong&gt;&lt;a href="https://openpipe.ai/blog/ruler-easy-mode-for-rl-rewards"&gt;RULER: Easy Mode for RL Rewards&lt;/a&gt;&lt;/strong&gt; is now available for automatic reward generation in reinforcement learning.&lt;/li&gt; 
 &lt;li&gt;ğŸ—ï¸ &lt;strong&gt;&lt;a href="https://openpipe.ai/blog/art-e-mail-agent"&gt;ARTÂ·E: How We Built an Email Research Agent That Beats o3&lt;/a&gt;&lt;/strong&gt; demonstrates a Qwen 2.5 14B email agent outperforming OpenAI's o3.&lt;/li&gt; 
 &lt;li&gt;ğŸ—ï¸ &lt;strong&gt;&lt;a href="https://openpipe.ai/blog/art-trainer"&gt;ART Trainer: A New RL Trainer for Agents&lt;/a&gt;&lt;/strong&gt; enables easy training of LLM-based agents using GRPO.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://openpipe.ai/blog"&gt;ğŸ“– See all blog posts â†’&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Why ART?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ART provides convenient wrappers for introducing RL training into &lt;strong&gt;existing applications&lt;/strong&gt;. We abstract the training server into a modular service that your code doesn't need to interface with.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Train from anywhere.&lt;/strong&gt; Run the ART client on your laptop and let the ART server kick off an ephemeral GPU-enabled environment, or run on a local GPU.&lt;/li&gt; 
 &lt;li&gt;Integrations with hosted platforms like W&amp;amp;B, Langfuse, and OpenPipe provide flexible observability and &lt;strong&gt;simplify debugging&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;ART is customizable with &lt;strong&gt;intelligent defaults&lt;/strong&gt;. You can configure training parameters and inference engine configurations to meet specific needs, or take advantage of the defaults, which have been optimized for training efficiency and stability.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;ART agents can be trained from any client machine that runs python. To add to an existing project, run this command:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install openpipe-art
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ğŸ¤– ARTâ€¢E Agent&lt;/h2&gt; 
&lt;p&gt;Curious about how to use ART for a real-world task? Check out the &lt;a href="https://openpipe.ai/blog/art-e-mail-agent"&gt;ARTâ€¢E Agent&lt;/a&gt; blog post, where we detail how we trained Qwen 2.5 14B to beat o3 at email retrieval!&lt;/p&gt; 
&lt;img src="https://github.com/openpipe/art/raw/main/assets/ART_E_graphs.png" width="700" /&gt; 
&lt;h2&gt;ğŸ” Training Loop Overview&lt;/h2&gt; 
&lt;p&gt;ART's functionality is divided into a &lt;strong&gt;client&lt;/strong&gt; and a &lt;strong&gt;server&lt;/strong&gt;. The OpenAI-compatible client is responsible for interfacing between ART and your codebase. Using the client, you can pass messages and get completions from your LLM as it improves. The server runs independently on any machine with a GPU. It abstracts away the complexity of the inference and training portions of the RL loop while allowing for some custom configuration. An outline of the training loop is shown below:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Inference&lt;/strong&gt;&lt;/p&gt; 
  &lt;ol&gt; 
   &lt;li&gt;Your code uses the ART client to perform an agentic workflow (usually executing several rollouts in parallel to gather data faster).&lt;/li&gt; 
   &lt;li&gt;Completion requests are routed to the ART server, which runs the model's latest LoRA in vLLM.&lt;/li&gt; 
   &lt;li&gt;As the agent executes, each &lt;code&gt;system&lt;/code&gt;, &lt;code&gt;user&lt;/code&gt;, and &lt;code&gt;assistant&lt;/code&gt; message is stored in a Trajectory.&lt;/li&gt; 
   &lt;li&gt;When a rollout finishes, your code assigns a &lt;code&gt;reward&lt;/code&gt; to its Trajectory, indicating the performance of the LLM.&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Training&lt;/strong&gt;&lt;/p&gt; 
  &lt;ol&gt; 
   &lt;li&gt;When each rollout has finished, Trajectories are grouped and sent to the server. Inference is blocked while training executes.&lt;/li&gt; 
   &lt;li&gt;The server trains your model using GRPO, initializing from the latest checkpoint (or an empty LoRA on the first iteration).&lt;/li&gt; 
   &lt;li&gt;The server saves the newly trained LoRA to a local directory and loads it into vLLM.&lt;/li&gt; 
   &lt;li&gt;Inference is unblocked and the loop resumes at step 1.&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;This training loop runs until a specified number of inference and training iterations have completed.&lt;/p&gt; 
&lt;h2&gt;ğŸ§© Supported Models&lt;/h2&gt; 
&lt;p&gt;ART should work with most vLLM/HuggingFace-transformers compatible causal language models, or at least the ones supported by &lt;a href="https://docs.unsloth.ai/get-started/all-our-models"&gt;Unsloth&lt;/a&gt;. Gemma 3 does not appear to be supported for the time being. If any other model isn't working for you, please let us know on &lt;a href="https://discord.gg/zbBHRUpwf4"&gt;Discord&lt;/a&gt; or open an issue on &lt;a href="https://github.com/openpipe/art/issues"&gt;GitHub&lt;/a&gt;!&lt;/p&gt; 
&lt;h2&gt;ğŸ¤ Contributing&lt;/h2&gt; 
&lt;p&gt;ART is in active development, and contributions are most welcome! Please see the &lt;a href="https://raw.githubusercontent.com/OpenPipe/ART/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; file for more information.&lt;/p&gt; 
&lt;h2&gt;ğŸ“– Citation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{hilton2025art,
  author = {Brad Hilton and Kyle Corbitt and David Corbitt and Saumya Gandhi and Angky William and Bohdan Kovalenskyi and Andie Jones},
  title = {ART: Agent Reinforcement Trainer},
  year = {2025},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/openpipe/art}}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;âš–ï¸ License&lt;/h2&gt; 
&lt;p&gt;This repository's source code is available under the &lt;a href="https://raw.githubusercontent.com/OpenPipe/ART/main/LICENSE"&gt;Apache-2.0 License&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;ğŸ™ Credits&lt;/h2&gt; 
&lt;p&gt;ART stands on the shoulders of giants. While we owe many of the ideas and early experiments that led to ART's development to the open source RL community at large, we're especially grateful to the authors of the following projects:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/unslothai/unsloth"&gt;Unsloth&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/vllm-project/vllm"&gt;vLLM&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/huggingface/trl"&gt;trl&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pytorch/torchtune"&gt;torchtune&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/skypilot-org/skypilot"&gt;SkyPilot&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Finally, thank you to our partners who've helped us test ART in the wild! We're excited to see what you all build with it.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>