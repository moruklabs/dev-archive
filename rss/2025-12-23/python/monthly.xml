<rss version="2.0">
  <channel>
    <title>GitHub Python Monthly Trending</title>
    <description>Monthly Trending of Python in GitHub</description>
    <pubDate>Mon, 22 Dec 2025 01:55:21 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>MetaCubeX/mihomo</title>
      <link>https://github.com/MetaCubeX/mihomo</link>
      <description>&lt;p&gt;A simple Python Pydantic model for Honkai: Star Rail parsed data from the Mihomo API.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;mihomo&lt;/h1&gt; 
&lt;p&gt;A simple python pydantic model (type hint and autocompletion support) for Honkai: Star Rail parsed data from the Mihomo API.&lt;/p&gt; 
&lt;p&gt;API url: &lt;a href="https://api.mihomo.me/sr_info_parsed/%7BUID%7D?lang=%7BLANG%7D"&gt;https://api.mihomo.me/sr_info_parsed/{UID}?lang={LANG}&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;pip install -U git+https://github.com/KT-Yeh/mihomo.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;h3&gt;Basic&lt;/h3&gt; 
&lt;p&gt;There are two parsed data formats:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;V1: 
  &lt;ul&gt; 
   &lt;li&gt;URL: &lt;a href="https://api.mihomo.me/sr_info_parsed/800333171?lang=en&amp;amp;version=v1"&gt;https://api.mihomo.me/sr_info_parsed/800333171?lang=en&amp;amp;version=v1&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Fetching: use &lt;code&gt;client.fetch_user_v1(800333171)&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Data model: &lt;code&gt;mihomo.models.v1.StarrailInfoParsedV1&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;All models defined in &lt;code&gt;mihomo/models/v1&lt;/code&gt; directory.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;V2: 
  &lt;ul&gt; 
   &lt;li&gt;URL: &lt;a href="https://api.mihomo.me/sr_info_parsed/800333171?lang=en"&gt;https://api.mihomo.me/sr_info_parsed/800333171?lang=en&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Fetching: use &lt;code&gt;client.fetch_user(800333171)&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Data model: &lt;code&gt;mihomo.models.StarrailInfoParsed&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;All models defined in &lt;code&gt;mihomo/models&lt;/code&gt; directory.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If you don't want to use &lt;code&gt;client.get_icon_url&lt;/code&gt; to get the image url everytime, you can use &lt;code&gt;client.fetch_user(800333171, replace_icon_name_with_url=True)&lt;/code&gt; to get the parsed data with asset urls.&lt;/p&gt; 
&lt;h3&gt;Example&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;import asyncio

from mihomo import Language, MihomoAPI
from mihomo.models import StarrailInfoParsed
from mihomo.models.v1 import StarrailInfoParsedV1

client = MihomoAPI(language=Language.EN)


async def v1():
    data: StarrailInfoParsedV1 = await client.fetch_user_v1(800333171)

    print(f"Name: {data.player.name}")
    print(f"Level: {data.player.level}")
    print(f"Signature: {data.player.signature}")
    print(f"Achievements: {data.player_details.achievements}")
    print(f"Characters count: {data.player_details.characters}")
    print(f"Profile picture url: {client.get_icon_url(data.player.icon)}")
    for character in data.characters:
        print("-----------")
        print(f"Name: {character.name}")
        print(f"Rarity: {character.rarity}")
        print(f"Level: {character.level}")
        print(f"Avatar url: {client.get_icon_url(character.icon)}")
        print(f"Preview url: {client.get_icon_url(character.preview)}")
        print(f"Portrait url: {client.get_icon_url(character.portrait)}")


async def v2():
    data: StarrailInfoParsed = await client.fetch_user(800333171, replace_icon_name_with_url=True)

    print(f"Name: {data.player.name}")
    print(f"Level: {data.player.level}")
    print(f"Signature: {data.player.signature}")
    print(f"Profile picture url: {data.player.avatar.icon}")
    for character in data.characters:
        print("-----------")
        print(f"Name: {character.name}")
        print(f"Rarity: {character.rarity}")
        print(f"Portrait url: {character.portrait}")

asyncio.run(v1())
asyncio.run(v2())
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Tools&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;from mihomo import tools&lt;/code&gt;&lt;/p&gt; 
&lt;h4&gt;Remove Duplicate Character&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;    data = await client.fetch_user(800333171)
    data = tools.remove_duplicate_character(data)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Merge Character Data&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;    old_data = await client.fetch_user(800333171)

    # Change characters in game and wait for the API to refresh
    # ...

    new_data = await client.fetch_user(800333171)
    data = tools.merge_character_data(new_data, old_data)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Data Persistence&lt;/h3&gt; 
&lt;p&gt;Take pickle and json as an example&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;import pickle
import zlib
from mihomo import MihomoAPI, Language, StarrailInfoParsed

client = MihomoAPI(language=Language.EN)
data = await client.fetch_user(800333171)

# Save
pickle_data = zlib.compress(pickle.dumps(data))
print(len(pickle_data))
json_data = data.json(by_alias=True, ensure_ascii=False)
print(len(json_data))

# Load
data_from_pickle = pickle.loads(zlib.decompress(pickle_data))
data_from_json = StarrailInfoParsed.parse_raw(json_data)
print(type(data_from_pickle))
print(type(data_from_json))
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>comfyanonymous/ComfyUI</title>
      <link>https://github.com/comfyanonymous/ComfyUI</link>
      <description>&lt;p&gt;The most powerful and modular diffusion model GUI, api and backend with a graph/nodes interface.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h1&gt;ComfyUI&lt;/h1&gt; 
 &lt;p&gt;&lt;strong&gt;The most powerful and modular visual AI engine and application.&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://www.comfy.org/"&gt;&lt;img src="https://img.shields.io/badge/ComfyOrg-4285F4?style=flat" alt="Website" /&gt;&lt;/a&gt; &lt;a href="https://www.comfy.org/discord"&gt;&lt;img src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fdiscord.com%2Fapi%2Finvites%2Fcomfyorg%3Fwith_counts%3Dtrue&amp;amp;query=%24.approximate_member_count&amp;amp;logo=discord&amp;amp;logoColor=white&amp;amp;label=Discord&amp;amp;color=green&amp;amp;suffix=%20total" alt="Dynamic JSON Badge" /&gt;&lt;/a&gt; &lt;a href="https://x.com/ComfyUI"&gt;&lt;img src="https://img.shields.io/twitter/follow/ComfyUI" alt="Twitter" /&gt;&lt;/a&gt; &lt;a href="https://app.element.io/#/room/%23comfyui_space%3Amatrix.org"&gt;&lt;img src="https://img.shields.io/badge/Matrix-000000?style=flat&amp;amp;logo=matrix&amp;amp;logoColor=white" alt="Matrix" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;a href="https://github.com/comfyanonymous/ComfyUI/releases"&gt;&lt;img src="https://img.shields.io/github/v/release/comfyanonymous/ComfyUI?style=flat&amp;amp;sort=semver" alt="" /&gt;&lt;/a&gt; &lt;a href="https://github.com/comfyanonymous/ComfyUI/releases"&gt;&lt;img src="https://img.shields.io/github/release-date/comfyanonymous/ComfyUI?style=flat" alt="" /&gt;&lt;/a&gt; &lt;a href="https://github.com/comfyanonymous/ComfyUI/releases"&gt;&lt;img src="https://img.shields.io/github/downloads/comfyanonymous/ComfyUI/total?style=flat" alt="" /&gt;&lt;/a&gt; &lt;a href="https://github.com/comfyanonymous/ComfyUI/releases"&gt;&lt;img src="https://img.shields.io/github/downloads/comfyanonymous/ComfyUI/latest/total?style=flat&amp;amp;label=downloads%40latest" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;!-- Workaround to display total user from https://github.com/badges/shields/issues/4500#issuecomment-2060079995 --&gt; 
 &lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/7ccaf2c1-9b72-41ae-9a89-5688c94b7abe" alt="ComfyUI Screenshot" /&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;ComfyUI lets you design and execute advanced stable diffusion pipelines using a graph/nodes/flowchart based interface. Available on Windows, Linux, and macOS.&lt;/p&gt; 
&lt;h2&gt;Get Started&lt;/h2&gt; 
&lt;h4&gt;&lt;a href="https://www.comfy.org/download"&gt;Desktop Application&lt;/a&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;The easiest way to get started.&lt;/li&gt; 
 &lt;li&gt;Available on Windows &amp;amp; macOS.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;&lt;a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/#installing"&gt;Windows Portable Package&lt;/a&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Get the latest commits and completely portable.&lt;/li&gt; 
 &lt;li&gt;Available on Windows.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;&lt;a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/#manual-install-windows-linux"&gt;Manual Install&lt;/a&gt;&lt;/h4&gt; 
&lt;p&gt;Supports all operating systems and GPU types (NVIDIA, AMD, Intel, Apple Silicon, Ascend).&lt;/p&gt; 
&lt;h2&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/"&gt;Examples&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;See what ComfyUI can do with the &lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/"&gt;example workflows&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Nodes/graph/flowchart interface to experiment and create complex Stable Diffusion workflows without needing to code anything.&lt;/li&gt; 
 &lt;li&gt;Image Models 
  &lt;ul&gt; 
   &lt;li&gt;SD1.x, SD2.x (&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/unclip/"&gt;unCLIP&lt;/a&gt;)&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/sdxl/"&gt;SDXL&lt;/a&gt;, &lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/sdturbo/"&gt;SDXL Turbo&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/stable_cascade/"&gt;Stable Cascade&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/sd3/"&gt;SD3 and SD3.5&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Pixart Alpha and Sigma&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/aura_flow/"&gt;AuraFlow&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/hunyuan_dit/"&gt;HunyuanDiT&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/flux/"&gt;Flux&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/lumina2/"&gt;Lumina Image 2.0&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/hidream/"&gt;HiDream&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/qwen_image/"&gt;Qwen Image&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/hunyuan_image/"&gt;Hunyuan Image 2.1&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/flux2/"&gt;Flux 2&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/z_image/"&gt;Z Image&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Image Editing Models 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/omnigen/"&gt;Omnigen 2&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/flux/#flux-kontext-image-editing-model"&gt;Flux Kontext&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/hidream/#hidream-e11"&gt;HiDream E1.1&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/qwen_image/#edit-model"&gt;Qwen Image Edit&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Video Models 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/video/"&gt;Stable Video Diffusion&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/mochi/"&gt;Mochi&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/ltxv/"&gt;LTX-Video&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/hunyuan_video/"&gt;Hunyuan Video&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/wan/"&gt;Wan 2.1&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/wan22/"&gt;Wan 2.2&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://docs.comfy.org/tutorials/video/hunyuan/hunyuan-video-1-5"&gt;Hunyuan Video 1.5&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Audio Models 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/audio/"&gt;Stable Audio&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/audio/"&gt;ACE Step&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;3D Models 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://docs.comfy.org/tutorials/3d/hunyuan3D-2"&gt;Hunyuan3D 2.0&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Asynchronous Queue system&lt;/li&gt; 
 &lt;li&gt;Many optimizations: Only re-executes the parts of the workflow that changes between executions.&lt;/li&gt; 
 &lt;li&gt;Smart memory management: can automatically run large models on GPUs with as low as 1GB vram with smart offloading.&lt;/li&gt; 
 &lt;li&gt;Works even if you don't have a GPU with: &lt;code&gt;--cpu&lt;/code&gt; (slow)&lt;/li&gt; 
 &lt;li&gt;Can load ckpt and safetensors: All in one checkpoints or standalone diffusion models, VAEs and CLIP models.&lt;/li&gt; 
 &lt;li&gt;Safe loading of ckpt, pt, pth, etc.. files.&lt;/li&gt; 
 &lt;li&gt;Embeddings/Textual inversion&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/lora/"&gt;Loras (regular, locon and loha)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/hypernetworks/"&gt;Hypernetworks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Loading full workflows (with seeds) from generated PNG, WebP and FLAC files.&lt;/li&gt; 
 &lt;li&gt;Saving/Loading workflows as Json files.&lt;/li&gt; 
 &lt;li&gt;Nodes interface can be used to create complex workflows like one for &lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/2_pass_txt2img/"&gt;Hires fix&lt;/a&gt; or much more advanced ones.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/area_composition/"&gt;Area Composition&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/inpaint/"&gt;Inpainting&lt;/a&gt; with both regular and inpainting models.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/controlnet/"&gt;ControlNet and T2I-Adapter&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/upscale_models/"&gt;Upscale Models (ESRGAN, ESRGAN variants, SwinIR, Swin2SR, etc...)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/gligen/"&gt;GLIGEN&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/model_merging/"&gt;Model Merging&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/lcm/"&gt;LCM models and Loras&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Latent previews with &lt;a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/#how-to-show-high-quality-previews"&gt;TAESD&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Works fully offline: core will never download anything unless you want to.&lt;/li&gt; 
 &lt;li&gt;Optional API nodes to use paid models from external providers through the online &lt;a href="https://docs.comfy.org/tutorials/api-nodes/overview"&gt;Comfy API&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/extra_model_paths.yaml.example"&gt;Config file&lt;/a&gt; to set the search paths for models.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Workflow examples can be found on the &lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/"&gt;Examples page&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Release Process&lt;/h2&gt; 
&lt;p&gt;ComfyUI follows a weekly release cycle targeting Monday but this regularly changes because of model releases or large changes to the codebase. There are three interconnected repositories:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/comfyanonymous/ComfyUI"&gt;ComfyUI Core&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Releases a new stable version (e.g., v0.7.0) roughly every week.&lt;/li&gt; 
   &lt;li&gt;Starting from v0.4.0 patch versions will be used for fixes backported onto the current stable release.&lt;/li&gt; 
   &lt;li&gt;Minor versions will be used for releases off the master branch.&lt;/li&gt; 
   &lt;li&gt;Patch versions may still be used for releases on the master branch in cases where a backport would not make sense.&lt;/li&gt; 
   &lt;li&gt;Commits outside of the stable release tags may be very unstable and break many custom nodes.&lt;/li&gt; 
   &lt;li&gt;Serves as the foundation for the desktop release&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/Comfy-Org/desktop"&gt;ComfyUI Desktop&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Builds a new release using the latest stable core version&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/Comfy-Org/ComfyUI_frontend"&gt;ComfyUI Frontend&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Weekly frontend updates are merged into the core repository&lt;/li&gt; 
   &lt;li&gt;Features are frozen for the upcoming core release&lt;/li&gt; 
   &lt;li&gt;Development continues for the next release cycle&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Shortcuts&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Keybind&lt;/th&gt; 
   &lt;th&gt;Explanation&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;Enter&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Queue up current graph for generation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;Shift&lt;/code&gt; + &lt;code&gt;Enter&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Queue up current graph as first for generation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;Alt&lt;/code&gt; + &lt;code&gt;Enter&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Cancel current generation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;Z&lt;/code&gt;/&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;Y&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Undo/Redo&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;S&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Save workflow&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;O&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Load workflow&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;A&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Select all nodes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Alt &lt;/code&gt;+ &lt;code&gt;C&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Collapse/uncollapse selected nodes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;M&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Mute/unmute selected nodes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;B&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Bypass selected nodes (acts like the node was removed from the graph and the wires reconnected through)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Delete&lt;/code&gt;/&lt;code&gt;Backspace&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Delete selected nodes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;Backspace&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Delete the current graph&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Space&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Move the canvas around when held and moving the cursor&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt;/&lt;code&gt;Shift&lt;/code&gt; + &lt;code&gt;Click&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Add clicked node to selection&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;C&lt;/code&gt;/&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;V&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Copy and paste selected nodes (without maintaining connections to outputs of unselected nodes)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;C&lt;/code&gt;/&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;Shift&lt;/code&gt; + &lt;code&gt;V&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Copy and paste selected nodes (maintaining connections from outputs of unselected nodes to inputs of pasted nodes)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Shift&lt;/code&gt; + &lt;code&gt;Drag&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Move multiple selected nodes at the same time&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;D&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Load default graph&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Alt&lt;/code&gt; + &lt;code&gt;+&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Canvas Zoom in&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Alt&lt;/code&gt; + &lt;code&gt;-&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Canvas Zoom out&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;Shift&lt;/code&gt; + LMB + Vertical drag&lt;/td&gt; 
   &lt;td&gt;Canvas Zoom in/out&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;P&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Pin/Unpin selected nodes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;G&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Group selected nodes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Q&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Toggle visibility of the queue&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;H&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Toggle visibility of history&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;R&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Refresh graph&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;F&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Show/Hide menu&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;.&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Fit view to selection (Whole graph when nothing is selected)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Double-Click LMB&lt;/td&gt; 
   &lt;td&gt;Open node quick search palette&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Shift&lt;/code&gt; + Drag&lt;/td&gt; 
   &lt;td&gt;Move multiple wires at once&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;Alt&lt;/code&gt; + LMB&lt;/td&gt; 
   &lt;td&gt;Disconnect all wires from clicked slot&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;code&gt;Ctrl&lt;/code&gt; can also be replaced with &lt;code&gt;Cmd&lt;/code&gt; instead for macOS users&lt;/p&gt; 
&lt;h1&gt;Installing&lt;/h1&gt; 
&lt;h2&gt;Windows Portable&lt;/h2&gt; 
&lt;p&gt;There is a portable standalone build for Windows that should work for running on Nvidia GPUs or for running on your CPU only on the &lt;a href="https://github.com/comfyanonymous/ComfyUI/releases"&gt;releases page&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;&lt;a href="https://github.com/comfyanonymous/ComfyUI/releases/latest/download/ComfyUI_windows_portable_nvidia.7z"&gt;Direct link to download&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Simply download, extract with &lt;a href="https://7-zip.org"&gt;7-Zip&lt;/a&gt; or with the windows explorer on recent windows versions and run. For smaller models you normally only need to put the checkpoints (the huge ckpt/safetensors files) in: ComfyUI\models\checkpoints but many of the larger models have multiple files. Make sure to follow the instructions to know which subfolder to put them in ComfyUI\models\&lt;/p&gt; 
&lt;p&gt;If you have trouble extracting it, right click the file -&amp;gt; properties -&amp;gt; unblock&lt;/p&gt; 
&lt;p&gt;Update your Nvidia drivers if it doesn't start.&lt;/p&gt; 
&lt;h4&gt;Alternative Downloads:&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://github.com/comfyanonymous/ComfyUI/releases/latest/download/ComfyUI_windows_portable_amd.7z"&gt;Experimental portable for AMD GPUs&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/comfyanonymous/ComfyUI/releases/latest/download/ComfyUI_windows_portable_nvidia_cu128.7z"&gt;Portable with pytorch cuda 12.8 and python 3.12&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/comfyanonymous/ComfyUI/releases/latest/download/ComfyUI_windows_portable_nvidia_cu126.7z"&gt;Portable with pytorch cuda 12.6 and python 3.12&lt;/a&gt; (Supports Nvidia 10 series and older GPUs).&lt;/p&gt; 
&lt;h4&gt;How do I share models between another UI and ComfyUI?&lt;/h4&gt; 
&lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/extra_model_paths.yaml.example"&gt;Config file&lt;/a&gt; to set the search paths for models. In the standalone windows build you can find this file in the ComfyUI directory. Rename this file to extra_model_paths.yaml and edit it with your favorite text editor.&lt;/p&gt; 
&lt;h2&gt;&lt;a href="https://docs.comfy.org/comfy-cli/getting-started"&gt;comfy-cli&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;You can install and start ComfyUI using comfy-cli:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install comfy-cli
comfy install
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Manual Install (Windows, Linux)&lt;/h2&gt; 
&lt;p&gt;Python 3.14 works but you may encounter issues with the torch compile node. The free threaded variant is still missing some dependencies.&lt;/p&gt; 
&lt;p&gt;Python 3.13 is very well supported. If you have trouble with some custom node dependencies on 3.13 you can try 3.12&lt;/p&gt; 
&lt;h3&gt;Instructions:&lt;/h3&gt; 
&lt;p&gt;Git clone this repo.&lt;/p&gt; 
&lt;p&gt;Put your SD checkpoints (the huge ckpt/safetensors files) in: models/checkpoints&lt;/p&gt; 
&lt;p&gt;Put your VAE in: models/vae&lt;/p&gt; 
&lt;h3&gt;AMD GPUs (Linux)&lt;/h3&gt; 
&lt;p&gt;AMD users can install rocm and pytorch with pip if you don't have it already installed, this is the command to install the stable version:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.4&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;This is the command to install the nightly with ROCm 7.0 which might have some performance improvements:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/rocm7.1&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;AMD GPUs (Experimental: Windows and Linux), RDNA 3, 3.5 and 4 only.&lt;/h3&gt; 
&lt;p&gt;These have less hardware support than the builds above but they work on windows. You also need to install the pytorch version specific to your hardware.&lt;/p&gt; 
&lt;p&gt;RDNA 3 (RX 7000 series):&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;pip install --pre torch torchvision torchaudio --index-url https://rocm.nightlies.amd.com/v2/gfx110X-dgpu/&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;RDNA 3.5 (Strix halo/Ryzen AI Max+ 365):&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;pip install --pre torch torchvision torchaudio --index-url https://rocm.nightlies.amd.com/v2/gfx1151/&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;RDNA 4 (RX 9000 series):&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;pip install --pre torch torchvision torchaudio --index-url https://rocm.nightlies.amd.com/v2/gfx120X-all/&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Intel GPUs (Windows and Linux)&lt;/h3&gt; 
&lt;p&gt;Intel Arc GPU users can install native PyTorch with torch.xpu support using pip. More information can be found &lt;a href="https://pytorch.org/docs/main/notes/get_start_xpu.html"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;To install PyTorch xpu, use the following command:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;code&gt;pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/xpu&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;This is the command to install the Pytorch xpu nightly which might have some performance improvements:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/xpu&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;NVIDIA&lt;/h3&gt; 
&lt;p&gt;Nvidia users should install stable pytorch using this command:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu130&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;This is the command to install pytorch nightly instead which might have performance improvements.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu130&lt;/code&gt;&lt;/p&gt; 
&lt;h4&gt;Troubleshooting&lt;/h4&gt; 
&lt;p&gt;If you get the "Torch not compiled with CUDA enabled" error, uninstall torch with:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;pip uninstall torch&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;And install it again with the command above.&lt;/p&gt; 
&lt;h3&gt;Dependencies&lt;/h3&gt; 
&lt;p&gt;Install the dependencies by opening your terminal inside the ComfyUI folder and:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;After this you should have everything installed and can proceed to running ComfyUI.&lt;/p&gt; 
&lt;h3&gt;Others:&lt;/h3&gt; 
&lt;h4&gt;Apple Mac silicon&lt;/h4&gt; 
&lt;p&gt;You can install ComfyUI in Apple Mac silicon (M1 or M2) with any recent macOS version.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install pytorch nightly. For instructions, read the &lt;a href="https://developer.apple.com/metal/pytorch/"&gt;Accelerated PyTorch training on Mac&lt;/a&gt; Apple Developer guide (make sure to install the latest pytorch nightly).&lt;/li&gt; 
 &lt;li&gt;Follow the &lt;a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/#manual-install-windows-linux"&gt;ComfyUI manual installation&lt;/a&gt; instructions for Windows and Linux.&lt;/li&gt; 
 &lt;li&gt;Install the ComfyUI &lt;a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/#dependencies"&gt;dependencies&lt;/a&gt;. If you have another Stable Diffusion UI &lt;a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/#i-already-have-another-ui-for-stable-diffusion-installed-do-i-really-have-to-install-all-of-these-dependencies"&gt;you might be able to reuse the dependencies&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Launch ComfyUI by running &lt;code&gt;python main.py&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Remember to add your models, VAE, LoRAs etc. to the corresponding Comfy folders, as discussed in &lt;a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/#manual-install-windows-linux"&gt;ComfyUI manual installation&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Ascend NPUs&lt;/h4&gt; 
&lt;p&gt;For models compatible with Ascend Extension for PyTorch (torch_npu). To get started, ensure your environment meets the prerequisites outlined on the &lt;a href="https://ascend.github.io/docs/sources/ascend/quick_install.html"&gt;installation&lt;/a&gt; page. Here's a step-by-step guide tailored to your platform and installation method:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Begin by installing the recommended or newer kernel version for Linux as specified in the Installation page of torch-npu, if necessary.&lt;/li&gt; 
 &lt;li&gt;Proceed with the installation of Ascend Basekit, which includes the driver, firmware, and CANN, following the instructions provided for your specific platform.&lt;/li&gt; 
 &lt;li&gt;Next, install the necessary packages for torch-npu by adhering to the platform-specific instructions on the &lt;a href="https://ascend.github.io/docs/sources/pytorch/install.html#pytorch"&gt;Installation&lt;/a&gt; page.&lt;/li&gt; 
 &lt;li&gt;Finally, adhere to the &lt;a href="https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/#manual-install-windows-linux"&gt;ComfyUI manual installation&lt;/a&gt; guide for Linux. Once all components are installed, you can run ComfyUI as described earlier.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Cambricon MLUs&lt;/h4&gt; 
&lt;p&gt;For models compatible with Cambricon Extension for PyTorch (torch_mlu). Here's a step-by-step guide tailored to your platform and installation method:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install the Cambricon CNToolkit by adhering to the platform-specific instructions on the &lt;a href="https://www.cambricon.com/docs/sdk_1.15.0/cntoolkit_3.7.2/cntoolkit_install_3.7.2/index.html"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Next, install the PyTorch(torch_mlu) following the instructions on the &lt;a href="https://www.cambricon.com/docs/sdk_1.15.0/cambricon_pytorch_1.17.0/user_guide_1.9/index.html"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Launch ComfyUI by running &lt;code&gt;python main.py&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Iluvatar Corex&lt;/h4&gt; 
&lt;p&gt;For models compatible with Iluvatar Extension for PyTorch. Here's a step-by-step guide tailored to your platform and installation method:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install the Iluvatar Corex Toolkit by adhering to the platform-specific instructions on the &lt;a href="https://support.iluvatar.com/#/DocumentCentre?id=1&amp;amp;nameCenter=2&amp;amp;productId=520117912052801536"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Launch ComfyUI by running &lt;code&gt;python main.py&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;&lt;a href="https://github.com/Comfy-Org/ComfyUI-Manager/tree/manager-v4"&gt;ComfyUI-Manager&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;ComfyUI-Manager&lt;/strong&gt; is an extension that allows you to easily install, update, and manage custom nodes for ComfyUI.&lt;/p&gt; 
&lt;h3&gt;Setup&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Install the manager dependencies:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r manager_requirements.txt
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Enable the manager with the &lt;code&gt;--enable-manager&lt;/code&gt; flag when running ComfyUI:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;python main.py --enable-manager
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Command Line Options&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Flag&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--enable-manager&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Enable ComfyUI-Manager&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--enable-manager-legacy-ui&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Use the legacy manager UI instead of the new UI (requires &lt;code&gt;--enable-manager&lt;/code&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--disable-manager-ui&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Disable the manager UI and endpoints while keeping background features like security checks and scheduled installation completion (requires &lt;code&gt;--enable-manager&lt;/code&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;Running&lt;/h1&gt; 
&lt;p&gt;&lt;code&gt;python main.py&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;For AMD cards not officially supported by ROCm&lt;/h3&gt; 
&lt;p&gt;Try running it with this command if you have issues:&lt;/p&gt; 
&lt;p&gt;For 6700, 6600 and maybe other RDNA2 or older: &lt;code&gt;HSA_OVERRIDE_GFX_VERSION=10.3.0 python main.py&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;For AMD 7600 and maybe other RDNA3 cards: &lt;code&gt;HSA_OVERRIDE_GFX_VERSION=11.0.0 python main.py&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;AMD ROCm Tips&lt;/h3&gt; 
&lt;p&gt;You can enable experimental memory efficient attention on recent pytorch in ComfyUI on some AMD GPUs using this command, it should already be enabled by default on RDNA3. If this improves speed for you on latest pytorch on your GPU please report it so that I can enable it by default.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL=1 python main.py --use-pytorch-cross-attention&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;You can also try setting this env variable &lt;code&gt;PYTORCH_TUNABLEOP_ENABLED=1&lt;/code&gt; which might speed things up at the cost of a very slow initial run.&lt;/p&gt; 
&lt;h1&gt;Notes&lt;/h1&gt; 
&lt;p&gt;Only parts of the graph that have an output with all the correct inputs will be executed.&lt;/p&gt; 
&lt;p&gt;Only parts of the graph that change from each execution to the next will be executed, if you submit the same graph twice only the first will be executed. If you change the last part of the graph only the part you changed and the part that depends on it will be executed.&lt;/p&gt; 
&lt;p&gt;Dragging a generated png on the webpage or loading one will give you the full workflow including seeds that were used to create it.&lt;/p&gt; 
&lt;p&gt;You can use () to change emphasis of a word or phrase like: (good code:1.2) or (bad code:0.8). The default emphasis for () is 1.1. To use () characters in your actual prompt escape them like \( or \).&lt;/p&gt; 
&lt;p&gt;You can use {day|night}, for wildcard/dynamic prompts. With this syntax "{wild|card|test}" will be randomly replaced by either "wild", "card" or "test" by the frontend every time you queue the prompt. To use {} characters in your actual prompt escape them like: \{ or \}.&lt;/p&gt; 
&lt;p&gt;Dynamic prompts also support C-style comments, like &lt;code&gt;// comment&lt;/code&gt; or &lt;code&gt;/* comment */&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;To use a textual inversion concepts/embeddings in a text prompt put them in the models/embeddings directory and use them in the CLIPTextEncode node like this (you can omit the .pt extension):&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;embedding:embedding_filename.pt&lt;/code&gt;&lt;/p&gt; 
&lt;h2&gt;How to show high-quality previews?&lt;/h2&gt; 
&lt;p&gt;Use &lt;code&gt;--preview-method auto&lt;/code&gt; to enable previews.&lt;/p&gt; 
&lt;p&gt;The default installation includes a fast latent preview method that's low-resolution. To enable higher-quality previews with &lt;a href="https://github.com/madebyollin/taesd"&gt;TAESD&lt;/a&gt;, download the &lt;a href="https://github.com/madebyollin/taesd/"&gt;taesd_decoder.pth, taesdxl_decoder.pth, taesd3_decoder.pth and taef1_decoder.pth&lt;/a&gt; and place them in the &lt;code&gt;models/vae_approx&lt;/code&gt; folder. Once they're installed, restart ComfyUI and launch it with &lt;code&gt;--preview-method taesd&lt;/code&gt; to enable high-quality previews.&lt;/p&gt; 
&lt;h2&gt;How to use TLS/SSL?&lt;/h2&gt; 
&lt;p&gt;Generate a self-signed certificate (not appropriate for shared/production use) and key by running the command: &lt;code&gt;openssl req -x509 -newkey rsa:4096 -keyout key.pem -out cert.pem -sha256 -days 3650 -nodes -subj "/C=XX/ST=StateName/L=CityName/O=CompanyName/OU=CompanySectionName/CN=CommonNameOrHostname"&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Use &lt;code&gt;--tls-keyfile key.pem --tls-certfile cert.pem&lt;/code&gt; to enable TLS/SSL, the app will now be accessible with &lt;code&gt;https://...&lt;/code&gt; instead of &lt;code&gt;http://...&lt;/code&gt;.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Note: Windows users can use &lt;a href="https://github.com/alexisrolland/docker-openssl"&gt;alexisrolland/docker-openssl&lt;/a&gt; or one of the &lt;a href="https://wiki.openssl.org/index.php/Binaries"&gt;3rd party binary distributions&lt;/a&gt; to run the command example above. &lt;br /&gt;&lt;br /&gt;If you use a container, note that the volume mount &lt;code&gt;-v&lt;/code&gt; can be a relative path so &lt;code&gt;... -v ".\:/openssl-certs" ...&lt;/code&gt; would create the key &amp;amp; cert files in the current directory of your command prompt or powershell terminal.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Support and dev channel&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://comfy.org/discord"&gt;Discord&lt;/a&gt;: Try the #help or #feedback channels.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://app.element.io/#/room/%23comfyui_space%3Amatrix.org"&gt;Matrix space: #comfyui_space:matrix.org&lt;/a&gt; (it's like discord but open source).&lt;/p&gt; 
&lt;p&gt;See also: &lt;a href="https://www.comfy.org/"&gt;https://www.comfy.org/&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Frontend Development&lt;/h2&gt; 
&lt;p&gt;As of August 15, 2024, we have transitioned to a new frontend, which is now hosted in a separate repository: &lt;a href="https://github.com/Comfy-Org/ComfyUI_frontend"&gt;ComfyUI Frontend&lt;/a&gt;. This repository now hosts the compiled JS (from TS/Vue) under the &lt;code&gt;web/&lt;/code&gt; directory.&lt;/p&gt; 
&lt;h3&gt;Reporting Issues and Requesting Features&lt;/h3&gt; 
&lt;p&gt;For any bugs, issues, or feature requests related to the frontend, please use the &lt;a href="https://github.com/Comfy-Org/ComfyUI_frontend"&gt;ComfyUI Frontend repository&lt;/a&gt;. This will help us manage and address frontend-specific concerns more efficiently.&lt;/p&gt; 
&lt;h3&gt;Using the Latest Frontend&lt;/h3&gt; 
&lt;p&gt;The new frontend is now the default for ComfyUI. However, please note:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;The frontend in the main ComfyUI repository is updated fortnightly.&lt;/li&gt; 
 &lt;li&gt;Daily releases are available in the separate frontend repository.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;To use the most up-to-date frontend version:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;For the latest daily release, launch ComfyUI with this command line argument:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;--front-end-version Comfy-Org/ComfyUI_frontend@latest
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;For a specific version, replace &lt;code&gt;latest&lt;/code&gt; with the desired version number:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;--front-end-version Comfy-Org/ComfyUI_frontend@1.2.2
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;This approach allows you to easily switch between the stable fortnightly release and the cutting-edge daily updates, or even specific versions for testing purposes.&lt;/p&gt; 
&lt;h3&gt;Accessing the Legacy Frontend&lt;/h3&gt; 
&lt;p&gt;If you need to use the legacy frontend for any reason, you can access it using the following command line argument:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;--front-end-version Comfy-Org/ComfyUI_legacy_frontend@latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will use a snapshot of the legacy frontend preserved in the &lt;a href="https://github.com/Comfy-Org/ComfyUI_legacy_frontend"&gt;ComfyUI Legacy Frontend repository&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;QA&lt;/h1&gt; 
&lt;h3&gt;Which GPU should I buy for this?&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/comfyanonymous/ComfyUI/wiki/Which-GPU-should-I-buy-for-ComfyUI"&gt;See this page for some recommendations&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>astral-sh/ty</title>
      <link>https://github.com/astral-sh/ty</link>
      <description>&lt;p&gt;An extremely fast Python type checker and language server, written in Rust.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ty&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/astral-sh/ty"&gt;&lt;img src="https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ty/main/assets/badge/v0.json" alt="ty" /&gt;&lt;/a&gt; &lt;a href="https://pypi.python.org/pypi/ty"&gt;&lt;img src="https://img.shields.io/pypi/v/ty.svg?sanitize=true" alt="PyPI" /&gt;&lt;/a&gt; &lt;a href="https://discord.com/invite/astral-sh"&gt;&lt;img src="https://img.shields.io/badge/Discord-%235865F2.svg?logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;An extremely fast Python type checker and language server, written in Rust.&lt;/p&gt; 
&lt;br /&gt; 
&lt;p align="center"&gt; &lt;img alt="Shows a bar chart with benchmark results." width="500px" src="https://raw.githubusercontent.com/astral-sh/ty/main/docs/assets/ty-benchmark-cli.svg?sanitize=true" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;i&gt;Type checking the &lt;a href="https://github.com/home-assistant/core"&gt;home-assistant&lt;/a&gt; project without caching.&lt;/i&gt; &lt;/p&gt; 
&lt;br /&gt; 
&lt;p&gt;ty is backed by &lt;a href="https://astral.sh"&gt;Astral&lt;/a&gt;, the creators of &lt;a href="https://github.com/astral-sh/uv"&gt;uv&lt;/a&gt; and &lt;a href="https://github.com/astral-sh/ruff"&gt;Ruff&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Highlights&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;10x - 100x faster than mypy and Pyright&lt;/li&gt; 
 &lt;li&gt;Comprehensive &lt;a href="https://docs.astral.sh/ty/features/diagnostics/"&gt;diagnostics&lt;/a&gt; with rich contextual information&lt;/li&gt; 
 &lt;li&gt;Configurable &lt;a href="https://docs.astral.sh/ty/rules/"&gt;rule levels&lt;/a&gt;, &lt;a href="https://docs.astral.sh/ty/reference/configuration/#overrides"&gt;per-file overrides&lt;/a&gt;, &lt;a href="https://docs.astral.sh/ty/suppression/"&gt;suppression comments&lt;/a&gt;, and first-class project support&lt;/li&gt; 
 &lt;li&gt;Designed for adoption, with support for &lt;a href="https://docs.astral.sh/ty/features/type-system/#redeclarations"&gt;redeclarations&lt;/a&gt; and &lt;a href="https://docs.astral.sh/ty/features/type-system/#gradual-guarantee"&gt;partially typed code&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.astral.sh/ty/features/language-server/"&gt;Language server&lt;/a&gt; with code navigation, completions, code actions, auto-import, inlay hints, on-hover help, etc.&lt;/li&gt; 
 &lt;li&gt;Fine-grained &lt;a href="https://docs.astral.sh/ty/features/language-server/#fine-grained-incrementality"&gt;incremental analysis&lt;/a&gt; designed for fast updates when editing files in an IDE&lt;/li&gt; 
 &lt;li&gt;Editor integrations for &lt;a href="https://docs.astral.sh/ty/editors/#vs-code"&gt;VS Code&lt;/a&gt;, &lt;a href="https://docs.astral.sh/ty/editors/#pycharm"&gt;PyCharm&lt;/a&gt;, &lt;a href="https://docs.astral.sh/ty/editors/#neovim"&gt;Neovim&lt;/a&gt; and more&lt;/li&gt; 
 &lt;li&gt;Advanced typing features like first-class &lt;a href="https://docs.astral.sh/ty/features/type-system/#intersection-types"&gt;intersection types&lt;/a&gt;, advanced &lt;a href="https://docs.astral.sh/ty/features/type-system/#top-and-bottom-materializations"&gt;type narrowing&lt;/a&gt;, and &lt;a href="https://docs.astral.sh/ty/features/type-system/#reachability-based-on-types"&gt;sophisticated reachability analysis&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Getting started&lt;/h2&gt; 
&lt;p&gt;Run ty with &lt;a href="https://docs.astral.sh/uv/guides/tools/#running-tools"&gt;uvx&lt;/a&gt; to get started quickly:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;uvx ty check
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or, check out the &lt;a href="https://play.ty.dev"&gt;ty playground&lt;/a&gt; to try it out in your browser.&lt;/p&gt; 
&lt;p&gt;To learn more about using ty, see the &lt;a href="https://docs.astral.sh/ty/"&gt;documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;To install ty, see the &lt;a href="https://docs.astral.sh/ty/installation/"&gt;installation&lt;/a&gt; documentation.&lt;/p&gt; 
&lt;p&gt;To add the ty language server to your editor, see the &lt;a href="https://docs.astral.sh/ty/editors/"&gt;editor integration&lt;/a&gt; guide.&lt;/p&gt; 
&lt;h2&gt;Getting help&lt;/h2&gt; 
&lt;p&gt;If you have questions or want to report a bug, please open an &lt;a href="https://github.com/astral-sh/ty/issues"&gt;issue&lt;/a&gt; in this repository.&lt;/p&gt; 
&lt;p&gt;You may also join our &lt;a href="https://discord.com/invite/astral-sh"&gt;Discord server&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Development of this project takes place in the &lt;a href="https://github.com/astral-sh/ruff"&gt;Ruff&lt;/a&gt; repository at this time. Please &lt;a href="https://github.com/astral-sh/ruff/pulls"&gt;open pull requests&lt;/a&gt; there for changes to anything in the &lt;code&gt;ruff&lt;/code&gt; submodule (which includes all of the Rust source code).&lt;/p&gt; 
&lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/astral-sh/ty/main/CONTRIBUTING.md"&gt;contributing guide&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;FAQ&lt;/h2&gt; 
&lt;!-- We intentionally use smaller headings for the FAQ items --&gt; 
&lt;!-- markdownlint-disable MD001 --&gt; 
&lt;h4&gt;Why is ty doing _____?&lt;/h4&gt; 
&lt;p&gt;See our &lt;a href="https://docs.astral.sh/ty/reference/typing-faq"&gt;typing FAQ&lt;/a&gt;.&lt;/p&gt; 
&lt;h4&gt;How do you pronounce ty?&lt;/h4&gt; 
&lt;p&gt;It's pronounced as "tee - why" (&lt;a href="https://en.wikipedia.org/wiki/Help:IPA/English#Key"&gt;&lt;code&gt;/tiː waɪ/&lt;/code&gt;&lt;/a&gt;)&lt;/p&gt; 
&lt;h4&gt;How should I stylize ty?&lt;/h4&gt; 
&lt;p&gt;Just "ty", please.&lt;/p&gt; 
&lt;!-- markdownlint-enable MD001 --&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;ty is licensed under the MIT license (&lt;a href="https://raw.githubusercontent.com/astral-sh/ty/main/LICENSE"&gt;LICENSE&lt;/a&gt; or &lt;a href="https://opensource.org/licenses/MIT"&gt;https://opensource.org/licenses/MIT&lt;/a&gt;).&lt;/p&gt; 
&lt;p&gt;Unless you explicitly state otherwise, any contribution intentionally submitted for inclusion in ty by you, as defined in the MIT license, shall be licensed as above, without any additional terms or conditions.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a target="_blank" href="https://astral.sh" style="background:none"&gt; &lt;img src="https://raw.githubusercontent.com/astral-sh/uv/main/assets/svg/Astral.svg?sanitize=true" alt="Made by Astral" /&gt; &lt;/a&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>OpenBMB/VoxCPM</title>
      <link>https://github.com/OpenBMB/VoxCPM</link>
      <description>&lt;p&gt;VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;🎙️ VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/OpenBMB/VoxCPM/"&gt;&lt;img src="https://img.shields.io/badge/Project%20Page-GitHub-blue" alt="Project Page" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2509.24650"&gt;&lt;img src="https://img.shields.io/badge/Technical%20Report-Arxiv-red" alt="Technical Report" /&gt;&lt;/a&gt;&lt;a href="https://huggingface.co/spaces/OpenBMB/VoxCPM-Demo"&gt;&lt;img src="https://img.shields.io/badge/Live%20PlayGround-Demo-orange" alt="Live Playground" /&gt;&lt;/a&gt; &lt;a href="https://openbmb.github.io/VoxCPM-demopage"&gt;&lt;img src="https://img.shields.io/badge/Audio%20Samples-Page-green" alt="Samples" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;VoxCPM1.5 Model Weights&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/openbmb/VoxCPM1.5"&gt;&lt;img src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-OpenBMB-yellow" alt="Hugging Face" /&gt;&lt;/a&gt; &lt;a href="https://modelscope.cn/models/OpenBMB/VoxCPM1.5"&gt;&lt;img src="https://img.shields.io/badge/ModelScope-OpenBMB-purple" alt="ModelScope" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenBMB/VoxCPM/main/assets/voxcpm_logo.png" alt="VoxCPM Logo" width="40%" /&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;👋 Contact us on &lt;a href="https://raw.githubusercontent.com/OpenBMB/VoxCPM/main/assets/wechat.png"&gt;WeChat&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;[2025.12.05] 🎉 🎉 🎉 We Open Source the VoxCPM1.5 &lt;a href="https://huggingface.co/openbmb/VoxCPM1.5"&gt;weights&lt;/a&gt;! The model now supports both full-parameter fine-tuning and efficient LoRA fine-tuning, empowering you to create your own tailored version. See &lt;a href="https://raw.githubusercontent.com/OpenBMB/VoxCPM/main/docs/release_note.md"&gt;Release Notes&lt;/a&gt; for details.&lt;/li&gt; 
 &lt;li&gt;[2025.09.30] 🔥 🔥 🔥 We Release VoxCPM &lt;a href="https://arxiv.org/abs/2509.24650"&gt;Technical Report&lt;/a&gt;!&lt;/li&gt; 
 &lt;li&gt;[2025.09.16] 🔥 🔥 🔥 We Open Source the VoxCPM-0.5B &lt;a href="https://huggingface.co/openbmb/VoxCPM-0.5B"&gt;weights&lt;/a&gt;!&lt;/li&gt; 
 &lt;li&gt;[2025.09.16] 🎉 🎉 🎉 We Provide the &lt;a href="https://huggingface.co/spaces/OpenBMB/VoxCPM-Demo"&gt;Gradio PlayGround&lt;/a&gt; for VoxCPM-0.5B, try it now!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Overview&lt;/h2&gt; 
&lt;p&gt;VoxCPM is a novel tokenizer-free Text-to-Speech (TTS) system that redefines realism in speech synthesis. By modeling speech in a continuous space, it overcomes the limitations of discrete tokenization and enables two flagship capabilities: context-aware speech generation and true-to-life zero-shot voice cloning.&lt;/p&gt; 
&lt;p&gt;Unlike mainstream approaches that convert speech to discrete tokens, VoxCPM uses an end-to-end diffusion autoregressive architecture that directly generates continuous speech representations from text. Built on &lt;a href="https://huggingface.co/openbmb/MiniCPM4-0.5B"&gt;MiniCPM-4&lt;/a&gt; backbone, it achieves implicit semantic-acoustic decoupling through hierachical language modeling and FSQ constraints, greatly enhancing both expressiveness and generation stability.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenBMB/VoxCPM/main/assets/voxcpm_model.png" alt="VoxCPM Model Architecture" width="90%" /&gt; 
&lt;/div&gt; 
&lt;h3&gt;🚀 Key Features&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Context-Aware, Expressive Speech Generation&lt;/strong&gt; - VoxCPM comprehends text to infer and generate appropriate prosody, delivering speech with remarkable expressiveness and natural flow. It spontaneously adapts speaking style based on content, producing highly fitting vocal expression trained on a massive 1.8 million-hour bilingual corpus.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;True-to-Life Voice Cloning&lt;/strong&gt; - With only a short reference audio clip, VoxCPM performs accurate zero-shot voice cloning, capturing not only the speaker's timbre but also fine-grained characteristics such as accent, emotional tone, rhythm, and pacing to create a faithful and natural replica.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;High-Efficiency Synthesis&lt;/strong&gt; - VoxCPM supports streaming synthesis with a Real-Time Factor (RTF) as low as 0.17 on a consumer-grade NVIDIA RTX 4090 GPU, making it possible for real-time applications.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;📦 Model Versions&lt;/h3&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/OpenBMB/VoxCPM/main/docs/release_note.md"&gt;Release Notes&lt;/a&gt; for details&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;VoxCPM1.5&lt;/strong&gt; (Latest):&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Model Params: 800M&lt;/li&gt; 
   &lt;li&gt;Sampling rate of AudioVAE: 44100&lt;/li&gt; 
   &lt;li&gt;Token rate in LM Backbone: 6.25Hz (patch-size=4)&lt;/li&gt; 
   &lt;li&gt;RTF in a single NVIDIA-RTX 4090 GPU: ~0.15&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;VoxCPM-0.5B&lt;/strong&gt; (Original):&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Model Params: 640M&lt;/li&gt; 
   &lt;li&gt;Sampling rate of AudioVAE: 16000&lt;/li&gt; 
   &lt;li&gt;Token rate in LM Backbone: 12.5Hz (patch-size=2)&lt;/li&gt; 
   &lt;li&gt;RTF in a single NVIDIA-RTX 4090 GPU: 0.17&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;h3&gt;🔧 Install from PyPI&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;pip install voxcpm
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;1. Model Download (Optional)&lt;/h3&gt; 
&lt;p&gt;By default, when you first run the script, the model will be downloaded automatically, but you can also download the model in advance.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Download VoxCPM1.5&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from huggingface_hub import snapshot_download
snapshot_download("openbmb/VoxCPM1.5")
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Or Download VoxCPM-0.5B&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from huggingface_hub import snapshot_download
snapshot_download("openbmb/VoxCPM-0.5B")
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Download ZipEnhancer and SenseVoice-Small. We use ZipEnhancer to enhance speech prompts and SenseVoice-Small for speech prompt ASR in the web demo.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from modelscope import snapshot_download
snapshot_download('iic/speech_zipenhancer_ans_multiloss_16k_base')
snapshot_download('iic/SenseVoiceSmall')
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;2. Basic Usage&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import soundfile as sf
import numpy as np
from voxcpm import VoxCPM

model = VoxCPM.from_pretrained("openbmb/VoxCPM1.5")

# Non-streaming
wav = model.generate(
    text="VoxCPM is an innovative end-to-end TTS model from ModelBest, designed to generate highly expressive speech.",
    prompt_wav_path=None,      # optional: path to a prompt speech for voice cloning
    prompt_text=None,          # optional: reference text
    cfg_value=2.0,             # LM guidance on LocDiT, higher for better adherence to the prompt, but maybe worse
    inference_timesteps=10,   # LocDiT inference timesteps, higher for better result, lower for fast speed
    normalize=False,           # enable external TN tool, but will disable native raw text support
    denoise=False,             # enable external Denoise tool, but it may cause some distortion and restrict the sampling rate to 16kHz
    retry_badcase=True,        # enable retrying mode for some bad cases (unstoppable)
    retry_badcase_max_times=3,  # maximum retrying times
    retry_badcase_ratio_threshold=6.0, # maximum length restriction for bad case detection (simple but effective), it could be adjusted for slow pace speech
)

sf.write("output.wav", wav, model.tts_model.sample_rate)
print("saved: output.wav")

# Streaming
chunks = []
for chunk in model.generate_streaming(
    text = "Streaming text to speech is easy with VoxCPM!",
    # supports same args as above
):
    chunks.append(chunk)
wav = np.concatenate(chunks)

sf.write("output_streaming.wav", wav, model.tts_model.sample_rate)
print("saved: output_streaming.wav")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;3. CLI Usage&lt;/h3&gt; 
&lt;p&gt;After installation, the entry point is &lt;code&gt;voxcpm&lt;/code&gt; (or use &lt;code&gt;python -m voxcpm.cli&lt;/code&gt;).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 1) Direct synthesis (single text)
voxcpm --text "VoxCPM is an innovative end-to-end TTS model from ModelBest, designed to generate highly expressive speech." --output out.wav

# 2) Voice cloning (reference audio + transcript)
voxcpm --text "VoxCPM is an innovative end-to-end TTS model from ModelBest, designed to generate highly expressive speech." \
  --prompt-audio path/to/voice.wav \
  --prompt-text "reference transcript" \
  --output out.wav \
  # --denoise

# (Optinal) Voice cloning (reference audio + transcript file)
voxcpm --text "VoxCPM is an innovative end-to-end TTS model from ModelBest, designed to generate highly expressive speech." \
  --prompt-audio path/to/voice.wav \
  --prompt-file "/path/to/text-file" \
  --output out.wav \
  # --denoise

# 3) Batch processing (one text per line)
voxcpm --input examples/input.txt --output-dir outs
# (optional) Batch + cloning
voxcpm --input examples/input.txt --output-dir outs \
  --prompt-audio path/to/voice.wav \
  --prompt-text "reference transcript" \
  # --denoise

# 4) Inference parameters (quality/speed)
voxcpm --text "..." --output out.wav \
  --cfg-value 2.0 --inference-timesteps 10 --normalize

# 5) Model loading
# Prefer local path
voxcpm --text "..." --output out.wav --model-path /path/to/VoxCPM_model_dir
# Or from Hugging Face (auto download/cache)
voxcpm --text "..." --output out.wav \
  --hf-model-id openbmb/VoxCPM1.5 --cache-dir ~/.cache/huggingface --local-files-only

# 6) Denoiser control
voxcpm --text "..." --output out.wav \
  --no-denoiser --zipenhancer-path iic/speech_zipenhancer_ans_multiloss_16k_base

# 7) Help
voxcpm --help
python -m voxcpm.cli --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;4. Start web demo&lt;/h3&gt; 
&lt;p&gt;You can start the UI interface by running &lt;code&gt;python app.py&lt;/code&gt;, which allows you to perform Voice Cloning and Voice Creation.&lt;/p&gt; 
&lt;h3&gt;5. Fine-tuning&lt;/h3&gt; 
&lt;p&gt;VoxCPM1.5 supports both full fine-tuning (SFT) and LoRA fine-tuning, allowing you to train personalized voice models on your own data. See the &lt;a href="https://raw.githubusercontent.com/OpenBMB/VoxCPM/main/docs/finetune.md"&gt;Fine-tuning Guide&lt;/a&gt; for detailed instructions.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Quick Start:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Full fine-tuning
python scripts/train_voxcpm_finetune.py \
    --config_path conf/voxcpm_v1.5/voxcpm_finetune_all.yaml

# LoRA fine-tuning
python scripts/train_voxcpm_finetune.py \
    --config_path conf/voxcpm_v1.5/voxcpm_finetune_lora.yaml
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;📚 Documentation&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/VoxCPM/main/docs/usage_guide.md"&gt;Usage Guide&lt;/a&gt;&lt;/strong&gt; - Detailed guide on how to use VoxCPM effectively, including text input modes, voice cloning tips, and parameter tuning&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/VoxCPM/main/docs/finetune.md"&gt;Fine-tuning Guide&lt;/a&gt;&lt;/strong&gt; - Complete guide for fine-tuning VoxCPM models with SFT and LoRA&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/VoxCPM/main/docs/release_note.md"&gt;Release Notes&lt;/a&gt;&lt;/strong&gt; - Version history and updates&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/VoxCPM/main/docs/performance.md"&gt;Performance Benchmarks&lt;/a&gt;&lt;/strong&gt; - Detailed performance comparisons on public benchmarks&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;📚 More Information&lt;/h2&gt; 
&lt;h3&gt;🌟 Community Projects&lt;/h3&gt; 
&lt;p&gt;We're excited to see the VoxCPM community growing! Here are some amazing projects and features built by our community:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/wildminder/ComfyUI-VoxCPM"&gt;ComfyUI-VoxCPM&lt;/a&gt;&lt;/strong&gt; A VoxCPM extension for ComfyUI.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/1038lab/ComfyUI-VoxCPMTTS"&gt;ComfyUI-VoxCPMTTS&lt;/a&gt;&lt;/strong&gt; A VoxCPM extension for ComfyUI.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/rsxdalv/tts_webui_extension.vox_cpm"&gt;WebUI-VoxCPM&lt;/a&gt;&lt;/strong&gt; A template extension for TTS WebUI.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/OpenBMB/VoxCPM/pull/26"&gt;PR: Streaming API Support (by AbrahamSanders)&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/a710128/nanovllm-voxcpm"&gt;VoxCPM-NanoVLLM&lt;/a&gt;&lt;/strong&gt; NanoVLLM integration for VoxCPM for faster, high-throughput inference on GPU.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/bluryar/VoxCPM-ONNX"&gt;VoxCPM-ONNX&lt;/a&gt;&lt;/strong&gt; ONNX export for VoxCPM supports faster CPU inference.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/0seba/VoxCPMANE"&gt;VoxCPMANE&lt;/a&gt;&lt;/strong&gt; VoxCPM TTS with Apple Neural Engine backend server.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/OpenBMB/VoxCPM/pull/100"&gt;PR: LoRA finetune web UI (by Ayin1412)&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/madushan1000/voxcpm_rs"&gt;voxcpm_rs&lt;/a&gt;&lt;/strong&gt; A re-implementation of VoxCPM-0.5B in Rust.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;em&gt;Note: The projects are not officially maintained by OpenBMB.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Have you built something cool with VoxCPM? We'd love to feature it here! Please open an issue or pull request to add your project.&lt;/em&gt;&lt;/p&gt; 
&lt;h3&gt;📊 Performance Highlights&lt;/h3&gt; 
&lt;p&gt;VoxCPM achieves competitive results on public zero-shot TTS benchmarks. See &lt;a href="https://raw.githubusercontent.com/OpenBMB/VoxCPM/main/docs/performance.md"&gt;Performance Benchmarks&lt;/a&gt; for detailed comparison tables.&lt;/p&gt; 
&lt;h2&gt;⚠️ Risks and limitations&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;General Model Behavior: While VoxCPM has been trained on a large-scale dataset, it may still produce outputs that are unexpected, biased, or contain artifacts.&lt;/li&gt; 
 &lt;li&gt;Potential for Misuse of Voice Cloning: VoxCPM's powerful zero-shot voice cloning capability can generate highly realistic synthetic speech. This technology could be misused for creating convincing deepfakes for purposes of impersonation, fraud, or spreading disinformation. Users of this model must not use it to create content that infringes upon the rights of individuals. It is strictly forbidden to use VoxCPM for any illegal or unethical purposes. We strongly recommend that any publicly shared content generated with this model be clearly marked as AI-generated.&lt;/li&gt; 
 &lt;li&gt;Current Technical Limitations: Although generally stable, the model may occasionally exhibit instability, especially with very long or expressive inputs. Furthermore, the current version offers limited direct control over specific speech attributes like emotion or speaking style.&lt;/li&gt; 
 &lt;li&gt;Bilingual Model: VoxCPM is trained primarily on Chinese and English data. Performance on other languages is not guaranteed and may result in unpredictable or low-quality audio.&lt;/li&gt; 
 &lt;li&gt;This model is released for research and development purposes only. We do not recommend its use in production or commercial applications without rigorous testing and safety evaluations. Please use VoxCPM responsibly.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;📝 TO-DO List&lt;/h2&gt; 
&lt;p&gt;Please stay tuned for updates!&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Release the VoxCPM technical report.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Support higher sampling rate (44.1kHz in VoxCPM-1.5).&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Support SFT and LoRA fine-tuning.&lt;/li&gt; 
 &lt;li&gt;[] Multilingual Support (besides ZH/EN).&lt;/li&gt; 
 &lt;li&gt;[] Controllable Speech Generation by Human Instruction.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;📄 License&lt;/h2&gt; 
&lt;p&gt;The VoxCPM model weights and code are open-sourced under the &lt;a href="https://raw.githubusercontent.com/OpenBMB/VoxCPM/main/LICENSE"&gt;Apache-2.0&lt;/a&gt; license.&lt;/p&gt; 
&lt;h2&gt;🙏 Acknowledgments&lt;/h2&gt; 
&lt;p&gt;We extend our sincere gratitude to the following works and resources for their inspiration and contributions:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/2502.03930"&gt;DiTAR&lt;/a&gt; for the diffusion autoregressive backbone used in speech generation&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenBMB/MiniCPM"&gt;MiniCPM-4&lt;/a&gt; for serving as the language model foundation&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/FunAudioLLM/CosyVoice"&gt;CosyVoice&lt;/a&gt; for the implementation of Flow Matching-based LocDiT&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/descriptinc/descript-audio-codec"&gt;DAC&lt;/a&gt; for providing the Audio VAE backbone&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Institutions&lt;/h2&gt; 
&lt;p&gt;This project is developed by the following institutions:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/VoxCPM/main/assets/modelbest_logo.png" width="28px" /&gt; &lt;a href="https://modelbest.cn/"&gt;ModelBest&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/VoxCPM/main/assets/thuhcsi_logo.png" width="28px" /&gt; &lt;a href="https://github.com/thuhcsi"&gt;THUHCSI&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;⭐ Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#OpenBMB/VoxCPM&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=OpenBMB/VoxCPM&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;📚 Citation&lt;/h2&gt; 
&lt;p&gt;If you find our model helpful, please consider citing our projects 📝 and staring us ⭐️！&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bib"&gt;@article{voxcpm2025,
  title        = {VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning},
  author       = {Zhou, Yixuan and Zeng, Guoyang and Liu, Xin and Li, Xiang and Yu, Renjie and Wang, Ziyang and Ye, Runchuan and Sun, Weiyue and Gui, Jiancheng and Li, Kehan and Wu, Zhiyong  and Liu, Zhiyuan},
  journal      = {arXiv preprint arXiv:2509.24650},
  year         = {2025},
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>anthropics/claude-quickstarts</title>
      <link>https://github.com/anthropics/claude-quickstarts</link>
      <description>&lt;p&gt;A collection of projects designed to help developers quickly get started with building deployable applications using the Claude API&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Claude Quickstarts&lt;/h1&gt; 
&lt;p&gt;Claude Quickstarts is a collection of projects designed to help developers quickly get started with building applications using the Claude API. Each quickstart provides a foundation that you can easily build upon and customize for your specific needs.&lt;/p&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;To use these quickstarts, you'll need an Claude API key. If you don't have one yet, you can sign up for free at &lt;a href="https://console.anthropic.com"&gt;console.anthropic.com&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Available Quickstarts&lt;/h2&gt; 
&lt;h3&gt;Customer Support Agent&lt;/h3&gt; 
&lt;p&gt;A customer support agent powered by Claude. This project demonstrates how to leverage Claude's natural language understanding and generation capabilities to create an AI-assisted customer support system with access to a knowledge base.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/anthropics/claude-quickstarts/main/customer-support-agent"&gt;Go to Customer Support Agent Quickstart&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Financial Data Analyst&lt;/h3&gt; 
&lt;p&gt;A financial data analyst powered by Claude. This project demonstrates how to leverage Claude's capabilities with interactive data visualization to analyze financial data via chat.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/anthropics/claude-quickstarts/main/financial-data-analyst"&gt;Go to Financial Data Analyst Quickstart&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Computer Use Demo&lt;/h3&gt; 
&lt;p&gt;An environment and tools that Claude can use to control a desktop computer. This project demonstrates how to leverage the computer use capabilities of Claude, including support for the latest &lt;code&gt;computer_use_20251124&lt;/code&gt; tool version with zoom actions.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/anthropics/claude-quickstarts/main/computer-use-demo"&gt;Go to Computer Use Demo Quickstart&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Browser Tools API Demo&lt;/h3&gt; 
&lt;p&gt;A complete reference implementation for browser automation powered by Claude. This project demonstrates how to leverage Claude's browser tools API for web interaction, including navigation, DOM inspection, and form manipulation using Playwright.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/anthropics/claude-quickstarts/main/browser-tools-api-demo"&gt;Go to Browser Tools API Demo Quickstart&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Autonomous Coding Agent&lt;/h3&gt; 
&lt;p&gt;An autonomous coding agent powered by the Claude Agent SDK. This project demonstrates a two-agent pattern (initializer + coding agent) that can build complete applications over multiple sessions, with progress persisted via git and a feature list that the agent works through incrementally.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/anthropics/claude-quickstarts/main/autonomous-coding"&gt;Go to Autonomous Coding Agent Quickstart&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;General Usage&lt;/h2&gt; 
&lt;p&gt;Each quickstart project comes with its own README and setup instructions. Generally, you'll follow these steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Clone this repository&lt;/li&gt; 
 &lt;li&gt;Navigate to the specific quickstart directory&lt;/li&gt; 
 &lt;li&gt;Install the required dependencies&lt;/li&gt; 
 &lt;li&gt;Set up your Claude API key as an environment variable&lt;/li&gt; 
 &lt;li&gt;Run the quickstart application&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Explore Further&lt;/h2&gt; 
&lt;p&gt;To deepen your understanding of working with Claude and the Claude API, check out these resources:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.claude.com"&gt;Claude API Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/anthropics/claude-cookbooks"&gt;Claude Cookbooks&lt;/a&gt; - A collection of code snippets and guides for common tasks&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/anthropics/courses/tree/master/anthropic_api_fundamentals"&gt;Claude API Fundamentals Course&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions to the Claude Quickstarts repository! If you have ideas for new quickstart projects or improvements to existing ones, please open an issue or submit a pull request.&lt;/p&gt; 
&lt;h2&gt;Community and Support&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Join our &lt;a href="https://www.anthropic.com/discord"&gt;Anthropic Discord community&lt;/a&gt; for discussions and support&lt;/li&gt; 
 &lt;li&gt;Check out the &lt;a href="https://support.anthropic.com"&gt;Anthropic support documentation&lt;/a&gt; for additional help&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the MIT License - see the &lt;a href="https://raw.githubusercontent.com/anthropics/claude-quickstarts/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>exo-explore/exo</title>
      <link>https://github.com/exo-explore/exo</link>
      <description>&lt;p&gt;Run your own AI cluster at home with everyday devices 📱💻 🖥️⌚&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="/docs/exo-logo-black-bg.jpg" /&gt; 
  &lt;img alt="exo logo" src="https://raw.githubusercontent.com/exo-explore/exo/main/docs/exo-logo-transparent.png" width="50%" height="50%" /&gt; 
 &lt;/picture&gt; 
 &lt;p&gt;exo: Run your own AI cluster at home with everyday devices. Maintained by &lt;a href="https://x.com/exolabs"&gt;exo labs&lt;/a&gt;.&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;a href="https://discord.gg/72NsF6ux" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://img.shields.io/badge/Discord-Join%20Server-5865F2?logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://x.com/exolabs" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://img.shields.io/twitter/follow/exolabs?style=social" alt="X" /&gt;&lt;/a&gt; &lt;a href="https://www.apache.org/licenses/LICENSE-2.0.html" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://img.shields.io/badge/License-Apache2.0-blue.svg?sanitize=true" alt="License: Apache-2.0" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;p&gt;exo connects all your devices into an AI cluster. Not only does exo enable running models larger than would fit on a single device, but with &lt;a href="https://x.com/exolabs/status/2001817749744476256?s=20"&gt;day-0 support for RDMA over Thunderbolt&lt;/a&gt;, makes models run faster as you add more devices.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Automatic Device Discovery&lt;/strong&gt;: Devices running exo automatically discover each other - no manual configuration.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;RDMA over Thunderbolt&lt;/strong&gt;: exo ships with &lt;a href="https://x.com/exolabs/status/2001817749744476256?s=20"&gt;day-0 support for RDMA over Thunderbolt 5&lt;/a&gt;, enabling 99% reduction in latency between devices.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Topology-Aware Auto Parallel&lt;/strong&gt;: exo figures out the best way to split your model across all available devices based on a realtime view of your device topology. It takes into account device resources and network latency/bandwidth between each link.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Tensor Parallelism&lt;/strong&gt;: exo supports sharding models, for up to 1.8x speedup on 2 devices and 3.2x speedup on 4 devices.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MLX Support&lt;/strong&gt;: exo uses &lt;a href="https://github.com/ml-explore/mlx"&gt;MLX&lt;/a&gt; as an inference backend and &lt;a href="https://ml-explore.github.io/mlx/build/html/usage/distributed.html"&gt;MLX distributed&lt;/a&gt; for distributed communication.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Benchmarks&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;Qwen3-235B (8-bit) on 4 × M3 Ultra Mac Studio with Tensor Parallel RDMA&lt;/summary&gt; 
 &lt;img src="https://raw.githubusercontent.com/exo-explore/exo/main/docs/benchmarks/jeffgeerling/mac-studio-cluster-ai-full-1-qwen3-235b.jpeg" alt="Benchmark - Qwen3-235B (8-bit) on 4 × M3 Ultra Mac Studio with Tensor Parallel RDMA" width="80%" /&gt; 
 &lt;p&gt; &lt;strong&gt;Source:&lt;/strong&gt; &lt;a href="https://www.jeffgeerling.com/blog/2025/15-tb-vram-on-mac-studio-rdma-over-thunderbolt-5"&gt;Jeff Geerling: 15 TB VRAM on Mac Studio – RDMA over Thunderbolt 5&lt;/a&gt; &lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;DeepSeek v3.1 671B (8-bit) on 4 × M3 Ultra Mac Studio with Tensor Parallel RDMA&lt;/summary&gt; 
 &lt;img src="https://raw.githubusercontent.com/exo-explore/exo/main/docs/benchmarks/jeffgeerling/mac-studio-cluster-ai-full-2-deepseek-3.1-671b.jpeg" alt="Benchmark - DeepSeek v3.1 671B (8-bit) on 4 × M3 Ultra Mac Studio with Tensor Parallel RDMA" width="80%" /&gt; 
 &lt;p&gt; &lt;strong&gt;Source:&lt;/strong&gt; &lt;a href="https://www.jeffgeerling.com/blog/2025/15-tb-vram-on-mac-studio-rdma-over-thunderbolt-5"&gt;Jeff Geerling: 15 TB VRAM on Mac Studio – RDMA over Thunderbolt 5&lt;/a&gt; &lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Kimi K2 Thinking (native 4-bit) on 4 × M3 Ultra Mac Studio with Tensor Parallel RDMA&lt;/summary&gt; 
 &lt;img src="https://raw.githubusercontent.com/exo-explore/exo/main/docs/benchmarks/jeffgeerling/mac-studio-cluster-ai-full-3-kimi-k2-thinking.jpeg" alt="Benchmark - Kimi K2 Thinking (native 4-bit) on 4 × M3 Ultra Mac Studio with Tensor Parallel RDMA" width="80%" /&gt; 
 &lt;p&gt; &lt;strong&gt;Source:&lt;/strong&gt; &lt;a href="https://www.jeffgeerling.com/blog/2025/15-tb-vram-on-mac-studio-rdma-over-thunderbolt-5"&gt;Jeff Geerling: 15 TB VRAM on Mac Studio – RDMA over Thunderbolt 5&lt;/a&gt; &lt;/p&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;p&gt;Devices running exo automatically discover each other, without needing any manual configuration. Each device provides an API and a dashboard for interacting with your cluster (runs at &lt;code&gt;http://localhost:52415&lt;/code&gt;).&lt;/p&gt; 
&lt;p&gt;There are two ways to run exo:&lt;/p&gt; 
&lt;h3&gt;Run from Source (Mac &amp;amp; Linux)&lt;/h3&gt; 
&lt;p&gt;Clone the repo, build the dashboard, and run exo:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Clone exo
git clone https://github.com/exo-explore/exo

# Build dashboard
cd exo/dashboard &amp;amp;&amp;amp; npm install &amp;amp;&amp;amp; npm run build &amp;amp;&amp;amp; cd ..

# Run exo
uv run exo
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This starts the exo dashboard and API at &lt;a href="http://localhost:52415/"&gt;http://localhost:52415/&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;macOS App&lt;/h3&gt; 
&lt;p&gt;exo ships a macOS app that runs in the background on your Mac.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/exo-explore/exo/main/docs/macos-app-one-macbook.png" alt="exo macOS App - running on a MacBook" width="35%" /&gt; 
&lt;p&gt;The macOS app requires macOS Tahoe 26.2 or later.&lt;/p&gt; 
&lt;p&gt;Download the latest build here: &lt;a href="https://assets.exolabs.net/EXO-latest.dmg"&gt;EXO-latest.dmg&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The app will ask for permission to modify system settings and install a new Network profile. Improvements to this are being worked on.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Using the API&lt;/h3&gt; 
&lt;p&gt;If you prefer to interact with exo via the API, here is an example creating an instance of a small model (&lt;code&gt;mlx-community/Llama-3.2-1B-Instruct-4bit&lt;/code&gt;), sending a chat completions request and deleting the instance.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;strong&gt;1. Preview instance placements&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;The &lt;code&gt;/instance/previews&lt;/code&gt; endpoint will preview all valid placements for your model.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl "http://localhost:52415/instance/previews?model_id=llama-3.2-1b"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Sample response:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "previews": [
    {
      "model_id": "mlx-community/Llama-3.2-1B-Instruct-4bit",
      "sharding": "Pipeline",
      "instance_meta": "MlxRing",
      "instance": {...},
      "memory_delta_by_node": {"local": 729808896},
      "error": null
    }
    // ...possibly more placements...
  ]
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will return all valid placements for this model. Pick a placement that you like. To pick the first one, pipe into &lt;code&gt;jq&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl "http://localhost:52415/instance/previews?model_id=llama-3.2-1b" | jq -c '.previews[] | select(.error == null) | .instance' | head -n1
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;strong&gt;2. Create a model instance&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Send a POST to &lt;code&gt;/instance&lt;/code&gt; with your desired placement in the &lt;code&gt;instance&lt;/code&gt; field (the full payload must match types as in &lt;code&gt;CreateInstanceParams&lt;/code&gt;), which you can copy from step 1:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -X POST http://localhost:52415/instance \
  -H 'Content-Type: application/json' \
  -d '{
    "instance": {...}
  }'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Sample response:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "message": "Command received.",
  "command_id": "e9d1a8ab-...."
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;strong&gt;3. Send a chat completion&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Now, make a POST to &lt;code&gt;/v1/chat/completions&lt;/code&gt; (the same format as OpenAI's API):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -N -X POST http://localhost:52415/v1/chat/completions \
  -H 'Content-Type: application/json' \
  -d '{
    "model": "mlx-community/Llama-3.2-1B-Instruct-4bit",
    "messages": [
      {"role": "user", "content": "What is Llama 3.2 1B?"}
    ],
    "stream": true
  }'
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;strong&gt;4. Delete the instance&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;When you're done, delete the instance by its ID (find it via &lt;code&gt;/state&lt;/code&gt; or &lt;code&gt;/instance&lt;/code&gt; endpoints):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -X DELETE http://localhost:52415/instance/YOUR_INSTANCE_ID
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;&lt;em&gt;Other useful API endpoints&lt;/em&gt;:&lt;/em&gt;*&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;List all models: &lt;code&gt;curl http://localhost:52415/models&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Inspect instance IDs and deployment state: &lt;code&gt;curl http://localhost:52415/state&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For further details, see API types and endpoints in &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/src/exo/master/api.py"&gt;src/exo/master/api.py&lt;/a&gt;.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Hardware Accelerator Support&lt;/h2&gt; 
&lt;p&gt;On macOS, exo uses the GPU. On Linux, exo currently runs on CPU. We are working on extending hardware accelerator support. If you'd like support for a new hardware platform, please &lt;a href="https://github.com/exo-explore/exo/issues"&gt;search for an existing feature request&lt;/a&gt; and add a thumbs up so we know what hardware is important to the community.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/exo-explore/exo/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for guidelines on how to contribute to exo.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>facebookresearch/sam3</title>
      <link>https://github.com/facebookresearch/sam3</link>
      <description>&lt;p&gt;The repository provides code for running inference and finetuning with the Meta Segment Anything Model 3 (SAM 3), links for downloading the trained model checkpoints, and example notebooks that show how to use the model.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;SAM 3: Segment Anything with Concepts&lt;/h1&gt; 
&lt;p&gt;Meta Superintelligence Labs&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.nicolascarion.com/"&gt;Nicolas Carion&lt;/a&gt;*, &lt;a href="https://scholar.google.com/citations?user=c8IpF9gAAAAJ&amp;amp;hl=en"&gt;Laura Gustafson&lt;/a&gt;*, &lt;a href="https://scholar.google.com/citations?user=E8DVVYQAAAAJ&amp;amp;hl=en"&gt;Yuan-Ting Hu&lt;/a&gt;*, &lt;a href="https://scholar.google.com/citations?user=fb6FOfsAAAAJ&amp;amp;hl=en"&gt;Shoubhik Debnath&lt;/a&gt;*, &lt;a href="https://ronghanghu.com/"&gt;Ronghang Hu&lt;/a&gt;*, &lt;a href="https://www.didacsuris.com/"&gt;Didac Suris&lt;/a&gt;*, &lt;a href="https://scholar.google.com/citations?user=4LWx24UAAAAJ&amp;amp;hl=en"&gt;Chaitanya Ryali&lt;/a&gt;*, &lt;a href="https://scholar.google.co.in/citations?user=m34oaWEAAAAJ&amp;amp;hl=en"&gt;Kalyan Vasudev Alwala&lt;/a&gt;*, &lt;a href="https://hkhedr.com/"&gt;Haitham Khedr&lt;/a&gt;*, Andrew Huang, &lt;a href="https://jayleicn.github.io/"&gt;Jie Lei&lt;/a&gt;, &lt;a href="https://scholar.google.com/citations?user=VeTSl0wAAAAJ&amp;amp;hl=en"&gt;Tengyu Ma&lt;/a&gt;, &lt;a href="https://scholar.google.com/citations?user=BC5wDu8AAAAJ&amp;amp;hl=en"&gt;Baishan Guo&lt;/a&gt;, Arpit Kalla, &lt;a href="https://damaggu.github.io/"&gt;Markus Marks&lt;/a&gt;, &lt;a href="https://scholar.google.com/citations?user=guL96CkAAAAJ&amp;amp;hl=en"&gt;Joseph Greer&lt;/a&gt;, Meng Wang, &lt;a href="https://peizesun.github.io/"&gt;Peize Sun&lt;/a&gt;, &lt;a href="https://scholar.google.com/citations?user=Tpt57v0AAAAJ&amp;amp;hl=en"&gt;Roman Rädle&lt;/a&gt;, &lt;a href="https://www.robots.ox.ac.uk/~afourast/"&gt;Triantafyllos Afouras&lt;/a&gt;, &lt;a href="https://scholar.google.com/citations?user=vYRzGGEAAAAJ&amp;amp;hl=en"&gt;Effrosyni Mavroudi&lt;/a&gt;, &lt;a href="https://k8xu.github.io/"&gt;Katherine Xu&lt;/a&gt;°, &lt;a href="https://patrickthwu.com/"&gt;Tsung-Han Wu&lt;/a&gt;°, &lt;a href="https://yu-bryan-zhou.github.io/"&gt;Yu Zhou&lt;/a&gt;°, &lt;a href="https://scholar.google.com/citations?user=Lb-KgVYAAAAJ&amp;amp;hl=en"&gt;Liliane Momeni&lt;/a&gt;°, &lt;a href="https://rishihazra.github.io/"&gt;Rishi Hazra&lt;/a&gt;°, &lt;a href="https://mark12ding.github.io/"&gt;Shuangrui Ding&lt;/a&gt;°, &lt;a href="https://sgvaze.github.io/"&gt;Sagar Vaze&lt;/a&gt;°, &lt;a href="https://scholar.google.com/citations?user=LgHZ8hUAAAAJ&amp;amp;hl=en"&gt;Francois Porcher&lt;/a&gt;°, &lt;a href="https://fengli-ust.github.io/"&gt;Feng Li&lt;/a&gt;°, &lt;a href="https://siyuanliii.github.io/"&gt;Siyuan Li&lt;/a&gt;°, &lt;a href="https://ashkamath.github.io/"&gt;Aishwarya Kamath&lt;/a&gt;°, &lt;a href="https://hkchengrex.com/"&gt;Ho Kei Cheng&lt;/a&gt;°, &lt;a href="https://pdollar.github.io/"&gt;Piotr Dollar&lt;/a&gt;†, &lt;a href="https://nikhilaravi.com/"&gt;Nikhila Ravi&lt;/a&gt;†, &lt;a href="https://ai.bu.edu/ksaenko.html"&gt;Kate Saenko&lt;/a&gt;†, &lt;a href="https://pzzhang.github.io/pzzhang/"&gt;Pengchuan Zhang&lt;/a&gt;†, &lt;a href="https://feichtenhofer.github.io/"&gt;Christoph Feichtenhofer&lt;/a&gt;†&lt;/p&gt; 
&lt;p&gt;* core contributor, ° intern, † project lead, order is random within groups&lt;/p&gt; 
&lt;p&gt;[&lt;a href="https://ai.meta.com/research/publications/sam-3-segment-anything-with-concepts/"&gt;&lt;code&gt;Paper&lt;/code&gt;&lt;/a&gt;] [&lt;a href="https://ai.meta.com/sam3"&gt;&lt;code&gt;Project&lt;/code&gt;&lt;/a&gt;] [&lt;a href="https://segment-anything.com/"&gt;&lt;code&gt;Demo&lt;/code&gt;&lt;/a&gt;] [&lt;a href="https://ai.meta.com/blog/segment-anything-model-3/"&gt;&lt;code&gt;Blog&lt;/code&gt;&lt;/a&gt;] [&lt;a href="https://raw.githubusercontent.com/facebookresearch/sam3/main/#citing-sam-3"&gt;&lt;code&gt;BibTeX&lt;/code&gt;&lt;/a&gt;]&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/facebookresearch/sam3/main/assets/model_diagram.png?raw=true" alt="SAM 3 architecture" /&gt; SAM 3 is a unified foundation model for promptable segmentation in images and videos. It can detect, segment, and track objects using text or visual prompts such as points, boxes, and masks. Compared to its predecessor &lt;a href="https://github.com/facebookresearch/sam2"&gt;SAM 2&lt;/a&gt;, SAM 3 introduces the ability to exhaustively segment all instances of an open-vocabulary concept specified by a short text phrase or exemplars. Unlike prior work, SAM 3 can handle a vastly larger set of open-vocabulary prompts. It achieves 75-80% of human performance on our new &lt;a href="https://github.com/facebookresearch/sam3?tab=readme-ov-file#sa-co-dataset"&gt;SA-CO benchmark&lt;/a&gt; which contains 270K unique concepts, over 50 times more than existing benchmarks.&lt;/p&gt; 
&lt;p&gt;This breakthrough is driven by an innovative data engine that has automatically annotated over 4 million unique concepts, creating the largest high-quality open-vocabulary segmentation dataset to date. In addition, SAM 3 introduces a new model architecture featuring a presence token that improves discrimination between closely related text prompts (e.g., “a player in white” vs. “a player in red”), as well as a decoupled detector–tracker design that minimizes task interference and scales efficiently with data.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/facebookresearch/sam3/main/assets/dog.gif" width="380" /&gt; &lt;img src="https://raw.githubusercontent.com/facebookresearch/sam3/main/assets/player.gif" width="380" /&gt; &lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.12 or higher&lt;/li&gt; 
 &lt;li&gt;PyTorch 2.7 or higher&lt;/li&gt; 
 &lt;li&gt;CUDA-compatible GPU with CUDA 12.6 or higher&lt;/li&gt; 
&lt;/ul&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Create a new Conda environment:&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;conda create -n sam3 python=3.12
conda deactivate
conda activate sam3
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;&lt;strong&gt;Install PyTorch with CUDA support:&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install torch==2.7.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;&lt;strong&gt;Clone the repository and install the package:&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/facebookresearch/sam3.git
cd sam3
pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="4"&gt; 
 &lt;li&gt;&lt;strong&gt;Install additional dependencies for example notebooks or development:&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# For running example notebooks
pip install -e ".[notebooks]"

# For development
pip install -e ".[train,dev]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;⚠️ Before using SAM 3, please request access to the checkpoints on the SAM 3 Hugging Face &lt;a href="https://huggingface.co/facebook/sam3"&gt;repo&lt;/a&gt;. Once accepted, you need to be authenticated to download the checkpoints. You can do this by running the following &lt;a href="https://huggingface.co/docs/huggingface_hub/en/quick-start#authentication"&gt;steps&lt;/a&gt; (e.g. &lt;code&gt;hf auth login&lt;/code&gt; after generating an access token.)&lt;/p&gt; 
&lt;h3&gt;Basic Usage&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import torch
#################################### For Image ####################################
from PIL import Image
from sam3.model_builder import build_sam3_image_model
from sam3.model.sam3_image_processor import Sam3Processor
# Load the model
model = build_sam3_image_model()
processor = Sam3Processor(model)
# Load an image
image = Image.open("&amp;lt;YOUR_IMAGE_PATH.jpg&amp;gt;")
inference_state = processor.set_image(image)
# Prompt the model with text
output = processor.set_text_prompt(state=inference_state, prompt="&amp;lt;YOUR_TEXT_PROMPT&amp;gt;")

# Get the masks, bounding boxes, and scores
masks, boxes, scores = output["masks"], output["boxes"], output["scores"]

#################################### For Video ####################################

from sam3.model_builder import build_sam3_video_predictor

video_predictor = build_sam3_video_predictor()
video_path = "&amp;lt;YOUR_VIDEO_PATH&amp;gt;" # a JPEG folder or an MP4 video file
# Start a session
response = video_predictor.handle_request(
    request=dict(
        type="start_session",
        resource_path=video_path,
    )
)
response = video_predictor.handle_request(
    request=dict(
        type="add_prompt",
        session_id=response["session_id"],
        frame_index=0, # Arbitrary frame index
        text="&amp;lt;YOUR_TEXT_PROMPT&amp;gt;",
    )
)
output = response["outputs"]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;p&gt;The &lt;code&gt;examples&lt;/code&gt; directory contains notebooks demonstrating how to use SAM3 with various types of prompts:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/facebookresearch/sam3/main/examples/sam3_image_predictor_example.ipynb"&gt;&lt;code&gt;sam3_image_predictor_example.ipynb&lt;/code&gt;&lt;/a&gt; : Demonstrates how to prompt SAM 3 with text and visual box prompts on images.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/facebookresearch/sam3/main/examples/sam3_video_predictor_example.ipynb"&gt;&lt;code&gt;sam3_video_predictor_example.ipynb&lt;/code&gt;&lt;/a&gt; : Demonstrates how to prompt SAM 3 with text prompts on videos, and doing further interactive refinements with points.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/facebookresearch/sam3/main/examples/sam3_image_batched_inference.ipynb"&gt;&lt;code&gt;sam3_image_batched_inference.ipynb&lt;/code&gt;&lt;/a&gt; : Demonstrates how to run batched inference with SAM 3 on images.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/facebookresearch/sam3/main/examples/sam3_agent.ipynb"&gt;&lt;code&gt;sam3_agent.ipynb&lt;/code&gt;&lt;/a&gt;: Demonsterates the use of SAM 3 Agent to segment complex text prompt on images.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/facebookresearch/sam3/main/examples/saco_gold_silver_vis_example.ipynb"&gt;&lt;code&gt;saco_gold_silver_vis_example.ipynb&lt;/code&gt;&lt;/a&gt; : Shows a few examples from SA-Co image evaluation set.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/facebookresearch/sam3/main/examples/saco_veval_vis_example.ipynb"&gt;&lt;code&gt;saco_veval_vis_example.ipynb&lt;/code&gt;&lt;/a&gt; : Shows a few examples from SA-Co video evaluation set.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;There are additional notebooks in the examples directory that demonstrate how to use SAM 3 for interactive instance segmentation in images and videos (SAM 1/2 tasks), or as a tool for an MLLM, and how to run evaluations on the SA-Co dataset.&lt;/p&gt; 
&lt;p&gt;To run the Jupyter notebook examples:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Make sure you have the notebooks dependencies installed
pip install -e ".[notebooks]"

# Start Jupyter notebook
jupyter notebook examples/sam3_image_predictor_example.ipynb
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Model&lt;/h2&gt; 
&lt;p&gt;SAM 3 consists of a detector and a tracker that share a vision encoder. It has 848M parameters. The detector is a DETR-based model conditioned on text, geometry, and image exemplars. The tracker inherits the SAM 2 transformer encoder-decoder architecture, supporting video segmentation and interactive refinement.&lt;/p&gt; 
&lt;h2&gt;Image Results&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;table style="min-width: 80%; border: 2px solid #ddd; border-collapse: collapse"&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th rowspan="3" style="border-right: 2px solid #ddd; padding: 12px 20px"&gt;Model&lt;/th&gt; 
    &lt;th colspan="3" style="text-align: center; border-right: 2px solid #ddd; padding: 12px 20px"&gt;Instance Segmentation&lt;/th&gt; 
    &lt;th colspan="5" style="text-align: center; padding: 12px 20px"&gt;Box Detection&lt;/th&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;th colspan="2" style="text-align: center; border-right: 1px solid #eee; padding: 12px 20px"&gt;LVIS&lt;/th&gt; 
    &lt;th style="text-align: center; border-right: 2px solid #ddd; padding: 12px 20px"&gt;SA-Co/Gold&lt;/th&gt; 
    &lt;th colspan="2" style="text-align: center; border-right: 1px solid #eee; padding: 12px 20px"&gt;LVIS&lt;/th&gt; 
    &lt;th colspan="2" style="text-align: center; border-right: 1px solid #eee; padding: 12px 20px"&gt;COCO&lt;/th&gt; 
    &lt;th style="text-align: center; padding: 12px 20px"&gt;SA-Co/Gold&lt;/th&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;th style="text-align: center; padding: 12px 20px"&gt;cgF1&lt;/th&gt; 
    &lt;th style="text-align: center; border-right: 1px solid #eee; padding: 12px 20px"&gt;AP&lt;/th&gt; 
    &lt;th style="text-align: center; border-right: 2px solid #ddd; padding: 12px 20px"&gt;cgF1&lt;/th&gt; 
    &lt;th style="text-align: center; padding: 12px 20px"&gt;cgF1&lt;/th&gt; 
    &lt;th style="text-align: center; border-right: 1px solid #eee; padding: 12px 20px"&gt;AP&lt;/th&gt; 
    &lt;th style="text-align: center; padding: 12px 20px"&gt;AP&lt;/th&gt; 
    &lt;th style="text-align: center; border-right: 1px solid #eee; padding: 12px 20px"&gt;AP&lt;sub&gt;o&lt;/sub&gt; &lt;/th&gt; 
    &lt;th style="text-align: center; padding: 12px 20px"&gt;cgF1&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td style="border-right: 2px solid #ddd; padding: 10px 20px"&gt;Human&lt;/td&gt; 
    &lt;td style="text-align: center; padding: 10px 20px"&gt;-&lt;/td&gt; 
    &lt;td style="text-align: center; border-right: 1px solid #eee; padding: 10px 20px"&gt;-&lt;/td&gt; 
    &lt;td style="text-align: center; border-right: 2px solid #ddd; padding: 10px 20px"&gt;72.8&lt;/td&gt; 
    &lt;td style="text-align: center; padding: 10px 20px"&gt;-&lt;/td&gt; 
    &lt;td style="text-align: center; border-right: 1px solid #eee; padding: 10px 20px"&gt;-&lt;/td&gt; 
    &lt;td style="text-align: center; padding: 10px 20px"&gt;-&lt;/td&gt; 
    &lt;td style="text-align: center; border-right: 1px solid #eee; padding: 10px 20px"&gt;-&lt;/td&gt; 
    &lt;td style="text-align: center; padding: 10px 20px"&gt;74.0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td style="border-right: 2px solid #ddd; padding: 10px 20px"&gt;OWLv2*&lt;/td&gt; 
    &lt;td style="text-align: center; padding: 10px 20px; color: #999"&gt;29.3&lt;/td&gt; 
    &lt;td style="text-align: center; border-right: 1px solid #eee; padding: 10px 20px; color: #999"&gt;43.4&lt;/td&gt; 
    &lt;td style="text-align: center; border-right: 2px solid #ddd; padding: 10px 20px"&gt;24.6&lt;/td&gt; 
    &lt;td style="text-align: center; padding: 10px 20px; color: #999"&gt;30.2&lt;/td&gt; 
    &lt;td style="text-align: center; border-right: 1px solid #eee; padding: 10px 20px; color: #999"&gt;45.5&lt;/td&gt; 
    &lt;td style="text-align: center; padding: 10px 20px"&gt;46.1&lt;/td&gt; 
    &lt;td style="text-align: center; border-right: 1px solid #eee; padding: 10px 20px"&gt;23.9&lt;/td&gt; 
    &lt;td style="text-align: center; padding: 10px 20px"&gt;24.5&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td style="border-right: 2px solid #ddd; padding: 10px 20px"&gt;DINO-X&lt;/td&gt; 
    &lt;td style="text-align: center; padding: 10px 20px"&gt;-&lt;/td&gt; 
    &lt;td style="text-align: center; border-right: 1px solid #eee; padding: 10px 20px"&gt;38.5&lt;/td&gt; 
    &lt;td style="text-align: center; border-right: 2px solid #ddd; padding: 10px 20px"&gt;21.3&lt;/td&gt; 
    &lt;td style="text-align: center; padding: 10px 20px"&gt;-&lt;/td&gt; 
    &lt;td style="text-align: center; border-right: 1px solid #eee; padding: 10px 20px"&gt;52.4&lt;/td&gt; 
    &lt;td style="text-align: center; padding: 10px 20px"&gt;56.0&lt;/td&gt; 
    &lt;td style="text-align: center; border-right: 1px solid #eee; padding: 10px 20px"&gt;-&lt;/td&gt; 
    &lt;td style="text-align: center; padding: 10px 20px"&gt;22.5&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td style="border-right: 2px solid #ddd; padding: 10px 20px"&gt;Gemini 2.5&lt;/td&gt; 
    &lt;td style="text-align: center; padding: 10px 20px"&gt;13.4&lt;/td&gt; 
    &lt;td style="text-align: center; border-right: 1px solid #eee; padding: 10px 20px"&gt;-&lt;/td&gt; 
    &lt;td style="text-align: center; border-right: 2px solid #ddd; padding: 10px 20px"&gt;13.0&lt;/td&gt; 
    &lt;td style="text-align: center; padding: 10px 20px"&gt;16.1&lt;/td&gt; 
    &lt;td style="text-align: center; border-right: 1px solid #eee; padding: 10px 20px"&gt;-&lt;/td&gt; 
    &lt;td style="text-align: center; padding: 10px 20px"&gt;-&lt;/td&gt; 
    &lt;td style="text-align: center; border-right: 1px solid #eee; padding: 10px 20px"&gt;-&lt;/td&gt; 
    &lt;td style="text-align: center; padding: 10px 20px"&gt;14.4&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr style="border-top: 2px solid #b19c9cff"&gt; 
    &lt;td style="border-right: 2px solid #ddd; padding: 10px 20px"&gt;SAM 3&lt;/td&gt; 
    &lt;td style="text-align: center; padding: 10px 20px"&gt;37.2&lt;/td&gt; 
    &lt;td style="text-align: center; border-right: 1px solid #eee; padding: 10px 20px"&gt;48.5&lt;/td&gt; 
    &lt;td style="text-align: center; border-right: 2px solid #ddd; padding: 10px 20px"&gt;54.1&lt;/td&gt; 
    &lt;td style="text-align: center; padding: 10px 20px"&gt;40.6&lt;/td&gt; 
    &lt;td style="text-align: center; border-right: 1px solid #eee; padding: 10px 20px"&gt;53.6&lt;/td&gt; 
    &lt;td style="text-align: center; padding: 10px 20px"&gt;56.4&lt;/td&gt; 
    &lt;td style="text-align: center; border-right: 1px solid #eee; padding: 10px 20px"&gt;55.7&lt;/td&gt; 
    &lt;td style="text-align: center; padding: 10px 20px"&gt;55.7&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;p style="text-align: center; margin-top: 10px; font-size: 0.9em; color: #ddd;"&gt;* Partially trained on LVIS, AP&lt;sub&gt;o&lt;/sub&gt; refers to COCO-O accuracy&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;Video Results&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;table style="min-width: 80%; border: 2px solid #ddd; border-collapse: collapse"&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th rowspan="2" style="border-right: 2px solid #ddd; padding: 12px 20px"&gt;Model&lt;/th&gt; 
    &lt;th colspan="2" style="text-align: center; border-right: 1px solid #eee; padding: 12px 20px"&gt;SA-V test&lt;/th&gt; 
    &lt;th colspan="2" style="text-align: center; border-right: 1px solid #eee; padding: 12px 20px"&gt;YT-Temporal-1B test&lt;/th&gt; 
    &lt;th colspan="2" style="text-align: center; border-right: 1px solid #eee; padding: 12px 20px"&gt;SmartGlasses test&lt;/th&gt; 
    &lt;th style="text-align: center; border-right: 1px solid #eee; padding: 12px 20px"&gt;LVVIS test&lt;/th&gt; 
    &lt;th style="text-align: center; padding: 12px 20px"&gt;BURST test&lt;/th&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;th style="text-align: center; padding: 12px 20px"&gt;cgF1&lt;/th&gt; 
    &lt;th style="text-align: center; border-right: 1px solid #eee; padding: 12px 20px"&gt;pHOTA&lt;/th&gt; 
    &lt;th style="text-align: center; padding: 12px 20px"&gt;cgF1&lt;/th&gt; 
    &lt;th style="text-align: center; border-right: 1px solid #eee; padding: 12px 20px"&gt;pHOTA&lt;/th&gt; 
    &lt;th style="text-align: center; padding: 12px 20px"&gt;cgF1&lt;/th&gt; 
    &lt;th style="text-align: center; border-right: 1px solid #eee; padding: 12px 20px"&gt;pHOTA&lt;/th&gt; 
    &lt;th style="text-align: center; border-right: 1px solid #eee; padding: 12px 20px"&gt;mAP&lt;/th&gt; 
    &lt;th style="text-align: center; padding: 12px 20px"&gt;HOTA&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td style="border-right: 2px solid #ddd; padding: 10px 20px"&gt;Human&lt;/td&gt; 
    &lt;td style="text-align: center; padding: 10px 20px"&gt;53.1&lt;/td&gt; 
    &lt;td style="text-align: center; border-right: 1px solid #eee; padding: 10px 20px"&gt;70.5&lt;/td&gt; 
    &lt;td style="text-align: center; padding: 10px 20px"&gt;71.2&lt;/td&gt; 
    &lt;td style="text-align: center; border-right: 1px solid #eee; padding: 10px 20px"&gt;78.4&lt;/td&gt; 
    &lt;td style="text-align: center; padding: 10px 20px"&gt;58.5&lt;/td&gt; 
    &lt;td style="text-align: center; border-right: 1px solid #eee; padding: 10px 20px"&gt;72.3&lt;/td&gt; 
    &lt;td style="text-align: center; border-right: 1px solid #eee; padding: 10px 20px"&gt;-&lt;/td&gt; 
    &lt;td style="text-align: center; padding: 10px 20px"&gt;-&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr style="border-top: 2px solid #b19c9cff"&gt; 
    &lt;td style="border-right: 2px solid #ddd; padding: 10px 20px"&gt;SAM 3&lt;/td&gt; 
    &lt;td style="text-align: center; padding: 10px 20px"&gt;30.3&lt;/td&gt; 
    &lt;td style="text-align: center; border-right: 1px solid #eee; padding: 10px 20px"&gt;58.0&lt;/td&gt; 
    &lt;td style="text-align: center; padding: 10px 20px"&gt;50.8&lt;/td&gt; 
    &lt;td style="text-align: center; border-right: 1px solid #eee; padding: 10px 20px"&gt;69.9&lt;/td&gt; 
    &lt;td style="text-align: center; padding: 10px 20px"&gt;36.4&lt;/td&gt; 
    &lt;td style="text-align: center; border-right: 1px solid #eee; padding: 10px 20px"&gt;63.6&lt;/td&gt; 
    &lt;td style="text-align: center; border-right: 1px solid #eee; padding: 10px 20px"&gt;36.3&lt;/td&gt; 
    &lt;td style="text-align: center; padding: 10px 20px"&gt;44.5&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h2&gt;SA-Co Dataset&lt;/h2&gt; 
&lt;p&gt;We release 2 image benchmarks, &lt;a href="https://raw.githubusercontent.com/facebookresearch/sam3/main/scripts/eval/gold/README.md"&gt;SA-Co/Gold&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/facebookresearch/sam3/main/scripts/eval/silver/README.md"&gt;SA-Co/Silver&lt;/a&gt;, and a video benchmark &lt;a href="https://raw.githubusercontent.com/facebookresearch/sam3/main/scripts/eval/veval/README.md"&gt;SA-Co/VEval&lt;/a&gt;. The datasets contain images (or videos) with annotated noun phrases. Each image/video and noun phrase pair is annotated with instance masks and unique IDs of each object matching the phrase. Phrases that have no matching objects (negative prompts) have no masks, shown in red font in the figure. See the linked READMEs for more details on how to download and run evaluations on the datasets.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;HuggingFace host: &lt;a href="https://huggingface.co/datasets/facebook/SACo-Gold"&gt;SA-Co/Gold&lt;/a&gt;, &lt;a href="https://huggingface.co/datasets/facebook/SACo-Silver"&gt;SA-Co/Silver&lt;/a&gt; and &lt;a href="https://huggingface.co/datasets/facebook/SACo-VEval"&gt;SA-Co/VEval&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Roboflow host: &lt;a href="https://universe.roboflow.com/sa-co-gold"&gt;SA-Co/Gold&lt;/a&gt;, &lt;a href="https://universe.roboflow.com/sa-co-silver"&gt;SA-Co/Silver&lt;/a&gt; and &lt;a href="https://universe.roboflow.com/sa-co-veval"&gt;SA-Co/VEval&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/facebookresearch/sam3/main/assets/sa_co_dataset.jpg?raw=true" alt="SA-Co dataset" /&gt;&lt;/p&gt; 
&lt;h2&gt;Development&lt;/h2&gt; 
&lt;p&gt;To set up the development environment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -e ".[dev,train]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To format the code:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;ufmt format .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/facebookresearch/sam3/main/CONTRIBUTING.md"&gt;contributing&lt;/a&gt; and the &lt;a href="https://raw.githubusercontent.com/facebookresearch/sam3/main/CODE_OF_CONDUCT.md"&gt;code of conduct&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the SAM License - see the &lt;a href="https://raw.githubusercontent.com/facebookresearch/sam3/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;p&gt;We would like to thank the following people for their contributions to the SAM 3 project: Alex He, Alexander Kirillov, Alyssa Newcomb, Ana Paula Kirschner Mofarrej, Andrea Madotto, Andrew Westbury, Ashley Gabriel, Azita Shokpour, Ben Samples, Bernie Huang, Carleigh Wood, Ching-Feng Yeh, Christian Puhrsch, Claudette Ward, Daniel Bolya, Daniel Li, Facundo Figueroa, Fazila Vhora, George Orlin, Hanzi Mao, Helen Klein, Hu Xu, Ida Cheng, Jake Kinney, Jiale Zhi, Jo Sampaio, Joel Schlosser, Justin Johnson, Kai Brown, Karen Bergan, Karla Martucci, Kenny Lehmann, Maddie Mintz, Mallika Malhotra, Matt Ward, Michelle Chan, Michelle Restrepo, Miranda Hartley, Muhammad Maaz, Nisha Deo, Peter Park, Phillip Thomas, Raghu Nayani, Rene Martinez Doehner, Robbie Adkins, Ross Girshik, Sasha Mitts, Shashank Jain, Spencer Whitehead, Ty Toledano, Valentin Gabeur, Vincent Cho, Vivian Lee, William Ngan, Xuehai He, Yael Yungster, Ziqi Pang, Ziyi Dou, Zoe Quake.&lt;/p&gt; 
&lt;h2&gt;Citing SAM 3&lt;/h2&gt; 
&lt;p&gt;If you use SAM 3 or the SA-Co dataset in your research, please use the following BibTeX entry.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{carion2025sam3segmentconcepts,
      title={SAM 3: Segment Anything with Concepts},
      author={Nicolas Carion and Laura Gustafson and Yuan-Ting Hu and Shoubhik Debnath and Ronghang Hu and Didac Suris and Chaitanya Ryali and Kalyan Vasudev Alwala and Haitham Khedr and Andrew Huang and Jie Lei and Tengyu Ma and Baishan Guo and Arpit Kalla and Markus Marks and Joseph Greer and Meng Wang and Peize Sun and Roman Rädle and Triantafyllos Afouras and Effrosyni Mavroudi and Katherine Xu and Tsung-Han Wu and Yu Zhou and Liliane Momeni and Rishi Hazra and Shuangrui Ding and Sagar Vaze and Francois Porcher and Feng Li and Siyuan Li and Aishwarya Kamath and Ho Kei Cheng and Piotr Dollár and Nikhila Ravi and Kate Saenko and Pengchuan Zhang and Christoph Feichtenhofer},
      year={2025},
      eprint={2511.16719},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2511.16719},
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>microsoft/magentic-ui</title>
      <link>https://github.com/microsoft/magentic-ui</link>
      <description>&lt;p&gt;A research prototype of a human-centered web agent&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/microsoft/magentic-ui/main/docs/img/magui-readme-logo.svg?sanitize=true" alt="Magentic-UI Logo" /&gt; 
 &lt;p&gt;&lt;em&gt;Automate your web tasks while you stay in control&lt;/em&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://pypi.python.org/pypi/magentic_ui"&gt;&lt;img src="https://img.shields.io/pypi/v/magentic_ui.svg?sanitize=true" alt="image" /&gt;&lt;/a&gt; &lt;a href="https://pypi.python.org/pypi/magentic_ui"&gt;&lt;img src="https://img.shields.io/pypi/l/magentic_ui.svg?sanitize=true" alt="image" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/badge/python-3.10%20%7C%203.11%20%7C%203.12%20%7C%203.13-blue" alt="Python Versions" /&gt; &lt;a href="https://arxiv.org/abs/2507.22358"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2507.22358-b31b1b.svg?sanitize=true" alt="arXiv" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;p&gt;Magentic-UI is a &lt;strong&gt;research prototype&lt;/strong&gt; human-centered AI agent that solves complex web and coding tasks that may require monitoring. Unlike other black-box agents, the system reveals its plan before executions, lets you guide its actions, and requests approval for sensitive operations while browsing websites, executing code, and analyzing files. &lt;em&gt;Check out the &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/#demos"&gt;demo section&lt;/a&gt; for inspiration on what tasks you can accomplish.&lt;/em&gt;&lt;/p&gt; 
&lt;h2&gt;✨ What's New&lt;/h2&gt; 
&lt;p&gt;Microsoft latest agentic model &lt;a href="https://www.microsoft.com/en-us/research/blog/fara-7b-an-efficient-agentic-model-for-computer-use/"&gt;Fara-7B&lt;/a&gt; is now integrated in Magentic-UI, read how to launch in &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/#fara-7b"&gt; Fara-7B guide&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;"Tell me When"&lt;/strong&gt;: Automate monitoring tasks and repeatable workflows that require web or API access that span minutes to days. &lt;em&gt;Learn more &lt;a href="https://www.microsoft.com/en-us/research/blog/tell-me-when-building-agents-that-can-wait-monitor-and-act/"&gt;here&lt;/a&gt;.&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;File Upload Support&lt;/strong&gt;: Upload any file through the UI for analysis or modification&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MCP Agents&lt;/strong&gt;: Extend capabilities with your favorite MCP servers&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Easier Installation&lt;/strong&gt;: We have uploaded our docker containers to GHCR so you no longer need to build any containers! Installation time now is much quicker.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🚀 Quick Start&lt;/h2&gt; 
&lt;p&gt;Here's how you can get started with Magentic-UI:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 1. Setup environment
python3 -m venv .venv
source .venv/bin/activate
pip install magentic-ui --upgrade

# 2. Set your API key
export OPENAI_API_KEY="your-api-key-here"

# 3. Launch Magentic-UI
magentic-ui --port 8081
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then open &lt;a href="http://localhost:8081"&gt;http://localhost:8081&lt;/a&gt; in your browser to interact with Magentic-UI!&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Prerequisites&lt;/strong&gt;: Requires Docker and Python 3.10+. Windows users should use WSL2. See &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/#%EF%B8%8F-installation"&gt;detailed installation&lt;/a&gt; for more info.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Alternative Usage Options&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Without Docker&lt;/strong&gt; (limited functionality: no code execution):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;magentic-ui --run-without-docker --port 8081
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Command Line Interface&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;magentic-cli --work-dir PATH/TO/STORE/DATA
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Custom LLM Clients&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Azure
pip install magentic-ui[azure]

# Ollama (local models)
pip install magentic-ui[ollama]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can then pass a config file to the &lt;code&gt;magentic-ui&lt;/code&gt; command (&lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/#model-client-configuration"&gt; client config&lt;/a&gt;) or change the model client inside the UI settings.&lt;/p&gt; 
&lt;p&gt;For further details on installation please read the &lt;a href="#️-installation"&gt;🛠️ Installation&lt;/a&gt; section. For common installation issues and their solutions, please refer to the &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/TROUBLESHOOTING.md"&gt;troubleshooting document&lt;/a&gt;. See advanced usage instructions with the command &lt;code&gt;magentic-ui --help&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Quick Navigation:&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/#demos"&gt;🎬 Demos&lt;/a&gt; &amp;nbsp;|&amp;nbsp; &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/#how-it-works"&gt;🟪 How it Works&lt;/a&gt; &amp;nbsp;|&amp;nbsp; &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/#installation"&gt;🛠️ Installation&lt;/a&gt; &amp;nbsp;|&amp;nbsp; &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/#troubleshooting"&gt;⚠️ Troubleshooting&lt;/a&gt; &amp;nbsp;|&amp;nbsp; &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/#contributing"&gt;🤝 Contributing&lt;/a&gt; &amp;nbsp;|&amp;nbsp; &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/#license"&gt;📄 License&lt;/a&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Demos&lt;/h2&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td width="33%" align="center"&gt; &lt;p&gt;&lt;strong&gt;🍕 Pizza Ordering&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;Web automation with human-in-the-loop&lt;/em&gt;&lt;/p&gt; 
    &lt;video src="https://github.com/user-attachments/assets/dc95cf5f-c4b4-4fe0-b708-158ff071e5a9" width="100%" style="max-height: 300px;"&gt; 
    &lt;/video&gt; &lt;/td&gt; 
   &lt;td width="33%" align="center"&gt; &lt;p&gt;&lt;strong&gt;🏠 Airbnb Price Analysis&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;MCP agent integration&lt;/em&gt;&lt;/p&gt; 
    &lt;video src="https://github.com/user-attachments/assets/c19ed8c2-e06f-43b7-bee3-5e2ffc4c5e02" width="100%" style="max-height: 300px;"&gt; 
    &lt;/video&gt; &lt;/td&gt; 
   &lt;td width="33%" align="center"&gt; &lt;p&gt;&lt;strong&gt;⭐ Star Monitoring&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;Long-running monitoring task&lt;/em&gt;&lt;/p&gt; 
    &lt;video src="https://github.com/user-attachments/assets/d2a463ca-7a94-4414-932d-a69f30fff63b" width="100%" style="max-height: 300px;"&gt; 
    &lt;/video&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;How it Works&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/magentic-ui/main/docs/img/magenticui_running.png" alt="Magentic-UI" height="400" /&gt; &lt;/p&gt; 
&lt;p&gt;Magentic-UI is especially useful for web tasks that require actions on the web (e.g., filling a form, customizing a food order), deep navigation through websites not indexed by search engines (e.g., filtering flights, finding a link from a personal site) or tasks that need web navigation and code execution (e.g., generate a chart from online data).&lt;/p&gt; 
&lt;p&gt;What differentiates Magentic-UI from other browser use offerings is its transparent and controllable interface that allows for efficient human-in-the-loop involvement. Magentic-UI is built using &lt;a href="https://github.com/microsoft/autogen"&gt;AutoGen&lt;/a&gt; and provides a platform to study human-agent interaction and experiment with web agents. Key features include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;🧑‍🤝‍🧑 &lt;strong&gt;Co-Planning&lt;/strong&gt;: Collaboratively create and approve step-by-step plans using chat and the plan editor.&lt;/li&gt; 
 &lt;li&gt;🤝 &lt;strong&gt;Co-Tasking&lt;/strong&gt;: Interrupt and guide the task execution using the web browser directly or through chat. Magentic-UI can also ask for clarifications and help when needed.&lt;/li&gt; 
 &lt;li&gt;🛡️ &lt;strong&gt;Action Guards&lt;/strong&gt;: Sensitive actions are only executed with explicit user approvals.&lt;/li&gt; 
 &lt;li&gt;🧠 &lt;strong&gt;Plan Learning and Retrieval&lt;/strong&gt;: Learn from previous runs to improve future task automation and save them in a plan gallery. Automatically or manually retrieve saved plans in future tasks.&lt;/li&gt; 
 &lt;li&gt;🔀 &lt;strong&gt;Parallel Task Execution&lt;/strong&gt;: You can run multiple tasks in parallel and session status indicators will let you know when Magentic-UI needs your input or has completed the task.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://www.youtube.com/watch?v=wOs-5SR8xOc" target="_blank"&gt; &lt;img src="https://img.youtube.com/vi/wOs-5SR8xOc/maxresdefault.jpg" alt="Watch the demo video" width="600" /&gt; &lt;/a&gt; 
 &lt;br /&gt; ▶️ 
 &lt;em&gt; Click to watch a video and learn more about Magentic-UI &lt;/em&gt; 
&lt;/div&gt; 
&lt;h3&gt;Autonomous Evaluation&lt;/h3&gt; 
&lt;p&gt;To evaluate its autonomous capabilities, Magentic-UI has been tested against several benchmarks when running with o4-mini: &lt;a href="https://huggingface.co/datasets/gaia-benchmark/GAIA"&gt;GAIA&lt;/a&gt; test set (42.52%), which assesses general AI assistants across reasoning, tool use, and web interaction tasks ; &lt;a href="https://huggingface.co/AssistantBench"&gt;AssistantBench&lt;/a&gt; test set (27.60%), focusing on realistic, time-consuming web tasks; &lt;a href="https://github.com/MinorJerry/WebVoyager"&gt;WebVoyager&lt;/a&gt; (82.2%), measuring end-to-end web navigation in real-world scenarios; and &lt;a href="https://webgames.convergence.ai/"&gt;WebGames&lt;/a&gt; (45.5%), evaluating general-purpose web-browsing agents through interactive challenges. To reproduce these experimental results, please see the following &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/experiments/eval/README.md"&gt;instructions&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you're interested in reading more checkout our &lt;a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/magentic-ui-report.pdf"&gt;technical report&lt;/a&gt; and &lt;a href="https://www.microsoft.com/en-us/research/blog/magentic-ui-an-experimental-human-centered-web-agent/"&gt;blog post&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;Pre-Requisites&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: If you're using Windows, we highly recommend using &lt;a href="https://docs.microsoft.com/en-us/windows/wsl/install"&gt;WSL2&lt;/a&gt; (Windows Subsystem for Linux).&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;If running on &lt;strong&gt;Windows&lt;/strong&gt; or &lt;strong&gt;Mac&lt;/strong&gt; you should use &lt;a href="https://www.docker.com/products/docker-desktop/"&gt;Docker Desktop&lt;/a&gt; or if inside WSL2 you can install Docker directly inside WSL &lt;a href="https://gist.github.com/dehsilvadeveloper/c3bdf0f4cdcc5c177e2fe9be671820c7"&gt;docker in WSL2 guide&lt;/a&gt;. If running on &lt;strong&gt;Linux&lt;/strong&gt;, you should use &lt;a href="https://docs.docker.com/engine/install/"&gt;Docker Engine&lt;/a&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;If using Docker Desktop, make sure it is set up to use WSL2: - Go to Settings &amp;gt; Resources &amp;gt; WSL Integration - Enable integration with your development distro You can find more detailed instructions about this step &lt;a href="https://docs.microsoft.com/en-us/windows/wsl/tutorials/wsl-containers"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt; &lt;p&gt;During the Installation step, you will need to set up your &lt;code&gt;OPENAI_API_KEY&lt;/code&gt;. To use other models, review the &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/#model-client-configuration"&gt;Model Client Configuration&lt;/a&gt; section below.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;You need at least &lt;a href="https://www.python.org/downloads/"&gt;Python 3.10&lt;/a&gt; installed.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;If you are on Windows, we recommend to run Magentic-UI inside &lt;a href="https://docs.microsoft.com/en-us/windows/wsl/install"&gt;WSL2&lt;/a&gt; (Windows Subsystem for Linux) for correct Docker and file path compatibility.&lt;/p&gt; 
&lt;h3&gt;PyPI Installation&lt;/h3&gt; 
&lt;p&gt;Magentic-UI is available on PyPI. We recommend using a virtual environment to avoid conflicts with other packages.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python3 -m venv .venv
source .venv/bin/activate
pip install magentic-ui
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, if you use &lt;a href="https://docs.astral.sh/uv/getting-started/installation/"&gt;&lt;code&gt;uv&lt;/code&gt;&lt;/a&gt; for dependency management, you can install Magentic-UI with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv venv --python=3.12 .venv
. .venv/bin/activate
uv pip install magentic-ui
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Running Magentic-UI&lt;/h3&gt; 
&lt;p&gt;To run Magentic-UI, make sure that Docker is running, then run the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;magentic-ui --port 8081
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Running this command for the first time will pull two docker images required for the Magentic-UI agents. If you encounter problems, you can build them directly with the following command:&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd docker
sh build-all.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you face issues with Docker, please refer to the &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/TROUBLESHOOTING.md"&gt;TROUBLESHOOTING.md&lt;/a&gt; document.&lt;/p&gt; 
&lt;p&gt;Once the server is running, you can access the UI at &lt;a href="http://localhost:8081"&gt;http://localhost:8081&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Fara-7B&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;First install magentic-ui with the fara extras:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python3 -m venv .venv
source .venv/bin/activate
pip install magentic-ui[fara]
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;In a seperate process, serve the Fara-7B model using vLLM:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;vllm serve "microsoft/Fara-7B" --port 5000 --dtype auto 
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;First create a &lt;code&gt;fara_config.yaml&lt;/code&gt; file with the following content:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;model_config_local_surfer: &amp;amp;client_surfer
  provider: OpenAIChatCompletionClient
  config:
    model: "microsoft/Fara-7B"
    base_url: http://localhost:5000/v1
    api_key: not-needed
    model_info:
      vision: true
      function_calling: true
      json_output: false
      family: "unknown" 
      structured_output: false
      multiple_system_messages: false

orchestrator_client: *client_surfer
coder_client: *client_surfer
web_surfer_client: *client_surfer
file_surfer_client: *client_surfer
action_guard_client: *client_surfer
model_client: *client_surfer
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note: if you are hosting vLLM on a different port or host, change the &lt;code&gt;base_url&lt;/code&gt; accordingly.&lt;/p&gt; 
&lt;p&gt;Then launch Magentic-UI with the fara agent:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;magentic-ui --fara --port 8081 --config fara_config.yaml 
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Finally, navigate to &lt;a href="http://localhost:8081"&gt;http://localhost:8081&lt;/a&gt; to access the interface!&lt;/p&gt; 
&lt;h3&gt;Configuration&lt;/h3&gt; 
&lt;h4&gt;Model Client Configuration&lt;/h4&gt; 
&lt;p&gt;If you want to use a different OpenAI key, or if you want to configure use with Azure OpenAI or Ollama, you can do so inside the UI by navigating to settings (top right icon) and changing model configuration. Another option is to pass a yaml config file when you start Magentic-UI which will override any settings in the UI:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;magentic-ui --port 8081 --config config.yaml
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Where the &lt;code&gt;config.yaml&lt;/code&gt; should look as follows with an AutoGen model client configuration:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;gpt4o_client: &amp;amp;gpt4o_client
    provider: OpenAIChatCompletionClient
    config:
      model: gpt-4o-2024-08-06
      api_key: null
      base_url: null
      max_retries: 5

orchestrator_client: *gpt4o_client
coder_client: *gpt4o_client
web_surfer_client: *gpt4o_client
file_surfer_client: *gpt4o_client
action_guard_client: *gpt4o_client
plan_learning_client: *gpt4o_client
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can change the client for each of the agents using the config file and use AzureOpenAI (&lt;code&gt;AzureOpenAIChatCompletionClient&lt;/code&gt;), Ollama and other clients.&lt;/p&gt; 
&lt;h4&gt;MCP Server Configuration&lt;/h4&gt; 
&lt;p&gt;You can also extend Magentic-UI's capabilities by adding custom "McpAgents" to the multi-agent team. Each McpAgent can have access to one or more MCP Servers. You can specify these agents via the &lt;code&gt;mcp_agent_configs&lt;/code&gt; parameter in your &lt;code&gt;config.yaml&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;For example, here's an agent called "airbnb_surfer" that has access to the OpenBnb MCP Server running locally via Stdio.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;mcp_agent_configs:
  - name: airbnb_surfer
    description: "The airbnb_surfer has direct access to AirBnB."
    model_client: 
      provider: OpenAIChatCompletionClient
      config:
        model: gpt-4.1-2025-04-14
      max_retries: 10
    system_message: |-
      You are AirBnb Surfer, a helpful digital assistant that can help users acces AirBnB.

      You have access to a suite of tools provided by the AirBnB API. Use those tools to satisfy the users requests.
    reflect_on_tool_use: false
    mcp_servers:
      - server_name: AirBnB
        server_params:
          type: StdioServerParams
          command: npx
          args:
            - -y
            - "@openbnb/mcp-server-airbnb"
            - --ignore-robots-txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Under the hood, each &lt;code&gt;McpAgent&lt;/code&gt; is just a &lt;code&gt;autogen_agentchat.agents.AssistantAgent&lt;/code&gt; with the set of MCP Servers exposed as an &lt;code&gt;AggregateMcpWorkbench&lt;/code&gt; which is simply a named collection of &lt;code&gt;autogen_ext.tools.mcp.McpWorkbench&lt;/code&gt; objects (one per MCP Server).&lt;/p&gt; 
&lt;p&gt;Currently the supported MCP Server types are &lt;code&gt;autogen_ext.tools.mcp.StdioServerParams&lt;/code&gt; and &lt;code&gt;autogen_ext.tools.mcp.SseServerParams&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Building Magentic-UI from source&lt;/h3&gt; 
&lt;p&gt;This step is primarily for users seeking to make modifications to the code, are having trouble with the pypi installation or want the latest code before a pypi version release.&lt;/p&gt; 
&lt;h4&gt;1. Make sure the above prerequisites are installed, and that Docker is running.&lt;/h4&gt; 
&lt;h4&gt;2. Clone the repository to your local machine:&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/microsoft/magentic-ui.git
cd magentic-ui
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;3. Install Magentic-UI's dependencies with uv or your favorite package manager:&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# install uv through https://docs.astral.sh/uv/getting-started/installation/
uv venv --python=3.12 .venv
uv sync --all-extras
source .venv/bin/activate
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;4. Build the frontend:&lt;/h4&gt; 
&lt;p&gt;First make sure to install node:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# install nvm to install node
curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.1/install.sh | bash
nvm install node
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then install the frontend:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd frontend
npm install -g gatsby-cli
npm install --global yarn
yarn install
yarn build
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;5. Run Magentic-UI, as usual.&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;magentic-ui --port 8081
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Running the UI from source&lt;/h4&gt; 
&lt;p&gt;If you are making changes to the source code of the UI, you can run the frontend in development mode so that it will automatically update when you make changes for faster development.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Open a separate terminal and change directory to the frontend&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd frontend
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Create a &lt;code&gt;.env.development&lt;/code&gt; file.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cp .env.default .env.development
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Launch frontend server&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npm run start
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="4"&gt; 
 &lt;li&gt;Then run the UI:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;magentic-ui --port 8081
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The frontend from source will be available at &lt;a href="http://localhost:8000"&gt;http://localhost:8000&lt;/a&gt;, and the compiled frontend will be available at &lt;a href="http://localhost:8081"&gt;http://localhost:8081&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Troubleshooting&lt;/h2&gt; 
&lt;p&gt;If you were unable to get Magentic-UI running, do not worry! The first step is to make sure you have followed the steps outlined above, particularly with the &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/#pre-requisites"&gt;pre-requisites&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For common issues and their solutions, please refer to the &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/TROUBLESHOOTING.md"&gt;TROUBLESHOOTING.md&lt;/a&gt; file in this repository. If you do not see your problem there, please open a &lt;code&gt;GitHub Issue&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;This project welcomes contributions and suggestions. For information about contributing to Magentic-UI, please see our &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; guide, which includes current issues to be resolved and other forms of contributing.&lt;/p&gt; 
&lt;p&gt;This project has adopted the &lt;a href="https://opensource.microsoft.com/codeofconduct/"&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information, see the &lt;a href="https://opensource.microsoft.com/codeofconduct/faq/"&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href="mailto:opencode@microsoft.com"&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;Please cite our paper if you use our work in your research:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@article{mozannar2025magentic,
  title={Magentic-UI: Towards Human-in-the-loop Agentic Systems},
  author={Mozannar, Hussein and Bansal, Gagan and Tan, Cheng and Fourney, Adam and Dibia, Victor and Chen, Jingya and Gerrits, Jack and Payne, Tyler and Maldaner, Matheus Kunzler and Grunde-McLaughlin, Madeleine and others},
  journal={arXiv preprint arXiv:2507.22358},
  year={2025}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Microsoft, and any contributors, grant you a license to any code in the repository under the &lt;a href="https://opensource.org/licenses/MIT"&gt;MIT License&lt;/a&gt;. See the &lt;a href="https://raw.githubusercontent.com/microsoft/magentic-ui/main/LICENSE"&gt;LICENSE&lt;/a&gt; file.&lt;/p&gt; 
&lt;p&gt;Microsoft, Windows, Microsoft Azure, and/or other Microsoft products and services referenced in the documentation may be either trademarks or registered trademarks of Microsoft in the United States and/or other countries. The licenses for this project do not grant you rights to use any Microsoft names, logos, or trademarks. Microsoft's general trademark guidelines can be found at &lt;a href="http://go.microsoft.com/fwlink/?LinkID=254653"&gt;http://go.microsoft.com/fwlink/?LinkID=254653&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Any use of third-party trademarks or logos are subject to those third-party's policies.&lt;/p&gt; 
&lt;p&gt;Privacy information can be found at &lt;a href="https://go.microsoft.com/fwlink/?LinkId=521839"&gt;https://go.microsoft.com/fwlink/?LinkId=521839&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Microsoft and any contributors reserve all other rights, whether under their respective copyrights, patents, or trademarks, whether by implication, estoppel, or otherwise.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>GoogleCloudPlatform/agent-starter-pack</title>
      <link>https://github.com/GoogleCloudPlatform/agent-starter-pack</link>
      <description>&lt;p&gt;Ship AI Agents to Google Cloud in minutes, not months. Production-ready templates with built-in CI/CD, evaluation, and observability.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;🚀 Agent Starter Pack&lt;/h1&gt; 
&lt;p&gt;&lt;img src="https://img.shields.io/pypi/v/agent-starter-pack?color=blue" alt="Version" /&gt; &lt;a href="https://youtu.be/jHt-ZVD660g"&gt;&lt;img src="https://img.shields.io/badge/1--Minute%20Overview-gray" alt="1-Minute Video Overview" /&gt;&lt;/a&gt; &lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/"&gt;&lt;img src="https://img.shields.io/badge/Documentation-gray" alt="Docs" /&gt;&lt;/a&gt; &lt;a href="https://studio.firebase.google.com/new?template=https%3A%2F%2Fgithub.com%2FGoogleCloudPlatform%2Fagent-starter-pack%2Ftree%2Fmain%2Fagent_starter_pack%2Fresources%2Fidx"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="https://cdn.firebasestudio.dev/btn/try_light_20.svg" /&gt; 
   &lt;source media="(prefers-color-scheme: light)" srcset="https://cdn.firebasestudio.dev/btn/try_dark_20.svg" /&gt; 
   &lt;img height="20" alt="Try in Firebase Studio" src="https://cdn.firebasestudio.dev/btn/try_blue_20.svg?sanitize=true" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; &lt;a href="https://shell.cloud.google.com/cloudshell/editor?cloudshell_git_repo=https%3A%2F%2Fgithub.com%2Feliasecchig%2Fasp-open-in-cloud-shell&amp;amp;cloudshell_print=open-in-cs"&gt;&lt;img src="https://img.shields.io/badge/Launch-in_Cloud_Shell-white" alt="Launch in Cloud Shell" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/github/stars/GoogleCloudPlatform/agent-starter-pack?color=yellow" alt="Stars" /&gt;&lt;/p&gt; 
&lt;p&gt;A Python package that provides &lt;strong&gt;production-ready templates&lt;/strong&gt; for GenAI agents on Google Cloud.&lt;/p&gt; 
&lt;p&gt;Focus on your agent logic—the starter pack provides everything else: infrastructure, CI/CD, observability, and security.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;⚡️ Launch&lt;/th&gt; 
   &lt;th&gt;🧪 Experiment&lt;/th&gt; 
   &lt;th&gt;✅ Deploy&lt;/th&gt; 
   &lt;th&gt;🛠️ Customize&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/GoogleCloudPlatform/agent-starter-pack/main/agent_starter_pack/agents/"&gt;Pre-built agent templates&lt;/a&gt; (ReAct, RAG, multi-agent, Live API).&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://cloud.google.com/vertex-ai/generative-ai/docs/models/evaluation-overview"&gt;Vertex AI evaluation&lt;/a&gt; and an interactive playground.&lt;/td&gt; 
   &lt;td&gt;Production-ready infra with &lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/guide/observability"&gt;monitoring, observability&lt;/a&gt;, and &lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/guide/deployment"&gt;CI/CD&lt;/a&gt; on &lt;a href="https://cloud.google.com/run"&gt;Cloud Run&lt;/a&gt; or &lt;a href="https://cloud.google.com/vertex-ai/generative-ai/docs/agent-engine/overview"&gt;Agent Engine&lt;/a&gt;.&lt;/td&gt; 
   &lt;td&gt;Extend and customize templates according to your needs. 🆕 Now integrating with &lt;a href="https://github.com/google-gemini/gemini-cli"&gt;Gemini CLI&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h2&gt;⚡ Get Started in 1 Minute&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;From zero to production-ready agent in 60 seconds using &lt;a href="https://docs.astral.sh/uv/getting-started/installation/"&gt;&lt;code&gt;uv&lt;/code&gt;&lt;/a&gt;:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uvx agent-starter-pack create
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt; ✨ Alternative: Using pip&lt;/summary&gt; 
 &lt;p&gt;If you don't have &lt;a href="https://github.com/astral-sh/uv"&gt;&lt;code&gt;uv&lt;/code&gt;&lt;/a&gt; installed, you can use pip:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Create and activate a Python virtual environment
python -m venv .venv &amp;amp;&amp;amp; source .venv/bin/activate

# Install the agent starter pack
pip install --upgrade agent-starter-pack

# Create a new agent project
agent-starter-pack create
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p&gt;&lt;strong&gt;That's it!&lt;/strong&gt; You now have a fully functional agent project—complete with backend, frontend, and deployment infrastructure—ready for you to explore and customize.&lt;/p&gt; 
&lt;h3&gt;🔧 Enhance Existing Agents&lt;/h3&gt; 
&lt;p&gt;Already have an agent? Add production-ready deployment and infrastructure by running this command in your project's root folder:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uvx agent-starter-pack enhance
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/guide/installation"&gt;Installation Guide&lt;/a&gt; for more options, or try with zero setup in &lt;a href="https://studio.firebase.google.com/new?template=https%3A%2F%2Fgithub.com%2FGoogleCloudPlatform%2Fagent-starter-pack%2Ftree%2Fmain%2Fsrc%2Fresources%2Fidx"&gt;Firebase Studio&lt;/a&gt; or &lt;a href="https://shell.cloud.google.com/cloudshell/editor?cloudshell_git_repo=https%3A%2F%2Fgithub.com%2Feliasecchig%2Fasp-open-in-cloud-shell&amp;amp;cloudshell_print=open-in-cs"&gt;Cloud Shell&lt;/a&gt;.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;🤖 Agents&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Agent Name&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;adk_base&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;A base ReAct agent implemented using Google's &lt;a href="https://github.com/google/adk-python"&gt;Agent Development Kit&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;adk_a2a_base&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;An ADK agent with &lt;a href="https://a2a-protocol.org/"&gt;Agent2Agent (A2A) Protocol&lt;/a&gt; support for distributed agent communication and interoperability&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;agentic_rag&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;A RAG agent for document retrieval and Q&amp;amp;A. Supporting &lt;a href="https://cloud.google.com/generative-ai-app-builder/docs/enterprise-search-introduction"&gt;Vertex AI Search&lt;/a&gt; and &lt;a href="https://cloud.google.com/vertex-ai/docs/vector-search/overview"&gt;Vector Search&lt;/a&gt;.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;langgraph_base&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;A base ReAct agent implemented using LangChain's &lt;a href="https://github.com/langchain-ai/langgraph"&gt;LangGraph&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;adk_live&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;A real-time multimodal RAG agent powered by Gemini, supporting audio/video/text chat&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;More agents are on the way!&lt;/strong&gt; We are continuously expanding our &lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/agents/overview"&gt;agent library&lt;/a&gt;. Have a specific agent type in mind? &lt;a href="https://github.com/GoogleCloudPlatform/agent-starter-pack/issues/new?labels=enhancement"&gt;Raise an issue as a feature request!&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;🔍 ADK Samples&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Looking to explore more ADK examples? Check out the &lt;a href="https://github.com/google/adk-samples"&gt;ADK Samples Repository&lt;/a&gt; for additional examples and use cases demonstrating ADK's capabilities.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;🌟 Community Showcase&lt;/h2&gt; 
&lt;p&gt;Explore amazing projects built with the Agent Starter Pack!&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/guide/community-showcase"&gt;View Community Showcase →&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;p&gt;The &lt;code&gt;agent-starter-pack&lt;/code&gt; offers key features to accelerate and simplify the development of your agent:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;🔄 &lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/cli/setup_cicd"&gt;CI/CD Automation&lt;/a&gt;&lt;/strong&gt; - A single command to set up a complete CI/CD pipeline for all environments, supporting both &lt;strong&gt;Google Cloud Build&lt;/strong&gt; and &lt;strong&gt;GitHub Actions&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;📥 &lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/guide/data-ingestion"&gt;Data Pipeline for RAG with Terraform/CI-CD&lt;/a&gt;&lt;/strong&gt; - Seamlessly integrate a data pipeline to process embeddings for RAG into your agent system. Supporting &lt;a href="https://cloud.google.com/generative-ai-app-builder/docs/enterprise-search-introduction"&gt;Vertex AI Search&lt;/a&gt; and &lt;a href="https://cloud.google.com/vertex-ai/docs/vector-search/overview"&gt;Vector Search&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/GoogleCloudPlatform/agent-starter-pack/main/docs/guide/remote-templating.md"&gt;Remote Templates&lt;/a&gt;&lt;/strong&gt;: Create and share your own agent starter packs templates from any Git repository.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🤖 Gemini CLI Integration&lt;/strong&gt; - Use the &lt;a href="https://github.com/google-gemini/gemini-cli"&gt;Gemini CLI&lt;/a&gt; and the included &lt;code&gt;GEMINI.md&lt;/code&gt; context file to ask questions about your template, agent architecture, and the path to production. Get instant guidance and code examples directly in your terminal.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;High-Level Architecture&lt;/h2&gt; 
&lt;p&gt;This starter pack covers all aspects of Agent development, from prototyping and evaluation to deployment and monitoring.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/GoogleCloudPlatform/agent-starter-pack/main/docs/images/ags_high_level_architecture.png" alt="High Level Architecture" title="Architecture" /&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;🔧 Requirements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.10+&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://cloud.google.com/sdk/docs/install"&gt;Google Cloud SDK&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://developer.hashicorp.com/terraform/downloads"&gt;Terraform&lt;/a&gt; (for deployment)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.gnu.org/software/make/"&gt;Make&lt;/a&gt; (for development tasks)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;📚 Documentation&lt;/h2&gt; 
&lt;p&gt;Visit our &lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/"&gt;documentation site&lt;/a&gt; for comprehensive guides and references!&lt;/p&gt; 
&lt;p&gt;🔍 &lt;strong&gt;New to the codebase?&lt;/strong&gt; Explore the &lt;a href="https://codewiki.google/github.com/googlecloudplatform/agent-starter-pack"&gt;CodeWiki&lt;/a&gt; for AI-powered code understanding and navigation.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/guide/getting-started"&gt;Getting Started Guide&lt;/a&gt; - First steps with agent-starter-pack&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/guide/installation"&gt;Installation Guide&lt;/a&gt; - Setting up your environment&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/guide/deployment"&gt;Deployment Guide&lt;/a&gt; - Taking your agent to production&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/agents/overview"&gt;Agent Templates Overview&lt;/a&gt; - Explore available agent patterns&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://googlecloudplatform.github.io/agent-starter-pack/cli/"&gt;CLI Reference&lt;/a&gt; - Command-line tool documentation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Video Walkthrough:&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://www.youtube.com/watch?v=9zqwym-N3lg"&gt;Exploring the Agent Starter Pack&lt;/a&gt;&lt;/strong&gt;: A comprehensive tutorial demonstrating how to rapidly deploy AI Agents using the Agent Starter Pack, covering architecture, templates, and step-by-step deployment.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://www.youtube.com/live/eZ-8UQ_t4YM?feature=shared&amp;amp;t=2791"&gt;6-minute introduction&lt;/a&gt;&lt;/strong&gt; (April 2024): Explaining the Agent Starter Pack and demonstrating its key features. Part of the Kaggle GenAI intensive course.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Looking for more examples and resources for Generative AI on Google Cloud? Check out the &lt;a href="https://github.com/GoogleCloudPlatform/generative-ai"&gt;GoogleCloudPlatform/generative-ai&lt;/a&gt; repository for notebooks, code samples, and more!&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Contributions are welcome! See the &lt;a href="https://raw.githubusercontent.com/GoogleCloudPlatform/agent-starter-pack/main/CONTRIBUTING.md"&gt;Contributing Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Feedback&lt;/h2&gt; 
&lt;p&gt;We value your input! Your feedback helps us improve this starter pack and make it more useful for the community.&lt;/p&gt; 
&lt;h3&gt;Getting Help&lt;/h3&gt; 
&lt;p&gt;If you encounter any issues or have specific suggestions, please first consider &lt;a href="https://github.com/GoogleCloudPlatform/generative-ai/issues"&gt;raising an issue&lt;/a&gt; on our GitHub repository.&lt;/p&gt; 
&lt;h3&gt;Share Your Experience&lt;/h3&gt; 
&lt;p&gt;For other types of feedback, or if you'd like to share a positive experience or success story using this starter pack, we'd love to hear from you! You can reach out to us at &lt;a href="mailto:agent-starter-pack@google.com"&gt;&lt;/a&gt;&lt;a href="mailto:agent-starter-pack@google.com"&gt;agent-starter-pack@google.com&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Thank you for your contributions!&lt;/p&gt; 
&lt;h2&gt;Disclaimer&lt;/h2&gt; 
&lt;p&gt;This repository is for demonstrative purposes only and is not an officially supported Google product.&lt;/p&gt; 
&lt;h2&gt;Terms of Service&lt;/h2&gt; 
&lt;p&gt;The agent-starter-pack templating CLI and the templates in this starter pack leverage Google Cloud APIs. When you use this starter pack, you'll be deploying resources in your own Google Cloud project and will be responsible for those resources. Please review the &lt;a href="https://cloud.google.com/terms/service-terms"&gt;Google Cloud Service Terms&lt;/a&gt; for details on the terms of service associated with these APIs.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>FunAudioLLM/CosyVoice</title>
      <link>https://github.com/FunAudioLLM/CosyVoice</link>
      <description>&lt;p&gt;Multi-lingual large voice generation model, providing inference, training and deployment full-stack ability.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://svg-banners.vercel.app/api?type=origin&amp;amp;text1=CosyVoice%F0%9F%A4%A0&amp;amp;text2=Text-to-Speech%20%F0%9F%92%96%20Large%20Language%20Model&amp;amp;width=800&amp;amp;height=210" alt="SVG Banners" /&gt;&lt;/p&gt; 
&lt;h2&gt;👉🏻 CosyVoice 👈🏻&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Fun-CosyVoice 3.0&lt;/strong&gt;: &lt;a href="https://funaudiollm.github.io/cosyvoice3/"&gt;Demos&lt;/a&gt;; &lt;a href="https://arxiv.org/pdf/2505.17589"&gt;Paper&lt;/a&gt;; &lt;a href="https://www.modelscope.cn/models/FunAudioLLM/Fun-CosyVoice3-0.5B-2512"&gt;Modelscope&lt;/a&gt;; &lt;a href="https://huggingface.co/FunAudioLLM/Fun-CosyVoice3-0.5B-2512"&gt;Huggingface&lt;/a&gt;; &lt;a href="https://github.com/FunAudioLLM/CV3-Eval"&gt;CV3-Eval&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;CosyVoice 2.0&lt;/strong&gt;: &lt;a href="https://funaudiollm.github.io/cosyvoice2/"&gt;Demos&lt;/a&gt;; &lt;a href="https://arxiv.org/pdf/2412.10117"&gt;Paper&lt;/a&gt;; &lt;a href="https://www.modelscope.cn/models/iic/CosyVoice2-0.5B"&gt;Modelscope&lt;/a&gt;; &lt;a href="https://huggingface.co/FunAudioLLM/CosyVoice2-0.5B"&gt;HuggingFace&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;CosyVoice 1.0&lt;/strong&gt;: &lt;a href="https://fun-audio-llm.github.io"&gt;Demos&lt;/a&gt;; &lt;a href="https://funaudiollm.github.io/pdf/CosyVoice_v1.pdf"&gt;Paper&lt;/a&gt;; &lt;a href="https://www.modelscope.cn/models/iic/CosyVoice-300M"&gt;Modelscope&lt;/a&gt;; &lt;a href="https://huggingface.co/FunAudioLLM/CosyVoice-300M"&gt;HuggingFace&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Highlight🔥&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Fun-CosyVoice 3.0&lt;/strong&gt; is an advanced text-to-speech (TTS) system based on large language models (LLM), surpassing its predecessor (CosyVoice 2.0) in content consistency, speaker similarity, and prosody naturalness. It is designed for zero-shot multilingual speech synthesis in the wild.&lt;/p&gt; 
&lt;h3&gt;Key Features&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Language Coverage&lt;/strong&gt;: Covers 9 common languages (Chinese, English, Japanese, Korean, German, Spanish, French, Italian, Russian), 18+ Chinese dialects/accents (Guangdong, Minnan, Sichuan, Dongbei, Shan3xi, Shan1xi, Shanghai, Tianjin, Shandong, Ningxia, Gansu, etc.) and meanwhile supports both multi-lingual/cross-lingual zero-shot voice cloning.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Content Consistency &amp;amp; Naturalness&lt;/strong&gt;: Achieves state-of-the-art performance in content consistency, speaker similarity, and prosody naturalness.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Pronunciation Inpainting&lt;/strong&gt;: Supports pronunciation inpainting of Chinese Pinyin and English CMU phonemes, providing more controllability and thus suitable for production use.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Text Normalization&lt;/strong&gt;: Supports reading of numbers, special symbols and various text formats without a traditional frontend module.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Bi-Streaming&lt;/strong&gt;: Support both text-in streaming and audio-out streaming, and achieves latency as low as 150ms while maintaining high-quality audio output.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Instruct Support&lt;/strong&gt;: Supports various instructions such as languages, dialects, emotions, speed, volume, etc.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Roadmap&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;p&gt;2025/12&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; release Fun-CosyVoice3-0.5B-2512 base model, rl model and its training/inference script&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; release Fun-CosyVoice3-0.5B modelscope gradio space&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;p&gt;2025/08&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Thanks to the contribution from NVIDIA Yuekai Zhang, add triton trtllm runtime support and cosyvoice2 grpo training support&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;p&gt;2025/07&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; release Fun-CosyVoice 3.0 eval set&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;p&gt;2025/05&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; add CosyVoice2-0.5B vllm support&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;p&gt;2024/12&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; 25hz CosyVoice2-0.5B released&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;p&gt;2024/09&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; 25hz CosyVoice-300M base model&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; 25hz CosyVoice-300M voice conversion function&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;p&gt;2024/08&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Repetition Aware Sampling(RAS) inference for llm stability&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Streaming inference mode support, including kv cache and sdpa for rtf optimization&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;p&gt;2024/07&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Flow matching training support&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; WeTextProcessing support when ttsfrd is not available&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Fastapi server and client&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Evaluation&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Model&lt;/th&gt; 
   &lt;th align="center"&gt;Open-Source&lt;/th&gt; 
   &lt;th align="center"&gt;Model Size&lt;/th&gt; 
   &lt;th align="center"&gt;test-zh&lt;br /&gt;CER (%) ↓&lt;/th&gt; 
   &lt;th align="center"&gt;test-zh&lt;br /&gt;Speaker Similarity (%) ↑&lt;/th&gt; 
   &lt;th align="center"&gt;test-en&lt;br /&gt;WER (%) ↓&lt;/th&gt; 
   &lt;th align="center"&gt;test-en&lt;br /&gt;Speaker Similarity (%) ↑&lt;/th&gt; 
   &lt;th align="center"&gt;test-hard&lt;br /&gt;CER (%) ↓&lt;/th&gt; 
   &lt;th align="center"&gt;test-hard&lt;br /&gt;Speaker Similarity (%) ↑&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;Human&lt;/td&gt; 
   &lt;td align="center"&gt;-&lt;/td&gt; 
   &lt;td align="center"&gt;-&lt;/td&gt; 
   &lt;td align="center"&gt;1.26&lt;/td&gt; 
   &lt;td align="center"&gt;75.5&lt;/td&gt; 
   &lt;td align="center"&gt;2.14&lt;/td&gt; 
   &lt;td align="center"&gt;73.4&lt;/td&gt; 
   &lt;td align="center"&gt;-&lt;/td&gt; 
   &lt;td align="center"&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;Seed-TTS&lt;/td&gt; 
   &lt;td align="center"&gt;❌&lt;/td&gt; 
   &lt;td align="center"&gt;-&lt;/td&gt; 
   &lt;td align="center"&gt;1.12&lt;/td&gt; 
   &lt;td align="center"&gt;79.6&lt;/td&gt; 
   &lt;td align="center"&gt;2.25&lt;/td&gt; 
   &lt;td align="center"&gt;76.2&lt;/td&gt; 
   &lt;td align="center"&gt;7.59&lt;/td&gt; 
   &lt;td align="center"&gt;77.6&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;MiniMax-Speech&lt;/td&gt; 
   &lt;td align="center"&gt;❌&lt;/td&gt; 
   &lt;td align="center"&gt;-&lt;/td&gt; 
   &lt;td align="center"&gt;0.83&lt;/td&gt; 
   &lt;td align="center"&gt;78.3&lt;/td&gt; 
   &lt;td align="center"&gt;1.65&lt;/td&gt; 
   &lt;td align="center"&gt;69.2&lt;/td&gt; 
   &lt;td align="center"&gt;-&lt;/td&gt; 
   &lt;td align="center"&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;F5-TTS&lt;/td&gt; 
   &lt;td align="center"&gt;✅&lt;/td&gt; 
   &lt;td align="center"&gt;0.3B&lt;/td&gt; 
   &lt;td align="center"&gt;1.52&lt;/td&gt; 
   &lt;td align="center"&gt;74.1&lt;/td&gt; 
   &lt;td align="center"&gt;2.00&lt;/td&gt; 
   &lt;td align="center"&gt;64.7&lt;/td&gt; 
   &lt;td align="center"&gt;8.67&lt;/td&gt; 
   &lt;td align="center"&gt;71.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;Spark TTS&lt;/td&gt; 
   &lt;td align="center"&gt;✅&lt;/td&gt; 
   &lt;td align="center"&gt;0.5B&lt;/td&gt; 
   &lt;td align="center"&gt;1.2&lt;/td&gt; 
   &lt;td align="center"&gt;66.0&lt;/td&gt; 
   &lt;td align="center"&gt;1.98&lt;/td&gt; 
   &lt;td align="center"&gt;57.3&lt;/td&gt; 
   &lt;td align="center"&gt;-&lt;/td&gt; 
   &lt;td align="center"&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;CosyVoice2&lt;/td&gt; 
   &lt;td align="center"&gt;✅&lt;/td&gt; 
   &lt;td align="center"&gt;0.5B&lt;/td&gt; 
   &lt;td align="center"&gt;1.45&lt;/td&gt; 
   &lt;td align="center"&gt;75.7&lt;/td&gt; 
   &lt;td align="center"&gt;2.57&lt;/td&gt; 
   &lt;td align="center"&gt;65.9&lt;/td&gt; 
   &lt;td align="center"&gt;6.83&lt;/td&gt; 
   &lt;td align="center"&gt;72.4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;FireRedTTS2&lt;/td&gt; 
   &lt;td align="center"&gt;✅&lt;/td&gt; 
   &lt;td align="center"&gt;1.5B&lt;/td&gt; 
   &lt;td align="center"&gt;1.14&lt;/td&gt; 
   &lt;td align="center"&gt;73.2&lt;/td&gt; 
   &lt;td align="center"&gt;1.95&lt;/td&gt; 
   &lt;td align="center"&gt;66.5&lt;/td&gt; 
   &lt;td align="center"&gt;-&lt;/td&gt; 
   &lt;td align="center"&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;Index-TTS2&lt;/td&gt; 
   &lt;td align="center"&gt;✅&lt;/td&gt; 
   &lt;td align="center"&gt;1.5B&lt;/td&gt; 
   &lt;td align="center"&gt;1.03&lt;/td&gt; 
   &lt;td align="center"&gt;76.5&lt;/td&gt; 
   &lt;td align="center"&gt;2.23&lt;/td&gt; 
   &lt;td align="center"&gt;70.6&lt;/td&gt; 
   &lt;td align="center"&gt;7.12&lt;/td&gt; 
   &lt;td align="center"&gt;75.5&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;VibeVoice-1.5B&lt;/td&gt; 
   &lt;td align="center"&gt;✅&lt;/td&gt; 
   &lt;td align="center"&gt;1.5B&lt;/td&gt; 
   &lt;td align="center"&gt;1.16&lt;/td&gt; 
   &lt;td align="center"&gt;74.4&lt;/td&gt; 
   &lt;td align="center"&gt;3.04&lt;/td&gt; 
   &lt;td align="center"&gt;68.9&lt;/td&gt; 
   &lt;td align="center"&gt;-&lt;/td&gt; 
   &lt;td align="center"&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;VibeVoice-Realtime&lt;/td&gt; 
   &lt;td align="center"&gt;✅&lt;/td&gt; 
   &lt;td align="center"&gt;0.5B&lt;/td&gt; 
   &lt;td align="center"&gt;-&lt;/td&gt; 
   &lt;td align="center"&gt;-&lt;/td&gt; 
   &lt;td align="center"&gt;2.05&lt;/td&gt; 
   &lt;td align="center"&gt;63.3&lt;/td&gt; 
   &lt;td align="center"&gt;-&lt;/td&gt; 
   &lt;td align="center"&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;HiggsAudio-v2&lt;/td&gt; 
   &lt;td align="center"&gt;✅&lt;/td&gt; 
   &lt;td align="center"&gt;3B&lt;/td&gt; 
   &lt;td align="center"&gt;1.50&lt;/td&gt; 
   &lt;td align="center"&gt;74.0&lt;/td&gt; 
   &lt;td align="center"&gt;2.44&lt;/td&gt; 
   &lt;td align="center"&gt;67.7&lt;/td&gt; 
   &lt;td align="center"&gt;-&lt;/td&gt; 
   &lt;td align="center"&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;VoxCPM&lt;/td&gt; 
   &lt;td align="center"&gt;✅&lt;/td&gt; 
   &lt;td align="center"&gt;0.5B&lt;/td&gt; 
   &lt;td align="center"&gt;0.93&lt;/td&gt; 
   &lt;td align="center"&gt;77.2&lt;/td&gt; 
   &lt;td align="center"&gt;1.85&lt;/td&gt; 
   &lt;td align="center"&gt;72.9&lt;/td&gt; 
   &lt;td align="center"&gt;8.87&lt;/td&gt; 
   &lt;td align="center"&gt;73.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;GLM-TTS&lt;/td&gt; 
   &lt;td align="center"&gt;✅&lt;/td&gt; 
   &lt;td align="center"&gt;1.5B&lt;/td&gt; 
   &lt;td align="center"&gt;1.03&lt;/td&gt; 
   &lt;td align="center"&gt;76.1&lt;/td&gt; 
   &lt;td align="center"&gt;-&lt;/td&gt; 
   &lt;td align="center"&gt;-&lt;/td&gt; 
   &lt;td align="center"&gt;-&lt;/td&gt; 
   &lt;td align="center"&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;GLM-TTS RL&lt;/td&gt; 
   &lt;td align="center"&gt;✅&lt;/td&gt; 
   &lt;td align="center"&gt;1.5B&lt;/td&gt; 
   &lt;td align="center"&gt;0.89&lt;/td&gt; 
   &lt;td align="center"&gt;76.4&lt;/td&gt; 
   &lt;td align="center"&gt;-&lt;/td&gt; 
   &lt;td align="center"&gt;-&lt;/td&gt; 
   &lt;td align="center"&gt;-&lt;/td&gt; 
   &lt;td align="center"&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;Fun-CosyVoice3-0.5B-2512&lt;/td&gt; 
   &lt;td align="center"&gt;✅&lt;/td&gt; 
   &lt;td align="center"&gt;0.5B&lt;/td&gt; 
   &lt;td align="center"&gt;1.21&lt;/td&gt; 
   &lt;td align="center"&gt;78.0&lt;/td&gt; 
   &lt;td align="center"&gt;2.24&lt;/td&gt; 
   &lt;td align="center"&gt;71.8&lt;/td&gt; 
   &lt;td align="center"&gt;6.71&lt;/td&gt; 
   &lt;td align="center"&gt;75.8&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;Fun-CosyVoice3-0.5B-2512_RL&lt;/td&gt; 
   &lt;td align="center"&gt;✅&lt;/td&gt; 
   &lt;td align="center"&gt;0.5B&lt;/td&gt; 
   &lt;td align="center"&gt;0.81&lt;/td&gt; 
   &lt;td align="center"&gt;77.4&lt;/td&gt; 
   &lt;td align="center"&gt;1.68&lt;/td&gt; 
   &lt;td align="center"&gt;69.5&lt;/td&gt; 
   &lt;td align="center"&gt;5.44&lt;/td&gt; 
   &lt;td align="center"&gt;75.0&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Install&lt;/h2&gt; 
&lt;h3&gt;Clone and install&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Clone the repo&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;git clone --recursive https://github.com/FunAudioLLM/CosyVoice.git
# If you failed to clone the submodule due to network failures, please run the following command until success
cd CosyVoice
git submodule update --init --recursive
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install Conda: please see &lt;a href="https://docs.conda.io/en/latest/miniconda.html"&gt;https://docs.conda.io/en/latest/miniconda.html&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Create Conda env:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;conda create -n cosyvoice -y python=3.10
conda activate cosyvoice
pip install -r requirements.txt -i https://mirrors.aliyun.com/pypi/simple/ --trusted-host=mirrors.aliyun.com

# If you encounter sox compatibility issues
# ubuntu
sudo apt-get install sox libsox-dev
# centos
sudo yum install sox sox-devel
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Model download&lt;/h3&gt; 
&lt;p&gt;We strongly recommend that you download our pretrained &lt;code&gt;Fun-CosyVoice3-0.5B&lt;/code&gt; &lt;code&gt;CosyVoice2-0.5B&lt;/code&gt; &lt;code&gt;CosyVoice-300M&lt;/code&gt; &lt;code&gt;CosyVoice-300M-SFT&lt;/code&gt; &lt;code&gt;CosyVoice-300M-Instruct&lt;/code&gt; model and &lt;code&gt;CosyVoice-ttsfrd&lt;/code&gt; resource.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# modelscope SDK model download
from modelscope import snapshot_download
snapshot_download('FunAudioLLM/Fun-CosyVoice3-0.5B-2512', local_dir='pretrained_models/Fun-CosyVoice3-0.5B')
snapshot_download('iic/CosyVoice2-0.5B', local_dir='pretrained_models/CosyVoice2-0.5B')
snapshot_download('iic/CosyVoice-300M', local_dir='pretrained_models/CosyVoice-300M')
snapshot_download('iic/CosyVoice-300M-SFT', local_dir='pretrained_models/CosyVoice-300M-SFT')
snapshot_download('iic/CosyVoice-300M-Instruct', local_dir='pretrained_models/CosyVoice-300M-Instruct')
snapshot_download('iic/CosyVoice-ttsfrd', local_dir='pretrained_models/CosyVoice-ttsfrd')

# for oversea users, huggingface SDK model download
from huggingface_hub import snapshot_download
snapshot_download('FunAudioLLM/Fun-CosyVoice3-0.5B-2512', local_dir='pretrained_models/Fun-CosyVoice3-0.5B')
snapshot_download('FunAudioLLM/CosyVoice2-0.5B', local_dir='pretrained_models/CosyVoice2-0.5B')
snapshot_download('FunAudioLLM/CosyVoice-300M', local_dir='pretrained_models/CosyVoice-300M')
snapshot_download('FunAudioLLM/CosyVoice-300M-SFT', local_dir='pretrained_models/CosyVoice-300M-SFT')
snapshot_download('FunAudioLLM/CosyVoice-300M-Instruct', local_dir='pretrained_models/CosyVoice-300M-Instruct')
snapshot_download('FunAudioLLM/CosyVoice-ttsfrd', local_dir='pretrained_models/CosyVoice-ttsfrd')
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Optionally, you can unzip &lt;code&gt;ttsfrd&lt;/code&gt; resource and install &lt;code&gt;ttsfrd&lt;/code&gt; package for better text normalization performance.&lt;/p&gt; 
&lt;p&gt;Notice that this step is not necessary. If you do not install &lt;code&gt;ttsfrd&lt;/code&gt; package, we will use wetext by default.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;cd pretrained_models/CosyVoice-ttsfrd/
unzip resource.zip -d .
pip install ttsfrd_dependency-0.1-py3-none-any.whl
pip install ttsfrd-0.4.2-cp310-cp310-linux_x86_64.whl
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Basic Usage&lt;/h3&gt; 
&lt;p&gt;We strongly recommend using &lt;code&gt;Fun-CosyVoice3-0.5B&lt;/code&gt; for better performance. Follow the code in &lt;code&gt;example.py&lt;/code&gt; for detailed usage of each model.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python example.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;CosyVoice2 vllm Usage&lt;/h4&gt; 
&lt;p&gt;If you want to use vllm for inference, please install &lt;code&gt;vllm==v0.9.0&lt;/code&gt;. Older vllm version do not support CosyVoice2 inference.&lt;/p&gt; 
&lt;p&gt;Notice that &lt;code&gt;vllm==v0.9.0&lt;/code&gt; has a lot of specific requirements, for example &lt;code&gt;torch==2.7.0&lt;/code&gt;. You can create a new env to in case your hardward do not support vllm and old env is corrupted.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;conda create -n cosyvoice_vllm --clone cosyvoice
conda activate cosyvoice_vllm
pip install vllm==v0.9.0 transformers==4.51.3 numpy==1.26.4 -i https://mirrors.aliyun.com/pypi/simple/ --trusted-host=mirrors.aliyun.com
python vllm_example.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Start web demo&lt;/h4&gt; 
&lt;p&gt;You can use our web demo page to get familiar with CosyVoice quickly.&lt;/p&gt; 
&lt;p&gt;Please see the demo website for details.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# change iic/CosyVoice-300M-SFT for sft inference, or iic/CosyVoice-300M-Instruct for instruct inference
python3 webui.py --port 50000 --model_dir pretrained_models/CosyVoice-300M
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Advanced Usage&lt;/h4&gt; 
&lt;p&gt;For advanced users, we have provided training and inference scripts in &lt;code&gt;examples/libritts/cosyvoice/run.sh&lt;/code&gt;.&lt;/p&gt; 
&lt;h4&gt;Build for deployment&lt;/h4&gt; 
&lt;p&gt;Optionally, if you want service deployment, You can run the following steps.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;cd runtime/python
docker build -t cosyvoice:v1.0 .
# change iic/CosyVoice-300M to iic/CosyVoice-300M-Instruct if you want to use instruct inference
# for grpc usage
docker run -d --runtime=nvidia -p 50000:50000 cosyvoice:v1.0 /bin/bash -c "cd /opt/CosyVoice/CosyVoice/runtime/python/grpc &amp;amp;&amp;amp; python3 server.py --port 50000 --max_conc 4 --model_dir iic/CosyVoice-300M &amp;amp;&amp;amp; sleep infinity"
cd grpc &amp;amp;&amp;amp; python3 client.py --port 50000 --mode &amp;lt;sft|zero_shot|cross_lingual|instruct&amp;gt;
# for fastapi usage
docker run -d --runtime=nvidia -p 50000:50000 cosyvoice:v1.0 /bin/bash -c "cd /opt/CosyVoice/CosyVoice/runtime/python/fastapi &amp;amp;&amp;amp; python3 server.py --port 50000 --model_dir iic/CosyVoice-300M &amp;amp;&amp;amp; sleep infinity"
cd fastapi &amp;amp;&amp;amp; python3 client.py --port 50000 --mode &amp;lt;sft|zero_shot|cross_lingual|instruct&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Using Nvidia TensorRT-LLM for deployment&lt;/h4&gt; 
&lt;p&gt;Using TensorRT-LLM to accelerate cosyvoice2 llm could give 4x acceleration comparing with huggingface transformers implementation. To quick start:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;cd runtime/triton_trtllm
docker compose up -d
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more details, you could check &lt;a href="https://github.com/FunAudioLLM/CosyVoice/tree/main/runtime/triton_trtllm"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Discussion &amp;amp; Communication&lt;/h2&gt; 
&lt;p&gt;You can directly discuss on &lt;a href="https://github.com/FunAudioLLM/CosyVoice/issues"&gt;Github Issues&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You can also scan the QR code to join our official Dingding chat group.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/FunAudioLLM/CosyVoice/main/asset/dingding.png" width="250px" /&gt; 
&lt;h2&gt;Acknowledge&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;We borrowed a lot of code from &lt;a href="https://github.com/modelscope/FunASR"&gt;FunASR&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;We borrowed a lot of code from &lt;a href="https://github.com/modelscope/FunCodec"&gt;FunCodec&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;We borrowed a lot of code from &lt;a href="https://github.com/shivammehta25/Matcha-TTS"&gt;Matcha-TTS&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;We borrowed a lot of code from &lt;a href="https://github.com/yangdongchao/AcademiCodec"&gt;AcademiCodec&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;We borrowed a lot of code from &lt;a href="https://github.com/wenet-e2e/wenet"&gt;WeNet&lt;/a&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Citations&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@article{du2024cosyvoice,
  title={Cosyvoice: A scalable multilingual zero-shot text-to-speech synthesizer based on supervised semantic tokens},
  author={Du, Zhihao and Chen, Qian and Zhang, Shiliang and Hu, Kai and Lu, Heng and Yang, Yexin and Hu, Hangrui and Zheng, Siqi and Gu, Yue and Ma, Ziyang and others},
  journal={arXiv preprint arXiv:2407.05407},
  year={2024}
}

@article{du2024cosyvoice,
  title={Cosyvoice 2: Scalable streaming speech synthesis with large language models},
  author={Du, Zhihao and Wang, Yuxuan and Chen, Qian and Shi, Xian and Lv, Xiang and Zhao, Tianyu and Gao, Zhifu and Yang, Yexin and Gao, Changfeng and Wang, Hui and others},
  journal={arXiv preprint arXiv:2412.10117},
  year={2024}
}

@article{du2025cosyvoice,
  title={CosyVoice 3: Towards In-the-wild Speech Generation via Scaling-up and Post-training},
  author={Du, Zhihao and Gao, Changfeng and Wang, Yuxuan and Yu, Fan and Zhao, Tianyu and Wang, Hao and Lv, Xiang and Wang, Hui and Shi, Xian and An, Keyu and others},
  journal={arXiv preprint arXiv:2505.17589},
  year={2025}
}

@inproceedings{lyu2025build,
  title={Build LLM-Based Zero-Shot Streaming TTS System with Cosyvoice},
  author={Lyu, Xiang and Wang, Yuxuan and Zhao, Tianyu and Wang, Hao and Liu, Huadai and Du, Zhihao},
  booktitle={ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={1--2},
  year={2025},
  organization={IEEE}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Disclaimer&lt;/h2&gt; 
&lt;p&gt;The content provided above is for academic purposes only and is intended to demonstrate technical capabilities. Some examples are sourced from the internet. If any content infringes on your rights, please contact us to request its removal.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>trustedsec/social-engineer-toolkit</title>
      <link>https://github.com/trustedsec/social-engineer-toolkit</link>
      <description>&lt;p&gt;The Social-Engineer Toolkit (SET) repository from TrustedSec - All new versions of SET will be deployed here.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;The Social-Engineer Toolkit (SET)&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;Copyright &lt;span&gt;©&lt;/span&gt; 2020&lt;/li&gt; 
 &lt;li&gt;Written by: David Kennedy (ReL1K) @HackingDave&lt;/li&gt; 
 &lt;li&gt;Company: &lt;a href="https://www.trustedsec.com"&gt;TrustedSec&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;h2&gt;Description&lt;/h2&gt; 
&lt;p&gt;The Social-Engineer Toolkit is an open-source penetration testing framework designed for social engineering. SET has a number of custom attack vectors that allow you to make a believable attack quickly. SET is a product of TrustedSec, LLC – an information security consulting firm located in Cleveland, Ohio.&lt;/p&gt; 
&lt;p&gt;DISCLAIMER: This is &lt;em&gt;only&lt;/em&gt; for testing purposes and can only be used where strict consent has been given. Do not use this for illegal purposes, period. Please read the LICENSE under readme/LICENSE for the licensing of SET.&lt;/p&gt; 
&lt;h4&gt;Supported platforms:&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Linux&lt;/li&gt; 
 &lt;li&gt;Mac OS X (experimental)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Installation&lt;/h1&gt; 
&lt;h2&gt;Install via requirements.txt&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip3 install -r requirements.txt
python3 setup.py 
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Install SET&lt;/h2&gt; 
&lt;p&gt;=======&lt;/p&gt; 
&lt;h4&gt;Mac OS X&lt;/h4&gt; 
&lt;p&gt;You will need to use a virtual environment for the Python install if you are using an M2 Macbook with the following instructions in your CLI within the social-engineer-toolkit directory.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;    # to install dependencies, run the following:
    python3 -m venv path/to/venv
    source path/to/venv/bin/activate
    python3 -m pip install -r requirements.txt

    # to install SET
    sudo python3 setup.py 
&lt;/code&gt;&lt;/pre&gt; 
&lt;br /&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h4&gt;Windows 10 WSL/WSL2 Kali Linux&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo apt install set -y
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Kali Linux on Windows 10 is a minimal installation so it doesn't have any tools installed. You can easily install Social Engineer Toolkit on WSL/WSL2 without needing pip using the above command.&lt;/p&gt; 
&lt;h4&gt;Linux&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/trustedsec/social-engineer-toolkit/ setoolkit/
cd setoolkit
pip3 install -r requirements.txt
python setup.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;br /&gt; 
&lt;h2&gt;SET Tutorial&lt;/h2&gt; 
&lt;p&gt;For a full document on how to use SET, &lt;a href="https://github.com/trustedsec/social-engineer-toolkit/raw/master/readme/User_Manual.pdf"&gt;visit the SET user manual&lt;/a&gt;.&lt;/p&gt; 
&lt;br /&gt; 
&lt;h2&gt;Bugs and enhancements&lt;/h2&gt; 
&lt;p&gt;For bug reports or enhancements, please open an &lt;a href="https://github.com/trustedsec/social-engineer-toolkit/issues"&gt;issue&lt;/a&gt; here. &lt;br /&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>TEN-framework/ten-framework</title>
      <link>https://github.com/TEN-framework/ten-framework</link>
      <description>&lt;p&gt;Open-source framework for conversational voice AI agents&lt;/p&gt;&lt;hr&gt;&lt;div align="center" id="readme-top"&gt; 
 &lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/799584b2-61ff-4255-bdd1-2548d0fdba52" alt="Image" /&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/TEN-framework/ten-framework/releases"&gt;&lt;img src="https://img.shields.io/github/v/release/ten-framework/ten-framework?color=369eff&amp;amp;labelColor=gray&amp;amp;logo=github&amp;amp;style=flat-square" alt="TEN Releases" /&gt;&lt;/a&gt; &lt;a href="https://coveralls.io/github/TEN-framework/ten-framework?branch=main"&gt;&lt;img src="https://coveralls.io/repos/github/TEN-framework/ten-framework/badge.svg?branch=main" alt="Coverage Status" /&gt;&lt;/a&gt; &lt;a href="https://github.com/TEN-framework/ten-framework/releases"&gt;&lt;img src="https://img.shields.io/github/release-date/ten-framework/ten-framework?labelColor=gray&amp;amp;style=flat-square" alt="Release Date" /&gt;&lt;/a&gt; &lt;a href="https://github.com/TEN-framework/ten-framework/graphs/commit-activity"&gt;&lt;img src="https://img.shields.io/github/commit-activity/m/TEN-framework/ten-framework?labelColor=gray&amp;amp;color=pink" alt="Commits" /&gt;&lt;/a&gt; &lt;a href="https://github.com/TEN-framework/ten-framework/issues"&gt;&lt;img src="https://img.shields.io/github/issues-search?query=repo%3ATEN-framework%2Ften-framework%20is%3Aclosed&amp;amp;label=issues%20closed&amp;amp;labelColor=gray&amp;amp;color=green" alt="Issues closed" /&gt;&lt;/a&gt; &lt;a href="https://github.com/TEN-framework/ten-framework/graphs/contributors"&gt;&lt;img src="https://img.shields.io/github/contributors/ten-framework/ten-framework?color=c4f042&amp;amp;labelColor=gray&amp;amp;style=flat-square" alt="Contributors" /&gt;&lt;/a&gt; &lt;a href="https://github.com/TEN-framework/ten-framework/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/License-Apache_2.0_with_certain_conditions-blue.svg?labelColor=%20%23155EEF&amp;amp;color=%20%23528bff" alt="GitHub license" /&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/TEN-framework/TEN-framework"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki" /&gt;&lt;/a&gt; &lt;a href="https://readmex.com/TEN-framework/ten-framework"&gt;&lt;img src="https://raw.githubusercontent.com/CodePhiliaX/resource-trusteeship/main/readmex.svg?sanitize=true" alt="ReadmeX" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/TEN-framework/ten-framework/raw/main/README.md"&gt;&lt;img src="https://img.shields.io/badge/English-lightgrey" alt="README in English" /&gt;&lt;/a&gt; &lt;a href="https://github.com/TEN-framework/ten-framework/raw/main/docs/README-CN.md"&gt;&lt;img src="https://img.shields.io/badge/%E7%AE%80%E4%BD%93%E4%B8%AD%E6%96%87-lightgrey" alt="简体中文操作指南" /&gt;&lt;/a&gt; &lt;a href="https://github.com/TEN-framework/ten-framework/raw/main/docs/README-JP.md"&gt;&lt;img src="https://img.shields.io/badge/%E6%97%A5%E6%9C%AC%E8%AA%9E-lightgrey" alt="日本語のREADME" /&gt;&lt;/a&gt; &lt;a href="https://github.com/TEN-framework/ten-framework/raw/main/docs/README-KR.md"&gt;&lt;img src="https://img.shields.io/badge/%ED%95%9C%EA%B5%AD%EC%96%B4-lightgrey" alt="README in 한국어" /&gt;&lt;/a&gt; &lt;a href="https://github.com/TEN-framework/ten-framework/raw/main/docs/README-ES.md"&gt;&lt;img src="https://img.shields.io/badge/Espa%C3%B1ol-lightgrey" alt="README en Español" /&gt;&lt;/a&gt; &lt;a href="https://github.com/TEN-framework/ten-framework/raw/main/docs/README-FR.md"&gt;&lt;img src="https://img.shields.io/badge/Fran%C3%A7ais-lightgrey" alt="README en Français" /&gt;&lt;/a&gt; &lt;a href="https://github.com/TEN-framework/ten-framework/raw/main/docs/README-IT.md"&gt;&lt;img src="https://img.shields.io/badge/Italiano-lightgrey" alt="README in Italiano" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/11978"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/11978" alt="TEN-framework%2Ften_framework | Trendshift" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://theten.ai"&gt;Official Site&lt;/a&gt; • &lt;a href="https://theten.ai/docs"&gt;Documentation&lt;/a&gt; • &lt;a href="https://theten.ai/blog"&gt;Blog&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;details open&gt; 
 &lt;summary&gt;&lt;kbd&gt;Table of Contents&lt;/kbd&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#welcome-to-ten"&gt;Welcome to TEN&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#agent-examples"&gt;Agent Examples&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#quick-start-with-agent-examples"&gt;Quick Start with Agent Examples&lt;/a&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#localhost"&gt;Localhost&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#codespaces"&gt;Codespaces&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#agent-examples-self-hosting"&gt;Agent Examples Self-Hosting&lt;/a&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#deploying-with-docker"&gt;Deploying with Docker&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#deploying-with-other-cloud-services"&gt;Deploying with other cloud services&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#stay-tuned"&gt;Stay Tuned&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#ten-ecosystem"&gt;TEN Ecosystem&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#questions"&gt;Questions&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#contributing"&gt;Contributing&lt;/a&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#code-contributors"&gt;Code Contributors&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#contribution-guidelines"&gt;Contribution Guidelines&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#license"&gt;License&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;br /&gt; 
&lt;/details&gt; 
&lt;h2&gt;Welcome to TEN&lt;/h2&gt; 
&lt;p&gt;TEN is an open-source framework for real-time multimodal conversational AI.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#ten-ecosystem"&gt;TEN Ecosystem&lt;/a&gt; includes &lt;a href="https://github.com/ten-framework/ten-framework"&gt;TEN Framework&lt;/a&gt;, &lt;a href="https://github.com/TEN-framework/ten-framework/tree/main/ai_agents/agents/examples"&gt;Agent Examples&lt;/a&gt;, &lt;a href="https://github.com/ten-framework/ten-vad"&gt;VAD&lt;/a&gt;, &lt;a href="https://github.com/ten-framework/ten-turn-detection"&gt;Turn Detection&lt;/a&gt; and &lt;a href="https://github.com/ten-framework/portal"&gt;Portal&lt;/a&gt;.&lt;/p&gt; 
&lt;br /&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Community Channel&lt;/th&gt; 
   &lt;th&gt;Purpose&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://twitter.com/intent/follow?screen_name=TenFramework"&gt;&lt;img src="https://img.shields.io/twitter/follow/TenFramework?logo=X&amp;amp;color=%20%23f5f5f5" alt="Follow on X" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Follow TEN Framework on X for updates and announcements&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://discord.gg/VnPftUzAMJ"&gt;&lt;img src="https://img.shields.io/badge/Discord-Join%20TEN%20Community-5865F2?style=flat&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Discord TEN Community" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Join our Discord community to connect with developers&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.linkedin.com/company/ten-framework"&gt;&lt;img src="https://custom-icon-badges.demolab.com/badge/LinkedIn-TEN_Framework-0A66C2?logo=linkedin-white&amp;amp;logoColor=fff" alt="Follow on LinkedIn" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Follow TEN Framework on LinkedIn for updates and announcements&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/TEN-framework"&gt;&lt;img src="https://img.shields.io/badge/Hugging%20Face-TEN%20Framework-yellow?style=flat&amp;amp;logo=huggingface" alt="Hugging Face Space" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Join our Hugging Face community to explore our spaces and models&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/TEN-framework/ten-agent/discussions/170"&gt;&lt;img src="https://img.shields.io/badge/TEN_Framework-WeChat_Group-%2307C160?logo=wechat&amp;amp;labelColor=darkgreen&amp;amp;color=gray" alt="WeChat" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Join our WeChat group for Chinese community discussions&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;br /&gt; 
&lt;h2&gt;Agent Examples&lt;/h2&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/dce3db80-fb48-4e2a-8ac7-33f50bcffa32" alt="Image" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Multi-Purpose Voice Assistant&lt;/strong&gt; — This low-latency, high-quality real-time assistant supports both RTC and &lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/ai_agents/agents/examples/websocket-example"&gt;WebSocket&lt;/a&gt; connections, and you can extend it with &lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/ai_agents/agents/examples/voice-assistant-with-memU"&gt;Memory&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/ai_agents/agents/examples/voice-assistant-with-ten-vad"&gt;VAD&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/ai_agents/agents/examples/voice-assistant-with-turn-detection"&gt;Turn Detection&lt;/a&gt;, and other extensions.&lt;/p&gt; 
&lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/ai_agents/agents/examples/voice-assistant"&gt;Example code&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/aec54c94-ced9-4683-ae58-0a5a7ed803bd#gh-light-mode-only" alt="divider" /&gt; &lt;img src="https://github.com/user-attachments/assets/d57fad08-4f49-4a1c-bdfc-f659a5d86150#gh-dark-mode-only" alt="divider" /&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/51ab1504-b67c-49d4-8a7a-5582d9b254da" alt="Image" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Lip Sync Avatars&lt;/strong&gt; — Works with multiple avatar vendors, the main character features Kei, an anime character with MotionSync-powered lip sync, and also supports realistic avatars from Trulience, HeyGen, and Tavus.&lt;/p&gt; 
&lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/ai_agents/agents/examples/voice-assistant-live2d"&gt;Example code&lt;/a&gt; for different Live2D characters.&lt;/p&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/aec54c94-ced9-4683-ae58-0a5a7ed803bd#gh-light-mode-only" alt="divider" /&gt; &lt;img src="https://github.com/user-attachments/assets/d57fad08-4f49-4a1c-bdfc-f659a5d86150#gh-dark-mode-only" alt="divider" /&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/f94b21b8-9dda-4efc-9274-b028cc01296a" alt="Image" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Speech Diarization&lt;/strong&gt; — Real-time diarization that detects and labels speakers, the Who Likes What game shows an interactive use case.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/ai_agents/agents/examples/speechmatics-diarization"&gt;Example code&lt;/a&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/aec54c94-ced9-4683-ae58-0a5a7ed803bd#gh-light-mode-only" alt="divider" /&gt; &lt;img src="https://github.com/user-attachments/assets/d57fad08-4f49-4a1c-bdfc-f659a5d86150#gh-dark-mode-only" alt="divider" /&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/6ed5b04d-945a-4a30-a1cc-f8014b602b38" alt="Image" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;SIP Call&lt;/strong&gt; — SIP extension that enables phone calls powered by TEN.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/ai_agents/agents/examples/voice-assistant-sip-twilio"&gt;Example code&lt;/a&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/aec54c94-ced9-4683-ae58-0a5a7ed803bd#gh-light-mode-only" alt="divider" /&gt; &lt;img src="https://github.com/user-attachments/assets/d57fad08-4f49-4a1c-bdfc-f659a5d86150#gh-dark-mode-only" alt="divider" /&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/d793bc6c-c8de-4996-bd85-9ce88c69dd8d" alt="Image" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Transcription&lt;/strong&gt; — A transcription tool that transcribes audio to text.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/ai_agents/agents/examples/transcription"&gt;Example code&lt;/a&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/aec54c94-ced9-4683-ae58-0a5a7ed803bd#gh-light-mode-only" alt="divider" /&gt; &lt;img src="https://github.com/user-attachments/assets/d57fad08-4f49-4a1c-bdfc-f659a5d86150#gh-dark-mode-only" alt="divider" /&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/3d60f1ff-0f82-4fe7-b5c2-ac03d284f60c" alt="Image" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;ESP32-S3 Korvo V3&lt;/strong&gt; — Runs TEN agent example on the Espressif ESP32-S3 Korvo V3 development board to integrate LLM-powered communication with hardware.&lt;/p&gt; 
&lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/ai_agents/esp32-client"&gt;integration guide&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;br /&gt; 
&lt;div align="right"&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#readme-top"&gt;&lt;img src="https://img.shields.io/badge/-Back_to_top-gray?style=flat-square" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;Quick Start with Agent Examples&lt;/h2&gt; 
&lt;h3&gt;Localhost&lt;/h3&gt; 
&lt;h4&gt;Step ⓵ - Prerequisites&lt;/h4&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Category&lt;/th&gt; 
   &lt;th&gt;Requirements&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Keys&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;• Agora &lt;a href="https://docs.agora.io/en/video-calling/get-started/manage-agora-account?platform=web#create-an-agora-project"&gt;App ID&lt;/a&gt; and &lt;a href="https://docs.agora.io/en/video-calling/get-started/manage-agora-account?platform=web#create-an-agora-project"&gt;App Certificate&lt;/a&gt;&lt;br /&gt;• &lt;a href="https://openai.com/index/openai-api/"&gt;OpenAI&lt;/a&gt; API key&lt;br /&gt;• &lt;a href="https://deepgram.com/"&gt;Deepgram&lt;/a&gt; ASR &lt;br /&gt;• &lt;a href="https://elevenlabs.io/"&gt;ElevenLabs&lt;/a&gt; TTS&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Installation&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;• &lt;a href="https://www.docker.com/"&gt;Docker&lt;/a&gt; / &lt;a href="https://docs.docker.com/compose/"&gt;Docker Compose&lt;/a&gt;&lt;br /&gt;• &lt;a href="https://nodejs.org/en"&gt;Node.js (LTS) v18&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Minimum System Requirements&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;• CPU &amp;gt;= 2 cores&lt;br /&gt;• RAM &amp;gt;= 4 GB&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/aec54c94-ced9-4683-ae58-0a5a7ed803bd#gh-light-mode-only" alt="divider" /&gt; &lt;img src="https://github.com/user-attachments/assets/d57fad08-4f49-4a1c-bdfc-f659a5d86150#gh-dark-mode-only" alt="divider" /&gt;&lt;/p&gt; 
&lt;!-- &gt; [!NOTE]
&gt; **macOS: Docker setting on Apple Silicon**
&gt;
&gt; Uncheck "Use Rosetta for x86/amd64 emulation" in Docker settings, it may result in slower build times on ARM, but performance will be normal when deployed to x64 servers. --&gt; 
&lt;h4&gt;Step ⓶ - Build agent examples in VM&lt;/h4&gt; 
&lt;h5&gt;1. Clone the repo, &lt;code&gt;cd&lt;/code&gt; into &lt;code&gt;ai_agents&lt;/code&gt;, and create a &lt;code&gt;.env&lt;/code&gt; file from &lt;code&gt;.env.example&lt;/code&gt;&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd ai_agents
cp ./.env.example ./.env
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;2. Set up the Agora App ID and App Certificate in &lt;code&gt;.env&lt;/code&gt;&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;AGORA_APP_ID=
AGORA_APP_CERTIFICATE=

# Deepgram (required for speech-to-text)
DEEPGRAM_API_KEY=

# OpenAI (required for language model)
OPENAI_API_KEY=

# ElevenLabs (required for text-to-speech)
ELEVENLABS_TTS_KEY=
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;3. Start agent development containers&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker compose up -d
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;4. Enter the container&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker exec -it ten_agent_dev bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;5. Build the agent with the default example (~5-8 min)&lt;/h5&gt; 
&lt;p&gt;Check the &lt;code&gt;agents/examples&lt;/code&gt; folder for additional samples. Start with one of these defaults:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# use the chained voice assistant
cd agents/examples/voice-assistant

# or use the speech-to-speech voice assistant in real time
cd agents/examples/voice-assistant-realtime
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;6. Start the web server&lt;/h5&gt; 
&lt;p&gt;Run &lt;code&gt;task build&lt;/code&gt; if you changed any local source code. This step is required for compiled languages (for example, TypeScript or Go) and not needed for Python.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;task install
task run
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;7. Access the agent&lt;/h5&gt; 
&lt;p&gt;Once the agent example is running, you can access the following interfaces:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;&lt;strong&gt;localhost:49483&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;strong&gt;localhost:3000&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;img src="https://github.com/user-attachments/assets/191a7c0a-d8e6-48f9-866f-6a70c58f0118" alt="Screenshot 1" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://github.com/user-attachments/assets/13e482b6-d907-4449-a779-9454bb24c0b1" alt="Screenshot 2" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ul&gt; 
 &lt;li&gt;TMAN Designer: &lt;a href="http://localhost:49483"&gt;localhost:49483&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Agent Examples UI: &lt;a href="http://localhost:3000"&gt;localhost:3000&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/aec54c94-ced9-4683-ae58-0a5a7ed803bd#gh-light-mode-only" alt="divider" /&gt; &lt;img src="https://github.com/user-attachments/assets/d57fad08-4f49-4a1c-bdfc-f659a5d86150#gh-dark-mode-only" alt="divider" /&gt;&lt;/p&gt; 
&lt;h4&gt;Step ⓷ - Customize your agent example&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;Open &lt;a href="http://localhost:49483"&gt;localhost:49483&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Right-click the STT, LLM, and TTS extensions.&lt;/li&gt; 
 &lt;li&gt;Open their properties and enter the corresponding API keys.&lt;/li&gt; 
 &lt;li&gt;Submit your changes, now you can see the updated Agent Example in &lt;a href="http://localhost:3000"&gt;localhost:3000&lt;/a&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/aec54c94-ced9-4683-ae58-0a5a7ed803bd#gh-light-mode-only" alt="divider" /&gt; &lt;img src="https://github.com/user-attachments/assets/d57fad08-4f49-4a1c-bdfc-f659a5d86150#gh-dark-mode-only" alt="divider" /&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;h4&gt;Run a transcriber app from TEN Manager without Docker (Beta)&lt;/h4&gt; 
&lt;p&gt;TEN also provides a transcriber app that you can run from TEN Manager without using Docker.&lt;/p&gt; 
&lt;p&gt;Check the &lt;a href="https://theten.ai/docs/ten_framework/getting-started/quick-start"&gt;quick start guide&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/aec54c94-ced9-4683-ae58-0a5a7ed803bd#gh-light-mode-only" alt="divider" /&gt; &lt;img src="https://github.com/user-attachments/assets/d57fad08-4f49-4a1c-bdfc-f659a5d86150#gh-dark-mode-only" alt="divider" /&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;h3&gt;Codespaces&lt;/h3&gt; 
&lt;p&gt;GitHub offers free Codespaces for each repository. You can run Agent Examples in Codespaces without using Docker. Codespaces typically start faster than local Docker environments.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://codespaces.new/ten-framework/ten-agent"&gt;&lt;img src="https://github.com/codespaces/badge.svg?sanitize=true" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Check out &lt;a href="https://theten.ai/docs/ten_agent_examples/setup_development_env/setting_up_development_inside_codespace"&gt;this guide&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;div align="right"&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#readme-top"&gt;&lt;img src="https://img.shields.io/badge/-Back_to_top-gray?style=flat-square" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;h2&gt;Agent Examples Self-Hosting&lt;/h2&gt; 
&lt;h3&gt;Deploying with Docker&lt;/h3&gt; 
&lt;p&gt;Once you have customized your agent (either by using the TMAN Designer or editing &lt;code&gt;property.json&lt;/code&gt; directly), you can deploy it by creating a release Docker image for your service.&lt;/p&gt; 
&lt;h5&gt;Release as Docker image&lt;/h5&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: The following commands need to be executed outside of any Docker container.&lt;/p&gt; 
&lt;h6&gt;Build image&lt;/h6&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd ai_agents
docker build -f agents/examples/&amp;lt;example-name&amp;gt;/Dockerfile -t example-app .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h6&gt;Run&lt;/h6&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run --rm -it --env-file .env -p 3000:3000 example-app
&lt;/code&gt;&lt;/pre&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/aec54c94-ced9-4683-ae58-0a5a7ed803bd#gh-light-mode-only" alt="divider" /&gt; &lt;img src="https://github.com/user-attachments/assets/d57fad08-4f49-4a1c-bdfc-f659a5d86150#gh-dark-mode-only" alt="divider" /&gt;&lt;/p&gt; 
&lt;h3&gt;Deploying with other cloud services&lt;/h3&gt; 
&lt;p&gt;You can split the deployment into two pieces when you want to host TEN on providers such as &lt;a href="https://vercel.com"&gt;Vercel&lt;/a&gt; or &lt;a href="https://www.netlify.com"&gt;Netlify&lt;/a&gt;.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Run the TEN backend on any container-friendly platform (a VM with Docker, Fly.io, Render, ECS, Cloud Run, or similar). Use the example Docker image without modifying it and expose port &lt;code&gt;8080&lt;/code&gt; from that service.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Deploy only the frontend to Vercel or Netlify. Point the project root to &lt;code&gt;ai_agents/agents/examples/&amp;lt;example&amp;gt;/frontend&lt;/code&gt;, run &lt;code&gt;pnpm install&lt;/code&gt; (or &lt;code&gt;bun install&lt;/code&gt;) followed by &lt;code&gt;pnpm build&lt;/code&gt; (or &lt;code&gt;bun run build&lt;/code&gt;), and keep the default &lt;code&gt;.next&lt;/code&gt; output directory.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Configure environment variables in your hosting dashboard so that &lt;code&gt;AGENT_SERVER_URL&lt;/code&gt; points to the backend URL, and add any &lt;code&gt;NEXT_PUBLIC_*&lt;/code&gt; keys the UI needs (for example, Agora credentials you surface to the browser).&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Ensure your backend accepts requests from the frontend origin — via open CORS or by using the built-in proxy middleware.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;With this setup, the backend handles long-running worker processes, while the hosted frontend simply forwards API traffic to it.&lt;/p&gt; 
&lt;div align="right"&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#readme-top"&gt;&lt;img src="https://img.shields.io/badge/-Back_to_top-gray?style=flat-square" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;h2&gt;Stay Tuned&lt;/h2&gt; 
&lt;p&gt;Get instant notifications for new releases and updates. Your support helps us grow and improve TEN!&lt;/p&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/72c6cc46-a2a2-484d-82a9-f3079269c815" alt="Image" /&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;div align="right"&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#readme-top"&gt;&lt;img src="https://img.shields.io/badge/-Back_to_top-gray?style=flat-square" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;h2&gt;TEN Ecosystem&lt;/h2&gt; 
&lt;br /&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Project&lt;/th&gt; 
   &lt;th&gt;Preview&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ten-framework/ten-framework"&gt;&lt;strong&gt;️TEN Framework&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt;Open-source framework for conversational AI Agents.&lt;br /&gt;&lt;br /&gt;&lt;img src="https://img.shields.io/github/stars/ten-framework/ten-framework?color=ffcb47&amp;amp;labelColor=gray&amp;amp;style=flat-square&amp;amp;logo=github" alt="" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/user-attachments/assets/799584b2-61ff-4255-bdd1-2548d0fdba52" alt="" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ten-framework/ten-vad"&gt;&lt;strong&gt;TEN VAD&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt;Low-latency, lightweight and high-performance streaming voice activity detector (VAD).&lt;br /&gt;&lt;br /&gt;&lt;img src="https://img.shields.io/github/stars/ten-framework/ten-vad?color=ffcb47&amp;amp;labelColor=gray&amp;amp;style=flat-square&amp;amp;logo=github" alt="" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/user-attachments/assets/e504135e-67fd-4fa1-b0e4-d495358d8aa5" alt="" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ten-framework/ten-turn-detection"&gt;&lt;strong&gt;️ TEN Turn Detection&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt;TEN Turn Detection enables full-duplex dialogue communication.&lt;br /&gt;&lt;br /&gt;&lt;img src="https://img.shields.io/github/stars/ten-framework/ten-turn-detection?color=ffcb47&amp;amp;labelColor=gray&amp;amp;style=flat-square&amp;amp;logo=github" alt="" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/user-attachments/assets/c72d82cc-3667-496c-8bd6-3d194a91c452" alt="" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/TEN-framework/ten-framework/tree/main/ai_agents/agents/examples"&gt;&lt;strong&gt;TEN Agent Examples&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt;Usecases powered by TEN.&lt;br /&gt;&lt;br /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/user-attachments/assets/7f735633-c7f6-4432-b6b4-d2a2977ca588" alt="" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ten-framework/portal"&gt;&lt;strong&gt;TEN Portal&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt;The official site of the TEN Framework with documentation and a blog.&lt;br /&gt;&lt;br /&gt;&lt;img src="https://img.shields.io/github/stars/ten-framework/portal?color=ffcb47&amp;amp;labelColor=gray&amp;amp;style=flat-square&amp;amp;logo=github" alt="" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/user-attachments/assets/f56c75b9-722c-4156-902d-ae98ce2b3b5e" alt="" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;br /&gt; 
&lt;div align="right"&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#readme-top"&gt;&lt;img src="https://img.shields.io/badge/-Back_to_top-gray?style=flat-square" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;h2&gt;Questions&lt;/h2&gt; 
&lt;p&gt;TEN Framework is available on these AI-powered Q&amp;amp;A platforms. They can help you find answers quickly and accurately in multiple languages, covering everything from basic setup to advanced implementation details.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Service&lt;/th&gt; 
   &lt;th&gt;Link&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;DeepWiki&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://deepwiki.com/TEN-framework/TEN-framework"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ReadmeX&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://readmex.com/TEN-framework/ten-framework"&gt;&lt;img src="https://raw.githubusercontent.com/CodePhiliaX/resource-trusteeship/main/readmex.svg?sanitize=true" alt="ReadmeX" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;br /&gt; 
&lt;div align="right"&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#readme-top"&gt;&lt;img src="https://img.shields.io/badge/-Back_to_top-gray?style=flat-square" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome all forms of open-source collaboration! Whether you're fixing bugs, adding features, improving documentation, or sharing ideas, your contributions help advance personalized AI tools. Check out our GitHub Issues and Projects to find ways to contribute and show your skills. Together, we can build something amazing!&lt;/p&gt; 
&lt;br /&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP]&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Welcome all kinds of contributions&lt;/strong&gt; 🙏&lt;/p&gt; 
 &lt;p&gt;Join us in building TEN better! Every contribution makes a difference, from code to documentation. Share your TEN Agent projects on social media to inspire others!&lt;/p&gt; 
 &lt;p&gt;Connect with one of the TEN maintainers &lt;a href="https://x.com/elliotchen200"&gt;@elliotchen200&lt;/a&gt; on 𝕏 or &lt;a href="https://github.com/cyfyifanchen"&gt;@cyfyifanchen&lt;/a&gt; on GitHub for project updates, discussions, and collaboration opportunities.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/aec54c94-ced9-4683-ae58-0a5a7ed803bd#gh-light-mode-only" alt="divider" /&gt; &lt;img src="https://github.com/user-attachments/assets/d57fad08-4f49-4a1c-bdfc-f659a5d86150#gh-dark-mode-only" alt="divider" /&gt;&lt;/p&gt; 
&lt;h3&gt;Code Contributors&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/TEN-framework/ten-framework/graphs/contributors"&gt;&lt;img src="https://contrib.rocks/image?repo=TEN-framework/ten-framework" alt="TEN" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Contribution Guidelines&lt;/h3&gt; 
&lt;p&gt;Contributions are welcome! Please read the &lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/docs/code-of-conduct/contributing.md"&gt;contribution guidelines&lt;/a&gt; first.&lt;/p&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/aec54c94-ced9-4683-ae58-0a5a7ed803bd#gh-light-mode-only" alt="divider" /&gt; &lt;img src="https://github.com/user-attachments/assets/d57fad08-4f49-4a1c-bdfc-f659a5d86150#gh-dark-mode-only" alt="divider" /&gt;&lt;/p&gt; 
&lt;h3&gt;License&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;The entire TEN framework (except for the folders explicitly listed below) is released under the Apache License, Version 2.0, with additional restrictions. For details, please refer to the &lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/LICENSE"&gt;LICENSE&lt;/a&gt; file located in the root directory of the TEN framework.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;The components within the &lt;code&gt;packages&lt;/code&gt; directory are released under the Apache License, Version 2.0. For details, please refer to the &lt;code&gt;LICENSE&lt;/code&gt; file located in each package's root directory.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;The third-party libraries used by the TEN framework are listed and described in detail. For more information, please refer to the &lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/third_party/"&gt;third_party&lt;/a&gt; folder.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;div align="right"&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/TEN-framework/ten-framework/main/#readme-top"&gt;&lt;img src="https://img.shields.io/badge/-Back_to_top-gray?style=flat-square" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;!-- Navigation --&gt; 
&lt;!-- Header badges --&gt; 
&lt;!-- Localized READMEs --&gt; 
&lt;!-- Primary sites --&gt; 
&lt;!-- Welcome --&gt; 
&lt;!-- Community --&gt; 
&lt;!-- Agent examples --&gt; 
&lt;!-- Quick start --&gt; 
&lt;!-- Codespaces --&gt; 
&lt;!-- Deployment --&gt; 
&lt;!-- Stay tuned --&gt; 
&lt;!-- TEN ecosystem --&gt; 
&lt;!-- Contributing --&gt;</description>
    </item>
    
    <item>
      <title>MemoriLabs/Memori</title>
      <link>https://github.com/MemoriLabs/Memori</link>
      <description>&lt;p&gt;SQL Native Memory Layer for LLMs, AI Agents &amp; Multi-Agent Systems&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href="https://memorilabs.ai/"&gt;&lt;img src="https://s3.us-east-1.amazonaws.com/images.memorilabs.ai/banner.png" alt="Memori Labs" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;strong&gt;The memory fabric for enterprise AI&lt;/strong&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;i&gt;Memori plugs into the software and infrastructure you already use. It is LLM, datastore and framework agnostic and seamlessly integrates into the architecture you've already designed.&lt;/i&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/15418"&gt; &lt;img src="https://trendshift.io/_next/image?url=https%3A%2F%2Ftrendshift.io%2Fapi%2Fbadge%2Frepositories%2F15418&amp;amp;w=640&amp;amp;q=75" alt="Memori%2fLabs%2FMemori | Trendshif" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://badge.fury.io/py/memori"&gt; &lt;img src="https://badge.fury.io/py/memori.svg?sanitize=true" alt="PyPI version" /&gt; &lt;/a&gt; &lt;a href="https://pepy.tech/projects/memori"&gt; &lt;img src="https://static.pepy.tech/badge/memori" alt="Downloads" /&gt; &lt;/a&gt; &lt;a href="https://opensource.org/license/apache-2-0"&gt; &lt;img src="https://img.shields.io/badge/license-Apache%202.0-blue" alt="License" /&gt; &lt;/a&gt; &lt;a href="https://www.python.org/downloads/"&gt; &lt;img src="https://img.shields.io/badge/python-3.8+-blue.svg?sanitize=true" alt="Python 3.8+" /&gt; &lt;/a&gt; &lt;a href="https://discord.gg/abD4eGym6v"&gt; &lt;img src="https://img.shields.io/discord/1042405378304004156?logo=discord" alt="Discord" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/MemoriLabs/Memori/stargazers"&gt; &lt;img src="https://img.shields.io/badge/⭐%20Give%20a%20Star-Support%20the%20project-orange?style=for-the-badge" alt="Give a Star" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;Install Memori:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install memori
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;What's New In v3?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Significant performance improvements using Advanced Augmentation.&lt;/li&gt; 
 &lt;li&gt;Threaded, zero latency replacement for the v2 extraction agent.&lt;/li&gt; 
 &lt;li&gt;LLM agnostic with support for all of the major foundational models.&lt;/li&gt; 
 &lt;li&gt;Datastore agnostic with support for all major databases and document stores.&lt;/li&gt; 
 &lt;li&gt;Adapter/driver architecture to make contributions easier.&lt;/li&gt; 
 &lt;li&gt;Vectorized memories and in-memory semantic search for more accurate context.&lt;/li&gt; 
 &lt;li&gt;Third normal form schema including storage of semantic triples for a knowledge graph.&lt;/li&gt; 
 &lt;li&gt;Reduced development overhead to a single line of code.&lt;/li&gt; 
 &lt;li&gt;Automatic schema migrations.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Example with OpenAI&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from openai import OpenAI
from memori import Memori

client = OpenAI(...)
mem = Memori().llm.register(client)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Attribution&lt;/h2&gt; 
&lt;p&gt;To get the most out of Memori, you want to attribute your LLM interactions to an entity (think person, place or thing; like a user) and a process (think your agent, LLM interaction or program).&lt;/p&gt; 
&lt;p&gt;If you do not provide any attribution, Memori cannot make memories for you.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;mem.attribution(entity_id="12345", process_id="my-ai-bot")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Session Management&lt;/h2&gt; 
&lt;p&gt;Memori uses sessions to group your LLM interactions together. For example, if you have an agent that executes multiple steps you want those to be recorded in a single session.&lt;/p&gt; 
&lt;p&gt;By default, Memori handles setting the session for you but you can start a new session or override the session by executing the following:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;mem.new_session()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;session_id = mem.config.session_id

# ...

mem.set_session(session_id)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Suggested Setup&lt;/h2&gt; 
&lt;p&gt;To make sure everything is installed in the most efficient manner, we suggest you execute the following once:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m memori setup
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This step is not necessary but will prep your environment for faster execution. If you do not perform this step, it will be executed the first time Memori is run which will cause the first execution (and only the first one) to be a little slower.&lt;/p&gt; 
&lt;h2&gt;Configure Your Database&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Run this command once, via CI/CD or anytime you update Memori.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-python"&gt;Memori(conn=db_session_factory).config.storage.build()
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Instantiate Memori with the connection factory.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-python"&gt;from openai import OpenAI
from memori import Memori

client = OpenAI(...)
mem = Memori(conn=db_session_factory).llm.register(client)
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Quickstart Example&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import os
import sqlite3

from memori import Memori
from openai import OpenAI


def get_sqlite_connection():
    return sqlite3.connect("memori.db")


client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

memori = Memori(conn=get_sqlite_connection).llm.register(client)
memori.attribution(entity_id="123456", process_id="test-ai-agent")
memori.config.storage.build()

response = client.chat.completions.create(
    model="gpt-4.1-mini",
    messages=[
        {"role": "user", "content": "My favorite color is blue."}
    ]
)
print(response.choices[0].message.content + "\n")

# Advanced Augmentation runs asynchronously to efficiently
# create memories. For this example, a short lived command
# line program, we need to wait for it to finish.

memori.augmentation.wait()

# Memori stored that your favorite color is blue in SQLite.
# Now reset everything so there's no prior context.

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

memori = Memori(conn=get_sqlite_connection).llm.register(client)
memori.attribution(entity_id="123456", process_id="test-ai-agent")

response = client.chat.completions.create(
    model="gpt-4.1-mini",
    messages=[
        {"role": "user", "content": "What's my favorite color?"}
    ]
)
print(response.choices[0].message.content + "\n")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Supported LLM&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Anthropic&lt;/li&gt; 
 &lt;li&gt;Bedrock&lt;/li&gt; 
 &lt;li&gt;Gemini&lt;/li&gt; 
 &lt;li&gt;Grok (xAI)&lt;/li&gt; 
 &lt;li&gt;OpenAI&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;em&gt;(unstreamed, streamed, synchronous and asynchronous)&lt;/em&gt;&lt;/p&gt; 
&lt;h2&gt;Supported Frameworks&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Agno&lt;/li&gt; 
 &lt;li&gt;LangChain&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Supported Platforms&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Nebius AI Studio&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Supported Database Integrations&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;DB API 2.0&lt;/strong&gt; - Direct support for any Python database driver that implements the &lt;a href="https://peps.python.org/pep-0249/"&gt;PEP 249 Database API Specification v2.0&lt;/a&gt;. This includes drivers like &lt;code&gt;psycopg&lt;/code&gt;, &lt;code&gt;pymysql&lt;/code&gt;, &lt;code&gt;MySQLdb&lt;/code&gt;, &lt;code&gt;cx_Oracle&lt;/code&gt;, &lt;code&gt;oracledb&lt;/code&gt;, and &lt;code&gt;sqlite3&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Django&lt;/strong&gt; - Native integration with Django's ORM and database layer&lt;/li&gt; 
 &lt;li&gt;SQLAlchemy&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Supported Datastores&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/MemoriLabs/Memori/tree/main/examples/cockroachdb"&gt;CockroachDB&lt;/a&gt; - Full example with setup instructions&lt;/li&gt; 
 &lt;li&gt;MariaDB&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/MemoriLabs/Memori/tree/main/examples/mongodb"&gt;MongoDB&lt;/a&gt; - Full example with setup instructions&lt;/li&gt; 
 &lt;li&gt;MySQL&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/MemoriLabs/Memori/tree/main/examples/neon"&gt;Neon&lt;/a&gt; - Full example with setup instructions&lt;/li&gt; 
 &lt;li&gt;Oracle&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/MemoriLabs/Memori/tree/main/examples/postgres"&gt;PostgreSQL&lt;/a&gt; - Full example with setup instructions&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/MemoriLabs/Memori/tree/main/examples/sqlite"&gt;SQLite&lt;/a&gt; - Full example with setup instructions&lt;/li&gt; 
 &lt;li&gt;Supabase&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;p&gt;For more examples and demos, check out the &lt;a href="https://github.com/MemoriLabs/memori-cookbook"&gt;Memori Cookbook&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Memori Advanced Augmentation&lt;/h2&gt; 
&lt;p&gt;Memories are tracked at several different levels:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;entity: think person, place, or thing; like a user&lt;/li&gt; 
 &lt;li&gt;process: think your agent, LLM interaction or program&lt;/li&gt; 
 &lt;li&gt;session: the current interactions between the entity, process and the LLM&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://github.com/MemoriLabs/Memori/raw/main/docs/advanced-augmentation.md"&gt;Memori's Advanced Augmentation&lt;/a&gt; enhances memories at each of these levels with:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;attributes&lt;/li&gt; 
 &lt;li&gt;events&lt;/li&gt; 
 &lt;li&gt;facts&lt;/li&gt; 
 &lt;li&gt;people&lt;/li&gt; 
 &lt;li&gt;preferences&lt;/li&gt; 
 &lt;li&gt;relationships&lt;/li&gt; 
 &lt;li&gt;rules&lt;/li&gt; 
 &lt;li&gt;skills&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Memori knows who your user is, what tasks your agent handles and creates unparalleled context between the two. Augmentation occurs in the background incurring no latency.&lt;/p&gt; 
&lt;p&gt;By default, Memori Advanced Augmentation is available without an account but rate limited. When you need increased limits, &lt;a href="https://app.memorilabs.ai/signup"&gt;sign up for Memori Advanced Augmentation&lt;/a&gt; or execute the following:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m memori sign-up &amp;lt;email_address&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Memori Advanced Augmentation is always free for developers!&lt;/p&gt; 
&lt;p&gt;Once you've obtained an API key, simply set the following environment variable:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export MEMORI_API_KEY=[api_key]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Managing Your Quota&lt;/h2&gt; 
&lt;p&gt;At any time, you can check your quota by executing the following:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m memori quota
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or by checking your account at &lt;a href="https://memorilabs.ai/"&gt;https://memorilabs.ai/&lt;/a&gt;. If you have reached your IP address quota, sign up and get an API key for increased limits.&lt;/p&gt; 
&lt;p&gt;If your API key exceeds its quota limits we will email you and let you know.&lt;/p&gt; 
&lt;h2&gt;Command Line Interface (CLI)&lt;/h2&gt; 
&lt;p&gt;To use the Memori CLI, execute the following from the command line:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m memori
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will display a menu of the available options. For more information about what you can do with the Memori CLI, please reference &lt;a href="https://github.com/MemoriLabs/Memori/raw/main/docs/cli.md"&gt;Command Line Interface&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions from the community! Please see our &lt;a href="https://github.com/MemoriLabs/Memori/raw/main/CONTRIBUTING.md"&gt;Contributing Guidelines&lt;/a&gt; for details on:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Setting up your development environment&lt;/li&gt; 
 &lt;li&gt;Code style and standards&lt;/li&gt; 
 &lt;li&gt;Submitting pull requests&lt;/li&gt; 
 &lt;li&gt;Reporting issues&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Support&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Documentation&lt;/strong&gt;: &lt;a href="https://memorilabs.ai/docs"&gt;https://memorilabs.ai/docs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Discord&lt;/strong&gt;: &lt;a href="https://discord.gg/abD4eGym6v"&gt;https://discord.gg/abD4eGym6v&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Issues&lt;/strong&gt;: &lt;a href="https://github.com/MemoriLabs/Memori/issues"&gt;GitHub Issues&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Apache 2.0 - see &lt;a href="https://github.com/MemoriLabs/Memori/raw/main/LICENSE"&gt;LICENSE&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;strong&gt;Star us on GitHub&lt;/strong&gt; to support the project&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#MemoriLabs/memori"&gt;&lt;img src="https://api.star-history.com/svg?repos=MemoriLabs/memori&amp;amp;type=date" alt="Star History" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>facebookresearch/sam-3d-body</title>
      <link>https://github.com/facebookresearch/sam-3d-body</link>
      <description>&lt;p&gt;The repository provides code for running inference with the SAM 3D Body Model (3DB), links for downloading the trained model checkpoints and datasets, and example notebooks that show how to use the model.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;SAM 3D&lt;/h1&gt; 
&lt;p&gt;SAM 3D Body is one part of SAM 3D, a pair of models for object and human mesh reconstruction. If you’re looking for SAM 3D Objects, &lt;a href="https://github.com/facebookresearch/sam-3d-objects"&gt;click here&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;SAM 3D Body: Robust Full-Body Human Mesh Recovery&lt;/h1&gt; 
&lt;p align="left"&gt; &lt;a href="https://ai.meta.com/research/publications/sam-3d-body-robust-full-body-human-mesh-recovery/"&gt;&lt;img src="https://img.shields.io/badge/Meta_AI-Paper-4A90E2?logo=meta&amp;amp;logoColor=white" alt="Paper" /&gt;&lt;/a&gt; &lt;a href="https://ai.meta.com/blog/sam-3d/"&gt;&lt;img src="https://img.shields.io/badge/Project_Page-Blog-9B72F0?logo=googledocs&amp;amp;logoColor=white" alt="Blog" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/datasets/facebook/sam-3d-body-dataset"&gt;&lt;img src="https://img.shields.io/badge/🤗_Hugging_Face-Dataset-F59500?logoColor=white" alt="Dataset" /&gt;&lt;/a&gt; &lt;a href="https://www.aidemos.meta.com/segment-anything/editor/convert-body-to-3d"&gt;&lt;img src="https://img.shields.io/badge/🤸_Playground-Live_Demo-E85D5D?logoColor=white" alt="Live Demo" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;&lt;a href="https://scholar.google.com/citations?user=k0qC-7AAAAAJ&amp;amp;hl=en"&gt;Xitong Yang&lt;/a&gt;*, &lt;a href="https://www.linkedin.com/in/devanshkukreja"&gt;Devansh Kukreja&lt;/a&gt;*, &lt;a href="https://www.linkedin.com/in/don-pinkus-9140702a"&gt;Don Pinkus&lt;/a&gt;*, &lt;a href="https://www.linkedin.com/in/anushkasagar"&gt;Anushka Sagar&lt;/a&gt;, &lt;a href="https://scholar.google.com/citations?user=3PJeg1wAAAAJ&amp;amp;hl=en"&gt;Taosha Fan&lt;/a&gt;, &lt;a href="https://jindapark.github.io/"&gt;Jinhyung Park&lt;/a&gt;⚬, &lt;a href="https://yohanshin.github.io/"&gt;Soyong Shin&lt;/a&gt;⚬, &lt;a href="https://www.jinkuncao.com/"&gt;Jinkun Cao&lt;/a&gt;, &lt;a href="https://jia-wei-liu.github.io/"&gt;Jiawei Liu&lt;/a&gt;, &lt;a href="https://www.iri.upc.edu/people/nugrinovic/"&gt;Nicolas Ugrinovic&lt;/a&gt;, &lt;a href="https://scholar.google.com/citations?user=A-wA73gAAAAJ&amp;amp;hl=en&amp;amp;oi=ao"&gt;Matt Feiszli&lt;/a&gt;†, &lt;a href="https://people.eecs.berkeley.edu/~malik/"&gt;Jitendra Malik&lt;/a&gt;†, &lt;a href="https://pdollar.github.io/"&gt;Piotr Dollar&lt;/a&gt;†, &lt;a href="https://kriskitani.github.io/"&gt;Kris Kitani&lt;/a&gt;†&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Meta Superintelligence Labs&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;*Core Contributor, ⚬Intern, †Project Lead&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/facebookresearch/sam-3d-body/main/assets/model_diagram.png?raw=true" alt="SAM 3D Body Model Architecture" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;SAM 3D Body (3DB)&lt;/strong&gt; is a promptable model for single-image full-body 3D human mesh recovery (HMR). Our method demonstrates state-of-the-art performance, with strong generalization and consistent accuracy in diverse in-the-wild conditions. 3DB estimates the human pose of the body, feet, and hands based on the &lt;a href="https://github.com/facebookresearch/MHR"&gt;Momentum Human Rig&lt;/a&gt; (MHR), a new parametric mesh representation that decouples skeletal structure and surface shape for improved accuracy and interpretability.&lt;/p&gt; 
&lt;p&gt;3DB employs an encoder-decoder architecture and supports auxiliary prompts, including 2D keypoints and masks, enabling user-guided inference similar to the SAM family of models. Our model is trained on high-quality annotations from a multi-stage annotation pipeline using differentiable optimization, multi-view geometry, dense keypoint detection, and a data engine to collect and annotated data covering both common and rare poses across a wide range of viewpoints.&lt;/p&gt; 
&lt;h2&gt;Qualitative Results&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;Input&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;strong&gt;SAM 3D Body&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;CameraHMR&lt;/th&gt; 
   &lt;th align="center"&gt;NLF&lt;/th&gt; 
   &lt;th align="center"&gt;HMR2.0b&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/facebookresearch/sam-3d-body/main/assets/qualitative_comparisons/sample1/input_bbox.png" alt="Sample 1 Input" width="160" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="assets/qualitative_comparisons/sample1/SAM 3D Body.png" alt="Sample 1 - SAM 3D Body" width="160" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/facebookresearch/sam-3d-body/main/assets/qualitative_comparisons/sample1/camerahmr.png" alt="Sample 1 - CameraHMR" width="160" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/facebookresearch/sam-3d-body/main/assets/qualitative_comparisons/sample1/nlf.png" alt="Sample 1 - NLF" width="160" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/facebookresearch/sam-3d-body/main/assets/qualitative_comparisons/sample1/4dhumans.png" alt="Sample 1 - 4DHumans (HMR2.0b)" width="160" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/facebookresearch/sam-3d-body/main/assets/qualitative_comparisons/sample2/input_bbox.png" alt="Sample 2 Input" width="160" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="assets/qualitative_comparisons/sample2/SAM 3D Body.png" alt="Sample 2 - SAM 3D Body" width="160" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/facebookresearch/sam-3d-body/main/assets/qualitative_comparisons/sample2/camerahmr.png" alt="Sample 2 - CameraHMR" width="160" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/facebookresearch/sam-3d-body/main/assets/qualitative_comparisons/sample2/nlf.png" alt="Sample 2 - NLF" width="160" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/facebookresearch/sam-3d-body/main/assets/qualitative_comparisons/sample2/4dhumans.png" alt="Sample 2 - 4DHumans (HMR2.0b)" width="160" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/facebookresearch/sam-3d-body/main/assets/qualitative_comparisons/sample3/input_bbox.png" alt="Sample 3 Input" width="160" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="assets/qualitative_comparisons/sample3/SAM 3D Body.png" alt="Sample 3 - SAM 3D Body" width="160" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/facebookresearch/sam-3d-body/main/assets/qualitative_comparisons/sample3/camerahmr.png" alt="Sample 3 - CameraHMR" width="160" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/facebookresearch/sam-3d-body/main/assets/qualitative_comparisons/sample3/nlf.png" alt="Sample 3 - NLF" width="160" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/facebookresearch/sam-3d-body/main/assets/qualitative_comparisons/sample3/4dhumans.png" alt="Sample 3 - 4DHumans (HMR2.0b)" width="160" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/facebookresearch/sam-3d-body/main/assets/qualitative_comparisons/sample4/input_bbox.png" alt="Sample 4 Input" width="160" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="assets/qualitative_comparisons/sample4/SAM 3D Body.png" alt="Sample 4 - SAM 3D Body" width="160" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/facebookresearch/sam-3d-body/main/assets/qualitative_comparisons/sample4/camerahmr.png" alt="Sample 4 - CameraHMR" width="160" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/facebookresearch/sam-3d-body/main/assets/qualitative_comparisons/sample4/nlf.png" alt="Sample 4 - NLF" width="160" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/facebookresearch/sam-3d-body/main/assets/qualitative_comparisons/sample4/4dhumans.png" alt="Sample 4 - 4DHumans (HMR2.0b)" width="160" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;em&gt;Our SAM 3D Body demonstrates superior reconstruction quality with more accurate pose estimation, better shape recovery, and improved handling of occlusions and challenging viewpoints compared to existing approaches.&lt;/em&gt;&lt;/p&gt; 
&lt;h2&gt;Latest updates&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;11/19/2025&lt;/strong&gt; -- Checkpoints Launched, Dataset Released, Web Demo and Paper are out!&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/facebookresearch/sam-3d-body/main/INSTALL.md"&gt;INSTALL.md&lt;/a&gt; for instructions for python environment setup and model checkpoint access.&lt;/p&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;3DB can reconstruct 3D full-body human mesh from a single image, optionally with keypoint/mask prompts and/or hand refinement from the hand decoder.&lt;/p&gt; 
&lt;p&gt;For a quick start, run our demo script for model inference and visualization with models from &lt;a href="https://huggingface.co/facebook"&gt;Hugging Face&lt;/a&gt; (please make sure to follow &lt;a href="https://raw.githubusercontent.com/facebookresearch/sam-3d-body/main/INSTALL.md"&gt;INSTALL.md&lt;/a&gt; to request access to our checkpoints.).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Download assets from HuggingFace
hf download facebook/sam-3d-body-dinov3 --local-dir checkpoints/sam-3d-body-dinov3

# Run demo script with default ViTdet detector and MoGe2 FOV model
python demo.py \
    --image_folder &amp;lt;path_to_images&amp;gt; \
    --output_folder &amp;lt;path_to_output&amp;gt; \
    --checkpoint_path ./checkpoints/sam-3d-body-dinov3/model.ckpt \
    --mhr_path ./checkpoints/sam-3d-body-dinov3/assets/mhr_model.pt

# To use SAM3 as the detector to align with online playground of SAM3D
python demo.py \
    --image_folder &amp;lt;path_to_images&amp;gt; \
    --output_folder &amp;lt;path_to_output&amp;gt; \
    --checkpoint_path ./checkpoints/sam-3d-body-dinov3/model.ckpt \
    --mhr_path ./checkpoints/sam-3d-body-dinov3/assets/mhr_model.pt \
    --detector_name sam3
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also try the following lines of code with models loaded directly from &lt;a href="https://huggingface.co/facebook"&gt;Hugging Face&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import cv2
import numpy as np
from notebook.utils import setup_sam_3d_body
from tools.vis_utils import visualize_sample_together

# Set up the estimator
estimator = setup_sam_3d_body(hf_repo_id="facebook/sam-3d-body-dinov3")

# Load and process image
img_bgr = cv2.imread("path/to/image.jpg")
outputs = estimator.process_one_image(cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB))

# Visualize and save results
rend_img = visualize_sample_together(img_bgr, outputs, estimator.faces)
cv2.imwrite("output.jpg", rend_img.astype(np.uint8))
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For a complete demo with visualization, see &lt;a href="https://raw.githubusercontent.com/facebookresearch/sam-3d-body/main/notebook/demo_human.ipynb"&gt;notebook/demo_human.ipynb&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Model Description&lt;/h2&gt; 
&lt;h3&gt;SAM 3D Body checkpoints&lt;/h3&gt; 
&lt;p&gt;The table below shows the performance of SAM 3D Body checkpoints released on 11/19/2025.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;&lt;strong&gt;Backbone (size)&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;strong&gt;3DPW (MPJPE)&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;strong&gt;EMDB (MPJPE)&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;strong&gt;RICH (PVE)&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;strong&gt;COCO (PCK@.05)&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;strong&gt;LSPET (PCK@.05)&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;strong&gt;Freihand (PA-MPJPE)&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;DINOv3-H+ (840M) &lt;br /&gt; (&lt;a href="https://huggingface.co/facebook/sam-3d-body-dinov3/blob/main/model_config.yaml"&gt;config&lt;/a&gt;, &lt;a href="https://huggingface.co/facebook/sam-3d-body-dinov3/blob/main/model.ckpt"&gt;checkpoint&lt;/a&gt;)&lt;/td&gt; 
   &lt;td align="center"&gt;54.8&lt;/td&gt; 
   &lt;td align="center"&gt;61.7&lt;/td&gt; 
   &lt;td align="center"&gt;60.3&lt;/td&gt; 
   &lt;td align="center"&gt;86.5&lt;/td&gt; 
   &lt;td align="center"&gt;68.0&lt;/td&gt; 
   &lt;td align="center"&gt;5.5&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;ViT-H (631M) &lt;br /&gt; (&lt;a href="https://huggingface.co/facebook/sam-3d-body-vith/blob/main/model_config.yaml"&gt;config&lt;/a&gt;, &lt;a href="https://huggingface.co/facebook/sam-3d-body-vith/blob/main/model.ckpt"&gt;checkpoint&lt;/a&gt;)&lt;/td&gt; 
   &lt;td align="center"&gt;54.8&lt;/td&gt; 
   &lt;td align="center"&gt;62.9&lt;/td&gt; 
   &lt;td align="center"&gt;61.7&lt;/td&gt; 
   &lt;td align="center"&gt;86.8&lt;/td&gt; 
   &lt;td align="center"&gt;68.9&lt;/td&gt; 
   &lt;td align="center"&gt;5.5&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;SAM 3D Body Dataset&lt;/h2&gt; 
&lt;p&gt;The SAM 3D Body data is released on &lt;a href="https://huggingface.co/datasets/facebook/sam-3d-body-dataset"&gt;Hugging Face&lt;/a&gt;. Please follow the &lt;a href="https://raw.githubusercontent.com/facebookresearch/sam-3d-body/main/data/README.md"&gt;instructions&lt;/a&gt; to download and process the data.&lt;/p&gt; 
&lt;h2&gt;SAM 3D Objects&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/facebookresearch/sam-3d-objects"&gt;SAM 3D Objects&lt;/a&gt; is a foundation model that reconstructs full 3D shape geometry, texture, and layout from a single image.&lt;/p&gt; 
&lt;p&gt;As a way to combine the strengths of both &lt;strong&gt;SAM 3D Objects&lt;/strong&gt; and &lt;strong&gt;SAM 3D Body&lt;/strong&gt;, we provide an example notebook that demonstrates how to combine the results of both models such that they are aligned in the same frame of reference. Check it out &lt;a href="https://github.com/facebookresearch/sam-3d-objects/raw/main/notebook/demo_3db_mesh_alignment.ipynb"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;The SAM 3D Body model checkpoints and code are licensed under &lt;a href="https://raw.githubusercontent.com/facebookresearch/sam-3d-body/main/LICENSE"&gt;SAM License&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/facebookresearch/sam-3d-body/main/CONTRIBUTING.md"&gt;contributing&lt;/a&gt; and the &lt;a href="https://raw.githubusercontent.com/facebookresearch/sam-3d-body/main/CODE_OF_CONDUCT.md"&gt;code of conduct&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributors&lt;/h2&gt; 
&lt;p&gt;The SAM 3D Body project was made possible with the help of many contributors: Vivian Lee, George Orlin, Nikhila Ravi, Andrew Westbury, Jyun-Ting Song, Zejia Weng, Xizi Zhang, Yuting Ye, Federica Bogo, Ronald Mallet, Ahmed Osman, Rawal Khirodkar, Javier Romero, Carsten Stoll, Jean-Charles Bazin, Sofien Bouaziz, Yuan Dong, Su Zhaoen, Fabian Prada, Alexander Richard, Michael Zollhoefer, Roman Rädle, Sasha Mitts, Michelle Chan, Yael Yungster, Azita Shokrpour, Helen Klein, Mallika Malhotra, Ida Cheng, Eva Galper.&lt;/p&gt; 
&lt;h2&gt;Citing SAM 3D Body&lt;/h2&gt; 
&lt;p&gt;If you use SAM 3D Body or the SAM 3D Body dataset in your research, please use the following BibTeX entry.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@article{yang2025sam3dbody,
  title={SAM 3D Body: Robust Full-Body Human Mesh Recovery},
  author={Yang, Xitong and Kukreja, Devansh and Pinkus, Don and Sagar, Anushka and Fan, Taosha and Park, Jinhyung and Shin, Soyong and Cao, Jinkun and Liu, Jiawei and Ugrinovic, Nicolas and Feiszli, Matt and Malik, Jitendra and Dollar, Piotr and Kitani, Kris},
  journal={arXiv preprint; identifier to be added},
  year={2025}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>datawhalechina/hello-agents</title>
      <link>https://github.com/datawhalechina/hello-agents</link>
      <description>&lt;p&gt;📚 《从零开始构建智能体》——从零开始的智能体原理与实践教程&lt;/p&gt;&lt;hr&gt;&lt;div align="right"&gt; 
 &lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/README_EN.md"&gt;English&lt;/a&gt; | 中文 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/images/hello-agents.png" alt="alt text" width="100%" /&gt; 
 &lt;h1&gt;Hello-Agents&lt;/h1&gt; 
 &lt;h3&gt;🤖 《从零开始构建智能体》&lt;/h3&gt; 
 &lt;div align="center"&gt; 
  &lt;a href="https://trendshift.io/repositories/15520" target="_blank"&gt; &lt;img src="https://trendshift.io/api/badge/repositories/15520" alt="datawhalechina%2Fhello-agents | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt; &lt;/a&gt; 
 &lt;/div&gt; 
 &lt;p&gt;&lt;em&gt;从基础理论到实际应用，全面掌握智能体系统的设计与实现&lt;/em&gt;&lt;/p&gt; 
 &lt;img src="https://img.shields.io/github/stars/datawhalechina/Hello-Agents?style=flat&amp;amp;logo=github" alt="GitHub stars" /&gt; 
 &lt;img src="https://img.shields.io/github/forks/datawhalechina/Hello-Agents?style=flat&amp;amp;logo=github" alt="GitHub forks" /&gt; 
 &lt;img src="https://img.shields.io/badge/language-Chinese-brightgreen?style=flat" alt="Language" /&gt; 
 &lt;a href="https://github.com/datawhalechina/Hello-Agents"&gt;&lt;img src="https://img.shields.io/badge/GitHub-Project-blue?style=flat&amp;amp;logo=github" alt="GitHub Project" /&gt;&lt;/a&gt; 
 &lt;a href="https://datawhalechina.github.io/hello-agents/"&gt;&lt;img src="https://img.shields.io/badge/在线阅读-Online%20Reading-green?style=flat&amp;amp;logo=gitbook" alt="Online Reading" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;🎯 项目介绍&lt;/h2&gt; 
&lt;p&gt;  如果说 2024 年是"百模大战"的元年，那么 2025 年无疑开启了"Agent 元年"。技术的焦点正从训练更大的基础模型，转向构建更聪明的智能体应用。然而，当前系统性、重实践的教程却极度匮乏。为此，我们发起了 Hello-Agents 项目，希望能为社区提供一本从零开始、理论与实战并重的智能体系统构建指南。&lt;/p&gt; 
&lt;p&gt;  Hello-Agents 是 Datawhale 社区的&lt;strong&gt;系统性智能体学习教程&lt;/strong&gt;。如今 Agent 构建主要分为两派，一派是 Dify，Coze，n8n 这类软件工程类 Agent，其本质是流程驱动的软件开发，LLM 作为数据处理的后端；另一派则是 AI 原生的 Agent，即真正以 AI 驱动的 Agent。本教程旨在带领大家深入理解并构建后者——真正的 AI Native Agent。教程将带领你穿透框架表象，从智能体的核心原理出发，深入其核心架构，理解其经典范式，并最终亲手构建起属于自己的多智能体应用。我们相信，最好的学习方式就是动手实践。希望这本教程能成为你探索智能体世界的起点，能够从一名大语言模型的"使用者"，蜕变为一名智能体系统的"构建者"。&lt;/p&gt; 
&lt;h2&gt;📚 快速开始&lt;/h2&gt; 
&lt;h3&gt;在线阅读&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://datawhalechina.github.io/hello-agents/"&gt;🌐 点击这里开始在线阅读&lt;/a&gt;&lt;/strong&gt; - 无需下载，随时随地学习&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://book.heterocat.com.cn/"&gt;📖 Cookbook(测试版)&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h3&gt;本地阅读&lt;/h3&gt; 
&lt;p&gt;如果您希望在本地阅读或贡献内容，请参考下方的学习指南。&lt;/p&gt; 
&lt;h3&gt;✨ 你将收获什么？&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;📖 &lt;strong&gt;Datawhale 开源免费&lt;/strong&gt; 完全免费学习本项目所有内容，与社区共同成长&lt;/li&gt; 
 &lt;li&gt;🔍 &lt;strong&gt;理解核心原理&lt;/strong&gt; 深入理解智能体的概念、历史与经典范式&lt;/li&gt; 
 &lt;li&gt;🏗️ &lt;strong&gt;亲手实现&lt;/strong&gt; 掌握热门低代码平台和智能体代码框架的使用&lt;/li&gt; 
 &lt;li&gt;🛠️ &lt;strong&gt;自研框架&lt;a href="https://github.com/jjyaoao/helloagents"&gt;HelloAgents&lt;/a&gt;&lt;/strong&gt; 基于 Openai 原生 API 从零构建一个自己的智能体框架&lt;/li&gt; 
 &lt;li&gt;⚙️ &lt;strong&gt;掌握高级技能&lt;/strong&gt; 一步步实现上下文工程、Memory、协议、评估等系统性技术&lt;/li&gt; 
 &lt;li&gt;🤝 &lt;strong&gt;模型训练&lt;/strong&gt; 掌握 Agentic RL，从 SFT 到 GRPO 的全流程实战训练 LLM&lt;/li&gt; 
 &lt;li&gt;🚀 &lt;strong&gt;驱动真实案例&lt;/strong&gt; 实战开发智能旅行助手、赛博小镇等综合项目&lt;/li&gt; 
 &lt;li&gt;📖 &lt;strong&gt;求职面试&lt;/strong&gt; 学习智能体求职相关面试问题&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;📖 内容导航&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;章节&lt;/th&gt; 
   &lt;th&gt;关键内容&lt;/th&gt; 
   &lt;th&gt;状态&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/%E5%89%8D%E8%A8%80.md"&gt;前言&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;项目的缘起、背景及读者建议&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;第一部分：智能体与语言模型基础&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter1/%E7%AC%AC%E4%B8%80%E7%AB%A0%20%E5%88%9D%E8%AF%86%E6%99%BA%E8%83%BD%E4%BD%93.md"&gt;第一章 初识智能体&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;智能体定义、类型、范式与应用&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter2/%E7%AC%AC%E4%BA%8C%E7%AB%A0%20%E6%99%BA%E8%83%BD%E4%BD%93%E5%8F%91%E5%B1%95%E5%8F%B2.md"&gt;第二章 智能体发展史&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;从符号主义到 LLM 驱动的智能体演进&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter3/%E7%AC%AC%E4%B8%89%E7%AB%A0%20%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80.md"&gt;第三章 大语言模型基础&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Transformer、提示、主流 LLM 及其局限&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;第二部分：构建你的大语言模型智能体&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter4/%E7%AC%AC%E5%9B%9B%E7%AB%A0%20%E6%99%BA%E8%83%BD%E4%BD%93%E7%BB%8F%E5%85%B8%E8%8C%83%E5%BC%8F%E6%9E%84%E5%BB%BA.md"&gt;第四章 智能体经典范式构建&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;手把手实现 ReAct、Plan-and-Solve、Reflection&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter5/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%9F%BA%E4%BA%8E%E4%BD%8E%E4%BB%A3%E7%A0%81%E5%B9%B3%E5%8F%B0%E7%9A%84%E6%99%BA%E8%83%BD%E4%BD%93%E6%90%AD%E5%BB%BA.md"&gt;第五章 基于低代码平台的智能体搭建&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;了解 Coze、Dify、n8n 等低代码智能体平台使用&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter6/%E7%AC%AC%E5%85%AD%E7%AB%A0%20%E6%A1%86%E6%9E%B6%E5%BC%80%E5%8F%91%E5%AE%9E%E8%B7%B5.md"&gt;第六章 框架开发实践&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;AutoGen、AgentScope、LangGraph 等主流框架应用&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter7/%E7%AC%AC%E4%B8%83%E7%AB%A0%20%E6%9E%84%E5%BB%BA%E4%BD%A0%E7%9A%84Agent%E6%A1%86%E6%9E%B6.md"&gt;第七章 构建你的Agent框架&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;从 0 开始构建智能体框架&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;第三部分：高级知识扩展&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter8/%E7%AC%AC%E5%85%AB%E7%AB%A0%20%E8%AE%B0%E5%BF%86%E4%B8%8E%E6%A3%80%E7%B4%A2.md"&gt;第八章 记忆与检索&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;记忆系统，RAG，存储&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter9/%E7%AC%AC%E4%B9%9D%E7%AB%A0%20%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B.md"&gt;第九章 上下文工程&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;持续交互的"情境理解"&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter10/%E7%AC%AC%E5%8D%81%E7%AB%A0%20%E6%99%BA%E8%83%BD%E4%BD%93%E9%80%9A%E4%BF%A1%E5%8D%8F%E8%AE%AE.md"&gt;第十章 智能体通信协议&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;MCP、A2A、ANP 等协议解析&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter11/%E7%AC%AC%E5%8D%81%E4%B8%80%E7%AB%A0%20Agentic-RL.md"&gt;第十一章 Agentic-RL&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;从 SFT 到 GRPO 的 LLM 训练实战&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter12/%E7%AC%AC%E5%8D%81%E4%BA%8C%E7%AB%A0%20%E6%99%BA%E8%83%BD%E4%BD%93%E6%80%A7%E8%83%BD%E8%AF%84%E4%BC%B0.md"&gt;第十二章 智能体性能评估&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;核心指标、基准测试与评估框架&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;第四部分：综合案例进阶&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter13/%E7%AC%AC%E5%8D%81%E4%B8%89%E7%AB%A0%20%E6%99%BA%E8%83%BD%E6%97%85%E8%A1%8C%E5%8A%A9%E6%89%8B.md"&gt;第十三章 智能旅行助手&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;MCP 与多智能体协作的真实世界应用&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter14/%E7%AC%AC%E5%8D%81%E5%9B%9B%E7%AB%A0%20%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A0%94%E7%A9%B6%E6%99%BA%E8%83%BD%E4%BD%93.md"&gt;第十四章 自动化深度研究智能体&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;DeepResearch Agent 复现与解析&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter15/%E7%AC%AC%E5%8D%81%E4%BA%94%E7%AB%A0%20%E6%9E%84%E5%BB%BA%E8%B5%9B%E5%8D%9A%E5%B0%8F%E9%95%87.md"&gt;第十五章 构建赛博小镇&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Agent 与游戏的结合，模拟社会动态&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;第五部分：毕业设计及未来展望&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/chapter16/%E7%AC%AC%E5%8D%81%E5%85%AD%E7%AB%A0%20%E6%AF%95%E4%B8%9A%E8%AE%BE%E8%AE%A1.md"&gt;第十六章 毕业设计&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;构建属于你的完整多智能体应用&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;社区贡献精选 (Community Blog)&lt;/h3&gt; 
&lt;p&gt;  欢迎大家将在学习 Hello-Agents 或 Agent 相关技术中的独到见解、实践总结，以 PR 的形式贡献到社区精选。如果是独立于正文的内容，也可以投稿至 Extra-Chapter！&lt;strong&gt;期待你的第一次贡献！&lt;/strong&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;社区精选&lt;/th&gt; 
   &lt;th&gt;内容总结&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/datawhalechina/hello-agents/raw/main/Extra-Chapter/Extra01-%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93.md"&gt;01-Agent面试题总结&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Agent 岗位相关面试问题&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/datawhalechina/hello-agents/raw/main/Extra-Chapter/Extra01-%E5%8F%82%E8%80%83%E7%AD%94%E6%A1%88.md"&gt;01-Agent面试题答案&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;相关面试问题答案&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/datawhalechina/hello-agents/raw/main/Extra-Chapter/Extra02-%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%E8%A1%A5%E5%85%85%E7%9F%A5%E8%AF%86.md"&gt;02-上下文工程内容补充&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;上下文工程内容扩展&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/datawhalechina/hello-agents/raw/main/Extra-Chapter/Extra03-Dify%E6%99%BA%E8%83%BD%E4%BD%93%E5%88%9B%E5%BB%BA%E4%BF%9D%E5%A7%86%E7%BA%A7%E6%93%8D%E4%BD%9C%E6%B5%81%E7%A8%8B.md"&gt;03-Dify智能体创建保姆级教程&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Dify智能体创建保姆级教程&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/datawhalechina/hello-agents/raw/main/Extra-Chapter/Extra04-DatawhaleFAQ.md"&gt;04-Hello-agents课程常见问题&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Datawhale课程常见问题&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/datawhalechina/hello-agents/raw/main/Extra-Chapter/Extra05-AgentSkills%E8%A7%A3%E8%AF%BB.md"&gt;05-Agent Skills与MCP对比解读&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Agent Skills与MCP技术对比&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/datawhalechina/hello-agents/raw/main/Extra-Chapter/Extra06-GUIAgent%E7%A7%91%E6%99%AE%E4%B8%8E%E5%AE%9E%E6%88%98.md"&gt;06-GUI Agent科普与实战&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;GUI Agent科普与多场景实战&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;PDF 版本下载&lt;/h3&gt; 
&lt;p&gt;  &lt;em&gt;&lt;strong&gt;本 Hello-Agents PDF 教程完全开源免费。为防止各类营销号加水印后贩卖给多智能体系统初学者，我们特地在 PDF 文件中预先添加了不影响阅读的 Datawhale 开源标志水印，敬请谅解～&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;em&gt;Hello-Agents PDF : &lt;a href="https://github.com/datawhalechina/hello-agents/releases/tag/V1.0.0"&gt;https://github.com/datawhalechina/hello-agents/releases/tag/V1.0.0&lt;/a&gt;&lt;/em&gt;&lt;br /&gt; &lt;em&gt;Hello-Agents PDF 国内下载地址 : &lt;a href="https://www.datawhale.cn/learn/summary/239"&gt;https://www.datawhale.cn/learn/summary/239&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;💡 如何学习&lt;/h2&gt; 
&lt;p&gt;  欢迎你，未来的智能系统构建者！在开启这段激动人心的旅程之前，请允许我们给你一些清晰的指引。&lt;/p&gt; 
&lt;p&gt;  本项目内容兼顾理论与实战，旨在帮助你系统性地掌握从单个智能体到多智能体系统的设计与开发全流程。因此，尤其适合有一定编程基础的 &lt;strong&gt;AI 开发者、软件工程师、在校学生&lt;/strong&gt; 以及对前沿 AI 技术抱有浓厚兴趣的 &lt;strong&gt;自学者&lt;/strong&gt;。在学习本项目之前，我们希望你具备基础的 Python 编程能力，并对大语言模型有基本的概念性了解（例如，知道如何通过 API 调用一个 LLM）。项目的重点是应用与构建，因此你无需具备深厚的算法或模型训练背景。&lt;/p&gt; 
&lt;p&gt;  项目分为五大部分，每一部分都是通往下一阶段的坚实阶梯：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;第一部分：智能体与语言模型基础&lt;/strong&gt;（第一章～第三章），我们将从智能体的定义、类型与发展历史讲起，为你梳理"智能体"这一概念的来龙去脉。随后，我们会快速巩固大语言模型的核心知识，为你的实践之旅打下坚实的理论地基。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;第二部分：构建你的大语言模型智能体&lt;/strong&gt;（第四章～第七章），这是你动手实践的起点。你将亲手实现 ReAct 等经典范式，体验 Coze 等低代码平台的便捷，并掌握 Langgraph 等主流框架的应用。最终，我们还会带你从零开始构建一个属于自己的智能体框架，让你兼具“用轮子”与“造轮子”的能力。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;第三部分：高级知识扩展&lt;/strong&gt;（第八章～第十二章），在这一部分，你的智能体将“学会”思考与协作。我们将使用第二部分的自研框架，深入探索记忆与检索、上下文工程、Agent 训练等核心技术，并学习多智能体间的通信协议。最终，你将掌握评估智能体系统性能的专业方法。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;第四部分：综合案例进阶&lt;/strong&gt;（第十三章～第十五章），这里是理论与实践的交汇点。你将把所学融会贯通，亲手打造智能旅行助手、自动化深度研究智能体，乃至一个模拟社会动态的赛博小镇，在真实有趣的项目中淬炼你的构建能力。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;第五部分：毕业设计及未来展望&lt;/strong&gt;（第十六章），在旅程的终点，你将迎来一个毕业设计，构建一个完整的、属于你自己的多智能体应用，全面检验你的学习成果。我们还将与你一同展望智能体的未来，探索激动人心的前沿方向。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;  智能体是一个飞速发展且极度依赖实践的领域。为了获得最佳的学习效果，我们在项目的&lt;code&gt;code&lt;/code&gt;文件夹内提供了配套的全部代码，强烈建议你&lt;strong&gt;将理论与实践相结合&lt;/strong&gt;。请务必亲手运行、调试甚至修改项目里提供的每一份代码。欢迎你随时关注 Datawhale 以及其他 Agent 相关社区，当遇到问题时，你可以随时在本项目的 issue 区提问。&lt;/p&gt; 
&lt;p&gt;  现在，准备好进入智能体的奇妙世界了吗？让我们即刻启程！&lt;/p&gt; 
&lt;h2&gt;下一步规划&lt;/h2&gt; 
&lt;p&gt;双语视频课程[英文+中文]（将会更加细致，实践课带领大家从设计思路到实施，授人以鱼也授人以渔）&lt;/p&gt; 
&lt;h2&gt;🤝 如何贡献&lt;/h2&gt; 
&lt;p&gt;我们是一个开放的开源社区，欢迎任何形式的贡献！&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;🐛 &lt;strong&gt;报告 Bug&lt;/strong&gt; - 发现内容或代码问题，请提交 Issue&lt;/li&gt; 
 &lt;li&gt;💡 &lt;strong&gt;提出建议&lt;/strong&gt; - 对项目有好想法，欢迎发起讨论&lt;/li&gt; 
 &lt;li&gt;📝 &lt;strong&gt;完善内容&lt;/strong&gt; - 帮助改进教程，提交你的 Pull Request&lt;/li&gt; 
 &lt;li&gt;✍️ &lt;strong&gt;分享实践&lt;/strong&gt; - 在"社区贡献精选"中分享你的学习笔记和项目&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🙏 致谢&lt;/h2&gt; 
&lt;h3&gt;核心贡献者&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/jjyaoao"&gt;陈思州-项目负责人&lt;/a&gt; (Datawhale 成员, 全文写作和校对)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/fengju0213"&gt;孙韬-项目负责人&lt;/a&gt; (Datawhale 成员, 第九章内容和校对)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Tsumugii24"&gt;姜舒凡-项目负责人&lt;/a&gt;（Datawhale 成员, 章节习题设计和校对）&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/HeteroCat"&gt;黄佩林-Datawhale意向成员&lt;/a&gt; (Agent 开发工程师, 第五章内容贡献者)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/fancyboi999"&gt;曾鑫民-Agent工程师&lt;/a&gt; (牛客科技, 第十四章案例开发)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://xinzhongzhu.github.io/"&gt;朱信忠-指导专家&lt;/a&gt; (Datawhale首席科学家-浙江师范大学杭州人工智能研究院教授)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Extra-Chapter 贡献者&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/WHQAQ11"&gt;WH&lt;/a&gt; (内容贡献者)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/thunderbolt-fire"&gt;周奥杰-DW贡献者团队&lt;/a&gt; (西安交通大学, Extra02 内容贡献)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Tasselszcx"&gt;张宸旭-个人开发者&lt;/a&gt;(帝国理工学院, Extra03 内容贡献)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/XiaoMa-PM"&gt;黄宏晗-DW贡献者团队&lt;/a&gt; (深圳大学, Extra04 内容贡献)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;特别感谢&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;感谢 &lt;a href="https://github.com/Sm1les"&gt;@Sm1les&lt;/a&gt; 对本项目的帮助与支持&lt;/li&gt; 
 &lt;li&gt;感谢所有为本项目做出贡献的开发者们 ❤️&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align="center" style="margin-top: 30px;"&gt; 
 &lt;a href="https://github.com/datawhalechina/Hello-Agents/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=datawhalechina/Hello-Agents" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/images/star-history-20251217.png" alt="Datawhale" width="90%" /&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;⭐ 如果这个项目对你有帮助，请给我们一个 Star！&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;关于 Datawhale&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/datawhalechina/hello-agents/main/docs/images/datawhale.png" alt="Datawhale" width="30%" /&gt; 
 &lt;p&gt;扫描二维码关注 Datawhale 公众号，获取更多优质开源内容&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;📜 开源协议&lt;/h2&gt; 
&lt;p&gt;本作品采用&lt;a href="http://creativecommons.org/licenses/by-nc-sa/4.0/"&gt;知识共享署名-非商业性使用-相同方式共享 4.0 国际许可协议&lt;/a&gt;进行许可。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>HKUDS/LightRAG</title>
      <link>https://github.com/HKUDS/LightRAG</link>
      <description>&lt;p&gt;[EMNLP2025] "LightRAG: Simple and Fast Retrieval-Augmented Generation"&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;div style="margin: 20px 0;"&gt; 
  &lt;img src="https://raw.githubusercontent.com/HKUDS/LightRAG/main/assets/logo.png" width="120" height="120" alt="LightRAG Logo" style="border-radius: 20px; box-shadow: 0 8px 32px rgba(0, 217, 255, 0.3);" /&gt; 
 &lt;/div&gt; 
 &lt;h1&gt;🚀 LightRAG: Simple and Fast Retrieval-Augmented Generation&lt;/h1&gt; 
 &lt;div align="center"&gt; 
  &lt;a href="https://trendshift.io/repositories/13043" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/13043" alt="HKUDS%2FLightRAG | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; 
 &lt;/div&gt; 
 &lt;div align="center"&gt; 
  &lt;div style="width: 100%; height: 2px; margin: 20px 0; background: linear-gradient(90deg, transparent, #00d9ff, transparent);"&gt;&lt;/div&gt; 
 &lt;/div&gt; 
 &lt;div align="center"&gt; 
  &lt;div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 15px; padding: 25px; text-align: center;"&gt; 
   &lt;p&gt; &lt;a href="https://github.com/HKUDS/LightRAG"&gt;&lt;img src="https://img.shields.io/badge/🔥Project-Page-00d9ff?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2410.05779"&gt;&lt;img src="https://img.shields.io/badge/📄arXiv-2410.05779-ff6b6b?style=for-the-badge&amp;amp;logo=arxiv&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;a href="https://github.com/HKUDS/LightRAG/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/HKUDS/LightRAG?color=00d9ff&amp;amp;style=for-the-badge&amp;amp;logo=star&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;/p&gt; 
   &lt;p&gt; &lt;img src="https://img.shields.io/badge/🐍Python-3.10-4ecdc4?style=for-the-badge&amp;amp;logo=python&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt; &lt;a href="https://pypi.org/project/lightrag-hku/"&gt;&lt;img src="https://img.shields.io/pypi/v/lightrag-hku.svg?style=for-the-badge&amp;amp;logo=pypi&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e&amp;amp;color=ff6b6b" /&gt;&lt;/a&gt; &lt;/p&gt; 
   &lt;p&gt; &lt;a href="https://discord.gg/yF2MmDJyGJ"&gt;&lt;img src="https://img.shields.io/badge/💬Discord-Community-7289da?style=for-the-badge&amp;amp;logo=discord&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;a href="https://github.com/HKUDS/LightRAG/issues/285"&gt;&lt;img src="https://img.shields.io/badge/💬WeChat-Group-07c160?style=for-the-badge&amp;amp;logo=wechat&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;/p&gt; 
   &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/LightRAG/main/README-zh.md"&gt;&lt;img src="https://img.shields.io/badge/🇨🇳中文版-1a1a2e?style=for-the-badge" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/LightRAG/main/README.md"&gt;&lt;img src="https://img.shields.io/badge/🇺🇸English-1a1a2e?style=for-the-badge" /&gt;&lt;/a&gt; &lt;/p&gt; 
   &lt;p&gt; &lt;a href="https://pepy.tech/projects/lightrag-hku"&gt;&lt;img src="https://static.pepy.tech/personalized-badge/lightrag-hku?period=total&amp;amp;units=INTERNATIONAL_SYSTEM&amp;amp;left_color=BLACK&amp;amp;right_color=GREEN&amp;amp;left_text=downloads" /&gt;&lt;/a&gt; &lt;/p&gt; 
  &lt;/div&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div align="center" style="margin: 30px 0;"&gt; 
 &lt;img src="https://user-images.githubusercontent.com/74038190/212284100-561aa473-3905-4a80-b561-0d28506553ee.gif" width="800" /&gt; 
&lt;/div&gt; 
&lt;div align="center" style="margin: 30px 0;"&gt; 
 &lt;img src="https://raw.githubusercontent.com/HKUDS/LightRAG/main/README.assets/b2aaf634151b4706892693ffb43d9093.png" width="800" alt="LightRAG Diagram" /&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;🎉 News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;[2025.11]🎯[New Feature]: Integrated &lt;strong&gt;RAGAS for Evaluation&lt;/strong&gt; and &lt;strong&gt;Langfuse for Tracing&lt;/strong&gt;. Updated the API to return retrieved contexts alongside query results to support context precision metrics.&lt;/li&gt; 
 &lt;li&gt;[2025.10]🎯[Scalability Enhancement]: Eliminated processing bottlenecks to support &lt;strong&gt;Large-Scale Datasets Efficiently&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;[2025.09]🎯[New Feature] Enhances knowledge graph extraction accuracy for &lt;strong&gt;Open-Sourced LLMs&lt;/strong&gt; such as Qwen3-30B-A3B.&lt;/li&gt; 
 &lt;li&gt;[2025.08]🎯[New Feature] &lt;strong&gt;Reranker&lt;/strong&gt; is now supported, significantly boosting performance for mixed queries (set as default query mode).&lt;/li&gt; 
 &lt;li&gt;[2025.08]🎯[New Feature] Added &lt;strong&gt;Document Deletion&lt;/strong&gt; with automatic KG regeneration to ensure optimal query performance.&lt;/li&gt; 
 &lt;li&gt;[2025.06]🎯[New Release] Our team has released &lt;a href="https://github.com/HKUDS/RAG-Anything"&gt;RAG-Anything&lt;/a&gt; — an &lt;strong&gt;All-in-One Multimodal RAG&lt;/strong&gt; system for seamless processing of text, images, tables, and equations.&lt;/li&gt; 
 &lt;li&gt;[2025.06]🎯[New Feature] LightRAG now supports comprehensive multimodal data handling through &lt;a href="https://github.com/HKUDS/RAG-Anything"&gt;RAG-Anything&lt;/a&gt; integration, enabling seamless document parsing and RAG capabilities across diverse formats including PDFs, images, Office documents, tables, and formulas. Please refer to the new &lt;a href="https://github.com/HKUDS/LightRAG/?tab=readme-ov-file#multimodal-document-processing-rag-anything-integration"&gt;multimodal section&lt;/a&gt; for details.&lt;/li&gt; 
 &lt;li&gt;[2025.03]🎯[New Feature] LightRAG now supports citation functionality, enabling proper source attribution and enhanced document traceability.&lt;/li&gt; 
 &lt;li&gt;[2025.02]🎯[New Feature] You can now use MongoDB as an all-in-one storage solution for unified data management.&lt;/li&gt; 
 &lt;li&gt;[2025.02]🎯[New Release] Our team has released &lt;a href="https://github.com/HKUDS/VideoRAG"&gt;VideoRAG&lt;/a&gt;-a RAG system for understanding extremely long-context videos&lt;/li&gt; 
 &lt;li&gt;[2025.01]🎯[New Release] Our team has released &lt;a href="https://github.com/HKUDS/MiniRAG"&gt;MiniRAG&lt;/a&gt; making RAG simpler with small models.&lt;/li&gt; 
 &lt;li&gt;[2025.01]🎯You can now use PostgreSQL as an all-in-one storage solution for data management.&lt;/li&gt; 
 &lt;li&gt;[2024.11]🎯[New Resource] A comprehensive guide to LightRAG is now available on &lt;a href="https://learnopencv.com/lightrag"&gt;LearnOpenCV&lt;/a&gt;. — explore in-depth tutorials and best practices. Many thanks to the blog author for this excellent contribution!&lt;/li&gt; 
 &lt;li&gt;[2024.11]🎯[New Feature] Introducing the LightRAG WebUI — an interface that allows you to insert, query, and visualize LightRAG knowledge through an intuitive web-based dashboard.&lt;/li&gt; 
 &lt;li&gt;[2024.11]🎯[New Feature] You can now &lt;a href="https://github.com/HKUDS/LightRAG?tab=readme-ov-file#using-neo4j-for-storage"&gt;use Neo4J for Storage&lt;/a&gt;-enabling graph database support.&lt;/li&gt; 
 &lt;li&gt;[2024.10]🎯[New Feature] We've added a link to a &lt;a href="https://youtu.be/oageL-1I0GE"&gt;LightRAG Introduction Video&lt;/a&gt;. — a walkthrough of LightRAG's capabilities. Thanks to the author for this excellent contribution!&lt;/li&gt; 
 &lt;li&gt;[2024.10]🎯[New Channel] We have created a &lt;a href="https://discord.gg/yF2MmDJyGJ"&gt;Discord channel&lt;/a&gt;!💬 Welcome to join our community for sharing, discussions, and collaboration! 🎉🎉&lt;/li&gt; 
 &lt;li&gt;[2024.10]🎯[New Feature] LightRAG now supports &lt;a href="https://github.com/HKUDS/LightRAG?tab=readme-ov-file#quick-start"&gt;Ollama models&lt;/a&gt;!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary style="font-size: 1.4em; font-weight: bold; cursor: pointer; display: list-item;"&gt; Algorithm Flowchart &lt;/summary&gt; 
 &lt;p&gt;&lt;img src="https://learnopencv.com/wp-content/uploads/2024/11/LightRAG-VectorDB-Json-KV-Store-Indexing-Flowchart-scaled.jpg" alt="LightRAG Indexing Flowchart" /&gt; &lt;em&gt;Figure 1: LightRAG Indexing Flowchart - Img Caption : &lt;a href="https://learnopencv.com/lightrag/"&gt;Source&lt;/a&gt;&lt;/em&gt; &lt;img src="https://learnopencv.com/wp-content/uploads/2024/11/LightRAG-Querying-Flowchart-Dual-Level-Retrieval-Generation-Knowledge-Graphs-scaled.jpg" alt="LightRAG Retrieval and Querying Flowchart" /&gt; &lt;em&gt;Figure 2: LightRAG Retrieval and Querying Flowchart - Img Caption : &lt;a href="https://learnopencv.com/lightrag/"&gt;Source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;💡 Using uv for Package Management&lt;/strong&gt;: This project uses &lt;a href="https://docs.astral.sh/uv/"&gt;uv&lt;/a&gt; for fast and reliable Python package management. Install uv first: &lt;code&gt;curl -LsSf https://astral.sh/uv/install.sh | sh&lt;/code&gt; (Unix/macOS) or &lt;code&gt;powershell -c "irm https://astral.sh/uv/install.ps1 | iex"&lt;/code&gt; (Windows)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: You can also use pip if you prefer, but uv is recommended for better performance and more reliable dependency management.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;📦 Offline Deployment&lt;/strong&gt;: For offline or air-gapped environments, see the &lt;a href="https://raw.githubusercontent.com/HKUDS/LightRAG/main/docs/OfflineDeployment.md"&gt;Offline Deployment Guide&lt;/a&gt; for instructions on pre-installing all dependencies and cache files.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Install LightRAG Server&lt;/h3&gt; 
&lt;p&gt;The LightRAG Server is designed to provide Web UI and API support. The Web UI facilitates document indexing, knowledge graph exploration, and a simple RAG query interface. LightRAG Server also provide an Ollama compatible interfaces, aiming to emulate LightRAG as an Ollama chat model. This allows AI chat bot, such as Open WebUI, to access LightRAG easily.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Install from PyPI&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Using uv (recommended)
uv pip install "lightrag-hku[api]"
# Or using pip
# pip install "lightrag-hku[api]"

cp env.example .env  # Update the .env with your LLM and embedding configurations

lightrag-server
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Installation from Source&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/HKUDS/LightRAG.git
cd LightRAG

# Using uv (recommended)
# Note: uv sync automatically creates a virtual environment in .venv/
uv sync --extra api
source .venv/bin/activate  # Activate the virtual environment (Linux/macOS)
# Or on Windows: .venv\Scripts\activate

# Or using pip with virtual environment
# python -m venv .venv
# source .venv/bin/activate  # Windows: .venv\Scripts\activate
# pip install -e ".[api]"

cp env.example .env  # Update the .env with your LLM and embedding configurations

# Build front-end artifacts
cd lightrag_webui
bun install --frozen-lockfile
bun run build
cd ..

lightrag-server
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Launching the LightRAG Server with Docker Compose&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/HKUDS/LightRAG.git
cd LightRAG
cp env.example .env  # Update the .env with your LLM and embedding configurations
# modify LLM and Embedding settings in .env
docker compose up
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Historical versions of LightRAG docker images can be found here: &lt;a href="https://github.com/HKUDS/LightRAG/pkgs/container/lightrag"&gt;LightRAG Docker Images&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Install LightRAG Core&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Install from source (Recommended)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd LightRAG
# Note: uv sync automatically creates a virtual environment in .venv/
uv sync
source .venv/bin/activate  # Activate the virtual environment (Linux/macOS)
# Or on Windows: .venv\Scripts\activate

# Or: pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Install from PyPI&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv pip install lightrag-hku
# Or: pip install lightrag-hku
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;h3&gt;LLM and Technology Stack Requirements for LightRAG&lt;/h3&gt; 
&lt;p&gt;LightRAG's demands on the capabilities of Large Language Models (LLMs) are significantly higher than those of traditional RAG, as it requires the LLM to perform entity-relationship extraction tasks from documents. Configuring appropriate Embedding and Reranker models is also crucial for improving query performance.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;LLM Selection&lt;/strong&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;It is recommended to use an LLM with at least 32 billion parameters.&lt;/li&gt; 
   &lt;li&gt;The context length should be at least 32KB, with 64KB being recommended.&lt;/li&gt; 
   &lt;li&gt;It is not recommended to choose reasoning models during the document indexing stage.&lt;/li&gt; 
   &lt;li&gt;During the query stage, it is recommended to choose models with stronger capabilities than those used in the indexing stage to achieve better query results.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Embedding Model&lt;/strong&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;A high-performance Embedding model is essential for RAG.&lt;/li&gt; 
   &lt;li&gt;We recommend using mainstream multilingual Embedding models, such as: &lt;code&gt;BAAI/bge-m3&lt;/code&gt; and &lt;code&gt;text-embedding-3-large&lt;/code&gt;.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Important Note&lt;/strong&gt;: The Embedding model must be determined before document indexing, and the same model must be used during the document query phase. For certain storage solutions (e.g., PostgreSQL), the vector dimension must be defined upon initial table creation. Therefore, when changing embedding models, it is necessary to delete the existing vector-related tables and allow LightRAG to recreate them with the new dimensions.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Reranker Model Configuration&lt;/strong&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;Configuring a Reranker model can significantly enhance LightRAG's retrieval performance.&lt;/li&gt; 
   &lt;li&gt;When a Reranker model is enabled, it is recommended to set the "mix mode" as the default query mode.&lt;/li&gt; 
   &lt;li&gt;We recommend using mainstream Reranker models, such as: &lt;code&gt;BAAI/bge-reranker-v2-m3&lt;/code&gt; or models provided by services like Jina.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Quick Start for LightRAG Server&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;For more information about LightRAG Server, please refer to &lt;a href="https://raw.githubusercontent.com/HKUDS/LightRAG/main/lightrag/api/README.md"&gt;LightRAG Server&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Quick Start for LightRAG core&lt;/h3&gt; 
&lt;p&gt;To get started with LightRAG core, refer to the sample codes available in the &lt;code&gt;examples&lt;/code&gt; folder. Additionally, a &lt;a href="https://www.youtube.com/watch?v=g21royNJ4fw"&gt;video demo&lt;/a&gt; demonstration is provided to guide you through the local setup process. If you already possess an OpenAI API key, you can run the demo right away:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;### you should run the demo code with project folder
cd LightRAG
### provide your API-KEY for OpenAI
export OPENAI_API_KEY="sk-...your_opeai_key..."
### download the demo document of "A Christmas Carol" by Charles Dickens
curl https://raw.githubusercontent.com/gusye1234/nano-graphrag/main/tests/mock_data.txt &amp;gt; ./book.txt
### run the demo code
python examples/lightrag_openai_demo.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For a streaming response implementation example, please see &lt;code&gt;examples/lightrag_openai_compatible_demo.py&lt;/code&gt;. Prior to execution, ensure you modify the sample code's LLM and embedding configurations accordingly.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note 1&lt;/strong&gt;: When running the demo program, please be aware that different test scripts may use different embedding models. If you switch to a different embedding model, you must clear the data directory (&lt;code&gt;./dickens&lt;/code&gt;); otherwise, the program may encounter errors. If you wish to retain the LLM cache, you can preserve the &lt;code&gt;kv_store_llm_response_cache.json&lt;/code&gt; file while clearing the data directory.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note 2&lt;/strong&gt;: Only &lt;code&gt;lightrag_openai_demo.py&lt;/code&gt; and &lt;code&gt;lightrag_openai_compatible_demo.py&lt;/code&gt; are officially supported sample codes. Other sample files are community contributions that haven't undergone full testing and optimization.&lt;/p&gt; 
&lt;h2&gt;Programming with LightRAG Core&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;⚠️ &lt;strong&gt;If you would like to integrate LightRAG into your project, we recommend utilizing the REST API provided by the LightRAG Server&lt;/strong&gt;. LightRAG Core is typically intended for embedded applications or for researchers who wish to conduct studies and evaluations.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;⚠️ Important: Initialization Requirements&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;LightRAG requires explicit initialization before use.&lt;/strong&gt; You must call &lt;code&gt;await rag.initialize_storages()&lt;/code&gt; after creating a LightRAG instance, otherwise you will encounter errors.&lt;/p&gt; 
&lt;h3&gt;A Simple Program&lt;/h3&gt; 
&lt;p&gt;Use the below Python snippet to initialize LightRAG, insert text to it, and perform queries:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import os
import asyncio
from lightrag import LightRAG, QueryParam
from lightrag.llm.openai import gpt_4o_mini_complete, gpt_4o_complete, openai_embed
from lightrag.utils import setup_logger

setup_logger("lightrag", level="INFO")

WORKING_DIR = "./rag_storage"
if not os.path.exists(WORKING_DIR):
    os.mkdir(WORKING_DIR)

async def initialize_rag():
    rag = LightRAG(
        working_dir=WORKING_DIR,
        embedding_func=openai_embed,
        llm_model_func=gpt_4o_mini_complete,
    )
    # IMPORTANT: Both initialization calls are required!
    await rag.initialize_storages()  # Initialize storage backends    return rag

async def main():
    try:
        # Initialize RAG instance
        rag = await initialize_rag()
        await rag.ainsert("Your text")

        # Perform hybrid search
        mode = "hybrid"
        print(
          await rag.aquery(
              "What are the top themes in this story?",
              param=QueryParam(mode=mode)
          )
        )

    except Exception as e:
        print(f"An error occurred: {e}")
    finally:
        if rag:
            await rag.finalize_storages()

if __name__ == "__main__":
    asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Important notes for the above snippet:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Export your OPENAI_API_KEY environment variable before running the script.&lt;/li&gt; 
 &lt;li&gt;This program uses the default storage settings for LightRAG, so all data will be persisted to WORKING_DIR/rag_storage.&lt;/li&gt; 
 &lt;li&gt;This program demonstrates only the simplest way to initialize a LightRAG object: Injecting the embedding and LLM functions, and initializing storage and pipeline status after creating the LightRAG object.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;LightRAG init parameters&lt;/h3&gt; 
&lt;p&gt;A full list of LightRAG init parameters:&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt; Parameters &lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;&lt;strong&gt;Parameter&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;Type&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;Explanation&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;Default&lt;/strong&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;working_dir&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;str&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Directory where the cache will be stored&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;lightrag_cache+timestamp&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;workspace&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;str&lt;/td&gt; 
    &lt;td&gt;Workspace name for data isolation between different LightRAG Instances&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;kv_storage&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;str&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Storage type for documents and text chunks. Supported types: &lt;code&gt;JsonKVStorage&lt;/code&gt;,&lt;code&gt;PGKVStorage&lt;/code&gt;,&lt;code&gt;RedisKVStorage&lt;/code&gt;,&lt;code&gt;MongoKVStorage&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;JsonKVStorage&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;vector_storage&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;str&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Storage type for embedding vectors. Supported types: &lt;code&gt;NanoVectorDBStorage&lt;/code&gt;,&lt;code&gt;PGVectorStorage&lt;/code&gt;,&lt;code&gt;MilvusVectorDBStorage&lt;/code&gt;,&lt;code&gt;ChromaVectorDBStorage&lt;/code&gt;,&lt;code&gt;FaissVectorDBStorage&lt;/code&gt;,&lt;code&gt;MongoVectorDBStorage&lt;/code&gt;,&lt;code&gt;QdrantVectorDBStorage&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;NanoVectorDBStorage&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;graph_storage&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;str&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Storage type for graph edges and nodes. Supported types: &lt;code&gt;NetworkXStorage&lt;/code&gt;,&lt;code&gt;Neo4JStorage&lt;/code&gt;,&lt;code&gt;PGGraphStorage&lt;/code&gt;,&lt;code&gt;AGEStorage&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;NetworkXStorage&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;doc_status_storage&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;str&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Storage type for documents process status. Supported types: &lt;code&gt;JsonDocStatusStorage&lt;/code&gt;,&lt;code&gt;PGDocStatusStorage&lt;/code&gt;,&lt;code&gt;MongoDocStatusStorage&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;JsonDocStatusStorage&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;chunk_token_size&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;int&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Maximum token size per chunk when splitting documents&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;1200&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;chunk_overlap_token_size&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;int&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Overlap token size between two chunks when splitting documents&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;100&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;tokenizer&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;Tokenizer&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;The function used to convert text into tokens (numbers) and back using .encode() and .decode() functions following &lt;code&gt;TokenizerInterface&lt;/code&gt; protocol. If you don't specify one, it will use the default Tiktoken tokenizer.&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;TiktokenTokenizer&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;tiktoken_model_name&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;str&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;If you're using the default Tiktoken tokenizer, this is the name of the specific Tiktoken model to use. This setting is ignored if you provide your own tokenizer.&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;gpt-4o-mini&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;entity_extract_max_gleaning&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;int&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Number of loops in the entity extraction process, appending history messages&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;node_embedding_algorithm&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;str&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Algorithm for node embedding (currently not used)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;node2vec&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;node2vec_params&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;dict&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Parameters for node embedding&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;{"dimensions": 1536,"num_walks": 10,"walk_length": 40,"window_size": 2,"iterations": 3,"random_seed": 3,}&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;embedding_func&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;EmbeddingFunc&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Function to generate embedding vectors from text&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;openai_embed&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;embedding_batch_num&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;int&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Maximum batch size for embedding processes (multiple texts sent per batch)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;32&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;embedding_func_max_async&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;int&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Maximum number of concurrent asynchronous embedding processes&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;16&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;llm_model_func&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;callable&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Function for LLM generation&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;gpt_4o_mini_complete&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;llm_model_name&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;str&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;LLM model name for generation&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;meta-llama/Llama-3.2-1B-Instruct&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;summary_context_size&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;int&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Maximum tokens send to LLM to generate summaries for entity relation merging&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;10000&lt;/code&gt;（configured by env var SUMMARY_CONTEXT_SIZE)&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;summary_max_tokens&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;int&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Maximum token size for entity/relation description&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;500&lt;/code&gt;（configured by env var SUMMARY_MAX_TOKENS)&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;llm_model_max_async&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;int&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Maximum number of concurrent asynchronous LLM processes&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;4&lt;/code&gt;（default value changed by env var MAX_ASYNC)&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;llm_model_kwargs&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;dict&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Additional parameters for LLM generation&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;vector_db_storage_cls_kwargs&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;dict&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Additional parameters for vector database, like setting the threshold for nodes and relations retrieval&lt;/td&gt; 
    &lt;td&gt;cosine_better_than_threshold: 0.2（default value changed by env var COSINE_THRESHOLD)&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;enable_llm_cache&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;bool&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;If &lt;code&gt;TRUE&lt;/code&gt;, stores LLM results in cache; repeated prompts return cached responses&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;TRUE&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;enable_llm_cache_for_entity_extract&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;bool&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;If &lt;code&gt;TRUE&lt;/code&gt;, stores LLM results in cache for entity extraction; Good for beginners to debug your application&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;TRUE&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;addon_params&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;dict&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Additional parameters, e.g., &lt;code&gt;{"language": "Simplified Chinese", "entity_types": ["organization", "person", "location", "event"]}&lt;/code&gt;: sets example limit, entity/relation extraction output language&lt;/td&gt; 
    &lt;td&gt;language: English`&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;embedding_cache_config&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;dict&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;Configuration for question-answer caching. Contains three parameters: &lt;code&gt;enabled&lt;/code&gt;: Boolean value to enable/disable cache lookup functionality. When enabled, the system will check cached responses before generating new answers. &lt;code&gt;similarity_threshold&lt;/code&gt;: Float value (0-1), similarity threshold. When a new question's similarity with a cached question exceeds this threshold, the cached answer will be returned directly without calling the LLM. &lt;code&gt;use_llm_check&lt;/code&gt;: Boolean value to enable/disable LLM similarity verification. When enabled, LLM will be used as a secondary check to verify the similarity between questions before returning cached answers.&lt;/td&gt; 
    &lt;td&gt;Default: &lt;code&gt;{"enabled": False, "similarity_threshold": 0.95, "use_llm_check": False}&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h3&gt;Query Param&lt;/h3&gt; 
&lt;p&gt;Use QueryParam to control the behavior your query:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;class QueryParam:
    """Configuration parameters for query execution in LightRAG."""

    mode: Literal["local", "global", "hybrid", "naive", "mix", "bypass"] = "global"
    """Specifies the retrieval mode:
    - "local": Focuses on context-dependent information.
    - "global": Utilizes global knowledge.
    - "hybrid": Combines local and global retrieval methods.
    - "naive": Performs a basic search without advanced techniques.
    - "mix": Integrates knowledge graph and vector retrieval.
    """

    only_need_context: bool = False
    """If True, only returns the retrieved context without generating a response."""

    only_need_prompt: bool = False
    """If True, only returns the generated prompt without producing a response."""

    response_type: str = "Multiple Paragraphs"
    """Defines the response format. Examples: 'Multiple Paragraphs', 'Single Paragraph', 'Bullet Points'."""

    stream: bool = False
    """If True, enables streaming output for real-time responses."""

    top_k: int = int(os.getenv("TOP_K", "60"))
    """Number of top items to retrieve. Represents entities in 'local' mode and relationships in 'global' mode."""

    chunk_top_k: int = int(os.getenv("CHUNK_TOP_K", "20"))
    """Number of text chunks to retrieve initially from vector search and keep after reranking.
    If None, defaults to top_k value.
    """

    max_entity_tokens: int = int(os.getenv("MAX_ENTITY_TOKENS", "6000"))
    """Maximum number of tokens allocated for entity context in unified token control system."""

    max_relation_tokens: int = int(os.getenv("MAX_RELATION_TOKENS", "8000"))
    """Maximum number of tokens allocated for relationship context in unified token control system."""

    max_total_tokens: int = int(os.getenv("MAX_TOTAL_TOKENS", "30000"))
    """Maximum total tokens budget for the entire query context (entities + relations + chunks + system prompt)."""

    # History messages are only sent to LLM for context, not used for retrieval
    conversation_history: list[dict[str, str]] = field(default_factory=list)
    """Stores past conversation history to maintain context.
    Format: [{"role": "user/assistant", "content": "message"}].
    """

    ids: list[str] | None = None
    """List of ids to filter the results."""

    model_func: Callable[..., object] | None = None
    """Optional override for the LLM model function to use for this specific query.
    If provided, this will be used instead of the global model function.
    This allows using different models for different query modes.
    """

    user_prompt: str | None = None
    """User-provided prompt for the query.
    Addition instructions for LLM. If provided, this will be inject into the prompt template.
    It's purpose is the let user customize the way LLM generate the response.
    """

    enable_rerank: bool = True
    """Enable reranking for retrieved text chunks. If True but no rerank model is configured, a warning will be issued.
    Default is True to enable reranking when rerank model is available.
    """
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;default value of Top_k can be change by environment variables TOP_K.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;LLM and Embedding Injection&lt;/h3&gt; 
&lt;p&gt;LightRAG requires the utilization of LLM and Embedding models to accomplish document indexing and querying tasks. During the initialization phase, it is necessary to inject the invocation methods of the relevant models into LightRAG：&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt;Using Open AI-like APIs&lt;/b&gt; &lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;LightRAG also supports Open AI-like chat/embeddings APIs:&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;import os
import numpy as np
from lightrag.utils import wrap_embedding_func_with_attrs
from lightrag.llm.openai import openai_complete_if_cache, openai_embed

async def llm_model_func(
    prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs
) -&amp;gt; str:
    return await openai_complete_if_cache(
        "solar-mini",
        prompt,
        system_prompt=system_prompt,
        history_messages=history_messages,
        api_key=os.getenv("UPSTAGE_API_KEY"),
        base_url="https://api.upstage.ai/v1/solar",
        **kwargs
    )

@wrap_embedding_func_with_attrs(embedding_dim=4096, max_token_size=8192, model_name="solar-embedding-1-large-query")
async def embedding_func(texts: list[str]) -&amp;gt; np.ndarray:
    return await openai_embed.func(
        texts,
        model="solar-embedding-1-large-query",
        api_key=os.getenv("UPSTAGE_API_KEY"),
        base_url="https://api.upstage.ai/v1/solar"
    )

async def initialize_rag():
    rag = LightRAG(
        working_dir=WORKING_DIR,
        llm_model_func=llm_model_func,
        embedding_func=embedding_func  # Pass the decorated function directly
    )

    await rag.initialize_storages()
    return rag
&lt;/code&gt;&lt;/pre&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;&lt;strong&gt;Important Note on Embedding Function Wrapping:&lt;/strong&gt;&lt;/p&gt; 
  &lt;p&gt;&lt;code&gt;EmbeddingFunc&lt;/code&gt; cannot be nested. Functions that have been decorated with &lt;code&gt;@wrap_embedding_func_with_attrs&lt;/code&gt; (such as &lt;code&gt;openai_embed&lt;/code&gt;, &lt;code&gt;ollama_embed&lt;/code&gt;, etc.) cannot be wrapped again using &lt;code&gt;EmbeddingFunc()&lt;/code&gt;. This is why we call &lt;code&gt;xxx_embed.func&lt;/code&gt; (the underlying unwrapped function) instead of &lt;code&gt;xxx_embed&lt;/code&gt; directly when creating custom embedding functions.&lt;/p&gt; 
 &lt;/blockquote&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt;Using Hugging Face Models&lt;/b&gt; &lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;If you want to use Hugging Face models, you only need to set LightRAG as follows:&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;See &lt;code&gt;lightrag_hf_demo.py&lt;/code&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Initialize LightRAG with Hugging Face model
rag = LightRAG(
    working_dir=WORKING_DIR,
    llm_model_func=hf_model_complete,  # Use Hugging Face model for text generation
    llm_model_name='meta-llama/Llama-3.1-8B-Instruct',  # Model name from Hugging Face
    # Use Hugging Face embedding function
    embedding_func=EmbeddingFunc(
        embedding_dim=384,
        func=lambda texts: hf_embed(
            texts,
            tokenizer=AutoTokenizer.from_pretrained("sentence-transformers/all-MiniLM-L6-v2"),
            embed_model=AutoModel.from_pretrained("sentence-transformers/all-MiniLM-L6-v2")
        )
    ),
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt;Using Ollama Models&lt;/b&gt; &lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;Overview&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;If you want to use Ollama models, you need to pull model you plan to use and embedding model, for example &lt;code&gt;nomic-embed-text&lt;/code&gt;.&lt;/p&gt; 
 &lt;p&gt;Then you only need to set LightRAG as follows:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;import numpy as np
from lightrag.utils import wrap_embedding_func_with_attrs
from lightrag.llm.ollama import ollama_model_complete, ollama_embed

@wrap_embedding_func_with_attrs(embedding_dim=768, max_token_size=8192, model_name="nomic-embed-text")
async def embedding_func(texts: list[str]) -&amp;gt; np.ndarray:
    return await ollama_embed.func(texts, embed_model="nomic-embed-text")

# Initialize LightRAG with Ollama model
rag = LightRAG(
    working_dir=WORKING_DIR,
    llm_model_func=ollama_model_complete,  # Use Ollama model for text generation
    llm_model_name='your_model_name', # Your model name
    embedding_func=embedding_func,  # Pass the decorated function directly
)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Increasing context size&lt;/strong&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;In order for LightRAG to work context should be at least 32k tokens. By default Ollama models have context size of 8k. You can achieve this using one of two ways:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Increasing the &lt;code&gt;num_ctx&lt;/code&gt; parameter in Modelfile&lt;/strong&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Pull the model:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;ollama pull qwen2
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="2"&gt; 
  &lt;li&gt;Display the model file:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;ollama show --modelfile qwen2 &amp;gt; Modelfile
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="3"&gt; 
  &lt;li&gt;Edit the Modelfile by adding the following line:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;PARAMETER num_ctx 32768
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="4"&gt; 
  &lt;li&gt;Create the modified model:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;ollama create -f Modelfile qwen2m
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Setup &lt;code&gt;num_ctx&lt;/code&gt; via Ollama API&lt;/strong&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;Tiy can use &lt;code&gt;llm_model_kwargs&lt;/code&gt; param to configure ollama:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;import numpy as np
from lightrag.utils import wrap_embedding_func_with_attrs
from lightrag.llm.ollama import ollama_model_complete, ollama_embed

@wrap_embedding_func_with_attrs(embedding_dim=768, max_token_size=8192, model_name="nomic-embed-text")
async def embedding_func(texts: list[str]) -&amp;gt; np.ndarray:
    return await ollama_embed.func(texts, embed_model="nomic-embed-text")

rag = LightRAG(
    working_dir=WORKING_DIR,
    llm_model_func=ollama_model_complete,  # Use Ollama model for text generation
    llm_model_name='your_model_name', # Your model name
    llm_model_kwargs={"options": {"num_ctx": 32768}},
    embedding_func=embedding_func,  # Pass the decorated function directly
)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;&lt;strong&gt;Important Note on Embedding Function Wrapping:&lt;/strong&gt;&lt;/p&gt; 
  &lt;p&gt;&lt;code&gt;EmbeddingFunc&lt;/code&gt; cannot be nested. Functions that have been decorated with &lt;code&gt;@wrap_embedding_func_with_attrs&lt;/code&gt; (such as &lt;code&gt;openai_embed&lt;/code&gt;, &lt;code&gt;ollama_embed&lt;/code&gt;, etc.) cannot be wrapped again using &lt;code&gt;EmbeddingFunc()&lt;/code&gt;. This is why we call &lt;code&gt;xxx_embed.func&lt;/code&gt; (the underlying unwrapped function) instead of &lt;code&gt;xxx_embed&lt;/code&gt; directly when creating custom embedding functions.&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Low RAM GPUs&lt;/strong&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;In order to run this experiment on low RAM GPU you should select small model and tune context window (increasing context increase memory consumption). For example, running this ollama example on repurposed mining GPU with 6Gb of RAM required to set context size to 26k while using &lt;code&gt;gemma2:2b&lt;/code&gt;. It was able to find 197 entities and 19 relations on &lt;code&gt;book.txt&lt;/code&gt;.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt;LlamaIndex&lt;/b&gt; &lt;/summary&gt; 
 &lt;p&gt;LightRAG supports integration with LlamaIndex (&lt;code&gt;llm/llama_index_impl.py&lt;/code&gt;):&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Integrates with OpenAI and other providers through LlamaIndex&lt;/li&gt; 
  &lt;li&gt;See &lt;a href="https://developers.llamaindex.ai/python/framework/"&gt;LlamaIndex Documentation&lt;/a&gt; for detailed setup or the &lt;a href="https://raw.githubusercontent.com/HKUDS/LightRAG/main/examples/unofficial-sample/"&gt;examples&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;Example Usage&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Using LlamaIndex with direct OpenAI access
import asyncio
from lightrag import LightRAG
from lightrag.llm.llama_index_impl import llama_index_complete_if_cache, llama_index_embed
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.llms.openai import OpenAI
from lightrag.utils import setup_logger

# Setup log handler for LightRAG
setup_logger("lightrag", level="INFO")

async def initialize_rag():
    rag = LightRAG(
        working_dir="your/path",
        llm_model_func=llama_index_complete_if_cache,  # LlamaIndex-compatible completion function
        embedding_func=EmbeddingFunc(    # LlamaIndex-compatible embedding function
            embedding_dim=1536,
            func=lambda texts: llama_index_embed(texts, embed_model=embed_model)
        ),
    )

    await rag.initialize_storages()
    return rag

def main():
    # Initialize RAG instance
    rag = asyncio.run(initialize_rag())

    with open("./book.txt", "r", encoding="utf-8") as f:
        rag.insert(f.read())

    # Perform naive search
    print(
        rag.query("What are the top themes in this story?", param=QueryParam(mode="naive"))
    )

    # Perform local search
    print(
        rag.query("What are the top themes in this story?", param=QueryParam(mode="local"))
    )

    # Perform global search
    print(
        rag.query("What are the top themes in this story?", param=QueryParam(mode="global"))
    )

    # Perform hybrid search
    print(
        rag.query("What are the top themes in this story?", param=QueryParam(mode="hybrid"))
    )

if __name__ == "__main__":
    main()
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;For detailed documentation and examples, see:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://developers.llamaindex.ai/python/framework/"&gt;LlamaIndex Documentation&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/LightRAG/main/examples/unofficial-sample/lightrag_llamaindex_direct_demo.py"&gt;Direct OpenAI Example&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/LightRAG/main/examples/unofficial-sample/lightrag_llamaindex_litellm_demo.py"&gt;LiteLLM Proxy Example&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/LightRAG/main/examples/unofficial-sample/lightrag_llamaindex_litellm_opik_demo.py"&gt;LiteLLM Proxy with Opik Example&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;Rerank Function Injection&lt;/h3&gt; 
&lt;p&gt;To enhance retrieval quality, documents can be re-ranked based on a more effective relevance scoring model. The &lt;code&gt;rerank.py&lt;/code&gt; file provides three Reranker provider driver functions:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Cohere / vLLM&lt;/strong&gt;: &lt;code&gt;cohere_rerank&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Jina AI&lt;/strong&gt;: &lt;code&gt;jina_rerank&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Aliyun&lt;/strong&gt;: &lt;code&gt;ali_rerank&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can inject one of these functions into the &lt;code&gt;rerank_model_func&lt;/code&gt; attribute of the LightRAG object. This will enable LightRAG's query function to re-order retrieved text blocks using the injected function. For detailed usage, please refer to the &lt;code&gt;examples/rerank_example.py&lt;/code&gt; file.&lt;/p&gt; 
&lt;h3&gt;User Prompt vs. Query&lt;/h3&gt; 
&lt;p&gt;When using LightRAG for content queries, avoid combining the search process with unrelated output processing, as this significantly impacts query effectiveness. The &lt;code&gt;user_prompt&lt;/code&gt; parameter in Query Param is specifically designed to address this issue — it does not participate in the RAG retrieval phase, but rather guides the LLM on how to process the retrieved results after the query is completed. Here's how to use it:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Create query parameters
query_param = QueryParam(
    mode = "hybrid",  # Other modes：local, global, hybrid, mix, naive
    user_prompt = "For diagrams, use mermaid format with English/Pinyin node names and Chinese display labels",
)

# Query and process
response_default = rag.query(
    "Please draw a character relationship diagram for Scrooge",
    param=query_param
)
print(response_default)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Insert&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt; Basic Insert &lt;/b&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Basic Insert
rag.insert("Text")
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt; Batch Insert &lt;/b&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Basic Batch Insert: Insert multiple texts at once
rag.insert(["TEXT1", "TEXT2",...])

# Batch Insert with custom batch size configuration
rag = LightRAG(
    ...
    working_dir=WORKING_DIR,
    max_parallel_insert = 4
)

rag.insert(["TEXT1", "TEXT2", "TEXT3", ...])  # Documents will be processed in batches of 4
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;The &lt;code&gt;max_parallel_insert&lt;/code&gt; parameter determines the number of documents processed concurrently in the document indexing pipeline. If unspecified, the default value is &lt;strong&gt;2&lt;/strong&gt;. We recommend keeping this setting &lt;strong&gt;below 10&lt;/strong&gt;, as the performance bottleneck typically lies with the LLM (Large Language Model) processing.The &lt;code&gt;max_parallel_insert&lt;/code&gt; parameter determines the number of documents processed concurrently in the document indexing pipeline. If unspecified, the default value is &lt;strong&gt;2&lt;/strong&gt;. We recommend keeping this setting &lt;strong&gt;below 10&lt;/strong&gt;, as the performance bottleneck typically lies with the LLM (Large Language Model) processing.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt; Insert with ID &lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;If you want to provide your own IDs for your documents, number of documents and number of IDs must be the same.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Insert single text, and provide ID for it
rag.insert("TEXT1", ids=["ID_FOR_TEXT1"])

# Insert multiple texts, and provide IDs for them
rag.insert(["TEXT1", "TEXT2",...], ids=["ID_FOR_TEXT1", "ID_FOR_TEXT2"])
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Insert using Pipeline&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;The &lt;code&gt;apipeline_enqueue_documents&lt;/code&gt; and &lt;code&gt;apipeline_process_enqueue_documents&lt;/code&gt; functions allow you to perform incremental insertion of documents into the graph.&lt;/p&gt; 
 &lt;p&gt;This is useful for scenarios where you want to process documents in the background while still allowing the main thread to continue executing.&lt;/p&gt; 
 &lt;p&gt;And using a routine to process new documents.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;rag = LightRAG(..)

await rag.apipeline_enqueue_documents(input)
# Your routine in loop
await rag.apipeline_process_enqueue_documents(input)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Insert Multi-file Type Support&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;The &lt;code&gt;textract&lt;/code&gt; supports reading file types such as TXT, DOCX, PPTX, CSV, and PDF.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;import textract

file_path = 'TEXT.pdf'
text_content = textract.process(file_path)

rag.insert(text_content.decode('utf-8'))
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Citation Functionality&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;By providing file paths, the system ensures that sources can be traced back to their original documents.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Define documents and their file paths
documents = ["Document content 1", "Document content 2"]
file_paths = ["path/to/doc1.txt", "path/to/doc2.txt"]

# Insert documents with file paths
rag.insert(documents, file_paths=file_paths)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;Storage&lt;/h3&gt; 
&lt;p&gt;LightRAG uses 4 types of storage for different purposes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;KV_STORAGE: llm response cache, text chunks, document information&lt;/li&gt; 
 &lt;li&gt;VECTOR_STORAGE: entities vectors, relation vectors, chunks vectors&lt;/li&gt; 
 &lt;li&gt;GRAPH_STORAGE: entity relation graph&lt;/li&gt; 
 &lt;li&gt;DOC_STATUS_STORAGE: document indexing status&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Each storage type has several implementations:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;KV_STORAGE supported implementations:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;JsonKVStorage    JsonFile (default)
PGKVStorage      Postgres
RedisKVStorage   Redis
MongoKVStorage   MongoDB
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;GRAPH_STORAGE supported implementations:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;NetworkXStorage      NetworkX (default)
Neo4JStorage         Neo4J
PGGraphStorage       PostgreSQL with AGE plugin
MemgraphStorage.     Memgraph
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Testing has shown that Neo4J delivers superior performance in production environments compared to PostgreSQL with AGE plugin.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;VECTOR_STORAGE supported implementations:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;NanoVectorDBStorage         NanoVector (default)
PGVectorStorage             Postgres
MilvusVectorDBStorage       Milvus
FaissVectorDBStorage        Faiss
QdrantVectorDBStorage       Qdrant
MongoVectorDBStorage        MongoDB
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;DOC_STATUS_STORAGE: supported implementations:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;JsonDocStatusStorage        JsonFile (default)
PGDocStatusStorage          Postgres
MongoDocStatusStorage       MongoDB
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Example connection configurations for each storage type can be found in the &lt;code&gt;env.example&lt;/code&gt; file. The database instance in the connection string needs to be created by you on the database server beforehand. LightRAG is only responsible for creating tables within the database instance, not for creating the database instance itself. If using Redis as storage, remember to configure automatic data persistence rules for Redis, otherwise data will be lost after the Redis service restarts. If using PostgreSQL, it is recommended to use version 16.6 or above.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt;Using Neo4J Storage&lt;/b&gt; &lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;For production level scenarios you will most likely want to leverage an enterprise solution&lt;/li&gt; 
  &lt;li&gt;for KG storage. Running Neo4J in Docker is recommended for seamless local testing.&lt;/li&gt; 
  &lt;li&gt;See: &lt;a href="https://hub.docker.com/_/neo4j"&gt;https://hub.docker.com/_/neo4j&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;export NEO4J_URI="neo4j://localhost:7687"
export NEO4J_USERNAME="neo4j"
export NEO4J_PASSWORD="password"

# Setup logger for LightRAG
setup_logger("lightrag", level="INFO")

# When you launch the project be sure to override the default KG: NetworkX
# by specifying kg="Neo4JStorage".

# Note: Default settings use NetworkX
# Initialize LightRAG with Neo4J implementation.
async def initialize_rag():
    rag = LightRAG(
        working_dir=WORKING_DIR,
        llm_model_func=gpt_4o_mini_complete,  # Use gpt_4o_mini_complete LLM model
        graph_storage="Neo4JStorage", #&amp;lt;-----------override KG default
    )

    # Initialize database connections
    await rag.initialize_storages()
    # Initialize pipeline status for document processing
    return rag
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;see test_neo4j.py for a working example.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt;Using PostgreSQL Storage&lt;/b&gt; &lt;/summary&gt; 
 &lt;p&gt;For production level scenarios you will most likely want to leverage an enterprise solution. PostgreSQL can provide a one-stop solution for you as KV store, VectorDB (pgvector) and GraphDB (apache AGE). PostgreSQL version 16.6 or higher is supported.&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;PostgreSQL is lightweight,the whole binary distribution including all necessary plugins can be zipped to 40MB: Ref to &lt;a href="https://github.com/ShanGor/apache-age-windows/releases/tag/PG17%2Fv1.5.0-rc0"&gt;Windows Release&lt;/a&gt; as it is easy to install for Linux/Mac.&lt;/li&gt; 
  &lt;li&gt;If you prefer docker, please start with this image if you are a beginner to avoid hiccups (Default user password:rag/rag): &lt;a href="https://hub.docker.com/r/gzdaniel/postgres-for-rag"&gt;https://hub.docker.com/r/gzdaniel/postgres-for-rag&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;How to start? Ref to: &lt;a href="https://github.com/HKUDS/LightRAG/raw/main/examples/lightrag_zhipu_postgres_demo.py"&gt;examples/lightrag_zhipu_postgres_demo.py&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;For high-performance graph database requirements, Neo4j is recommended as Apache AGE's performance is not as competitive.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt;Using Faiss Storage&lt;/b&gt; &lt;/summary&gt; Before using Faiss vector database, you must manually install `faiss-cpu` or `faiss-gpu`. 
 &lt;ul&gt; 
  &lt;li&gt;Install the required dependencies:&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;pre&gt;&lt;code&gt;pip install faiss-cpu
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;You can also install &lt;code&gt;faiss-gpu&lt;/code&gt; if you have GPU support.&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Here we are using &lt;code&gt;sentence-transformers&lt;/code&gt; but you can also use &lt;code&gt;OpenAIEmbedding&lt;/code&gt; model with &lt;code&gt;3072&lt;/code&gt; dimensions.&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;async def embedding_func(texts: list[str]) -&amp;gt; np.ndarray:
    model = SentenceTransformer('all-MiniLM-L6-v2')
    embeddings = model.encode(texts, convert_to_numpy=True)
    return embeddings

# Initialize LightRAG with the LLM model function and embedding function
rag = LightRAG(
    working_dir=WORKING_DIR,
    llm_model_func=llm_model_func,
    embedding_func=EmbeddingFunc(
        embedding_dim=384,
        func=embedding_func,
    ),
    vector_storage="FaissVectorDBStorage",
    vector_db_storage_cls_kwargs={
        "cosine_better_than_threshold": 0.3  # Your desired threshold
    }
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt;Using Memgraph for Storage&lt;/b&gt; &lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Memgraph is a high-performance, in-memory graph database compatible with the Neo4j Bolt protocol.&lt;/li&gt; 
  &lt;li&gt;You can run Memgraph locally using Docker for easy testing:&lt;/li&gt; 
  &lt;li&gt;See: &lt;a href="https://memgraph.com/download"&gt;https://memgraph.com/download&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;export MEMGRAPH_URI="bolt://localhost:7687"

# Setup logger for LightRAG
setup_logger("lightrag", level="INFO")

# When you launch the project, override the default KG: NetworkX
# by specifying kg="MemgraphStorage".

# Note: Default settings use NetworkX
# Initialize LightRAG with Memgraph implementation.
async def initialize_rag():
    rag = LightRAG(
        working_dir=WORKING_DIR,
        llm_model_func=gpt_4o_mini_complete,  # Use gpt_4o_mini_complete LLM model
        graph_storage="MemgraphStorage", #&amp;lt;-----------override KG default
    )

    # Initialize database connections
    await rag.initialize_storages()
    # Initialize pipeline status for document processing
    return rag
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt;Using MongoDB Storage&lt;/b&gt; &lt;/summary&gt; 
 &lt;p&gt;MongoDB provides a one-stop storage solution for LightRAG. MongoDB offers native KV storage and vector storage. LightRAG uses MongoDB collections to implement a simple graph storage. MongoDB's official vector search functionality (&lt;code&gt;$vectorSearch&lt;/code&gt;) currently requires their official cloud service MongoDB Atlas. This functionality cannot be used on self-hosted MongoDB Community/Enterprise versions.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt;Using Redis Storage&lt;/b&gt; &lt;/summary&gt; 
 &lt;p&gt;LightRAG supports using Redis as KV storage. When using Redis storage, attention should be paid to persistence configuration and memory usage configuration. The following is the recommended Redis configuration:&lt;/p&gt; 
 &lt;pre&gt;&lt;code&gt;save 900 1
save 300 10
save 60 1000
stop-writes-on-bgsave-error yes
maxmemory 4gb
maxmemory-policy noeviction
maxclients 500
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;Data Isolation Between LightRAG Instances&lt;/h3&gt; 
&lt;p&gt;The &lt;code&gt;workspace&lt;/code&gt; parameter ensures data isolation between different LightRAG instances. Once initialized, the &lt;code&gt;workspace&lt;/code&gt; is immutable and cannot be changed.Here is how workspaces are implemented for different types of storage:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;For local file-based databases, data isolation is achieved through workspace subdirectories:&lt;/strong&gt; &lt;code&gt;JsonKVStorage&lt;/code&gt;, &lt;code&gt;JsonDocStatusStorage&lt;/code&gt;, &lt;code&gt;NetworkXStorage&lt;/code&gt;, &lt;code&gt;NanoVectorDBStorage&lt;/code&gt;, &lt;code&gt;FaissVectorDBStorage&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;For databases that store data in collections, it's done by adding a workspace prefix to the collection name:&lt;/strong&gt; &lt;code&gt;RedisKVStorage&lt;/code&gt;, &lt;code&gt;RedisDocStatusStorage&lt;/code&gt;, &lt;code&gt;MilvusVectorDBStorage&lt;/code&gt;, &lt;code&gt;MongoKVStorage&lt;/code&gt;, &lt;code&gt;MongoDocStatusStorage&lt;/code&gt;, &lt;code&gt;MongoVectorDBStorage&lt;/code&gt;, &lt;code&gt;MongoGraphStorage&lt;/code&gt;, &lt;code&gt;PGGraphStorage&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;For Qdrant vector database, data isolation is achieved through payload-based partitioning (Qdrant's recommended multitenancy approach):&lt;/strong&gt; &lt;code&gt;QdrantVectorDBStorage&lt;/code&gt; uses shared collections with payload filtering for unlimited workspace scalability.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;For relational databases, data isolation is achieved by adding a &lt;code&gt;workspace&lt;/code&gt; field to the tables for logical data separation:&lt;/strong&gt; &lt;code&gt;PGKVStorage&lt;/code&gt;, &lt;code&gt;PGVectorStorage&lt;/code&gt;, &lt;code&gt;PGDocStatusStorage&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;For the Neo4j graph database, logical data isolation is achieved through labels:&lt;/strong&gt; &lt;code&gt;Neo4JStorage&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To maintain compatibility with legacy data, the default workspace for PostgreSQL non-graph storage is &lt;code&gt;default&lt;/code&gt; and, for PostgreSQL AGE graph storage is null, for Neo4j graph storage is &lt;code&gt;base&lt;/code&gt; when no workspace is configured. For all external storages, the system provides dedicated workspace environment variables to override the common &lt;code&gt;WORKSPACE&lt;/code&gt; environment variable configuration. These storage-specific workspace environment variables are: &lt;code&gt;REDIS_WORKSPACE&lt;/code&gt;, &lt;code&gt;MILVUS_WORKSPACE&lt;/code&gt;, &lt;code&gt;QDRANT_WORKSPACE&lt;/code&gt;, &lt;code&gt;MONGODB_WORKSPACE&lt;/code&gt;, &lt;code&gt;POSTGRES_WORKSPACE&lt;/code&gt;, &lt;code&gt;NEO4J_WORKSPACE&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;AGENTS.md -- Guiding Coding Agents&lt;/h3&gt; 
&lt;p&gt;AGENTS.md is a simple, open format for guiding coding agents (&lt;a href="https://agents.md/"&gt;https://agents.md/&lt;/a&gt;). It is a dedicated, predictable place to provide the context and instructions to help AI coding agents work on LightRAG project. Different AI coders should not maintain separate guidance files individually. If any AI coder cannot automatically recognize AGENTS.md, symbolic links can be used as a solution. After establishing symbolic links, you can prevent them from being committed to the Git repository by configuring your local &lt;code&gt;.gitignore_global&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Edit Entities and Relations&lt;/h2&gt; 
&lt;p&gt;LightRAG now supports comprehensive knowledge graph management capabilities, allowing you to create, edit, and delete entities and relationships within your knowledge graph.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt; Create Entities and Relations &lt;/b&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Create new entity
entity = rag.create_entity("Google", {
    "description": "Google is a multinational technology company specializing in internet-related services and products.",
    "entity_type": "company"
})

# Create another entity
product = rag.create_entity("Gmail", {
    "description": "Gmail is an email service developed by Google.",
    "entity_type": "product"
})

# Create relation between entities
relation = rag.create_relation("Google", "Gmail", {
    "description": "Google develops and operates Gmail.",
    "keywords": "develops operates service",
    "weight": 2.0
})
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt; Edit Entities and Relations &lt;/b&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Edit an existing entity
updated_entity = rag.edit_entity("Google", {
    "description": "Google is a subsidiary of Alphabet Inc., founded in 1998.",
    "entity_type": "tech_company"
})

# Rename an entity (with all its relationships properly migrated)
renamed_entity = rag.edit_entity("Gmail", {
    "entity_name": "Google Mail",
    "description": "Google Mail (formerly Gmail) is an email service."
})

# Edit a relation between entities
updated_relation = rag.edit_relation("Google", "Google Mail", {
    "description": "Google created and maintains Google Mail service.",
    "keywords": "creates maintains email service",
    "weight": 3.0
})
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;All operations are available in both synchronous and asynchronous versions. The asynchronous versions have the prefix "a" (e.g., &lt;code&gt;acreate_entity&lt;/code&gt;, &lt;code&gt;aedit_relation&lt;/code&gt;).&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt; Insert Custom KG &lt;/b&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;custom_kg = {
        "chunks": [
            {
                "content": "Alice and Bob are collaborating on quantum computing research.",
                "source_id": "doc-1",
                "file_path": "test_file",
            }
        ],
        "entities": [
            {
                "entity_name": "Alice",
                "entity_type": "person",
                "description": "Alice is a researcher specializing in quantum physics.",
                "source_id": "doc-1",
                "file_path": "test_file"
            },
            {
                "entity_name": "Bob",
                "entity_type": "person",
                "description": "Bob is a mathematician.",
                "source_id": "doc-1",
                "file_path": "test_file"
            },
            {
                "entity_name": "Quantum Computing",
                "entity_type": "technology",
                "description": "Quantum computing utilizes quantum mechanical phenomena for computation.",
                "source_id": "doc-1",
                "file_path": "test_file"
            }
        ],
        "relationships": [
            {
                "src_id": "Alice",
                "tgt_id": "Bob",
                "description": "Alice and Bob are research partners.",
                "keywords": "collaboration research",
                "weight": 1.0,
                "source_id": "doc-1",
                "file_path": "test_file"
            },
            {
                "src_id": "Alice",
                "tgt_id": "Quantum Computing",
                "description": "Alice conducts research on quantum computing.",
                "keywords": "research expertise",
                "weight": 1.0,
                "source_id": "doc-1",
                "file_path": "test_file"
            },
            {
                "src_id": "Bob",
                "tgt_id": "Quantum Computing",
                "description": "Bob researches quantum computing.",
                "keywords": "research application",
                "weight": 1.0,
                "source_id": "doc-1",
                "file_path": "test_file"
            }
        ]
    }

rag.insert_custom_kg(custom_kg)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt;Other Entity and Relation Operations&lt;/b&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;create_entity&lt;/strong&gt;: Creates a new entity with specified attributes&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;edit_entity&lt;/strong&gt;: Updates an existing entity's attributes or renames it&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;create_relation&lt;/strong&gt;: Creates a new relation between existing entities&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;edit_relation&lt;/strong&gt;: Updates an existing relation's attributes&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;These operations maintain data consistency across both the graph database and vector database components, ensuring your knowledge graph remains coherent.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;Delete Functions&lt;/h2&gt; 
&lt;p&gt;LightRAG provides comprehensive deletion capabilities, allowing you to delete documents, entities, and relationships.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt;Delete Entities&lt;/b&gt; &lt;/summary&gt; 
 &lt;p&gt;You can delete entities by their name along with all associated relationships:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Delete entity and all its relationships (synchronous version)
rag.delete_by_entity("Google")

# Asynchronous version
await rag.adelete_by_entity("Google")
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;When deleting an entity:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Removes the entity node from the knowledge graph&lt;/li&gt; 
  &lt;li&gt;Deletes all associated relationships&lt;/li&gt; 
  &lt;li&gt;Removes related embedding vectors from the vector database&lt;/li&gt; 
  &lt;li&gt;Maintains knowledge graph integrity&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt;Delete Relations&lt;/b&gt; &lt;/summary&gt; 
 &lt;p&gt;You can delete relationships between two specific entities:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Delete relationship between two entities (synchronous version)
rag.delete_by_relation("Google", "Gmail")

# Asynchronous version
await rag.adelete_by_relation("Google", "Gmail")
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;When deleting a relationship:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Removes the specified relationship edge&lt;/li&gt; 
  &lt;li&gt;Deletes the relationship's embedding vector from the vector database&lt;/li&gt; 
  &lt;li&gt;Preserves both entity nodes and their other relationships&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt;Delete by Document ID&lt;/b&gt; &lt;/summary&gt; 
 &lt;p&gt;You can delete an entire document and all its related knowledge through document ID:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Delete by document ID (asynchronous version)
await rag.adelete_by_doc_id("doc-12345")
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Optimized processing when deleting by document ID:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Smart Cleanup&lt;/strong&gt;: Automatically identifies and removes entities and relationships that belong only to this document&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Preserve Shared Knowledge&lt;/strong&gt;: If entities or relationships exist in other documents, they are preserved and their descriptions are rebuilt&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Cache Optimization&lt;/strong&gt;: Clears related LLM cache to reduce storage overhead&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Incremental Rebuilding&lt;/strong&gt;: Reconstructs affected entity and relationship descriptions from remaining documents&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;The deletion process includes:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Delete all text chunks related to the document&lt;/li&gt; 
  &lt;li&gt;Identify and delete entities and relationships that belong only to this document&lt;/li&gt; 
  &lt;li&gt;Rebuild entities and relationships that still exist in other documents&lt;/li&gt; 
  &lt;li&gt;Update all related vector indexes&lt;/li&gt; 
  &lt;li&gt;Clean up document status records&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;Note: Deletion by document ID is an asynchronous operation as it involves complex knowledge graph reconstruction processes.&lt;/p&gt; 
&lt;/details&gt; 
&lt;p&gt;&lt;strong&gt;Important Reminders:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Irreversible Operations&lt;/strong&gt;: All deletion operations are irreversible, please use with caution&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Performance Considerations&lt;/strong&gt;: Deleting large amounts of data may take some time, especially deletion by document ID&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Data Consistency&lt;/strong&gt;: Deletion operations automatically maintain consistency between the knowledge graph and vector database&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Backup Recommendations&lt;/strong&gt;: Consider backing up data before performing important deletion operations&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Batch Deletion Recommendations:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;For batch deletion operations, consider using asynchronous methods for better performance&lt;/li&gt; 
 &lt;li&gt;For large-scale deletions, consider processing in batches to avoid excessive system load&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Entity Merging&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt;Merge Entities and Their Relationships&lt;/b&gt; &lt;/summary&gt; 
 &lt;p&gt;LightRAG now supports merging multiple entities into a single entity, automatically handling all relationships:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Basic entity merging
rag.merge_entities(
    source_entities=["Artificial Intelligence", "AI", "Machine Intelligence"],
    target_entity="AI Technology"
)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;With custom merge strategy:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Define custom merge strategy for different fields
rag.merge_entities(
    source_entities=["John Smith", "Dr. Smith", "J. Smith"],
    target_entity="John Smith",
    merge_strategy={
        "description": "concatenate",  # Combine all descriptions
        "entity_type": "keep_first",   # Keep the entity type from the first entity
        "source_id": "join_unique"     # Combine all unique source IDs
    }
)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;With custom target entity data:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Specify exact values for the merged entity
rag.merge_entities(
    source_entities=["New York", "NYC", "Big Apple"],
    target_entity="New York City",
    target_entity_data={
        "entity_type": "LOCATION",
        "description": "New York City is the most populous city in the United States.",
    }
)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Advanced usage combining both approaches:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Merge company entities with both strategy and custom data
rag.merge_entities(
    source_entities=["Microsoft Corp", "Microsoft Corporation", "MSFT"],
    target_entity="Microsoft",
    merge_strategy={
        "description": "concatenate",  # Combine all descriptions
        "source_id": "join_unique"     # Combine source IDs
    },
    target_entity_data={
        "entity_type": "ORGANIZATION",
    }
)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;When merging entities:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;All relationships from source entities are redirected to the target entity&lt;/li&gt; 
  &lt;li&gt;Duplicate relationships are intelligently merged&lt;/li&gt; 
  &lt;li&gt;Self-relationships (loops) are prevented&lt;/li&gt; 
  &lt;li&gt;Source entities are removed after merging&lt;/li&gt; 
  &lt;li&gt;Relationship weights and attributes are preserved&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;Multimodal Document Processing (RAG-Anything Integration)&lt;/h2&gt; 
&lt;p&gt;LightRAG now seamlessly integrates with &lt;a href="https://github.com/HKUDS/RAG-Anything"&gt;RAG-Anything&lt;/a&gt;, a comprehensive &lt;strong&gt;All-in-One Multimodal Document Processing RAG system&lt;/strong&gt; built specifically for LightRAG. RAG-Anything enables advanced parsing and retrieval-augmented generation (RAG) capabilities, allowing you to handle multimodal documents seamlessly and extract structured content—including text, images, tables, and formulas—from various document formats for integration into your RAG pipeline.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;End-to-End Multimodal Pipeline&lt;/strong&gt;: Complete workflow from document ingestion and parsing to intelligent multimodal query answering&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Universal Document Support&lt;/strong&gt;: Seamless processing of PDFs, Office documents (DOC/DOCX/PPT/PPTX/XLS/XLSX), images, and diverse file formats&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Specialized Content Analysis&lt;/strong&gt;: Dedicated processors for images, tables, mathematical equations, and heterogeneous content types&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multimodal Knowledge Graph&lt;/strong&gt;: Automatic entity extraction and cross-modal relationship discovery for enhanced understanding&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Hybrid Intelligent Retrieval&lt;/strong&gt;: Advanced search capabilities spanning textual and multimodal content with contextual understanding&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Quick Start:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Install RAG-Anything:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pip install raganything
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Process multimodal documents:&lt;/p&gt; 
  &lt;details&gt; 
   &lt;summary&gt; &lt;b&gt; RAGAnything Usage Example &lt;/b&gt;&lt;/summary&gt; 
   &lt;pre&gt;&lt;code class="language-python"&gt;    import asyncio
    from raganything import RAGAnything
    from lightrag import LightRAG
    from lightrag.llm.openai import openai_complete_if_cache, openai_embed
    from lightrag.utils import EmbeddingFunc
    import os

    async def load_existing_lightrag():
        # First, create or load an existing LightRAG instance
        lightrag_working_dir = "./existing_lightrag_storage"

        # Check if previous LightRAG instance exists
        if os.path.exists(lightrag_working_dir) and os.listdir(lightrag_working_dir):
            print("✅ Found existing LightRAG instance, loading...")
        else:
            print("❌ No existing LightRAG instance found, will create new one")

        # Create/Load LightRAG instance with your configurations
        lightrag_instance = LightRAG(
            working_dir=lightrag_working_dir,
            llm_model_func=lambda prompt, system_prompt=None, history_messages=[], **kwargs: openai_complete_if_cache(
                "gpt-4o-mini",
                prompt,
                system_prompt=system_prompt,
                history_messages=history_messages,
                api_key="your-api-key",
                **kwargs,
            ),
            embedding_func=EmbeddingFunc(
                embedding_dim=3072,
                func=lambda texts: openai_embed(
                    texts,
                    model="text-embedding-3-large",
                    api_key=api_key,
                    base_url=base_url,
                ),
            )
        )

        # Initialize storage (this will load existing data if available)
        await lightrag_instance.initialize_storages()

        # Now initialize RAGAnything with the existing LightRAG instance
        rag = RAGAnything(
            lightrag=lightrag_instance,  # Pass the existing LightRAG instance
            # Only need vision model for multimodal processing
            vision_model_func=lambda prompt, system_prompt=None, history_messages=[], image_data=None, **kwargs: openai_complete_if_cache(
                "gpt-4o",
                "",
                system_prompt=None,
                history_messages=[],
                messages=[
                    {"role": "system", "content": system_prompt} if system_prompt else None,
                    {"role": "user", "content": [
                        {"type": "text", "text": prompt},
                        {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_data}"}}
                    ]} if image_data else {"role": "user", "content": prompt}
                ],
                api_key="your-api-key",
                **kwargs,
            ) if image_data else openai_complete_if_cache(
                "gpt-4o-mini",
                prompt,
                system_prompt=system_prompt,
                history_messages=history_messages,
                api_key="your-api-key",
                **kwargs,
            )
            # Note: working_dir, llm_model_func, embedding_func, etc. are inherited from lightrag_instance
        )

        # Query the existing knowledge base
        result = await rag.query_with_multimodal(
            "What data has been processed in this LightRAG instance?",
            mode="hybrid"
        )
        print("Query result:", result)

        # Add new multimodal documents to the existing LightRAG instance
        await rag.process_document_complete(
            file_path="path/to/new/multimodal_document.pdf",
            output_dir="./output"
        )

    if __name__ == "__main__":
        asyncio.run(load_existing_lightrag())
&lt;/code&gt;&lt;/pre&gt; 
  &lt;/details&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;For detailed documentation and advanced usage, please refer to the &lt;a href="https://github.com/HKUDS/RAG-Anything"&gt;RAG-Anything repository&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Token Usage Tracking&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt;Overview and Usage&lt;/b&gt; &lt;/summary&gt; 
 &lt;p&gt;LightRAG provides a TokenTracker tool to monitor and manage token consumption by large language models. This feature is particularly useful for controlling API costs and optimizing performance.&lt;/p&gt; 
 &lt;h3&gt;Usage&lt;/h3&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from lightrag.utils import TokenTracker

# Create TokenTracker instance
token_tracker = TokenTracker()

# Method 1: Using context manager (Recommended)
# Suitable for scenarios requiring automatic token usage tracking
with token_tracker:
    result1 = await llm_model_func("your question 1")
    result2 = await llm_model_func("your question 2")

# Method 2: Manually adding token usage records
# Suitable for scenarios requiring more granular control over token statistics
token_tracker.reset()

rag.insert()

rag.query("your question 1", param=QueryParam(mode="naive"))
rag.query("your question 2", param=QueryParam(mode="mix"))

# Display total token usage (including insert and query operations)
print("Token usage:", token_tracker.get_usage())
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;Usage Tips&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Use context managers for long sessions or batch operations to automatically track all token consumption&lt;/li&gt; 
  &lt;li&gt;For scenarios requiring segmented statistics, use manual mode and call reset() when appropriate&lt;/li&gt; 
  &lt;li&gt;Regular checking of token usage helps detect abnormal consumption early&lt;/li&gt; 
  &lt;li&gt;Actively use this feature during development and testing to optimize production costs&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;Practical Examples&lt;/h3&gt; 
 &lt;p&gt;You can refer to these examples for implementing token tracking:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;examples/lightrag_gemini_track_token_demo.py&lt;/code&gt;: Token tracking example using Google Gemini model&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;examples/lightrag_siliconcloud_track_token_demo.py&lt;/code&gt;: Token tracking example using SiliconCloud model&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;These examples demonstrate how to effectively use the TokenTracker feature with different models and scenarios.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;Data Export Functions&lt;/h2&gt; 
&lt;h3&gt;Overview&lt;/h3&gt; 
&lt;p&gt;LightRAG allows you to export your knowledge graph data in various formats for analysis, sharing, and backup purposes. The system supports exporting entities, relations, and relationship data.&lt;/p&gt; 
&lt;h3&gt;Export Functions&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt; Basic Usage &lt;/b&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Basic CSV export (default format)
rag.export_data("knowledge_graph.csv")

# Specify any format
rag.export_data("output.xlsx", file_format="excel")
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt; Different File Formats supported &lt;/b&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;#Export data in CSV format
rag.export_data("graph_data.csv", file_format="csv")

# Export data in Excel sheet
rag.export_data("graph_data.xlsx", file_format="excel")

# Export data in markdown format
rag.export_data("graph_data.md", file_format="md")

# Export data in Text
rag.export_data("graph_data.txt", file_format="txt")
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt; Additional Options &lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;Include vector embeddings in the export (optional):&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;rag.export_data("complete_data.csv", include_vector_data=True)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;Data Included in Export&lt;/h3&gt; 
&lt;p&gt;All exports include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Entity information (names, IDs, metadata)&lt;/li&gt; 
 &lt;li&gt;Relation data (connections between entities)&lt;/li&gt; 
 &lt;li&gt;Relationship information from vector database&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Cache&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;b&gt;Clear Cache&lt;/b&gt; &lt;/summary&gt; 
 &lt;p&gt;You can clear the LLM response cache with different modes:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Clear all cache
await rag.aclear_cache()

# Clear local mode cache
await rag.aclear_cache(modes=["local"])

# Clear extraction cache
await rag.aclear_cache(modes=["default"])

# Clear multiple modes
await rag.aclear_cache(modes=["local", "global", "hybrid"])

# Synchronous version
rag.clear_cache(modes=["local"])
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Valid modes are:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;"default"&lt;/code&gt;: Extraction cache&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;"naive"&lt;/code&gt;: Naive search cache&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;"local"&lt;/code&gt;: Local search cache&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;"global"&lt;/code&gt;: Global search cache&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;"hybrid"&lt;/code&gt;: Hybrid search cache&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;"mix"&lt;/code&gt;: Mix search cache&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;Troubleshooting&lt;/h2&gt; 
&lt;h3&gt;Common Initialization Errors&lt;/h3&gt; 
&lt;p&gt;If you encounter these errors when using LightRAG:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;AttributeError: __aenter__&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Cause&lt;/strong&gt;: Storage backends not initialized&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Solution&lt;/strong&gt;: Call &lt;code&gt;await rag.initialize_storages()&lt;/code&gt; after creating the LightRAG instance&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;KeyError: 'history_messages'&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Cause&lt;/strong&gt;: Pipeline status not initialized&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Solution&lt;/strong&gt;: Call `&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Both errors in sequence&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Cause&lt;/strong&gt;: Neither initialization method was called&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Solution&lt;/strong&gt;: Always follow this pattern:&lt;/li&gt; 
  &lt;/ul&gt; &lt;pre&gt;&lt;code class="language-python"&gt;rag = LightRAG(...)
await rag.initialize_storages()   ```

&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Model Switching Issues&lt;/h3&gt; 
&lt;p&gt;When switching between different embedding models, you must clear the data directory to avoid errors. The only file you may want to preserve is &lt;code&gt;kv_store_llm_response_cache.json&lt;/code&gt; if you wish to retain the LLM cache.&lt;/p&gt; 
&lt;h2&gt;LightRAG API&lt;/h2&gt; 
&lt;p&gt;The LightRAG Server is designed to provide Web UI and API support. &lt;strong&gt;For more information about LightRAG Server, please refer to &lt;a href="https://raw.githubusercontent.com/HKUDS/LightRAG/main/lightrag/api/README.md"&gt;LightRAG Server&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;Graph Visualization&lt;/h2&gt; 
&lt;p&gt;The LightRAG Server offers a comprehensive knowledge graph visualization feature. It supports various gravity layouts, node queries, subgraph filtering, and more. &lt;strong&gt;For more information about LightRAG Server, please refer to &lt;a href="https://raw.githubusercontent.com/HKUDS/LightRAG/main/lightrag/api/README.md"&gt;LightRAG Server&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/HKUDS/LightRAG/main/README.assets/iShot_2025-03-23_12.40.08.png" alt="iShot_2025-03-23_12.40.08" /&gt;&lt;/p&gt; 
&lt;h2&gt;Langfuse observability integration&lt;/h2&gt; 
&lt;p&gt;Langfuse provides a drop-in replacement for the OpenAI client that automatically tracks all LLM interactions, enabling developers to monitor, debug, and optimize their RAG systems without code changes.&lt;/p&gt; 
&lt;h3&gt;Installation with Langfuse option&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;pip install lightrag-hku
pip install lightrag-hku[observability]

# Or install from source code with debug mode enabled
pip install -e .
pip install -e ".[observability]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Config Langfuse env vars&lt;/h3&gt; 
&lt;p&gt;modify .env file:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;## Langfuse Observability (Optional)
# LLM observability and tracing platform
# Install with: pip install lightrag-hku[observability]
# Sign up at: https://cloud.langfuse.com or self-host
LANGFUSE_SECRET_KEY=""
LANGFUSE_PUBLIC_KEY=""
LANGFUSE_HOST="https://cloud.langfuse.com"  # or your self-hosted instance
LANGFUSE_ENABLE_TRACE=true
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Langfuse Usage&lt;/h3&gt; 
&lt;p&gt;Once installed and configured, Langfuse automatically traces all OpenAI LLM calls. Langfuse dashboard features include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Tracing&lt;/strong&gt;: View complete LLM call chains&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Analytics&lt;/strong&gt;: Token usage, latency, cost metrics&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Debugging&lt;/strong&gt;: Inspect prompts and responses&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Evaluation&lt;/strong&gt;: Compare model outputs&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Monitoring&lt;/strong&gt;: Real-time alerting&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Important Notice&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: LightRAG currently only integrates OpenAI-compatible API calls with Langfuse. APIs such as Ollama, Azure, and AWS Bedrock are not yet supported for Langfuse observability.&lt;/p&gt; 
&lt;h2&gt;RAGAS-based Evaluation&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;RAGAS&lt;/strong&gt; (Retrieval Augmented Generation Assessment) is a framework for reference-free evaluation of RAG systems using LLMs. There is an evaluation script based on RAGAS. For detailed information, please refer to &lt;a href="https://raw.githubusercontent.com/HKUDS/LightRAG/main/lightrag/evaluation/README_EVALUASTION_RAGAS.md"&gt;RAGAS-based Evaluation Framework&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Evaluation&lt;/h2&gt; 
&lt;h3&gt;Dataset&lt;/h3&gt; 
&lt;p&gt;The dataset used in LightRAG can be downloaded from &lt;a href="https://huggingface.co/datasets/TommyChien/UltraDomain"&gt;TommyChien/UltraDomain&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Generate Query&lt;/h3&gt; 
&lt;p&gt;LightRAG uses the following prompt to generate high-level queries, with the corresponding code in &lt;code&gt;examples/generate_query.py&lt;/code&gt;.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt; Prompt &lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;Given the following description of a dataset:

{description}

Please identify 5 potential users who would engage with this dataset. For each user, list 5 tasks they would perform with this dataset. Then, for each (user, task) combination, generate 5 questions that require a high-level understanding of the entire dataset.

Output the results in the following structure:
- User 1: [user description]
    - Task 1: [task description]
        - Question 1:
        - Question 2:
        - Question 3:
        - Question 4:
        - Question 5:
    - Task 2: [task description]
        ...
    - Task 5: [task description]
- User 2: [user description]
    ...
- User 5: [user description]
    ...
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;Batch Eval&lt;/h3&gt; 
&lt;p&gt;To evaluate the performance of two RAG systems on high-level queries, LightRAG uses the following prompt, with the specific code available in &lt;code&gt;reproduce/batch_eval.py&lt;/code&gt;.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt; Prompt &lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;---Role---
You are an expert tasked with evaluating two answers to the same question based on three criteria: **Comprehensiveness**, **Diversity**, and **Empowerment**.
---Goal---
You will evaluate two answers to the same question based on three criteria: **Comprehensiveness**, **Diversity**, and **Empowerment**.

- **Comprehensiveness**: How much detail does the answer provide to cover all aspects and details of the question?
- **Diversity**: How varied and rich is the answer in providing different perspectives and insights on the question?
- **Empowerment**: How well does the answer help the reader understand and make informed judgments about the topic?

For each criterion, choose the better answer (either Answer 1 or Answer 2) and explain why. Then, select an overall winner based on these three categories.

Here is the question:
{query}

Here are the two answers:

**Answer 1:**
{answer1}

**Answer 2:**
{answer2}

Evaluate both answers using the three criteria listed above and provide detailed explanations for each criterion.

Output your evaluation in the following JSON format:

{{
    "Comprehensiveness": {{
        "Winner": "[Answer 1 or Answer 2]",
        "Explanation": "[Provide explanation here]"
    }},
    "Empowerment": {{
        "Winner": "[Answer 1 or Answer 2]",
        "Explanation": "[Provide explanation here]"
    }},
    "Overall Winner": {{
        "Winner": "[Answer 1 or Answer 2]",
        "Explanation": "[Summarize why this answer is the overall winner based on the three criteria]"
    }}
}}
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;Overall Performance Table&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Agriculture&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;CS&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Legal&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Mix&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;NaiveRAG&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;LightRAG&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;NaiveRAG&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;LightRAG&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;NaiveRAG&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;LightRAG&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;NaiveRAG&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;LightRAG&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Comprehensiveness&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;32.4%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;67.6%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;38.4%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;61.6%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;16.4%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;83.6%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;38.8%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;61.2%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Diversity&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;23.6%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;76.4%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;38.0%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;62.0%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;13.6%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;86.4%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;32.4%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;67.6%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Empowerment&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;32.4%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;67.6%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;38.8%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;61.2%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;16.4%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;83.6%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;42.8%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;57.2%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Overall&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;32.4%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;67.6%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;38.8%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;61.2%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;15.2%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;84.8%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;40.0%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;60.0%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;RQ-RAG&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;LightRAG&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;RQ-RAG&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;LightRAG&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;RQ-RAG&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;LightRAG&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;RQ-RAG&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;LightRAG&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Comprehensiveness&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;31.6%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;68.4%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;38.8%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;61.2%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;15.2%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;84.8%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;39.2%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;60.8%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Diversity&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;29.2%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;70.8%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;39.2%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;60.8%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;11.6%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;88.4%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;30.8%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;69.2%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Empowerment&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;31.6%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;68.4%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;36.4%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;63.6%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;15.2%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;84.8%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;42.4%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;57.6%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Overall&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;32.4%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;67.6%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;38.0%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;62.0%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;14.4%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;85.6%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;40.0%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;60.0%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;HyDE&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;LightRAG&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;HyDE&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;LightRAG&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;HyDE&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;LightRAG&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;HyDE&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;LightRAG&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Comprehensiveness&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;26.0%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;74.0%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;41.6%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;58.4%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;26.8%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;73.2%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;40.4%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;59.6%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Diversity&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;24.0%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;76.0%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;38.8%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;61.2%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;20.0%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;80.0%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;32.4%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;67.6%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Empowerment&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;25.2%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;74.8%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;40.8%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;59.2%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;26.0%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;74.0%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;46.0%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;54.0%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Overall&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;24.8%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;75.2%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;41.6%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;58.4%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;26.4%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;73.6%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;42.4%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;57.6%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;GraphRAG&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;LightRAG&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;GraphRAG&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;LightRAG&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;GraphRAG&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;LightRAG&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;GraphRAG&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;LightRAG&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Comprehensiveness&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;45.6%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;54.4%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;48.4%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;51.6%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;48.4%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;51.6%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;50.4%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;49.6%&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Diversity&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;22.8%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;77.2%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;40.8%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;59.2%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;26.4%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;73.6%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;36.0%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;64.0%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Empowerment&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;41.2%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;58.8%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;45.2%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;54.8%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;43.6%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;56.4%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;50.8%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;49.2%&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Overall&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;45.2%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;54.8%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;48.0%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;52.0%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;47.2%&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;52.8%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;50.4%&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;49.6%&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Reproduce&lt;/h2&gt; 
&lt;p&gt;All the code can be found in the &lt;code&gt;./reproduce&lt;/code&gt; directory.&lt;/p&gt; 
&lt;h3&gt;Step-0 Extract Unique Contexts&lt;/h3&gt; 
&lt;p&gt;First, we need to extract unique contexts in the datasets.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt; Code &lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;def extract_unique_contexts(input_directory, output_directory):

    os.makedirs(output_directory, exist_ok=True)

    jsonl_files = glob.glob(os.path.join(input_directory, '*.jsonl'))
    print(f"Found {len(jsonl_files)} JSONL files.")

    for file_path in jsonl_files:
        filename = os.path.basename(file_path)
        name, ext = os.path.splitext(filename)
        output_filename = f"{name}_unique_contexts.json"
        output_path = os.path.join(output_directory, output_filename)

        unique_contexts_dict = {}

        print(f"Processing file: {filename}")

        try:
            with open(file_path, 'r', encoding='utf-8') as infile:
                for line_number, line in enumerate(infile, start=1):
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        json_obj = json.loads(line)
                        context = json_obj.get('context')
                        if context and context not in unique_contexts_dict:
                            unique_contexts_dict[context] = None
                    except json.JSONDecodeError as e:
                        print(f"JSON decoding error in file {filename} at line {line_number}: {e}")
        except FileNotFoundError:
            print(f"File not found: {filename}")
            continue
        except Exception as e:
            print(f"An error occurred while processing file {filename}: {e}")
            continue

        unique_contexts_list = list(unique_contexts_dict.keys())
        print(f"There are {len(unique_contexts_list)} unique `context` entries in the file {filename}.")

        try:
            with open(output_path, 'w', encoding='utf-8') as outfile:
                json.dump(unique_contexts_list, outfile, ensure_ascii=False, indent=4)
            print(f"Unique `context` entries have been saved to: {output_filename}")
        except Exception as e:
            print(f"An error occurred while saving to the file {output_filename}: {e}")

    print("All files have been processed.")

&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;Step-1 Insert Contexts&lt;/h3&gt; 
&lt;p&gt;For the extracted contexts, we insert them into the LightRAG system.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt; Code &lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;def insert_text(rag, file_path):
    with open(file_path, mode='r') as f:
        unique_contexts = json.load(f)

    retries = 0
    max_retries = 3
    while retries &amp;lt; max_retries:
        try:
            rag.insert(unique_contexts)
            break
        except Exception as e:
            retries += 1
            print(f"Insertion failed, retrying ({retries}/{max_retries}), error: {e}")
            time.sleep(10)
    if retries == max_retries:
        print("Insertion failed after exceeding the maximum number of retries")
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;Step-2 Generate Queries&lt;/h3&gt; 
&lt;p&gt;We extract tokens from the first and the second half of each context in the dataset, then combine them as dataset descriptions to generate queries.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt; Code &lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

def get_summary(context, tot_tokens=2000):
    tokens = tokenizer.tokenize(context)
    half_tokens = tot_tokens // 2

    start_tokens = tokens[1000:1000 + half_tokens]
    end_tokens = tokens[-(1000 + half_tokens):1000]

    summary_tokens = start_tokens + end_tokens
    summary = tokenizer.convert_tokens_to_string(summary_tokens)

    return summary
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;Step-3 Query&lt;/h3&gt; 
&lt;p&gt;For the queries generated in Step-2, we will extract them and query LightRAG.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt; Code &lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;def extract_queries(file_path):
    with open(file_path, 'r') as f:
        data = f.read()

    data = data.replace('**', '')

    queries = re.findall(r'- Question \d+: (.+)', data)

    return queries
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;🔗 Related Projects&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;Ecosystem &amp;amp; Extensions&lt;/em&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td align="center"&gt; &lt;a href="https://github.com/HKUDS/RAG-Anything"&gt; 
      &lt;div style="width: 100px; height: 100px; background: linear-gradient(135deg, rgba(0, 217, 255, 0.1) 0%, rgba(0, 217, 255, 0.05) 100%); border-radius: 15px; border: 1px solid rgba(0, 217, 255, 0.2); display: flex; align-items: center; justify-content: center; margin-bottom: 10px;"&gt; 
       &lt;span style="font-size: 32px;"&gt;📸&lt;/span&gt; 
      &lt;/div&gt; &lt;b&gt;RAG-Anything&lt;/b&gt;&lt;br /&gt; &lt;sub&gt;Multimodal RAG&lt;/sub&gt; &lt;/a&gt; &lt;/td&gt; 
    &lt;td align="center"&gt; &lt;a href="https://github.com/HKUDS/VideoRAG"&gt; 
      &lt;div style="width: 100px; height: 100px; background: linear-gradient(135deg, rgba(0, 217, 255, 0.1) 0%, rgba(0, 217, 255, 0.05) 100%); border-radius: 15px; border: 1px solid rgba(0, 217, 255, 0.2); display: flex; align-items: center; justify-content: center; margin-bottom: 10px;"&gt; 
       &lt;span style="font-size: 32px;"&gt;🎥&lt;/span&gt; 
      &lt;/div&gt; &lt;b&gt;VideoRAG&lt;/b&gt;&lt;br /&gt; &lt;sub&gt;Extreme Long-Context Video RAG&lt;/sub&gt; &lt;/a&gt; &lt;/td&gt; 
    &lt;td align="center"&gt; &lt;a href="https://github.com/HKUDS/MiniRAG"&gt; 
      &lt;div style="width: 100px; height: 100px; background: linear-gradient(135deg, rgba(0, 217, 255, 0.1) 0%, rgba(0, 217, 255, 0.05) 100%); border-radius: 15px; border: 1px solid rgba(0, 217, 255, 0.2); display: flex; align-items: center; justify-content: center; margin-bottom: 10px;"&gt; 
       &lt;span style="font-size: 32px;"&gt;✨&lt;/span&gt; 
      &lt;/div&gt; &lt;b&gt;MiniRAG&lt;/b&gt;&lt;br /&gt; &lt;sub&gt;Extremely Simple RAG&lt;/sub&gt; &lt;/a&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;⭐ Star History&lt;/h2&gt; 
&lt;a href="https://star-history.com/#HKUDS/LightRAG&amp;amp;Date"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=HKUDS/LightRAG&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=HKUDS/LightRAG&amp;amp;type=Date" /&gt; 
  &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=HKUDS/LightRAG&amp;amp;type=Date" /&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;h2&gt;🤝 Contribution&lt;/h2&gt; 
&lt;div align="center"&gt;
  We thank all our contributors for their valuable contributions. 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/HKUDS/LightRAG/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=HKUDS/LightRAG" style="border-radius: 15px; box-shadow: 0 0 20px rgba(0, 217, 255, 0.3);" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;📖 Citation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;@article{guo2024lightrag,
title={LightRAG: Simple and Fast Retrieval-Augmented Generation},
author={Zirui Guo and Lianghao Xia and Yanhua Yu and Tu Ao and Chao Huang},
year={2024},
eprint={2410.05779},
archivePrefix={arXiv},
primaryClass={cs.IR}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;div align="center" style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 15px; padding: 30px; margin: 30px 0;"&gt; 
 &lt;div&gt; 
  &lt;img src="https://user-images.githubusercontent.com/74038190/212284100-561aa473-3905-4a80-b561-0d28506553ee.gif" width="500" /&gt; 
 &lt;/div&gt; 
 &lt;div style="margin-top: 20px;"&gt; 
  &lt;a href="https://github.com/HKUDS/LightRAG" style="text-decoration: none;"&gt; &lt;img src="https://img.shields.io/badge/⭐%20Star%20us%20on%20GitHub-1a1a2e?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white" /&gt; &lt;/a&gt; 
  &lt;a href="https://github.com/HKUDS/LightRAG/issues" style="text-decoration: none;"&gt; &lt;img src="https://img.shields.io/badge/🐛%20Report%20Issues-ff6b6b?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white" /&gt; &lt;/a&gt; 
  &lt;a href="https://github.com/HKUDS/LightRAG/discussions" style="text-decoration: none;"&gt; &lt;img src="https://img.shields.io/badge/💬%20Discussions-4ecdc4?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white" /&gt; &lt;/a&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;div style="width: 100%; max-width: 600px; margin: 20px auto; padding: 20px; background: linear-gradient(135deg, rgba(0, 217, 255, 0.1) 0%, rgba(0, 217, 255, 0.05) 100%); border-radius: 15px; border: 1px solid rgba(0, 217, 255, 0.2);"&gt; 
  &lt;div style="display: flex; justify-content: center; align-items: center; gap: 15px;"&gt; 
   &lt;span style="font-size: 24px;"&gt;⭐&lt;/span&gt; 
   &lt;span style="color: #00d9ff; font-size: 18px;"&gt;Thank you for visiting LightRAG!&lt;/span&gt; 
   &lt;span style="font-size: 24px;"&gt;⭐&lt;/span&gt; 
  &lt;/div&gt; 
 &lt;/div&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>ostris/ai-toolkit</title>
      <link>https://github.com/ostris/ai-toolkit</link>
      <description>&lt;p&gt;The ultimate training toolkit for finetuning diffusion models&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AI Toolkit by Ostris&lt;/h1&gt; 
&lt;p&gt;AI Toolkit is an all in one training suite for diffusion models. I try to support all the latest models on consumer grade hardware. Image and video models. It can be run as a GUI or CLI. It is designed to be easy to use but still have every feature imaginable.&lt;/p&gt; 
&lt;h2&gt;Support My Work&lt;/h2&gt; 
&lt;p&gt;If you enjoy my projects or use them commercially, please consider sponsoring me. Every bit helps! 💖&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/orgs/ostris"&gt;Sponsor on GitHub&lt;/a&gt; | &lt;a href="https://www.patreon.com/ostris"&gt;Support on Patreon&lt;/a&gt; | &lt;a href="https://www.paypal.com/donate/?hosted_button_id=9GEFUKC8T9R9W"&gt;Donate on PayPal&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Current Sponsors&lt;/h3&gt; 
&lt;p&gt;All of these people / organizations are the ones who selflessly make this project possible. Thank you!!&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Last updated: 2025-12-17 22:19 UTC&lt;/em&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://x.com/NuxZoe" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://pbs.twimg.com/profile_images/1919488160125616128/QAZXTMEj_400x400.png" alt="a16z" width="280" height="280" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;a href="https://github.com/replicate" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://avatars.githubusercontent.com/u/60410876?v=4" alt="Replicate" width="280" height="280" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;a href="https://github.com/huggingface" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://avatars.githubusercontent.com/u/25720743?v=4" alt="Hugging Face" width="280" height="280" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;hr style="width:100%;border:none;height:2px;background:#ddd;margin:30px 0;" /&gt; 
&lt;p align="center"&gt; &lt;a href="https://www.pixelcut.ai/" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://pbs.twimg.com/profile_images/1496882159658885133/11asz2Sc_400x400.jpg" alt="Pixelcut" width="200" height="200" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;a href="https://github.com/weights-ai" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://avatars.githubusercontent.com/u/185568492?v=4" alt="Weights" width="200" height="200" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;a href="https://github.com/josephrocca" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://avatars.githubusercontent.com/u/1167575?u=92d92921b4cb5c8c7e225663fed53c4b41897736&amp;amp;v=4" alt="josephrocca" width="200" height="200" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;img src="https://c8.patreon.com/4/200/93304/J" alt="Joseph Rocca" width="200" height="200" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/161471720/dd330b4036d44a5985ed5985c12a5def/eyJ3IjoyMDB9/1.jpeg?token-hash=k1f4Vv7TevzYa9tqlzAjsogYmkZs8nrXQohPCDGJGkc%3D" alt="Vladimir Sotnikov" width="200" height="200" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/33158543/C" alt="clement Delangue" width="200" height="200" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/8654302/b0f5ebedc62a47c4b56222693e1254e9/eyJ3IjoyMDB9/2.jpeg?token-hash=suI7_QjKUgWpdPuJPaIkElkTrXfItHlL8ZHLPT-w_d4%3D" alt="Misch Strotz" width="200" height="200" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;/p&gt; 
&lt;hr style="width:100%;border:none;height:2px;background:#ddd;margin:30px 0;" /&gt; 
&lt;p align="center"&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/120239481/49b1ce70d3d24704b8ec34de24ec8f55/eyJ3IjoyMDB9/1.jpeg?token-hash=o0y1JqSXqtGvVXnxb06HMXjQXs6OII9yMMx5WyyUqT4%3D" alt="nitish PNR" width="150" height="150" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/152118848/3b15a43d71714552b5ed1c9f84e66adf/eyJ3IjoyMDB9/1.png?token-hash=MKf3sWHz0MFPm_OAFjdsNvxoBfN5B5l54mn1ORdlRy8%3D" alt="Kristjan Retter" width="150" height="150" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/2298192/1228b69bd7d7481baf3103315183250d/eyJ3IjoyMDB9/1.jpg?token-hash=opN1e4r4Nnvqbtr8R9HI8eyf9m5F50CiHDOdHzb4UcA%3D" alt="Mohamed Oumoumad" width="150" height="150" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/548524/S" alt="Steve Hanff" width="150" height="150" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/8449560/P" alt="Patron" width="150" height="150" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/169502989/220069e79ce745b29237e94c22a729df/eyJ3IjoyMDB9/1.png?token-hash=E8E2JOqx66k2zMtYUw8Gy57dw-gVqA6OPpdCmWFFSFw%3D" alt="Timothy Bielec" width="150" height="150" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;/p&gt; 
&lt;hr style="width:100%;border:none;height:2px;background:#ddd;margin:30px 0;" /&gt; 
&lt;p align="center"&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/9547341/bb35d9a222fd460e862e960ba3eacbaf/eyJ3IjoyMDB9/1.jpeg?token-hash=Q2XGDvkCbiONeWNxBCTeTMOcuwTjOaJ8Z-CAf5xq3Hs%3D" alt="Travis Harrington" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/5021048/c6beacab0fdb4568bf9f0d549aa4bc44/eyJ3IjoyMDB9/1.jpeg?token-hash=JTEtFVzUeU7pQw4R3eSn6rGgqgi44uc2rDBAv6F6A4o%3D" alt="Infinite " width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;a href="https://x.com/NuxZoe" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://pbs.twimg.com/profile_images/1916482710069014528/RDLnPRSg_400x400.jpg" alt="tungsten" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;a href="https://github.com/E2GO" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://avatars.githubusercontent.com/u/1776669?u=bf52b2691fa7d1e421d6167b804a2c1cf3b229e7&amp;amp;v=4" alt="E2GO" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;img src="https://c8.patreon.com/4/200/7478272/T" alt="Totoro " width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;a href="https://clwill.com/" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://images.squarespace-cdn.com/content/v1/63d444727a5d5f304f89eebe/c9def9ce-3824-404d-a8bb-96b6236338ca/favicon.ico?format=100w" alt="Christopher Williams" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;a href="http://www.ir-ltd.net" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://pbs.twimg.com/profile_images/1602579392198283264/6Tm2GYus_400x400.jpg" alt="IR-Entertainment Ltd" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/27288932/6c35d2d961ee4e14a7a368c990791315/eyJ3IjoyMDB9/1.jpeg?token-hash=TGIto_PGEG2NEKNyqwzEnRStOkhrjb3QlMhHA3raKJY%3D" alt="David Garrido" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/33228112/J" alt="Jimmy Simmons" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;a href="https://www.runcomfy.com/trainer/ai-toolkit/app" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://pbs.twimg.com/profile_images/1747828425736273922/nlPQTDYO_400x400.jpg" alt="RunComfy" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;img src="https://c8.patreon.com/4/200/55206617/X" alt="xv" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/3712451/432e22a355494ec0a1ea1927ff8d452e/eyJ3IjoyMDB9/7.jpeg?token-hash=OpQ9SAfVQ4Un9dSYlGTHuApZo5GlJ797Mo0DtVtMOSc%3D" alt="David Shorey" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/80767260/1fa7b3119f9f4f40a68452e57de59bfe/eyJ3IjoyMDB9/1.jpeg?token-hash=H34Vxnd58NtbuJU1XFYPkQnraVXSynZHSL3SMMcdKbI%3D" alt="nuliajuk" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/40761075/R" alt="Randy McEntee" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/98811435/3a3632d1795b4c2b9f8f0270f2f6a650/eyJ3IjoyMDB9/1.jpeg?token-hash=657rzuJ0bZavMRZW3XZ-xQGqm3Vk6FkMZgFJVMCOPdk%3D" alt="EmmanuelMr18" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/93348210/5c650f32a0bc481d80900d2674528777/eyJ3IjoyMDB9/1.jpeg?token-hash=0jiknRw3jXqYWW6En8bNfuHgVDj4LI_rL7lSS4-_xlo%3D" alt="Armin Behjati" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/155963250/6f8fd7075c3b4247bfeb054ba49172d6/eyJ3IjoyMDB9/1.png?token-hash=z81EHmdU2cqSrwa9vJmZTV3h0LG-z9Qakhxq34FrYT4%3D" alt="Un Defined" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;a href="https://github.com/squewel" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://avatars.githubusercontent.com/u/97603184?v=4" alt="squewel" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;img src="https://c8.patreon.com/4/200/27791680/J" alt="Jean-Tristan Marin" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/570742/4ceb33453a5a4745b430a216aba9280f/eyJ3IjoyMDB9/1.jpg?token-hash=nPcJ2zj3sloND9jvbnbYnob2vMXRnXdRuujthqDLWlU%3D" alt="Al H" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/82763/f99cc484361d4b9d94fe4f0814ada303/eyJ3IjoyMDB9/1.jpeg?token-hash=A3JWlBNL0b24FFWb-FCRDAyhs-OAxg-zrhfBXP_axuU%3D" alt="Doron Adler" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/103077711/bb215761cc004e80bd9cec7d4bcd636d/eyJ3IjoyMDB9/2.jpeg?token-hash=3U8kdZSUpnmeYIDVK4zK9TTXFpnAud_zOwBRXx18018%3D" alt="John Dopamine" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/99036356/7ae9c4d80e604e739b68cca12ee2ed01/eyJ3IjoyMDB9/3.png?token-hash=ZhsBMoTOZjJ-Y6h5NOmU5MT-vDb2fjK46JDlpEehkVQ%3D" alt="njgnfhahfnhnwir" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/141098579/1a9f0a1249d447a7a0df718a57343912/eyJ3IjoyMDB9/2.png?token-hash=_n-AQmPgY0FP9zCGTIEsr5ka4Y7YuaMkt3qL26ZqGg8%3D" alt="The Local Lab" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;a href="https://x.com/RalFingerLP" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://pbs.twimg.com/profile_images/919595465041162241/ZU7X3T5k_400x400.jpg" alt="RalFinger" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;img src="https://c8.patreon.com/4/200/53077895/M" alt="Marc" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/157407541/bb9d80cffdab4334ad78366060561520/eyJ3IjoyMDB9/2.png?token-hash=WYz-U_9zabhHstOT5UIa5jBaoFwrwwqyWxWEzIR2m_c%3D" alt="Tokio Studio srl IT10640050968" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/128354277/52c073d323924b02ada90c9eacc6b0a0/eyJ3IjoyMDB9/1.png?token-hash=Oc0mVzELN1s1r0lLQTEO_sfJ2lEMC3X-By2O2bG6h_Q%3D" alt="Alastair Green" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/134129880/680c7e14cd1a4d1a9face921fb010f88/eyJ3IjoyMDB9/1.png?token-hash=5fqqHE6DCTbt7gDQL7VRcWkV71jF7FvWcLhpYl5aMXA%3D" alt="Bharat Prabhakar" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/70218846/C" alt="Cosmosis" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;a href="https://github.com/dylanzonix" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://avatars.githubusercontent.com/u/167351340?v=4" alt="Dylan" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/30931983/54ab4e4ceab946e79a6418d205f9ed51/eyJ3IjoyMDB9/1.png?token-hash=j2phDrgd6IWuqKqNIDbq9fR2B3fMF-GUCQSdETS1w5Y%3D" alt="HestoySeghuro ." width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/4105384/J" alt="Jack Blakely" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;a href="https://github.com/jakeblakeley" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://avatars.githubusercontent.com/u/2407659?u=be0bc786663527f2346b2e99ff608796bce19b26&amp;amp;v=4" alt="Jake Blakeley" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/47255859/7d68bf494f7645a382875fbaf901bf90/eyJ3IjoyMDB9/1.jpeg?token-hash=GUJtLcSZhj0sEvBWB1EiLXEw0hVQxr2Mf7YMUharte0%3D" alt="momen sree" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;a href="https://github.com/Slartibart23" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://avatars.githubusercontent.com/u/133593860?u=31217adb2522fb295805824ffa7e14e8f0fca6fa&amp;amp;v=4" alt="Slarti" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/97985240/3d1d0e6905d045aba713e8132cab4a30/eyJ3IjoyMDB9/1.png?token-hash=fRavvbO_yqWKA_OsJb5DzjfKZ1Yt-TG-ihMoeVBvlcM%3D" alt="עומר מכלוף" width="100" height="100" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;/p&gt; 
&lt;hr style="width:100%;border:none;height:2px;background:#ddd;margin:30px 0;" /&gt; 
&lt;p align="center"&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/44568304/a9d83a0e786b41b4bdada150f7c9271c/eyJ3IjoyMDB9/1.jpeg?token-hash=FtxnwrSrknQUQKvDRv2rqPceX2EF23eLq4pNQYM_fmw%3D" alt="Albert Bukoski" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/5048649/B" alt="Ben Ward" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/111904990/08b1cf65be6a4de091c9b73b693b3468/eyJ3IjoyMDB9/1.png?token-hash=_Odz6RD3CxtubEHbUxYujcjw6zAajbo3w8TRz249VBA%3D" alt="Brian Smith" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/494309/J" alt="Julian Tsependa" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/5602036/c7b6e02bab1241fc83ff5a0cedf19b43/eyJ3IjoyMDB9/1.jpeg?token-hash=nnd10QRNxqaHmhwr-zQh4EIlBDIFJEvt65YB3ebjhNw%3D" alt="Kelevra Quackenstien" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/358350/L" alt="L D" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/159203973/36c817f941ac4fa18103a4b8c0cb9cae/eyJ3IjoyMDB9/1.png?token-hash=zkt72HW3EoiIEAn3LSk9gJPBsXfuTVcc4rRBS3CeR8w%3D" alt="Marko jak" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/11198131/e696d9647feb4318bcf16243c2425805/eyJ3IjoyMDB9/1.jpeg?token-hash=c2c2p1SaiX86iXAigvGRvzm4jDHvIFCg298A49nIfUM%3D" alt="Nicholas Agranoff" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/785333/bdb9ede5765d42e5a2021a86eebf0d8f/eyJ3IjoyMDB9/2.jpg?token-hash=l_rajMhxTm6wFFPn7YdoKBxeUqhdRXKdy6_8SGCuNsE%3D" alt="Sapjes " width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/76566911/6485eaf5ec6249a7b524ee0b979372f0/eyJ3IjoyMDB9/1.jpeg?token-hash=mwCSkTelDBaengG32NkN0lVl5mRjB-cwo6-a47wnOsU%3D" alt="the biitz" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/83034/W" alt="william tatum" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/32633822/1ab5612efe80417cbebfe91e871fc052/eyJ3IjoyMDB9/1.png?token-hash=pOS_IU3b3RL5-iL96A3Xqoj2bQ-dDo4RUkBylcMED_s%3D" alt="Zack Abrams" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/25209707/36ae876d662d4d85aaf162b6d67d31e7/eyJ3IjoyMDB9/1.png?token-hash=Zows_A6uqlY5jClhfr4Y3QfMnDKVkS3mbxNHUDkVejo%3D" alt="fjioq8" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/46680573/ee3d99c04a674dd5a8e1ecfb926db6a2/eyJ3IjoyMDB9/1.jpeg?token-hash=cgD4EXyfZMPnXIrcqWQ5jGqzRUfqjPafb9yWfZUPB4Q%3D" alt="Neil Murray" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;a href="https://github.com/julien-blanchon" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://avatars.githubusercontent.com/u/11278197?v=4" alt="Blanchon" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Wallawalla47" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://avatars.githubusercontent.com/u/46779408?v=4" alt="Ian R" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;img src="https://c8.patreon.com/4/200/63510241/A" alt="Andrew Park" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;a href="https://github.com/Spikhalskiy" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://avatars.githubusercontent.com/u/532108?u=2464983638afea8caf4cd9f0e4a7bc3e6a63bb0a&amp;amp;v=4" alt="Dmitry Spikhalsky" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;img src="https://c8.patreon.com/4/200/88567307/E" alt="el Chavo" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/117569999/55f75c57f95343e58402529cec852b26/eyJ3IjoyMDB9/1.jpeg?token-hash=squblHZH4-eMs3gI46Uqu1oTOK9sQ-0gcsFdZcB9xQg%3D" alt="James Thompson" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/28533016/e8f6044ccfa7483f87eeaa01c894a773/eyJ3IjoyMDB9/2.png?token-hash=ak-h3JWB50hyenCavcs32AAPw6nNhmH2nBFKpdk5hvM%3D" alt="William Tatum" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="Gage Siuniak" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/11180426/J" alt="jarrett towe" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;a href="https://github.com/zappazack" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://avatars.githubusercontent.com/u/74406132?u=356e66c964f9ca4859b274ff6788aebd16e218d4&amp;amp;v=4" alt="zappazack" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/91298241/1b1e6d698cde4faaaae6fc4c2d95d257/eyJ3IjoyMDB9/1.jpeg?token-hash=GCo7gAF_UUdJqz3FsCq8p1pq3AEoRAoC6YIvy5xEeZk%3D" alt="Daniel Partzsch" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;a href="https://www.youtube.com/@happyme7055" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://yt3.googleusercontent.com/ytc/AIdro_mFqhIRk99SoEWY2gvSvVp6u1SkCGMkRqYQ1OlBBeoOVp8=s160-c-k-c0x00ffffff-no-rj" alt="Marcus Rass" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/59408413/a0530a7770b6444bafdf0bc9f589eff0/eyJ3IjoyMDB9/1.jpg?token-hash=BlbxZsQpgchtqjByDuW9T8NoFWmCor5sWI0umhUKNlA%3D" alt="ByteC" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/55160464/42d4719ba0834e5d83aa989c04e762da/eyJ3IjoyMDB9/1.jpeg?token-hash=_twZUkW3NREIxGUOWskUdvuZQGEcRv9XMfu5NrnCe5M%3D" alt="Chris Canterbury" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/7208949/D" alt="D G" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/33866796/7fd2a214fd5c4062b0dd63a29f8de5bd/eyJ3IjoyMDB9/1.png?token-hash=8s-7yi8GawIlqr0FCTk5JWKy26acMiYlOD8LAk2HqqU%3D" alt="James" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/84891403/83682a2a2d3b49ba9d28e7221edd5752/eyJ3IjoyMDB9/1.jpeg?token-hash=LVB6lta4BonhfPwSUnZIDmSW3IU-eEO4sXD7NSK367g%3D" alt="Koray Birand" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/63232055/2300b4ab370341b5b476902c9b8218ee/eyJ3IjoyMDB9/1.png?token-hash=R9Nb4O0aLBRwxT1cGHUMThlvf6A2MD5SO88lpZBdH7M%3D" alt="Marek P" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/902918/5344727418634dc7b7fe7709d515a1d9/eyJ3IjoyMDB9/2.jpg?token-hash=myqV_oclkicVk9BDrvTO50jyjxJJGZ8i7oVJHwc05to%3D" alt="Michael Carychao" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/179944/P" alt="Paul Kroll" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/31613309/434500d03f714dc18049306ed3f0165c/eyJ3IjoyMDB9/1.jpg?token-hash=acILbq09wxUfJe-G2nMYUYkvHJ88ZxkzU4JebRPw2P0%3D" alt="Theta Graphics" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/10876902/T" alt="Tyssel" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/137975346/b0ac50eb2432471897ce59ddf1cb6b3d/eyJ3IjoyMDB9/1.png?token-hash=6iqhqukfgHK2IjlwTMsmBj3vratcfJ9pmxCmRkBu22s%3D" alt="Göran Burlin" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/84873332/H" alt="Htango2" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="Gary Joseph" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="keonmin lee" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="yvggeniy romanskiy" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/89623281/28d0cb75fc68439d9491f4343966f56e/eyJ3IjoyMDB9/1.jpeg?token-hash=Zt5UxtzvxDJGTPVh5Yr5rTY8JrcDsni0Mi89nZuYrp4%3D" alt="michele carlone" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/48109692/4237f732212343448ee87f5badc26e2c/eyJ3IjoyMDB9/1.jpeg?token-hash=gGqrOyctiITIyPZgjmF6YQKNf6cS9OeY4waIav3OAiU%3D" alt="Yves Poezevara" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/88656169/dd8943d7421d41bb9a8eb99f6d1279da/eyJ3IjoyMDB9/1.jpeg?token-hash=wT5j273p5pV10l81yR6kYdfYHR_yQ81xUzr3OfcSf7s%3D" alt="Ame Ame" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/5155933/C" alt="Chris Dermody" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/63920575/D" alt="Dutchman5oh" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/27580949/97c7dd2456a34c71b6429612a9e20462/eyJ3IjoyMDB9/1.jpeg?token-hash=cASxwWk8joAXx4tUAHch5CvTiYBR2UOHMeJK6se5fl0%3D" alt="Gergely Madácsi" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/44200812/f84fd628abb243bbaded4203761aca29/eyJ3IjoyMDB9/1.png?token-hash=ArthznCCT4BqOSMj_9oP4ECWWHnrb8nYPUDZ6DqSvMU%3D" alt="kingroka" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;a href="https://github.com/mertguvencli" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://avatars.githubusercontent.com/u/29762151?u=16a906d90df96c8cff9ea131a575c4bc171b1523&amp;amp;v=4" alt="Mert Guvencli" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/174319926/f16dc35b5c4741bd9c79fac3a8c8044d/eyJ3IjoyMDB9/1.jpeg?token-hash=GvYgc-XaRGI8BPnoMOo_txDfW0BjVayFdcxkshPyrvg%3D" alt="Philip Ring" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;a href="https://github.com/rickrender" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://avatars.githubusercontent.com/u/121735855?u=a8187fe40cec7f3afdd7c4bb128e0cca500fc220&amp;amp;v=4" alt="renderartist" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/27667925/6dac043a087e4c498e842dfad193baae/eyJ3IjoyMDB9/1.jpeg?token-hash=0bSVQo7QMMdGxFazeM099gsR0wtf28_ZTXeLIHEbIVk%3D" alt="S.Hasan Rizvi" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/2986571/S" alt="stev " width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/2472633/fea4a2888ea74c029e282fcc7ba76dd0/eyJ3IjoyMDB9/1.jpeg?token-hash=9O0lv1GQqftKoo8my9NrWSrRzHu-3IT_6VpCjHYixL8%3D" alt="Teemu Berglund" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="Joakim Sällström" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/2888571/65c717bd8a564e469c25aa5858f9821b/eyJ3IjoyMDB9/1.png?token-hash=zwMOgNEoC9hlr2KamiB7TG004gCfJ2exSRDO4dhxo5Q%3D" alt="Derrick Schultz" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/5233761/N" alt="Newtown " width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/7979776/P" alt="PizzaOrNot " width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/82707622/3f0de2ffd6eb4074ba91e81381146e1c/eyJ3IjoyMDB9/1.jpeg?token-hash=wk6wjILO2dDHJla7gn3MH9mEKl08e7PuBDwZRUtEQAw%3D" alt="Russell Norris" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;a href="https://github.com/lirexxx" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://avatars.githubusercontent.com/u/94787562?u=ed7e681cbc200269a081c4151d6adfa6ef728f85&amp;amp;v=4" alt="Dimitar A." width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="Heikki Rinkinen" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="Josh Lindo" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="StrictLine e.U." width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="The Rope Dude" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="Till Meyer" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="Valarm, LLC" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="Valarm, LLC" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="Xavier Climent" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/94453070/S" alt="Speedy2023" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/151413472/1f97b80a64fd4aa69412c065246eb83d/eyJ3IjoyMDB9/1.jpeg?token-hash=nfsls-Qt-4JatAmeloyK6SRuJgXfpCf1nxBkTK7QiI0%3D" alt="Richard Spain" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/14029622/d81798dedfad4bff8b2c76e55c7b695f/eyJ3IjoyMDB9/1.png?token-hash=1qP6r5SXiAjLEly9PMFbWMo7Bl9lPahSisCSsMqYNOw%3D" alt="A1PHA " width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/136770679/bfc06edc256e4e8c9d5e69669400ce80/eyJ3IjoyMDB9/1.png?token-hash=syeNGY9CgVD1D6v_EPNGafyTrzeXH_JMF3EAFyFJhvw%3D" alt="Ben May | sofsy" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;a href="https://github.com/claygraffix" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://avatars.githubusercontent.com/u/1283083?v=4" alt="claygraffix" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;img src="https://c8.patreon.com/4/200/22711368/C" alt="CrypticWit" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="David Hooper" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/7436837/K" alt="Ken Finlayson" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/14767188/1f22bccbf86b45a2b32642c3f5a493b3/eyJ3IjoyMDB9/1.png?token-hash=cJhOEsMXSv_d5fcqCu8Q_idyYtqc4UocsOaTflsSmT8%3D" alt="Kukee" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/93681621/d638ff4a9e0a40a7bc2c24bae4d6f353/eyJ3IjoyMDB9/1.png?token-hash=AxFFly1YYJskPzdkaU_M5jgyb0kZijSxB1Yb2AbE9h0%3D" alt="Manuel2Santos" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/138787313/c809120005024afa959231fe8b253fd9/eyJ3IjoyMDB9/1.png?token-hash=O6x0kkR4uKBsg_OODFHjZqwAupVztiZEOiXYF_7yKxM%3D" alt="Metryman55" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="Rudolf Goertz" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/40431966/190f07a0828d4f8190539c518c7d3115/eyJ3IjoyMDB9/1.png?token-hash=co9yehrBdxsPSKQGDB-sGQNB_g3HPfg4pckMXoCU4Ck%3D" alt="ShadowForge" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="Tommy Falkowski" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="Victor-Ray Valdez" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;a href="https://github.com/caleboleary" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://avatars.githubusercontent.com/u/12816579?u=d7f6ec4b7caf3c4535385a5fa3d7c155057ef664&amp;amp;v=4" alt="Caleb O'Leary" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="Florian Fiegl" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="Karol Stępień" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="manuel landron" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="Paul Vu Nguyen" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/160878212/1c34e798b2a9420991bab7ccc0067463/eyJ3IjoyMDB9/1.jpeg?token-hash=V_omrlVIeWKw0vovf92DJHSft-fXiksP0Fqa-qbAUxM%3D" alt="Danwich Gaming" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/5752417/G" alt="Guillaume Roy" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/154134231/5d307160968b4c29922e2729bb555c99/eyJ3IjoyMDB9/1.jpeg?token-hash=dNP94e42G_A9CHO5zYfUunS2K80y3BPDHQ3NdzphNRY%3D" alt="Colin Boyd" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/122373805/d0d995f2a7d6483cbbe0e9b14391d1ed/eyJ3IjoyMDB9/1.png?token-hash=oQCZooskREZOB36TW0KNZASDeLc88yswNzF-PqcVQyw%3D" alt="DavidO" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/2697420/C" alt="Craig Penn" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/45804549/8117b86a8c4145348ed392d3ea8c9dde/eyJ3IjoyMDB9/2.png?token-hash=ej_ln6ecs0-Cija3vrXaWYFFyWEK2TWmItJE5ALWP4s%3D" alt="Jadev1311" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/194433979/a18cf671feef435c9a93080f11cc8cf3/eyJ3IjoyMDB9/1.png?token-hash=TN6zMy2-V1Wg5uSpZHstYAZAdb_DYk9Erk3XDjE8--M%3D" alt="Cyril Diagne" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/29010107/37b05d32281f460baa28b4a2d5f8dd52/eyJ3IjoyMDB9/3.jpg?token-hash=5FngEN5rK-hCAgHUM0EybhMTuHwRZI1gbbZyntuuH6g%3D" alt="Adel Gamal" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/15407925/B" alt="Brian M" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/17697321/C" alt="Chris Day" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/73708729/52866102958248c19e646b6b62c7c51a/eyJ3IjoyMDB9/1.png?token-hash=S_haqcc-5zBK1tefXbphLzvA-MGtmstPNlaHch3k4zo%3D" alt="Cora Nox" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/5103404/7dd6a7be5dd640038c426c61419a5aad/eyJ3IjoyMDB9/2.jpg?token-hash=7ref-eq7sSeODxCWYcfh-wrQkhnS8L4ujGIWjlV8HdE%3D" alt="Corey Corza" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/12844508/fd08528fbed74a359acb1f8d06181c0c/eyJ3IjoyMDB9/1.jpeg?token-hash=TNDGh5TSWmlteKxsvB6FLE9wwawPMyvNBaim2U2KRC4%3D" alt="Dave Talbott" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/49455/F" alt="freke70 " width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/58082790/5f425b9f949047f78d9ae98e86faad35/eyJ3IjoyMDB9/1.png?token-hash=WYfg_M7cLsY-crrv71jcy6LLV77bB0_uD2_aw2f9nJ0%3D" alt="Greg Lemons" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/195837329/a136ba74b4d94df3a2b37e944beb6b9d/eyJ3IjoyMDB9/1.png?token-hash=oAIpcAmkts3GjjTjJVg2QrYs4UdcXgbW8q11p4kjVqQ%3D" alt="Greg Richards" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/100521338/J" alt="Jayson King" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/12128150/J" alt="Joshua Genke" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/75353/cff7a01bb97a45bba9023f1ff4a5f07a/eyJ3IjoyMDB9/1.jpeg?token-hash=3TxvQTWQSYWeqK4Elb6lX9y5ts21jh5jsWa1cXykcG8%3D" alt="Kenneth Loebenberg" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/31096978/f36222d290d2438cba8cfa3de63453c9/eyJ3IjoyMDB9/1.JPG?token-hash=0gwLI-GVquqxBj3FRR4XqJuRonvT5FsN5rdND2jApL0%3D" alt="Le_Fourbe " width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/97609519/M" alt="Mollie" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/683426/N" alt="Noodles" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c8.patreon.com/4/200/4544036/O" alt="Osman Bayazit" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/106121692/060eb9f09ecc4dceb7fa0a6d3c330b85/eyJ3IjoyMDB9/1.jpeg?token-hash=K6vA5Foyh9tAy3yzCtuYKDRF9McrCbQaEUC61x2x1Ic%3D" alt="Pablo Fonseca" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/188726649/6db3706d63f14468a58535ae5fd1344c/eyJ3IjoyMDB9/1.png?token-hash=QzCqu543VaxIuxyXo_1qrYqBQAyOhprcfNfNSIN3TYk%3D" alt="Phil Ring" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;a href="https://github.com/ProPatte" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://avatars.githubusercontent.com/u/228614493?u=45908a4a76165a83ce0b20a474a4d7fd027d67af&amp;amp;v=4" alt="ProPatte" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/196027905/1d46e7eeaf7e45229bbbf0f64683f337/eyJ3IjoyMDB9/1.jpeg?token-hash=JkpA_7525wqMEeVAFU6Qb7AK4lrAnjtisI7i4U8NrwI%3D" alt="qassem benhayoun" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/25199293/e967e5c4ed884f07b705271e253fd584/eyJ3IjoyMDB9/1.png?token-hash=uupXPicJ3Glks9mm5WDriIb1PBUbRmoVgSR6vcMPjlY%3D" alt="Rob Stevens" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/2622685/bddc4b42c82c47d8b30b05c000b8127b/eyJ3IjoyMDB9/1.jpg?token-hash=4tEFL9DP2L5dpg7rxUcFBlw27qnHO2ceyG38RtI9_Hg%3D" alt="Saftle " width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/45125613/8a45d1081bfc43b0bf4cb523558cab65/eyJ3IjoyMDB9/3.jpeg?token-hash=iUZhvndnfAiT97FacklmB4XvnMxj0pvepaHsU7JBxLg%3D" alt="Tiny Tsuruta" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/7408850/e90af02547724fc59ca1f21565df93b1/eyJ3IjoyMDB9/1.png?token-hash=RnqIUjFVhT7ZpV79q8cgTxCULkVfyQxpWNy4yIQIhlk%3D" alt="Virtamouse" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/107652364/5cae258ff5cd4c9a8e104861e63d5180/eyJ3IjoyMDB9/1.png?token-hash=qkRK53prBXDFG4b_Opnb80wcvWj6q0FjgNqPoSz24yU%3D" alt="Yi Chen" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="Boris HANSSEN" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="Juan Franco" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;a href="https://github.com/marksverdhei" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://avatars.githubusercontent.com/u/46672778?u=d1ba8b17516e6ecf1cd55ca4db2b770f82285aad&amp;amp;v=4" alt="Markus / Mark" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt;&lt;/a&gt; &lt;img src="https://c10.patreonusercontent.com/4/patreon-media/p/user/134029856/d1c895bf165149f69ad81ac426e617e9/eyJ3IjoyMDB9/1.jpeg?token-hash=FPzyMI3pAjnZmRlH_nmy2baIRcGKtQrDnN6aMCOHVwo%3D" alt="v33ts" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;img src="https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg" alt="Fabrizio Pasqualicchio" width="60" height="60" style="border-radius:8px;margin:5px;display: inline-block;" /&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;Requirements:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;python &amp;gt;3.10&lt;/li&gt; 
 &lt;li&gt;Nvidia GPU with enough ram to do what you need&lt;/li&gt; 
 &lt;li&gt;python venv&lt;/li&gt; 
 &lt;li&gt;git&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Linux:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/ostris/ai-toolkit.git
cd ai-toolkit
python3 -m venv venv
source venv/bin/activate
# install torch first
pip3 install --no-cache-dir torch==2.7.0 torchvision==0.22.0 torchaudio==2.7.0 --index-url https://download.pytorch.org/whl/cu126
pip3 install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For devices running &lt;strong&gt;DGX OS&lt;/strong&gt; (including DGX Spark), follow &lt;a href="https://raw.githubusercontent.com/ostris/ai-toolkit/main/dgx_instructions.md"&gt;these&lt;/a&gt; instructions.&lt;/p&gt; 
&lt;p&gt;Windows:&lt;/p&gt; 
&lt;p&gt;If you are having issues with Windows. I recommend using the easy install script at &lt;a href="https://github.com/Tavris1/AI-Toolkit-Easy-Install"&gt;https://github.com/Tavris1/AI-Toolkit-Easy-Install&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/ostris/ai-toolkit.git
cd ai-toolkit
python -m venv venv
.\venv\Scripts\activate
pip install --no-cache-dir torch==2.7.0 torchvision==0.22.0 torchaudio==2.7.0 --index-url https://download.pytorch.org/whl/cu126
pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;AI Toolkit UI&lt;/h1&gt; 
&lt;img src="https://ostris.com/wp-content/uploads/2025/02/toolkit-ui.jpg" alt="AI Toolkit UI" width="100%" /&gt; 
&lt;p&gt;The AI Toolkit UI is a web interface for the AI Toolkit. It allows you to easily start, stop, and monitor jobs. It also allows you to easily train models with a few clicks. It also allows you to set a token for the UI to prevent unauthorized access so it is mostly safe to run on an exposed server.&lt;/p&gt; 
&lt;h2&gt;Running the UI&lt;/h2&gt; 
&lt;p&gt;Requirements:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Node.js &amp;gt; 18&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The UI does not need to be kept running for the jobs to run. It is only needed to start/stop/monitor jobs. The commands below will install / update the UI and it's dependencies and start the UI.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd ui
npm run build_and_start
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can now access the UI at &lt;code&gt;http://localhost:8675&lt;/code&gt; or &lt;code&gt;http://&amp;lt;your-ip&amp;gt;:8675&lt;/code&gt; if you are running it on a server.&lt;/p&gt; 
&lt;h2&gt;Securing the UI&lt;/h2&gt; 
&lt;p&gt;If you are hosting the UI on a cloud provider or any network that is not secure, I highly recommend securing it with an auth token. You can do this by setting the environment variable &lt;code&gt;AI_TOOLKIT_AUTH&lt;/code&gt; to super secure password. This token will be required to access the UI. You can set this when starting the UI like so:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Linux
AI_TOOLKIT_AUTH=super_secure_password npm run build_and_start

# Windows
set AI_TOOLKIT_AUTH=super_secure_password &amp;amp;&amp;amp; npm run build_and_start

# Windows Powershell
$env:AI_TOOLKIT_AUTH="super_secure_password"; npm run build_and_start
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;FLUX.1 Training&lt;/h2&gt; 
&lt;h3&gt;Tutorial&lt;/h3&gt; 
&lt;p&gt;To get started quickly, check out &lt;a href="https://x.com/araminta_k"&gt;@araminta_k&lt;/a&gt; tutorial on &lt;a href="https://www.youtube.com/watch?v=HzGW_Kyermg"&gt;Finetuning Flux Dev on a 3090&lt;/a&gt; with 24GB VRAM.&lt;/p&gt; 
&lt;h3&gt;Requirements&lt;/h3&gt; 
&lt;p&gt;You currently need a GPU with &lt;strong&gt;at least 24GB of VRAM&lt;/strong&gt; to train FLUX.1. If you are using it as your GPU to control your monitors, you probably need to set the flag &lt;code&gt;low_vram: true&lt;/code&gt; in the config file under &lt;code&gt;model:&lt;/code&gt;. This will quantize the model on CPU and should allow it to train with monitors attached. Users have gotten it to work on Windows with WSL, but there are some reports of a bug when running on windows natively. I have only tested on linux for now. This is still extremely experimental and a lot of quantizing and tricks had to happen to get it to fit on 24GB at all.&lt;/p&gt; 
&lt;h3&gt;FLUX.1-dev&lt;/h3&gt; 
&lt;p&gt;FLUX.1-dev has a non-commercial license. Which means anything you train will inherit the non-commercial license. It is also a gated model, so you need to accept the license on HF before using it. Otherwise, this will fail. Here are the required steps to setup a license.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Sign into HF and accept the model access here &lt;a href="https://huggingface.co/black-forest-labs/FLUX.1-dev"&gt;black-forest-labs/FLUX.1-dev&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Make a file named &lt;code&gt;.env&lt;/code&gt; in the root on this folder&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/settings/tokens/new?"&gt;Get a READ key from huggingface&lt;/a&gt; and add it to the &lt;code&gt;.env&lt;/code&gt; file like so &lt;code&gt;HF_TOKEN=your_key_here&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;FLUX.1-schnell&lt;/h3&gt; 
&lt;p&gt;FLUX.1-schnell is Apache 2.0. Anything trained on it can be licensed however you want and it does not require a HF_TOKEN to train. However, it does require a special adapter to train with it, &lt;a href="https://huggingface.co/ostris/FLUX.1-schnell-training-adapter"&gt;ostris/FLUX.1-schnell-training-adapter&lt;/a&gt;. It is also highly experimental. For best overall quality, training on FLUX.1-dev is recommended.&lt;/p&gt; 
&lt;p&gt;To use it, You just need to add the assistant to the &lt;code&gt;model&lt;/code&gt; section of your config file like so:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;      model:
        name_or_path: "black-forest-labs/FLUX.1-schnell"
        assistant_lora_path: "ostris/FLUX.1-schnell-training-adapter"
        is_flux: true
        quantize: true
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You also need to adjust your sample steps since schnell does not require as many&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;      sample:
        guidance_scale: 1  # schnell does not do guidance
        sample_steps: 4  # 1 - 4 works well
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Training&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Copy the example config file located at &lt;code&gt;config/examples/train_lora_flux_24gb.yaml&lt;/code&gt; (&lt;code&gt;config/examples/train_lora_flux_schnell_24gb.yaml&lt;/code&gt; for schnell) to the &lt;code&gt;config&lt;/code&gt; folder and rename it to &lt;code&gt;whatever_you_want.yml&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Edit the file following the comments in the file&lt;/li&gt; 
 &lt;li&gt;Run the file like so &lt;code&gt;python run.py config/whatever_you_want.yml&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;A folder with the name and the training folder from the config file will be created when you start. It will have all checkpoints and images in it. You can stop the training at any time using ctrl+c and when you resume, it will pick back up from the last checkpoint.&lt;/p&gt; 
&lt;p&gt;IMPORTANT. If you press crtl+c while it is saving, it will likely corrupt that checkpoint. So wait until it is done saving&lt;/p&gt; 
&lt;h3&gt;Need help?&lt;/h3&gt; 
&lt;p&gt;Please do not open a bug report unless it is a bug in the code. You are welcome to &lt;a href="https://discord.gg/VXmU2f5WEU"&gt;Join my Discord&lt;/a&gt; and ask for help there. However, please refrain from PMing me directly with general question or support. Ask in the discord and I will answer when I can.&lt;/p&gt; 
&lt;h2&gt;Gradio UI&lt;/h2&gt; 
&lt;p&gt;To get started training locally with a with a custom UI, once you followed the steps above and &lt;code&gt;ai-toolkit&lt;/code&gt; is installed:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd ai-toolkit #in case you are not yet in the ai-toolkit folder
huggingface-cli login #provide a `write` token to publish your LoRA at the end
python flux_train_ui.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You will instantiate a UI that will let you upload your images, caption them, train and publish your LoRA &lt;img src="https://raw.githubusercontent.com/ostris/ai-toolkit/main/assets/lora_ease_ui.png" alt="image" /&gt;&lt;/p&gt; 
&lt;h2&gt;Training in RunPod&lt;/h2&gt; 
&lt;p&gt;If you would like to use Runpod, but have not signed up yet, please consider using &lt;a href="https://runpod.io?ref=h0y9jyr2"&gt;my Runpod affiliate link&lt;/a&gt; to help support this project.&lt;/p&gt; 
&lt;p&gt;I maintain an official Runpod Pod template here which can be accessed &lt;a href="https://console.runpod.io/deploy?template=0fqzfjy6f3&amp;amp;ref=h0y9jyr2"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;I have also created a short video showing how to get started using AI Toolkit with Runpod &lt;a href="https://youtu.be/HBNeS-F6Zz8"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Training in Modal&lt;/h2&gt; 
&lt;h3&gt;1. Setup&lt;/h3&gt; 
&lt;h4&gt;ai-toolkit:&lt;/h4&gt; 
&lt;pre&gt;&lt;code&gt;git clone https://github.com/ostris/ai-toolkit.git
cd ai-toolkit
git submodule update --init --recursive
python -m venv venv
source venv/bin/activate
pip install torch
pip install -r requirements.txt
pip install --upgrade accelerate transformers diffusers huggingface_hub #Optional, run it if you run into issues
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Modal:&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Run &lt;code&gt;pip install modal&lt;/code&gt; to install the modal Python package.&lt;/li&gt; 
 &lt;li&gt;Run &lt;code&gt;modal setup&lt;/code&gt; to authenticate (if this doesn’t work, try &lt;code&gt;python -m modal setup&lt;/code&gt;).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Hugging Face:&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Get a READ token from &lt;a href="https://huggingface.co/settings/tokens"&gt;here&lt;/a&gt; and request access to Flux.1-dev model from &lt;a href="https://huggingface.co/black-forest-labs/FLUX.1-dev"&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Run &lt;code&gt;huggingface-cli login&lt;/code&gt; and paste your token.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;2. Upload your dataset&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Drag and drop your dataset folder containing the .jpg, .jpeg, or .png images and .txt files in &lt;code&gt;ai-toolkit&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;3. Configs&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Copy an example config file located at &lt;code&gt;config/examples/modal&lt;/code&gt; to the &lt;code&gt;config&lt;/code&gt; folder and rename it to &lt;code&gt;whatever_you_want.yml&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Edit the config following the comments in the file, &lt;strong&gt;&lt;ins&gt;be careful and follow the example &lt;code&gt;/root/ai-toolkit&lt;/code&gt; paths&lt;/ins&gt;&lt;/strong&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;4. Edit run_modal.py&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Set your entire local &lt;code&gt;ai-toolkit&lt;/code&gt; path at &lt;code&gt;code_mount = modal.Mount.from_local_dir&lt;/code&gt; like:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;code_mount = modal.Mount.from_local_dir("/Users/username/ai-toolkit", remote_path="/root/ai-toolkit")
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Choose a &lt;code&gt;GPU&lt;/code&gt; and &lt;code&gt;Timeout&lt;/code&gt; in &lt;code&gt;@app.function&lt;/code&gt; &lt;em&gt;(default is A100 40GB and 2 hour timeout)&lt;/em&gt;.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;5. Training&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Run the config file in your terminal: &lt;code&gt;modal run run_modal.py --config-file-list-str=/root/ai-toolkit/config/whatever_you_want.yml&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;You can monitor your training in your local terminal, or on &lt;a href="https://modal.com/"&gt;modal.com&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Models, samples and optimizer will be stored in &lt;code&gt;Storage &amp;gt; flux-lora-models&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;6. Saving the model&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Check contents of the volume by running &lt;code&gt;modal volume ls flux-lora-models&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Download the content by running &lt;code&gt;modal volume get flux-lora-models your-model-name&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Example: &lt;code&gt;modal volume get flux-lora-models my_first_flux_lora_v1&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Screenshot from Modal&lt;/h3&gt; 
&lt;img width="1728" alt="Modal Traning Screenshot" src="https://github.com/user-attachments/assets/7497eb38-0090-49d6-8ad9-9c8ea7b5388b" /&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Dataset Preparation&lt;/h2&gt; 
&lt;p&gt;Datasets generally need to be a folder containing images and associated text files. Currently, the only supported formats are jpg, jpeg, and png. Webp currently has issues. The text files should be named the same as the images but with a &lt;code&gt;.txt&lt;/code&gt; extension. For example &lt;code&gt;image2.jpg&lt;/code&gt; and &lt;code&gt;image2.txt&lt;/code&gt;. The text file should contain only the caption. You can add the word &lt;code&gt;[trigger]&lt;/code&gt; in the caption file and if you have &lt;code&gt;trigger_word&lt;/code&gt; in your config, it will be automatically replaced.&lt;/p&gt; 
&lt;p&gt;Images are never upscaled but they are downscaled and placed in buckets for batching. &lt;strong&gt;You do not need to crop/resize your images&lt;/strong&gt;. The loader will automatically resize them and can handle varying aspect ratios.&lt;/p&gt; 
&lt;h2&gt;Training Specific Layers&lt;/h2&gt; 
&lt;p&gt;To train specific layers with LoRA, you can use the &lt;code&gt;only_if_contains&lt;/code&gt; network kwargs. For instance, if you want to train only the 2 layers used by The Last Ben, &lt;a href="https://x.com/__TheBen/status/1829554120270987740"&gt;mentioned in this post&lt;/a&gt;, you can adjust your network kwargs like so:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;      network:
        type: "lora"
        linear: 128
        linear_alpha: 128
        network_kwargs:
          only_if_contains:
            - "transformer.single_transformer_blocks.7.proj_out"
            - "transformer.single_transformer_blocks.20.proj_out"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The naming conventions of the layers are in diffusers format, so checking the state dict of a model will reveal the suffix of the name of the layers you want to train. You can also use this method to only train specific groups of weights. For instance to only train the &lt;code&gt;single_transformer&lt;/code&gt; for FLUX.1, you can use the following:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;      network:
        type: "lora"
        linear: 128
        linear_alpha: 128
        network_kwargs:
          only_if_contains:
            - "transformer.single_transformer_blocks."
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also exclude layers by their names by using &lt;code&gt;ignore_if_contains&lt;/code&gt; network kwarg. So to exclude all the single transformer blocks,&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;      network:
        type: "lora"
        linear: 128
        linear_alpha: 128
        network_kwargs:
          ignore_if_contains:
            - "transformer.single_transformer_blocks."
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;code&gt;ignore_if_contains&lt;/code&gt; takes priority over &lt;code&gt;only_if_contains&lt;/code&gt;. So if a weight is covered by both, if will be ignored.&lt;/p&gt; 
&lt;h2&gt;LoKr Training&lt;/h2&gt; 
&lt;p&gt;To learn more about LoKr, read more about it at &lt;a href="https://github.com/KohakuBlueleaf/LyCORIS/raw/main/docs/Guidelines.md"&gt;KohakuBlueleaf/LyCORIS&lt;/a&gt;. To train a LoKr model, you can adjust the network type in the config file like so:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;      network:
        type: "lokr"
        lokr_full_rank: true
        lokr_factor: 8
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Everything else should work the same including layer targeting.&lt;/p&gt; 
&lt;h2&gt;Updates&lt;/h2&gt; 
&lt;p&gt;Only larger updates are listed here. There are usually smaller daily updated that are omitted.&lt;/p&gt; 
&lt;h3&gt;Jul 17, 2025&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Make it easy to add control images to the samples in the ui&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Jul 11, 2025&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Added better video config settings to the UI for video models.&lt;/li&gt; 
 &lt;li&gt;Added Wan I2V training to the UI&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;June 29, 2025&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Fixed issue where Kontext forced sizes on sampling&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;June 26, 2025&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Added support for FLUX.1 Kontext training&lt;/li&gt; 
 &lt;li&gt;added support for instruction dataset training&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;June 25, 2025&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Added support for OmniGen2 training&lt;/li&gt; 
 &lt;li&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;June 17, 2025&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Performance optimizations for batch preparation&lt;/li&gt; 
 &lt;li&gt;Added some docs via a popup for items in the simple ui explaining what settings do. Still a WIP&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;June 16, 2025&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Hide control images in the UI when viewing datasets&lt;/li&gt; 
 &lt;li&gt;WIP on mean flow loss&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;June 12, 2025&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Fixed issue that resulted in blank captions in the dataloader&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;June 10, 2025&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Decided to keep track up updates in the readme&lt;/li&gt; 
 &lt;li&gt;Added support for SDXL in the UI&lt;/li&gt; 
 &lt;li&gt;Added support for SD 1.5 in the UI&lt;/li&gt; 
 &lt;li&gt;Fixed UI Wan 2.1 14b name bug&lt;/li&gt; 
 &lt;li&gt;Added support for for conv training in the UI for models that support it&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>sansan0/TrendRadar</title>
      <link>https://github.com/sansan0/TrendRadar</link>
      <description>&lt;p&gt;🎯 告别信息过载，AI 助你看懂新闻资讯热点，简单的舆情监控分析 - 多平台热点聚合+基于 MCP 的AI分析工具。监控35个平台（抖音、知乎、B站、华尔街见闻、财联社等），智能筛选+自动推送+AI对话分析（用自然语言深度挖掘新闻：趋势追踪、情感分析、相似检索等13种工具）。支持企业微信/个人微信/飞书/钉钉/Telegram/邮件/ntfy/bark/slack 推送，1分钟手机通知，无需编程。支持Docker部署，支持数据远程云存储⭐ 让算法为你服务，用AI理解热点&lt;/p&gt;&lt;hr&gt;&lt;div align="center" id="trendradar"&gt; 
 &lt;a href="https://github.com/sansan0/TrendRadar" title="TrendRadar"&gt; &lt;img src="https://raw.githubusercontent.com/sansan0/TrendRadar/master/_image/banner.webp" alt="TrendRadar Banner" width="80%" /&gt; &lt;/a&gt; 
 &lt;p&gt;🚀 最快&lt;strong&gt;30秒&lt;/strong&gt;部署的热点助手 —— 告别无效刷屏，只看真正关心的新闻资讯&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/14726" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/14726" alt="sansan0%2FTrendRadar | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://shandianshuo.cn" target="_blank" title="AI 语音输入，比打字快 4 倍 ⚡"&gt;&lt;img src="https://raw.githubusercontent.com/sansan0/TrendRadar/master/_image/shandianshuo.png" alt="闪电说 logo" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/sansan0/TrendRadar/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/sansan0/TrendRadar?style=flat-square&amp;amp;logo=github&amp;amp;color=yellow" alt="GitHub Stars" /&gt;&lt;/a&gt; &lt;a href="https://github.com/sansan0/TrendRadar/network/members"&gt;&lt;img src="https://img.shields.io/github/forks/sansan0/TrendRadar?style=flat-square&amp;amp;logo=github&amp;amp;color=blue" alt="GitHub Forks" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/license-GPL--3.0-blue.svg?style=flat-square" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://github.com/sansan0/TrendRadar"&gt;&lt;img src="https://img.shields.io/badge/version-v4.0.3-blue.svg?sanitize=true" alt="Version" /&gt;&lt;/a&gt; &lt;a href="https://github.com/sansan0/TrendRadar"&gt;&lt;img src="https://img.shields.io/badge/MCP-v1.1.0-green.svg?sanitize=true" alt="MCP" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://work.weixin.qq.com/"&gt;&lt;img src="https://img.shields.io/badge/%E4%BC%81%E4%B8%9A%E5%BE%AE%E4%BF%A1-%E9%80%9A%E7%9F%A5-00D4AA?style=flat-square" alt="企业微信通知" /&gt;&lt;/a&gt; &lt;a href="https://weixin.qq.com/"&gt;&lt;img src="https://img.shields.io/badge/%E4%B8%AA%E4%BA%BA%E5%BE%AE%E4%BF%A1-%E9%80%9A%E7%9F%A5-00D4AA?style=flat-square" alt="个人微信通知" /&gt;&lt;/a&gt; &lt;a href="https://telegram.org/"&gt;&lt;img src="https://img.shields.io/badge/Telegram-%E9%80%9A%E7%9F%A5-00D4AA?style=flat-square" alt="Telegram通知" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#"&gt;&lt;img src="https://img.shields.io/badge/%E9%92%89%E9%92%89-%E9%80%9A%E7%9F%A5-00D4AA?style=flat-square" alt="dingtalk通知" /&gt;&lt;/a&gt; &lt;a href="https://www.feishu.cn/"&gt;&lt;img src="https://img.shields.io/badge/%E9%A3%9E%E4%B9%A6-%E9%80%9A%E7%9F%A5-00D4AA?style=flat-square" alt="飞书通知" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#"&gt;&lt;img src="https://img.shields.io/badge/Email-%E9%80%9A%E7%9F%A5-00D4AA?style=flat-square" alt="邮件通知" /&gt;&lt;/a&gt; &lt;a href="https://github.com/binwiederhier/ntfy"&gt;&lt;img src="https://img.shields.io/badge/ntfy-%E9%80%9A%E7%9F%A5-00D4AA?style=flat-square" alt="ntfy通知" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Finb/Bark"&gt;&lt;img src="https://img.shields.io/badge/Bark-%E9%80%9A%E7%9F%A5-00D4AA?style=flat-square" alt="Bark通知" /&gt;&lt;/a&gt; &lt;a href="https://slack.com/"&gt;&lt;img src="https://img.shields.io/badge/Slack-%E9%80%9A%E7%9F%A5-00D4AA?style=flat-square" alt="Slack通知" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/sansan0/TrendRadar"&gt;&lt;img src="https://img.shields.io/badge/GitHub_Actions-%E8%87%AA%E5%8A%A8%E5%8C%96-2088FF?style=flat-square&amp;amp;logo=github-actions&amp;amp;logoColor=white" alt="GitHub Actions" /&gt;&lt;/a&gt; &lt;a href="https://sansan0.github.io/TrendRadar"&gt;&lt;img src="https://img.shields.io/badge/GitHub_Pages-%E9%83%A8%E7%BD%B2-4285F4?style=flat-square&amp;amp;logo=github&amp;amp;logoColor=white" alt="GitHub Pages" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/wantcat/trendradar"&gt;&lt;img src="https://img.shields.io/badge/Docker-%E9%83%A8%E7%BD%B2-2496ED?style=flat-square&amp;amp;logo=docker&amp;amp;logoColor=white" alt="Docker" /&gt;&lt;/a&gt; &lt;a href="https://modelcontextprotocol.io/"&gt;&lt;img src="https://img.shields.io/badge/MCP-AI%E5%88%86%E6%9E%90%E6%94%AF%E6%8C%81-FF6B6B?style=flat-square&amp;amp;logo=ai&amp;amp;logoColor=white" alt="MCP Support" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;strong&gt;中文&lt;/strong&gt; | &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/README-EN.md"&gt;English&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;本项目以轻量，易部署为目标&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;br /&gt; 
&lt;details&gt; 
 &lt;summary&gt;🚨 &lt;strong&gt;【必读】重要公告：v4.0.0 部署方式与存储架构变更&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;h3&gt;🛠️ 请选择适合你的部署方式&lt;/h3&gt; 
 &lt;h4&gt;🅰️ 方案一：Docker 部署（推荐 🔥）&lt;/h4&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;特点&lt;/strong&gt;：最稳定、最简单，数据存储在 &lt;strong&gt;本地 SQLite&lt;/strong&gt;，完全自主可控。&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;适用&lt;/strong&gt;：有自己的服务器、NAS 或长期运行的电脑。&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;👉 &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#6-docker-%E9%83%A8%E7%BD%B2"&gt;跳转到 Docker 部署教程&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
 &lt;hr /&gt; 
 &lt;h4&gt;🅱️ 方案二：GitHub Actions 部署（已恢复 ✅）&lt;/h4&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;特点&lt;/strong&gt;：数据不再直接写入仓库（Git Commit），而是存储在 &lt;strong&gt;远程云存储&lt;/strong&gt;。&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;推荐&lt;/strong&gt;：配置一个远程云存储服务（Cloudflare R2、阿里云 OSS、腾讯云 COS 等）。&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;👉 &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#-%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B"&gt;点击查看详细配置教程&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;br /&gt; 
&lt;h2&gt;📑 快速导航&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;&lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#-%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B"&gt;🚀 快速开始&lt;/a&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#-ai-%E6%99%BA%E8%83%BD%E5%88%86%E6%9E%90"&gt;🤖 AI 智能分析&lt;/a&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#%E9%85%8D%E7%BD%AE%E8%AF%A6%E8%A7%A3"&gt;⚙️ 配置详解&lt;/a&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#-%E6%9B%B4%E6%96%B0%E6%97%A5%E5%BF%97"&gt;📝 更新日志&lt;/a&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#%E9%97%AE%E9%A2%98%E7%AD%94%E7%96%91%E4%B8%8E%E4%BA%A4%E6%B5%81"&gt;❓ 答疑与交流&lt;/a&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#6-docker-%E9%83%A8%E7%BD%B2"&gt;🐳 Docker部署&lt;/a&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#-mcp-%E5%AE%A2%E6%88%B7%E7%AB%AF"&gt;🔌 MCP客户端&lt;/a&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#-%E9%A1%B9%E7%9B%AE%E7%9B%B8%E5%85%B3"&gt;📚 项目相关&lt;/a&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#-%E8%B5%9E%E5%8A%A9%E5%95%86"&gt;🪄 赞助商&lt;/a&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;ul&gt; 
 &lt;li&gt;感谢&lt;strong&gt;耐心反馈 bug&lt;/strong&gt; 的贡献者，你们的每一条反馈让项目更加完善😉;&lt;/li&gt; 
 &lt;li&gt;感谢&lt;strong&gt;为项目点 star&lt;/strong&gt; 的观众们，&lt;strong&gt;fork&lt;/strong&gt; 你所欲也，&lt;strong&gt;star&lt;/strong&gt; 我所欲也，两者得兼😍是对开源精神最好的支持;&lt;/li&gt; 
 &lt;li&gt;感谢&lt;strong&gt;关注&lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#%E9%97%AE%E9%A2%98%E7%AD%94%E7%96%91%E4%B8%8E%E4%BA%A4%E6%B5%81"&gt;公众号&lt;/a&gt;&lt;/strong&gt; 的读者们，你们的留言、点赞、分享和推荐等积极互动让内容更有温度😎。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;👉 点击展开：&lt;strong&gt;致谢名单&lt;/strong&gt; (当前 &lt;strong&gt;🔥73🔥&lt;/strong&gt; 位)&lt;/summary&gt; 
 &lt;h3&gt;基础设施支持&lt;/h3&gt; 
 &lt;p&gt;感谢 &lt;strong&gt;GitHub&lt;/strong&gt; 免费提供的基础设施，这是本项目得以&lt;strong&gt;一键 fork&lt;/strong&gt;便捷运行的最大前提。&lt;/p&gt; 
 &lt;h3&gt;数据支持&lt;/h3&gt; 
 &lt;p&gt;本项目使用 &lt;a href="https://github.com/ourongxing/newsnow"&gt;newsnow&lt;/a&gt; 项目的 API 获取多平台数据，特别感谢作者提供的服务。&lt;/p&gt; 
 &lt;p&gt;经联系，作者表示无需担心服务器压力，但这是基于他的善意和信任。请大家：&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;前往 &lt;a href="https://github.com/ourongxing/newsnow"&gt;newsnow 项目&lt;/a&gt; 点 star 支持&lt;/strong&gt;&lt;/li&gt; 
  &lt;li&gt;Docker 部署时，请合理控制推送频率，勿竭泽而渔&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;推广助力&lt;/h3&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;感谢以下平台和个人的推荐(按时间排列)&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://mp.weixin.qq.com/s/fvutkJ_NPUelSW9OGK39aA"&gt;小众软件&lt;/a&gt; - 开源软件推荐平台&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://linux.do/"&gt;LinuxDo 社区&lt;/a&gt; - 技术爱好者的聚集地&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/ruanyf/weekly"&gt;阮一峰周刊&lt;/a&gt; - 技术圈有影响力的周刊&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;观众支持&lt;/h3&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;感谢&lt;strong&gt;给予资金支持&lt;/strong&gt;的朋友们，你们的慷慨已化身为键盘旁的零食饮料，陪伴着项目的每一次迭代。&lt;/p&gt; 
  &lt;p&gt;&lt;strong&gt;"一元点赞"已暂停&lt;/strong&gt;，如仍想支持作者，可前往&lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#%E9%97%AE%E9%A2%98%E7%AD%94%E7%96%91%E4%B8%8E%E4%BA%A4%E6%B5%81"&gt;公众号&lt;/a&gt;文章底部点击"喜欢作者"。&lt;/p&gt; 
  &lt;p&gt;一位可爱猫头像的朋友，不知你从哪个角落翻到了我的收款码，三连了 1.8，心意已收到，感谢厚爱&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;点赞人&lt;/th&gt; 
    &lt;th align="center"&gt;金额&lt;/th&gt; 
    &lt;th align="center"&gt;日期&lt;/th&gt; 
    &lt;th align="center"&gt;备注&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;D*5&lt;/td&gt; 
    &lt;td align="center"&gt;1.8 * 3&lt;/td&gt; 
    &lt;td align="center"&gt;2025.11.24&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*鬼&lt;/td&gt; 
    &lt;td align="center"&gt;1&lt;/td&gt; 
    &lt;td align="center"&gt;2025.11.17&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*超&lt;/td&gt; 
    &lt;td align="center"&gt;10&lt;/td&gt; 
    &lt;td align="center"&gt;2025.11.17&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;R*w&lt;/td&gt; 
    &lt;td align="center"&gt;10&lt;/td&gt; 
    &lt;td align="center"&gt;2025.11.17&lt;/td&gt; 
    &lt;td align="center"&gt;这 agent 做的牛逼啊,兄弟&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;J*o&lt;/td&gt; 
    &lt;td align="center"&gt;1&lt;/td&gt; 
    &lt;td align="center"&gt;2025.11.17&lt;/td&gt; 
    &lt;td align="center"&gt;感谢开源,祝大佬事业有成&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*晨&lt;/td&gt; 
    &lt;td align="center"&gt;8.88&lt;/td&gt; 
    &lt;td align="center"&gt;2025.11.16&lt;/td&gt; 
    &lt;td align="center"&gt;项目不错,研究学习中&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*海&lt;/td&gt; 
    &lt;td align="center"&gt;1&lt;/td&gt; 
    &lt;td align="center"&gt;2025.11.15&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*德&lt;/td&gt; 
    &lt;td align="center"&gt;1.99&lt;/td&gt; 
    &lt;td align="center"&gt;2025.11.15&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*疏&lt;/td&gt; 
    &lt;td align="center"&gt;8.8&lt;/td&gt; 
    &lt;td align="center"&gt;2025.11.14&lt;/td&gt; 
    &lt;td align="center"&gt;感谢开源，项目很棒，支持一下&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;M*e&lt;/td&gt; 
    &lt;td align="center"&gt;10&lt;/td&gt; 
    &lt;td align="center"&gt;2025.11.14&lt;/td&gt; 
    &lt;td align="center"&gt;开源不易，大佬辛苦了&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;**柯&lt;/td&gt; 
    &lt;td align="center"&gt;1&lt;/td&gt; 
    &lt;td align="center"&gt;2025.11.14&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*云&lt;/td&gt; 
    &lt;td align="center"&gt;88&lt;/td&gt; 
    &lt;td align="center"&gt;2025.11.13&lt;/td&gt; 
    &lt;td align="center"&gt;好项目，感谢开源&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*W&lt;/td&gt; 
    &lt;td align="center"&gt;6&lt;/td&gt; 
    &lt;td align="center"&gt;2025.11.13&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*凯&lt;/td&gt; 
    &lt;td align="center"&gt;1&lt;/td&gt; 
    &lt;td align="center"&gt;2025.11.13&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;对*.&lt;/td&gt; 
    &lt;td align="center"&gt;1&lt;/td&gt; 
    &lt;td align="center"&gt;2025.11.13&lt;/td&gt; 
    &lt;td align="center"&gt;Thanks for your TrendRadar&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;s*y&lt;/td&gt; 
    &lt;td align="center"&gt;1&lt;/td&gt; 
    &lt;td align="center"&gt;2025.11.13&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;**翔&lt;/td&gt; 
    &lt;td align="center"&gt;10&lt;/td&gt; 
    &lt;td align="center"&gt;2025.11.13&lt;/td&gt; 
    &lt;td align="center"&gt;好项目，相见恨晚，感谢开源！&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*韦&lt;/td&gt; 
    &lt;td align="center"&gt;9.9&lt;/td&gt; 
    &lt;td align="center"&gt;2025.11.13&lt;/td&gt; 
    &lt;td align="center"&gt;TrendRadar超赞，请老师喝咖啡~&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;h*p&lt;/td&gt; 
    &lt;td align="center"&gt;5&lt;/td&gt; 
    &lt;td align="center"&gt;2025.11.12&lt;/td&gt; 
    &lt;td align="center"&gt;支持中国开源力量，加油！&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;c*r&lt;/td&gt; 
    &lt;td align="center"&gt;6&lt;/td&gt; 
    &lt;td align="center"&gt;2025.11.12&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;a*n&lt;/td&gt; 
    &lt;td align="center"&gt;5&lt;/td&gt; 
    &lt;td align="center"&gt;2025.11.12&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;。*c&lt;/td&gt; 
    &lt;td align="center"&gt;1&lt;/td&gt; 
    &lt;td align="center"&gt;2025.11.12&lt;/td&gt; 
    &lt;td align="center"&gt;感谢开源分享&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*记&lt;/td&gt; 
    &lt;td align="center"&gt;1&lt;/td&gt; 
    &lt;td align="center"&gt;2025.11.11&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*主&lt;/td&gt; 
    &lt;td align="center"&gt;1&lt;/td&gt; 
    &lt;td align="center"&gt;2025.11.10&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*了&lt;/td&gt; 
    &lt;td align="center"&gt;10&lt;/td&gt; 
    &lt;td align="center"&gt;2025.11.09&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*杰&lt;/td&gt; 
    &lt;td align="center"&gt;5&lt;/td&gt; 
    &lt;td align="center"&gt;2025.11.08&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*点&lt;/td&gt; 
    &lt;td align="center"&gt;8.80&lt;/td&gt; 
    &lt;td align="center"&gt;2025.11.07&lt;/td&gt; 
    &lt;td align="center"&gt;开发不易，支持一下。&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;Q*Q&lt;/td&gt; 
    &lt;td align="center"&gt;6.66&lt;/td&gt; 
    &lt;td align="center"&gt;2025.11.07&lt;/td&gt; 
    &lt;td align="center"&gt;感谢开源！&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;C*e&lt;/td&gt; 
    &lt;td align="center"&gt;1&lt;/td&gt; 
    &lt;td align="center"&gt;2025.11.05&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;Peter Fan&lt;/td&gt; 
    &lt;td align="center"&gt;20&lt;/td&gt; 
    &lt;td align="center"&gt;2025.10.29&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;M*n&lt;/td&gt; 
    &lt;td align="center"&gt;1&lt;/td&gt; 
    &lt;td align="center"&gt;2025.10.27&lt;/td&gt; 
    &lt;td align="center"&gt;感谢开源&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*许&lt;/td&gt; 
    &lt;td align="center"&gt;8.88&lt;/td&gt; 
    &lt;td align="center"&gt;2025.10.23&lt;/td&gt; 
    &lt;td align="center"&gt;老师 小白一枚，摸了几天了还没整起来，求教&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;Eason&lt;/td&gt; 
    &lt;td align="center"&gt;1&lt;/td&gt; 
    &lt;td align="center"&gt;2025.10.22&lt;/td&gt; 
    &lt;td align="center"&gt;还没整明白，但你在做好事&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;P*n&lt;/td&gt; 
    &lt;td align="center"&gt;1&lt;/td&gt; 
    &lt;td align="center"&gt;2025.10.20&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*杰&lt;/td&gt; 
    &lt;td align="center"&gt;1&lt;/td&gt; 
    &lt;td align="center"&gt;2025.10.19&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*徐&lt;/td&gt; 
    &lt;td align="center"&gt;1&lt;/td&gt; 
    &lt;td align="center"&gt;2025.10.18&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*志&lt;/td&gt; 
    &lt;td align="center"&gt;1&lt;/td&gt; 
    &lt;td align="center"&gt;2025.10.17&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*😀&lt;/td&gt; 
    &lt;td align="center"&gt;10&lt;/td&gt; 
    &lt;td align="center"&gt;2025.10.16&lt;/td&gt; 
    &lt;td align="center"&gt;点赞&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;**杰&lt;/td&gt; 
    &lt;td align="center"&gt;10&lt;/td&gt; 
    &lt;td align="center"&gt;2025.10.16&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*啸&lt;/td&gt; 
    &lt;td align="center"&gt;10&lt;/td&gt; 
    &lt;td align="center"&gt;2025.10.16&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*纪&lt;/td&gt; 
    &lt;td align="center"&gt;5&lt;/td&gt; 
    &lt;td align="center"&gt;2025.10.14&lt;/td&gt; 
    &lt;td align="center"&gt;TrendRadar&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;J*d&lt;/td&gt; 
    &lt;td align="center"&gt;1&lt;/td&gt; 
    &lt;td align="center"&gt;2025.10.14&lt;/td&gt; 
    &lt;td align="center"&gt;谢谢你的工具，很好玩...&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*H&lt;/td&gt; 
    &lt;td align="center"&gt;1&lt;/td&gt; 
    &lt;td align="center"&gt;2025.10.14&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;那*O&lt;/td&gt; 
    &lt;td align="center"&gt;10&lt;/td&gt; 
    &lt;td align="center"&gt;2025.10.13&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*圆&lt;/td&gt; 
    &lt;td align="center"&gt;1&lt;/td&gt; 
    &lt;td align="center"&gt;2025.10.13&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;P*g&lt;/td&gt; 
    &lt;td align="center"&gt;6&lt;/td&gt; 
    &lt;td align="center"&gt;2025.10.13&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;Ocean&lt;/td&gt; 
    &lt;td align="center"&gt;20&lt;/td&gt; 
    &lt;td align="center"&gt;2025.10.12&lt;/td&gt; 
    &lt;td align="center"&gt;...真的太棒了！！！小白级别也能直接用...&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;**培&lt;/td&gt; 
    &lt;td align="center"&gt;5.2&lt;/td&gt; 
    &lt;td align="center"&gt;2025.10.2&lt;/td&gt; 
    &lt;td align="center"&gt;github-yzyf1312:开源万岁&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*椿&lt;/td&gt; 
    &lt;td align="center"&gt;3&lt;/td&gt; 
    &lt;td align="center"&gt;2025.9.23&lt;/td&gt; 
    &lt;td align="center"&gt;加油，很不错&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*🍍&lt;/td&gt; 
    &lt;td align="center"&gt;10&lt;/td&gt; 
    &lt;td align="center"&gt;2025.9.21&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;E*f&lt;/td&gt; 
    &lt;td align="center"&gt;1&lt;/td&gt; 
    &lt;td align="center"&gt;2025.9.20&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*记&lt;/td&gt; 
    &lt;td align="center"&gt;1&lt;/td&gt; 
    &lt;td align="center"&gt;2025.9.20&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;z*u&lt;/td&gt; 
    &lt;td align="center"&gt;2&lt;/td&gt; 
    &lt;td align="center"&gt;2025.9.19&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;**昊&lt;/td&gt; 
    &lt;td align="center"&gt;5&lt;/td&gt; 
    &lt;td align="center"&gt;2025.9.17&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*号&lt;/td&gt; 
    &lt;td align="center"&gt;1&lt;/td&gt; 
    &lt;td align="center"&gt;2025.9.15&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;T*T&lt;/td&gt; 
    &lt;td align="center"&gt;2&lt;/td&gt; 
    &lt;td align="center"&gt;2025.9.15&lt;/td&gt; 
    &lt;td align="center"&gt;点赞&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*家&lt;/td&gt; 
    &lt;td align="center"&gt;10&lt;/td&gt; 
    &lt;td align="center"&gt;2025.9.10&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*X&lt;/td&gt; 
    &lt;td align="center"&gt;1.11&lt;/td&gt; 
    &lt;td align="center"&gt;2025.9.3&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*飙&lt;/td&gt; 
    &lt;td align="center"&gt;20&lt;/td&gt; 
    &lt;td align="center"&gt;2025.8.31&lt;/td&gt; 
    &lt;td align="center"&gt;来自老童谢谢&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*下&lt;/td&gt; 
    &lt;td align="center"&gt;1&lt;/td&gt; 
    &lt;td align="center"&gt;2025.8.30&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;2*D&lt;/td&gt; 
    &lt;td align="center"&gt;88&lt;/td&gt; 
    &lt;td align="center"&gt;2025.8.13 下午&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;2*D&lt;/td&gt; 
    &lt;td align="center"&gt;1&lt;/td&gt; 
    &lt;td align="center"&gt;2025.8.13 上午&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;S*o&lt;/td&gt; 
    &lt;td align="center"&gt;1&lt;/td&gt; 
    &lt;td align="center"&gt;2025.8.05&lt;/td&gt; 
    &lt;td align="center"&gt;支持一下&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*侠&lt;/td&gt; 
    &lt;td align="center"&gt;10&lt;/td&gt; 
    &lt;td align="center"&gt;2025.8.04&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;x*x&lt;/td&gt; 
    &lt;td align="center"&gt;2&lt;/td&gt; 
    &lt;td align="center"&gt;2025.8.03&lt;/td&gt; 
    &lt;td align="center"&gt;trendRadar 好项目 点赞&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*远&lt;/td&gt; 
    &lt;td align="center"&gt;1&lt;/td&gt; 
    &lt;td align="center"&gt;2025.8.01&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*邪&lt;/td&gt; 
    &lt;td align="center"&gt;5&lt;/td&gt; 
    &lt;td align="center"&gt;2025.8.01&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*梦&lt;/td&gt; 
    &lt;td align="center"&gt;0.1&lt;/td&gt; 
    &lt;td align="center"&gt;2025.7.30&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;**龙&lt;/td&gt; 
    &lt;td align="center"&gt;10&lt;/td&gt; 
    &lt;td align="center"&gt;2025.7.29&lt;/td&gt; 
    &lt;td align="center"&gt;支持一下&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;br /&gt; 
&lt;h2&gt;✨ 核心功能&lt;/h2&gt; 
&lt;h3&gt;&lt;strong&gt;全网热点聚合&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;知乎&lt;/li&gt; 
 &lt;li&gt;抖音&lt;/li&gt; 
 &lt;li&gt;bilibili 热搜&lt;/li&gt; 
 &lt;li&gt;华尔街见闻&lt;/li&gt; 
 &lt;li&gt;贴吧&lt;/li&gt; 
 &lt;li&gt;百度热搜&lt;/li&gt; 
 &lt;li&gt;财联社热门&lt;/li&gt; 
 &lt;li&gt;澎湃新闻&lt;/li&gt; 
 &lt;li&gt;凤凰网&lt;/li&gt; 
 &lt;li&gt;今日头条&lt;/li&gt; 
 &lt;li&gt;微博&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;默认监控 11 个主流平台，也可自行增加额外的平台&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;💡 详细配置教程见 &lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#1-%E5%B9%B3%E5%8F%B0%E9%85%8D%E7%BD%AE"&gt;配置详解 - 平台配置&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;&lt;strong&gt;智能推送策略&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;三种推送模式&lt;/strong&gt;：&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;模式&lt;/th&gt; 
   &lt;th&gt;适用场景&lt;/th&gt; 
   &lt;th&gt;推送特点&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;当日汇总&lt;/strong&gt; (daily)&lt;/td&gt; 
   &lt;td&gt;企业管理者/普通用户&lt;/td&gt; 
   &lt;td&gt;按时推送当日所有匹配新闻（会包含之前推送过的）&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;当前榜单&lt;/strong&gt; (current)&lt;/td&gt; 
   &lt;td&gt;自媒体人/内容创作者&lt;/td&gt; 
   &lt;td&gt;按时推送当前榜单匹配新闻（持续在榜的每次都出现）&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;增量监控&lt;/strong&gt; (incremental)&lt;/td&gt; 
   &lt;td&gt;投资者/交易员&lt;/td&gt; 
   &lt;td&gt;仅推送新增内容，零重复&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;💡 &lt;strong&gt;快速选择指南：&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;🔄 不想看到重复新闻 → 用 &lt;code&gt;incremental&lt;/code&gt;（增量监控）&lt;/li&gt; 
  &lt;li&gt;📊 想看完整榜单趋势 → 用 &lt;code&gt;current&lt;/code&gt;（当前榜单）&lt;/li&gt; 
  &lt;li&gt;📝 需要每日汇总报告 → 用 &lt;code&gt;daily&lt;/code&gt;（当日汇总）&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;详细对比和配置教程见 &lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#3-%E6%8E%A8%E9%80%81%E6%A8%A1%E5%BC%8F%E8%AF%A6%E8%A7%A3"&gt;配置详解 - 推送模式详解&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;附加功能&lt;/strong&gt;（可选）：&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;功能&lt;/th&gt; 
   &lt;th&gt;说明&lt;/th&gt; 
   &lt;th&gt;默认&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;推送时间窗口控制&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;设定推送时间范围（如 09:00-18:00），避免非工作时间打扰&lt;/td&gt; 
   &lt;td&gt;关闭&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;内容顺序配置&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;调整"热点词汇统计"和"新增热点新闻"的显示顺序（v3.5.0 新增）&lt;/td&gt; 
   &lt;td&gt;统计在前&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;💡 详细配置教程见 &lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#7-%E6%8A%A5%E5%91%8A%E9%85%8D%E7%BD%AE"&gt;配置详解 - 报告配置&lt;/a&gt; 和 &lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#8-%E6%8E%A8%E9%80%81%E6%97%B6%E9%97%B4%E7%AA%97%E5%8F%A3%E9%85%8D%E7%BD%AE"&gt;配置详解 - 推送时间窗口&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;&lt;strong&gt;精准内容筛选&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;设置个人关键词（如：AI、比亚迪、教育政策），只推送相关热点，过滤无关信息&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;基础语法&lt;/strong&gt;（5种）：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;普通词：基础匹配&lt;/li&gt; 
 &lt;li&gt;必须词 &lt;code&gt;+&lt;/code&gt;：限定范围&lt;/li&gt; 
 &lt;li&gt;过滤词 &lt;code&gt;!&lt;/code&gt;：排除干扰&lt;/li&gt; 
 &lt;li&gt;数量限制 &lt;code&gt;@&lt;/code&gt;：控制显示数量（v3.2.0 新增）&lt;/li&gt; 
 &lt;li&gt;全局过滤 &lt;code&gt;[GLOBAL_FILTER]&lt;/code&gt;：全局排除指定内容（v3.5.0 新增）&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;高级功能&lt;/strong&gt;（v3.2.0 新增）：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;🔢 &lt;strong&gt;关键词排序控制&lt;/strong&gt;：按热度优先 or 配置顺序优先&lt;/li&gt; 
 &lt;li&gt;📊 &lt;strong&gt;显示数量精准限制&lt;/strong&gt;：全局配置 + 单独配置，灵活控制推送长度&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;词组化管理&lt;/strong&gt;：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;空行分隔，独立统计不同主题热点&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;💡 &lt;strong&gt;基础配置教程&lt;/strong&gt;：&lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#%E5%85%B3%E9%94%AE%E8%AF%8D%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95"&gt;关键词配置 - 基础语法&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;💡 &lt;strong&gt;高级配置教程&lt;/strong&gt;：&lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#%E5%85%B3%E9%94%AE%E8%AF%8D%E9%AB%98%E7%BA%A7%E9%85%8D%E7%BD%AE"&gt;关键词配置 - 高级配置&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;💡 也可以不做筛选，完整推送所有热点（将 frequency_words.txt 留空）&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;&lt;strong&gt;热点趋势分析&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;实时追踪新闻热度变化，让你不仅知道"什么在热搜"，更了解"热点如何演变"&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;时间轴追踪&lt;/strong&gt;：记录每条新闻从首次出现到最后出现的完整时间跨度&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;热度变化&lt;/strong&gt;：统计新闻在不同时间段的排名变化和出现频次&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;新增检测&lt;/strong&gt;：实时识别新出现的热点话题，用🆕标记第一时间提醒&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;持续性分析&lt;/strong&gt;：区分一次性热点话题和持续发酵的深度新闻&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;跨平台对比&lt;/strong&gt;：同一新闻在不同平台的排名表现，看出媒体关注度差异&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;💡 推送格式说明见 &lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#5-%E6%8E%A8%E9%80%81%E6%A0%BC%E5%BC%8F%E5%8F%82%E8%80%83"&gt;配置详解 - 推送格式参考&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;&lt;strong&gt;个性化热点算法&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;不再被各个平台的算法牵着走，TrendRadar 会重新整理全网热搜：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;看重排名高的新闻&lt;/strong&gt;（占60%）：各平台前几名的新闻优先显示&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;关注持续出现的话题&lt;/strong&gt;（占30%）：反复出现的新闻更重要&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;考虑排名质量&lt;/strong&gt;（占10%）：不仅多次出现，还经常排在前列&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;💡 这三个比例可以调整，详见 &lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#4-%E7%83%AD%E7%82%B9%E6%9D%83%E9%87%8D%E8%B0%83%E6%95%B4"&gt;配置详解 - 热点权重调整&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;&lt;strong&gt;多渠道实时推送&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;支持&lt;strong&gt;企业微信&lt;/strong&gt;(+ 微信推送方案)、&lt;strong&gt;飞书&lt;/strong&gt;、&lt;strong&gt;钉钉&lt;/strong&gt;、&lt;strong&gt;Telegram&lt;/strong&gt;、&lt;strong&gt;邮件&lt;/strong&gt;、&lt;strong&gt;ntfy&lt;/strong&gt;、&lt;strong&gt;Bark&lt;/strong&gt;、&lt;strong&gt;Slack&lt;/strong&gt;，消息直达手机和邮箱&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;📌 多账号推送说明（v3.5.0 新增）：&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;✅ &lt;strong&gt;支持多账号配置&lt;/strong&gt;：所有推送渠道（飞书、钉钉、企业微信、Telegram、ntfy、Bark、Slack）均支持配置多个账号&lt;/li&gt; 
 &lt;li&gt;✅ &lt;strong&gt;配置方式&lt;/strong&gt;：使用英文分号 &lt;code&gt;;&lt;/code&gt; 分隔多个账号值&lt;/li&gt; 
 &lt;li&gt;✅ &lt;strong&gt;示例&lt;/strong&gt;：&lt;code&gt;FEISHU_WEBHOOK_URL&lt;/code&gt; 的 Secret 值填写 &lt;code&gt;https://webhook1;https://webhook2&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;⚠️ &lt;strong&gt;配对配置&lt;/strong&gt;：Telegram 和 ntfy 需要保证配对参数数量一致（如 token 和 chat_id 都是 2 个）&lt;/li&gt; 
 &lt;li&gt;⚠️ &lt;strong&gt;数量限制&lt;/strong&gt;：默认每个渠道最多 3 个账号，超出会被截断&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;strong&gt;灵活存储架构&lt;/strong&gt;（v4.0.0 重大更新）&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;多存储后端支持&lt;/strong&gt;：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;☁️ &lt;strong&gt;远程云存储&lt;/strong&gt;：GitHub Actions 环境默认，支持 S3 兼容协议（R2/OSS/COS 等），数据存储在云端，不污染仓库&lt;/li&gt; 
 &lt;li&gt;💾 &lt;strong&gt;本地 SQLite 数据库&lt;/strong&gt;：Docker/本地环境默认，数据完全可控&lt;/li&gt; 
 &lt;li&gt;🔄 &lt;strong&gt;自动后端选择&lt;/strong&gt;：根据运行环境智能切换存储方式&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;数据格式&lt;/strong&gt;：&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;格式&lt;/th&gt; 
   &lt;th&gt;用途&lt;/th&gt; 
   &lt;th&gt;说明&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;SQLite&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;主存储&lt;/td&gt; 
   &lt;td&gt;单文件数据库，查询快速，支持 MCP AI 分析&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;TXT&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;可选快照&lt;/td&gt; 
   &lt;td&gt;可读文本格式，方便直接查看&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;HTML&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;报告展示&lt;/td&gt; 
   &lt;td&gt;精美可视化页面，PC/移动端适配&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;数据管理&lt;/strong&gt;：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;✅ 自动清理过期数据（可配置保留天数）&lt;/li&gt; 
 &lt;li&gt;✅ 时区配置支持（全球时区）&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;💡 详细说明见 &lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#9-%E5%AD%98%E5%82%A8%E9%85%8D%E7%BD%AE"&gt;配置详解 - 存储配置&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;&lt;strong&gt;多端部署&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;GitHub Actions&lt;/strong&gt;：定时自动爬取 + 远程云存储（需签到续期）&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Docker 部署&lt;/strong&gt;：支持多架构容器化运行，数据本地存储&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;本地运行&lt;/strong&gt;：Windows/Mac/Linux 直接运行&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;strong&gt;AI 智能分析（v3.0.0 新增）&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;基于 MCP (Model Context Protocol) 协议的 AI 对话分析系统，让你用自然语言深度挖掘新闻数据&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;对话式查询&lt;/strong&gt;：用自然语言提问，如"查询昨天知乎的热点"、"分析比特币最近的热度趋势"&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;13 种分析工具&lt;/strong&gt;：涵盖基础查询、智能检索、趋势分析、数据洞察、情感分析等&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;多客户端支持&lt;/strong&gt;：Cherry Studio（GUI 配置）、Claude Desktop、Cursor、Cline 等&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;深度分析能力&lt;/strong&gt;： 
  &lt;ul&gt; 
   &lt;li&gt;话题趋势追踪（热度变化、生命周期、爆火检测、趋势预测）&lt;/li&gt; 
   &lt;li&gt;跨平台数据对比（活跃度统计、关键词共现）&lt;/li&gt; 
   &lt;li&gt;智能摘要生成、相似新闻查找、历史关联检索&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;💡 使用提示&lt;/strong&gt;：AI 功能需要本地新闻数据支持&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;项目自带 &lt;strong&gt;11月1-15日&lt;/strong&gt; 测试数据，可立即体验&lt;/li&gt; 
  &lt;li&gt;建议自行部署运行项目，获取更实时的数据&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;详见 &lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#-ai-%E6%99%BA%E8%83%BD%E5%88%86%E6%9E%90"&gt;AI 智能分析&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;&lt;strong&gt;零技术门槛部署&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;GitHub 一键 Fork 即可使用，无需编程基础。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;30秒部署： GitHub Pages（网页浏览）支持一键保存成图片，随时分享给他人&lt;/p&gt; 
 &lt;p&gt;1分钟部署： 企业微信（手机通知）&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;💡 提示：&lt;/strong&gt; 想要&lt;strong&gt;实时更新&lt;/strong&gt;的网页版？fork 后，进入你的仓库 Settings → Pages，启用 GitHub Pages。&lt;a href="https://sansan0.github.io/TrendRadar/"&gt;效果预览&lt;/a&gt;。&lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;减少 APP 依赖&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;从"被算法推荐绑架"变成"主动获取自己想要的信息"&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;适合人群：&lt;/strong&gt; 投资者、自媒体人、企业公关、关心时事的普通用户&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;典型场景：&lt;/strong&gt; 股市投资监控、品牌舆情追踪、行业动态关注、生活资讯获取&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;Github Pages 效果(手机端适配、邮箱推送效果)&lt;/th&gt; 
   &lt;th align="center"&gt;飞书推送效果&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/sansan0/TrendRadar/master/_image/github-pages.png" alt="Github Pages效果" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/sansan0/TrendRadar/master/_image/feishu.jpg" alt="飞书推送效果" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;br /&gt; 
&lt;h2&gt;📝 更新日志&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;升级说明&lt;/strong&gt;：&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;📌 查看最新更新&lt;/strong&gt;：&lt;strong&gt;&lt;a href="https://github.com/sansan0/TrendRadar?tab=readme-ov-file#-%E6%9B%B4%E6%96%B0%E6%97%A5%E5%BF%97"&gt;原仓库更新日志&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;提示&lt;/strong&gt;：不要通过 &lt;strong&gt;Sync fork&lt;/strong&gt; 更新本项目，建议查看【历史更新】，明确具体的【升级方式】和【功能内容】&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;大版本升级&lt;/strong&gt;：从 v1.x 升级到 v2.y，建议删除现有 fork 后重新 fork，这样更省力且避免配置冲突&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;2025/12/20 - v4.0.3&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;新增 URL 标准化功能，解决微博等平台因动态参数（如 &lt;code&gt;band_rank&lt;/code&gt;）导致的重复推送问题&lt;/li&gt; 
 &lt;li&gt;修复增量模式检测逻辑，正确识别历史标题&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;2025/12/13 - mcp-v1.1.0&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;MCP 模块更新:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;适配 v4.0.0，同时也兼容 v3.x 的数据&lt;/li&gt; 
 &lt;li&gt;新增存储同步工具： 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;sync_from_remote&lt;/code&gt;: 从远程存储拉取数据到本地&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;get_storage_status&lt;/code&gt;: 获取存储配置和状态&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;list_available_dates&lt;/code&gt;: 列出本地/远程可用日期范围&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;👉 点击展开：&lt;strong&gt;历史更新&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h3&gt;2025/12/17 - v4.0.1&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;StorageManager 添加推送记录代理方法&lt;/li&gt; 
  &lt;li&gt;S3 客户端切换至 virtual-hosted style 以提升兼容性（支持腾讯云 COS 等更多服务）&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/12/13 - v4.0.0&lt;/h3&gt; 
 &lt;p&gt;&lt;strong&gt;🎉 重大更新：全面重构存储和核心架构&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;多存储后端支持&lt;/strong&gt;：引入全新的存储模块，支持本地 SQLite 和远程云存储（S3 兼容协议，推荐免费的 Cloudflare R2），适应 GitHub Actions、Docker 和本地环境。&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;数据库结构优化&lt;/strong&gt;：重构 SQLite 数据库表结构，提升数据效率和查询能力。&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;核心代码模块化&lt;/strong&gt;：将主程序逻辑拆分为 trendradar 包的多个模块，显著提升代码可维护性。&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;增强功能&lt;/strong&gt;：实现日期格式标准化、数据保留策略、时区配置支持、时间显示优化，并修复远程存储数据持久化问题，确保数据合并的准确性。&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;清理和兼容&lt;/strong&gt;：移除了大部分历史兼容代码，统一了数据存储和读取方式。&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/12/03 - v3.5.0&lt;/h3&gt; 
 &lt;p&gt;&lt;strong&gt;🎉 核心功能增强&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;多账号推送支持&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;所有推送渠道（飞书、钉钉、企业微信、Telegram、ntfy、Bark、Slack）支持多账号配置&lt;/li&gt; 
    &lt;li&gt;使用分号 &lt;code&gt;;&lt;/code&gt; 分隔多个账号，例如：&lt;code&gt;FEISHU_WEBHOOK_URL=url1;url2&lt;/code&gt;&lt;/li&gt; 
    &lt;li&gt;自动验证配对配置（如 Telegram 的 token 和 chat_id）数量一致性&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;推送内容顺序可配置&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;新增 &lt;code&gt;reverse_content_order&lt;/code&gt; 配置项&lt;/li&gt; 
    &lt;li&gt;支持自定义热点词汇统计与新增热点新闻的显示顺序&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;全局过滤关键词&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;新增 &lt;code&gt;[GLOBAL_FILTER]&lt;/code&gt; 区域标记，支持全局过滤不想看到的内容&lt;/li&gt; 
    &lt;li&gt;适用场景：过滤广告、营销、低质内容等&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;&lt;strong&gt;🐳 Docker 双路径 HTML 生成优化&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;问题修复&lt;/strong&gt;：解决 Docker 环境下 &lt;code&gt;index.html&lt;/code&gt; 无法同步到宿主机的问题&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;双路径生成&lt;/strong&gt;：当日汇总 HTML 同时生成到两个位置 
   &lt;ul&gt; 
    &lt;li&gt;&lt;code&gt;index.html&lt;/code&gt;（项目根目录）：供 GitHub Pages 访问&lt;/li&gt; 
    &lt;li&gt;&lt;code&gt;output/index.html&lt;/code&gt;：通过 Docker Volume 挂载，宿主机可直接访问&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;兼容性&lt;/strong&gt;：确保 Docker、GitHub Actions、本地运行环境均能正常访问网页版报告&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;🐳 Docker MCP 镜像支持&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;新增独立的 MCP 服务镜像 &lt;code&gt;wantcat/trendradar-mcp&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;支持 Docker 部署 AI 分析功能，通过 HTTP 接口（端口 3333）提供服务&lt;/li&gt; 
  &lt;li&gt;双容器架构：新闻推送服务与 MCP 服务独立运行，可分别扩展和重启&lt;/li&gt; 
  &lt;li&gt;详见 &lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#6-docker-%E9%83%A8%E7%BD%B2"&gt;Docker 部署 - MCP 服务&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;🌐 Web 服务器支持&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;新增内置 Web 服务器，支持通过浏览器访问生成的报告&lt;/li&gt; 
  &lt;li&gt;通过 &lt;code&gt;manage.py&lt;/code&gt; 命令控制启动/停止：&lt;code&gt;docker exec -it trend-radar python manage.py start_webserver&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;访问地址：&lt;code&gt;http://localhost:8080&lt;/code&gt;（端口可配置）&lt;/li&gt; 
  &lt;li&gt;安全特性：静态文件服务、目录限制、本地访问&lt;/li&gt; 
  &lt;li&gt;支持自动启动和手动控制两种模式&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;📖 文档优化&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;新增 &lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#7-%E6%8A%A5%E5%91%8A%E9%85%8D%E7%BD%AE"&gt;报告配置&lt;/a&gt; 章节：report 相关参数详解&lt;/li&gt; 
  &lt;li&gt;新增 &lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#8-%E6%8E%A8%E9%80%81%E6%97%B6%E9%97%B4%E7%AA%97%E5%8F%A3%E9%85%8D%E7%BD%AE"&gt;推送时间窗口配置&lt;/a&gt; 章节：push_window 配置教程&lt;/li&gt; 
  &lt;li&gt;新增 &lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#9-%E6%89%A7%E8%A1%8C%E9%A2%91%E7%8E%87%E9%85%8D%E7%BD%AE"&gt;执行频率配置&lt;/a&gt; 章节：Cron 表达式说明和常用示例&lt;/li&gt; 
  &lt;li&gt;新增 &lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#10-%E5%A4%9A%E8%B4%A6%E5%8F%B7%E6%8E%A8%E9%80%81%E9%85%8D%E7%BD%AE"&gt;多账号推送配置&lt;/a&gt; 章节：多账号推送配置详解&lt;/li&gt; 
  &lt;li&gt;优化各配置章节：统一添加"配置位置"说明&lt;/li&gt; 
  &lt;li&gt;简化快速开始配置说明：三个核心文件一目了然&lt;/li&gt; 
  &lt;li&gt;优化 &lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#6-docker-%E9%83%A8%E7%BD%B2"&gt;Docker 部署&lt;/a&gt; 章节：新增镜像说明、推荐 git clone 部署、重组部署方式&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;🔧 升级说明&lt;/strong&gt;：&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;GitHub Fork 用户&lt;/strong&gt;：更新 &lt;code&gt;main.py&lt;/code&gt;、&lt;code&gt;config/config.yaml&lt;/code&gt;（新增多账号推送支持，无需修改现有配置）&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;多账号推送&lt;/strong&gt;：新功能，默认不启用，现有单账号配置不受影响&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/11/26 - mcp-v1.0.3&lt;/h3&gt; 
 &lt;p&gt;&lt;strong&gt;MCP 模块更新:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;新增日期解析工具 resolve_date_range,解决 AI 模型计算日期不一致的问题&lt;/li&gt; 
  &lt;li&gt;支持自然语言日期表达式解析(本周、最近7天、上月等)&lt;/li&gt; 
  &lt;li&gt;工具总数从 13 个增加到 14 个&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/11/28 - v3.4.1&lt;/h3&gt; 
 &lt;p&gt;&lt;strong&gt;🔧 格式优化&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Bark 推送增强&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Bark 现支持 Markdown 渲染&lt;/li&gt; 
    &lt;li&gt;启用原生 Markdown 格式：粗体、链接、列表、代码块等&lt;/li&gt; 
    &lt;li&gt;移除纯文本转换，充分利用 Bark 原生渲染能力&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Slack 格式精准化&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;使用专用 mrkdwn 格式处理分批内容&lt;/li&gt; 
    &lt;li&gt;提升字节大小估算准确性（避免消息超限）&lt;/li&gt; 
    &lt;li&gt;优化链接格式：&lt;code&gt;&amp;lt;url|text&amp;gt;&lt;/code&gt; 和加粗语法：&lt;code&gt;*text*&lt;/code&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;性能提升&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;格式转换在分批过程中完成，避免二次处理&lt;/li&gt; 
    &lt;li&gt;准确估算消息大小，减少发送失败率&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;&lt;strong&gt;🔧 升级说明&lt;/strong&gt;：&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;GitHub Fork 用户&lt;/strong&gt;：更新 &lt;code&gt;main.py&lt;/code&gt;，&lt;code&gt;config.yaml&lt;/code&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/11/25 - v3.4.0&lt;/h3&gt; 
 &lt;p&gt;&lt;strong&gt;🎉 新增 Slack 推送支持&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;团队协作推送渠道&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;支持 Slack Incoming Webhooks（全球流行的团队协作工具）&lt;/li&gt; 
    &lt;li&gt;消息集中管理，适合团队共享热点资讯&lt;/li&gt; 
    &lt;li&gt;支持 mrkdwn 格式（粗体、链接等）&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;多种部署方式&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;GitHub Actions：配置 &lt;code&gt;SLACK_WEBHOOK_URL&lt;/code&gt; Secret&lt;/li&gt; 
    &lt;li&gt;Docker：环境变量 &lt;code&gt;SLACK_WEBHOOK_URL&lt;/code&gt;&lt;/li&gt; 
    &lt;li&gt;本地运行：&lt;code&gt;config/config.yaml&lt;/code&gt; 配置文件&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;📖 &lt;strong&gt;详细配置教程&lt;/strong&gt;：&lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#-%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B"&gt;快速开始 - Slack 推送&lt;/a&gt;&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;ul&gt; 
  &lt;li&gt;优化 setup-windows.bat 和 setup-windows-en.bat 一键安装 MCP 的体验&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;🔧 升级说明&lt;/strong&gt;：&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;GitHub Fork 用户&lt;/strong&gt;：更新 &lt;code&gt;main.py&lt;/code&gt;、&lt;code&gt;config/config.yaml&lt;/code&gt;、&lt;code&gt;.github/workflows/crawler.yml&lt;/code&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/11/24 - v3.3.0&lt;/h3&gt; 
 &lt;p&gt;&lt;strong&gt;🎉 新增 Bark 推送支持&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;iOS 专属推送渠道&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;支持 Bark 推送（基于 APNs，iOS 平台）&lt;/li&gt; 
    &lt;li&gt;免费开源，简洁高效，无广告干扰&lt;/li&gt; 
    &lt;li&gt;支持官方服务器和自建服务器两种方式&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;多种部署方式&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;GitHub Actions：配置 &lt;code&gt;BARK_URL&lt;/code&gt; Secret&lt;/li&gt; 
    &lt;li&gt;Docker：环境变量 &lt;code&gt;BARK_URL&lt;/code&gt;&lt;/li&gt; 
    &lt;li&gt;本地运行：&lt;code&gt;config/config.yaml&lt;/code&gt; 配置文件&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;📖 &lt;strong&gt;详细配置教程&lt;/strong&gt;：&lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#-%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B"&gt;快速开始 - Bark 推送&lt;/a&gt;&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;🐛 Bug 修复&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;修复 &lt;code&gt;config.yaml&lt;/code&gt; 中 &lt;code&gt;ntfy_server_url&lt;/code&gt; 配置不生效的问题 (&lt;a href="https://github.com/sansan0/TrendRadar/issues/345"&gt;#345&lt;/a&gt;)&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;🔧 升级说明&lt;/strong&gt;：&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;GitHub Fork 用户&lt;/strong&gt;：更新 &lt;code&gt;main.py&lt;/code&gt;、&lt;code&gt;config/config.yaml&lt;/code&gt;、&lt;code&gt;.github/workflows/crawler.yml&lt;/code&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/11/23 - v3.2.0&lt;/h3&gt; 
 &lt;p&gt;&lt;strong&gt;🎯 新增高级定制功能&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;关键词排序优先级配置&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;支持两种排序策略：热度优先 vs 配置顺序优先&lt;/li&gt; 
    &lt;li&gt;满足不同使用场景：热点追踪 or 个性化关注&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;显示数量精准控制&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;全局配置：统一限制所有关键词显示数量&lt;/li&gt; 
    &lt;li&gt;单独配置：使用 &lt;code&gt;@数字&lt;/code&gt; 语法为特定关键词设置限制&lt;/li&gt; 
    &lt;li&gt;有效控制推送长度，突出重点内容&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;📖 &lt;strong&gt;详细配置教程&lt;/strong&gt;：&lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#%E5%85%B3%E9%94%AE%E8%AF%8D%E9%AB%98%E7%BA%A7%E9%85%8D%E7%BD%AE"&gt;关键词配置 - 高级配置&lt;/a&gt;&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;🔧 升级说明&lt;/strong&gt;：&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;GitHub Fork 用户&lt;/strong&gt;：更新 &lt;code&gt;main.py&lt;/code&gt;、&lt;code&gt;config/config.yaml&lt;/code&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/11/18 - mcp-v1.0.2&lt;/h3&gt; 
 &lt;p&gt;&lt;strong&gt;MCP 模块更新:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;优化查询今日新闻却可能错误返回过去日期的情况&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/11/22 - v3.1.1&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;修复数据异常导致的崩溃问题&lt;/strong&gt;：解决部分用户在 GitHub Actions 环境中遇到的 &lt;code&gt;'float' object has no attribute 'lower'&lt;/code&gt; 错误&lt;/li&gt; 
  &lt;li&gt;新增双重防护机制：在数据获取阶段过滤无效标题（None、float、空字符串），同时在函数调用处添加类型检查&lt;/li&gt; 
  &lt;li&gt;提升系统稳定性，确保在数据源返回异常格式时仍能正常运行&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;升级说明&lt;/strong&gt;（GitHub Fork 用户）：&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;必须更新：&lt;code&gt;main.py&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;建议使用小版本升级方式：复制替换上述文件&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/11/20 - v3.1.0&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;新增个人微信推送支持&lt;/strong&gt;：企业微信应用可推送到个人微信，无需安装企业微信 APP&lt;/li&gt; 
  &lt;li&gt;支持两种消息格式：&lt;code&gt;markdown&lt;/code&gt;（企业微信群机器人）和 &lt;code&gt;text&lt;/code&gt;（个人微信应用）&lt;/li&gt; 
  &lt;li&gt;新增 &lt;code&gt;WEWORK_MSG_TYPE&lt;/code&gt; 环境变量配置，支持 GitHub Actions、Docker、docker compose 等多种部署方式&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;text&lt;/code&gt; 模式自动清除 Markdown 语法，提供纯文本推送效果&lt;/li&gt; 
  &lt;li&gt;详见快速开始中的「个人微信推送」配置说明&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;升级说明&lt;/strong&gt;（GitHub Fork 用户）：&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;必须更新：&lt;code&gt;main.py&lt;/code&gt;、&lt;code&gt;config/config.yaml&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;可选更新：&lt;code&gt;.github/workflows/crawler.yml&lt;/code&gt;（如使用 GitHub Actions 部署）&lt;/li&gt; 
  &lt;li&gt;建议使用小版本升级方式：复制替换上述文件&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/11/12 - v3.0.5&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;修复邮件发送 SSL/TLS 端口配置逻辑错误&lt;/li&gt; 
  &lt;li&gt;优化邮箱服务商（QQ/163/126）默认使用 465 端口（SSL）&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;新增 Docker 环境变量支持&lt;/strong&gt;：核心配置项（&lt;code&gt;enable_crawler&lt;/code&gt;、&lt;code&gt;report_mode&lt;/code&gt;、&lt;code&gt;push_window&lt;/code&gt; 等）支持通过环境变量覆盖，解决 NAS 用户修改配置文件不生效的问题（详见 &lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#-docker-%E9%83%A8%E7%BD%B2"&gt;🐳 Docker 部署&lt;/a&gt; 章节）&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/10/26 - mcp-v1.0.1&lt;/h3&gt; 
 &lt;p&gt;&lt;strong&gt;MCP 模块更新:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;修复日期查询参数传递错误&lt;/li&gt; 
  &lt;li&gt;统一所有工具的时间参数格式&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/10/31 - v3.0.4&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;解决飞书因推送内容过长而产生的错误，实现了分批推送&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/10/23 - v3.0.3&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;扩大 ntfy 错误信息显示范围&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/10/21 - v3.0.2&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;修复 ntfy 推送编码问题&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/10/20 - v3.0.0&lt;/h3&gt; 
 &lt;p&gt;&lt;strong&gt;重大更新 - AI 分析功能上线&lt;/strong&gt; 🤖&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;核心功能&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;新增基于 MCP (Model Context Protocol) 的 AI 分析服务器&lt;/li&gt; 
    &lt;li&gt;支持13种智能分析工具：基础查询、智能检索、高级分析、系统管理&lt;/li&gt; 
    &lt;li&gt;自然语言交互：通过对话方式查询和分析新闻数据&lt;/li&gt; 
    &lt;li&gt;多客户端支持：Claude Desktop、Cherry Studio、Cursor、Cline 等&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;分析能力&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;话题趋势分析（热度追踪、生命周期、爆火检测、趋势预测）&lt;/li&gt; 
    &lt;li&gt;数据洞察（平台对比、活跃度统计、关键词共现）&lt;/li&gt; 
    &lt;li&gt;情感分析、相似新闻查找、智能摘要生成&lt;/li&gt; 
    &lt;li&gt;历史相关新闻检索、多模式搜索&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;更新提示&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;这是独立的 AI 分析功能，不影响现有的推送功能&lt;/li&gt; 
    &lt;li&gt;可选择性使用，无需升级现有部署&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/10/15 - v2.4.4&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;更新内容&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;修复 ntfy 推送编码问题 + 1&lt;/li&gt; 
    &lt;li&gt;修复推送时间窗口判断问题&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;更新提示&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;建议【小版本升级】&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/10/10 - v2.4.3&lt;/h3&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;感谢 &lt;a href="https://github.com/sansan0/TrendRadar/issues/98"&gt;nidaye996&lt;/a&gt; 发现的体验问题&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;更新内容&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;重构"静默推送模式"命名为"推送时间窗口控制"，提升功能理解度&lt;/li&gt; 
    &lt;li&gt;明确推送时间窗口作为可选附加功能，可与三种推送模式搭配使用&lt;/li&gt; 
    &lt;li&gt;改进注释和文档描述，使功能定位更加清晰&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;更新提示&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;这个仅仅是重构，可以不用升级&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/10/8 - v2.4.2&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;更新内容&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;修复 ntfy 推送编码问题&lt;/li&gt; 
    &lt;li&gt;修复配置文件缺失问题&lt;/li&gt; 
    &lt;li&gt;优化 ntfy 推送效果&lt;/li&gt; 
    &lt;li&gt;增加 github page 图片分段导出功能&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;更新提示&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;建议使用【大版本更新】&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/10/2 - v2.4.0&lt;/h3&gt; 
 &lt;p&gt;&lt;strong&gt;新增 ntfy 推送通知&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;核心功能&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;支持 ntfy.sh 公共服务和自托管服务器&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;使用场景&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;适合追求隐私的用户（支持自托管）&lt;/li&gt; 
    &lt;li&gt;跨平台推送（iOS、Android、Desktop、Web）&lt;/li&gt; 
    &lt;li&gt;无需注册账号（公共服务器）&lt;/li&gt; 
    &lt;li&gt;开源免费（MIT 协议）&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;更新提示&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;建议使用【大版本更新】&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/09/26 - v2.3.2&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;修正了邮件通知配置检查被遗漏的问题（&lt;a href="https://github.com/sansan0/TrendRadar/issues/88"&gt;#88&lt;/a&gt;）&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;修复说明&lt;/strong&gt;：&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;解决了即使正确配置邮件通知，系统仍提示"未配置任何webhook"的问题&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/09/22 - v2.3.1&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;新增邮件推送功能&lt;/strong&gt;，支持将热点新闻报告发送到邮箱&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;智能 SMTP 识别&lt;/strong&gt;：自动识别 Gmail、QQ邮箱、Outlook、网易邮箱等 10+ 种邮箱服务商配置&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;HTML 精美格式&lt;/strong&gt;：邮件内容采用与网页版相同的 HTML 格式，排版精美，移动端适配&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;批量发送支持&lt;/strong&gt;：支持多个收件人，用逗号分隔即可同时发送给多人&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;自定义 SMTP&lt;/strong&gt;：可自定义 SMTP 服务器和端口&lt;/li&gt; 
  &lt;li&gt;修复Docker构建网络连接问题&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;使用说明&lt;/strong&gt;：&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;适用场景：适合需要邮件归档、团队分享、定时报告的用户&lt;/li&gt; 
  &lt;li&gt;支持邮箱：Gmail、QQ邮箱、Outlook/Hotmail、163/126邮箱、新浪邮箱、搜狐邮箱等&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;更新提示&lt;/strong&gt;：&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;此次更新的内容比较多，如果想升级，建议采用【大版本升级】&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/09/17 - v2.2.0&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;新增一键保存新闻图片功能，让你轻松分享关注的热点&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;使用说明&lt;/strong&gt;：&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;适用场景：当你按照教程开启了网页版功能后(GitHub Pages)&lt;/li&gt; 
  &lt;li&gt;使用方法：用手机或电脑打开该网页链接，点击页面顶部的"保存为图片"按钮&lt;/li&gt; 
  &lt;li&gt;实际效果：系统会自动将当前的新闻报告制作成一张精美图片，保存到你的手机相册或电脑桌面&lt;/li&gt; 
  &lt;li&gt;分享便利：你可以直接把这张图片发给朋友、发到朋友圈，或分享到工作群，让别人也能看到你发现的重要资讯&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/09/13 - v2.1.2&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;解决钉钉的推送容量限制导致的新闻推送失败问题(采用分批推送)&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/09/04 - v2.1.1&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;修复docker在某些架构中无法正常运行的问题&lt;/li&gt; 
  &lt;li&gt;正式发布官方 Docker 镜像 wantcat/trendradar，支持多架构&lt;/li&gt; 
  &lt;li&gt;优化 Docker 部署流程，无需本地构建即可快速使用&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/08/30 - v2.1.0&lt;/h3&gt; 
 &lt;p&gt;&lt;strong&gt;核心改进&lt;/strong&gt;：&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;推送逻辑优化&lt;/strong&gt;：从"每次执行都推送"改为"时间窗口内可控推送"&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;时间窗口控制&lt;/strong&gt;：可设定推送时间范围，避免非工作时间打扰&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;推送频率可选&lt;/strong&gt;：时间段内支持单次推送或多次推送&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;更新提示&lt;/strong&gt;：&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;本功能默认关闭，需手动在 config.yaml 中开启推送时间窗口控制&lt;/li&gt; 
  &lt;li&gt;升级需同时更新 main.py 和 config.yaml 两个文件&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/08/27 - v2.0.4&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;本次版本不是功能修复，而是重要提醒&lt;/li&gt; 
  &lt;li&gt;请务必妥善保管好 webhooks，不要公开，不要公开，不要公开&lt;/li&gt; 
  &lt;li&gt;如果你以 fork 的方式将本项目部署在 GitHub 上，请将 webhooks 填入 GitHub Secret，而非 config.yaml&lt;/li&gt; 
  &lt;li&gt;如果你已经暴露了 webhooks 或将其填入了 config.yaml，建议删除后重新生成&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/08/06 - v2.0.3&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;优化 github page 的网页版效果，方便移动端使用&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/07/28 - v2.0.2&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;重构代码&lt;/li&gt; 
  &lt;li&gt;解决版本号容易被遗漏修改的问题&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/07/27 - v2.0.1&lt;/h3&gt; 
 &lt;p&gt;&lt;strong&gt;修复问题&lt;/strong&gt;:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;docker 的 shell 脚本的换行符为 CRLF 导致的执行异常问题&lt;/li&gt; 
  &lt;li&gt;frequency_words.txt 为空时，导致新闻发送也为空的逻辑问题&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;ul&gt; 
  &lt;li&gt;修复后，当你选择 frequency_words.txt 为空时，将&lt;strong&gt;推送所有新闻&lt;/strong&gt;，但受限于消息推送大小限制，请做如下调整 
   &lt;ul&gt; 
    &lt;li&gt;方案一：关闭手机推送，只选择 Github Pages 布置(这是能获得最完整信息的方案，将把所有平台的热点按照你&lt;strong&gt;自定义的热搜算法&lt;/strong&gt;进行重新排序)&lt;/li&gt; 
    &lt;li&gt;方案二：减少推送平台，优先选择&lt;strong&gt;企业微信&lt;/strong&gt;或&lt;strong&gt;Telegram&lt;/strong&gt;，这两个推送我做了分批推送功能(因为分批推送影响推送体验，且只有这两个平台只给一点点推送容量，所以才不得已做了分批推送功能，但至少能保证获得的信息完整)&lt;/li&gt; 
    &lt;li&gt;方案三：可与方案二结合，模式选择 current 或 incremental 可有效减少一次性推送的内容&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/07/17 - v2.0.0&lt;/h3&gt; 
 &lt;p&gt;&lt;strong&gt;重大重构&lt;/strong&gt;：&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;配置管理重构：所有配置现在通过 &lt;code&gt;config/config.yaml&lt;/code&gt; 文件管理（main.py 我依旧没拆分，方便你们复制升级）&lt;/li&gt; 
  &lt;li&gt;运行模式升级：支持三种模式 - &lt;code&gt;daily&lt;/code&gt;（当日汇总）、&lt;code&gt;current&lt;/code&gt;（当前榜单）、&lt;code&gt;incremental&lt;/code&gt;（增量监控）&lt;/li&gt; 
  &lt;li&gt;Docker 支持：完整的 Docker 部署方案，支持容器化运行&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;配置文件说明&lt;/strong&gt;：&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;config/config.yaml&lt;/code&gt; - 主配置文件（应用设置、爬虫配置、通知配置、平台配置等）&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;config/frequency_words.txt&lt;/code&gt; - 关键词配置（监控词汇设置）&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/07/09 - v1.4.1&lt;/h3&gt; 
 &lt;p&gt;&lt;strong&gt;功能新增&lt;/strong&gt;：增加增量推送(在 main.py 头部配置 FOCUS_NEW_ONLY)，该开关只关心新话题而非持续热度，只在有新内容时才发通知。&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;修复问题&lt;/strong&gt;: 某些情况下，由于新闻本身含有特殊符号导致的偶发性排版异常。&lt;/p&gt; 
 &lt;h3&gt;2025/06/23 - v1.3.0&lt;/h3&gt; 
 &lt;p&gt;企业微信 和 Telegram 的推送消息有长度限制，对此我采用将消息拆分推送的方式。开发文档详见&lt;a href="https://developer.work.weixin.qq.com/document/path/91770"&gt;企业微信&lt;/a&gt; 和 &lt;a href="https://core.telegram.org/bots/api"&gt;Telegram&lt;/a&gt;&lt;/p&gt; 
 &lt;h3&gt;2025/06/21 - v1.2.1&lt;/h3&gt; 
 &lt;p&gt;在本版本之前的旧版本，不仅 main.py 需要复制替换， crawler.yml 也需要你复制替换 &lt;a href="https://github.com/sansan0/TrendRadar/raw/master/.github/workflows/crawler.yml"&gt;https://github.com/sansan0/TrendRadar/blob/master/.github/workflows/crawler.yml&lt;/a&gt;&lt;/p&gt; 
 &lt;h3&gt;2025/06/19 - v1.2.0&lt;/h3&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;感谢 claude research 整理的各平台 api ,让我快速完成各平台适配（虽然代码更多冗余了~&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;ol&gt; 
  &lt;li&gt;支持 telegram ，企业微信，钉钉推送渠道, 支持多渠道配置和同时推送&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;h3&gt;2025/06/18 - v1.1.0&lt;/h3&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;&lt;strong&gt;200 star⭐&lt;/strong&gt; 了, 继续给大伙儿助兴~近期，在我的"怂恿"下，挺多人在我公众号点赞分享推荐助力了我，我都在后台看见了具体账号的鼓励数据，很多都成了天使轮老粉（我玩公众号才一个多月，虽然注册是七八年前的事了哈哈，属于上车早，发车晚），但因为你们没有留言或私信我，所以我也无法一一回应并感谢支持，在此一并谢谢！&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;ol&gt; 
  &lt;li&gt;重要的更新，加了权重，你现在看到的新闻都是最热点最有关注度的出现在最上面&lt;/li&gt; 
  &lt;li&gt;更新文档使用，因为近期更新了很多功能，而且之前的使用文档我偷懒写的简单（见下面的 ⚙️ frequency_words.txt 配置完整教程）&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;h3&gt;2025/06/16 - v1.0.0&lt;/h3&gt; 
 &lt;ol&gt; 
  &lt;li&gt;增加了一个项目新版本更新提示，默认打开，如要关掉，可以在 main.py 中把 "FEISHU_SHOW_VERSION_UPDATE": True 中的 True 改成 False 即可&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;h3&gt;2025/06/13+14&lt;/h3&gt; 
 &lt;ol&gt; 
  &lt;li&gt;去掉了兼容代码，之前 fork 的同学，直接复制代码会在当天显示异常（第二天会恢复正常）&lt;/li&gt; 
  &lt;li&gt;feishu 和 html 底部增加一个新增新闻显示&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;h3&gt;2025/06/09&lt;/h3&gt; 
 &lt;p&gt;&lt;strong&gt;100 star⭐&lt;/strong&gt; 了，写个小功能给大伙儿助助兴 frequency_words.txt 文件增加了一个【必须词】功能，使用 + 号&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;必须词语法如下：&lt;br /&gt; 唐僧或者猪八戒必须在标题里同时出现，才会收录到推送新闻中&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code&gt;+唐僧
+猪八戒
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="2"&gt; 
  &lt;li&gt;过滤词的优先级更高：&lt;br /&gt; 如果标题中过滤词匹配到唐僧念经，那么即使必须词里有唐僧，也不显示&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code&gt;+唐僧
!唐僧念经
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;2025/06/02&lt;/h3&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;strong&gt;网页&lt;/strong&gt;和&lt;strong&gt;飞书消息&lt;/strong&gt;支持手机直接跳转详情新闻&lt;/li&gt; 
  &lt;li&gt;优化显示效果 + 1&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;h3&gt;2025/05/26&lt;/h3&gt; 
 &lt;ol&gt; 
  &lt;li&gt;飞书消息显示效果优化&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;table&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td align="center"&gt; 优化前&lt;br /&gt; &lt;img src="https://raw.githubusercontent.com/sansan0/TrendRadar/master/_image/before.jpg" alt="飞书消息界面 - 优化前" width="400" /&gt; &lt;/td&gt; 
    &lt;td align="center"&gt; 优化后&lt;br /&gt; &lt;img src="https://raw.githubusercontent.com/sansan0/TrendRadar/master/_image/after.jpg" alt="飞书消息界面 - 优化后" width="400" /&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;br /&gt; 
&lt;h2&gt;🚀 快速开始&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;📖 提醒&lt;/strong&gt;：Fork 用户建议先 &lt;strong&gt;&lt;a href="https://github.com/sansan0/TrendRadar?tab=readme-ov-file"&gt;查看最新官方文档&lt;/a&gt;&lt;/strong&gt;，确保配置步骤是最新的。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;1️⃣ &lt;strong&gt;获取项目代码&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;点击本仓库页面右上角的绿色 &lt;strong&gt;[Use this template]&lt;/strong&gt; 按钮 → 选择 "Create a new repository"。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;⚠️ 提醒：&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;后续文档中提到的 "Fork" 均可理解为 "Use this template"&lt;/li&gt; 
  &lt;li&gt;使用 Fork 可能导致运行异常，详见 &lt;a href="https://github.com/sansan0/TrendRadar/issues/606"&gt;Issue #606&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;2️⃣ &lt;strong&gt;设置 GitHub Secrets（必需 + 可选平台）&lt;/strong&gt;:&lt;/p&gt; 
&lt;p&gt;在你 Fork 后的仓库中，进入 &lt;code&gt;Settings&lt;/code&gt; &amp;gt; &lt;code&gt;Secrets and variables&lt;/code&gt; &amp;gt; &lt;code&gt;Actions&lt;/code&gt; &amp;gt; &lt;code&gt;New repository secret&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;⚠️ GitHub Actions 使用说明&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;v4.0.0 重要变更&lt;/strong&gt;：引入「活跃度检测」机制，GitHub Actions 需定期签到以维持运行。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;🔄 签到续期机制&lt;/strong&gt;：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;运行周期&lt;/strong&gt;：有效期为 &lt;strong&gt;7 天&lt;/strong&gt;，倒计时结束后服务将自动挂起。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;续期方式&lt;/strong&gt;：在 Actions 页面手动触发 "Check In" workflow，即可重置 7 天有效期。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;操作路径&lt;/strong&gt;：&lt;code&gt;Actions&lt;/code&gt; → &lt;code&gt;Check In&lt;/code&gt; → &lt;code&gt;Run workflow&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;设计理念&lt;/strong&gt;： 
  &lt;ul&gt; 
   &lt;li&gt;如果 7 天都忘了签到，或许这些资讯对你来说并非刚需。适时的暂停，能帮你从信息流中抽离，给大脑留出喘息的空间。&lt;/li&gt; 
   &lt;li&gt;GitHub Actions 是宝贵的公共计算资源。引入签到机制旨在避免算力的无效空转，确保资源能分配给真正活跃且需要的用户。感谢你的理解与支持。&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;📌 重要说明（请务必仔细阅读）：&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;一个 Name 对应一个 Secret&lt;/strong&gt;：每添加一个配置项，点击一次"New repository secret"按钮，填写一对"Name"和"Secret"&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;保存后看不到值是正常的&lt;/strong&gt;：出于安全考虑，保存后重新编辑时，只能看到 Name（名称），看不到 Secret（值）的内容&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;严禁自创名称&lt;/strong&gt;：Secret 的 Name（名称）必须&lt;strong&gt;严格使用&lt;/strong&gt;下方列出的名称（如 &lt;code&gt;WEWORK_WEBHOOK_URL&lt;/code&gt;、&lt;code&gt;FEISHU_WEBHOOK_URL&lt;/code&gt; 等），不能自己随意修改或创造新名称，否则系统无法识别&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;可以同时配置多个平台&lt;/strong&gt;：系统会向所有配置的平台发送通知&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;👉 点击展开：&lt;strong&gt;轻量模式 vs 完整模式 + AI分析&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;strong&gt;两种部署模式：&lt;/strong&gt;&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;模式&lt;/th&gt; 
    &lt;th&gt;配置要求&lt;/th&gt; 
    &lt;th&gt;功能范围&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;轻量模式&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;无需配置存储&lt;/td&gt; 
    &lt;td&gt;实时抓取 + 关键词筛选 + 多渠道推送&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;完整模式&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;配置远程云存储&lt;/td&gt; 
    &lt;td&gt;轻量模式 + 新增检测 + 趋势追踪 + 增量推送 + AI分析&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;p&gt;&lt;strong&gt;轻量模式说明&lt;/strong&gt;：&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;✅ 可用：实时新闻抓取、关键词筛选、热点权重排序、当前榜单推送&lt;/li&gt; 
  &lt;li&gt;❌ 不可用：新增新闻检测(🆕)、热度趋势追踪、增量模式、每日汇总累积、MCP AI分析&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;完整模式说明&lt;/strong&gt;： 配置远程云存储后解锁全部功能（见下方 &lt;strong&gt;推荐配置：远程云存储&lt;/strong&gt;）&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;🚀 推荐：Docker 部署&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;如需长期稳定运行，建议使用 &lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#6-docker-%E9%83%A8%E7%BD%B2"&gt;Docker 部署&lt;/a&gt;，数据存储在本地，无需签到，不过需要额外付费购买云服务器。&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;👉 点击展开：&lt;strong&gt;多账号推送说明（v3.5.0 新增）&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;支持多账号配置&lt;/strong&gt;：所有推送渠道（飞书、钉钉、企业微信、Telegram、ntfy、Bark、Slack）均支持配置多个账号&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;配置方式&lt;/strong&gt;：使用英文分号 &lt;code&gt;;&lt;/code&gt; 分隔多个账号值&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;示例&lt;/strong&gt;：&lt;code&gt;FEISHU_WEBHOOK_URL&lt;/code&gt; 的 Secret 值填写 &lt;code&gt;https://webhook1;https://webhook2&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;配对配置&lt;/strong&gt;：Telegram 和 ntfy 需要保证配对参数数量一致（如 token 和 chat_id 都是 2 个）&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;数量限制&lt;/strong&gt;：默认每个渠道最多 3 个账号，超出部分被截断&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;多账号配置示例&lt;/strong&gt;：&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Name（名称）&lt;/th&gt; 
    &lt;th&gt;Secret（值）示例&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;FEISHU_WEBHOOK_URL&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;https://webhook1;https://webhook2;https://webhook3&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;TELEGRAM_BOT_TOKEN&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;token1;token2&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;TELEGRAM_CHAT_ID&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;chatid1;chatid2&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;NTFY_TOPIC&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;topic1;topic2&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;NTFY_TOKEN&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;;token2&lt;/code&gt;（第一个无 token 时留空占位）&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;p&gt;&lt;strong&gt;配置示例：&lt;/strong&gt;&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/sansan0/TrendRadar/master/_image/secrets.png" alt="GitHub Secrets 配置示例" /&gt; 
&lt;p&gt;如上图所示，每一行是一个配置项：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Name（名称）&lt;/strong&gt;：必须使用下方展开内容中列出的固定名称（如 &lt;code&gt;WEWORK_WEBHOOK_URL&lt;/code&gt;）&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Secret（值）&lt;/strong&gt;：填写你从对应平台获取的实际内容（如 Webhook 地址、Token 等）&lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;details&gt; 
 &lt;summary&gt;👉 点击展开：&lt;strong&gt;推荐配置：远程云存储&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;strong&gt;⚠️ 以 Cloudflare R2 为例的配置前置条件：&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;根据 Cloudflare 平台规则，开通 R2 需绑定支付方式。&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;目的&lt;/strong&gt;：仅作身份验证（Verify Only），&lt;strong&gt;不产生扣费&lt;/strong&gt;。&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;支付&lt;/strong&gt;：支持双币信用卡或国区 PayPal。&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;用量&lt;/strong&gt;：R2 的免费额度（10GB存储/月）足以覆盖本项目日常运行，无需担心付费。&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;hr /&gt; 
 &lt;p&gt;&lt;strong&gt;GitHub Secret 配置：&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;必需配置（4 项）：&lt;/strong&gt;&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Name（名称）&lt;/th&gt; 
    &lt;th&gt;Secret（值）说明&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;S3_BUCKET_NAME&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;存储桶名称（如 &lt;code&gt;trendradar-data&lt;/code&gt;）&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;S3_ACCESS_KEY_ID&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;访问密钥 ID（Access Key ID）&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;S3_SECRET_ACCESS_KEY&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;访问密钥（Secret Access Key）&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;S3_ENDPOINT_URL&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;S3 API 端点（如 R2：&lt;code&gt;https://&amp;lt;account-id&amp;gt;.r2.cloudflarestorage.com&lt;/code&gt;）&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;p&gt;&lt;strong&gt;可选配置：&lt;/strong&gt;&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Name（名称）&lt;/th&gt; 
    &lt;th&gt;Secret（值）说明&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;S3_REGION&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;区域（默认 &lt;code&gt;auto&lt;/code&gt;，部分服务商可能需要指定）&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;💡 &lt;strong&gt;更多存储配置选项&lt;/strong&gt;：参见 &lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#11-%E5%AD%98%E5%82%A8%E9%85%8D%E7%BD%AE"&gt;存储配置详解&lt;/a&gt;&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;strong&gt;如何获取凭据（以 Cloudflare R2 为例）：&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;进入 R2 概览&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;登录 &lt;a href="https://dash.cloudflare.com/"&gt;Cloudflare Dashboard&lt;/a&gt;。&lt;/li&gt; 
    &lt;li&gt;在左侧侧边栏找到并点击 &lt;code&gt;R2对象存储&lt;/code&gt;。&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;创建存储桶&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;点击&lt;code&gt;概述&lt;/code&gt;&lt;/li&gt; 
    &lt;li&gt;点击右上角的 &lt;code&gt;创建存储桶&lt;/code&gt; (Create bucket)。&lt;/li&gt; 
    &lt;li&gt;输入名称（例如 &lt;code&gt;trendradar-data&lt;/code&gt;），点击 &lt;code&gt;创建存储桶&lt;/code&gt;。&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;创建 API 令牌&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;回到 &lt;strong&gt;概述&lt;/strong&gt;页面。&lt;/li&gt; 
    &lt;li&gt;点击&lt;strong&gt;右下角&lt;/strong&gt; &lt;code&gt;Account Details &lt;/code&gt;找到并点击 &lt;code&gt;Manage&lt;/code&gt; (Manage R2 API Tokens)。&lt;/li&gt; 
    &lt;li&gt;同时你会看到 &lt;code&gt;S3 API&lt;/code&gt;：&lt;code&gt;https://&amp;lt;account-id&amp;gt;.r2.cloudflarestorage.com&lt;/code&gt;(这就是 S3_ENDPOINT_URL)&lt;/li&gt; 
    &lt;li&gt;点击 &lt;code&gt;创建 Account APl 令牌&lt;/code&gt; 。&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;⚠️ 关键设置&lt;/strong&gt;： 
     &lt;ul&gt; 
      &lt;li&gt;&lt;strong&gt;令牌名称&lt;/strong&gt;：随意填写（如 &lt;code&gt;github-action-write&lt;/code&gt;）。&lt;/li&gt; 
      &lt;li&gt;&lt;strong&gt;权限&lt;/strong&gt;：选择 &lt;code&gt;管理员读和写&lt;/code&gt; 。&lt;/li&gt; 
      &lt;li&gt;&lt;strong&gt;指定存储桶&lt;/strong&gt;：为了安全，建议选择 &lt;code&gt;仅适用于指定存储桶&lt;/code&gt; 并选中你的桶（如 &lt;code&gt;trendradar-data&lt;/code&gt;）。&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
    &lt;li&gt;点击 &lt;code&gt;创建 API 令牌&lt;/code&gt;，&lt;strong&gt;立即复制&lt;/strong&gt; 显示的 &lt;code&gt;Access Key ID&lt;/code&gt; 和 &lt;code&gt;Secret Access Key&lt;/code&gt;（只显示一次！）。&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt;
 &lt;/ol&gt;
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;👉 点击展开：&lt;strong&gt;企业微信机器人&lt;/strong&gt;（配置最简单最迅速）&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;strong&gt;GitHub Secret 配置（⚠️ Name 名称必须严格一致）：&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Name（名称）&lt;/strong&gt;：&lt;code&gt;WEWORK_WEBHOOK_URL&lt;/code&gt;（请复制粘贴此名称，不要手打，避免打错）&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Secret（值）&lt;/strong&gt;：你的企业微信机器人 Webhook 地址&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;strong&gt;机器人设置步骤：&lt;/strong&gt;&lt;/p&gt; 
 &lt;h4&gt;手机端设置：&lt;/h4&gt; 
 &lt;ol&gt; 
  &lt;li&gt;打开企业微信 App → 进入目标内部群聊&lt;/li&gt; 
  &lt;li&gt;点击右上角"…"按钮 → 选择"消息推送"&lt;/li&gt; 
  &lt;li&gt;点击"添加" → 名称输入"TrendRadar"&lt;/li&gt; 
  &lt;li&gt;复制 Webhook 地址，点击保存，复制的内容配置到上方的 GitHub Secret 中&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;h4&gt;PC 端设置流程类似&lt;/h4&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;👉 点击展开：&lt;strong&gt;个人微信推送&lt;/strong&gt;（基于企业微信应用，推送到个人微信）&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;由于该方案是基于企业微信的插件机制，推送样式为纯文本（无 markdown 格式），但可以直接推送到个人微信，无需安装企业微信 App。&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;GitHub Secret 配置（⚠️ Name 名称必须严格一致）：&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Name（名称）&lt;/strong&gt;：&lt;code&gt;WEWORK_WEBHOOK_URL&lt;/code&gt;（请复制粘贴此名称，不要手打）&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Secret（值）&lt;/strong&gt;：你的企业微信应用 Webhook 地址&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Name（名称）&lt;/strong&gt;：&lt;code&gt;WEWORK_MSG_TYPE&lt;/code&gt;（请复制粘贴此名称，不要手打）&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Secret（值）&lt;/strong&gt;：&lt;code&gt;text&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;strong&gt;设置步骤：&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;完成上方的企业微信机器人 Webhook 设置&lt;/li&gt; 
  &lt;li&gt;添加 &lt;code&gt;WEWORK_MSG_TYPE&lt;/code&gt; Secret，值设为 &lt;code&gt;text&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;按照下面图片操作，关联个人微信&lt;/li&gt; 
  &lt;li&gt;配置好后，手机上的企业微信 App 可以删除&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;img src="https://raw.githubusercontent.com/sansan0/TrendRadar/master/_image/wework.png" title="个人微信推送配置" /&gt; 
 &lt;p&gt;&lt;strong&gt;说明&lt;/strong&gt;：&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;与企业微信机器人使用相同的 Webhook 地址&lt;/li&gt; 
  &lt;li&gt;区别在于消息格式：&lt;code&gt;text&lt;/code&gt; 为纯文本，&lt;code&gt;markdown&lt;/code&gt; 为富文本（默认）&lt;/li&gt; 
  &lt;li&gt;纯文本格式会自动去除所有 markdown 语法（粗体、链接等）&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;👉 点击展开：&lt;strong&gt;飞书机器人&lt;/strong&gt;（消息显示最友好）&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;strong&gt;GitHub Secret 配置（⚠️ Name 名称必须严格一致）：&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Name（名称）&lt;/strong&gt;：&lt;code&gt;FEISHU_WEBHOOK_URL&lt;/code&gt;（请复制粘贴此名称，不要手打）&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Secret（值）&lt;/strong&gt;：你的飞书机器人 Webhook 地址（该链接开头类似 &lt;a href="https://www.feishu.cn/flow/api/trigger-webhook/********%EF%BC%89"&gt;https://www.feishu.cn/flow/api/trigger-webhook/********）&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;br /&gt; 
 &lt;p&gt;有两个方案，&lt;strong&gt;方案一&lt;/strong&gt;配置简单，&lt;strong&gt;方案二&lt;/strong&gt;配置复杂(但是稳定推送)&lt;/p&gt; 
 &lt;p&gt;其中方案一，由 &lt;strong&gt;ziventian&lt;/strong&gt;发现并提供建议，在这里感谢他，默认是个人推送，也可以配置群组推送操作&lt;a href="https://github.com/sansan0/TrendRadar/issues/97"&gt;#97&lt;/a&gt; ，&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;方案一：&lt;/strong&gt;&lt;/p&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;对部分人存在额外操作，否则会报"系统错误"。需要手机端搜索下机器人，然后开启飞书机器人应用(该建议来自于网友，可参考)&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;电脑浏览器打开 &lt;a href="https://botbuilder.feishu.cn/home/my-command"&gt;https://botbuilder.feishu.cn/home/my-command&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;点击"新建机器人指令"&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;点击"选择触发器"，往下滑动，点击"Webhook 触发"&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;此时你会看到"Webhook 地址"，把这个链接先复制到本地记事本暂存，继续接下来的操作&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;"参数"里面放上下面的内容，然后点击"完成"&lt;/p&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
  "message_type": "text",
  "content": {
    "total_titles": "{{内容}}",
    "timestamp": "{{内容}}",
    "report_type": "{{内容}}",
    "text": "{{内容}}"
  }
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="6"&gt; 
  &lt;li&gt; &lt;p&gt;点击"选择操作" &amp;gt; "通过官方机器人发消息"&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;消息标题填写"TrendRadar 热点监控"&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;最关键的部分来了，点击 + 按钮，选择"Webhook 触发"，然后按照下面的图片摆放&lt;/p&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/sansan0/TrendRadar/master/_image/image.png" alt="飞书机器人配置示例" /&gt;&lt;/p&gt; 
 &lt;ol start="9"&gt; 
  &lt;li&gt;配置完成后，将第 4 步复制的 Webhook 地址配置到 GitHub Secrets 中的 &lt;code&gt;FEISHU_WEBHOOK_URL&lt;/code&gt;&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;strong&gt;方案二：&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;电脑浏览器打开 &lt;a href="https://botbuilder.feishu.cn/home/my-app"&gt;https://botbuilder.feishu.cn/home/my-app&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;点击"新建机器人应用"&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;进入创建的应用后，点击"流程涉及" &amp;gt; "创建流程" &amp;gt; "选择触发器"&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;往下滑动，点击"Webhook 触发"&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;此时你会看到"Webhook 地址"，把这个链接先复制到本地记事本暂存，继续接下来的操作&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;"参数"里面放上下面的内容，然后点击"完成"&lt;/p&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
  "message_type": "text",
  "content": {
    "total_titles": "{{内容}}",
    "timestamp": "{{内容}}",
    "report_type": "{{内容}}",
    "text": "{{内容}}"
  }
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="7"&gt; 
  &lt;li&gt; &lt;p&gt;点击"选择操作" &amp;gt; "发送飞书消息"，勾选 "群消息"，然后点击下面的输入框，点击"我管理的群组"（如果没有群组，你可以在飞书 app 上创建群组）&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;消息标题填写"TrendRadar 热点监控"&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;最关键的部分来了，点击 + 按钮，选择"Webhook 触发"，然后按照下面的图片摆放&lt;/p&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/sansan0/TrendRadar/master/_image/image.png" alt="飞书机器人配置示例" /&gt;&lt;/p&gt; 
 &lt;ol start="10"&gt; 
  &lt;li&gt;配置完成后，将第 5 步复制的 Webhook 地址配置到 GitHub Secrets 中的 &lt;code&gt;FEISHU_WEBHOOK_URL&lt;/code&gt;&lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;👉 点击展开：&lt;strong&gt;钉钉机器人&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;strong&gt;GitHub Secret 配置（⚠️ Name 名称必须严格一致）：&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Name（名称）&lt;/strong&gt;：&lt;code&gt;DINGTALK_WEBHOOK_URL&lt;/code&gt;（请复制粘贴此名称，不要手打）&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Secret（值）&lt;/strong&gt;：你的钉钉机器人 Webhook 地址&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;strong&gt;机器人设置步骤：&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;创建机器人（仅 PC 端支持）&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;打开钉钉 PC 客户端，进入目标群聊&lt;/li&gt; 
    &lt;li&gt;点击群设置图标（⚙️）→ 往下翻找到"机器人"点开&lt;/li&gt; 
    &lt;li&gt;选择"添加机器人" → "自定义"&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;配置机器人&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;设置机器人名称&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;安全设置&lt;/strong&gt;： 
     &lt;ul&gt; 
      &lt;li&gt;&lt;strong&gt;自定义关键词&lt;/strong&gt;：设置 "热点"&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;完成设置&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;勾选服务条款协议 → 点击"完成"&lt;/li&gt; 
    &lt;li&gt;复制获得的 Webhook URL&lt;/li&gt; 
    &lt;li&gt;将 URL 配置到 GitHub Secrets 中的 &lt;code&gt;DINGTALK_WEBHOOK_URL&lt;/code&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt;：移动端只能接收消息，无法创建新机器人。&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;👉 点击展开：&lt;strong&gt;Telegram Bot&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;strong&gt;GitHub Secret 配置（⚠️ Name 名称必须严格一致）：&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Name（名称）&lt;/strong&gt;：&lt;code&gt;TELEGRAM_BOT_TOKEN&lt;/code&gt;（请复制粘贴此名称，不要手打）&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Secret（值）&lt;/strong&gt;：你的 Telegram Bot Token&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Name（名称）&lt;/strong&gt;：&lt;code&gt;TELEGRAM_CHAT_ID&lt;/code&gt;（请复制粘贴此名称，不要手打）&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Secret（值）&lt;/strong&gt;：你的 Telegram Chat ID&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;说明&lt;/strong&gt;：Telegram 需要配置&lt;strong&gt;两个&lt;/strong&gt; Secret，请分别点击两次"New repository secret"按钮添加&lt;/p&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;strong&gt;机器人设置步骤：&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;创建机器人&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;在 Telegram 中搜索 &lt;code&gt;@BotFather&lt;/code&gt;（大小写注意，有蓝色徽章勾勾，有类似 37849827 monthly users，这个才是官方的，有一些仿官方的账号注意辨别）&lt;/li&gt; 
    &lt;li&gt;发送 &lt;code&gt;/newbot&lt;/code&gt; 命令创建新机器人&lt;/li&gt; 
    &lt;li&gt;设置机器人名称（必须以"bot"结尾，很容易遇到重复名字，所以你要绞尽脑汁想不同的名字）&lt;/li&gt; 
    &lt;li&gt;获取 Bot Token（格式如：&lt;code&gt;123456789:AAHfiqksKZ8WmR2zSjiQ7_v4TMAKdiHm9T0&lt;/code&gt;）&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;获取 Chat ID&lt;/strong&gt;：&lt;/p&gt; &lt;p&gt;&lt;strong&gt;方法一：通过官方 API 获取&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;先向你的机器人发送一条消息&lt;/li&gt; 
    &lt;li&gt;访问：&lt;code&gt;https://api.telegram.org/bot&amp;lt;你的Bot Token&amp;gt;/getUpdates&lt;/code&gt;&lt;/li&gt; 
    &lt;li&gt;在返回的 JSON 中找到 &lt;code&gt;"chat":{"id":数字}&lt;/code&gt; 中的数字&lt;/li&gt; 
   &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;方法二：使用第三方工具&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;搜索 &lt;code&gt;@userinfobot&lt;/code&gt; 并发送 &lt;code&gt;/start&lt;/code&gt;&lt;/li&gt; 
    &lt;li&gt;获取你的用户 ID 作为 Chat ID&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;配置到 GitHub&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;code&gt;TELEGRAM_BOT_TOKEN&lt;/code&gt;：填入第 1 步获得的 Bot Token&lt;/li&gt; 
    &lt;li&gt;&lt;code&gt;TELEGRAM_CHAT_ID&lt;/code&gt;：填入第 2 步获得的 Chat ID&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;👉 点击展开：&lt;strong&gt;邮件推送&lt;/strong&gt;（支持所有主流邮箱）&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;ul&gt; 
  &lt;li&gt;注意事项：为防止邮件群发功能被&lt;strong&gt;滥用&lt;/strong&gt;，当前的群发是所有收件人都能看到彼此的邮箱地址。&lt;/li&gt; 
  &lt;li&gt;如果你没有过配置下面这种邮箱发送的经历，不建议尝试&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;⚠️ &lt;strong&gt;重要配置依赖&lt;/strong&gt;：邮件推送需要 HTML 报告文件。请确保 &lt;code&gt;config/config.yaml&lt;/code&gt; 中的 &lt;code&gt;formats.html&lt;/code&gt; 设置为 &lt;code&gt;true&lt;/code&gt;：&lt;/p&gt; 
  &lt;pre&gt;&lt;code class="language-yaml"&gt;formats:
  sqlite: true
  txt: false
  html: true   # 必须启用，否则邮件推送会失败
&lt;/code&gt;&lt;/pre&gt; 
  &lt;p&gt;如果设置为 &lt;code&gt;false&lt;/code&gt;，邮件推送时会报错：&lt;code&gt;错误：HTML文件不存在或未提供: None&lt;/code&gt;&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;strong&gt;GitHub Secret 配置（⚠️ Name 名称必须严格一致）：&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Name（名称）&lt;/strong&gt;：&lt;code&gt;EMAIL_FROM&lt;/code&gt;（请复制粘贴此名称，不要手打）&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Secret（值）&lt;/strong&gt;：发件人邮箱地址&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Name（名称）&lt;/strong&gt;：&lt;code&gt;EMAIL_PASSWORD&lt;/code&gt;（请复制粘贴此名称，不要手打）&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Secret（值）&lt;/strong&gt;：邮箱密码或授权码&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Name（名称）&lt;/strong&gt;：&lt;code&gt;EMAIL_TO&lt;/code&gt;（请复制粘贴此名称，不要手打）&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Secret（值）&lt;/strong&gt;：收件人邮箱地址（多个收件人用英文逗号分隔，也可以和 EMAIL_FROM 一样，自己发送给自己）&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Name（名称）&lt;/strong&gt;：&lt;code&gt;EMAIL_SMTP_SERVER&lt;/code&gt;（可选配置，请复制粘贴此名称）&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Secret（值）&lt;/strong&gt;：SMTP服务器地址（可留空，系统会自动识别）&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Name（名称）&lt;/strong&gt;：&lt;code&gt;EMAIL_SMTP_PORT&lt;/code&gt;（可选配置，请复制粘贴此名称）&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Secret（值）&lt;/strong&gt;：SMTP端口（可留空，系统会自动识别）&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;说明&lt;/strong&gt;：邮件推送需要配置至少&lt;strong&gt;3个必需&lt;/strong&gt; Secret（EMAIL_FROM、EMAIL_PASSWORD、EMAIL_TO），后两个为可选配置&lt;/p&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;strong&gt;支持的邮箱服务商&lt;/strong&gt;（自动识别 SMTP 配置）：&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;邮箱服务商&lt;/th&gt; 
    &lt;th&gt;域名&lt;/th&gt; 
    &lt;th&gt;SMTP 服务器&lt;/th&gt; 
    &lt;th&gt;端口&lt;/th&gt; 
    &lt;th&gt;加密方式&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Gmail&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;gmail.com&lt;/td&gt; 
    &lt;td&gt;smtp.gmail.com&lt;/td&gt; 
    &lt;td&gt;587&lt;/td&gt; 
    &lt;td&gt;TLS&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;QQ邮箱&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;qq.com&lt;/td&gt; 
    &lt;td&gt;smtp.qq.com&lt;/td&gt; 
    &lt;td&gt;465&lt;/td&gt; 
    &lt;td&gt;SSL&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Outlook&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;outlook.com&lt;/td&gt; 
    &lt;td&gt;smtp-mail.outlook.com&lt;/td&gt; 
    &lt;td&gt;587&lt;/td&gt; 
    &lt;td&gt;TLS&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Hotmail&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;hotmail.com&lt;/td&gt; 
    &lt;td&gt;smtp-mail.outlook.com&lt;/td&gt; 
    &lt;td&gt;587&lt;/td&gt; 
    &lt;td&gt;TLS&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Live&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;live.com&lt;/td&gt; 
    &lt;td&gt;smtp-mail.outlook.com&lt;/td&gt; 
    &lt;td&gt;587&lt;/td&gt; 
    &lt;td&gt;TLS&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;163邮箱&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;163.com&lt;/td&gt; 
    &lt;td&gt;smtp.163.com&lt;/td&gt; 
    &lt;td&gt;465&lt;/td&gt; 
    &lt;td&gt;SSL&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;126邮箱&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;126.com&lt;/td&gt; 
    &lt;td&gt;smtp.126.com&lt;/td&gt; 
    &lt;td&gt;465&lt;/td&gt; 
    &lt;td&gt;SSL&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;新浪邮箱&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;sina.com&lt;/td&gt; 
    &lt;td&gt;smtp.sina.com&lt;/td&gt; 
    &lt;td&gt;465&lt;/td&gt; 
    &lt;td&gt;SSL&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;搜狐邮箱&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;sohu.com&lt;/td&gt; 
    &lt;td&gt;smtp.sohu.com&lt;/td&gt; 
    &lt;td&gt;465&lt;/td&gt; 
    &lt;td&gt;SSL&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;天翼邮箱&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;189.cn&lt;/td&gt; 
    &lt;td&gt;smtp.189.cn&lt;/td&gt; 
    &lt;td&gt;465&lt;/td&gt; 
    &lt;td&gt;SSL&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;阿里云邮箱&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;aliyun.com&lt;/td&gt; 
    &lt;td&gt;smtp.aliyun.com&lt;/td&gt; 
    &lt;td&gt;465&lt;/td&gt; 
    &lt;td&gt;TLS&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Yandex邮箱&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;yandex.com&lt;/td&gt; 
    &lt;td&gt;smtp.yandex.com&lt;/td&gt; 
    &lt;td&gt;465&lt;/td&gt; 
    &lt;td&gt;TLS&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;&lt;strong&gt;自动识别&lt;/strong&gt;：使用以上邮箱时，无需手动配置 &lt;code&gt;EMAIL_SMTP_SERVER&lt;/code&gt; 和 &lt;code&gt;EMAIL_SMTP_PORT&lt;/code&gt;，系统会自动识别。&lt;/p&gt; 
  &lt;p&gt;&lt;strong&gt;反馈说明&lt;/strong&gt;：&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;如果你使用&lt;strong&gt;其他邮箱&lt;/strong&gt;测试成功，欢迎开 &lt;a href="https://github.com/sansan0/TrendRadar/issues"&gt;Issues&lt;/a&gt; 告知，我会添加到支持列表&lt;/li&gt; 
   &lt;li&gt;如果上述邮箱配置有误或无法使用，也请开 &lt;a href="https://github.com/sansan0/TrendRadar/issues"&gt;Issues&lt;/a&gt; 反馈，帮助改进项目&lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;p&gt;&lt;strong&gt;特别感谢&lt;/strong&gt;：&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;感谢 &lt;a href="https://github.com/DYZYD"&gt;@DYZYD&lt;/a&gt; 贡献天翼邮箱（189.cn）配置并完成自发自收测试 (&lt;a href="https://github.com/sansan0/TrendRadar/issues/291"&gt;#291&lt;/a&gt;)&lt;/li&gt; 
   &lt;li&gt;感谢 &lt;a href="https://github.com/longzhenren"&gt;@longzhenren&lt;/a&gt; 贡献阿里云邮箱（aliyun.com）配置并完成测试 (&lt;a href="https://github.com/sansan0/TrendRadar/issues/344"&gt;#344&lt;/a&gt;)&lt;/li&gt; 
   &lt;li&gt;感谢 &lt;a href="https://github.com/ACANX"&gt;@ACANX&lt;/a&gt; 贡献 Yandex 邮箱（yandex.com）配置并完成测试 (&lt;a href="https://github.com/sansan0/TrendRadar/issues/663"&gt;#663&lt;/a&gt;)&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;常见邮箱设置：&lt;/strong&gt;&lt;/p&gt; 
 &lt;h4&gt;QQ邮箱：&lt;/h4&gt; 
 &lt;ol&gt; 
  &lt;li&gt;登录 QQ邮箱网页版 → 设置 → 账户&lt;/li&gt; 
  &lt;li&gt;开启 POP3/SMTP 服务&lt;/li&gt; 
  &lt;li&gt;生成授权码（16位字母）&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;EMAIL_PASSWORD&lt;/code&gt; 填写授权码，而非 QQ 密码&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;h4&gt;Gmail：&lt;/h4&gt; 
 &lt;ol&gt; 
  &lt;li&gt;开启两步验证&lt;/li&gt; 
  &lt;li&gt;生成应用专用密码&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;EMAIL_PASSWORD&lt;/code&gt; 填写应用专用密码&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;h4&gt;163/126邮箱：&lt;/h4&gt; 
 &lt;ol&gt; 
  &lt;li&gt;登录网页版 → 设置 → POP3/SMTP/IMAP&lt;/li&gt; 
  &lt;li&gt;开启 SMTP 服务&lt;/li&gt; 
  &lt;li&gt;设置客户端授权码&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;EMAIL_PASSWORD&lt;/code&gt; 填写授权码&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;strong&gt;高级配置&lt;/strong&gt;： 如果自动识别失败，可手动配置 SMTP：&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;EMAIL_SMTP_SERVER&lt;/code&gt;：如 smtp.gmail.com&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;EMAIL_SMTP_PORT&lt;/code&gt;：如 587（TLS）或 465（SSL）&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;strong&gt;如果有多个收件人(注意是英文逗号分隔)&lt;/strong&gt;：&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;EMAIL_TO="&lt;a href="mailto:user1@example.com"&gt;user1@example.com&lt;/a&gt;,&lt;a href="mailto:user2@example.com"&gt;user2@example.com&lt;/a&gt;,&lt;a href="mailto:user3@example.com"&gt;user3@example.com&lt;/a&gt;"&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;👉 点击展开：&lt;strong&gt;ntfy 推送&lt;/strong&gt;（开源免费，支持自托管）&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;strong&gt;两种使用方式：&lt;/strong&gt;&lt;/p&gt; 
 &lt;h3&gt;方式一：免费使用（推荐新手） 🆓&lt;/h3&gt; 
 &lt;p&gt;&lt;strong&gt;特点&lt;/strong&gt;：&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;✅ 无需注册账号，立即使用&lt;/li&gt; 
  &lt;li&gt;✅ 每天 250 条消息（足够 90% 用户）&lt;/li&gt; 
  &lt;li&gt;✅ Topic 名称即"密码"（需选择不易猜测的名称）&lt;/li&gt; 
  &lt;li&gt;⚠️ 消息未加密，不适合敏感信息, 但适合我们这个项目的不敏感信息&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;快速开始：&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;下载 ntfy 应用&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Android：&lt;a href="https://play.google.com/store/apps/details?id=io.heckel.ntfy"&gt;Google Play&lt;/a&gt; / &lt;a href="https://f-droid.org/en/packages/io.heckel.ntfy/"&gt;F-Droid&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;iOS：&lt;a href="https://apps.apple.com/us/app/ntfy/id1625396347"&gt;App Store&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;桌面：访问 &lt;a href="https://ntfy.sh"&gt;ntfy.sh&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;订阅主题&lt;/strong&gt;（选择一个难猜的名称）：&lt;/p&gt; &lt;pre&gt;&lt;code&gt;建议格式：trendradar-{你的名字缩写}-{随机数字}

不能使用中文

✅ 好例子：trendradar-zs-8492
❌ 坏例子：news、alerts（太容易被猜到）
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;配置 GitHub Secret（⚠️ Name 名称必须严格一致）&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt; &lt;p&gt;&lt;strong&gt;Name（名称）&lt;/strong&gt;：&lt;code&gt;NTFY_TOPIC&lt;/code&gt;（请复制粘贴此名称，不要手打）&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;&lt;strong&gt;Secret（值）&lt;/strong&gt;：填写你刚才订阅的主题名称&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;&lt;strong&gt;Name（名称）&lt;/strong&gt;：&lt;code&gt;NTFY_SERVER_URL&lt;/code&gt;（可选配置，请复制粘贴此名称）&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;&lt;strong&gt;Secret（值）&lt;/strong&gt;：留空（默认使用 ntfy.sh）&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;&lt;strong&gt;Name（名称）&lt;/strong&gt;：&lt;code&gt;NTFY_TOKEN&lt;/code&gt;（可选配置，请复制粘贴此名称）&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;&lt;strong&gt;Secret（值）&lt;/strong&gt;：留空&lt;/p&gt; &lt;/li&gt; 
   &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;说明&lt;/strong&gt;：ntfy 至少需要配置 1 个必需 Secret (NTFY_TOPIC)，后两个为可选配置&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;测试&lt;/strong&gt;：&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;curl -d "测试消息" ntfy.sh/你的主题名称
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;hr /&gt; 
 &lt;h3&gt;方式二：自托管（完全隐私控制） 🔒&lt;/h3&gt; 
 &lt;p&gt;&lt;strong&gt;适合人群&lt;/strong&gt;：有服务器、追求完全隐私、技术能力强&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;优势&lt;/strong&gt;：&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;✅ 完全开源（Apache 2.0 + GPLv2）&lt;/li&gt; 
  &lt;li&gt;✅ 数据完全自主控制&lt;/li&gt; 
  &lt;li&gt;✅ 无任何限制&lt;/li&gt; 
  &lt;li&gt;✅ 零费用&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;Docker 一键部署&lt;/strong&gt;：&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;docker run -d \
  --name ntfy \
  -p 80:80 \
  -v /var/cache/ntfy:/var/cache/ntfy \
  binwiederhier/ntfy \
  serve --cache-file /var/cache/ntfy/cache.db
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;配置 TrendRadar&lt;/strong&gt;：&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-yaml"&gt;NTFY_SERVER_URL: https://ntfy.yourdomain.com
NTFY_TOPIC: trendradar-alerts  # 自托管可用简单名称
NTFY_TOKEN: tk_your_token  # 可选：启用访问控制
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;在应用中订阅&lt;/strong&gt;：&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;点击"Use another server"&lt;/li&gt; 
  &lt;li&gt;输入你的服务器地址&lt;/li&gt; 
  &lt;li&gt;输入主题名称&lt;/li&gt; 
  &lt;li&gt;（可选）输入登录凭据&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;hr /&gt; 
 &lt;p&gt;&lt;strong&gt;常见问题：&lt;/strong&gt;&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;&lt;strong&gt;Q1: 免费版够用吗？&lt;/strong&gt;&lt;/summary&gt; 
  &lt;p&gt;每天 250 条消息对大多数用户足够。按 30 分钟抓取一次计算，每天约 48 次推送，完全够用。&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;&lt;strong&gt;Q2: Topic 名称真的安全吗？&lt;/strong&gt;&lt;/summary&gt; 
  &lt;p&gt;如果你选择随机的、足够长的名称（如 &lt;code&gt;trendradar-zs-8492-news&lt;/code&gt;），暴力破解几乎不可能：&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;ntfy 有严格的速率限制（1 秒 1 次请求）&lt;/li&gt; 
   &lt;li&gt;64 个字符选择（A-Z, a-z, 0-9, _, -）&lt;/li&gt; 
   &lt;li&gt;10 位随机字符串有 64^10 种可能性（需要数年才能破解）&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;hr /&gt; 
 &lt;p&gt;&lt;strong&gt;推荐选择：&lt;/strong&gt;&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;用户类型&lt;/th&gt; 
    &lt;th&gt;推荐方案&lt;/th&gt; 
    &lt;th&gt;理由&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;普通用户&lt;/td&gt; 
    &lt;td&gt;方式一（免费）&lt;/td&gt; 
    &lt;td&gt;简单快速，够用&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;技术用户&lt;/td&gt; 
    &lt;td&gt;方式二（自托管）&lt;/td&gt; 
    &lt;td&gt;完全控制，无限制&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;高频用户&lt;/td&gt; 
    &lt;td&gt;方式三（付费）&lt;/td&gt; 
    &lt;td&gt;这个自己去官网看吧&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;p&gt;&lt;strong&gt;相关链接：&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://docs.ntfy.sh/"&gt;ntfy 官方文档&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://docs.ntfy.sh/install/"&gt;自托管教程&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/binwiederhier/ntfy"&gt;GitHub 仓库&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;👉 点击展开：&lt;strong&gt;Bark 推送&lt;/strong&gt;（iOS 专属，简洁高效）&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;strong&gt;GitHub Secret 配置（⚠️ Name 名称必须严格一致）：&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Name（名称）&lt;/strong&gt;：&lt;code&gt;BARK_URL&lt;/code&gt;（请复制粘贴此名称，不要手打）&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Secret（值）&lt;/strong&gt;：你的 Bark 推送 URL&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;strong&gt;Bark 简介：&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;Bark 是一款 iOS 平台的免费开源推送工具，特点是简单、快速、无广告。&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;使用方式：&lt;/strong&gt;&lt;/p&gt; 
 &lt;h3&gt;方式一：使用官方服务器（推荐新手） 🆓&lt;/h3&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;下载 Bark App&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;iOS：&lt;a href="https://apps.apple.com/cn/app/bark-%E7%BB%99%E4%BD%A0%E7%9A%84%E6%89%8B%E6%9C%BA%E5%8F%91%E6%8E%A8%E9%80%81/id1403753865"&gt;App Store&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;获取推送 URL&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;打开 Bark App&lt;/li&gt; 
    &lt;li&gt;复制首页显示的推送 URL（格式如：&lt;code&gt;https://api.day.app/your_device_key&lt;/code&gt;）&lt;/li&gt; 
    &lt;li&gt;将 URL 配置到 GitHub Secrets 中的 &lt;code&gt;BARK_URL&lt;/code&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;h3&gt;方式二：自建服务器（完全隐私控制） 🔒&lt;/h3&gt; 
 &lt;p&gt;&lt;strong&gt;适合人群&lt;/strong&gt;：有服务器、追求完全隐私、技术能力强&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Docker 一键部署&lt;/strong&gt;：&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;docker run -d \
  --name bark-server \
  -p 8080:8080 \
  finab/bark-server
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;配置 TrendRadar&lt;/strong&gt;：&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-yaml"&gt;BARK_URL: http://your-server-ip:8080/your_device_key
&lt;/code&gt;&lt;/pre&gt; 
 &lt;hr /&gt; 
 &lt;p&gt;&lt;strong&gt;注意事项：&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;✅ Bark 使用 APNs 推送，单条消息最大 4KB&lt;/li&gt; 
  &lt;li&gt;✅ 支持自动分批推送，无需担心消息过长&lt;/li&gt; 
  &lt;li&gt;✅ 推送格式为纯文本（自动去除 Markdown 语法）&lt;/li&gt; 
  &lt;li&gt;⚠️ 仅支持 iOS 平台&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;相关链接：&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://bark.day.app/"&gt;Bark 官方网站&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/Finb/Bark"&gt;Bark GitHub 仓库&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/Finb/bark-server"&gt;Bark Server 自建教程&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;👉 点击展开：&lt;strong&gt;Slack 推送&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;strong&gt;GitHub Secret 配置（⚠️ Name 名称必须严格一致）：&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Name（名称）&lt;/strong&gt;：&lt;code&gt;SLACK_WEBHOOK_URL&lt;/code&gt;（请复制粘贴此名称，不要手打）&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Secret（值）&lt;/strong&gt;：你的 Slack Incoming Webhook URL&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;strong&gt;Slack 简介：&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;Slack 是团队协作工具，Incoming Webhooks 可以将消息推送到 Slack 频道。&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;设置步骤：&lt;/strong&gt;&lt;/p&gt; 
 &lt;h3&gt;步骤 1：创建 Slack App&lt;/h3&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;访问 Slack API 页面&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;打开 &lt;a href="https://api.slack.com/apps?new_app=1"&gt;https://api.slack.com/apps?new_app=1&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;如果未登录，先登录你的 Slack 工作空间&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;选择创建方式&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;点击 &lt;strong&gt;"From scratch"&lt;/strong&gt;（从头开始创建）&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;填写 App 信息&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;App Name&lt;/strong&gt;：填写应用名称（如 &lt;code&gt;TrendRadar&lt;/code&gt; 或 &lt;code&gt;热点新闻监控&lt;/code&gt;）&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Workspace&lt;/strong&gt;：从下拉列表选择你的工作空间&lt;/li&gt; 
    &lt;li&gt;点击 &lt;strong&gt;"Create App"&lt;/strong&gt; 按钮&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;h3&gt;步骤 2：启用 Incoming Webhooks&lt;/h3&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;导航到 Incoming Webhooks&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;在左侧菜单中找到并点击 &lt;strong&gt;"Incoming Webhooks"&lt;/strong&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;启用功能&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;找到 &lt;strong&gt;"Activate Incoming Webhooks"&lt;/strong&gt; 开关&lt;/li&gt; 
    &lt;li&gt;将开关从 &lt;code&gt;OFF&lt;/code&gt; 切换到 &lt;code&gt;ON&lt;/code&gt;&lt;/li&gt; 
    &lt;li&gt;页面会自动刷新显示新的配置选项&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;h3&gt;步骤 3：生成 Webhook URL&lt;/h3&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;添加新的 Webhook&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;滚动到页面底部&lt;/li&gt; 
    &lt;li&gt;点击 &lt;strong&gt;"Add New Webhook to Workspace"&lt;/strong&gt; 按钮&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;选择目标频道&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;系统会弹出授权页面&lt;/li&gt; 
    &lt;li&gt;从下拉列表中选择要接收消息的频道（如 &lt;code&gt;#热点新闻&lt;/code&gt;）&lt;/li&gt; 
    &lt;li&gt;⚠️ 如果要选择私有频道，必须先加入该频道&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;授权应用&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;点击 &lt;strong&gt;"Allow"&lt;/strong&gt; 按钮完成授权&lt;/li&gt; 
    &lt;li&gt;系统会自动跳转回配置页面&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;h3&gt;步骤 4：复制并保存 Webhook URL&lt;/h3&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;查看生成的 URL&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;在 "Webhook URLs for Your Workspace" 区域&lt;/li&gt; 
    &lt;li&gt;会看到刚刚生成的 Webhook URL&lt;/li&gt; 
    &lt;li&gt;格式如：&lt;code&gt;https://hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXXXXXXXXXXXXXX&lt;/code&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;复制 URL&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;点击 URL 右侧的 &lt;strong&gt;"Copy"&lt;/strong&gt; 按钮&lt;/li&gt; 
    &lt;li&gt;或手动选中 URL 并复制&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;配置到 TrendRadar&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;GitHub Actions&lt;/strong&gt;：将 URL 添加到 GitHub Secrets 中的 &lt;code&gt;SLACK_WEBHOOK_URL&lt;/code&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;本地测试&lt;/strong&gt;：将 URL 填入 &lt;code&gt;config/config.yaml&lt;/code&gt; 的 &lt;code&gt;slack_webhook_url&lt;/code&gt; 字段&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Docker 部署&lt;/strong&gt;：将 URL 添加到 &lt;code&gt;docker/.env&lt;/code&gt; 文件的 &lt;code&gt;SLACK_WEBHOOK_URL&lt;/code&gt; 变量&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;hr /&gt; 
 &lt;p&gt;&lt;strong&gt;注意事项：&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;✅ 支持 Markdown 格式（自动转换为 Slack mrkdwn）&lt;/li&gt; 
  &lt;li&gt;✅ 支持自动分批推送（每批 4KB）&lt;/li&gt; 
  &lt;li&gt;✅ 适合团队协作，消息集中管理&lt;/li&gt; 
  &lt;li&gt;⚠️ Webhook URL 包含密钥，切勿公开&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;消息格式预览：&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code&gt;*[第 1/2 批次]*

📊 *热点词汇统计*

🔥 *[1/3] AI ChatGPT* : 2 条

  1. [百度热搜] 🆕 ChatGPT-5正式发布 *[1]* - 09时15分 (1次)

  2. [今日头条] AI芯片概念股暴涨 *[3]* - [08时30分 ~ 10时45分] (3次)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;相关链接：&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://api.slack.com/messaging/webhooks"&gt;Slack Incoming Webhooks 官方文档&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://api.slack.com/apps"&gt;Slack API 应用管理&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt;   
&lt;p&gt;3️⃣ &lt;strong&gt;手动测试新闻推送&lt;/strong&gt;：&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;⚠️ 提醒：&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;完成第 1-2 步后，请立即测试！测试成功后再根据需要调整配置（第 4 步）&lt;/li&gt; 
  &lt;li&gt;请进入你自己的项目，不是本项目！&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;如何找到你的 Actions 页面&lt;/strong&gt;：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;方法一&lt;/strong&gt;：打开你 fork 的项目主页，点击顶部的 &lt;strong&gt;Actions&lt;/strong&gt; 标签&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;方法二&lt;/strong&gt;：直接访问 &lt;code&gt;https://github.com/你的用户名/TrendRadar/actions&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;示例对比&lt;/strong&gt;：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;❌ 作者的项目：&lt;code&gt;https://github.com/sansan0/TrendRadar/actions&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;✅ 你的项目：&lt;code&gt;https://github.com/你的用户名/TrendRadar/actions&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;测试步骤&lt;/strong&gt;：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;进入你项目的 Actions 页面&lt;/li&gt; 
 &lt;li&gt;找到 &lt;strong&gt;"Get Hot News"&lt;/strong&gt;(必须得是这个字)点进去，点击右侧的 &lt;strong&gt;"Run workflow"&lt;/strong&gt; 按钮运行 
  &lt;ul&gt; 
   &lt;li&gt;如果看不到该字样，参照 &lt;a href="https://github.com/sansan0/TrendRadar/issues/109"&gt;#109&lt;/a&gt; 解决&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;3 分钟左右，消息会推送到你配置的平台&lt;/li&gt; 
&lt;/ol&gt; 
&lt;br /&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;⚠️ 提醒：&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;手动测试不要太频繁，避免触发 GitHub Actions 限制&lt;/li&gt; 
  &lt;li&gt;点击 Run workflow 后需要刷新浏览器页面才能看到新的运行记录&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;4️⃣ &lt;strong&gt;配置说明（可选）&lt;/strong&gt;：&lt;/p&gt; 
&lt;p&gt;默认配置已可正常使用，如需个性化调整，了解以下三个文件即可：&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;文件&lt;/th&gt; 
   &lt;th&gt;作用&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;config/config.yaml&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;主配置文件：推送模式、时间窗口、平台列表、热点权重等&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;config/frequency_words.txt&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;关键词文件：设置你关心的词汇，筛选推送内容&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;.github/workflows/crawler.yml&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;执行频率：控制多久运行一次（⚠️ 谨慎修改）&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;👉 &lt;strong&gt;详细配置教程&lt;/strong&gt;：&lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#%E9%85%8D%E7%BD%AE%E8%AF%A6%E8%A7%A3"&gt;配置详解&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;5️⃣ &lt;strong&gt;🎉 部署成功！分享你的使用体验&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;恭喜你完成了 TrendRadar 的配置！现在你可以开始追踪热点资讯了。&lt;/p&gt; 
&lt;p&gt;💬 有更多小伙伴在公众号交流使用心得，期待你的分享~&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;想了解更多玩法和高级技巧？&lt;/li&gt; 
 &lt;li&gt;遇到问题需要快速解答？&lt;/li&gt; 
 &lt;li&gt;有好的想法想要交流？&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;👉 欢迎关注公众号「&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#%E9%97%AE%E9%A2%98%E7%AD%94%E7%96%91%E4%B8%8E%E4%BA%A4%E6%B5%81"&gt;硅基茶水间&lt;/a&gt;&lt;/strong&gt;」，你的点赞和留言都是项目持续更新的动力。&lt;/p&gt; 
&lt;p&gt;6️⃣ &lt;strong&gt;想要更智能的分析？试试 AI 增强功能&lt;/strong&gt;（可选）&lt;/p&gt; 
&lt;p&gt;基础配置已经能满足日常使用，但如果你想要：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;让 AI 自动分析热点趋势和数据洞察&lt;/li&gt; 
 &lt;li&gt;通过自然语言搜索和查询新闻&lt;/li&gt; 
 &lt;li&gt;获得情感分析、话题预测等深度分析&lt;/li&gt; 
 &lt;li&gt;在 Claude、Cursor 等 AI 工具中直接调用数据&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;👉 &lt;strong&gt;了解更多&lt;/strong&gt;：&lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#-ai-%E6%99%BA%E8%83%BD%E5%88%86%E6%9E%90"&gt;AI 智能分析&lt;/a&gt; — 解锁项目的隐藏能力，让热点追踪更高效！&lt;/p&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;a name="配置详解"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;⚙️ 配置详解&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;📖 提醒&lt;/strong&gt;：本章节提供详细的配置说明，建议先完成 &lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#-%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B"&gt;快速开始&lt;/a&gt; 的基础配置，再根据需要回来查看详细选项。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;1. 平台配置&lt;/h3&gt; 
&lt;details id="自定义监控平台"&gt; 
 &lt;summary&gt;👉 点击展开：&lt;strong&gt;自定义监控平台&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;strong&gt;配置位置：&lt;/strong&gt; &lt;code&gt;config/config.yaml&lt;/code&gt; 的 &lt;code&gt;platforms&lt;/code&gt; 部分&lt;/p&gt; 
 &lt;p&gt;本项目的资讯数据来源于 &lt;a href="https://github.com/ourongxing/newsnow"&gt;newsnow&lt;/a&gt; ，你可以点击&lt;a href="https://newsnow.busiyi.world/"&gt;网站&lt;/a&gt;，点击[更多]，查看是否有你想要的平台。&lt;/p&gt; 
 &lt;p&gt;具体添加可访问 &lt;a href="https://github.com/ourongxing/newsnow/tree/main/server/sources"&gt;项目源代码&lt;/a&gt;，根据里面的文件名，在 &lt;code&gt;config/config.yaml&lt;/code&gt; 文件中修改 &lt;code&gt;platforms&lt;/code&gt; 配置：&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-yaml"&gt;platforms:
  - id: "toutiao"
    name: "今日头条"
  - id: "baidu"
    name: "百度热搜"
  - id: "wallstreetcn-hot"
    name: "华尔街见闻"
  # 添加更多平台...
&lt;/code&gt;&lt;/pre&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;💡 &lt;strong&gt;快捷方式&lt;/strong&gt;：如果不会看源代码，可以复制他人整理好的 &lt;a href="https://github.com/sansan0/TrendRadar/issues/95"&gt;平台配置汇总&lt;/a&gt;&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;⚠️ &lt;strong&gt;注意&lt;/strong&gt;：平台不是越多越好，建议选择 10-15 个核心平台。过多平台会导致信息过载，反而降低使用体验。&lt;/p&gt; 
 &lt;/blockquote&gt; 
&lt;/details&gt; 
&lt;h3&gt;2. 关键词配置&lt;/h3&gt; 
&lt;p&gt;在 &lt;code&gt;frequency_words.txt&lt;/code&gt; 文件中配置监控的关键词，支持五种语法、区域标记和词组功能。&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;语法类型&lt;/th&gt; 
   &lt;th&gt;符号&lt;/th&gt; 
   &lt;th&gt;作用&lt;/th&gt; 
   &lt;th&gt;示例&lt;/th&gt; 
   &lt;th&gt;匹配逻辑&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;普通词&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;无&lt;/td&gt; 
   &lt;td&gt;基础匹配&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;华为&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;包含任意一个即可&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;必须词&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;+&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;限定范围&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;+手机&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;必须同时包含&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;过滤词&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;!&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;排除干扰&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;!广告&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;包含则直接排除&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;数量限制&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;@&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;控制显示数量&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;@10&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;最多显示10条新闻（v3.2.0新增）&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;全局过滤&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;[GLOBAL_FILTER]&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;全局排除指定内容&lt;/td&gt; 
   &lt;td&gt;见下方示例&lt;/td&gt; 
   &lt;td&gt;任何情况下都过滤（v3.5.0新增）&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h4&gt;2.1 基础语法&lt;/h4&gt; 
&lt;p&gt;&lt;a name="关键词基础语法"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;👉 点击展开：&lt;strong&gt;基础语法教程&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;strong&gt;配置位置：&lt;/strong&gt; &lt;code&gt;config/frequency_words.txt&lt;/code&gt;&lt;/p&gt; 
 &lt;h5&gt;1. &lt;strong&gt;普通关键词&lt;/strong&gt; - 基础匹配&lt;/h5&gt; 
 &lt;pre&gt;&lt;code class="language-txt"&gt;华为
OPPO
苹果
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;作用：&lt;/strong&gt; 新闻标题包含其中&lt;strong&gt;任意一个词&lt;/strong&gt;就会被捕获&lt;/p&gt; 
 &lt;h5&gt;2. &lt;strong&gt;必须词&lt;/strong&gt; &lt;code&gt;+词汇&lt;/code&gt; - 限定范围&lt;/h5&gt; 
 &lt;pre&gt;&lt;code class="language-txt"&gt;华为
OPPO
+手机
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;作用：&lt;/strong&gt; 必须同时包含普通词&lt;strong&gt;和&lt;/strong&gt;必须词才会被捕获&lt;/p&gt; 
 &lt;h5&gt;3. &lt;strong&gt;过滤词&lt;/strong&gt; &lt;code&gt;!词汇&lt;/code&gt; - 排除干扰&lt;/h5&gt; 
 &lt;pre&gt;&lt;code class="language-txt"&gt;苹果
华为
!水果
!价格
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;作用：&lt;/strong&gt; 包含过滤词的新闻会被&lt;strong&gt;直接排除&lt;/strong&gt;，即使包含关键词&lt;/p&gt; 
 &lt;h5&gt;4. &lt;strong&gt;数量限制&lt;/strong&gt; &lt;code&gt;@数字&lt;/code&gt; - 控制显示数量（v3.2.0 新增）&lt;/h5&gt; 
 &lt;pre&gt;&lt;code class="language-txt"&gt;特斯拉
马斯克
@5
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;作用：&lt;/strong&gt; 限制该关键词组最多显示的新闻条数&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;配置优先级：&lt;/strong&gt; &lt;code&gt;@数字&lt;/code&gt; &amp;gt; 全局配置 &amp;gt; 不限制&lt;/p&gt; 
 &lt;h5&gt;5. &lt;strong&gt;全局过滤&lt;/strong&gt; &lt;code&gt;[GLOBAL_FILTER]&lt;/code&gt; - 全局排除指定内容（v3.5.0 新增）&lt;/h5&gt; 
 &lt;pre&gt;&lt;code class="language-txt"&gt;[GLOBAL_FILTER]
广告
推广
营销
震惊
标题党

[WORD_GROUPS]
科技
AI

华为
鸿蒙
!车
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;作用：&lt;/strong&gt; 在任何情况下过滤包含指定词的新闻，&lt;strong&gt;优先级最高&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;使用场景：&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;过滤低质内容：震惊、标题党、爆料等&lt;/li&gt; 
  &lt;li&gt;过滤营销内容：广告、推广、赞助等&lt;/li&gt; 
  &lt;li&gt;过滤特定主题：娱乐、八卦（根据需求）&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;过滤优先级：&lt;/strong&gt; 全局过滤 &amp;gt; 词组内过滤(&lt;code&gt;!&lt;/code&gt;) &amp;gt; 词组匹配&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;区域说明：&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;[GLOBAL_FILTER]&lt;/code&gt;：全局过滤区，包含的词在任何情况下都会被过滤&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;[WORD_GROUPS]&lt;/code&gt;：词组区，保持现有语法（&lt;code&gt;!&lt;/code&gt;、&lt;code&gt;+&lt;/code&gt;、&lt;code&gt;@&lt;/code&gt;）&lt;/li&gt; 
  &lt;li&gt;如果不使用区域标记，默认全部作为词组处理（向后兼容）&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;匹配示例：&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-txt"&gt;[GLOBAL_FILTER]
广告

[WORD_GROUPS]
科技
AI
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ul&gt; 
  &lt;li&gt;❌ "广告：最新科技产品发布" ← 包含全局过滤词"广告"，直接拒绝&lt;/li&gt; 
  &lt;li&gt;✅ "科技公司发布AI新产品" ← 不包含全局过滤词，匹配"科技"词组&lt;/li&gt; 
  &lt;li&gt;✅ "AI技术突破引发关注" ← 不包含全局过滤词，匹配"科技"词组中的"AI"&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;注意事项：&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;全局过滤词应谨慎使用，避免过度过滤导致遗漏有价值内容&lt;/li&gt; 
  &lt;li&gt;建议全局过滤词控制在 5-15 个以内&lt;/li&gt; 
  &lt;li&gt;对于特定词组的过滤，优先使用词组内过滤词（&lt;code&gt;!&lt;/code&gt; 前缀）&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;hr /&gt; 
 &lt;h4&gt;🔗 词组功能 - 空行分隔的重要作用&lt;/h4&gt; 
 &lt;p&gt;&lt;strong&gt;核心规则：&lt;/strong&gt; 用&lt;strong&gt;空行&lt;/strong&gt;分隔不同的词组，每个词组独立统计&lt;/p&gt; 
 &lt;h5&gt;示例配置：&lt;/h5&gt; 
 &lt;pre&gt;&lt;code class="language-txt"&gt;iPhone
华为
OPPO
+发布

A股
上证
深证
+涨跌
!预测

世界杯
欧洲杯
亚洲杯
+比赛
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h5&gt;词组解释及匹配效果：&lt;/h5&gt; 
 &lt;p&gt;&lt;strong&gt;第1组 - 手机新品类：&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;关键词：iPhone、华为、OPPO&lt;/li&gt; 
  &lt;li&gt;必须词：发布&lt;/li&gt; 
  &lt;li&gt;效果：必须包含手机品牌名，同时包含"发布"&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;匹配示例：&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;✅ "iPhone 15正式发布售价公布" ← 有"iPhone"+"发布"&lt;/li&gt; 
  &lt;li&gt;✅ "华为Mate60系列发布会直播" ← 有"华为"+"发布"&lt;/li&gt; 
  &lt;li&gt;✅ "OPPO Find X7发布时间确定" ← 有"OPPO"+"发布"&lt;/li&gt; 
  &lt;li&gt;❌ "iPhone销量创新高" ← 有"iPhone"但缺少"发布"&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;第2组 - 股市行情类：&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;关键词：A股、上证、深证&lt;/li&gt; 
  &lt;li&gt;必须词：涨跌&lt;/li&gt; 
  &lt;li&gt;过滤词：预测&lt;/li&gt; 
  &lt;li&gt;效果：关注股市涨跌实况，排除预测类内容&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;匹配示例：&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;✅ "A股今日大幅涨跌分析" ← 有"A股"+"涨跌"&lt;/li&gt; 
  &lt;li&gt;✅ "上证指数涨跌幅创新高" ← 有"上证"+"涨跌"&lt;/li&gt; 
  &lt;li&gt;❌ "专家预测A股涨跌趋势" ← 有"A股"+"涨跌"但包含"预测"&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;第3组 - 足球赛事类：&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;关键词：世界杯、欧洲杯、亚洲杯&lt;/li&gt; 
  &lt;li&gt;必须词：比赛&lt;/li&gt; 
  &lt;li&gt;效果：只关注比赛相关新闻&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;hr /&gt; 
 &lt;h4&gt;📝 配置技巧&lt;/h4&gt; 
 &lt;h5&gt;1. &lt;strong&gt;从宽到严&lt;/strong&gt;&lt;/h5&gt; 
 &lt;pre&gt;&lt;code class="language-txt"&gt;# 第一步：先用宽泛关键词测试
人工智能
AI
ChatGPT

# 第二步：发现误匹配后，加入必须词限定
人工智能
AI
ChatGPT
+技术

# 第三步：发现干扰内容后，加入过滤词
人工智能
AI
ChatGPT
+技术
!广告
!培训
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h5&gt;2. &lt;strong&gt;避免过度复杂&lt;/strong&gt;&lt;/h5&gt; 
 &lt;p&gt;❌ &lt;strong&gt;不推荐：&lt;/strong&gt; 一个词组包含太多词汇&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-txt"&gt;华为
OPPO
苹果
三星
vivo
一加
魅族
+手机
+发布
+销量
!假货
!维修
!二手
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;✅ &lt;strong&gt;推荐：&lt;/strong&gt; 拆分成多个精确的词组&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-txt"&gt;华为
OPPO
+新品

苹果
三星
+发布

手机
销量
+市场
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h4&gt;2.2 高级配置（v3.2.0 新增）&lt;/h4&gt; 
&lt;p&gt;&lt;a name="关键词高级配置"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;👉 点击展开：&lt;strong&gt;高级配置教程&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;h5&gt;关键词排序优先级&lt;/h5&gt; 
 &lt;p&gt;&lt;strong&gt;配置位置：&lt;/strong&gt; &lt;code&gt;config/config.yaml&lt;/code&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-yaml"&gt;report:
  sort_by_position_first: false  # 排序优先级配置
&lt;/code&gt;&lt;/pre&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;配置值&lt;/th&gt; 
    &lt;th&gt;排序规则&lt;/th&gt; 
    &lt;th&gt;适用场景&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;false&lt;/code&gt;（默认）&lt;/td&gt; 
    &lt;td&gt;热点条数 ↓ → 配置位置 ↑&lt;/td&gt; 
    &lt;td&gt;关注热度趋势&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;配置位置 ↑ → 热点条数 ↓&lt;/td&gt; 
    &lt;td&gt;关注个人优先级&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;p&gt;&lt;strong&gt;示例：&lt;/strong&gt; 配置顺序 A、B、C，热点数 A(3条)、B(10条)、C(5条)&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;false&lt;/code&gt;：B(10条) → C(5条) → A(3条)&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;true&lt;/code&gt;：A(3条) → B(10条) → C(5条)&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h5&gt;全局显示数量限制&lt;/h5&gt; 
 &lt;pre&gt;&lt;code class="language-yaml"&gt;report:
  max_news_per_keyword: 10  # 每个关键词最多显示10条（0=不限制）
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Docker 环境变量：&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;SORT_BY_POSITION_FIRST=true
MAX_NEWS_PER_KEYWORD=10
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;综合示例：&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-yaml"&gt;# config.yaml
report:
  sort_by_position_first: true   # 按配置顺序优先
  max_news_per_keyword: 10       # 全局默认每个关键词最多10条
&lt;/code&gt;&lt;/pre&gt; 
 &lt;pre&gt;&lt;code class="language-txt"&gt;# frequency_words.txt
特斯拉
马斯克
@20              # 重点关注，显示20条（覆盖全局配置）

华为            # 使用全局配置，显示10条

比亚迪
@5               # 限制5条
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;最终效果：&lt;/strong&gt; 按配置顺序显示 特斯拉(20条) → 华为(10条) → 比亚迪(5条)&lt;/p&gt; 
&lt;/details&gt; 
&lt;h3&gt;3. 推送模式详解&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;👉 点击展开：&lt;strong&gt;三种推送模式详细对比&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;strong&gt;配置位置：&lt;/strong&gt; &lt;code&gt;config/config.yaml&lt;/code&gt; 的 &lt;code&gt;report.mode&lt;/code&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-yaml"&gt;report:
  mode: "daily"  # 可选: "daily" | "incremental" | "current"
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Docker 环境变量：&lt;/strong&gt; &lt;code&gt;REPORT_MODE=incremental&lt;/code&gt;&lt;/p&gt; 
 &lt;h4&gt;详细对比表格&lt;/h4&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;模式&lt;/th&gt; 
    &lt;th&gt;适用人群&lt;/th&gt; 
    &lt;th&gt;推送时机&lt;/th&gt; 
    &lt;th&gt;显示内容&lt;/th&gt; 
    &lt;th&gt;典型使用场景&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;当日汇总&lt;/strong&gt;&lt;br /&gt;&lt;code&gt;daily&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;📋 企业管理者/普通用户&lt;/td&gt; 
    &lt;td&gt;按时推送(默认每小时推送一次)&lt;/td&gt; 
    &lt;td&gt;当日所有匹配新闻&lt;br /&gt;+ 新增新闻区域&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;案例&lt;/strong&gt;：每天下午6点查看今天所有重要新闻&lt;br /&gt;&lt;strong&gt;特点&lt;/strong&gt;：看全天完整趋势，不漏掉任何热点&lt;br /&gt;&lt;strong&gt;提醒&lt;/strong&gt;：会包含之前推送过的新闻&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;当前榜单&lt;/strong&gt;&lt;br /&gt;&lt;code&gt;current&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;📰 自媒体人/内容创作者&lt;/td&gt; 
    &lt;td&gt;按时推送(默认每小时推送一次)&lt;/td&gt; 
    &lt;td&gt;当前榜单匹配新闻&lt;br /&gt;+ 新增新闻区域&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;案例&lt;/strong&gt;：每小时追踪"哪些话题现在最火"&lt;br /&gt;&lt;strong&gt;特点&lt;/strong&gt;：实时了解当前热度排名变化&lt;br /&gt;&lt;strong&gt;提醒&lt;/strong&gt;：持续在榜的新闻每次都会出现&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;增量监控&lt;/strong&gt;&lt;br /&gt;&lt;code&gt;incremental&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;📈 投资者/交易员&lt;/td&gt; 
    &lt;td&gt;有新增才推送&lt;/td&gt; 
    &lt;td&gt;新出现的匹配频率词新闻&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;案例&lt;/strong&gt;：监控"特斯拉"，只在有新消息时通知&lt;br /&gt;&lt;strong&gt;特点&lt;/strong&gt;：零重复，只看首次出现的新闻&lt;br /&gt;&lt;strong&gt;适合&lt;/strong&gt;：高频监控、避免信息打扰&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;h4&gt;实际推送效果举例&lt;/h4&gt; 
 &lt;p&gt;假设你监控"苹果"关键词，每小时执行一次：&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;时间&lt;/th&gt; 
    &lt;th&gt;daily 模式推送&lt;/th&gt; 
    &lt;th&gt;current 模式推送&lt;/th&gt; 
    &lt;th&gt;incremental 模式推送&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;10:00&lt;/td&gt; 
    &lt;td&gt;新闻A、新闻B&lt;/td&gt; 
    &lt;td&gt;新闻A、新闻B&lt;/td&gt; 
    &lt;td&gt;新闻A、新闻B&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;11:00&lt;/td&gt; 
    &lt;td&gt;新闻A、新闻B、新闻C&lt;/td&gt; 
    &lt;td&gt;新闻B、新闻C、新闻D&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;仅&lt;/strong&gt;新闻C&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;12:00&lt;/td&gt; 
    &lt;td&gt;新闻A、新闻B、新闻C&lt;/td&gt; 
    &lt;td&gt;新闻C、新闻D、新闻E&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;仅&lt;/strong&gt;新闻D、新闻E&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;p&gt;&lt;strong&gt;说明&lt;/strong&gt;：&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;daily&lt;/code&gt;：累积展示当天所有新闻（A、B、C 都保留）&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;current&lt;/code&gt;：展示当前榜单的新闻（排名变化，新闻D上榜，新闻A掉榜）&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;incremental&lt;/code&gt;：&lt;strong&gt;只推送新出现的新闻&lt;/strong&gt;（避免重复干扰）&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h4&gt;常见问题&lt;/h4&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;&lt;strong&gt;💡 遇到这个问题？&lt;/strong&gt; 👉 "每个小时执行一次，第一次执行完输出的新闻，在下一个小时执行时还会出现"&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;原因&lt;/strong&gt;：你可能选择了 &lt;code&gt;daily&lt;/code&gt;（当日汇总）或 &lt;code&gt;current&lt;/code&gt;（当前榜单）模式&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;解决&lt;/strong&gt;：改用 &lt;code&gt;incremental&lt;/code&gt;（增量监控）模式，只推送新增内容&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/blockquote&gt; 
 &lt;h4&gt;⚠️ 增量模式重要提示&lt;/h4&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;&lt;strong&gt;选择了 &lt;code&gt;incremental&lt;/code&gt;（增量监控）模式的用户请注意：&lt;/strong&gt;&lt;/p&gt; 
  &lt;p&gt;📌 &lt;strong&gt;增量模式只在有新增匹配新闻时才会推送&lt;/strong&gt;&lt;/p&gt; 
  &lt;p&gt;&lt;strong&gt;如果长时间没有收到推送，可能是因为：&lt;/strong&gt;&lt;/p&gt; 
  &lt;ol&gt; 
   &lt;li&gt;当前时段没有符合你关键词的新热点出现&lt;/li&gt; 
   &lt;li&gt;关键词配置过于严格或过于宽泛&lt;/li&gt; 
   &lt;li&gt;监控平台数量较少&lt;/li&gt; 
  &lt;/ol&gt; 
  &lt;p&gt;&lt;strong&gt;解决方案：&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;方案1：👉 &lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#2-%E5%85%B3%E9%94%AE%E8%AF%8D%E9%85%8D%E7%BD%AE"&gt;优化关键词配置&lt;/a&gt; - 调整关键词的精准度，增加或修改监控词汇&lt;/li&gt; 
   &lt;li&gt;方案2：切换推送模式 - 改用 &lt;code&gt;current&lt;/code&gt; 或 &lt;code&gt;daily&lt;/code&gt; 模式，可以定时接收推送&lt;/li&gt; 
   &lt;li&gt;方案3：👉 &lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#1-%E5%B9%B3%E5%8F%B0%E9%85%8D%E7%BD%AE"&gt;增加监控平台&lt;/a&gt; - 添加更多新闻平台，扩大信息来源&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/blockquote&gt; 
&lt;/details&gt; 
&lt;h3&gt;4. 热点权重调整&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;👉 点击展开：&lt;strong&gt;热点权重调整&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;strong&gt;配置位置：&lt;/strong&gt; &lt;code&gt;config/config.yaml&lt;/code&gt; 的 &lt;code&gt;weight&lt;/code&gt; 部分&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-yaml"&gt;weight:
  rank_weight: 0.6       # 排名权重
  frequency_weight: 0.3  # 频次权重
  hotness_weight: 0.1    # 热度权重
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;当前默认的配置是平衡性配置&lt;/p&gt; 
 &lt;h4&gt;两个核心场景&lt;/h4&gt; 
 &lt;p&gt;&lt;strong&gt;追实时热点型&lt;/strong&gt;：&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-yaml"&gt;weight:
  rank_weight: 0.8    # 主要看排名
  frequency_weight: 0.1  # 不太在乎持续性
  hotness_weight: 0.1
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;适用人群&lt;/strong&gt;：自媒体博主、营销人员、想快速了解当下最火话题的用户&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;追深度话题型&lt;/strong&gt;：&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-yaml"&gt;weight:
  rank_weight: 0.4    # 适度看排名
  frequency_weight: 0.5  # 重视当天内的持续热度
  hotness_weight: 0.1
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;适用人群&lt;/strong&gt;：投资者、研究人员、新闻工作者、需要深度分析趋势的用户&lt;/p&gt; 
 &lt;h4&gt;调整的方法&lt;/h4&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;strong&gt;三个数字加起来必须等于 1.0&lt;/strong&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;哪个重要就调大哪个&lt;/strong&gt;：在乎排名就调大 rank_weight，在乎持续性就调大 frequency_weight&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;建议每次只调 0.1-0.2&lt;/strong&gt;，观察效果&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;核心思路：追求速度和时效性的用户提高排名权重，追求深度和稳定性的用户提高频次权重。&lt;/p&gt; 
&lt;/details&gt; 
&lt;h3&gt;5. 推送格式参考&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;👉 点击展开：&lt;strong&gt;推送格式说明&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;h4&gt;推送示例&lt;/h4&gt; 
 &lt;p&gt;📊 热点词汇统计&lt;/p&gt; 
 &lt;p&gt;🔥 [1/3] AI ChatGPT : 2 条&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;[百度热搜] 🆕 ChatGPT-5正式发布 [&lt;strong&gt;1&lt;/strong&gt;] - 09时15分 (1次)&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[今日头条] AI芯片概念股暴涨 [&lt;strong&gt;3&lt;/strong&gt;] - [08时30分 ~ 10时45分] (3次)&lt;/p&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;━━━━━━━━━━━━━━━━━━━&lt;/p&gt; 
 &lt;p&gt;📈 [2/3] 比亚迪 特斯拉 : 2 条&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;[微博] 🆕 比亚迪月销量破纪录 [&lt;strong&gt;2&lt;/strong&gt;] - 10时20分 (1次)&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[抖音] 特斯拉降价促销 [&lt;strong&gt;4&lt;/strong&gt;] - [07时45分 ~ 09时15分] (2次)&lt;/p&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;━━━━━━━━━━━━━━━━━━━&lt;/p&gt; 
 &lt;p&gt;📌 [3/3] A股 股市 : 1 条&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;[华尔街见闻] A股午盘点评分析 [&lt;strong&gt;5&lt;/strong&gt;] - [11时30分 ~ 12时00分] (2次)&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;🆕 本次新增热点新闻 (共 2 条)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;百度热搜&lt;/strong&gt; (1 条):&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;ChatGPT-5正式发布 [&lt;strong&gt;1&lt;/strong&gt;]&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;&lt;strong&gt;微博&lt;/strong&gt; (1 条):&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;比亚迪月销量破纪录 [&lt;strong&gt;2&lt;/strong&gt;]&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;更新时间：2025-01-15 12:30:15&lt;/p&gt; 
 &lt;h4&gt;消息格式说明&lt;/h4&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;格式元素&lt;/th&gt; 
    &lt;th&gt;示例&lt;/th&gt; 
    &lt;th&gt;含义&lt;/th&gt; 
    &lt;th&gt;说明&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;🔥📈📌&lt;/td&gt; 
    &lt;td&gt;🔥 [1/3] AI ChatGPT&lt;/td&gt; 
    &lt;td&gt;热度等级&lt;/td&gt; 
    &lt;td&gt;🔥高热度(≥10条) 📈中热度(5-9条) 📌普通热度(&amp;lt;5条)&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;[序号/总数]&lt;/td&gt; 
    &lt;td&gt;[1/3]&lt;/td&gt; 
    &lt;td&gt;排序位置&lt;/td&gt; 
    &lt;td&gt;当前词组在所有匹配词组中的排名&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;频率词组&lt;/td&gt; 
    &lt;td&gt;AI ChatGPT&lt;/td&gt; 
    &lt;td&gt;关键词组&lt;/td&gt; 
    &lt;td&gt;配置文件中的词组，标题必须包含其中词汇&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;: N 条&lt;/td&gt; 
    &lt;td&gt;: 2 条&lt;/td&gt; 
    &lt;td&gt;匹配数量&lt;/td&gt; 
    &lt;td&gt;该词组匹配的新闻总数&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;[平台名]&lt;/td&gt; 
    &lt;td&gt;[百度热搜]&lt;/td&gt; 
    &lt;td&gt;来源平台&lt;/td&gt; 
    &lt;td&gt;新闻所属的平台名称&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;🆕&lt;/td&gt; 
    &lt;td&gt;🆕 ChatGPT-5正式发布&lt;/td&gt; 
    &lt;td&gt;新增标记&lt;/td&gt; 
    &lt;td&gt;本轮抓取中首次出现的热点&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;[&lt;strong&gt;数字&lt;/strong&gt;]&lt;/td&gt; 
    &lt;td&gt;[&lt;strong&gt;1&lt;/strong&gt;]&lt;/td&gt; 
    &lt;td&gt;高排名&lt;/td&gt; 
    &lt;td&gt;排名≤阈值的热搜，红色加粗显示&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;[数字]&lt;/td&gt; 
    &lt;td&gt;[7]&lt;/td&gt; 
    &lt;td&gt;普通排名&lt;/td&gt; 
    &lt;td&gt;排名&amp;gt;阈值的热搜，普通显示&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;- 时间&lt;/td&gt; 
    &lt;td&gt;- 09时15分&lt;/td&gt; 
    &lt;td&gt;首次时间&lt;/td&gt; 
    &lt;td&gt;该新闻首次被发现的时间&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;[时间~时间]&lt;/td&gt; 
    &lt;td&gt;[08时30分 ~ 10时45分]&lt;/td&gt; 
    &lt;td&gt;持续时间&lt;/td&gt; 
    &lt;td&gt;从首次出现到最后出现的时间范围&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;(N次)&lt;/td&gt; 
    &lt;td&gt;(3次)&lt;/td&gt; 
    &lt;td&gt;出现频率&lt;/td&gt; 
    &lt;td&gt;在监控期间出现的总次数&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;新增区域&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;🆕 &lt;strong&gt;本次新增热点新闻&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;新话题汇总&lt;/td&gt; 
    &lt;td&gt;单独展示本轮新出现的热点话题&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h3&gt;6. Docker 部署&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;👉 点击展开：&lt;strong&gt;Docker 部署完整指南&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;strong&gt;镜像说明：&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;TrendRadar 提供两个独立的 Docker 镜像，可根据需求选择部署：&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;镜像名称&lt;/th&gt; 
    &lt;th&gt;用途&lt;/th&gt; 
    &lt;th&gt;说明&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;wantcat/trendradar&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;新闻推送服务&lt;/td&gt; 
    &lt;td&gt;定时抓取新闻、推送通知（必选）&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;wantcat/trendradar-mcp&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;AI 分析服务&lt;/td&gt; 
    &lt;td&gt;MCP 协议支持、AI 对话分析（可选）&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;💡 &lt;strong&gt;建议&lt;/strong&gt;：&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;只需要推送功能：仅部署 &lt;code&gt;wantcat/trendradar&lt;/code&gt; 镜像&lt;/li&gt; 
   &lt;li&gt;需要 AI 分析功能：同时部署两个镜像&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/blockquote&gt; 
 &lt;hr /&gt; 
 &lt;h4&gt;方式一：使用 docker compose（推荐）&lt;/h4&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;创建项目目录和配置&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;方式 1-A：使用 git clone（推荐，最简单）&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# 克隆项目到本地
git clone https://github.com/sansan0/TrendRadar.git
cd TrendRadar
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;方式 1-B：使用 wget 下载配置文件&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# 创建目录结构
mkdir -p trendradar/{config,docker}
cd trendradar

# 下载配置文件模板
wget https://raw.githubusercontent.com/sansan0/TrendRadar/master/config/config.yaml -P config/
wget https://raw.githubusercontent.com/sansan0/TrendRadar/master/config/frequency_words.txt -P config/

# 下载 docker compose 配置
wget https://raw.githubusercontent.com/sansan0/TrendRadar/master/docker/.env  -P docker/
wget https://raw.githubusercontent.com/sansan0/TrendRadar/master/docker/docker-compose.yml  -P docker/
&lt;/code&gt;&lt;/pre&gt; 
   &lt;blockquote&gt; 
    &lt;p&gt;💡 &lt;strong&gt;说明&lt;/strong&gt;：Docker 部署需要的关键目录结构如下：&lt;/p&gt; 
   &lt;/blockquote&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code&gt;当前目录/
├── config/
│   ├── config.yaml
│   └── frequency_words.txt
└── docker/
    ├── .env
    └── docker-compose.yml
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="2"&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;配置文件说明&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;code&gt;config/config.yaml&lt;/code&gt; - 应用主配置（报告模式、推送设置等）&lt;/li&gt; 
    &lt;li&gt;&lt;code&gt;config/frequency_words.txt&lt;/code&gt; - 关键词配置（设置你关心的热点词汇）&lt;/li&gt; 
    &lt;li&gt;&lt;code&gt;.env&lt;/code&gt; - 环境变量配置（webhook URLs 和定时任务）&lt;/li&gt; 
   &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;⚙️ 环境变量覆盖机制（v3.0.5+）&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;如果你在 NAS 或其他 Docker 环境中遇到&lt;strong&gt;修改 &lt;code&gt;config.yaml&lt;/code&gt; 后配置不生效&lt;/strong&gt;的问题，可以通过环境变量直接覆盖配置：&lt;/p&gt; 
   &lt;table&gt; 
    &lt;thead&gt; 
     &lt;tr&gt; 
      &lt;th&gt;环境变量&lt;/th&gt; 
      &lt;th&gt;对应配置&lt;/th&gt; 
      &lt;th&gt;示例值&lt;/th&gt; 
      &lt;th&gt;说明&lt;/th&gt; 
     &lt;/tr&gt; 
    &lt;/thead&gt; 
    &lt;tbody&gt; 
     &lt;tr&gt; 
      &lt;td&gt;&lt;code&gt;ENABLE_CRAWLER&lt;/code&gt;&lt;/td&gt; 
      &lt;td&gt;&lt;code&gt;crawler.enable_crawler&lt;/code&gt;&lt;/td&gt; 
      &lt;td&gt;&lt;code&gt;true&lt;/code&gt; / &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
      &lt;td&gt;是否启用爬虫&lt;/td&gt; 
     &lt;/tr&gt; 
     &lt;tr&gt; 
      &lt;td&gt;&lt;code&gt;ENABLE_NOTIFICATION&lt;/code&gt;&lt;/td&gt; 
      &lt;td&gt;&lt;code&gt;notification.enable_notification&lt;/code&gt;&lt;/td&gt; 
      &lt;td&gt;&lt;code&gt;true&lt;/code&gt; / &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
      &lt;td&gt;是否启用通知&lt;/td&gt; 
     &lt;/tr&gt; 
     &lt;tr&gt; 
      &lt;td&gt;&lt;code&gt;REPORT_MODE&lt;/code&gt;&lt;/td&gt; 
      &lt;td&gt;&lt;code&gt;report.mode&lt;/code&gt;&lt;/td&gt; 
      &lt;td&gt;&lt;code&gt;daily&lt;/code&gt; / &lt;code&gt;incremental&lt;/code&gt; / &lt;code&gt;current&lt;/code&gt;&lt;/td&gt; 
      &lt;td&gt;报告模式&lt;/td&gt; 
     &lt;/tr&gt; 
     &lt;tr&gt; 
      &lt;td&gt;&lt;code&gt;MAX_ACCOUNTS_PER_CHANNEL&lt;/code&gt;&lt;/td&gt; 
      &lt;td&gt;&lt;code&gt;notification.max_accounts_per_channel&lt;/code&gt;&lt;/td&gt; 
      &lt;td&gt;&lt;code&gt;3&lt;/code&gt;&lt;/td&gt; 
      &lt;td&gt;每个渠道最大账号数&lt;/td&gt; 
     &lt;/tr&gt; 
     &lt;tr&gt; 
      &lt;td&gt;&lt;code&gt;PUSH_WINDOW_ENABLED&lt;/code&gt;&lt;/td&gt; 
      &lt;td&gt;&lt;code&gt;notification.push_window.enabled&lt;/code&gt;&lt;/td&gt; 
      &lt;td&gt;&lt;code&gt;true&lt;/code&gt; / &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
      &lt;td&gt;推送时间窗口开关&lt;/td&gt; 
     &lt;/tr&gt; 
     &lt;tr&gt; 
      &lt;td&gt;&lt;code&gt;PUSH_WINDOW_START&lt;/code&gt;&lt;/td&gt; 
      &lt;td&gt;&lt;code&gt;notification.push_window.time_range.start&lt;/code&gt;&lt;/td&gt; 
      &lt;td&gt;&lt;code&gt;08:00&lt;/code&gt;&lt;/td&gt; 
      &lt;td&gt;推送开始时间&lt;/td&gt; 
     &lt;/tr&gt; 
     &lt;tr&gt; 
      &lt;td&gt;&lt;code&gt;PUSH_WINDOW_END&lt;/code&gt;&lt;/td&gt; 
      &lt;td&gt;&lt;code&gt;notification.push_window.time_range.end&lt;/code&gt;&lt;/td&gt; 
      &lt;td&gt;&lt;code&gt;22:00&lt;/code&gt;&lt;/td&gt; 
      &lt;td&gt;推送结束时间&lt;/td&gt; 
     &lt;/tr&gt; 
     &lt;tr&gt; 
      &lt;td&gt;&lt;code&gt;ENABLE_WEBSERVER&lt;/code&gt;&lt;/td&gt; 
      &lt;td&gt;-&lt;/td&gt; 
      &lt;td&gt;&lt;code&gt;true&lt;/code&gt; / &lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
      &lt;td&gt;是否自动启动 Web 服务器&lt;/td&gt; 
     &lt;/tr&gt; 
     &lt;tr&gt; 
      &lt;td&gt;&lt;code&gt;WEBSERVER_PORT&lt;/code&gt;&lt;/td&gt; 
      &lt;td&gt;-&lt;/td&gt; 
      &lt;td&gt;&lt;code&gt;8080&lt;/code&gt;&lt;/td&gt; 
      &lt;td&gt;Web 服务器端口（默认 8080）&lt;/td&gt; 
     &lt;/tr&gt; 
     &lt;tr&gt; 
      &lt;td&gt;&lt;code&gt;FEISHU_WEBHOOK_URL&lt;/code&gt;&lt;/td&gt; 
      &lt;td&gt;&lt;code&gt;notification.webhooks.feishu_url&lt;/code&gt;&lt;/td&gt; 
      &lt;td&gt;&lt;code&gt;https://...&lt;/code&gt;&lt;/td&gt; 
      &lt;td&gt;飞书 Webhook（支持多账号，用 &lt;code&gt;;&lt;/code&gt; 分隔）&lt;/td&gt; 
     &lt;/tr&gt; 
    &lt;/tbody&gt; 
   &lt;/table&gt; &lt;p&gt;&lt;strong&gt;配置优先级&lt;/strong&gt;：环境变量 &amp;gt; config.yaml&lt;/p&gt; &lt;p&gt;&lt;strong&gt;使用方法&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;修改 &lt;code&gt;.env&lt;/code&gt; 文件，取消注释并填写需要的配置&lt;/li&gt; 
    &lt;li&gt;或在 NAS/群晖 Docker 管理界面的"环境变量"中直接添加&lt;/li&gt; 
    &lt;li&gt;重启容器后生效：&lt;code&gt;docker compose up -d&lt;/code&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;启动服务&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;选项 A：启动所有服务（推送 + AI 分析）&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# 拉取最新镜像
docker compose pull

# 启动所有服务（trend-radar + trend-radar-mcp）
docker compose up -d
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;选项 B：仅启动新闻推送服务&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# 只启动 trend-radar（定时抓取和推送）
docker compose pull trend-radar
docker compose up -d trend-radar
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;选项 C：仅启动 MCP AI 分析服务&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# 只启动 trend-radar-mcp（提供 AI 分析接口）
docker compose pull trend-radar-mcp
docker compose up -d trend-radar-mcp
&lt;/code&gt;&lt;/pre&gt; 
   &lt;blockquote&gt; 
    &lt;p&gt;💡 &lt;strong&gt;提示&lt;/strong&gt;：&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;大多数用户只需启动 &lt;code&gt;trend-radar&lt;/code&gt; 即可实现新闻推送功能&lt;/li&gt; 
     &lt;li&gt;只有需要使用 Claude/ChatGPT 进行 AI 对话分析时，才需启动 &lt;code&gt;trend-radar-mcp&lt;/code&gt;&lt;/li&gt; 
     &lt;li&gt;两个服务相互独立，可根据需求灵活组合&lt;/li&gt; 
    &lt;/ul&gt; 
   &lt;/blockquote&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;查看运行状态&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# 查看新闻推送服务日志
docker logs -f trend-radar

# 查看 MCP AI 分析服务日志
docker logs -f trend-radar-mcp

# 查看所有容器状态
docker ps | grep trend-radar

# 停止特定服务
docker compose stop trend-radar      # 停止推送服务
docker compose stop trend-radar-mcp  # 停止 MCP 服务
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;h4&gt;方式二：本地构建（开发者选项）&lt;/h4&gt; 
 &lt;p&gt;如果需要自定义修改代码或构建自己的镜像：&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# 克隆项目
git clone https://github.com/sansan0/TrendRadar.git
cd TrendRadar

# 修改配置文件
vim config/config.yaml
vim config/frequency_words.txt

# 使用构建版本的 docker compose
cd docker
cp docker-compose-build.yml docker-compose.yml
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;构建并启动服务&lt;/strong&gt;：&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# 选项 A：构建并启动所有服务
docker compose build
docker compose up -d

# 选项 B：仅构建并启动新闻推送服务
docker compose build trend-radar
docker compose up -d trend-radar

# 选项 C：仅构建并启动 MCP AI 分析服务
docker compose build trend-radar-mcp
docker compose up -d trend-radar-mcp
&lt;/code&gt;&lt;/pre&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;💡 &lt;strong&gt;架构参数说明&lt;/strong&gt;：&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;默认构建 &lt;code&gt;amd64&lt;/code&gt; 架构镜像（适用于大多数 x86_64 服务器）&lt;/li&gt; 
   &lt;li&gt;如需构建 &lt;code&gt;arm64&lt;/code&gt; 架构（Apple Silicon、树莓派等），设置环境变量： &lt;pre&gt;&lt;code class="language-bash"&gt;export DOCKER_ARCH=arm64
docker compose build
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/blockquote&gt; 
 &lt;h4&gt;镜像更新&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# 方式一：手动更新（爬虫 + MCP 镜像）
docker pull wantcat/trendradar:latest
docker pull wantcat/trendradar-mcp:latest
docker compose down
docker compose up -d

# 方式二：使用 docker compose 更新
docker compose pull
docker compose up -d
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;可用镜像&lt;/strong&gt;：&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;镜像名称&lt;/th&gt; 
    &lt;th&gt;用途&lt;/th&gt; 
    &lt;th&gt;说明&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;wantcat/trendradar&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;新闻推送服务&lt;/td&gt; 
    &lt;td&gt;定时抓取新闻、推送通知&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;wantcat/trendradar-mcp&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;MCP 服务&lt;/td&gt; 
    &lt;td&gt;AI 分析功能（可选）&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;h4&gt;服务管理命令&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# 查看运行状态
docker exec -it trend-radar python manage.py status

# 手动执行一次爬虫
docker exec -it trend-radar python manage.py run

# 查看实时日志
docker exec -it trend-radar python manage.py logs

# 显示当前配置
docker exec -it trend-radar python manage.py config

# 显示输出文件
docker exec -it trend-radar python manage.py files

# Web 服务器管理（用于浏览器访问生成的报告）
docker exec -it trend-radar python manage.py start_webserver   # 启动 Web 服务器
docker exec -it trend-radar python manage.py stop_webserver    # 停止 Web 服务器
docker exec -it trend-radar python manage.py webserver_status  # 查看 Web 服务器状态

# 查看帮助信息
docker exec -it trend-radar python manage.py help

# 重启容器
docker restart trend-radar

# 停止容器
docker stop trend-radar

# 删除容器（保留数据）
docker rm trend-radar
&lt;/code&gt;&lt;/pre&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;💡 &lt;strong&gt;Web 服务器说明&lt;/strong&gt;：&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;启动后可通过浏览器访问 &lt;code&gt;http://localhost:8080&lt;/code&gt; 查看最新报告&lt;/li&gt; 
   &lt;li&gt;通过目录导航访问历史报告（如：&lt;code&gt;http://localhost:8080/2025-xx-xx/&lt;/code&gt;）&lt;/li&gt; 
   &lt;li&gt;端口可在 &lt;code&gt;.env&lt;/code&gt; 文件中配置 &lt;code&gt;WEBSERVER_PORT&lt;/code&gt; 参数&lt;/li&gt; 
   &lt;li&gt;自动启动：在 &lt;code&gt;.env&lt;/code&gt; 中设置 &lt;code&gt;ENABLE_WEBSERVER=true&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;安全提示：仅提供静态文件访问，限制在 output 目录，只绑定本地访问&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/blockquote&gt; 
 &lt;h4&gt;数据持久化&lt;/h4&gt; 
 &lt;p&gt;生成的报告和数据默认保存在 &lt;code&gt;./output&lt;/code&gt; 目录下，即使容器重启或删除，数据也会保留。&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;📊 网页版报告访问路径&lt;/strong&gt;：&lt;/p&gt; 
 &lt;p&gt;TrendRadar 生成的当日汇总 HTML 报告会同时保存到两个位置：&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;文件位置&lt;/th&gt; 
    &lt;th&gt;访问方式&lt;/th&gt; 
    &lt;th&gt;适用场景&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;output/index.html&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;宿主机直接访问&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;Docker 部署&lt;/strong&gt;（通过 Volume 挂载，宿主机可见）&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;index.html&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;根目录访问&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;GitHub Pages&lt;/strong&gt;（仓库根目录，Pages 自动识别）&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;output/YYYY-MM-DD/html/当日汇总.html&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;历史报告访问&lt;/td&gt; 
    &lt;td&gt;所有环境（按日期归档）&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;p&gt;&lt;strong&gt;本地访问示例&lt;/strong&gt;：&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# 方式 1：通过 Web 服务器访问（推荐，Docker 环境）
# 1. 启动 Web 服务器
docker exec -it trend-radar python manage.py start_webserver
# 2. 在浏览器访问
http://localhost:8080                           # 访问最新报告（默认 index.html）
http://localhost:8080/2025-xx-xx/               # 访问指定日期的报告
http://localhost:8080/2025-xx-xx/html/          # 浏览该日期下的所有 HTML 文件

# 方式 2：直接打开文件（本地环境）
open ./output/index.html             # macOS
start ./output/index.html            # Windows
xdg-open ./output/index.html         # Linux

# 方式 3：访问历史归档
open ./output/2025-xx-xx/html/当日汇总.html
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;为什么有两个 index.html？&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;output/index.html&lt;/code&gt;：Docker Volume 挂载到宿主机，本地可直接打开&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;index.html&lt;/code&gt;：GitHub Actions 推送到仓库，GitHub Pages 自动部署&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;💡 &lt;strong&gt;提示&lt;/strong&gt;：两个文件内容完全相同，选择任意一个访问即可。&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;h4&gt;故障排查&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# 检查容器状态
docker inspect trend-radar

# 查看容器日志
docker logs --tail 100 trend-radar

# 进入容器调试
docker exec -it trend-radar /bin/bash

# 验证配置文件
docker exec -it trend-radar ls -la /app/config/
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;MCP 服务部署（AI 分析功能）&lt;/h4&gt; 
 &lt;p&gt;如果需要使用 AI 分析功能，可以部署独立的 MCP 服务容器。&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;架构说明&lt;/strong&gt;：&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-mermaid"&gt;flowchart TB
    subgraph trend-radar["trend-radar"]
        A1[定时抓取新闻]
        A2[推送通知]
    end
    
    subgraph trend-radar-mcp["trend-radar-mcp"]
        B1[127.0.0.1:3333]
        B2[AI 分析接口]
    end
    
    subgraph shared["共享卷"]
        C1["config/ (ro)"]
        C2["output/ (ro)"]
    end
    
    trend-radar --&amp;gt; shared
    trend-radar-mcp --&amp;gt; shared
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;快速启动&lt;/strong&gt;：&lt;/p&gt; 
 &lt;p&gt;如果已按照 &lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#%E6%96%B9%E5%BC%8F%E4%B8%80%E4%BD%BF%E7%94%A8-docker-compose%E6%8E%A8%E8%8D%90"&gt;方式一：使用 docker compose&lt;/a&gt; 完成部署，只需启动 MCP 服务：&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;cd TrendRadar/docker
docker compose up -d trend-radar-mcp

# 查看运行状态
docker ps | grep trend-radar-mcp
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;单独启动 MCP 服务&lt;/strong&gt;（不使用 docker compose）：&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Linux/Mac
docker run -d --name trend-radar-mcp \
  -p 127.0.0.1:3333:3333 \
  -v $(pwd)/config:/app/config:ro \
  -v $(pwd)/output:/app/output:ro \
  -e TZ=Asia/Shanghai \
  wantcat/trendradar-mcp:latest

# Windows PowerShell
docker run -d --name trend-radar-mcp `
  -p 127.0.0.1:3333:3333 `
  -v ${PWD}/config:/app/config:ro `
  -v ${PWD}/output:/app/output:ro `
  -e TZ=Asia/Shanghai `
  wantcat/trendradar-mcp:latest
&lt;/code&gt;&lt;/pre&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;⚠️ &lt;strong&gt;注意&lt;/strong&gt;：单独运行时，确保当前目录下有 &lt;code&gt;config/&lt;/code&gt; 和 &lt;code&gt;output/&lt;/code&gt; 文件夹，且包含配置文件和新闻数据。&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;验证服务&lt;/strong&gt;：&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# 检查 MCP 服务健康状态
curl http://127.0.0.1:3333/mcp

# 查看 MCP 服务日志
docker logs -f trend-radar-mcp
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;在 AI 客户端中配置&lt;/strong&gt;：&lt;/p&gt; 
 &lt;p&gt;MCP 服务启动后，根据不同客户端进行配置：&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Cherry Studio&lt;/strong&gt;（推荐，GUI 配置）：&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;设置 → MCP 服务器 → 添加&lt;/li&gt; 
  &lt;li&gt;类型：&lt;code&gt;streamableHttp&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;URL：&lt;code&gt;http://127.0.0.1:3333/mcp&lt;/code&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;Claude Desktop / Cline&lt;/strong&gt;（JSON 配置）：&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
  "mcpServers": {
    "trendradar": {
      "url": "http://127.0.0.1:3333/mcp",
      "type": "streamableHttp"
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;💡 &lt;strong&gt;提示&lt;/strong&gt;：MCP 服务仅监听本地端口（127.0.0.1），确保安全性。如需远程访问，请自行配置反向代理和认证。&lt;/p&gt; 
 &lt;/blockquote&gt; 
&lt;/details&gt; 
&lt;h3&gt;7. 报告配置&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;👉 点击展开：&lt;strong&gt;报告相关参数配置&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;strong&gt;配置位置：&lt;/strong&gt; &lt;code&gt;config/config.yaml&lt;/code&gt; 的 &lt;code&gt;report&lt;/code&gt; 部分&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-yaml"&gt;report:
  mode: "daily"                    # 推送模式
  rank_threshold: 5                # 排名高亮阈值
  sort_by_position_first: false    # 排序优先级
  max_news_per_keyword: 0          # 每个关键词最大显示数量
  reverse_content_order: false     # 内容顺序配置
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;配置项详解&lt;/h4&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;配置项&lt;/th&gt; 
    &lt;th&gt;类型&lt;/th&gt; 
    &lt;th&gt;默认值&lt;/th&gt; 
    &lt;th&gt;说明&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;mode&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;string&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;daily&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;推送模式，可选 &lt;code&gt;daily&lt;/code&gt;/&lt;code&gt;incremental&lt;/code&gt;/&lt;code&gt;current&lt;/code&gt;，详见 &lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#3-%E6%8E%A8%E9%80%81%E6%A8%A1%E5%BC%8F%E8%AF%A6%E8%A7%A3"&gt;推送模式详解&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;rank_threshold&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;5&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;排名高亮阈值，排名 ≤ 该值的新闻会加粗显示&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;sort_by_position_first&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;排序优先级：&lt;code&gt;false&lt;/code&gt;=按热点条数排序，&lt;code&gt;true&lt;/code&gt;=按配置位置排序&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;max_news_per_keyword&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;int&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;0&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;每个关键词最大显示数量，&lt;code&gt;0&lt;/code&gt;=不限制&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;reverse_content_order&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;内容顺序：&lt;code&gt;false&lt;/code&gt;=热点词汇统计在前，&lt;code&gt;true&lt;/code&gt;=新增热点新闻在前&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;h4&gt;内容顺序配置（v3.5.0 新增）&lt;/h4&gt; 
 &lt;p&gt;控制推送消息和 HTML 报告中两部分内容的显示顺序：&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;配置值&lt;/th&gt; 
    &lt;th&gt;显示顺序&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;false&lt;/code&gt;（默认）&lt;/td&gt; 
    &lt;td&gt;① 热点词汇统计 → ② 新增热点新闻&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;① 新增热点新闻 → ② 热点词汇统计&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;p&gt;&lt;strong&gt;适用场景：&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;false&lt;/code&gt;（默认）：适合关注关键词匹配结果的用户，先看分类统计&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;true&lt;/code&gt;：适合关注最新动态的用户，优先查看新增热点&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;Docker 环境变量：&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;REVERSE_CONTENT_ORDER=true
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;排序优先级配置&lt;/h4&gt; 
 &lt;p&gt;&lt;strong&gt;示例场景：&lt;/strong&gt; 配置顺序 A、B、C，热点数 A(3条)、B(10条)、C(5条)&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;配置值&lt;/th&gt; 
    &lt;th&gt;显示顺序&lt;/th&gt; 
    &lt;th&gt;适用场景&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;false&lt;/code&gt;（默认）&lt;/td&gt; 
    &lt;td&gt;B(10条) → C(5条) → A(3条)&lt;/td&gt; 
    &lt;td&gt;关注热度趋势&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;A(3条) → B(10条) → C(5条)&lt;/td&gt; 
    &lt;td&gt;关注个人优先级&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;p&gt;&lt;strong&gt;Docker 环境变量：&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;SORT_BY_POSITION_FIRST=true
MAX_NEWS_PER_KEYWORD=10
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;8. 推送时间窗口配置&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;👉 点击展开：&lt;strong&gt;推送时间窗口控制详解&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;strong&gt;配置位置：&lt;/strong&gt; &lt;code&gt;config/config.yaml&lt;/code&gt; 的 &lt;code&gt;notification.push_window&lt;/code&gt; 部分&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-yaml"&gt;notification:
  push_window:
    enabled: false                    # 是否启用
    time_range:
      start: "20:00"                  # 开始时间（北京时间）
      end: "22:00"                    # 结束时间（北京时间）
    once_per_day: true                # 每天只推送一次
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;配置项详解&lt;/h4&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;配置项&lt;/th&gt; 
    &lt;th&gt;类型&lt;/th&gt; 
    &lt;th&gt;默认值&lt;/th&gt; 
    &lt;th&gt;说明&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;enabled&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;是否启用推送时间窗口控制&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;time_range.start&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;string&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;"20:00"&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;推送时间窗口开始时间（北京时间，HH:MM 格式）&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;time_range.end&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;string&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;"22:00"&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;推送时间窗口结束时间（北京时间，HH:MM 格式）&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;once_per_day&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;bool&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;true&lt;/code&gt;=每天在窗口内只推送一次，&lt;code&gt;false&lt;/code&gt;=窗口内每次执行都推送&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;h4&gt;使用场景&lt;/h4&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;场景&lt;/th&gt; 
    &lt;th&gt;配置示例&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;工作时间推送&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;start: "09:00"&lt;/code&gt;, &lt;code&gt;end: "18:00"&lt;/code&gt;, &lt;code&gt;once_per_day: false&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;晚间汇总推送&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;start: "20:00"&lt;/code&gt;, &lt;code&gt;end: "22:00"&lt;/code&gt;, &lt;code&gt;once_per_day: true&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;午休时间推送&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;start: "12:00"&lt;/code&gt;, &lt;code&gt;end: "13:00"&lt;/code&gt;, &lt;code&gt;once_per_day: true&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;h4&gt;重要提示&lt;/h4&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;⚠️ &lt;strong&gt;GitHub Actions 用户注意：&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;GitHub Actions 执行时间不稳定，可能有 ±15 分钟的偏差&lt;/li&gt; 
   &lt;li&gt;时间范围建议至少留足 &lt;strong&gt;2 小时&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;如果想要精准的定时推送，建议使用 &lt;strong&gt;Docker 部署&lt;/strong&gt;在个人服务器上&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/blockquote&gt; 
 &lt;h4&gt;Docker 环境变量&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;PUSH_WINDOW_ENABLED=true
PUSH_WINDOW_START=09:00
PUSH_WINDOW_END=18:00
PUSH_WINDOW_ONCE_PER_DAY=false
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;完整配置示例&lt;/h4&gt; 
 &lt;p&gt;&lt;strong&gt;场景：每天晚上 8-10 点只推送一次汇总&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-yaml"&gt;notification:
  push_window:
    enabled: true
    time_range:
      start: "20:00"
      end: "22:00"
    once_per_day: true
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;场景：工作时间内每小时推送&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-yaml"&gt;notification:
  push_window:
    enabled: true
    time_range:
      start: "09:00"
      end: "18:00"
    once_per_day: false
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;9. 执行频率配置&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;👉 点击展开：&lt;strong&gt;自动运行频率设置&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;strong&gt;配置位置：&lt;/strong&gt; &lt;code&gt;.github/workflows/crawler.yml&lt;/code&gt; 的 &lt;code&gt;schedule&lt;/code&gt; 部分&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-yaml"&gt;on:
  schedule:
    - cron: "0 * * * *"  # 每小时运行一次
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;什么是 Cron 表达式？&lt;/h4&gt; 
 &lt;p&gt;Cron 是一种定时任务格式，由 5 个部分组成：&lt;code&gt;分 时 日 月 周&lt;/code&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code&gt;┌───────────── 分钟 (0-59)
│ ┌───────────── 小时 (0-23)
│ │ ┌───────────── 日期 (1-31)
│ │ │ ┌───────────── 月份 (1-12)
│ │ │ │ ┌───────────── 星期 (0-6，0=周日)
│ │ │ │ │
* * * * *
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;常用配置示例&lt;/h4&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;想要的效果&lt;/th&gt; 
    &lt;th&gt;Cron 表达式&lt;/th&gt; 
    &lt;th&gt;说明&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;每小时运行&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;0 * * * *&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;每小时的第 0 分钟运行（默认）&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;每 30 分钟运行&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;*/30 * * * *&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;每隔 30 分钟运行一次&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;每天早 8 点运行&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;0 0 * * *&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;UTC 0:00 = 北京时间 8:00&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;工作时间运行&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;*/30 0-14 * * *&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;北京 8:00-22:00，每 30 分钟&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;每天 3 次&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;0 0,6,12 * * *&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;北京 8:00、14:00、20:00&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;h4&gt;重要提示&lt;/h4&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;⚠️ &lt;strong&gt;时区注意&lt;/strong&gt;：GitHub Actions 使用 &lt;strong&gt;UTC 时间&lt;/strong&gt;，北京时间需要 &lt;strong&gt;减 8 小时&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;想要北京时间 8:00 运行 → 设置 UTC 0:00&lt;/li&gt; 
   &lt;li&gt;想要北京时间 20:00 运行 → 设置 UTC 12:00&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/blockquote&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;⚠️ &lt;strong&gt;频率限制&lt;/strong&gt;：GitHub 对每个账号的 Actions 运行次数有限额&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;建议&lt;/strong&gt;：不要设置比 30 分钟更短的间隔&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;原因&lt;/strong&gt;：过于频繁可能被判定为滥用，面临封号风险&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;实际情况&lt;/strong&gt;：GitHub Actions 执行时间本身就有偏差，设置太精确意义不大&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/blockquote&gt; 
 &lt;h4&gt;修改方法&lt;/h4&gt; 
 &lt;ol&gt; 
  &lt;li&gt;打开你 fork 的仓库&lt;/li&gt; 
  &lt;li&gt;找到 &lt;code&gt;.github/workflows/crawler.yml&lt;/code&gt; 文件&lt;/li&gt; 
  &lt;li&gt;点击编辑（铅笔图标）&lt;/li&gt; 
  &lt;li&gt;修改 &lt;code&gt;cron: "0 * * * *"&lt;/code&gt; 中的表达式&lt;/li&gt; 
  &lt;li&gt;点击 "Commit changes" 保存&lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/details&gt; 
&lt;h3&gt;10. 多账号推送配置&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;👉 点击展开：&lt;strong&gt;多账号推送配置详解&lt;/strong&gt;&lt;/summary&gt; 
 &lt;blockquote&gt; 
  &lt;h3&gt;⚠️ &lt;strong&gt;安全警告&lt;/strong&gt;&lt;/h3&gt; 
  &lt;p&gt;&lt;strong&gt;GitHub Fork 用户请勿在 &lt;code&gt;config.yaml&lt;/code&gt; 中配置推送信息！&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;风险说明&lt;/strong&gt;：&lt;code&gt;config.yaml&lt;/code&gt; 会被提交到公开的 Git 仓库，配置推送信息（Webhook URL、Token 等）会泄露敏感数据&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;推荐方式&lt;/strong&gt;： 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;GitHub Actions 用户&lt;/strong&gt; → 使用 GitHub Secrets 环境变量&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Docker 用户&lt;/strong&gt; → 使用 &lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#6-docker-%E9%83%A8%E7%BD%B2"&gt;&lt;code&gt;.env&lt;/code&gt; 文件配置&lt;/a&gt;（&lt;code&gt;.env&lt;/code&gt; 已在 &lt;code&gt;.gitignore&lt;/code&gt; 中，不会被提交）&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;本地开发用户&lt;/strong&gt;：可以在 &lt;code&gt;config.yaml&lt;/code&gt; 中配置（确保不会 push 到公开仓库）&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/blockquote&gt; 
 &lt;h4&gt;支持的渠道&lt;/h4&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;渠道&lt;/th&gt; 
    &lt;th&gt;配置项&lt;/th&gt; 
    &lt;th&gt;是否需要配对&lt;/th&gt; 
    &lt;th&gt;说明&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;飞书&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;feishu_url&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;否&lt;/td&gt; 
    &lt;td&gt;多个 webhook URL&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;钉钉&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;dingtalk_url&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;否&lt;/td&gt; 
    &lt;td&gt;多个 webhook URL&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;企业微信&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;wework_url&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;否&lt;/td&gt; 
    &lt;td&gt;多个 webhook URL&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Telegram&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;telegram_bot_token&lt;/code&gt; + &lt;code&gt;telegram_chat_id&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;✅ 是&lt;/td&gt; 
    &lt;td&gt;token 和 chat_id 数量必须一致&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;ntfy&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;ntfy_topic&lt;/code&gt; + &lt;code&gt;ntfy_token&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;✅ 是&lt;/td&gt; 
    &lt;td&gt;topic 和 token 数量必须一致（token 可选）&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Bark&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;bark_url&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;否&lt;/td&gt; 
    &lt;td&gt;多个推送 URL&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Slack&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;slack_webhook_url&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;否&lt;/td&gt; 
    &lt;td&gt;多个 webhook URL&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;邮件&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;email_to&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;已支持多收件人（逗号分隔），无需修改&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;h4&gt;推荐配置方式 1：GitHub Actions 环境变量&lt;/h4&gt; 
 &lt;p&gt;&lt;strong&gt;配置位置&lt;/strong&gt;：GitHub Repo → Settings → Secrets and variables → Actions → Repository secrets&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;基础配置示例&lt;/strong&gt;：&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# 多账号数量限制
MAX_ACCOUNTS_PER_CHANNEL=3

# 飞书多账号（3个群组）
FEISHU_WEBHOOK_URL=https://hook1.feishu.cn/xxx;https://hook2.feishu.cn/yyy;https://hook3.feishu.cn/zzz

# 钉钉多账号（2个群组）
DINGTALK_WEBHOOK_URL=https://oapi.dingtalk.com/xxx;https://oapi.dingtalk.com/yyy

# 企业微信多账号（2个群组）
WEWORK_WEBHOOK_URL=https://qyapi.weixin.qq.com/cgi-bin/webhook/send?key=xxx;https://qyapi.weixin.qq.com/cgi-bin/webhook/send?key=yyy

# Bark多账号（2个设备）
BARK_URL=https://api.day.app/key1;https://api.day.app/key2

# Slack多账号（2个频道）
SLACK_WEBHOOK_URL=https://hooks.slack.com/xxx;https://hooks.slack.com/yyy
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;配对配置示例（Telegram 和 ntfy）&lt;/strong&gt;：&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;&lt;strong&gt;Telegram 配对配置&lt;/strong&gt;&lt;/summary&gt; 
  &lt;pre&gt;&lt;code class="language-bash"&gt;# ✅ 正确配置：2个token对应2个chat_id
TELEGRAM_BOT_TOKEN=123456:AAA-BBB;789012:CCC-DDD
TELEGRAM_CHAT_ID=-100111;-100222

# ❌ 错误配置：数量不一致，将跳过推送
TELEGRAM_BOT_TOKEN=token1;token2;token3
TELEGRAM_CHAT_ID=id1;id2
&lt;/code&gt;&lt;/pre&gt; 
  &lt;p&gt;&lt;strong&gt;说明&lt;/strong&gt;：&lt;code&gt;token&lt;/code&gt; 和 &lt;code&gt;chat_id&lt;/code&gt; 的数量必须完全一致，否则该渠道推送会被跳过。&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;&lt;strong&gt;ntfy 配对配置&lt;/strong&gt;&lt;/summary&gt; 
  &lt;pre&gt;&lt;code class="language-bash"&gt;# ✅ 正确配置：3个topic，只有第2个需要token
NTFY_TOPIC=topic1;topic2;topic3
NTFY_TOKEN=;token_for_topic2;

# ✅ 正确配置：2个topic都需要token
NTFY_TOPIC=topic1;topic2
NTFY_TOKEN=token1;token2

# ❌ 错误配置：topic和token数量不匹配
NTFY_TOPIC=topic1;topic2
NTFY_TOKEN=token1;token2;token3
&lt;/code&gt;&lt;/pre&gt; 
  &lt;p&gt;&lt;strong&gt;说明&lt;/strong&gt;：&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;如果某个 topic 不需要 token，在对应位置留空（两个分号之间）&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;topic&lt;/code&gt; 和 &lt;code&gt;token&lt;/code&gt; 的数量必须一致&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;hr /&gt; 
 &lt;h4&gt;推荐配置方式 2：Docker 环境变量（.env）&lt;/h4&gt; 
 &lt;p&gt;&lt;strong&gt;配置位置&lt;/strong&gt;：项目根目录 &lt;code&gt;docker/.env&lt;/code&gt; 文件&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;基础配置示例&lt;/strong&gt;：&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# 多账号数量限制
MAX_ACCOUNTS_PER_CHANNEL=3

# 飞书多账号（3个群组）
FEISHU_WEBHOOK_URL=https://hook1.feishu.cn/xxx;https://hook2.feishu.cn/yyy;https://hook3.feishu.cn/zzz

# 钉钉多账号（2个群组）
DINGTALK_WEBHOOK_URL=https://oapi.dingtalk.com/xxx;https://oapi.dingtalk.com/yyy

# 企业微信多账号（2个群组）
WEWORK_WEBHOOK_URL=https://qyapi.weixin.qq.com/cgi-bin/webhook/send?key=xxx;https://qyapi.weixin.qq.com/cgi-bin/webhook/send?key=yyy

# Bark多账号（2个设备）
BARK_URL=https://api.day.app/key1;https://api.day.app/key2

# Slack多账号（2个频道）
SLACK_WEBHOOK_URL=https://hooks.slack.com/xxx;https://hooks.slack.com/yyy
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;配对配置示例（Telegram 和 ntfy）&lt;/strong&gt;：&lt;/p&gt; 
 &lt;details&gt; 
  &lt;summary&gt;&lt;strong&gt;Telegram 配对配置&lt;/strong&gt;&lt;/summary&gt; 
  &lt;pre&gt;&lt;code class="language-bash"&gt;# ✅ 正确配置：2个token对应2个chat_id
TELEGRAM_BOT_TOKEN=123456:AAA-BBB;789012:CCC-DDD
TELEGRAM_CHAT_ID=-100111;-100222

# ❌ 错误配置：数量不一致，将跳过推送
TELEGRAM_BOT_TOKEN=token1;token2;token3
TELEGRAM_CHAT_ID=id1;id2
&lt;/code&gt;&lt;/pre&gt; 
  &lt;p&gt;&lt;strong&gt;说明&lt;/strong&gt;：&lt;code&gt;token&lt;/code&gt; 和 &lt;code&gt;chat_id&lt;/code&gt; 的数量必须完全一致，否则该渠道推送会被跳过。&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;&lt;strong&gt;ntfy 配对配置&lt;/strong&gt;&lt;/summary&gt; 
  &lt;pre&gt;&lt;code class="language-bash"&gt;# ✅ 正确配置：3个topic，只有第2个需要token
NTFY_TOPIC=topic1;topic2;topic3
NTFY_TOKEN=;token_for_topic2;

# ✅ 正确配置：2个topic都需要token
NTFY_TOPIC=topic1;topic2
NTFY_TOKEN=token1;token2

# ❌ 错误配置：topic和token数量不匹配
NTFY_TOPIC=topic1;topic2
NTFY_TOKEN=token1;token2;token3
&lt;/code&gt;&lt;/pre&gt; 
  &lt;p&gt;&lt;strong&gt;说明&lt;/strong&gt;：&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;如果某个 topic 不需要 token，在对应位置留空（两个分号之间）&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;topic&lt;/code&gt; 和 &lt;code&gt;token&lt;/code&gt; 的数量必须一致&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;hr /&gt; 
 &lt;h4&gt;推送行为说明&lt;/h4&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;strong&gt;独立推送&lt;/strong&gt;：每个账号独立发送，一个失败不影响其他账号&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;部分成功判定&lt;/strong&gt;：只要有一个账号发送成功，整体视为成功&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;日志区分&lt;/strong&gt;：多账号时日志会显示"账号1"、"账号2"等标签&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;批次间隔&lt;/strong&gt;：多账号会增加总发送时间（每个账号独立计算批次间隔）&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;hr /&gt; 
 &lt;h4&gt;常见问题&lt;/h4&gt; 
 &lt;details&gt; 
  &lt;summary&gt;&lt;strong&gt;Q1: 超过 3 个账号会怎样？&lt;/strong&gt;&lt;/summary&gt; 
  &lt;br /&gt; 
  &lt;p&gt;系统会自动截断到配置的最大数量，并输出警告日志。可通过 &lt;code&gt;max_accounts_per_channel&lt;/code&gt; 调整限制。&lt;/p&gt; 
  &lt;p&gt;&lt;strong&gt;⚠️ GitHub Actions 用户特别注意&lt;/strong&gt;：&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;不建议配置过多账号&lt;/strong&gt;（建议不超过 3 个），可能导致： 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;触发 GitHub Actions 速率限制&lt;/strong&gt;：频繁的网络请求可能被识别为异常行为&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;潜在账号风险&lt;/strong&gt;：过度使用 GitHub Actions 资源可能影响账号状态&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;&lt;strong&gt;Q2: 多账号会影响推送速度吗？&lt;/strong&gt;&lt;/summary&gt; 
  &lt;br /&gt; 
  &lt;p&gt;会。每个账号独立发送，总时间 = 账号数 × 单账号发送时间。建议控制账号数量。&lt;/p&gt; 
 &lt;/details&gt; 
 &lt;details&gt; 
  &lt;summary&gt;&lt;strong&gt;Q3: 本地开发用户如何在 config.yaml 中配置？&lt;/strong&gt;&lt;/summary&gt; 
  &lt;br /&gt; 
  &lt;p&gt;如果你是本地开发且&lt;strong&gt;不会将代码推送到公开仓库&lt;/strong&gt;，可以直接在 &lt;code&gt;config/config.yaml&lt;/code&gt; 中配置：&lt;/p&gt; 
  &lt;pre&gt;&lt;code class="language-yaml"&gt;notification:
  enable_notification: true
  max_accounts_per_channel: 3

  webhooks:
    feishu_url: "https://hook1.feishu.cn/xxx;https://hook2.feishu.cn/yyy"
    telegram_bot_token: "token1;token2"
    telegram_chat_id: "id1;id2"
&lt;/code&gt;&lt;/pre&gt; 
  &lt;p&gt;&lt;strong&gt;⚠️ 重要提醒&lt;/strong&gt;：&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;确保 &lt;code&gt;config/config.yaml&lt;/code&gt; 在 &lt;code&gt;.gitignore&lt;/code&gt; 中（如果会提交代码）&lt;/li&gt; 
   &lt;li&gt;或者只在本地开发环境使用，&lt;strong&gt;绝不提交到公开仓库&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/details&gt; 
&lt;/details&gt; 
&lt;h3&gt;11. 存储配置&lt;/h3&gt; 
&lt;details id="storage-config"&gt; 
 &lt;summary&gt;👉 点击展开：&lt;strong&gt;存储架构配置详解&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;h4&gt;存储后端选择&lt;/h4&gt; 
 &lt;p&gt;&lt;strong&gt;配置位置&lt;/strong&gt;：&lt;code&gt;config/config.yaml&lt;/code&gt; 的 &lt;code&gt;storage&lt;/code&gt; 部分&lt;/p&gt; 
 &lt;p&gt;v4.0.0 版本重构了存储架构，支持多种存储后端：&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-yaml"&gt;storage:
  backend: auto  # 存储后端：auto（自动选择）/ local（本地SQLite）/ remote（远程云存储）

  formats:
    sqlite: true   # 是否启用SQLite存储
    txt: true      # 是否生成TXT快照
    html: true     # 是否生成HTML报告

  local:
    data_dir: "output"    # 本地存储目录
    retention_days: 0     # 本地数据保留天数，0表示永久保留

  remote:
    endpoint_url: ""      # S3 API 端点
    bucket_name: ""       # 存储桶名称
    access_key_id: ""     # 访问密钥ID
    secret_access_key: "" # 访问密钥
    region: ""            # 区域（可选）
    retention_days: 0     # 远程数据保留天数，0表示永久保留

  pull:
    enabled: false        # 是否启用启动时从远程拉取数据
    days: 7               # 拉取最近N天的数据
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;后端选择策略&lt;/h4&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;backend 值&lt;/th&gt; 
    &lt;th&gt;说明&lt;/th&gt; 
    &lt;th&gt;适用场景&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;auto&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;自动选择&lt;/strong&gt;（推荐）&lt;/td&gt; 
    &lt;td&gt;根据运行环境智能选择：&lt;br /&gt;• GitHub Actions → Remote&lt;br /&gt;• Docker/本地 → Local&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;local&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;本地 SQLite 数据库&lt;/td&gt; 
    &lt;td&gt;Docker 部署、本地开发&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;code&gt;remote&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;远程云存储（S3 兼容，如 Cloudflare R2）&lt;/td&gt; 
    &lt;td&gt;GitHub Actions、多机器同步&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;h4&gt;远程云存储配置&lt;/h4&gt; 
 &lt;p&gt;&lt;strong&gt;环境变量&lt;/strong&gt;（推荐方式）：&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# GitHub Actions / Docker 环境变量
STORAGE_BACKEND=remote  # 或 auto

# 本地/远程数据保留天数（0 表示永久保留）
LOCAL_RETENTION_DAYS=0
REMOTE_RETENTION_DAYS=0

# S3 兼容存储配置（以 Cloudflare R2 为例）
S3_BUCKET_NAME=your-bucket-name
S3_ACCESS_KEY_ID=your-access-key-id
S3_SECRET_ACCESS_KEY=your-secret-access-key
S3_ENDPOINT_URL=https://&amp;lt;account-id&amp;gt;.r2.cloudflarestorage.com
S3_REGION=auto

# 数据拉取配置（可选，从远程同步到本地）
PULL_ENABLED=false
PULL_DAYS=7
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;获取凭据&lt;/strong&gt;：参见 &lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#-%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B"&gt;快速开始 - 远程存储配置&lt;/a&gt;&lt;/p&gt; 
 &lt;h4&gt;数据清理策略&lt;/h4&gt; 
 &lt;p&gt;&lt;strong&gt;自动清理&lt;/strong&gt;：每次运行结束时检查并删除超过保留天数的数据。&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-yaml"&gt;storage:
  local:
    retention_days: 30  # 本地保留最近30天数据
  remote:
    retention_days: 30  # 远程保留最近30天数据
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;清理逻辑&lt;/strong&gt;：&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;本地存储：删除过期日期的文件夹（如 &lt;code&gt;output/2025-11-10/&lt;/code&gt;）&lt;/li&gt; 
  &lt;li&gt;远程存储：批量删除过期的云端对象（如 &lt;code&gt;news/2025-11-10.db&lt;/code&gt;）&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h4&gt;时区配置（v4.0.0 新增）&lt;/h4&gt; 
 &lt;p&gt;&lt;strong&gt;全球时区支持&lt;/strong&gt;：解决非中国用户推送时间窗口问题。&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-yaml"&gt;app:
  timezone: "Asia/Shanghai"  # 默认中国时区
  # 其他示例：
  # timezone: "America/Los_Angeles"  # 美西时间
  # timezone: "Europe/London"        # 英国时间
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;支持所有 IANA 时区名称&lt;/strong&gt;：&lt;a href="https://en.wikipedia.org/wiki/List_of_tz_database_time_zones"&gt;时区列表&lt;/a&gt;&lt;/p&gt; 
 &lt;h4&gt;不兼容变更&lt;/h4&gt; 
 &lt;p&gt;⚠️ &lt;strong&gt;v4.0.0 不兼容 v3.x 数据&lt;/strong&gt;：&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;数据库结构完全重构，无法读取旧数据&lt;/li&gt; 
  &lt;li&gt;文件路径格式变更（ISO 格式）&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;&lt;strong&gt;迁移建议&lt;/strong&gt;：&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;从 v4.0.0 开始重新收集数据&lt;/li&gt; 
  &lt;li&gt;旧数据如需保留，请手动重命名目录格式（不推荐）&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;br /&gt; 
&lt;h2&gt;🤖 AI 智能分析&lt;/h2&gt; 
&lt;p&gt;TrendRadar v3.0.0 新增了基于 &lt;strong&gt;MCP (Model Context Protocol)&lt;/strong&gt; 的 AI 分析功能，让你可以通过自然语言与新闻数据对话，进行深度分析。&lt;/p&gt; 
&lt;h3&gt;⚠️ 使用前必读&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;重要提示：AI 功能需要本地新闻数据支持&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;AI 分析功能&lt;strong&gt;不是&lt;/strong&gt;直接查询网络实时数据，而是分析你&lt;strong&gt;本地已积累的新闻数据&lt;/strong&gt;（存储在 &lt;code&gt;output&lt;/code&gt; 文件夹中）&lt;/p&gt; 
&lt;h4&gt;使用说明：&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;项目自带测试数据&lt;/strong&gt;：&lt;code&gt;output&lt;/code&gt; 目录默认包含 &lt;strong&gt;2025-11-01～2025-11-15&lt;/strong&gt; 的新闻数据，可用于快速体验 AI 功能&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;查询限制&lt;/strong&gt;：&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;✅ 只能查询已有日期范围内的数据（11月1-15日）&lt;/li&gt; 
   &lt;li&gt;❌ 无法查询实时新闻或未来日期&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;获取最新数据&lt;/strong&gt;：&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;测试数据仅供快速体验，&lt;strong&gt;建议自行部署项目&lt;/strong&gt;获取实时数据&lt;/li&gt; 
   &lt;li&gt;按照 &lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#-%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B"&gt;快速开始&lt;/a&gt; 部署运行项目&lt;/li&gt; 
   &lt;li&gt;等待至少 1 天积累新闻数据后，即可查询最新热点&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;1. 快速部署&lt;/h3&gt; 
&lt;p&gt;Cherry Studio 提供 GUI 配置界面，5 分钟快速部署，复杂的部分是一键安装的。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;图文部署教程&lt;/strong&gt;：现已更新到我的&lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#%E9%97%AE%E9%A2%98%E7%AD%94%E7%96%91%E4%B8%8E%E4%BA%A4%E6%B5%81"&gt;公众号&lt;/a&gt;，回复 "mcp" 即可&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;详细部署教程&lt;/strong&gt;：&lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/README-Cherry-Studio.md"&gt;README-Cherry-Studio.md&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;部署模式说明&lt;/strong&gt;：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;STDIO 模式（推荐）&lt;/strong&gt;：一次配置后续无需重复配置，&lt;strong&gt;图文部署教程&lt;/strong&gt;中仅以此模式的配置为例。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;HTTP 模式（备选）&lt;/strong&gt;：如果 STDIO 模式配置遇到问题，可使用 HTTP 模式。此模式的配置方式与 STDIO 基本一致，但复制粘贴的内容就一行，不易出错。唯一需要注意的是每次使用前都需要手动启动一下服务。详细请参考 &lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/README-Cherry-Studio.md"&gt;README-Cherry-Studio.md&lt;/a&gt; 底部的 HTTP 模式说明。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;2. 学习与 AI 对话的姿势&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;详细对话教程&lt;/strong&gt;：&lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/README-MCP-FAQ.md"&gt;README-MCP-FAQ.md&lt;/a&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;👉 点击展开：&lt;strong&gt;查看 AI 对话示例图&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;💡 &lt;strong&gt;提示&lt;/strong&gt;：实际不建议一次性问多个问题。如果你选择的 AI 模型连下图的按顺序调用都无法做到，建议换一个。&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;img src="https://raw.githubusercontent.com/sansan0/TrendRadar/master/_image/ai2.png" alt="mcp 使用效果图" width="600" /&gt; 
&lt;/details&gt; 
&lt;br /&gt; 
&lt;h2&gt;🔌 MCP 客户端&lt;/h2&gt; 
&lt;p&gt;TrendRadar MCP 服务支持标准的 Model Context Protocol (MCP) 协议，可以接入各种支持 MCP 的 AI 客户端进行智能分析。&lt;/p&gt; 
&lt;h3&gt;支持的客户端&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;注意事项&lt;/strong&gt;：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;将 &lt;code&gt;/path/to/TrendRadar&lt;/code&gt; 替换为你的项目实际路径&lt;/li&gt; 
 &lt;li&gt;Windows 路径使用双反斜杠：&lt;code&gt;C:\\Users\\YourName\\TrendRadar&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;保存后记得重启&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;👉 点击展开：&lt;b&gt;Claude Desktop&lt;/b&gt;&lt;/summary&gt; 
 &lt;h4&gt;配置文件方式&lt;/h4&gt; 
 &lt;p&gt;编辑 Claude Desktop 的 MCP 配置文件：&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Windows&lt;/strong&gt;： &lt;code&gt;%APPDATA%\Claude\claude_desktop_config.json&lt;/code&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Mac&lt;/strong&gt;： &lt;code&gt;~/Library/Application Support/Claude/claude_desktop_config.json&lt;/code&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;配置内容&lt;/strong&gt;：&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
  "mcpServers": {
    "trendradar": {
      "command": "uv",
      "args": [
        "--directory",
        "/path/to/TrendRadar",
        "run",
        "python",
        "-m",
        "mcp_server.server"
      ],
      "env": {},
      "disabled": false,
      "alwaysAllow": []
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;👉 点击展开：&lt;b&gt;Cursor&lt;/b&gt;&lt;/summary&gt; 
 &lt;h4&gt;方式一：HTTP 模式&lt;/h4&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;启动 HTTP 服务&lt;/strong&gt;：&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# Windows
start-http.bat

# Mac/Linux
./start-http.sh
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;配置 Cursor&lt;/strong&gt;：&lt;/p&gt; &lt;p&gt;&lt;strong&gt;项目级配置&lt;/strong&gt;（推荐）： 在项目根目录创建 &lt;code&gt;.cursor/mcp.json&lt;/code&gt;：&lt;/p&gt; &lt;pre&gt;&lt;code class="language-json"&gt;{
  "mcpServers": {
    "trendradar": {
      "url": "http://localhost:3333/mcp",
      "description": "TrendRadar 新闻热点聚合分析"
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;全局配置&lt;/strong&gt;： 在用户目录创建 &lt;code&gt;~/.cursor/mcp.json&lt;/code&gt;（同样内容）&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;使用步骤&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;保存配置文件后重启 Cursor&lt;/li&gt; 
    &lt;li&gt;在聊天界面的 "Available Tools" 中查看已连接的工具&lt;/li&gt; 
    &lt;li&gt;开始使用：&lt;code&gt;搜索今天的"AI"相关新闻&lt;/code&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;h4&gt;方式二：STDIO 模式（推荐）&lt;/h4&gt; 
 &lt;p&gt;创建 &lt;code&gt;.cursor/mcp.json&lt;/code&gt;：&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
  "mcpServers": {
    "trendradar": {
      "command": "uv",
      "args": [
        "--directory",
        "/path/to/TrendRadar",
        "run",
        "python",
        "-m",
        "mcp_server.server"
      ]
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;👉 点击展开：&lt;b&gt;VSCode (Cline/Continue)&lt;/b&gt;&lt;/summary&gt; 
 &lt;h4&gt;Cline 配置&lt;/h4&gt; 
 &lt;p&gt;在 Cline 的 MCP 设置中添加：&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;HTTP 模式&lt;/strong&gt;：&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
  "trendradar": {
    "url": "http://localhost:3333/mcp",
    "type": "streamableHttp",
    "autoApprove": [],
    "disabled": false
  }
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;STDIO 模式&lt;/strong&gt;（推荐）：&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
  "trendradar": {
    "command": "uv",
    "args": [
      "--directory",
      "/path/to/TrendRadar",
      "run",
      "python",
      "-m",
      "mcp_server.server"
    ],
    "type": "stdio",
    "disabled": false
  }
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Continue 配置&lt;/h4&gt; 
 &lt;p&gt;编辑 &lt;code&gt;~/.continue/config.json&lt;/code&gt;：&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
  "experimental": {
    "modelContextProtocolServers": [
      {
        "transport": {
          "type": "stdio",
          "command": "uv",
          "args": [
            "--directory",
            "/path/to/TrendRadar",
            "run",
            "python",
            "-m",
            "mcp_server.server"
          ]
        }
      }
    ]
  }
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;使用示例&lt;/strong&gt;：&lt;/p&gt; 
 &lt;pre&gt;&lt;code&gt;分析最近7天"特斯拉"的热度变化趋势
生成今天的热点摘要报告
搜索"比特币"相关新闻并分析情感倾向
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;👉 点击展开：&lt;b&gt;Claude Code CLI&lt;/b&gt;&lt;/summary&gt; 
 &lt;h4&gt;HTTP 模式配置&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# 1. 启动 HTTP 服务
# Windows: start-http.bat
# Mac/Linux: ./start-http.sh

# 2. 添加 MCP 服务器
claude mcp add --transport http trendradar http://localhost:3333/mcp

# 3. 验证连接（确保服务已启动）
claude mcp list
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;使用示例&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# 查询新闻
claude "搜索今天知乎的热点新闻，前10条"

# 趋势分析
claude "分析'人工智能'这个话题最近一周的热度趋势"

# 数据对比
claude "对比知乎和微博平台对'比特币'的关注度"
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;👉 点击展开：&lt;b&gt;MCP Inspector&lt;/b&gt;（调试工具）&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p&gt;MCP Inspector 是官方调试工具，用于测试 MCP 连接：&lt;/p&gt; 
 &lt;h4&gt;使用步骤&lt;/h4&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;启动 TrendRadar HTTP 服务&lt;/strong&gt;：&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# Windows
start-http.bat

# Mac/Linux
./start-http.sh
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;启动 MCP Inspector&lt;/strong&gt;：&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;npx @modelcontextprotocol/inspector
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;在浏览器中连接&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;访问：&lt;code&gt;http://localhost:3333/mcp&lt;/code&gt;&lt;/li&gt; 
    &lt;li&gt;测试 "Ping Server" 功能验证连接&lt;/li&gt; 
    &lt;li&gt;检查 "List Tools" 是否返回 13 个工具： 
     &lt;ul&gt; 
      &lt;li&gt;基础查询：get_latest_news, get_news_by_date, get_trending_topics&lt;/li&gt; 
      &lt;li&gt;智能检索：search_news, search_related_news_history&lt;/li&gt; 
      &lt;li&gt;高级分析：analyze_topic_trend, analyze_data_insights, analyze_sentiment, find_similar_news, generate_summary_report&lt;/li&gt; 
      &lt;li&gt;系统管理：get_current_config, get_system_status, trigger_crawl&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;👉 点击展开：&lt;b&gt;其他支持 MCP 的客户端&lt;/b&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p&gt;任何支持 Model Context Protocol 的客户端都可以连接 TrendRadar：&lt;/p&gt; 
 &lt;h4&gt;HTTP 模式&lt;/h4&gt; 
 &lt;p&gt;&lt;strong&gt;服务地址&lt;/strong&gt;：&lt;code&gt;http://localhost:3333/mcp&lt;/code&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;基本配置模板&lt;/strong&gt;：&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
  "name": "trendradar",
  "url": "http://localhost:3333/mcp",
  "type": "http",
  "description": "新闻热点聚合分析"
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;STDIO 模式（推荐）&lt;/h4&gt; 
 &lt;p&gt;&lt;strong&gt;基本配置模板&lt;/strong&gt;：&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-json"&gt;{
  "name": "trendradar",
  "command": "uv",
  "args": [
    "--directory",
    "/path/to/TrendRadar",
    "run",
    "python",
    "-m",
    "mcp_server.server"
  ],
  "type": "stdio"
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;注意事项&lt;/strong&gt;：&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;替换 &lt;code&gt;/path/to/TrendRadar&lt;/code&gt; 为实际项目路径&lt;/li&gt; 
  &lt;li&gt;Windows 路径使用反斜杠转义：&lt;code&gt;C:\\Users\\...&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;确保已完成项目依赖安装（运行过 setup 脚本）&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;常见问题&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;👉 点击展开：&lt;b&gt;Q1: HTTP 服务无法启动？&lt;/b&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;strong&gt;检查步骤&lt;/strong&gt;：&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;确认端口 3333 未被占用：&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# Windows
netstat -ano | findstr :3333

# Mac/Linux
lsof -i :3333
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;检查项目依赖是否安装：&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# 重新运行安装脚本
# Windows: setup-windows.bat 或者 setup-windows-en.bat
# Mac/Linux: ./setup-mac.sh
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;查看详细错误日志：&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;uv run python -m mcp_server.server --transport http --port 3333
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;尝试自定义端口:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;uv run python -m mcp_server.server --transport http --port 33333
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;👉 点击展开：&lt;b&gt;Q2: 客户端无法连接到 MCP 服务？&lt;/b&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;strong&gt;解决方案&lt;/strong&gt;：&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;STDIO 模式&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;确认 UV 路径正确（运行 &lt;code&gt;which uv&lt;/code&gt; 或 &lt;code&gt;where uv&lt;/code&gt;）&lt;/li&gt; 
    &lt;li&gt;确认项目路径正确且无中文字符&lt;/li&gt; 
    &lt;li&gt;查看客户端错误日志&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;HTTP 模式&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;确认服务已启动（访问 &lt;code&gt;http://localhost:3333/mcp&lt;/code&gt;）&lt;/li&gt; 
    &lt;li&gt;检查防火墙设置&lt;/li&gt; 
    &lt;li&gt;尝试使用 127.0.0.1 替代 localhost&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;通用检查&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;重启客户端应用&lt;/li&gt; 
    &lt;li&gt;查看 MCP 服务日志&lt;/li&gt; 
    &lt;li&gt;使用 MCP Inspector 测试连接&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;👉 点击展开：&lt;b&gt;Q3: 工具调用失败或返回错误？&lt;/b&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;strong&gt;可能原因&lt;/strong&gt;：&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;数据不存在&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;确认已运行过爬虫（有 output 目录数据）&lt;/li&gt; 
    &lt;li&gt;检查查询日期范围是否有数据&lt;/li&gt; 
    &lt;li&gt;查看 output 目录的可用日期&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;参数错误&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;检查日期格式：&lt;code&gt;YYYY-MM-DD&lt;/code&gt;&lt;/li&gt; 
    &lt;li&gt;确认平台 ID 正确：&lt;code&gt;zhihu&lt;/code&gt;, &lt;code&gt;weibo&lt;/code&gt; 等&lt;/li&gt; 
    &lt;li&gt;查看工具文档中的参数说明&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;配置问题&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;确认 &lt;code&gt;config/config.yaml&lt;/code&gt; 存在&lt;/li&gt; 
    &lt;li&gt;确认 &lt;code&gt;config/frequency_words.txt&lt;/code&gt; 存在&lt;/li&gt; 
    &lt;li&gt;检查配置文件格式是否正确&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/details&gt; 
&lt;br /&gt; 
&lt;h2&gt;☕问题答疑与交流&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;如果你想支持本项目，可通过微信搜索&lt;strong&gt;腾讯公益&lt;/strong&gt;，对里面的&lt;strong&gt;助学&lt;/strong&gt;相关的项目随心捐助&lt;/p&gt; 
 &lt;p&gt;感谢参与过&lt;strong&gt;一元点赞&lt;/strong&gt;的朋友，已收录至顶部&lt;strong&gt;致谢名单&lt;/strong&gt;！你们的支持让开源维护更有动力，个人打赏码现已移除。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;GitHub Issues&lt;/strong&gt;：适合针对性强的解答。提问时请提供完整信息（截图、错误日志、系统环境等）。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;公众号交流&lt;/strong&gt;：适合快速咨询。建议优先在相关文章下的公共留言区交流，如私信，请文明礼貌用语😉&lt;/li&gt; 
 &lt;li&gt;💡 部署成功了？来公众号说说感受吧，你的点赞和留言都是我继续更新的动力~&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;公众号关注&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/sansan0/TrendRadar/master/_image/weixin.png" width="400" title="硅基茶水间" /&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;h2&gt;🪄 赞助商&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;每天追踪这么多热点，写报告、回复消息是否让手腕疲惫？&lt;br /&gt; 试试「闪电说」AI 语音输入法 —— 用说的，比打字快 4 倍 ⚡ 。从看热点到输出内容，让效率翻倍 👇&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://shandianshuo.cn"&gt;&lt;img src="https://img.shields.io/badge/Mac-%E5%85%8D%E8%B4%B9%E4%B8%8B%E8%BD%BD-FF6B6B?style=for-the-badge&amp;amp;logo=apple&amp;amp;logoColor=white" alt="Mac下载" /&gt;&lt;/a&gt; &lt;a href="https://shandianshuo.cn"&gt;&lt;img src="https://img.shields.io/badge/Windows-%E5%85%8D%E8%B4%B9%E4%B8%8B%E8%BD%BD-FF6B6B?style=for-the-badge&amp;amp;logo=lightning&amp;amp;logoColor=white" alt="Windows下载" /&gt;&lt;/a&gt; &lt;a href="https://shandianshuo.cn" target="_blank"&gt; &lt;img src="https://raw.githubusercontent.com/sansan0/TrendRadar/master/_image/banner-shandianshuo.png" alt="闪电说" width="700" /&gt; &lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;br /&gt; 
&lt;h2&gt;📚 项目相关&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;4 篇文章&lt;/strong&gt;：&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://mp.weixin.qq.com/s/KYEPfTPVzZNWFclZh4am_g"&gt;可在该文章下方留言，方便项目作者用手机答疑&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://mp.weixin.qq.com/s/jzn0vLiQFX408opcfpPPxQ"&gt;2个月破 1000 star，我的GitHub项目推广实战经验&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://mp.weixin.qq.com/s/C8evK-U7onG1sTTdwdW2zg"&gt;github fork 运行本项目的注意事项 &lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://mp.weixin.qq.com/s/8ghyfDAtQZjLrnWTQabYOQ"&gt;基于本项目，如何开展公众号或者新闻资讯类文章写作&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;AI 开发&lt;/strong&gt;：&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;如果你有小众需求，完全可以基于我的项目自行开发，零编程基础的也可以试试&lt;/li&gt; 
 &lt;li&gt;我所有的开源项目或多或少都使用了自己写的&lt;strong&gt;AI辅助软件&lt;/strong&gt;来提升开发效率，这款工具已开源&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;核心功能&lt;/strong&gt;：迅速筛选项目代码喂给AI，你只需要补充个人需求即可&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;项目地址&lt;/strong&gt;：&lt;a href="https://github.com/sansan0/ai-code-context-helper"&gt;https://github.com/sansan0/ai-code-context-helper&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;其余项目&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;📍 毛主席足迹地图 - 交互式动态展示1893-1976年完整轨迹。欢迎诸位同志贡献数据&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/sansan0/mao-map"&gt;https://github.com/sansan0/mao-map&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;哔哩哔哩(bilibili)评论区数据可视化分析软件&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/sansan0/bilibili-comment-analyzer"&gt;https://github.com/sansan0/bilibili-comment-analyzer&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;本项目流程图&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-mermaid"&gt;flowchart TD
    A[👤 用户开始] --&amp;gt; B{🚀 选择部署方式}
    
    B --&amp;gt;|云端部署| C1[🍴 Fork 项目到 GitHub]
    B --&amp;gt;|本地部署| C2[🐳 Docker 部署]
    
    C1 --&amp;gt; D[⚙️ 配置通知渠道&amp;lt;br/&amp;gt;可同时配置多个]
    C2 --&amp;gt; D
    
    D --&amp;gt; E[选择通知方式：&amp;lt;br/&amp;gt;📱企业微信 💬飞书 🔔钉钉&amp;lt;br/&amp;gt;📟Telegram 📧邮件]
    
    E --&amp;gt; F[🔑 填写通知参数&amp;lt;br/&amp;gt;GitHub Secrets 或环境变量]
    
    F --&amp;gt; G[📝 配置关键词&amp;lt;br/&amp;gt;config/frequency_words.txt&amp;lt;br/&amp;gt;普通词/必须词+/过滤词!]
    
    G --&amp;gt; H[🎯 选择运行模式&amp;lt;br/&amp;gt;config/config.yaml]
    
    H --&amp;gt; H1[📋 daily - 当日汇总&amp;lt;br/&amp;gt;定时推送所有匹配新闻]
    H --&amp;gt; H2[📰 current - 当前榜单&amp;lt;br/&amp;gt;定时推送最新榜单]
    H --&amp;gt; H3[📈 incremental - 增量监控&amp;lt;br/&amp;gt;仅推送新增内容]
    
    H1 --&amp;gt; I[可选：推送时间窗口控制&amp;lt;br/&amp;gt;⏰ 限制推送时间范围]
    H2 --&amp;gt; I
    H3 --&amp;gt; I
    
    I --&amp;gt; J[✅ 配置完成]
    
    J --&amp;gt; K[🤖 系统自动运行]
    
    K --&amp;gt; L[🕷️ 爬取11+平台热点]
    L --&amp;gt; M[🔍 关键词筛选]
    M --&amp;gt; N[⚖️ 权重算法排序&amp;lt;br/&amp;gt;排名60% + 频次30% + 热度10%]
    N --&amp;gt; O[📊 生成报告&amp;lt;br/&amp;gt;HTML网页 + 推送消息]
    O --&amp;gt; P[📱 多渠道推送通知]
    
    P --&amp;gt; Q[🎉 持续接收精准推送&amp;lt;br/&amp;gt;告别信息过载]
    
    style A fill:#e3f2fd
    style B fill:#f3e5f5
    style D fill:#fff3e0
    style F fill:#fff9c4
    style G fill:#e8f5e9
    style H fill:#e0f2f1
    style I fill:#fce4ec
    style O fill:#e1bee7
    style Q fill:#c8e6c9
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href="https://www.star-history.com/#sansan0/TrendRadar&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=sansan0/TrendRadar&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;h2&gt;📄 许可证&lt;/h2&gt; 
&lt;p&gt;GPL-3.0 License&lt;/p&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#trendradar"&gt;🔝 回到顶部&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt;</description>
    </item>
    
  </channel>
</rss>