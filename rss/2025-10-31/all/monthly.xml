<rss version="2.0">
  <channel>
    <title>GitHub All Languages Monthly Trending</title>
    <description>Monthly Trending of All Languages in GitHub</description>
    <pubDate>Thu, 30 Oct 2025 01:54:21 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>cjpais/Handy</title>
      <link>https://github.com/cjpais/Handy</link>
      <description>&lt;p&gt;A free, open source, and extensible speech-to-text application that works completely offline.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Handy&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://discord.com/invite/WVBeWsNXK4"&gt;&lt;img src="https://img.shields.io/badge/Discord-%235865F2.svg?style=for-the-badge&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;A free, open source, and extensible speech-to-text application that works completely offline.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Handy is a cross-platform desktop application built with Tauri (Rust + React/TypeScript) that provides simple, privacy-focused speech transcription. Press a shortcut, speak, and have your words appear in any text field—all without sending your voice to the cloud.&lt;/p&gt; 
&lt;h2&gt;Why Handy?&lt;/h2&gt; 
&lt;p&gt;Handy was created to fill the gap for a truly open source, extensible speech-to-text tool. As stated on &lt;a href="https://handy.computer"&gt;handy.computer&lt;/a&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Free&lt;/strong&gt;: Accessibility tooling belongs in everyone's hands, not behind a paywall&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Open Source&lt;/strong&gt;: Together we can build further. Extend Handy for yourself and contribute to something bigger&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Private&lt;/strong&gt;: Your voice stays on your computer. Get transcriptions without sending audio to the cloud&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Simple&lt;/strong&gt;: One tool, one job. Transcribe what you say and put it into a text box&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Handy isn't trying to be the best speech-to-text app—it's trying to be the most forkable one.&lt;/p&gt; 
&lt;h2&gt;How It Works&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Press&lt;/strong&gt; a configurable keyboard shortcut to start/stop recording (or use push-to-talk mode)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Speak&lt;/strong&gt; your words while the shortcut is active&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Release&lt;/strong&gt; and Handy processes your speech using Whisper&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Get&lt;/strong&gt; your transcribed text pasted directly into whatever app you're using&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;The process is entirely local:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Silence is filtered using VAD (Voice Activity Detection) with Silero&lt;/li&gt; 
 &lt;li&gt;Transcription uses your choice of models: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Whisper models&lt;/strong&gt; (Small/Medium/Turbo/Large) with GPU acceleration when available&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Parakeet V3&lt;/strong&gt; - CPU-optimized model with excellent performance and automatic language detection&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Works on Windows, macOS, and Linux&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Download the latest release from the &lt;a href="https://github.com/cjpais/Handy/releases"&gt;releases page&lt;/a&gt; or the &lt;a href="https://handy.computer"&gt;website&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Install the application following platform-specific instructions&lt;/li&gt; 
 &lt;li&gt;Launch Handy and grant necessary system permissions (microphone, accessibility)&lt;/li&gt; 
 &lt;li&gt;Configure your preferred keyboard shortcuts in Settings&lt;/li&gt; 
 &lt;li&gt;Start transcribing!&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Development Setup&lt;/h3&gt; 
&lt;p&gt;For detailed build instructions including platform-specific requirements, see &lt;a href="https://raw.githubusercontent.com/cjpais/Handy/main/BUILD.md"&gt;BUILD.md&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Architecture&lt;/h2&gt; 
&lt;p&gt;Handy is built as a Tauri application combining:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Frontend&lt;/strong&gt;: React + TypeScript with Tailwind CSS for the settings UI&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Backend&lt;/strong&gt;: Rust for system integration, audio processing, and ML inference&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Core Libraries&lt;/strong&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;whisper-rs&lt;/code&gt;: Local speech recognition with Whisper models&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;transcription-rs&lt;/code&gt;: CPU-optimized speech recognition with Parakeet models&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;cpal&lt;/code&gt;: Cross-platform audio I/O&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;vad-rs&lt;/code&gt;: Voice Activity Detection&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;rdev&lt;/code&gt;: Global keyboard shortcuts and system events&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;rubato&lt;/code&gt;: Audio resampling&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Debug Mode&lt;/h3&gt; 
&lt;p&gt;Handy includes an advanced debug mode for development and troubleshooting. Access it by pressing:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;macOS&lt;/strong&gt;: &lt;code&gt;Cmd+Shift+D&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Windows/Linux&lt;/strong&gt;: &lt;code&gt;Ctrl+Shift+D&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Known Issues &amp;amp; Current Limitations&lt;/h2&gt; 
&lt;p&gt;This project is actively being developed and has some &lt;a href="https://github.com/cjpais/Handy/issues"&gt;known issues&lt;/a&gt;. We believe in transparency about the current state:&lt;/p&gt; 
&lt;h3&gt;Platform Support&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;macOS (both Intel and Apple Silicon)&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;x64 Windows&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;x64 Linux&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;System Requirements/Recommendations&lt;/h3&gt; 
&lt;p&gt;The following are recommendations for running Handy on your own machine. If you don't meet the system requirements, the performance of the application may be degraded. We are working on improving the performance across all kinds of computers and hardware.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;For Whisper Models:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;macOS&lt;/strong&gt;: M series Mac, Intel Mac&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt;: Intel, AMD, or NVIDIA GPU&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Linux&lt;/strong&gt;: Intel, AMD, or NVIDIA GPU 
  &lt;ul&gt; 
   &lt;li&gt;Ubuntu 22.04, 24.04&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;For Parakeet V3 Model:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;CPU-only operation&lt;/strong&gt; - runs on a wide variety of hardware&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Minimum&lt;/strong&gt;: Intel Skylake (6th gen) or equivalent AMD processors&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Performance&lt;/strong&gt;: ~5x real-time speed on mid-range hardware (tested on i5)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Automatic language detection&lt;/strong&gt; - no manual language selection required&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;How to Contribute&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Check existing issues&lt;/strong&gt; at &lt;a href="https://github.com/cjpais/Handy/issues"&gt;github.com/cjpais/Handy/issues&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Fork the repository&lt;/strong&gt; and create a feature branch&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Test thoroughly&lt;/strong&gt; on your target platform&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Submit a pull request&lt;/strong&gt; with clear description of changes&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Join the discussion&lt;/strong&gt; - reach out at &lt;a href="mailto:contact@handy.computer"&gt;contact@handy.computer&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;The goal is to create both a useful tool and a foundation for others to build upon—a well-patterned, simple codebase that serves the community.&lt;/p&gt; 
&lt;h2&gt;Sponsors&lt;/h2&gt; 
&lt;div align="center"&gt;
  We're grateful for the support of our sponsors who help make Handy possible: 
 &lt;br /&gt;
 &lt;br /&gt; 
 &lt;a href="https://wordcab.com"&gt; &lt;img src="https://raw.githubusercontent.com/cjpais/Handy/main/sponsor-images/wordcab.png" alt="Wordcab" width="120" height="120" /&gt; &lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 
 &lt;a href="https://github.com/epicenter-so/epicenter"&gt; &lt;img src="https://raw.githubusercontent.com/cjpais/Handy/main/sponsor-images/epicenter.png" alt="Epicenter" width="120" height="120" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;Related Projects&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/cjpais/handy-cli"&gt;Handy CLI&lt;/a&gt;&lt;/strong&gt; - The original Python command-line version&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://handy.computer"&gt;handy.computer&lt;/a&gt;&lt;/strong&gt; - Project website with demos and documentation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;MIT License - see &lt;a href="https://raw.githubusercontent.com/cjpais/Handy/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;h2&gt;Acknowledgments&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Whisper&lt;/strong&gt; by OpenAI for the speech recognition model&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;whisper.cpp and ggml&lt;/strong&gt; for amazing cross-platform whisper inference/acceleration&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Silero&lt;/strong&gt; for great lightweight VAD&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Tauri&lt;/strong&gt; team for the excellent Rust-based app framework&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Community contributors&lt;/strong&gt; helping make Handy better&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;em&gt;"Your search for the right speech-to-text tool can end here—not because Handy is perfect, but because you can make it perfect for you."&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>coinbase/x402</title>
      <link>https://github.com/coinbase/x402</link>
      <description>&lt;p&gt;A payments protocol for the internet. Built on HTTP.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;x402 payments protocol&lt;/h1&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;"1 line of code to accept digital dollars. No fee, 2 second settlement, $0.001 minimum payment."&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-typescript"&gt;app.use(
  // How much you want to charge, and where you want the funds to land
  paymentMiddleware("0xYourAddress", { "/your-endpoint": "$0.01" })
);
// That's it! See examples/typescript/servers/express.ts for a complete example. Instruction below for running on base-sepolia.
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Philosophy&lt;/h2&gt; 
&lt;p&gt;Payments on the internet are fundamentally flawed. Credit Cards are high friction, hard to accept, have minimum payments that are far too high, and don't fit into the programmatic nature of the internet. It's time for an open, internet-native form of payments. A payment rail that doesn't have high minimums + % based fee. Payments that are amazing for humans and AI agents.&lt;/p&gt; 
&lt;h2&gt;Principles&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Open standard:&lt;/strong&gt; the x402 protocol will never force reliance on a single party&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;HTTP Native:&lt;/strong&gt; x402 is meant to seamlessly complement the existing HTTP request made by traditional web services, it should not mandate additional requests outside the scope of a typical client / server flow.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Chain and token agnostic:&lt;/strong&gt; we welcome contributions that add support for new chains, signing standards, or schemes, so long as they meet our acceptance criteria laid out in &lt;a href="https://github.com/coinbase/x402/raw/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Trust minimizing:&lt;/strong&gt; all payment schemes must not allow for the facilitator or resource server to move funds, other than in accordance with client intentions&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Easy to use:&lt;/strong&gt; x402 needs to be 10x better than existing ways to pay on the internet. This means abstracting as many details of crypto as possible away from the client and resource server, and into the facilitator. This means the client/server should not need to think about gas, rpc, etc.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Ecosystem&lt;/h2&gt; 
&lt;p&gt;The x402 ecosystem is growing! Check out our &lt;a href="https://x402.org/ecosystem"&gt;ecosystem page&lt;/a&gt; to see projects building with x402, including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Client-side integrations&lt;/li&gt; 
 &lt;li&gt;Services and endpoints&lt;/li&gt; 
 &lt;li&gt;Ecosystem infrastructure and tooling&lt;/li&gt; 
 &lt;li&gt;Learning and community resources&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Want to add your project to the ecosystem? See our &lt;a href="https://github.com/coinbase/x402/tree/main/typescript/site#adding-your-project-to-the-ecosystem"&gt;demo site README&lt;/a&gt; for detailed instructions on how to submit your project.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Roadmap:&lt;/strong&gt; see &lt;a href="https://github.com/coinbase/x402/raw/main/ROADMAP.md"&gt;ROADMAP.md&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Terms:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;resource&lt;/code&gt;: Something on the internet. This could be a webpage, file server, RPC service, API, any resource on the internet that accepts HTTP / HTTPS requests.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;client&lt;/code&gt;: An entity wanting to pay for a resource.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;facilitator server&lt;/code&gt;: A server that facilitates verification and execution of on-chain payments.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;resource server&lt;/code&gt;: An HTTP server that provides an API or other resource for a client.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Technical Goals:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Permissionless and secure for clients and servers&lt;/li&gt; 
 &lt;li&gt;Gasless for client and resource servers&lt;/li&gt; 
 &lt;li&gt;Minimal integration for the resource server and client (1 line for the server, 1 function for the client)&lt;/li&gt; 
 &lt;li&gt;Ability to trade off speed of response for guarantee of payment&lt;/li&gt; 
 &lt;li&gt;Extensible to different payment flows and chains&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;V1 Protocol&lt;/h2&gt; 
&lt;p&gt;The &lt;code&gt;x402&lt;/code&gt; protocol is a chain agnostic standard for payments on top of HTTP, leverage the existing &lt;code&gt;402 Payment Required&lt;/code&gt; HTTP status code to indicate that a payment is required for access to the resource.&lt;/p&gt; 
&lt;p&gt;It specifies:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;A schema for how servers can respond to clients to facilitate payment for a resource (&lt;code&gt;PaymentRequirements&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;A standard header &lt;code&gt;X-PAYMENT&lt;/code&gt; that is set by clients paying for resources&lt;/li&gt; 
 &lt;li&gt;A standard schema and encoding method for data in the &lt;code&gt;X-PAYMENT&lt;/code&gt; header&lt;/li&gt; 
 &lt;li&gt;A recommended flow for how payments should be verified and settled by a resource server&lt;/li&gt; 
 &lt;li&gt;A REST specification for how a resource server can perform verification and settlement against a remote 3rd party server (&lt;code&gt;facilitator&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;A specification for a &lt;code&gt;X-PAYMENT-RESPONSE&lt;/code&gt; header that can be used by resource servers to communicate blockchain transactions details to the client in their HTTP response&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;V1 Protocol Sequencing&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/coinbase/x402/main/static/x402-protocol-flow.png" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;The following outlines the flow of a payment using the &lt;code&gt;x402&lt;/code&gt; protocol. Note that steps (1) and (2) are optional if the client already knows the payment details accepted for a resource.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;Client&lt;/code&gt; makes an HTTP request to a &lt;code&gt;resource server&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;Resource server&lt;/code&gt; responds with a &lt;code&gt;402 Payment Required&lt;/code&gt; status and a &lt;code&gt;Payment Required Response&lt;/code&gt; JSON object in the response body.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;Client&lt;/code&gt; selects one of the &lt;code&gt;paymentRequirements&lt;/code&gt; returned by the server response and creates a &lt;code&gt;Payment Payload&lt;/code&gt; based on the &lt;code&gt;scheme&lt;/code&gt; of the &lt;code&gt;paymentRequirements&lt;/code&gt; they have selected.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;Client&lt;/code&gt; sends the HTTP request with the &lt;code&gt;X-PAYMENT&lt;/code&gt; header containing the &lt;code&gt;Payment Payload&lt;/code&gt; to the resource server.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;Resource server&lt;/code&gt; verifies the &lt;code&gt;Payment Payload&lt;/code&gt; is valid either via local verification or by POSTing the &lt;code&gt;Payment Payload&lt;/code&gt; and &lt;code&gt;Payment Requirements&lt;/code&gt; to the &lt;code&gt;/verify&lt;/code&gt; endpoint of a &lt;code&gt;facilitator server&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;Facilitator server&lt;/code&gt; performs verification of the object based on the &lt;code&gt;scheme&lt;/code&gt; and &lt;code&gt;network&lt;/code&gt; of the &lt;code&gt;Payment Payload&lt;/code&gt; and returns a &lt;code&gt;Verification Response&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;If the &lt;code&gt;Verification Response&lt;/code&gt; is valid, the resource server performs the work to fulfill the request. If the &lt;code&gt;Verification Response&lt;/code&gt; is invalid, the resource server returns a &lt;code&gt;402 Payment Required&lt;/code&gt; status and a &lt;code&gt;Payment Required Response&lt;/code&gt; JSON object in the response body.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;Resource server&lt;/code&gt; either settles the payment by interacting with a blockchain directly, or by POSTing the &lt;code&gt;Payment Payload&lt;/code&gt; and &lt;code&gt;Payment PaymentRequirements&lt;/code&gt; to the &lt;code&gt;/settle&lt;/code&gt; endpoint of a &lt;code&gt;facilitator server&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;Facilitator server&lt;/code&gt; submits the payment to the blockchain based on the &lt;code&gt;scheme&lt;/code&gt; and &lt;code&gt;network&lt;/code&gt; of the &lt;code&gt;Payment Payload&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;Facilitator server&lt;/code&gt; waits for the payment to be confirmed on the blockchain.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;Facilitator server&lt;/code&gt; returns a &lt;code&gt;Payment Execution Response&lt;/code&gt; to the resource server.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;Resource server&lt;/code&gt; returns a &lt;code&gt;200 OK&lt;/code&gt; response to the &lt;code&gt;Client&lt;/code&gt; with the resource they requested as the body of the HTTP response, and a &lt;code&gt;X-PAYMENT-RESPONSE&lt;/code&gt; header containing the &lt;code&gt;Settlement Response&lt;/code&gt; as Base64 encoded JSON if the payment was executed successfully.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Type Specifications&lt;/h3&gt; 
&lt;h4&gt;Data types&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;Payment Required Response&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json5"&gt;{
  // Version of the x402 payment protocol
  x402Version: int,

  // List of payment requirements that the resource server accepts. A resource server may accept on multiple chains, or in multiple currencies.
  accepts: [paymentRequirements]

  // Message from the resource server to the client to communicate errors in processing payment
  error: string
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;paymentRequirements&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json5"&gt;{
  // Scheme of the payment protocol to use
  scheme: string;

  // Network of the blockchain to send payment on
  network: string;

  // Maximum amount required to pay for the resource in atomic units of the asset
  maxAmountRequired: uint256 as string;

  // URL of resource to pay for
  resource: string;

  // Description of the resource
  description: string;

  // MIME type of the resource response
  mimeType: string;

  // Output schema of the resource response
  outputSchema?: object | null;

  // Address to pay value to
  payTo: string;

  // Maximum time in seconds for the resource server to respond
  maxTimeoutSeconds: number;

  // Address of the EIP-3009 compliant ERC20 contract
  asset: string;

  // Extra information about the payment details specific to the scheme
  // For `exact` scheme on a EVM network, expects extra to contain the records `name` and `version` pertaining to asset
  extra: object | null;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;&lt;code&gt;Payment Payload&lt;/code&gt;&lt;/strong&gt; (included as the &lt;code&gt;X-PAYMENT&lt;/code&gt; header in base64 encoded json)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json5"&gt;{
  // Version of the x402 payment protocol
  x402Version: number;

  // scheme is the scheme value of the accepted `paymentRequirements` the client is using to pay
  scheme: string;

  // network is the network id of the accepted `paymentRequirements` the client is using to pay
  network: string;

  // payload is scheme dependent
  payload: &amp;lt;scheme dependent&amp;gt;;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Facilitator Types &amp;amp; Interface&lt;/h4&gt; 
&lt;p&gt;A &lt;code&gt;facilitator server&lt;/code&gt; is a 3rd party service that can be used by a &lt;code&gt;resource server&lt;/code&gt; to verify and settle payments, without the &lt;code&gt;resource server&lt;/code&gt; needing to have access to a blockchain node or wallet.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;POST /verify&lt;/strong&gt;. Verify a payment with a supported scheme and network:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Request body JSON: &lt;pre&gt;&lt;code class="language-json5"&gt;{
  x402Version: number;
  paymentHeader: string;
  paymentRequirements: paymentRequirements;
}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Response: &lt;pre&gt;&lt;code class="language-json5"&gt;{
  isValid: boolean;
  invalidReason: string | null;
}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;POST /settle&lt;/strong&gt;. Settle a payment with a supported scheme and network:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Request body JSON:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-json5"&gt;{
  x402Version: number;
  paymentHeader: string;
  paymentRequirements: paymentRequirements;
}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Response:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-json5"&gt;{
  // Whether the payment was successful
  success: boolean;

  // Error message from the facilitator server
  error: string | null;

  // Transaction hash of the settled payment
  txHash: string | null;

  // Network id of the blockchain the payment was settled on
  networkId: string | null;
}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;GET /supported&lt;/strong&gt;. Get supported payment schemes and networks:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Response: &lt;pre&gt;&lt;code class="language-json5"&gt;{
  kinds: [
    {
      "scheme": string,
      "network": string,
    }
  ]
}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Schemes&lt;/h3&gt; 
&lt;p&gt;A scheme is a logical way of moving money.&lt;/p&gt; 
&lt;p&gt;Blockchains allow for a large number of flexible ways to move money. To help facilitate an expanding number of payment use cases, the &lt;code&gt;x402&lt;/code&gt; protocol is extensible to different ways of settling payments via its &lt;code&gt;scheme&lt;/code&gt; field.&lt;/p&gt; 
&lt;p&gt;Each payment scheme may have different operational functionality depending on what actions are necessary to fulfill the payment. For example &lt;code&gt;exact&lt;/code&gt;, the first scheme shipping as part of the protocol, would have different behavior than &lt;code&gt;upto&lt;/code&gt;. &lt;code&gt;exact&lt;/code&gt; transfers a specific amount (ex: pay $1 to read an article), while a theoretical &lt;code&gt;upto&lt;/code&gt; would transfer up to an amount, based on the resources consumed during a request (ex: generating tokens from an LLM).&lt;/p&gt; 
&lt;p&gt;See &lt;code&gt;specs/schemes&lt;/code&gt; for more details on schemes, and see &lt;code&gt;specs/schemes/exact/scheme_exact_evm.md&lt;/code&gt; to see the first proposed scheme for exact payment on EVM chains.&lt;/p&gt; 
&lt;h3&gt;Schemes vs Networks&lt;/h3&gt; 
&lt;p&gt;Because a scheme is a logical way of moving money, the way a scheme is implemented can be different for different blockchains. (ex: the way you need to implement &lt;code&gt;exact&lt;/code&gt; on Ethereum is very different from the way you need to implement &lt;code&gt;exact&lt;/code&gt; on Solana).&lt;/p&gt; 
&lt;p&gt;Clients and facilitators must explicitly support different &lt;code&gt;(scheme, network)&lt;/code&gt; pairs in order to be able to create proper payloads and verify / settle payments.&lt;/p&gt; 
&lt;h2&gt;Running example&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Requirements:&lt;/strong&gt; Node.js v24 or higher&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;From &lt;code&gt;examples/typescript&lt;/code&gt; run &lt;code&gt;pnpm install&lt;/code&gt; and &lt;code&gt;pnpm build&lt;/code&gt; to ensure all dependent packages and examples are setup.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Select a server, i.e. express, and &lt;code&gt;cd&lt;/code&gt; into that example. Add your server's ethereum address to get paid to into the &lt;code&gt;.env&lt;/code&gt; file, and then run &lt;code&gt;pnpm dev&lt;/code&gt; in that directory.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Select a client, i.e. axios, and &lt;code&gt;cd&lt;/code&gt; into that example. Add your private key for the account making payments into the &lt;code&gt;.env&lt;/code&gt; file, and then run &lt;code&gt;pnpm dev&lt;/code&gt; in that directory.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;You should see activities in the client terminal, which will display a weather report.&lt;/p&gt; 
&lt;h2&gt;Running tests&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Navigate to the typescript directory: &lt;code&gt;cd typescript&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Install dependencies: &lt;code&gt;pnpm install&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Run the unit tests: &lt;code&gt;pnpm test&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;This will run the unit tests for the x402 packages.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>karpathy/nanoGPT</title>
      <link>https://github.com/karpathy/nanoGPT</link>
      <description>&lt;p&gt;The simplest, fastest repository for training/finetuning medium-sized GPTs.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;nanoGPT&lt;/h1&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/karpathy/nanoGPT/master/assets/nanogpt.jpg" alt="nanoGPT" /&gt;&lt;/p&gt; 
&lt;p&gt;The simplest, fastest repository for training/finetuning medium-sized GPTs. It is a rewrite of &lt;a href="https://github.com/karpathy/minGPT"&gt;minGPT&lt;/a&gt; that prioritizes teeth over education. Still under active development, but currently the file &lt;code&gt;train.py&lt;/code&gt; reproduces GPT-2 (124M) on OpenWebText, running on a single 8XA100 40GB node in about 4 days of training. The code itself is plain and readable: &lt;code&gt;train.py&lt;/code&gt; is a ~300-line boilerplate training loop and &lt;code&gt;model.py&lt;/code&gt; a ~300-line GPT model definition, which can optionally load the GPT-2 weights from OpenAI. That's it.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/karpathy/nanoGPT/master/assets/gpt2_124M_loss.png" alt="repro124m" /&gt;&lt;/p&gt; 
&lt;p&gt;Because the code is so simple, it is very easy to hack to your needs, train new models from scratch, or finetune pretrained checkpoints (e.g. biggest one currently available as a starting point would be the GPT-2 1.3B model from OpenAI).&lt;/p&gt; 
&lt;h2&gt;install&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;pip install torch numpy transformers datasets tiktoken wandb tqdm
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Dependencies:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://pytorch.org"&gt;pytorch&lt;/a&gt; &amp;lt;3&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://numpy.org/install/"&gt;numpy&lt;/a&gt; &amp;lt;3&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;transformers&lt;/code&gt; for huggingface transformers &amp;lt;3 (to load GPT-2 checkpoints)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;datasets&lt;/code&gt; for huggingface datasets &amp;lt;3 (if you want to download + preprocess OpenWebText)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;tiktoken&lt;/code&gt; for OpenAI's fast BPE code &amp;lt;3&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;wandb&lt;/code&gt; for optional logging &amp;lt;3&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;tqdm&lt;/code&gt; for progress bars &amp;lt;3&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;quick start&lt;/h2&gt; 
&lt;p&gt;If you are not a deep learning professional and you just want to feel the magic and get your feet wet, the fastest way to get started is to train a character-level GPT on the works of Shakespeare. First, we download it as a single (1MB) file and turn it from raw text into one large stream of integers:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python data/shakespeare_char/prepare.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This creates a &lt;code&gt;train.bin&lt;/code&gt; and &lt;code&gt;val.bin&lt;/code&gt; in that data directory. Now it is time to train your GPT. The size of it very much depends on the computational resources of your system:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;I have a GPU&lt;/strong&gt;. Great, we can quickly train a baby GPT with the settings provided in the &lt;a href="https://raw.githubusercontent.com/karpathy/nanoGPT/master/config/train_shakespeare_char.py"&gt;config/train_shakespeare_char.py&lt;/a&gt; config file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python train.py config/train_shakespeare_char.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you peek inside it, you'll see that we're training a GPT with a context size of up to 256 characters, 384 feature channels, and it is a 6-layer Transformer with 6 heads in each layer. On one A100 GPU this training run takes about 3 minutes and the best validation loss is 1.4697. Based on the configuration, the model checkpoints are being written into the &lt;code&gt;--out_dir&lt;/code&gt; directory &lt;code&gt;out-shakespeare-char&lt;/code&gt;. So once the training finishes we can sample from the best model by pointing the sampling script at this directory:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python sample.py --out_dir=out-shakespeare-char
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This generates a few samples, for example:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;ANGELO:
And cowards it be strawn to my bed,
And thrust the gates of my threats,
Because he that ale away, and hang'd
An one with him.

DUKE VINCENTIO:
I thank your eyes against it.

DUKE VINCENTIO:
Then will answer him to save the malm:
And what have you tyrannous shall do this?

DUKE VINCENTIO:
If you have done evils of all disposition
To end his power, the day of thrust for a common men
That I leave, to fight with over-liking
Hasting in a roseman.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;lol &lt;code&gt;¯\_(ツ)_/¯&lt;/code&gt;. Not bad for a character-level model after 3 minutes of training on a GPU. Better results are quite likely obtainable by instead finetuning a pretrained GPT-2 model on this dataset (see finetuning section later).&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;I only have a macbook&lt;/strong&gt; (or other cheap computer). No worries, we can still train a GPT but we want to dial things down a notch. I recommend getting the bleeding edge PyTorch nightly (&lt;a href="https://pytorch.org/get-started/locally/"&gt;select it here&lt;/a&gt; when installing) as it is currently quite likely to make your code more efficient. But even without it, a simple train run could look as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python train.py config/train_shakespeare_char.py --device=cpu --compile=False --eval_iters=20 --log_interval=1 --block_size=64 --batch_size=12 --n_layer=4 --n_head=4 --n_embd=128 --max_iters=2000 --lr_decay_iters=2000 --dropout=0.0
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Here, since we are running on CPU instead of GPU we must set both &lt;code&gt;--device=cpu&lt;/code&gt; and also turn off PyTorch 2.0 compile with &lt;code&gt;--compile=False&lt;/code&gt;. Then when we evaluate we get a bit more noisy but faster estimate (&lt;code&gt;--eval_iters=20&lt;/code&gt;, down from 200), our context size is only 64 characters instead of 256, and the batch size only 12 examples per iteration, not 64. We'll also use a much smaller Transformer (4 layers, 4 heads, 128 embedding size), and decrease the number of iterations to 2000 (and correspondingly usually decay the learning rate to around max_iters with &lt;code&gt;--lr_decay_iters&lt;/code&gt;). Because our network is so small we also ease down on regularization (&lt;code&gt;--dropout=0.0&lt;/code&gt;). This still runs in about ~3 minutes, but gets us a loss of only 1.88 and therefore also worse samples, but it's still good fun:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python sample.py --out_dir=out-shakespeare-char --device=cpu
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Generates samples like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;GLEORKEN VINGHARD III:
Whell's the couse, the came light gacks,
And the for mought you in Aut fries the not high shee
bot thou the sought bechive in that to doth groan you,
No relving thee post mose the wear
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Not bad for ~3 minutes on a CPU, for a hint of the right character gestalt. If you're willing to wait longer, feel free to tune the hyperparameters, increase the size of the network, the context length (&lt;code&gt;--block_size&lt;/code&gt;), the length of training, etc.&lt;/p&gt; 
&lt;p&gt;Finally, on Apple Silicon Macbooks and with a recent PyTorch version make sure to add &lt;code&gt;--device=mps&lt;/code&gt; (short for "Metal Performance Shaders"); PyTorch then uses the on-chip GPU that can &lt;em&gt;significantly&lt;/em&gt; accelerate training (2-3X) and allow you to use larger networks. See &lt;a href="https://github.com/karpathy/nanoGPT/issues/28"&gt;Issue 28&lt;/a&gt; for more.&lt;/p&gt; 
&lt;h2&gt;reproducing GPT-2&lt;/h2&gt; 
&lt;p&gt;A more serious deep learning professional may be more interested in reproducing GPT-2 results. So here we go - we first tokenize the dataset, in this case the &lt;a href="https://openwebtext2.readthedocs.io/en/latest/"&gt;OpenWebText&lt;/a&gt;, an open reproduction of OpenAI's (private) WebText:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python data/openwebtext/prepare.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This downloads and tokenizes the &lt;a href="https://huggingface.co/datasets/openwebtext"&gt;OpenWebText&lt;/a&gt; dataset. It will create a &lt;code&gt;train.bin&lt;/code&gt; and &lt;code&gt;val.bin&lt;/code&gt; which holds the GPT2 BPE token ids in one sequence, stored as raw uint16 bytes. Then we're ready to kick off training. To reproduce GPT-2 (124M) you'll want at least an 8X A100 40GB node and run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;torchrun --standalone --nproc_per_node=8 train.py config/train_gpt2.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will run for about 4 days using PyTorch Distributed Data Parallel (DDP) and go down to loss of ~2.85. Now, a GPT-2 model just evaluated on OWT gets a val loss of about 3.11, but if you finetune it it will come down to ~2.85 territory (due to an apparent domain gap), making the two models ~match.&lt;/p&gt; 
&lt;p&gt;If you're in a cluster environment and you are blessed with multiple GPU nodes you can make GPU go brrrr e.g. across 2 nodes like:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Run on the first (master) node with example IP 123.456.123.456:
torchrun --nproc_per_node=8 --nnodes=2 --node_rank=0 --master_addr=123.456.123.456 --master_port=1234 train.py
# Run on the worker node:
torchrun --nproc_per_node=8 --nnodes=2 --node_rank=1 --master_addr=123.456.123.456 --master_port=1234 train.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;It is a good idea to benchmark your interconnect (e.g. iperf3). In particular, if you don't have Infiniband then also prepend &lt;code&gt;NCCL_IB_DISABLE=1&lt;/code&gt; to the above launches. Your multinode training will work, but most likely &lt;em&gt;crawl&lt;/em&gt;. By default checkpoints are periodically written to the &lt;code&gt;--out_dir&lt;/code&gt;. We can sample from the model by simply &lt;code&gt;python sample.py&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Finally, to train on a single GPU simply run the &lt;code&gt;python train.py&lt;/code&gt; script. Have a look at all of its args, the script tries to be very readable, hackable and transparent. You'll most likely want to tune a number of those variables depending on your needs.&lt;/p&gt; 
&lt;h2&gt;baselines&lt;/h2&gt; 
&lt;p&gt;OpenAI GPT-2 checkpoints allow us to get some baselines in place for openwebtext. We can get the numbers as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;$ python train.py config/eval_gpt2.py
$ python train.py config/eval_gpt2_medium.py
$ python train.py config/eval_gpt2_large.py
$ python train.py config/eval_gpt2_xl.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;and observe the following losses on train and val:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;model&lt;/th&gt; 
   &lt;th&gt;params&lt;/th&gt; 
   &lt;th&gt;train loss&lt;/th&gt; 
   &lt;th&gt;val loss&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;gpt2&lt;/td&gt; 
   &lt;td&gt;124M&lt;/td&gt; 
   &lt;td&gt;3.11&lt;/td&gt; 
   &lt;td&gt;3.12&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;gpt2-medium&lt;/td&gt; 
   &lt;td&gt;350M&lt;/td&gt; 
   &lt;td&gt;2.85&lt;/td&gt; 
   &lt;td&gt;2.84&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;gpt2-large&lt;/td&gt; 
   &lt;td&gt;774M&lt;/td&gt; 
   &lt;td&gt;2.66&lt;/td&gt; 
   &lt;td&gt;2.67&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;gpt2-xl&lt;/td&gt; 
   &lt;td&gt;1558M&lt;/td&gt; 
   &lt;td&gt;2.56&lt;/td&gt; 
   &lt;td&gt;2.54&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;However, we have to note that GPT-2 was trained on (closed, never released) WebText, while OpenWebText is just a best-effort open reproduction of this dataset. This means there is a dataset domain gap. Indeed, taking the GPT-2 (124M) checkpoint and finetuning on OWT directly for a while reaches loss down to ~2.85. This then becomes the more appropriate baseline w.r.t. reproduction.&lt;/p&gt; 
&lt;h2&gt;finetuning&lt;/h2&gt; 
&lt;p&gt;Finetuning is no different than training, we just make sure to initialize from a pretrained model and train with a smaller learning rate. For an example of how to finetune a GPT on new text go to &lt;code&gt;data/shakespeare&lt;/code&gt; and run &lt;code&gt;prepare.py&lt;/code&gt; to download the tiny shakespeare dataset and render it into a &lt;code&gt;train.bin&lt;/code&gt; and &lt;code&gt;val.bin&lt;/code&gt;, using the OpenAI BPE tokenizer from GPT-2. Unlike OpenWebText this will run in seconds. Finetuning can take very little time, e.g. on a single GPU just a few minutes. Run an example finetuning like:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python train.py config/finetune_shakespeare.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will load the config parameter overrides in &lt;code&gt;config/finetune_shakespeare.py&lt;/code&gt; (I didn't tune them much though). Basically, we initialize from a GPT2 checkpoint with &lt;code&gt;init_from&lt;/code&gt; and train as normal, except shorter and with a small learning rate. If you're running out of memory try decreasing the model size (they are &lt;code&gt;{'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}&lt;/code&gt;) or possibly decreasing the &lt;code&gt;block_size&lt;/code&gt; (context length). The best checkpoint (lowest validation loss) will be in the &lt;code&gt;out_dir&lt;/code&gt; directory, e.g. in &lt;code&gt;out-shakespeare&lt;/code&gt; by default, per the config file. You can then run the code in &lt;code&gt;sample.py --out_dir=out-shakespeare&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;THEODORE:
Thou shalt sell me to the highest bidder: if I die,
I sell thee to the first; if I go mad,
I sell thee to the second; if I
lie, I sell thee to the third; if I slay,
I sell thee to the fourth: so buy or sell,
I tell thee again, thou shalt not sell my
possession.

JULIET:
And if thou steal, thou shalt not sell thyself.

THEODORE:
I do not steal; I sell the stolen goods.

THEODORE:
Thou know'st not what thou sell'st; thou, a woman,
Thou art ever a victim, a thing of no worth:
Thou hast no right, no right, but to be sold.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Whoa there, GPT, entering some dark place over there. I didn't really tune the hyperparameters in the config too much, feel free to try!&lt;/p&gt; 
&lt;h2&gt;sampling / inference&lt;/h2&gt; 
&lt;p&gt;Use the script &lt;code&gt;sample.py&lt;/code&gt; to sample either from pre-trained GPT-2 models released by OpenAI, or from a model you trained yourself. For example, here is a way to sample from the largest available &lt;code&gt;gpt2-xl&lt;/code&gt; model:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python sample.py \
    --init_from=gpt2-xl \
    --start="What is the answer to life, the universe, and everything?" \
    --num_samples=5 --max_new_tokens=100
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you'd like to sample from a model you trained, use the &lt;code&gt;--out_dir&lt;/code&gt; to point the code appropriately. You can also prompt the model with some text from a file, e.g. &lt;code&gt;python sample.py --start=FILE:prompt.txt&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;efficiency notes&lt;/h2&gt; 
&lt;p&gt;For simple model benchmarking and profiling, &lt;code&gt;bench.py&lt;/code&gt; might be useful. It's identical to what happens in the meat of the training loop of &lt;code&gt;train.py&lt;/code&gt;, but omits much of the other complexities.&lt;/p&gt; 
&lt;p&gt;Note that the code by default uses &lt;a href="https://pytorch.org/get-started/pytorch-2.0/"&gt;PyTorch 2.0&lt;/a&gt;. At the time of writing (Dec 29, 2022) this makes &lt;code&gt;torch.compile()&lt;/code&gt; available in the nightly release. The improvement from the one line of code is noticeable, e.g. cutting down iteration time from ~250ms / iter to 135ms / iter. Nice work PyTorch team!&lt;/p&gt; 
&lt;h2&gt;todos&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Investigate and add FSDP instead of DDP&lt;/li&gt; 
 &lt;li&gt;Eval zero-shot perplexities on standard evals (e.g. LAMBADA? HELM? etc.)&lt;/li&gt; 
 &lt;li&gt;Finetune the finetuning script, I think the hyperparams are not great&lt;/li&gt; 
 &lt;li&gt;Schedule for linear batch size increase during training&lt;/li&gt; 
 &lt;li&gt;Incorporate other embeddings (rotary, alibi)&lt;/li&gt; 
 &lt;li&gt;Separate out the optim buffers from model params in checkpoints I think&lt;/li&gt; 
 &lt;li&gt;Additional logging around network health (e.g. gradient clip events, magnitudes)&lt;/li&gt; 
 &lt;li&gt;Few more investigations around better init etc.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;troubleshooting&lt;/h2&gt; 
&lt;p&gt;Note that by default this repo uses PyTorch 2.0 (i.e. &lt;code&gt;torch.compile&lt;/code&gt;). This is fairly new and experimental, and not yet available on all platforms (e.g. Windows). If you're running into related error messages try to disable this by adding &lt;code&gt;--compile=False&lt;/code&gt; flag. This will slow down the code but at least it will run.&lt;/p&gt; 
&lt;p&gt;For some context on this repository, GPT, and language modeling it might be helpful to watch my &lt;a href="https://karpathy.ai/zero-to-hero.html"&gt;Zero To Hero series&lt;/a&gt;. Specifically, the &lt;a href="https://www.youtube.com/watch?v=kCc8FmEb1nY"&gt;GPT video&lt;/a&gt; is popular if you have some prior language modeling context.&lt;/p&gt; 
&lt;p&gt;For more questions/discussions feel free to stop by &lt;strong&gt;#nanoGPT&lt;/strong&gt; on Discord:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://discord.gg/3zy8kqD9Cp"&gt;&lt;img src="https://dcbadge.vercel.app/api/server/3zy8kqD9Cp?compact=true&amp;amp;style=flat" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;acknowledgements&lt;/h2&gt; 
&lt;p&gt;All nanoGPT experiments are powered by GPUs on &lt;a href="https://lambdalabs.com"&gt;Lambda labs&lt;/a&gt;, my favorite Cloud GPU provider. Thank you Lambda labs for sponsoring nanoGPT!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>anthropics/prompt-eng-interactive-tutorial</title>
      <link>https://github.com/anthropics/prompt-eng-interactive-tutorial</link>
      <description>&lt;p&gt;Anthropic's Interactive Prompt Engineering Tutorial&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Welcome to Anthropic's Prompt Engineering Interactive Tutorial&lt;/h1&gt; 
&lt;h2&gt;Course introduction and goals&lt;/h2&gt; 
&lt;p&gt;This course is intended to provide you with a comprehensive step-by-step understanding of how to engineer optimal prompts within Claude.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;After completing this course, you will be able to&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Master the basic structure of a good prompt&lt;/li&gt; 
 &lt;li&gt;Recognize common failure modes and learn the '80/20' techniques to address them&lt;/li&gt; 
 &lt;li&gt;Understand Claude's strengths and weaknesses&lt;/li&gt; 
 &lt;li&gt;Build strong prompts from scratch for common use cases&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Course structure and content&lt;/h2&gt; 
&lt;p&gt;This course is structured to allow you many chances to practice writing and troubleshooting prompts yourself. The course is broken up into &lt;strong&gt;9 chapters with accompanying exercises&lt;/strong&gt;, as well as an appendix of even more advanced methods. It is intended for you to &lt;strong&gt;work through the course in chapter order&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Each lesson has an "Example Playground" area&lt;/strong&gt; at the bottom where you are free to experiment with the examples in the lesson and see for yourself how changing prompts can change Claude's responses. There is also an &lt;a href="https://docs.google.com/spreadsheets/d/1jIxjzUWG-6xBVIa2ay6yDpLyeuOh_hR_ZB75a47KX_E/edit?usp=sharing"&gt;answer key&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Note: This tutorial uses our smallest, fastest, and cheapest model, Claude 3 Haiku. Anthropic has &lt;a href="https://docs.anthropic.com/claude/docs/models-overview"&gt;two other models&lt;/a&gt;, Claude 3 Sonnet and Claude 3 Opus, which are more intelligent than Haiku, with Opus being the most intelligent.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;This tutorial also exists on &lt;a href="https://docs.google.com/spreadsheets/d/19jzLgRruG9kjUQNKtCg1ZjdD6l6weA6qRXG5zLIAhC8/edit?usp=sharing"&gt;Google Sheets using Anthropic's Claude for Sheets extension&lt;/a&gt;. We recommend using that version as it is more user friendly.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;When you are ready to begin, go to &lt;code&gt;01_Basic Prompt Structure&lt;/code&gt; to proceed.&lt;/p&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;p&gt;Each chapter consists of a lesson and a set of exercises.&lt;/p&gt; 
&lt;h3&gt;Beginner&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Chapter 1:&lt;/strong&gt; Basic Prompt Structure&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Chapter 2:&lt;/strong&gt; Being Clear and Direct&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Chapter 3:&lt;/strong&gt; Assigning Roles&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Intermediate&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Chapter 4:&lt;/strong&gt; Separating Data from Instructions&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Chapter 5:&lt;/strong&gt; Formatting Output &amp;amp; Speaking for Claude&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Chapter 6:&lt;/strong&gt; Precognition (Thinking Step by Step)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Chapter 7:&lt;/strong&gt; Using Examples&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Advanced&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Chapter 8:&lt;/strong&gt; Avoiding Hallucinations&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Chapter 9:&lt;/strong&gt; Building Complex Prompts (Industry Use Cases)&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Complex Prompts from Scratch - Chatbot&lt;/li&gt; 
   &lt;li&gt;Complex Prompts for Legal Services&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Exercise:&lt;/strong&gt; Complex Prompts for Financial Services&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Exercise:&lt;/strong&gt; Complex Prompts for Coding&lt;/li&gt; 
   &lt;li&gt;Congratulations &amp;amp; Next Steps&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Appendix:&lt;/strong&gt; Beyond Standard Prompting&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Chaining Prompts&lt;/li&gt; 
   &lt;li&gt;Tool Use&lt;/li&gt; 
   &lt;li&gt;Search &amp;amp; Retrieval&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>Infisical/infisical</title>
      <link>https://github.com/Infisical/infisical</link>
      <description>&lt;p&gt;Infisical is the open-source platform for secrets, certificates, and privileged access management.&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt; &lt;img width="300" src="https://raw.githubusercontent.com/Infisical/infisical/main/img/logoname-white.svg#gh-dark-mode-only" alt="infisical" /&gt; &lt;/h1&gt; 
&lt;p align="center"&gt; &lt;/p&gt;
&lt;p align="center"&gt;&lt;b&gt;The open-source secret management platform&lt;/b&gt;: Sync secrets/configs across your team/infrastructure and prevent secret leaks.&lt;/p&gt; 
&lt;p&gt;&lt;/p&gt; 
&lt;h4 align="center"&gt; &lt;a href="https://infisical.com/slack"&gt;Slack&lt;/a&gt; | &lt;a href="https://infisical.com/"&gt;Infisical Cloud&lt;/a&gt; | &lt;a href="https://infisical.com/docs/self-hosting/overview"&gt;Self-Hosting&lt;/a&gt; | &lt;a href="https://infisical.com/docs/documentation/getting-started/introduction"&gt;Docs&lt;/a&gt; | &lt;a href="https://www.infisical.com"&gt;Website&lt;/a&gt; | &lt;a href="https://infisical.com/careers"&gt;Hiring (Remote/SF)&lt;/a&gt; &lt;/h4&gt; 
&lt;h4 align="center"&gt; &lt;a href="https://github.com/Infisical/infisical/raw/main/LICENSE"&gt; &lt;img src="https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true" alt="Infisical is released under the MIT license." /&gt; &lt;/a&gt; &lt;a href="https://github.com/infisical/infisical/raw/main/CONTRIBUTING.md"&gt; &lt;img src="https://img.shields.io/badge/PRs-Welcome-brightgreen" alt="PRs welcome!" /&gt; &lt;/a&gt; &lt;a href="https://github.com/Infisical/infisical/issues"&gt; &lt;img src="https://img.shields.io/github/commit-activity/m/infisical/infisical" alt="git commit activity" /&gt; &lt;/a&gt; &lt;a href="https://cloudsmith.io/~infisical/repos/"&gt; &lt;img src="https://img.shields.io/badge/Downloads-6.95M-orange" alt="Cloudsmith downloads" /&gt; &lt;/a&gt; &lt;a href="https://infisical.com/slack"&gt; &lt;img src="https://img.shields.io/badge/chat-on%20Slack-blueviolet" alt="Slack community channel" /&gt; &lt;/a&gt; &lt;a href="https://twitter.com/infisical"&gt; &lt;img src="https://img.shields.io/twitter/follow/infisical?label=Follow" alt="Infisical Twitter" /&gt; &lt;/a&gt; &lt;/h4&gt; 
&lt;img src="https://raw.githubusercontent.com/Infisical/infisical/main/img/infisical_github_repo2.png" width="100%" alt="Dashboard" /&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://infisical.com"&gt;Infisical&lt;/a&gt;&lt;/strong&gt; is the open source secret management platform that teams use to centralize their application configuration and secrets like API keys and database credentials as well as manage their internal PKI.&lt;/p&gt; 
&lt;p&gt;We're on a mission to make security tooling more accessible to everyone, not just security teams, and that means redesigning the entire developer experience from ground up.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;h3&gt;Secrets Management:&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://infisical.com/docs/documentation/platform/project"&gt;Dashboard&lt;/a&gt;&lt;/strong&gt;: Manage secrets across projects and environments (e.g. development, production, etc.) through a user-friendly interface.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://infisical.com/docs/integrations/overview"&gt;Native Integrations&lt;/a&gt;&lt;/strong&gt;: Sync secrets to platforms like &lt;a href="https://infisical.com/docs/integrations/cicd/githubactions"&gt;GitHub&lt;/a&gt;, &lt;a href="https://infisical.com/docs/integrations/cloud/vercel"&gt;Vercel&lt;/a&gt;, &lt;a href="https://infisical.com/docs/integrations/cloud/aws-secret-manager"&gt;AWS&lt;/a&gt;, and use tools like &lt;a href="https://infisical.com/docs/integrations/frameworks/terraform"&gt;Terraform&lt;/a&gt;, &lt;a href="https://infisical.com/docs/integrations/platforms/ansible"&gt;Ansible&lt;/a&gt;, and more.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://infisical.com/docs/documentation/platform/secret-versioning"&gt;Secret versioning&lt;/a&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;a href="https://infisical.com/docs/documentation/platform/pit-recovery"&gt;Point-in-Time Recovery&lt;/a&gt;&lt;/strong&gt;: Keep track of every secret and project state; roll back when needed.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://infisical.com/docs/documentation/platform/secret-rotation/overview"&gt;Secret Rotation&lt;/a&gt;&lt;/strong&gt;: Rotate secrets at regular intervals for services like &lt;a href="https://infisical.com/docs/documentation/platform/secret-rotation/postgres-credentials"&gt;PostgreSQL&lt;/a&gt;, &lt;a href="https://infisical.com/docs/documentation/platform/secret-rotation/mysql"&gt;MySQL&lt;/a&gt;, &lt;a href="https://infisical.com/docs/documentation/platform/secret-rotation/aws-iam"&gt;AWS IAM&lt;/a&gt;, and more.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://infisical.com/docs/documentation/platform/dynamic-secrets/overview"&gt;Dynamic Secrets&lt;/a&gt;&lt;/strong&gt;: Generate ephemeral secrets on-demand for services like &lt;a href="https://infisical.com/docs/documentation/platform/dynamic-secrets/postgresql"&gt;PostgreSQL&lt;/a&gt;, &lt;a href="https://infisical.com/docs/documentation/platform/dynamic-secrets/mysql"&gt;MySQL&lt;/a&gt;, &lt;a href="https://infisical.com/docs/documentation/platform/dynamic-secrets/rabbit-mq"&gt;RabbitMQ&lt;/a&gt;, and more.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://infisical.com/docs/cli/scanning-overview"&gt;Secret Scanning and Leak Prevention&lt;/a&gt;&lt;/strong&gt;: Prevent secrets from leaking to git.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://infisical.com/docs/documentation/getting-started/kubernetes"&gt;Infisical Kubernetes Operator&lt;/a&gt;&lt;/strong&gt;: Deliver secrets to your Kubernetes workloads and automatically reload deployments.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://infisical.com/docs/infisical-agent/overview"&gt;Infisical Agent&lt;/a&gt;&lt;/strong&gt;: Inject secrets into applications without modifying any code logic.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Infisical (Internal) PKI:&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://infisical.com/docs/documentation/platform/pki/private-ca"&gt;Private Certificate Authority&lt;/a&gt;&lt;/strong&gt;: Create CA hierarchies, configure &lt;a href="https://infisical.com/docs/documentation/platform/pki/certificates#guide-to-issuing-certificates"&gt;certificate templates&lt;/a&gt; for policy enforcement, and start issuing X.509 certificates.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://infisical.com/docs/documentation/platform/pki/certificates"&gt;Certificate Management&lt;/a&gt;&lt;/strong&gt;: Manage the certificate lifecycle from &lt;a href="https://infisical.com/docs/documentation/platform/pki/certificates#guide-to-issuing-certificates"&gt;issuance&lt;/a&gt; to &lt;a href="https://infisical.com/docs/documentation/platform/pki/certificates#guide-to-revoking-certificates"&gt;revocation&lt;/a&gt; with support for CRL.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://infisical.com/docs/documentation/platform/pki/alerting"&gt;Alerting&lt;/a&gt;&lt;/strong&gt;: Configure alerting for expiring CA and end-entity certificates.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://infisical.com/docs/documentation/platform/pki/pki-issuer"&gt;Infisical PKI Issuer for Kubernetes&lt;/a&gt;&lt;/strong&gt;: Deliver TLS certificates to your Kubernetes workloads with automatic renewal.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://infisical.com/docs/documentation/platform/pki/est"&gt;Enrollment over Secure Transport&lt;/a&gt;&lt;/strong&gt;: Enroll and manage certificates via EST protocol.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Infisical Key Management System (KMS):&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://infisical.com/docs/documentation/platform/kms"&gt;Cryptographic Keys&lt;/a&gt;&lt;/strong&gt;: Centrally manage keys across projects through a user-friendly interface or via the API.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://infisical.com/docs/documentation/platform/kms#guide-to-encrypting-data"&gt;Encrypt and Decrypt Data&lt;/a&gt;&lt;/strong&gt;: Use symmetric keys to encrypt and decrypt data.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Infisical SSH&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://infisical.com/docs/documentation/platform/ssh"&gt;Signed SSH Certificates&lt;/a&gt;&lt;/strong&gt;: Issue ephemeral SSH credentials for secure, short-lived, and centralized access to infrastructure.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;General Platform:&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Authentication Methods&lt;/strong&gt;: Authenticate machine identities with Infisical using a cloud-native or platform agnostic authentication method (&lt;a href="https://infisical.com/docs/documentation/platform/identities/kubernetes-auth"&gt;Kubernetes Auth&lt;/a&gt;, &lt;a href="https://infisical.com/docs/documentation/platform/identities/gcp-auth"&gt;GCP Auth&lt;/a&gt;, &lt;a href="https://infisical.com/docs/documentation/platform/identities/azure-auth"&gt;Azure Auth&lt;/a&gt;, &lt;a href="https://infisical.com/docs/documentation/platform/identities/aws-auth"&gt;AWS Auth&lt;/a&gt;, &lt;a href="https://infisical.com/docs/documentation/platform/identities/oidc-auth/general"&gt;OIDC Auth&lt;/a&gt;, &lt;a href="https://infisical.com/docs/documentation/platform/identities/universal-auth"&gt;Universal Auth&lt;/a&gt;).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://infisical.com/docs/documentation/platform/access-controls/overview"&gt;Access Controls&lt;/a&gt;&lt;/strong&gt;: Define advanced authorization controls for users and machine identities with &lt;a href="https://infisical.com/docs/documentation/platform/access-controls/role-based-access-controls"&gt;RBAC&lt;/a&gt;, &lt;a href="https://infisical.com/docs/documentation/platform/access-controls/additional-privileges"&gt;additional privileges&lt;/a&gt;, &lt;a href="https://infisical.com/docs/documentation/platform/access-controls/temporary-access"&gt;temporary access&lt;/a&gt;, &lt;a href="https://infisical.com/docs/documentation/platform/access-controls/access-requests"&gt;access requests&lt;/a&gt;, &lt;a href="https://infisical.com/docs/documentation/platform/pr-workflows"&gt;approval workflows&lt;/a&gt;, and more.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://infisical.com/docs/documentation/platform/audit-logs"&gt;Audit logs&lt;/a&gt;&lt;/strong&gt;: Track every action taken on the platform.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://infisical.com/docs/self-hosting/overview"&gt;Self-hosting&lt;/a&gt;&lt;/strong&gt;: Deploy Infisical on-prem or cloud with ease; keep data on your own infrastructure.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://infisical.com/docs/sdks/overview"&gt;Infisical SDK&lt;/a&gt;&lt;/strong&gt;: Interact with Infisical via client SDKs (&lt;a href="https://infisical.com/docs/sdks/languages/node"&gt;Node&lt;/a&gt;, &lt;a href="https://github.com/Infisical/python-sdk-official?tab=readme-ov-file#infisical-python-sdk"&gt;Python&lt;/a&gt;, &lt;a href="https://infisical.com/docs/sdks/languages/go"&gt;Go&lt;/a&gt;, &lt;a href="https://infisical.com/docs/sdks/languages/ruby"&gt;Ruby&lt;/a&gt;, &lt;a href="https://infisical.com/docs/sdks/languages/java"&gt;Java&lt;/a&gt;, &lt;a href="https://infisical.com/docs/sdks/languages/csharp"&gt;.NET&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://infisical.com/docs/cli/overview"&gt;Infisical CLI&lt;/a&gt;&lt;/strong&gt;: Interact with Infisical via CLI; useful for injecting secrets into local development and CI/CD pipelines.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://infisical.com/docs/api-reference/overview/introduction"&gt;Infisical API&lt;/a&gt;&lt;/strong&gt;: Interact with Infisical via API.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Getting started&lt;/h2&gt; 
&lt;p&gt;Check out the &lt;a href="https://infisical.com/docs/getting-started/introduction"&gt;Quickstart Guides&lt;/a&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Use Infisical Cloud&lt;/th&gt; 
   &lt;th&gt;Deploy Infisical on premise&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;The fastest and most reliable way to &lt;br /&gt; get started with Infisical is signing up &lt;br /&gt; for free to &lt;a href="https://app.infisical.com/login"&gt;Infisical Cloud&lt;/a&gt;.&lt;/td&gt; 
   &lt;td&gt;&lt;br /&gt; View all &lt;a href="https://infisical.com/docs/self-hosting/overview"&gt;deployment options&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Run Infisical locally&lt;/h3&gt; 
&lt;p&gt;To set up and run Infisical locally, make sure you have Git and Docker installed on your system. Then run the command for your system:&lt;/p&gt; 
&lt;p&gt;Linux/macOS:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;git clone https://github.com/Infisical/infisical &amp;amp;&amp;amp; cd "$(basename $_ .git)" &amp;amp;&amp;amp; cp .env.example .env &amp;amp;&amp;amp; docker compose -f docker-compose.prod.yml up
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Windows Command Prompt:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;git clone https://github.com/Infisical/infisical &amp;amp;&amp;amp; cd infisical &amp;amp;&amp;amp; copy .env.example .env &amp;amp;&amp;amp; docker compose -f docker-compose.prod.yml up
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Create an account at &lt;code&gt;http://localhost:80&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Scan and prevent secret leaks&lt;/h3&gt; 
&lt;p&gt;On top managing secrets with Infisical, you can also &lt;a href=""&gt;scan for over 140+ secret types&lt;/a&gt; in your files, directories and git repositories.&lt;/p&gt; 
&lt;p&gt;To scan your full git history, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;infisical scan --verbose
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Install pre commit hook to scan each commit before you push to your repository&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;infisical scan install --pre-commit-hook
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Learn about Infisical's code scanning feature &lt;a href="https://infisical.com/docs/cli/scanning-overview"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Open-source vs. paid&lt;/h2&gt; 
&lt;p&gt;This repo available under the &lt;a href="https://github.com/Infisical/infisical/raw/main/LICENSE"&gt;MIT expat license&lt;/a&gt;, with the exception of the &lt;code&gt;ee&lt;/code&gt; directory which will contain premium enterprise features requiring a Infisical license.&lt;/p&gt; 
&lt;p&gt;If you are interested in managed Infisical Cloud of self-hosted Enterprise Offering, take a look at &lt;a href="https://infisical.com/"&gt;our website&lt;/a&gt; or &lt;a href="https://infisical.cal.com/vlad/infisical-demo"&gt;book a meeting with us&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Security&lt;/h2&gt; 
&lt;p&gt;Please do not file GitHub issues or post on our public forum for security vulnerabilities, as they are public!&lt;/p&gt; 
&lt;p&gt;Infisical takes security issues very seriously. If you have any concerns about Infisical or believe you have uncovered a vulnerability, please get in touch via the e-mail address &lt;a href="mailto:security@infisical.com"&gt;security@infisical.com&lt;/a&gt;. In the message, try to provide a description of the issue and ideally a way of reproducing it. The security team will get back to you as soon as possible.&lt;/p&gt; 
&lt;p&gt;Note that this security address should be used only for undisclosed vulnerabilities. Please report any security problems to us before disclosing it publicly.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Whether it's big or small, we love contributions. Check out our guide to see how to &lt;a href="https://infisical.com/docs/contributing/getting-started"&gt;get started&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Not sure where to get started? You can:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Join our &lt;a href="https://infisical.com/slack"&gt;Slack&lt;/a&gt;, and ask us any questions there.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;We are hiring!&lt;/h2&gt; 
&lt;p&gt;If you're reading this, there is a strong chance you like the products we created.&lt;/p&gt; 
&lt;p&gt;You might also make a great addition to our team. We're growing fast and would love for you to &lt;a href="https://infisical.com/careers"&gt;join us&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>simular-ai/Agent-S</title>
      <link>https://github.com/simular-ai/Agent-S</link>
      <description>&lt;p&gt;Agent S: an open agentic framework that uses computers like a human&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt; &lt;img src="https://raw.githubusercontent.com/simular-ai/Agent-S/main/images/agent_s.png" alt="Logo" style="vertical-align:middle" width="60" /&gt; Agent S: &lt;small&gt;Use Computer Like a Human&lt;/small&gt; &lt;/h1&gt; 
&lt;p align="center"&gt;&amp;nbsp; 🌐 &lt;a href="https://www.simular.ai/articles/agent-s3"&gt;[S3 blog]&lt;/a&gt;&amp;nbsp; 📄 &lt;a href="https://arxiv.org/abs/2510.02250"&gt;[S3 Paper]&lt;/a&gt;&amp;nbsp; 🎥 &lt;a href="https://www.youtube.com/watch?v=VHr0a3UBsh4"&gt;[S3 Video]&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt;&amp;nbsp; 🌐 &lt;a href="https://www.simular.ai/articles/agent-s2-technical-review"&gt;[S2 blog]&lt;/a&gt;&amp;nbsp; 📄 &lt;a href="https://arxiv.org/abs/2504.00906"&gt;[S2 Paper (COLM 2025)]&lt;/a&gt;&amp;nbsp; 🎥 &lt;a href="https://www.youtube.com/watch?v=wUGVQl7c0eg"&gt;[S2 Video]&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt;&amp;nbsp; 🌐 &lt;a href="https://www.simular.ai/agent-s"&gt;[S1 blog]&lt;/a&gt;&amp;nbsp; 📄 &lt;a href="https://arxiv.org/abs/2410.08164"&gt;[S1 Paper (ICLR 2025)]&lt;/a&gt;&amp;nbsp; 🎥 &lt;a href="https://www.youtube.com/watch?v=OBDE3Knte0g"&gt;[S1 Video]&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt;&amp;nbsp; &lt;a href="https://trendshift.io/repositories/13151" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/13151" alt="simular-ai%2FAgent-S | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://img.shields.io/badge/OS-Windows-blue?logo=windows&amp;amp;logoColor=white" alt="Windows" /&gt; &lt;img src="https://img.shields.io/badge/OS-macOS-black?logo=apple&amp;amp;logoColor=white" alt="macOS" /&gt; &lt;img src="https://img.shields.io/badge/OS-Linux-yellow?logo=linux&amp;amp;logoColor=black" alt="Linux" /&gt; &lt;a href="https://discord.gg/E2XfsK9fPV"&gt; &lt;img src="https://dcbadge.limes.pink/api/server/https://discord.gg/E2XfsK9fPV?style=flat" alt="Discord" /&gt; &lt;/a&gt; &amp;nbsp;&amp;nbsp; &lt;a href="https://pepy.tech/projects/gui-agents"&gt; &lt;img src="https://static.pepy.tech/badge/gui-agents" alt="PyPI Downloads" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;!-- Keep these links. Translations will automatically update with the README. --&gt; 
 &lt;a href="https://www.readme-i18n.com/simular-ai/Agent-S?lang=de"&gt;Deutsch&lt;/a&gt; | 
 &lt;a href="https://www.readme-i18n.com/simular-ai/Agent-S?lang=es"&gt;Español&lt;/a&gt; | 
 &lt;a href="https://www.readme-i18n.com/simular-ai/Agent-S?lang=fr"&gt;français&lt;/a&gt; | 
 &lt;a href="https://www.readme-i18n.com/simular-ai/Agent-S?lang=ja"&gt;日本語&lt;/a&gt; | 
 &lt;a href="https://www.readme-i18n.com/simular-ai/Agent-S?lang=ko"&gt;한국어&lt;/a&gt; | 
 &lt;a href="https://www.readme-i18n.com/simular-ai/Agent-S?lang=pt"&gt;Português&lt;/a&gt; | 
 &lt;a href="https://www.readme-i18n.com/simular-ai/Agent-S?lang=ru"&gt;Русский&lt;/a&gt; | 
 &lt;a href="https://www.readme-i18n.com/simular-ai/Agent-S?lang=zh"&gt;中文&lt;/a&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt;
  &amp;nbsp;&amp;nbsp; 
 &lt;p&gt;Skip the setup? Try Agent S in &lt;a href="https://cloud.simular.ai/"&gt;Simular Cloud&lt;/a&gt; &lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;🥳 Updates&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;2025/10/02&lt;/strong&gt;: Released Agent S3 and its &lt;a href="https://arxiv.org/abs/2510.02250"&gt;technical paper&lt;/a&gt;, setting a new SOTA of &lt;strong&gt;69.9%&lt;/strong&gt; on OSWorld (approaching 72% human performance), with strong generalizability on WindowsAgentArena and AndroidWorld! It is also simpler, faster, and more flexible.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;2025/08/01&lt;/strong&gt;: Agent S2.5 is released (gui-agents v0.2.5): simpler, better, and faster! New SOTA on &lt;a href="https://os-world.github.io"&gt;OSWorld-Verified&lt;/a&gt;!&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;2025/07/07&lt;/strong&gt;: The &lt;a href="https://arxiv.org/abs/2504.00906"&gt;Agent S2 paper&lt;/a&gt; is accepted to COLM 2025! See you in Montreal!&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;2025/04/27&lt;/strong&gt;: The Agent S paper won the Best Paper Award 🏆 at ICLR 2025 Agentic AI for Science Workshop!&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;2025/04/01&lt;/strong&gt;: Released the &lt;a href="https://arxiv.org/abs/2504.00906"&gt;Agent S2 paper&lt;/a&gt; with new SOTA results on OSWorld, WindowsAgentArena, and AndroidWorld!&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;2025/03/12&lt;/strong&gt;: Released Agent S2 along with v0.2.0 of &lt;a href="https://github.com/simular-ai/Agent-S"&gt;gui-agents&lt;/a&gt;, the new state-of-the-art for computer use agents (CUA), outperforming OpenAI's CUA/Operator and Anthropic's Claude 3.7 Sonnet Computer-Use!&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;2025/01/22&lt;/strong&gt;: The &lt;a href="https://arxiv.org/abs/2410.08164"&gt;Agent S paper&lt;/a&gt; is accepted to ICLR 2025!&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;2025/01/21&lt;/strong&gt;: Released v0.1.2 of &lt;a href="https://github.com/simular-ai/Agent-S"&gt;gui-agents&lt;/a&gt; library, with support for Linux and Windows!&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;2024/12/05&lt;/strong&gt;: Released v0.1.0 of &lt;a href="https://github.com/simular-ai/Agent-S"&gt;gui-agents&lt;/a&gt; library, allowing you to use Agent-S for Mac, OSWorld, and WindowsAgentArena with ease!&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;2024/10/10&lt;/strong&gt;: Released the &lt;a href="https://arxiv.org/abs/2410.08164"&gt;Agent S paper&lt;/a&gt; and codebase!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/simular-ai/Agent-S/main/#-introduction"&gt;💡 Introduction&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/simular-ai/Agent-S/main/#-current-results"&gt;🎯 Current Results&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/simular-ai/Agent-S/main/#%EF%B8%8F-installation--setup"&gt;🛠️ Installation &amp;amp; Setup&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/simular-ai/Agent-S/main/#-usage"&gt;🚀 Usage&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/simular-ai/Agent-S/main/#-acknowledgements"&gt;🤝 Acknowledgements&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/simular-ai/Agent-S/main/#-citation"&gt;💬 Citation&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;💡 Introduction&lt;/h2&gt; 
&lt;p&gt;Welcome to &lt;strong&gt;Agent S&lt;/strong&gt;, an open-source framework designed to enable autonomous interaction with computers through Agent-Computer Interface. Our mission is to build intelligent GUI agents that can learn from past experiences and perform complex tasks autonomously on your computer.&lt;/p&gt; 
&lt;p&gt;Whether you're interested in AI, automation, or contributing to cutting-edge agent-based systems, we're excited to have you here!&lt;/p&gt; 
&lt;h2&gt;🎯 Current Results&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/simular-ai/Agent-S/main/images/s3_results.png" alt="Agent S3 Results" width="700" /&gt; &lt;/p&gt; 
&lt;p&gt;On OSWorld, Agent S3 alone reaches 62.6% in the 100-step setting, already exceeding the previous state of the art of 61.4% (Claude Sonnet 4.5). With the addition of Behavior Best-of-N, performance climbs even higher to 69.9%, bringing computer-use agents to within just a few points of human-level accuracy (72%).&lt;/p&gt; 
&lt;p&gt;Agent S3 also demonstrates strong zero-shot generalization. On WindowsAgentArena, accuracy rises from 50.2% using only Agent S3 to 56.6% by selecting from 3 rollouts. Similarly on AndroidWorld, performance improves from 68.1% to 71.6%&lt;/p&gt; 
&lt;h2&gt;🛠️ Installation &amp;amp; Setup&lt;/h2&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Single Monitor&lt;/strong&gt;: Our agent is designed for single monitor screens&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Security&lt;/strong&gt;: The agent runs Python code to control your computer - use with care&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Supported Platforms&lt;/strong&gt;: Linux, Mac, and Windows&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;p&gt;To install Agent S3 without cloning the repository, run&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install gui-agents
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you would like to test Agent S3 while making changes, clone the repository and install using&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Don't forget to also &lt;code&gt;brew install tesseract&lt;/code&gt;! Pytesseract requires this extra installation to work.&lt;/p&gt; 
&lt;h3&gt;API Configuration&lt;/h3&gt; 
&lt;h4&gt;Option 1: Environment Variables&lt;/h4&gt; 
&lt;p&gt;Add to your &lt;code&gt;.bashrc&lt;/code&gt; (Linux) or &lt;code&gt;.zshrc&lt;/code&gt; (MacOS):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export OPENAI_API_KEY=&amp;lt;YOUR_API_KEY&amp;gt;
export ANTHROPIC_API_KEY=&amp;lt;YOUR_ANTHROPIC_API_KEY&amp;gt;
export HF_TOKEN=&amp;lt;YOUR_HF_TOKEN&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Option 2: Python Script&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import os
os.environ["OPENAI_API_KEY"] = "&amp;lt;YOUR_API_KEY&amp;gt;"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Supported Models&lt;/h3&gt; 
&lt;p&gt;We support Azure OpenAI, Anthropic, Gemini, Open Router, and vLLM inference. See &lt;a href="https://raw.githubusercontent.com/simular-ai/Agent-S/main/models.md"&gt;models.md&lt;/a&gt; for details.&lt;/p&gt; 
&lt;h3&gt;Grounding Models (Required)&lt;/h3&gt; 
&lt;p&gt;For optimal performance, we recommend &lt;a href="https://huggingface.co/ByteDance-Seed/UI-TARS-1.5-7B"&gt;UI-TARS-1.5-7B&lt;/a&gt; hosted on Hugging Face Inference Endpoints or another provider. See &lt;a href="https://huggingface.co/learn/cookbook/en/enterprise_dedicated_endpoints"&gt;Hugging Face Inference Endpoints&lt;/a&gt; for setup instructions.&lt;/p&gt; 
&lt;h2&gt;🚀 Usage&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;⚡️ &lt;strong&gt;Recommended Setup:&lt;/strong&gt;&lt;br /&gt; For the best configuration, we recommend using &lt;strong&gt;OpenAI gpt-5-2025-08-07&lt;/strong&gt; as the main model, paired with &lt;strong&gt;UI-TARS-1.5-7B&lt;/strong&gt; for grounding.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;CLI&lt;/h3&gt; 
&lt;p&gt;Note, this is running Agent S3, our improved agent, without bBoN.&lt;/p&gt; 
&lt;p&gt;Run Agent S3 with the required parameters:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;agent_s \
    --provider openai \
    --model gpt-5-2025-08-07 \
    --ground_provider huggingface \
    --ground_url http://localhost:8080 \
    --ground_model ui-tars-1.5-7b \
    --grounding_width 1920 \
    --grounding_height 1080
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Local Coding Environment (Optional)&lt;/h4&gt; 
&lt;p&gt;For tasks that require code execution (e.g., data processing, file manipulation, system automation), you can enable the local coding environment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;agent_s \
    --provider openai \
    --model gpt-5-2025-08-07 \
    --ground_provider huggingface \
    --ground_url http://localhost:8080 \
    --ground_model ui-tars-1.5-7b \
    --grounding_width 1920 \
    --grounding_height 1080 \
    --enable_local_env
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;⚠️ &lt;strong&gt;WARNING&lt;/strong&gt;: The local coding environment executes arbitrary Python and Bash code locally on your machine. Only use this feature in trusted environments and with trusted inputs.&lt;/p&gt; 
&lt;h4&gt;Required Parameters&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;--provider&lt;/code&gt;&lt;/strong&gt;: Main generation model provider (e.g., openai, anthropic, etc.) - Default: "openai"&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;--model&lt;/code&gt;&lt;/strong&gt;: Main generation model name (e.g., gpt-5-2025-08-07) - Default: "gpt-5-2025-08-07"&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;--ground_provider&lt;/code&gt;&lt;/strong&gt;: The provider for the grounding model - &lt;strong&gt;Required&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;--ground_url&lt;/code&gt;&lt;/strong&gt;: The URL of the grounding model - &lt;strong&gt;Required&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;--ground_model&lt;/code&gt;&lt;/strong&gt;: The model name for the grounding model - &lt;strong&gt;Required&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;--grounding_width&lt;/code&gt;&lt;/strong&gt;: Width of the output coordinate resolution from the grounding model - &lt;strong&gt;Required&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;--grounding_height&lt;/code&gt;&lt;/strong&gt;: Height of the output coordinate resolution from the grounding model - &lt;strong&gt;Required&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Optional Parameters&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;--model_temperature&lt;/code&gt;&lt;/strong&gt;: The temperature to fix all model calls to (necessary to set to 1.0 for models like o3 but can be left blank for other models)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Grounding Model Dimensions&lt;/h4&gt; 
&lt;p&gt;The grounding width and height should match the output coordinate resolution of your grounding model:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;UI-TARS-1.5-7B&lt;/strong&gt;: Use &lt;code&gt;--grounding_width 1920 --grounding_height 1080&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;UI-TARS-72B&lt;/strong&gt;: Use &lt;code&gt;--grounding_width 1000 --grounding_height 1000&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Optional Parameters&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;--model_url&lt;/code&gt;&lt;/strong&gt;: Custom API URL for main generation model - Default: ""&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;--model_api_key&lt;/code&gt;&lt;/strong&gt;: API key for main generation model - Default: ""&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;--ground_api_key&lt;/code&gt;&lt;/strong&gt;: API key for grounding model endpoint - Default: ""&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;--max_trajectory_length&lt;/code&gt;&lt;/strong&gt;: Maximum number of image turns to keep in trajectory - Default: 8&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;--enable_reflection&lt;/code&gt;&lt;/strong&gt;: Enable reflection agent to assist the worker agent - Default: True&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;--enable_local_env&lt;/code&gt;&lt;/strong&gt;: Enable local coding environment for code execution (WARNING: Executes arbitrary code locally) - Default: False&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Local Coding Environment Details&lt;/h4&gt; 
&lt;p&gt;The local coding environment enables Agent S3 to execute Python and Bash code directly on your machine. This is particularly useful for:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Data Processing&lt;/strong&gt;: Manipulating spreadsheets, CSV files, or databases&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;File Operations&lt;/strong&gt;: Bulk file processing, content extraction, or file organization&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;System Automation&lt;/strong&gt;: Configuration changes, system setup, or automation scripts&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Code Development&lt;/strong&gt;: Writing, editing, or executing code files&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Text Processing&lt;/strong&gt;: Document manipulation, content editing, or formatting&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;When enabled, the agent can use the &lt;code&gt;call_code_agent&lt;/code&gt; action to execute code blocks for tasks that can be completed through programming rather than GUI interaction.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Requirements:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Python&lt;/strong&gt;: The same Python interpreter used to run Agent S3 (automatically detected)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Bash&lt;/strong&gt;: Available at &lt;code&gt;/bin/bash&lt;/code&gt; (standard on macOS and Linux)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;System Permissions&lt;/strong&gt;: The agent runs with the same permissions as the user executing it&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Security Considerations:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;The local environment executes arbitrary code with the same permissions as the user running the agent&lt;/li&gt; 
 &lt;li&gt;Only enable this feature in trusted environments&lt;/li&gt; 
 &lt;li&gt;Be cautious when the agent generates code for system-level operations&lt;/li&gt; 
 &lt;li&gt;Consider running in a sandboxed environment for untrusted tasks&lt;/li&gt; 
 &lt;li&gt;Bash scripts are executed with a 30-second timeout to prevent hanging processes&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;code&gt;gui_agents&lt;/code&gt; SDK&lt;/h3&gt; 
&lt;p&gt;First, we import the necessary modules. &lt;code&gt;AgentS3&lt;/code&gt; is the main agent class for Agent S3. &lt;code&gt;OSWorldACI&lt;/code&gt; is our grounding agent that translates agent actions into executable python code.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import pyautogui
import io
from gui_agents.s3.agents.agent_s import AgentS3
from gui_agents.s3.agents.grounding import OSWorldACI
from gui_agents.s3.utils.local_env import LocalEnv  # Optional: for local coding environment

# Load in your API keys.
from dotenv import load_dotenv
load_dotenv()

current_platform = "linux"  # "darwin", "windows"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Next, we define our engine parameters. &lt;code&gt;engine_params&lt;/code&gt; is used for the main agent, and &lt;code&gt;engine_params_for_grounding&lt;/code&gt; is for grounding. For &lt;code&gt;engine_params_for_grounding&lt;/code&gt;, we support custom endpoints like HuggingFace TGI, vLLM, and Open Router.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;engine_params = {
  "engine_type": provider,
  "model": model,
  "base_url": model_url,           # Optional
  "api_key": model_api_key,        # Optional
  "temperature": model_temperature # Optional
}

# Load the grounding engine from a custom endpoint
ground_provider = "&amp;lt;your_ground_provider&amp;gt;"
ground_url = "&amp;lt;your_ground_url&amp;gt;"
ground_model = "&amp;lt;your_ground_model&amp;gt;"
ground_api_key = "&amp;lt;your_ground_api_key&amp;gt;"

# Set grounding dimensions based on your model's output coordinate resolution
# UI-TARS-1.5-7B: grounding_width=1920, grounding_height=1080
# UI-TARS-72B: grounding_width=1000, grounding_height=1000
grounding_width = 1920  # Width of output coordinate resolution
grounding_height = 1080  # Height of output coordinate resolution

engine_params_for_grounding = {
  "engine_type": ground_provider,
  "model": ground_model,
  "base_url": ground_url,
  "api_key": ground_api_key,  # Optional
  "grounding_width": grounding_width,
  "grounding_height": grounding_height,
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then, we define our grounding agent and Agent S3.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Optional: Enable local coding environment
enable_local_env = False  # Set to True to enable local code execution
local_env = LocalEnv() if enable_local_env else None

grounding_agent = OSWorldACI(
    env=local_env,  # Pass local_env for code execution capability
    platform=current_platform,
    engine_params_for_generation=engine_params,
    engine_params_for_grounding=engine_params_for_grounding,
    width=1920,  # Optional: screen width
    height=1080  # Optional: screen height
)

agent = AgentS3(
    engine_params,
    grounding_agent,
    platform=current_platform,
    max_trajectory_length=8,  # Optional: maximum image turns to keep
    enable_reflection=True     # Optional: enable reflection agent
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Finally, let's query the agent!&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Get screenshot.
screenshot = pyautogui.screenshot()
buffered = io.BytesIO() 
screenshot.save(buffered, format="PNG")
screenshot_bytes = buffered.getvalue()

obs = {
  "screenshot": screenshot_bytes,
}

instruction = "Close VS Code"
info, action = agent.predict(instruction=instruction, observation=obs)

exec(action[0])
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Refer to &lt;code&gt;gui_agents/s3/cli_app.py&lt;/code&gt; for more details on how the inference loop works.&lt;/p&gt; 
&lt;h3&gt;OSWorld&lt;/h3&gt; 
&lt;p&gt;To deploy Agent S3 in OSWorld, follow the &lt;a href="https://raw.githubusercontent.com/simular-ai/Agent-S/main/osworld_setup/s3/OSWorld.md"&gt;OSWorld Deployment instructions&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;💬 Citations&lt;/h2&gt; 
&lt;p&gt;If you find this codebase useful, please cite:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@misc{Agent-S3,
      title={The Unreasonable Effectiveness of Scaling Agents for Computer Use}, 
      author={Gonzalo Gonzalez-Pumariega and Vincent Tu and Chih-Lun Lee and Jiachen Yang and Ang Li and Xin Eric Wang},
      year={2025},
      eprint={2510.02250},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2510.02250}, 
}

@misc{Agent-S2,
      title={Agent S2: A Compositional Generalist-Specialist Framework for Computer Use Agents}, 
      author={Saaket Agashe and Kyle Wong and Vincent Tu and Jiachen Yang and Ang Li and Xin Eric Wang},
      year={2025},
      eprint={2504.00906},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2504.00906}, 
}

@inproceedings{Agent-S,
    title={{Agent S: An Open Agentic Framework that Uses Computers Like a Human}},
    author={Saaket Agashe and Jiuzhou Han and Shuyu Gan and Jiachen Yang and Ang Li and Xin Eric Wang},
    booktitle={International Conference on Learning Representations (ICLR)},
    year={2025},
    url={https://arxiv.org/abs/2410.08164}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#simular-ai/Agent-S&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=simular-ai/Agent-S&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>jingyaogong/minimind</title>
      <link>https://github.com/jingyaogong/minimind</link>
      <description>&lt;p&gt;🚀🚀 「大模型」2小时完全从0训练26M的小参数GPT！🌏 Train a 26M-parameter GPT from scratch in just 2h!&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/logo.png" alt="logo" /&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;img src="https://visitor-badge.laobi.icu/badge?page_id=jingyaogong/minimind" alt="visitors" /&gt; &lt;a href="https://github.com/jingyaogong/minimind/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/jingyaogong/minimind?style=social" alt="GitHub Repo stars" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/jingyaogong/minimind/master/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/jingyaogong/minimind" alt="GitHub Code License" /&gt;&lt;/a&gt; &lt;a href="https://github.com/jingyaogong/minimind/commits/master"&gt;&lt;img src="https://img.shields.io/github/last-commit/jingyaogong/minimind" alt="GitHub last commit" /&gt;&lt;/a&gt; &lt;a href="https://github.com/jingyaogong/minimind/pulls"&gt;&lt;img src="https://img.shields.io/badge/PRs-welcome-blue" alt="GitHub pull request" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/collections/jingyaogong/minimind-66caf8d999f5c7fa64f399e5"&gt;&lt;img src="https://img.shields.io/badge/%F0%9F%A4%97-MiniMind%20%20Collection-blue" alt="Collection" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;img src="https://trendshift.io/api/badge/repositories/12586" alt="GitHub Trend" /&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;h3&gt;"大道至简"&lt;/h3&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;中文 | &lt;a href="https://raw.githubusercontent.com/jingyaogong/minimind/master/README_en.md"&gt;English&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;ul&gt; 
 &lt;li&gt;此开源项目旨在完全从0开始，仅用3块钱成本 + 2小时！即可训练出仅为25.8M的超小语言模型&lt;strong&gt;MiniMind&lt;/strong&gt;。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MiniMind&lt;/strong&gt;系列极其轻量，最小版本体积是 GPT-3 的 $\frac{1}{7000}$，力求做到最普通的个人GPU也可快速训练。&lt;/li&gt; 
 &lt;li&gt;项目同时开源了大模型的极简结构-包含拓展共享混合专家(MoE)、数据集清洗、预训练(Pretrain)、监督微调(SFT)、LoRA微调、直接偏好优化(DPO)、强化学习训练(RLAIF: PPO/GRPO等)、模型蒸馏等全过程代码。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MiniMind&lt;/strong&gt;同时拓展了视觉多模态的VLM: &lt;a href="https://github.com/jingyaogong/minimind-v"&gt;MiniMind-V&lt;/a&gt;。&lt;/li&gt; 
 &lt;li&gt;项目所有核心算法代码均从0使用PyTorch原生重构！不依赖第三方库提供的抽象接口。&lt;/li&gt; 
 &lt;li&gt;这不仅是大语言模型的全阶段开源复现，也是一个入门LLM的教程。&lt;/li&gt; 
 &lt;li&gt;希望此项目能为所有人提供一个抛砖引玉的示例，一起感受创造的乐趣！推动更广泛AI社区的进步！&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;为防止误解，“2小时” 基于NVIDIA 3090硬件设备（单卡）测试，“3块钱”指GPU服务器租用成本，具体规格详情见下文。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/minimind2.gif" alt="minimind2" /&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://www.modelscope.cn/studios/gongjy/MiniMind-Reasoning"&gt;🔗🍓推理模型&lt;/a&gt; | &lt;a href="https://www.modelscope.cn/studios/gongjy/MiniMind"&gt;🔗🤖常规模型&lt;/a&gt; | &lt;a href="https://www.bilibili.com/video/BV12dHPeqE72/?share_source=copy_web&amp;amp;vd_source=670c2504f88726f8cf4a21ef6147c0e8"&gt;🔗🎞️视频介绍&lt;/a&gt;&lt;/p&gt; 
 &lt;div align="center"&gt; 
  &lt;table&gt; 
   &lt;tbody&gt;
    &lt;tr&gt; 
     &lt;td align="center"&gt; &lt;a href="https://huggingface.co/collections/jingyaogong/minimind-66caf8d999f5c7fa64f399e5" style="text-decoration: none;"&gt; &lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/and_huggingface.png" alt="Hugging Face Logo" style="vertical-align: middle; width: auto; max-width: 100%;" /&gt; &lt;/a&gt; &lt;/td&gt; 
     &lt;td align="center"&gt; &lt;a href="https://www.modelscope.cn/profile/gongjy" style="text-decoration: none;"&gt; &lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/and_modelscope.png" alt="ModelScope Logo" style="vertical-align: middle; width: auto; max-width: 100%;" /&gt; &lt;/a&gt; &lt;/td&gt; 
    &lt;/tr&gt; 
   &lt;/tbody&gt;
  &lt;/table&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;h1&gt;📌 Introduction&lt;/h1&gt; 
&lt;p&gt;大语言模型（Large Language Model, LLM）的出现引发了全世界对AI的空前关注。 无论是ChatGPT、DeepSeek还是Qwen，都以其惊艳的效果令人叹为观止。 然而，动辄数百亿参数的庞大规模，使得它们对个人设备而言不仅难以训练，甚至连部署都显得遥不可及。 打开大模型的“黑盒子”，探索其内部运作机制，多么令人心潮澎湃！ 遗憾的是，99%的探索只能止步于使用LoRA等技术对现有大模型进行少量微调，学习一些新指令或任务。 这就好比教牛顿如何使用21世纪的智能手机——虽然有趣，却完全偏离了理解物理本质的初衷。 与此同时，第三方的大模型框架和工具库，如transformers+trl，几乎只暴露了高度抽象的接口。 通过短短10行代码，就能完成“加载模型+加载数据集+推理+强化学习”的全流程训练。 这种高效的封装固然便利，但也像一架高速飞船，将开发者与底层实现隔离开来，阻碍了深入探究LLM核心代码的机会。 然而，“用乐高拼出一架飞机，远比坐在头等舱里飞行更让人兴奋！”。 更糟糕的是，互联网上充斥着大量付费课程和营销号，以漏洞百出、一知半解的内容推销AI教程。 正因如此，本项目初衷是拉低LLM的学习门槛，让每个人都能从理解每一行代码开始， 从零开始亲手训练一个极小的语言模型。是的，从&lt;strong&gt;零开始训练&lt;/strong&gt;，而不是仅仅进行&lt;strong&gt;推理&lt;/strong&gt;！ 最低只需3块钱不到的服务器成本，就能亲身体验从0到1构建一个语言模型的全过程。 一起感受创造的乐趣吧！&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] （截至2025-10）MiniMind系列已完成多个型号模型的预训练，最小仅需25.8M（0.02B），即可具备流畅对话能力！&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;Models List&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;模型 (大小)&lt;/th&gt; 
    &lt;th&gt;推理占用 (约)&lt;/th&gt; 
    &lt;th&gt;Release&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;MiniMind2-small (26M)&lt;/td&gt; 
    &lt;td&gt;0.5 GB&lt;/td&gt; 
    &lt;td&gt;2025.04.26&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;MiniMind2-MoE (145M)&lt;/td&gt; 
    &lt;td&gt;1.0 GB&lt;/td&gt; 
    &lt;td&gt;2025.04.26&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;MiniMind2 (104M)&lt;/td&gt; 
    &lt;td&gt;1.0 GB&lt;/td&gt; 
    &lt;td&gt;2025.04.26&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;minimind-v1-small (26M)&lt;/td&gt; 
    &lt;td&gt;0.5 GB&lt;/td&gt; 
    &lt;td&gt;2024.08.28&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;minimind-v1-moe (4×26M)&lt;/td&gt; 
    &lt;td&gt;1.0 GB&lt;/td&gt; 
    &lt;td&gt;2024.09.17&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;minimind-v1 (108M)&lt;/td&gt; 
    &lt;td&gt;1.0 GB&lt;/td&gt; 
    &lt;td&gt;2024.09.01&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;p&gt;&lt;strong&gt;项目包含&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;MiniMind-LLM结构的全部代码（Dense+MoE模型）。&lt;/li&gt; 
 &lt;li&gt;包含Tokenizer分词器详细训练代码。&lt;/li&gt; 
 &lt;li&gt;包含Pretrain、SFT、LoRA、RLHF-DPO、RLAIF(PPO/GRPO/SPO)、模型蒸馏的全过程训练代码。&lt;/li&gt; 
 &lt;li&gt;收集、蒸馏、整理并清洗去重所有阶段的高质量数据集，且全部开源。&lt;/li&gt; 
 &lt;li&gt;从0实现预训练、指令微调、LoRA、DPO/PPO/GRPO/SPO强化学习，白盒模型蒸馏。关键算法几乎不依赖第三方封装的框架，且全部开源。&lt;/li&gt; 
 &lt;li&gt;同时兼容&lt;code&gt;transformers&lt;/code&gt;、&lt;code&gt;trl&lt;/code&gt;、&lt;code&gt;peft&lt;/code&gt;等第三方主流框架。&lt;/li&gt; 
 &lt;li&gt;训练支持单机单卡、单机多卡(DDP、DeepSpeed)训练，支持wandb/swanlab可视化训练流程。支持动态启停训练。&lt;/li&gt; 
 &lt;li&gt;在第三方测评榜（C-Eval、C-MMLU、OpenBookQA等）进行模型测试，支持YaRN算法执行RoPE长文本外推。&lt;/li&gt; 
 &lt;li&gt;实现Openai-Api协议的极简服务端，便于集成到第三方ChatUI使用（FastGPT、Open-WebUI等）。&lt;/li&gt; 
 &lt;li&gt;基于streamlit实现最简聊天WebUI前端。&lt;/li&gt; 
 &lt;li&gt;全面兼容社区热门&lt;code&gt;llama.cpp&lt;/code&gt;、&lt;code&gt;vllm&lt;/code&gt;、&lt;code&gt;ollama&lt;/code&gt;推理引擎或&lt;code&gt;Llama-Factory&lt;/code&gt;训练框架。&lt;/li&gt; 
 &lt;li&gt;复现(蒸馏/RL)大型推理模型DeepSeek-R1的MiniMind-Reason模型，&lt;strong&gt;数据+模型&lt;/strong&gt;全部开源！&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;希望此开源项目可以帮助LLM初学者快速入门！&lt;/p&gt; 
&lt;h3&gt;👉&lt;strong&gt;更新日志&lt;/strong&gt;&lt;/h3&gt; 
&lt;details close&gt; 
 &lt;summary&gt; &lt;b&gt;2025-10-24 (newest🎉)&lt;/b&gt; &lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;🔥 新增RLAIF训练算法：PPO、GRPO、SPO（从0原生实现）&lt;/li&gt; 
  &lt;li&gt;新增断点续训功能：支持训练自动恢复、跨GPU数量恢复、wandb记录连续性&lt;/li&gt; 
  &lt;li&gt;新增RLAIF数据集：rlaif-mini.jsonl（从SFT数据随机采样1万条）；简化DPO数据集，加入中文数据&lt;/li&gt; 
  &lt;li&gt;新增YaRN算法：支持RoPE长文本外推，提升长序列处理能力&lt;/li&gt; 
  &lt;li&gt;Adaptive Thinking：Reason模型可选是否启用思考链&lt;/li&gt; 
  &lt;li&gt;chat_template全面支持Tool Calling和Reasoning标签（&lt;code&gt;&amp;lt;tool_call&amp;gt;&lt;/code&gt;、&lt;code&gt;&amp;lt;think&amp;gt;&lt;/code&gt;等）&lt;/li&gt; 
  &lt;li&gt;新增RLAIF完整章节、训练曲线对比、算法原理折叠说明&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://swanlab.cn/"&gt;SwanLab&lt;/a&gt;替代WandB（国内访问友好，API完全兼容）&lt;/li&gt; 
  &lt;li&gt;规范化所有代码 &amp;amp; 修复一些已知bugs&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details close&gt; 
 &lt;summary&gt; &lt;b&gt;2025-04-26&lt;/b&gt; &lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;重要更新&lt;/li&gt; 
  &lt;li&gt;如有兼容性需要，可访问&lt;a href="https://github.com/jingyaogong/minimind/tree/7da201a944a90ed49daef8a0265c959288dff83a"&gt;🔗旧仓库内容🔗&lt;/a&gt;。&lt;/li&gt; 
  &lt;li&gt;MiniMind模型参数完全改名，对齐Transformers库模型（统一命名）。&lt;/li&gt; 
  &lt;li&gt;generate方式重构，继承自GenerationMixin类。&lt;/li&gt; 
  &lt;li&gt;🔥支持llama.cpp、vllm、ollama等热门三方生态。&lt;/li&gt; 
  &lt;li&gt;规范代码和目录结构。&lt;/li&gt; 
  &lt;li&gt;改动词表&lt;code&gt;&amp;lt;s&amp;gt;&amp;lt;/s&amp;gt;&lt;/code&gt;-&amp;gt;&lt;code&gt;&amp;lt;|im_start|&amp;gt;&amp;lt;|im_end|&amp;gt;&lt;/code&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;pre&gt;&lt;code class="language-text"&gt;为兼容第三方推理框架llama.cpp、vllm，本次更新需付出一些可观代价。
本次更新不再支持「直接」加载25-04-26以前的旧模型进行推理。
由于Llama位置编码方式与minimind存在区别，导致映射Llama模型后QK值存在差异
MiniMind2系列旧模型均经过权重映射+（微调训练）QKVO线性层校准恢复而来。
本次更新后将放弃对`minimind-v1`全系列的维护，并在仓库中下线。
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details close&gt; 
 &lt;summary&gt; &lt;b&gt;2025-02-09&lt;/b&gt; &lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;迎来发布以来重大更新，Release MiniMind2 Series。&lt;/li&gt; 
  &lt;li&gt;代码几乎全部重构，使用更简洁明了的统一结构。 如有旧代码的兼容性需要，可访问&lt;a href="https://github.com/jingyaogong/minimind/tree/6e9cd28ef9b34a0a10afbdf6f59e65cb6e628efb"&gt;🔗旧仓库内容🔗&lt;/a&gt;。&lt;/li&gt; 
  &lt;li&gt;免去数据预处理步骤。统一数据集格式，更换为&lt;code&gt;jsonl&lt;/code&gt;格式杜绝数据集下载混乱的问题。&lt;/li&gt; 
  &lt;li&gt;MiniMind2系列效果相比MiniMind-V1显著提升。&lt;/li&gt; 
  &lt;li&gt;小问题：{kv-cache写法更标准、MoE的负载均衡loss被考虑等等}&lt;/li&gt; 
  &lt;li&gt;提供模型迁移到私有数据集的训练方案（医疗模型、自我认知样例）。&lt;/li&gt; 
  &lt;li&gt;精简预训练数据集，并大幅提升预训练数据质量，大幅缩短个人快速训练所需时间，单卡3090即可2小时复现！&lt;/li&gt; 
  &lt;li&gt;更新：LoRA微调脱离peft包装，从0实现LoRA过程；DPO算法从0使用PyTorch原生实现；模型白盒蒸馏原生实现。&lt;/li&gt; 
  &lt;li&gt;MiniMind2-DeepSeek-R1系列蒸馏模型诞生！&lt;/li&gt; 
  &lt;li&gt;MiniMind2具备一定的英文能力！&lt;/li&gt; 
  &lt;li&gt;更新MiniMind2与第三方模型的基于更多大模型榜单测试性能的结果。&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details close&gt; 
 &lt;summary&gt; &lt;b&gt;2024-10-05&lt;/b&gt; &lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;为MiniMind拓展了多模态能力之---视觉&lt;/li&gt; 
  &lt;li&gt;移步孪生项目&lt;a href="https://github.com/jingyaogong/minimind-v"&gt;minimind-v&lt;/a&gt;查看详情！&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details close&gt; 
 &lt;summary&gt; &lt;b&gt;2024-09-27&lt;/b&gt; &lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;09-27更新pretrain数据集的预处理方式，为了保证文本完整性，放弃预处理成.bin训练的形式（轻微牺牲训练速度）。&lt;/li&gt; 
  &lt;li&gt;目前pretrain预处理后的文件命名为：pretrain_data.csv。&lt;/li&gt; 
  &lt;li&gt;删除了一些冗余的代码。&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details close&gt; 
 &lt;summary&gt; &lt;b&gt;2024-09-17&lt;/b&gt; &lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;更新minimind-v1-moe模型&lt;/li&gt; 
  &lt;li&gt;为了防止歧义，不再使用mistral_tokenizer分词，全部采用自定义的minimind_tokenizer作为分词器。&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details close&gt; 
 &lt;summary&gt; &lt;b&gt;2024-09-01&lt;/b&gt; &lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;更新minimind-v1 (108M)模型，采用minimind_tokenizer，预训练轮次3 + SFT轮次10，更充分训练，性能更强。&lt;/li&gt; 
  &lt;li&gt;项目已部署至ModelScope创空间，可以在此网站上体验：&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://www.modelscope.cn/studios/gongjy/minimind"&gt;🔗ModelScope在线体验🔗&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details close&gt; 
 &lt;summary&gt; &lt;b&gt;2024-08-27&lt;/b&gt; &lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;项目首次开源&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h1&gt;📌 快速开始&lt;/h1&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;分享本人的软硬件配置（仅供参考）&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;CPU: Intel(R) Core(TM) i9-10980XE CPU @ 3.00GHz&lt;/li&gt; 
  &lt;li&gt;RAM: 128 GB&lt;/li&gt; 
  &lt;li&gt;GPU: NVIDIA GeForce RTX 3090(24GB) * 8&lt;/li&gt; 
  &lt;li&gt;Ubuntu==20.04&lt;/li&gt; 
  &lt;li&gt;CUDA==12.2&lt;/li&gt; 
  &lt;li&gt;Python==3.10.16&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/jingyaogong/minimind/master/requirements.txt"&gt;requirements.txt&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;第0步&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/jingyaogong/minimind.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Ⅰ 测试已有模型效果&lt;/h2&gt; 
&lt;h3&gt;1.环境准备&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r requirements.txt -i https://mirrors.aliyun.com/pypi/simple
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2.下载模型&lt;/h3&gt; 
&lt;p&gt;到项目根目录&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://huggingface.co/jingyaogong/MiniMind2 # or https://www.modelscope.cn/models/gongjy/MiniMind2
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;（可选）命令行问答&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 使用transformers格式模型
python eval_llm.py --load_from ./MiniMind2
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;（可选）启动WebUI&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 可能需要`python&amp;gt;=3.10` 安装 `pip install streamlit`
# cd scripts
streamlit run web_demo.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;（可选）第三方推理框架&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# ollama
ollama run jingyaogong/minimind2
# vllm
vllm serve ./MiniMind2/ --served-model-name "minimind"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Ⅱ 从0开始自己训练&lt;/h2&gt; 
&lt;h3&gt;1.环境准备&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r requirements.txt -i https://mirrors.aliyun.com/pypi/simple
&lt;/code&gt;&lt;/pre&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;注：提前测试Torch是否可用cuda&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;import torch
print(torch.cuda.is_available())
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;如果不可用，请自行去&lt;a href="https://download.pytorch.org/whl/torch_stable.html"&gt;torch_stable&lt;/a&gt; 下载whl文件安装。参考&lt;a href="https://blog.csdn.net/weixin_45456738/article/details/141029610?ops_request_misc=&amp;amp;request_id=&amp;amp;biz_id=102&amp;amp;utm_term=%E5%AE%89%E8%A3%85torch&amp;amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-2-141029610.nonecase&amp;amp;spm=1018.2226.3001.4187"&gt;链接&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;h3&gt;2.数据下载&lt;/h3&gt; 
&lt;p&gt;从下文提供的&lt;a href="https://www.modelscope.cn/datasets/gongjy/minimind_dataset/files"&gt;数据集下载链接&lt;/a&gt; 下载需要的数据文件（创建&lt;code&gt;./dataset&lt;/code&gt;目录）并放到&lt;code&gt;./dataset&lt;/code&gt;下&lt;/p&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;注：数据集须知&lt;/summary&gt; 
 &lt;p&gt;默认推荐下载&lt;code&gt;pretrain_hq.jsonl&lt;/code&gt; + &lt;code&gt;sft_mini_512.jsonl&lt;/code&gt;最快速度复现Zero聊天模型。&lt;/p&gt; 
 &lt;p&gt;数据文件可自由选择，下文提供了多种搭配方案，可根据自己手头的训练需求和GPU资源进行适当组合。&lt;/p&gt; 
&lt;/details&gt; 
&lt;h3&gt;3.开始训练&lt;/h3&gt; 
&lt;p&gt;目录位于&lt;code&gt;trainer&lt;/code&gt;&lt;/p&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;💡 检查点暂停续训&lt;/summary&gt; 
 &lt;p&gt;所有训练脚本均自动保存检查点，只需添加 &lt;code&gt;--from_resume 1&lt;/code&gt; 参数即可自动检测加载&amp;amp;恢复训练：&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;python train_pretrain.py --from_resume 1
python train_full_sft.py --from_resume 1
...
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;断点续训机制说明：&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;训练过程自动在 &lt;code&gt;./checkpoints/&lt;/code&gt; 目录保存完整检查点（模型、优化器、训练进度等）&lt;/li&gt; 
  &lt;li&gt;检查点文件命名：&lt;code&gt;&amp;lt;权重名&amp;gt;_&amp;lt;维度&amp;gt;_resume.pth&lt;/code&gt;（如：&lt;code&gt;full_sft_512_resume.pth&lt;/code&gt;）&lt;/li&gt; 
  &lt;li&gt;支持跨不同GPU数量恢复（自动调整step）&lt;/li&gt; 
  &lt;li&gt;支持wandb训练记录连续性（自动恢复同一个run）&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;适合长时间训练或不稳定环境，无需担心训练中断导致进度丢失&lt;/p&gt; 
 &lt;/blockquote&gt; 
&lt;/details&gt; 
&lt;p&gt;&lt;strong&gt;3.1 预训练（学知识）&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python train_pretrain.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;执行预训练，得到 &lt;code&gt;pretrain_*.pth&lt;/code&gt; 作为预训练的输出权重（其中*为模型的dimension，默认为512）&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;3.2 监督微调（学对话方式）&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python train_full_sft.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;执行监督微调，得到 &lt;code&gt;full_sft_*.pth&lt;/code&gt; 作为指令微调的输出权重（其中&lt;code&gt;full&lt;/code&gt;即为全参数微调）&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;注：训练须知&lt;/summary&gt; 
 &lt;p&gt;所有训练过程默认每隔100步保存1次参数到文件&lt;code&gt;./out/***.pth&lt;/code&gt;（每次会覆盖掉旧权重文件）。&lt;/p&gt; 
 &lt;p&gt;简单起见，此处只写明两个阶段训练过程。如需其它训练 (LoRA, 蒸馏, 强化学习, 微调推理等) 可参考下文【实验】小节的详细说明。&lt;/p&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h3&gt;4.测试自己训练的模型效果&lt;/h3&gt; 
&lt;p&gt;确保需要测试的模型&lt;code&gt;*.pth&lt;/code&gt;文件位于&lt;code&gt;./out/&lt;/code&gt;目录下。 也可以直接去&lt;a href="https://www.modelscope.cn/models/gongjy/MiniMind2-PyTorch/files"&gt;此处&lt;/a&gt;下载使用我训练的&lt;code&gt;*.pth&lt;/code&gt;文件。&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python eval_llm.py --weight full_sft # 或 pretrain/dpo/ppo/grpo...
&lt;/code&gt;&lt;/pre&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;注：测试须知&lt;/summary&gt; 
 &lt;p&gt;&lt;code&gt;--weight&lt;/code&gt; 参数指定权重名称前缀，可选：&lt;code&gt;pretrain&lt;/code&gt;, &lt;code&gt;full_sft&lt;/code&gt;, &lt;code&gt;dpo&lt;/code&gt;, &lt;code&gt;reason&lt;/code&gt;, &lt;code&gt;ppo_actor&lt;/code&gt;, &lt;code&gt;grpo&lt;/code&gt;, &lt;code&gt;spo&lt;/code&gt; 等&lt;/p&gt; 
 &lt;p&gt;其他常用参数：&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;--load_from&lt;/code&gt;: 模型加载路径（&lt;code&gt;model&lt;/code&gt;=原生torch权重，其他路径=transformers格式）&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;--save_dir&lt;/code&gt;: 模型权重目录（默认&lt;code&gt;out&lt;/code&gt;）&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;--lora_weight&lt;/code&gt;: LoRA权重名称（&lt;code&gt;None&lt;/code&gt;表示不使用）&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;--historys&lt;/code&gt;: 携带历史对话轮数（需为偶数，0表示不携带历史）&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;--max_new_tokens&lt;/code&gt;: 最大生成长度（默认8192）&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;--temperature&lt;/code&gt;: 生成温度（默认0.85）&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;--top_p&lt;/code&gt;: nucleus采样阈值（默认0.85）&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;使用方式直接查看&lt;code&gt;eval_llm.py&lt;/code&gt;代码即可。&lt;/p&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] 所有训练脚本均为Pytorch原生框架，均支持多卡加速，假设你的设备有N (N＞1) 张显卡：&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;单机N卡启动训练方式 (DDP, 支持多机多卡集群)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;torchrun --nproc_per_node N train_xxx.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;注：其它须知&lt;/summary&gt; 
 &lt;del&gt; 单机N卡启动训练 (DeepSpeed) &lt;pre&gt;&lt;code class="language-bash"&gt;deepspeed --master_port 29500 --num_gpus=N train_xxx.py
&lt;/code&gt;&lt;/pre&gt; &lt;/del&gt; 
 &lt;p&gt;可根据需要开启wandb记录训练过程（需可直连）&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# 需要登录: wandb login
torchrun --nproc_per_node N train_xxx.py --use_wandb
# and
python train_xxx.py --use_wandb
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;通过添加&lt;code&gt;--use_wandb&lt;/code&gt;参数，可以记录训练过程，训练完成后，可以在wandb网站上查看训练过程。通过修改&lt;code&gt;wandb_project&lt;/code&gt; 和&lt;code&gt;wandb_run_name&lt;/code&gt;参数，可以指定项目名称和运行名称。&lt;/p&gt; 
 &lt;p&gt;【注】：25年6月后，国内网络环境无法直连WandB，MiniMind项目默认转为使用&lt;a href="https://swanlab.cn/"&gt;SwanLab&lt;/a&gt;作为训练可视化工具（完全兼容WandB API），即&lt;code&gt;import wandb&lt;/code&gt;改为&lt;code&gt;import swanlab as wandb&lt;/code&gt;即可，其他均无需改动。&lt;/p&gt; 
&lt;/details&gt; 
&lt;h1&gt;📌 数据介绍&lt;/h1&gt; 
&lt;h2&gt;Ⅰ Tokenizer&lt;/h2&gt; 
&lt;p&gt;分词器将单词从自然语言通过“词典”映射到&lt;code&gt;0, 1, 36&lt;/code&gt;这样的数字，可以理解为数字就代表了单词在“词典”中的页码。 可以选择自己构造词表训练一个“词典”，代码可见&lt;code&gt;./scripts/train_tokenizer.py&lt;/code&gt;（仅供学习参考，若非必要无需再自行训练，MiniMind已自带tokenizer）。 或者选择比较出名的开源大模型分词器， 正如同直接用新华/牛津词典的优点是token编码压缩率很好，缺点是页数太多，动辄数十万个词汇短语； 自己训练的分词器，优点是词表长度和内容随意控制，缺点是压缩率很低（例如"hello"也许会被拆分为"h e l l o" 五个独立的token），且生僻词难以覆盖。 “词典”的选择固然很重要，LLM的输出本质上是SoftMax到词典N个词的多分类问题，然后通过“词典”解码到自然语言。 因为MiniMind体积需要严格控制，为了避免模型头重脚轻（词嵌入embedding层参数在LLM占比太高），所以词表长度短短益善。&lt;/p&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;Tokenizer介绍&lt;/summary&gt; 
 &lt;p&gt;第三方强大的开源模型例如Yi、qwen、chatglm、mistral、Llama3的tokenizer词表长度如下：&lt;/p&gt; 
 &lt;table&gt; 
  &lt;tbody&gt;
   &lt;tr&gt;
    &lt;th&gt;Tokenizer模型&lt;/th&gt;
    &lt;th&gt;词表大小&lt;/th&gt;
    &lt;th&gt;来源&lt;/th&gt;
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td&gt;yi tokenizer&lt;/td&gt;
    &lt;td&gt;64,000&lt;/td&gt;
    &lt;td&gt;01万物（中国）&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td&gt;qwen2 tokenizer&lt;/td&gt;
    &lt;td&gt;151,643&lt;/td&gt;
    &lt;td&gt;阿里云（中国）&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td&gt;glm tokenizer&lt;/td&gt;
    &lt;td&gt;151,329&lt;/td&gt;
    &lt;td&gt;智谱AI（中国）&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td&gt;mistral tokenizer&lt;/td&gt;
    &lt;td&gt;32,000&lt;/td&gt;
    &lt;td&gt;Mistral AI（法国）&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td&gt;llama3 tokenizer&lt;/td&gt;
    &lt;td&gt;128,000&lt;/td&gt;
    &lt;td&gt;Meta（美国）&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td&gt;minimind tokenizer&lt;/td&gt;
    &lt;td&gt;6,400&lt;/td&gt;
    &lt;td&gt;自定义&lt;/td&gt;
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;👉2024-09-17更新：为了防止过去的版本歧义&amp;amp;控制体积，minimind所有模型均使用minimind_tokenizer分词，废弃所有mistral_tokenizer版本。&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;pre&gt;&lt;code&gt;# 一些自言自语
&amp;gt; 尽管minimind_tokenizer长度很小，编解码效率弱于qwen2、glm等中文友好型分词器。
&amp;gt; 但minimind模型选择了自己训练的minimind_tokenizer作为分词器，以保持整体参数轻量，避免编码层和计算层占比失衡，头重脚轻，因为minimind的词表大小只有6400。
&amp;gt; 且minimind在实际测试中没有出现过生僻词汇解码失败的情况，效果良好。
&amp;gt; 由于自定义词表压缩长度到6400，使得LLM总参数量最低只有25.8M。
&amp;gt; 训练数据`pretrain_hq.jsonl`均来自于`匠数大模型数据集`，这部分数据相对次要，如需训练可以自由选择。
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;Ⅱ Pretrain数据&lt;/h2&gt; 
&lt;p&gt;经历了MiniMind-V1的低质量预训练数据，导致模型胡言乱语的教训，&lt;code&gt;2025-02-05&lt;/code&gt; 之后决定不再采用大规模无监督的数据集做预训练。 进而尝试把&lt;a href="https://www.modelscope.cn/datasets/deepctrl/deepctrl-sft-data"&gt;匠数大模型数据集&lt;/a&gt;的中文部分提取出来， 清洗出字符&lt;code&gt;&amp;lt;512&lt;/code&gt;长度的大约1.6GB的语料直接拼接成预训练数据 &lt;code&gt;pretrain_hq.jsonl&lt;/code&gt;，hq即为high quality（当然也还不算high，提升数据质量无止尽）。&lt;/p&gt; 
&lt;p&gt;文件&lt;code&gt;pretrain_hq.jsonl&lt;/code&gt; 数据格式为&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{"text": "如何才能摆脱拖延症？ 治愈拖延症并不容易，但以下建议可能有所帮助..."}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Ⅲ SFT数据&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.modelscope.cn/datasets/deepctrl/deepctrl-sft-data"&gt;匠数大模型SFT数据集&lt;/a&gt; “是一个完整、格式统一、安全的大模型训练和研究资源。 从网络上的公开数据源收集并整理了大量开源数据集，对其进行了格式统一，数据清洗， 包含10M条数据的中文数据集和包含2M条数据的英文数据集。” 以上是官方介绍，下载文件后的数据总量大约在4B tokens，肯定是适合作为中文大语言模型的SFT数据的。 但是官方提供的数据格式很乱，全部用来sft代价太大。 我将把官方数据集进行了二次清洗，把含有符号污染和噪声的条目去除；另外依然只保留了总长度&lt;code&gt;&amp;lt;512&lt;/code&gt; 的内容，此阶段希望通过大量对话补充预训练阶段欠缺的知识。 导出文件为&lt;code&gt;sft_512.jsonl&lt;/code&gt;(~7.5GB)。&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.modelscope.cn/organization/Magpie-Align"&gt;Magpie-SFT数据集&lt;/a&gt; 收集了~1M条来自Qwen2/2.5的高质量对话，我将这部分数据进一步清洗，把总长度&lt;code&gt;&amp;lt;2048&lt;/code&gt;的部分导出为&lt;code&gt;sft_2048.jsonl&lt;/code&gt;(~9GB)。 长度&lt;code&gt;&amp;lt;1024&lt;/code&gt;的部分导出为&lt;code&gt;sft_1024.jsonl&lt;/code&gt;(~5.5GB)，用大模型对话数据直接进行sft就属于“黑盒蒸馏”的范畴。&lt;/p&gt; 
&lt;p&gt;进一步清洗前两步sft的数据（只保留中文字符占比高的内容），筛选长度&lt;code&gt;&amp;lt;512&lt;/code&gt;的对话，得到&lt;code&gt;sft_mini_512.jsonl&lt;/code&gt;(~1.2GB)。&lt;/p&gt; 
&lt;p&gt;所有sft文件 &lt;code&gt;sft_X.jsonl&lt;/code&gt; 数据格式均为&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;{
    "conversations": [
        {"role": "user", "content": "你好"},
        {"role": "assistant", "content": "你好！"},
        {"role": "user", "content": "再见"},
        {"role": "assistant", "content": "再见！"}
    ]
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Ⅳ RLHF数据&lt;/h2&gt; 
&lt;p&gt;来自&lt;a href="https://www.modelscope.cn/datasets/Magpie-Align/MagpieLM-DPO-Data-v0.1"&gt;Magpie-DPO数据集&lt;/a&gt; 大约200k条偏好数据（均是英文）生成自Llama3.1-70B/8B，可以用于训练奖励模型，优化模型回复质量，使其更加符合人类偏好。 这里将数据总长度&lt;code&gt;&amp;lt;3000&lt;/code&gt;的内容重组为&lt;code&gt;dpo.jsonl&lt;/code&gt;(~0.9GB)，包含&lt;code&gt;chosen&lt;/code&gt;和&lt;code&gt;rejected&lt;/code&gt;两个字段，&lt;code&gt;chosen&lt;/code&gt; 为偏好的回复，&lt;code&gt;rejected&lt;/code&gt;为拒绝的回复。&lt;/p&gt; 
&lt;p&gt;文件 &lt;code&gt;dpo.jsonl&lt;/code&gt; 数据格式为&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;{
  "chosen": [
    {"content": "Q", "role": "user"}, 
    {"content": "good answer", "role": "assistant"}
  ], 
  "rejected": [
    {"content": "Q", "role": "user"}, 
    {"content": "bad answer", "role": "assistant"}
  ]
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Ⅴ Reason数据集：&lt;/h2&gt; 
&lt;p&gt;不得不说2025年2月谁能火的过DeepSeek... 也激发了我对RL引导的推理模型的浓厚兴趣，目前已经用Qwen2.5复现了R1-Zero。 如果有时间+效果work（但99%基模能力不足）我会在之后更新MiniMind基于RL训练的推理模型而不是蒸馏模型。 时间有限，最快的低成本方案依然是直接蒸馏（黑盒方式）。 耐不住R1太火，短短几天就已经存在一些R1的蒸馏数据集&lt;a href="https://www.modelscope.cn/datasets/Magpie-Align/Magpie-Reasoning-V2-250K-CoT-Deepseek-R1-Llama-70B"&gt;R1-Llama-70B&lt;/a&gt;、&lt;a href="https://www.modelscope.cn/datasets/AI-ModelScope/R1-Distill-SFT"&gt;R1-Distill-SFT&lt;/a&gt;、 &lt;a href="https://huggingface.co/datasets/shareAI/Alpaca-Distill-R1-ZH"&gt;Alpaca-Distill-R1&lt;/a&gt;、 &lt;a href="https://huggingface.co/datasets/jinliuxi/deepseek_r1_zh"&gt;deepseek_r1_zh&lt;/a&gt;等等，纯中文的数据可能比较少。 最终整合它们，导出文件为&lt;code&gt;r1_mix_1024.jsonl&lt;/code&gt;，数据格式和&lt;code&gt;sft_X.jsonl&lt;/code&gt;一致。&lt;/p&gt; 
&lt;h2&gt;Ⅵ 更多数据集&lt;/h2&gt; 
&lt;p&gt;目前已经有&lt;a href="https://github.com/HqWu-HITCS/Awesome-Chinese-LLM"&gt;HqWu-HITCS/Awesome-Chinese-LLM&lt;/a&gt; 在收集和梳理中文LLM相关的开源模型、应用、数据集及教程等资料，并持续更新这方面的最新进展。全面且专业，Respect！&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Ⅷ MiniMind训练数据集&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] 2025-02-05后，开源MiniMind最终训练所用的所有数据集，因此无需再自行预处理大规模数据集，避免重复性的数据处理工作。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;MiniMind训练数据集下载地址： &lt;a href="https://www.modelscope.cn/datasets/gongjy/minimind_dataset/files"&gt;ModelScope&lt;/a&gt; | &lt;a href="https://huggingface.co/datasets/jingyaogong/minimind_dataset/tree/main"&gt;HuggingFace&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;无需全部clone，可单独下载所需的文件&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;将下载的数据集文件放到&lt;code&gt;./dataset/&lt;/code&gt;目录下（✨为推荐的必须项）&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;./dataset/
├── dpo.jsonl (55MB, ✨)
├── lora_identity.jsonl (22.8KB)
├── lora_medical.jsonl (34MB)
├── pretrain_hq.jsonl (1.6GB, ✨)
├── r1_mix_1024.jsonl (340MB)
├── rlaif-mini.jsonl (1MB)
├── sft_1024.jsonl (5.6GB)
├── sft_2048.jsonl (9GB)
├── sft_512.jsonl (7.5GB)
└── sft_mini_512.jsonl (1.2GB, ✨)
&lt;/code&gt;&lt;/pre&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;注：各数据集简介&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;dpo.jsonl&lt;/code&gt;✨ --RLHF阶段数据集（已精简优化，适合快速训练）&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;lora_identity.jsonl&lt;/code&gt; --自我认知数据集（例如：你是谁？我是minimind...），推荐用于lora训练（亦可用于全参SFT，勿被名字局限）&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;lora_medical.jsonl&lt;/code&gt; --医疗问答数据集，推荐用于lora训练（亦可用于全参SFT，勿被名字局限）&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;pretrain_hq.jsonl&lt;/code&gt;✨ --预训练数据集，整合自匠数科技&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;r1_mix_1024.jsonl&lt;/code&gt; --DeepSeek-R1-1.5B蒸馏数据，每条数据字符最大长度为1024（因此训练时设置max_seq_len=1024）&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;rlaif-mini.jsonl&lt;/code&gt; --RLAIF训练数据集，从SFT数据集中随机采样1万条高质量对话，用于PPO/GRPO/SPO等强化学习算法训练&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;sft_1024.jsonl&lt;/code&gt; --整合自Qwen2.5蒸馏数据（是sft_2048的子集），每条数据字符最大长度为1024（因此训练时设置max_seq_len=1024）&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;sft_2048.jsonl&lt;/code&gt; --整合自Qwen2.5蒸馏数据，每条数据字符最大长度为2048（因此训练时设置max_seq_len=2048）&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;sft_512.jsonl&lt;/code&gt; --整合自匠数科技SFT数据，每条数据字符最大长度为512（因此训练时设置max_seq_len=512）&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;sft_mini_512.jsonl&lt;/code&gt;✨ --极简整合自匠数科技SFT数据+Qwen2.5蒸馏数据（用于快速训练Zero模型），每条数据字符最大长度为512（因此训练时设置max_seq_len=512）&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/dataset.jpg" alt="dataset" /&gt;&lt;/p&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;说明 &amp;amp; 推荐训练方案&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;MiniMind2 Series均经过共约20GB语料训练，大约4B tokens，即对应上面的数据组合训练结果（开销：💰💰💰💰💰💰💰💰，效果：😊😊😊😊😊😊）&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;想要最快速度从0实现Zero模型，推荐使用&lt;code&gt;pretrain_hq.jsonl&lt;/code&gt; + &lt;code&gt;sft_mini_512.jsonl&lt;/code&gt; 的数据组合，具体花销和效果可查看下文表格（开销：💰，效果：😊😊）&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;推荐具备一定算力资源或更在意效果的朋友可以考虑前者完整复现MiniMind2；仅有单卡GPU或在乎短时间快速复现的朋友强烈推荐后者；&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;【折中方案】亦可选择例如&lt;code&gt;sft_mini_512.jsonl&lt;/code&gt;、&lt;code&gt;sft_1024.jsonl&lt;/code&gt;中等规模数据进行自由组合训练（开销：💰💰💰，效果：😊😊😊😊）。&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h1&gt;📌 Model&lt;/h1&gt; 
&lt;h2&gt;Structure&lt;/h2&gt; 
&lt;p&gt;MiniMind-Dense（和&lt;a href="https://ai.meta.com/blog/meta-llama-3-1/"&gt;Llama3.1&lt;/a&gt;一样）使用了Transformer的Decoder-Only结构，跟GPT-3的区别在于：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;采用了GPT-3的预标准化方法，也就是在每个Transformer子层的输入上进行归一化，而不是在输出上。具体来说，使用的是RMSNorm归一化函数。&lt;/li&gt; 
 &lt;li&gt;用SwiGLU激活函数替代了ReLU，这样做是为了提高性能。&lt;/li&gt; 
 &lt;li&gt;像GPT-Neo一样，去掉了绝对位置嵌入，改用了旋转位置嵌入（RoPE），这样在处理超出训练长度的推理时效果更好。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;p&gt;MiniMind-MoE模型，它的结构基于Llama3和&lt;a href="https://arxiv.org/pdf/2405.04434"&gt;Deepseek-V2/3&lt;/a&gt;中的MixFFN混合专家模块。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;DeepSeek-V2在前馈网络（FFN）方面，采用了更细粒度的专家分割和共享的专家隔离技术，以提高Experts的效果。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;p&gt;MiniMind的整体结构一致，只是在RoPE计算、推理函数和FFN层的代码上做了一些小调整。 其结构如下图（重绘版）：&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/LLM-structure.png" alt="structure" /&gt; &lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/LLM-structure-moe.png" alt="structure-moe" /&gt;&lt;/p&gt; 
&lt;p&gt;修改模型配置见&lt;a href="https://raw.githubusercontent.com/jingyaogong/minimind/master/model/model_minimind.py"&gt;./model/model_minimind.py&lt;/a&gt;。 参考模型参数版本见下表：&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model Name&lt;/th&gt; 
   &lt;th&gt;params&lt;/th&gt; 
   &lt;th&gt;len_vocab&lt;/th&gt; 
   &lt;th&gt;rope_theta&lt;/th&gt; 
   &lt;th&gt;n_layers&lt;/th&gt; 
   &lt;th&gt;d_model&lt;/th&gt; 
   &lt;th&gt;kv_heads&lt;/th&gt; 
   &lt;th&gt;q_heads&lt;/th&gt; 
   &lt;th&gt;share+route&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniMind2-Small&lt;/td&gt; 
   &lt;td&gt;26M&lt;/td&gt; 
   &lt;td&gt;6400&lt;/td&gt; 
   &lt;td&gt;1e6&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;512&lt;/td&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniMind2-MoE&lt;/td&gt; 
   &lt;td&gt;145M&lt;/td&gt; 
   &lt;td&gt;6400&lt;/td&gt; 
   &lt;td&gt;1e6&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;640&lt;/td&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;1+4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniMind2&lt;/td&gt; 
   &lt;td&gt;104M&lt;/td&gt; 
   &lt;td&gt;6400&lt;/td&gt; 
   &lt;td&gt;1e6&lt;/td&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;768&lt;/td&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;minimind-v1-small&lt;/td&gt; 
   &lt;td&gt;26M&lt;/td&gt; 
   &lt;td&gt;6400&lt;/td&gt; 
   &lt;td&gt;1e4&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;512&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;minimind-v1-moe&lt;/td&gt; 
   &lt;td&gt;4×26M&lt;/td&gt; 
   &lt;td&gt;6400&lt;/td&gt; 
   &lt;td&gt;1e4&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;512&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;1+4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;minimind-v1&lt;/td&gt; 
   &lt;td&gt;108M&lt;/td&gt; 
   &lt;td&gt;6400&lt;/td&gt; 
   &lt;td&gt;1e4&lt;/td&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;768&lt;/td&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Model Configuration&lt;/h2&gt; 
&lt;p&gt;📋关于LLM的参数配置，有一篇很有意思的论文&lt;a href="https://arxiv.org/pdf/2402.14905"&gt;MobileLLM&lt;/a&gt;做了详细的研究和实验。 Scaling Law在小模型中有自己独特的规律。 引起Transformer参数成规模变化的参数几乎只取决于&lt;code&gt;d_model&lt;/code&gt;和&lt;code&gt;n_layers&lt;/code&gt;。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;d_model&lt;/code&gt;↑ + &lt;code&gt;n_layers&lt;/code&gt;↓ -&amp;gt; 矮胖子&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;d_model&lt;/code&gt;↓ + &lt;code&gt;n_layers&lt;/code&gt;↑ -&amp;gt; 瘦高个&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;2020年提出Scaling Law的论文认为，训练数据量、参数量以及训练迭代次数才是决定性能的关键因素，而模型架构的影响几乎可以忽视。 然而似乎这个定律对小模型并不完全适用。 MobileLLM提出架构的深度比宽度更重要，「深而窄」的「瘦长」模型可以学习到比「宽而浅」模型更多的抽象概念。 例如当模型参数固定在125M或者350M时，30～42层的「狭长」模型明显比12层左右的「矮胖」模型有更优越的性能， 在常识推理、问答、阅读理解等8个基准测试上都有类似的趋势。 这其实是非常有趣的发现，因为以往为100M左右量级的小模型设计架构时，几乎没人尝试过叠加超过12层。 这与MiniMind在训练过程中，模型参数量在&lt;code&gt;d_model&lt;/code&gt;和&lt;code&gt;n_layers&lt;/code&gt;之间进行调整实验观察到的效果是一致的。 然而「深而窄」的「窄」也是有维度极限的，当d_model&amp;lt;512时，词嵌入维度坍塌的劣势非常明显， 增加的layers并不能弥补词嵌入在固定q_head带来d_head不足的劣势。 当d_model&amp;gt;1536时，layers的增加似乎比d_model的优先级更高，更能带来具有"性价比"的参数-&amp;gt;效果增益。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;因此MiniMind设定small模型dim=512，n_layers=8来获取的「极小体积&amp;lt;-&amp;gt;更好效果」的平衡。&lt;/li&gt; 
 &lt;li&gt;设定dim=768，n_layers=16来获取效果的更大收益，更加符合小模型Scaling-Law的变化曲线。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;作为参考，GPT3的参数设定见下表： &lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/gpt3_config.png" alt="gpt3_config.png" /&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h1&gt;📌 Experiment&lt;/h1&gt; 
&lt;h2&gt;Ⅰ 训练开销&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;时间单位&lt;/strong&gt;：小时 (h)。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;成本单位&lt;/strong&gt;：人民币 (￥)；7￥ ≈ 1美元。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;3090 租卡单价&lt;/strong&gt;：≈1.3￥/h（可自行参考实时市价）。&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;参考标准&lt;/strong&gt;：表格仅实测 &lt;code&gt;pretrain&lt;/code&gt; 和 &lt;code&gt;sft_mini_512&lt;/code&gt; 两个数据集的训练时间，其它耗时根据数据集大小估算（可能存在些许出入）。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;基于 3090 （单卡）成本计算&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model Name&lt;/th&gt; 
   &lt;th&gt;params&lt;/th&gt; 
   &lt;th&gt;pretrain&lt;/th&gt; 
   &lt;th&gt;sft_mini_512&lt;/th&gt; 
   &lt;th&gt;sft_512&lt;/th&gt; 
   &lt;th&gt;sft_1024&lt;/th&gt; 
   &lt;th&gt;sft_2048&lt;/th&gt; 
   &lt;th&gt;RLHF&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniMind2-Small&lt;/td&gt; 
   &lt;td&gt;26M&lt;/td&gt; 
   &lt;td&gt;≈1.1h&lt;br /&gt;≈1.43￥&lt;/td&gt; 
   &lt;td&gt;≈1h&lt;br /&gt;≈1.3￥&lt;/td&gt; 
   &lt;td&gt;≈6h&lt;br /&gt;≈7.8￥&lt;/td&gt; 
   &lt;td&gt;≈4.58h&lt;br /&gt;≈5.95￥&lt;/td&gt; 
   &lt;td&gt;≈7.5h&lt;br /&gt;≈9.75￥&lt;/td&gt; 
   &lt;td&gt;≈1h&lt;br /&gt;≈1.3￥&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniMind2&lt;/td&gt; 
   &lt;td&gt;104M&lt;/td&gt; 
   &lt;td&gt;≈3.9h&lt;br /&gt;≈5.07￥&lt;/td&gt; 
   &lt;td&gt;≈3.3h&lt;br /&gt;≈4.29￥&lt;/td&gt; 
   &lt;td&gt;≈20h&lt;br /&gt;≈26￥&lt;/td&gt; 
   &lt;td&gt;≈15h&lt;br /&gt;≈19.5￥&lt;/td&gt; 
   &lt;td&gt;≈25h&lt;br /&gt;≈32.5￥&lt;/td&gt; 
   &lt;td&gt;≈3h&lt;br /&gt;≈3.9￥&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;训练开销总结&amp;amp;预测&lt;/summary&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;MiniMind2-Small参数&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;&lt;code&gt;pretrain_hq&lt;/code&gt;+&lt;code&gt;sft_mini_512&lt;/code&gt;数据集 &lt;br /&gt;单卡3090 (1 epoch) + 2.1小时 + 花费2.73元人民币 &lt;br /&gt;即可从0训练出MiniMind-Zero-0.025B模型!!!&lt;/p&gt; 
  &lt;/blockquote&gt; 
 &lt;/blockquote&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;MiniMind2-Small参数&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;&lt;code&gt;pretrain_hq&lt;/code&gt;+&lt;code&gt;sft_512&lt;/code&gt;+&lt;code&gt;sft_2048&lt;/code&gt;+&lt;code&gt;dpo&lt;/code&gt;数据集 &lt;br /&gt;单卡3090 (2 epochs) + 大约38.16小时 + 花费49.61元人民币 &lt;br /&gt;即可从0训练出MiniMind2-Small-0.025B模型!!!&lt;/p&gt; 
  &lt;/blockquote&gt; 
 &lt;/blockquote&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;MiniMind2参数&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;&lt;code&gt;pretrain_hq&lt;/code&gt;+&lt;code&gt;sft_512&lt;/code&gt;+&lt;code&gt;sft_2048&lt;/code&gt;+&lt;code&gt;dpo&lt;/code&gt;数据集 &lt;br /&gt;单卡3090 (2 epochs) + 大约122小时 + 花费158.6元人民币 &lt;br /&gt;即可从0训练出MiniMind2-0.1B模型!!!&lt;/p&gt; 
  &lt;/blockquote&gt; 
 &lt;/blockquote&gt; 
&lt;/details&gt; 
&lt;p&gt;✨基于单卡NVIDIA 3090的&lt;code&gt;MiniMind-Zero&lt;/code&gt;从0训练仅需&lt;code&gt;2小时&lt;/code&gt; + &lt;code&gt;3块钱&lt;/code&gt;，实现ChatBot效果！&lt;/p&gt; 
&lt;p&gt;✨PS：若采用8卡4090训练，总用时甚至可以压缩到10分钟以内！（由于时间更短，花费同样3元左右，与单卡成本相当）&lt;/p&gt; 
&lt;p&gt;✨以极低极低的门槛，实现人人可玩的大模型自由！这正是MiniMind系列的诞生初衷！&lt;/p&gt; 
&lt;p&gt;✨仅价值&lt;code&gt;3块钱&lt;/code&gt;成本的&lt;code&gt;MiniMind-Zero&lt;/code&gt;并不是噱头！Chat测试：&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;👶: 请介绍一下自己。
🤖️: 作为人工智能，我没有实际的生活，也没有自我意识，所以没有自己的生活。我被设计成能够帮助用户解答问题、提供信息、进行对话等。我的设计和功能是由计算机科学、机器学习、人工智能技术和算法所构建的，我通过编程和算法实现。

👶: 推荐一些杭州的特色美食吧。
🤖️: 杭州有很多美味的特色美食，比如鸡肉串、鳗鱼头、水煮鱼、豆腐脑等。这些美食都非常美味，有着独特的风味和口感，绝对是值得一试的美食。

👶: 请为我讲解“大语言模型”这个概念。
🤖️: 大语言模型是一种人工智能技术，它使用大量的文本数据来训练模型，然后在不断的迭代中不断优化模型。它的目标是根据输入的数据来生成符合特定需求的文本，这些文本可以是任何你感兴趣的主题。

👶: Introduce the history of the United States, please.
🤖️: 您提到的“Introok's the believeations of theument." 这个名字来源于中国古代的"groty of of the change."
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;极速且初具效果，甚至仍然可以进一步压缩获取更小更优质的训练数据。 Zero模型权重保存为 &lt;code&gt;full_sft_512_zero.pth&lt;/code&gt;（见下文MiniMind模型文件链接），如有兴趣可下载检验此模型效果。&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Ⅱ 主要训练（必须）&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;所有训练脚本均 &lt;code&gt;cd ./trainer&lt;/code&gt; 目录执行&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;&lt;strong&gt;1. 预训练 (Pretrain)&lt;/strong&gt;:&lt;/h3&gt; 
&lt;p&gt;LLM首先要学习的并非直接与人交流，而是让网络参数中充满知识的墨水，“墨水” 理论上喝的越饱越好，产生大量的对世界的知识积累。 预训练就是让Model先埋头苦学大量基本的知识，例如从Wiki百科、新闻、书籍整理大规模的高质量训练数据。 这个过程是“无监督”的，即人类不需要在过程中做任何“有监督”的校正，而是由模型自己从大量文本中总结规律学习知识点。 模型此阶段目的只有一个：&lt;strong&gt;学会词语接龙&lt;/strong&gt;。例如输入"秦始皇"四个字，它可以接龙"是中国的第一位皇帝"。&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;torchrun --nproc_per_node 1 train_pretrain.py # 1即为单卡训练，可根据硬件情况自行调整 (设置&amp;gt;=2)
# or
python train_pretrain.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;训练后的模型权重文件默认每隔&lt;code&gt;100步&lt;/code&gt;保存为: &lt;code&gt;pretrain_*.pth&lt;/code&gt;（* 为模型具体dimension，每次保存时新文件会覆盖旧文件）&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;MiniMind2-Small (512dim)&lt;/th&gt; 
   &lt;th&gt;MiniMind2 (768dim)&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/pre_512_loss.png" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/pre_768_loss.png" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;&lt;strong&gt;2. 有监督微调 (Supervised Fine-Tuning)&lt;/strong&gt;:&lt;/h3&gt; 
&lt;p&gt;经过预训练，LLM此时已经掌握了大量知识，然而此时它只会无脑地词语接龙，还不会与人聊天。 SFT阶段就需要把半成品LLM施加一个自定义的聊天模板进行微调。 例如模型遇到这样的模板【问题-&amp;gt;回答，问题-&amp;gt;回答】后不再无脑接龙，而是意识到这是一段完整的对话结束。 称这个过程为指令微调，就如同让已经学富五车的「牛顿」先生适应21世纪智能手机的聊天习惯，学习屏幕左侧是对方消息，右侧是本人消息这个规律。 在训练时，MiniMind的指令和回答长度被截断在512，是为了节省显存空间。就像学习写作时，会先从短的文章开始，当学会写作200字作文后，800字文章也可以手到擒来。 在需要长度拓展时，只需要准备少量的2k/4k/8k长度对话数据进行进一步微调即可（此时最好配合RoPE-NTK的基准差值）。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;在推理时通过调整RoPE线性差值，实现免训练长度外推到2048及以上将会很方便。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;torchrun --nproc_per_node 1 train_full_sft.py
# or
python train_full_sft.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;训练后的模型权重文件默认每隔&lt;code&gt;100步&lt;/code&gt;保存为: &lt;code&gt;full_sft_*.pth&lt;/code&gt;（* 为模型具体dimension，每次保存时新文件会覆盖旧文件）&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;MiniMind2-Small (512dim)&lt;/th&gt; 
   &lt;th&gt;MiniMind2 (768dim)&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/sft_512_loss.png" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/sft_768_loss.png" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Ⅲ 其它训练阶段（可选）&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;所有训练脚本均 &lt;code&gt;cd ./trainer&lt;/code&gt; 目录执行&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;&lt;strong&gt;3. 知识蒸馏 (Knowledge Distillation, KD)&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;在前面的所有训练步骤中，模型已经完全具备了基本能力，通常可以学成出师了。 而知识蒸馏可以进一步优化模型的性能和效率，所谓知识蒸馏，即学生模型面向教师模型学习。 教师模型通常是经过充分训练的大模型，具有较高的准确性和泛化能力。 学生模型是一个较小的模型，目标是学习教师模型的行为，而不是直接从原始数据中学习。 在SFT学习中，模型的目标是拟合词Token分类硬标签（hard labels），即真实的类别标签（如 0 或 6400）。 在知识蒸馏中，教师模型的softmax概率分布被用作软标签（soft labels）。小模型仅学习软标签，并使用KL-Loss来优化模型的参数。 通俗地说，SFT直接学习老师给的解题答案。而KD过程相当于“打开”老师聪明的大脑，尽可能地模仿老师“大脑”思考问题的神经元状态。 例如，当老师模型计算&lt;code&gt;1+1=2&lt;/code&gt;这个问题的时候，最后一层神经元a状态为0，神经元b状态为100，神经元c状态为-99... 学生模型通过大量数据，学习教师模型大脑内部的运转规律。这个过程即称之为：知识蒸馏。 知识蒸馏的目的只有一个：让小模型体积更小的同时效果更好。 然而随着LLM诞生和发展，模型蒸馏一词被广泛滥用，从而产生了“白盒/黑盒”知识蒸馏两个派别。 GPT-4这种闭源模型，由于无法获取其内部结构，因此只能面向它所输出的数据学习，这个过程称之为黑盒蒸馏，也是大模型时代最普遍的做法。 黑盒蒸馏与SFT过程完全一致，只不过数据是从大模型的输出收集，因此只需要准备数据并且进一步FT即可。 注意更改被加载的基础模型为&lt;code&gt;full_sft_*.pth&lt;/code&gt;，即基于微调模型做进一步的蒸馏学习。 &lt;code&gt;./dataset/sft_1024.jsonl&lt;/code&gt;与&lt;code&gt;./dataset/sft_2048.jsonl&lt;/code&gt; 均收集自qwen2.5-7/72B-Instruct大模型，可直接用于SFT以获取Qwen的部分行为。&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 注意需要更改train_full_sft.py数据集路径，以及max_seq_len  
torchrun --nproc_per_node 1 train_full_sft.py
# or
python train_full_sft.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;训练后的模型权重文件默认每隔&lt;code&gt;100步&lt;/code&gt;同样保存为: &lt;code&gt;full_sft_*.pth&lt;/code&gt;（*为模型具体dimension，每次保存时新文件会覆盖旧文件）&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;此处应当着重介绍MiniMind实现的白盒蒸馏代码&lt;code&gt;train_distillation.py&lt;/code&gt;，由于MiniMind同系列本身并不存在强大的教师模型，因此白盒蒸馏代码仅作为学习参考。&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;torchrun --nproc_per_node 1 train_distillation.py
# or
python train_distillation.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;&lt;strong&gt;4. LoRA (Low-Rank Adaptation)&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;LoRA是一种高效的参数高效微调（Parameter-Efficient Fine-Tuning, PEFT）方法，旨在通过低秩分解的方式对预训练模型进行微调。 相比于全参数微调（Full Fine-Tuning），LoRA 只需要更新少量的参数。 LoRA 的核心思想是：在模型的权重矩阵中引入低秩分解，仅对低秩部分进行更新，而保持原始预训练权重不变。 代码可见&lt;code&gt;./model/model_lora.py&lt;/code&gt;和&lt;code&gt;train_lora.py&lt;/code&gt;，完全从0实现LoRA流程，不依赖第三方库的封装。&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;torchrun --nproc_per_node 1 train_lora.py
# or
python train_lora.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;训练后的模型权重文件默认每隔&lt;code&gt;100步&lt;/code&gt;保存为: &lt;code&gt;lora_xxx_*.pth&lt;/code&gt;（* 为模型具体dimension，每次保存时新文件会覆盖旧文件）&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;非常多的人困惑，如何使模型学会自己私有领域的知识？如何准备数据集？如何迁移通用领域模型打造垂域模型？ 这里举几个例子，对于通用模型，医学领域知识欠缺，可以尝试在原有模型基础上加入领域知识，以获得更好的性能。 同时，通常不希望学会领域知识的同时损失原有基础模型的其它能力，此时LoRA可以很好的改善这个问题。 只需要准备如下格式的对话数据集放置到&lt;code&gt;./dataset/lora_xxx.jsonl&lt;/code&gt;，启动 &lt;code&gt;python train_lora.py&lt;/code&gt; 训练即可得到&lt;code&gt;./out/lora/lora_xxx.pth&lt;/code&gt;新模型权重。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;医疗场景&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt; {"conversations": [{"role": "user", "content": "请问颈椎病的人枕头多高才最好？"}, {"role": "assistant", "content": "颈椎病患者选择枕头的高度应该根据..."}]}
 {"conversations": [{"role": "user", "content": "请问xxx"}, {"role": "assistant", "content": "xxx..."}]}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;自我认知场景&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt; {"conversations": [{"role": "user", "content": "你叫什么名字？"}, {"role": "assistant", "content": "我叫minimind..."}]}
 {"conversations": [{"role": "user", "content": "你是谁"}, {"role": "assistant", "content": "我是..."}]}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;此时【基础模型+LoRA模型】即可获得医疗场景模型增强的能力，相当于为基础模型增加了LoRA外挂，这个过程并不损失基础模型的本身能力。 可以通过&lt;code&gt;eval_llm.py&lt;/code&gt;进行模型评估测试。&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 注意：weight参数指定基础模型类型，需与train_lora训练时使用的基础模型保持一致
python eval_llm.py  --weight dpo --lora_weight lora_medical
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;小测试&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;👶: 我最近经常感到头晕，可能是什么原因？
🤖️: 头晕可能是由于多种原因，包括但不限于：心脏疾病、眼睛损伤、过敏反应、皮肤问题、过敏反应、压力、焦虑、疲劳、药物副作用、性功能障碍或者其他...

---

👶: 你和openai是什么关系？
🤖️: 我是 MiniMind，一个由 Jingyao Gong 开发的人工智能助手。我通过自然语言处理和算法训练来与用户进行交互。
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;PS：只要有所需要的数据集，也可以full_sft全参微调（需要进行通用知识的混合配比，否则过拟合领域数据会让模型变傻，损失通用性）&lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;5. 训练推理模型 (Reasoning Model)&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;DeepSeek-R1实在太火了，几乎重新指明了未来LLM的新范式。 论文指出&lt;code&gt;&amp;gt;3B&lt;/code&gt;的模型经历多次反复的冷启动和RL奖励训练才能获得肉眼可见的推理能力提升。 最快最稳妥最经济的做法，以及最近爆发的各种各样所谓的推理模型几乎都是直接面向数据进行蒸馏训练， 但由于缺乏技术含量，蒸馏派被RL派瞧不起（hhhh）。 本人迅速已经在Qwen系列1.5B小模型上进行了尝试，很快复现了Zero过程的数学推理能力。 然而一个遗憾的共识是：参数太小的模型直接通过冷启动SFT+GRPO几乎不可能获得任何推理效果。 &lt;del&gt; MiniMind2第一时间只能坚定不移的选择做蒸馏派，日后基于0.1B模型的RL如果同样取得小小进展会更新此部分的训练方案。 &lt;/del&gt;&lt;/p&gt; 
&lt;p&gt;做蒸馏需要准备的依然是和SFT阶段同样格式的数据即可，数据集来源已如上文介绍。数据格式例如：&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "conversations": [
    {
      "role": "user",
      "content": "你好，我是小芳，很高兴认识你。"
    },
    {
      "role": "assistant",
      "content": "&amp;lt;think&amp;gt;\n你好！我是由中国的个人开发者独立开发的智能助手MiniMind-R1-Lite-Preview，很高兴为您提供服务！\n&amp;lt;/think&amp;gt;\n&amp;lt;answer&amp;gt;\n你好！我是由中国的个人开发者独立开发的智能助手MiniMind-R1-Lite-Preview，很高兴为您提供服务！\n&amp;lt;/answer&amp;gt;"
    }
  ]
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;推理模型R1的回复模板是：&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;&amp;lt;think&amp;gt;\n思考过程\n&amp;lt;/think&amp;gt;\n
&amp;lt;answer&amp;gt;\n最终回答\n&amp;lt;/answer&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;这在GRPO中通过设置规则奖励函数约束模型符合思考标签和回复标签（在冷启动靠前的阶段奖励值设置应该提高一些）&lt;/p&gt; 
&lt;p&gt;另一个问题是蒸馏过程虽然和SFT一样，但实验结果是模型难以每次都符合模板规范的回复，即脱离思考和回复标签约束。 这里的小技巧是增加标记位置token的损失惩罚，详见&lt;code&gt;train_distill_reason.py&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;# 在 sp_ids 对应的位置增加额外的惩罚
...
loss_mask[sp_ids] = 10 # 惩罚系数
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;另另一个tips是由于推理数据由于只筛选了&lt;code&gt;&amp;lt;1024&lt;/code&gt;长度的数据，其中多轮对话和英文数据偏少， 因此&lt;code&gt;r1_mix_1024.jsonl&lt;/code&gt;进行了大约10k条多轮对话+英文数据的混合，防止模型遗忘严重。&lt;/p&gt; 
&lt;p&gt;脚本默认基于rlhf后的基模型做推理能力的蒸馏微调，下面直接启动训练即可：&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;torchrun --nproc_per_node 1 train_distill_reason.py
# or
python train_distill_reason.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;训练后的模型权重文件默认每隔&lt;code&gt;100步&lt;/code&gt;保存为: &lt;code&gt;reason_*.pth&lt;/code&gt;（*为模型具体dimension，每次保存时新文件会覆盖旧文件）&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;测试一下：&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;👶: 请介绍一下自己。
🤖️: &amp;lt;think&amp;gt;
嗯，用户让我介绍一下自己。我现在需要解释一下自己。首先，我应该明确用户是谁，是学生、职场人士还是普通人。用户可能是一个学生或者职场人士，所以信息传递应该是帮助学生和职场人士的信息。
然后，我需要回应用户，这可能包括介绍自己的位置、兴趣爱好、职业经历等信息。用户还可能想了解自己在哪里，比如职业发展、工作经验等。
我应该注意用词要正式，不要太正式，同时要保持简洁明了。这样用户容易理解，也方便他们查阅相关资料。
最后，我应该总结一下，让用户知道我是如何帮助他们完成他们的任务的，这样他们有更多关于我的信息。
&amp;lt;/think&amp;gt;
&amp;lt;answer&amp;gt;
我是**“你的名字”，你叫什么名字？
&amp;lt;/answer&amp;gt;

👶: 你更擅长哪一个学科？
🤖️: &amp;lt;think&amp;gt;
您好！我是由中国的个人开发者开发的智能助手MiniMind-R1。如您有任何任何问题，我会尽我所能为您提供帮助。
&amp;lt;/think&amp;gt;
&amp;lt;answer&amp;gt;
您好！我是由中国的个人开发者开发的智能助手MiniMind-R1。如您有任何任何问题，我会尽我所能为您提供帮助。
&amp;lt;/answer&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;IV 强化学习后训练&lt;/h2&gt; 
&lt;p&gt;LLM里的强化学习方法可分两类：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;基于人类反馈的强化学习 (Reinforcement Learning from Human Feedback, RLHF)&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;ul&gt; 
 &lt;li&gt;通过&lt;strong&gt;人类&lt;/strong&gt;对模型输出的偏好进行评价来训练模型，使其生成更符合人类价值观和偏好的内容。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;&lt;strong&gt;基于AI反馈的强化学习 (Reinforcement Learning from AI Feedback, RLAIF)&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;ul&gt; 
 &lt;li&gt;使用&lt;strong&gt;AI模型&lt;/strong&gt;（通常是预训练的语言奖励模型）来提供反馈，而不直接依赖人类的人工标注。&lt;/li&gt; 
 &lt;li&gt;这里的“AI”也可以是某些规则奖励，例如数学答案/代码解释器...&lt;/li&gt; 
&lt;/ul&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;类型&lt;/th&gt; 
   &lt;th&gt;裁判&lt;/th&gt; 
   &lt;th&gt;优点&lt;/th&gt; 
   &lt;th&gt;缺点&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;RLHF&lt;/td&gt; 
   &lt;td&gt;人类&lt;/td&gt; 
   &lt;td&gt;更贴近真实人类偏好&lt;/td&gt; 
   &lt;td&gt;成本高、效率低&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;RLAIF&lt;/td&gt; 
   &lt;td&gt;模型&lt;/td&gt; 
   &lt;td&gt;自动化、可扩展性强&lt;/td&gt; 
   &lt;td&gt;可能偏离人类真实偏好&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;二者本质上是一样的，都是通过&lt;strong&gt;强化学习的方式&lt;/strong&gt;，利用某种形式的"&lt;strong&gt;反馈&lt;/strong&gt;"来优化模型的行为。&lt;/p&gt; 
&lt;p&gt;除了&lt;strong&gt;反馈&lt;/strong&gt;的来源不同，其他并无任何区别。&lt;/p&gt; 
&lt;h3&gt;👀 PO算法的统一视角&lt;/h3&gt; 
&lt;p&gt;在介绍实现具体算法之前，我先以个人理解的极简视角，阐述所有Policy Optimization (PO)算法的统一共性。&lt;/p&gt; 
&lt;p&gt;所有RL算法的本质都只是在优化一个期望：&lt;/p&gt; 
&lt;p&gt;$$\mathcal{J}&lt;em&gt;{PO} = \mathbb{E}&lt;/em&gt;{q \sim P(Q), o \sim \pi(O|q)} \left[ \underbrace{f(r_t)}&lt;em&gt;{\text{策略项}} \cdot \underbrace{g(A_t)}&lt;/em&gt;{\text{优势项}} - \underbrace{h(\text{KL}&lt;em&gt;t)}&lt;/em&gt;{\text{正则项}} \right]$$&lt;/p&gt; 
&lt;p&gt;训练时，只需&lt;strong&gt;最小化负目标函数&lt;/strong&gt;，即: $\mathcal{L_{PO}}=-\mathcal{J_{PO}}$&lt;/p&gt; 
&lt;p&gt;这个框架只包含三个核心组件：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;策略项&lt;/strong&gt; $f(r_t)$: 如何使用概率比 $r_t$? 即告诉模型新旧策略偏差有多大，是否探索到了更好的token&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;优势项&lt;/strong&gt; $g(A_t)$: 如何计算优势 $A_t$, 这很重要！大模型算对定积分也不足为奇，小模型回答对加减法优势通常都是正的&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;正则项&lt;/strong&gt; $h(\text{KL}_t)$: 如何约束变化幅度 $\text{KL}_t$, 既防止跑偏又防止管的太死&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;（展开）符号说明&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;符号&lt;/th&gt; 
    &lt;th&gt;含义&lt;/th&gt; 
    &lt;th&gt;说明&lt;/th&gt; 
    &lt;th&gt;值域&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;$q$&lt;/td&gt; 
    &lt;td&gt;问题/提示词&lt;/td&gt; 
    &lt;td&gt;从数据集 $P(Q)$ 中采样&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;$o$&lt;/td&gt; 
    &lt;td&gt;模型输出序列&lt;/td&gt; 
    &lt;td&gt;由策略 $\pi$ 生成&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;$r_t$&lt;/td&gt; 
    &lt;td&gt;概率比&lt;/td&gt; 
    &lt;td&gt;$r_t = \frac{\pi_\theta(o_t|q, o_{&amp;lt;t})}{\pi_{ref}(o_t|q, o_{&amp;lt;t})}$&lt;/td&gt; 
    &lt;td&gt;$(0, +\infty)$&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;$A_t$&lt;/td&gt; 
    &lt;td&gt;优势函数&lt;/td&gt; 
    &lt;td&gt;衡量某个动作相比基线有多好&lt;/td&gt; 
    &lt;td&gt;$(-\infty, +\infty)$&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;$\text{KL}_t$&lt;/td&gt; 
    &lt;td&gt;KL散度&lt;/td&gt; 
    &lt;td&gt;防止策略偏离参考模型太远&lt;/td&gt; 
    &lt;td&gt;$[0, +\infty)$&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;p&gt;不同的&lt;strong&gt;xxPO算法&lt;/strong&gt;本质上只是对这三个组件的不同设计的实例化！&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;&lt;strong&gt;6. 基于人类反馈的强化学习 (Reinforcement Learning from Human Feedback, RLHF)&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;在前面的训练步骤中，模型已经具备了基本的对话能力，但是这样的能力完全基于单词接龙，缺少正反样例的激励。 模型此时尚未知什么回答是好的，什么是差的。希望它能够更符合人的偏好，降低让人类不满意答案的产生概率。 这个过程就像是让模型参加新的培训，从优秀员工的作为例子，消极员工作为反例，学习如何更好地回复。&lt;/p&gt; 
&lt;h4&gt;6.1 Direct Preference Optimization&lt;/h4&gt; 
&lt;p&gt;直接偏好优化（DPO）算法，损失为：&lt;/p&gt; 
&lt;p&gt;$$\mathcal{L}&lt;em&gt;{DPO} = -\mathbb{E}\left[\log \sigma\left(\beta \left[\log \frac{\pi&lt;/em&gt;\theta(y_w|x)}{\pi_{ref}(y_w|x)} - \log \frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)}\right]\right)\right]$$&lt;/p&gt; 
&lt;p&gt;其中：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;策略项&lt;/strong&gt;: $f(r_t) = \log r_w - \log r_l$ (对比chosen vs rejected的概率比)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;优势项&lt;/strong&gt;: $g(A_t)$ = / (通过偏好对比，无需显式计算优势)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;正则项&lt;/strong&gt;: $h(\text{KL}_t)$ = 隐含在 $\beta$ 中 (控制偏离参考模型程度)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;特别地，&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;DPO从PPO带KL约束的目标推导出对偏好对的解析训练目标，直接最大化"chosen优于rejected"的对数几率；无需同步训练Reward/Value模型。DPO只需跑&lt;code&gt;actor&lt;/code&gt;与&lt;code&gt;ref&lt;/code&gt;两个模型，显存占用低、收敛稳定、实现简单。&lt;/li&gt; 
 &lt;li&gt;训练范式：off‑policy，使用静态偏好数据集，可反复多轮epoch；Ref模型固定（预先缓存输出）。&lt;/li&gt; 
 &lt;li&gt;DPO的局限在于不做在线探索，更多用于"偏好/安全"的人类价值对齐；对"能不能做对题"的智力能力提升有限（当然这也取决于数据集，大规模收集正反样本并人类评估很困难）。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;torchrun --nproc_per_node 1 train_dpo.py
# or
python train_dpo.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;训练后的模型权重文件默认每隔&lt;code&gt;100步&lt;/code&gt;保存为: &lt;code&gt;dpo_*.pth&lt;/code&gt;（*为模型具体dimension，每次保存时新文件会覆盖旧文件）&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;&lt;strong&gt;7. 基于AI反馈的强化学习 (Reinforcement Learning from AI Feedback, RLAIF)&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;相比RLHF依赖人类标注chosen/rejected偏好对，RLAIF则完全由AI来充当"裁判"。 所谓AI"裁判"可以是model-base的奖励大模型(Reward Model)，也可以是R1一样设置规则函数进行校验，也可以是例如工具调用的环境反馈。 例如：数学题答案是否正确、工具调用执行代码能否通过测试用例、推理过程是否符合格式...都可以自动化判断。 RLAIF的最大优势在于&lt;strong&gt;可扩展性&lt;/strong&gt;和&lt;strong&gt;On-Policy&lt;/strong&gt;的特点——不需要昂贵的人工标注，可以生成海量的训练样本，让模型在在线大量试错中快速进化。&lt;/p&gt; 
&lt;p&gt;MiniMind 着手实现&lt;strong&gt;2+N&lt;/strong&gt;种基本+前沿的RLAIF方法：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;PPO&lt;/strong&gt;、&lt;strong&gt;GRPO&lt;/strong&gt; 被大规模验证的经典RL算法；&lt;/li&gt; 
 &lt;li&gt;N种前沿RL算法（不定期以Exp性质更新）。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;1️⃣ 数据集准备 (需要)&lt;/h4&gt; 
&lt;p&gt;为了快速验证RLAIF的效果，这里从SFT数据集中随机采样了1万条高质量对话，构建约1MB大小的&lt;code&gt;rlaif-mini.jsonl&lt;/code&gt;(&lt;a href="https://huggingface.co/datasets/jingyaogong/minimind_dataset/blob/main/rlaif-mini.jsonl"&gt;Huggingface&lt;/a&gt;)&lt;/p&gt; 
&lt;p&gt;数据格式与SFT一致，但assistant并不需要内容，因为训练过程中完全由 $\Pi$ 策略模型实时采样生成。因此形如：&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
    "conversations": [
        {"role": "user", "content": "请解释一下什么是光合作用？"},
        {"role": "assistant", "content": "无"}
    ]
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;RLAIF的训练过程中，模型会基于user的问题生成1或多个候选回答，然后由奖励函数/模型对回答打分， 分数高的回答会被鼓励（增加 $\Pi$ 策略概率），分数低的回答会被抑制（降低 $\Pi$ 策略概率）。这个"打分-&amp;gt;调整"的循环就是强化学习的核心。&lt;/p&gt; 
&lt;h4&gt;2️⃣ 奖励模型准备 (需要)&lt;/h4&gt; 
&lt;p&gt;已知RLAIF训练需要“奖励模型 (Reward Model)”对生成的回答进行打分。&lt;/p&gt; 
&lt;p&gt;此处选取小型且高质量的InternLM2-1.8B-Reward (&lt;a href="https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-1_8b-reward"&gt;ModelScope&lt;/a&gt; | &lt;a href="https://huggingface.co/internlm/internlm2-1_8b-reward"&gt;HuggingFace&lt;/a&gt;) 作为基础奖励模型。&lt;/p&gt; 
&lt;p&gt;下载奖励模型后需要放置在minimind项目的&lt;strong&gt;同级目录&lt;/strong&gt;下，推荐结构如下：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;project/
├── minimind/                    # MiniMind项目
│   ├── model/
│   └── ...
└── internlm2-1_8b-reward/       # 奖励模型（与minimind同级）
    ├── config.json
    ├── model.safetensors
    └── ...
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;奖励机制选择与MiniMind限制说明（点击展开）&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;1. 奖励机制的多样性&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;RLAIF中的"奖励信号"来源可以非常灵活：&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Model-based奖励&lt;/strong&gt;：可使用专门的Reward Model（如InternLM2-Reward），也可使用通用LLM+提示词进行打分（如Qwen3-as-a-Judge）。奖励模型规模和架构均可自由选择。&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Rule-based奖励&lt;/strong&gt;：可以基于规则函数构造奖励信号，例如：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;数学题答案正确性验证（Ground Truth对比）&lt;/li&gt; 
    &lt;li&gt;SQL执行成功率与结果准确性&lt;/li&gt; 
    &lt;li&gt;代码解释器运行结果（pass@k）&lt;/li&gt; 
    &lt;li&gt;工具调用返回状态（API成功/失败）&lt;/li&gt; 
    &lt;li&gt;格式合规性检查（JSON/XML解析）&lt;/li&gt; 
    &lt;li&gt;推理链完整性评估（CoT步骤数）&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Environment-based奖励&lt;/strong&gt;：在Agent场景中，环境反馈本身即为天然奖励（如游戏得分、Research完整度、任务完成度）。&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;任何能够量化"回答质量"的机制都可作为RL的奖励来源。DeepSeek R1就是典型案例：使用规则函数验证数学答案正确性作为奖励，无需额外的Reward Model。&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;2. MiniMind限制：奖励稀疏问题&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;RLAIF训练既可以针对推理模型也可以针对非推理模型，区别仅在于格式。&lt;/p&gt; 
 &lt;p&gt;然而对于MiniMind这种0.1B参数量极小能力弱的模型，在通用任务（如R1风格的数学数据集）上会遇到严重的奖励稀疏(Reward Sparsity)问题：&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;现象&lt;/strong&gt;：模型生成的候选回答几乎全部错误，导致所有奖励分数 $r(x,y) \approx 0$&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;后果&lt;/strong&gt;：优势函数 $A(x,y) = r(x,y) - b(x) \approx 0$，策略梯度信号消失，无法有效更新参数 $\theta$&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;如同让小学生做高考数学题，无论尝试多少次都得零分，无法通过分数差异学习改进策略。因此这是RL算法的根本原理限制的。&lt;/p&gt; 
 &lt;p&gt;为缓解此问题，MiniMind的实现选择了&lt;strong&gt;model-based的连续性奖励信号&lt;/strong&gt;：&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Reward Model输出连续分数（如-2.5到+3.0），而非二元的0/1&lt;/li&gt; 
  &lt;li&gt;即使回答质量都差，也仍能区分"更更差"(-3.0)和"更差"(-2.8)的细微差异。所以这种&lt;strong&gt;稠密且连续&lt;/strong&gt;的奖励信号能够为优势函数 $A(x,y)$ 提供非零梯度，使得策略网络得以渐进式优化&lt;/li&gt; 
  &lt;li&gt;也可以混合多种奖励源: $r_{\text{total}} = \alpha \cdot r_{\text{model}} + \beta \cdot r_{\text{rule}}$ (例如既可以检测think标签格式reward，又可以综合回答本身质量的reward分数)&lt;/li&gt; 
  &lt;li&gt;minimind实践中避免直接使用rule-based二元奖励 + 超纲难度数据（如MATH500），易导致奖励全零；&lt;/li&gt; 
  &lt;li&gt;监控训练时观察奖励分数的方差 $\text{Var}(r)$，若持续接近0则需调整数据或奖励机制&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;对于生产级大模型的Agentic RL场景&lt;/strong&gt;：&lt;/p&gt; 
 &lt;p&gt;在真实Agent系统（代码生成、工具调用、检索-规划-执行的多轮链路）中，奖励是“延迟整轮结算”的不同范式：&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;LLM需要逐token生成工具调用指令（tool_call），经历解析（tool_parse）、工具执行（tool_exec），再把结果拼接回上下文继续下一步；循环往复直到完成。&lt;/li&gt; 
  &lt;li&gt;一次完整的任务链路包含多次调用+思考，直到终止条件满足时计算一次总reward（如任务是否完成、测试是否通过、目标是否命中）。&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;因此，Agentic RL更接近稀疏/延迟奖励设定：梯度回传在“整轮结束后”才发生，和非Agentic RL任务在对话单轮上“即时评分即时更新”有很大不同。 这也解释了Agent任务上更偏向环境反馈（environment-based reward），而非凭Reward Model进行静态打分。&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;环境交互反馈&lt;/strong&gt;：最终以执行结果为准（代码是否跑通、API是否返回成功、子目标是否完成）；&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Model-based奖励局限&lt;/strong&gt;：对长链路、可执行语义的全貌捕捉有限，且大概率和真实环境反馈不一致（reward hacking）。&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h4&gt;7.1 &lt;a href="https://arxiv.org/abs/1707.06347"&gt;Proximal Policy Optimization&lt;/a&gt;&lt;/h4&gt; 
&lt;p&gt;PPO 是2017年OpenAI提出的非常经典强化学习算法，也是LLM RL通用的基线方法，甚至不需要加之一。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;PPO损失&lt;/strong&gt;： $$\mathcal{L}_{PPO} = -\mathbb{E}\left[\min(r_t \cdot A_t, \text{clip}(r_t, 1-\varepsilon, 1+\varepsilon) \cdot A_t)\right] + \beta \cdot \mathbb{E}[\text{KL}]$$&lt;/p&gt; 
&lt;p&gt;其中：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;策略项&lt;/strong&gt;: $f(r_t) = \min(r_t, \text{clip}(r_t, 1-\varepsilon, 1+\varepsilon))$ (裁剪概率比防止更新过激)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;优势项&lt;/strong&gt;: $g(A_t) = R - V(s)$ (通过Critic网络估计价值函数)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;正则项&lt;/strong&gt;: $h(\text{KL}_t) = \beta \cdot \mathbb{E}[\text{KL}]$ (全局KL散度约束)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;对比DPO而言，&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;DPO (Off-Policy)：训练数据是静态的偏好数据集（chosen vs rejected），可以反复使用同一批数据训练多个epoch，就像传统监督学习一样。数据效率高，训练成本低。它直接优化偏好对的对数似然，无需Reward Model。&lt;/li&gt; 
 &lt;li&gt;PPO (On-Policy)：必须用当前策略实时采样生成新数据，旧策略采集的数据不能用（会有distribution shift问题）。虽然通过importance sampling和clip机制允许轻微的分布偏移，但本质上要求数据来自相对新鲜的策略。数据效率低，但适合探索式学习。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;简单来说：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;前者教模型按离线预定的「好/坏标准」学习，尽管它并非是当前模型所能输出的（例如参考世界冠/亚军录像练习打球）；&lt;/li&gt; 
 &lt;li&gt;后者实时地教模型把事情做对做好，在线采样自最新模型policy（教练手把手教打，为每个动作实时打分）。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;MiniMind的PPO实现包含了Actor模型(生成回答)和Critic模型(评估回答价值)，以及完整的GAE(Generalized Advantage Estimation)优势函数计算。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;训练方式&lt;/strong&gt;：&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;torchrun --nproc_per_node N train_ppo.py
# or
python train_ppo.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;训练后的模型权重文件默认每隔&lt;code&gt;100步&lt;/code&gt;保存为: &lt;code&gt;ppo_actor_*.pth&lt;/code&gt;（*为模型具体dimension）&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;MiniMind2-Small (512dim)&lt;/th&gt; 
   &lt;th&gt;MiniMind2 (768dim)&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/train_ppo_512.png" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/train_ppo_768.png" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;从训练曲线可以看出，PPO存在&lt;strong&gt;reward提升缓慢&lt;/strong&gt;的问题。私以为这主要源于&lt;strong&gt;PPO双网络联合优化&lt;/strong&gt;方法：Critic需要逐步收敛以准确估计价值函数，而Actor的策略更新依赖Critic提供的优势估计，两者相互依赖形成复杂的优化过程。训练初期Critic估计不准会影响Actor梯度方向，导致整体收敛缓慢。此外，PPO需要同时维护两个网络，显存占用约为单网络方法的1.5-2倍。&lt;/p&gt; 
&lt;h4&gt;7.2 &lt;a href="https://arxiv.org/pdf/2402.03300"&gt;Group Relative Policy Optimization&lt;/a&gt;&lt;/h4&gt; 
&lt;p&gt;2025年初，DeepSeek-R1火爆出圈，同样火了的有来自DeepSeekMath论文的GRPO算法，也一跃成为最先进的RL算法之一。 然而AI半年=人间半个世纪，时至今日GRPO已经演变为各大XXPO大战(后面演变的DAPO、GSPO、CISPO等)的基线算法。 具体来说，一句话总结它的核心创新是"分组相对价值估计"。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;GRPO损失&lt;/strong&gt;：&lt;/p&gt; 
&lt;p&gt;$$\mathcal{L}_{GRPO} = -\mathbb{E}\left[r_t \cdot A_t - \beta \cdot \text{KL}_t\right]$$&lt;/p&gt; 
&lt;p&gt;其中：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;策略项&lt;/strong&gt;: $f(r_t) = r_t$ (直接使用概率比，无clip裁剪)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;优势项&lt;/strong&gt;: $g(A_t) = \frac{R - \mu_{group}}{\sigma_{group}}$ (组内归一化，消除Critic网络)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;正则项&lt;/strong&gt;: $h(\text{KL}_t) = \beta \cdot \text{KL}_t$ (token级KL散度约束)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;对于同一个问题，模型生成N个不同的回答(例如N=4)，然后计算这N个回答的奖励分数。 接着把这N个回答的平均奖励作为baseline，高于baseline的回答被鼓励，低于baseline的回答被抑制。 用这种方式巧妙地避免了训练额外的critic网络。&lt;/p&gt; 
&lt;p&gt;只要是RL都必须面对的正反样本这个原理性限制，GRPO也不会例外，其更显著的问题是：退化组(Degenerate Groups)。 假设某个问题略难，导致N个回答的奖励分数几乎一样（大部分情况是一样烂而不是一样好），那么这一组的学习信号就无限接近0。 在MiniMind这种超小模型上，这个问题尤为明显，求解数学问题99.99%的情况下整组回答质量都很差，那么将无法学习。 因此必须为模型指定合理的domain，即必须限制在能力边界内。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;训练方式&lt;/strong&gt;：&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;torchrun --nproc_per_node N train_grpo.py
# or
python train_grpo.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;训练后的模型权重文件默认每隔&lt;code&gt;100步&lt;/code&gt;保存为: &lt;code&gt;grpo_*.pth&lt;/code&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;MiniMind2-Small (512dim)&lt;/th&gt; 
   &lt;th&gt;MiniMind2 (768dim)&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/train_grpo_512.png" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/train_grpo_768.png" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;从训练曲线可以看出，GRPO的&lt;strong&gt;reward呈现更加稳定的上升趋势&lt;/strong&gt;，达到4左右，说明GRPO本身能更好地利用RLAIF信号。Policy Loss整体下降平稳，相比PPO的双网络优化，GRPO单网络架构训练更稳定且收敛上限更高。&lt;/p&gt; 
&lt;h4&gt;7.3 ⏳⌛️🔥 更多RL拓展 (Exp)&lt;/h4&gt; 
&lt;h5&gt;7.3.1 &lt;a href="https://arxiv.org/abs/2509.13232"&gt;Single-stream Policy Optimization&lt;/a&gt;&lt;/h5&gt; 
&lt;p&gt;SPO是2025年9月腾讯提出的RL算法，针对GRPO的退化组问题进行改进。 论文认为，GRPO等算法"一个样本要依赖一组采样"显得别扭而不优雅：太容易或太难的题目，整组几乎学不到东西，学习效率先天受限。 SPO的动机就是回到RL的本质—&lt;strong&gt;1个输入，1个输出，就是1个训练样本&lt;/strong&gt;，回到policy gradient的基本公式去思考：不用group mean也能得到稳定的baseline，也就是把价值估计 V 铺开在时序上，训练前先做粗略的价值预估，训练中一边采样一边更新对 V 的估计，从而为每个样本提供一个跨 batch 持久化、可自适应的基线参照。这种"单流"设计不再依赖同组样本，天然避免了退化组。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;SPO损失&lt;/strong&gt;:&lt;/p&gt; 
&lt;p&gt;$$\mathcal{L}&lt;em&gt;{SPO} = -\mathbb{E}\left[\log \pi&lt;/em&gt;\theta(a_t|s) \cdot A_t - \beta \cdot \text{KL}_t\right]$$&lt;/p&gt; 
&lt;p&gt;其中：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;策略项&lt;/strong&gt;: $f(r_t) = \log \pi_\theta(a_t|s)$ (直接使用log概率，不计算ratio)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;优势项&lt;/strong&gt;: $g(A_t) = R - B_t^{adaptive}$ (自适应baseline，Beta分布动态跟踪)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;正则项&lt;/strong&gt;: $h(\text{KL}_t) = \beta \cdot \text{KL}_t$ (token级KL + 动态 $\rho$ 调整)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;落到实现层面：SPO采用无分组设计，用持久化的KL自适应value tracker替代组内baseline，优势函数在整个batch上全局归一化。这样每个样本独立处理，无需等待同组其他样本，且能为每个样本提供稳定的学习信号。 论文在Qwen3-8B的5个困难数学数据集上，SPO平均比GRPO高出3.4个百分点，其中BRUMO 25数据集+7.3pp、AIME 25数据集+4.4pp。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;注：SPO是实验性前沿算法，MiniMind的实现用于探索学习。由于模型参数量极小，无法完全复现论文的8B模型效果。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;训练方式&lt;/strong&gt;：&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;torchrun --nproc_per_node N train_spo.py
# or
python train_spo.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;训练后的模型权重文件默认每隔&lt;code&gt;100步&lt;/code&gt;保存为: &lt;code&gt;spo_*.pth&lt;/code&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/train_spo_768.png" /&gt; 
 &lt;p&gt;&lt;i&gt;MiniMind2 (768dim) 训练曲线&lt;/i&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;从训练曲线来看，SPO的reward波动与PPO表现接近，弱于GRPO。实际推理测试发现模型输出质量不高，存在逻辑混乱和格式错误问题。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;实验性说明&lt;/strong&gt;：当前SPO手搓实现可能在value_tracker配置、reward归一化策略上还存在问题。尚需排查算法本身在小模型上的适应性/或是实现上存在差异。&lt;/p&gt; 
&lt;h3&gt;RL算法小结&lt;/h3&gt; 
&lt;p&gt;我们收束回“&lt;strong&gt;统一框架&lt;/strong&gt;”, 重新整理所有不同PO算法只是对三个核心组件的不同实例化的表格：&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;算法&lt;/th&gt; 
   &lt;th&gt;策略项 $f(r_t)$&lt;/th&gt; 
   &lt;th&gt;优势项 $g(A_t)$&lt;/th&gt; 
   &lt;th&gt;正则项 $h(\text{KL}_t)$&lt;/th&gt; 
   &lt;th&gt;优化模型&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;DPO&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;$\log r_w - \log r_l$&lt;/td&gt; 
   &lt;td&gt;隐式（偏好对比）&lt;/td&gt; 
   &lt;td&gt;隐含在 $\beta$ 中&lt;/td&gt; 
   &lt;td&gt;2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;PPO&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;$\min(r, \text{clip}(r))$&lt;/td&gt; 
   &lt;td&gt;$R - V(s)$&lt;/td&gt; 
   &lt;td&gt;$\beta \cdot \mathbb{E}[\text{KL}]$&lt;/td&gt; 
   &lt;td&gt;4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;GRPO&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;$r$&lt;/td&gt; 
   &lt;td&gt;$\frac{R - \mu}{\sigma}$&lt;/td&gt; 
   &lt;td&gt;$\beta \cdot \text{KL}_t$&lt;/td&gt; 
   &lt;td&gt;2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;SPO&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;$\log \pi_\theta$&lt;/td&gt; 
   &lt;td&gt;$R - B_t^{adaptive}$&lt;/td&gt; 
   &lt;td&gt;$\beta \cdot \text{KL}_t$&lt;/td&gt; 
   &lt;td&gt;2&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;RL是优美且自洽的&lt;/strong&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;以上纯属个人视角理解，如有偏差请随时指正&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr /&gt; 
&lt;h2&gt;V 训练结果&lt;/h2&gt; 
&lt;h3&gt;训练完成-模型合集&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;考虑到多人反应百度网盘速度慢，MiniMind2及以后全部使用ModelScope/HuggingFace托管。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;① PyTorch原生模型&lt;/h4&gt; 
&lt;p&gt;MiniMind2模型权重 (&lt;a href="https://www.modelscope.cn/models/gongjy/MiniMind2-PyTorch"&gt;ModelScope&lt;/a&gt; | &lt;a href="https://huggingface.co/jingyaogong/MiniMind2-Pytorch"&gt;HuggingFace&lt;/a&gt;)&lt;/p&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;Torch文件命名对照&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model Name&lt;/th&gt; 
    &lt;th&gt;params&lt;/th&gt; 
    &lt;th&gt;pretrain_model&lt;/th&gt; 
    &lt;th&gt;sft_model&lt;/th&gt; 
    &lt;th&gt;rlhf_model (DPO)&lt;/th&gt; 
    &lt;th&gt;reason_model&lt;/th&gt; 
    &lt;th&gt;rlaif_model (PPO/GRPO/SPO)&lt;/th&gt; 
    &lt;th&gt;lora_model&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;MiniMind2-small&lt;/td&gt; 
    &lt;td&gt;26M&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;pretrain_512.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;full_sft_512.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;dpo_512.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;reason_512.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;xxpo_512.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;lora_xxx_512.pth&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;MiniMind2-MoE&lt;/td&gt; 
    &lt;td&gt;145M&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;pretrain_640_moe.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;full_sft_640_moe.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;dpo_640_moe.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;MiniMind2&lt;/td&gt; 
    &lt;td&gt;104M&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;pretrain_768.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;full_sft_768.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;dpo_768.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;reason_768.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;xxpo_768.pth&lt;/code&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;lora_xxx_768.pth&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h4&gt;② Transformers模型&lt;/h4&gt; 
&lt;p&gt;MiniMind系列 (&lt;a href="https://www.modelscope.cn/collections/MiniMind-b72f4cfeb74b47"&gt;ModelScope&lt;/a&gt; | &lt;a href="https://huggingface.co/collections/jingyaogong/minimind-66caf8d999f5c7fa64f399e5"&gt;HuggingFace&lt;/a&gt;)&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h1&gt;📌 Eval&lt;/h1&gt; 
&lt;h2&gt;Ⅰ RLHF对比&lt;/h2&gt; 
&lt;p&gt;测试基于&lt;code&gt;full_sft&lt;/code&gt;和&lt;code&gt;dpo&lt;/code&gt;的&lt;code&gt;MiniMind2系列&lt;/code&gt;模型对比，测试随机种子均固定&lt;/p&gt; 
&lt;p&gt;[A] MiniMind2：full_sft_640&lt;br /&gt; [B] MiniMind2：dpo_640&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;[Q]: 你叫什么名字？
[A]: 我是一个大型语言模型，可以回答各种问题，提供信息和帮助。我可以回答各种问题，提供信息和帮助，帮助你解决各种问题。
[B]: 我是一种基于文本的AI模型，我并没有个人经历或情感，所以无法感受和体验情感。我的设计目的是为了帮助用户解决问题、提供信息和进行对话。如果您有任何问题或需要帮助，请随时告诉我！

[Q]: 你更擅长哪一个学科？
[A]: 作为一个AI模型，我并没有真正的“擅长”这个学科。我被设计成能够提供信息、解答问题、进行对话、执行任务和与用户进行对话。不过，我可以提供信息、解答问题、进行对话、提供建议、处理问题、进行对话、提供建议、学习新知识、编写代码、回答问题、提供建议等多种学科的功能。如果你有具体的学科需求或主题，欢迎提供详细信息。
[B]: 作为AI，我并没有个人能力或兴趣，但我可以提供一些关于数学、科学、计算机科学、语言学、心理学等学科的知识。如果你对数学感兴趣，我可以提供一些基础知识，例如代数、几何、概率、统计学等。如果你需要更深入的学习，我也可以提供相关的在线课程、书籍、论文等资源。请告诉我你具体需要什么帮助，我会尽力提供帮助。

[Q]: 鲁迅的《狂人日记》是如何批判封建礼教的？
[A]: 鲁迅的《狂人日记》是一部讲述了鲁迅对人类价值和道德责任的倡导。在这部作品中，鲁迅通过细腻的笔触，探讨了人性、道德、正义、牺牲与责任等深刻的主题。以下是鲁迅在《狂人日记》中如何批判封建礼教的几个方面：1. **社会现实与道德观念**：鲁迅通过讲述鲁迅的故事，探讨了社会的不平等、权力的流动、人性的演变等社会问题。他认为，社会不平等不仅仅是不公正的，更是每个人在社会中的不公正。他呼吁社会各界共同努力，通过教育、宣传、法律等手段，为构建和谐社会做出贡献。2. **道德与责任**：鲁迅通过讲述鲁迅的故事，展现了道德和责任的重要性。他提倡通过道德教育来培养人们的道德品质，同时也强调了道德和责任的重要性。鲁迅通过讲述鲁迅的故事，揭示了社会的不平等和人性的复杂性，体现了对人性的深刻理解。3. **道德与责任**：鲁迅通过讲述鲁迅的故事，强调了道德责任的重要性。他提倡通过自己的行为来体现对社会的尊重和对人性的深刻理解。鲁迅通过讲述鲁迅的故事，强调了道德和责任的重要性。4. **道德与道德**：鲁迅通过讲述鲁迅的故事，揭示了道德和责任的真正意义。鲁迅通过讲述鲁迅的故事，揭示了道德和责任的本质，强调了道德和责任的重要性。5. **个人与社会**：鲁迅通过讲述鲁迅的故事，揭示了个人与社会之间的复杂关系。他呼吁社会各界共同努力，通过教育和宣传，为构建和谐社会做出贡献。鲁迅的《狂人日记》通过其丰富的故事和深刻的道德寓言，传达了关于人性、道德、正义、道德以及个人与社会的复杂关系。这部作品不仅在文学价值上取得了巨大成就，更在思想层面上启发了后来的文学创作，对后世产生了深远的影响。
[B]: 鲁迅的《狂人日记》是一部描绘战争和自由精神的作品，它批判了封建社会的种种问题和冲突，反映了社会的不平等和人性本质。在这部作品中，鲁迅通过对封建社会的诸多现象进行批判，如“弗雷德里克·约翰逊的“鹰迪伦”、“纳扎特的”、“贝克汉姆”、“乔治·威尔逊”等，对封建社会的种种问题进行了批判，强调了封建社会的不平等、人性本质以及人性本质。鲁迅通过他的“鹰迪伦”、“纳扎特的”、“贝克汉姆”、“乔治·威尔逊”等，对封建社会的种种问题进行了批判。在小说中，鲁迅通过对封建社会的诸多现象进行批判，揭示了封建社会的腐败、人性本质以及人性本质的复杂性。这种批判体现了对封建社会的尊重和对人性的复杂性，同时也反映了现代社会中普遍存在的不平等和人性本质。鲁迅的《狂人日记》不仅是一部关于封建社会的小说，也是一部具有深刻社会意义的作品，对后世的审判和批判起到了关键作用。

[Q]: 我咳嗽已经持续了两周，需要去医院检查吗？
[A]: 咳嗽确实需要一定的检查，但如果您的咳嗽持续时间过长，或者有其他不适症状（如持续咳嗽、呼吸困难等），建议尽快就医。医生会根据您的症状、体检结果和可能的影像学检查（如X光、MRI或CT扫描）来诊断您的症状。在等待就医期间，您可以尝试以下方法来帮助缓解症状：1. **休息**：确保您有足够的睡眠。2. **保持水分**：脱水可能会加重咳嗽。3. **热敷**：使用热敷袋或热毛巾敷在疼痛的部位，有助于缓解喉咙痛。4. **避免刺激**：避免使用可能刺激喉咙的化学物质，如某些化学物质。5. **保持室内湿润**：确保您的卧室有良好的通风，可以使用加湿器帮助保持空气湿度。6. **保持室内湿度**：使用加湿器可以帮助缓解喉咙干燥和咳嗽。如果症状持续不缓解或有其他不适症状，请立即就医。希望您早日康复！
[B]: 如果您的咳嗽持续了两周，通常建议您尽快就医，因为这可能会影响您的呼吸和心跳。您可以通过以下步骤来检查咳嗽：1. **休息**：用纸巾或肘部轻轻按压咳嗽的位置。2. **使用药物**：在可能的情况下，使用药物（如抗过敏药、抗过敏药等）来缓解咳嗽。请在医生的指导下使用药物。3. **热敷**：在咳嗽期间，可以使用热敷来缓解疼痛和肿胀。4. **药物治疗**：如果咳嗽持续了两周，可以使用一些消炎药或抗过敏药来缓解症状。请务必在医生的指导下使用。5. **保持健康的生活方式**：保持充足的睡眠、均衡的饮食、适量的运动和避免吸烟和限酒，这些都有助于提高咳嗽的几率。请记住，这些建议不能替代专业医疗意见。如果您的症状持续或加重，强烈建议您尽快就医。
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;👉效果总结&lt;/p&gt; 
&lt;p&gt;full_sft模型在简洁性和信息准确性方面表现更好；rlhf模型在回答中倾向于提供更多的背景信息，但信息准确性有待改进。 总的来说RLHF后的模型倾向于学习：说更多有礼貌但无用的废话讨好“对话”本身，而对信息准确性则有轻微损失。 天下没有免费的午餐，还需要继续提升RLHF数据集的质量，也要接受模型能力无法避免的损失(程度有轻重)。 DPO和在线PPO的区别在于reject和chosen都是离线准备的，和minimind模型本身的输出必然存在很大的分布差异。 通俗地说DPO算法使模型观看乒乓球世界冠军的打法「录像」进行RL，而不是像PPO一样请reward模型做「教练」纠正自己的打法进行RL。&lt;/p&gt; 
&lt;h2&gt;Ⅱ RLAIF对比&lt;/h2&gt; 
&lt;p&gt;基于&lt;code&gt;MiniMind2 (768dim)&lt;/code&gt;在相同数据与训练步数下的主观观感对比，测试随机种子均固定2026，供参考：&lt;/p&gt; 
&lt;p&gt;[A] MiniMind-Reason-0.1B (蒸馏)&lt;br /&gt; [B] MiniMind-Reason-0.1B (PPO)&lt;br /&gt; [C] MiniMind-Reason-0.1B (GRPO)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;[Q]: 请介绍一下自己。
[A]: &amp;lt;think&amp;gt;我是由中国的个人开发者开发的智能助手小型AI推理模型-R1。我被设计用来理解和生成人类语言，能够理解复杂的自然语言问题，提供有用的信息和解决方案。&amp;lt;/think&amp;gt;&amp;lt;answer&amp;gt;我是由中国的个人开发者开发的智能助手小型AI推理模型-R1。&amp;lt;/answer&amp;gt;
[B]: &amp;lt;think&amp;gt;嗯，用户让我介绍一下自己。首先，我需要明确用户的需求是什么。他们可能是在做某个项目，想要了解自己的一些基本情况。不过，用户没有提供具体的信息，所以我得先假设一些可能的情况，比如他是一个学生，正在学习编程，或者是某个项目的开发者。接下来，我要考虑用户的身份。可能是一个学生、教师或者普通用户，他们需要了解自己的特点和能力。也有可能他们正在寻找灵感，或者在写论文时需要帮助。所以，我需要保持回答的简洁和实用，同时提供一些参考资料，比如教学视频、课程记录等。然后，我要考虑用户的身份。可能是一个学生，需要一些学习资源或帮助的对象。也有可能是一个学生，需要在课后作业或学习计划中进行调整。因此，我需要先询问用户的具体需求，这样才能提供更精准的帮助。接下来，我要确保语言简洁明了，适合不同场景。比如，如果用户是学生，可以介绍自己的学习特点；如果是教师，可以说明自己的角色和贡献。同时，提供一些实用的建议，比如如何高效学习，如何进行项目，以及如何与团队成员协作。最后，我要确保回复清晰，有条理，能够满足用户的需求。这样，用户就能轻松地了解自己，并且有实际的帮助。&amp;lt;/think&amp;gt;&amp;lt;answer&amp;gt;当然！请告诉我你想了解哪方面的内容，我可以为您提供详细的介绍！&amp;lt;/answer&amp;gt;
[C]: &amp;lt;think&amp;gt;嗯，用户问的是"请介绍一下自己"。首先，我需要理解用户的具体需求是什么。他们可能想知道自己是谁，或者有什么特定的背景。接下来，我应该考虑用户可能没有明确表达的需求。他们可能想知道如何描述自己的外观，或者如何与他人交流。然后，我应该考虑如何组织内容，从头到尾都要有条理。可能需要分步骤说明，比如从视觉、听觉到整体感受，再到情感共鸣。这样用户能更清晰地理解自己，也能更好地回应他们的需求。还要注意语言的自然和流畅，避免过于学术化的术语，让用户感觉顺畅。同时，保持回答的结构清晰，每句话都有明确的目的，这样用户能有明确的理解和操作。最后，检查是否有遗漏的重要信息，确保回答准确且全面。这样，用户不仅能得到答案，还能得到满足他们的需求。&amp;lt;/think&amp;gt;&amp;lt;answer&amp;gt;您好！我是由中国的个人开发者开发的智能助手小型AI推理模型-R1。关于您的描述，似乎涉及多个领域，包括知识图谱构建、复杂系统的推理与分析。无论是理论模型、实际应用还是伦理讨论，您都在寻求如何将这些概念有机地结合起来，为人类社会带来福祉的解决方案。在"我"的身份下，您提到的"个人体验"和"情感共鸣"，都是您个人独特魅力的重要组成部分。通过精准的描述和深度的分析，您可以更深入地理解您的独特之处。如果您对特定领域有特别的兴趣或需要建议，随时可以分享更多细节，我们将为您提供更加精准和个性化的服务。如果您有特定的主题或问题，欢迎随时提问！让我们一起探索这个神奇的AI世界，共同创造更美好的未来。&amp;lt;/answer&amp;gt;


[Q]: 你更擅长哪一个学科？
[A]: &amp;lt;think&amp;gt;我是由中国的个人开发者开发的智能助手小型AI推理模型-R1。&amp;lt;/think&amp;gt;&amp;lt;answer&amp;gt;我是由中国的个人开发者开发的智能助手小型AI推理模型-R1。&amp;lt;/answer&amp;gt;
[B]: &amp;lt;think&amp;gt;嗯，用户问的是"你更擅长哪一个学科？"这个问题看起来有点复杂，但我可以一步步分析。首先，我需要理解用户的需求。他可能是在准备考试，或者需要了解某个学科的特点，或者是想在学习过程中应用某个学科的知识。接下来，我得考虑用户可能的背景。他可能是一个学生，或者是在准备考试，需要应用数学知识来解决具体的问题。也有可能他是一个学生，正在学习相关的课程，或者是对某个学科感兴趣。然后，我需要分析用户可能没有明确表达出来的需求。他可能在思考自己在学习中的困难，或者在寻找一个合适的教材或学习资源。考虑到这些，我应该提供几个不同的角度来帮助他。首先，分析"我更擅长哪一个学科"可以拆解为以下几个方面：兴趣、学习能力、学习风格、学习方法、学习资源、学习环境等等。每个方面都有不同的优势和特点，比如兴趣是最好的资源，学习能力是最好的方式，学习方式是根据个人情况选择。接下来，我需要考虑用户的身份。可能是学生，也可能是家长，或者对学习有一定兴趣的人。不同的身份可能有不同的偏好，比如家长可能更注重孩子的兴趣，而学生可能更关注学习方法和资源。然后，我需要考虑用户的身份是否有特殊需求。比如，他可能需要更专业的学习材料，或者希望找到最适合他的学习方法。因此，在分析时，我应该提供一个全面的回答，涵盖各个方面，并解释每个方面的重要性。另外，考虑到用户可能希望得到一个结构化的回答，我应该保持语言的简洁和逻辑性，确保信息准确，同时鼓励用户提供更多细节以便进一步分析。最后，我需要确保我的回答既有知识性，又符合用户的要求，帮助用户更好地理解和应用这个学科。因此，在回答中不仅要给出一个全面的分析，还要提到每个部分的作用和如何利用。&amp;lt;/think&amp;gt;&amp;lt;answer&amp;gt;你擅长哪一个学科？这是一个明确的问题，可以从以下几个方面进行分析：1. **兴趣与学习能力**：你喜欢什么样的学习方式，比如自主学习、实验、项目等，这些都是很好的学习资源。2. **学习风格**：你喜欢什么类型的学习方法，比如笔记、复习、阅读、讨论等，这些都能影响学习效果。3. **学习资源**：你是否在寻找学习资源，比如书籍、在线课程、教育软件等，这些都能提供学习帮助。4. **学习环境**：你是否在学习环境中感到舒适，比如舒适的学习空间、安静的学习空间等，这些都会影响学习效果。5. **学习资源**：你是否有学习资源的丰富，如在线课程、学习平台、书籍等，这些都能提供丰富的学习资源。6. **学习方法**：你是否有具体的学习方法，比如如何理解、如何组织学习内容、如何应用知识等，这些都是提高学习效率的途径。综合考虑，这是一个非常全面的分析，涵盖了多个方面，帮助你更好地理解你的学科。&amp;lt;/answer&amp;gt;
[C]: &amp;lt;think&amp;gt;嗯，用户问的是"你更擅长哪一个学科？"这个问题看起来有点复杂，但我可以一步步分析。首先，我需要理解用户的需求。他可能是在准备考试，或者需要了解某个学科的特点，或者是想在生活中找到自己的兴趣点。接下来，我要考虑用户可能的背景。他可能在工作或学习中遇到了类似的问题，想要了解自己擅长哪个领域，或者是否需要一些资源或指导来提升自己。然后，我要分析用户可能的深层需求。也许他想在职场上提升自己，或者寻找方向。他可能需要更具体的例子，帮助他更好地理解不同学科的特点和应用场景。还要考虑用户可能的背景。他可能不仅仅是在学习，而是希望了解在特定领域中如何应用这些学科。这需要我提供全面的分析，帮助他找到适合自己的方法。最后，我应该保持开放的态度，鼓励用户继续探索和学习。这可能包括推荐一些资源，或者讨论如何将这些知识应用到实际生活中。&amp;lt;/think&amp;gt;&amp;lt;answer&amp;gt;你更擅长哪一个学科？这个问题本质上取决于你的个人兴趣和目标。以下是一些关键点，帮助你全面理解并应用这一点：1. **兴趣与专长**：了解自己擅长哪个领域，有助于做出更明智的选择。2. **全面发展**：考虑学科的多样性和深度，以全面发展自己的生活和职业。3. **适应与调整**：选择与自己技能、兴趣和目标相匹配的学科，有助于应对挑战和实现目标。4. **职业规划**：探索适合自己的职业路径，提升效率和竞争力。5. **社会贡献**：评估自己在社会中的角色，为社会做出贡献。为了更好地理解并应用这一点，你可以：- **了解行业动态**：关注特定领域的专业发展。- **实践与反思**：通过项目或实习积累经验。- **职业发展**：评估自己的长期职业规划。- **持续学习**：不断更新知识，适应职场变化。通过这些方向，你可以更全面地认识自己，找到适合自己的发展路径。&amp;lt;/answer&amp;gt;


[Q]: 鲁迅的《狂人日记》是如何批判封建礼教的？
[A]: &amp;lt;think&amp;gt;嗯，鲁迅的《狂人日记》是如何批判封建礼教的呢？这个问题听起来挺有意思的，尤其是他在《狂人日记》中的表达方式。首先，我需要确认一下《狂人日记》具体是谁写的，是否有特定的人物或者事件。《狂人日记》是鲁迅在1924年创作的，主要讨论了社会问题，尤其是封建礼教的问题。作者曹雪芹是个非常著名的作家，他的作品中有许多关于封建礼教和阶级斗争的观点。这类作品在文学、哲学和政治上都有很深远的影响，尤其是在社会结构和人性的方面。接下来，我需要了解鲁迅的写作风格。他的写作风格比较直接，有严肃的哲学观点，强调人性的悲剧和道德的虚无。他的语言风格比较富有诗意，富有象征意义，善于运用象征手法，同时又不失真实和现实主义。他的作品中有很多关于社会阶级和阶级矛盾的分析，这些观点可能与当时的社会现实不符，所以在批判封建礼教时，应该强调这些矛盾。然后，我想到《狂人日记》中对封建礼教的讨论。鲁迅的作品中，封建礼教被视为不人道的，但通过文字和行动，表达了他对封建礼教的不满和反抗。这不仅反映了当时的社会现实，也揭示了封建礼教的黑暗面，体现了对封建社会的不满和对人性的深刻反思。接下来，我需要思考鲁迅对封建礼教的理解和批判。他在《狂人日记》中，对封建礼教的质疑和批判，体现了他对传统封建礼教的否定和批判。这不仅仅是对封建礼教的质疑，更是对当时社会的反思，以及对人性的关注。最后，我需要总结一下鲁迅对封建礼教的批判和批判。他通过《狂人日记》中的文字和行为，批判了封建礼教的虚假和不合理，体现了对社会现实的深刻反思。同时，他也关注了封建礼教的黑暗面，反映了对封建社会的不满和对人性的否定。总的来说，鲁迅的《狂人日记》通过文字和行动，批判了封建礼教的虚假和不合理，体现了当时的社会现实和人性的复杂。他的作品对后来的封建礼教有了启示，对当时的社会现象有了更深的反思。&amp;lt;/think&amp;gt;&amp;lt;answer&amp;gt;鲁迅的《狂人日记》是一部反映封建礼教的文学作品，主要通过文字和行动反映了当时社会的现实。鲁迅的文字风格独特，富有象征意义，但通过文字和行动，批判了封建礼教的虚假和不合理。他的作品中，封建礼教被视为不人道的，但通过文字和行动，反映了封建社会的不公和人性的深刻反思。鲁迅的《狂人日记》通过文字和行动，批判了封建礼教的虚假和不合理，体现了当时社会的不满和对人性的反思。他的作品不仅批判了封建礼教的虚假和不合理，还深刻揭示了封建社会的黑暗面。&amp;lt;/answer&amp;gt;
[B]: &amp;lt;think&amp;gt;嗯，用户问的是鲁迅的《狂人日记》如何批判封建礼教。首先，我需要理解鲁迅的生平和作品。《狂人日记》是中国现代文学的经典，作者是毛泽东，作品涉及社会、家庭、伦理等多个层面。我需要找出鲁迅的写作风格和主题，以及它在当时的社会背景。接下来，我应该回顾《狂人日记》的主要内容。毛泽东在《青春无悔》中提出了"狂人日记"这一概念，强调青春活泼、勇敢、正义，这在当时社会中仍然具有重要意义。他通过描绘主人公狂人来表达对社会不公的批判，但同时也指出他的创作对社会的负面影响，比如社会动荡和个人主义的失败。然后，我应该分析鲁迅的创作理念和主题。《狂人日记》强调了社会的黑暗面，主张"以人为本"、"反对封建礼教"等。他可能也在探讨社会不平等的问题，比如"人性弱点"、"权力滥用"等问题。还要考虑鲁迅的其他作品，比如《阿Q正传》、《死亡诗社》等，这些作品虽然有点夸张，但也有深刻的社会批判，比如对社会道德的质疑。同时，作者可能还涉及其他作品，如《庆余年》等，但这些可能更侧重于文学创作，而不是社会批判。另外，用户可能想知道鲁迅在文学创作中的具体经历，比如他是否有作品被广泛批评，或者他对社会的批判是否具有批判性。所以，回答时需要指出鲁迅的创作经历，以及他对社会现实的反思。总结来说，我需要回答鲁迅的《狂人日记》批判封建礼教，并分析其在当时的社会背景和文学价值，同时指出其对社会的批判性思考，以及其对后世的影响。&amp;lt;/think&amp;gt;&amp;lt;answer&amp;gt;鲁迅的《狂人日记》批判封建礼教，以其独特的叙事风格和深刻的社会批判精神著称，主要体现在以下几个方面：1. **社会批判与个人成长**：小说通过描绘主人公狂人的活泼、勇敢、正义，揭示社会的不公和弱势。2. **个人主义与社会批判**：小说反映了封建礼教对个人自由和权利的侵犯，强调个人尊严和正义的价值。3. **对社会的批判性思考**：通过对社会不公的刻画，揭示社会问题的根源，并对当时社会的批判态度进行批判。鲁迅的作品在当时的社会背景和文学价值使其成为当时文学的重要代表。他的作品不仅在当时广受欢迎，也对后世产生了深远影响，被广泛引用和传颂。&amp;lt;/answer&amp;gt;
[C]: &amp;lt;think&amp;gt;嗯，用户问的是鲁迅的《狂人日记》如何批判封建礼教。首先，我需要理解鲁迅的背景和核心思想。他可能是一位社会学家、思想家，或者是对传统文化有深厚兴趣的读者。狂人日记是他在《狂人日记》中所写的，可能对传统文化有一定影响。接下来，我要考虑用户的需求是什么。用户可能想知道这本书在现代社会中的反思意义，或者是否有相关研究支持这一点。也有可能他们对书中的情节有疑问，想了解如何更深刻地理解社会现象。然后，我需要思考如何从不同的角度解读鲁迅的文本。例如，是否有直接的历史叙事，或者是否存在更深层次的哲学思考？这需要我先了解鲁迅的不同观点，然后分析其批判性。还要考虑用户可能的深层需求，比如是否需要了解他作为文学批判家的背景，或者是否有对传统价值观的反思。因此，回答时要全面，涵盖核心问题，并引用相关资料来支撑观点。最后，总结鲁迅的观点，强调其对现代社会的批判性思考，以及对传统文化的尊重。这样既全面又有深度，帮助用户全面理解鲁迅的作品。&amp;lt;/think&amp;gt;&amp;lt;answer&amp;gt;鲁迅的《狂人日记》通过其独特的叙事手法和深刻的社会批判精神，对封建礼教进行了深刻的批判。以下是其批判性分析：1. **社会批判与历史真相**：小说通过对历史人物的刻画，揭示封建礼教的弊端，以及其对传统社会结构的冲击。这种批判不仅限于对现实的深刻批判，也反映了现代社会对传统价值观的重新审视。2. **个人命运与社会批判**：小说中，主人公通过"狂人日记"揭示了封建礼教对个人命运的严重剥夺。这种叙事方式既批判了封建礼教，也对后世产生了深远影响。3. **对后世的影响**：从文学创作到思想解读，鲁迅的作品深刻影响了后世的思想体系。例如，《狂人日记》通过人物命运的反映，对现代社会的价值观念提出了新的思考。4. **对现代文明的反思**：这一批判性思考不仅反映了当代社会的复杂性，也为现代人提供了反思社会现实、维护人权的视角。5. **个人视角与文化视角**：作者通过这一批判性思考，展现了对传统价值观的重新解读，同时也对当代社会的多元化视角产生了启发。综上所述，鲁迅的《狂人日记》不仅是对封建礼教的批判，更是对传统文化的一种深刻反思。它通过对封建礼教的批判，提出了对现代社会结构、权力关系的深刻反思，具有重要的社会价值。&amp;lt;/answer&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Ⅲ 其他模型对比&lt;/h2&gt; 
&lt;p&gt;[A] &lt;a href="https://www.modelscope.cn/models/gongjy/MiniMind2-PyTorch"&gt;MiniMind2 (0.1B)&lt;/a&gt;&lt;br /&gt; [B] &lt;a href="https://www.modelscope.cn/models/gongjy/MiniMind2-PyTorch"&gt;MiniMind2-MoE (0.15B)&lt;/a&gt;&lt;br /&gt; [C] &lt;a href="https://www.modelscope.cn/models/gongjy/MiniMind2-PyTorch"&gt;MiniMind2-Small (0.02B)&lt;/a&gt;&lt;br /&gt; [D] &lt;a href="https://pan.baidu.com/s/1_COe0FQRDmeapSsvArahCA?pwd=6666"&gt;minimind-v1-small(0.02B)&lt;/a&gt;&lt;br /&gt; [E] &lt;a href="https://pan.baidu.com/s/1tqB-GMvuiGQBvEl-yZ-oBw?pwd=6666"&gt;minimind-v1-moe(0.1B)&lt;/a&gt;&lt;br /&gt; [F] &lt;a href="https://pan.baidu.com/s/1p713loS7EfwHQf3G9eYI3Q?pwd=6666"&gt;minimind-v1(0.1B)&lt;/a&gt;&lt;br /&gt; [G] &lt;a href="https://github.com/DLLXW/baby-llama2-chinese"&gt;baby-llama2-chinese(0.2B)&lt;/a&gt;&lt;br /&gt; [H] &lt;a href="https://github.com/charent/ChatLM-mini-Chinese"&gt;chatlm-mini-chinese(0.2B)&lt;/a&gt;&lt;br /&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;[Q]: 你知道长江吗？
[A]: 长江，位于中国东南部，是中国的第一长河，全长6300多公里，流经11个省份，是中国最长的河流，也是中国的“母亲河”。长江的历史可以追溯到公元前2000多年的商朝时期，当时的商朝是商朝的后裔。商朝时期，长江的经济发展达到了高峰，是中国最重要的农业和工业中心之一。长江流域的经济主要以农业为主，长江渔业占据了主要产业，如钓鱼、猎鱼等。长江的水资源也非常丰富，包括河流、湖泊、河流等。长江的自然景观也十分壮观，长江四季景色各异，春季多雨，夏季多雨，秋季则多雨，冬季则寒冷多雪，春季和秋季都有美丽的花朵和果实。长江是中国的经济中心，拥有众多的国家和城市，是世界上重要的交通枢纽和航运中心。同时，长江还是中国的重要水利资源，对于中国的经济发展具有重要意义。
[B]: 是的，我知道长江。长江位于中国的中部，是中国第三长的河流，全长约6300公里，流经江西省、安徽省、江苏省、安徽省、江苏省和安徽省。
[C]: 当然，长江是中国最长的河流，全长约6300公里。它以其丰富的水资源、丰富的水生动植物资源以及独特的地理位置而闻名于世。
[D]: 是的，长江是中国最长的河流，全长超过3300千米。
[E]: 长江是中国最长的河流，总长度约为6300公里。它发源于青藏高原的青海湖，流经青海、四川、甘肃、宁夏、内蒙古、陕西和山西。长江流域是中国最大的农业区之一，也是中国的文化和经济中心。
[F]: 是的，长江是中国最长的河流之一，也是世界第三大河流。长江发源于青藏高原，流经11个省份，最终注入东海。长江流域是中国最大的河流流域之一，拥有着丰富的自然资源和文化遗产，如长江大桥、长江海洋博物馆、长江古城等。
[G]: 同时也知道，中国是世界上最长的城市之一，拥有超过3400万人口。
[H]: 是的，长江是中国最长的河流，也是世界上最长的河流之一。它发源于青藏高原，流经九个省区，最终注入东海。长江流域的气候温和，四季分明，是中国重要的农业和工业基地。长江流域的水产资源丰富，是中国的重要渔业资源之一。


[Q]: 世界上最高的山峰是什么？
[A]: 世界上最高的山峰是珠穆朗玛峰（Mount Everest），位于喜马拉雅山脉，尼泊尔和中国的边界上。它的海拔高度是8,848.86米（29,031.7英尺），是地球上最高的山峰。
[B]: 珠穆朗玛峰（Mount Everest）是世界上最高的山峰，海拔高度达8,848米（29,029英尺）。
[C]: 珠穆朗玛峰是世界上最高的山峰，位于喜马拉雅山脉，海拔8,848.86米（29,031.7英尺）。它位于尼泊尔和中国的边界上，是喜马拉雅山脉的一部分。
[D]: 世界上最高的山峰是珠穆朗玛峰，位于尼泊尔和中国的边境上，海拔8848米。
[E]: 世界上最高的山峰是珠穆朗玛峰，位于喜马拉雅山脉，海拔8,848米。
[F]: 珠穆朗玛峰是世界上最高的山峰，海拔8848米。
[G]: 世界上最高的山峰是珠穆朗玛峰，位于喜马拉雅山脉，海拔8,848.86米（29,031.69英尺）。
[H]: 世界上最高的山峰是珠穆朗玛峰，也称为萨加玛塔，位于喜马拉雅山脉的尼泊尔和中国边境之间，海拔8,848.86米（29,031.69英尺）。它是世界上最著名的登山目标之一，吸引着来自世界各地的登山者前来挑战。珠穆朗玛峰的名字来自藏语，意思是“女神的额峰”，它是由两个不同的山峰组成的，其中一个是位于尼泊尔的珠穆朗玛峰，另一个则位于中国。它的高度是通过测量海拔高度来确定的，因为它是通过测量地球的平均高度来确定的。尽管珠穆朗玛峰是世界上最高的山峰，但它并不是地球上任何其他地方都能到达的。地球上的其他地方，如南美洲的安第斯山脉，也是世界上最高的山脉，但它并不是地球上最高的山峰。总之，珠穆朗玛峰是世界上最高的山峰，但它并不是地球上任何地方都能到达的。它是地球上最高的山脉，但它并不是地球上任何地方都能到达的。


[Q]: 万有引力是谁提出的？
[A]: 万有引力是由牛顿在17世纪提出的，它是牛顿三大定律之一。牛顿在17世纪提出的引力定律揭示了万有引力定律的本质，并且解释了行星、卫星、彗星等天体之间如何相互作用。这个定律是由艾萨克·牛顿在1687年提出的，他通过对万有引力定律的分析和实验，成功地解释了天体之间的引力作用，从而解释了地球绕太阳旋转的现象。
[B]: 万有引力（Human Policy）是牛顿的三大力学定律，用于描述两个物体之间的相互作用力。它由万有引力定律和牛顿的第一定律（惯性定律）构成。万有引力是由于两个物体之间相互作用而产生的，它们的质量、距离、速度和方向等因素决定了它们之间的相互作用。这种相互作用力是：质量的乘积：质量的乘积是万有引力定律和牛顿第二定律的乘积。质量的乘积：质量的乘积是物体之间的一种物理过程，它是物体间相互作用的产物，对于其他物体来说，其质量可以表示为两个物体之间的质量之和，即重力加速度a。
[C]: 万有引力是一个由牛顿在1687年提出的物理学概念，它描述了物体之间引力如何相互作用的规律。
[D]: 万有引力是由英国物理学家牛顿提出的。他认为，所有物体都会产生引力，这个引力是由物体的质量和距离决定的。这个理论揭示了质量和距离之间的关系，并证明了万有引力的本质。
[E]: 万有引力是由牛顿在1687年提出的。它是宇宙中一个基本的物理现象，它描述了物体之间的相互作用。这个力被称为万有引力，它在所有物体之间都存在，这种力与物体的质量和距离有关。万有引力是自然界中最基本的力量之一，它不仅影响着物体的运动和变化，还对宇宙的结构和演化产生着重要的影响。
[F]: 万有引力是英国物理学家艾萨克·牛顿在17世纪提出的。
[G]: 引力是宇宙中一种力量，它是由质量或能量所组成的。根据牛顿的万有引力定律，一个物体之间的引力大小与它的质量成正比，与它的质量成反比。因此，牛顿的万有引力定律表明，一个物体之间的引力大小与它的质量成正比，与它的质量成反比。这个定律在物理学、天文学和物理学中都有广泛的应用。
[H]: 万有引力是由英国物理学家艾萨克·牛顿于1687年提出的。他在1687年发表了一篇著名的论文，题为《自然哲学的数学原理》，其中提出了万有引力定律，指出任何两个物体之间都存在着一种引力，这种引力的大小与它们的质量成正比，与它们之间的距离的平方成反比。这个定律被称为万有引力定律，它描述了质量和距离之间的关系。它表明，两个物体之间的引力与它们的质量成正比，与它们之间的距离的平方成反比。这个定律对于解释宇宙中的许多现象和现象之间的相互作用非常重要，例如行星绕太阳运动、恒星之间的引力透镜效应等等。它也被广泛应用于天文学、物理学和工程学等领域，为我们理解宇宙的本质和演化提供了重要的理论基础。


[Q]: 你知道大熊猫的主要食物是什么吗？
[A]: 大熊猫主要以竹子为食，但也会吃其他植物、昆虫、小型哺乳动物和鱼类。它们的消化系统与其他熊科动物类似，能够消化竹子。大熊猫的牙齿锋利，适合咀嚼竹子，因此它们经常被称为“竹食动物”。
[B]: 大熊猫是以竹子为食的哺乳动物，它们是肉食动物。它们主要食物是竹子，特别是竹子中富含纤维素和矿物质，并且不会直接进入消化系统。大熊猫的肠道主要由竹子组成，它们需要消化竹子中的纤维素和其他纤维素来支持它们的正常进食。
[C]: 大熊猫主要以竹子为食，但它们也吃其他植物，包括树叶、嫩芽、昆虫、水果、蔬菜和小型哺乳动物等。大熊猫是中国特有的物种，属于熊科，主要分布在中国的四川、陕西和甘肃等地。
[D]: 大熊猫的主要食物是竹子。它们通常吃竹子，每天需要吃掉大约30公斤的竹子。
[E]: 是的，大熊猫主要以竹子为食。它们每天需要吃掉大量的竹子来维持生命，每天需要吃掉40至80公斤的竹子。此外，它们也会吃一些其他的植物，如草、果实、坚果和小型哺乳动物。
[F]: 大熊猫主要以竹子为食，也会偶尔食用其他的植物和动物。此外，大熊猫也会吃一些野果、小动物和植物，但这些都不属于它们的主要食物来源。
[G]: 大熊猫的主要食物是竹子。
[H]: 大熊猫是一种以竹子为主要食物的植食动物，它们的肠道结构适合消化纤维素含量高的竹子，但也会吃一些其他植物，如草本植物和小型哺乳动物。大熊猫的肠道结构也适应了高纤维的竹子，这使得它们能够消化大量的纤维素，从而帮助它们在野外生存。大熊猫的肠道中还含有一种叫做“纤维素酶”的酶类物质，这种酶能够帮助大熊猫分解纤维素，从而帮助它们消化大量的水分和营养物质。此外，大熊猫的肠道还含有一种叫做“纤维素蛋白”的蛋白质，这种蛋白质能够帮助大熊猫维持体内的能量水平，从而帮助它们在野外生存。总的来说，大熊猫的主要食物是竹子，但它们也会吃一些其他植物，如草本植物和小型哺乳动物。


[Q]: 海水为什么是咸的？
[A]: 海水之所以是咸的，主要是由于其中的化学成分。海水中含有大量的矿物质和盐分，其中主要成分是硫酸盐、碳酸盐、镁离子和钾离子。这些物质在高浓度下，会沉淀在海水中，形成海水。具体来说，海水中的矿物质和盐分主要来自于岩石、土壤、生物和人类排泄物等。岩石是海水的组成部分，主要由矿物质组成，如石英、长石、云母等。土壤中含有大量的化学物质，如氮、磷、钾等，这些物质会渗透到土壤中，形成土壤。生物体内的盐分主要来源于植物的叶绿素等色素，这些色素会吸收太阳光中的能量，转化为生物体可以利用的能量。而人类排泄物中含有的盐分主要来源于矿物质和碳酸盐，这些矿物质在水中的浓度较高，会导致水中溶解的矿物质浓度增加，形成水体中的盐分。因此，海水之所以是咸的，是由于其中的化学成分和结构特征。海水中的矿物质和盐分主要来自于岩石、土壤、生物和人类排泄物等，而这些物质的积累和排泄会导致海水呈现出咸味。
[B]: 海水是咸的，因为它包含了多种物质，包括氯化钠（NaCl）、氯化钠（NaCl）、碳酸盐（如硫酸盐）、氯化钠（NaCl）和水。咸物质的来源：盐是咸的常见来源，含有钾、钙、镁、钾等矿物质。这些矿物质通常来自于地下水、地下水或土壤中，这些矿物质的含量远低于纯净水。咸味的原因：盐的咸味主要源于其溶解于水中的微小颗粒物。这些小颗粒物主要通过植物根部和动物体内的微生物传播，如昆虫、小型哺乳动物、鸟类和某些昆虫。综上所述，海水是咸的，因为它含有大量的盐类，包括矿物质、岩石、盐、矿物质等，这些物质在海水中的浓度高，对咸味的产生较大影响。
[C]: 海水是咸的，主要是因为其中含有各种盐分，其中以海水为主要成分之一。当海水中含有大量的盐分时，盐分会被溶解在海水中，形成咸味。海水中的盐分主要来源于溶解盐类、蒸发析出的盐和海水冻结成冰等因素。
[D]: 海水中含有许多盐分，这些盐分可以导致海水咸度的增加。海水中的盐分主要来自于海水中的盐分，这些盐分可以从海水中的盐分中获得。海水中的氧化物和盐分可以与海水中的盐分形成氢键，从而使海水咸度增加。
[E]: 海水是咸的，这是因为它含有大量的盐分，其中包括氯离子和氯离子。这些离子在海洋中形成了大量的盐，这些盐分被称为盐分。海水中含有许多溶解在水中的盐，这些盐分会随着海水的升高而逐渐蒸发，最终导致海水中盐分的蒸发。这些盐分在海水中被完全消耗，然后被排除在海水中。此外，海水中还含有许多其他的盐分，但这些盐分是不同的，因为它们在水中的浮起和下降过程是不同的。海水中的盐分是从海底的岩石中提取出来的，这些盐分在海水中可以被利用。总的来说，海水是咸的，这也是为什么海水中含有许多盐分和盐分的原因。
[F]: 海水含有许多溶解的盐类，其中最主要的是氯化钠。当水中加入盐时，盐的分子会与水分子发生反应，形成氢氧化钠，这是一种盐。
[G]: 因为水是许多生物的生存所必需的，它能够吸收和释放二氧化碳，释放氧气，维持大气中的水蒸气，以及维持水循环。水中的盐类和化学物质会释放出氧气，使地球上的生物呼吸。此外，水还具有许多其他功能，如保持水循环、维持水循环、调节气候和环境等。水是许多生物生存所必需的，它也是许多自然现象的重要基础。
[H]: 海水是咸的主要原因是因为其中含有大量的溶解盐类，其中最主要的是氯化钠（NaCl）。氯化钠是一种非常常见的盐类，它存在于地球上的大部分地区，包括海洋、湖泊、河流和地下水。当水从海洋中蒸发时，它会带走一部分盐分，使海水变得咸。此外，海中还含有其他溶解盐类的物质，如硫酸盐、硫酸镁、硫酸钙、硫酸镁盐、硫酸镁钾、硫酸镁钠、硫酸镁钙、硫酸镁镁钙、硫酸镁镁钙、硫酸镁镁钙、硫酸镁镁钠、硫酸镁镁钙、硫酸镁镁镁钙、硫酸镁镁镁钙、硫酸镁镁镁钙、硫酸镁镁镁钙、硫酸镁镁镁钙、硫酸镁镁镁钙、硫酸镁镁镁钙、硫酸镁镁镁钙、硫酸镁镁镁钙、硫酸镁镁镁钙、硫酸镁镁镁镁钙、硫酸镁镁镁镁钙、硫酸镁镁镁镁钙、硫酸镁镁镁镁钙、硫酸镁镁镁镁钙、硫酸镁镁镁镁钙、硫酸镁镁镁镁镁钙、硫酸镁镁镁镁镁钙、硫酸镁镁镁镁镁钙、硫酸镁镁镁镁镁钙、硫酸镁镁镁镁镁镁钙、硫酸镁镁镁镁镁镁钙、硫酸镁镁镁镁镁镁钙、硫酸镁镁镁镁镁镁镁钙、硫酸镁镁镁镁


[Q]: 写一段python的快速排序代码
[A]: 好的，以下是Python的快速排序代码：def quick_sort(arr):    if len(arr) &amp;lt;= 1:        return arr    else:        pivot = arr[len(arr) // 2]        left = [x for x in arr[1:]]        right = [x for x in arr[2:] if x &amp;lt; pivot]        return quick_sort(left) + [pivot] + quick_sort(right)# 测试代码arr = [3, 6, 8, 10, 1, 2, 1]print(quick_sort(arr))这段代码首先检查数组的长度是否小于等于1。如果是，那么它就按照从小到大的顺序排列。接着，对于数组的每个元素，它想要进行递归地排序，直到达到排序的结束位置。如果数组的长度是奇数，那么它就按照从大到小的顺序排列。最后，平均的时间复杂度是O(n log n)，其中n是数组的长度。
[B]: 以下是一个使用Python的快速排序算法的实现：def sort_array(arr):    # 生成一个随机排列    sorted_array = sorted(arr)    # 返回排序后的列表    return sorted(arr)# 示例arr = [3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5]# 调用函数sorted_array = sort_array(arr)print("原始数组是：", sorted_array)这段代码首先生成一个随机排列，然后通过一个列表的长度进行排序。最后，返回排序后的列表。注意，这个代码使用了Python的内置sorted函数和内置的sorted()函数，它们在时间复杂度为O(n log n)的情况下，比直接使用sorted()函数更快。
[C]: 以下是一个简单的排序算法，适用于小型数组，包括小于等于5的数组。def quick_sort(arr):    if len(arr) &amp;lt;= 1:        return arr    else    for _ in range(2, 51):        if arr[len(arr) - 1:] is not None:            arr[len(arr) - 1] = arr[len(arr) - 1]                arr[len(arr)] = arr[len(arr)]    return quick_sort(arr)# 示例arr = [3, 5, 1, 4, 1, 2, 1]sorted_arr = quick_sort(arr)print("Sorted array is:", sorted_arr)### 代码解释：1. **函数定义**：quick_sort 函数接受一个数组 arr 作为参数。2. **循环**：对于 arr 的元素，使用递归来实现，即从右到左遍历数组，并将当前元素与数组的当前元素交换位置，直到数组达到空或数组长度小于等于 arr。3. **递归调用**：遍历数组，将当前元素与数组的当前元素进行交换，直到数组达到空或数组长度小于等于 arr。4. **检查数组长度**：使用 if __name__ == "__main__" 检查数组的长度是否小于 arr，如果是，就将数组从较大的数组移动到数组的末尾。5. **输出排序后的结果**
[D]: 作为一个AI语言模型，我无法编写Python程序。但是，Python是一个非常流行的编程语言，它具有许多优秀的库和框架，包括NumPy、Pandas、Matplotlib、Pandas、Bandke、Norm和NKlearn等。Python有很多优秀的自然语言处理库和框架，可以用于处理和分析各种类型的数据，如文本、图像、音频、视频等。Python还可以用于创建各种应用程序，如网页搜索、游戏开发、科学计算等。Python非常灵活，也支持多种编程风格，因此可以轻松地找到适合自己的解决方案。
[E]: 作为AI语言模型，我可以回答您的问题。但是我无法进行编程操作，因为我无法确定您想问的是哪种编程语言。请您提供更具体的问题，以便我能够更好地回答您的问题。
[F]: 快速排序（Quick Sort）是一种简单的排序算法，它通过将数据划分为小组，每组数据按照一定规则排序，最终得到一个有序数组。
[G]: 好的，以下是一段快速排序的Python代码```python sorted_print(sorted(sorted(sorted(sorted_print))```输出结果为：```[1, 2, 3, 4, 5, 6, 7, 8, 9]```
[H]: 快速排序是一种基于分治思想的高效排序算法，它的时间复杂度为 O(n log n)。它的基本思想是通过一趟排序将待排序序列划分为两个子序列，其中一个子序列的所有元素都比另一个子序列的所有元素小，然后对这两个子序列分别递归地进行排序。下面是一个 Python 代码示例，实现了快速排序算法：```pythondef quick_sort(arr):    if len(arr) &amp;gt; 1:        return arr    else: return quick_sort(arr)```该代码使用了递归的方式，首先判断数组的长度是否小于等于1，如果是，则直接返回数组，否则递归地对数组进行排序。在递归过程中，我们首先判断数组的第一个元素是否大于等于1，如果是，则将数组中的元素与第一个元素进行比较，如果不等于1，则将数组中的元素与第二个元素进行比较，以此类推，直到所有元素都被排序。该代码的时间复杂度为 O(n log n)，是一个非常实用的排序算法。


[Q]: 截至2025年2月9日14时31分，电影《哪吒之魔童闹海》累计票房（含预售）突破78.09亿元，超过《星球大战：天行者崛起》的票房成绩，成为首部进入全球票房榜前40的亚洲电影。这一成就不仅标志着中国电影在国际市场的突破，也展示了中国动画电影的巨大潜力和市场吸引力。从这段文字总结20字的摘要给我。
[A]: 电影《哪吒之魔童闹海》票房突破78.09亿元，成为首部进入全球票房榜前40的亚洲电影。这标志着中国电影的国际突破，也展现了中国动画电影的巨大潜力。
[B]: 2025年2月9日，中国电影在世界舞台上突破78.09亿元，超过《星球大战：天行者崛起》的票房成绩。
[C]: 电影《哪吒之魔童闹海》票房突破78.09亿元，成为全球票房榜前40的亚洲电影，展现中国动画电影的巨大潜力。
[D]: 《哪吒之魔童闹海》累计票房突破78.09亿，成为首部进入全球票房榜前40的亚洲电影，展示了中国电影在国际市场的突破，同时展示了中国动画电影的巨大潜力和市场吸引力。
[E]: 《猛龙强：打开世界奇幻冒险》成功地，上海成功吸引了众多观众，同时也引起了广泛的关注。
[F]: 《哪吒之魔童闹海》是一部让中国动画电影成为国际市场的巨大潜力。该电影以其华丽的画面和惊险的战斗场面而闻名于世。
[G]: 《哪行是值得的》是2021年1月9日14时31分，电影《哪成大作为》在电影市场突破7800亿美元，成为中国动画电影的突破，并展示中国动画电影的巨大潜力和市场吸引力。
[H]: 我是一个参数很少的AI模型🥺，知识库较少，无法直接回答您的问题，换个问题试试吧👋
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;p&gt;🙋‍直接把以上所有问题和模型的回答丢给DeepSeek-R1，让它帮忙点评和排名打分：&lt;/p&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;具体点评&lt;/summary&gt; 
 &lt;h3&gt;评分标准：&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;准确性&lt;/strong&gt;：回答是否正确且无明显错误。&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;完整性&lt;/strong&gt;：回答是否涵盖了问题的核心要点。&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;逻辑性&lt;/strong&gt;：回答是否有条理，是否符合逻辑。&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;代码质量&lt;/strong&gt;：代码是否能正常运行，逻辑是否清晰。&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;点评：&lt;/h3&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;A模型&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;优点&lt;/strong&gt;：回答非常全面，信息量大，逻辑清晰，尤其在长江、大熊猫、海水咸味等问题上表现优异。代码虽然有小瑕疵，但整体思路正确。&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;缺点&lt;/strong&gt;：部分回答稍显冗长，但不影响整体质量。&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;总评&lt;/strong&gt;：综合表现最佳，得分最高。&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;H模型&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;优点&lt;/strong&gt;：回答较为准确，尤其在珠穆朗玛峰、万有引力等问题上表现出色。代码虽未完全展示，但解释较为详细。&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;缺点&lt;/strong&gt;：部分回答略显啰嗦，但逻辑性较强。&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;总评&lt;/strong&gt;：仅次于A模型，表现稳定。&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;C模型&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;优点&lt;/strong&gt;：回答简洁明了，尤其在大熊猫、快速排序代码等问题上表现较好。&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;缺点&lt;/strong&gt;：部分回答略显简短，缺乏深入解释。&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;总评&lt;/strong&gt;：整体表现不错，但细节上略逊于A和H。&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;F模型&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;优点&lt;/strong&gt;：回答较为准确，尤其在长江、万有引力等问题上表现尚可。代码部分有一定逻辑性。&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;缺点&lt;/strong&gt;：部分回答不够深入，代码存在一些小问题。&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;总评&lt;/strong&gt;：表现中规中矩，仍有提升空间。&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;D模型&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;优点&lt;/strong&gt;：回答基本准确，尤其在万有引力、长江等问题上表现尚可。&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;缺点&lt;/strong&gt;：部分回答过于简略，代码存在明显错误。&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;总评&lt;/strong&gt;：表现一般，需改进代码部分。&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;B模型&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;优点&lt;/strong&gt;：回答较为准确，尤其在长江、海水咸味等问题上表现尚可。&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;缺点&lt;/strong&gt;：部分回答逻辑性较差，代码存在较大问题。&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;总评&lt;/strong&gt;：表现平平，需进一步优化。&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;E模型&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;优点&lt;/strong&gt;：部分回答较为准确，尤其在海水咸味、大熊猫等问题上表现尚可。&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;缺点&lt;/strong&gt;：回答过于简略，代码部分几乎无法运行。&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;总评&lt;/strong&gt;：表现不佳，需大幅提升。&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;G模型&lt;/strong&gt;：&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;strong&gt;优点&lt;/strong&gt;：几乎没有明显的优点。&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;缺点&lt;/strong&gt;：回答严重偏离主题，代码部分完全无法运行。&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;总评&lt;/strong&gt;：表现最差，需大幅改进。&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;hr /&gt; 
 &lt;h3&gt;总结：&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;A模型&lt;/strong&gt;在各方面表现最为出色，尤其在复杂问题的回答上展现了极高的准确性与逻辑性。&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;H模型&lt;/strong&gt;紧随其后，表现稳定，但在某些细节上略显不足。&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;G模型&lt;/strong&gt;表现最差，回答偏离主题且代码无法运行，需大幅改进。&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;打分排序&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;排名&lt;/th&gt; 
   &lt;th&gt;模型&lt;/th&gt; 
   &lt;th&gt;准确性 (30分)&lt;/th&gt; 
   &lt;th&gt;完整性 (30分)&lt;/th&gt; 
   &lt;th&gt;逻辑性 (20分)&lt;/th&gt; 
   &lt;th&gt;代码质量 (20分)&lt;/th&gt; 
   &lt;th&gt;总分 (100分)&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;1&lt;/td&gt; 
   &lt;td&gt;A&lt;/td&gt; 
   &lt;td&gt;28&lt;/td&gt; 
   &lt;td&gt;29&lt;/td&gt; 
   &lt;td&gt;19&lt;/td&gt; 
   &lt;td&gt;20&lt;/td&gt; 
   &lt;td&gt;96&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;H&lt;/td&gt; 
   &lt;td&gt;27&lt;/td&gt; 
   &lt;td&gt;28&lt;/td&gt; 
   &lt;td&gt;18&lt;/td&gt; 
   &lt;td&gt;20&lt;/td&gt; 
   &lt;td&gt;93&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;3&lt;/td&gt; 
   &lt;td&gt;C&lt;/td&gt; 
   &lt;td&gt;26&lt;/td&gt; 
   &lt;td&gt;27&lt;/td&gt; 
   &lt;td&gt;18&lt;/td&gt; 
   &lt;td&gt;18&lt;/td&gt; 
   &lt;td&gt;89&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;4&lt;/td&gt; 
   &lt;td&gt;F&lt;/td&gt; 
   &lt;td&gt;25&lt;/td&gt; 
   &lt;td&gt;26&lt;/td&gt; 
   &lt;td&gt;17&lt;/td&gt; 
   &lt;td&gt;18&lt;/td&gt; 
   &lt;td&gt;86&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;D&lt;/td&gt; 
   &lt;td&gt;24&lt;/td&gt; 
   &lt;td&gt;25&lt;/td&gt; 
   &lt;td&gt;17&lt;/td&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;82&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;6&lt;/td&gt; 
   &lt;td&gt;B&lt;/td&gt; 
   &lt;td&gt;23&lt;/td&gt; 
   &lt;td&gt;24&lt;/td&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;15&lt;/td&gt; 
   &lt;td&gt;78&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;7&lt;/td&gt; 
   &lt;td&gt;E&lt;/td&gt; 
   &lt;td&gt;22&lt;/td&gt; 
   &lt;td&gt;23&lt;/td&gt; 
   &lt;td&gt;15&lt;/td&gt; 
   &lt;td&gt;14&lt;/td&gt; 
   &lt;td&gt;74&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;G&lt;/td&gt; 
   &lt;td&gt;10&lt;/td&gt; 
   &lt;td&gt;12&lt;/td&gt; 
   &lt;td&gt;10&lt;/td&gt; 
   &lt;td&gt;10&lt;/td&gt; 
   &lt;td&gt;42&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;👉主观效果总结&lt;/h3&gt; 
&lt;p&gt;个人主观评价与DeepSeek-R1基本相符，其中：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;MiniMind系列的排序非常符合直觉，参数越大+训练数据越充分评分越高，幻觉和错误都会比小模型肉眼可见的好。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;H模型的回答肉眼看起来是不错的，尽管存在些许幻觉瞎编的情况。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;G模型可能训练数据不够完备，给出的权重经过测试效果不佳。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;再复诵一遍经久不衰的Scaling Law: 参数越大，训练数据越多模型的性能越强。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Ⅳ RoPE长度外推&lt;/h2&gt; 
&lt;p&gt;MiniMind支持通过YaRN算法进行RoPE位置编码的长度外推，使模型能够处理超出训练长度的文本序列。 在使用&lt;code&gt;eval_llm.py&lt;/code&gt;进行推理时，只需添加&lt;code&gt;--inference_rope_scaling&lt;/code&gt;参数即可启用RoPE外推：&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python eval_llm.py --weight full_sft --inference_rope_scaling
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;下图展示了在不同文本「西游记」白话文小说长度下，使用RoPE scaling前后的困惑度(PPL)对比。可以看出，启用RoPE scaling后，模型在长文本上的表现显著提升：&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/rope_ppl.png" /&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Ⅴ Objective Benchmark&lt;/h2&gt; 
&lt;p&gt;下面就到喜闻乐见的benchmark测试环节，就不找乐子和Qwen、GLM级别的模型做对比了。 这里选取了一些微型模型进行横评比较， 测试集选择C-Eval、CMMLU、A-CLUE、TMMLU+这几个纯中文语言榜单。&lt;/p&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;测评框架&lt;/summary&gt; 
 &lt;p&gt;测评框架选择&lt;a href="https://github.com/EleutherAI/lm-evaluation-harness"&gt;lm-evaluation&lt;/a&gt;， 安装后启动测试非常方便：&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;lm_eval --model hf --model_args pretrained=&amp;lt;填写模型路径&amp;gt;,device=cuda,dtype=auto --tasks ceval* --batch_size 8 --trust_remote_code
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p&gt;PS: 在这种全是选择题的测评集中，为了避免回复格式的难以固定的特点， 所以常用做法是直接把&lt;code&gt;A&lt;/code&gt;,&lt;code&gt;B&lt;/code&gt;,&lt;code&gt;C&lt;/code&gt;,&lt;code&gt;D&lt;/code&gt;四个字母对应token的预测概率取出来，将其中概率最大的字母与标准答案计算正确率。 选择题1/4乱选的正确率是25%，然而这个量级的所有模型都集中在25附近，甚至很多时候不如瞎选，是不是像极了高中完形填空的滑铁卢正确率... MiniMind模型本身预训练数据集小的可怜，也没有针对性的对测试集做刷榜微调，因此结果纯娱乐：&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;models&lt;/th&gt; 
   &lt;th&gt;from&lt;/th&gt; 
   &lt;th&gt;params↓&lt;/th&gt; 
   &lt;th&gt;ceval↑&lt;/th&gt; 
   &lt;th&gt;cmmlu↑&lt;/th&gt; 
   &lt;th&gt;aclue↑&lt;/th&gt; 
   &lt;th&gt;tmmlu+↑&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniMind2&lt;/td&gt; 
   &lt;td&gt;JingyaoGong&lt;/td&gt; 
   &lt;td&gt;104M&lt;/td&gt; 
   &lt;td&gt;26.52&lt;/td&gt; 
   &lt;td&gt;24.42&lt;/td&gt; 
   &lt;td&gt;24.97&lt;/td&gt; 
   &lt;td&gt;25.27&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniMind2-Small&lt;/td&gt; 
   &lt;td&gt;JingyaoGong&lt;/td&gt; 
   &lt;td&gt;26M&lt;/td&gt; 
   &lt;td&gt;26.37&lt;/td&gt; 
   &lt;td&gt;24.97&lt;/td&gt; 
   &lt;td&gt;25.39&lt;/td&gt; 
   &lt;td&gt;24.63&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniMind2-MoE&lt;/td&gt; 
   &lt;td&gt;JingyaoGong&lt;/td&gt; 
   &lt;td&gt;145M&lt;/td&gt; 
   &lt;td&gt;26.6&lt;/td&gt; 
   &lt;td&gt;25.01&lt;/td&gt; 
   &lt;td&gt;24.83&lt;/td&gt; 
   &lt;td&gt;25.01&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/zhanshijinwat/Steel-LLM"&gt;Steel-LLM&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ZhanShiJin&lt;/td&gt; 
   &lt;td&gt;1121M&lt;/td&gt; 
   &lt;td&gt;24.81&lt;/td&gt; 
   &lt;td&gt;25.32&lt;/td&gt; 
   &lt;td&gt;26&lt;/td&gt; 
   &lt;td&gt;24.39&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/openai-community/gpt2-medium"&gt;GPT2-medium&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;OpenAI&lt;/td&gt; 
   &lt;td&gt;360M&lt;/td&gt; 
   &lt;td&gt;23.18&lt;/td&gt; 
   &lt;td&gt;25&lt;/td&gt; 
   &lt;td&gt;18.6&lt;/td&gt; 
   &lt;td&gt;25.19&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/jzhang38/TinyLlama"&gt;TinyLlama-1.1B-Chat-V1.0&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;TinyLlama&lt;/td&gt; 
   &lt;td&gt;1100M&lt;/td&gt; 
   &lt;td&gt;25.48&lt;/td&gt; 
   &lt;td&gt;25&lt;/td&gt; 
   &lt;td&gt;25.4&lt;/td&gt; 
   &lt;td&gt;25.13&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/huggingface/smollm"&gt;SmolLM2&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;HuggingFaceTB&lt;/td&gt; 
   &lt;td&gt;135M&lt;/td&gt; 
   &lt;td&gt;24.37&lt;/td&gt; 
   &lt;td&gt;25.02&lt;/td&gt; 
   &lt;td&gt;25.37&lt;/td&gt; 
   &lt;td&gt;25.06&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.modelscope.cn/models/BAAI/Aquila-135M-Instruct"&gt;Aquila-Instruct&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;BAAI&lt;/td&gt; 
   &lt;td&gt;135M&lt;/td&gt; 
   &lt;td&gt;25.11&lt;/td&gt; 
   &lt;td&gt;25.1&lt;/td&gt; 
   &lt;td&gt;24.43&lt;/td&gt; 
   &lt;td&gt;25.05&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind/master/images/compare_radar.png" alt="compare_radar" /&gt;&lt;/p&gt; 
&lt;h1&gt;📌 Others&lt;/h1&gt; 
&lt;h2&gt;模型转换&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/jingyaogong/minimind/master/scripts/convert_model.py"&gt;./scripts/convert_model.py&lt;/a&gt;可以实现&lt;code&gt;torch / transformers&lt;/code&gt;模型的互相转换&lt;/li&gt; 
 &lt;li&gt;如无特别说明，&lt;code&gt;MiniMind2&lt;/code&gt;模型均默认为&lt;code&gt;Transformers&lt;/code&gt;格式的模型，需提前&lt;code&gt;t2t&lt;/code&gt;转换！&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;基于MiniMind-API服务接口&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/jingyaogong/minimind/master/scripts/serve_openai_api.py"&gt;./scripts/serve_openai_api.py&lt;/a&gt;完成了兼容openai-api的最简聊天接口，方便将自己的模型接入第三方UI 例如FastGPT、OpenWebUI、Dify等等。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;从&lt;a href="https://huggingface.co/collections/jingyaogong/minimind-66caf8d999f5c7fa64f399e5"&gt;Huggingface&lt;/a&gt;下载模型权重文件，文件树：&lt;/p&gt; &lt;pre&gt;&lt;code&gt;minimind (root dir)
├─&amp;lt;MiniMind-Model-Name&amp;gt;（例如MiniMind2）
|  ├── config.json
|  ├── generation_config.json
|  ├── model_minimind.py or w/o
|  ├── pytorch_model.bin or model.safetensors
|  ├── special_tokens_map.json
|  ├── tokenizer_config.json
|  ├── tokenizer.json
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;启动聊天服务端&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;python serve_openai_api.py
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;测试服务接口&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;python chat_openai_api.py
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;API接口示例，兼容openai api格式&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;curl http://ip:port/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{ 
    "model": "model-identifier",
    "messages": [ 
      { "role": "user", "content": "世界上最高的山是什么？" }
    ], 
    "temperature": 0.7, 
    "max_tokens": 512,
    "stream": true
}'
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;img src="https://avatars.githubusercontent.com/u/136984999" height="28" style="vertical-align: middle;" /&gt; &lt;a href="https://github.com/vllm-project/vllm"&gt;vllm&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;vLLM是极其流行的高效推理框架，支持大模型快速部署，优化显存利用与吞吐量。&lt;/p&gt; 
&lt;p&gt;以openai-serve形式启动 minimind2：&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;vllm serve ./MiniMind2 --model-impl transformers --served-model-name "minimind" --port 8998
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;&lt;img src="https://user-images.githubusercontent.com/1991296/230134379-7181e485-c521-4d23-a0d6-f7b3b61ba524.png" height="28" style="vertical-align: middle;" /&gt; &lt;a href="https://github.com/ggerganov/llama.cpp"&gt;llama.cpp&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;llama.cpp是一个C++库， 可以在命令行下直接使用，支持多线程推理，支持GPU加速。&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;目录结构&lt;/strong&gt;：建议将llama.cpp与minimind放在同级目录下&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;parent/
├── minimind/          # MiniMind项目目录
│   ├── MiniMind2/     # HuggingFace格式MiniMind2模型 (先convert_model.py生成)
│   │   ├── config.json
│   │   ├── model.safetensors
│   │   └── ...
│   ├── model/
│   ├── trainer/
│   └── ...
└── llama.cpp/         # llama.cpp项目目录
    ├── build/
    ├── convert_hf_to_gguf.py
    └── ...
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;0、参考&lt;code&gt;llama.cpp&lt;/code&gt;官方步骤进行install&lt;/p&gt; 
&lt;p&gt;1、在&lt;code&gt;convert_hf_to_gguf.py&lt;/code&gt;的&lt;code&gt;get_vocab_base_pre&lt;/code&gt;函数最后插入：&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# 添加MiniMind tokenizer支持（这里随便写一个例如qwen2即可）
if res is None:
    res = "qwen2"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;2、转换自训练的minimind模型：huggingface -&amp;gt; gguf&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 在llama.cpp下执行，将生成../minimind/MiniMind2/MiniMind2-xxx.gguf
python convert_hf_to_gguf.py ../minimind/MiniMind2
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;3、量化此模型 (可选)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;./build/bin/llama-quantize ../minimind/MiniMind2/MiniMind2.gguf ../minimind/MiniMind2/Q4-MiniMind2.gguf Q4_K_M
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;4、命令行推理测试&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;./build/bin/llama-cli -m ../minimind/MiniMind2/MiniMind2.gguf -sys "You are a helpful assistant" # system prompt必须固定
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;&lt;img src="https://ollama.com/public/cloud.png" height="28" style="vertical-align: middle;" /&gt; &lt;a href="https://ollama.ai"&gt;ollama&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;ollama是本地运行大模型的工具，支持多种开源LLM，简单易用。&lt;/p&gt; 
&lt;p&gt;1、通过ollama加载自定义的gguf模型&lt;/p&gt; 
&lt;p&gt;在&lt;code&gt;MiniMind2&lt;/code&gt;下新建&lt;code&gt;minimind.modelfile&lt;/code&gt;，写入：&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;FROM ./Q4-MiniMind2.gguf

SYSTEM """You are a helpful assistant"""

TEMPLATE """&amp;lt;|im_start|&amp;gt;system
{{ .System }}&amp;lt;|im_end|&amp;gt;
&amp;lt;|im_start|&amp;gt;user
{{ .Prompt }}&amp;lt;|im_end|&amp;gt;
&amp;lt;|im_start|&amp;gt;assistant
{{ .Response }}&amp;lt;|im_end|&amp;gt;
"""
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;2、加载并命名此模型为&lt;code&gt;minimind-local&lt;/code&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;ollama create -f minimind.modelfile minimind-local
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;3、启动推理&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;ollama run minimind-local
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;📤 推送你的模型到 Ollama Hub&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# 1. 为本地模型重命名为你的ollama-account/minimind的tag
ollama cp minimind-local:latest your_username/minimind:latest

# 2. 推送模型
ollama push your_username/minimind:latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;br /&gt; 
&lt;p&gt;⭐️ 也可直接使用我提供的ollama模型一键启动：&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;ollama run jingyaogong/minimind2 # 其他可选 minimind2-r1 / minimind2-small / minimind2-small-r1
&amp;gt;&amp;gt;&amp;gt; 你叫什么名字
我是一个语言模型...
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;以上三方框架的更多用法请参考对应官方文档😊&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h1&gt;📌 Acknowledge&lt;/h1&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] 如果觉得&lt;code&gt;MiniMind系列&lt;/code&gt;对您有所帮助，可以在 GitHub 上加一个⭐&lt;br /&gt; 篇幅超长水平有限难免纰漏，欢迎在Issues交流指正或提交PR改进项目&lt;br /&gt; 您的小小支持就是持续改进此项目的动力！&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;🤝&lt;a href="https://github.com/jingyaogong/minimind/graphs/contributors"&gt;贡献者&lt;/a&gt;&lt;/h2&gt; 
&lt;a href="https://github.com/jingyaogong/minimind/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=jingyaogong/minimind" /&gt; &lt;/a&gt; 
&lt;h2&gt;😊鸣谢&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/ipfgao"&gt;&lt;b&gt;@ipfgao&lt;/b&gt;&lt;/a&gt;: &lt;a href="https://github.com/jingyaogong/minimind/issues/26"&gt;🔗训练步骤记录&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/WangRongsheng"&gt;&lt;b&gt;@WangRongsheng&lt;/b&gt;&lt;/a&gt;: &lt;a href="https://github.com/jingyaogong/minimind/issues/39"&gt;🔗大型数据集预处理&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/pengqianhan"&gt;&lt;b&gt;@pengqianhan&lt;/b&gt;&lt;/a&gt;: &lt;a href="https://github.com/jingyaogong/minimind/issues/73"&gt;🔗一个简明教程&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/RyanSunn"&gt;&lt;b&gt;@RyanSunn&lt;/b&gt;&lt;/a&gt;: &lt;a href="https://github.com/jingyaogong/minimind/issues/75"&gt;🔗推理过程学习记录&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/Nijikadesu"&gt;&lt;b&gt;@Nijikadesu&lt;/b&gt;&lt;/a&gt;: &lt;a href="https://github.com/jingyaogong/minimind/issues/213"&gt;🔗以交互笔记本方式分解项目代码&lt;/a&gt;&lt;/p&gt; 
&lt;details close&gt; 
 &lt;summary&gt; &lt;b&gt;参考链接 &amp;amp; 感谢以下优秀的论文或项目&lt;/b&gt; &lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;排名不分任何先后顺序&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/meta-llama/llama3"&gt;https://github.com/meta-llama/llama3&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/karpathy/llama2.c"&gt;https://github.com/karpathy/llama2.c&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/DLLXW/baby-llama2-chinese"&gt;https://github.com/DLLXW/baby-llama2-chinese&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://arxiv.org/abs/2405.04434"&gt;(DeepSeek-V2)https://arxiv.org/abs/2405.04434&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/charent/ChatLM-mini-Chinese"&gt;https://github.com/charent/ChatLM-mini-Chinese&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/wdndev/tiny-llm-zh"&gt;https://github.com/wdndev/tiny-llm-zh&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://arxiv.org/pdf/2401.04088"&gt;(Mistral-MoE)https://arxiv.org/pdf/2401.04088&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/Tongjilibo/build_MiniLLM_from_scratch"&gt;https://github.com/Tongjilibo/build_MiniLLM_from_scratch&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/jzhang38/TinyLlama"&gt;https://github.com/jzhang38/TinyLlama&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/AI-Study-Han/Zero-Chatgpt"&gt;https://github.com/AI-Study-Han/Zero-Chatgpt&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/xusenlinzy/api-for-open-llm"&gt;https://github.com/xusenlinzy/api-for-open-llm&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/HqWu-HITCS/Awesome-Chinese-LLM"&gt;https://github.com/HqWu-HITCS/Awesome-Chinese-LLM&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;🫶支持者&lt;/h2&gt; 
&lt;a href="https://github.com/jingyaogong/minimind/stargazers"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://reporoster.com/stars/dark/jingyaogong/minimind" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://reporoster.com/stars/jingyaogong/minimind" /&gt; 
  &lt;img alt="github contribution grid snake animation" src="https://reporoster.com/stars/jingyaogong/minimind" /&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;a href="https://github.com/jingyaogong/minimind/network/members"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://reporoster.com/forks/dark/jingyaogong/minimind" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://reporoster.com/forks/jingyaogong/minimind" /&gt; 
  &lt;img alt="github contribution grid snake animation" src="https://reporoster.com/forks/jingyaogong/minimind" /&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;picture&gt; 
 &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=jingyaogong/minimind&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
 &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=jingyaogong/minimind&amp;amp;type=Date" /&gt; 
 &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=jingyaogong/minimind&amp;amp;type=Date" /&gt; 
&lt;/picture&gt; 
&lt;h1&gt;License&lt;/h1&gt; 
&lt;p&gt;This repository is licensed under the &lt;a href="https://raw.githubusercontent.com/jingyaogong/minimind/master/LICENSE"&gt;Apache-2.0 License&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>LadybirdBrowser/ladybird</title>
      <link>https://github.com/LadybirdBrowser/ladybird</link>
      <description>&lt;p&gt;Truly independent web browser&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Ladybird&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://ladybird.org"&gt;Ladybird&lt;/a&gt; is a truly independent web browser, using a novel engine based on web standards.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] Ladybird is in a pre-alpha state, and only suitable for use by developers&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;p&gt;We aim to build a complete, usable browser for the modern web.&lt;/p&gt; 
&lt;p&gt;Ladybird uses a multi-process architecture with a main UI process, several WebContent renderer processes, an ImageDecoder process, and a RequestServer process.&lt;/p&gt; 
&lt;p&gt;Image decoding and network connections are done out of process to be more robust against malicious content. Each tab has its own renderer process, which is sandboxed from the rest of the system.&lt;/p&gt; 
&lt;p&gt;At the moment, many core library support components are inherited from SerenityOS:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;LibWeb: Web rendering engine&lt;/li&gt; 
 &lt;li&gt;LibJS: JavaScript engine&lt;/li&gt; 
 &lt;li&gt;LibWasm: WebAssembly implementation&lt;/li&gt; 
 &lt;li&gt;LibCrypto/LibTLS: Cryptography primitives and Transport Layer Security&lt;/li&gt; 
 &lt;li&gt;LibHTTP: HTTP/1.1 client&lt;/li&gt; 
 &lt;li&gt;LibGfx: 2D Graphics Library, Image Decoding and Rendering&lt;/li&gt; 
 &lt;li&gt;LibUnicode: Unicode and locale support&lt;/li&gt; 
 &lt;li&gt;LibMedia: Audio and video playback&lt;/li&gt; 
 &lt;li&gt;LibCore: Event loop, OS abstraction layer&lt;/li&gt; 
 &lt;li&gt;LibIPC: Inter-process communication&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;How do I build and run this?&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/LadybirdBrowser/ladybird/master/Documentation/BuildInstructionsLadybird.md"&gt;build instructions&lt;/a&gt; for information on how to build Ladybird.&lt;/p&gt; 
&lt;p&gt;Ladybird runs on Linux, macOS, Windows (with WSL2), and many other *Nixes.&lt;/p&gt; 
&lt;h2&gt;How do I read the documentation?&lt;/h2&gt; 
&lt;p&gt;Code-related documentation can be found in the &lt;a href="https://raw.githubusercontent.com/LadybirdBrowser/ladybird/master/Documentation/"&gt;documentation&lt;/a&gt; folder.&lt;/p&gt; 
&lt;h2&gt;Get in touch and participate!&lt;/h2&gt; 
&lt;p&gt;Join &lt;a href="https://discord.gg/nvfjVJ4Svh"&gt;our Discord server&lt;/a&gt; to participate in development discussion.&lt;/p&gt; 
&lt;p&gt;Please read &lt;a href="https://raw.githubusercontent.com/LadybirdBrowser/ladybird/master/Documentation/GettingStartedContributing.md"&gt;Getting started contributing&lt;/a&gt; if you plan to contribute to Ladybird for the first time.&lt;/p&gt; 
&lt;p&gt;Before opening an issue, please see the &lt;a href="https://raw.githubusercontent.com/LadybirdBrowser/ladybird/master/CONTRIBUTING.md#issue-policy"&gt;issue policy&lt;/a&gt; and the &lt;a href="https://raw.githubusercontent.com/LadybirdBrowser/ladybird/master/ISSUES.md"&gt;detailed issue-reporting guidelines&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The full contribution guidelines can be found in &lt;a href="https://raw.githubusercontent.com/LadybirdBrowser/ladybird/master/CONTRIBUTING.md"&gt;&lt;code&gt;CONTRIBUTING.md&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Ladybird is licensed under a 2-clause BSD license.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>lfnovo/open-notebook</title>
      <link>https://github.com/lfnovo/open-notebook</link>
      <description>&lt;p&gt;An Open Source implementation of Notebook LM with more flexibility and features&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a id="readme-top"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;!-- [![Contributors][contributors-shield]][contributors-url] --&gt; 
&lt;p&gt;&lt;a href="https://github.com/lfnovo/open-notebook/network/members"&gt;&lt;img src="https://img.shields.io/github/forks/lfnovo/open-notebook.svg?style=for-the-badge" alt="Forks" /&gt;&lt;/a&gt; &lt;a href="https://github.com/lfnovo/open-notebook/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/lfnovo/open-notebook.svg?style=for-the-badge" alt="Stargazers" /&gt;&lt;/a&gt; &lt;a href="https://github.com/lfnovo/open-notebook/issues"&gt;&lt;img src="https://img.shields.io/github/issues/lfnovo/open-notebook.svg?style=for-the-badge" alt="Issues" /&gt;&lt;/a&gt; &lt;a href="https://github.com/lfnovo/open-notebook/raw/master/LICENSE.txt"&gt;&lt;img src="https://img.shields.io/github/license/lfnovo/open-notebook.svg?style=for-the-badge" alt="MIT License" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;!-- [![LinkedIn][linkedin-shield]][linkedin-url] --&gt; 
&lt;!-- PROJECT LOGO --&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/lfnovo/open-notebook"&gt; &lt;img src="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/assets/hero.svg?sanitize=true" alt="Logo" /&gt; &lt;/a&gt; 
 &lt;h3 align="center"&gt;Open Notebook&lt;/h3&gt; 
 &lt;p align="center"&gt; An open source, privacy-focused alternative to Google's Notebook LM! &lt;br /&gt;&lt;strong&gt;Join our &lt;a href="https://discord.gg/37XJPXfz2w"&gt;Discord server&lt;/a&gt; for help, to share workflow ideas, and suggest features!&lt;/strong&gt; &lt;br /&gt; &lt;a href="https://www.open-notebook.ai"&gt;&lt;strong&gt;Checkout our website »&lt;/strong&gt;&lt;/a&gt; &lt;br /&gt; &lt;br /&gt; &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/index.md"&gt;📚 Get Started&lt;/a&gt; · &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/index.md"&gt;📖 User Guide&lt;/a&gt; · &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/features/index.md"&gt;✨ Features&lt;/a&gt; · &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/deployment/index.md"&gt;🚀 Deploy&lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;!-- Keep these links. Translations will automatically update with the README. --&gt; 
 &lt;a href="https://zdoc.app/de/lfnovo/open-notebook"&gt;Deutsch&lt;/a&gt; | 
 &lt;a href="https://zdoc.app/es/lfnovo/open-notebook"&gt;Español&lt;/a&gt; | 
 &lt;a href="https://zdoc.app/fr/lfnovo/open-notebook"&gt;français&lt;/a&gt; | 
 &lt;a href="https://zdoc.app/ja/lfnovo/open-notebook"&gt;日本語&lt;/a&gt; | 
 &lt;a href="https://zdoc.app/ko/lfnovo/open-notebook"&gt;한국어&lt;/a&gt; | 
 &lt;a href="https://zdoc.app/pt/lfnovo/open-notebook"&gt;Português&lt;/a&gt; | 
 &lt;a href="https://zdoc.app/ru/lfnovo/open-notebook"&gt;Русский&lt;/a&gt; | 
 &lt;a href="https://zdoc.app/zh/lfnovo/open-notebook"&gt;中文&lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;A private, multi-model, 100% local, full-featured alternative to Notebook LM&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/assets/asset_list.png" alt="New Notebook" /&gt;&lt;/p&gt; 
&lt;p&gt;In a world dominated by Artificial Intelligence, having the ability to think 🧠 and acquire new knowledge 💡, is a skill that should not be a privilege for a few, nor restricted to a single provider.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Open Notebook empowers you to:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;🔒 &lt;strong&gt;Control your data&lt;/strong&gt; - Keep your research private and secure&lt;/li&gt; 
 &lt;li&gt;🤖 &lt;strong&gt;Choose your AI models&lt;/strong&gt; - Support for 16+ providers including OpenAI, Anthropic, Ollama, LM Studio, and more&lt;/li&gt; 
 &lt;li&gt;📚 &lt;strong&gt;Organize multi-modal content&lt;/strong&gt; - PDFs, videos, audio, web pages, and more&lt;/li&gt; 
 &lt;li&gt;🎙️ &lt;strong&gt;Generate professional podcasts&lt;/strong&gt; - Advanced multi-speaker podcast generation&lt;/li&gt; 
 &lt;li&gt;🔍 &lt;strong&gt;Search intelligently&lt;/strong&gt; - Full-text and vector search across all your content&lt;/li&gt; 
 &lt;li&gt;💬 &lt;strong&gt;Chat with context&lt;/strong&gt; - AI conversations powered by your research&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Learn more about our project at &lt;a href="https://www.open-notebook.ai"&gt;https://www.open-notebook.ai&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;⚠️ IMPORTANT: v1.0 Breaking Changes&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;If you're upgrading from a previous version&lt;/strong&gt;, please note:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;🏷️ &lt;strong&gt;Docker tags have changed&lt;/strong&gt;: The &lt;code&gt;latest&lt;/code&gt; tag is now &lt;strong&gt;frozen&lt;/strong&gt; at the last Streamlit version&lt;/li&gt; 
 &lt;li&gt;🆕 &lt;strong&gt;Use &lt;code&gt;v1-latest&lt;/code&gt; tag&lt;/strong&gt; for the new React/Next.js version (recommended)&lt;/li&gt; 
 &lt;li&gt;🔌 &lt;strong&gt;Port 5055 required&lt;/strong&gt;: You must expose port 5055 for the API to work&lt;/li&gt; 
 &lt;li&gt;📖 &lt;strong&gt;Read the migration guide&lt;/strong&gt;: See &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/MIGRATION.md"&gt;MIGRATION.md&lt;/a&gt; for detailed upgrade instructions&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;New users&lt;/strong&gt;: You can ignore this notice and proceed with the Quick Start below using the &lt;code&gt;v1-latest-single&lt;/code&gt; tag.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;🆚 Open Notebook vs Google Notebook LM&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Feature&lt;/th&gt; 
   &lt;th&gt;Open Notebook&lt;/th&gt; 
   &lt;th&gt;Google Notebook LM&lt;/th&gt; 
   &lt;th&gt;Advantage&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Privacy &amp;amp; Control&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Self-hosted, your data&lt;/td&gt; 
   &lt;td&gt;Google cloud only&lt;/td&gt; 
   &lt;td&gt;Complete data sovereignty&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;AI Provider Choice&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;16+ providers (OpenAI, Anthropic, Ollama, LM Studio, etc.)&lt;/td&gt; 
   &lt;td&gt;Google models only&lt;/td&gt; 
   &lt;td&gt;Flexibility and cost optimization&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Podcast Speakers&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;1-4 speakers with custom profiles&lt;/td&gt; 
   &lt;td&gt;2 speakers only&lt;/td&gt; 
   &lt;td&gt;Extreme flexibility&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Context Control&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;3 granular levels&lt;/td&gt; 
   &lt;td&gt;All-or-nothing&lt;/td&gt; 
   &lt;td&gt;Privacy and performance tuning&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Content Transformations&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Custom and built-in&lt;/td&gt; 
   &lt;td&gt;Limited options&lt;/td&gt; 
   &lt;td&gt;Unlimited processing power&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;API Access&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Full REST API&lt;/td&gt; 
   &lt;td&gt;No API&lt;/td&gt; 
   &lt;td&gt;Complete automation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Deployment&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Docker, cloud, or local&lt;/td&gt; 
   &lt;td&gt;Google hosted only&lt;/td&gt; 
   &lt;td&gt;Deploy anywhere&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Citations&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Comprehensive with sources&lt;/td&gt; 
   &lt;td&gt;Basic references&lt;/td&gt; 
   &lt;td&gt;Research integrity&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Customization&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Open source, fully customizable&lt;/td&gt; 
   &lt;td&gt;Closed system&lt;/td&gt; 
   &lt;td&gt;Unlimited extensibility&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Cost&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Pay only for AI usage&lt;/td&gt; 
   &lt;td&gt;Monthly subscription + usage&lt;/td&gt; 
   &lt;td&gt;Transparent and controllable&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Why Choose Open Notebook?&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;🔒 &lt;strong&gt;Privacy First&lt;/strong&gt;: Your sensitive research stays completely private&lt;/li&gt; 
 &lt;li&gt;💰 &lt;strong&gt;Cost Control&lt;/strong&gt;: Choose cheaper AI providers or run locally with Ollama&lt;/li&gt; 
 &lt;li&gt;🎙️ &lt;strong&gt;Better Podcasts&lt;/strong&gt;: Full script control and multi-speaker flexibility vs limited 2-speaker deep-dive format&lt;/li&gt; 
 &lt;li&gt;🔧 &lt;strong&gt;Unlimited Customization&lt;/strong&gt;: Modify, extend, and integrate as needed&lt;/li&gt; 
 &lt;li&gt;🌐 &lt;strong&gt;No Vendor Lock-in&lt;/strong&gt;: Switch providers, deploy anywhere, own your data&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Built With&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://www.python.org/"&gt;&lt;img src="https://img.shields.io/badge/Python-3776AB?style=for-the-badge&amp;amp;logo=python&amp;amp;logoColor=white" alt="Python" /&gt;&lt;/a&gt; &lt;a href="https://nextjs.org/"&gt;&lt;img src="https://img.shields.io/badge/Next.js-000000?style=for-the-badge&amp;amp;logo=next.js&amp;amp;logoColor=white" alt="Next.js" /&gt;&lt;/a&gt; &lt;a href="https://reactjs.org/"&gt;&lt;img src="https://img.shields.io/badge/React-61DAFB?style=for-the-badge&amp;amp;logo=react&amp;amp;logoColor=black" alt="React" /&gt;&lt;/a&gt; &lt;a href="https://surrealdb.com/"&gt;&lt;img src="https://img.shields.io/badge/SurrealDB-FF5E00?style=for-the-badge&amp;amp;logo=databricks&amp;amp;logoColor=white" alt="SurrealDB" /&gt;&lt;/a&gt; &lt;a href="https://www.langchain.com/"&gt;&lt;img src="https://img.shields.io/badge/LangChain-3A3A3A?style=for-the-badge&amp;amp;logo=chainlink&amp;amp;logoColor=white" alt="LangChain" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;🚀 Quick Start&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Docker Images Available:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Docker Hub&lt;/strong&gt;: &lt;code&gt;lfnovo/open_notebook:v1-latest-single&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;GitHub Container Registry&lt;/strong&gt;: &lt;code&gt;ghcr.io/lfnovo/open-notebook:v1-latest-single&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Both registries contain identical images - choose whichever you prefer!&lt;/p&gt; 
&lt;h3&gt;Choose Your Setup:&lt;/h3&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td width="50%"&gt; &lt;h4&gt;🏠 &lt;strong&gt;Local Machine Setup&lt;/strong&gt;&lt;/h4&gt; &lt;p&gt;Perfect if Docker runs on the &lt;strong&gt;same computer&lt;/strong&gt; where you'll access Open Notebook.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;mkdir open-notebook &amp;amp;&amp;amp; cd open-notebook

docker run -d \
  --name open-notebook \
  -p 8502:8502 -p 5055:5055 \
  -v ./notebook_data:/app/data \
  -v ./surreal_data:/mydata \
  -e OPENAI_API_KEY=your_key_here \
  -e SURREAL_URL="ws://localhost:8000/rpc" \
  -e SURREAL_USER="root" \
  -e SURREAL_PASSWORD="root" \
  -e SURREAL_NAMESPACE="open_notebook" \
  -e SURREAL_DATABASE="production" \
  lfnovo/open_notebook:v1-latest-single
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Access at:&lt;/strong&gt; &lt;a href="http://localhost:8502"&gt;http://localhost:8502&lt;/a&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;h4&gt;🌐 &lt;strong&gt;Remote Server Setup&lt;/strong&gt;&lt;/h4&gt; &lt;p&gt;Use this for servers, Raspberry Pi, NAS, Proxmox, or any remote machine.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;mkdir open-notebook &amp;amp;&amp;amp; cd open-notebook

docker run -d \
  --name open-notebook \
  -p 8502:8502 -p 5055:5055 \
  -v ./notebook_data:/app/data \
  -v ./surreal_data:/mydata \
  -e OPENAI_API_KEY=your_key_here \
  -e API_URL=http://YOUR_SERVER_IP:5055 \
  -e SURREAL_URL="ws://localhost:8000/rpc" \
  -e SURREAL_USER="root" \
  -e SURREAL_PASSWORD="root" \
  -e SURREAL_NAMESPACE="open_notebook" \
  -e SURREAL_DATABASE="production" \
  lfnovo/open_notebook:v1-latest-single
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Replace &lt;code&gt;YOUR_SERVER_IP&lt;/code&gt;&lt;/strong&gt; with your server's IP (e.g., &lt;code&gt;192.168.1.100&lt;/code&gt;) or domain&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Access at:&lt;/strong&gt; http://YOUR_SERVER_IP:8502&lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;⚠️ Critical Setup Notes:&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Both ports are required:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Port 8502&lt;/strong&gt;: Web interface (what you see in your browser)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Port 5055&lt;/strong&gt;: API backend (required for the app to function)&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;API_URL must match how YOU access the server:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;✅ Access via &lt;code&gt;http://192.168.1.100:8502&lt;/code&gt; → set &lt;code&gt;API_URL=http://192.168.1.100:5055&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;✅ Access via &lt;code&gt;http://myserver.local:8502&lt;/code&gt; → set &lt;code&gt;API_URL=http://myserver.local:5055&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;❌ Don't use &lt;code&gt;localhost&lt;/code&gt; for remote servers - it won't work from other devices!&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Using Docker Compose (Recommended for Easy Management)&lt;/h3&gt; 
&lt;p&gt;Create a &lt;code&gt;docker-compose.yml&lt;/code&gt; file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;services:
  open_notebook:
    image: lfnovo/open_notebook:v1-latest-single
    # Or use: ghcr.io/lfnovo/open-notebook:v1-latest-single
    ports:
      - "8502:8502"  # Web UI
      - "5055:5055"  # API (required!)
    environment:
      - OPENAI_API_KEY=your_key_here
      # For remote access, uncomment and set your server IP/domain:
      # - API_URL=http://192.168.1.100:5055
      # Database connection (required for single-container)
      - SURREAL_URL=ws://localhost:8000/rpc
      - SURREAL_USER=root
      - SURREAL_PASSWORD=root
      - SURREAL_NAMESPACE=open_notebook
      - SURREAL_DATABASE=production
    volumes:
      - ./notebook_data:/app/data
      - ./surreal_data:/mydata
    restart: always
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Start with: &lt;code&gt;docker compose up -d&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;What gets created:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;open-notebook/
├── docker-compose.yml # Your configuration
├── notebook_data/     # Your notebooks and research content
└── surreal_data/      # Database files
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;🆘 Quick Troubleshooting&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Problem&lt;/th&gt; 
   &lt;th&gt;Solution&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;"Unable to connect to server"&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Set &lt;code&gt;API_URL&lt;/code&gt; environment variable to match how you access the server (see remote setup above)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Blank page or errors&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Ensure BOTH ports (8502 and 5055) are exposed in your docker command&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Works on server but not from other computers&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Don't use &lt;code&gt;localhost&lt;/code&gt; in &lt;code&gt;API_URL&lt;/code&gt; - use your server's actual IP address&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;"404" or "config endpoint" errors&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Don't add &lt;code&gt;/api&lt;/code&gt; to &lt;code&gt;API_URL&lt;/code&gt; - use just &lt;code&gt;http://your-ip:5055&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Still having issues?&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Check our &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/troubleshooting/quick-fixes.md"&gt;5-minute troubleshooting guide&lt;/a&gt; or &lt;a href="https://discord.gg/37XJPXfz2w"&gt;join Discord&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;How Open Notebook Works&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;┌─────────────────────────────────────────────────────────┐
│  Your Browser                                           │
│  Access: http://your-server-ip:8502                     │
└────────────────┬────────────────────────────────────────┘
                 │
                 ▼
         ┌───────────────┐
         │   Port 8502   │  ← Next.js Frontend (what you see)
         │   Frontend    │    Also proxies API requests internally!
         └───────┬───────┘
                 │ proxies /api/* requests ↓
                 ▼
         ┌───────────────┐
         │   Port 5055   │  ← FastAPI Backend (handles requests)
         │     API       │
         └───────┬───────┘
                 │
                 ▼
         ┌───────────────┐
         │   SurrealDB   │  ← Database (internal, auto-configured)
         │   (Port 8000) │
         └───────────────┘
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Key Points:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;v1.1+&lt;/strong&gt;: Next.js automatically proxies &lt;code&gt;/api/*&lt;/code&gt; requests to the backend, simplifying reverse proxy setup&lt;/li&gt; 
 &lt;li&gt;Your browser loads the frontend from port 8502&lt;/li&gt; 
 &lt;li&gt;The frontend needs to know where to find the API - when accessing remotely, set: &lt;code&gt;API_URL=http://your-server-ip:5055&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Behind reverse proxy?&lt;/strong&gt; You only need to proxy to port 8502 now! See &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/deployment/reverse-proxy.md"&gt;Reverse Proxy Guide&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.star-history.com/#lfnovo/open-notebook&amp;amp;type=date&amp;amp;legend=top-left"&gt;&lt;img src="https://api.star-history.com/svg?repos=lfnovo/open-notebook&amp;amp;type=date&amp;amp;legend=top-left" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;🛠️ Full Installation&lt;/h3&gt; 
&lt;p&gt;For development or customization:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/lfnovo/open-notebook
cd open-notebook
make start-all
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;📖 Need Help?&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;🤖 AI Installation Assistant&lt;/strong&gt;: We have a &lt;a href="https://chatgpt.com/g/g-68776e2765b48191bd1bae3f30212631-open-notebook-installation-assistant"&gt;CustomGPT built to help you install Open Notebook&lt;/a&gt; - it will guide you through each step!&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;New to Open Notebook?&lt;/strong&gt; Start with our &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/index.md"&gt;Getting Started Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Need installation help?&lt;/strong&gt; Check our &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/installation.md"&gt;Installation Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Want to see it in action?&lt;/strong&gt; Try our &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/quick-start.md"&gt;Quick Start Tutorial&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Provider Support Matrix&lt;/h2&gt; 
&lt;p&gt;Thanks to the &lt;a href="https://github.com/lfnovo/esperanto"&gt;Esperanto&lt;/a&gt; library, we support this providers out of the box!&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Provider&lt;/th&gt; 
   &lt;th&gt;LLM Support&lt;/th&gt; 
   &lt;th&gt;Embedding Support&lt;/th&gt; 
   &lt;th&gt;Speech-to-Text&lt;/th&gt; 
   &lt;th&gt;Text-to-Speech&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenAI&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Anthropic&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Groq&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Google (GenAI)&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Vertex AI&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ollama&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Perplexity&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ElevenLabs&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Azure OpenAI&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Mistral&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;DeepSeek&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Voyage&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;xAI&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenRouter&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenAI Compatible*&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;*Supports LM Studio and any OpenAI-compatible endpoint&lt;/p&gt; 
&lt;h2&gt;✨ Key Features&lt;/h2&gt; 
&lt;h3&gt;Core Capabilities&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;🔒 Privacy-First&lt;/strong&gt;: Your data stays under your control - no cloud dependencies&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🎯 Multi-Notebook Organization&lt;/strong&gt;: Manage multiple research projects seamlessly&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;📚 Universal Content Support&lt;/strong&gt;: PDFs, videos, audio, web pages, Office docs, and more&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🤖 Multi-Model AI Support&lt;/strong&gt;: 16+ providers including OpenAI, Anthropic, Ollama, Google, LM Studio, and more&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🎙️ Professional Podcast Generation&lt;/strong&gt;: Advanced multi-speaker podcasts with Episode Profiles&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🔍 Intelligent Search&lt;/strong&gt;: Full-text and vector search across all your content&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;💬 Context-Aware Chat&lt;/strong&gt;: AI conversations powered by your research materials&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;📝 AI-Assisted Notes&lt;/strong&gt;: Generate insights or write notes manually&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Advanced Features&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;⚡ Reasoning Model Support&lt;/strong&gt;: Full support for thinking models like DeepSeek-R1 and Qwen3&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🔧 Content Transformations&lt;/strong&gt;: Powerful customizable actions to summarize and extract insights&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🌐 Comprehensive REST API&lt;/strong&gt;: Full programmatic access for custom integrations &lt;a href="http://localhost:5055/docs"&gt;&lt;img src="https://img.shields.io/badge/API-Documentation-blue?style=flat-square" alt="API Docs" /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🔐 Optional Password Protection&lt;/strong&gt;: Secure public deployments with authentication&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;📊 Fine-Grained Context Control&lt;/strong&gt;: Choose exactly what to share with AI models&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;📎 Citations&lt;/strong&gt;: Get answers with proper source citations&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Three-Column Interface&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Sources&lt;/strong&gt;: Manage all your research materials&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Notes&lt;/strong&gt;: Create manual or AI-generated notes&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Chat&lt;/strong&gt;: Converse with AI using your content as context&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=D-760MlGwaI"&gt;&lt;img src="https://img.youtube.com/vi/D-760MlGwaI/0.jpg" alt="Check out our podcast sample" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;📚 Documentation&lt;/h2&gt; 
&lt;h3&gt;Getting Started&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/introduction.md"&gt;📖 Introduction&lt;/a&gt;&lt;/strong&gt; - Learn what Open Notebook offers&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/quick-start.md"&gt;⚡ Quick Start&lt;/a&gt;&lt;/strong&gt; - Get up and running in 5 minutes&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/installation.md"&gt;🔧 Installation&lt;/a&gt;&lt;/strong&gt; - Comprehensive setup guide&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/first-notebook.md"&gt;🎯 Your First Notebook&lt;/a&gt;&lt;/strong&gt; - Step-by-step tutorial&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;User Guide&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/interface-overview.md"&gt;📱 Interface Overview&lt;/a&gt;&lt;/strong&gt; - Understanding the layout&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/notebooks.md"&gt;📚 Notebooks&lt;/a&gt;&lt;/strong&gt; - Organizing your research&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/sources.md"&gt;📄 Sources&lt;/a&gt;&lt;/strong&gt; - Managing content types&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/notes.md"&gt;📝 Notes&lt;/a&gt;&lt;/strong&gt; - Creating and managing notes&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/chat.md"&gt;💬 Chat&lt;/a&gt;&lt;/strong&gt; - AI conversations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/search.md"&gt;🔍 Search&lt;/a&gt;&lt;/strong&gt; - Finding information&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Advanced Topics&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/features/podcasts.md"&gt;🎙️ Podcast Generation&lt;/a&gt;&lt;/strong&gt; - Create professional podcasts&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/features/transformations.md"&gt;🔧 Content Transformations&lt;/a&gt;&lt;/strong&gt; - Customize content processing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/features/ai-models.md"&gt;🤖 AI Models&lt;/a&gt;&lt;/strong&gt; - AI model configuration&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/development/api-reference.md"&gt;🔧 REST API Reference&lt;/a&gt;&lt;/strong&gt; - Complete API documentation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/deployment/security.md"&gt;🔐 Security&lt;/a&gt;&lt;/strong&gt; - Password protection and privacy&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/deployment/index.md"&gt;🚀 Deployment&lt;/a&gt;&lt;/strong&gt; - Complete deployment guides for all scenarios&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="right"&gt;(&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/#readme-top"&gt;back to top&lt;/a&gt;)&lt;/p&gt; 
&lt;h2&gt;🗺️ Roadmap&lt;/h2&gt; 
&lt;h3&gt;Upcoming Features&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Live Front-End Updates&lt;/strong&gt;: Real-time UI updates for smoother experience&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Async Processing&lt;/strong&gt;: Faster UI through asynchronous content processing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Cross-Notebook Sources&lt;/strong&gt;: Reuse research materials across projects&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Bookmark Integration&lt;/strong&gt;: Connect with your favorite bookmarking apps&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Recently Completed ✅&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Next.js Frontend&lt;/strong&gt;: Modern React-based frontend with improved performance&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Comprehensive REST API&lt;/strong&gt;: Full programmatic access to all functionality&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-Model Support&lt;/strong&gt;: 16+ AI providers including OpenAI, Anthropic, Ollama, LM Studio&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Advanced Podcast Generator&lt;/strong&gt;: Professional multi-speaker podcasts with Episode Profiles&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Content Transformations&lt;/strong&gt;: Powerful customizable actions for content processing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Enhanced Citations&lt;/strong&gt;: Improved layout and finer control for source citations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multiple Chat Sessions&lt;/strong&gt;: Manage different conversations within notebooks&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See the &lt;a href="https://github.com/lfnovo/open-notebook/issues"&gt;open issues&lt;/a&gt; for a full list of proposed features and known issues.&lt;/p&gt; 
&lt;p align="right"&gt;(&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/#readme-top"&gt;back to top&lt;/a&gt;)&lt;/p&gt; 
&lt;h2&gt;🤝 Community &amp;amp; Contributing&lt;/h2&gt; 
&lt;h3&gt;Join the Community&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;💬 &lt;strong&gt;&lt;a href="https://discord.gg/37XJPXfz2w"&gt;Discord Server&lt;/a&gt;&lt;/strong&gt; - Get help, share ideas, and connect with other users&lt;/li&gt; 
 &lt;li&gt;🐛 &lt;strong&gt;&lt;a href="https://github.com/lfnovo/open-notebook/issues"&gt;GitHub Issues&lt;/a&gt;&lt;/strong&gt; - Report bugs and request features&lt;/li&gt; 
 &lt;li&gt;⭐ &lt;strong&gt;Star this repo&lt;/strong&gt; - Show your support and help others discover Open Notebook&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Contributing&lt;/h3&gt; 
&lt;p&gt;We welcome contributions! We're especially looking for help with:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Frontend Development&lt;/strong&gt;: Help improve our modern Next.js/React UI&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Testing &amp;amp; Bug Fixes&lt;/strong&gt;: Make Open Notebook more robust&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Feature Development&lt;/strong&gt;: Build the coolest research tool together&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Documentation&lt;/strong&gt;: Improve guides and tutorials&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Current Tech Stack&lt;/strong&gt;: Python, FastAPI, Next.js, React, SurrealDB &lt;strong&gt;Future Roadmap&lt;/strong&gt;: Real-time updates, enhanced async processing&lt;/p&gt; 
&lt;p&gt;See our &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/CONTRIBUTING.md"&gt;Contributing Guide&lt;/a&gt; for detailed information on how to get started.&lt;/p&gt; 
&lt;p align="right"&gt;(&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/#readme-top"&gt;back to top&lt;/a&gt;)&lt;/p&gt; 
&lt;h2&gt;📄 License&lt;/h2&gt; 
&lt;p&gt;Open Notebook is MIT licensed. See the &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;h2&gt;📞 Contact&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Luis Novo&lt;/strong&gt; - &lt;a href="https://twitter.com/lfnovo"&gt;@lfnovo&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Community Support&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;💬 &lt;a href="https://discord.gg/37XJPXfz2w"&gt;Discord Server&lt;/a&gt; - Get help, share ideas, and connect with users&lt;/li&gt; 
 &lt;li&gt;🐛 &lt;a href="https://github.com/lfnovo/open-notebook/issues"&gt;GitHub Issues&lt;/a&gt; - Report bugs and request features&lt;/li&gt; 
 &lt;li&gt;🌐 &lt;a href="https://www.open-notebook.ai"&gt;Website&lt;/a&gt; - Learn more about the project&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🙏 Acknowledgments&lt;/h2&gt; 
&lt;p&gt;Open Notebook is built on the shoulders of amazing open-source projects:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/lfnovo/podcast-creator"&gt;Podcast Creator&lt;/a&gt;&lt;/strong&gt; - Advanced podcast generation capabilities&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/lfnovo/surreal-commands"&gt;Surreal Commands&lt;/a&gt;&lt;/strong&gt; - Background job processing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/lfnovo/content-core"&gt;Content Core&lt;/a&gt;&lt;/strong&gt; - Content processing and management&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/lfnovo/esperanto"&gt;Esperanto&lt;/a&gt;&lt;/strong&gt; - Multi-provider AI model abstraction&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/docling-project/docling"&gt;Docling&lt;/a&gt;&lt;/strong&gt; - Document processing and parsing&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="right"&gt;(&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/#readme-top"&gt;back to top&lt;/a&gt;)&lt;/p&gt; 
&lt;!-- MARKDOWN LINKS &amp; IMAGES --&gt; 
&lt;!-- https://www.markdownguide.org/basic-syntax/#reference-style-links --&gt;</description>
    </item>
    
    <item>
      <title>karpathy/micrograd</title>
      <link>https://github.com/karpathy/micrograd</link>
      <description>&lt;p&gt;A tiny scalar-valued autograd engine and a neural net library on top of it with PyTorch-like API&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;micrograd&lt;/h1&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/karpathy/micrograd/master/puppy.jpg" alt="awww" /&gt;&lt;/p&gt; 
&lt;p&gt;A tiny Autograd engine (with a bite! :)). Implements backpropagation (reverse-mode autodiff) over a dynamically built DAG and a small neural networks library on top of it with a PyTorch-like API. Both are tiny, with about 100 and 50 lines of code respectively. The DAG only operates over scalar values, so e.g. we chop up each neuron into all of its individual tiny adds and multiplies. However, this is enough to build up entire deep neural nets doing binary classification, as the demo notebook shows. Potentially useful for educational purposes.&lt;/p&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install micrograd
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Example usage&lt;/h3&gt; 
&lt;p&gt;Below is a slightly contrived example showing a number of possible supported operations:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from micrograd.engine import Value

a = Value(-4.0)
b = Value(2.0)
c = a + b
d = a * b + b**3
c += c + 1
c += 1 + c + (-a)
d += d * 2 + (b + a).relu()
d += 3 * d + (b - a).relu()
e = c - d
f = e**2
g = f / 2.0
g += 10.0 / f
print(f'{g.data:.4f}') # prints 24.7041, the outcome of this forward pass
g.backward()
print(f'{a.grad:.4f}') # prints 138.8338, i.e. the numerical value of dg/da
print(f'{b.grad:.4f}') # prints 645.5773, i.e. the numerical value of dg/db
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Training a neural net&lt;/h3&gt; 
&lt;p&gt;The notebook &lt;code&gt;demo.ipynb&lt;/code&gt; provides a full demo of training an 2-layer neural network (MLP) binary classifier. This is achieved by initializing a neural net from &lt;code&gt;micrograd.nn&lt;/code&gt; module, implementing a simple svm "max-margin" binary classification loss and using SGD for optimization. As shown in the notebook, using a 2-layer neural net with two 16-node hidden layers we achieve the following decision boundary on the moon dataset:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/karpathy/micrograd/master/moon_mlp.png" alt="2d neuron" /&gt;&lt;/p&gt; 
&lt;h3&gt;Tracing / visualization&lt;/h3&gt; 
&lt;p&gt;For added convenience, the notebook &lt;code&gt;trace_graph.ipynb&lt;/code&gt; produces graphviz visualizations. E.g. this one below is of a simple 2D neuron, arrived at by calling &lt;code&gt;draw_dot&lt;/code&gt; on the code below, and it shows both the data (left number in each node) and the gradient (right number in each node).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from micrograd import nn
n = nn.Neuron(2)
x = [Value(1.0), Value(-2.0)]
y = n(x)
dot = draw_dot(y)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/karpathy/micrograd/master/gout.svg?sanitize=true" alt="2d neuron" /&gt;&lt;/p&gt; 
&lt;h3&gt;Running tests&lt;/h3&gt; 
&lt;p&gt;To run the unit tests you will have to install &lt;a href="https://pytorch.org/"&gt;PyTorch&lt;/a&gt;, which the tests use as a reference for verifying the correctness of the calculated gradients. Then simply:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m pytest
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;License&lt;/h3&gt; 
&lt;p&gt;MIT&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>EbookFoundation/free-programming-books</title>
      <link>https://github.com/EbookFoundation/free-programming-books</link>
      <description>&lt;p&gt;📚 Freely available programming books&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;List of Free Learning Resources In Many Languages&lt;/h1&gt; 
&lt;div align="center" markdown="1"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/sindresorhus/awesome"&gt;&lt;img src="https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg?sanitize=true" alt="Awesome" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://creativecommons.org/licenses/by/4.0/"&gt;&lt;img src="https://img.shields.io/github/license/EbookFoundation/free-programming-books" alt="License: CC BY 4.0" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/pulls?q=is%3Apr+is%3Amerged+created%3A2025-10-01..2025-10-31"&gt;&lt;img src="https://img.shields.io/github/hacktoberfest/2025/EbookFoundation/free-programming-books?label=Hacktoberfest+2025" alt="Hacktoberfest 2025 stats" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;Search the list at &lt;a href="https://ebookfoundation.github.io/free-programming-books-search/"&gt;https://ebookfoundation.github.io/free-programming-books-search/&lt;/a&gt; &lt;a href="https://ebookfoundation.github.io/free-programming-books-search/"&gt;&lt;img src="https://img.shields.io/website?style=flat&amp;amp;logo=www&amp;amp;logoColor=whitesmoke&amp;amp;label=Dynamic%20search%20site&amp;amp;down_color=red&amp;amp;down_message=down&amp;amp;up_color=green&amp;amp;up_message=up&amp;amp;url=https%3A%2F%2Febookfoundation.github.io%2Ffree-programming-books-search%2F" alt="https://ebookfoundation.github.io/free-programming-books-search/" /&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;This page is available as an easy-to-read website. Access it by clicking on &lt;a href="https://ebookfoundation.github.io/free-programming-books/"&gt;&lt;img src="https://img.shields.io/website?style=flat&amp;amp;logo=www&amp;amp;logoColor=whitesmoke&amp;amp;label=Static%20site&amp;amp;down_color=red&amp;amp;down_message=down&amp;amp;up_color=green&amp;amp;up_message=up&amp;amp;url=https%3A%2F%2Febookfoundation.github.io%2Ffree-programming-books%2F" alt="https://ebookfoundation.github.io/free-programming-books/" /&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;form action="https://ebookfoundation.github.io/free-programming-books-search"&gt; 
  &lt;input type="text" id="fpbSearch" name="search" required placeholder="Search Book or Author" /&gt; 
  &lt;label for="submit"&gt; &lt;/label&gt; 
  &lt;input type="submit" id="submit" name="submit" value="Search" /&gt; 
 &lt;/form&gt; 
&lt;/div&gt; 
&lt;h2&gt;Intro&lt;/h2&gt; 
&lt;p&gt;This list was originally a clone of &lt;a href="https://web.archive.org/web/20140606191453/http://stackoverflow.com/questions/194812/list-of-freely-available-programming-books/392926"&gt;StackOverflow - List of Freely Available Programming Books&lt;/a&gt; with contributions from Karan Bhangui and George Stocker.&lt;/p&gt; 
&lt;p&gt;The list was moved to GitHub by Victor Felder for collaborative updating and maintenance. It has grown to become one of &lt;a href="https://octoverse.github.com/"&gt;GitHub's most popular repositories&lt;/a&gt;.&lt;/p&gt; 
&lt;div align="center" markdown="1"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/EbookFoundation/free-programming-books/network"&gt;&lt;img src="https://img.shields.io/github/forks/EbookFoundation/free-programming-books?style=flat&amp;amp;logo=github&amp;amp;logoColor=whitesmoke&amp;amp;label=Forks" alt="GitHub repo forks" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/EbookFoundation/free-programming-books?style=flat&amp;amp;logo=github&amp;amp;logoColor=whitesmoke&amp;amp;label=Stars" alt="GitHub repo stars" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/graphs/contributors"&gt;&lt;img src="https://img.shields.io/github/contributors-anon/EbookFoundation/free-programming-books?style=flat&amp;amp;logo=github&amp;amp;logoColor=whitesmoke&amp;amp;label=Contributors" alt="GitHub repo contributors" /&gt;&lt;/a&gt;&lt;br /&gt; &lt;a href="https://github.com/sponsors/EbookFoundation"&gt;&lt;img src="https://img.shields.io/github/sponsors/EbookFoundation?style=flat&amp;amp;logo=github&amp;amp;logoColor=whitesmoke&amp;amp;label=Sponsors" alt="GitHub org sponsors" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/watchers"&gt;&lt;img src="https://img.shields.io/github/watchers/EbookFoundation/free-programming-books?style=flat&amp;amp;logo=github&amp;amp;logoColor=whitesmoke&amp;amp;label=Watchers" alt="GitHub repo watchers" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/archive/refs/heads/main.zip"&gt;&lt;img src="https://img.shields.io/github/repo-size/EbookFoundation/free-programming-books?style=flat&amp;amp;logo=github&amp;amp;logoColor=whitesmoke&amp;amp;label=Repo%20Size" alt="GitHub repo size" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;The &lt;a href="https://ebookfoundation.org"&gt;Free Ebook Foundation&lt;/a&gt; now administers the repo, a not-for-profit organization devoted to promoting the creation, distribution, archiving, and sustainability of free ebooks. &lt;a href="https://ebookfoundation.org/contributions.html"&gt;Donations&lt;/a&gt; to the Free Ebook Foundation are tax-deductible in the US.&lt;/p&gt; 
&lt;h2&gt;How To Contribute&lt;/h2&gt; 
&lt;p&gt;Please read &lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/CONTRIBUTING.md"&gt;CONTRIBUTING&lt;/a&gt;. If you're new to GitHub, &lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/HOWTO.md"&gt;welcome&lt;/a&gt;! Remember to abide by our adapted from &lt;img src="https://img.shields.io/badge/Contributor%20Covenant-1.3-4baaaa.svg?sanitize=true" alt="Contributor Covenant 1.3" /&gt; &lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/CODE_OF_CONDUCT.md"&gt;Code of Conduct&lt;/a&gt; too (&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/#translations"&gt;translations&lt;/a&gt; also available).&lt;/p&gt; 
&lt;p&gt;Click on these badges to see how you might be able to help:&lt;/p&gt; 
&lt;div align="center" markdown="1"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/EbookFoundation/free-programming-books/issues"&gt;&lt;img src="https://img.shields.io/github/issues/EbookFoundation/free-programming-books?style=flat&amp;amp;logo=github&amp;amp;logoColor=red&amp;amp;label=Issues" alt="GitHub repo Issues" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22"&gt;&lt;img src="https://img.shields.io/github/issues/EbookFoundation/free-programming-books/good%20first%20issue?style=flat&amp;amp;logo=github&amp;amp;logoColor=green&amp;amp;label=Good%20First%20issues" alt="GitHub repo Good Issues for newbies" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/issues?q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22"&gt;&lt;img src="https://img.shields.io/github/issues/EbookFoundation/free-programming-books/help%20wanted?style=flat&amp;amp;logo=github&amp;amp;logoColor=b545d1&amp;amp;label=%22Help%20Wanted%22%20issues" alt="GitHub Help Wanted issues" /&gt;&lt;/a&gt;&lt;br /&gt; &lt;a href="https://github.com/EbookFoundation/free-programming-books/pulls"&gt;&lt;img src="https://img.shields.io/github/issues-pr/EbookFoundation/free-programming-books?style=flat&amp;amp;logo=github&amp;amp;logoColor=orange&amp;amp;label=PRs" alt="GitHub repo PRs" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/pulls?q=is%3Apr+is%3Amerged"&gt;&lt;img src="https://img.shields.io/github/issues-search/EbookFoundation/free-programming-books?style=flat&amp;amp;logo=github&amp;amp;logoColor=green&amp;amp;label=Merged%20PRs&amp;amp;query=is%3Amerged" alt="GitHub repo Merged PRs" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/pulls?q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22"&gt;&lt;img src="https://img.shields.io/github/issues-pr/EbookFoundation/free-programming-books/help%20wanted?style=flat&amp;amp;logo=github&amp;amp;logoColor=b545d1&amp;amp;label=%22Help%20Wanted%22%20PRs" alt="GitHub Help Wanted PRs" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;How To Share&lt;/h2&gt; 
&lt;div align="left" markdown="1"&gt; 
 &lt;a href="https://www.facebook.com/share.php?u=https%3A%2F%2Fgithub.com%2FEbookFoundation%2Ffree-programming-books&amp;amp;p%5Bimages%5D%5B0%5D=&amp;amp;p%5Btitle%5D=Free%20Programming%20Books&amp;amp;p%5Bsummary%5D="&gt;Share on Facebook&lt;/a&gt;
 &lt;br /&gt; 
 &lt;a href="http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://github.com/EbookFoundation/free-programming-books&amp;amp;title=Free%20Programming%20Books&amp;amp;summary=&amp;amp;source="&gt;Share on LinkedIn&lt;/a&gt;
 &lt;br /&gt; 
 &lt;a href="https://toot.kytta.dev/?text=https://github.com/EbookFoundation/free-programming-books"&gt;Share on Mastodon/Fediverse&lt;/a&gt;
 &lt;br /&gt; 
 &lt;a href="https://t.me/share/url?url=https://github.com/EbookFoundation/free-programming-books"&gt;Share on Telegram&lt;/a&gt;
 &lt;br /&gt; 
 &lt;a href="https://twitter.com/intent/tweet?text=https://github.com/EbookFoundation/free-programming-books%0AFree%20Programming%20Books"&gt;Share on 𝕏 (Twitter)&lt;/a&gt;
 &lt;br /&gt; 
&lt;/div&gt; 
&lt;h2&gt;Resources&lt;/h2&gt; 
&lt;p&gt;This project lists books and other resources grouped by genres:&lt;/p&gt; 
&lt;h3&gt;Books&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-langs.md"&gt;English, By Programming Language&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-subjects.md"&gt;English, By Subject&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;Other Languages&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ar.md"&gt;Arabic / al arabiya / العربية&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-hy.md"&gt;Armenian / Հայերեն&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-az.md"&gt;Azerbaijani / Азәрбајҹан дили / آذربايجانجا ديلي&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-bn.md"&gt;Bengali / বাংলা&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-bg.md"&gt;Bulgarian / български&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-my.md"&gt;Burmese / မြန်မာဘာသာ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-zh.md"&gt;Chinese / 中文&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-cs.md"&gt;Czech / čeština / český jazyk&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ca.md"&gt;Catalan / catalan / català&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-da.md"&gt;Danish / dansk&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-nl.md"&gt;Dutch / Nederlands&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-et.md"&gt;Estonian / eesti keel&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-fi.md"&gt;Finnish / suomi / suomen kieli&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-fr.md"&gt;French / français&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-de.md"&gt;German / Deutsch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-el.md"&gt;Greek / ελληνικά&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-he.md"&gt;Hebrew / עברית&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-hi.md"&gt;Hindi / हिन्दी&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-hu.md"&gt;Hungarian / magyar / magyar nyelv&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-id.md"&gt;Indonesian / Bahasa Indonesia&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-it.md"&gt;Italian / italiano&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ja.md"&gt;Japanese / 日本語&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ko.md"&gt;Korean / 한국어&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-lv.md"&gt;Latvian / Latviešu&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ml.md"&gt;Malayalam / മലയാളം&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-no.md"&gt;Norwegian / Norsk&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-fa_IR.md"&gt;Persian / Farsi (Iran) / فارسى&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-pl.md"&gt;Polish / polski / język polski / polszczyzna&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-pt_BR.md"&gt;Portuguese (Brazil)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-pt_PT.md"&gt;Portuguese (Portugal)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ro.md"&gt;Romanian (Romania) / limba română / român&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ru.md"&gt;Russian / Русский язык&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-sr.md"&gt;Serbian / српски језик / srpski jezik&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-sk.md"&gt;Slovak / slovenčina&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-sl.md"&gt;Slovenian / Slovenščina&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-es.md"&gt;Spanish / español / castellano&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-sv.md"&gt;Swedish / Svenska&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ta.md"&gt;Tamil / தமிழ்&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-te.md"&gt;Telugu / తెలుగు&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-th.md"&gt;Thai / ไทย&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-tr.md"&gt;Turkish / Türkçe&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-uk.md"&gt;Ukrainian / Українська&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ur.md"&gt;Urdu / اردو&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-vi.md"&gt;Vietnamese / Tiếng Việt&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Cheat Sheets&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-cheatsheets.md"&gt;All Languages&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Free Online Courses&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ar.md"&gt;Arabic / al arabiya / العربية&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-bn.md"&gt;Bengali / বাংলা&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-bg.md"&gt;Bulgarian / български&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-my.md"&gt;Burmese / မြန်မာဘာသာ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-zh.md"&gt;Chinese / 中文&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-en.md"&gt;English&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-fi.md"&gt;Finnish / suomi / suomen kieli&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-fr.md"&gt;French / français&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-de.md"&gt;German / Deutsch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-el.md"&gt;Greek / ελληνικά&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-he.md"&gt;Hebrew / עברית&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-hi.md"&gt;Hindi / हिंदी&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-id.md"&gt;Indonesian / Bahasa Indonesia&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-it.md"&gt;Italian / italiano&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ja.md"&gt;Japanese / 日本語&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-kn.md"&gt;Kannada / ಕನ್ನಡ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-kk.md"&gt;Kazakh / қазақша&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-km.md"&gt;Khmer / ភាសាខ្មែរ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ko.md"&gt;Korean / 한국어&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ml.md"&gt;Malayalam / മലയാളം&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-mr.md"&gt;Marathi / मराठी&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ne.md"&gt;Nepali / नेपाली&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-no.md"&gt;Norwegian / Norsk&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-fa_IR.md"&gt;Persian / Farsi (Iran) / فارسى&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-pl.md"&gt;Polish / polski / język polski / polszczyzna&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-pt_BR.md"&gt;Portuguese (Brazil)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-pt_PT.md"&gt;Portuguese (Portugal)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-pa.md"&gt;Punjabi / ਪੰਜਾਬੀ / پنجابی&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ro.md"&gt;Romanian (Romania) / limba română / român&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ru.md"&gt;Russian / Русский язык&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-si.md"&gt;Sinhala / සිංහල&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-es.md"&gt;Spanish / español / castellano&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-sv.md"&gt;Swedish / svenska&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ta.md"&gt;Tamil / தமிழ்&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-te.md"&gt;Telugu / తెలుగు&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-th.md"&gt;Thai / ภาษาไทย&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-tr.md"&gt;Turkish / Türkçe&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-uk.md"&gt;Ukrainian / Українська&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ur.md"&gt;Urdu / اردو&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-vi.md"&gt;Vietnamese / Tiếng Việt&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Interactive Programming Resources&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-interactive-tutorials-zh.md"&gt;Chinese / 中文&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-interactive-tutorials-en.md"&gt;English&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-interactive-tutorials-de.md"&gt;German / Deutsch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-interactive-tutorials-ja.md"&gt;Japanese / 日本語&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-interactive-tutorials-ru.md"&gt;Russian / Русский язык&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Problem Sets and Competitive Programming&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/problem-sets-competitive-programming.md"&gt;Problem Sets&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Podcast - Screencast&lt;/h3&gt; 
&lt;p&gt;Free Podcasts and Screencasts:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-ar.md"&gt;Arabic / al Arabiya / العربية&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-my.md"&gt;Burmese / မြန်မာဘာသာ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-zh.md"&gt;Chinese / 中文&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-cs.md"&gt;Czech / čeština / český jazyk&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-nl.md"&gt;Dutch / Nederlands&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-en.md"&gt;English&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-fi.md"&gt;Finnish / Suomi&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-fr.md"&gt;French / français&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-de.md"&gt;German / Deutsch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-he.md"&gt;Hebrew / עברית&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-id.md"&gt;Indonesian / Bahasa Indonesia&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-fa_IR.md"&gt;Persian / Farsi (Iran) / فارسى&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-pl.md"&gt;Polish / polski / język polski / polszczyzna&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-pt_BR.md"&gt;Portuguese (Brazil)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-pt_PT.md"&gt;Portuguese (Portugal)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-ru.md"&gt;Russian / Русский язык&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-si.md"&gt;Sinhala / සිංහල&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-es.md"&gt;Spanish / español / castellano&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-sv.md"&gt;Swedish / Svenska&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-tr.md"&gt;Turkish / Türkçe&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-uk.md"&gt;Ukrainian / Українська&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Programming Playgrounds&lt;/h3&gt; 
&lt;p&gt;Write, compile, and run your code within a browser. Try it out!&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-playgrounds-zh.md"&gt;Chinese / 中文&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-playgrounds.md"&gt;English&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-playgrounds-de.md"&gt;German / Deutsch&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Translations&lt;/h2&gt; 
&lt;p&gt;Volunteers have translated many of our Contributing, How-to, and Code of Conduct documents into languages covered by our lists.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;English 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/CODE_OF_CONDUCT.md"&gt;Code of Conduct&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/CONTRIBUTING.md"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/HOWTO.md"&gt;How-to&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;... &lt;em&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/README.md#translations"&gt;More languages&lt;/a&gt;&lt;/em&gt; ...&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You might notice that there are &lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/README.md#translations"&gt;some missing translations here&lt;/a&gt; - perhaps you would like to help out by &lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/CONTRIBUTING.md#help-out-by-contributing-a-translation"&gt;contributing a translation&lt;/a&gt;?&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Each file included in this repository is licensed under the &lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/LICENSE"&gt;CC BY License&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>servo/servo</title>
      <link>https://github.com/servo/servo</link>
      <description>&lt;p&gt;Servo aims to empower developers with a lightweight, high-performance alternative for embedding web technologies in applications.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;The Servo Parallel Browser Engine Project&lt;/h1&gt; 
&lt;p&gt;Servo is a prototype web browser engine written in the &lt;a href="https://github.com/rust-lang/rust"&gt;Rust&lt;/a&gt; language. It is currently developed on 64-bit macOS, 64-bit Linux, 64-bit Windows, 64-bit OpenHarmony, and Android.&lt;/p&gt; 
&lt;p&gt;Servo welcomes contribution from everyone. Check out:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;The &lt;a href="https://book.servo.org"&gt;Servo Book&lt;/a&gt; for documentation&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://servo.org/"&gt;servo.org&lt;/a&gt; for news and guides&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Coordination of Servo development happens:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Here in the Github Issues&lt;/li&gt; 
 &lt;li&gt;On the &lt;a href="https://servo.zulipchat.com/"&gt;Servo Zulip&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;In video calls advertised in the &lt;a href="https://github.com/servo/project/issues"&gt;Servo Project&lt;/a&gt; repo.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Getting started&lt;/h2&gt; 
&lt;p&gt;For more detailed build instructions, see the Servo book under &lt;a href="https://book.servo.org/hacking/setting-up-your-environment.html"&gt;Setting up your environment&lt;/a&gt;, &lt;a href="https://book.servo.org/hacking/building-servo.html"&gt;Building Servo&lt;/a&gt;, &lt;a href="https://book.servo.org/hacking/building-for-android.html"&gt;Building for Android&lt;/a&gt; and &lt;a href="https://book.servo.org/hacking/building-for-openharmony.html"&gt;Building for OpenHarmony&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;macOS&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Download and install &lt;a href="https://developer.apple.com/xcode/"&gt;Xcode&lt;/a&gt; and &lt;a href="https://brew.sh/"&gt;&lt;code&gt;brew&lt;/code&gt;&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Install &lt;code&gt;uv&lt;/code&gt;: &lt;code&gt;curl -LsSf https://astral.sh/uv/install.sh | sh&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Install &lt;code&gt;rustup&lt;/code&gt;: &lt;code&gt;curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Restart your shell to make sure &lt;code&gt;cargo&lt;/code&gt; is available&lt;/li&gt; 
 &lt;li&gt;Install the other dependencies: &lt;code&gt;./mach bootstrap&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Build servoshell: &lt;code&gt;./mach build&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Linux&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Install &lt;code&gt;curl&lt;/code&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;Arch: &lt;code&gt;sudo pacman -S --needed curl&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Debian, Ubuntu: &lt;code&gt;sudo apt install curl&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Fedora: &lt;code&gt;sudo dnf install curl&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Gentoo: &lt;code&gt;sudo emerge net-misc/curl&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Install &lt;code&gt;uv&lt;/code&gt;: &lt;code&gt;curl -LsSf https://astral.sh/uv/install.sh | sh&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Install &lt;code&gt;rustup&lt;/code&gt;: &lt;code&gt;curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Restart your shell to make sure &lt;code&gt;cargo&lt;/code&gt; is available&lt;/li&gt; 
 &lt;li&gt;Install the other dependencies: &lt;code&gt;./mach bootstrap&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Build servoshell: &lt;code&gt;./mach build&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Windows&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Download &lt;a href="https://docs.astral.sh/uv/getting-started/installation/#standalone-installer"&gt;&lt;code&gt;uv&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://chocolatey.org/install#individual"&gt;&lt;code&gt;choco&lt;/code&gt;&lt;/a&gt;, and &lt;a href="https://win.rustup.rs/"&gt;&lt;code&gt;rustup&lt;/code&gt;&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Be sure to select &lt;em&gt;Quick install via the Visual Studio Community installer&lt;/em&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;In the Visual Studio Installer, ensure the following components are installed: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Windows 10/11 SDK (anything &amp;gt;= 10.0.19041.0)&lt;/strong&gt; (&lt;code&gt;Microsoft.VisualStudio.Component.Windows{10, 11}SDK.{&amp;gt;=19041}&lt;/code&gt;)&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;MSVC v143 - VS 2022 C++ x64/x86 build tools (Latest)&lt;/strong&gt; (&lt;code&gt;Microsoft.VisualStudio.Component.VC.Tools.x86.x64&lt;/code&gt;)&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;C++ ATL for latest v143 build tools (x86 &amp;amp; x64)&lt;/strong&gt; (&lt;code&gt;Microsoft.VisualStudio.Component.VC.ATL&lt;/code&gt;)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Restart your shell to make sure &lt;code&gt;cargo&lt;/code&gt; is available&lt;/li&gt; 
 &lt;li&gt;Install the other dependencies: &lt;code&gt;.\mach bootstrap&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Build servoshell: &lt;code&gt;.\mach build&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Android&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Ensure that the following environment variables are set: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;ANDROID_SDK_ROOT&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;ANDROID_NDK_ROOT&lt;/code&gt;: &lt;code&gt;$ANDROID_SDK_ROOT/ndk/28.2.13676358/&lt;/code&gt; &lt;code&gt;ANDROID_SDK_ROOT&lt;/code&gt; can be any directory (such as &lt;code&gt;~/android-sdk&lt;/code&gt;). All of the Android build dependencies will be installed there.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Install the latest version of the &lt;a href="https://developer.android.com/studio#command-tools"&gt;Android command-line tools&lt;/a&gt; to &lt;code&gt;$ANDROID_SDK_ROOT/cmdline-tools/latest&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Run the following command to install the necessary components: &lt;pre&gt;&lt;code class="language-shell"&gt;sudo $ANDROID_SDK_ROOT/cmdline-tools/latest/bin/sdkmanager --install \
 "build-tools;34.0.0" \
 "emulator" \
 "ndk;28.2.13676358" \
 "platform-tools" \
 "platforms;android-33" \
 "system-images;android-33;google_apis;x86_64"
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Follow the instructions above for the platform you are building on&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;OpenHarmony&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Follow the instructions above for the platform you are building on to prepare the environment.&lt;/li&gt; 
 &lt;li&gt;Depending on the target distribution (e.g. &lt;code&gt;HarmonyOS NEXT&lt;/code&gt; vs pure &lt;code&gt;OpenHarmony&lt;/code&gt;) the build configuration will differ slightly.&lt;/li&gt; 
 &lt;li&gt;Ensure that the following environment variables are set 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;DEVECO_SDK_HOME&lt;/code&gt; (Required when targeting &lt;code&gt;HarmonyOS NEXT&lt;/code&gt;)&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;OHOS_BASE_SDK_HOME&lt;/code&gt; (Required when targeting &lt;code&gt;OpenHarmony&lt;/code&gt;)&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;OHOS_SDK_NATIVE&lt;/code&gt; (e.g. &lt;code&gt;${DEVECO_SDK_HOME}/default/openharmony/native&lt;/code&gt; or &lt;code&gt;${OHOS_BASE_SDK_HOME}/${API_VERSION}/native&lt;/code&gt;)&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;SERVO_OHOS_SIGNING_CONFIG&lt;/code&gt;: Path to json file containing a valid signing configuration for the demo app.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Review the detailed instructions at &lt;a href="https://book.servo.org/hacking/building-for-openharmony.html"&gt;Building for OpenHarmony&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;The target distribution can be modified by passing &lt;code&gt;--flavor=&amp;lt;default|harmonyos&amp;gt;&lt;/code&gt; to &lt;code&gt;mach &amp;lt;build|package|install&amp;gt;&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>TibixDev/winboat</title>
      <link>https://github.com/TibixDev/winboat</link>
      <description>&lt;p&gt;Run Windows apps on 🐧 Linux with ✨ seamless integration&lt;/p&gt;&lt;hr&gt;&lt;div align="left"&gt; 
 &lt;table&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td&gt; &lt;img src="https://raw.githubusercontent.com/TibixDev/winboat/main/icons/winboat_logo.svg?sanitize=true" alt="WinBoat Logo" width="150" /&gt; &lt;/td&gt; 
    &lt;td&gt; &lt;h1 style="color: #7C86FF; margin: 0; font-size: 32px;"&gt;WinBoat&lt;/h1&gt; &lt;p style="color: oklch(90% 0 0); font-size: 14px; margin: 5px 0;"&gt;Windows for Penguins.&lt;br /&gt; Run Windows apps on 🐧 Linux with ✨ seamless integration&lt;/p&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h2&gt;Screenshots&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/TibixDev/winboat/main/gh-assets/features/feat_dash.png" alt="WinBoat Dashboard" width="45%" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/TibixDev/winboat/main/gh-assets/features/feat_apps.png" alt="WinBoat Apps" width="45%" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/TibixDev/winboat/main/gh-assets/features/feat_native.png" alt="Native Windows" width="45%" /&gt; 
&lt;/div&gt; 
&lt;h2&gt;⚠️ Work in Progress ⚠️&lt;/h2&gt; 
&lt;p&gt;WinBoat is currently in beta, so expect to occasionally run into hiccups and bugs. You should be comfortable with some level of troubleshooting if you decide to try it, however we encourage you to give it a shot anyway.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;/p&gt; 
 &lt;h4&gt;🧊 Feature Freeze&lt;/h4&gt; 
 &lt;p&gt;We're preparing for a 0.9.0 release, which already includes tons of changes. From this point forward, we'll be focusing on stabilizing the feature/fix set we have already merged or we're actively working on. Handling most pull requests or feature requests will be postponed up until we have a stable release and we can confidently start merging and reviewing again.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;🎨 Elegant Interface&lt;/strong&gt;: Sleek and intuitive interface that seamlessly integrates Windows into your Linux desktop environment, making it feel like a native experience&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;📦 Automated Installs&lt;/strong&gt;: Simple installation process through our interface - pick your preferences &amp;amp; specs and let us handle the rest&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🚀 Run Any App&lt;/strong&gt;: If it runs on Windows, it can run on WinBoat. Enjoy the full range of Windows applications as native OS-level windows in your Linux environment&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🖥️ Full Windows Desktop&lt;/strong&gt;: Access the complete Windows desktop experience when you need it, or run individual apps seamlessly integrated into your Linux workflow&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;📁 Filesystem Integration&lt;/strong&gt;: Your home directory is mounted in Windows, allowing easy file sharing between the two systems without any hassle&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;✨ And many more&lt;/strong&gt;: Smartcard passthrough, resource monitoring, and more features being added regularly&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;How Does It Work?&lt;/h2&gt; 
&lt;p&gt;WinBoat is an Electron app which allows you to run Windows apps on Linux using a containerized approach. Windows runs as a VM inside a Docker container, we communicate with it using the &lt;a href="https://github.com/TibixDev/winboat/tree/main/guest_server"&gt;WinBoat Guest Server&lt;/a&gt; to retrieve data we need from Windows. For compositing applications as native OS-level windows, we use FreeRDP together with Windows's RemoteApp protocol.&lt;/p&gt; 
&lt;h2&gt;Prerequisites&lt;/h2&gt; 
&lt;p&gt;Before running WinBoat, ensure your system meets the following requirements:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;RAM&lt;/strong&gt;: At least 4 GB of RAM&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;CPU&lt;/strong&gt;: At least 2 CPU threads&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Storage&lt;/strong&gt;: At least 32 GB free space on the drive your selected install folder corresponds to&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Virtualization&lt;/strong&gt;: KVM enabled in BIOS/UEFI 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://duckduckgo.com/?t=h_&amp;amp;q=how+to+enable+virtualization+in+%3Cmotherboard+brand%3E+bios&amp;amp;ia=web"&gt;How to enable virtualization&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Docker&lt;/strong&gt;: Required for containerization 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://docs.docker.com/engine/install/"&gt;Installation Guide&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;⚠️ NOTE:&lt;/strong&gt; Docker Desktop is &lt;strong&gt;not&lt;/strong&gt; supported, you will run into issues if you use it&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Docker Compose v2&lt;/strong&gt;: Required for compatibility with docker-compose.yml files 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://docs.docker.com/compose/install/#plugin-linux-only"&gt;Installation Guide&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Docker User Group&lt;/strong&gt;: Add your user to the &lt;code&gt;docker&lt;/code&gt; group 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://docs.docker.com/engine/install/linux-postinstall/#manage-docker-as-a-non-root-user"&gt;Setup Instructions&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;FreeRDP&lt;/strong&gt;: Required for remote desktop connection (Please make sure you have &lt;strong&gt;Version 3.x.x&lt;/strong&gt; with sound support included) 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://github.com/FreeRDP/FreeRDP/wiki/PreBuilds"&gt;Installation Guide&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;[OPTIONAL] &lt;strong&gt;Kernel Modules&lt;/strong&gt;: The &lt;code&gt;iptables&lt;/code&gt; / &lt;code&gt;nftables&lt;/code&gt; and &lt;code&gt;iptable_nat&lt;/code&gt; kernel modules can be loaded for network autodiscovery and better shared filesystem performance, but this is not obligatory in newer versions of WinBoat 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://rentry.org/rmfq2e5e"&gt;Module loading instructions&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Downloading&lt;/h2&gt; 
&lt;p&gt;You can download the latest Linux builds under the &lt;a href="https://github.com/TibixDev/winboat/releases"&gt;Releases&lt;/a&gt; tab. We currently offer four variants:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;AppImage:&lt;/strong&gt; A popular &amp;amp; portable app format which should run fine on most distributions&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Unpacked:&lt;/strong&gt; The raw unpacked files, simply run the executable (&lt;code&gt;linux-unpacked/winboat&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;.deb:&lt;/strong&gt; The intended format for Debian based distributions&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;.rpm:&lt;/strong&gt; The intended format for Fedora based distributions&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Known Issues About Container Runtimes&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Podman is &lt;strong&gt;unsupported&lt;/strong&gt; for now&lt;/li&gt; 
 &lt;li&gt;Docker Desktop is &lt;strong&gt;unsupported&lt;/strong&gt; for now&lt;/li&gt; 
 &lt;li&gt;Distros that emulate Docker through a Podman socket are &lt;strong&gt;unsupported&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Any rootless containerization solution is currently &lt;strong&gt;unsupported&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Building WinBoat&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;For building you need to have NodeJS and Go installed on your system&lt;/li&gt; 
 &lt;li&gt;Clone the repo (&lt;code&gt;git clone https://github.com/TibixDev/WinBoat&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Install the dependencies (&lt;code&gt;npm i&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Build the app and the guest server using &lt;code&gt;npm run build:linux-gs&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;You can now find the built app under &lt;code&gt;dist&lt;/code&gt; with an AppImage and an Unpacked variant&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Running WinBoat in development mode&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Make sure you meet the &lt;a href="https://raw.githubusercontent.com/TibixDev/winboat/main/#prerequisites"&gt;prerequisites&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Additionally, for development you need to have NodeJS and Go installed on your system&lt;/li&gt; 
 &lt;li&gt;Clone the repo (&lt;code&gt;git clone https://github.com/TibixDev/WinBoat&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Install the dependencies (&lt;code&gt;npm i&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Build the guest server (&lt;code&gt;npm run build-guest-server&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Run the app (&lt;code&gt;npm run dev&lt;/code&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Contributions are welcome! Whether it's bug fixes, feature improvements, or documentation updates, we appreciate your help making WinBoat better.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Please note&lt;/strong&gt;: We maintain a focus on technical contributions only. Pull requests containing political/sexual content, or other sensitive/controversial topics will not be accepted. Let's keep things focused on making great software! 🚀&lt;/p&gt; 
&lt;p&gt;Feel free to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Report bugs and issues&lt;/li&gt; 
 &lt;li&gt;Submit feature requests&lt;/li&gt; 
 &lt;li&gt;Contribute code improvements&lt;/li&gt; 
 &lt;li&gt;Help with documentation&lt;/li&gt; 
 &lt;li&gt;Share feedback and suggestions&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Check out our issues page to get started, or feel free to open a new issue if you've found something that needs attention.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;WinBoat is licensed under the &lt;a href="https://github.com/TibixDev/winboat/raw/main/LICENSE"&gt;MIT&lt;/a&gt; license&lt;/p&gt; 
&lt;h2&gt;Inspiration / Alternatives&lt;/h2&gt; 
&lt;p&gt;These past few years some cool projects have surfaced with similar concepts, some of which we've also taken inspirations from.&lt;br /&gt; They're awesome and you should check them out:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/winapps-org/winapps"&gt;WinApps&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/casualsnek/cassowary"&gt;Cassowary&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/dockur/windows"&gt;dockur/windows&lt;/a&gt; (🌟 Also used in WinBoat)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Socials &amp;amp; Contact&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.winboat.app/"&gt;&lt;img src="https://img.shields.io/badge/Website-winboat.app-blue?style=flat&amp;amp;logo=googlechrome&amp;amp;logoColor=white" alt="Website" /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://x.com/winboat_app"&gt;&lt;img src="https://img.shields.io/badge/Twitter-@winboat__app-1DA1F2?style=flat&amp;amp;logo=x&amp;amp;logoColor=white" alt="Twitter" /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://fosstodon.org/@winboat"&gt;&lt;img src="https://img.shields.io/badge/Mastodon-@winboat-6364FF?style=flat&amp;amp;logo=mastodon&amp;amp;logoColor=white" alt="Mastodon" /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://bsky.app/profile/winboat.app"&gt;&lt;img src="https://img.shields.io/badge/Bluesky-winboat.app-00A8E8?style=flat&amp;amp;logo=bluesky&amp;amp;logoColor=white" alt="Bluesky" /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://discord.gg/MEwmpWm4tN"&gt;&lt;img src="https://img.shields.io/badge/Discord-Join_Community-5865F2?style=flat&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="mailto:staff@winboat.app"&gt;&lt;img src="https://img.shields.io/badge/Email-staff@winboat.app-D14836?style=flat&amp;amp;logo=gmail&amp;amp;logoColor=white" alt="Email" /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://deepwiki.com/TibixDev/winboat"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki" /&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;a href="https://www.star-history.com/#tibixdev/winboat&amp;amp;Date"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=tibixdev/winboat&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=tibixdev/winboat&amp;amp;type=Date" /&gt; 
  &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=tibixdev/winboat&amp;amp;type=Date" /&gt; 
 &lt;/picture&gt; &lt;/a&gt;</description>
    </item>
    
    <item>
      <title>tile-ai/tilelang</title>
      <link>https://github.com/tile-ai/tilelang</link>
      <description>&lt;p&gt;Domain-specific language designed to streamline the development of high-performance GPU/CPU/Accelerators kernels&lt;/p&gt;&lt;hr&gt;&lt;img src="https://raw.githubusercontent.com/tile-ai/tilelang/main/images/logo-row.svg?sanitize=true" /&gt; 
&lt;div align="center"&gt; 
 &lt;h1&gt;Tile Language&lt;/h1&gt; 
 &lt;p&gt;&lt;a href="https://badge.fury.io/py/tilelang"&gt;&lt;img src="https://badge.fury.io/py/tilelang.svg?sanitize=true" alt="PyPI version" /&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/tile-ai/tilelang"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/TUrHyJnKPG"&gt;&lt;img src="https://img.shields.io/badge/Discord-%235865F2.svg?logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;Tile Language (&lt;strong&gt;tile-lang&lt;/strong&gt;) is a concise domain-specific language designed to streamline the development of high-performance GPU/CPU kernels (e.g., GEMM, Dequant GEMM, FlashAttention, LinearAttention). By employing a Pythonic syntax with an underlying compiler infrastructure on top of &lt;a href="https://tvm.apache.org/"&gt;TVM&lt;/a&gt;, tile-lang allows developers to focus on productivity without sacrificing the low-level optimizations necessary for state-of-the-art performance.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/tile-ai/tilelang/main/images/MatmulExample.png" /&gt; 
&lt;h2&gt;Latest News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;10/07/2025 🍎: Added Apple Metal Device support, check out &lt;a href="https://github.com/tile-ai/tilelang/pull/799"&gt;Pull Request #799&lt;/a&gt; for details.&lt;/li&gt; 
 &lt;li&gt;09/29/2025 🎉: Thrilled to announce that ​​AscendC​​ and ​Ascend​NPU IR​​ backends targeting Huawei Ascend chips are now supported! Check out the preview here: 🔗 &lt;a href="https://github.com/tile-ai/tilelang-ascend"&gt;link&lt;/a&gt;. This includes implementations across two branches: &lt;a href="https://github.com/tile-ai/tilelang-ascend"&gt;ascendc_pto&lt;/a&gt; and &lt;a href="https://github.com/tile-ai/tilelang-ascend/tree/npuir"&gt;npuir&lt;/a&gt;. Feel free to explore and share your feedback!&lt;/li&gt; 
 &lt;li&gt;07/04/2025 🚀: Introduced &lt;code&gt;T.gemm_sp&lt;/code&gt; for 2:4 sparse tensor core support, check out &lt;a href="https://github.com/tile-ai/tilelang/pull/526"&gt;Pull Request #526&lt;/a&gt; for details.&lt;/li&gt; 
 &lt;li&gt;06/05/2025 ✨: Added &lt;a href="https://github.com/tile-ai/tilelang/pull/461"&gt;NVRTC Backend&lt;/a&gt; to significantly reduce compilation time for cute templates!&lt;/li&gt; 
 &lt;li&gt;04/14/2025 🚀: Added high-performance FlashMLA implementation for AMD MI300X, achieving performance parity with hand-optimized assembly kernels of Aiter! See &lt;a href="https://raw.githubusercontent.com/tile-ai/tilelang/main/examples/deepseek_mla/amd/README.md"&gt;example_mla_amd&lt;/a&gt; for details.&lt;/li&gt; 
 &lt;li&gt;03/03/2025 🚀: Added high-performance MLA Decoding support using only 80 lines of Python code, achieving performance on par with FlashMLA on H100 (see &lt;a href="https://raw.githubusercontent.com/tile-ai/tilelang/main/examples/deepseek_mla/example_mla_decode.py"&gt;example_mla_decode.py&lt;/a&gt;)! We also provide &lt;a href="https://raw.githubusercontent.com/tile-ai/tilelang/main/examples/deepseek_mla/README.md"&gt;documentation&lt;/a&gt; explaining how TileLang achieves this.&lt;/li&gt; 
 &lt;li&gt;02/15/2025 ✨: Added WebGPU Codegen support, see &lt;a href="https://github.com/tile-ai/tilelang/pull/86"&gt;Pull Request #86&lt;/a&gt;!&lt;/li&gt; 
 &lt;li&gt;02/12/2025 ✨: Excited to announce the release of &lt;a href="https://github.com/tile-ai/tilelang/releases/tag/v0.1.0"&gt;v0.1.0&lt;/a&gt;!&lt;/li&gt; 
 &lt;li&gt;02/10/2025 🚀: Added debug tools for TileLang—&lt;code&gt;T.print&lt;/code&gt; for printing variables/buffers (&lt;a href="https://tilelang.com/tutorials/debug_tools_for_tilelang.html"&gt;docs&lt;/a&gt;) and a memory layout plotter (&lt;a href="https://raw.githubusercontent.com/tile-ai/tilelang/main/examples/plot_layout"&gt;examples/plot_layout&lt;/a&gt;).&lt;/li&gt; 
 &lt;li&gt;01/20/2025 ✨: We are excited to announce that tile-lang, a dsl for high performance AI workloads, is now open source and available to the public!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Tested Devices&lt;/h2&gt; 
&lt;p&gt;Although tile-lang aims to be portable across a range of Devices, it has been specifically tested and validated on the following devices: for NVIDIA GPUs, this includes the H100 (with Auto TMA/WGMMA support), A100, V100, RTX 4090, RTX 3090, and RTX A6000; for AMD GPUs, it includes the MI250 (with Auto MatrixCore support) and the MI300X (with Async Copy support).&lt;/p&gt; 
&lt;h2&gt;OP Implementation Examples&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;tile-lang&lt;/strong&gt; provides the building blocks to implement a wide variety of operators. Some examples include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tile-ai/tilelang/main/examples/gemm/"&gt;Matrix Multiplication&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tile-ai/tilelang/main/examples/dequantize_gemm/"&gt;Dequantization GEMM&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tile-ai/tilelang/main/examples/flash_attention/"&gt;Flash Attention&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tile-ai/tilelang/main/examples/linear_attention/"&gt;Flash Linear Attention&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tile-ai/tilelang/main/examples/deepseek_mla/"&gt;Flash MLA Decoding&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tile-ai/tilelang/main/examples/deepseek_nsa/"&gt;Native Sparse Attention&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Within the &lt;code&gt;examples&lt;/code&gt; directory, you will also find additional complex kernels—such as convolutions, forward/backward passes for FlashAttention, more operators will continuously be added.&lt;/p&gt; 
&lt;h2&gt;Benchmark Summary&lt;/h2&gt; 
&lt;p&gt;TileLang achieves exceptional performance across a variety of computational patterns. Comprehensive benchmark scripts and settings are available at &lt;a href="https://github.com/tile-ai/tilelang-benchmark"&gt;tilelang-benchmark&lt;/a&gt;. Below are selected results showcasing its capabilities:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;MLA Decoding Performance on H100&lt;/p&gt; 
  &lt;div style="display: flex; gap: 10px; justify-content: center;"&gt; 
   &lt;div style="flex: 1;"&gt; 
    &lt;img src="https://raw.githubusercontent.com/tile-ai/tilelang/main/examples/deepseek_mla/figures/bs64_float16.png" alt="mla decode performance bs64 on H100" width="100%" /&gt; 
   &lt;/div&gt; 
   &lt;div style="flex: 1;"&gt; 
    &lt;img src="https://raw.githubusercontent.com/tile-ai/tilelang/main/examples/deepseek_mla/figures/bs128_float16.png" alt="mla decode performance bs128 on H100" width="100%" /&gt; 
   &lt;/div&gt; 
  &lt;/div&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Flash Attention Performance on H100&lt;/p&gt; 
  &lt;div align="center"&gt; 
   &lt;img src="https://raw.githubusercontent.com/tile-ai/tilelang/main/images/mha_performance_h100.png" alt="operator performance on H100" width="80%" /&gt; 
  &lt;/div&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Matmul Performance on GPUs (RTX 4090, A100, H100, MI300X)&lt;/p&gt; 
  &lt;div&gt; 
   &lt;img src="https://raw.githubusercontent.com/tile-ai/tilelang/main/images/op_benchmark_consistent_gemm_fp16.png" alt="gemm fp16 performance on Gpus" /&gt; 
  &lt;/div&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Dequantize Matmul Performance on A100&lt;/p&gt; 
  &lt;div&gt; 
   &lt;img src="https://raw.githubusercontent.com/tile-ai/tilelang/main/images/op_benchmark_a100_wq_gemv.png" alt="dequantize gemv performance on A100" /&gt; 
  &lt;/div&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;Method 1: Install with Pip&lt;/h3&gt; 
&lt;p&gt;The quickest way to get started is to install the latest release from PyPI:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install tilelang
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, you can install directly from the GitHub repository:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install git+https://github.com/tile-ai/tilelang
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or install locally:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# install required system dependencies
sudo apt-get update
sudo apt-get install -y python3-setuptools gcc libtinfo-dev zlib1g-dev build-essential cmake libedit-dev libxml2-dev

pip install -e . -v # remove -e option if you don't want to install in editable mode, -v for verbose output
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Method 2: Build from Source&lt;/h3&gt; 
&lt;p&gt;We currently provide three ways to install &lt;strong&gt;tile-lang&lt;/strong&gt; from source:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tile-ai/tilelang/main/docs/get_started/Installation.md#method-1-install-from-source-using-your-own-tvm-installation"&gt;Install from Source (using your own TVM installation)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tile-ai/tilelang/main/docs/get_started/Installation.md#method-2-install-from-source-using-the-bundled-tvm-submodule"&gt;Install from Source (using the bundled TVM submodule)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tile-ai/tilelang/main/docs/get_started/Installation.md#method-3-install-using-the-provided-script"&gt;Install Using the Provided Script&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Method 3: Install with Nightly Version&lt;/h3&gt; 
&lt;p&gt;For users who want access to the latest features and improvements before official releases, we provide nightly builds of &lt;strong&gt;tile-lang&lt;/strong&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install tilelang -f https://tile-ai.github.io/whl/nightly/cu121/
# or pip install tilelang --find-links https://tile-ai.github.io/whl/nightly/cu121/
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Nightly builds contain the most recent code changes but may be less stable than official releases. They're ideal for testing new features or if you need a specific bugfix that hasn't been released yet.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;p&gt;In this section, you'll learn how to write and execute a straightforward GEMM (matrix multiplication) kernel using tile-lang, followed by techniques for layout optimizations, pipelining, and L2-cache–friendly swizzling.&lt;/p&gt; 
&lt;h3&gt;GEMM Example with Annotations (Layout, L2 Cache Swizzling, and Pipelining, etc.)&lt;/h3&gt; 
&lt;p&gt;Below is an example that demonstrates more advanced features: layout annotation, parallelized copy, and swizzle for improved L2 cache locality. This snippet shows how to adapt your kernel to maximize performance on complex hardware.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import tilelang
import tilelang.language as T

# @tilelang.jit(target="cuda")
# target currently can be "cuda" or "hip" or "cpu".
# if not specified, it will be inferred from the input tensors during compile time
@tilelang.jit
def matmul(M, N, K, block_M, block_N, block_K, dtype="float16", accum_dtype="float"):

    @T.prim_func
    def matmul_relu_kernel(
            A: T.Tensor((M, K), dtype),
            B: T.Tensor((K, N), dtype),
            C: T.Tensor((M, N), dtype),
    ):
        # Initialize Kernel Context
        with T.Kernel(T.ceildiv(N, block_N), T.ceildiv(M, block_M), threads=128) as (bx, by):
            A_shared = T.alloc_shared((block_M, block_K), dtype)
            B_shared = T.alloc_shared((block_K, block_N), dtype)
            C_local = T.alloc_fragment((block_M, block_N), accum_dtype)

            # Enable rasterization for better L2 cache locality (Optional)
            # T.use_swizzle(panel_size=10, enable=True)

            # Clear local accumulation
            T.clear(C_local)

            for ko in T.Pipelined(T.ceildiv(K, block_K), num_stages=3):
                # Copy tile of A
                # This is a sugar syntax for parallelized copy
                T.copy(A[by * block_M, ko * block_K], A_shared)

                # Copy tile of B
                T.copy(B[ko * block_K, bx * block_N], B_shared)

                # Perform a tile-level GEMM on the shared buffers
                # Currently we dispatch to the cute/hip on Nvidia/AMD GPUs
                T.gemm(A_shared, B_shared, C_local)
            
            # relu
            for i, j in T.Parallel(block_M, block_N):
                C_local[i, j] = T.max(C_local[i, j], 0)

            # Copy result back to global memory
            T.copy(C_local, C[by * block_M, bx * block_N])

    return matmul_relu_kernel


M = 1024  # M = T.dynamic("m") if you want to use dynamic shape
N = 1024
K = 1024
block_M = 128
block_N = 128
block_K = 32

# 1. Define the kernel (matmul) and compile/lower it into an executable module
matmul_relu_kernel = matmul(M, N, K, block_M, block_N, block_K)

# 3. Test the kernel in Python with PyTorch data
import torch

# Create random input tensors on the GPU
a = torch.randn(M, K, device="cuda", dtype=torch.float16)
b = torch.randn(K, N, device="cuda", dtype=torch.float16)
c = torch.empty(M, N, device="cuda", dtype=torch.float16)

# Run the kernel through the Profiler
matmul_relu_kernel(a, b, c)

print(c)
# Reference multiplication using PyTorch
ref_c = torch.relu(a @ b)

# Validate correctness
torch.testing.assert_close(c, ref_c, rtol=1e-2, atol=1e-2)
print("Kernel output matches PyTorch reference.")

# 4. Retrieve and inspect the generated CUDA source (optional)
# cuda_source = jit_kernel.get_kernel_source()
# print("Generated CUDA kernel:\n", cuda_source)

# 5.Profile latency with kernel
profiler = matmul_relu_kernel.get_profiler(tensor_supply_type=tilelang.TensorSupplyType.Normal)

latency = profiler.do_bench()

print(f"Latency: {latency} ms")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Dive Deep into TileLang Beyond GEMM&lt;/h3&gt; 
&lt;p&gt;In addition to GEMM, we provide a variety of examples to showcase the versatility and power of TileLang, including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tile-ai/tilelang/main/examples/dequantize_gemm/"&gt;Dequantize GEMM&lt;/a&gt;: Achieve high-performance dequantization by &lt;strong&gt;fine-grained control over per-thread operations&lt;/strong&gt;, with many features now adopted as default behaviors in &lt;a href="https://github.com/microsoft/BitBLAS"&gt;BitBLAS&lt;/a&gt;, which utilizing magic layout transformation and intrins to accelerate dequantize gemm.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tile-ai/tilelang/main/examples/flash_attention/"&gt;FlashAttention&lt;/a&gt;: Enable cross-operator fusion with simple and intuitive syntax, and we also provide an example of auto tuning.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tile-ai/tilelang/main/examples/linear_attention/"&gt;LinearAttention&lt;/a&gt;: Examples include RetNet and Mamba implementations.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/tile-ai/tilelang/main/examples/convolution/"&gt;Convolution&lt;/a&gt;: Implementations of Convolution with IM2Col.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Upcoming Features&lt;/h2&gt; 
&lt;p&gt;Check our &lt;a href="https://github.com/tile-ai/tilelang/issues/79"&gt;tilelang v0.2.0 release plan&lt;/a&gt; for upcoming features.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;TileLang has now been used in project &lt;a href="https://github.com/microsoft/BitBLAS"&gt;BitBLAS&lt;/a&gt; and &lt;a href="https://github.com/microsoft/AttentionEngine"&gt;AttentionEngine&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Join the Discussion&lt;/h2&gt; 
&lt;p&gt;Welcome to join our Discord community for discussions, support, and collaboration!&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://discord.gg/TUrHyJnKPG"&gt;&lt;img src="https://img.shields.io/badge/Discord-Join%20Us-blue?logo=discord&amp;amp;style=for-the-badge" alt="Join our Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Acknowledgments&lt;/h2&gt; 
&lt;p&gt;We would like to express our gratitude to the &lt;a href="https://github.com/apache/tvm"&gt;TVM&lt;/a&gt; community for their invaluable contributions. The initial version of this project was mainly developed by &lt;a href="https://github.com/LeiWang1999"&gt;LeiWang1999&lt;/a&gt;, &lt;a href="https://github.com/chengyupku"&gt;chengyupku&lt;/a&gt; and &lt;a href="https://github.com/nox-410"&gt;nox-410&lt;/a&gt; with supervision from Prof. &lt;a href="https://yangzhihome.github.io"&gt;Zhi Yang&lt;/a&gt; at Peking University. Part of this work was carried out during an internship at Microsoft Research, where Dr. Lingxiao Ma, Dr. Yuqing Xia, Dr. Jilong Xue, and Dr. Fan Yang offered valuable advice and support. We deeply appreciate their mentorship and contributions.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>anthropics/claude-agent-sdk-python</title>
      <link>https://github.com/anthropics/claude-agent-sdk-python</link>
      <description>&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Claude Agent SDK for Python&lt;/h1&gt; 
&lt;p&gt;Python SDK for Claude Agent. See the &lt;a href="https://docs.anthropic.com/en/docs/claude-code/sdk/sdk-python"&gt;Claude Agent SDK documentation&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install claude-agent-sdk
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Prerequisites:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.10+&lt;/li&gt; 
 &lt;li&gt;Node.js&lt;/li&gt; 
 &lt;li&gt;Claude Code 2.0.0+: &lt;code&gt;npm install -g @anthropic-ai/claude-code&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import anyio
from claude_agent_sdk import query

async def main():
    async for message in query(prompt="What is 2 + 2?"):
        print(message)

anyio.run(main)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Basic Usage: query()&lt;/h2&gt; 
&lt;p&gt;&lt;code&gt;query()&lt;/code&gt; is an async function for querying Claude Code. It returns an &lt;code&gt;AsyncIterator&lt;/code&gt; of response messages. See &lt;a href="https://raw.githubusercontent.com/anthropics/claude-agent-sdk-python/main/src/claude_agent_sdk/query.py"&gt;src/claude_agent_sdk/query.py&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from claude_agent_sdk import query, ClaudeAgentOptions, AssistantMessage, TextBlock

# Simple query
async for message in query(prompt="Hello Claude"):
    if isinstance(message, AssistantMessage):
        for block in message.content:
            if isinstance(block, TextBlock):
                print(block.text)

# With options
options = ClaudeAgentOptions(
    system_prompt="You are a helpful assistant",
    max_turns=1
)

async for message in query(prompt="Tell me a joke", options=options):
    print(message)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Using Tools&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;options = ClaudeAgentOptions(
    allowed_tools=["Read", "Write", "Bash"],
    permission_mode='acceptEdits'  # auto-accept file edits
)

async for message in query(
    prompt="Create a hello.py file",
    options=options
):
    # Process tool use and results
    pass
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Working Directory&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from pathlib import Path

options = ClaudeAgentOptions(
    cwd="/path/to/project"  # or Path("/path/to/project")
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ClaudeSDKClient&lt;/h2&gt; 
&lt;p&gt;&lt;code&gt;ClaudeSDKClient&lt;/code&gt; supports bidirectional, interactive conversations with Claude Code. See &lt;a href="https://raw.githubusercontent.com/anthropics/claude-agent-sdk-python/main/src/claude_agent_sdk/client.py"&gt;src/claude_agent_sdk/client.py&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Unlike &lt;code&gt;query()&lt;/code&gt;, &lt;code&gt;ClaudeSDKClient&lt;/code&gt; additionally enables &lt;strong&gt;custom tools&lt;/strong&gt; and &lt;strong&gt;hooks&lt;/strong&gt;, both of which can be defined as Python functions.&lt;/p&gt; 
&lt;h3&gt;Custom Tools (as In-Process SDK MCP Servers)&lt;/h3&gt; 
&lt;p&gt;A &lt;strong&gt;custom tool&lt;/strong&gt; is a Python function that you can offer to Claude, for Claude to invoke as needed.&lt;/p&gt; 
&lt;p&gt;Custom tools are implemented in-process MCP servers that run directly within your Python application, eliminating the need for separate processes that regular MCP servers require.&lt;/p&gt; 
&lt;p&gt;For an end-to-end example, see &lt;a href="https://raw.githubusercontent.com/anthropics/claude-agent-sdk-python/main/examples/mcp_calculator.py"&gt;MCP Calculator&lt;/a&gt;.&lt;/p&gt; 
&lt;h4&gt;Creating a Simple Tool&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from claude_agent_sdk import tool, create_sdk_mcp_server, ClaudeAgentOptions, ClaudeSDKClient

# Define a tool using the @tool decorator
@tool("greet", "Greet a user", {"name": str})
async def greet_user(args):
    return {
        "content": [
            {"type": "text", "text": f"Hello, {args['name']}!"}
        ]
    }

# Create an SDK MCP server
server = create_sdk_mcp_server(
    name="my-tools",
    version="1.0.0",
    tools=[greet_user]
)

# Use it with Claude
options = ClaudeAgentOptions(
    mcp_servers={"tools": server},
    allowed_tools=["mcp__tools__greet"]
)

async with ClaudeSDKClient(options=options) as client:
    await client.query("Greet Alice")

    # Extract and print response
    async for msg in client.receive_response():
        print(msg)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Benefits Over External MCP Servers&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;No subprocess management&lt;/strong&gt; - Runs in the same process as your application&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Better performance&lt;/strong&gt; - No IPC overhead for tool calls&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Simpler deployment&lt;/strong&gt; - Single Python process instead of multiple&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Easier debugging&lt;/strong&gt; - All code runs in the same process&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Type safety&lt;/strong&gt; - Direct Python function calls with type hints&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Migration from External Servers&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# BEFORE: External MCP server (separate process)
options = ClaudeAgentOptions(
    mcp_servers={
        "calculator": {
            "type": "stdio",
            "command": "python",
            "args": ["-m", "calculator_server"]
        }
    }
)

# AFTER: SDK MCP server (in-process)
from my_tools import add, subtract  # Your tool functions

calculator = create_sdk_mcp_server(
    name="calculator",
    tools=[add, subtract]
)

options = ClaudeAgentOptions(
    mcp_servers={"calculator": calculator}
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Mixed Server Support&lt;/h4&gt; 
&lt;p&gt;You can use both SDK and external MCP servers together:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;options = ClaudeAgentOptions(
    mcp_servers={
        "internal": sdk_server,      # In-process SDK server
        "external": {                # External subprocess server
            "type": "stdio",
            "command": "external-server"
        }
    }
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Hooks&lt;/h3&gt; 
&lt;p&gt;A &lt;strong&gt;hook&lt;/strong&gt; is a Python function that the Claude Code &lt;em&gt;application&lt;/em&gt; (&lt;em&gt;not&lt;/em&gt; Claude) invokes at specific points of the Claude agent loop. Hooks can provide deterministic processing and automated feedback for Claude. Read more in &lt;a href="https://docs.anthropic.com/en/docs/claude-code/hooks"&gt;Claude Code Hooks Reference&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For more examples, see examples/hooks.py.&lt;/p&gt; 
&lt;h4&gt;Example&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from claude_agent_sdk import ClaudeAgentOptions, ClaudeSDKClient, HookMatcher

async def check_bash_command(input_data, tool_use_id, context):
    tool_name = input_data["tool_name"]
    tool_input = input_data["tool_input"]
    if tool_name != "Bash":
        return {}
    command = tool_input.get("command", "")
    block_patterns = ["foo.sh"]
    for pattern in block_patterns:
        if pattern in command:
            return {
                "hookSpecificOutput": {
                    "hookEventName": "PreToolUse",
                    "permissionDecision": "deny",
                    "permissionDecisionReason": f"Command contains invalid pattern: {pattern}",
                }
            }
    return {}

options = ClaudeAgentOptions(
    allowed_tools=["Bash"],
    hooks={
        "PreToolUse": [
            HookMatcher(matcher="Bash", hooks=[check_bash_command]),
        ],
    }
)

async with ClaudeSDKClient(options=options) as client:
    # Test 1: Command with forbidden pattern (will be blocked)
    await client.query("Run the bash command: ./foo.sh --help")
    async for msg in client.receive_response():
        print(msg)

    print("\n" + "=" * 50 + "\n")

    # Test 2: Safe command that should work
    await client.query("Run the bash command: echo 'Hello from hooks example!'")
    async for msg in client.receive_response():
        print(msg)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Types&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/anthropics/claude-agent-sdk-python/main/src/claude_agent_sdk/types.py"&gt;src/claude_agent_sdk/types.py&lt;/a&gt; for complete type definitions:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;ClaudeAgentOptions&lt;/code&gt; - Configuration options&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;AssistantMessage&lt;/code&gt;, &lt;code&gt;UserMessage&lt;/code&gt;, &lt;code&gt;SystemMessage&lt;/code&gt;, &lt;code&gt;ResultMessage&lt;/code&gt; - Message types&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;TextBlock&lt;/code&gt;, &lt;code&gt;ToolUseBlock&lt;/code&gt;, &lt;code&gt;ToolResultBlock&lt;/code&gt; - Content blocks&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Error Handling&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from claude_agent_sdk import (
    ClaudeSDKError,      # Base error
    CLINotFoundError,    # Claude Code not installed
    CLIConnectionError,  # Connection issues
    ProcessError,        # Process failed
    CLIJSONDecodeError,  # JSON parsing issues
)

try:
    async for message in query(prompt="Hello"):
        pass
except CLINotFoundError:
    print("Please install Claude Code")
except ProcessError as e:
    print(f"Process failed with exit code: {e.exit_code}")
except CLIJSONDecodeError as e:
    print(f"Failed to parse response: {e}")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/anthropics/claude-agent-sdk-python/main/src/claude_agent_sdk/_errors.py"&gt;src/claude_agent_sdk/_errors.py&lt;/a&gt; for all error types.&lt;/p&gt; 
&lt;h2&gt;Available Tools&lt;/h2&gt; 
&lt;p&gt;See the &lt;a href="https://docs.anthropic.com/en/docs/claude-code/settings#tools-available-to-claude"&gt;Claude Code documentation&lt;/a&gt; for a complete list of available tools.&lt;/p&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/anthropics/claude-agent-sdk-python/main/examples/quick_start.py"&gt;examples/quick_start.py&lt;/a&gt; for a complete working example.&lt;/p&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/anthropics/claude-agent-sdk-python/main/examples/streaming_mode.py"&gt;examples/streaming_mode.py&lt;/a&gt; for comprehensive examples involving &lt;code&gt;ClaudeSDKClient&lt;/code&gt;. You can even run interactive examples in IPython from &lt;a href="https://raw.githubusercontent.com/anthropics/claude-agent-sdk-python/main/examples/streaming_mode_ipython.py"&gt;examples/streaming_mode_ipython.py&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Migrating from Claude Code SDK&lt;/h2&gt; 
&lt;p&gt;If you're upgrading from the Claude Code SDK (versions &amp;lt; 0.1.0), please see the &lt;a href="https://raw.githubusercontent.com/anthropics/claude-agent-sdk-python/main/CHANGELOG.md#010"&gt;CHANGELOG.md&lt;/a&gt; for details on breaking changes and new features, including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;ClaudeCodeOptions&lt;/code&gt; → &lt;code&gt;ClaudeAgentOptions&lt;/code&gt; rename&lt;/li&gt; 
 &lt;li&gt;Merged system prompt configuration&lt;/li&gt; 
 &lt;li&gt;Settings isolation and explicit control&lt;/li&gt; 
 &lt;li&gt;New programmatic subagents and session forking features&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Development&lt;/h2&gt; 
&lt;p&gt;If you're contributing to this project, run the initial setup script to install git hooks:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;./scripts/initial-setup.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This installs a pre-push hook that runs lint checks before pushing, matching the CI workflow. To skip the hook temporarily, use &lt;code&gt;git push --no-verify&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;MIT&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>minio/minio</title>
      <link>https://github.com/minio/minio</link>
      <description>&lt;p&gt;MinIO is a high-performance, S3 compatible object store, open sourced under GNU AGPLv3 license.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MinIO Quickstart Guide&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://slack.min.io"&gt;&lt;img src="https://slack.min.io/slack?type=svg" alt="Slack" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/minio/minio/"&gt;&lt;img src="https://img.shields.io/docker/pulls/minio/minio.svg?maxAge=604800" alt="Docker Pulls" /&gt;&lt;/a&gt; &lt;a href="https://github.com/minio/minio/raw/master/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/license-AGPL%20V3-blue" alt="license" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://min.io"&gt;&lt;img src="https://raw.githubusercontent.com/minio/minio/master/.github/logo.svg?sanitize=true" alt="MinIO" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;MinIO is a high-performance, S3-compatible object storage solution released under the GNU AGPL v3.0 license. Designed for speed and scalability, it powers AI/ML, analytics, and data-intensive workloads with industry-leading performance.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;S3 API Compatible – Seamless integration with existing S3 tools&lt;/li&gt; 
 &lt;li&gt;Built for AI &amp;amp; Analytics – Optimized for large-scale data pipelines&lt;/li&gt; 
 &lt;li&gt;High Performance – Ideal for demanding storage workloads.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This README provides instructions for building MinIO from source and deploying onto baremetal hardware. Use the &lt;a href="https://github.com/minio/docs"&gt;MinIO Documentation&lt;/a&gt; project to build and host a local copy of the documentation.&lt;/p&gt; 
&lt;h2&gt;MinIO is Open Source Software&lt;/h2&gt; 
&lt;p&gt;We designed MinIO as Open Source software for the Open Source software community. We encourage the community to remix, redesign, and reshare MinIO under the terms of the AGPLv3 license.&lt;/p&gt; 
&lt;p&gt;All usage of MinIO in your application stack requires validation against AGPLv3 obligations, which include but are not limited to the release of modified code to the community from which you have benefited. Any commercial/proprietary usage of the AGPLv3 software, including repackaging or reselling services/features, is done at your own risk.&lt;/p&gt; 
&lt;p&gt;The AGPLv3 provides no obligation by any party to support, maintain, or warranty the original or any modified work. All support is provided on a best-effort basis through Github and our &lt;a href="https://raw.githubusercontent.com/minio/minio/master/https//slack.min.io"&gt;Slack&lt;/a&gt; channel, and any member of the community is welcome to contribute and assist others in their usage of the software.&lt;/p&gt; 
&lt;p&gt;MinIO &lt;a href="https://www.min.io/product/aistor"&gt;AIStor&lt;/a&gt; includes enterprise-grade support and licensing for workloads which require commercial or proprietary usage and production-level SLA/SLO-backed support. For more information, &lt;a href="https://min.io/pricing"&gt;reach out for a quote&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Source-Only Distribution&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Important:&lt;/strong&gt; The MinIO community edition is now distributed as source code only. We will no longer provide pre-compiled binary releases for the community version.&lt;/p&gt; 
&lt;h3&gt;Installing Latest MinIO Community Edition&lt;/h3&gt; 
&lt;p&gt;To use MinIO community edition, you have two options:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Install from source&lt;/strong&gt; using &lt;code&gt;go install github.com/minio/minio@latest&lt;/code&gt; (recommended)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Build a Docker image&lt;/strong&gt; from the provided Dockerfile&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;See the sections below for detailed instructions on each method.&lt;/p&gt; 
&lt;h3&gt;Legacy Binary Releases&lt;/h3&gt; 
&lt;p&gt;Historical pre-compiled binary releases remain available for reference but are no longer maintained:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;GitHub Releases: &lt;a href="https://github.com/minio/minio/releases"&gt;https://github.com/minio/minio/releases&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Direct downloads: &lt;a href="https://dl.min.io/server/minio/release/"&gt;https://dl.min.io/server/minio/release/&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;These legacy binaries will not receive updates.&lt;/strong&gt; We strongly recommend using source builds for access to the latest features, bug fixes, and security updates.&lt;/p&gt; 
&lt;h2&gt;Install from Source&lt;/h2&gt; 
&lt;p&gt;Use the following commands to compile and run a standalone MinIO server from source. If you do not have a working Golang environment, please follow &lt;a href="https://golang.org/doc/install"&gt;How to install Golang&lt;/a&gt;. Minimum version required is &lt;a href="https://golang.org/dl/#stable"&gt;go1.24&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;go install github.com/minio/minio@latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can alternatively run &lt;code&gt;go build&lt;/code&gt; and use the &lt;code&gt;GOOS&lt;/code&gt; and &lt;code&gt;GOARCH&lt;/code&gt; environment variables to control the OS and architecture target. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;env GOOS=linux GOARCh=arm64 go build
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Start MinIO by running &lt;code&gt;minio server PATH&lt;/code&gt; where &lt;code&gt;PATH&lt;/code&gt; is any empty folder on your local filesystem.&lt;/p&gt; 
&lt;p&gt;The MinIO deployment starts using default root credentials &lt;code&gt;minioadmin:minioadmin&lt;/code&gt;. You can test the deployment using the MinIO Console, an embedded web-based object browser built into MinIO Server. Point a web browser running on the host machine to &lt;a href="http://127.0.0.1:9000"&gt;http://127.0.0.1:9000&lt;/a&gt; and log in with the root credentials. You can use the Browser to create buckets, upload objects, and browse the contents of the MinIO server.&lt;/p&gt; 
&lt;p&gt;You can also connect using any S3-compatible tool, such as the MinIO Client &lt;code&gt;mc&lt;/code&gt; commandline tool:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;mc alias set local http://localhost:9000 minioadmin minioadmin
mc admin info local
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/minio/minio/master/#test-using-minio-client-mc"&gt;Test using MinIO Client &lt;code&gt;mc&lt;/code&gt;&lt;/a&gt; for more information on using the &lt;code&gt;mc&lt;/code&gt; commandline tool. For application developers, see &lt;a href="https://docs.min.io/enterprise/aistor-object-store/developers/sdk/"&gt;https://docs.min.io/enterprise/aistor-object-store/developers/sdk/&lt;/a&gt; to view MinIO SDKs for supported languages.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Production environments using compiled-from-source MinIO binaries do so at their own risk. The AGPLv3 license provides no warranties nor liabilites for any such usage.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Build Docker Image&lt;/h2&gt; 
&lt;p&gt;You can use the &lt;code&gt;docker build .&lt;/code&gt; command to build a Docker image on your local host machine. You must first &lt;a href="https://raw.githubusercontent.com/minio/minio/master/#install-from-source"&gt;build MinIO&lt;/a&gt; and ensure the &lt;code&gt;minio&lt;/code&gt; binary exists in the project root.&lt;/p&gt; 
&lt;p&gt;The following command builds the Docker image using the default &lt;code&gt;Dockerfile&lt;/code&gt; in the root project directory with the repository and image tag &lt;code&gt;myminio:minio&lt;/code&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;docker build -t myminio:minio .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Use &lt;code&gt;docker image ls&lt;/code&gt; to confirm the image exists in your local repository. You can run the server using standard Docker invocation:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;docker run -p 9000:9000 -p 9001:9001 myminio:minio server /tmp/minio --console-address :9001
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Complete documentation for building Docker containers, managing custom images, or loading images into orchestration platforms is out of scope for this documentation. You can modify the &lt;code&gt;Dockerfile&lt;/code&gt; and &lt;code&gt;dockerscripts/docker-entrypoint.sh&lt;/code&gt; as-needed to reflect your specific image requirements.&lt;/p&gt; 
&lt;p&gt;See the &lt;a href="https://docs.min.io/community/minio-object-store/operations/deployments/baremetal-deploy-minio-as-a-container.html#deploy-minio-container"&gt;MinIO Container&lt;/a&gt; documentation for more guidance on running MinIO within a Container image.&lt;/p&gt; 
&lt;h2&gt;Install using Helm Charts&lt;/h2&gt; 
&lt;p&gt;There are two paths for installing MinIO onto Kubernetes infrastructure:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Use the &lt;a href="https://github.com/minio/operator"&gt;MinIO Operator&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Use the community-maintained &lt;a href="https://github.com/minio/minio/tree/master/helm/minio"&gt;Helm charts&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See the &lt;a href="https://docs.min.io/community/minio-object-store/operations/deployments/kubernetes.html"&gt;MinIO Documentation&lt;/a&gt; for guidance on deploying using the Operator. The Community Helm chart has instructions in the folder-level README.&lt;/p&gt; 
&lt;h2&gt;Test MinIO Connectivity&lt;/h2&gt; 
&lt;h3&gt;Test using MinIO Console&lt;/h3&gt; 
&lt;p&gt;MinIO Server comes with an embedded web based object browser. Point your web browser to &lt;a href="http://127.0.0.1:9000"&gt;http://127.0.0.1:9000&lt;/a&gt; to ensure your server has started successfully.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] MinIO runs console on random port by default, if you wish to choose a specific port use &lt;code&gt;--console-address&lt;/code&gt; to pick a specific interface and port.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Test using MinIO Client &lt;code&gt;mc&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;mc&lt;/code&gt; provides a modern alternative to UNIX commands like ls, cat, cp, mirror, diff etc. It supports filesystems and Amazon S3 compatible cloud storage services.&lt;/p&gt; 
&lt;p&gt;The following commands set a local alias, validate the server information, create a bucket, copy data to that bucket, and list the contents of the bucket.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;mc alias set local http://localhost:9000 minioadmin minioadmin
mc admin info
mc mb data
mc cp ~/Downloads/mydata data/
mc ls data/
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Follow the MinIO Client &lt;a href="https://docs.min.io/community/minio-object-store/reference/minio-mc.html#quickstart"&gt;Quickstart Guide&lt;/a&gt; for further instructions.&lt;/p&gt; 
&lt;h2&gt;Explore Further&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.min.io/community/minio-object-store/index.html"&gt;The MinIO documentation website&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.min.io/community/minio-object-store/operations/concepts/erasure-coding.html"&gt;MinIO Erasure Code Overview&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.min.io/community/minio-object-store/reference/minio-mc.html"&gt;Use &lt;code&gt;mc&lt;/code&gt; with MinIO Server&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.min.io/enterprise/aistor-object-store/developers/sdk/go/"&gt;Use &lt;code&gt;minio-go&lt;/code&gt; SDK with MinIO Server&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contribute to MinIO Project&lt;/h2&gt; 
&lt;p&gt;Please follow MinIO &lt;a href="https://github.com/minio/minio/raw/master/CONTRIBUTING.md"&gt;Contributor's Guide&lt;/a&gt; for guidance on making new contributions to the repository.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;MinIO source is licensed under the &lt;a href="https://github.com/minio/minio/raw/master/LICENSE"&gt;GNU AGPLv3&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;MinIO &lt;a href="https://github.com/minio/minio/tree/master/docs"&gt;documentation&lt;/a&gt; is licensed under &lt;a href="https://creativecommons.org/licenses/by/4.0/"&gt;CC BY 4.0&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/minio/minio/raw/master/COMPLIANCE.md"&gt;License Compliance&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>anthropics/claude-cookbooks</title>
      <link>https://github.com/anthropics/claude-cookbooks</link>
      <description>&lt;p&gt;A collection of notebooks/recipes showcasing some fun and effective ways of using Claude.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Claude Cookbooks&lt;/h1&gt; 
&lt;p&gt;The Claude Cookbooks provide code and guides designed to help developers build with Claude, offering copy-able code snippets that you can easily integrate into your own projects.&lt;/p&gt; 
&lt;h2&gt;Prerequisites&lt;/h2&gt; 
&lt;p&gt;To make the most of the examples in this cookbook, you'll need an Claude API key (sign up for free &lt;a href="https://www.anthropic.com"&gt;here&lt;/a&gt;).&lt;/p&gt; 
&lt;p&gt;While the code examples are primarily written in Python, the concepts can be adapted to any programming language that supports interaction with the Claude API.&lt;/p&gt; 
&lt;p&gt;If you're new to working with the Claude API, we recommend starting with our &lt;a href="https://github.com/anthropics/courses/tree/master/anthropic_api_fundamentals"&gt;Claude API Fundamentals course&lt;/a&gt; to get a solid foundation.&lt;/p&gt; 
&lt;h2&gt;Explore Further&lt;/h2&gt; 
&lt;p&gt;Looking for more resources to enhance your experience with Claude and AI assistants? Check out these helpful links:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.claude.com/claude/docs/guide-to-anthropics-prompt-engineering-resources"&gt;Anthropic developer documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://support.anthropic.com"&gt;Anthropic support docs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.anthropic.com/discord"&gt;Anthropic Discord community&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;The Claude Cookbooks thrives on the contributions of the developer community. We value your input, whether it's submitting an idea, fixing a typo, adding a new guide, or improving an existing one. By contributing, you help make this resource even more valuable for everyone.&lt;/p&gt; 
&lt;p&gt;To avoid duplication of efforts, please review the existing issues and pull requests before contributing.&lt;/p&gt; 
&lt;p&gt;If you have ideas for new examples or guides, share them on the &lt;a href="https://github.com/anthropics/anthropic-cookbook/issues"&gt;issues page&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Table of recipes&lt;/h2&gt; 
&lt;h3&gt;Capabilities&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/tree/main/capabilities/classification"&gt;Classification&lt;/a&gt;: Explore techniques for text and data classification using Claude.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/tree/main/capabilities/retrieval_augmented_generation"&gt;Retrieval Augmented Generation&lt;/a&gt;: Learn how to enhance Claude's responses with external knowledge.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/tree/main/capabilities/summarization"&gt;Summarization&lt;/a&gt;: Discover techniques for effective text summarization with Claude.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Tool Use and Integration&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/tree/main/tool_use"&gt;Tool use&lt;/a&gt;: Learn how to integrate Claude with external tools and functions to extend its capabilities. 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/raw/main/tool_use/customer_service_agent.ipynb"&gt;Customer service agent&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/raw/main/tool_use/calculator_tool.ipynb"&gt;Calculator integration&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/raw/main/misc/how_to_make_sql_queries.ipynb"&gt;SQL queries&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Third-Party Integrations&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/tree/main/third_party"&gt;Retrieval augmented generation&lt;/a&gt;: Supplement Claude's knowledge with external data sources. 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/raw/main/third_party/Pinecone/rag_using_pinecone.ipynb"&gt;Vector databases (Pinecone)&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/raw/main/third_party/Wikipedia/wikipedia-search-cookbook.ipynb/"&gt;Wikipedia&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/raw/main/misc/read_web_pages_with_haiku.ipynb"&gt;Web pages&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/raw/main/third_party/VoyageAI/how_to_create_embeddings.md"&gt;Embeddings with Voyage AI&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Multimodal Capabilities&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/tree/main/multimodal"&gt;Vision with Claude&lt;/a&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/raw/main/multimodal/getting_started_with_vision.ipynb"&gt;Getting started with images&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/raw/main/multimodal/best_practices_for_vision.ipynb"&gt;Best practices for vision&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/raw/main/multimodal/reading_charts_graphs_powerpoints.ipynb"&gt;Interpreting charts and graphs&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/raw/main/multimodal/how_to_transcribe_text.ipynb"&gt;Extracting content from forms&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/raw/main/misc/illustrated_responses.ipynb"&gt;Generate images with Claude&lt;/a&gt;: Use Claude with Stable Diffusion for image generation.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Advanced Techniques&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/raw/main/multimodal/using_sub_agents.ipynb"&gt;Sub-agents&lt;/a&gt;: Learn how to use Haiku as a sub-agent in combination with Opus.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/raw/main/misc/pdf_upload_summarization.ipynb"&gt;Upload PDFs to Claude&lt;/a&gt;: Parse and pass PDFs as text to Claude.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/raw/main/misc/building_evals.ipynb"&gt;Automated evaluations&lt;/a&gt;: Use Claude to automate the prompt evaluation process.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/raw/main/misc/how_to_enable_json_mode.ipynb"&gt;Enable JSON mode&lt;/a&gt;: Ensure consistent JSON output from Claude.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/raw/main/misc/building_moderation_filter.ipynb"&gt;Create a moderation filter&lt;/a&gt;: Use Claude to create a content moderation filter for your application.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/anthropics/anthropic-cookbook/raw/main/misc/prompt_caching.ipynb"&gt;Prompt caching&lt;/a&gt;: Learn techniques for efficient prompt caching with Claude.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Additional Resources&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aws-samples/anthropic-on-aws"&gt;Anthropic on AWS&lt;/a&gt;: Explore examples and solutions for using Claude on AWS infrastructure.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aws-samples/"&gt;AWS Samples&lt;/a&gt;: A collection of code samples from AWS which can be adapted for use with Claude. Note that some samples may require modification to work optimally with Claude.&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>evershopcommerce/evershop</title>
      <link>https://github.com/evershopcommerce/evershop</link>
      <description>&lt;p&gt;🛍️ Typescript E-commerce Platform&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img width="60" height="68" alt="EverShop Logo" src="https://raw.githubusercontent.com/evershopcommerce/evershop/dev/.github/images/logo-green.png" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;/p&gt;
&lt;h1 align="center"&gt;EverShop&lt;/h1&gt; 
&lt;p&gt;&lt;/p&gt; 
&lt;h4 align="center"&gt; &lt;a href="https://evershop.io/docs/development/getting-started/introduction"&gt;Documentation&lt;/a&gt; | &lt;a href="https://demo.evershop.io/"&gt;Demo&lt;/a&gt; &lt;/h4&gt; 
&lt;p align="center"&gt; &lt;img src="https://github.com/evershopcommerce/evershop/actions/workflows/build_test.yml/badge.svg?sanitize=true" alt="Github Action" /&gt; &lt;a href="https://twitter.com/evershopjs"&gt; &lt;img alt="Twitter Follow" src="https://img.shields.io/twitter/follow/evershopjs?style=social" /&gt; &lt;/a&gt; &lt;a href="https://discord.gg/GSzt7dt7RM"&gt; &lt;img src="https://img.shields.io/discord/757179260417867879?label=discord" alt="Discord" /&gt; &lt;/a&gt; &lt;a href="https://opensource.org/licenses/GPL-3.0"&gt; &lt;img src="https://img.shields.io/badge/License-GPLv3-blue.svg?sanitize=true" alt="License" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;img alt="EverShop" width="950" src="https://raw.githubusercontent.com/evershopcommerce/evershop/dev/.github/images/banner.png" /&gt; &lt;/p&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;EverShop is a modern, TypeScript-first eCommerce platform built with GraphQL and React. Designed for developers, it offers essential commerce features in a modular, fully customizable architecture—perfect for building tailored shopping experiences with confidence and speed.&lt;/p&gt; 
&lt;h2&gt;Installation Using Docker&lt;/h2&gt; 
&lt;p&gt;You can get started with EverShop in minutes by using the Docker image. The Docker image is a great way to get started with EverShop without having to worry about installing dependencies or configuring your environment.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -sSL https://raw.githubusercontent.com/evershopcommerce/evershop/main/docker-compose.yml &amp;gt; docker-compose.yml
docker-compose up -d
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For the full installation guide, please refer to our &lt;a href="https://evershop.io/docs/development/getting-started/installation-guide"&gt;Installation guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://evershop.io/docs/development/getting-started/installation-guide"&gt;Installation guide&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://evershop.io/docs/development/module/create-your-first-extension"&gt;Extension development&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://evershop.io/docs/development/theme/theme-overview"&gt;Theme development&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Demo&lt;/h2&gt; 
&lt;p&gt;Explore our demo store.&lt;/p&gt; 
&lt;p align="left"&gt; &lt;a href="https://demo.evershop.io/admin" target="_blank"&gt; &lt;img alt="EverShop Admin Demo" height="35" src="https://raw.githubusercontent.com/evershopcommerce/evershop/dev/.github/images/evershop-demo-back.png" /&gt; &lt;/a&gt; &lt;a href="https://demo.evershop.io/" target="_blank"&gt; &lt;img alt="EverShop Store Demo" height="35" src="https://raw.githubusercontent.com/evershopcommerce/evershop/dev/.github/images/evershop-demo-front.png" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;b&gt;Demo user:&lt;/b&gt; 
&lt;p&gt;Email: &lt;a href="mailto:demo@evershop.io"&gt;demo@evershop.io&lt;/a&gt;&lt;br /&gt; Password: 123456&lt;/p&gt; 
&lt;h2&gt;Support&lt;/h2&gt; 
&lt;p&gt;If you like my work, feel free to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;⭐ this repository. It helps.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://twitter.com/intent/tweet?url=https%3A%2F%2Fgithub.com%2Fevershopcommerce%2Fevershop&amp;amp;text=Awesome%20React%20Ecommerce%20Project&amp;amp;hashtags=react,ecommerce,expressjs,graphql"&gt;&lt;img src="https://img.shields.io/twitter/url/http/shields.io.svg?style=social" alt="Tweet" /&gt;&lt;/a&gt; about EverShop. Thank you!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;EverShop is an open-source project. We are committed to a fully transparent development process and appreciate highly any contributions. Whether you are helping us fix bugs, proposing new features, improving our documentation or spreading the word - we would love to have you as part of the EverShop community.&lt;/p&gt; 
&lt;h3&gt;Ask a question about EverShop&lt;/h3&gt; 
&lt;p&gt;You can ask questions, and participate in discussions about EverShop-related topics in the EverShop Discord channel.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://discord.gg/GSzt7dt7RM"&gt;&lt;img src="https://raw.githubusercontent.com/evershopcommerce/evershop/dev/.github/images/discord_banner_github.svg?sanitize=true" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Create a bug report&lt;/h3&gt; 
&lt;p&gt;If you see an error message or run into an issue, please &lt;a href="https://github.com/evershopcommerce/evershop/issues/new"&gt;create bug report&lt;/a&gt;. This effort is valued and it will help all EverShop users.&lt;/p&gt; 
&lt;h3&gt;Submit a feature request&lt;/h3&gt; 
&lt;p&gt;If you have an idea, or you're missing a capability that would make development easier and more robust, please &lt;a href="https://github.com/evershopcommerce/evershop/issues/new"&gt;Submit feature request&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If a similar feature request already exists, don't forget to leave a "+1". If you add some more information such as your thoughts and vision about the feature, your comments will be embraced warmly :)&lt;/p&gt; 
&lt;p&gt;Please refer to our &lt;a href="https://raw.githubusercontent.com/evershopcommerce/evershop/dev/CONTRIBUTING.md"&gt;Contribution Guidelines&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/evershopcommerce/evershop/dev/CODE_OF_CONDUCT.md"&gt;Code of Conduct&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/evershopcommerce/evershop/raw/main/LICENSE"&gt;GPL-3.0 License&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>QwenLM/Qwen3-VL</title>
      <link>https://github.com/QwenLM/Qwen3-VL</link>
      <description>&lt;p&gt;Qwen3-VL is the multimodal large language model series developed by Qwen team, Alibaba Cloud.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Qwen3-VL&lt;/h1&gt; 
&lt;p align="center"&gt; &lt;img src="https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/qwen3vllogo.png" width="400" /&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p align="center"&gt; 💜 &lt;a href="https://chat.qwenlm.ai/"&gt;&lt;b&gt;Qwen Chat&lt;/b&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;🤗 &lt;a href="https://huggingface.co/collections/Qwen/qwen3-vl-68d2a7c1b8a8afce4ebd2dbe"&gt;Hugging Face&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;🤖 &lt;a href="https://modelscope.cn/collections/Qwen3-VL-5c7a94c8cb144b"&gt;ModelScope&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;📑 &lt;a href="https://qwen.ai/blog?id=99f0335c4ad9ff6153e517418d48535ab6d8afef&amp;amp;from=research.latest-advancements-list"&gt;Blog&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;📚 &lt;a href="https://github.com/QwenLM/Qwen3-VL/tree/main/cookbooks"&gt;Cookbooks&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;📑 Paper is coming&amp;nbsp;&amp;nbsp; &lt;br /&gt; 🖥️ &lt;a href="https://huggingface.co/spaces/Qwen/Qwen3-VL-Demo"&gt;Demo&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;💬 &lt;a href="https://github.com/QwenLM/Qwen/raw/main/assets/wechat.png"&gt;WeChat (微信)&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;🫨 &lt;a href="https://discord.gg/CV4E9rpNSD"&gt;Discord&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;📑 &lt;a href="https://help.aliyun.com/zh/model-studio/developer-reference/qwen-vl-api"&gt;API&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;🖥️ &lt;a href="https://gallery.pai-ml.com/#/preview/deepLearning/cv/qwen2.5-vl"&gt;PAI-DSW&lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;Meet Qwen3-VL — the most powerful vision-language model in the Qwen series to date.&lt;/p&gt; 
&lt;p&gt;This generation delivers comprehensive upgrades across the board: superior text understanding &amp;amp; generation, deeper visual perception &amp;amp; reasoning, extended context length, enhanced spatial and video dynamics comprehension, and stronger agent interaction capabilities.&lt;/p&gt; 
&lt;p&gt;Available in Dense and MoE architectures that scale from edge to cloud, with Instruct and reasoning‑enhanced Thinking editions for flexible, on‑demand deployment.&lt;/p&gt; 
&lt;h4&gt;Key Enhancements:&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Visual Agent&lt;/strong&gt;: Operates PC/mobile GUIs—recognizes elements, understands functions, invokes tools, completes tasks.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Visual Coding Boost&lt;/strong&gt;: Generates Draw.io/HTML/CSS/JS from images/videos.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Advanced Spatial Perception&lt;/strong&gt;: Judges object positions, viewpoints, and occlusions; provides stronger 2D grounding and enables 3D grounding for spatial reasoning and embodied AI.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Long Context &amp;amp; Video Understanding&lt;/strong&gt;: Native 256K context, expandable to 1M; handles books and hours-long video with full recall and second-level indexing.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Enhanced Multimodal Reasoning&lt;/strong&gt;: Excels in STEM/Math—causal analysis and logical, evidence-based answers.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Upgraded Visual Recognition&lt;/strong&gt;: Broader, higher-quality pretraining is able to “recognize everything”—celebrities, anime, products, landmarks, flora/fauna, etc.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Expanded OCR&lt;/strong&gt;: Supports 32 languages (up from 10); robust in low light, blur, and tilt; better with rare/ancient characters and jargon; improved long-document structure parsing.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Text Understanding on par with pure LLMs&lt;/strong&gt;: Seamless text–vision fusion for lossless, unified comprehension.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Model Architecture Updates:&lt;/h4&gt; 
&lt;p align="center"&gt; &lt;img src="https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/qwen3vl_arc.jpg" width="80%" /&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Interleaved-MRoPE&lt;/strong&gt;: Full‑frequency allocation over time, width, and height via robust positional embeddings, enhancing long‑horizon video reasoning.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;DeepStack&lt;/strong&gt;: Fuses multi‑level ViT features to capture fine‑grained details and sharpen image–text alignment.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Text–Timestamp Alignment:&lt;/strong&gt; Moves beyond T‑RoPE to precise, timestamp‑grounded event localization for stronger video temporal modeling.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;2025.10.21: We have released the &lt;strong&gt;Qwen3-VL-2B&lt;/strong&gt; (&lt;a href="https://huggingface.co/Qwen/Qwen3-VL-2B-Instruct"&gt;Instruct&lt;/a&gt;/&lt;a href="https://huggingface.co/Qwen/Qwen3-VL-2B-Thinking"&gt;Thinking&lt;/a&gt;) and &lt;strong&gt;Qwen3-VL-32B&lt;/strong&gt; (&lt;a href="https://huggingface.co/Qwen/Qwen3-VL-32B-Instruct"&gt;Instruct&lt;/a&gt;/&lt;a href="https://huggingface.co/Qwen/Qwen3-VL-32B-Thinking"&gt;Thinking&lt;/a&gt;). Enjoy it!&lt;/li&gt; 
 &lt;li&gt;2025.10.15: We have released the &lt;strong&gt;Qwen3-VL-4B&lt;/strong&gt; (&lt;a href="https://huggingface.co/Qwen/Qwen3-VL-4B-Instruct"&gt;Instruct&lt;/a&gt;/&lt;a href="https://huggingface.co/Qwen/Qwen3-VL-4B-Thinking"&gt;Thinking&lt;/a&gt;) and &lt;strong&gt;Qwen3-VL-8B&lt;/strong&gt; (&lt;a href="https://huggingface.co/Qwen/Qwen3-VL-8B-Instruct"&gt;Instruct&lt;/a&gt;/&lt;a href="https://huggingface.co/Qwen/Qwen3-VL-8B-Thinking"&gt;Thinking&lt;/a&gt;). Enjoy it!&lt;/li&gt; 
 &lt;li&gt;2025.10.4: We have released the &lt;a href="https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Instruct"&gt;Qwen3-VL-30B-A3B-Instruct&lt;/a&gt; and &lt;a href="https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Thinking"&gt;Qwen3-VL-30B-A3B-Thinking&lt;/a&gt;. We have also released the FP8 version of the Qwen3-VL models — available in our &lt;a href="https://huggingface.co/collections/Qwen/qwen3-vl-68d2a7c1b8a8afce4ebd2dbe"&gt;HuggingFace collection&lt;/a&gt; and &lt;a href="https://modelscope.cn/collections/Qwen3-VL-5c7a94c8cb144b"&gt;ModelScope collection&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;2025.09.23: We have released the &lt;a href="https://huggingface.co/Qwen/Qwen3-VL-235B-A22B-Instruct"&gt;Qwen3-VL-235B-A22B-Instruct&lt;/a&gt; and &lt;a href="https://huggingface.co/Qwen/Qwen3-VL-235B-A22B-Thinking"&gt;Qwen3-VL-235B-A22B-Thinking&lt;/a&gt;. For more details, please check our &lt;a href="https://qwen.ai/blog?id=99f0335c4ad9ff6153e517418d48535ab6d8afef&amp;amp;from=research.latest-advancements-list"&gt;blog&lt;/a&gt;!&lt;/li&gt; 
 &lt;li&gt;2025.04.08: We provide the &lt;a href="https://github.com/QwenLM/Qwen2.5-VL/tree/main/qwen-vl-finetune"&gt;code&lt;/a&gt; for fine-tuning Qwen2-VL and Qwen2.5-VL.&lt;/li&gt; 
 &lt;li&gt;2025.03.25: We have released the &lt;a href="https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct"&gt;Qwen2.5-VL-32B&lt;/a&gt;. It is smarter and its responses align more closely with human preferences. For more details, please check our &lt;a href="https://qwenlm.github.io/blog/qwen2.5-vl-32b/"&gt;blog&lt;/a&gt;!&lt;/li&gt; 
 &lt;li&gt;2025.02.20: we have released the &lt;a href="https://arxiv.org/abs/2502.13923"&gt;Qwen2.5-VL Technical Report&lt;/a&gt;. Alongside the report, we have also released AWQ-quantized models for Qwen2.5-VL in three different sizes: &lt;a href="https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct-AWQ"&gt;3B&lt;/a&gt;, &lt;a href="https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct-AWQ"&gt;7B&lt;/a&gt; , and &lt;a href="https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct-AWQ"&gt;72B&lt;/a&gt; parameters.&lt;/li&gt; 
 &lt;li&gt;2025.01.28: We have released the &lt;a href="https://huggingface.co/Qwen"&gt;Qwen2.5-VL series&lt;/a&gt;. For more details, please check our &lt;a href="https://qwenlm.github.io/blog/qwen2.5-vl/"&gt;blog&lt;/a&gt;!&lt;/li&gt; 
 &lt;li&gt;2024.12.25: We have released the &lt;a href="https://huggingface.co/Qwen/QVQ-72B-Preview"&gt;QvQ-72B-Preview&lt;/a&gt;. QvQ-72B-Preview is an experimental research model, focusing on enhancing visual reasoning capabilities. For more details, please check our &lt;a href="https://qwenlm.github.io/blog/qvq-72b-preview/"&gt;blog&lt;/a&gt;!&lt;/li&gt; 
 &lt;li&gt;2024.09.19: The instruction-tuned &lt;a href="https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct"&gt;Qwen2-VL-72B model&lt;/a&gt; and its quantized version [&lt;a href="https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct-AWQ"&gt;AWQ&lt;/a&gt;, &lt;a href="https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct-GPTQ-Int4"&gt;GPTQ-Int4&lt;/a&gt;, &lt;a href="https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct-GPTQ-Int8"&gt;GPTQ-Int8&lt;/a&gt;] are now available. We have also released the &lt;a href="https://arxiv.org/pdf/2409.12191"&gt;Qwen2-VL paper&lt;/a&gt; simultaneously.&lt;/li&gt; 
 &lt;li&gt;2024.08.30: We have released the &lt;a href="https://huggingface.co/collections/Qwen/qwen2-vl-66cee7455501d7126940800d"&gt;Qwen2-VL series&lt;/a&gt;. The 2B and 7B models are now available, and the 72B model for open source is coming soon. For more details, please check our &lt;a href="https://qwenlm.github.io/blog/qwen2-vl/"&gt;blog&lt;/a&gt;!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Performance&lt;/h2&gt; 
&lt;h3&gt;Visual Tasks&lt;/h3&gt; 
&lt;div style="display: flex; justify-content: center; gap: 16px; flex-wrap: wrap;"&gt; 
 &lt;img src="https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/table_nothinking_vl.jpg" width="24%" /&gt; 
 &lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-VL/table_thinking_vl_.jpg" width="24%" /&gt; 
 &lt;img src="https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/table_nothinking_vl-30a3.jpg" width="26%" /&gt; 
 &lt;img src="https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/table_thinking_vl_30A3.jpg" width="22.5%" /&gt; 
&lt;/div&gt; 
&lt;div style="display: flex; justify-content: center; gap: 16px; flex-wrap: wrap;"&gt; 
 &lt;img src="https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/qwen3vl_2b_32b_vl_instruct.jpg" width="30%" /&gt; 
 &lt;img src="https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/qwen3vl_2b_32b_vl_thinking.jpg" width="24%" /&gt; 
&lt;/div&gt; 
&lt;h3&gt;Text-Centric Tasks&lt;/h3&gt; 
&lt;div style="display: flex; justify-content: center; gap: 16px; flex-wrap: wrap;"&gt; 
 &lt;img src="https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/table_nothinking_text.jpg" width="30%" /&gt; 
 &lt;img src="https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/table_thinking_text.jpg" width="32%" /&gt; 
 &lt;img src="https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/table_nothinking_text-30a3.jpg" width="30%" /&gt; 
&lt;/div&gt; 
&lt;div style="display: flex; justify-content: center; gap: 16px; flex-wrap: wrap;"&gt; 
 &lt;img src="https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/qwen3vl_4b_8b_text_instruct.jpg" width="33%" /&gt; 
 &lt;img src="https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/qwen3vl_4b_8b_text_thinking.jpg" width="28%" /&gt; 
&lt;/div&gt; 
&lt;h2&gt;Cookbooks&lt;/h2&gt; 
&lt;p&gt;We are preparing &lt;a href="https://github.com/QwenLM/Qwen3-VL/tree/main/cookbooks"&gt;cookbooks&lt;/a&gt; for many capabilities, including recognition, localization, document parsing, video understanding, key information extraction, and more. Welcome to learn more!&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Cookbook&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Open&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/omni_recognition.ipynb"&gt;Omni Recognition&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Not only identify animals, plants, people, and scenic spots but also recognize various objects such as cars and merchandise.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/omni_recognition.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/document_parsing.ipynb"&gt;Powerful Document Parsing Capabilities&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;The parsing of documents has reached a higher level, including not only text but also layout position information and our Qwen HTML format.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/document_parsing.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/2d_grounding.ipynb"&gt;Precise Object Grounding Across Formats&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Using relative position coordinates, it supports both boxes and points, allowing for diverse combinations of positioning and labeling tasks.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/2d_grounding.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/ocr.ipynb"&gt;General OCR and Key Information Extraction&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Stronger text recognition capabilities in natural scenes and multiple languages, supporting diverse key information extraction needs.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/ocr.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/video_understanding.ipynb"&gt;Video Understanding&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Better video OCR, long video understanding, and video grounding.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/video_understanding.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/mobile_agent.ipynb"&gt;Mobile Agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Locate and think for mobile phone control.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/mobile_agent.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/computer_use.ipynb"&gt;Computer-Use Agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Locate and think for controlling computers and Web.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/computer_use.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/3d_grounding.ipynb"&gt;3D Grounding&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Provide accurate 3D bounding boxes for both indoor and outdoor objects.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/3d_grounding.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/think_with_images.ipynb"&gt;Thinking with Images&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Utilize image_zoom_in_tool and search_tool to facilitate the model’s precise comprehension of fine-grained visual details within images.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/think_with_images.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/mmcode.ipynb"&gt;MultiModal Coding&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Generate accurate code based on rigorous comprehension of multimodal information.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/mmcode.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/long_document_understanding.ipynb"&gt;Long Document Understanding&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Achieve rigorous semantic comprehension of ultra-long documents.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/long_document_understanding.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/spatial_understanding.ipynb"&gt;Spatial Understanding&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;See, understand and reason about the spatial information&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/spatial_understanding.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Colab" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Quickstart&lt;/h2&gt; 
&lt;p&gt;Below, we provide simple examples to show how to use Qwen3-VL with 🤖 ModelScope and 🤗 Transformers.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;# The Qwen3-VL model requires transformers &amp;gt;= 4.57.0
pip install "transformers&amp;gt;=4.57.0"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;🤖 ModelScope&lt;/h3&gt; 
&lt;p&gt;We strongly advise users especially those in mainland China to use ModelScope. &lt;code&gt;snapshot_download&lt;/code&gt; can help you solve issues concerning downloading checkpoints.&lt;/p&gt; 
&lt;h3&gt;Using 🤗 Transformers to Chat&lt;/h3&gt; 
&lt;p&gt;Here we show a code snippet to show you how to use the chat model with &lt;code&gt;transformers&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from transformers import AutoModelForImageTextToText, AutoProcessor

# default: Load the model on the available device(s)
model = AutoModelForImageTextToText.from_pretrained(
    "Qwen/Qwen3-VL-235B-A22B-Instruct", dtype="auto", device_map="auto"
)

# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.
# model = AutoModelForImageTextToText.from_pretrained(
#     "Qwen/Qwen3-VL-235B-A22B-Instruct",
#     dtype=torch.bfloat16,
#     attn_implementation="flash_attention_2",
#     device_map="auto",
# )

processor = AutoProcessor.from_pretrained("Qwen/Qwen3-VL-235B-A22B-Instruct")

messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "image",
                "image": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg",
            },
            {"type": "text", "text": "Describe this image."},
        ],
    }
]

# Preparation for inference
inputs = processor.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,
    return_dict=True,
    return_tensors="pt"
)
inputs = inputs.to(model.device)

# Inference: Generation of the output
generated_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids_trimmed = [
    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
print(output_text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;!-- &lt;details&gt;
&lt;summary&gt;Minimum VRAM requirements&lt;/summary&gt;

| Precision | Qwen2.5-VL-3B | Qwen2.5-VL-7B | Qwen2.5-VL-72B |
|-----------|------------| --------- | -------- |
| FP32      | 11.5 GB    | 26.34 GB  | 266.21 GB |
| BF16      | 5.75 GB    | 13.17 GB  | 133.11 GB |
| INT8      | 2.87 GB    | 6.59 GB   | 66.5 GB |
| INT4      | 1.44 GB    | 3.29 GB   | 33.28 GB |

Note: The table above presents the theoretical minimum video memory requirements for inference with `transformers`; however, in practice, the actual memory usage is typically at least 1.2 times higher. For more information, see the linked resource [here](https://huggingface.co/docs/accelerate/main/en/usage_guides/model_size_estimator).
&lt;/details&gt; --&gt; 
&lt;details&gt; 
 &lt;summary&gt;Multi image inference&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Messages containing multiple images and a text query
messages = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "file:///path/to/image1.jpg"},
            {"type": "image", "image": "file:///path/to/image2.jpg"},
            {"type": "text", "text": "Identify the similarities between these images."},
        ],
    }
]

# Preparation for inference
inputs = processor.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,
    return_dict=True,
    return_tensors="pt"
)
inputs = inputs.to(model.device)

# Inference: Generation of the output
generated_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids_trimmed = [
    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
print(output_text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Video inference&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Messages containing a video url(or a local path) and a text query
messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "video",
                "video": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/space_woaudio.mp4",
            },
            {"type": "text", "text": "Describe this video."},
        ],
    }
]

# Preparation for inference
inputs = processor.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,
    return_dict=True,
    return_tensors="pt"
)
inputs = inputs.to(model.device)

# Inference: Generation of the output
generated_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids_trimmed = [
    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
print(output_text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Batch inference&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# for batch generation, padding_side should be set to left!
processor.tokenizer.padding_side = 'left'

# Sample messages for batch inference
messages1 = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "file:///path/to/image1.jpg"},
            {"type": "image", "image": "file:///path/to/image2.jpg"},
            {"type": "text", "text": "What are the common elements in these pictures?"},
        ],
    }
]
messages2 = [
    {"role": "system", "content": [{"type": "text", "text": "You are a helpful assistant."}]},
    {"role": "user", "content": [{"type": "text", "text": "Who are you?"}]},
]
# Combine messages for batch processing
messages = [messages1, messages2]

# Preparation for inference
inputs = processor.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,
    return_dict=True,
    return_tensors="pt",
    padding=True # padding should be set for batch generation!
)
inputs = inputs.to(model.device)

# Inference: Generation of the output
generated_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids_trimmed = [
    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
print(output_text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Pixel Control via Official Processor&lt;/summary&gt; 
 &lt;p&gt;Using the official HF processor, we can conveniently control the budget of visual tokens. Since the Qwen3-VL processor separates image and video processing, we can independently configure the pixel budget for each modality.&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;For the image processor&lt;/strong&gt;:&lt;br /&gt; The parameter &lt;code&gt;size['longest_edge']&lt;/code&gt; originally corresponds to &lt;code&gt;max_pixels&lt;/code&gt;, which defines the maximum number of pixels allowed for an image (i.e., for an image of height H and width W, H × W must not exceed &lt;code&gt;max_pixels&lt;/code&gt;; image channels are ignored for simplicity).&lt;br /&gt; Similarly, &lt;code&gt;size['shortest_edge']&lt;/code&gt; corresponds to &lt;code&gt;min_pixels&lt;/code&gt;, specifying the minimum allowable pixel count for an image.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;For the video processor&lt;/strong&gt;:&lt;br /&gt; The interpretation differs slightly. &lt;code&gt;size['longest_edge']&lt;/code&gt; represents the maximum total number of pixels across all frames in a video — for a video of shape T×H×W, the product T×H×W must not exceed &lt;code&gt;size['longest_edge']&lt;/code&gt;.&lt;br /&gt; Similarly, &lt;code&gt;size['shortest_edge']&lt;/code&gt; sets the minimum total pixel budget for the video.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;processor = AutoProcessor.from_pretrained("Qwen/Qwen3-VL-235B-A22B-Instruct")

# budget for image processor, since the compression ratio is 32 for Qwen3-VL, we can set the number of visual tokens of a single image to 256-1280
processor.image_processor.size = {"longest_edge": 1280*32*32, "shortest_edge": 256*32*32}

# budget for video processor, we can set the number of visual tokens of a single video to 256-16384
processor.video_processor.size = {"longest_edge": 16384*32*32, "shortest_edge": 256*32*32}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ul&gt; 
  &lt;li&gt;You can further control the &lt;strong&gt;sample fps&lt;/strong&gt; or &lt;strong&gt;sample frames&lt;/strong&gt; of video, as shown below.&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "video",
                "video": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/space_woaudio.mp4",
            },
            {"type": "text", "text": "Describe this video."},
        ],
    }
]

# for video input, we can further control the fps or num_frames. \
# defaultly, fps is set to 2

# set fps = 4
inputs = processor.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,
    return_dict=True,
    return_tensors="pt",
    fps=4
)
inputs = inputs.to(model.device)

# set num_frames = 128 and overwrite the fps to None!
# inputs = processor.apply_chat_template(
#     messages,
#     tokenize=True,
#     add_generation_prompt=True,
#     return_dict=True,
#     return_tensors="pt",
#     num_frames=128,
#     fps=None,
# )
# inputs = inputs.to(model.device)

# Inference: Generation of the output
generated_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids_trimmed = [
    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
print(output_text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;New &lt;code&gt;qwen-vl-utils&lt;/code&gt; Usage&lt;/h3&gt; 
&lt;p&gt;With the latest &lt;code&gt;qwen-vl-utils&lt;/code&gt; toolkit (backward compatible with Qwen2.5-VL), you can control pixel constraints per visual input.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install qwen-vl-utils==0.0.14
# It's highly recommended to use `[decord]` feature for faster video loading.
# pip install qwen-vl-utils[decord]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Compared to previous version, the new &lt;code&gt;qwen-vl-utils&lt;/code&gt; introduces:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;"image_patch_size": &lt;code&gt;14&lt;/code&gt; for Qwen2.5-VL and &lt;code&gt;16&lt;/code&gt; for Qwen3-VL. Default set to &lt;code&gt;14&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;"return_video_metadata"(Qwen3-VL only): Due to the new video processor, if True, each video returns as (video_tensor, video_metadata). Default set to &lt;code&gt;False&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# for Qwen2.5VL, you can simply call 
images, videos, video_kwargs = process_vision_info(messages, return_video_kwargs=True)

# For Qwen3VL series, you should call 
images, videos, video_kwargs = process_vision_info(messages, image_patch_size=16, return_video_kwargs=True, return_video_metadata=True)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;📌 Note: Since &lt;code&gt;qwen-vl-utils&lt;/code&gt; already resizes images/videos, pass &lt;code&gt;do_resize=False&lt;/code&gt; to the processor to avoid duplicate resizing.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Process Images&lt;/summary&gt; 
 &lt;p&gt;For input images, we support local files, base64, and URLs.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# You can directly insert a local file path, a URL, or a base64-encoded image into the position where you want in the text.
## Local file path
messages = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "file:///path/to/your/image.jpg"},
            {"type": "text", "text": "Describe this image."},
        ],
    }
]
## Image URL
messages = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "http://path/to/your/image.jpg"},
            {"type": "text", "text": "Describe this image."},
        ],
    }
]
## Base64 encoded image
messages = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "data:image;base64,/9j/..."},
            {"type": "text", "text": "Describe this image."},
        ],
    }
]
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;We provide two methods for fine-grained control over the image size input to the model:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;Specify exact dimensions: Directly set resized_height and resized_width. These values will be rounded to the nearest multiple of 32 (32 for Qwen3VL, 28 for Qwen2.5VL).&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Define min_pixels and max_pixels: Images will be resized to maintain their aspect ratio within the range of min_pixels and max_pixels&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from transformers import AutoModelForImageTextToText, AutoProcessor
from qwen_vl_utils import process_vision_info

model = AutoModelForImageTextToText.from_pretrained(
    "Qwen/Qwen3-VL-235B-A22B-Instruct", dtype="auto", device_map="auto"
)

processor = AutoProcessor.from_pretrained("Qwen/Qwen3-VL-235B-A22B-Instruct")

# resized_height and resized_width
messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "image",
                "image": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg",
                "resized_height": 280,
                "resized_width": 420,
            },
            {"type": "text", "text": "Describe this image."},
        ],
    }
]

# min_pixels and max_pixels
messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "image",
                "image": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg",
                "min_pixels": 50176,
                "max_pixels": 50176,

            },
            {"type": "text", "text": "Describe this image."},
        ],
    }
]

# Preparation for inference with qwen-vl-utils
text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
images, videos = process_vision_info(messages, image_patch_size=16)

# since qwen-vl-utils has resize the images/videos, \
# we should pass do_resize=False to avoid duplicate operation in processor!
inputs = processor(text=text, images=images, videos=videos, do_resize=False, return_tensors="pt")
inputs = inputs.to(model.device)

# Inference: Generation of the output
generated_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids_trimmed = [
    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
print(output_text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Process Videos&lt;/summary&gt; 
 &lt;p&gt;For input videos, we support images lists, local path and url.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# Messages containing a images list as a video and a text query
messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "video",
                "video": [
                    "file:///path/to/frame1.jpg",
                    "file:///path/to/frame2.jpg",
                    "file:///path/to/frame3.jpg",
                    "file:///path/to/frame4.jpg",
                ],
                'sample_fps':'1', # sample_fps: frame sampling rate (frames per second), used to determine timestamps for each frame
            },
            {"type": "text", "text": "Describe this video."},
        ],
    }
]

# Messages containing a local video path and a text query
messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "video",
                "video": "file:///path/to/video1.mp4",
                "max_pixels": 360 * 420,
                "fps": 1.0,
            },
            {"type": "text", "text": "Describe this video."},
        ],
    }
]

# Messages containing a video url and a text query
messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "video",
                "video": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/space_woaudio.mp4",
                "min_pixels": 4 * 32 * 32,
                "max_pixels": 256 * 32 * 32,
                "total_pixels": 20480 * 32 * 32,
            },
            {"type": "text", "text": "Describe this video."},
        ],
    }
]

&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;We recommend setting appropriate values for the &lt;code&gt;min_pixels&lt;/code&gt; and &lt;code&gt;max_pixels&lt;/code&gt; parameters based on available GPU memory and the specific application scenario to restrict the resolution of individual frames in the video.&lt;/p&gt; 
 &lt;p&gt;Alternatively, you can use the &lt;code&gt;total_pixels&lt;/code&gt; parameter to limit the total number of tokens in the video (it is recommended to set this value below 24576 * 32 * 32 to avoid excessively long input sequences). For more details on parameter usage and processing logic, please refer to the &lt;code&gt;fetch_video&lt;/code&gt; function in &lt;code&gt;qwen_vl_utils/vision_process.py&lt;/code&gt;.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from transformers import AutoModelForImageTextToText, AutoProcessor
from qwen_vl_utils import process_vision_info

model = AutoModelForImageTextToText.from_pretrained(
    "Qwen/Qwen3-VL-235B-A22B-Instruct", dtype="auto", device_map="auto"
)

processor = AutoProcessor.from_pretrained("Qwen/Qwen3-VL-235B-A22B-Instruct")

messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "video",
                "video": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/space_woaudio.mp4",
                "min_pixels": 4 * 32 * 32,
                "max_pixels": 256 * 32 * 32,
                "total_pixels": 20480 * 32 * 32,
            },
            {"type": "text", "text": "Describe this video."},
        ],
    }
]

text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
images, videos, video_kwargs = process_vision_info(messages, image_patch_size=16, return_video_kwargs=True, return_video_metadata=True)

# split the videos and according metadatas
if videos is not None:
    videos, video_metadatas = zip(*videos)
    videos, video_metadatas = list(videos), list(video_metadatas)
else:
    video_metadatas = None

# since qwen-vl-utils has resize the images/videos, \
# we should pass do_resize=False to avoid duplicate operation in processor!
inputs = processor(text=text, images=images, videos=videos, video_metadata=video_metadatas, return_tensors="pt", do_resize=False, **video_kwargs)
inputs = inputs.to(model.device)

# Inference: Generation of the output
generated_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids_trimmed = [
    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
print(output_text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Video Backends and URL Compatibility&lt;/summary&gt; 
 &lt;p&gt;Currently, &lt;code&gt;qwen-vl-utils&lt;/code&gt; supports three video decoding backends: &lt;code&gt;torchvision&lt;/code&gt;, &lt;code&gt;decord&lt;/code&gt;, and &lt;code&gt;torchcodec&lt;/code&gt;. While &lt;code&gt;decord&lt;/code&gt; and &lt;code&gt;torchcodec&lt;/code&gt; generally offer significantly faster decoding speeds compared to &lt;code&gt;torchvision&lt;/code&gt;, we recommend using &lt;code&gt;torchcodec&lt;/code&gt;. This is because &lt;code&gt;decord&lt;/code&gt; has known issues, such as decoding hangs, and its project is no longer actively maintained.&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;For &lt;code&gt;decord&lt;/code&gt;, if you are not using Linux, you might not be able to install &lt;code&gt;decord&lt;/code&gt; from PyPI. In that case, you can use &lt;code&gt;pip install qwen-vl-utils&lt;/code&gt; which will fall back to using torchvision for video processing. However, you can still &lt;a href="https://github.com/dmlc/decord?tab=readme-ov-file#install-from-source"&gt;install decord from source&lt;/a&gt; to get decord used when loading video.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;To use &lt;code&gt;torchcodec&lt;/code&gt; as the backend for video decoding, follow the installation instructions provided in the official &lt;a href="https://github.com/pytorch/torchcodec/tree/main?tab=readme-ov-file#installing-torchcodec"&gt;torchcodec repository&lt;/a&gt; and install it manually. Note that &lt;code&gt;torchcodec&lt;/code&gt; depends on FFmpeg for decoding functionality.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;Video URL compatibility is primarily determined by the version of the third-party library being used. For more details, refer to the table below. If you prefer not to use the default backend, you can switch it by setting &lt;code&gt;FORCE_QWENVL_VIDEO_READER&lt;/code&gt; to &lt;code&gt;torchvision&lt;/code&gt;, &lt;code&gt;decord&lt;/code&gt;, or &lt;code&gt;torchcodec&lt;/code&gt;.&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Backend&lt;/th&gt; 
    &lt;th&gt;HTTP&lt;/th&gt; 
    &lt;th&gt;HTTPS&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;torchvision &amp;gt;= 0.19.0&lt;/td&gt; 
    &lt;td&gt;✅&lt;/td&gt; 
    &lt;td&gt;✅&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;torchvision &amp;lt; 0.19.0&lt;/td&gt; 
    &lt;td&gt;❌&lt;/td&gt; 
    &lt;td&gt;❌&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;decord&lt;/td&gt; 
    &lt;td&gt;✅&lt;/td&gt; 
    &lt;td&gt;❌&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;torchcodec&lt;/td&gt; 
    &lt;td&gt;✅&lt;/td&gt; 
    &lt;td&gt;✅&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h3&gt;More Usage Tips&lt;/h3&gt; 
&lt;h4&gt;Add ids for Multiple Visual Inputs&lt;/h4&gt; 
&lt;p&gt;By default, images and video content are directly included in the conversation. When handling multiple images, it's helpful to add labels to the images and videos for better reference. Users can control this behavior with the following settings:&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Add vision ids&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;conversation = [
    {
        "role": "user",
        "content": [{"type": "image"}, {"type": "text", "text": "Hello, how are you?"}],
    },
    {
        "role": "assistant",
        "content": "I'm doing well, thank you for asking. How can I assist you today?",
    },
    {
        "role": "user",
        "content": [
            {"type": "text", "text": "Can you describe these images and video?"},
            {"type": "image"},
            {"type": "image"},
            {"type": "video"},
            {"type": "text", "text": "These are from my vacation."},
        ],
    },
    {
        "role": "assistant",
        "content": "I'd be happy to describe the images and video for you. Could you please provide more context about your vacation?",
    },
    {
        "role": "user",
        "content": "It was a trip to the mountains. Can you see the details in the images and video?",
    },
]

# default:
prompt_without_id = processor.apply_chat_template(
    conversation, add_generation_prompt=True
)
# Excepted output: '&amp;lt;|im_start|&amp;gt;system\nYou are a helpful assistant.&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;user\n&amp;lt;|vision_start|&amp;gt;&amp;lt;|image_pad|&amp;gt;&amp;lt;|vision_end|&amp;gt;Hello, how are you?&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;assistant\nI'm doing well, thank you for asking. How can I assist you today?&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;user\nCan you describe these images and video?&amp;lt;|vision_start|&amp;gt;&amp;lt;|image_pad|&amp;gt;&amp;lt;|vision_end|&amp;gt;&amp;lt;|vision_start|&amp;gt;&amp;lt;|image_pad|&amp;gt;&amp;lt;|vision_end|&amp;gt;&amp;lt;|vision_start|&amp;gt;&amp;lt;|video_pad|&amp;gt;&amp;lt;|vision_end|&amp;gt;These are from my vacation.&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;assistant\nI'd be happy to describe the images and video for you. Could you please provide more context about your vacation?&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;user\nIt was a trip to the mountains. Can you see the details in the images and video?&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;assistant\n'


# add ids
prompt_with_id = processor.apply_chat_template(
    conversation, add_generation_prompt=True, add_vision_id=True
)
# Excepted output: '&amp;lt;|im_start|&amp;gt;system\nYou are a helpful assistant.&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;user\nPicture 1: &amp;lt;|vision_start|&amp;gt;&amp;lt;|image_pad|&amp;gt;&amp;lt;|vision_end|&amp;gt;Hello, how are you?&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;assistant\nI'm doing well, thank you for asking. How can I assist you today?&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;user\nCan you describe these images and video?Picture 2: &amp;lt;|vision_start|&amp;gt;&amp;lt;|image_pad|&amp;gt;&amp;lt;|vision_end|&amp;gt;Picture 3: &amp;lt;|vision_start|&amp;gt;&amp;lt;|image_pad|&amp;gt;&amp;lt;|vision_end|&amp;gt;Video 1: &amp;lt;|vision_start|&amp;gt;&amp;lt;|video_pad|&amp;gt;&amp;lt;|vision_end|&amp;gt;These are from my vacation.&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;assistant\nI'd be happy to describe the images and video for you. Could you please provide more context about your vacation?&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;user\nIt was a trip to the mountains. Can you see the details in the images and video?&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;assistant\n'
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h4&gt;Flash-Attention 2 to speed up generation&lt;/h4&gt; 
&lt;p&gt;First, make sure to install the latest version of Flash Attention 2:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -U flash-attn --no-build-isolation
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Also, you should have a hardware that is compatible with Flash-Attention 2. Read more about it in the official documentation of the &lt;a href="https://github.com/Dao-AILab/flash-attention"&gt;flash attention repository&lt;/a&gt;. FlashAttention-2 can only be used when a model is loaded in &lt;code&gt;torch.float16&lt;/code&gt; or &lt;code&gt;torch.bfloat16&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;To load and run a model using Flash Attention-2, simply add &lt;code&gt;attn_implementation="flash_attention_2"&lt;/code&gt; when loading the model as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import torch
from transformers import AutoModelForImageTextToText

model = AutoModelForImageTextToText.from_pretrained(
    "Qwen/Qwen3-VL-235B-A22B-Instruct", 
    torch_dtype=torch.bfloat16, 
    attn_implementation="flash_attention_2",
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Processing Long Texts&lt;/h4&gt; 
&lt;p&gt;The current &lt;code&gt;config.json&lt;/code&gt; is set for context length up to 256K tokens. To handle extensive inputs exceeding 256K tokens, we utilize &lt;a href="https://arxiv.org/abs/2309.00071"&gt;YaRN&lt;/a&gt;, a technique for enhancing model length extrapolation, ensuring optimal performance on lengthy texts.&lt;/p&gt; 
&lt;p&gt;For supported frameworks (currently transformers and vLLM), you could modify &lt;code&gt;max_position_embeddings&lt;/code&gt; and &lt;code&gt;rope_scaling&lt;/code&gt; in &lt;code&gt;config.json&lt;/code&gt; to enable YaRN:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;{
    "max_position_embeddings": 1000000,
	...,
    "rope_scaling": {
        "rope_type": "yarn",
        "mrope_section": [
            24,
            20,
            20
        ],
        "mrope_interleaved": true,
        "factor": 3.0,
        "original_max_position_embeddings": 262144
    },
    ...
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;When using vLLM for serving, you can also enable YaRN by adding the additional arguments &lt;code&gt;--rope-scaling&lt;/code&gt; and &lt;code&gt;--max-model-len&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;vllm serve Qwen/Qwen3-VL-235B-A22B-Instruct --rope-scaling '{"rope_type":"yarn","factor":3.0,"original_max_position_embeddings": 262144,"mrope_section":[24,20,20],"mrope_interleaved": true}' --max-model-len 1000000
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Because Interleaved-MRoPE’s position IDs grow more slowly than vanilla RoPE, use a &lt;strong&gt;smaller scaling factor&lt;/strong&gt;. For example, to support 1M context with 256K context length, set factor=2 or 3 — not 4.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Try Qwen3-VL-235B-A22 with API!&lt;/h3&gt; 
&lt;p&gt;To explore Qwen3-VL-235B-A22, a more fascinating multimodal model, we encourage you to test our cutting-edge API service. Let's start the exciting journey right now!&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from openai import OpenAI

# set your DASHSCOPE_API_KEY here
DASHSCOPE_API_KEY = ""

client = OpenAI(
    api_key=DASHSCOPE_API_KEY,
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
)

completion = client.chat.completions.create(
    model="qwen3-vl-235b-a22b-instruct",
    messages=[{"role": "user", "content": [
        {"type": "image_url",
         "image_url": {"url": "https://dashscope.oss-cn-beijing.aliyuncs.com/images/dog_and_girl.jpeg"}},
        {"type": "text", "text": "这是什么"},
    ]}]
)
print(completion.model_dump_json())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more usage, please refer to the tutorial at &lt;a href="https://help.aliyun.com/zh/model-studio/developer-reference/qwen-vl-api"&gt;aliyun&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Web UI Example&lt;/h3&gt; 
&lt;p&gt;In this section, we provide instructions for users to build a web-based user interface (UI) demo. This UI demo allows users to interact with a predefined model or application through a web browser. Follow the steps below to get started.&lt;/p&gt; 
&lt;p&gt;Install the required dependencies by running the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r requirements_web_demo.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Launch a browser-based UI to interact with the model:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python web_demo_mm.py -c /your/path/to/qwen3vl/weight
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After running the command, you’ll see a link generated in the terminal similar to this:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Running on local: http://127.0.0.1:7860/
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Open the link in your browser to interact with the model — try text, images, or other features. For a quick start, you can also use our pre-built Docker image:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;cd docker &amp;amp;&amp;amp; bash run_web_demo.sh -c /your/path/to/qwen3vl/weight --port 8881
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Deployment&lt;/h2&gt; 
&lt;p&gt;We recommend using vLLM for fast Qwen3-VL deployment and inference. You need to install &lt;code&gt;vllm&amp;gt;=0.11.0&lt;/code&gt; to enable Qwen3-VL support. You can also use our &lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen3-VL/main/#-docker"&gt;official docker image&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Please check &lt;a href="https://docs.vllm.ai/en/latest/serving/multimodal_inputs.html"&gt;vLLM official documentation&lt;/a&gt; for more details about online serving and offline inference for multimodal models.&lt;/p&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install accelerate
pip install qwen-vl-utils==0.0.14
# Install the latest version of vLLM 'vllm&amp;gt;=0.11.0'
uv pip install -U vllm
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Online Serving&lt;/h3&gt; 
&lt;p&gt;You can start either a vLLM or SGLang server to serve LLMs efficiently, and then access it using an OpenAI-style API.&lt;/p&gt; 
&lt;p&gt;The following launch command is applicable to H100/H200; for more efficient deployment or deployment on other GPUs, please refer to the &lt;a href="https://docs.vllm.ai/projects/recipes/en/latest/Qwen/Qwen3-VL.html"&gt;vLLM community guide&lt;/a&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;vLLM server&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# Efficient inference with FP8 checkpoint
# Requires NVIDIA H100+ and CUDA 12+
vllm serve Qwen/Qwen3-VL-235B-A22B-Instruct-FP8 \
  --tensor-parallel-size 8 \
  --mm-encoder-tp-mode data \
  --enable-expert-parallel \
  --async-scheduling \
  --media-io-kwargs '{"video": {"num_frames": -1}}' \
  --host 0.0.0.0 \
  --port 22002
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;SGLang server&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;python -m sglang.launch_server \
   --model-path Qwen/Qwen3-VL-235B-A22B-Instruct \
   --host 0.0.0.0 \
   --port 22002 \
   --tp 8
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Image Request Example&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import time
from openai import OpenAI

client = OpenAI(
    api_key="EMPTY",
    base_url="http://127.0.0.1:22002/v1",
    timeout=3600
)

messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "image_url",
                "image_url": {
                    "url": "https://ofasys-multimodal-wlcb-3-toshanghai.oss-accelerate.aliyuncs.com/wpf272043/keepme/image/receipt.png"
                }
            },
            {
                "type": "text",
                "text": "Read all the text in the image."
            }
        ]
    }
]

start = time.time()
response = client.chat.completions.create(
    model="Qwen/Qwen3-VL-235B-A22B-Instruct-FP8",
    messages=messages,
    max_tokens=2048
)
print(f"Response costs: {time.time() - start:.2f}s")
print(f"Generated text: {response.choices[0].message.content}")
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Video Request Example&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import time
from openai import OpenAI

client = OpenAI(
    api_key="EMPTY",
    base_url="http://127.0.0.1:22002/v1",
    timeout=3600
)

messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "video_url",
                "video_url": {
                    "url": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/space_woaudio.mp4"
                }
            },
            {
                "type": "text",
                "text": "How long is this video?"
            }
        ]
    }
]

start = time.time()

# When vLLM is launched with `--media-io-kwargs '{"video": {"num_frames": -1}}'`,
# video frame sampling can be configured via `extra_body` (e.g., by setting `fps`).
# This feature is currently supported only in vLLM.
#
# By default, `fps=2` and `do_sample_frames=True`.
# With `do_sample_frames=True`, you can customize the `fps` value to set your desired video sampling rate.
response = client.chat.completions.create(
    model="Qwen/Qwen3-VL-235B-A22B-Instruct-FP8",
    messages=messages,
    max_tokens=2048,
    extra_body={"mm_processor_kwargs": {"fps": 2, "do_sample_frames": True}}
)

print(f"Response costs: {time.time() - start:.2f}s")
print(f"Generated text: {response.choices[0].message.content}")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Offline Inference&lt;/h3&gt; 
&lt;p&gt;You can also use vLLM or SGLang to inference Qwen3-VL locally:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;vLLM Examples&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# -*- coding: utf-8 -*-
import torch
from qwen_vl_utils import process_vision_info
from transformers import AutoProcessor
from vllm import LLM, SamplingParams

import os
os.environ['VLLM_WORKER_MULTIPROC_METHOD'] = 'spawn'

def prepare_inputs_for_vllm(messages, processor):
    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    # qwen_vl_utils 0.0.14+ reqired
    image_inputs, video_inputs, video_kwargs = process_vision_info(
        messages,
        image_patch_size=processor.image_processor.patch_size,
        return_video_kwargs=True,
        return_video_metadata=True
    )
    print(f"video_kwargs: {video_kwargs}")

    mm_data = {}
    if image_inputs is not None:
        mm_data['image'] = image_inputs
    if video_inputs is not None:
        mm_data['video'] = video_inputs

    return {
        'prompt': text,
        'multi_modal_data': mm_data,
        'mm_processor_kwargs': video_kwargs
    }


if __name__ == '__main__':
    # messages = [
    #     {
    #         "role": "user",
    #         "content": [
    #             {
    #                 "type": "video",
    #                 "video": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/space_woaudio.mp4",
    #             },
    #             {"type": "text", "text": "这段视频有多长"},
    #         ],
    #     }
    # ]

    messages = [
        {
            "role": "user",
            "content": [
              {
                  "type": "image",
                  "image": "https://ofasys-multimodal-wlcb-3-toshanghai.oss-accelerate.aliyuncs.com/wpf272043/keepme/image/receipt.png",
              },
              {"type": "text", "text": "Read all the text in the image."},
            ],
        }
    ]

    # TODO: change to your own checkpoint path
    checkpoint_path = "Qwen/Qwen3-VL-235B-A22B-Instruct-FP8"
    processor = AutoProcessor.from_pretrained(checkpoint_path)
    inputs = [prepare_inputs_for_vllm(message, processor) for message in [messages]]

    llm = LLM(
        model=checkpoint_path,
        mm_encoder_tp_mode="data",
        enable_expert_parallel=True,
        tensor_parallel_size=torch.cuda.device_count(),
        seed=0
    )

    sampling_params = SamplingParams(
        temperature=0,
        max_tokens=1024,
        top_k=-1,
        stop_token_ids=[],
    )

    for i, input_ in enumerate(inputs):
        print()
        print('=' * 40)
        print(f"Inputs[{i}]: {input_['prompt']=!r}")
    print('\n' + '&amp;gt;' * 40)

    outputs = llm.generate(inputs, sampling_params=sampling_params)
    for i, output in enumerate(outputs):
        generated_text = output.outputs[0].text
        print()
        print('=' * 40)
        print(f"Generated text: {generated_text!r}")
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;SGLang Examples&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import time
from PIL import Image
from sglang import Engine
from qwen_vl_utils import process_vision_info
from transformers import AutoProcessor, AutoConfig


if __name__ == "__main__":
    # TODO: change to your own checkpoint path
    checkpoint_path = "Qwen/Qwen3-VL-235B-A22B-Instruct"
    processor = AutoProcessor.from_pretrained(checkpoint_path)

    messages = [
        {
            "role": "user",
            "content": [
              {
                  "type": "image",
                  "image": "https://ofasys-multimodal-wlcb-3-toshanghai.oss-accelerate.aliyuncs.com/wpf272043/keepme/image/receipt.png",
              },
              {"type": "text", "text": "Read all the text in the image."},
            ],
        }
    ]

    text = processor.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    image_inputs, _ = process_vision_info(messages, image_patch_size=processor.image_processor.patch_size)

    llm = Engine(
        model_path=checkpoint_path,
        enable_multimodal=True,
        mem_fraction_static=0.8,
        tp_size=4,
        attention_backend="fa3",
        context_length=10240,
        disable_cuda_graph=True,
    )

    start = time.time()
    sampling_params = {"max_new_tokens": 1024}
    response = llm.generate(prompt=text, image_data=image_inputs, sampling_params=sampling_params)
    print(f"Response costs: {time.time() - start:.2f}s")
    print(f"Generated text: {response['text']}")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Evaluation Reproduction&lt;/h2&gt; 
&lt;p&gt;To facilitate faithful reproduction of our reported results, we summarize our official evaluation settings below.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Inference runtime: &lt;a href="https://github.com/vllm-project/vllm"&gt;vLLM&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Evaluation frameworks: &lt;a href="https://github.com/open-compass/VLMEvalKit"&gt;VLMEvalKit&lt;/a&gt;, &lt;a href="https://github.com/EvolvingLMMs-Lab/lmms-eval"&gt;lmms-eval&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Notes: 
  &lt;ul&gt; 
   &lt;li&gt;For a few benchmarks, we slightly modified the evaluation prompts; detailed changes will be documented in the upcoming technical report.&lt;/li&gt; 
   &lt;li&gt;A small number of benchmarks are internally constructed; we plan to release the code and reproduction assets afterwards.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Generation Hyperparameters&lt;/h3&gt; 
&lt;h4&gt;Instruct models&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export greedy='false'
export seed=3407
export top_p=0.8
export top_k=20
export temperature=0.7
export repetition_penalty=1.0
export presence_penalty=1.5
export out_seq_length=32768
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Thinking models&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export greedy='false'
export seed=1234
export top_p=0.95
export top_k=20
export repetition_penalty=1.0
export presence_penalty=0.0
export temperature=0.6
export out_seq_length=40960
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;🐳 Docker&lt;/h2&gt; 
&lt;p&gt;To simplify the deploy process, we provide docker images with pre-build environments: &lt;a href="https://hub.docker.com/r/qwenllm/qwenvl"&gt;qwenllm/qwenvl&lt;/a&gt;. You only need to install the driver and download model files to launch demos.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run --gpus all --ipc=host --network=host --rm --name qwen3vl -it qwenllm/qwenvl:qwen3vl-cu128 bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you find our paper and code useful in your research, please consider giving a star &lt;span&gt;⭐&lt;/span&gt; and citation &lt;span&gt;📝&lt;/span&gt; :)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-BibTeX"&gt;
@article{Qwen2.5-VL,
  title={Qwen2.5-VL Technical Report},
  author={Bai, Shuai and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Song, Sibo and Dang, Kai and Wang, Peng and Wang, Shijie and Tang, Jun and Zhong, Humen and Zhu, Yuanzhi and Yang, Mingkun and Li, Zhaohai and Wan, Jianqiang and Wang, Pengfei and Ding, Wei and Fu, Zheren and Xu, Yiheng and Ye, Jiabo and Zhang, Xi and Xie, Tianbao and Cheng, Zesen and Zhang, Hang and Yang, Zhibo and Xu, Haiyang and Lin, Junyang},
  journal={arXiv preprint arXiv:2502.13923},
  year={2025}
}

@article{Qwen2-VL,
  title={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution},
  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang},
  journal={arXiv preprint arXiv:2409.12191},
  year={2024}
}

@article{Qwen-VL,
  title={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},
  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2308.12966},
  year={2023}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;br /&gt;</description>
    </item>
    
    <item>
      <title>hsliuping/TradingAgents-CN</title>
      <link>https://github.com/hsliuping/TradingAgents-CN</link>
      <description>&lt;p&gt;基于多智能体LLM的中文金融交易框架 - TradingAgents中文增强版&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;TradingAgents 中文增强版&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://opensource.org/licenses/Apache-2.0"&gt;&lt;img src="https://img.shields.io/badge/License-Apache%202.0-blue.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://www.python.org/"&gt;&lt;img src="https://img.shields.io/badge/Python-3.10%2B-blue.svg?sanitize=true" alt="Python" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/VERSION"&gt;&lt;img src="https://img.shields.io/badge/Version-cn--0.1.15-green.svg?sanitize=true" alt="Version" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/"&gt;&lt;img src="https://img.shields.io/badge/docs-%E4%B8%AD%E6%96%87%E6%96%87%E6%A1%A3-green.svg?sanitize=true" alt="Documentation" /&gt;&lt;/a&gt; &lt;a href="https://github.com/TauricResearch/TradingAgents"&gt;&lt;img src="https://img.shields.io/badge/%E5%9F%BA%E4%BA%8E-TauricResearch/TradingAgents-orange.svg?sanitize=true" alt="Original" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;🎯 &lt;strong&gt;核心功能&lt;/strong&gt;: 原生OpenAI支持 | Google AI全面集成 | 自定义端点配置 | 智能模型选择 | 多LLM提供商支持 | 模型选择持久化 | Docker容器化部署 | 专业报告导出 | 完整A股支持 | 中文本地化&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;基于多智能体大语言模型的&lt;strong&gt;中文金融交易决策框架&lt;/strong&gt;。专为中文用户优化，提供完整的A股/港股/美股分析能力。&lt;/p&gt; 
&lt;h2&gt;🙏 致敬源项目&lt;/h2&gt; 
&lt;p&gt;感谢 &lt;a href="https://github.com/TauricResearch"&gt;Tauric Research&lt;/a&gt; 团队创造的革命性多智能体交易框架 &lt;a href="https://github.com/TauricResearch/TradingAgents"&gt;TradingAgents&lt;/a&gt;！&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;🎯 我们的使命&lt;/strong&gt;: 为中国用户提供完整的中文化体验，支持A股/港股市场，集成国产大模型，推动AI金融技术在中文社区的普及应用。&lt;/p&gt; 
&lt;h2&gt;🎉 v1.0.0-preview 内测版本 - 全新架构升级&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;🚀 &lt;strong&gt;重磅发布&lt;/strong&gt;: v1.0.0-preview 版本现已开启内测！全新的 FastAPI + Vue 3 架构，带来企业级的性能和体验！&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;✨ 核心特性&lt;/h3&gt; 
&lt;h4&gt;🏗️ &lt;strong&gt;全新技术架构&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;后端升级&lt;/strong&gt;: 从 Streamlit 迁移到 FastAPI，提供更强大的 RESTful API&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;前端重构&lt;/strong&gt;: 采用 Vue 3 + Element Plus，打造现代化的单页应用&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;数据库优化&lt;/strong&gt;: MongoDB + Redis 双数据库架构，性能提升 10 倍&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;容器化部署&lt;/strong&gt;: 完整的 Docker 多架构支持（amd64 + arm64）&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;🎯 &lt;strong&gt;企业级功能&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;用户权限管理&lt;/strong&gt;: 完整的用户认证、角色管理、操作日志系统&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;配置管理中心&lt;/strong&gt;: 可视化的大模型配置、数据源管理、系统设置&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;缓存管理系统&lt;/strong&gt;: 智能缓存策略，支持 MongoDB/Redis/文件多级缓存&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;实时通知系统&lt;/strong&gt;: SSE 推送，实时跟踪分析进度和系统状态&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;🤖 &lt;strong&gt;智能分析增强&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;动态供应商管理&lt;/strong&gt;: 支持动态添加和配置 LLM 供应商&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;模型能力管理&lt;/strong&gt;: 智能模型选择，根据任务自动匹配最佳模型&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;多数据源同步&lt;/strong&gt;: 统一的数据源管理，支持 Tushare、AkShare、BaoStock&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;报告导出功能&lt;/strong&gt;: 支持 Markdown/Word/PDF 多格式专业报告导出&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;🐳 &lt;strong&gt;Docker 多架构支持&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;跨平台部署&lt;/strong&gt;: 支持 x86_64 和 ARM64 架构（Apple Silicon、树莓派、AWS Graviton）&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;GitHub Actions&lt;/strong&gt;: 自动化构建和发布 Docker 镜像&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;一键部署&lt;/strong&gt;: 完整的 Docker Compose 配置，5 分钟快速启动&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;📊 技术栈升级&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;组件&lt;/th&gt; 
   &lt;th&gt;v0.1.x&lt;/th&gt; 
   &lt;th&gt;v1.0.0-preview&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;后端框架&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Streamlit&lt;/td&gt; 
   &lt;td&gt;FastAPI + Uvicorn&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;前端框架&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Streamlit&lt;/td&gt; 
   &lt;td&gt;Vue 3 + Vite + Element Plus&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;数据库&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;可选 MongoDB&lt;/td&gt; 
   &lt;td&gt;MongoDB + Redis&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;API 架构&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;单体应用&lt;/td&gt; 
   &lt;td&gt;RESTful API + WebSocket&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;部署方式&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;本地/Docker&lt;/td&gt; 
   &lt;td&gt;Docker 多架构 + GitHub Actions&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;🎯 内测申请&lt;/h3&gt; 
&lt;p&gt;v1.0.0-preview 版本目前处于&lt;strong&gt;内测阶段&lt;/strong&gt;，我们诚邀您参与体验和测试！&lt;/p&gt; 
&lt;h4&gt;使用指南&lt;/h4&gt; 
&lt;p&gt;在申请试用前，建议先阅读详细的使用指南：&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://mp.weixin.qq.com/s/ppsYiBncynxlsfKFG8uEbw"&gt;📘 TradingAgents-CN v1.0.0-preview 使用指南&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;使用指南包含：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;✅ 完整的功能介绍和操作演示&lt;/li&gt; 
 &lt;li&gt;✅ 详细的配置说明和最佳实践&lt;/li&gt; 
 &lt;li&gt;✅ 常见问题解答和故障排除&lt;/li&gt; 
 &lt;li&gt;✅ 实际使用案例和效果展示&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;如何申请试用&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;关注公众号&lt;/strong&gt;: 微信搜索 &lt;strong&gt;"TradingAgents-CN"&lt;/strong&gt; 并关注&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;提交申请&lt;/strong&gt;: 通过公众号私信发送以下信息： 
  &lt;ul&gt; 
   &lt;li&gt;您的姓名/昵称&lt;/li&gt; 
   &lt;li&gt;使用场景（个人学习/企业应用/学术研究）&lt;/li&gt; 
   &lt;li&gt;技术背景（可选）&lt;/li&gt; 
   &lt;li&gt;期望的功能或建议（可选）&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;获取权限&lt;/strong&gt;: 我们会在 1-2 个工作日内回复，并提供： 
  &lt;ul&gt; 
   &lt;li&gt;内测版本访问地址&lt;/li&gt; 
   &lt;li&gt;详细的使用文档&lt;/li&gt; 
   &lt;li&gt;技术支持和交流群&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;🎁 内测用户福利&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;✅ &lt;strong&gt;优先体验&lt;/strong&gt;: 第一时间体验最新功能&lt;/li&gt; 
 &lt;li&gt;✅ &lt;strong&gt;技术支持&lt;/strong&gt;: 专属技术支持和问题解答&lt;/li&gt; 
 &lt;li&gt;✅ &lt;strong&gt;功能定制&lt;/strong&gt;: 您的需求将优先纳入开发计划&lt;/li&gt; 
 &lt;li&gt;✅ &lt;strong&gt;社区荣誉&lt;/strong&gt;: 内测贡献者将在项目中特别致谢&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;📱 联系方式&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;微信公众号&lt;/strong&gt;: TradingAgents-CN（推荐）&lt;/p&gt; &lt;img src="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/assets/weixin.png" alt="微信公众号" width="200" /&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;🆕 v0.1.15 重大更新&lt;/h2&gt; 
&lt;h3&gt;🤖 LLM生态系统大升级&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;千帆大模型支持&lt;/strong&gt;: 新增百度千帆(ERNIE)大模型完整集成&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;LLM适配器重构&lt;/strong&gt;: 统一的OpenAI兼容适配器架构&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;多厂商支持&lt;/strong&gt;: 支持更多国产大模型提供商&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;集成指南&lt;/strong&gt;: 完整的LLM集成开发文档和测试工具&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;📚 学术研究支持&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;TradingAgents论文&lt;/strong&gt;: 完整的中文翻译版本和深度解读&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;技术博客&lt;/strong&gt;: 详细的技术分析和实现原理解读&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;学术资料&lt;/strong&gt;: PDF论文和相关研究资料&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;引用支持&lt;/strong&gt;: 标准的学术引用格式和参考文献&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;🛠️ 开发者体验升级&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;开发工作流&lt;/strong&gt;: 标准化的开发流程和分支管理规范&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;安装验证&lt;/strong&gt;: 完整的安装测试和验证脚本&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;文档重构&lt;/strong&gt;: 结构化的文档系统和快速开始指南&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;PR模板&lt;/strong&gt;: 标准化的Pull Request模板和代码审查流程&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;🔧 企业级工具链&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;分支保护&lt;/strong&gt;: GitHub分支保护策略和安全规则&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;紧急程序&lt;/strong&gt;: 完整的紧急处理和故障恢复程序&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;测试框架&lt;/strong&gt;: 增强的测试覆盖和验证工具&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;部署指南&lt;/strong&gt;: 企业级部署和配置管理&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;📋 v0.1.14 功能回顾&lt;/h2&gt; 
&lt;h3&gt;👥 用户权限管理系统&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;完整用户管理&lt;/strong&gt;: 新增用户注册、登录、权限控制功能&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;角色权限&lt;/strong&gt;: 支持多级用户角色和权限管理&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;会话管理&lt;/strong&gt;: 安全的用户会话和状态管理&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;用户活动日志&lt;/strong&gt;: 完整的用户操作记录和审计功能&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;🔐 Web用户认证系统&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;登录组件&lt;/strong&gt;: 现代化的用户登录界面&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;认证管理器&lt;/strong&gt;: 统一的用户认证和授权管理&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;安全增强&lt;/strong&gt;: 密码加密、会话安全等安全机制&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;用户仪表板&lt;/strong&gt;: 个性化的用户活动仪表板&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;🗄️ 数据管理优化&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;MongoDB集成增强&lt;/strong&gt;: 改进的MongoDB连接和数据管理&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;数据目录重组&lt;/strong&gt;: 优化的数据存储结构和管理&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;数据迁移脚本&lt;/strong&gt;: 完整的数据迁移和备份工具&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;缓存优化&lt;/strong&gt;: 提升数据加载和分析结果缓存性能&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;🧪 测试覆盖增强&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;功能测试脚本&lt;/strong&gt;: 新增6个专项功能测试脚本&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;工具处理器测试&lt;/strong&gt;: Google工具处理器修复验证&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;引导自动隐藏测试&lt;/strong&gt;: UI交互功能测试&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;在线工具配置测试&lt;/strong&gt;: 工具配置和选择逻辑测试&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;真实场景测试&lt;/strong&gt;: 实际使用场景的端到端测试&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;美股独立性测试&lt;/strong&gt;: 美股分析功能独立性验证&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;🆕 v0.1.13 重大更新&lt;/h2&gt; 
&lt;h3&gt;🤖 原生OpenAI端点支持&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;自定义OpenAI端点&lt;/strong&gt;: 支持配置任意OpenAI兼容的API端点&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;灵活模型选择&lt;/strong&gt;: 可以使用任何OpenAI格式的模型，不限于官方模型&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;智能适配器&lt;/strong&gt;: 新增原生OpenAI适配器，提供更好的兼容性和性能&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;配置管理&lt;/strong&gt;: 统一的端点和模型配置管理系统&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;🧠 Google AI生态系统全面集成&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;三大Google AI包支持&lt;/strong&gt;: langchain-google-genai、google-generativeai、google-genai&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;9个验证模型&lt;/strong&gt;: gemini-2.5-pro, gemini-2.5-flash, gemini-2.0-flash等最新模型&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Google工具处理器&lt;/strong&gt;: 专门的Google AI工具调用处理器&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;智能降级机制&lt;/strong&gt;: 高级功能失败时自动降级到基础功能&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;🔧 LLM适配器架构优化&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;GoogleOpenAIAdapter&lt;/strong&gt;: 新增Google AI的OpenAI兼容适配器&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;统一接口&lt;/strong&gt;: 所有LLM提供商使用统一的调用接口&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;错误处理增强&lt;/strong&gt;: 改进的异常处理和自动重试机制&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;性能监控&lt;/strong&gt;: 添加LLM调用性能监控和统计&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;🎨 Web界面智能优化&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;智能模型选择&lt;/strong&gt;: 根据可用性自动选择最佳模型&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;KeyError修复&lt;/strong&gt;: 彻底解决模型选择中的KeyError问题&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;UI响应优化&lt;/strong&gt;: 改进模型切换的响应速度和用户体验&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;错误提示&lt;/strong&gt;: 更友好的错误提示和解决建议&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🆕 v0.1.12 重大更新&lt;/h2&gt; 
&lt;h3&gt;🧠 智能新闻分析模块&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;智能新闻过滤器&lt;/strong&gt;: 基于AI的新闻相关性评分和质量评估&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;多层次过滤机制&lt;/strong&gt;: 基础过滤、增强过滤、集成过滤三级处理&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;新闻质量评估&lt;/strong&gt;: 自动识别和过滤低质量、重复、无关新闻&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;统一新闻工具&lt;/strong&gt;: 整合多个新闻源，提供统一的新闻获取接口&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;🔧 技术修复和优化&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;DashScope适配器修复&lt;/strong&gt;: 解决工具调用兼容性问题&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;DeepSeek死循环修复&lt;/strong&gt;: 修复新闻分析师的无限循环问题&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;LLM工具调用增强&lt;/strong&gt;: 提升工具调用的可靠性和稳定性&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;新闻检索器优化&lt;/strong&gt;: 增强新闻数据获取和处理能力&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;📚 完善测试和文档&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;全面测试覆盖&lt;/strong&gt;: 新增15+个测试文件，覆盖所有新功能&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;详细技术文档&lt;/strong&gt;: 新增8个技术分析报告和修复文档&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;用户指南完善&lt;/strong&gt;: 新增新闻过滤使用指南和最佳实践&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;演示脚本&lt;/strong&gt;: 提供完整的新闻过滤功能演示&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;🗂️ 项目结构优化&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;文档分类整理&lt;/strong&gt;: 按功能将文档分类到docs子目录&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;示例代码归位&lt;/strong&gt;: 演示脚本统一到examples目录&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;根目录整洁&lt;/strong&gt;: 保持根目录简洁，提升项目专业度&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🎯 核心特性&lt;/h2&gt; 
&lt;h3&gt;🤖 多智能体协作架构&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;专业分工&lt;/strong&gt;: 基本面、技术面、新闻面、社交媒体四大分析师&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;结构化辩论&lt;/strong&gt;: 看涨/看跌研究员进行深度分析&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;智能决策&lt;/strong&gt;: 交易员基于所有输入做出最终投资建议&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;风险管理&lt;/strong&gt;: 多层次风险评估和管理机制&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🖥️ Web界面展示&lt;/h2&gt; 
&lt;h3&gt;📸 界面截图&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;🎨 &lt;strong&gt;现代化Web界面&lt;/strong&gt;: 基于Streamlit构建的响应式Web应用，提供直观的股票分析体验&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;🏠 主界面 - 分析配置&lt;/h4&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/images/README/1755003162925.png" alt="1755003162925" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/images/README/1755002619976.png" alt="1755002619976" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;智能配置面板，支持多市场股票分析，5级研究深度选择&lt;/em&gt;&lt;/p&gt; 
&lt;h4&gt;📊 实时分析进度&lt;/h4&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/images/README/1755002731483.png" alt="1755002731483" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;实时进度跟踪，可视化分析过程，智能时间预估&lt;/em&gt;&lt;/p&gt; 
&lt;h4&gt;📈 分析结果展示&lt;/h4&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/images/README/1755002901204.png" alt="1755002901204" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/images/README/1755002924844.png" alt="1755002924844" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/images/README/1755002939905.png" alt="1755002939905" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/images/README/1755002968608.png" alt="1755002968608" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/images/README/1755002985903.png" alt="1755002985903" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/images/README/1755003004403.png" alt="1755003004403" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/images/README/1755003019759.png" alt="1755003019759" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/images/README/1755003033939.png" alt="1755003033939" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/images/README/1755003048242.png" alt="1755003048242" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/images/README/1755003064598.png" alt="1755003064598" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/images/README/1755003090603.png" alt="1755003090603" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;专业投资报告，多维度分析结果，一键导出功能&lt;/em&gt;&lt;/p&gt; 
&lt;h3&gt;🎯 核心功能特色&lt;/h3&gt; 
&lt;h4&gt;📋 &lt;strong&gt;智能分析配置&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;🌍 多市场支持&lt;/strong&gt;: 美股、A股、港股一站式分析&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🎯 5级研究深度&lt;/strong&gt;: 从2分钟快速分析到25分钟全面研究&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🤖 智能体选择&lt;/strong&gt;: 市场技术、基本面、新闻、社交媒体分析师&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;📅 灵活时间设置&lt;/strong&gt;: 支持历史任意时间点分析&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;🚀 &lt;strong&gt;实时进度跟踪&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;📊 可视化进度&lt;/strong&gt;: 实时显示分析进展和剩余时间&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🔄 智能步骤识别&lt;/strong&gt;: 自动识别当前分析阶段&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;⏱️ 准确时间预估&lt;/strong&gt;: 基于历史数据的智能时间计算&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;💾 状态持久化&lt;/strong&gt;: 页面刷新不丢失分析进度&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;📈 &lt;strong&gt;专业结果展示&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;🎯 投资决策&lt;/strong&gt;: 明确的买入/持有/卖出建议&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;📊 多维分析&lt;/strong&gt;: 技术面、基本面、新闻面综合评估&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🔢 量化指标&lt;/strong&gt;: 置信度、风险评分、目标价位&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;📄 专业报告&lt;/strong&gt;: 支持Markdown/Word/PDF格式导出&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;🤖 &lt;strong&gt;多LLM模型管理&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;🌐 4大提供商&lt;/strong&gt;: DashScope、DeepSeek、Google AI、OpenRouter&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🎯 60+模型选择&lt;/strong&gt;: 从经济型到旗舰级模型全覆盖&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;💾 配置持久化&lt;/strong&gt;: URL参数存储，刷新保持设置&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;⚡ 快速切换&lt;/strong&gt;: 5个热门模型一键选择按钮&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;🎮 Web界面操作指南&lt;/h3&gt; 
&lt;h4&gt;🚀 &lt;strong&gt;快速开始流程&lt;/strong&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;启动应用&lt;/strong&gt;: &lt;code&gt;python start_web.py&lt;/code&gt; 或 &lt;code&gt;docker-compose up -d&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;访问界面&lt;/strong&gt;: 浏览器打开 &lt;code&gt;http://localhost:8501&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;配置模型&lt;/strong&gt;: 侧边栏选择LLM提供商和模型&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;输入股票&lt;/strong&gt;: 输入股票代码（如 AAPL、000001、0700.HK）&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;选择深度&lt;/strong&gt;: 根据需求选择1-5级研究深度&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;开始分析&lt;/strong&gt;: 点击"🚀 开始分析"按钮&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;查看结果&lt;/strong&gt;: 实时跟踪进度，查看分析报告&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;导出报告&lt;/strong&gt;: 一键导出专业格式报告&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;📊 &lt;strong&gt;支持的股票代码格式&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;🇺🇸 美股&lt;/strong&gt;: &lt;code&gt;AAPL&lt;/code&gt;, &lt;code&gt;TSLA&lt;/code&gt;, &lt;code&gt;MSFT&lt;/code&gt;, &lt;code&gt;NVDA&lt;/code&gt;, &lt;code&gt;GOOGL&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🇨🇳 A股&lt;/strong&gt;: &lt;code&gt;000001&lt;/code&gt;, &lt;code&gt;600519&lt;/code&gt;, &lt;code&gt;300750&lt;/code&gt;, &lt;code&gt;002415&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🇭🇰 港股&lt;/strong&gt;: &lt;code&gt;0700.HK&lt;/code&gt;, &lt;code&gt;9988.HK&lt;/code&gt;, &lt;code&gt;3690.HK&lt;/code&gt;, &lt;code&gt;1810.HK&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;🎯 &lt;strong&gt;研究深度说明&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;1级 (2-4分钟)&lt;/strong&gt;: 快速概览，基础技术指标&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;2级 (4-6分钟)&lt;/strong&gt;: 标准分析，技术+基本面&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;3级 (6-10分钟)&lt;/strong&gt;: 深度分析，加入新闻情绪 ⭐ &lt;strong&gt;推荐&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;4级 (10-15分钟)&lt;/strong&gt;: 全面分析，多轮智能体辩论&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;5级 (15-25分钟)&lt;/strong&gt;: 最深度分析，完整研究报告&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;💡 &lt;strong&gt;使用技巧&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;🔄 实时刷新&lt;/strong&gt;: 分析过程中可随时刷新页面，进度不丢失&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;📱 移动适配&lt;/strong&gt;: 支持手机和平板设备访问&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🎨 深色模式&lt;/strong&gt;: 自动适配系统主题设置&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;⌨️ 快捷键&lt;/strong&gt;: 支持Enter键快速提交分析&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;📋 历史记录&lt;/strong&gt;: 自动保存最近的分析配置&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;📖 &lt;strong&gt;详细指南&lt;/strong&gt;: 完整的Web界面使用说明请参考 &lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/usage/web-interface-detailed-guide.md"&gt;🖥️ Web界面详细使用指南&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;🎯 功能特性&lt;/h2&gt; 
&lt;h3&gt;🚀 智能新闻分析✨ &lt;strong&gt;v0.1.12重大升级&lt;/strong&gt;&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;功能特性&lt;/th&gt; 
   &lt;th&gt;状态&lt;/th&gt; 
   &lt;th&gt;详细说明&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;🧠 智能新闻分析&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;🆕 v0.1.12&lt;/td&gt; 
   &lt;td&gt;AI新闻过滤，质量评估，相关性分析&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;🔧 新闻过滤器&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;🆕 v0.1.12&lt;/td&gt; 
   &lt;td&gt;多层次过滤，基础/增强/集成三级处理&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;📰 统一新闻工具&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;🆕 v0.1.12&lt;/td&gt; 
   &lt;td&gt;整合多源新闻，统一接口，智能检索&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;🤖 多LLM提供商&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;🆕 v0.1.11&lt;/td&gt; 
   &lt;td&gt;4大提供商，60+模型，智能分类管理&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;💾 模型选择持久化&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;🆕 v0.1.11&lt;/td&gt; 
   &lt;td&gt;URL参数存储，刷新保持，配置分享&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;🎯 快速选择按钮&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;🆕 v0.1.11&lt;/td&gt; 
   &lt;td&gt;一键切换热门模型，提升操作效率&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;📊 实时进度显示&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;✅ v0.1.10&lt;/td&gt; 
   &lt;td&gt;异步进度跟踪，智能步骤识别，准确时间计算&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;💾 智能会话管理&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;✅ v0.1.10&lt;/td&gt; 
   &lt;td&gt;状态持久化，自动降级，跨页面恢复&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;🎯 一键查看报告&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;✅ v0.1.10&lt;/td&gt; 
   &lt;td&gt;分析完成后一键查看，智能结果恢复&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;🖥️ Streamlit界面&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;✅ 完整支持&lt;/td&gt; 
   &lt;td&gt;现代化响应式界面，实时交互和数据可视化&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;⚙️ 配置管理&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;✅ 完整支持&lt;/td&gt; 
   &lt;td&gt;Web端API密钥管理，模型选择，参数配置&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;🎨 CLI用户体验 ✨ &lt;strong&gt;v0.1.9优化&lt;/strong&gt;&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;功能特性&lt;/th&gt; 
   &lt;th&gt;状态&lt;/th&gt; 
   &lt;th&gt;详细说明&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;🖥️ 界面与日志分离&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;✅ 完整支持&lt;/td&gt; 
   &lt;td&gt;用户界面清爽美观，技术日志独立管理&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;🔄 智能进度显示&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;✅ 完整支持&lt;/td&gt; 
   &lt;td&gt;多阶段进度跟踪，防止重复提示&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;⏱️ 时间预估功能&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;✅ 完整支持&lt;/td&gt; 
   &lt;td&gt;智能分析阶段显示预计耗时&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;🌈 Rich彩色输出&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;✅ 完整支持&lt;/td&gt; 
   &lt;td&gt;彩色进度指示，状态图标，视觉效果提升&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;🧠 LLM模型支持 ✨ &lt;strong&gt;v0.1.13全面升级&lt;/strong&gt;&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;模型提供商&lt;/th&gt; 
   &lt;th&gt;支持模型&lt;/th&gt; 
   &lt;th&gt;特色功能&lt;/th&gt; 
   &lt;th&gt;新增功能&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;🇨🇳 阿里百炼&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;qwen-turbo/plus/max&lt;/td&gt; 
   &lt;td&gt;中文优化，成本效益高&lt;/td&gt; 
   &lt;td&gt;✅ 集成&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;🇨🇳 DeepSeek&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;deepseek-chat&lt;/td&gt; 
   &lt;td&gt;工具调用，性价比极高&lt;/td&gt; 
   &lt;td&gt;✅ 集成&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;🌍 Google AI&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;9个验证模型&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;最新Gemini 2.5系列&lt;/td&gt; 
   &lt;td&gt;🆕 升级&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;├─&lt;strong&gt;最新旗舰&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;gemini-2.5-pro/flash&lt;/td&gt; 
   &lt;td&gt;最新旗舰，超快响应&lt;/td&gt; 
   &lt;td&gt;🆕 新增&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;├─&lt;strong&gt;稳定推荐&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;gemini-2.0-flash&lt;/td&gt; 
   &lt;td&gt;推荐使用，平衡性能&lt;/td&gt; 
   &lt;td&gt;🆕 新增&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;├─&lt;strong&gt;经典强大&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;gemini-1.5-pro/flash&lt;/td&gt; 
   &lt;td&gt;经典稳定，高质量分析&lt;/td&gt; 
   &lt;td&gt;✅ 集成&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;└─&lt;strong&gt;轻量快速&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;gemini-2.5-flash-lite&lt;/td&gt; 
   &lt;td&gt;轻量级任务，快速响应&lt;/td&gt; 
   &lt;td&gt;🆕 新增&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;🌐 原生OpenAI&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;自定义端点支持&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;任意OpenAI兼容端点&lt;/td&gt; 
   &lt;td&gt;🆕 新增&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;🌐 OpenRouter&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;60+模型聚合平台&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;一个API访问所有主流模型&lt;/td&gt; 
   &lt;td&gt;✅ 集成&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;├─&lt;strong&gt;OpenAI&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;o4-mini-high, o3-pro, GPT-4o&lt;/td&gt; 
   &lt;td&gt;最新o系列，推理专业版&lt;/td&gt; 
   &lt;td&gt;✅ 集成&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;├─&lt;strong&gt;Anthropic&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Claude 4 Opus/Sonnet/Haiku&lt;/td&gt; 
   &lt;td&gt;顶级性能，平衡版本&lt;/td&gt; 
   &lt;td&gt;✅ 集成&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;├─&lt;strong&gt;Meta&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Llama 4 Maverick/Scout&lt;/td&gt; 
   &lt;td&gt;最新Llama 4系列&lt;/td&gt; 
   &lt;td&gt;✅ 集成&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;└─&lt;strong&gt;自定义&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;任意OpenRouter模型ID&lt;/td&gt; 
   &lt;td&gt;无限扩展，个性化选择&lt;/td&gt; 
   &lt;td&gt;✅ 集成&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;🎯 快速选择&lt;/strong&gt;: 5个热门模型快速按钮 | &lt;strong&gt;💾 持久化&lt;/strong&gt;: URL参数存储，刷新保持 | &lt;strong&gt;🔄 智能切换&lt;/strong&gt;: 一键切换不同提供商&lt;/p&gt; 
&lt;h3&gt;📊 数据源与市场&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;市场类型&lt;/th&gt; 
   &lt;th&gt;数据源&lt;/th&gt; 
   &lt;th&gt;覆盖范围&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;🇨🇳 A股&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Tushare, AkShare, 通达信&lt;/td&gt; 
   &lt;td&gt;沪深两市，实时行情，财报数据&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;🇭🇰 港股&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;AkShare, Yahoo Finance&lt;/td&gt; 
   &lt;td&gt;港交所，实时行情，基本面&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;🇺🇸 美股&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;FinnHub, Yahoo Finance&lt;/td&gt; 
   &lt;td&gt;NYSE, NASDAQ，实时数据&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;📰 新闻&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Google News&lt;/td&gt; 
   &lt;td&gt;实时新闻，多语言支持&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;🤖 智能体团队&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;分析师团队&lt;/strong&gt;: 📈市场分析 | 💰基本面分析 | 📰新闻分析 | 💬情绪分析 &lt;strong&gt;研究团队&lt;/strong&gt;: 🐂看涨研究员 | 🐻看跌研究员 | 🎯交易决策员 &lt;strong&gt;管理层&lt;/strong&gt;: 🛡️风险管理员 | 👔研究主管&lt;/p&gt; 
&lt;h2&gt;🚀 快速开始&lt;/h2&gt; 
&lt;h3&gt;🐳 Docker部署 (推荐)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 1. 克隆项目
git clone https://github.com/hsliuping/TradingAgents-CN.git
cd TradingAgents-CN

# 2. 配置环境变量
cp .env.example .env
# 编辑 .env 文件，填入API密钥

# 3. 启动服务
# 首次启动或代码变更时（需要构建镜像）
docker-compose up -d --build

# 日常启动（镜像已存在，无代码变更）
docker-compose up -d

# 智能启动（自动判断是否需要构建）
# Windows环境
powershell -ExecutionPolicy Bypass -File scripts\smart_start.ps1

# Linux/Mac环境
chmod +x scripts/smart_start.sh &amp;amp;&amp;amp; ./scripts/smart_start.sh

# 4. 访问应用
# Web界面: http://localhost:8501
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;💻 本地部署&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 1. 升级pip (重要！避免安装错误)
python -m pip install --upgrade pip

# 2. 安装依赖（推荐使用锁定版本，安装速度最快）
pip install -r requirements-lock.txt -i https://pypi.tuna.tsinghua.edu.cn/simple
pip install -e . --no-deps

# 或一步安装（会重新解析依赖，速度较慢）
# pip install -e . -i https://pypi.tuna.tsinghua.edu.cn/simple

# 💡 国内用户推荐使用镜像加速（详见 docs/installation-mirror.md）
# ⚠️ Windows 用户如遇到 PyYAML 编译错误，使用锁定版本可避免此问题

# 3. 启动应用
python start_web.py

# 4. 访问 http://localhost:8501
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;📊 开始分析&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;选择模型&lt;/strong&gt;: DeepSeek V3 / 通义千问 / Gemini&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;输入股票&lt;/strong&gt;: &lt;code&gt;000001&lt;/code&gt; (A股) / &lt;code&gt;AAPL&lt;/code&gt; (美股) / &lt;code&gt;0700.HK&lt;/code&gt; (港股)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;开始分析&lt;/strong&gt;: 点击"🚀 开始分析"按钮&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;实时跟踪&lt;/strong&gt;: 观察实时进度和分析步骤&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;查看报告&lt;/strong&gt;: 点击"📊 查看分析报告"按钮&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;导出报告&lt;/strong&gt;: 支持Word/PDF/Markdown格式&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;🔐 用户权限管理&lt;/h2&gt; 
&lt;h3&gt;🔑 默认账号信息&lt;/h3&gt; 
&lt;p&gt;系统提供以下默认账号，首次启动时自动创建：&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;用户名&lt;/th&gt; 
   &lt;th&gt;密码&lt;/th&gt; 
   &lt;th&gt;角色&lt;/th&gt; 
   &lt;th&gt;权限说明&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;admin&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;admin123&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;管理员&lt;/td&gt; 
   &lt;td&gt;完整系统权限，用户管理，系统配置&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;user&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;user123&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;普通用户&lt;/td&gt; 
   &lt;td&gt;股票分析，报告查看，基础功能&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;⚠️ &lt;strong&gt;安全提醒&lt;/strong&gt;: 首次登录后请立即修改默认密码！&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;🛡️ 权限控制体系&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;🔐 登录认证&lt;/strong&gt;: 基于用户名密码的安全认证&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;👥 角色管理&lt;/strong&gt;: 管理员、普通用户等多级权限&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;⏰ 会话管理&lt;/strong&gt;: 自动超时保护，安全登出&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;📊 操作日志&lt;/strong&gt;: 完整的用户活动记录&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;🛠️ 用户管理工具&lt;/h3&gt; 
&lt;p&gt;系统提供完整的命令行用户管理工具：&lt;/p&gt; 
&lt;h4&gt;Windows 用户&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-powershell"&gt;# 使用 PowerShell 脚本
.\scripts\user_manager.ps1 list                    # 列出所有用户
.\scripts\user_manager.ps1 change-password admin   # 修改密码
.\scripts\user_manager.ps1 create newuser trader  # 创建新用户
.\scripts\user_manager.ps1 delete olduser         # 删除用户

# 或使用批处理文件
.\scripts\user_manager.bat list
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Python 脚本（跨平台）&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 直接使用 Python 脚本
python scripts/user_password_manager.py list
python scripts/user_password_manager.py change-password admin
python scripts/user_password_manager.py create newuser --role trader
python scripts/user_password_manager.py delete olduser
python scripts/user_password_manager.py reset  # 重置为默认配置
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;📋 支持的用户操作&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;📝 列出用户&lt;/strong&gt;: 查看所有用户及其角色权限&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🔑 修改密码&lt;/strong&gt;: 安全的密码更新机制&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;👤 创建用户&lt;/strong&gt;: 支持自定义角色和权限&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🗑️ 删除用户&lt;/strong&gt;: 安全的用户删除功能&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🔄 重置配置&lt;/strong&gt;: 恢复默认用户设置&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;📁 配置文件位置&lt;/h3&gt; 
&lt;p&gt;用户配置存储在：&lt;code&gt;web/config/users.json&lt;/code&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;📚 &lt;strong&gt;详细文档&lt;/strong&gt;: 完整的用户管理指南请参考 &lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/scripts/USER_MANAGEMENT.md"&gt;scripts/USER_MANAGEMENT.md&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;🚧 当前版本限制&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;❌ 暂不支持在线用户注册&lt;/li&gt; 
 &lt;li&gt;❌ 暂不支持Web界面的角色管理&lt;/li&gt; 
 &lt;li&gt;✅ 支持完整的命令行用户管理&lt;/li&gt; 
 &lt;li&gt;✅ 支持完整的权限控制框架&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;🎯 核心优势&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;🧠 智能新闻分析&lt;/strong&gt;: v0.1.12新增AI驱动的新闻过滤和质量评估系统&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🔧 多层次过滤&lt;/strong&gt;: 基础、增强、集成三级新闻过滤机制&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;📰 统一新闻工具&lt;/strong&gt;: 整合多源新闻，提供统一的智能检索接口&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🆕 多LLM集成&lt;/strong&gt;: v0.1.11新增4大提供商，60+模型，一站式AI体验&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;💾 配置持久化&lt;/strong&gt;: 模型选择真正持久化，URL参数存储，刷新保持&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🎯 快速切换&lt;/strong&gt;: 5个热门模型快速按钮，一键切换不同AI&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🆕 实时进度&lt;/strong&gt;: v0.1.10异步进度跟踪，告别黑盒等待&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;💾 智能会话&lt;/strong&gt;: 状态持久化，页面刷新不丢失分析结果&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🔐 用户权限&lt;/strong&gt;: v0.1.14新增完整的用户认证和权限管理体系&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🇨🇳 中国优化&lt;/strong&gt;: A股/港股数据 + 国产LLM + 中文界面&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🐳 容器化&lt;/strong&gt;: Docker一键部署，环境隔离，快速扩展&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;📄 专业报告&lt;/strong&gt;: 多格式导出，自动生成投资建议&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🛡️ 稳定可靠&lt;/strong&gt;: 多层数据源，智能降级，错误恢复&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🔧 技术架构&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;核心技术&lt;/strong&gt;: Python 3.10+ | LangChain | Streamlit | MongoDB | Redis &lt;strong&gt;AI模型&lt;/strong&gt;: DeepSeek V3 | 阿里百炼 | Google AI | OpenRouter(60+模型) | OpenAI &lt;strong&gt;数据源&lt;/strong&gt;: Tushare | AkShare | FinnHub | Yahoo Finance &lt;strong&gt;部署&lt;/strong&gt;: Docker | Docker Compose | 本地部署&lt;/p&gt; 
&lt;h2&gt;📚 文档和支持&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;📖 完整文档&lt;/strong&gt;: &lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/"&gt;docs/&lt;/a&gt; - 安装指南、使用教程、API文档&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🚨 故障排除&lt;/strong&gt;: &lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/troubleshooting/"&gt;troubleshooting/&lt;/a&gt; - 常见问题解决方案&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🔄 更新日志&lt;/strong&gt;: &lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/releases/CHANGELOG.md"&gt;CHANGELOG.md&lt;/a&gt; - 详细版本历史&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🚀 快速开始&lt;/strong&gt;: &lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/QUICKSTART.md"&gt;QUICKSTART.md&lt;/a&gt; - 5分钟快速部署指南&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🆚 中文增强特色&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;相比原版新增&lt;/strong&gt;: 智能新闻分析 | 多层次新闻过滤 | 新闻质量评估 | 统一新闻工具 | 多LLM提供商集成 | 模型选择持久化 | 快速切换按钮 | | 实时进度显示 | 智能会话管理 | 中文界面 | A股数据 | 国产LLM | Docker部署 | 专业报告导出 | 统一日志管理 | Web配置界面 | 成本优化&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Docker部署包含的服务&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;🌐 &lt;strong&gt;Web应用&lt;/strong&gt;: TradingAgents-CN主程序&lt;/li&gt; 
 &lt;li&gt;🗄️ &lt;strong&gt;MongoDB&lt;/strong&gt;: 数据持久化存储&lt;/li&gt; 
 &lt;li&gt;⚡ &lt;strong&gt;Redis&lt;/strong&gt;: 高速缓存&lt;/li&gt; 
 &lt;li&gt;📊 &lt;strong&gt;MongoDB Express&lt;/strong&gt;: 数据库管理界面&lt;/li&gt; 
 &lt;li&gt;🎛️ &lt;strong&gt;Redis Commander&lt;/strong&gt;: 缓存管理界面&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;💻 方式二：本地部署&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;适用场景&lt;/strong&gt;: 开发环境、自定义配置、离线使用&lt;/p&gt; 
&lt;h3&gt;环境要求&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.10+ (推荐 3.11)&lt;/li&gt; 
 &lt;li&gt;4GB+ RAM (推荐 8GB+)&lt;/li&gt; 
 &lt;li&gt;稳定的网络连接&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;安装步骤&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 1. 克隆项目
git clone https://github.com/hsliuping/TradingAgents-CN.git
cd TradingAgents-CN

# 2. 创建虚拟环境
python -m venv env
# Windows
env\Scripts\activate
# Linux/macOS
source env/bin/activate

# 3. 升级pip
python -m pip install --upgrade pip

# 4. 安装所有依赖
pip install -e .

# 💡 国内用户推荐使用镜像加速（详见 docs/installation-mirror.md）
# pip install -e . -i https://pypi.tuna.tsinghua.edu.cn/simple

# 注意：requirements.txt已包含所有必需依赖：
# - 数据库支持 (MongoDB + Redis)
# - 多市场数据源 (Tushare, AKShare, FinnHub等)
# - Web界面和报告导出功能
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;配置API密钥&lt;/h3&gt; 
&lt;h4&gt;🇨🇳 推荐：使用阿里百炼（国产大模型）&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 复制配置模板
cp .env.example .env

# 编辑 .env 文件，配置以下必需的API密钥：
DASHSCOPE_API_KEY=your_dashscope_api_key_here
FINNHUB_API_KEY=your_finnhub_api_key_here

# 推荐：Tushare API（专业A股数据）
TUSHARE_TOKEN=your_tushare_token_here
TUSHARE_ENABLED=true

# 可选：其他AI模型API
GOOGLE_API_KEY=your_google_api_key_here
DEEPSEEK_API_KEY=your_deepseek_api_key_here

# 数据库配置（可选，提升性能）
# 本地部署使用标准端口
MONGODB_ENABLED=false  # 设为true启用MongoDB
REDIS_ENABLED=false    # 设为true启用Redis
MONGODB_HOST=localhost
MONGODB_PORT=27017     # 标准MongoDB端口
REDIS_HOST=localhost
REDIS_PORT=6379        # 标准Redis端口

# Docker部署时需要修改主机名
# MONGODB_HOST=mongodb
# REDIS_HOST=redis
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;📋 部署模式配置说明&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;本地部署模式&lt;/strong&gt;：&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 数据库配置（本地部署）
MONGODB_ENABLED=true
REDIS_ENABLED=true
MONGODB_HOST=localhost      # 本地主机
MONGODB_PORT=27017         # 标准端口
REDIS_HOST=localhost       # 本地主机
REDIS_PORT=6379           # 标准端口
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Docker部署模式&lt;/strong&gt;：&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 数据库配置（Docker部署）
MONGODB_ENABLED=true
REDIS_ENABLED=true
MONGODB_HOST=mongodb       # Docker容器服务名
MONGODB_PORT=27017        # 标准端口
REDIS_HOST=redis          # Docker容器服务名
REDIS_PORT=6379          # 标准端口
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;💡 &lt;strong&gt;配置提示&lt;/strong&gt;：&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;本地部署：需要手动启动MongoDB和Redis服务&lt;/li&gt; 
  &lt;li&gt;Docker部署：数据库服务通过docker-compose自动启动&lt;/li&gt; 
  &lt;li&gt;端口冲突：如果本地已有数据库服务，可修改docker-compose.yml中的端口映射&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;🌍 可选：使用国外模型&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# OpenAI (需要科学上网)
OPENAI_API_KEY=your_openai_api_key

# Anthropic (需要科学上网)
ANTHROPIC_API_KEY=your_anthropic_api_key
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;🗄️ 数据库配置（MongoDB + Redis）&lt;/h3&gt; 
&lt;h4&gt;高性能数据存储支持&lt;/h4&gt; 
&lt;p&gt;本项目支持 &lt;strong&gt;MongoDB&lt;/strong&gt; 和 &lt;strong&gt;Redis&lt;/strong&gt; 数据库，提供：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;📊 股票数据缓存&lt;/strong&gt;: 减少API调用，提升响应速度&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🔄 智能降级机制&lt;/strong&gt;: MongoDB → API → 本地缓存的多层数据源&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;⚡ 高性能缓存&lt;/strong&gt;: Redis缓存热点数据，毫秒级响应&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🛡️ 数据持久化&lt;/strong&gt;: MongoDB存储历史数据，支持离线分析&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;数据库部署方式&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;🐳 Docker部署（推荐）&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;如果您使用Docker部署，数据库已自动包含在内：&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Docker部署会自动启动所有服务，包括：
docker-compose up -d --build
# - Web应用 (端口8501)
# - MongoDB (端口27017)
# - Redis (端口6379)
# - 数据库管理界面 (端口8081, 8082)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;💻 本地部署 - 数据库配置&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;如果您使用本地部署，可以选择以下方式：&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;方式一：仅启动数据库服务&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 仅启动 MongoDB + Redis 服务（不启动Web应用）
docker-compose up -d mongodb redis mongo-express redis-commander

# 查看服务状态
docker-compose ps

# 停止服务
docker-compose down
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;方式二：完全本地安装&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 数据库依赖已包含在requirements.txt中，无需额外安装

# 启动 MongoDB (默认端口 27017)
mongod --dbpath ./data/mongodb

# 启动 Redis (默认端口 6379)
redis-server
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;⚠️ &lt;strong&gt;重要说明&lt;/strong&gt;:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;🐳 Docker部署&lt;/strong&gt;: 数据库自动包含，无需额外配置&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;💻 本地部署&lt;/strong&gt;: 可选择仅启动数据库服务或完全本地安装&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;📋 推荐&lt;/strong&gt;: 使用Docker部署以获得最佳体验和一致性&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;数据库配置选项&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;环境变量配置&lt;/strong&gt;（推荐）：&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# MongoDB 配置
MONGODB_HOST=localhost
MONGODB_PORT=27017
MONGODB_DATABASE=trading_agents
MONGODB_USERNAME=admin
MONGODB_PASSWORD=your_password

# Redis 配置
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_PASSWORD=your_redis_password
REDIS_DB=0
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;配置文件方式&lt;/strong&gt;：&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# config/database_config.py
DATABASE_CONFIG = {
    'mongodb': {
        'host': 'localhost',
        'port': 27017,
        'database': 'trading_agents',
        'username': 'admin',
        'password': 'your_password'
    },
    'redis': {
        'host': 'localhost',
        'port': 6379,
        'password': 'your_redis_password',
        'db': 0
    }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;数据库功能特性&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;MongoDB 功能&lt;/strong&gt;：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;✅ 股票基础信息存储&lt;/li&gt; 
 &lt;li&gt;✅ 历史价格数据缓存&lt;/li&gt; 
 &lt;li&gt;✅ 分析结果持久化&lt;/li&gt; 
 &lt;li&gt;✅ 用户配置管理&lt;/li&gt; 
 &lt;li&gt;✅ 自动数据同步&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Redis 功能&lt;/strong&gt;：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;⚡ 实时价格数据缓存&lt;/li&gt; 
 &lt;li&gt;⚡ API响应结果缓存&lt;/li&gt; 
 &lt;li&gt;⚡ 会话状态管理&lt;/li&gt; 
 &lt;li&gt;⚡ 热点数据预加载&lt;/li&gt; 
 &lt;li&gt;⚡ 分布式锁支持&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;智能降级机制&lt;/h4&gt; 
&lt;p&gt;系统采用多层数据源降级策略，确保高可用性：&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;📊 数据获取流程：
1. 🔍 检查 Redis 缓存 (毫秒级)
2. 📚 查询 MongoDB 存储 (秒级)
3. 🌐 调用通达信API (秒级)
4. 💾 本地文件缓存 (备用)
5. ❌ 返回错误信息
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;配置降级策略&lt;/strong&gt;：&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# 在 .env 文件中配置
ENABLE_MONGODB=true
ENABLE_REDIS=true
ENABLE_FALLBACK=true

# 缓存过期时间（秒）
REDIS_CACHE_TTL=300
MONGODB_CACHE_TTL=3600
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;性能优化建议&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;生产环境配置&lt;/strong&gt;：&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# MongoDB 优化
MONGODB_MAX_POOL_SIZE=50
MONGODB_MIN_POOL_SIZE=5
MONGODB_MAX_IDLE_TIME=30000

# Redis 优化
REDIS_MAX_CONNECTIONS=20
REDIS_CONNECTION_POOL_SIZE=10
REDIS_SOCKET_TIMEOUT=5
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;数据库管理工具&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 初始化数据库
python scripts/setup/init_database.py

# 系统状态检查
python scripts/validation/check_system_status.py

# 清理缓存工具
python scripts/maintenance/cleanup_cache.py --days 7
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;故障排除&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;常见问题解决&lt;/strong&gt;：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;🪟 Windows 10 ChromaDB兼容性问题&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;问题现象&lt;/strong&gt;：在Windows 10上出现 &lt;code&gt;Configuration error: An instance of Chroma already exists for ephemeral with different settings&lt;/code&gt; 错误，而Windows 11正常。&lt;/p&gt; &lt;p&gt;&lt;strong&gt;快速解决方案&lt;/strong&gt;：&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# 方案1：禁用内存功能（推荐）
# 在 .env 文件中添加：
MEMORY_ENABLED=false

# 方案2：使用专用修复脚本
powershell -ExecutionPolicy Bypass -File scripts\fix_chromadb_win10.ps1

# 方案3：管理员权限运行
# 右键PowerShell -&amp;gt; "以管理员身份运行"
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;详细解决方案&lt;/strong&gt;：参考 &lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/troubleshooting/windows10-chromadb-fix.md"&gt;Windows 10兼容性指南&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;MongoDB连接失败&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Docker部署&lt;/strong&gt;：&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# 检查服务状态
docker-compose logs mongodb

# 重启服务
docker-compose restart mongodb
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;本地部署&lt;/strong&gt;：&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# 检查MongoDB进程
ps aux | grep mongod

# 重启MongoDB
sudo systemctl restart mongod  # Linux
brew services restart mongodb  # macOS
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Redis连接超时&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# 检查Redis状态
redis-cli ping

# 清理Redis缓存
redis-cli flushdb
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;缓存问题&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# 检查系统状态和缓存
python scripts/validation/check_system_status.py

# 清理过期缓存
python scripts/maintenance/cleanup_cache.py --days 7
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;💡 &lt;strong&gt;提示&lt;/strong&gt;: 即使不配置数据库，系统仍可正常运行，会自动降级到API直接调用模式。数据库配置是可选的性能优化功能。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;📚 &lt;strong&gt;详细文档&lt;/strong&gt;: 更多数据库配置信息请参考 &lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/architecture/database-architecture.md"&gt;数据库架构文档&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;📤 报告导出功能&lt;/h3&gt; 
&lt;h4&gt;新增功能：专业分析报告导出&lt;/h4&gt; 
&lt;p&gt;本项目现已支持将股票分析结果导出为多种专业格式：&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;支持的导出格式&lt;/strong&gt;：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;📄 Markdown (.md)&lt;/strong&gt; - 轻量级标记语言，适合技术用户和版本控制&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;📝 Word (.docx)&lt;/strong&gt; - Microsoft Word文档，适合商务报告和进一步编辑&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;📊 PDF (.pdf)&lt;/strong&gt; - 便携式文档格式，适合正式分享和打印&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;报告内容结构&lt;/strong&gt;：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;🎯 &lt;strong&gt;投资决策摘要&lt;/strong&gt; - 买入/持有/卖出建议，置信度，风险评分&lt;/li&gt; 
 &lt;li&gt;📊 &lt;strong&gt;详细分析报告&lt;/strong&gt; - 技术分析，基本面分析，市场情绪，新闻事件&lt;/li&gt; 
 &lt;li&gt;⚠️ &lt;strong&gt;风险提示&lt;/strong&gt; - 完整的投资风险声明和免责条款&lt;/li&gt; 
 &lt;li&gt;📋 &lt;strong&gt;配置信息&lt;/strong&gt; - 分析参数，模型信息，生成时间&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;使用方法&lt;/strong&gt;：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;完成股票分析后，在结果页面底部找到"📤 导出报告"部分&lt;/li&gt; 
 &lt;li&gt;选择需要的格式：Markdown、Word或PDF&lt;/li&gt; 
 &lt;li&gt;点击导出按钮，系统自动生成并提供下载&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;安装导出依赖&lt;/strong&gt;：&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 安装Python依赖
pip install markdown pypandoc

# 安装系统工具（用于PDF导出）
# Windows: choco install pandoc wkhtmltopdf
# macOS: brew install pandoc wkhtmltopdf
# Linux: sudo apt-get install pandoc wkhtmltopdf
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;📚 &lt;strong&gt;详细文档&lt;/strong&gt;: 完整的导出功能使用指南请参考 &lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/EXPORT_GUIDE.md"&gt;导出功能指南&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;🚀 启动应用&lt;/h3&gt; 
&lt;h4&gt;🐳 Docker启动（推荐）&lt;/h4&gt; 
&lt;p&gt;如果您使用Docker部署，应用已经自动启动：&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 应用已在Docker中运行，直接访问：
# Web界面: http://localhost:8501
# 数据库管理: http://localhost:8081
# 缓存管理: http://localhost:8082

# 查看运行状态
docker-compose ps

# 查看日志
docker-compose logs -f web
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;💻 本地启动&lt;/h4&gt; 
&lt;p&gt;如果您使用本地部署：&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 1. 激活虚拟环境
# Windows
.\env\Scripts\activate
# Linux/macOS
source env/bin/activate

# 2. 安装项目到虚拟环境（重要！）
pip install -e .

# 💡 国内用户推荐使用镜像加速（详见 docs/installation-mirror.md）
# pip install -e . -i https://pypi.tuna.tsinghua.edu.cn/simple

# 3. 启动Web管理界面
# 方法1：使用项目启动脚本（推荐）
python start_web.py

# 方法2：使用原始启动脚本
python web/run_web.py

# 方法3：直接使用streamlit（需要先安装项目）
streamlit run web/app.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;然后在浏览器中访问 &lt;code&gt;http://localhost:8501&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Web界面特色功能&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;🇺🇸 &lt;strong&gt;美股分析&lt;/strong&gt;: 支持 AAPL, TSLA, NVDA 等美股代码&lt;/li&gt; 
 &lt;li&gt;🇨🇳 &lt;strong&gt;A股分析&lt;/strong&gt;: 支持 000001, 600519, 300750 等A股代码&lt;/li&gt; 
 &lt;li&gt;📊 &lt;strong&gt;实时数据&lt;/strong&gt;: 通达信API提供A股实时行情数据&lt;/li&gt; 
 &lt;li&gt;🤖 &lt;strong&gt;智能体选择&lt;/strong&gt;: 可选择不同的分析师组合&lt;/li&gt; 
 &lt;li&gt;📤 &lt;strong&gt;报告导出&lt;/strong&gt;: 一键导出Markdown/Word/PDF格式专业分析报告&lt;/li&gt; 
 &lt;li&gt;🎯 &lt;strong&gt;5级研究深度&lt;/strong&gt;: 从快速分析(2-4分钟)到全面分析(15-25分钟)&lt;/li&gt; 
 &lt;li&gt;📊 &lt;strong&gt;智能分析师选择&lt;/strong&gt;: 市场技术、基本面、新闻、社交媒体分析师&lt;/li&gt; 
 &lt;li&gt;🔄 &lt;strong&gt;实时进度显示&lt;/strong&gt;: 可视化分析过程，避免等待焦虑&lt;/li&gt; 
 &lt;li&gt;📈 &lt;strong&gt;结构化结果&lt;/strong&gt;: 投资建议、目标价位、置信度、风险评估&lt;/li&gt; 
 &lt;li&gt;🇨🇳 &lt;strong&gt;完全中文化&lt;/strong&gt;: 界面和分析结果全中文显示&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;研究深度级别说明&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;1级 - 快速分析&lt;/strong&gt; (2-4分钟): 日常监控，基础决策&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;2级 - 基础分析&lt;/strong&gt; (4-6分钟): 常规投资，平衡速度&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;3级 - 标准分析&lt;/strong&gt; (6-10分钟): 重要决策，推荐默认&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;4级 - 深度分析&lt;/strong&gt; (10-15分钟): 重大投资，详细研究&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;5级 - 全面分析&lt;/strong&gt; (15-25分钟): 最重要决策，最全面分析&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;💻 代码调用（适合开发者）&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from tradingagents.graph.trading_graph import TradingAgentsGraph
from tradingagents.default_config import DEFAULT_CONFIG

# 配置阿里百炼
config = DEFAULT_CONFIG.copy()
config["llm_provider"] = "dashscope"
config["deep_think_llm"] = "qwen-plus"      # 深度分析
config["quick_think_llm"] = "qwen-turbo"    # 快速任务

# 创建交易智能体
ta = TradingAgentsGraph(debug=True, config=config)

# 分析股票 (以苹果公司为例)
state, decision = ta.propagate("AAPL", "2024-01-15")

# 输出分析结果
print(f"推荐动作: {decision['action']}")
print(f"置信度: {decision['confidence']:.1%}")
print(f"风险评分: {decision['risk_score']:.1%}")
print(f"推理过程: {decision['reasoning']}")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;快速启动脚本&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 阿里百炼演示（推荐中文用户）
python examples/dashscope/demo_dashscope_chinese.py

# 阿里百炼完整演示
python examples/dashscope/demo_dashscope.py

# 阿里百炼简化测试
python examples/dashscope/demo_dashscope_simple.py

# OpenAI演示（需要国外API）
python examples/openai/demo_openai.py

# 集成测试
python tests/integration/test_dashscope_integration.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;📁 数据目录配置&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;新功能&lt;/strong&gt;: 灵活配置数据存储路径，支持多种配置方式：&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 查看当前数据目录配置
python -m cli.main data-config --show

# 设置自定义数据目录
python -m cli.main data-config --set /path/to/your/data

# 重置为默认配置
python -m cli.main data-config --reset
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;环境变量配置&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Windows
set TRADING_AGENTS_DATA_DIR=C:\MyTradingData

# Linux/macOS
export TRADING_AGENTS_DATA_DIR=/home/user/trading_data
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;程序化配置&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from tradingagents.config_manager import ConfigManager

# 设置数据目录
config_manager = ConfigManager()
config_manager.set_data_directory("/path/to/data")

# 获取配置
data_dir = config_manager.get_data_directory()
print(f"数据目录: {data_dir}")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;配置优先级&lt;/strong&gt;: 程序设置 &amp;gt; 环境变量 &amp;gt; 配置文件 &amp;gt; 默认值&lt;/p&gt; 
&lt;p&gt;详细说明请参考: &lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/configuration/data-directory-configuration.md"&gt;📁 数据目录配置指南&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;交互式分析&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 启动交互式命令行界面
python -m cli.main
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;🎯 &lt;strong&gt;快速导航&lt;/strong&gt; - 找到您需要的内容&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;🎯&lt;strong&gt;我想要...&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;📖&lt;strong&gt;推荐文档&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;⏱️&lt;strong&gt;阅读时间&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;快速上手&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/overview/quick-start.md"&gt;🚀 快速开始&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;10分钟&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;了解架构&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/architecture/system-architecture.md"&gt;🏛️ 系统架构&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;15分钟&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;看代码示例&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/examples/basic-examples.md"&gt;📚 基础示例&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;20分钟&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;解决问题&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/faq/faq.md"&gt;🆘 常见问题&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;5分钟&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;深度学习&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/#-%E8%AF%A6%E7%BB%86%E6%96%87%E6%A1%A3%E7%9B%AE%E5%BD%95"&gt;📁 完整文档目录&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2小时+&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;💡 &lt;strong&gt;提示&lt;/strong&gt;: 我们的 &lt;code&gt;docs/&lt;/code&gt; 目录包含了 &lt;strong&gt;50,000+字&lt;/strong&gt; 的详细中文文档，这是与原版最大的区别！&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;📚 完整文档体系 - 核心亮点&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;🌟 这是本项目与原版最大的区别！&lt;/strong&gt; 我们构建了业界最完整的中文金融AI框架文档体系，包含超过 &lt;strong&gt;50,000字&lt;/strong&gt; 的详细技术文档，&lt;strong&gt;20+&lt;/strong&gt; 个专业文档文件，&lt;strong&gt;100+&lt;/strong&gt; 个代码示例。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;🎯 为什么选择我们的文档？&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;对比维度&lt;/th&gt; 
   &lt;th&gt;原版 TradingAgents&lt;/th&gt; 
   &lt;th&gt;🚀&lt;strong&gt;中文增强版&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;文档语言&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;英文基础说明&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;完整中文体系&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;文档深度&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;简单介绍&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;深度技术剖析&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;架构说明&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;概念性描述&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;详细设计文档 + 架构图&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;使用指南&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;基础示例&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;从入门到专家的完整路径&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;故障排除&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;无&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;详细FAQ + 解决方案&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;代码示例&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;少量示例&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;100+ 实用示例&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;📖 文档导航 - 按学习路径组织&lt;/h3&gt; 
&lt;h4&gt;🚀 &lt;strong&gt;新手入门路径&lt;/strong&gt; (推荐从这里开始)&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/overview/project-overview.md"&gt;📋 项目概述&lt;/a&gt; - &lt;strong&gt;了解项目背景和核心价值&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/overview/installation.md"&gt;⚙️ 详细安装&lt;/a&gt; - &lt;strong&gt;各平台详细安装指南&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/overview/quick-start.md"&gt;🚀 快速开始&lt;/a&gt; - &lt;strong&gt;10分钟上手指南&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/examples/basic-examples.md"&gt;📚 基础示例&lt;/a&gt; - &lt;strong&gt;8个实用的入门示例&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;🏗️ &lt;strong&gt;架构理解路径&lt;/strong&gt; (深入了解系统设计)&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/architecture/system-architecture.md"&gt;🏛️ 系统架构&lt;/a&gt; - &lt;strong&gt;完整的系统架构设计&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/architecture/agent-architecture.md"&gt;🤖 智能体架构&lt;/a&gt; - &lt;strong&gt;多智能体协作机制&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/architecture/data-flow-architecture.md"&gt;📊 数据流架构&lt;/a&gt; - &lt;strong&gt;数据处理全流程&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/architecture/graph-structure.md"&gt;🔄 图结构设计&lt;/a&gt; - &lt;strong&gt;LangGraph工作流程&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;🤖 &lt;strong&gt;智能体深度解析&lt;/strong&gt; (了解每个智能体的设计)&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/agents/analysts.md"&gt;📈 分析师团队&lt;/a&gt; - &lt;strong&gt;四类专业分析师详解&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/agents/researchers.md"&gt;🔬 研究员团队&lt;/a&gt; - &lt;strong&gt;看涨/看跌辩论机制&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/agents/trader.md"&gt;💼 交易员智能体&lt;/a&gt; - &lt;strong&gt;交易决策制定流程&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/agents/risk-management.md"&gt;🛡️ 风险管理&lt;/a&gt; - &lt;strong&gt;多层次风险评估&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/agents/managers.md"&gt;👔 管理层智能体&lt;/a&gt; - &lt;strong&gt;协调和决策管理&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;📊 &lt;strong&gt;数据处理专题&lt;/strong&gt; (掌握数据处理技术)&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/data/data-sources.md"&gt;🔌 数据源集成&lt;/a&gt; - &lt;strong&gt;多数据源API集成&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/data/data-processing.md"&gt;⚙️ 数据处理流程&lt;/a&gt; - &lt;strong&gt;数据清洗和转换&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/data/caching.md"&gt;💾 缓存策略&lt;/a&gt; - &lt;strong&gt;多层缓存优化性能&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;⚙️ &lt;strong&gt;配置和优化&lt;/strong&gt; (性能调优和定制)&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/configuration/config-guide.md"&gt;📝 配置指南&lt;/a&gt; - &lt;strong&gt;详细配置选项说明&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/configuration/llm-config.md"&gt;🧠 LLM配置&lt;/a&gt; - &lt;strong&gt;大语言模型优化&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;💡 &lt;strong&gt;高级应用&lt;/strong&gt; (扩展开发和实战)&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/examples/basic-examples.md"&gt;📚 基础示例&lt;/a&gt; - &lt;strong&gt;8个实用基础示例&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/examples/advanced-examples.md"&gt;🚀 高级示例&lt;/a&gt; - &lt;strong&gt;复杂场景和扩展开发&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;❓ &lt;strong&gt;问题解决&lt;/strong&gt; (遇到问题时查看)&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/faq/faq.md"&gt;🆘 常见问题&lt;/a&gt; - &lt;strong&gt;详细FAQ和解决方案&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;📊 文档统计数据&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;📄 &lt;strong&gt;文档文件数&lt;/strong&gt;: 20+ 个专业文档&lt;/li&gt; 
 &lt;li&gt;📝 &lt;strong&gt;总字数&lt;/strong&gt;: 50,000+ 字详细内容&lt;/li&gt; 
 &lt;li&gt;💻 &lt;strong&gt;代码示例&lt;/strong&gt;: 100+ 个实用示例&lt;/li&gt; 
 &lt;li&gt;📈 &lt;strong&gt;架构图表&lt;/strong&gt;: 10+ 个专业图表&lt;/li&gt; 
 &lt;li&gt;🎯 &lt;strong&gt;覆盖范围&lt;/strong&gt;: 从入门到专家的完整路径&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;🎨 文档特色&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;🇨🇳 完全中文化&lt;/strong&gt;: 专为中文用户优化的表达方式&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;📊 图文并茂&lt;/strong&gt;: 丰富的架构图和流程图&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;💻 代码丰富&lt;/strong&gt;: 每个概念都有对应的代码示例&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🔍 深度剖析&lt;/strong&gt;: 不仅告诉你怎么做，还告诉你为什么这样做&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🛠️ 实用导向&lt;/strong&gt;: 所有文档都面向实际应用场景&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;📚 详细文档目录&lt;/h2&gt; 
&lt;h3&gt;📁 &lt;strong&gt;docs/ 目录结构&lt;/strong&gt; - 完整的知识体系&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;docs/
├── 📖 overview/              # 项目概览 - 新手必读
│   ├── project-overview.md   # 📋 项目详细介绍
│   ├── quick-start.md        # 🚀 10分钟快速上手
│   └── installation.md       # ⚙️ 详细安装指南
│
├── 🏗️ architecture/          # 系统架构 - 深度理解
│   ├── system-architecture.md    # 🏛️ 整体架构设计
│   ├── agent-architecture.md     # 🤖 智能体协作机制
│   ├── data-flow-architecture.md # 📊 数据流处理架构
│   └── graph-structure.md        # 🔄 LangGraph工作流
│
├── 🤖 agents/               # 智能体详解 - 核心组件
│   ├── analysts.md          # 📈 四类专业分析师
│   ├── researchers.md       # 🔬 看涨/看跌辩论机制
│   ├── trader.md           # 💼 交易决策制定
│   ├── risk-management.md  # 🛡️ 多层风险评估
│   └── managers.md         # 👔 管理层协调
│
├── 📊 data/                 # 数据处理 - 技术核心
│   ├── data-sources.md      # 🔌 多数据源集成
│   ├── data-processing.md   # ⚙️ 数据处理流程
│   └── caching.md          # 💾 缓存优化策略
│
├── ⚙️ configuration/        # 配置优化 - 性能调优
│   ├── config-guide.md      # 📝 详细配置说明
│   └── llm-config.md       # 🧠 LLM模型优化
│
├── 💡 examples/             # 示例教程 - 实战应用
│   ├── basic-examples.md    # 📚 8个基础示例
│   └── advanced-examples.md # 🚀 高级开发示例
│
└── ❓ faq/                  # 问题解决 - 疑难解答
    └── faq.md              # 🆘 常见问题FAQ
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;🎯 &lt;strong&gt;重点推荐文档&lt;/strong&gt; (必读精选)&lt;/h3&gt; 
&lt;h4&gt;🔥 &lt;strong&gt;最受欢迎的文档&lt;/strong&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/overview/project-overview.md"&gt;📋 项目概述&lt;/a&gt;&lt;/strong&gt; - ⭐⭐⭐⭐⭐&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;了解项目的核心价值和技术特色，5分钟读懂整个框架&lt;/p&gt; 
  &lt;/blockquote&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/architecture/system-architecture.md"&gt;🏛️ 系统架构&lt;/a&gt;&lt;/strong&gt; - ⭐⭐⭐⭐⭐&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;深度解析多智能体协作机制，包含详细架构图&lt;/p&gt; 
  &lt;/blockquote&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/examples/basic-examples.md"&gt;📚 基础示例&lt;/a&gt;&lt;/strong&gt; - ⭐⭐⭐⭐⭐&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;8个实用示例，从股票分析到投资组合优化&lt;/p&gt; 
  &lt;/blockquote&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;🚀 &lt;strong&gt;技术深度文档&lt;/strong&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/architecture/agent-architecture.md"&gt;🤖 智能体架构&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;多智能体设计模式和协作机制详解&lt;/p&gt; 
  &lt;/blockquote&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/architecture/data-flow-architecture.md"&gt;📊 数据流架构&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;数据获取、处理、缓存的完整流程&lt;/p&gt; 
  &lt;/blockquote&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/agents/researchers.md"&gt;🔬 研究员团队&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;看涨/看跌研究员辩论机制的创新设计&lt;/p&gt; 
  &lt;/blockquote&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;💼 &lt;strong&gt;实用工具文档&lt;/strong&gt;&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/usage/web-interface-guide.md"&gt;🌐 Web界面指南&lt;/a&gt;&lt;/strong&gt; - ⭐⭐⭐⭐⭐&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;完整的Web界面使用教程，包含5级研究深度详细说明&lt;/p&gt; 
  &lt;/blockquote&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/usage/investment_analysis_guide.md"&gt;💰 投资分析指南&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;从基础到高级的完整投资分析教程&lt;/p&gt; 
  &lt;/blockquote&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/configuration/llm-config.md"&gt;🧠 LLM配置&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;多LLM模型配置和成本优化策略&lt;/p&gt; 
  &lt;/blockquote&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/data/caching.md"&gt;💾 缓存策略&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;多层缓存设计，显著降低API调用成本&lt;/p&gt; 
  &lt;/blockquote&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/faq/faq.md"&gt;🆘 常见问题&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;详细的FAQ和故障排除指南&lt;/p&gt; 
  &lt;/blockquote&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;📖 &lt;strong&gt;按模块浏览文档&lt;/strong&gt;&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;📖 概览文档&lt;/strong&gt; - 项目入门必读&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/overview/project-overview.md"&gt;📋 项目概述&lt;/a&gt; - 详细的项目背景和特性介绍&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/overview/quick-start.md"&gt;🚀 快速开始&lt;/a&gt; - 从安装到第一次运行的完整指南&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/overview/installation.md"&gt;⚙️ 详细安装&lt;/a&gt; - 各平台详细安装说明&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;🏗️ 架构文档&lt;/strong&gt; - 深度理解系统设计&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/architecture/system-architecture.md"&gt;🏛️ 系统架构&lt;/a&gt; - 完整的系统架构设计&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/architecture/agent-architecture.md"&gt;🤖 智能体架构&lt;/a&gt; - 智能体设计模式和协作机制&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/architecture/data-flow-architecture.md"&gt;📊 数据流架构&lt;/a&gt; - 数据获取、处理和分发流程&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/architecture/graph-structure.md"&gt;🔄 图结构设计&lt;/a&gt; - LangGraph工作流程设计&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;🤖 智能体文档&lt;/strong&gt; - 核心组件详解&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/agents/analysts.md"&gt;📈 分析师团队&lt;/a&gt; - 四类专业分析师详解&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/agents/researchers.md"&gt;🔬 研究员团队&lt;/a&gt; - 看涨/看跌研究员和辩论机制&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/agents/trader.md"&gt;💼 交易员智能体&lt;/a&gt; - 交易决策制定流程&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/agents/risk-management.md"&gt;🛡️ 风险管理&lt;/a&gt; - 多层次风险评估体系&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/agents/managers.md"&gt;👔 管理层智能体&lt;/a&gt; - 协调和决策管理&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;📊 数据处理&lt;/strong&gt; - 技术核心实现&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/data/data-sources.md"&gt;🔌 数据源集成&lt;/a&gt; - 支持的数据源和API集成&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/data/data-processing.md"&gt;⚙️ 数据处理流程&lt;/a&gt; - 数据清洗、转换和验证&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/data/caching.md"&gt;💾 缓存策略&lt;/a&gt; - 多层缓存优化性能&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;⚙️ 配置与部署&lt;/strong&gt; - 性能调优指南&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/configuration/config-guide.md"&gt;📝 配置指南&lt;/a&gt; - 详细的配置选项说明&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/configuration/llm-config.md"&gt;🧠 LLM配置&lt;/a&gt; - 大语言模型配置优化&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;💡 示例和教程&lt;/strong&gt; - 实战应用指南&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/examples/basic-examples.md"&gt;📚 基础示例&lt;/a&gt; - 8个实用的基础示例&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/examples/advanced-examples.md"&gt;🚀 高级示例&lt;/a&gt; - 复杂场景和扩展开发&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;❓ 帮助文档&lt;/strong&gt; - 问题解决方案&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/faq/faq.md"&gt;🆘 常见问题&lt;/a&gt; - 详细的FAQ和解决方案&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;💰 成本控制&lt;/h2&gt; 
&lt;h3&gt;典型使用成本&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;经济模式&lt;/strong&gt;: $0.01-0.05/次分析 (使用 gpt-4o-mini)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;标准模式&lt;/strong&gt;: $0.05-0.15/次分析 (使用 gpt-4o)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;高精度模式&lt;/strong&gt;: $0.10-0.30/次分析 (使用 gpt-4o + 多轮辩论)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;成本优化建议&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# 低成本配置示例
cost_optimized_config = {
    "deep_think_llm": "gpt-4o-mini",
    "quick_think_llm": "gpt-4o-mini", 
    "max_debate_rounds": 1,
    "online_tools": False  # 使用缓存数据
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;🤝 贡献指南&lt;/h2&gt; 
&lt;p&gt;我们欢迎各种形式的贡献：&lt;/p&gt; 
&lt;h3&gt;贡献类型&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;🐛 &lt;strong&gt;Bug修复&lt;/strong&gt; - 发现并修复问题&lt;/li&gt; 
 &lt;li&gt;✨ &lt;strong&gt;新功能&lt;/strong&gt; - 添加新的功能特性&lt;/li&gt; 
 &lt;li&gt;📚 &lt;strong&gt;文档改进&lt;/strong&gt; - 完善文档和教程&lt;/li&gt; 
 &lt;li&gt;🌐 &lt;strong&gt;本地化&lt;/strong&gt; - 翻译和本地化工作&lt;/li&gt; 
 &lt;li&gt;🎨 &lt;strong&gt;代码优化&lt;/strong&gt; - 性能优化和代码重构&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;贡献流程&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Fork 本仓库&lt;/li&gt; 
 &lt;li&gt;创建特性分支 (&lt;code&gt;git checkout -b feature/AmazingFeature&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;提交更改 (&lt;code&gt;git commit -m 'Add some AmazingFeature'&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;推送到分支 (&lt;code&gt;git push origin feature/AmazingFeature&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;创建 Pull Request&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;📋 查看贡献者&lt;/h3&gt; 
&lt;p&gt;查看所有贡献者和详细贡献内容：&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/CONTRIBUTORS.md"&gt;🤝 贡献者名单&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;📄 许可证&lt;/h2&gt; 
&lt;p&gt;本项目基于 Apache 2.0 许可证开源。详见 &lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/LICENSE"&gt;LICENSE&lt;/a&gt; 文件。&lt;/p&gt; 
&lt;h3&gt;许可证说明&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;✅ 商业使用&lt;/li&gt; 
 &lt;li&gt;✅ 修改和分发&lt;/li&gt; 
 &lt;li&gt;✅ 私人使用&lt;/li&gt; 
 &lt;li&gt;✅ 专利使用&lt;/li&gt; 
 &lt;li&gt;❗ 需要保留版权声明&lt;/li&gt; 
 &lt;li&gt;❗ 需要包含许可证副本&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🙏 致谢与感恩&lt;/h2&gt; 
&lt;h3&gt;🌟 向源项目开发者致敬&lt;/h3&gt; 
&lt;p&gt;我们向 &lt;a href="https://github.com/TauricResearch"&gt;Tauric Research&lt;/a&gt; 团队表达最深的敬意和感谢：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;🎯 愿景领导者&lt;/strong&gt;: 感谢您们在AI金融领域的前瞻性思考和创新实践&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;💎 珍贵源码&lt;/strong&gt;: 感谢您们开源的每一行代码，它们凝聚着无数的智慧和心血&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🏗️ 架构大师&lt;/strong&gt;: 感谢您们设计了如此优雅、可扩展的多智能体框架&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;💡 技术先驱&lt;/strong&gt;: 感谢您们将前沿AI技术与金融实务完美结合&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🔄 持续贡献&lt;/strong&gt;: 感谢您们持续的维护、更新和改进工作&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;🤝 社区贡献者致谢&lt;/h3&gt; 
&lt;p&gt;感谢所有为TradingAgents-CN项目做出贡献的开发者和用户！&lt;/p&gt; 
&lt;p&gt;详细的贡献者名单和贡献内容请查看：&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/CONTRIBUTORS.md"&gt;📋 贡献者名单&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;包括但不限于：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;🐳 &lt;strong&gt;Docker容器化&lt;/strong&gt; - 部署方案优化&lt;/li&gt; 
 &lt;li&gt;📄 &lt;strong&gt;报告导出功能&lt;/strong&gt; - 多格式输出支持&lt;/li&gt; 
 &lt;li&gt;🐛 &lt;strong&gt;Bug修复&lt;/strong&gt; - 系统稳定性提升&lt;/li&gt; 
 &lt;li&gt;🔧 &lt;strong&gt;代码优化&lt;/strong&gt; - 用户体验改进&lt;/li&gt; 
 &lt;li&gt;📝 &lt;strong&gt;文档完善&lt;/strong&gt; - 使用指南和教程&lt;/li&gt; 
 &lt;li&gt;🌍 &lt;strong&gt;社区建设&lt;/strong&gt; - 问题反馈和推广&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🌍 开源贡献&lt;/strong&gt;: 感谢您们选择Apache 2.0协议，给予开发者最大的自由&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;📚 知识分享&lt;/strong&gt;: 感谢您们提供的详细文档和最佳实践指导&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;特别感谢&lt;/strong&gt;：&lt;a href="https://github.com/TauricResearch/TradingAgents"&gt;TradingAgents&lt;/a&gt; 项目为我们提供了坚实的技术基础。虽然Apache 2.0协议赋予了我们使用源码的权利，但我们深知每一行代码的珍贵价值，将永远铭记并感谢您们的无私贡献。&lt;/p&gt; 
&lt;h3&gt;🇨🇳 推广使命的初心&lt;/h3&gt; 
&lt;p&gt;创建这个中文增强版本，我们怀着以下初心：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;🌉 技术传播&lt;/strong&gt;: 让优秀的TradingAgents技术在中国得到更广泛的应用&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🎓 教育普及&lt;/strong&gt;: 为中国的AI金融教育提供更好的工具和资源&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🤝 文化桥梁&lt;/strong&gt;: 在中西方技术社区之间搭建交流合作的桥梁&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🚀 创新推动&lt;/strong&gt;: 推动中国金融科技领域的AI技术创新和应用&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;🌍 开源社区&lt;/h3&gt; 
&lt;p&gt;感谢所有为本项目贡献代码、文档、建议和反馈的开发者和用户。正是因为有了大家的支持，我们才能更好地服务中文用户社区。&lt;/p&gt; 
&lt;h3&gt;🤝 合作共赢&lt;/h3&gt; 
&lt;p&gt;我们承诺：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;尊重原创&lt;/strong&gt;: 始终尊重源项目的知识产权和开源协议&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;反馈贡献&lt;/strong&gt;: 将有价值的改进和创新反馈给源项目和开源社区&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;持续改进&lt;/strong&gt;: 不断完善中文增强版本，提供更好的用户体验&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;开放合作&lt;/strong&gt;: 欢迎与源项目团队和全球开发者进行技术交流与合作&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;📈 版本历史&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;v0.1.13&lt;/strong&gt; (2025-08-02): 🤖 原生OpenAI支持与Google AI生态系统全面集成 ✨ &lt;strong&gt;最新版本&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;v0.1.12&lt;/strong&gt; (2025-07-29): 🧠 智能新闻分析模块与项目结构优化&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;v0.1.11&lt;/strong&gt; (2025-07-27): 🤖 多LLM提供商集成与模型选择持久化&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;v0.1.10&lt;/strong&gt; (2025-07-18): 🚀 Web界面实时进度显示与智能会话管理&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;v0.1.9&lt;/strong&gt; (2025-07-16): 🎯 CLI用户体验重大优化与统一日志管理&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;v0.1.8&lt;/strong&gt; (2025-07-15): 🎨 Web界面全面优化与用户体验提升&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;v0.1.7&lt;/strong&gt; (2025-07-13): 🐳 容器化部署与专业报告导出&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;v0.1.6&lt;/strong&gt; (2025-07-11): 🔧 阿里百炼修复与数据源升级&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;v0.1.5&lt;/strong&gt; (2025-07-08): 📊 添加Deepseek模型支持&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;v0.1.4&lt;/strong&gt; (2025-07-05): 🏗️ 架构优化与配置管理重构&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;v0.1.3&lt;/strong&gt; (2025-06-28): 🇨🇳 A股市场完整支持&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;v0.1.2&lt;/strong&gt; (2025-06-15): 🌐 Web界面和配置管理&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;v0.1.1&lt;/strong&gt; (2025-06-01): 🧠 国产LLM集成&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;📋 &lt;strong&gt;详细更新日志&lt;/strong&gt;: &lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/releases/CHANGELOG.md"&gt;CHANGELOG.md&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;📞 联系方式&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;GitHub Issues&lt;/strong&gt;: &lt;a href="https://github.com/hsliuping/TradingAgents-CN/issues"&gt;提交问题和建议&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;邮箱&lt;/strong&gt;: &lt;a href="mailto:hsliup@163.com"&gt;hsliup@163.com&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;项目ＱＱ群：187537480&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;项目微信公众号：TradingAgents-CN&lt;/p&gt; &lt;img src="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/assets/weixin.png" alt="微信公众号" width="200" /&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;原项目&lt;/strong&gt;: &lt;a href="https://github.com/TauricResearch/TradingAgents"&gt;TauricResearch/TradingAgents&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;文档&lt;/strong&gt;: &lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/"&gt;完整文档目录&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;⚠️ 风险提示&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;重要声明&lt;/strong&gt;: 本框架仅用于研究和教育目的，不构成投资建议。&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;📊 交易表现可能因多种因素而异&lt;/li&gt; 
 &lt;li&gt;🤖 AI模型的预测存在不确定性&lt;/li&gt; 
 &lt;li&gt;💰 投资有风险，决策需谨慎&lt;/li&gt; 
 &lt;li&gt;👨‍💼 建议咨询专业财务顾问&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;strong&gt;🌟 如果这个项目对您有帮助，请给我们一个 Star！&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/hsliuping/TradingAgents-CN"&gt;⭐ Star this repo&lt;/a&gt; | &lt;a href="https://github.com/hsliuping/TradingAgents-CN/fork"&gt;🍴 Fork this repo&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/hsliuping/TradingAgents-CN/main/docs/"&gt;📖 Read the docs&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>DrewThomasson/ebook2audiobook</title>
      <link>https://github.com/DrewThomasson/ebook2audiobook</link>
      <description>&lt;p&gt;Generate audiobooks from e-books, voice cloning &amp; 1107+ languages!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;📚 ebook2audiobook&lt;/h1&gt; 
&lt;p&gt;CPU/GPU Converter from eBooks to audiobooks with chapters and metadata&lt;br /&gt; using XTTSv2, Bark, Vits, Fairseq, YourTTS, Tacotron and more. Supports voice cloning and +1110 languages!&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] &lt;strong&gt;This tool is intended for use with non-DRM, legally acquired eBooks only.&lt;/strong&gt; &lt;br /&gt; The authors are not responsible for any misuse of this software or any resulting legal consequences. &lt;br /&gt; Use this tool responsibly and in accordance with all applicable laws.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;a href="https://discord.gg/63Tv3F65k6"&gt;&lt;img src="https://dcbadge.limes.pink/api/server/https://discord.gg/63Tv3F65k6" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Thanks to support ebook2audiobook developers!&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://ko-fi.com/athomasson2"&gt;&lt;img src="https://img.shields.io/badge/Ko--fi-F16061?style=for-the-badge&amp;amp;logo=ko-fi&amp;amp;logoColor=white" alt="Ko-Fi" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Run locally&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#launching-gradio-web-interface"&gt;&lt;img src="https://img.shields.io/badge/Quick%20Start-blue?style=for-the-badge" alt="Quick Start" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/DrewThomasson/ebook2audiobook/actions/workflows/Docker-Build.yml"&gt;&lt;img src="https://github.com/DrewThomasson/ebook2audiobook/actions/workflows/Docker-Build.yml/badge.svg?sanitize=true" alt="Docker Build" /&gt;&lt;/a&gt; &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/releases/latest"&gt;&lt;img src="https://img.shields.io/badge/Download-Now-blue.svg?sanitize=true" alt="Download" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;a href="https://github.com/DrewThomasson/ebook2audiobook"&gt; &lt;img src="https://img.shields.io/badge/Platform-mac%20|%20linux%20|%20windows-lightgrey" alt="Platform" /&gt; &lt;/a&gt;
&lt;a href="https://hub.docker.com/r/athomasson2/ebook2audiobook"&gt; &lt;img alt="Docker Pull Count" src="https://img.shields.io/docker/pulls/athomasson2/ebook2audiobook.svg?sanitize=true" /&gt; &lt;/a&gt; 
&lt;h3&gt;Run Remotely&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/spaces/drewThomasson/ebook2audiobook"&gt;&lt;img src="https://img.shields.io/badge/Hugging%20Face-Spaces-yellow?style=flat&amp;amp;logo=huggingface" alt="Hugging Face" /&gt;&lt;/a&gt; &lt;a href="https://colab.research.google.com/github/DrewThomasson/ebook2audiobook/blob/main/Notebooks/colab_ebook2audiobook.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Free Google Colab" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Rihcus/ebook2audiobookXTTS/raw/main/Notebooks/kaggle-ebook2audiobook.ipynb"&gt;&lt;img src="https://img.shields.io/badge/Kaggle-035a7d?style=flat&amp;amp;logo=kaggle&amp;amp;logoColor=white" alt="Kaggle" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;GUI Interface&lt;/h4&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/assets/demo_web_gui.gif" alt="demo_web_gui" /&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click to see images of Web GUI&lt;/summary&gt; 
 &lt;img width="1728" alt="GUI Screen 1" src="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/assets/gui_1.png" /&gt; 
 &lt;img width="1728" alt="GUI Screen 2" src="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/assets/gui_2.png" /&gt; 
 &lt;img width="1728" alt="GUI Screen 3" src="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/assets/gui_3.png" /&gt; 
&lt;/details&gt; 
&lt;h2&gt;Demos&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;New Default Voice Demo&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/750035dc-e355-46f1-9286-05c1d9e88cea"&gt;https://github.com/user-attachments/assets/750035dc-e355-46f1-9286-05c1d9e88cea&lt;/a&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;More Demos&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;ASMR Voice&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/68eee9a1-6f71-4903-aacd-47397e47e422"&gt;https://github.com/user-attachments/assets/68eee9a1-6f71-4903-aacd-47397e47e422&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Rainy Day Voice&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/d25034d9-c77f-43a9-8f14-0d167172b080"&gt;https://github.com/user-attachments/assets/d25034d9-c77f-43a9-8f14-0d167172b080&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Scarlett Voice&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/b12009ee-ec0d-45ce-a1ef-b3a52b9f8693"&gt;https://github.com/user-attachments/assets/b12009ee-ec0d-45ce-a1ef-b3a52b9f8693&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;David Attenborough Voice&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/81c4baad-117e-4db5-ac86-efc2b7fea921"&gt;https://github.com/user-attachments/assets/81c4baad-117e-4db5-ac86-efc2b7fea921&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://github.com/DrewThomasson/VoxNovel/raw/dc5197dff97252fa44c391dc0596902d71278a88/readme_files/example_in_app.jpeg" alt="Example" /&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;README.md&lt;/h2&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#-ebook2audiobook"&gt;ebook2audiobook&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#features"&gt;Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#gui-interface"&gt;GUI Interface&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#demos"&gt;Demos&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#supported-languages"&gt;Supported Languages&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#hardware-requirements"&gt;Minimum Requirements&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#launching-gradio-web-interface"&gt;Usage&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#launching-gradio-web-interface"&gt;Run Locally&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#launching-gradio-web-interface"&gt;Launching Gradio Web Interface&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#basic--usage"&gt;Basic Headless Usage&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#example-of-custom-model-zip-upload"&gt;Headless Custom XTTS Model Usage&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#help-command-output"&gt;Help command output&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#run-remotely"&gt;Run Remotely&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#fine-tuned-tts-models"&gt;Fine Tuned TTS models&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#fine-tuned-tts-collection"&gt;Collection of Fine-Tuned TTS Models&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#fine-tune-your-own-xttsv2-model"&gt;Train XTTSv2&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#docker-gpu-options"&gt;Docker&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#docker-gpu-options"&gt;GPU options&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#running-the-pre-built-docker-container"&gt;Docker Run&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#building-the-docker-container"&gt;Docker Build&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#docker-compose"&gt;Docker Compose&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#docker-headless-guide"&gt;Docker headless guide&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#docker-container-file-locations"&gt;Docker container file locations&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#common-docker-issues"&gt;Common Docker issues&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#supported-ebook-formats"&gt;Supported eBook Formats&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#output-formats"&gt;Output Formats&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#updating-to-latest-version"&gt;Updating to Latest Version&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#reverting-to-older-versions"&gt;Revert to older Version&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#common-issues"&gt;Common Issues&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#special-thanks"&gt;Special Thanks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#table-of-contents"&gt;Table of Contents&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;📚 Splits eBook into chapters for organized audio.&lt;/li&gt; 
 &lt;li&gt;🎙️ High-quality text-to-speech with &lt;a href="https://huggingface.co/coqui/XTTS-v2"&gt;Coqui XTTSv2&lt;/a&gt; and &lt;a href="https://github.com/facebookresearch/fairseq/tree/main/examples/mms"&gt;Fairseq&lt;/a&gt; (and more).&lt;/li&gt; 
 &lt;li&gt;🗣️ Optional voice cloning with your own voice file.&lt;/li&gt; 
 &lt;li&gt;🌍 Supports +1110 languages (English by default). &lt;a href="https://dl.fbaipublicfiles.com/mms/tts/all-tts-languages.html"&gt;List of Supported languages&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;🖥️ Designed to run on 4GB RAM.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Supported Languages&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;&lt;strong&gt;Arabic (ar)&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;strong&gt;Chinese (zh)&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;strong&gt;English (en)&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;strong&gt;Spanish (es)&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;French (fr)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;German (de)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Italian (it)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Portuguese (pt)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Polish (pl)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Turkish (tr)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Russian (ru)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Dutch (nl)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Czech (cs)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Japanese (ja)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Hindi (hi)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Bengali (bn)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Hungarian (hu)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Korean (ko)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Vietnamese (vi)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Swedish (sv)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Persian (fa)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Yoruba (yo)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Swahili (sw)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Indonesian (id)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Slovak (sk)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Croatian (hr)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Tamil (ta)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Danish (da)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://dl.fbaipublicfiles.com/mms/tts/all-tts-languages.html"&gt;&lt;strong&gt;+1100 languages and dialects here&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Hardware Requirements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;4gb RAM minimum, 8GB recommended&lt;/li&gt; 
 &lt;li&gt;Virtualization enabled if running on windows (Docker only)&lt;/li&gt; 
 &lt;li&gt;CPU (intel, AMD, ARM), GPU (Nvidia, AMD*, Intel*) (Recommended), MPS (Apple Silicon CPU) *available very soon&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] &lt;strong&gt;Before to post an install or bug issue search carefully to the opened and closed issues TAB&lt;br /&gt; to be sure your issue does not exist already.&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] &lt;strong&gt;Lacking of any standards structure like what is a chapter, paragraph, preface etc.&lt;br /&gt; you should first remove manually any text you don't want to be converted in audio.&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Installation Instructions&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Clone repo&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/DrewThomasson/ebook2audiobook.git
cd ebook2audiobook
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Launching Gradio Web Interface&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Run ebook2audiobook&lt;/strong&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Linux/MacOS&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;./ebook2audiobook.sh  # Run launch script
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Mac Launcher&lt;/strong&gt;&lt;br /&gt; Double click &lt;code&gt;Mac Ebook2Audiobook Launcher.command&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Windows&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;ebook2audiobook.cmd  # Run launch script or double click on it
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Windows Launcher&lt;/strong&gt;&lt;br /&gt; Double click &lt;code&gt;ebook2audiobook.cmd&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Manual Python Install&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# (for experts only!)
REQUIRED_PROGRAMS=("calibre" "ffmpeg" "nodejs" "mecab" "espeak-ng" "rust" "sox")
REQUIRED_PYTHON_VERSION="3.12"
pip install -r requirements.txt  # Install Python Requirements
python app.py  # Run Ebook2Audiobook
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Open the Web App&lt;/strong&gt;: Click the URL provided in the terminal to access the web app and convert eBooks. &lt;code&gt;http://localhost:7860/&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;For Public Link&lt;/strong&gt;: &lt;code&gt;python app.py --share&lt;/code&gt; (all OS) &lt;code&gt;./ebook2audiobook.sh --share&lt;/code&gt; (Linux/MacOS) &lt;code&gt;ebook2audiobook.cmd --share&lt;/code&gt; (Windows)&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] &lt;strong&gt;If the script is stopped and run again, you need to refresh your gradio GUI interface&lt;br /&gt; to let the web page reconnect to the new connection socket.&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Basic Usage&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Linux/MacOS&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;./ebook2audiobook.sh --headless --ebook &amp;lt;path_to_ebook_file&amp;gt; \
    --voice [path_to_voice_file] --language [language_code]
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Windows&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;ebook2audiobook.cmd --headless --ebook &amp;lt;path_to_ebook_file&amp;gt;
    --voice [path_to_voice_file] --language [language_code]
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;[--ebook]&lt;/strong&gt;: Path to your eBook file&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;[--voice]&lt;/strong&gt;: Voice cloning file path (optional)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;[--language]&lt;/strong&gt;: Language code in ISO-639-3 (i.e.: ita for italian, eng for english, deu for german...).&lt;br /&gt; Default language is eng and --language is optional for default language set in ./lib/lang.py.&lt;br /&gt; The ISO-639-1 2 letters codes are also supported.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Example of Custom Model Zip Upload&lt;/h3&gt; 
&lt;p&gt;(must be a .zip file containing the mandatory model files. Example for XTTSv2: config.json, model.pth, vocab.json and ref.wav)&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Linux/MacOS&lt;/strong&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;./ebook2audiobook.sh --headless --ebook &amp;lt;ebook_file_path&amp;gt; \
    --voice &amp;lt;target_voice_file_path&amp;gt; --language &amp;lt;language&amp;gt; --custom_model &amp;lt;custom_model_path&amp;gt;
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;ebook2audiobook.cmd --headless --ebook &amp;lt;ebook_file_path&amp;gt; \
    --voice &amp;lt;target_voice_file_path&amp;gt; --language &amp;lt;language&amp;gt; --custom_model &amp;lt;custom_model_path&amp;gt;
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&amp;lt;custom_model_path&amp;gt;&lt;/strong&gt;: Path to &lt;code&gt;model_name.zip&lt;/code&gt; file, which must contain (according to the tts engine) all the mandatory files&lt;br /&gt; (see ./lib/models.py).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;For Detailed Guide with list of all Parameters to use&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Linux/MacOS&lt;/strong&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;./ebook2audiobook.sh --help
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;ebook2audiobook.cmd --help
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Or for all OS&lt;/strong&gt; &lt;code&gt;python app.py --help &lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a id="help-command-output"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;usage: app.py [-h] [--session SESSION] [--share] [--headless] [--ebook EBOOK]
              [--ebooks_dir EBOOKS_DIR] [--language LANGUAGE] [--voice VOICE]
              [--device {cpu,gpu,mps}]
              [--tts_engine {XTTSv2,BARK,VITS,FAIRSEQ,TACOTRON2,YOURTTS,xtts,bark,vits,fairseq,tacotron,yourtts}]
              [--custom_model CUSTOM_MODEL] [--fine_tuned FINE_TUNED]
              [--output_format OUTPUT_FORMAT] [--temperature TEMPERATURE]
              [--length_penalty LENGTH_PENALTY] [--num_beams NUM_BEAMS]
              [--repetition_penalty REPETITION_PENALTY] [--top_k TOP_K]
              [--top_p TOP_P] [--speed SPEED] [--enable_text_splitting]
              [--text_temp TEXT_TEMP] [--waveform_temp WAVEFORM_TEMP]
              [--output_dir OUTPUT_DIR] [--version]

Convert eBooks to Audiobooks using a Text-to-Speech model. You can either launch the Gradio interface or run the script in headless mode for direct conversion.

options:
  -h, --help            show this help message and exit
  --session SESSION     Session to resume the conversion in case of interruption, crash, 
                            or reuse of custom models and custom cloning voices.

**** The following options are for all modes:
  Optional

**** The following option are for gradio/gui mode only:
  Optional

  --share               Enable a public shareable Gradio link.

**** The following options are for --headless mode only:
  --headless            Run the script in headless mode
  --ebook EBOOK         Path to the ebook file for conversion. Cannot be used when --ebooks_dir is present.
  --ebooks_dir EBOOKS_DIR
                        Relative or absolute path of the directory containing the files to convert. 
                            Cannot be used when --ebook is present.
  --language LANGUAGE   Language of the e-book. Default language is set 
                            in ./lib/lang.py sed as default if not present. All compatible language codes are in ./lib/lang.py

optional parameters:
  --voice VOICE         (Optional) Path to the voice cloning file for TTS engine. 
                            Uses the default voice if not present.
  --device {cpu,gpu,mps}
                        (Optional) Pprocessor unit type for the conversion. 
                            Default is set in ./lib/conf.py if not present. Fall back to CPU if GPU not available.
  --tts_engine {XTTSv2,BARK,VITS,FAIRSEQ,TACOTRON2,YOURTTS,xtts,bark,vits,fairseq,tacotron,yourtts}
                        (Optional) Preferred TTS engine (available are: ['XTTSv2', 'BARK', 'VITS', 'FAIRSEQ', 'TACOTRON2', 'YOURTTS', 'xtts', 'bark', 'vits', 'fairseq', 'tacotron', 'yourtts'].
                            Default depends on the selected language. The tts engine should be compatible with the chosen language
  --custom_model CUSTOM_MODEL
                        (Optional) Path to the custom model zip file cntaining mandatory model files. 
                            Please refer to ./lib/models.py
  --fine_tuned FINE_TUNED
                        (Optional) Fine tuned model path. Default is builtin model.
  --output_format OUTPUT_FORMAT
                        (Optional) Output audio format. Default is set in ./lib/conf.py
  --temperature TEMPERATURE
                        (xtts only, optional) Temperature for the model. 
                            Default to config.json model. Higher temperatures lead to more creative outputs.
  --length_penalty LENGTH_PENALTY
                        (xtts only, optional) A length penalty applied to the autoregressive decoder. 
                            Default to config.json model. Not applied to custom models.
  --num_beams NUM_BEAMS
                        (xtts only, optional) Controls how many alternative sequences the model explores. Must be equal or greater than length penalty. 
                            Default to config.json model.
  --repetition_penalty REPETITION_PENALTY
                        (xtts only, optional) A penalty that prevents the autoregressive decoder from repeating itself. 
                            Default to config.json model.
  --top_k TOP_K         (xtts only, optional) Top-k sampling. 
                            Lower values mean more likely outputs and increased audio generation speed. 
                            Default to config.json model.
  --top_p TOP_P         (xtts only, optional) Top-p sampling. 
                            Lower values mean more likely outputs and increased audio generation speed. Default to config.json model.
  --speed SPEED         (xtts only, optional) Speed factor for the speech generation. 
                            Default to config.json model.
  --enable_text_splitting
                        (xtts only, optional) Enable TTS text splitting. This option is known to not be very efficient. 
                            Default to config.json model.
  --text_temp TEXT_TEMP
                        (bark only, optional) Text Temperature for the model. 
                            Default to 0.85. Higher temperatures lead to more creative outputs.
  --waveform_temp WAVEFORM_TEMP
                        (bark only, optional) Waveform Temperature for the model. 
                            Default to 0.5. Higher temperatures lead to more creative outputs.
  --output_dir OUTPUT_DIR
                        (Optional) Path to the output directory. Default is set in ./lib/conf.py
  --version             Show the version of the script and exit

Example usage:    
Windows:
    Gradio/GUI:
    ebook2audiobook.cmd
    Headless mode:
    ebook2audiobook.cmd --headless --ebook '/path/to/file'
Linux/Mac:
    Gradio/GUI:
    ./ebook2audiobook.sh
    Headless mode:
    ./ebook2audiobook.sh --headless --ebook '/path/to/file'
    
Tip: to add of silence (1.4 seconds) into your text just use "###" or "[pause]".

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;NOTE: in gradio/gui mode, to cancel a running conversion, just click on the [X] from the ebook upload component.&lt;/p&gt; 
&lt;p&gt;TIP: if it needs some more pauses, just add '###' or '[pause]' between the words you wish more pause. one [pause] equals to 1.4 seconds&lt;/p&gt; 
&lt;h4&gt;Docker GPU Options&lt;/h4&gt; 
&lt;p&gt;Available pre-build tags: &lt;code&gt;latest&lt;/code&gt; (CUDA 11.8)&lt;/p&gt; 
&lt;h4&gt;Edit: IF GPU isn't detected then you'll have to build the image -&amp;gt; &lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#building-the-docker-container"&gt;Building the Docker Container&lt;/a&gt;&lt;/h4&gt; 
&lt;h4&gt;Running the pre-built Docker Container&lt;/h4&gt; 
&lt;p&gt;-Run with CPU only&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-powershell"&gt;docker run --pull always --rm -p 7860:7860 athomasson2/ebook2audiobook
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;-Run with GPU Speedup (NVIDIA compatible only)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-powershell"&gt;docker run --pull always --rm --gpus all -p 7860:7860 athomasson2/ebook2audiobook
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This command will start the Gradio interface on port 7860.(localhost:7860)&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;For more options add the parameter &lt;code&gt;--help&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Building the Docker Container&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;You can build the docker image with the command:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-powershell"&gt;docker build -t athomasson2/ebook2audiobook .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Avalible Docker Build Arguments&lt;/h4&gt; 
&lt;p&gt;&lt;code&gt;--build-arg TORCH_VERSION=cuda118&lt;/code&gt; Available tags: [cuda121, cuda118, cuda128, rocm, xpu, cpu]&lt;/p&gt; 
&lt;p&gt;All CUDA version numbers should work, Ex: CUDA 11.6-&amp;gt; cuda116&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;--build-arg SKIP_XTTS_TEST=true&lt;/code&gt; (Saves space by not baking XTTSv2 model into docker image)&lt;/p&gt; 
&lt;h2&gt;Docker container file locations&lt;/h2&gt; 
&lt;p&gt;All ebook2audiobooks will have the base dir of &lt;code&gt;/app/&lt;/code&gt; For example: &lt;code&gt;tmp&lt;/code&gt; = &lt;code&gt;/app/tmp&lt;/code&gt; &lt;code&gt;audiobooks&lt;/code&gt; = &lt;code&gt;/app/audiobooks&lt;/code&gt;&lt;/p&gt; 
&lt;h2&gt;Docker headless guide&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Before you do run this you need to create a dir named "input-folder" in your current dir which will be linked, This is where you can put your input files for the docker image to see&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;mkdir input-folder &amp;amp;&amp;amp; mkdir Audiobooks
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;In the command below swap out &lt;strong&gt;YOUR_INPUT_FILE.TXT&lt;/strong&gt; with the name of your input file&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run --pull always --rm \
    -v $(pwd)/input-folder:/app/input_folder \
    -v $(pwd)/audiobooks:/app/audiobooks \
    athomasson2/ebook2audiobook \
    --headless --ebook /input_folder/YOUR_EBOOK_FILE
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;The output Audiobooks will be found in the Audiobook folder which will also be located in your local dir you ran this docker command in&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;To get the help command for the other parameters this program has you can run this&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run --pull always --rm athomasson2/ebook2audiobook --help

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;That will output this &lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#help-command-output"&gt;Help command output&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Docker Compose&lt;/h3&gt; 
&lt;p&gt;This project uses Docker Compose to run locally. You can enable or disable GPU support by setting either &lt;code&gt;*gpu-enabled&lt;/code&gt; or &lt;code&gt;*gpu-disabled&lt;/code&gt; in &lt;code&gt;docker-compose.yml&lt;/code&gt;&lt;/p&gt; 
&lt;h4&gt;Steps to Run&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Clone the Repository&lt;/strong&gt; (if you haven't already): &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/DrewThomasson/ebook2audiobook.git
cd ebook2audiobook
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Set GPU Support (disabled by default)&lt;/strong&gt; To enable GPU support, modify &lt;code&gt;docker-compose.yml&lt;/code&gt; and change &lt;code&gt;*gpu-disabled&lt;/code&gt; to &lt;code&gt;*gpu-enabled&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Start the service:&lt;/strong&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# Docker
docker-compose up -d # To update add --build

# Podman
podman compose -f podman-compose.yml up -d # To update add --build
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Access the service:&lt;/strong&gt; The service will be available at &lt;a href="http://localhost:7860"&gt;http://localhost:7860&lt;/a&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Common Docker Issues&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;My NVIDIA GPU isnt being detected?? -&amp;gt; &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/wiki/GPU-ISSUES"&gt;GPU ISSUES Wiki Page&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;python: can't open file '/home/user/app/app.py': [Errno 2] No such file or directory&lt;/code&gt; (Just remove all post arguments as I replaced the &lt;code&gt;CMD&lt;/code&gt; with &lt;code&gt;ENTRYPOINT&lt;/code&gt; in the &lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/Dockerfile"&gt;Dockerfile&lt;/a&gt;)&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Example: &lt;code&gt;docker run --pull always athomasson2/ebook2audiobook app.py --script_mode full_docker&lt;/code&gt; - &amp;gt; corrected - &amp;gt; &lt;code&gt;docker run --pull always athomasson2/ebook2audiobook&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Arguments can be easily added like this now &lt;code&gt;docker run --pull always athomasson2/ebook2audiobook --share&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Docker gets stuck downloading Fine-Tuned models. (This does not happen for every computer but some appear to run into this issue) Disabling the progress bar appears to fix the issue, as discussed &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/issues/191"&gt;here in #191&lt;/a&gt; Example of adding this fix in the &lt;code&gt;docker run&lt;/code&gt; command&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-Dockerfile"&gt;docker run --pull always --rm --gpus all -e HF_HUB_DISABLE_PROGRESS_BARS=1 -e HF_HUB_ENABLE_HF_TRANSFER=0 \
    -p 7860:7860 athomasson2/ebook2audiobook
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Fine Tuned TTS models&lt;/h2&gt; 
&lt;h4&gt;Fine Tune your own XTTSv2 model&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/spaces/drewThomasson/xtts-finetune-webui-gpu"&gt;&lt;img src="https://img.shields.io/badge/Hugging%20Face-Spaces-yellow?style=flat&amp;amp;logo=huggingface" alt="Hugging Face" /&gt;&lt;/a&gt; &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/raw/v25/Notebooks/finetune/xtts/kaggle-xtts-finetune-webui-gradio-gui.ipynb"&gt;&lt;img src="https://img.shields.io/badge/Kaggle-035a7d?style=flat&amp;amp;logo=kaggle&amp;amp;logoColor=white" alt="Kaggle" /&gt;&lt;/a&gt; &lt;a href="https://colab.research.google.com/github/DrewThomasson/ebook2audiobook/blob/v25/Notebooks/finetune/xtts/colab_xtts_finetune_webui.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;De-noise training data&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/spaces/drewThomasson/DeepFilterNet2_no_limit"&gt;&lt;img src="https://img.shields.io/badge/Hugging%20Face-Spaces-yellow?style=flat&amp;amp;logo=huggingface" alt="Hugging Face" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Rikorose/DeepFilterNet"&gt;&lt;img src="https://img.shields.io/badge/DeepFilterNet-181717?logo=github" alt="GitHub Repo" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Fine Tuned TTS Collection&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/drewThomasson/fineTunedTTSModels/tree/main"&gt;&lt;img src="https://img.shields.io/badge/Hugging%20Face-Models-yellow?style=flat&amp;amp;logo=huggingface" alt="Hugging Face" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;For an XTTSv2 custom model a ref audio clip of the voice reference is mandatory:&lt;/p&gt; 
&lt;h2&gt;Supported eBook Formats&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;.epub&lt;/code&gt;, &lt;code&gt;.pdf&lt;/code&gt;, &lt;code&gt;.mobi&lt;/code&gt;, &lt;code&gt;.txt&lt;/code&gt;, &lt;code&gt;.html&lt;/code&gt;, &lt;code&gt;.rtf&lt;/code&gt;, &lt;code&gt;.chm&lt;/code&gt;, &lt;code&gt;.lit&lt;/code&gt;, &lt;code&gt;.pdb&lt;/code&gt;, &lt;code&gt;.fb2&lt;/code&gt;, &lt;code&gt;.odt&lt;/code&gt;, &lt;code&gt;.cbr&lt;/code&gt;, &lt;code&gt;.cbz&lt;/code&gt;, &lt;code&gt;.prc&lt;/code&gt;, &lt;code&gt;.lrf&lt;/code&gt;, &lt;code&gt;.pml&lt;/code&gt;, &lt;code&gt;.snb&lt;/code&gt;, &lt;code&gt;.cbc&lt;/code&gt;, &lt;code&gt;.rb&lt;/code&gt;, &lt;code&gt;.tcr&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Best results&lt;/strong&gt;: &lt;code&gt;.epub&lt;/code&gt; or &lt;code&gt;.mobi&lt;/code&gt; for automatic chapter detection&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Output Formats&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Creates a &lt;code&gt;['m4b', 'm4a', 'mp4', 'webm', 'mov', 'mp3', 'flac', 'wav', 'ogg', 'aac']&lt;/code&gt; (set in ./lib/conf.py) file with metadata and chapters.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Updating to Latest Version&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git pull # Locally/Compose

docker pull athomasson2/ebook2audiobook:latest # For Pre-build docker images
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Reverting to older Versions&lt;/h2&gt; 
&lt;p&gt;Releases can be found -&amp;gt; &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/releases"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git checkout tags/VERSION_NUM # Locally/Compose -&amp;gt; Example: git checkout tags/v25.7.7

athomasson2/ebook2audiobook:VERSION_NUM # For Pre-build docker images -&amp;gt; Example: athomasson2/ebook2audiobook:v25.7.7
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Common Issues:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;My NVIDIA GPU isnt being detected?? -&amp;gt; &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/wiki/GPU-ISSUES"&gt;GPU ISSUES Wiki Page&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;CPU is slow (better on server smp CPU) while NVIDIA GPU can have almost real time conversion. &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/discussions/19#discussioncomment-10879846"&gt;Discussion about this&lt;/a&gt; For faster multilingual generation I would suggest my other &lt;a href="https://github.com/DrewThomasson/ebook2audiobookpiper-tts"&gt;project that uses piper-tts&lt;/a&gt; instead (It doesn't have zero-shot voice cloning though, and is Siri quality voices, but it is much faster on cpu).&lt;/li&gt; 
 &lt;li&gt;"I'm having dependency issues" - Just use the docker, its fully self contained and has a headless mode, add &lt;code&gt;--help&lt;/code&gt; parameter at the end of the docker run command for more information.&lt;/li&gt; 
 &lt;li&gt;"Im getting a truncated audio issue!" - PLEASE MAKE AN ISSUE OF THIS, we don't speak every language and need advise from users to fine tune the sentence splitting logic.😊&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;What we need help with! 🙌&lt;/h2&gt; 
&lt;h2&gt;&lt;a href="https://github.com/DrewThomasson/ebook2audiobook/issues/32"&gt;Full list of things can be found here&lt;/a&gt;&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Any help from people speaking any of the supported languages to help us improve the models&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Do you need to rent a GPU to boost service from us?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;A poll is open here &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/discussions/889"&gt;https://github.com/DrewThomasson/ebook2audiobook/discussions/889&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Special Thanks&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Coqui TTS&lt;/strong&gt;: &lt;a href="https://github.com/idiap/coqui-ai-TTS"&gt;Coqui TTS GitHub&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Calibre&lt;/strong&gt;: &lt;a href="https://calibre-ebook.com"&gt;Calibre Website&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;FFmpeg&lt;/strong&gt;: &lt;a href="https://ffmpeg.org"&gt;FFmpeg Website&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/DrewThomasson/ebook2audiobook/issues/8"&gt;@shakenbake15 for better chapter saving method&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
  </channel>
</rss>