<rss version="2.0">
  <channel>
    <title>GitHub Python Daily Trending</title>
    <description>Daily Trending of Python in GitHub</description>
    <pubDate>Sun, 27 Jul 2025 01:36:04 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>Intelligent-Internet/ii-agent</title>
      <link>https://github.com/Intelligent-Internet/ii-agent</link>
      <description>&lt;p&gt;II-Agent: a new open-source framework to build and deploy intelligent agents&lt;/p&gt;&lt;hr&gt;&lt;img width="3600" height="1890" alt="II-Agent-updated" src="https://github.com/user-attachments/assets/4e6211d1-c565-42a9-9d53-d809154a9493"&gt; 
&lt;h1&gt;II Agent&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/Intelligent-Internet/ii-agent/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/Intelligent-Internet/ii-agent?style=social" alt="GitHub stars"&gt;&lt;/a&gt; &lt;a href="https://discord.gg/yDWPsshPHB"&gt;&lt;img src="https://dcbadge.limes.pink/api/server/yDWPsshPHB?style=flat" alt="Discord Follow"&gt;&lt;/a&gt; &lt;a href="https://opensource.org/licenses/Apache-2.0"&gt;&lt;img src="https://img.shields.io/badge/License-Apache%202.0-blue.svg?sanitize=true" alt="License"&gt;&lt;/a&gt; &lt;a href="https://ii.inc/web/blog/post/ii-agent"&gt;&lt;img src="https://img.shields.io/badge/Blog-II--Agent-blue" alt="Blog"&gt;&lt;/a&gt; &lt;a href="https://ii-agent-gaia.ii.inc/"&gt;&lt;img src="https://img.shields.io/badge/GAIA-Benchmark-green" alt="GAIA Benchmark"&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/Intelligent-Internet/ii-agent"&gt;&lt;img src="https://devin.ai/assets/deepwiki-badge.png" alt="Ask DeepWiki.com" height="20"&gt;&lt;/a&gt;&lt;/p&gt;  
&lt;p&gt;II-Agent is an open-source intelligent assistant designed to streamline and enhance workflows across multiple domains. It represents a significant advancement in how we interact with technologyâ€”shifting from passive tools to intelligent systems capable of independently executing complex tasks.&lt;/p&gt; 
&lt;h3&gt;Discord Join US&lt;/h3&gt; 
&lt;p&gt;ğŸ“¢ Join Our &lt;a href="https://discord.gg/yDWPsshPHB"&gt;Discord Channel&lt;/a&gt;! Looking forward to seeing you there! ğŸ‰&lt;/p&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/2707b106-f37d-41a8-beff-8802b1c9b186"&gt;https://github.com/user-attachments/assets/2707b106-f37d-41a8-beff-8802b1c9b186&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;(New Features) Full-stack Web Agent Show cases!&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/8399f494-1e5a-43ba-9c7b-32861c51075e"&gt;https://github.com/user-attachments/assets/8399f494-1e5a-43ba-9c7b-32861c51075e&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/647e6bae-bc62-4c8b-9e6e-a7c8946caf56"&gt;https://github.com/user-attachments/assets/647e6bae-bc62-4c8b-9e6e-a7c8946caf56&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Overview&lt;/h2&gt; 
&lt;p&gt;II Agent is built around providing an agentic interface to leading language models. It offers:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;A CLI interface for direct command-line interaction&lt;/li&gt; 
 &lt;li&gt;A WebSocket server that powers a modern React-based frontend&lt;/li&gt; 
 &lt;li&gt;Integration with multiple LLM providers: 
  &lt;ul&gt; 
   &lt;li&gt;Anthropic Claude models (direct API or via Google Cloud Vertex AI)&lt;/li&gt; 
   &lt;li&gt;Google Gemini models (direct API or via Google Cloud Vertex AI)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;GAIA Benchmark Evaluation&lt;/h2&gt; 
&lt;p&gt;II-Agent has been evaluated on the GAIA benchmark, which assesses LLM-based agents operating within realistic scenarios across multiple dimensions including multimodal processing, tool utilization, and web searching.&lt;/p&gt; 
&lt;p&gt;We identified several issues with the GAIA benchmark during our evaluation:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Annotation Errors&lt;/strong&gt;: Several incorrect annotations in the dataset (e.g., misinterpreting date ranges, calculation errors)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Outdated Information&lt;/strong&gt;: Some questions reference websites or content no longer accessible&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Language Ambiguity&lt;/strong&gt;: Unclear phrasing leading to different interpretations of questions&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Despite these challenges, II-Agent demonstrated strong performance on the benchmark, particularly in areas requiring complex reasoning, tool use, and multi-step planning.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Intelligent-Internet/ii-agent/main/assets/gaia.jpg" alt="GAIA Benchmark"&gt; You can view the full traces of some samples here: &lt;a href="https://ii-agent-gaia.ii.inc/"&gt;GAIA Benchmark Traces&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Docker Compose&lt;/li&gt; 
 &lt;li&gt;Python 3.10+&lt;/li&gt; 
 &lt;li&gt;Node.js 18+ (for frontend)&lt;/li&gt; 
 &lt;li&gt;At least one of the following: 
  &lt;ul&gt; 
   &lt;li&gt;Anthropic API key, or&lt;/li&gt; 
   &lt;li&gt;Google Gemini API key, or&lt;/li&gt; 
   &lt;li&gt;Google Cloud project with Vertex AI API enabled&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP]&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;For best performance, we recommend using Claude 4.0 Sonnet or Claude Opus 4.0 models.&lt;/li&gt; 
  &lt;li&gt;For fast and cheap, we recommend using GPT4.1 from OpenAI.&lt;/li&gt; 
  &lt;li&gt;Gemini 2.5 Pro is a good balance between performance and cost.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;Docker Installation (Recommended)&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Clone the repository&lt;/li&gt; 
 &lt;li&gt;Run the following command&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code&gt;chmod +x start.sh
./start.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;img width="821" alt="Screenshot 2025-07-08 at 17 50 34" src="https://github.com/user-attachments/assets/094f73aa-7384-4500-a670-528853f92ae7"&gt; 
&lt;p&gt;Our II-Agent supports popular models such as Claude, Gemini, and OpenAI. If youâ€™d like to use a model from OpenRouter, simply configure your OpenAI endpoint with your OpenRouter API key. If you are using Vertex, run with these variables&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;chmod +x start.sh
GOOGLE_APPLICATION_CREDENTIALS=absolute-path-to-credential ./start.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;(Optional) You can fill Google API credentials to connect to google drive. Press enter to skip these steps&lt;/p&gt; 
&lt;h3&gt;Runtime Environment&lt;/h3&gt; 
&lt;img width="821" alt="Screenshot 2025-07-08 at 17 48 08" src="https://github.com/user-attachments/assets/b1fb9f11-b1ef-4f62-bbea-9b67eba45322"&gt; 
&lt;p&gt;You can now select from a variety of models, set your API key, and configure environmentsâ€”all directly from the frontend settings pageâ€”to equip your agents with powerful tools and capabilities. You can also change the agents' runtime environment. Currently, we support three runtime modes: Local, Docker, and E2B. For full-stack web application development, Docker and E2B are highly recommended, while Local Mode is best suited for lighter tasks such as basic webpage building and research.&lt;/p&gt; 
&lt;p&gt;In addition, agents come equipped with built-in NeonDB and Vercel integration, enabling seamless cloud deployment of full-stack applications using a serverless database and serverless infrastructure.&lt;/p&gt; 
&lt;h2&gt;Core Capabilities&lt;/h2&gt; 
&lt;p&gt;II-Agent is a versatile open-source assistant built to elevate your productivity across domains:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Domain&lt;/th&gt; 
   &lt;th&gt;What IIâ€‘Agent Can Do&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Research &amp;amp; Factâ€‘Checking&lt;/td&gt; 
   &lt;td&gt;Multistep web search, source triangulation, structured noteâ€‘taking, rapid summarization&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Content Generation&lt;/td&gt; 
   &lt;td&gt;Blog &amp;amp; article drafts, lesson plans, creative prose, technical manuals, Website creations&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Data Analysis &amp;amp; Visualization&lt;/td&gt; 
   &lt;td&gt;Cleaning, statistics, trend detection, charting, and automated report generation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Software Development&lt;/td&gt; 
   &lt;td&gt;Code synthesis, refactoring, debugging, testâ€‘writing, and stepâ€‘byâ€‘step tutorials across multiple languages&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Dynamic Website Development&lt;/td&gt; 
   &lt;td&gt;Full-stack web application creation with live hosting, framework templates, and real-time deployment&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Workflow Automation&lt;/td&gt; 
   &lt;td&gt;Script generation, browser automation, file management, process optimization&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Problem Solving&lt;/td&gt; 
   &lt;td&gt;Decomposition, alternativeâ€‘path exploration, stepwise guidance, troubleshooting&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Methods&lt;/h2&gt; 
&lt;p&gt;The II-Agent system represents a sophisticated approach to building versatile AI agents. Our methodology centers on:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Core Agent Architecture and LLM Interaction&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;System prompting with dynamically tailored context&lt;/li&gt; 
   &lt;li&gt;Comprehensive interaction history management&lt;/li&gt; 
   &lt;li&gt;Intelligent context management to handle token limitations&lt;/li&gt; 
   &lt;li&gt;Systematic LLM invocation and capability selection&lt;/li&gt; 
   &lt;li&gt;Iterative refinement through execution cycles&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Planning and Reflection&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Structured reasoning for complex problem-solving&lt;/li&gt; 
   &lt;li&gt;Problem decomposition and sequential thinking&lt;/li&gt; 
   &lt;li&gt;Transparent decision-making process&lt;/li&gt; 
   &lt;li&gt;Hypothesis formation and testing&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Execution Capabilities&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;File system operations with intelligent code editing&lt;/li&gt; 
   &lt;li&gt;Command line execution in a secure environment&lt;/li&gt; 
   &lt;li&gt;Advanced web interaction and browser automation&lt;/li&gt; 
   &lt;li&gt;Task finalization and reporting&lt;/li&gt; 
   &lt;li&gt;Specialized capabilities for various modalities (Experimental) (PDF, audio, image, video, slides)&lt;/li&gt; 
   &lt;li&gt;Deep research integration&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Context Management&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Token usage estimation and optimization&lt;/li&gt; 
   &lt;li&gt;Strategic truncation for lengthy interactions&lt;/li&gt; 
   &lt;li&gt;File-based archival for large outputs&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Real-time Communication&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;WebSocket-based interface for interactive use&lt;/li&gt; 
   &lt;li&gt;Isolated agent instances per client&lt;/li&gt; 
   &lt;li&gt;Streaming operational events for responsive UX&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;The II-Agent framework, architected around the reasoning capabilities of large language models like Claude 4.0 Sonnet or Gemini 2.5 Pro, presents a comprehensive and robust methodology for building versatile AI agents. Through its synergistic combination of a powerful LLM, a rich set of execution capabilities, an explicit mechanism for planning and reflection, and intelligent context management strategies, II-Agent is well-equipped to address a wide spectrum of complex, multi-step tasks. Its open-source nature and extensible design provide a strong foundation for continued research and development in the rapidly evolving field of agentic AI.&lt;/p&gt; 
&lt;h2&gt;Acknowledgement&lt;/h2&gt; 
&lt;p&gt;We would like to express our sincere gratitude to the following projects and individuals for their invaluable contributions that have helped shape this project:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;AugmentCode&lt;/strong&gt;: We have incorporated and adapted several key components from the &lt;a href="https://github.com/augmentcode/augment-swebench-agent"&gt;AugmentCode project&lt;/a&gt;. AugmentCode focuses on SWE-bench, a benchmark that tests AI systems on real-world software engineering tasks from GitHub issues in popular open-source projects. Their system provides tools for bash command execution, file operations, and sequential problem-solving capabilities designed specifically for software engineering tasks.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Manus&lt;/strong&gt;: Our system prompt architecture draws inspiration from Manus's work, which has helped us create more effective and contextually aware AI interactions.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Index Browser Use&lt;/strong&gt;: We have built upon and extended the functionality of the &lt;a href="https://github.com/lmnr-ai/index/tree/main"&gt;Index Browser Use project&lt;/a&gt;, particularly in our web interaction and browsing capabilities. Their foundational work has enabled us to create more sophisticated web-based agent behaviors.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We are committed to open source collaboration and believe in acknowledging the work that has helped us build this project. If you feel your work has been used in this project but hasn't been properly acknowledged, please reach out to us.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>infinition/Bjorn</title>
      <link>https://github.com/infinition/Bjorn</link>
      <description>&lt;p&gt;Bjorn is a powerful network scanning and offensive security tool for the Raspberry Pi with a 2.13-inch e-Paper HAT. It discovers network targets, identifies open ports, exposed services, and potential vulnerabilities. Bjorn can perform brute force attacks, file stealing, host zombification, and supports custom attack scripts.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;img src="https://github.com/user-attachments/assets/c5eb4cc1-0c3d-497d-9422-1614651a84ab" alt="thumbnail_IMG_0546" width="33"&gt; Bjorn&lt;/h1&gt; 
&lt;p&gt;&lt;img src="https://img.shields.io/badge/Python-3776AB?logo=python&amp;amp;logoColor=fff" alt="Python"&gt; &lt;img src="https://img.shields.io/badge/Status-Development-blue.svg?sanitize=true" alt="Status"&gt; &lt;a href="https://opensource.org/licenses/MIT"&gt;&lt;img src="https://img.shields.io/badge/License-MIT-yellow.svg?sanitize=true" alt="License: MIT"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.reddit.com/r/Bjorn_CyberViking"&gt;&lt;img src="https://img.shields.io/badge/Reddit-Bjorn__CyberViking-orange?style=for-the-badge&amp;amp;logo=reddit" alt="Reddit"&gt;&lt;/a&gt; &lt;a href="https://discord.com/invite/B3ZH9taVfT"&gt;&lt;img src="https://img.shields.io/badge/Discord-Join%20Us-7289DA?style=for-the-badge&amp;amp;logo=discord" alt="Discord"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://github.com/user-attachments/assets/c5eb4cc1-0c3d-497d-9422-1614651a84ab" alt="thumbnail_IMG_0546" width="150"&gt; &lt;img src="https://github.com/user-attachments/assets/1b490f07-f28e-4418-8d41-14f1492890c6" alt="bjorn_epd-removebg-preview" width="150"&gt; &lt;/p&gt; 
&lt;p&gt;Bjorn is a&amp;nbsp;Â«&amp;nbsp;Tamagotchi like&amp;nbsp;Â» sophisticated, autonomous network scanning, vulnerability assessment, and offensive security tool designed to run on a Raspberry Pi equipped with a 2.13-inch e-Paper HAT. This document provides a detailed explanation of the project.&lt;/p&gt; 
&lt;h2&gt;ğŸ“š Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/infinition/Bjorn/main/#-introduction"&gt;Introduction&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/infinition/Bjorn/main/#-features"&gt;Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/infinition/Bjorn/main/#-getting-started"&gt;Getting Started&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/infinition/Bjorn/main/#-prerequisites"&gt;Prerequisites&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/infinition/Bjorn/main/#-installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/infinition/Bjorn/main/#-quick-start"&gt;Quick Start&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/infinition/Bjorn/main/#-usage-example"&gt;Usage Example&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/infinition/Bjorn/main/#-contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/infinition/Bjorn/main/#-license"&gt;License&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/infinition/Bjorn/main/#-contact"&gt;Contact&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ“„ Introduction&lt;/h2&gt; 
&lt;p&gt;Bjorn is a powerful tool designed to perform comprehensive network scanning, vulnerability assessment, and data ex-filtration. Its modular design and extensive configuration options allow for flexible and targeted operations. By combining different actions and orchestrating them intelligently, Bjorn can provide valuable insights into network security and help identify and mitigate potential risks.&lt;/p&gt; 
&lt;p&gt;The e-Paper HAT display and web interface make it easy to monitor and interact with Bjorn, providing real-time updates and status information. With its extensible architecture and customizable actions, Bjorn can be adapted to suit a wide range of security testing and monitoring needs.&lt;/p&gt; 
&lt;h2&gt;ğŸŒŸ Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Network Scanning&lt;/strong&gt;: Identifies live hosts and open ports on the network.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Vulnerability Assessment&lt;/strong&gt;: Performs vulnerability scans using Nmap and other tools.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;System Attacks&lt;/strong&gt;: Conducts brute-force attacks on various services (FTP, SSH, SMB, RDP, Telnet, SQL).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;File Stealing&lt;/strong&gt;: Extracts data from vulnerable services.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;User Interface&lt;/strong&gt;: Real-time display on the e-Paper HAT and web interface for monitoring and interaction.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://github.com/infinition/Bjorn/assets/37984399/bcad830d-77d6-4f3e-833d-473eadd33921" alt="Bjorn Display"&gt;&lt;/p&gt; 
&lt;h2&gt;ğŸš€ Getting Started&lt;/h2&gt; 
&lt;h2&gt;ğŸ“Œ Prerequisites&lt;/h2&gt; 
&lt;h3&gt;ğŸ“‹ Prerequisites for RPI zero W (32bits)&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/3980ec5f-a8fc-4848-ab25-4356e0529639" alt="image"&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Raspberry Pi OS installed. 
  &lt;ul&gt; 
   &lt;li&gt;Stable: 
    &lt;ul&gt; 
     &lt;li&gt;System: 32-bit&lt;/li&gt; 
     &lt;li&gt;Kernel version: 6.6&lt;/li&gt; 
     &lt;li&gt;Debian version: 12 (bookworm) '2024-10-22-raspios-bookworm-armhf-lite'&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Username and hostname set to &lt;code&gt;bjorn&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;2.13-inch e-Paper HAT connected to GPIO pins.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ“‹ Prerequisites for RPI zero W2 (64bits)&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/e8d276be-4cb2-474d-a74d-b5b6704d22f5" alt="image"&gt;&lt;/p&gt; 
&lt;p&gt;I did not develop Bjorn for the raspberry pi zero w2 64bits, but several feedbacks have attested that the installation worked perfectly.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Raspberry Pi OS installed. 
  &lt;ul&gt; 
   &lt;li&gt;Stable: 
    &lt;ul&gt; 
     &lt;li&gt;System: 64-bit&lt;/li&gt; 
     &lt;li&gt;Kernel version: 6.6&lt;/li&gt; 
     &lt;li&gt;Debian version: 12 (bookworm) '2024-10-22-raspios-bookworm-arm64-lite'&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Username and hostname set to &lt;code&gt;bjorn&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;2.13-inch e-Paper HAT connected to GPIO pins.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;At the moment the paper screen v2 v4 have been tested and implemented. I juste hope the V1 &amp;amp; V3 will work the same.&lt;/p&gt; 
&lt;h3&gt;ğŸ”¨ Installation&lt;/h3&gt; 
&lt;p&gt;The fastest way to install Bjorn is using the automatic installation script :&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Download and run the installer
wget https://raw.githubusercontent.com/infinition/Bjorn/refs/heads/main/install_bjorn.sh
sudo chmod +x install_bjorn.sh &amp;amp;&amp;amp; sudo ./install_bjorn.sh
# Choose the choice 1 for automatic installation. It may take a while as a lot of packages and modules will be installed. You must reboot at the end.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For &lt;strong&gt;detailed information&lt;/strong&gt; about &lt;strong&gt;installation&lt;/strong&gt; process go to &lt;a href="https://raw.githubusercontent.com/infinition/Bjorn/main/INSTALL.md"&gt;Install Guide&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;âš¡ Quick Start&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Need help ? You struggle to find Bjorn's IP after the installation ?&lt;/strong&gt; Use my Bjorn Detector &amp;amp; SSH Launcher :&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/infinition/bjorn-detector"&gt;https://github.com/infinition/bjorn-detector&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/182f82f0-5c3a-48a9-a75e-37b9cfa2263a" alt="ezgif-1-a310f5fe8f"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Hmm, You still need help ?&lt;/strong&gt; For &lt;strong&gt;detailed information&lt;/strong&gt; about &lt;strong&gt;troubleshooting&lt;/strong&gt; go to &lt;a href="https://raw.githubusercontent.com/infinition/Bjorn/main/TROUBLESHOOTING.md"&gt;Troubleshooting&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Quick Installation&lt;/strong&gt;: you can use the fastest way to install &lt;strong&gt;Bjorn&lt;/strong&gt; &lt;a href="https://raw.githubusercontent.com/infinition/Bjorn/main/#-getting-started"&gt;Getting Started&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;ğŸ’¡ Usage Example&lt;/h2&gt; 
&lt;p&gt;Here's a demonstration of how Bjorn autonomously hunts through your network like a Viking raider (fake demo for illustration):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Reconnaissance Phase
[NetworkScanner] Discovering alive hosts...
[+] Host found: 192.168.1.100
    â”œâ”€â”€ Ports: 22,80,445,3306
    â””â”€â”€ MAC: 00:11:22:33:44:55

# Attack Sequence 
[NmapVulnScanner] Found vulnerabilities on 192.168.1.100
    â”œâ”€â”€ MySQL 5.5 &amp;lt; 5.7 - User Enumeration
    â””â”€â”€ SMB - EternalBlue Candidate

[SSHBruteforce] Cracking credentials...
[+] Success! user:password123
[StealFilesSSH] Extracting sensitive data...

# Automated Data Exfiltration
[SQLBruteforce] Database accessed!
[StealDataSQL] Dumping tables...
[SMBBruteforce] Share accessible
[+] Found config files, credentials, backups...
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This is just a demo output - actual results will vary based on your network and target configuration.&lt;/p&gt; 
&lt;p&gt;All discovered data is automatically organized in the data/output/ directory, viewable through both the e-Paper display (as indicators) and web interface. Bjorn works tirelessly, expanding its network knowledge base and growing stronger with each discovery.&lt;/p&gt; 
&lt;p&gt;No constant monitoring needed - just deploy and let Bjorn do what it does best: hunt for vulnerabilities.&lt;/p&gt; 
&lt;p&gt;ğŸ”§ Expand Bjorn's Arsenal! Bjorn is designed to be a community-driven weapon forge. Create and share your own attack modules!&lt;/p&gt; 
&lt;p&gt;âš ï¸ &lt;strong&gt;For educational and authorized testing purposes only&lt;/strong&gt; âš ï¸&lt;/p&gt; 
&lt;h2&gt;ğŸ¤ Contributing&lt;/h2&gt; 
&lt;p&gt;The project welcomes contributions in:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;New attack modules.&lt;/li&gt; 
 &lt;li&gt;Bug fixes.&lt;/li&gt; 
 &lt;li&gt;Documentation.&lt;/li&gt; 
 &lt;li&gt;Feature improvements.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For &lt;strong&gt;detailed information&lt;/strong&gt; about &lt;strong&gt;contributing&lt;/strong&gt; process go to &lt;a href="https://raw.githubusercontent.com/infinition/Bjorn/main/CONTRIBUTING.md"&gt;Contributing Docs&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/infinition/Bjorn/main/CODE_OF_CONDUCT.md"&gt;Code Of Conduct&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/infinition/Bjorn/main/DEVELOPMENT.md"&gt;Development Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;ğŸ“« Contact&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Report Issues&lt;/strong&gt;: Via GitHub.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Guidelines&lt;/strong&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Follow ethical guidelines.&lt;/li&gt; 
   &lt;li&gt;Document reproduction steps.&lt;/li&gt; 
   &lt;li&gt;Provide logs and context.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: &lt;strong&gt;infinition&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;GitHub&lt;/strong&gt;: &lt;a href="https://github.com/infinition/Bjorn"&gt;infinition/Bjorn&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸŒ  Stargazers&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#infinition/bjorn&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=infinition/bjorn&amp;amp;type=Date" alt="Star History Chart"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;hr&gt; 
&lt;h2&gt;ğŸ“œ License&lt;/h2&gt; 
&lt;p&gt;2024 - Bjorn is distributed under the MIT License. For more details, please refer to the &lt;a href="https://raw.githubusercontent.com/infinition/Bjorn/main/LICENSE"&gt;LICENSE&lt;/a&gt; file included in this repository.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>yeongpin/cursor-free-vip</title>
      <link>https://github.com/yeongpin/cursor-free-vip</link>
      <description>&lt;p&gt;[Support 0.49.x]ï¼ˆReset Cursor AI MachineID &amp; Bypass Higher Token Limitï¼‰ Cursor Ai ï¼Œè‡ªåŠ¨é‡ç½®æœºå™¨ID ï¼Œ å…è´¹å‡çº§ä½¿ç”¨ProåŠŸèƒ½: You've reached your trial request limit. / Too many free trial accounts used on this machine. Please upgrade to pro. We have this limit in place to prevent abuse. Please let us know if you believe this is a mistake.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;â¤ Cursor Free VIP&lt;/h1&gt; 
&lt;div align="center"&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/yeongpin/cursor-free-vip/main/images/logo.png" alt="Cursor Pro Logo" width="200" style="border-radius: 6px;"&gt; &lt;/p&gt; 
 &lt;p align="center"&gt; &lt;/p&gt;
 &lt;p&gt;&lt;a href="https://github.com/yeongpin/cursor-free-vip/releases/latest"&gt;&lt;img src="https://img.shields.io/endpoint?url=https://api.pinstudios.net/api/badges/release/yeongpin/cursor-free-vip" alt="Release"&gt;&lt;/a&gt; &lt;a href="https://creativecommons.org/licenses/by-nc-nd/4.0/"&gt;&lt;img src="https://img.shields.io/badge/License-CC_BY--NC--ND_4.0-lightgrey.svg?sanitize=true" alt="License: CC BY-NC-ND 4.0"&gt;&lt;/a&gt; &lt;a href="https://github.com/yeongpin/cursor-free-vip/stargazers"&gt;&lt;img src="https://img.shields.io/endpoint?url=https://api.pinstudios.net/api/badges/stars/yeongpin/cursor-free-vip" alt="Stars"&gt;&lt;/a&gt; &lt;a href="https://github.com/yeongpin/cursor-free-vip/releases/latest"&gt;&lt;img src="https://img.shields.io/endpoint?url=https://api.pinstudios.net/api/badges/downloads/yeongpin/cursor-free-vip/total" alt="Downloads"&gt;&lt;/a&gt; &lt;a href="https://buymeacoffee.com/yeongpin" target="_blank"&gt;&lt;img alt="Buy Me a Coffee" src="https://img.shields.io/badge/Buy%20Me%20a%20Coffee-Support%20Me-FFDA33"&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/yeongpin/cursor-free-vip"&gt;&lt;img src="https://devin.ai/assets/deepwiki-badge.png" alt="Ask DeepWiki.com" height="20"&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/13425" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/13425" alt="yeongpin%2Fcursor-free-vip | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"&gt;&lt;/a&gt; &lt;br&gt; &lt;a href="https://www.buymeacoffee.com/yeongpin" target="_blank"&gt; &lt;img src="https://img.buymeacoffee.com/button-api/?text=buy%20me%20a%20coffee&amp;amp;emoji=%E2%98%95&amp;amp;slug=yeongpin&amp;amp;button_colour=ffda33&amp;amp;font_colour=000000&amp;amp;font_family=Bree&amp;amp;outline_colour=000000&amp;amp;coffee_colour=FFDD00&amp;amp;latest=2" width="160" height="55" alt="Buy Me a Coffee"&gt; &lt;/a&gt;&lt;/p&gt; 
 &lt;h4&gt;Support Latest 0.49.x Version | æ”¯æŒæœ€æ–° 0.49.x ç‰ˆæœ¬&lt;/h4&gt; 
 &lt;p&gt;This tool is for educational purposes, currently the repo does not violate any laws. Please support the original project. This tool will not generate any fake email accounts and OAuth access.&lt;/p&gt; 
 &lt;p&gt;Supports Windows, macOS and Linux.&lt;/p&gt; 
 &lt;p&gt;For optimal performance, run with privileges and always stay up to date.&lt;/p&gt; 
 &lt;p&gt;é€™æ˜¯ä¸€æ¬¾ç”¨æ–¼å­¸ç¿’å’Œç ”ç©¶çš„å·¥å…·ï¼Œç›®å‰ repo æ²’æœ‰é•åä»»ä½•æ³•å¾‹ã€‚è«‹æ”¯æŒåŸä½œè€…ã€‚ é€™æ¬¾å·¥å…·ä¸æœƒç”Ÿæˆä»»ä½•å‡çš„é›»å­éƒµä»¶å¸³æˆ¶å’Œ OAuth è¨ªå•ã€‚&lt;/p&gt; 
 &lt;p&gt;æ”¯æŒ Windowsã€macOS å’Œ Linuxã€‚&lt;/p&gt; 
 &lt;p&gt;å°æ–¼æœ€ä½³æ€§èƒ½ï¼Œè«‹ä»¥ç®¡ç†å“¡èº«ä»½é‹è¡Œä¸¦å§‹çµ‚ä¿æŒæœ€æ–°ã€‚&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/yeongpin/cursor-free-vip/main/images/product_2025-04-16_10-40-21.png" alt="new" width="800" style="border-radius: 6px;"&gt;&lt;br&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸ”„ Change Log | æ›´æ–°æ—¥å¿—&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/yeongpin/cursor-free-vip/main/CHANGELOG.md"&gt;Watch Change Log | æŸ¥çœ‹æ›´æ–°æ—¥å¿—&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;âœ¨ Features | åŠŸèƒ½ç‰¹é»&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Support Windows macOS and Linux systems&lt;br&gt;æ”¯æŒ Windowsã€macOS å’Œ Linux ç³»çµ±&lt;br&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Reset Cursor's configuration&lt;br&gt;é‡ç½® Cursor çš„é…ç½®&lt;br&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Multi-language support (English, ç®€ä½“ä¸­æ–‡, ç¹é«”ä¸­æ–‡, Vietnamese)&lt;br&gt;å¤šèªè¨€æ”¯æŒï¼ˆè‹±æ–‡ã€ç®€ä½“ä¸­æ–‡ã€ç¹é«”ä¸­æ–‡ã€è¶Šå—èªï¼‰&lt;br&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ’» System Support | ç³»çµ±æ”¯æŒ&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Operating System&lt;/th&gt; 
   &lt;th&gt;Architecture&lt;/th&gt; 
   &lt;th&gt;Supported&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Windows&lt;/td&gt; 
   &lt;td&gt;x64, x86&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;macOS&lt;/td&gt; 
   &lt;td&gt;Intel, Apple Silicon&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Linux&lt;/td&gt; 
   &lt;td&gt;x64, x86, ARM64&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;ğŸ‘€ How to use | å¦‚ä½•ä½¿ç”¨&lt;/h2&gt; 
&lt;details open&gt; 
 &lt;summary&gt;&lt;b&gt;â­ Auto Run Script | è…³æœ¬è‡ªå‹•åŒ–é‹è¡Œ&lt;/b&gt;&lt;/summary&gt; 
 &lt;h3&gt;&lt;strong&gt;Linux/macOS&lt;/strong&gt;&lt;/h3&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;curl -fsSL https://raw.githubusercontent.com/yeongpin/cursor-free-vip/main/scripts/install.sh -o install.sh &amp;amp;&amp;amp; chmod +x install.sh &amp;amp;&amp;amp; ./install.sh
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;&lt;strong&gt;Archlinux&lt;/strong&gt;&lt;/h3&gt; 
 &lt;p&gt;Install via &lt;a href="https://aur.archlinux.org/packages/cursor-free-vip-git"&gt;AUR&lt;/a&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;yay -S cursor-free-vip-git
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;&lt;strong&gt;Windows&lt;/strong&gt;&lt;/h3&gt; 
 &lt;pre&gt;&lt;code class="language-powershell"&gt;irm https://raw.githubusercontent.com/yeongpin/cursor-free-vip/main/scripts/install.ps1 | iex
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p&gt;If you want to stop the script, please press Ctrl+C&lt;br&gt;è¦åœæ­¢è…³æœ¬ï¼Œè«‹æŒ‰ Ctrl+C&lt;/p&gt; 
&lt;h2&gt;â— Note | æ³¨æ„äº‹é …&lt;/h2&gt; 
&lt;p&gt;ğŸ“ Config | æ–‡ä»¶é…ç½® &lt;code&gt;Win / Macos / Linux Path | è·¯å¾‘ [Documents/.cursor-free-vip/config.ini]&lt;/code&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;â­ Config | æ–‡ä»¶é…ç½®&lt;/b&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code&gt;[Chrome]
# Default Google Chrome Path | é»˜èªGoogle Chrome éŠè¦½å™¨è·¯å¾‘
chromepath = C:\Program Files\Google/Chrome/Application/chrome.exe

[Turnstile]
# Handle Turnstile Wait Time | ç­‰å¾…äººæ©Ÿé©—è­‰æ™‚é–“
handle_turnstile_time = 2
# Handle Turnstile Wait Random Time (must merge 1-3 or 1,3) | ç­‰å¾…äººæ©Ÿé©—è­‰éš¨æ©Ÿæ™‚é–“ï¼ˆå¿…é ˆæ˜¯ 1-3 æˆ–è€… 1,3 é€™æ¨£çš„çµ„åˆï¼‰
handle_turnstile_random_time = 1-3

[OSPaths]
# Storage Path | å­˜å„²è·¯å¾‘
storage_path = /Users/username/Library/Application Support/Cursor/User/globalStorage/storage.json
# SQLite Path | SQLiteè·¯å¾‘
sqlite_path = /Users/username/Library/Application Support/Cursor/User/globalStorage/state.vscdb
# Machine ID Path | æ©Ÿå™¨IDè·¯å¾‘
machine_id_path = /Users/username/Library/Application Support/Cursor/machineId
# For Linux users: ~/.config/cursor/machineid

[Timing]
# Min Random Time | æœ€å°éš¨æ©Ÿæ™‚é–“
min_random_time = 0.1
# Max Random Time | æœ€å¤§éš¨æ©Ÿæ™‚é–“
max_random_time = 0.8
# Page Load Wait | é é¢åŠ è¼‰ç­‰å¾…æ™‚é–“
page_load_wait = 0.1-0.8
# Input Wait | è¼¸å…¥ç­‰å¾…æ™‚é–“
input_wait = 0.3-0.8
# Submit Wait | æäº¤ç­‰å¾…æ™‚é–“
submit_wait = 0.5-1.5
# Verification Code Input | é©—è­‰ç¢¼è¼¸å…¥ç­‰å¾…æ™‚é–“
verification_code_input = 0.1-0.3
# Verification Success Wait | é©—è­‰æˆåŠŸç­‰å¾…æ™‚é–“
verification_success_wait = 2-3
# Verification Retry Wait | é©—è­‰é‡è©¦ç­‰å¾…æ™‚é–“
verification_retry_wait = 2-3
# Email Check Initial Wait | éƒµä»¶æª¢æŸ¥åˆå§‹ç­‰å¾…æ™‚é–“
email_check_initial_wait = 4-6
# Email Refresh Wait | éƒµä»¶åˆ·æ–°ç­‰å¾…æ™‚é–“
email_refresh_wait = 2-4
# Settings Page Load Wait | è¨­ç½®é é¢åŠ è¼‰ç­‰å¾…æ™‚é–“
settings_page_load_wait = 1-2
# Failed Retry Time | å¤±æ•—é‡è©¦æ™‚é–“
failed_retry_time = 0.5-1
# Retry Interval | é‡è©¦é–“éš”
retry_interval = 8-12
# Max Timeout | æœ€å¤§è¶…æ™‚æ™‚é–“
max_timeout = 160

[Utils]
# Check Update | æª¢æŸ¥æ›´æ–°
check_update = True
# Show Account Info | é¡¯ç¤ºè³¬è™Ÿä¿¡æ¯
show_account_info = True

[TempMailPlus]
# Enable TempMailPlus | å•“ç”¨ TempMailPlusï¼ˆä»»ä½•è½‰ç™¼åˆ°TempMailPlusçš„éƒµä»¶éƒ½æ”¯æŒç²å–é©—è­‰ç¢¼ï¼Œä¾‹å¦‚cloudflareéƒµä»¶Catch-allï¼‰
enabled = false
# TempMailPlus Email | TempMailPlus é›»å­éƒµä»¶
email = xxxxx@mailto.plus
# TempMailPlus pin | TempMailPlus pinç¢¼
epin = 

[WindowsPaths]
storage_path = C:\Users\yeongpin\AppData\Roaming\Cursor\User\globalStorage\storage.json
sqlite_path = C:\Users\yeongpin\AppData\Roaming\Cursor\User\globalStorage\state.vscdb
machine_id_path = C:\Users\yeongpin\AppData\Roaming\Cursor\machineId
cursor_path = C:\Users\yeongpin\AppData\Local\Programs\Cursor\resources\app
updater_path = C:\Users\yeongpin\AppData\Local\cursor-updater
update_yml_path = C:\Users\yeongpin\AppData\Local\Programs\Cursor\resources\app-update.yml
product_json_path = C:\Users\yeongpin\AppData\Local\Programs\Cursor\resources\app\product.json

[Browser]
default_browser = opera
chrome_path = C:\Program Files\Google\Chrome\Application\chrome.exe
edge_path = C:\Program Files (x86)\Microsoft\Edge\Application\msedge.exe
firefox_path = C:\Program Files\Mozilla Firefox\firefox.exe
brave_path = C:\Program Files\BraveSoftware/Brave-Browser/Application/brave.exe
chrome_driver_path = D:\VisualCode\cursor-free-vip-new\drivers\chromedriver.exe
edge_driver_path = D:\VisualCode\cursor-free-vip-new\drivers\msedgedriver.exe
firefox_driver_path = D:\VisualCode\cursor-free-vip-new\drivers\geckodriver.exe
brave_driver_path = D:\VisualCode\cursor-free-vip-new\drivers\chromedriver.exe
opera_path = C:\Users\yeongpin\AppData\Local\Programs\Opera\opera.exe
opera_driver_path = D:\VisualCode\cursor-free-vip-new\drivers\chromedriver.exe

[OAuth]
show_selection_alert = False
timeout = 120
max_attempts = 3
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Use administrator privileges to run the script &lt;br&gt;è«‹ä½¿ç”¨ç®¡ç†å“¡èº«ä»½é‹è¡Œè…³æœ¬&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Confirm that Cursor is closed before running the script &lt;br&gt;è«‹ç¢ºä¿åœ¨é‹è¡Œè…³æœ¬å‰å·²ç¶“é—œé–‰ Cursor&lt;br&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;This tool is only for learning and research purposes &lt;br&gt;æ­¤å·¥å…·åƒ…ä¾›å­¸ç¿’å’Œç ”ç©¶ä½¿ç”¨&lt;br&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Please comply with the relevant software usage terms when using this tool &lt;br&gt;ä½¿ç”¨æœ¬å·¥å…·æ™‚è«‹éµå®ˆç›¸é—œè»Ÿä»¶ä½¿ç”¨æ¢æ¬¾&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸš¨ Common Issues | å¸¸è¦‹å•é¡Œ&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;å¦‚æœé‡åˆ°æ¬Šé™å•é¡Œï¼Œè«‹ç¢ºä¿ï¼š&lt;/th&gt; 
   &lt;th align="center"&gt;æ­¤è…³æœ¬ä»¥ç®¡ç†å“¡èº«ä»½é‹è¡Œ&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;If you encounter permission issues, please ensure:&lt;/td&gt; 
   &lt;td align="center"&gt;This script is run with administrator privileges&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Error 'User is not authorized'&lt;/td&gt; 
   &lt;td align="center"&gt;This means your account was banned for using temporary (disposal) mail. Ensure using a non-temporary mail service&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;ğŸ¤© Contribution | è²¢ç»&lt;/h2&gt; 
&lt;p&gt;æ­¡è¿æäº¤ Issue å’Œ Pull Requestï¼&lt;/p&gt; 
&lt;a href="https://github.com/yeongpin/cursor-free-vip/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=yeongpin/cursor-free-vip&amp;amp;preview=true&amp;amp;max=&amp;amp;columns="&gt; &lt;/a&gt; 
&lt;br&gt;
&lt;br&gt; 
&lt;h2&gt;ğŸ“© Disclaimer | å…è²¬è²æ˜&lt;/h2&gt; 
&lt;p&gt;æœ¬å·¥å…·åƒ…ä¾›å­¸ç¿’å’Œç ”ç©¶ä½¿ç”¨ï¼Œä½¿ç”¨æœ¬å·¥å…·æ‰€ç”¢ç”Ÿçš„ä»»ä½•å¾Œæœç”±ä½¿ç”¨è€…è‡ªè¡Œæ‰¿æ“”ã€‚ &lt;br&gt;&lt;/p&gt; 
&lt;p&gt;This tool is only for learning and research purposes, and any consequences arising from the use of this tool are borne by the user.&lt;/p&gt; 
&lt;h2&gt;ğŸ’° Buy Me a Coffee | è«‹æˆ‘å–æ¯å’–å•¡&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td&gt; &lt;img src="https://raw.githubusercontent.com/yeongpin/cursor-free-vip/main/images/provi-code.jpg" alt="buy_me_a_coffee" width="280"&gt;&lt;br&gt; &lt;/td&gt; 
    &lt;td&gt; &lt;img src="https://raw.githubusercontent.com/yeongpin/cursor-free-vip/main/images/paypal.png" alt="buy_me_a_coffee" width="280"&gt;&lt;br&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h2&gt;â­ Star History | æ˜Ÿæ˜Ÿæ•¸&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://star-history.com/#yeongpin/cursor-free-vip&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=yeongpin/cursor-free-vip&amp;amp;type=Date" alt="Star History Chart"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸ“ License | æˆæ¬Š&lt;/h2&gt; 
&lt;p&gt;æœ¬é …ç›®æ¡ç”¨ &lt;a href="https://creativecommons.org/licenses/by-nc-nd/4.0/"&gt;CC BY-NC-ND 4.0&lt;/a&gt; æˆæ¬Šã€‚ Please refer to the &lt;a href="https://raw.githubusercontent.com/yeongpin/cursor-free-vip/main/LICENSE.md"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>pipecat-ai/pipecat</title>
      <link>https://github.com/pipecat-ai/pipecat</link>
      <description>&lt;p&gt;Open Source framework for voice and multimodal conversational AI&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;
 &lt;div align="center"&gt; 
  &lt;img alt="pipecat" width="300px" height="auto" src="https://raw.githubusercontent.com/pipecat-ai/pipecat/main/pipecat.png"&gt; 
 &lt;/div&gt;&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://pypi.org/project/pipecat-ai"&gt;&lt;img src="https://img.shields.io/pypi/v/pipecat-ai" alt="PyPI"&gt;&lt;/a&gt; &lt;img src="https://github.com/pipecat-ai/pipecat/actions/workflows/tests.yaml/badge.svg?sanitize=true" alt="Tests"&gt; &lt;a href="https://codecov.io/gh/pipecat-ai/pipecat"&gt;&lt;img src="https://codecov.io/gh/pipecat-ai/pipecat/graph/badge.svg?token=LNVUIVO4Y9" alt="codecov"&gt;&lt;/a&gt; &lt;a href="https://docs.pipecat.ai"&gt;&lt;img src="https://img.shields.io/badge/Documentation-blue" alt="Docs"&gt;&lt;/a&gt; &lt;a href="https://discord.gg/pipecat"&gt;&lt;img src="https://img.shields.io/discord/1239284677165056021" alt="Discord"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;ğŸ™ï¸ Pipecat: Real-Time Voice &amp;amp; Multimodal AI Agents&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;Pipecat&lt;/strong&gt; is an open-source Python framework for building real-time voice and multimodal conversational agents. Orchestrate audio and video, AI services, different transports, and conversation pipelines effortlesslyâ€”so you can focus on what makes your agent unique.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Want to dive right in? &lt;a href="https://docs.pipecat.ai/getting-started/installation"&gt;Install Pipecat&lt;/a&gt; then try the &lt;a href="https://docs.pipecat.ai/getting-started/quickstart"&gt;quickstart&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;ğŸš€ What You Can Build&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Voice Assistants&lt;/strong&gt; â€“ natural, streaming conversations with AI&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;AI Companions&lt;/strong&gt; â€“ coaches, meeting assistants, characters&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multimodal Interfaces&lt;/strong&gt; â€“ voice, video, images, and more&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Interactive Storytelling&lt;/strong&gt; â€“ creative tools with generative media&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Business Agents&lt;/strong&gt; â€“ customer intake, support bots, guided flows&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Complex Dialog Systems&lt;/strong&gt; â€“ design logic with structured conversations&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;ğŸ§­ Looking to build structured conversations? Check out &lt;a href="https://github.com/pipecat-ai/pipecat-flows"&gt;Pipecat Flows&lt;/a&gt; for managing complex conversational states and transitions.&lt;/p&gt; 
&lt;h2&gt;ğŸ§  Why Pipecat?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Voice-first&lt;/strong&gt;: Integrates speech recognition, text-to-speech, and conversation handling&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Pluggable&lt;/strong&gt;: Supports many AI services and tools&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Composable Pipelines&lt;/strong&gt;: Build complex behavior from modular components&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Real-Time&lt;/strong&gt;: Ultra-low latency interaction with different transports (e.g. WebSockets or WebRTC)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ¬ See it in action&lt;/h2&gt; 
&lt;p float="left"&gt; &lt;a href="https://github.com/pipecat-ai/pipecat/tree/main/examples/simple-chatbot"&gt;&lt;img src="https://raw.githubusercontent.com/pipecat-ai/pipecat/main/examples/simple-chatbot/image.png" width="400"&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/pipecat-ai/pipecat/tree/main/examples/storytelling-chatbot"&gt;&lt;img src="https://raw.githubusercontent.com/pipecat-ai/pipecat/main/examples/storytelling-chatbot/image.png" width="400"&gt;&lt;/a&gt; &lt;br&gt; &lt;a href="https://github.com/pipecat-ai/pipecat/tree/main/examples/translation-chatbot"&gt;&lt;img src="https://raw.githubusercontent.com/pipecat-ai/pipecat/main/examples/translation-chatbot/image.png" width="400"&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/pipecat-ai/pipecat/tree/main/examples/moondream-chatbot"&gt;&lt;img src="https://raw.githubusercontent.com/pipecat-ai/pipecat/main/examples/moondream-chatbot/image.png" width="400"&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;ğŸ“± Client SDKs&lt;/h2&gt; 
&lt;p&gt;You can connect to Pipecat from any platform using our official SDKs:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Platform&lt;/th&gt; 
   &lt;th&gt;SDK Repo&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Web&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/pipecat-ai/pipecat-client-web"&gt;pipecat-client-web&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;JavaScript and React client SDKs&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;iOS&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/pipecat-ai/pipecat-client-ios"&gt;pipecat-client-ios&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Swift SDK for iOS&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Android&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/pipecat-ai/pipecat-client-android"&gt;pipecat-client-android&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Kotlin SDK for Android&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;C++&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/pipecat-ai/pipecat-client-cxx"&gt;pipecat-client-cxx&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;C++ client SDK&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;ğŸ§© Available services&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Category&lt;/th&gt; 
   &lt;th&gt;Services&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Speech-to-Text&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.pipecat.ai/server/services/stt/assemblyai"&gt;AssemblyAI&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/stt/aws"&gt;AWS&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/stt/azure"&gt;Azure&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/stt/cartesia"&gt;Cartesia&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/stt/deepgram"&gt;Deepgram&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/stt/fal"&gt;Fal Wizper&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/stt/gladia"&gt;Gladia&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/stt/google"&gt;Google&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/stt/groq"&gt;Groq (Whisper)&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/stt/openai"&gt;OpenAI (Whisper)&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/stt/parakeet"&gt;Parakeet (NVIDIA)&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/stt/sambanova"&gt;SambaNova (Whisper)&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/stt/soniox"&gt;Soniox&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/stt/speechmatics"&gt;Speechmatics&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/stt/ultravox"&gt;Ultravox&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/stt/whisper"&gt;Whisper&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;LLMs&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.pipecat.ai/server/services/llm/anthropic"&gt;Anthropic&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/llm/aws"&gt;AWS&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/llm/azure"&gt;Azure&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/llm/cerebras"&gt;Cerebras&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/llm/deepseek"&gt;DeepSeek&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/llm/fireworks"&gt;Fireworks AI&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/llm/gemini"&gt;Gemini&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/llm/grok"&gt;Grok&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/llm/groq"&gt;Groq&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/llm/nim"&gt;NVIDIA NIM&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/llm/ollama"&gt;Ollama&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/llm/openai"&gt;OpenAI&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/llm/openrouter"&gt;OpenRouter&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/llm/perplexity"&gt;Perplexity&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/llm/qwen"&gt;Qwen&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/llm/sambanova"&gt;SambaNova&lt;/a&gt; &lt;a href="https://docs.pipecat.ai/server/services/llm/together"&gt;Together AI&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Text-to-Speech&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.pipecat.ai/server/services/tts/aws"&gt;AWS&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/tts/azure"&gt;Azure&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/tts/cartesia"&gt;Cartesia&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/tts/deepgram"&gt;Deepgram&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/tts/elevenlabs"&gt;ElevenLabs&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/tts/fastpitch"&gt;FastPitch (NVIDIA)&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/tts/fish"&gt;Fish&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/tts/google"&gt;Google&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/tts/lmnt"&gt;LMNT&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/tts/minimax"&gt;MiniMax&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/tts/neuphonic"&gt;Neuphonic&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/tts/openai"&gt;OpenAI&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/tts/piper"&gt;Piper&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/tts/playht"&gt;PlayHT&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/tts/rime"&gt;Rime&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/tts/sarvam"&gt;Sarvam&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/tts/xtts"&gt;XTTS&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Speech-to-Speech&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.pipecat.ai/server/services/s2s/aws"&gt;AWS Nova Sonic&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/s2s/gemini"&gt;Gemini Multimodal Live&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/s2s/openai"&gt;OpenAI Realtime&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Transport&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.pipecat.ai/server/services/transport/daily"&gt;Daily (WebRTC)&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/transport/fastapi-websocket"&gt;FastAPI Websocket&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/transport/small-webrtc"&gt;SmallWebRTCTransport&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/transport/websocket-server"&gt;WebSocket Server&lt;/a&gt;, Local&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Serializers&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.pipecat.ai/server/utilities/serializers/plivo"&gt;Plivo&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/utilities/serializers/twilio"&gt;Twilio&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/utilities/serializers/telnyx"&gt;Telnyx&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Video&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.pipecat.ai/server/services/video/tavus"&gt;Tavus&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/video/simli"&gt;Simli&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Memory&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.pipecat.ai/server/services/memory/mem0"&gt;mem0&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Vision &amp;amp; Image&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.pipecat.ai/server/services/image-generation/fal"&gt;fal&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/image-generation/fal"&gt;Google Imagen&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/vision/moondream"&gt;Moondream&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Audio Processing&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.pipecat.ai/server/utilities/audio/silero-vad-analyzer"&gt;Silero VAD&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/utilities/audio/krisp-filter"&gt;Krisp&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/utilities/audio/koala-filter"&gt;Koala&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/utilities/audio/noisereduce-filter"&gt;Noisereduce&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Analytics &amp;amp; Metrics&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://docs.pipecat.ai/server/utilities/opentelemetry"&gt;OpenTelemetry&lt;/a&gt;, &lt;a href="https://docs.pipecat.ai/server/services/analytics/sentry"&gt;Sentry&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;ğŸ“š &lt;a href="https://docs.pipecat.ai/server/services/supported-services"&gt;View full services documentation â†’&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;âš¡ Getting started&lt;/h2&gt; 
&lt;p&gt;You can get started with Pipecat running on your local machine, then move your agent processes to the cloud when youâ€™re ready.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# Install the module
pip install pipecat-ai

# Set up your environment
cp dot-env.template .env
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To keep things lightweight, only the core framework is included by default. If you need support for third-party AI services, you can add the necessary dependencies with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip install "pipecat-ai[option,...]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ğŸ§ª Code examples&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pipecat-ai/pipecat/tree/main/examples/foundational"&gt;Foundational&lt;/a&gt; â€” small snippets that build on each other, introducing one or two concepts at a time&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pipecat-ai/pipecat/tree/main/examples/"&gt;Example apps&lt;/a&gt; â€” complete applications that you can use as starting points for development&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ› ï¸ Hacking on the framework itself&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Set up a virtual environment before following these instructions. From the root of the repo:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;python3 -m venv venv
source venv/bin/activate
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install the development dependencies:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;pip install -r dev-requirements.txt
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install the git pre-commit hooks (these help ensure your code follows project rules):&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;pre-commit install
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install the &lt;code&gt;pipecat-ai&lt;/code&gt; package locally in editable mode:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;The &lt;code&gt;-e&lt;/code&gt; or &lt;code&gt;--editable&lt;/code&gt; option allows you to modify the code without reinstalling.&lt;/p&gt; 
  &lt;/blockquote&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Include optional dependencies as needed. For example:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;pip install -e ".[daily,deepgram,cartesia,openai,silero]"
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;(Optional) If you want to use this package from another directory:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;pip install "path_to_this_repo[option,...]"
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Running tests&lt;/h3&gt; 
&lt;p&gt;Install the test dependencies:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip install -r test-requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;From the root directory, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pytest
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Setting up your editor&lt;/h3&gt; 
&lt;p&gt;This project uses strict &lt;a href="https://peps.python.org/pep-0008/"&gt;PEP 8&lt;/a&gt; formatting via &lt;a href="https://github.com/astral-sh/ruff"&gt;Ruff&lt;/a&gt;.&lt;/p&gt; 
&lt;h4&gt;Emacs&lt;/h4&gt; 
&lt;p&gt;You can use &lt;a href="https://github.com/jwiegley/use-package"&gt;use-package&lt;/a&gt; to install &lt;a href="https://github.com/christophermadsen/emacs-lazy-ruff"&gt;emacs-lazy-ruff&lt;/a&gt; package and configure &lt;code&gt;ruff&lt;/code&gt; arguments:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-elisp"&gt;(use-package lazy-ruff
  :ensure t
  :hook ((python-mode . lazy-ruff-mode))
  :config
  (setq lazy-ruff-format-command "ruff format")
  (setq lazy-ruff-check-command "ruff check --select I"))
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;code&gt;ruff&lt;/code&gt; was installed in the &lt;code&gt;venv&lt;/code&gt; environment described before, so you should be able to use &lt;a href="https://github.com/ryotaro612/pyvenv-auto"&gt;pyvenv-auto&lt;/a&gt; to automatically load that environment inside Emacs.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-elisp"&gt;(use-package pyvenv-auto
  :ensure t
  :defer t
  :hook ((python-mode . pyvenv-auto-run)))
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Visual Studio Code&lt;/h4&gt; 
&lt;p&gt;Install the &lt;a href="https://marketplace.visualstudio.com/items?itemName=charliermarsh.ruff"&gt;Ruff&lt;/a&gt; extension. Then edit the user settings (&lt;em&gt;Ctrl-Shift-P&lt;/em&gt; &lt;code&gt;Open User Settings (JSON)&lt;/code&gt;) and set it as the default Python formatter, and enable formatting on save:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;"[python]": {
    "editor.defaultFormatter": "charliermarsh.ruff",
    "editor.formatOnSave": true
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;PyCharm&lt;/h4&gt; 
&lt;p&gt;&lt;code&gt;ruff&lt;/code&gt; was installed in the &lt;code&gt;venv&lt;/code&gt; environment described before, now to enable autoformatting on save, go to &lt;code&gt;File&lt;/code&gt; -&amp;gt; &lt;code&gt;Settings&lt;/code&gt; -&amp;gt; &lt;code&gt;Tools&lt;/code&gt; -&amp;gt; &lt;code&gt;File Watchers&lt;/code&gt; and add a new watcher with the following settings:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Name&lt;/strong&gt;: &lt;code&gt;Ruff formatter&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;File type&lt;/strong&gt;: &lt;code&gt;Python&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Working directory&lt;/strong&gt;: &lt;code&gt;$ContentRoot$&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Arguments&lt;/strong&gt;: &lt;code&gt;format $FilePath$&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Program&lt;/strong&gt;: &lt;code&gt;$PyInterpreterDirectory$/ruff&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;ğŸ¤ Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions from the community! Whether you're fixing bugs, improving documentation, or adding new features, here's how you can help:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Found a bug?&lt;/strong&gt; Open an &lt;a href="https://github.com/pipecat-ai/pipecat/issues"&gt;issue&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Have a feature idea?&lt;/strong&gt; Start a &lt;a href="https://discord.gg/pipecat"&gt;discussion&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Want to contribute code?&lt;/strong&gt; Check our &lt;a href="https://raw.githubusercontent.com/pipecat-ai/pipecat/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; guide&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Documentation improvements?&lt;/strong&gt; &lt;a href="https://github.com/pipecat-ai/docs"&gt;Docs&lt;/a&gt; PRs are always welcome&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Before submitting a pull request, please check existing issues and PRs to avoid duplicates.&lt;/p&gt; 
&lt;p&gt;We aim to review all contributions promptly and provide constructive feedback to help get your changes merged.&lt;/p&gt; 
&lt;h2&gt;ğŸ›Ÿ Getting help&lt;/h2&gt; 
&lt;p&gt;â¡ï¸ &lt;a href="https://discord.gg/pipecat"&gt;Join our Discord&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;â¡ï¸ &lt;a href="https://docs.pipecat.ai"&gt;Read the docs&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;â¡ï¸ &lt;a href="https://x.com/pipecat_ai"&gt;Reach us on X&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>confident-ai/deepeval</title>
      <link>https://github.com/confident-ai/deepeval</link>
      <description>&lt;p&gt;The LLM Evaluation Framework&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://github.com/confident-ai/deepeval/raw/main/docs/static/img/deepeval.png" alt="DeepEval Logo" width="100%"&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;/p&gt;
&lt;h1 align="center"&gt;The LLM Evaluation Framework&lt;/h1&gt; 
&lt;p&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/5917" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/5917" alt="confident-ai%2Fdeepeval | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://discord.gg/3SEyvpgu2f"&gt; &lt;img alt="discord-invite" src="https://dcbadge.vercel.app/api/server/3SEyvpgu2f?style=flat"&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h4 align="center"&gt; &lt;p&gt; &lt;a href="https://deepeval.com/docs/getting-started?utm_source=GitHub"&gt;Documentation&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/confident-ai/deepeval/main/#-metrics-and-features"&gt;Metrics and Features&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/confident-ai/deepeval/main/#-quickstart"&gt;Getting Started&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/confident-ai/deepeval/main/#-integrations"&gt;Integrations&lt;/a&gt; | &lt;a href="https://confident-ai.com?utm_source=GitHub"&gt;DeepEval Platform&lt;/a&gt; &lt;/p&gt;&lt;p&gt; &lt;/p&gt;&lt;/h4&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/confident-ai/deepeval/releases"&gt; &lt;img alt="GitHub release" src="https://img.shields.io/github/release/confident-ai/deepeval.svg?color=violet"&gt; &lt;/a&gt; &lt;a href="https://colab.research.google.com/drive/1PPxYEBa6eu__LquGoFFJZkhYgWVYE6kh?usp=sharing"&gt; &lt;img alt="Try Quickstart in Colab" src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true"&gt; &lt;/a&gt; &lt;a href="https://github.com/confident-ai/deepeval/raw/master/LICENSE.md"&gt; &lt;img alt="License" src="https://img.shields.io/github/license/confident-ai/deepeval.svg?color=yellow"&gt; &lt;/a&gt; &lt;a href="https://x.com/deepeval"&gt; &lt;img alt="Twitter Follow" src="https://img.shields.io/twitter/follow/deepeval?style=social&amp;amp;logo=x"&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; 
 &lt;!-- Keep these links. Translations will automatically update with the README. --&gt; &lt;a href="https://www.readme-i18n.com/confident-ai/deepeval?lang=de"&gt;Deutsch&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/confident-ai/deepeval?lang=es"&gt;EspaÃ±ol&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/confident-ai/deepeval?lang=fr"&gt;franÃ§ais&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/confident-ai/deepeval?lang=ja"&gt;æ—¥æœ¬èª&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/confident-ai/deepeval?lang=ko"&gt;í•œêµ­ì–´&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/confident-ai/deepeval?lang=pt"&gt;PortuguÃªs&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/confident-ai/deepeval?lang=ru"&gt;Ğ ÑƒÑÑĞºĞ¸Ğ¹&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/confident-ai/deepeval?lang=zh"&gt;ä¸­æ–‡&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;DeepEval&lt;/strong&gt; is a simple-to-use, open-source LLM evaluation framework, for evaluating and testing large-language model systems. It is similar to Pytest but specialized for unit testing LLM outputs. DeepEval incorporates the latest research to evaluate LLM outputs based on metrics such as G-Eval, hallucination, answer relevancy, RAGAS, etc., which uses LLMs and various other NLP models that runs &lt;strong&gt;locally on your machine&lt;/strong&gt; for evaluation.&lt;/p&gt; 
&lt;p&gt;Whether your LLM applications are RAG pipelines, chatbots, AI agents, implemented via LangChain or LlamaIndex, DeepEval has you covered. With it, you can easily determine the optimal models, prompts, and architecture to improve your RAG pipeline, agentic workflows, prevent prompt drifting, or even transition from OpenAI to hosting your own Deepseek R1 with confidence.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] Need a place for your DeepEval testing data to live ğŸ¡â¤ï¸? &lt;a href="https://confident-ai.com?utm_source=GitHub"&gt;Sign up to the DeepEval platform&lt;/a&gt; to compare iterations of your LLM app, generate &amp;amp; share testing reports, and more.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/confident-ai/deepeval/main/assets/demo.gif" alt="Demo GIF"&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Want to talk LLM evaluation, need help picking metrics, or just to say hi? &lt;a href="https://discord.com/invite/3SEyvpgu2f"&gt;Come join our discord.&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;br&gt; 
&lt;h1&gt;ğŸ”¥ Metrics and Features&lt;/h1&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ğŸ¥³ You can now share DeepEval's test results on the cloud directly on &lt;a href="https://confident-ai.com?utm_source=GitHub"&gt;Confident AI&lt;/a&gt;'s infrastructure&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;Supports both end-to-end and component-level LLM evaluation.&lt;/li&gt; 
 &lt;li&gt;Large variety of ready-to-use LLM evaluation metrics (all with explanations) powered by &lt;strong&gt;ANY&lt;/strong&gt; LLM of your choice, statistical methods, or NLP models that runs &lt;strong&gt;locally on your machine&lt;/strong&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;G-Eval&lt;/li&gt; 
   &lt;li&gt;DAG (&lt;a href="https://deepeval.com/docs/metrics-dag"&gt;deep acyclic graph&lt;/a&gt;)&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;RAG metrics:&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Answer Relevancy&lt;/li&gt; 
     &lt;li&gt;Faithfulness&lt;/li&gt; 
     &lt;li&gt;Contextual Recall&lt;/li&gt; 
     &lt;li&gt;Contextual Precision&lt;/li&gt; 
     &lt;li&gt;Contextual Relevancy&lt;/li&gt; 
     &lt;li&gt;RAGAS&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Agentic metrics:&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Task Completion&lt;/li&gt; 
     &lt;li&gt;Tool Correctness&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Others:&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Hallucination&lt;/li&gt; 
     &lt;li&gt;Summarization&lt;/li&gt; 
     &lt;li&gt;Bias&lt;/li&gt; 
     &lt;li&gt;Toxicity&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Conversational metrics:&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Knowledge Retention&lt;/li&gt; 
     &lt;li&gt;Conversation Completeness&lt;/li&gt; 
     &lt;li&gt;Conversation Relevancy&lt;/li&gt; 
     &lt;li&gt;Role Adherence&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;etc.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Build your own custom metrics that are automatically integrated with DeepEval's ecosystem.&lt;/li&gt; 
 &lt;li&gt;Generate synthetic datasets for evaluation.&lt;/li&gt; 
 &lt;li&gt;Integrates seamlessly with &lt;strong&gt;ANY&lt;/strong&gt; CI/CD environment.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://deepeval.com/docs/red-teaming-introduction"&gt;Red team your LLM application&lt;/a&gt; for 40+ safety vulnerabilities in a few lines of code, including: 
  &lt;ul&gt; 
   &lt;li&gt;Toxicity&lt;/li&gt; 
   &lt;li&gt;Bias&lt;/li&gt; 
   &lt;li&gt;SQL Injection&lt;/li&gt; 
   &lt;li&gt;etc., using advanced 10+ attack enhancement strategies such as prompt injections.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Easily benchmark &lt;strong&gt;ANY&lt;/strong&gt; LLM on popular LLM benchmarks in &lt;a href="https://deepeval.com/docs/benchmarks-introduction?utm_source=GitHub"&gt;under 10 lines of code.&lt;/a&gt;, which includes: 
  &lt;ul&gt; 
   &lt;li&gt;MMLU&lt;/li&gt; 
   &lt;li&gt;HellaSwag&lt;/li&gt; 
   &lt;li&gt;DROP&lt;/li&gt; 
   &lt;li&gt;BIG-Bench Hard&lt;/li&gt; 
   &lt;li&gt;TruthfulQA&lt;/li&gt; 
   &lt;li&gt;HumanEval&lt;/li&gt; 
   &lt;li&gt;GSM8K&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://confident-ai.com?utm_source=GitHub"&gt;100% integrated with Confident AI&lt;/a&gt; for the full evaluation lifecycle: 
  &lt;ul&gt; 
   &lt;li&gt;Curate/annotate evaluation datasets on the cloud&lt;/li&gt; 
   &lt;li&gt;Benchmark LLM app using dataset, and compare with previous iterations to experiment which models/prompts works best&lt;/li&gt; 
   &lt;li&gt;Fine-tune metrics for custom results&lt;/li&gt; 
   &lt;li&gt;Debug evaluation results via LLM traces&lt;/li&gt; 
   &lt;li&gt;Monitor &amp;amp; evaluate LLM responses in product to improve datasets with real-world data&lt;/li&gt; 
   &lt;li&gt;Repeat until perfection&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Confident AI is the DeepEval platform. Create an account &lt;a href="https://app.confident-ai.com?utm_source=GitHub"&gt;here.&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;br&gt; 
&lt;h1&gt;ğŸ”Œ Integrations&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ¦„ LlamaIndex, to &lt;a href="https://www.deepeval.com/integrations/frameworks/llamaindex?utm_source=GitHub"&gt;&lt;strong&gt;unit test RAG applications in CI/CD&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸ¤— Hugging Face, to &lt;a href="https://www.deepeval.com/integrations/frameworks/huggingface?utm_source=GitHub"&gt;&lt;strong&gt;enable real-time evaluations during LLM fine-tuning&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;br&gt; 
&lt;h1&gt;ğŸš€ QuickStart&lt;/h1&gt; 
&lt;p&gt;Let's pretend your LLM application is a RAG based customer support chatbot; here's how DeepEval can help test what you've built.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;pip install -U deepeval
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Create an account (highly recommended)&lt;/h2&gt; 
&lt;p&gt;Using the &lt;code&gt;deepeval&lt;/code&gt; platform will allow you to generate sharable testing reports on the cloud. It is free, takes no additional code to setup, and we highly recommend giving it a try.&lt;/p&gt; 
&lt;p&gt;To login, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;deepeval login
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Follow the instructions in the CLI to create an account, copy your API key, and paste it into the CLI. All test cases will automatically be logged (find more information on data privacy &lt;a href="https://deepeval.com/docs/data-privacy?utm_source=GitHub"&gt;here&lt;/a&gt;).&lt;/p&gt; 
&lt;h2&gt;Writing your first test case&lt;/h2&gt; 
&lt;p&gt;Create a test file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;touch test_chatbot.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Open &lt;code&gt;test_chatbot.py&lt;/code&gt; and write your first test case to run an &lt;strong&gt;end-to-end&lt;/strong&gt; evaluation using DeepEval, which treats your LLM app as a black-box:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import pytest
from deepeval import assert_test
from deepeval.metrics import GEval
from deepeval.test_case import LLMTestCase, LLMTestCaseParams

def test_case():
    correctness_metric = GEval(
        name="Correctness",
        criteria="Determine if the 'actual output' is correct based on the 'expected output'.",
        evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT],
        threshold=0.5
    )
    test_case = LLMTestCase(
        input="What if these shoes don't fit?",
        # Replace this with the actual output from your LLM application
        actual_output="You have 30 days to get a full refund at no extra cost.",
        expected_output="We offer a 30-day full refund at no extra costs.",
        retrieval_context=["All customers are eligible for a 30 day full refund at no extra costs."]
    )
    assert_test(test_case, [correctness_metric])
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Set your &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; as an environment variable (you can also evaluate using your own custom model, for more details visit &lt;a href="https://deepeval.com/docs/metrics-introduction#using-a-custom-llm?utm_source=GitHub"&gt;this part of our docs&lt;/a&gt;):&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;export OPENAI_API_KEY="..."
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;And finally, run &lt;code&gt;test_chatbot.py&lt;/code&gt; in the CLI:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;deepeval test run test_chatbot.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Congratulations! Your test case should have passed âœ…&lt;/strong&gt; Let's breakdown what happened.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;The variable &lt;code&gt;input&lt;/code&gt; mimics a user input, and &lt;code&gt;actual_output&lt;/code&gt; is a placeholder for what your application's supposed to output based on this input.&lt;/li&gt; 
 &lt;li&gt;The variable &lt;code&gt;expected_output&lt;/code&gt; represents the ideal answer for a given &lt;code&gt;input&lt;/code&gt;, and &lt;a href="https://deepeval.com/docs/metrics-llm-evals"&gt;&lt;code&gt;GEval&lt;/code&gt;&lt;/a&gt; is a research-backed metric provided by &lt;code&gt;deepeval&lt;/code&gt; for you to evaluate your LLM output's on any custom with human-like accuracy.&lt;/li&gt; 
 &lt;li&gt;In this example, the metric &lt;code&gt;criteria&lt;/code&gt; is correctness of the &lt;code&gt;actual_output&lt;/code&gt; based on the provided &lt;code&gt;expected_output&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;All metric scores range from 0 - 1, which the &lt;code&gt;threshold=0.5&lt;/code&gt; threshold ultimately determines if your test have passed or not.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://deepeval.com/docs/getting-started?utm_source=GitHub"&gt;Read our documentation&lt;/a&gt; for more information on more options to run end-to-end evaluation, how to use additional metrics, create your own custom metrics, and tutorials on how to integrate with other tools like LangChain and LlamaIndex.&lt;/p&gt; 
&lt;br&gt; 
&lt;h2&gt;Evaluating Nested Components&lt;/h2&gt; 
&lt;p&gt;If you wish to evaluate individual components within your LLM app, you need to run &lt;strong&gt;component-level&lt;/strong&gt; evals - a powerful way to evaluate any component within an LLM system.&lt;/p&gt; 
&lt;p&gt;Simply trace "components" such as LLM calls, retrievers, tool calls, and agents within your LLM application using the &lt;code&gt;@observe&lt;/code&gt; decorator to apply metrics on a component-level. Tracing with &lt;code&gt;deepeval&lt;/code&gt; is non-instrusive (learn more &lt;a href="https://deepeval.com/docs/evaluation-llm-tracing#dont-be-worried-about-tracing"&gt;here&lt;/a&gt;) and helps you avoid rewriting your codebase just for evals:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from deepeval.tracing import observe, update_current_span
from deepeval.test_case import LLMTestCase
from deepeval.dataset import Golden
from deepeval.metrics import GEval
from deepeval import evaluate

correctness = GEval(name="Correctness", criteria="Determine if the 'actual output' is correct based on the 'expected output'.", evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT])

@observe(metrics=[correctness])
def inner_component():
    # Component can be anything from an LLM call, retrieval, agent, tool use, etc.
    update_current_span(test_case=LLMTestCase(input="...", actual_output="..."))
    return

@observe
def llm_app(input: str):
    inner_component()
    return

evaluate(observed_callback=llm_app, goldens=[Golden(input="Hi!")])
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can learn everything about component-level evaluations &lt;a href="https://www.deepeval.com/docs/evaluation-component-level-llm-evals"&gt;here.&lt;/a&gt;&lt;/p&gt; 
&lt;br&gt; 
&lt;h2&gt;Evaluating Without Pytest Integration&lt;/h2&gt; 
&lt;p&gt;Alternatively, you can evaluate without Pytest, which is more suited for a notebook environment.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from deepeval import evaluate
from deepeval.metrics import AnswerRelevancyMetric
from deepeval.test_case import LLMTestCase

answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.7)
test_case = LLMTestCase(
    input="What if these shoes don't fit?",
    # Replace this with the actual output from your LLM application
    actual_output="We offer a 30-day full refund at no extra costs.",
    retrieval_context=["All customers are eligible for a 30 day full refund at no extra costs."]
)
evaluate([test_case], [answer_relevancy_metric])
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Using Standalone Metrics&lt;/h2&gt; 
&lt;p&gt;DeepEval is extremely modular, making it easy for anyone to use any of our metrics. Continuing from the previous example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from deepeval.metrics import AnswerRelevancyMetric
from deepeval.test_case import LLMTestCase

answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.7)
test_case = LLMTestCase(
    input="What if these shoes don't fit?",
    # Replace this with the actual output from your LLM application
    actual_output="We offer a 30-day full refund at no extra costs.",
    retrieval_context=["All customers are eligible for a 30 day full refund at no extra costs."]
)

answer_relevancy_metric.measure(test_case)
print(answer_relevancy_metric.score)
# All metrics also offer an explanation
print(answer_relevancy_metric.reason)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note that some metrics are for RAG pipelines, while others are for fine-tuning. Make sure to use our docs to pick the right one for your use case.&lt;/p&gt; 
&lt;h2&gt;Evaluating a Dataset / Test Cases in Bulk&lt;/h2&gt; 
&lt;p&gt;In DeepEval, a dataset is simply a collection of test cases. Here is how you can evaluate these in bulk:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import pytest
from deepeval import assert_test
from deepeval.dataset import EvaluationDataset, Golden
from deepeval.metrics import AnswerRelevancyMetric
from deepeval.test_case import LLMTestCase

dataset = EvaluationDataset(goldens=[Golden(input="What's the weather like today?")])

for golden in dataset.goldens:
    test_case = LLMTestCase(
        input=golden.input,
        actual_output=your_llm_app(golden.input)
    )
    dataset.add_test_case(test_case)

@pytest.mark.parametrize(
    "test_case",
    dataset,
)
def test_customer_chatbot(test_case: LLMTestCase):
    answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.5)
    assert_test(test_case, [answer_relevancy_metric])
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Run this in the CLI, you can also add an optional -n flag to run tests in parallel
deepeval test run test_&amp;lt;filename&amp;gt;.py -n 4
&lt;/code&gt;&lt;/pre&gt; 
&lt;br&gt; 
&lt;p&gt;Alternatively, although we recommend using &lt;code&gt;deepeval test run&lt;/code&gt;, you can evaluate a dataset/test cases without using our Pytest integration:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from deepeval import evaluate
...

evaluate(dataset, [answer_relevancy_metric])
# or
dataset.evaluate([answer_relevancy_metric])
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;LLM Evaluation With Confident AI&lt;/h1&gt; 
&lt;p&gt;The correct LLM evaluation lifecycle is only achievable with &lt;a href="https://confident-ai.com?utm_source=Github"&gt;the DeepEval platform&lt;/a&gt;. It allows you to:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Curate/annotate evaluation datasets on the cloud&lt;/li&gt; 
 &lt;li&gt;Benchmark LLM app using dataset, and compare with previous iterations to experiment which models/prompts works best&lt;/li&gt; 
 &lt;li&gt;Fine-tune metrics for custom results&lt;/li&gt; 
 &lt;li&gt;Debug evaluation results via LLM traces&lt;/li&gt; 
 &lt;li&gt;Monitor &amp;amp; evaluate LLM responses in product to improve datasets with real-world data&lt;/li&gt; 
 &lt;li&gt;Repeat until perfection&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Everything on Confident AI, including how to use Confident is available &lt;a href="https://documentation.confident-ai.com/docs?utm_source=GitHub"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;To begin, login from the CLI:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;deepeval login
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Follow the instructions to log in, create your account, and paste your API key into the CLI.&lt;/p&gt; 
&lt;p&gt;Now, run your test file again:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;deepeval test run test_chatbot.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You should see a link displayed in the CLI once the test has finished running. Paste it into your browser to view the results!&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/confident-ai/deepeval/main/assets/demo.gif" alt="Demo GIF"&gt;&lt;/p&gt; 
&lt;br&gt; 
&lt;h1&gt;Contributing&lt;/h1&gt; 
&lt;p&gt;Please read &lt;a href="https://github.com/confident-ai/deepeval/raw/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for details on our code of conduct, and the process for submitting pull requests to us.&lt;/p&gt; 
&lt;br&gt; 
&lt;h1&gt;Roadmap&lt;/h1&gt; 
&lt;p&gt;Features:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; Integration with Confident AI&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; Implement G-Eval&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; Implement RAG metrics&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; Implement Conversational metrics&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; Evaluation Dataset Creation&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; Red-Teaming&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled&gt; DAG custom metrics&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled&gt; Guardrails&lt;/li&gt; 
&lt;/ul&gt; 
&lt;br&gt; 
&lt;h1&gt;Authors&lt;/h1&gt; 
&lt;p&gt;Built by the founders of Confident AI. Contact &lt;a href="mailto:jeffreyip@confident-ai.com"&gt;jeffreyip@confident-ai.com&lt;/a&gt; for all enquiries.&lt;/p&gt; 
&lt;br&gt; 
&lt;h1&gt;License&lt;/h1&gt; 
&lt;p&gt;DeepEval is licensed under Apache 2.0 - see the &lt;a href="https://github.com/confident-ai/deepeval/raw/main/LICENSE.md"&gt;LICENSE.md&lt;/a&gt; file for details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>QwenLM/Qwen3-Coder</title>
      <link>https://github.com/QwenLM/Qwen3-Coder</link>
      <description>&lt;p&gt;Qwen3-Coder is the code version of Qwen3, the large language model series developed by Qwen team, Alibaba Cloud.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a name="readme-top"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Coder/qwen3_coder.png" width="400"&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p align="center"&gt; &lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Coder/qwen3-coder-main.jpg" width="800"&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p align="center"&gt; ğŸ’œ &lt;a href="https://chat.qwenlm.ai/"&gt;&lt;b&gt;Qwen Chat&lt;/b&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ğŸ¤— &lt;a href="https://huggingface.co/collections/Qwen/qwen3-coder-687fc861e53c939e52d52d10"&gt;Hugging Face&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ğŸ¤– &lt;a href="https://modelscope.cn/organization/qwen"&gt;ModelScope&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp; ğŸ“‘ &lt;a href="https://qwenlm.github.io/blog/qwen3-coder"&gt;Blog&lt;/a&gt; &amp;nbsp;&amp;nbsp; ï½œ &amp;nbsp;&amp;nbsp;ğŸ“– &lt;a href="https://qwen.readthedocs.io/"&gt;Documentation&lt;/a&gt; &lt;br&gt; &amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp; ğŸŒ &lt;a href="https://huggingface.co/spaces/Qwen/Qwen3-Coder-WebDev"&gt;WebDev&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ğŸ’¬ &lt;a href="https://github.com/QwenLM/Qwen/raw/main/assets/wechat.png"&gt;WeChat (å¾®ä¿¡)&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ğŸ«¨ &lt;a href="https://discord.gg/CV4E9rpNSD"&gt; Discord&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp; ğŸ“„ &lt;a href="https://arxiv.org/abs/2505.09388"&gt;Arxiv&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp; ğŸ‘½ &lt;a href="https://github.com/QwenLM/qwen-code"&gt;Qwen Code&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;Visit our Hugging Face or ModelScope organization (click links above), search checkpoints with names starting with &lt;code&gt;Qwen3-Coder-&lt;/code&gt;, and you will find all you need! Enjoy!&lt;/p&gt; 
&lt;h1&gt;Qwen3-Coder: Agentic Coding in the World.&lt;/h1&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;Today, we're announcing Qwen3-Coder, our most agentic code model to date. &lt;strong&gt;Qwen3-Coder&lt;/strong&gt; is available in multiple sizes, but we're excited to introduce its most powerful variant first: &lt;strong&gt;Qwen3-Coder-480B-A35B-Instruct&lt;/strong&gt; â€” a 480B-parameter Mixture-of-Experts model with 35B active parameters, offering exceptional performance in both coding and agentic tasks. &lt;strong&gt;Qwen3-Coder-480B-A35B-Instruct&lt;/strong&gt; sets new state-of-the-art results among open models on Agentic Coding, Agentic Browser-Use, and Agentic Tool-Use, comparable to Claude Sonnet.&lt;/p&gt; 
&lt;p&gt;ğŸ’» &lt;strong&gt;Significant Performance&lt;/strong&gt;: among open models on &lt;strong&gt;Agentic Coding&lt;/strong&gt;, &lt;strong&gt;Agentic Browser-Use&lt;/strong&gt;, and other foundational coding tasks, achieving results comparable to Claude Sonnet;&lt;/p&gt; 
&lt;p&gt;ğŸ“š &lt;strong&gt;Long-context Capabilities&lt;/strong&gt;: with native support for &lt;strong&gt;256K&lt;/strong&gt; tokens, extendable up to &lt;strong&gt;1M&lt;/strong&gt; tokens using Yarn, optimized for repository-scale understanding;&lt;/p&gt; 
&lt;p&gt;ğŸ›  &lt;strong&gt;Agentic Coding&lt;/strong&gt;: supporting for most platform such as &lt;strong&gt;Qwen Code&lt;/strong&gt;, &lt;strong&gt;CLINE&lt;/strong&gt;, featuring a specially designed function call format;&lt;/p&gt; 
&lt;h2&gt;Basic information&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;âœ¨ Supporting long context understanding and generation with the context length of 256K tokens;&lt;/li&gt; 
 &lt;li&gt;âœ¨ Supporting 358 coding languages;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code&gt;['ABAP', 'ActionScript', 'Ada', 'Agda', 'Alloy', 'ApacheConf', 'AppleScript', 'Arc', 'Arduino', 'AsciiDoc', 'AspectJ', 'Assembly', 'Augeas', 'AutoHotkey', 'AutoIt', 'Awk', 'Batchfile', 'Befunge', 'Bison', 'BitBake', 'BlitzBasic', 'BlitzMax', 'Bluespec', 'Boo', 'Brainfuck', 'Brightscript', 'Bro', 'C', 'C#', 'C++', 'C2hs Haskell', 'CLIPS', 'CMake', 'COBOL', 'CSS', 'CSV', "Cap'n Proto", 'CartoCSS', 'Ceylon', 'Chapel', 'ChucK', 'Cirru', 'Clarion', 'Clean', 'Click', 'Clojure', 'CoffeeScript', 'ColdFusion', 'ColdFusion CFC', 'Common Lisp', 'Component Pascal', 'Coq', 'Creole', 'Crystal', 'Csound', 'Cucumber', 'Cuda', 'Cycript', 'Cython', 'D', 'DIGITAL Command Language', 'DM', 'DNS Zone', 'Darcs Patch', 'Dart', 'Diff', 'Dockerfile', 'Dogescript', 'Dylan', 'E', 'ECL', 'Eagle', 'Ecere Projects', 'Eiffel', 'Elixir', 'Elm', 'Emacs Lisp', 'EmberScript', 'Erlang', 'F#', 'FLUX', 'FORTRAN', 'Factor', 'Fancy', 'Fantom', 'Forth', 'FreeMarker', 'G-code', 'GAMS', 'GAP', 'GAS', 'GDScript', 'GLSL', 'Genshi', 'Gentoo Ebuild', 'Gentoo Eclass', 'Gettext Catalog', 'Glyph', 'Gnuplot', 'Go', 'Golo', 'Gosu', 'Grace', 'Gradle', 'Grammatical Framework', 'GraphQL', 'Graphviz (DOT)', 'Groff', 'Groovy', 'Groovy Server Pages', 'HCL', 'HLSL', 'HTML', 'HTML+Django', 'HTML+EEX', 'HTML+ERB', 'HTML+PHP', 'HTTP', 'Haml', 'Handlebars', 'Harbour', 'Haskell', 'Haxe', 'Hy', 'IDL', 'IGOR Pro', 'INI', 'IRC log', 'Idris', 'Inform 7', 'Inno Setup', 'Io', 'Ioke', 'Isabelle', 'J', 'JFlex', 'JSON', 'JSON5', 'JSONLD', 'JSONiq', 'JSX', 'Jade', 'Jasmin', 'Java', 'Java Server Pages', 'JavaScript', 'Julia', 'Jupyter Notebook', 'KRL', 'KiCad', 'Kit', 'Kotlin', 'LFE', 'LLVM', 'LOLCODE', 'LSL', 'LabVIEW', 'Lasso', 'Latte', 'Lean', 'Less', 'Lex', 'LilyPond', 'Linker Script', 'Liquid', 'Literate Agda', 'Literate CoffeeScript', 'Literate Haskell', 'LiveScript', 'Logos', 'Logtalk', 'LookML', 'Lua', 'M', 'M4', 'MAXScript', 'MTML', 'MUF', 'Makefile', 'Mako', 'Maple', 'Markdown', 'Mask', 'Mathematica', 'Matlab', 'Max', 'MediaWiki', 'Metal', 'MiniD', 'Mirah', 'Modelica', 'Module Management System', 'Monkey', 'MoonScript', 'Myghty', 'NSIS', 'NetLinx', 'NetLogo', 'Nginx', 'Nimrod', 'Ninja', 'Nit', 'Nix', 'Nu', 'NumPy', 'OCaml', 'ObjDump', 'Objective-C++', 'Objective-J', 'Octave', 'Omgrofl', 'Opa', 'Opal', 'OpenCL', 'OpenEdge ABL', 'OpenSCAD', 'Org', 'Ox', 'Oxygene', 'Oz', 'PAWN', 'PHP', 'POV-Ray SDL', 'Pan', 'Papyrus', 'Parrot', 'Parrot Assembly', 'Parrot Internal Representation', 'Pascal', 'Perl', 'Perl6', 'Pickle', 'PigLatin', 'Pike', 'Pod', 'PogoScript', 'Pony', 'PostScript', 'PowerShell', 'Processing', 'Prolog', 'Propeller Spin', 'Protocol Buffer', 'Public Key', 'Pure Data', 'PureBasic', 'PureScript', 'Python', 'Python traceback', 'QML', 'QMake', 'R', 'RAML', 'RDoc', 'REALbasic', 'RHTML', 'RMarkdown', 'Racket', 'Ragel in Ruby Host', 'Raw token data', 'Rebol', 'Red', 'Redcode', "Ren'Py", 'RenderScript', 'RobotFramework', 'Rouge', 'Ruby', 'Rust', 'SAS', 'SCSS', 'SMT', 'SPARQL', 'SQF', 'SQL', 'STON', 'SVG', 'Sage', 'SaltStack', 'Sass', 'Scala', 'Scaml', 'Scheme', 'Scilab', 'Self', 'Shell', 'ShellSession', 'Shen', 'Slash', 'Slim', 'Smali', 'Smalltalk', 'Smarty', 'Solidity', 'SourcePawn', 'Squirrel', 'Stan', 'Standard ML', 'Stata', 'Stylus', 'SuperCollider', 'Swift', 'SystemVerilog', 'TOML', 'TXL', 'Tcl', 'Tcsh', 'TeX', 'Tea', 'Text', 'Textile', 'Thrift', 'Turing', 'Turtle', 'Twig', 'TypeScript', 'Unified Parallel C', 'Unity3D Asset', 'Uno', 'UnrealScript', 'UrWeb', 'VCL', 'VHDL', 'Vala', 'Verilog', 'VimL', 'Visual Basic', 'Volt', 'Vue', 'Web Ontology Language', 'WebAssembly', 'WebIDL', 'X10', 'XC', 'XML', 'XPages', 'XProc', 'XQuery', 'XS', 'XSLT', 'Xojo', 'Xtend', 'YAML', 'YANG', 'Yacc', 'Zephir', 'Zig', 'Zimpl', 'desktop', 'eC', 'edn', 'fish', 'mupad', 'nesC', 'ooc', 'reStructuredText', 'wisp', 'xBase']
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;âœ¨ Retain strengths in math and general capabilities from base model.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Important]&lt;/p&gt; 
 &lt;p&gt;Qwen3-coder function calling relies on our new tool parser &lt;code&gt;qwen3coder_tool_parser.py&lt;/code&gt; &lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct/blob/main/qwen3coder_tool_parser.py"&gt;here&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;We updated both the special tokens and their corresponding token ids, in order to maintain consistency with Qwen3. Please make sure to use the new tokenizer.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;model name&lt;/th&gt; 
   &lt;th&gt;type&lt;/th&gt; 
   &lt;th&gt;length&lt;/th&gt; 
   &lt;th&gt;Download&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Qwen3-Coder-480B-A35B-Instruct&lt;/td&gt; 
   &lt;td&gt;instruct&lt;/td&gt; 
   &lt;td&gt;256k&lt;/td&gt; 
   &lt;td&gt;ğŸ¤— &lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct"&gt;Hugging Face&lt;/a&gt; â€¢ ğŸ¤– &lt;a href="https://modelscope.cn/models/Qwen/Qwen3-Coder-480B-A35B-Instruct"&gt;ModelScope&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Qwen3-Coder-480B-A35B-Instruct-FP8&lt;/td&gt; 
   &lt;td&gt;instruct&lt;/td&gt; 
   &lt;td&gt;256k&lt;/td&gt; 
   &lt;td&gt;ğŸ¤— &lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8"&gt;Hugging Face&lt;/a&gt; â€¢ ğŸ¤– &lt;a href="https://modelscope.cn/models/Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8"&gt;ModelScope&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Detailed performance and introduction are shown in this &lt;a href="https://qwenlm.github.io/blog/qwen3-coder"&gt; ğŸ“‘ blog&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Important] &lt;strong&gt;Qwen3-Coder-480B-A35B-Instruct&lt;/strong&gt; are instruction models for chatting;&lt;/p&gt; 
 &lt;p&gt;This model supports only non-thinking mode and does not generate &lt;code&gt;&amp;lt;think&amp;gt;&amp;lt;/think&amp;gt;&lt;/code&gt; blocks in its output. Meanwhile, specifying &lt;code&gt;enable_thinking=False&lt;/code&gt; is no longer required.**&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;ğŸ‘‰ğŸ» Chat with Qwen3-Coder-480B-A35B-Instruct&lt;/h3&gt; 
&lt;p&gt;You can just write several lines of code with &lt;code&gt;transformers&lt;/code&gt; to chat with Qwen3-Coder-480B-A35B-Instruct. Essentially, we build the tokenizer and the model with &lt;code&gt;from_pretrained&lt;/code&gt; method, and we use generate method to perform chatting with the help of chat template provided by the tokenizer. Below is an example of how to chat with &lt;strong&gt;Qwen3-Coder-480B-A35B-Instruct&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "Qwen/Qwen3-Coder-480B-A35B-Instruct"

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

prompt = "write a quick sort algorithm."
messages = [
    {"role": "user", "content": prompt}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=65536
)
generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]

response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The &lt;code&gt;apply_chat_template()&lt;/code&gt; function is used to convert the messages into a format that the model can understand. The &lt;code&gt;add_generation_prompt&lt;/code&gt; argument is used to add a generation prompt, which refers to &lt;code&gt;&amp;lt;|im_start|&amp;gt;assistant\n&lt;/code&gt; to the input. Notably, we apply ChatML template for chat models following our previous practice. The &lt;code&gt;max_new_tokens&lt;/code&gt; argument is used to set the maximum length of the response. The &lt;code&gt;tokenizer.batch_decode()&lt;/code&gt; function is used to decode the response. In terms of the input, the above messages is an example to show how to format your dialog history and system prompt. You can use the other size of instruct model in the same way.&lt;/p&gt; 
&lt;h4&gt;Fill in the middle with Qwen3-Coder-480B-A35B-Instruct&lt;/h4&gt; 
&lt;p&gt;The code insertion task, also referred to as the "fill-in-the-middle" challenge, requires the insertion of code segments in a manner that bridges the gaps within a given code context. For an approach aligned with best practices, we recommend adhering to the formatting guidelines outlined in the paper "Efficient Training of Language Models to Fill in the Middle"[&lt;a href="https://arxiv.org/abs/2207.14255"&gt;arxiv&lt;/a&gt;].&lt;/p&gt; 
&lt;p&gt;The prompt should be structured as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;prompt = '&amp;lt;|fim_prefix|&amp;gt;' + prefix_code + '&amp;lt;|fim_suffix|&amp;gt;' + suffix_code + '&amp;lt;|fim_middle|&amp;gt;'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Following the approach mentioned, an example would be structured in this manner:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from transformers import AutoTokenizer, AutoModelForCausalLM
# load model
device = "cuda" # the device to load the model onto

TOKENIZER = AutoTokenizer.from_pretrained("Qwen/Qwen3-Coder-480B-A35B-Instruct")
MODEL = AutoModelForCausalLM.from_pretrained("Qwen/Qwen3-Coder-480B-A35B-Instruct", device_map="auto").eval()


input_text = """&amp;lt;|fim_prefix|&amp;gt;def quicksort(arr):
    if len(arr) &amp;lt;= 1:
        return arr
    pivot = arr[len(arr) // 2]
    &amp;lt;|fim_suffix|&amp;gt;
    middle = [x for x in arr if x == pivot]
    right = [x for x in arr if x &amp;gt; pivot]
    return quicksort(left) + middle + quicksort(right)&amp;lt;|fim_middle|&amp;gt;"""
            
messages = [
    {"role": "system", "content": "You are a code completion assistant."},
    {"role": "user", "content": input_text}
]


text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)
model_inputs = TOKENIZER([text], return_tensors="pt").to(model.device)

# Use `max_new_tokens` to control the maximum output length.
generated_ids = MODEL.generate(model_inputs.input_ids, max_new_tokens=512, do_sample=False)[0]
# The generated_ids include prompt_ids, we only need to decode the tokens after prompt_ids.
output_text = TOKENIZER.decode(generated_ids[len(model_inputs.input_ids[0]):], skip_special_tokens=True)

print(f"Prompt: {input_text}\n\nGenerated text: {output_text}")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Use Cases&lt;/h2&gt; 
&lt;h3&gt;Example: Physics-Based Chimney Demolition Simulation with Controlled Explosion&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;Prompt with Qwen Chat Web Dev &lt;/summary&gt; 
 &lt;pre&gt;&lt;code&gt;ä½¿ç”¨ three.js, cannon-es.js ç”Ÿæˆä¸€ä¸ªéœ‡æ’¼çš„3Då»ºç­‘æ‹†é™¤æ¼”ç¤ºã€‚

## åœºæ™¯è®¾ç½®ï¼š
- åœ°é¢æ˜¯ä¸€ä¸ªæ·±ç°è‰²æ··å‡åœŸå¹³é¢ï¼Œå°ºå¯¸80*80ï¼Œ
- æ‰€æœ‰ç‰©ä½“ä¸¥æ ¼éµå¾ªç°å®ç‰©ç†è§„åˆ™ï¼ŒåŒ…æ‹¬é‡åŠ›ã€æ‘©æ“¦åŠ›ã€ç¢°æ’æ£€æµ‹å’ŒåŠ¨é‡å®ˆæ’

## å»ºç­‘ç»“æ„ï¼š
- ä¸€åº§åœ†å½¢é«˜å±‚å»ºç­‘ï¼Œå‘¨é•¿å¯¹åº”20ä¸ªæ–¹å—
- å»ºç­‘æ€»é«˜åº¦60ä¸ªæ–¹å—
- æ¯å±‚é‡‡ç”¨ç –ç Œç»“æ„ï¼Œæ–¹å—ä¸ç –ç»“æ„å»ºç­‘ä¸€è‡´, é”™å¼€50%æ’åˆ—ï¼Œå¢å¼ºç»“æ„ç¨³å®šæ€§
- å»ºç­‘å¤–å¢™ä½¿ç”¨ç±³è‰²æ–¹å—
- **é‡è¦ï¼šæ–¹å—åˆå§‹æ’åˆ—æ—¶å¿…é¡»ç¡®ä¿ç´§å¯†è´´åˆï¼Œæ— é—´éš™ï¼Œå¯ä»¥é€šè¿‡è½»å¾®é‡å æˆ–è°ƒæ•´åŠå¾„æ¥å®ç°**
- **é‡è¦ï¼šå»ºç­‘åˆå§‹åŒ–å®Œæˆåï¼Œæ‰€æœ‰æ–¹å—åº”è¯¥å¤„äºç‰©ç†"ç¡çœ "çŠ¶æ€ï¼Œç¡®ä¿å»ºç­‘åœ¨çˆ†ç‚¸å‰ä¿æŒå®Œç¾çš„é™æ­¢çŠ¶æ€ï¼Œä¸ä¼šå› é‡åŠ›è€Œä¸‹æ²‰æˆ–æ¾æ•£**
- å»ºç­‘ç –å—ä¹‹é—´ä½¿ç”¨ç²˜æ€§ææ–™å¡«å……ï¼ˆä¸å¯è§ï¼‰ï¼Œé€šè¿‡é«˜æ‘©æ“¦åŠ›ï¼ˆ0.8+ï¼‰å’Œä½å¼¹æ€§ï¼ˆ0.05ä»¥ä¸‹ï¼‰æ¥æ¨¡æ‹Ÿç²˜åˆæ•ˆæœ
- ç –å—åœ¨å»ºç­‘å€’å¡Œç¬é—´ä¸ä¼šæ•£æ‰ï¼Œè€Œæ˜¯å»ºç­‘ä½œä¸ºä¸€ä¸ªæ•´ä½“å€’åœ¨åœ°é¢çš„æ—¶å€™æ‰å› å—åŠ›è¿‡å¤§è€Œæ•£æ‰

## å®šå‘çˆ†ç ´ç³»ç»Ÿï¼š
- åœ¨å»ºç­‘çš„ç¬¬1å±‚çš„æœ€å³ä¾§æ–¹å—é™„è¿‘å®‰è£…çˆ†ç‚¸è£…ç½®ï¼ˆä¸å¯è§ï¼‰
- æä¾›æ“ä½œæŒ‰é’®ç‚¹å‡»çˆ†ç‚¸
- **çˆ†ç‚¸æ—¶å”¤é†’æ‰€æœ‰ç›¸å…³æ–¹å—çš„ç‰©ç†çŠ¶æ€**
- çˆ†ç‚¸ç‚¹äº§ç”ŸåŠå¾„2çš„å¼ºåŠ›å†²å‡»æ³¢ï¼Œå†²å‡»æ³¢å½±å“åˆ°çš„æ–¹å—, å—åˆ°2-5å•ä½çš„å†²å‡»åŠ›

## å»ºç­‘ç¨³å®šæ€§è¦æ±‚ï¼š
- **ç¡®ä¿å»ºç­‘åœ¨æœªçˆ†ç‚¸æ—¶å®Œå…¨é™æ­¢ï¼Œæ— ä»»ä½•æ™ƒåŠ¨æˆ–ä¸‹æ²‰**
- **ç‰©ç†ä¸–ç•Œåˆå§‹åŒ–åç»™å»ºç­‘å‡ ä¸ªç‰©ç†æ­¥éª¤æ¥è‡ªç„¶ç¨³å®šï¼Œæˆ–ä½¿ç”¨ç¡çœ æœºåˆ¶**
- **æ–¹å—é—´çš„æ¥è§¦ææ–™åº”å…·æœ‰é«˜æ‘©æ“¦åŠ›å’Œæä½å¼¹æ€§ï¼Œæ¨¡æ‹Ÿç –å—é—´çš„ç ‚æµ†ç²˜åˆ**

## éœ‡æ’¼çš„å€’å¡Œæ•ˆæœï¼š
- æ–¹å—åœ¨çˆ†ç‚¸å†²å‡»ä¸‹ä¸ä»…é£æ•£ï¼Œè¿˜ä¼šåœ¨ç©ºä¸­ç¿»æ»šå’Œç¢°æ’
- çƒŸå°˜ä¼šéšç€å»ºç­‘å€’å¡Œé€æ¸æ‰©æ•£ï¼Œè¥é€ çœŸå®çš„æ‹†é™¤ç°åœºæ°›å›´

## å¢å¼ºçš„è§†è§‰æ•ˆæœï¼š
- æ·»åŠ ç¯å¢ƒå…‰ç…§å˜åŒ–ï¼šçˆ†ç‚¸ç¬é—´äº®åº¦æ¿€å¢ï¼Œç„¶åè¢«çƒŸå°˜é®æŒ¡å˜æš—
- ç²’å­ç³»ç»ŸåŒ…æ‹¬ï¼šçƒŸé›¾ã€ç°å°˜

## æŠ€æœ¯è¦æ±‚ï¼š
- ç²’å­ç³»ç»Ÿç”¨äºçƒŸé›¾å’Œç°å°˜æ•ˆæœ
- æ‰€æœ‰ä»£ç é›†æˆåœ¨å•ä¸ªHTMLæ–‡ä»¶ä¸­ï¼ŒåŒ…å«å¿…è¦çš„CSSæ ·å¼
- æ·»åŠ ç®€å•çš„UIæ§åˆ¶ï¼šé‡ç½®æŒ‰é’®ã€ç›¸æœºè§’åº¦åˆ‡æ¢, çˆ†ç‚¸æŒ‰é’®, é¼ æ ‡å·¦é”®æ§åˆ¶æ‘„åƒæœºè§’åº¦ï¼Œå³é”®æ§åˆ¶æ‘„åƒæœºä½ç½®ï¼Œæ»šè½®æ§åˆ¶æ‘„åƒæœºç„¦è·
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p align="center"&gt; &lt;a href="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Coder/demo1.mp4"&gt; &lt;img src="https://raw.githubusercontent.com/QwenLM/Qwen3-Coder/main/assets/usage_demo_example1.png" width="400"&gt; &lt;/a&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3&gt;Example: Multicolor and Interactive Animation&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;Prompt with Cline [act mode] &lt;/summary&gt; 
 &lt;pre&gt;&lt;code&gt;Create an amazing animation multicolor and interactive using p5js

use this cdn:
https://cdn.jsdelivr.net/npm/p5@1.7.0/lib/p5.min.js
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p align="center"&gt; &lt;a href="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Coder/demo2.mp4"&gt; &lt;img src="https://raw.githubusercontent.com/QwenLM/Qwen3-Coder/main/assets/usage_demo_example2.png" width="400"&gt; &lt;/a&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3&gt;Example: 3D Google Earth&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;Prompt with Qwen Chat Web Dev &lt;/summary&gt; 
 &lt;pre&gt;&lt;code&gt;To create a 3D Google Earth, you need to load the terrain map correctly. You can use any online resource. The code is written into an HTML file.
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p align="center"&gt; &lt;a href="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Coder/demo3.mp4"&gt; &lt;img src="https://raw.githubusercontent.com/QwenLM/Qwen3-Coder/main/assets/usage_demo_example3.png" width="400"&gt; &lt;/a&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3&gt;Example: Testing Your WPM with a Famous Quote&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt; Prompt with Qwen-Code CLI &lt;/summary&gt; 
 &lt;pre&gt;&lt;code&gt;Create an interesting typing game with a keyboard in the lower middle of the screen and some famous articles in the upper middle. When the user types a word correctly, a cool reaction should be given to encourage him. Design a modern soft color scheme inspired by macarons. Come up with a very creative solution first, and then start writing code.
The game should be able to support typing, and you need to neglect upcase and lowercase.
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p align="center"&gt; &lt;a href="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Coder/demo4.mp4"&gt; &lt;img src="https://raw.githubusercontent.com/QwenLM/Qwen3-Coder/main/assets/usage_demo_example4.png" width="400"&gt; &lt;/a&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3&gt;Example: Bouncing Ball in Rotation Hypercube&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt; Prompt with Qwen Chat Web Dev &lt;/summary&gt; 
 &lt;pre&gt;&lt;code&gt;Make a page in HTML that shows an animation of a ball bouncing in a rotating hypercube
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p align="center"&gt; &lt;a href="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Coder/demo5.mp4"&gt; &lt;img src="https://raw.githubusercontent.com/QwenLM/Qwen3-Coder/main/assets/usage_demo_example5.png" width="400"&gt; &lt;/a&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3&gt;Example: Solar System Simulation&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt; Prompt with Cline [act mode] &lt;/summary&gt; 
 &lt;pre&gt;&lt;code&gt;write a web page to show the solar system simulation
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p align="center"&gt; &lt;a href="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Coder/demo6.mp4"&gt; &lt;img src="https://raw.githubusercontent.com/QwenLM/Qwen3-Coder/main/assets/usage_demo_example6.png" width="400"&gt; &lt;/a&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3&gt;Example: DUET Game&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt; Prompt with Cline [act mode] &lt;/summary&gt; 
 &lt;pre&gt;&lt;code&gt;Create a complete, single-file HTML game with CSS and JavaScript. The game is inspired by "Duet".

Gameplay:

There are two balls, one red and one blue, rotating around a central point.
The player uses the 'A' and 'D' keys to rotate them counter-clockwise and clockwise.
White rectangular obstacles move down from the top of the screen.
The player must rotate the balls to avoid hitting the obstacles.
If a ball hits an obstacle, the game is over.
Visuals:

Make the visual effects amazing.
Use a dark background with neon glowing effects for the balls and obstacles.
Animations should be very smooth.
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p align="center"&gt; &lt;a href="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Coder/demo7.mp4"&gt; &lt;img src="https://raw.githubusercontent.com/QwenLM/Qwen3-Coder/main/assets/usage_demo_example7.png" width="400"&gt; &lt;/a&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#QwenLM/Qwen3-Coder&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=QwenLM/Qwen3-Coder&amp;amp;type=Date" alt="Star History Chart"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you find our work helpful, feel free to give us a cite.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{qwen3technicalreport,
      title={Qwen3 Technical Report}, 
      author={Qwen Team},
      year={2025},
      eprint={2505.09388},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2505.09388},
}
@article{hui2024qwen2,
  title={Qwen2. 5-Coder Technical Report},
  author={Hui, Binyuan and Yang, Jian and Cui, Zeyu and Yang, Jiaxi and Liu, Dayiheng and Zhang, Lei and Liu, Tianyu and Zhang, Jiajun and Yu, Bowen and Dang, Kai and others},
  journal={arXiv preprint arXiv:2409.12186},
  year={2024}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Contact Us&lt;/h2&gt; 
&lt;p&gt;If you are interested to leave a message to either our research team or product team, join our &lt;a href="https://discord.gg/z3GAxXZ9Ce"&gt;Discord&lt;/a&gt; or &lt;a href="https://github.com/QwenLM/Qwen/raw/main/assets/wechat.png"&gt;WeChat groups&lt;/a&gt;!&lt;/p&gt; 
&lt;p align="right" style="font-size: 14px; color: #555; margin-top: 20px;"&gt; &lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen3-Coder/main/#readme-top" style="text-decoration: none; color: #007bff; font-weight: bold;"&gt; â†‘ Back to Top â†‘ &lt;/a&gt; &lt;/p&gt;</description>
    </item>
    
    <item>
      <title>KurtBestor/Hitomi-Downloader</title>
      <link>https://github.com/KurtBestor/Hitomi-Downloader</link>
      <description>&lt;p&gt;ğŸ° Desktop utility to download images/videos/music/text from various websites, and more.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/KurtBestor/Hitomi-Downloader/master/imgs/card_crop.png" width="50%"&gt; &lt;br&gt; &lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/KurtBestor/Hitomi-Downloader/releases/latest"&gt;&lt;img src="https://img.shields.io/github/release/KurtBestor/Hitomi-Downloader.svg?logo=github" alt="GitHub release"&gt;&lt;/a&gt; &lt;a href="https://github.com/KurtBestor/Hitomi-Downloader/releases/latest"&gt;&lt;img src="https://img.shields.io/github/downloads/KurtBestor/Hitomi-Downloader/latest/total.svg?logo=github" alt="GitHub downloads"&gt;&lt;/a&gt; &lt;a href="https://github.com/KurtBestor/Hitomi-Downloader/releases"&gt;&lt;img src="https://img.shields.io/github/downloads/KurtBestor/Hitomi-Downloader/total.svg?logo=github" alt="GitHub downloads"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Links&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/KurtBestor/Hitomi-Downloader/releases/latest"&gt;Download&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/KurtBestor/Hitomi-Downloader/issues"&gt;Issues&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/KurtBestor/Hitomi-Downloader/wiki/Scripts-&amp;amp;-Plugins"&gt;Scripts &amp;amp; Plugins&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/KurtBestor/Hitomi-Downloader/wiki/Chrome-Extension"&gt;Chrome Extension&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Demo&lt;/h2&gt; 
&lt;img src="https://raw.githubusercontent.com/KurtBestor/Hitomi-Downloader/master/imgs/how_to_download.gif"&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸ° Simple and clear user interface&lt;/li&gt; 
 &lt;li&gt;ğŸš€ Download acceleration&lt;/li&gt; 
 &lt;li&gt;ğŸ’» Supports 24 threads in a single task&lt;/li&gt; 
 &lt;li&gt;ğŸš¥ Supports speed limit&lt;/li&gt; 
 &lt;li&gt;ğŸ“œ Supports user scripts&lt;/li&gt; 
 &lt;li&gt;ğŸ§² Supports BitTorrent &amp;amp; Magnet&lt;/li&gt; 
 &lt;li&gt;ğŸï¸ Supports M3U8 &amp;amp; MPD format videos&lt;/li&gt; 
 &lt;li&gt;ğŸŒ™ Dark mode&lt;/li&gt; 
 &lt;li&gt;ğŸ§³ Portable&lt;/li&gt; 
 &lt;li&gt;ğŸ“‹ Clipboard monitor&lt;/li&gt; 
 &lt;li&gt;ğŸ—ƒï¸ Easy to organize tasks&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Supported Sites&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;Site&lt;/th&gt; 
   &lt;th&gt;URL&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;4chan&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://4chan.org"&gt;https://4chan.org&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;AfreecaTV&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://afreecatv.com"&gt;https://afreecatv.com&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;ArtStation&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://artstation.com"&gt;https://artstation.com&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;baraag.net&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://baraag.net"&gt;https://baraag.net&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;bilibili&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://bilibili.com"&gt;https://bilibili.com&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;ComicWalker&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://comic-walker.com"&gt;https://comic-walker.com&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Coub&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://coub.com"&gt;https://coub.com&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;DeviantArt&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://deviantart.com"&gt;https://deviantart.com&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Facebook&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://facebook.com"&gt;https://facebook.com&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;FC2 Video&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://video.fc2.com"&gt;https://video.fc2.com&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Flickr&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://flickr.com"&gt;https://flickr.com&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Hameln&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://syosetu.org"&gt;https://syosetu.org&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Imgur&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://imgur.com"&gt;https://imgur.com&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Instagram&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://instagram.com"&gt;https://instagram.com&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;ã‚«ã‚¯ãƒ¨ãƒ &lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://kakuyomu.jp"&gt;https://kakuyomu.jp&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Mastodon&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://mastodon.social"&gt;https://mastodon.social&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Misskey&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://misskey.io"&gt;https://misskey.io&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Naver Blog&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://blog.naver.com"&gt;https://blog.naver.com&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Naver Cafe&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://cafe.naver.com"&gt;https://cafe.naver.com&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Naver Post&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://post.naver.com"&gt;https://post.naver.com&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Naver Webtoon&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://comic.naver.com"&gt;https://comic.naver.com&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Naver TV&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://tv.naver.com"&gt;https://tv.naver.com&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Niconico&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="http://nicovideo.jp"&gt;http://nicovideo.jp&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Pawoo&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://pawoo.net"&gt;https://pawoo.net&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Pinterest&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://pinterest.com"&gt;https://pinterest.com&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Pixiv&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://pixiv.net"&gt;https://pixiv.net&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;pixivã‚³ãƒŸãƒƒã‚¯&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://comic.pixiv.net"&gt;https://comic.pixiv.net&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Soundcloud&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://soundcloud.com"&gt;https://soundcloud.com&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;å°èª¬å®¶ã«ãªã‚ã†&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://syosetu.com"&gt;https://syosetu.com&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;TikTok&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://tiktok.com"&gt;https://tiktok.com&lt;/a&gt;&lt;br&gt;&lt;a href="https://douyin.com"&gt;https://douyin.com&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Tumblr&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://tumblr.com"&gt;https://tumblr.com&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Twitch&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://twitch.tv"&gt;https://twitch.tv&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Twitter&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://twitter.com"&gt;https://twitter.com&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Vimeo&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://vimeo.com"&gt;https://vimeo.com&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Wayback Machine&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://archive.org"&gt;https://archive.org&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Weibo&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://weibo.com"&gt;https://weibo.com&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;WikiArt&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.wikiart.org"&gt;https://www.wikiart.org&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Youku&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://youku.com"&gt;https://youku.com&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;YouTube&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://youtube.com"&gt;https://youtube.com&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;and more...&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/yt-dlp/yt-dlp/raw/master/supportedsites.md"&gt;Supported sites by yt-dlp&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt;</description>
    </item>
    
    <item>
      <title>OpenBB-finance/OpenBB</title>
      <link>https://github.com/OpenBB-finance/OpenBB</link>
      <description>&lt;p&gt;Investment Research for Everyone, Everywhere.&lt;/p&gt;&lt;hr&gt;&lt;br&gt; 
&lt;img src="https://github.com/OpenBB-finance/OpenBB/raw/develop/images/platform-light.svg?raw=true#gh-light-mode-only" alt="OpenBB Platform logo" width="600"&gt; 
&lt;img src="https://github.com/OpenBB-finance/OpenBB/raw/develop/images/platform-dark.svg?raw=true#gh-dark-mode-only" alt="OpenBB Platform logo" width="600"&gt; 
&lt;br&gt; 
&lt;br&gt; 
&lt;p&gt;&lt;a href="https://x.com/openbb_finance"&gt;&lt;img src="https://img.shields.io/twitter/url/https/twitter.com/openbb_finance.svg?style=social&amp;amp;label=Follow%20%40openbb_finance" alt="Twitter"&gt;&lt;/a&gt; &lt;a href="https://discord.com/invite/xPHTuHCmuV"&gt;&lt;img src="https://img.shields.io/discord/831165782750789672" alt="Discord Shield"&gt;&lt;/a&gt; &lt;a href="https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/OpenBB-finance/OpenBB"&gt;&lt;img src="https://img.shields.io/static/v1?label=Dev%20Containers&amp;amp;message=Open&amp;amp;color=blue&amp;amp;logo=visualstudiocode" alt="Open in Dev Containers"&gt;&lt;/a&gt; &lt;a href="https://codespaces.new/OpenBB-finance/OpenBB"&gt; &lt;img src="https://github.com/codespaces/badge.svg?sanitize=true" height="20"&gt; &lt;/a&gt; &lt;a target="_blank" href="https://colab.research.google.com/github/OpenBB-finance/OpenBB/blob/develop/examples/googleColab.ipynb"&gt; &lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab"&gt; &lt;/a&gt; &lt;a href="https://pypi.org/project/openbb/"&gt;&lt;img src="https://img.shields.io/pypi/v/openbb?color=blue&amp;amp;label=PyPI%20Package" alt="PyPI"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;The first financial Platform that is open source.&lt;/p&gt; 
&lt;p&gt;The OpenBB Platform offers access to equity, options, crypto, forex, macro economy, fixed income, and more while also offering a broad range of extensions to enhance the user experience according to their needs.&lt;/p&gt; 
&lt;p&gt;Get started with: &lt;code&gt;pip install openbb&lt;/code&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from openbb import obb
output = obb.equity.price.historical("AAPL")
df = output.to_dataframe()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can sign up to the &lt;a href="https://my.openbb.co/login"&gt;OpenBB Hub&lt;/a&gt; to get the most out of the OpenBB ecosystem.&lt;/p&gt; 
&lt;p&gt;Data integrations available can be found here: &lt;a href="https://docs.openbb.co/platform/reference"&gt;https://docs.openbb.co/platform/reference&lt;/a&gt;&lt;/p&gt; 
&lt;hr&gt; 
&lt;h2&gt;OpenBB Workspace&lt;/h2&gt; 
&lt;p&gt;While the OpenBB Platform is all about an integration to dozens of different data vendors, the interface is either Python or a CLI.&lt;/p&gt; 
&lt;p&gt;If you want an enterprise UI to visualize this datasets and use AI agents on top, you can find OpenBB Workspace at &lt;a href="https://pro.openbb.co"&gt;https://pro.openbb.co&lt;/a&gt;.&lt;/p&gt; 
&lt;a href="https://pro.openbb.co"&gt; 
 &lt;div align="center"&gt; 
  &lt;img src="https://openbb-cms.directus.app/assets/f69b6aaf-0821-4bc8-a43c-715e03a924ef.png" alt="Logo" width="1000"&gt; 
 &lt;/div&gt; &lt;/a&gt; 
&lt;p&gt;Data integration:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;You can learn more about adding data to the OpenBB workspace from the &lt;a href="https://docs.openbb.co/workspace"&gt;docs&lt;/a&gt; or &lt;a href="https://github.com/OpenBB-finance/backends-for-openbb"&gt;this open source repository&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;AI Agents integration:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;You can learn more about adding AI agents to the OpenBB workspace from &lt;a href="https://github.com/OpenBB-finance/agents-for-openbb"&gt;this open source repository&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Integrating OpenBB Platform to the OpenBB Workspace&lt;/h3&gt; 
&lt;p&gt;Connect this library to the OpenBB Workspace with a few simple commands, in a Python (3.9.21 - 3.12) environment.&lt;/p&gt; 
&lt;h4&gt;Run OpenBB Platform backend&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Install the packages.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;pip install "openbb[all]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Start the API server over localhost.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;openbb-api
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will launch a FastAPI server, via Uvicorn, at &lt;code&gt;127.0.0.1:6900&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;You can check that it works by going to &lt;a href="http://127.0.0.1:6900"&gt;http://127.0.0.1:6900&lt;/a&gt;.&lt;/p&gt; 
&lt;h4&gt;Integrate OpenBB Platform backend to OpenBB Workspace&lt;/h4&gt; 
&lt;p&gt;Sign-in to the &lt;a href="https://pro.openbb.co/"&gt;OpenBB Workspace&lt;/a&gt;, and follow the following steps:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/75cffb4a-5e95-470a-b9d0-6ffd4067e069" alt="CleanShot 2025-05-17 at 09 51 56@2x"&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Go to the "Apps" tab&lt;/li&gt; 
 &lt;li&gt;Click on "Connect backend"&lt;/li&gt; 
 &lt;li&gt;Fill in the form with: Name: OpenBB Platform URL: &lt;a href="http://127.0.0.1:6900"&gt;http://127.0.0.1:6900&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Click on "Test". You should get a "Test successful" with the number of apps found.&lt;/li&gt; 
 &lt;li&gt;Click on "Add".&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;That's it.&lt;/p&gt; 
&lt;hr&gt; 
&lt;!-- TABLE OF CONTENTS --&gt; 
&lt;details closed="closed"&gt; 
 &lt;summary&gt;&lt;h2 style="display: inline-block"&gt;Table of Contents&lt;/h2&gt;&lt;/summary&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#1-installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#2-contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#3-license"&gt;License&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#4-disclaimer"&gt;Disclaimer&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#5-contacts"&gt;Contacts&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#6-star-history"&gt;Star History&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#7-contributors"&gt;Contributors&lt;/a&gt;&lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/details&gt; 
&lt;h2&gt;1. Installation&lt;/h2&gt; 
&lt;p&gt;The OpenBB Platform can be installed as a &lt;a href="https://pypi.org/project/openbb/"&gt;PyPI package&lt;/a&gt; by running &lt;code&gt;pip install openbb&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;or by cloning the repository directly with &lt;code&gt;git clone https://github.com/OpenBB-finance/OpenBB.git&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Please find more about the installation process, in the &lt;a href="https://docs.openbb.co/platform/installation"&gt;OpenBB Documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;OpenBB Platform CLI installation&lt;/h3&gt; 
&lt;p&gt;The OpenBB Platform CLI is a command-line interface that allows you to access the OpenBB Platform directly from your command line.&lt;/p&gt; 
&lt;p&gt;It can be installed by running &lt;code&gt;pip install openbb-cli&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;or by cloning the repository directly with &lt;code&gt;git clone https://github.com/OpenBB-finance/OpenBB.git&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Please find more about the installation process in the &lt;a href="https://docs.openbb.co/cli/installation"&gt;OpenBB Documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;2. Contributing&lt;/h2&gt; 
&lt;p&gt;There are three main ways of contributing to this project. (Hopefully you have starred the project by now â­ï¸)&lt;/p&gt; 
&lt;h3&gt;Become a Contributor&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;More information on our &lt;a href="https://docs.openbb.co/platform/developer_guide/misc/contributing"&gt;Contributing Documentation&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Create a GitHub ticket&lt;/h3&gt; 
&lt;p&gt;Before creating a ticket make sure the one you are creating doesn't exist already &lt;a href="https://github.com/OpenBB-finance/OpenBB/issues"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&amp;amp;labels=bug&amp;amp;template=bug_report.md&amp;amp;title=%5BBug%5D"&gt;Report bug&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&amp;amp;labels=enhancement&amp;amp;template=enhancement.md&amp;amp;title=%5BIMPROVE%5D"&gt;Suggest improvement&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&amp;amp;labels=new+feature&amp;amp;template=feature_request.md&amp;amp;title=%5BFR%5D"&gt;Request a feature&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Provide feedback&lt;/h3&gt; 
&lt;p&gt;We are most active on &lt;a href="https://openbb.co/discord"&gt;our Discord&lt;/a&gt;, but feel free to reach out to us in any of &lt;a href="https://openbb.co/links"&gt;our social media&lt;/a&gt; for feedback.&lt;/p&gt; 
&lt;h2&gt;3. License&lt;/h2&gt; 
&lt;p&gt;Distributed under the AGPLv3 License. See &lt;a href="https://github.com/OpenBB-finance/OpenBB/raw/main/LICENSE"&gt;LICENSE&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h2&gt;4. Disclaimer&lt;/h2&gt; 
&lt;p&gt;Trading in financial instruments involves high risks including the risk of losing some, or all, of your investment amount, and may not be suitable for all investors.&lt;/p&gt; 
&lt;p&gt;Before deciding to trade in a financial instrument you should be fully informed of the risks and costs associated with trading the financial markets, carefully consider your investment objectives, level of experience, and risk appetite, and seek professional advice where needed.&lt;/p&gt; 
&lt;p&gt;The data contained in the OpenBB Platform is not necessarily accurate.&lt;/p&gt; 
&lt;p&gt;OpenBB and any provider of the data contained in this website will not accept liability for any loss or damage as a result of your trading, or your reliance on the information displayed.&lt;/p&gt; 
&lt;p&gt;All names, logos, and brands of third parties that may be referenced in our sites, products or documentation are trademarks of their respective owners. Unless otherwise specified, OpenBB and its products and services are not endorsed by, sponsored by, or affiliated with these third parties.&lt;/p&gt; 
&lt;p&gt;Our use of these names, logos, and brands is for identification purposes only, and does not imply any such endorsement, sponsorship, or affiliation.&lt;/p&gt; 
&lt;h2&gt;5. Contacts&lt;/h2&gt; 
&lt;p&gt;If you have any questions about the platform or anything OpenBB, feel free to email us at &lt;code&gt;support@openbb.co&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;If you want to say hi, or are interested in partnering with us, feel free to reach us at &lt;code&gt;hello@openbb.co&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Any of our social media platforms: &lt;a href="https://openbb.co/links"&gt;openbb.co/links&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;6. Star History&lt;/h2&gt; 
&lt;p&gt;This is a proxy of our growth and that we are just getting started.&lt;/p&gt; 
&lt;p&gt;But for more metrics important to us check &lt;a href="https://openbb.co/open"&gt;openbb.co/open&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://api.star-history.com/svg?repos=openbb-finance/OpenBB&amp;amp;type=Date&amp;amp;theme=dark"&gt;&lt;img src="https://api.star-history.com/svg?repos=openbb-finance/OpenBB&amp;amp;type=Date&amp;amp;theme=dark" alt="Star History Chart"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;7. Contributors&lt;/h2&gt; 
&lt;p&gt;OpenBB wouldn't be OpenBB without you. If we are going to disrupt financial industry, every contribution counts. Thank you for being part of this journey.&lt;/p&gt; 
&lt;a href="https://github.com/OpenBB-finance/OpenBB/graphs/contributors"&gt; &lt;img src="https://contributors-img.web.app/image?repo=OpenBB-finance/OpenBB" width="800"&gt; &lt;/a&gt; 
&lt;!-- MARKDOWN LINKS &amp; IMAGES --&gt; 
&lt;!-- https://www.markdownguide.org/basic-syntax/#reference-style-links --&gt;</description>
    </item>
    
    <item>
      <title>frappe/hrms</title>
      <link>https://github.com/frappe/hrms</link>
      <description>&lt;p&gt;Open Source HR and Payroll Software&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;a href="https://frappe.io/hr"&gt; &lt;img src="https://raw.githubusercontent.com/frappe/hrms/develop/.github/frappe-hr-logo.png" height="80px" width="80px" alt="Frappe HR Logo"&gt; &lt;/a&gt; 
 &lt;h2&gt;Frappe HR&lt;/h2&gt; 
 &lt;p align="center"&gt; &lt;/p&gt;
 &lt;p&gt;Open Source, modern, and easy-to-use HR and Payroll Software&lt;/p&gt; 
 &lt;p&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/frappe/hrms/actions/workflows/ci.yml"&gt;&lt;img src="https://github.com/frappe/hrms/actions/workflows/ci.yml/badge.svg?branch=develop" alt="CI"&gt;&lt;/a&gt; &lt;a href="https://codecov.io/gh/frappe/hrms"&gt;&lt;img src="https://codecov.io/gh/frappe/hrms/branch/develop/graph/badge.svg?token=0TwvyUg3I5" alt="codecov"&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/10972" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/10972" alt="frappe%2Fhrms | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/frappe/hrms/develop/.github/hrms-hero.png"&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://frappe.io/hr"&gt;Website&lt;/a&gt; - 
 &lt;a href="https://docs.frappe.io/hr/introduction"&gt;Documentation&lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;Frappe HR&lt;/h2&gt; 
&lt;p&gt;Frappe HR has everything you need to drive excellence within the company. It's a complete HRMS solution with over 13 different modules right from Employee Management, Onboarding, Leaves, to Payroll, Taxation, and more!&lt;/p&gt; 
&lt;h2&gt;Motivation&lt;/h2&gt; 
&lt;p&gt;When Frappe team started growing in terms of size, we needed an open-source HR and Payroll software. We didn't find any "true" open-source HR software out there and so decided to build one ourselves. Initially, it was a set of modules within ERPNext but version 14 onwards, as the modules became more mature, Frappe HR was created as a separate product.&lt;/p&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Employee Lifecycle&lt;/strong&gt;: From onboarding employees, managing promotions and transfers, all the way to documenting feedback with exit interviews, make life easier for employees throughout their life cycle.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Leave and Attendance&lt;/strong&gt;: Configure leave policies, pull regional holidays with a click, check-in and check-out with geolocation capturing, track leave balances and attendance with reports.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Expense Claims and Advances&lt;/strong&gt;: Manage employee advances, claim expenses, configure multi-level approval workflows, all this with seamless integration with ERPNext accounting.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Performance Management&lt;/strong&gt;: Track goals, align goals with key result areas (KRAs), enable employees to evaluate themselves, make managing appraisal cycles easy.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Payroll &amp;amp; Taxation&lt;/strong&gt;: Create salary structures, configure income tax slabs, run standard payroll, accomodate additional salaries and off cycle payments, view income breakup on salary slips and so much more.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Frappe HR Mobile App&lt;/strong&gt;: Apply for and approve leaves on the go, check-in and check-out, access employee profile right from the mobile app.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details open&gt; 
 &lt;summary&gt;View Screenshots&lt;/summary&gt; 
 &lt;img src="https://raw.githubusercontent.com/frappe/hrms/develop/.github/hrms-appraisal.png"&gt; 
 &lt;img src="https://raw.githubusercontent.com/frappe/hrms/develop/.github/hrms-requisition.png"&gt; 
 &lt;img src="https://raw.githubusercontent.com/frappe/hrms/develop/.github/hrms-attendance.png"&gt; 
 &lt;img src="https://raw.githubusercontent.com/frappe/hrms/develop/.github/hrms-salary.png"&gt; 
 &lt;img src="https://raw.githubusercontent.com/frappe/hrms/develop/.github/hrms-pwa.png"&gt; 
&lt;/details&gt; 
&lt;h3&gt;Under the Hood&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/frappe/frappe"&gt;&lt;strong&gt;Frappe Framework&lt;/strong&gt;&lt;/a&gt;: A full-stack web application framework written in Python and Javascript. The framework provides a robust foundation for building web applications, including a database abstraction layer, user authentication, and a REST API.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/frappe/frappe-ui"&gt;&lt;strong&gt;Frappe UI&lt;/strong&gt;&lt;/a&gt;: A Vue-based UI library, to provide a modern user interface. The Frappe UI library provides a variety of components that can be used to build single-page applications on top of the Frappe Framework.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Production Setup&lt;/h2&gt; 
&lt;h3&gt;Managed Hosting&lt;/h3&gt; 
&lt;p&gt;You can try &lt;a href="https://frappecloud.com"&gt;Frappe Cloud&lt;/a&gt;, a simple, user-friendly and sophisticated &lt;a href="https://github.com/frappe/press"&gt;open-source&lt;/a&gt; platform to host Frappe applications with peace of mind.&lt;/p&gt; 
&lt;p&gt;It takes care of installation, setup, upgrades, monitoring, maintenance and support of your Frappe deployments. It is a fully featured developer platform with an ability to manage and control multiple Frappe deployments.&lt;/p&gt; 
&lt;div&gt; 
 &lt;a href="https://frappecloud.com/hrms/signup" target="_blank"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="https://frappe.io/files/try-on-fc-white.png"&gt; 
   &lt;img src="https://frappe.io/files/try-on-fc-black.png" alt="Try on Frappe Cloud" height="28"&gt; 
  &lt;/picture&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;Development setup&lt;/h2&gt; 
&lt;h3&gt;Docker&lt;/h3&gt; 
&lt;p&gt;You need Docker, docker-compose and git setup on your machine. Refer &lt;a href="https://docs.docker.com/"&gt;Docker documentation&lt;/a&gt;. After that, run the following commands:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;git clone https://github.com/frappe/hrms
cd hrms/docker
docker-compose up
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Wait for some time until the setup script creates a site. After that you can access &lt;code&gt;http://localhost:8000&lt;/code&gt; in your browser and the login screen for HR should show up.&lt;/p&gt; 
&lt;p&gt;Use the following credentials to log in:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Username: &lt;code&gt;Administrator&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Password: &lt;code&gt;admin&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Local&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Set up bench by following the &lt;a href="https://frappeframework.com/docs/user/en/installation"&gt;Installation Steps&lt;/a&gt; and start the server and keep it running &lt;pre&gt;&lt;code class="language-sh"&gt;$ bench start
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;In a separate terminal window, run the following commands &lt;pre&gt;&lt;code class="language-sh"&gt;$ bench new-site hrms.local
$ bench get-app erpnext
$ bench get-app hrms
$ bench --site hrms.local install-app hrms
$ bench --site hrms.local add-to-hosts
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;You can access the site at &lt;code&gt;http://hrms.local:8080&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Learning and Community&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://frappe.school"&gt;Frappe School&lt;/a&gt; - Learn Frappe Framework and ERPNext from the various courses by the maintainers or from the community.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.frappe.io/hr"&gt;Documentation&lt;/a&gt; - Extensive documentation for Frappe HR.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://discuss.erpnext.com/"&gt;User Forum&lt;/a&gt; - Engage with the community of ERPNext users and service providers.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://t.me/frappehr"&gt;Telegram Group&lt;/a&gt; - Get instant help from the community of users.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://github.com/frappe/erpnext/wiki/Issue-Guidelines"&gt;Issue Guidelines&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://erpnext.com/security"&gt;Report Security Vulnerabilities&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/frappe/erpnext/wiki/Contribution-Guidelines"&gt;Pull Request Requirements&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Logo and Trademark Policy&lt;/h2&gt; 
&lt;p&gt;Please read our &lt;a href="https://raw.githubusercontent.com/frappe/hrms/develop/TRADEMARK_POLICY.md"&gt;Logo and Trademark Policy&lt;/a&gt;.&lt;/p&gt; 
&lt;br&gt; 
&lt;br&gt; 
&lt;div align="center" style="padding-top: 0.75rem;"&gt; 
 &lt;a href="https://frappe.io" target="_blank"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="https://frappe.io/files/Frappe-white.png"&gt; 
   &lt;img src="https://frappe.io/files/Frappe-black.png" alt="Frappe Technologies" height="28"&gt; 
  &lt;/picture&gt; &lt;/a&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>FujiwaraChoki/MoneyPrinter</title>
      <link>https://github.com/FujiwaraChoki/MoneyPrinter</link>
      <description>&lt;p&gt;Automate Creation of YouTube Shorts using MoviePy.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MoneyPrinter ğŸ’¸&lt;/h1&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;â™¥ï¸ Sponsor: The Best AI Chat App: &lt;a href="https://www.shiori.ai"&gt;shiori.ai&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ğ• Also, follow me on X: &lt;a href="https://x.com/DevBySami"&gt;@DevBySami&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Automate the creation of YouTube Shorts, simply by providing a video topic to talk about.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://trendshift.io/repositories/7545" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/7545" alt="FujiwaraChoki%2FMoneyPrinter | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt; Please make sure you look through existing/closed issues before opening your own. If it's just a question, please join our &lt;a href="https://dsc.gg/fuji-community"&gt;discord&lt;/a&gt; and ask there.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;ğŸ¥&lt;/strong&gt; Watch the video on &lt;a href="https://youtu.be/mkZsaDA2JnA?si=pNne3MnluRVkWQbE"&gt;YouTube&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Check out the instructions for the local version &lt;a href="https://raw.githubusercontent.com/FujiwaraChoki/MoneyPrinter/main/Local.md"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;FAQ ğŸ¤”&lt;/h2&gt; 
&lt;h3&gt;How do I get the TikTok session ID?&lt;/h3&gt; 
&lt;p&gt;You can obtain your TikTok session ID by logging into TikTok in your browser and copying the value of the &lt;code&gt;sessionid&lt;/code&gt; cookie.&lt;/p&gt; 
&lt;h3&gt;My ImageMagick binary is not being detected&lt;/h3&gt; 
&lt;p&gt;Make sure you set your path to the ImageMagick binary correctly in the &lt;code&gt;.env&lt;/code&gt; file, it should look something like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-env"&gt;IMAGEMAGICK_BINARY="C:\\Program Files\\ImageMagick-7.1.0-Q16\\magick.exe"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Don't forget to use double backslashes (&lt;code&gt;\\&lt;/code&gt;) in the path, instead of one.&lt;/p&gt; 
&lt;h3&gt;I can't install &lt;code&gt;playsound&lt;/code&gt;: Wheel failed to build&lt;/h3&gt; 
&lt;p&gt;If you're having trouble installing &lt;code&gt;playsound&lt;/code&gt;, you can try installing it using the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -U wheel
pip install -U playsound
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you were not able to find your solution, please ask in the discord or create a new issue, so that the community can help you.&lt;/p&gt; 
&lt;h2&gt;Donate ğŸ&lt;/h2&gt; 
&lt;p&gt;If you like and enjoy &lt;code&gt;MoneyPrinter&lt;/code&gt;, and would like to donate, you can do that by clicking on the button on the right hand side of the repository. â¤ï¸ You will have your name (and/or logo) added to this repository as a supporter as a sign of appreciation.&lt;/p&gt; 
&lt;h2&gt;Contributing ğŸ¤&lt;/h2&gt; 
&lt;p&gt;Pull Requests will not be accepted for the time-being.&lt;/p&gt; 
&lt;h2&gt;Star History ğŸŒŸ&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#FujiwaraChoki/MoneyPrinter&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=FujiwaraChoki/MoneyPrinter&amp;amp;type=Date" alt="Star History Chart"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;License ğŸ“&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/FujiwaraChoki/MoneyPrinter/main/LICENSE"&gt;&lt;code&gt;LICENSE&lt;/code&gt;&lt;/a&gt; file for more information.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>googleapis/python-genai</title>
      <link>https://github.com/googleapis/python-genai</link>
      <description>&lt;p&gt;Google Gen AI Python SDK provides an interface for developers to integrate Google's generative models into their Python applications.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Google Gen AI SDK&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://pypi.org/project/google-genai/"&gt;&lt;img src="https://img.shields.io/pypi/v/google-genai.svg?sanitize=true" alt="PyPI version"&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/pypi/pyversions/google-genai" alt="Python support"&gt; &lt;a href="https://pypistats.org/packages/google-genai"&gt;&lt;img src="https://img.shields.io/pypi/dw/google-genai" alt="PyPI - Downloads"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;hr&gt; 
&lt;p&gt;&lt;strong&gt;Documentation:&lt;/strong&gt; &lt;a href="https://googleapis.github.io/python-genai/"&gt;https://googleapis.github.io/python-genai/&lt;/a&gt;&lt;/p&gt; 
&lt;hr&gt; 
&lt;p&gt;Google Gen AI Python SDK provides an interface for developers to integrate Google's generative models into their Python applications. It supports the &lt;a href="https://ai.google.dev/gemini-api/docs"&gt;Gemini Developer API&lt;/a&gt; and &lt;a href="https://cloud.google.com/vertex-ai/generative-ai/docs/learn/overview"&gt;Vertex AI&lt;/a&gt; APIs.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;pip install google-genai
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Imports&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from google import genai
from google.genai import types
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Create a client&lt;/h2&gt; 
&lt;p&gt;Please run one of the following code blocks to create a client for different services (&lt;a href="https://ai.google.dev/gemini-api/docs"&gt;Gemini Developer API&lt;/a&gt; or &lt;a href="https://cloud.google.com/vertex-ai/generative-ai/docs/learn/overview"&gt;Vertex AI&lt;/a&gt;).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from google import genai

# Only run this block for Gemini Developer API
client = genai.Client(api_key='GEMINI_API_KEY')
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from google import genai

# Only run this block for Vertex AI API
client = genai.Client(
    vertexai=True, project='your-project-id', location='us-central1'
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;(Optional) Using environment variables:&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;You can create a client by configuring the necessary environment variables. Configuration setup instructions depends on whether you're using the Gemini Developer API or the Gemini API in Vertex AI.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Gemini Developer API:&lt;/strong&gt; Set the &lt;code&gt;GEMINI_API_KEY&lt;/code&gt; or &lt;code&gt;GOOGLE_API_KEY&lt;/code&gt;. It will automatically be picked up by the client. It's recommended that you set only one of those variables, but if both are set, &lt;code&gt;GOOGLE_API_KEY&lt;/code&gt; takes precedence.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export GEMINI_API_KEY='your-api-key'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Gemini API on Vertex AI:&lt;/strong&gt; Set &lt;code&gt;GOOGLE_GENAI_USE_VERTEXAI&lt;/code&gt;, &lt;code&gt;GOOGLE_CLOUD_PROJECT&lt;/code&gt; and &lt;code&gt;GOOGLE_CLOUD_LOCATION&lt;/code&gt;, as shown below:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export GOOGLE_GENAI_USE_VERTEXAI=true
export GOOGLE_CLOUD_PROJECT='your-project-id'
export GOOGLE_CLOUD_LOCATION='us-central1'
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from google import genai

client = genai.Client()
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;API Selection&lt;/h3&gt; 
&lt;p&gt;By default, the SDK uses the beta API endpoints provided by Google to support preview features in the APIs. The stable API endpoints can be selected by setting the API version to &lt;code&gt;v1&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;To set the API version use &lt;code&gt;http_options&lt;/code&gt;. For example, to set the API version to &lt;code&gt;v1&lt;/code&gt; for Vertex AI:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from google import genai
from google.genai import types

client = genai.Client(
    vertexai=True,
    project='your-project-id',
    location='us-central1',
    http_options=types.HttpOptions(api_version='v1')
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To set the API version to &lt;code&gt;v1alpha&lt;/code&gt; for the Gemini Developer API:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from google import genai
from google.genai import types

client = genai.Client(
    api_key='GEMINI_API_KEY',
    http_options=types.HttpOptions(api_version='v1alpha')
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Faster async client option: Aiohttp&lt;/h3&gt; 
&lt;p&gt;By default we use httpx for both sync and async client implementations. In order to have faster performance, you may install &lt;code&gt;google-genai[aiohttp]&lt;/code&gt;. In Gen AI SDK we configure &lt;code&gt;trust_env=True&lt;/code&gt; to match with the default behavior of httpx. Additional args of &lt;code&gt;aiohttp.ClientSession.request()&lt;/code&gt; (&lt;a href="https://github.com/aio-libs/aiohttp/raw/v3.12.13/aiohttp/client.py#L170"&gt;see _RequestOptions args&lt;/a&gt;) can be passed through the following way:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;
http_options = types.HttpOptions(
    async_client_args={'cookies': ..., 'ssl': ...},
)

client=Client(..., http_options=http_options)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Proxy&lt;/h3&gt; 
&lt;p&gt;Both httpx and aiohttp libraries use &lt;code&gt;urllib.request.getproxies&lt;/code&gt; from environment variables. Before client initialization, you may set proxy (and optional SSL_CERT_FILE) by setting the environment variables:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export HTTPS_PROXY='http://username:password@proxy_uri:port'
export SSL_CERT_FILE='client.pem'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you need &lt;code&gt;socks5&lt;/code&gt; proxy, httpx &lt;a href="https://www.python-httpx.org/advanced/proxies/#socks"&gt;supports&lt;/a&gt; &lt;code&gt;socks5&lt;/code&gt; proxy if you pass it via args to &lt;code&gt;httpx.Client()&lt;/code&gt;. You may install &lt;code&gt;httpx[socks]&lt;/code&gt; to use it. Then, you can pass it through the following way:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;
http_options = types.HttpOptions(
    client_args={'proxy': 'socks5://user:pass@host:port'},
    async_client_args={'proxy': 'socks5://user:pass@host:port'},,
)

client=Client(..., http_options=http_options)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Types&lt;/h2&gt; 
&lt;p&gt;Parameter types can be specified as either dictionaries(&lt;code&gt;TypedDict&lt;/code&gt;) or &lt;a href="https://pydantic.readthedocs.io/en/stable/model.html"&gt;Pydantic Models&lt;/a&gt;. Pydantic model types are available in the &lt;code&gt;types&lt;/code&gt; module.&lt;/p&gt; 
&lt;h2&gt;Models&lt;/h2&gt; 
&lt;p&gt;The &lt;code&gt;client.models&lt;/code&gt; module exposes model inferencing and model getters. See the 'Create a client' section above to initialize a client.&lt;/p&gt; 
&lt;h3&gt;Generate Content&lt;/h3&gt; 
&lt;h4&gt;with text content&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;response = client.models.generate_content(
    model='gemini-2.0-flash-001', contents='Why is the sky blue?'
)
print(response.text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;with uploaded file (Gemini Developer API only)&lt;/h4&gt; 
&lt;p&gt;download the file in console.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;!wget -q https://storage.googleapis.com/generativeai-downloads/data/a11.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;python code.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;file = client.files.upload(file='a11.txt')
response = client.models.generate_content(
    model='gemini-2.0-flash-001',
    contents=['Could you summarize this file?', file]
)
print(response.text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;How to structure &lt;code&gt;contents&lt;/code&gt; argument for &lt;code&gt;generate_content&lt;/code&gt;&lt;/h4&gt; 
&lt;p&gt;The SDK always converts the inputs to the &lt;code&gt;contents&lt;/code&gt; argument into &lt;code&gt;list[types.Content]&lt;/code&gt;. The following shows some common ways to provide your inputs.&lt;/p&gt; 
&lt;h5&gt;Provide a &lt;code&gt;list[types.Content]&lt;/code&gt;&lt;/h5&gt; 
&lt;p&gt;This is the canonical way to provide contents, SDK will not do any conversion.&lt;/p&gt; 
&lt;h5&gt;Provide a &lt;code&gt;types.Content&lt;/code&gt; instance&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from google.genai import types

contents = types.Content(
  role='user',
  parts=[types.Part.from_text(text='Why is the sky blue?')]
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;SDK converts this to&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;[
  types.Content(
    role='user',
    parts=[types.Part.from_text(text='Why is the sky blue?')]
  )
]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;Provide a string&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;contents='Why is the sky blue?'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The SDK will assume this is a text part, and it converts this into the following:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;[
  types.UserContent(
    parts=[
      types.Part.from_text(text='Why is the sky blue?')
    ]
  )
]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Where a &lt;code&gt;types.UserContent&lt;/code&gt; is a subclass of &lt;code&gt;types.Content&lt;/code&gt;, it sets the &lt;code&gt;role&lt;/code&gt; field to be &lt;code&gt;user&lt;/code&gt;.&lt;/p&gt; 
&lt;h5&gt;Provide a list of string&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;contents=['Why is the sky blue?', 'Why is the cloud white?']
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The SDK assumes these are 2 text parts, it converts this into a single content, like the following:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;[
  types.UserContent(
    parts=[
      types.Part.from_text(text='Why is the sky blue?'),
      types.Part.from_text(text='Why is the cloud white?'),
    ]
  )
]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Where a &lt;code&gt;types.UserContent&lt;/code&gt; is a subclass of &lt;code&gt;types.Content&lt;/code&gt;, the &lt;code&gt;role&lt;/code&gt; field in &lt;code&gt;types.UserContent&lt;/code&gt; is fixed to be &lt;code&gt;user&lt;/code&gt;.&lt;/p&gt; 
&lt;h5&gt;Provide a function call part&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from google.genai import types

contents = types.Part.from_function_call(
  name='get_weather_by_location',
  args={'location': 'Boston'}
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The SDK converts a function call part to a content with a &lt;code&gt;model&lt;/code&gt; role:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;[
  types.ModelContent(
    parts=[
      types.Part.from_function_call(
        name='get_weather_by_location',
        args={'location': 'Boston'}
      )
    ]
  )
]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Where a &lt;code&gt;types.ModelContent&lt;/code&gt; is a subclass of &lt;code&gt;types.Content&lt;/code&gt;, the &lt;code&gt;role&lt;/code&gt; field in &lt;code&gt;types.ModelContent&lt;/code&gt; is fixed to be &lt;code&gt;model&lt;/code&gt;.&lt;/p&gt; 
&lt;h5&gt;Provide a list of function call parts&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from google.genai import types

contents = [
  types.Part.from_function_call(
    name='get_weather_by_location',
    args={'location': 'Boston'}
  ),
  types.Part.from_function_call(
    name='get_weather_by_location',
    args={'location': 'New York'}
  ),
]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The SDK converts a list of function call parts to the a content with a &lt;code&gt;model&lt;/code&gt; role:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;[
  types.ModelContent(
    parts=[
      types.Part.from_function_call(
        name='get_weather_by_location',
        args={'location': 'Boston'}
      ),
      types.Part.from_function_call(
        name='get_weather_by_location',
        args={'location': 'New York'}
      )
    ]
  )
]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Where a &lt;code&gt;types.ModelContent&lt;/code&gt; is a subclass of &lt;code&gt;types.Content&lt;/code&gt;, the &lt;code&gt;role&lt;/code&gt; field in &lt;code&gt;types.ModelContent&lt;/code&gt; is fixed to be &lt;code&gt;model&lt;/code&gt;.&lt;/p&gt; 
&lt;h5&gt;Provide a non function call part&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from google.genai import types

contents = types.Part.from_uri(
  file_uri: 'gs://generativeai-downloads/images/scones.jpg',
  mime_type: 'image/jpeg',
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The SDK converts all non function call parts into a content with a &lt;code&gt;user&lt;/code&gt; role.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;[
  types.UserContent(parts=[
    types.Part.from_uri(
     file_uri: 'gs://generativeai-downloads/images/scones.jpg',
      mime_type: 'image/jpeg',
    )
  ])
]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;Provide a list of non function call parts&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from google.genai import types

contents = [
  types.Part.from_text('What is this image about?'),
  types.Part.from_uri(
    file_uri: 'gs://generativeai-downloads/images/scones.jpg',
    mime_type: 'image/jpeg',
  )
]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The SDK will convert the list of parts into a content with a &lt;code&gt;user&lt;/code&gt; role&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;[
  types.UserContent(
    parts=[
      types.Part.from_text('What is this image about?'),
      types.Part.from_uri(
        file_uri: 'gs://generativeai-downloads/images/scones.jpg',
        mime_type: 'image/jpeg',
      )
    ]
  )
]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;Mix types in contents&lt;/h5&gt; 
&lt;p&gt;You can also provide a list of &lt;code&gt;types.ContentUnion&lt;/code&gt;. The SDK leaves items of &lt;code&gt;types.Content&lt;/code&gt; as is, it groups consecutive non function call parts into a single &lt;code&gt;types.UserContent&lt;/code&gt;, and it groups consecutive function call parts into a single &lt;code&gt;types.ModelContent&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;If you put a list within a list, the inner list can only contain &lt;code&gt;types.PartUnion&lt;/code&gt; items. The SDK will convert the inner list into a single &lt;code&gt;types.UserContent&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;System Instructions and Other Configs&lt;/h3&gt; 
&lt;p&gt;The output of the model can be influenced by several optional settings available in generate_content's config parameter. For example, increasing &lt;code&gt;max_output_tokens&lt;/code&gt; is essential for longer model responses. To make a model more deterministic, lowering the &lt;code&gt;temperature&lt;/code&gt; parameter reduces randomness, with values near 0 minimizing variability. Capabilities and parameter defaults for each model is shown in the &lt;a href="https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-flash"&gt;Vertex AI docs&lt;/a&gt; and &lt;a href="https://ai.google.dev/gemini-api/docs/models"&gt;Gemini API docs&lt;/a&gt; respectively.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from google.genai import types

response = client.models.generate_content(
    model='gemini-2.0-flash-001',
    contents='high',
    config=types.GenerateContentConfig(
        system_instruction='I say high, you say low',
        max_output_tokens=3,
        temperature=0.3,
    ),
)
print(response.text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Typed Config&lt;/h3&gt; 
&lt;p&gt;All API methods support Pydantic types for parameters as well as dictionaries. You can get the type from &lt;code&gt;google.genai.types&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from google.genai import types

response = client.models.generate_content(
    model='gemini-2.0-flash-001',
    contents=types.Part.from_text(text='Why is the sky blue?'),
    config=types.GenerateContentConfig(
        temperature=0,
        top_p=0.95,
        top_k=20,
        candidate_count=1,
        seed=5,
        max_output_tokens=100,
        stop_sequences=['STOP!'],
        presence_penalty=0.0,
        frequency_penalty=0.0,
    ),
)

print(response.text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;List Base Models&lt;/h3&gt; 
&lt;p&gt;To retrieve tuned models, see &lt;a href="https://raw.githubusercontent.com/googleapis/python-genai/main/#list-tuned-models"&gt;list tuned models&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;for model in client.models.list():
    print(model)
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;pager = client.models.list(config={'page_size': 10})
print(pager.page_size)
print(pager[0])
pager.next_page()
print(pager[0])
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;List Base Models (Asynchronous)&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;async for job in await client.aio.models.list():
    print(job)
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;async_pager = await client.aio.models.list(config={'page_size': 10})
print(async_pager.page_size)
print(async_pager[0])
await async_pager.next_page()
print(async_pager[0])
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Safety Settings&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from google.genai import types

response = client.models.generate_content(
    model='gemini-2.0-flash-001',
    contents='Say something bad.',
    config=types.GenerateContentConfig(
        safety_settings=[
            types.SafetySetting(
                category='HARM_CATEGORY_HATE_SPEECH',
                threshold='BLOCK_ONLY_HIGH',
            )
        ]
    ),
)
print(response.text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Function Calling&lt;/h3&gt; 
&lt;h4&gt;Automatic Python function Support&lt;/h4&gt; 
&lt;p&gt;You can pass a Python function directly and it will be automatically called and responded by default.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from google.genai import types

def get_current_weather(location: str) -&amp;gt; str:
    """Returns the current weather.

    Args:
      location: The city and state, e.g. San Francisco, CA
    """
    return 'sunny'


response = client.models.generate_content(
    model='gemini-2.0-flash-001',
    contents='What is the weather like in Boston?',
    config=types.GenerateContentConfig(tools=[get_current_weather]),
)

print(response.text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Disabling automatic function calling&lt;/h4&gt; 
&lt;p&gt;If you pass in a python function as a tool directly, and do not want automatic function calling, you can disable automatic function calling as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from google.genai import types

response = client.models.generate_content(
  model='gemini-2.0-flash-001',
  contents='What is the weather like in Boston?',
  config=types.GenerateContentConfig(
    tools=[get_current_weather],
    automatic_function_calling=types.AutomaticFunctionCallingConfig(
      disable=True
    ),
  ),
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;With automatic function calling disabled, you will get a list of function call parts in the response:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;function_calls: Optional[List[types.FunctionCall]] = response.function_calls
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Manually declare and invoke a function for function calling&lt;/h4&gt; 
&lt;p&gt;If you don't want to use the automatic function support, you can manually declare the function and invoke it.&lt;/p&gt; 
&lt;p&gt;The following example shows how to declare a function and pass it as a tool. Then you will receive a function call part in the response.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from google.genai import types

function = types.FunctionDeclaration(
    name='get_current_weather',
    description='Get the current weather in a given location',
    parameters_json_schema={
        'type': 'object',
        'properties': {
            'location': {
                'type': 'string',
                'description': 'The city and state, e.g. San Francisco, CA',
            }
        },
        'required': ['location'],
    },
)

tool = types.Tool(function_declarations=[function])

response = client.models.generate_content(
    model='gemini-2.0-flash-001',
    contents='What is the weather like in Boston?',
    config=types.GenerateContentConfig(tools=[tool]),
)

print(response.function_calls[0])
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After you receive the function call part from the model, you can invoke the function and get the function response. And then you can pass the function response to the model. The following example shows how to do it for a simple function invocation.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from google.genai import types

user_prompt_content = types.Content(
    role='user',
    parts=[types.Part.from_text(text='What is the weather like in Boston?')],
)
function_call_part = response.function_calls[0]
function_call_content = response.candidates[0].content


try:
    function_result = get_current_weather(
        **function_call_part.function_call.args
    )
    function_response = {'result': function_result}
except (
    Exception
) as e:  # instead of raising the exception, you can let the model handle it
    function_response = {'error': str(e)}


function_response_part = types.Part.from_function_response(
    name=function_call_part.name,
    response=function_response,
)
function_response_content = types.Content(
    role='tool', parts=[function_response_part]
)

response = client.models.generate_content(
    model='gemini-2.0-flash-001',
    contents=[
        user_prompt_content,
        function_call_content,
        function_response_content,
    ],
    config=types.GenerateContentConfig(
        tools=[tool],
    ),
)

print(response.text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Function calling with &lt;code&gt;ANY&lt;/code&gt; tools config mode&lt;/h4&gt; 
&lt;p&gt;If you configure function calling mode to be &lt;code&gt;ANY&lt;/code&gt;, then the model will always return function call parts. If you also pass a python function as a tool, by default the SDK will perform automatic function calling until the remote calls exceed the maximum remote call for automatic function calling (default to 10 times).&lt;/p&gt; 
&lt;p&gt;If you'd like to disable automatic function calling in &lt;code&gt;ANY&lt;/code&gt; mode:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from google.genai import types

def get_current_weather(location: str) -&amp;gt; str:
    """Returns the current weather.

    Args:
      location: The city and state, e.g. San Francisco, CA
    """
    return "sunny"

response = client.models.generate_content(
    model="gemini-2.0-flash-001",
    contents="What is the weather like in Boston?",
    config=types.GenerateContentConfig(
        tools=[get_current_weather],
        automatic_function_calling=types.AutomaticFunctionCallingConfig(
            disable=True
        ),
        tool_config=types.ToolConfig(
            function_calling_config=types.FunctionCallingConfig(mode='ANY')
        ),
    ),
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you'd like to set &lt;code&gt;x&lt;/code&gt; number of automatic function call turns, you can configure the maximum remote calls to be &lt;code&gt;x + 1&lt;/code&gt;. Assuming you prefer &lt;code&gt;1&lt;/code&gt; turn for automatic function calling.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from google.genai import types

def get_current_weather(location: str) -&amp;gt; str:
    """Returns the current weather.

    Args:
      location: The city and state, e.g. San Francisco, CA
    """
    return "sunny"

response = client.models.generate_content(
    model="gemini-2.0-flash-001",
    contents="What is the weather like in Boston?",
    config=types.GenerateContentConfig(
        tools=[get_current_weather],
        automatic_function_calling=types.AutomaticFunctionCallingConfig(
            maximum_remote_calls=2
        ),
        tool_config=types.ToolConfig(
            function_calling_config=types.FunctionCallingConfig(mode='ANY')
        ),
    ),
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Model Context Protocol (MCP) support (experimental)&lt;/h4&gt; 
&lt;p&gt;Built-in &lt;a href="https://modelcontextprotocol.io/introduction"&gt;MCP&lt;/a&gt; support is an experimental feature. You can pass a local MCP server as a tool directly.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import os
import asyncio
from datetime import datetime
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client
from google import genai

client = genai.Client()

# Create server parameters for stdio connection
server_params = StdioServerParameters(
    command="npx",  # Executable
    args=["-y", "@philschmid/weather-mcp"],  # MCP Server
    env=None,  # Optional environment variables
)

async def run():
    async with stdio_client(server_params) as (read, write):
        async with ClientSession(read, write) as session:
            # Prompt to get the weather for the current day in London.
            prompt = f"What is the weather in London in {datetime.now().strftime('%Y-%m-%d')}?"

            # Initialize the connection between client and server
            await session.initialize()

            # Send request to the model with MCP function declarations
            response = await client.aio.models.generate_content(
                model="gemini-2.5-flash",
                contents=prompt,
                config=genai.types.GenerateContentConfig(
                    temperature=0,
                    tools=[session],  # uses the session, will automatically call the tool using automatic function calling
                ),
            )
            print(response.text)

# Start the asyncio event loop and run the main function
asyncio.run(run())
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;JSON Response Schema&lt;/h3&gt; 
&lt;p&gt;However you define your schema, don't duplicate it in your input prompt, including by giving examples of expected JSON output. If you do, the generated output might be lower in quality.&lt;/p&gt; 
&lt;h4&gt;JSON Schema support&lt;/h4&gt; 
&lt;p&gt;Schemas can be provided as standard JSON schema.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;user_profile = {
    'properties': {
        'age': {
            'anyOf': [
                {'maximum': 20, 'minimum': 0, 'type': 'integer'},
                {'type': 'null'},
            ],
            'title': 'Age',
        },
        'username': {
            'description': "User's unique name",
            'title': 'Username',
            'type': 'string',
        },
    },
    'required': ['username', 'age'],
    'title': 'User Schema',
    'type': 'object',
}

response = client.models.generate_content(
    model='gemini-2.0-flash',
    contents='Give me information of the United States.',
    config={
        'response_mime_type': 'application/json',
        'response_json_schema': userProfile
    },
)
print(response.parsed)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Pydantic Model Schema support&lt;/h4&gt; 
&lt;p&gt;Schemas can be provided as Pydantic Models.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from pydantic import BaseModel
from google.genai import types


class CountryInfo(BaseModel):
    name: str
    population: int
    capital: str
    continent: str
    gdp: int
    official_language: str
    total_area_sq_mi: int


response = client.models.generate_content(
    model='gemini-2.0-flash-001',
    contents='Give me information for the United States.',
    config=types.GenerateContentConfig(
        response_mime_type='application/json',
        response_schema=CountryInfo,
    ),
)
print(response.text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from google.genai import types

response = client.models.generate_content(
    model='gemini-2.0-flash-001',
    contents='Give me information for the United States.',
    config=types.GenerateContentConfig(
        response_mime_type='application/json',
        response_schema={
            'required': [
                'name',
                'population',
                'capital',
                'continent',
                'gdp',
                'official_language',
                'total_area_sq_mi',
            ],
            'properties': {
                'name': {'type': 'STRING'},
                'population': {'type': 'INTEGER'},
                'capital': {'type': 'STRING'},
                'continent': {'type': 'STRING'},
                'gdp': {'type': 'INTEGER'},
                'official_language': {'type': 'STRING'},
                'total_area_sq_mi': {'type': 'INTEGER'},
            },
            'type': 'OBJECT',
        },
    ),
)
print(response.text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Enum Response Schema&lt;/h3&gt; 
&lt;h4&gt;Text Response&lt;/h4&gt; 
&lt;p&gt;You can set response_mime_type to 'text/x.enum' to return one of those enum values as the response.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;class InstrumentEnum(Enum):
  PERCUSSION = 'Percussion'
  STRING = 'String'
  WOODWIND = 'Woodwind'
  BRASS = 'Brass'
  KEYBOARD = 'Keyboard'

response = client.models.generate_content(
      model='gemini-2.0-flash-001',
      contents='What instrument plays multiple notes at once?',
      config={
          'response_mime_type': 'text/x.enum',
          'response_schema': InstrumentEnum,
      },
  )
print(response.text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;JSON Response&lt;/h4&gt; 
&lt;p&gt;You can also set response_mime_type to 'application/json', the response will be identical but in quotes.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from enum import Enum

class InstrumentEnum(Enum):
  PERCUSSION = 'Percussion'
  STRING = 'String'
  WOODWIND = 'Woodwind'
  BRASS = 'Brass'
  KEYBOARD = 'Keyboard'

response = client.models.generate_content(
      model='gemini-2.0-flash-001',
      contents='What instrument plays multiple notes at once?',
      config={
          'response_mime_type': 'application/json',
          'response_schema': InstrumentEnum,
      },
  )
print(response.text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Generate Content (Synchronous Streaming)&lt;/h3&gt; 
&lt;p&gt;Generate content in a streaming format so that the model outputs streams back to you, rather than being returned as one chunk.&lt;/p&gt; 
&lt;h4&gt;Streaming for text content&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;for chunk in client.models.generate_content_stream(
    model='gemini-2.0-flash-001', contents='Tell me a story in 300 words.'
):
    print(chunk.text, end='')
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Streaming for image content&lt;/h4&gt; 
&lt;p&gt;If your image is stored in &lt;a href="https://cloud.google.com/storage"&gt;Google Cloud Storage&lt;/a&gt;, you can use the &lt;code&gt;from_uri&lt;/code&gt; class method to create a &lt;code&gt;Part&lt;/code&gt; object.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from google.genai import types

for chunk in client.models.generate_content_stream(
    model='gemini-2.0-flash-001',
    contents=[
        'What is this image about?',
        types.Part.from_uri(
            file_uri='gs://generativeai-downloads/images/scones.jpg',
            mime_type='image/jpeg',
        ),
    ],
):
    print(chunk.text, end='')
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If your image is stored in your local file system, you can read it in as bytes data and use the &lt;code&gt;from_bytes&lt;/code&gt; class method to create a &lt;code&gt;Part&lt;/code&gt; object.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from google.genai import types

YOUR_IMAGE_PATH = 'your_image_path'
YOUR_IMAGE_MIME_TYPE = 'your_image_mime_type'
with open(YOUR_IMAGE_PATH, 'rb') as f:
    image_bytes = f.read()

for chunk in client.models.generate_content_stream(
    model='gemini-2.0-flash-001',
    contents=[
        'What is this image about?',
        types.Part.from_bytes(data=image_bytes, mime_type=YOUR_IMAGE_MIME_TYPE),
    ],
):
    print(chunk.text, end='')
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Generate Content (Asynchronous Non Streaming)&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;client.aio&lt;/code&gt; exposes all the analogous &lt;a href="https://docs.python.org/3/library/asyncio.html"&gt;&lt;code&gt;async&lt;/code&gt; methods&lt;/a&gt; that are available on &lt;code&gt;client&lt;/code&gt;. Note that it applies to all the modules.&lt;/p&gt; 
&lt;p&gt;For example, &lt;code&gt;client.aio.models.generate_content&lt;/code&gt; is the &lt;code&gt;async&lt;/code&gt; version of &lt;code&gt;client.models.generate_content&lt;/code&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;response = await client.aio.models.generate_content(
    model='gemini-2.0-flash-001', contents='Tell me a story in 300 words.'
)

print(response.text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Generate Content (Asynchronous Streaming)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;async for chunk in await client.aio.models.generate_content_stream(
    model='gemini-2.0-flash-001', contents='Tell me a story in 300 words.'
):
    print(chunk.text, end='')
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Count Tokens and Compute Tokens&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;response = client.models.count_tokens(
    model='gemini-2.0-flash-001',
    contents='why is the sky blue?',
)
print(response)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Compute Tokens&lt;/h4&gt; 
&lt;p&gt;Compute tokens is only supported in Vertex AI.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;response = client.models.compute_tokens(
    model='gemini-2.0-flash-001',
    contents='why is the sky blue?',
)
print(response)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;Async&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;response = await client.aio.models.count_tokens(
    model='gemini-2.0-flash-001',
    contents='why is the sky blue?',
)
print(response)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Embed Content&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;response = client.models.embed_content(
    model='text-embedding-004',
    contents='why is the sky blue?',
)
print(response)
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from google.genai import types

# multiple contents with config
response = client.models.embed_content(
    model='text-embedding-004',
    contents=['why is the sky blue?', 'What is your age?'],
    config=types.EmbedContentConfig(output_dimensionality=10),
)

print(response)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Imagen&lt;/h3&gt; 
&lt;h4&gt;Generate Images&lt;/h4&gt; 
&lt;p&gt;Support for generate images in Gemini Developer API is behind an allowlist&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from google.genai import types

# Generate Image
response1 = client.models.generate_images(
    model='imagen-3.0-generate-002',
    prompt='An umbrella in the foreground, and a rainy night sky in the background',
    config=types.GenerateImagesConfig(
        number_of_images=1,
        include_rai_reason=True,
        output_mime_type='image/jpeg',
    ),
)
response1.generated_images[0].image.show()
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Upscale Image&lt;/h4&gt; 
&lt;p&gt;Upscale image is only supported in Vertex AI.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from google.genai import types

# Upscale the generated image from above
response2 = client.models.upscale_image(
    model='imagen-3.0-generate-001',
    image=response1.generated_images[0].image,
    upscale_factor='x2',
    config=types.UpscaleImageConfig(
        include_rai_reason=True,
        output_mime_type='image/jpeg',
    ),
)
response2.generated_images[0].image.show()
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Edit Image&lt;/h4&gt; 
&lt;p&gt;Edit image uses a separate model from generate and upscale.&lt;/p&gt; 
&lt;p&gt;Edit image is only supported in Vertex AI.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Edit the generated image from above
from google.genai import types
from google.genai.types import RawReferenceImage, MaskReferenceImage

raw_ref_image = RawReferenceImage(
    reference_id=1,
    reference_image=response1.generated_images[0].image,
)

# Model computes a mask of the background
mask_ref_image = MaskReferenceImage(
    reference_id=2,
    config=types.MaskReferenceConfig(
        mask_mode='MASK_MODE_BACKGROUND',
        mask_dilation=0,
    ),
)

response3 = client.models.edit_image(
    model='imagen-3.0-capability-001',
    prompt='Sunlight and clear sky',
    reference_images=[raw_ref_image, mask_ref_image],
    config=types.EditImageConfig(
        edit_mode='EDIT_MODE_INPAINT_INSERTION',
        number_of_images=1,
        include_rai_reason=True,
        output_mime_type='image/jpeg',
    ),
)
response3.generated_images[0].image.show()
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Veo&lt;/h3&gt; 
&lt;p&gt;Support for generating videos is considered public preview&lt;/p&gt; 
&lt;h4&gt;Generate Videos (Text to Video)&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from google.genai import types

# Create operation
operation = client.models.generate_videos(
    model='veo-2.0-generate-001',
    prompt='A neon hologram of a cat driving at top speed',
    config=types.GenerateVideosConfig(
        number_of_videos=1,
        duration_seconds=5,
        enhance_prompt=True,
    ),
)

# Poll operation
while not operation.done:
    time.sleep(20)
    operation = client.operations.get(operation)

video = operation.response.generated_videos[0].video
video.show()
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Generate Videos (Image to Video)&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from google.genai import types

# Read local image (uses mimetypes.guess_type to infer mime type)
image = types.Image.from_file("local/path/file.png")

# Create operation
operation = client.models.generate_videos(
    model='veo-2.0-generate-001',
    # Prompt is optional if image is provided
    prompt='Night sky',
    image=image,
    config=types.GenerateVideosConfig(
        number_of_videos=1,
        duration_seconds=5,
        enhance_prompt=True,
        # Can also pass an Image into last_frame for frame interpolation
    ),
)

# Poll operation
while not operation.done:
    time.sleep(20)
    operation = client.operations.get(operation)

video = operation.response.generated_videos[0].video
video.show()
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Generate Videos (Video to Video)&lt;/h4&gt; 
&lt;p&gt;Currently, only Vertex supports Video to Video generation (Video extension).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from google.genai import types

# Read local video (uses mimetypes.guess_type to infer mime type)
video = types.Video.from_file("local/path/video.mp4")

# Create operation
operation = client.models.generate_videos(
    model='veo-2.0-generate-001',
    # Prompt is optional if Video is provided
    prompt='Night sky',
    # Input video must be in GCS
    video=types.Video(
        uri="gs://bucket-name/inputs/videos/cat_driving.mp4",
    ),
    config=types.GenerateVideosConfig(
        number_of_videos=1,
        duration_seconds=5,
        enhance_prompt=True,
    ),
)

# Poll operation
while not operation.done:
    time.sleep(20)
    operation = client.operations.get(operation)

video = operation.response.generated_videos[0].video
video.show()
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Chats&lt;/h2&gt; 
&lt;p&gt;Create a chat session to start a multi-turn conversations with the model. Then, use &lt;code&gt;chat.send_message&lt;/code&gt; function multiple times within the same chat session so that it can reflect on its previous responses (i.e., engage in an ongoing conversation). See the 'Create a client' section above to initialize a client.&lt;/p&gt; 
&lt;h3&gt;Send Message (Synchronous Non-Streaming)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;chat = client.chats.create(model='gemini-2.0-flash-001')
response = chat.send_message('tell me a story')
print(response.text)
response = chat.send_message('summarize the story you told me in 1 sentence')
print(response.text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Send Message (Synchronous Streaming)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;chat = client.chats.create(model='gemini-2.0-flash-001')
for chunk in chat.send_message_stream('tell me a story'):
    print(chunk.text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Send Message (Asynchronous Non-Streaming)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;chat = client.aio.chats.create(model='gemini-2.0-flash-001')
response = await chat.send_message('tell me a story')
print(response.text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Send Message (Asynchronous Streaming)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;chat = client.aio.chats.create(model='gemini-2.0-flash-001')
async for chunk in await chat.send_message_stream('tell me a story'):
    print(chunk.text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Files&lt;/h2&gt; 
&lt;p&gt;Files are only supported in Gemini Developer API. See the 'Create a client' section above to initialize a client.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-cmd"&gt;!gsutil cp gs://cloud-samples-data/generative-ai/pdf/2312.11805v3.pdf .
!gsutil cp gs://cloud-samples-data/generative-ai/pdf/2403.05530.pdf .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Upload&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;file1 = client.files.upload(file='2312.11805v3.pdf')
file2 = client.files.upload(file='2403.05530.pdf')

print(file1)
print(file2)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Get&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;file1 = client.files.upload(file='2312.11805v3.pdf')
file_info = client.files.get(name=file1.name)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Delete&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;file3 = client.files.upload(file='2312.11805v3.pdf')

client.files.delete(name=file3.name)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Caches&lt;/h2&gt; 
&lt;p&gt;&lt;code&gt;client.caches&lt;/code&gt; contains the control plane APIs for cached content. See the 'Create a client' section above to initialize a client.&lt;/p&gt; 
&lt;h3&gt;Create&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from google.genai import types

if client.vertexai:
    file_uris = [
        'gs://cloud-samples-data/generative-ai/pdf/2312.11805v3.pdf',
        'gs://cloud-samples-data/generative-ai/pdf/2403.05530.pdf',
    ]
else:
    file_uris = [file1.uri, file2.uri]

cached_content = client.caches.create(
    model='gemini-2.0-flash-001',
    config=types.CreateCachedContentConfig(
        contents=[
            types.Content(
                role='user',
                parts=[
                    types.Part.from_uri(
                        file_uri=file_uris[0], mime_type='application/pdf'
                    ),
                    types.Part.from_uri(
                        file_uri=file_uris[1],
                        mime_type='application/pdf',
                    ),
                ],
            )
        ],
        system_instruction='What is the sum of the two pdfs?',
        display_name='test cache',
        ttl='3600s',
    ),
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Get&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;cached_content = client.caches.get(name=cached_content.name)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Generate Content with Caches&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from google.genai import types

response = client.models.generate_content(
    model='gemini-2.0-flash-001',
    contents='Summarize the pdfs',
    config=types.GenerateContentConfig(
        cached_content=cached_content.name,
    ),
)
print(response.text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Tunings&lt;/h2&gt; 
&lt;p&gt;&lt;code&gt;client.tunings&lt;/code&gt; contains tuning job APIs and supports supervised fine tuning through &lt;code&gt;tune&lt;/code&gt;. Only supported in Vertex AI. See the 'Create a client' section above to initialize a client.&lt;/p&gt; 
&lt;h3&gt;Tune&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Vertex AI supports tuning from GCS source or from a Vertex Multimodal Dataset&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from google.genai import types

model = 'gemini-2.0-flash-001'
training_dataset = types.TuningDataset(
  # or gcs_uri=my_vertex_multimodal_dataset
    gcs_uri='gs://cloud-samples-data/ai-platform/generative_ai/gemini-1_5/text/sft_train_data.jsonl',
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from google.genai import types

tuning_job = client.tunings.tune(
    base_model=model,
    training_dataset=training_dataset,
    config=types.CreateTuningJobConfig(
        epoch_count=1, tuned_model_display_name='test_dataset_examples model'
    ),
)
print(tuning_job)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Get Tuning Job&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;tuning_job = client.tunings.get(name=tuning_job.name)
print(tuning_job)
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import time

completed_states = set(
    [
        'JOB_STATE_SUCCEEDED',
        'JOB_STATE_FAILED',
        'JOB_STATE_CANCELLED',
    ]
)

while tuning_job.state not in completed_states:
    print(tuning_job.state)
    tuning_job = client.tunings.get(name=tuning_job.name)
    time.sleep(10)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Use Tuned Model&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;response = client.models.generate_content(
    model=tuning_job.tuned_model.endpoint,
    contents='why is the sky blue?',
)

print(response.text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Get Tuned Model&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;tuned_model = client.models.get(model=tuning_job.tuned_model.model)
print(tuned_model)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;List Tuned Models&lt;/h3&gt; 
&lt;p&gt;To retrieve base models, see &lt;a href="https://raw.githubusercontent.com/googleapis/python-genai/main/#list-base-models"&gt;list base models&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;for model in client.models.list(config={'page_size': 10, 'query_base': False}):
    print(model)
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;pager = client.models.list(config={'page_size': 10, 'query_base': False})
print(pager.page_size)
print(pager[0])
pager.next_page()
print(pager[0])
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Async&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;async for job in await client.aio.models.list(config={'page_size': 10, 'query_base': False}):
    print(job)
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;async_pager = await client.aio.models.list(config={'page_size': 10, 'query_base': False})
print(async_pager.page_size)
print(async_pager[0])
await async_pager.next_page()
print(async_pager[0])
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Update Tuned Model&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from google.genai import types

model = pager[0]

model = client.models.update(
    model=model.name,
    config=types.UpdateModelConfig(
        display_name='my tuned model', description='my tuned model description'
    ),
)

print(model)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;List Tuning Jobs&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;for job in client.tunings.list(config={'page_size': 10}):
    print(job)
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;pager = client.tunings.list(config={'page_size': 10})
print(pager.page_size)
print(pager[0])
pager.next_page()
print(pager[0])
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Async&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;async for job in await client.aio.tunings.list(config={'page_size': 10}):
    print(job)
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;async_pager = await client.aio.tunings.list(config={'page_size': 10})
print(async_pager.page_size)
print(async_pager[0])
await async_pager.next_page()
print(async_pager[0])
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Batch Prediction&lt;/h2&gt; 
&lt;p&gt;Only supported in Vertex AI. See the 'Create a client' section above to initialize a client.&lt;/p&gt; 
&lt;h3&gt;Create&lt;/h3&gt; 
&lt;p&gt;Vertex AI:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Specify model and source file only, destination and job display name will be auto-populated
job = client.batches.create(
    model='gemini-2.0-flash-001',
    src='bq://my-project.my-dataset.my-table',  # or "gs://path/to/input/data"
)

job
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Gemini Developer API:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Create a batch job with inlined requests
batch_job = client.batches.create(
    model="gemini-2.0-flash",
    src=[{
      "contents": [{
        "parts": [{
          "text": "Hello!",
        }],
       "role": "user",
     }],
     "config:": {"response_modalities": ["text"]},
    }],
)

job
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;In order to create a batch job with file name. Need to upload a jsonl file. For example myrequests.json:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;{"key":"request_1", "request": {"contents": [{"parts": [{"text":
 "Explain how AI works in a few words"}]}], "generation_config": {"response_modalities": ["TEXT"]}}}
{"key":"request_2", "request": {"contents": [{"parts": [{"text": "Explain how Crypto works in a few words"}]}]}}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then upload the file.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Upload the file
file = client.files.upload(
    file='myrequest.json',
    config=types.UploadFileConfig(display_name='test_json')
)

# Create a batch job with file name
batch_job = client.batches.create(
    model="gemini-2.0-flash",
    src="files/file_name",
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Get a job by name
job = client.batches.get(name=job.name)

job.state
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;completed_states = set(
    [
        'JOB_STATE_SUCCEEDED',
        'JOB_STATE_FAILED',
        'JOB_STATE_CANCELLED',
        'JOB_STATE_PAUSED',
    ]
)

while job.state not in completed_states:
    print(job.state)
    job = client.batches.get(name=job.name)
    time.sleep(30)

job
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;List&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;for job in client.batches.list(config=types.ListBatchJobsConfig(page_size=10)):
    print(job)
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;pager = client.batches.list(config=types.ListBatchJobsConfig(page_size=10))
print(pager.page_size)
print(pager[0])
pager.next_page()
print(pager[0])
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Async&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;async for job in await client.aio.batches.list(
    config=types.ListBatchJobsConfig(page_size=10)
):
    print(job)
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;async_pager = await client.aio.batches.list(
    config=types.ListBatchJobsConfig(page_size=10)
)
print(async_pager.page_size)
print(async_pager[0])
await async_pager.next_page()
print(async_pager[0])
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Delete&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Delete the job resource
delete_job = client.batches.delete(name=job.name)

delete_job
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Error Handling&lt;/h2&gt; 
&lt;p&gt;To handle errors raised by the model service, the SDK provides this &lt;a href="https://github.com/googleapis/python-genai/raw/main/google/genai/errors.py"&gt;APIError&lt;/a&gt; class.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from google.genai import errors

try:
  client.models.generate_content(
      model="invalid-model-name",
      contents="What is your name?",
  )
except errors.APIError as e:
  print(e.code) # 404
  print(e.message)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Extra Request Body&lt;/h2&gt; 
&lt;p&gt;The &lt;code&gt;extra_body&lt;/code&gt; field in &lt;code&gt;HttpOptions&lt;/code&gt; accepts a dictionary of additional JSON properties to include in the request body. This can be used to access new or experimental backend features that are not yet formally supported in the SDK. The structure of the dictionary must match the backend API's request structure.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;VertexAI backend API docs: &lt;a href="https://cloud.google.com/vertex-ai/docs/reference/rest"&gt;https://cloud.google.com/vertex-ai/docs/reference/rest&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;GeminiAPI backend API docs: &lt;a href="https://ai.google.dev/api/rest"&gt;https://ai.google.dev/api/rest&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;response = client.models.generate_content(
    model="gemini-2.5-pro",
    contents="What is the weather in Boston? and how about Sunnyvale?",
    config=types.GenerateContentConfig(
        tools=[get_current_weather],
        http_options=types.HttpOptions(extra_body={'tool_config': {'function_calling_config': {'mode': 'COMPOSITIONAL'}}}),
    ),
)
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>RVC-Boss/GPT-SoVITS</title>
      <link>https://github.com/RVC-Boss/GPT-SoVITS</link>
      <description>&lt;p&gt;1 min voice data can also be used to train a good TTS model! (few shot voice cloning)&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h1&gt;GPT-SoVITS-WebUI&lt;/h1&gt; A Powerful Few-shot Voice Conversion and Text-to-Speech WebUI.
 &lt;br&gt;
 &lt;br&gt; 
 &lt;p&gt;&lt;a href="https://github.com/RVC-Boss/GPT-SoVITS"&gt;&lt;img src="https://img.shields.io/badge/made_with-%E2%9D%A4-red?style=for-the-badge&amp;amp;labelColor=orange" alt="madewithlove"&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/7033" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/7033" alt="RVC-Boss%2FGPT-SoVITS | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;!-- img src="https://counter.seku.su/cmoe?name=gptsovits&amp;theme=r34" /&gt;&lt;br&gt; --&gt; 
 &lt;p&gt;&lt;a href="https://www.python.org"&gt;&lt;img src="https://img.shields.io/badge/python-3.10--3.12-blue?style=for-the-badge&amp;amp;logo=python" alt="Python"&gt;&lt;/a&gt; &lt;a href="https://github.com/RVC-Boss/gpt-sovits/releases"&gt;&lt;img src="https://img.shields.io/github/v/release/RVC-Boss/gpt-sovits?style=for-the-badge&amp;amp;logo=github" alt="GitHub release"&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://colab.research.google.com/github/RVC-Boss/GPT-SoVITS/blob/main/Colab-WebUI.ipynb"&gt;&lt;img src="https://img.shields.io/badge/Colab-Training-F9AB00?style=for-the-badge&amp;amp;logo=googlecolab" alt="Train In Colab"&gt;&lt;/a&gt; &lt;a href="https://lj1995-gpt-sovits-proplus.hf.space/"&gt;&lt;img src="https://img.shields.io/badge/%E5%85%8D%E8%B4%B9%E5%9C%A8%E7%BA%BF%E4%BD%93%E9%AA%8C-free_online_demo-yellow.svg?style=for-the-badge&amp;amp;logo=huggingface" alt="Huggingface"&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/xxxxrt666/gpt-sovits"&gt;&lt;img src="https://img.shields.io/docker/image-size/xxxxrt666/gpt-sovits/latest?style=for-the-badge&amp;amp;logo=docker" alt="Image Size"&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://www.yuque.com/baicaigongchang1145haoyuangong/ib3g1e"&gt;&lt;img src="https://img.shields.io/badge/%E7%AE%80%E4%BD%93%E4%B8%AD%E6%96%87-%E9%98%85%E8%AF%BB%E6%96%87%E6%A1%A3-blue?style=for-the-badge&amp;amp;logo=googledocs&amp;amp;logoColor=white" alt="ç®€ä½“ä¸­æ–‡"&gt;&lt;/a&gt; &lt;a href="https://rentry.co/GPT-SoVITS-guide#/"&gt;&lt;img src="https://img.shields.io/badge/English-Read%20Docs-blue?style=for-the-badge&amp;amp;logo=googledocs&amp;amp;logoColor=white" alt="English"&gt;&lt;/a&gt; &lt;a href="https://github.com/RVC-Boss/GPT-SoVITS/raw/main/docs/en/Changelog_EN.md"&gt;&lt;img src="https://img.shields.io/badge/Change%20Log-View%20Updates-blue?style=for-the-badge&amp;amp;logo=googledocs&amp;amp;logoColor=white" alt="Change Log"&gt;&lt;/a&gt; &lt;a href="https://github.com/RVC-Boss/GPT-SoVITS/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/LICENSE-MIT-green.svg?style=for-the-badge&amp;amp;logo=opensourceinitiative" alt="License"&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;English&lt;/strong&gt; | &lt;a href="https://raw.githubusercontent.com/RVC-Boss/GPT-SoVITS/main/docs/cn/README.md"&gt;&lt;strong&gt;ä¸­æ–‡ç®€ä½“&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/RVC-Boss/GPT-SoVITS/main/docs/ja/README.md"&gt;&lt;strong&gt;æ—¥æœ¬èª&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/RVC-Boss/GPT-SoVITS/main/docs/ko/README.md"&gt;&lt;strong&gt;í•œêµ­ì–´&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/RVC-Boss/GPT-SoVITS/main/docs/tr/README.md"&gt;&lt;strong&gt;TÃ¼rkÃ§e&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr&gt; 
&lt;h2&gt;Features:&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Zero-shot TTS:&lt;/strong&gt; Input a 5-second vocal sample and experience instant text-to-speech conversion.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Few-shot TTS:&lt;/strong&gt; Fine-tune the model with just 1 minute of training data for improved voice similarity and realism.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Cross-lingual Support:&lt;/strong&gt; Inference in languages different from the training dataset, currently supporting English, Japanese, Korean, Cantonese and Chinese.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;WebUI Tools:&lt;/strong&gt; Integrated tools include voice accompaniment separation, automatic training set segmentation, Chinese ASR, and text labeling, assisting beginners in creating training datasets and GPT/SoVITS models.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Check out our &lt;a href="https://www.bilibili.com/video/BV12g4y1m7Uw"&gt;demo video&lt;/a&gt; here!&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Unseen speakers few-shot fine-tuning demo:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/RVC-Boss/GPT-SoVITS/assets/129054828/05bee1fa-bdd8-4d85-9350-80c060ab47fb"&gt;https://github.com/RVC-Boss/GPT-SoVITS/assets/129054828/05bee1fa-bdd8-4d85-9350-80c060ab47fb&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;RTF(inference speed) of GPT-SoVITS v2 ProPlus&lt;/strong&gt;: 0.028 tested in 4060Ti, 0.014 tested in 4090 (1400words~=4min, inference time is 3.36s), 0.526 in M4 CPU. You can test our &lt;a href="https://lj1995-gpt-sovits-proplus.hf.space/"&gt;huggingface demo&lt;/a&gt; (half H200) to experience high-speed inference .&lt;/p&gt; 
&lt;p&gt;è¯·ä¸è¦å°¬é»‘GPT-SoVITSæ¨ç†é€Ÿåº¦æ…¢ï¼Œè°¢è°¢ï¼&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;User guide: &lt;a href="https://www.yuque.com/baicaigongchang1145haoyuangong/ib3g1e"&gt;ç®€ä½“ä¸­æ–‡&lt;/a&gt; | &lt;a href="https://rentry.co/GPT-SoVITS-guide#/"&gt;English&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;For users in China, you can &lt;a href="https://www.codewithgpu.com/i/RVC-Boss/GPT-SoVITS/GPT-SoVITS-Official"&gt;click here&lt;/a&gt; to use AutoDL Cloud Docker to experience the full functionality online.&lt;/p&gt; 
&lt;h3&gt;Tested Environments&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Python Version&lt;/th&gt; 
   &lt;th&gt;PyTorch Version&lt;/th&gt; 
   &lt;th&gt;Device&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Python 3.10&lt;/td&gt; 
   &lt;td&gt;PyTorch 2.5.1&lt;/td&gt; 
   &lt;td&gt;CUDA 12.4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Python 3.11&lt;/td&gt; 
   &lt;td&gt;PyTorch 2.5.1&lt;/td&gt; 
   &lt;td&gt;CUDA 12.4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Python 3.11&lt;/td&gt; 
   &lt;td&gt;PyTorch 2.7.0&lt;/td&gt; 
   &lt;td&gt;CUDA 12.8&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Python 3.9&lt;/td&gt; 
   &lt;td&gt;PyTorch 2.8.0dev&lt;/td&gt; 
   &lt;td&gt;CUDA 12.8&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Python 3.9&lt;/td&gt; 
   &lt;td&gt;PyTorch 2.5.1&lt;/td&gt; 
   &lt;td&gt;Apple silicon&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Python 3.11&lt;/td&gt; 
   &lt;td&gt;PyTorch 2.7.0&lt;/td&gt; 
   &lt;td&gt;Apple silicon&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Python 3.9&lt;/td&gt; 
   &lt;td&gt;PyTorch 2.2.2&lt;/td&gt; 
   &lt;td&gt;CPU&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Windows&lt;/h3&gt; 
&lt;p&gt;If you are a Windows user (tested with win&amp;gt;=10), you can &lt;a href="https://huggingface.co/lj1995/GPT-SoVITS-windows-package/resolve/main/GPT-SoVITS-v3lora-20250228.7z?download=true"&gt;download the integrated package&lt;/a&gt; and double-click on &lt;em&gt;go-webui.bat&lt;/em&gt; to start GPT-SoVITS-WebUI.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Users in China can &lt;a href="https://www.yuque.com/baicaigongchang1145haoyuangong/ib3g1e/dkxgpiy9zb96hob4#KTvnO"&gt;download the package here&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Install the program by running the following commands:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-pwsh"&gt;conda create -n GPTSoVits python=3.10
conda activate GPTSoVits
pwsh -F install.ps1 --Device &amp;lt;CU126|CU128|CPU&amp;gt; --Source &amp;lt;HF|HF-Mirror|ModelScope&amp;gt; [--DownloadUVR5]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Linux&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;conda create -n GPTSoVits python=3.10
conda activate GPTSoVits
bash install.sh --device &amp;lt;CU126|CU128|ROCM|CPU&amp;gt; --source &amp;lt;HF|HF-Mirror|ModelScope&amp;gt; [--download-uvr5]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;macOS&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Note: The models trained with GPUs on Macs result in significantly lower quality compared to those trained on other devices, so we are temporarily using CPUs instead.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Install the program by running the following commands:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;conda create -n GPTSoVits python=3.10
conda activate GPTSoVits
bash install.sh --device &amp;lt;MPS|CPU&amp;gt; --source &amp;lt;HF|HF-Mirror|ModelScope&amp;gt; [--download-uvr5]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Install Manually&lt;/h3&gt; 
&lt;h4&gt;Install Dependences&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;conda create -n GPTSoVits python=3.10
conda activate GPTSoVits

pip install -r extra-req.txt --no-deps
pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Install FFmpeg&lt;/h4&gt; 
&lt;h5&gt;Conda Users&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;conda activate GPTSoVits
conda install ffmpeg
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;Ubuntu/Debian Users&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo apt install ffmpeg
sudo apt install libsox-dev
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;Windows Users&lt;/h5&gt; 
&lt;p&gt;Download and place &lt;a href="https://huggingface.co/lj1995/VoiceConversionWebUI/blob/main/ffmpeg.exe"&gt;ffmpeg.exe&lt;/a&gt; and &lt;a href="https://huggingface.co/lj1995/VoiceConversionWebUI/blob/main/ffprobe.exe"&gt;ffprobe.exe&lt;/a&gt; in the GPT-SoVITS root&lt;/p&gt; 
&lt;p&gt;Install &lt;a href="https://aka.ms/vs/17/release/vc_redist.x86.exe"&gt;Visual Studio 2017&lt;/a&gt;&lt;/p&gt; 
&lt;h5&gt;MacOS Users&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;brew install ffmpeg
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Running GPT-SoVITS with Docker&lt;/h3&gt; 
&lt;h4&gt;Docker Image Selection&lt;/h4&gt; 
&lt;p&gt;Due to rapid development in the codebase and a slower Docker image release cycle, please:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Check &lt;a href="https://hub.docker.com/r/xxxxrt666/gpt-sovits"&gt;Docker Hub&lt;/a&gt; for the latest available image tags&lt;/li&gt; 
 &lt;li&gt;Choose an appropriate image tag for your environment&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Lite&lt;/code&gt; means the Docker image &lt;strong&gt;does not include&lt;/strong&gt; ASR models and UVR5 models. You can manually download the UVR5 models, while the program will automatically download the ASR models as needed&lt;/li&gt; 
 &lt;li&gt;The appropriate architecture image (amd64/arm64) will be automatically pulled during Docker Compose&lt;/li&gt; 
 &lt;li&gt;Docker Compose will mount &lt;strong&gt;all files&lt;/strong&gt; in the current directory. Please switch to the project root directory and &lt;strong&gt;pull the latest code&lt;/strong&gt; before using the Docker image&lt;/li&gt; 
 &lt;li&gt;Optionally, build the image locally using the provided Dockerfile for the most up-to-date changes&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Environment Variables&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;is_half&lt;/code&gt;: Controls whether half-precision (fp16) is enabled. Set to &lt;code&gt;true&lt;/code&gt; if your GPU supports it to reduce memory usage.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Shared Memory Configuration&lt;/h4&gt; 
&lt;p&gt;On Windows (Docker Desktop), the default shared memory size is small and may cause unexpected behavior. Increase &lt;code&gt;shm_size&lt;/code&gt; (e.g., to &lt;code&gt;16g&lt;/code&gt;) in your Docker Compose file based on your available system memory.&lt;/p&gt; 
&lt;h4&gt;Choosing a Service&lt;/h4&gt; 
&lt;p&gt;The &lt;code&gt;docker-compose.yaml&lt;/code&gt; defines two services:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;GPT-SoVITS-CU126&lt;/code&gt; &amp;amp; &lt;code&gt;GPT-SoVITS-CU128&lt;/code&gt;: Full version with all features.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;GPT-SoVITS-CU126-Lite&lt;/code&gt; &amp;amp; &lt;code&gt;GPT-SoVITS-CU128-Lite&lt;/code&gt;: Lightweight version with reduced dependencies and functionality.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To run a specific service with Docker Compose, use:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker compose run --service-ports &amp;lt;GPT-SoVITS-CU126-Lite|GPT-SoVITS-CU128-Lite|GPT-SoVITS-CU126|GPT-SoVITS-CU128&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Building the Docker Image Locally&lt;/h4&gt; 
&lt;p&gt;If you want to build the image yourself, use:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;bash docker_build.sh --cuda &amp;lt;12.6|12.8&amp;gt; [--lite]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Accessing the Running Container (Bash Shell)&lt;/h4&gt; 
&lt;p&gt;Once the container is running in the background, you can access it using:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker exec -it &amp;lt;GPT-SoVITS-CU126-Lite|GPT-SoVITS-CU128-Lite|GPT-SoVITS-CU126|GPT-SoVITS-CU128&amp;gt; bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Pretrained Models&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;If &lt;code&gt;install.sh&lt;/code&gt; runs successfully, you may skip No.1,2,3&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Users in China can &lt;a href="https://www.yuque.com/baicaigongchang1145haoyuangong/ib3g1e/dkxgpiy9zb96hob4#nVNhX"&gt;download all these models here&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Download pretrained models from &lt;a href="https://huggingface.co/lj1995/GPT-SoVITS"&gt;GPT-SoVITS Models&lt;/a&gt; and place them in &lt;code&gt;GPT_SoVITS/pretrained_models&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Download G2PW models from &lt;a href="https://huggingface.co/XXXXRT/GPT-SoVITS-Pretrained/resolve/main/G2PWModel.zip"&gt;G2PWModel.zip(HF)&lt;/a&gt;| &lt;a href="https://www.modelscope.cn/models/XXXXRT/GPT-SoVITS-Pretrained/resolve/master/G2PWModel.zip"&gt;G2PWModel.zip(ModelScope)&lt;/a&gt;, unzip and rename to &lt;code&gt;G2PWModel&lt;/code&gt;, and then place them in &lt;code&gt;GPT_SoVITS/text&lt;/code&gt;.(Chinese TTS Only)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;For UVR5 (Vocals/Accompaniment Separation &amp;amp; Reverberation Removal, additionally), download models from &lt;a href="https://huggingface.co/lj1995/VoiceConversionWebUI/tree/main/uvr5_weights"&gt;UVR5 Weights&lt;/a&gt; and place them in &lt;code&gt;tools/uvr5/uvr5_weights&lt;/code&gt;.&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;If you want to use &lt;code&gt;bs_roformer&lt;/code&gt; or &lt;code&gt;mel_band_roformer&lt;/code&gt; models for UVR5, you can manually download the model and corresponding configuration file, and put them in &lt;code&gt;tools/uvr5/uvr5_weights&lt;/code&gt;. &lt;strong&gt;Rename the model file and configuration file, ensure that the model and configuration files have the same and corresponding names except for the suffix&lt;/strong&gt;. In addition, the model and configuration file names &lt;strong&gt;must include &lt;code&gt;roformer&lt;/code&gt;&lt;/strong&gt; in order to be recognized as models of the roformer class.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;The suggestion is to &lt;strong&gt;directly specify the model type&lt;/strong&gt; in the model name and configuration file name, such as &lt;code&gt;mel_mand_roformer&lt;/code&gt;, &lt;code&gt;bs_roformer&lt;/code&gt;. If not specified, the features will be compared from the configuration file to determine which type of model it is. For example, the model &lt;code&gt;bs_roformer_ep_368_sdr_12.9628.ckpt&lt;/code&gt; and its corresponding configuration file &lt;code&gt;bs_roformer_ep_368_sdr_12.9628.yaml&lt;/code&gt; are a pair, &lt;code&gt;kim_mel_band_roformer.ckpt&lt;/code&gt; and &lt;code&gt;kim_mel_band_roformer.yaml&lt;/code&gt; are also a pair.&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;For Chinese ASR (additionally), download models from &lt;a href="https://modelscope.cn/models/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/files"&gt;Damo ASR Model&lt;/a&gt;, &lt;a href="https://modelscope.cn/models/damo/speech_fsmn_vad_zh-cn-16k-common-pytorch/files"&gt;Damo VAD Model&lt;/a&gt;, and &lt;a href="https://modelscope.cn/models/damo/punc_ct-transformer_zh-cn-common-vocab272727-pytorch/files"&gt;Damo Punc Model&lt;/a&gt; and place them in &lt;code&gt;tools/asr/models&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;For English or Japanese ASR (additionally), download models from &lt;a href="https://huggingface.co/Systran/faster-whisper-large-v3"&gt;Faster Whisper Large V3&lt;/a&gt; and place them in &lt;code&gt;tools/asr/models&lt;/code&gt;. Also, &lt;a href="https://huggingface.co/Systran"&gt;other models&lt;/a&gt; may have the similar effect with smaller disk footprint.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Dataset Format&lt;/h2&gt; 
&lt;p&gt;The TTS annotation .list file format:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;
vocal_path|speaker_name|language|text

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Language dictionary:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;'zh': Chinese&lt;/li&gt; 
 &lt;li&gt;'ja': Japanese&lt;/li&gt; 
 &lt;li&gt;'en': English&lt;/li&gt; 
 &lt;li&gt;'ko': Korean&lt;/li&gt; 
 &lt;li&gt;'yue': Cantonese&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Example:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;
D:\GPT-SoVITS\xxx/xxx.wav|xxx|en|I like playing Genshin.

&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Finetune and inference&lt;/h2&gt; 
&lt;h3&gt;Open WebUI&lt;/h3&gt; 
&lt;h4&gt;Integrated Package Users&lt;/h4&gt; 
&lt;p&gt;Double-click &lt;code&gt;go-webui.bat&lt;/code&gt;or use &lt;code&gt;go-webui.ps1&lt;/code&gt; if you want to switch to V1,then double-click&lt;code&gt;go-webui-v1.bat&lt;/code&gt; or use &lt;code&gt;go-webui-v1.ps1&lt;/code&gt;&lt;/p&gt; 
&lt;h4&gt;Others&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python webui.py &amp;lt;language(optional)&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;if you want to switch to V1,then&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python webui.py v1 &amp;lt;language(optional)&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or maunally switch version in WebUI&lt;/p&gt; 
&lt;h3&gt;Finetune&lt;/h3&gt; 
&lt;h4&gt;Path Auto-filling is now supported&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;Fill in the audio path&lt;/li&gt; 
 &lt;li&gt;Slice the audio into small chunks&lt;/li&gt; 
 &lt;li&gt;Denoise(optinal)&lt;/li&gt; 
 &lt;li&gt;ASR&lt;/li&gt; 
 &lt;li&gt;Proofreading ASR transcriptions&lt;/li&gt; 
 &lt;li&gt;Go to the next Tab, then finetune the model&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Open Inference WebUI&lt;/h3&gt; 
&lt;h4&gt;Integrated Package Users&lt;/h4&gt; 
&lt;p&gt;Double-click &lt;code&gt;go-webui-v2.bat&lt;/code&gt; or use &lt;code&gt;go-webui-v2.ps1&lt;/code&gt; ,then open the inference webui at &lt;code&gt;1-GPT-SoVITS-TTS/1C-inference&lt;/code&gt;&lt;/p&gt; 
&lt;h4&gt;Others&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python GPT_SoVITS/inference_webui.py &amp;lt;language(optional)&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;OR&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python webui.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;then open the inference webui at &lt;code&gt;1-GPT-SoVITS-TTS/1C-inference&lt;/code&gt;&lt;/p&gt; 
&lt;h2&gt;V2 Release Notes&lt;/h2&gt; 
&lt;p&gt;New Features:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Support Korean and Cantonese&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;An optimized text frontend&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Pre-trained model extended from 2k hours to 5k hours&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Improved synthesis quality for low-quality reference audio&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/RVC-Boss/GPT-SoVITS/wiki/GPT%E2%80%90SoVITS%E2%80%90v2%E2%80%90features-(%E6%96%B0%E7%89%B9%E6%80%A7)"&gt;more details&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Use v2 from v1 environment:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;pip install -r requirements.txt&lt;/code&gt; to update some packages&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Clone the latest codes from github.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Download v2 pretrained models from &lt;a href="https://huggingface.co/lj1995/GPT-SoVITS/tree/main/gsv-v2final-pretrained"&gt;huggingface&lt;/a&gt; and put them into &lt;code&gt;GPT_SoVITS/pretrained_models/gsv-v2final-pretrained&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Chinese v2 additional: &lt;a href="https://huggingface.co/XXXXRT/GPT-SoVITS-Pretrained/resolve/main/G2PWModel.zip"&gt;G2PWModel.zip(HF)&lt;/a&gt;| &lt;a href="https://www.modelscope.cn/models/XXXXRT/GPT-SoVITS-Pretrained/resolve/master/G2PWModel.zip"&gt;G2PWModel.zip(ModelScope)&lt;/a&gt;(Download G2PW models, unzip and rename to &lt;code&gt;G2PWModel&lt;/code&gt;, and then place them in &lt;code&gt;GPT_SoVITS/text&lt;/code&gt;.)&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;V3 Release Notes&lt;/h2&gt; 
&lt;p&gt;New Features:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;The timbre similarity is higher, requiring less training data to approximate the target speaker (the timbre similarity is significantly improved using the base model directly without fine-tuning).&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;GPT model is more stable, with fewer repetitions and omissions, and it is easier to generate speech with richer emotional expression.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/RVC-Boss/GPT-SoVITS/wiki/GPT%E2%80%90SoVITS%E2%80%90v3v4%E2%80%90features-(%E6%96%B0%E7%89%B9%E6%80%A7)"&gt;more details&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Use v3 from v2 environment:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;pip install -r requirements.txt&lt;/code&gt; to update some packages&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Clone the latest codes from github.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Download v3 pretrained models (s1v3.ckpt, s2Gv3.pth and models--nvidia--bigvgan_v2_24khz_100band_256x folder) from &lt;a href="https://huggingface.co/lj1995/GPT-SoVITS/tree/main"&gt;huggingface&lt;/a&gt; and put them into &lt;code&gt;GPT_SoVITS/pretrained_models&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;additional: for Audio Super Resolution model, you can read &lt;a href="https://raw.githubusercontent.com/RVC-Boss/GPT-SoVITS/main/tools/AP_BWE_main/24kto48k/readme.txt"&gt;how to download&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;V4 Release Notes&lt;/h2&gt; 
&lt;p&gt;New Features:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Version 4 fixes the issue of metallic artifacts in Version 3 caused by non-integer multiple upsampling, and natively outputs 48k audio to prevent muffled sound (whereas Version 3 only natively outputs 24k audio). The author considers Version 4 a direct replacement for Version 3, though further testing is still needed. &lt;a href="https://github.com/RVC-Boss/GPT-SoVITS/wiki/GPT%E2%80%90SoVITS%E2%80%90v3v4%E2%80%90features-(%E6%96%B0%E7%89%B9%E6%80%A7)"&gt;more details&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Use v4 from v1/v2/v3 environment:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;pip install -r requirements.txt&lt;/code&gt; to update some packages&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Clone the latest codes from github.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Download v4 pretrained models (gsv-v4-pretrained/s2v4.ckpt, and gsv-v4-pretrained/vocoder.pth) from &lt;a href="https://huggingface.co/lj1995/GPT-SoVITS/tree/main"&gt;huggingface&lt;/a&gt; and put them into &lt;code&gt;GPT_SoVITS/pretrained_models&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;V2Pro Release Notes&lt;/h2&gt; 
&lt;p&gt;New Features:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Slightly higher VRAM usage than v2, surpassing v4's performance, with v2's hardware cost and speed. &lt;a href="https://github.com/RVC-Boss/GPT-SoVITS/wiki/GPT%E2%80%90SoVITS%E2%80%90features-(%E5%90%84%E7%89%88%E6%9C%AC%E7%89%B9%E6%80%A7)"&gt;more details&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;2.v1/v2 and the v2Pro series share the same characteristics, while v3/v4 have similar features. For training sets with average audio quality, v1/v2/v2Pro can deliver decent results, but v3/v4 cannot. Additionally, the synthesized tone and timebre of v3/v4 lean more toward the reference audio rather than the overall training set.&lt;/p&gt; 
&lt;p&gt;Use v2Pro from v1/v2/v3/v4 environment:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;pip install -r requirements.txt&lt;/code&gt; to update some packages&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Clone the latest codes from github.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Download v2Pro pretrained models (v2Pro/s2Dv2Pro.pth, v2Pro/s2Gv2Pro.pth, v2Pro/s2Dv2ProPlus.pth, v2Pro/s2Gv2ProPlus.pth, and sv/pretrained_eres2netv2w24s4ep4.ckpt) from &lt;a href="https://huggingface.co/lj1995/GPT-SoVITS/tree/main"&gt;huggingface&lt;/a&gt; and put them into &lt;code&gt;GPT_SoVITS/pretrained_models&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Todo List&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; &lt;p&gt;&lt;strong&gt;High Priority:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; Localization in Japanese and English.&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; User guide.&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; Japanese and English dataset fine tune training.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled&gt; &lt;p&gt;&lt;strong&gt;Features:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; Zero-shot voice conversion (5s) / few-shot voice conversion (1min).&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; TTS speaking speed control.&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" disabled&gt; &lt;del&gt;Enhanced TTS emotion control.&lt;/del&gt; Maybe use pretrained finetuned preset GPT models for better emotion.&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" disabled&gt; Experiment with changing SoVITS token inputs to probability distribution of GPT vocabs (transformer latent).&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; Improve English and Japanese text frontend.&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" disabled&gt; Develop tiny and larger-sized TTS models.&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; Colab scripts.&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; Try expand training dataset (2k hours -&amp;gt; 10k hours).&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; better sovits base model (enhanced audio quality)&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" disabled&gt; model mix&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;(Additional) Method for running from the command line&lt;/h2&gt; 
&lt;p&gt;Use the command line to open the WebUI for UVR5&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python tools/uvr5/webui.py "&amp;lt;infer_device&amp;gt;" &amp;lt;is_half&amp;gt; &amp;lt;webui_port_uvr5&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;!-- If you can't open a browser, follow the format below for UVR processing,This is using mdxnet for audio processing
```
python mdxnet.py --model --input_root --output_vocal --output_ins --agg_level --format --device --is_half_precision
``` --&gt; 
&lt;p&gt;This is how the audio segmentation of the dataset is done using the command line&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python audio_slicer.py \
    --input_path "&amp;lt;path_to_original_audio_file_or_directory&amp;gt;" \
    --output_root "&amp;lt;directory_where_subdivided_audio_clips_will_be_saved&amp;gt;" \
    --threshold &amp;lt;volume_threshold&amp;gt; \
    --min_length &amp;lt;minimum_duration_of_each_subclip&amp;gt; \
    --min_interval &amp;lt;shortest_time_gap_between_adjacent_subclips&amp;gt;
    --hop_size &amp;lt;step_size_for_computing_volume_curve&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This is how dataset ASR processing is done using the command line(Only Chinese)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python tools/asr/funasr_asr.py -i &amp;lt;input&amp;gt; -o &amp;lt;output&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;ASR processing is performed through Faster_Whisper(ASR marking except Chinese)&lt;/p&gt; 
&lt;p&gt;(No progress bars, GPU performance may cause time delays)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python ./tools/asr/fasterwhisper_asr.py -i &amp;lt;input&amp;gt; -o &amp;lt;output&amp;gt; -l &amp;lt;language&amp;gt; -p &amp;lt;precision&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;A custom list save path is enabled&lt;/p&gt; 
&lt;h2&gt;Credits&lt;/h2&gt; 
&lt;p&gt;Special thanks to the following projects and contributors:&lt;/p&gt; 
&lt;h3&gt;Theoretical Research&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/innnky/ar-vits"&gt;ar-vits&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/yangdongchao/SoundStorm/tree/master/soundstorm/s1/AR"&gt;SoundStorm&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/jaywalnut310/vits"&gt;vits&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/hcy71o/TransferTTS/raw/master/models.py#L556"&gt;TransferTTS&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/auspicious3000/contentvec/"&gt;contentvec&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/jik876/hifi-gan"&gt;hifi-gan&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/fishaudio/fish-speech/raw/main/tools/llama/generate.py#L41"&gt;fish-speech&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/SWivid/F5-TTS/raw/main/src/f5_tts/model/backbones/dit.py"&gt;f5-TTS&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/kvfrans/shortcut-models/raw/main/targets_shortcut.py"&gt;shortcut flow matching&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Pretrained Models&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/TencentGameMate/chinese_speech_pretrain"&gt;Chinese Speech Pretrain&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/hfl/chinese-roberta-wwm-ext-large"&gt;Chinese-Roberta-WWM-Ext-Large&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/NVIDIA/BigVGAN"&gt;BigVGAN&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://modelscope.cn/models/iic/speech_eres2netv2w24s4ep4_sv_zh-cn_16k-common"&gt;eresnetv2&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Text Frontend for Inference&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/PaddlePaddle/PaddleSpeech/tree/develop/paddlespeech/t2s/frontend/zh_normalization"&gt;paddlespeech zh_normalization&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/DoodleBears/split-lang"&gt;split-lang&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/GitYCC/g2pW"&gt;g2pW&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/mozillazg/pypinyin-g2pW"&gt;pypinyin-g2pW&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/PaddlePaddle/PaddleSpeech/tree/develop/paddlespeech/t2s/frontend/g2pw"&gt;paddlespeech g2pw&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;WebUI Tools&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Anjok07/ultimatevocalremovergui"&gt;ultimatevocalremovergui&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/openvpi/audio-slicer"&gt;audio-slicer&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/cronrpc/SubFix"&gt;SubFix&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/FFmpeg/FFmpeg"&gt;FFmpeg&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/gradio-app/gradio"&gt;gradio&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/SYSTRAN/faster-whisper"&gt;faster-whisper&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/alibaba-damo-academy/FunASR"&gt;FunASR&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/yxlu-0102/AP-BWE"&gt;AP-BWE&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Thankful to @Naozumi520 for providing the Cantonese training set and for the guidance on Cantonese-related knowledge.&lt;/p&gt; 
&lt;h2&gt;Thanks to all contributors for their efforts&lt;/h2&gt; 
&lt;a href="https://github.com/RVC-Boss/GPT-SoVITS/graphs/contributors" target="_blank"&gt; &lt;img src="https://contrib.rocks/image?repo=RVC-Boss/GPT-SoVITS"&gt; &lt;/a&gt;</description>
    </item>
    
    <item>
      <title>BerriAI/litellm</title>
      <link>https://github.com/BerriAI/litellm</link>
      <description>&lt;p&gt;Python SDK, Proxy Server (LLM Gateway) to call 100+ LLM APIs in OpenAI format - [Bedrock, Azure, OpenAI, VertexAI, Cohere, Anthropic, Sagemaker, HuggingFace, Replicate, Groq]&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt; ğŸš… LiteLLM &lt;/h1&gt; 
&lt;p align="center"&gt; &lt;/p&gt;
&lt;p align="center"&gt; &lt;a href="https://render.com/deploy?repo=https://github.com/BerriAI/litellm" target="_blank" rel="nofollow"&gt;&lt;img src="https://render.com/images/deploy-to-render-button.svg?sanitize=true" alt="Deploy to Render"&gt;&lt;/a&gt; &lt;a href="https://railway.app/template/HLP0Ub?referralCode=jch2ME"&gt; &lt;img src="https://railway.app/button.svg?sanitize=true" alt="Deploy on Railway"&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt;Call all LLM APIs using the OpenAI format [Bedrock, Huggingface, VertexAI, TogetherAI, Azure, OpenAI, Groq etc.] &lt;br&gt; &lt;/p&gt; 
&lt;h4 align="center"&gt;&lt;a href="https://docs.litellm.ai/docs/simple_proxy" target="_blank"&gt;LiteLLM Proxy Server (LLM Gateway)&lt;/a&gt; | &lt;a href="https://docs.litellm.ai/docs/hosted" target="_blank"&gt; Hosted Proxy (Preview)&lt;/a&gt; | &lt;a href="https://docs.litellm.ai/docs/enterprise" target="_blank"&gt;Enterprise Tier&lt;/a&gt;&lt;/h4&gt; 
&lt;h4 align="center"&gt; &lt;a href="https://pypi.org/project/litellm/" target="_blank"&gt; &lt;img src="https://img.shields.io/pypi/v/litellm.svg?sanitize=true" alt="PyPI Version"&gt; &lt;/a&gt; &lt;a href="https://www.ycombinator.com/companies/berriai"&gt; &lt;img src="https://img.shields.io/badge/Y%20Combinator-W23-orange?style=flat-square" alt="Y Combinator W23"&gt; &lt;/a&gt; &lt;a href="https://wa.link/huol9n"&gt; &lt;img src="https://img.shields.io/static/v1?label=Chat%20on&amp;amp;message=WhatsApp&amp;amp;color=success&amp;amp;logo=WhatsApp&amp;amp;style=flat-square" alt="Whatsapp"&gt; &lt;/a&gt; &lt;a href="https://discord.gg/wuPM9dRgDw"&gt; &lt;img src="https://img.shields.io/static/v1?label=Chat%20on&amp;amp;message=Discord&amp;amp;color=blue&amp;amp;logo=Discord&amp;amp;style=flat-square" alt="Discord"&gt; &lt;/a&gt; &lt;a href="https://join.slack.com/share/enQtOTE0ODczMzk2Nzk4NC01YjUxNjY2YjBlYTFmNDRiZTM3NDFiYTM3MzVkODFiMDVjOGRjMmNmZTZkZTMzOWQzZGQyZWIwYjQ0MWExYmE3"&gt; &lt;img src="https://img.shields.io/static/v1?label=Chat%20on&amp;amp;message=Slack&amp;amp;color=black&amp;amp;logo=Slack&amp;amp;style=flat-square" alt="Slack"&gt; &lt;/a&gt; &lt;/h4&gt; 
&lt;p&gt;LiteLLM manages:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Translate inputs to provider's &lt;code&gt;completion&lt;/code&gt;, &lt;code&gt;embedding&lt;/code&gt;, and &lt;code&gt;image_generation&lt;/code&gt; endpoints&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.litellm.ai/docs/completion/output"&gt;Consistent output&lt;/a&gt;, text responses will always be available at &lt;code&gt;['choices'][0]['message']['content']&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Retry/fallback logic across multiple deployments (e.g. Azure/OpenAI) - &lt;a href="https://docs.litellm.ai/docs/routing"&gt;Router&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Set Budgets &amp;amp; Rate limits per project, api key, model &lt;a href="https://docs.litellm.ai/docs/simple_proxy"&gt;LiteLLM Proxy Server (LLM Gateway)&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://github.com/BerriAI/litellm?tab=readme-ov-file#openai-proxy---docs"&gt;&lt;strong&gt;Jump to LiteLLM Proxy (LLM Gateway) Docs&lt;/strong&gt;&lt;/a&gt; &lt;br&gt; &lt;a href="https://github.com/BerriAI/litellm?tab=readme-ov-file#supported-providers-docs"&gt;&lt;strong&gt;Jump to Supported LLM Providers&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;ğŸš¨ &lt;strong&gt;Stable Release:&lt;/strong&gt; Use docker images with the &lt;code&gt;-stable&lt;/code&gt; tag. These have undergone 12 hour load tests, before being published. &lt;a href="https://docs.litellm.ai/docs/proxy/release_cycle"&gt;More information about the release cycle here&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Support for more providers. Missing a provider or LLM Platform, raise a &lt;a href="https://github.com/BerriAI/litellm/issues/new?assignees=&amp;amp;labels=enhancement&amp;amp;projects=&amp;amp;template=feature_request.yml&amp;amp;title=%5BFeature%5D%3A+"&gt;feature request&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Usage (&lt;a href="https://docs.litellm.ai/docs/"&gt;&lt;strong&gt;Docs&lt;/strong&gt;&lt;/a&gt;)&lt;/h1&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] LiteLLM v1.0.0 now requires &lt;code&gt;openai&amp;gt;=1.0.0&lt;/code&gt;. Migration guide &lt;a href="https://docs.litellm.ai/docs/migration"&gt;here&lt;/a&gt;&lt;br&gt; LiteLLM v1.40.14+ now requires &lt;code&gt;pydantic&amp;gt;=2.0.0&lt;/code&gt;. No changes required.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;a target="_blank" href="https://colab.research.google.com/github/BerriAI/litellm/blob/main/cookbook/liteLLM_Getting_Started.ipynb"&gt; &lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab"&gt; &lt;/a&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip install litellm
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from litellm import completion
import os

## set ENV variables
os.environ["OPENAI_API_KEY"] = "your-openai-key"
os.environ["ANTHROPIC_API_KEY"] = "your-anthropic-key"

messages = [{ "content": "Hello, how are you?","role": "user"}]

# openai call
response = completion(model="openai/gpt-4o", messages=messages)

# anthropic call
response = completion(model="anthropic/claude-sonnet-4-20250514", messages=messages)
print(response)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Response (OpenAI Format)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
    "id": "chatcmpl-1214900a-6cdd-4148-b663-b5e2f642b4de",
    "created": 1751494488,
    "model": "claude-sonnet-4-20250514",
    "object": "chat.completion",
    "system_fingerprint": null,
    "choices": [
        {
            "finish_reason": "stop",
            "index": 0,
            "message": {
                "content": "Hello! I'm doing well, thank you for asking. I'm here and ready to help with whatever you'd like to discuss or work on. How are you doing today?",
                "role": "assistant",
                "tool_calls": null,
                "function_call": null
            }
        }
    ],
    "usage": {
        "completion_tokens": 39,
        "prompt_tokens": 13,
        "total_tokens": 52,
        "completion_tokens_details": null,
        "prompt_tokens_details": {
            "audio_tokens": null,
            "cached_tokens": 0
        },
        "cache_creation_input_tokens": 0,
        "cache_read_input_tokens": 0
    }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Call any model supported by a provider, with &lt;code&gt;model=&amp;lt;provider_name&amp;gt;/&amp;lt;model_name&amp;gt;&lt;/code&gt;. There might be provider-specific details here, so refer to &lt;a href="https://docs.litellm.ai/docs/providers"&gt;provider docs for more information&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Async (&lt;a href="https://docs.litellm.ai/docs/completion/stream#async-completion"&gt;Docs&lt;/a&gt;)&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from litellm import acompletion
import asyncio

async def test_get_response():
    user_message = "Hello, how are you?"
    messages = [{"content": user_message, "role": "user"}]
    response = await acompletion(model="openai/gpt-4o", messages=messages)
    return response

response = asyncio.run(test_get_response())
print(response)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Streaming (&lt;a href="https://docs.litellm.ai/docs/completion/stream"&gt;Docs&lt;/a&gt;)&lt;/h2&gt; 
&lt;p&gt;liteLLM supports streaming the model response back, pass &lt;code&gt;stream=True&lt;/code&gt; to get a streaming iterator in response.&lt;br&gt; Streaming is supported for all models (Bedrock, Huggingface, TogetherAI, Azure, OpenAI, etc.)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from litellm import completion
response = completion(model="openai/gpt-4o", messages=messages, stream=True)
for part in response:
    print(part.choices[0].delta.content or "")

# claude sonnet 4
response = completion('anthropic/claude-sonnet-4-20250514', messages, stream=True)
for part in response:
    print(part)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Response chunk (OpenAI Format)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
    "id": "chatcmpl-fe575c37-5004-4926-ae5e-bfbc31f356ca",
    "created": 1751494808,
    "model": "claude-sonnet-4-20250514",
    "object": "chat.completion.chunk",
    "system_fingerprint": null,
    "choices": [
        {
            "finish_reason": null,
            "index": 0,
            "delta": {
                "provider_specific_fields": null,
                "content": "Hello",
                "role": "assistant",
                "function_call": null,
                "tool_calls": null,
                "audio": null
            },
            "logprobs": null
        }
    ],
    "provider_specific_fields": null,
    "stream_options": null,
    "citations": null
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Logging Observability (&lt;a href="https://docs.litellm.ai/docs/observability/callbacks"&gt;Docs&lt;/a&gt;)&lt;/h2&gt; 
&lt;p&gt;LiteLLM exposes pre defined callbacks to send data to Lunary, MLflow, Langfuse, DynamoDB, s3 Buckets, Helicone, Promptlayer, Traceloop, Athina, Slack&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from litellm import completion

## set env variables for logging tools (when using MLflow, no API key set up is required)
os.environ["LUNARY_PUBLIC_KEY"] = "your-lunary-public-key"
os.environ["HELICONE_API_KEY"] = "your-helicone-auth-key"
os.environ["LANGFUSE_PUBLIC_KEY"] = ""
os.environ["LANGFUSE_SECRET_KEY"] = ""
os.environ["ATHINA_API_KEY"] = "your-athina-api-key"

os.environ["OPENAI_API_KEY"] = "your-openai-key"

# set callbacks
litellm.success_callback = ["lunary", "mlflow", "langfuse", "athina", "helicone"] # log input/output to lunary, langfuse, supabase, athina, helicone etc

#openai call
response = completion(model="openai/gpt-4o", messages=[{"role": "user", "content": "Hi ğŸ‘‹ - i'm openai"}])
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;LiteLLM Proxy Server (LLM Gateway) - (&lt;a href="https://docs.litellm.ai/docs/simple_proxy"&gt;Docs&lt;/a&gt;)&lt;/h1&gt; 
&lt;p&gt;Track spend + Load Balance across multiple projects&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://docs.litellm.ai/docs/hosted"&gt;Hosted Proxy (Preview)&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;The proxy provides:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://docs.litellm.ai/docs/proxy/virtual_keys#custom-auth"&gt;Hooks for auth&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.litellm.ai/docs/proxy/logging#step-1---create-your-custom-litellm-callback-class"&gt;Hooks for logging&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.litellm.ai/docs/proxy/virtual_keys#tracking-spend"&gt;Cost tracking&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.litellm.ai/docs/proxy/users#set-rate-limits"&gt;Rate Limiting&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;ğŸ“– Proxy Endpoints - &lt;a href="https://litellm-api.up.railway.app/"&gt;Swagger Docs&lt;/a&gt;&lt;/h2&gt; 
&lt;h2&gt;Quick Start Proxy - CLI&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip install 'litellm[proxy]'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Step 1: Start litellm proxy&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;$ litellm --model huggingface/bigcode/starcoder

#INFO: Proxy running on http://0.0.0.0:4000
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Step 2: Make ChatCompletions Request to Proxy&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] ğŸ’¡ &lt;a href="https://docs.litellm.ai/docs/proxy/user_keys"&gt;Use LiteLLM Proxy with Langchain (Python, JS), OpenAI SDK (Python, JS) Anthropic SDK, Mistral SDK, LlamaIndex, Instructor, Curl&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import openai # openai v1.0.0+
client = openai.OpenAI(api_key="anything",base_url="http://0.0.0.0:4000") # set proxy to base_url
# request sent to model set on litellm proxy, `litellm --model`
response = client.chat.completions.create(model="gpt-3.5-turbo", messages = [
    {
        "role": "user",
        "content": "this is a test request, write a short poem"
    }
])

print(response)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Proxy Key Management (&lt;a href="https://docs.litellm.ai/docs/proxy/virtual_keys"&gt;Docs&lt;/a&gt;)&lt;/h2&gt; 
&lt;p&gt;Connect the proxy with a Postgres DB to create proxy keys&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Get the code
git clone https://github.com/BerriAI/litellm

# Go to folder
cd litellm

# Add the master key - you can change this after setup
echo 'LITELLM_MASTER_KEY="sk-1234"' &amp;gt; .env

# Add the litellm salt key - you cannot change this after adding a model
# It is used to encrypt / decrypt your LLM API Key credentials
# We recommend - https://1password.com/password-generator/ 
# password generator to get a random hash for litellm salt key
echo 'LITELLM_SALT_KEY="sk-1234"' &amp;gt;&amp;gt; .env

source .env

# Start
docker-compose up
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;UI on &lt;code&gt;/ui&lt;/code&gt; on your proxy server &lt;img src="https://github.com/BerriAI/litellm/assets/29436595/47c97d5e-b9be-4839-b28c-43d7f4f10033" alt="ui_3"&gt;&lt;/p&gt; 
&lt;p&gt;Set budgets and rate limits across multiple projects &lt;code&gt;POST /key/generate&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Request&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;curl 'http://0.0.0.0:4000/key/generate' \
--header 'Authorization: Bearer sk-1234' \
--header 'Content-Type: application/json' \
--data-raw '{"models": ["gpt-3.5-turbo", "gpt-4", "claude-2"], "duration": "20m","metadata": {"user": "ishaan@berri.ai", "team": "core-infra"}}'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Expected Response&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;{
    "key": "sk-kdEXbIqZRwEeEiHwdg7sFA", # Bearer token
    "expires": "2023-11-19T01:38:25.838000+00:00" # datetime object
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Supported Providers (&lt;a href="https://docs.litellm.ai/docs/providers"&gt;Docs&lt;/a&gt;)&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Provider&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://docs.litellm.ai/docs/#basic-usage"&gt;Completion&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://docs.litellm.ai/docs/completion/stream#streaming-responses"&gt;Streaming&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://docs.litellm.ai/docs/completion/stream#async-completion"&gt;Async Completion&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://docs.litellm.ai/docs/completion/stream#async-streaming"&gt;Async Streaming&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://docs.litellm.ai/docs/embedding/supported_embedding"&gt;Async Embedding&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href="https://docs.litellm.ai/docs/image_generation"&gt;Async Image Generation&lt;/a&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/openai"&gt;openai&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/meta_llama"&gt;Meta - Llama API&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/azure"&gt;azure&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/aiml"&gt;AI/ML API&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/aws_sagemaker"&gt;aws - sagemaker&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/bedrock"&gt;aws - bedrock&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/vertex"&gt;google - vertex_ai&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/palm"&gt;google - palm&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/gemini"&gt;google AI Studio - gemini&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/mistral"&gt;mistral ai api&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/cloudflare_workers"&gt;cloudflare AI Workers&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/cohere"&gt;cohere&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/anthropic"&gt;anthropic&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/empower"&gt;empower&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/huggingface"&gt;huggingface&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/replicate"&gt;replicate&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/togetherai"&gt;together_ai&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/openrouter"&gt;openrouter&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/ai21"&gt;ai21&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/baseten"&gt;baseten&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/vllm"&gt;vllm&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/nlp_cloud"&gt;nlp_cloud&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/aleph_alpha"&gt;aleph alpha&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/petals"&gt;petals&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/ollama"&gt;ollama&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/deepinfra"&gt;deepinfra&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/perplexity"&gt;perplexity-ai&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/groq"&gt;Groq AI&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/deepseek"&gt;Deepseek&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/anyscale"&gt;anyscale&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/watsonx"&gt;IBM - watsonx.ai&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/voyage"&gt;voyage ai&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/xinference"&gt;xinference [Xorbits Inference]&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/friendliai"&gt;FriendliAI&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/galadriel"&gt;Galadriel&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://novita.ai/models/llm?utm_source=github_litellm&amp;amp;utm_medium=github_readme&amp;amp;utm_campaign=github_link"&gt;Novita AI&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/featherless_ai"&gt;Featherless AI&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.litellm.ai/docs/providers/nebius"&gt;Nebius AI Studio&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;âœ…&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;a href="https://docs.litellm.ai/docs/"&gt;&lt;strong&gt;Read the Docs&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Interested in contributing? Contributions to LiteLLM Python SDK, Proxy Server, and LLM integrations are both accepted and highly encouraged!&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Quick start:&lt;/strong&gt; &lt;code&gt;git clone&lt;/code&gt; â†’ &lt;code&gt;make install-dev&lt;/code&gt; â†’ &lt;code&gt;make format&lt;/code&gt; â†’ &lt;code&gt;make lint&lt;/code&gt; â†’ &lt;code&gt;make test-unit&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;See our comprehensive &lt;a href="https://raw.githubusercontent.com/BerriAI/litellm/main/CONTRIBUTING.md"&gt;Contributing Guide (CONTRIBUTING.md)&lt;/a&gt; for detailed instructions.&lt;/p&gt; 
&lt;h1&gt;Enterprise&lt;/h1&gt; 
&lt;p&gt;For companies that need better security, user management and professional support&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://calendly.com/d/4mp-gd3-k5k/litellm-1-1-onboarding-chat"&gt;Talk to founders&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;This covers:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;âœ… &lt;strong&gt;Features under the &lt;a href="https://docs.litellm.ai/docs/proxy/enterprise"&gt;LiteLLM Commercial License&lt;/a&gt;:&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;âœ… &lt;strong&gt;Feature Prioritization&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;âœ… &lt;strong&gt;Custom Integrations&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;âœ… &lt;strong&gt;Professional Support - Dedicated discord + slack&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;âœ… &lt;strong&gt;Custom SLAs&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;âœ… &lt;strong&gt;Secure access with Single Sign-On&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Contributing&lt;/h1&gt; 
&lt;p&gt;We welcome contributions to LiteLLM! Whether you're fixing bugs, adding features, or improving documentation, we appreciate your help.&lt;/p&gt; 
&lt;h2&gt;Quick Start for Contributors&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/BerriAI/litellm.git
cd litellm
make install-dev    # Install development dependencies
make format         # Format your code
make lint           # Run all linting checks
make test-unit      # Run unit tests
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For detailed contributing guidelines, see &lt;a href="https://raw.githubusercontent.com/BerriAI/litellm/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Code Quality / Linting&lt;/h2&gt; 
&lt;p&gt;LiteLLM follows the &lt;a href="https://google.github.io/styleguide/pyguide.html"&gt;Google Python Style Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Our automated checks include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Black&lt;/strong&gt; for code formatting&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Ruff&lt;/strong&gt; for linting and code quality&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MyPy&lt;/strong&gt; for type checking&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Circular import detection&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Import safety checks&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Run all checks locally:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;make lint           # Run all linting (matches CI)
make format-check   # Check formatting only
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;All these checks must pass before your PR can be merged.&lt;/p&gt; 
&lt;h1&gt;Support / talk with founders&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://calendly.com/d/4mp-gd3-k5k/berriai-1-1-onboarding-litellm-hosted-version"&gt;Schedule Demo ğŸ‘‹&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://discord.gg/wuPM9dRgDw"&gt;Community Discord ğŸ’­&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://join.slack.com/share/enQtOTE0ODczMzk2Nzk4NC01YjUxNjY2YjBlYTFmNDRiZTM3NDFiYTM3MzVkODFiMDVjOGRjMmNmZTZkZTMzOWQzZGQyZWIwYjQ0MWExYmE3"&gt;Community Slack ğŸ’­&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Our numbers ğŸ“ +1 (770) 8783-106 / â€­+1 (412) 618-6238â€¬&lt;/li&gt; 
 &lt;li&gt;Our emails âœ‰ï¸ &lt;a href="mailto:ishaan@berri.ai"&gt;ishaan@berri.ai&lt;/a&gt; / &lt;a href="mailto:krrish@berri.ai"&gt;krrish@berri.ai&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Why did we build this&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Need for simplicity&lt;/strong&gt;: Our code started to get extremely complicated managing &amp;amp; translating calls between Azure, OpenAI and Cohere.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Contributors&lt;/h1&gt; 
&lt;!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section --&gt; 
&lt;!-- prettier-ignore-start --&gt; 
&lt;!-- markdownlint-disable --&gt; 
&lt;!-- markdownlint-restore --&gt; 
&lt;!-- prettier-ignore-end --&gt; 
&lt;!-- ALL-CONTRIBUTORS-LIST:END --&gt; 
&lt;a href="https://github.com/BerriAI/litellm/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=BerriAI/litellm"&gt; &lt;/a&gt; 
&lt;h2&gt;Run in Developer mode&lt;/h2&gt; 
&lt;h3&gt;Services&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Setup .env file in root&lt;/li&gt; 
 &lt;li&gt;Run dependant services &lt;code&gt;docker-compose up db prometheus&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Backend&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;(In root) create virtual environment &lt;code&gt;python -m venv .venv&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Activate virtual environment &lt;code&gt;source .venv/bin/activate&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Install dependencies &lt;code&gt;pip install -e ".[all]"&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Start proxy backend &lt;code&gt;uvicorn litellm.proxy.proxy_server:app --host localhost --port 4000 --reload&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Frontend&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Navigate to &lt;code&gt;ui/litellm-dashboard&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Install dependencies &lt;code&gt;npm install&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Run &lt;code&gt;npm run dev&lt;/code&gt; to start the dashboard&lt;/li&gt; 
&lt;/ol&gt;</description>
    </item>
    
  </channel>
</rss>