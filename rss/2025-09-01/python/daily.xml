<rss version="2.0">
  <channel>
    <title>GitHub Python Daily Trending</title>
    <description>Daily Trending of Python in GitHub</description>
    <pubDate>Sun, 31 Aug 2025 01:36:57 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>VectifyAI/PageIndex</title>
      <link>https://github.com/VectifyAI/PageIndex</link>
      <description>&lt;p&gt;üìÑüß† PageIndex: Document Index for Reasoning-based RAG&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;a href="https://vectify.ai/pageindex" target="_blank"&gt; &lt;img src="https://github.com/user-attachments/assets/46201e72-675b-43bc-bfbd-081cc6b65a1d" alt="PageIndex Banner" /&gt; &lt;/a&gt; 
 &lt;br /&gt; 
 &lt;br /&gt; 
 &lt;p align="center"&gt;&lt;i&gt;Reasoning-based RAG&amp;nbsp; ‚úß &amp;nbsp;No Vector DB&amp;nbsp; ‚úß &amp;nbsp;No Chunking&amp;nbsp; ‚úß &amp;nbsp;Human-like Retrieval&lt;/i&gt;&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;a href="https://vectify.ai"&gt;üè† Homepage&lt;/a&gt;&amp;nbsp; ‚Ä¢ &amp;nbsp; &lt;a href="https://dash.pageindex.ai"&gt;üñ•Ô∏è Dashboard&lt;/a&gt;&amp;nbsp; ‚Ä¢ &amp;nbsp; &lt;a href="https://docs.pageindex.ai/quickstart"&gt;üìö API Docs&lt;/a&gt;&amp;nbsp; ‚Ä¢ &amp;nbsp; &lt;a href="https://discord.com/invite/VuXuf29EUj"&gt;üí¨ Discord&lt;/a&gt;&amp;nbsp; ‚Ä¢ &amp;nbsp; &lt;a href="https://ii2abc2jejf.typeform.com/to/tK3AXl8T"&gt;‚úâÔ∏è Contact&lt;/a&gt;&amp;nbsp; &lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h1&gt;üìÑ Introduction to PageIndex&lt;/h1&gt; 
&lt;p&gt;Are you frustrated with vector database retrieval accuracy for long professional documents? Traditional vector-based RAG relies on semantic &lt;em&gt;similarity&lt;/em&gt; rather than true &lt;em&gt;relevance&lt;/em&gt;. But &lt;strong&gt;similarity ‚â† relevance&lt;/strong&gt; ‚Äî what we truly need in retrieval is &lt;strong&gt;relevance&lt;/strong&gt;, and that requires &lt;strong&gt;reasoning&lt;/strong&gt;. When working with professional documents that demand domain expertise and multi-step reasoning, similarity search often falls short.&lt;/p&gt; 
&lt;p&gt;Inspired by&amp;nbsp;AlphaGo, we propose&amp;nbsp;&lt;a href="https://vectify.ai/pageindex"&gt;PageIndex&lt;/a&gt;, a &lt;strong&gt;reasoning-based RAG&lt;/strong&gt; system that simulates how &lt;strong&gt;human experts&lt;/strong&gt; navigate and extract knowledge from long documents through &lt;strong&gt;tree search&lt;/strong&gt;, enabling LLMs to &lt;em&gt;think&lt;/em&gt; and &lt;em&gt;reason&lt;/em&gt; their way to the most relevant document sections. It performs retrieval in two steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Generate a "Table-of-Contents" &lt;strong&gt;tree structure index&lt;/strong&gt; of documents&lt;/li&gt; 
 &lt;li&gt;Perform reasoning-based retrieval through &lt;strong&gt;tree search&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://docs.pageindex.ai/images/cookbook/vectorless-rag.png" width="90%" /&gt; 
&lt;/div&gt; 
&lt;h3&gt;üí° Features&lt;/h3&gt; 
&lt;p&gt;Compared to traditional vector-based RAG, PageIndex features:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;No Vectors Needed&lt;/strong&gt;: Uses document structure and LLM reasoning for retrieval.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;No Chunking Needed&lt;/strong&gt;: Documents are organized into natural sections, not artificial chunks.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Human-like Retrieval&lt;/strong&gt;: Simulates how human experts navigate and extract knowledge from complex documents.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Transparent Retrieval Process&lt;/strong&gt;: Retrieval based on reasoning ‚Äî say goodbye to approximate vector search ("vibe retrieval").&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;PageIndex powers a reasoning-based RAG system that achieved &lt;a href="https://github.com/VectifyAI/Mafin2.5-FinanceBench"&gt;98.7% accuracy&lt;/a&gt; on FinanceBench, showing state-of-the-art performance in professional document analysis (see our &lt;a href="https://vectify.ai/blog/Mafin2.5"&gt;blog post&lt;/a&gt; for details).&lt;/p&gt; 
&lt;h3&gt;üöÄ Deployment Options&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;üõ†Ô∏è Self-host ‚Äî run locally with this open-source repo&lt;/li&gt; 
 &lt;li&gt;‚òÅÔ∏è &lt;strong&gt;&lt;a href="https://dash.pageindex.ai/"&gt;Cloud Service&lt;/a&gt;&lt;/strong&gt; ‚Äî try instantly with our üñ•Ô∏è &lt;a href="https://dash.pageindex.ai/"&gt;Dashboard&lt;/a&gt; or üîå &lt;a href="https://docs.pageindex.ai/quickstart"&gt;API&lt;/a&gt;, no setup required&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;‚ö° Quick Hands-on&lt;/h3&gt; 
&lt;p&gt;Check out this simple &lt;a href="https://github.com/VectifyAI/PageIndex/raw/main/cookbook/pageindex_RAG_simple.ipynb"&gt;&lt;em&gt;Vectorless RAG Notebook&lt;/em&gt;&lt;/a&gt; ‚Äî a minimal, hands-on, reasoning-based RAG pipeline using &lt;strong&gt;PageIndex&lt;/strong&gt;.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://colab.research.google.com/github/VectifyAI/PageIndex/blob/main/cookbook/pageindex_RAG_simple.ipynb"&gt; &lt;img src="https://img.shields.io/badge/Open_In_Colab-Vectorless_RAG_With_PageIndex-orange?style=for-the-badge&amp;amp;logo=googlecolab" alt="Open in Colab" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;h1&gt;üì¶ PageIndex Tree Structure&lt;/h1&gt; 
&lt;p&gt;PageIndex can transform lengthy PDF documents into a semantic&amp;nbsp;&lt;strong&gt;tree structure&lt;/strong&gt;, similar to a&amp;nbsp;&lt;em&gt;"table of contents"&lt;/em&gt;&amp;nbsp;but optimized for use with Large Language Models (LLMs). It's ideal for: financial reports, regulatory filings, academic textbooks, legal or technical manuals, and any document that exceeds LLM context limits.&lt;/p&gt; 
&lt;p&gt;Here is an example output. See more &lt;a href="https://github.com/VectifyAI/PageIndex/tree/main/tests/pdfs"&gt;example documents&lt;/a&gt; and &lt;a href="https://github.com/VectifyAI/PageIndex/tree/main/tests/results"&gt;generated trees&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;...
{
  "title": "Financial Stability",
  "node_id": "0006",
  "start_index": 21,
  "end_index": 22,
  "summary": "The Federal Reserve ...",
  "nodes": [
    {
      "title": "Monitoring Financial Vulnerabilities",
      "node_id": "0007",
      "start_index": 22,
      "end_index": 28,
      "summary": "The Federal Reserve's monitoring ..."
    },
    {
      "title": "Domestic and International Cooperation and Coordination",
      "node_id": "0008",
      "start_index": 28,
      "end_index": 31,
      "summary": "In 2023, the Federal Reserve collaborated ..."
    }
  ]
}
...
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can either generate the PageIndex tree structure with this open-source repo or try our ‚òÅÔ∏è &lt;strong&gt;&lt;a href="https://dash.pageindex.ai/"&gt;Cloud Service&lt;/a&gt;&lt;/strong&gt; ‚Äî instantly accessible via our üñ•Ô∏è &lt;a href="https://dash.pageindex.ai/"&gt;Dashboard&lt;/a&gt; or üîå &lt;a href="https://docs.pageindex.ai/quickstart"&gt;API&lt;/a&gt;, with no setup required.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h1&gt;üöÄ Package Usage&lt;/h1&gt; 
&lt;p&gt;You can follow these steps to generate a PageIndex tree from a PDF document.&lt;/p&gt; 
&lt;h3&gt;1. Install dependencies&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip3 install --upgrade -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. Set your OpenAI API key&lt;/h3&gt; 
&lt;p&gt;Create a &lt;code&gt;.env&lt;/code&gt; file in the root directory and add your API key:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;CHATGPT_API_KEY=your_openai_key_here
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;3. Run PageIndex on your PDF&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python3 run_pageindex.py --pdf_path /path/to/your/document.pdf
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;Optional parameters&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;p&gt;You can customize the processing with additional optional arguments:&lt;/p&gt; 
 &lt;pre&gt;&lt;code&gt;--model                 OpenAI model to use (default: gpt-4o-2024-11-20)
--toc-check-pages       Pages to check for table of contents (default: 20)
--max-pages-per-node    Max pages per node (default: 10)
--max-tokens-per-node   Max tokens per node (default: 20000)
--if-add-node-id        Add node ID (yes/no, default: yes)
--if-add-node-summary   Add node summary (yes/no, default: no)
--if-add-doc-description Add doc description (yes/no, default: yes)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h1&gt;‚òÅÔ∏è Improved Tree Generation with PageIndex OCR&lt;/h1&gt; 
&lt;p&gt;This repo is designed for generating PageIndex tree structure for simple PDFs, but many real-world use cases involve complex PDFs that are hard to parsed by classic python tools. However, extracting high-quality text from PDF documents remains a non-trivial challenge. Most OCR tools only extract page-level content, losing the broader document context and hierarchy.&lt;/p&gt; 
&lt;p&gt;To address this, we introduced PageIndex OCR ‚Äî the first long-context OCR model designed to preserve the global structure of documents. PageIndex OCR significantly outperforms other leading OCR tools, such as those from Mistral and Contextual AI, in recognizing true hierarchy and semantic relationships across document pages.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Experience next-level OCR quality with PageIndex OCR at our&amp;nbsp;&lt;a href="https://dash.pageindex.ai/"&gt;Dashboard&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Integrate seamlessly PageIndex OCR into your stack via our&amp;nbsp;&lt;a href="https://docs.pageindex.ai/quickstart"&gt;API&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="center"&gt; &lt;img src="https://github.com/user-attachments/assets/eb35d8ae-865c-4e60-a33b-ebbd00c41732" width="90%" /&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;h1&gt;üìà Case Study: Mafin 2.5 on FinanceBench&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://vectify.ai/mafin"&gt;Mafin 2.5&lt;/a&gt; is a state-of-the-art reasoning-based RAG model designed specifically for financial document analysis. Powered by &lt;strong&gt;PageIndex&lt;/strong&gt;, it achieved a market-leading &lt;a href="https://vectify.ai/blog/Mafin2.5"&gt;&lt;strong&gt;98.7% accuracy&lt;/strong&gt;&lt;/a&gt; on the &lt;a href="https://arxiv.org/abs/2311.11944"&gt;FinanceBench&lt;/a&gt; benchmark ‚Äî significantly outperforming traditional vector-based RAG systems.&lt;/p&gt; 
&lt;p&gt;PageIndex's hierarchical indexing enabled precise navigation and extraction of relevant content from complex financial reports, such as SEC filings and earnings disclosures.&lt;/p&gt; 
&lt;p&gt;üëâ See the full &lt;a href="https://github.com/VectifyAI/Mafin2.5-FinanceBench"&gt;benchmark results&lt;/a&gt; and our &lt;a href="https://vectify.ai/blog/Mafin2.5"&gt;blog post&lt;/a&gt; for detailed comparisons and performance metrics.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/VectifyAI/Mafin2.5-FinanceBench"&gt; &lt;img src="https://github.com/user-attachments/assets/571aa074-d803-43c7-80c4-a04254b782a3" width="90%" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h1&gt;üîé Learn More about PageIndex&lt;/h1&gt; 
&lt;p&gt;See the &lt;a href="https://docs.pageindex.ai/doc-search"&gt;Tutorials&lt;/a&gt; for step-by-step guides, including Document Search and Tree Search.&lt;/p&gt; 
&lt;p&gt;Check out the &lt;a href="https://docs.pageindex.ai/cookbook/vectorless-rag-pageindex"&gt;Cookbook&lt;/a&gt; for practical recipes and advanced use cases.&lt;/p&gt; 
&lt;p&gt;Refer to the &lt;a href="https://docs.pageindex.ai/quickstart"&gt;API Documentation&lt;/a&gt; for integration details and options.&lt;/p&gt; 
&lt;h1&gt;‚≠ê Support Us&lt;/h1&gt; 
&lt;p&gt;Leave a star if you like our project ‚Äî thank you!&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://github.com/user-attachments/assets/eae4ff38-48ae-4a7c-b19f-eab81201d794" width="75%" /&gt; &lt;/p&gt; 
&lt;p&gt;¬© 2025 &lt;a href="https://vectify.ai"&gt;Vectify AI&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>pathwaycom/pathway</title>
      <link>https://github.com/pathwaycom/pathway</link>
      <description>&lt;p&gt;Python ETL framework for stream processing, real-time analytics, LLM pipelines, and RAG.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;a href="https://pathway.com/"&gt; &lt;img src="https://pathway.com/logo-light.svg?sanitize=true" /&gt; &lt;/a&gt; 
 &lt;br /&gt;
 &lt;br /&gt; 
 &lt;a href="https://trendshift.io/repositories/10388" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/10388" alt="pathwaycom%2Fpathway | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; 
 &lt;br /&gt;
 &lt;br /&gt; 
&lt;/div&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/pathwaycom/pathway/actions/workflows/ubuntu_test.yml"&gt; &lt;img src="https://github.com/pathwaycom/pathway/actions/workflows/ubuntu_test.yml/badge.svg?sanitize=true" alt="ubuntu" /&gt; &lt;br /&gt; &lt;/a&gt;&lt;a href="https://github.com/pathwaycom/pathway/actions/workflows/release.yml"&gt; &lt;img src="https://github.com/pathwaycom/pathway/actions/workflows/release.yml/badge.svg?sanitize=true" alt="Last release" /&gt;&lt;/a&gt; &lt;a href="https://badge.fury.io/py/pathway"&gt;&lt;img src="https://badge.fury.io/py/pathway.svg?sanitize=true" alt="PyPI version" height="18" /&gt;&lt;/a&gt; &lt;a href="https://badge.fury.io/py/pathway"&gt;&lt;img src="https://static.pepy.tech/badge/pathway" alt="PyPI downloads" height="18" /&gt;&lt;/a&gt; &lt;a href="https://github.com/pathwaycom/pathway/raw/main/LICENSE.txt"&gt; &lt;img src="https://img.shields.io/badge/license-BSL-green" alt="License: BSL" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;a href="https://discord.gg/pathway"&gt; &lt;img src="https://img.shields.io/discord/1042405378304004156?logo=discord" alt="chat on Discord" /&gt;&lt;/a&gt; &lt;a href="https://twitter.com/intent/follow?screen_name=pathway_com"&gt; &lt;img src="https://img.shields.io/twitter/follow/pathwaycom" alt="follow on Twitter" /&gt;&lt;/a&gt; &lt;a href="https://linkedin.com/company/pathway"&gt; &lt;img src="https://img.shields.io/badge/pathway-0077B5?style=social&amp;amp;logo=linkedin" alt="follow on LinkedIn" /&gt;&lt;/a&gt; &lt;a href="https://github.com/dylanhogg/awesome-python/raw/main/README.md"&gt; &lt;img src="https://awesome.re/badge.svg?sanitize=true" alt="Awesome Python" /&gt;&lt;/a&gt; &lt;a href="https://gurubase.io/g/pathway"&gt; &lt;img src="https://img.shields.io/badge/Gurubase-Ask%20Pathway%20Guru-006BFF" alt="Pathway Guru" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;a href="https://raw.githubusercontent.com/pathwaycom/pathway/main/#getting-started"&gt;Getting Started&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/pathwaycom/pathway/main/#deployment"&gt;Deployment&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/pathwaycom/pathway/main/#resources"&gt;Documentation and Support&lt;/a&gt; | &lt;a href="https://pathway.com/blog/"&gt;Blog&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/pathwaycom/pathway/main/#license"&gt;License&lt;/a&gt; &lt;/p&gt; 
&lt;h1&gt;Pathway&lt;a id="pathway"&gt; Live Data Framework&lt;/a&gt;&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://pathway.com"&gt;Pathway&lt;/a&gt; is a Python ETL framework for stream processing, real-time analytics, LLM pipelines, and RAG.&lt;/p&gt; 
&lt;p&gt;Pathway comes with an &lt;strong&gt;easy-to-use Python API&lt;/strong&gt;, allowing you to seamlessly integrate your favorite Python ML libraries. Pathway code is versatile and robust: &lt;strong&gt;you can use it in both development and production environments, handling both batch and streaming data effectively&lt;/strong&gt;. The same code can be used for local development, CI/CD tests, running batch jobs, handling stream replays, and processing data streams.&lt;/p&gt; 
&lt;p&gt;Pathway is powered by a &lt;strong&gt;scalable Rust engine&lt;/strong&gt; based on Differential Dataflow and performs incremental computation. Your Pathway code, despite being written in Python, is run by the Rust engine, enabling multithreading, multiprocessing, and distributed computations. All the pipeline is kept in memory and can be easily deployed with &lt;strong&gt;Docker and Kubernetes&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;You can install Pathway with pip:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install -U pathway
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For any questions, you will find the community and team behind the project &lt;a href="https://discord.com/invite/pathway"&gt;on Discord&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Use-cases and templates&lt;/h2&gt; 
&lt;p&gt;Ready to see what Pathway can do?&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://pathway.com/developers/templates"&gt;Try one of our easy-to-run examples&lt;/a&gt;!&lt;/p&gt; 
&lt;p&gt;Available in both notebook and docker formats, these ready-to-launch examples can be launched in just a few clicks. Pick one and start your hands-on experience with Pathway today!&lt;/p&gt; 
&lt;h3&gt;Event processing and real-time analytics pipelines&lt;/h3&gt; 
&lt;p&gt;With its unified engine for batch and streaming and its full Python compatibility, Pathway makes data processing as easy as possible. It's the ideal solution for a wide range of data processing pipelines, including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://pathway.com/developers/templates/kafka-etl"&gt;Showcase: Real-time ETL.&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pathway.com/developers/templates/realtime-log-monitoring"&gt;Showcase: Event-driven pipelines with alerting.&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pathway.com/developers/templates/linear_regression_with_kafka/"&gt;Showcase: Realtime analytics.&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pathway.com/developers/user-guide/connecting-to-data/switch-from-batch-to-streaming"&gt;Docs: Switch from batch to streaming.&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;AI Pipelines&lt;/h3&gt; 
&lt;p&gt;Pathway provides dedicated LLM tooling to build live LLM and RAG pipelines. Wrappers for most common LLM services and utilities are included, making working with LLMs and RAGs pipelines incredibly easy. Check out our &lt;a href="https://pathway.com/developers/user-guide/llm-xpack/overview"&gt;LLM xpack documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Don't hesitate to try one of our runnable examples featuring LLM tooling. You can find such examples &lt;a href="https://pathway.com/developers/user-guide/llm-xpack/llm-examples"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://pathway.com/developers/templates/unstructured-to-structured/"&gt;Template: Unstructured data to SQL on-the-fly.&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pathway.com/developers/templates/private-rag-ollama-mistral"&gt;Template: Private RAG with Ollama and Mistral AI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pathway.com/developers/templates/adaptive-rag"&gt;Template: Adaptive RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pathway.com/developers/templates/multimodal-rag"&gt;Template: Multimodal RAG with gpt-4o&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;A wide range of connectors&lt;/strong&gt;: Pathway comes with connectors that connect to external data sources such as Kafka, GDrive, PostgreSQL, or SharePoint. Its Airbyte connector allows you to connect to more than 300 different data sources. If the connector you want is not available, you can build your own custom connector using Pathway Python connector.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Stateless and stateful transformations&lt;/strong&gt;: Pathway supports stateful transformations such as joins, windowing, and sorting. It provides many transformations directly implemented in Rust. In addition to the provided transformation, you can use any Python function. You can implement your own or you can use any Python library to process your data.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Persistence&lt;/strong&gt;: Pathway provides persistence to save the state of the computation. This allows you to restart your pipeline after an update or a crash. Your pipelines are in good hands with Pathway!&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Consistency&lt;/strong&gt;: Pathway handles the time for you, making sure all your computations are consistent. In particular, Pathway manages late and out-of-order points by updating its results whenever new (or late, in this case) data points come into the system. The free version of Pathway gives the "at least once" consistency while the enterprise version provides the "exactly once" consistency.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Scalable Rust engine&lt;/strong&gt;: with Pathway Rust engine, you are free from the usual limits imposed by Python. You can easily do multithreading, multiprocessing, and distributed computations.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;LLM helpers&lt;/strong&gt;: Pathway provides an LLM extension with all the utilities to integrate LLMs with your data pipelines (LLM wrappers, parsers, embedders, splitters), including an in-memory real-time Vector Index, and integrations with LLamaIndex and LangChain. You can quickly build and deploy RAG applications with your live documents.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Getting started&lt;a id="getting-started"&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;h3&gt;Installation&lt;a id="installation"&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Pathway requires Python 3.10 or above.&lt;/p&gt; 
&lt;p&gt;You can install the current release of Pathway using &lt;code&gt;pip&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ pip install -U pathway
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;‚ö†Ô∏è Pathway is available on MacOS and Linux. Users of other systems should run Pathway on a Virtual Machine.&lt;/p&gt; 
&lt;h3&gt;Example: computing the sum of positive values in real time.&lt;a id="example"&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import pathway as pw

# Define the schema of your data (Optional)
class InputSchema(pw.Schema):
  value: int

# Connect to your data using connectors
input_table = pw.io.csv.read(
  "./input/",
  schema=InputSchema
)

#Define your operations on the data
filtered_table = input_table.filter(input_table.value&amp;gt;=0)
result_table = filtered_table.reduce(
  sum_value = pw.reducers.sum(filtered_table.value)
)

# Load your results to external systems
pw.io.jsonlines.write(result_table, "output.jsonl")

# Run the computation
pw.run()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Run Pathway &lt;a href="https://colab.research.google.com/drive/1aBIJ2HCng-YEUOMrr0qtj0NeZMEyRz55?usp=sharing"&gt;in Google Colab&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You can find more examples &lt;a href="https://github.com/pathwaycom/pathway/tree/main/examples"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Deployment&lt;a id="deployment"&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;h3&gt;Locally&lt;a id="running-pathway-locally"&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;To use Pathway, you only need to import it:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import pathway as pw
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Now, you can easily create your processing pipeline, and let Pathway handle the updates. Once your pipeline is created, you can launch the computation on streaming data with a one-line command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;pw.run()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can then run your Pathway project (say, &lt;code&gt;main.py&lt;/code&gt;) just like a normal Python script: &lt;code&gt;$ python main.py&lt;/code&gt;. Pathway comes with a monitoring dashboard that allows you to keep track of the number of messages sent by each connector and the latency of the system. The dashboard also includes log messages.&lt;/p&gt; 
&lt;img src="https://d14l3brkh44201.cloudfront.net/pathway-dashboard.png" width="1326" alt="Pathway dashboard" /&gt; 
&lt;p&gt;Alternatively, you can use the pathway'ish version:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ pathway spawn python main.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Pathway natively supports multithreading. To launch your application with 3 threads, you can do as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ pathway spawn --threads 3 python main.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To jumpstart a Pathway project, you can use our &lt;a href="https://github.com/pathwaycom/cookiecutter-pathway"&gt;cookiecutter template&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Docker&lt;a id="docker"&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;You can easily run Pathway using docker.&lt;/p&gt; 
&lt;h4&gt;Pathway image&lt;/h4&gt; 
&lt;p&gt;You can use the &lt;a href="https://hub.docker.com/r/pathwaycom/pathway"&gt;Pathway docker image&lt;/a&gt;, using a Dockerfile:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-dockerfile"&gt;FROM pathwaycom/pathway:latest

WORKDIR /app

COPY requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD [ "python", "./your-script.py" ]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can then build and run the Docker image:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;docker build -t my-pathway-app .
docker run -it --rm --name my-pathway-app my-pathway-app
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Run a single Python script&lt;/h4&gt; 
&lt;p&gt;When dealing with single-file projects, creating a full-fledged &lt;code&gt;Dockerfile&lt;/code&gt; might seem unnecessary. In such scenarios, you can execute a Python script directly using the Pathway Docker image. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;docker run -it --rm --name my-pathway-app -v "$PWD":/app pathwaycom/pathway:latest python my-pathway-app.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Python docker image&lt;/h4&gt; 
&lt;p&gt;You can also use a standard Python image and install Pathway using pip with a Dockerfile:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-dockerfile"&gt;FROM --platform=linux/x86_64 python:3.10

RUN pip install -U pathway
COPY ./pathway-script.py pathway-script.py

CMD ["python", "-u", "pathway-script.py"]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Kubernetes and cloud&lt;a id="k8s"&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Docker containers are ideally suited for deployment on the cloud with Kubernetes. If you want to scale your Pathway application, you may be interested in our Pathway for Enterprise. Pathway for Enterprise is specially tailored towards end-to-end data processing and real time intelligent analytics. It scales using distributed computing on the cloud and supports distributed Kubernetes deployment, with external persistence setup.&lt;/p&gt; 
&lt;p&gt;You can easily deploy Pathway using services like Render: see &lt;a href="https://pathway.com/developers/user-guide/deployment/render-deploy/"&gt;how to deploy Pathway in a few clicks&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you are interested, don't hesitate to &lt;a href="mailto:contact@pathway.com"&gt;contact us&lt;/a&gt; to learn more.&lt;/p&gt; 
&lt;h2&gt;Performance&lt;a id="performance"&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;Pathway is made to outperform state-of-the-art technologies designed for streaming and batch data processing tasks, including: Flink, Spark, and Kafka Streaming. It also makes it possible to implement a lot of algorithms/UDF's in streaming mode which are not readily supported by other streaming frameworks (especially: temporal joins, iterative graph algorithms, machine learning routines).&lt;/p&gt; 
&lt;p&gt;If you are curious, here are &lt;a href="https://github.com/pathwaycom/pathway-benchmarks"&gt;some benchmarks to play with&lt;/a&gt;.&lt;/p&gt; 
&lt;img src="https://github.com/pathwaycom/pathway-benchmarks/raw/main/images/bm-wordcount-lineplot.png" width="1326" alt="WordCount Graph" /&gt; 
&lt;h2&gt;Documentation and Support&lt;a id="resources"&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;The entire documentation of Pathway is available at &lt;a href="https://pathway.com/developers/user-guide/introduction/welcome"&gt;pathway.com/developers/&lt;/a&gt;, including the &lt;a href="https://pathway.com/developers/api-docs/pathway"&gt;API Docs&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you have any question, don't hesitate to &lt;a href="https://github.com/pathwaycom/pathway/issues"&gt;open an issue on GitHub&lt;/a&gt;, join us on &lt;a href="https://discord.com/invite/pathway"&gt;Discord&lt;/a&gt;, or send us an email at &lt;a href="mailto:contact@pathway.com"&gt;contact@pathway.com&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;a id="license"&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;Pathway is distributed on a &lt;a href="https://github.com/pathwaycom/pathway/raw/main/LICENSE.txt"&gt;BSL 1.1 License&lt;/a&gt; which allows for unlimited non-commercial use, as well as use of the Pathway package &lt;a href="https://pathway.com/license/"&gt;for most commercial purposes&lt;/a&gt;, free of charge. Code in this repository automatically converts to Open Source (Apache 2.0 License) after 4 years. Some &lt;a href="https://github.com/pathwaycom"&gt;public repos&lt;/a&gt; which are complementary to this one (examples, libraries, connectors, etc.) are licensed as Open Source, under the MIT license.&lt;/p&gt; 
&lt;h2&gt;Contribution guidelines&lt;a id="contribution-guidelines"&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;If you develop a library or connector which you would like to integrate with this repo, we suggest releasing it first as a separate repo on a MIT/Apache 2.0 license.&lt;/p&gt; 
&lt;p&gt;For all concerns regarding core Pathway functionalities, Issues are encouraged. For further information, don't hesitate to engage with Pathway's &lt;a href="https://discord.gg/pathway"&gt;Discord community&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>laramies/theHarvester</title>
      <link>https://github.com/laramies/theHarvester</link>
      <description>&lt;p&gt;E-mails, subdomains and names Harvester - OSINT&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://github.com/laramies/theHarvester/raw/master/theHarvester-logo.webp" alt="theHarvester" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/laramies/theHarvester/workflows/TheHarvester%20Python%20CI/badge.svg?sanitize=true" alt="TheHarvester CI" /&gt; &lt;img src="https://github.com/laramies/theHarvester/workflows/TheHarvester%20Docker%20Image%20CI/badge.svg?sanitize=true" alt="TheHarvester Docker Image CI" /&gt; &lt;a href="https://inventory.raw.pm/"&gt;&lt;img src="https://inventory.raw.pm/img/badges/Rawsec-inventoried-FF5050_flat_without_logo.svg?sanitize=true" alt="Rawsec's CyberSecurity Inventory" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;About&lt;/h2&gt; 
&lt;p&gt;theHarvester is a simple to use, yet powerful tool designed to be used during the reconnaissance stage of a red team assessment or penetration test. It performs open source intelligence (OSINT) gathering to help determine a domain's external threat landscape. The tool gathers names, emails, IPs, subdomains, and URLs by using multiple public resources that include:&lt;/p&gt; 
&lt;h2&gt;Install and dependencies&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.12 or higher.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/laramies/theHarvester/wiki/Installation"&gt;https://github.com/laramies/theHarvester/wiki/Installation&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Install uv:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -LsSf https://astral.sh/uv/install.sh | sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Clone the repository:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/laramies/theHarvester
cd theHarvester
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Install dependencies and create a virtual environment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv sync
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Run theHarvester:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run theHarvester
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Development&lt;/h2&gt; 
&lt;p&gt;To install development dependencies:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv sync --extra dev
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To run tests:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run pytest
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To run linting and formatting:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run ruff check
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run ruff format
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Passive modules&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;baidu: Baidu search engine (&lt;a href="https://www.baidu.com"&gt;https://www.baidu.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;bevigil: CloudSEK BeVigil scans mobile application for OSINT assets (&lt;a href="https://bevigil.com/osint-api"&gt;https://bevigil.com/osint-api&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;brave: Brave search engine - now uses official Brave Search API (&lt;a href="https://api-dashboard.search.brave.com"&gt;https://api-dashboard.search.brave.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;bufferoverun: Fast domain name lookups for TLS certificates in IPv4 space (&lt;a href="https://tls.bufferover.run"&gt;https://tls.bufferover.run&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;builtwith: Find out what websites are built with (&lt;a href="https://builtwith.com"&gt;https://builtwith.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;censys: Uses certificates searches to enumerate subdomains and gather emails (&lt;a href="https://censys.io"&gt;https://censys.io&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;certspotter: Cert Spotter monitors Certificate Transparency logs (&lt;a href="https://sslmate.com/certspotter"&gt;https://sslmate.com/certspotter&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;criminalip: Specialized Cyber Threat Intelligence (CTI) search engine (&lt;a href="https://www.criminalip.io"&gt;https://www.criminalip.io&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;crtsh: Comodo Certificate search (&lt;a href="https://crt.sh"&gt;https://crt.sh&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;dehashed: Take your data security to the next level is (&lt;a href="https://dehashed.com"&gt;https://dehashed.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;dnsdumpster: Domain research tool that can discover hosts related to a domain (&lt;a href="https://dnsdumpster.com"&gt;https://dnsdumpster.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;duckduckgo: DuckDuckGo search engine (&lt;a href="https://duckduckgo.com"&gt;https://duckduckgo.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;fullhunt: Next-generation attack surface security platform (&lt;a href="https://fullhunt.io"&gt;https://fullhunt.io&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;github-code: GitHub code search engine (&lt;a href="https://www.github.com"&gt;https://www.github.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;hackertarget: Online vulnerability scanners and network intelligence to help organizations (&lt;a href="https://hackertarget.com"&gt;https://hackertarget.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;haveibeenpwned: Check if your email address is in a data breach (&lt;a href="https://haveibeenpwned.com"&gt;https://haveibeenpwned.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;hunter: Hunter search engine (&lt;a href="https://hunter.io"&gt;https://hunter.io&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;hunterhow: Internet search engines for security researchers (&lt;a href="https://hunter.how"&gt;https://hunter.how&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;intelx: Intelx search engine (&lt;a href="https://intelx.io"&gt;https://intelx.io&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;leaklookup: Data breach search engine (&lt;a href="https://leak-lookup.com"&gt;https://leak-lookup.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;netlas: A Shodan or Censys competitor (&lt;a href="https://app.netlas.io"&gt;https://app.netlas.io&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;onyphe: Cyber defense search engine (&lt;a href="https://www.onyphe.io"&gt;https://www.onyphe.io&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;otx: AlienVault open threat exchange (&lt;a href="https://otx.alienvault.com"&gt;https://otx.alienvault.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;pentesttools: Cloud-based toolkit for offensive security testing, focused on web applications and network penetration testing (&lt;a href="https://pentest-tools.com"&gt;https://pentest-tools.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;projecdiscovery: Actively collects and maintains internet-wide assets data, to enhance research and analyse changes around DNS for better insights (&lt;a href="https://chaos.projectdiscovery.io"&gt;https://chaos.projectdiscovery.io&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;rapiddns: DNS query tool which make querying subdomains or sites of a same IP easy (&lt;a href="https://rapiddns.io"&gt;https://rapiddns.io&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;rocketreach: Access real-time verified personal/professional emails, phone numbers, and social media links (&lt;a href="https://rocketreach.co"&gt;https://rocketreach.co&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;securityscorecard: helps TPRM and SOC teams detect, prioritize, and remediate vendor risk across their entire supplier ecosystem at scale (&lt;a href="https://securityscorecard.com"&gt;https://securityscorecard.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;securityTrails: Security Trails search engine, the world's largest repository of historical DNS data (&lt;a href="https://securitytrails.com"&gt;https://securitytrails.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;-s, --shodan: Shodan search engine will search for ports and banners from discovered hosts (&lt;a href="https://shodan.io"&gt;https://shodan.io&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;subdomaincenter: A subdomain finder tool used to find subdomains of a given domain (&lt;a href="https://www.subdomain.center"&gt;https://www.subdomain.center&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;subdomainfinderc99: A subdomain finder is a tool used to find the subdomains of a given domain (&lt;a href="https://subdomainfinder.c99.nl"&gt;https://subdomainfinder.c99.nl&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;threatminer: Data mining for threat intelligence (&lt;a href="https://www.threatminer.org"&gt;https://www.threatminer.org&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;tomba: Tomba search engine (&lt;a href="https://tomba.io"&gt;https://tomba.io&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;urlscan: A sandbox for the web that is a URL and website scanner (&lt;a href="https://urlscan.io"&gt;https://urlscan.io&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;venacus: Venacus search engine (&lt;a href="https://venacus.com"&gt;https://venacus.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;virustotal: Domain search (&lt;a href="https://www.virustotal.com"&gt;https://www.virustotal.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;whoisxml: Subdomain search (&lt;a href="https://subdomains.whoisxmlapi.com/api/pricing"&gt;https://subdomains.whoisxmlapi.com/api/pricing&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;yahoo: Yahoo search engine (&lt;a href="https://www.yahoo.com"&gt;https://www.yahoo.com&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;zoomeye: China's version of Shodan (&lt;a href="https://www.zoomeye.org"&gt;https://www.zoomeye.org&lt;/a&gt;)&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Active modules&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;DNS brute force: dictionary brute force enumeration&lt;/li&gt; 
 &lt;li&gt;Screenshots: Take screenshots of subdomains that were found&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Modules that require an API key&lt;/h2&gt; 
&lt;p&gt;Documentation to setup API keys can be found at - &lt;a href="https://github.com/laramies/theHarvester/wiki/Installation#api-keys"&gt;https://github.com/laramies/theHarvester/wiki/Installation#api-keys&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;bevigil - 50 free queries/month, 1k queries/month $50&lt;/li&gt; 
 &lt;li&gt;brave - Free plan available, Pro plans for higher limits&lt;/li&gt; 
 &lt;li&gt;bufferoverun - 100 free queries/month, 10k/month $25&lt;/li&gt; 
 &lt;li&gt;builtwith - 50 free queries ever, $2950/yr&lt;/li&gt; 
 &lt;li&gt;censys - 500 credits $100&lt;/li&gt; 
 &lt;li&gt;criminalip - 100 free queries/month, 700k/month $59&lt;/li&gt; 
 &lt;li&gt;dehashed - 500 credts $15, 5k credits $150&lt;/li&gt; 
 &lt;li&gt;dnsdumpster - 50 free querries/day, $49&lt;/li&gt; 
 &lt;li&gt;fullhunt - 50 free queries, 200 queries $29/month, 500 queries $59/month&lt;/li&gt; 
 &lt;li&gt;github-code&lt;/li&gt; 
 &lt;li&gt;haveibeenpwned - 10 email searches/min $4.50, 50 email searches/min $22&lt;/li&gt; 
 &lt;li&gt;hunter - 50 credits/month free, 12k credits/yr $34&lt;/li&gt; 
 &lt;li&gt;hunterhow - 10k free API results per 30 days, 50k API results per 30 days $10&lt;/li&gt; 
 &lt;li&gt;intelx&lt;/li&gt; 
 &lt;li&gt;leaklookup - 20 credits $10, 50 credits $20, 140 credits $50, 300 credits $100&lt;/li&gt; 
 &lt;li&gt;netlas - 50 free requests/day, 1k requests $49, 10k requests $249&lt;/li&gt; 
 &lt;li&gt;onyphe - 10M results/month $587&lt;/li&gt; 
 &lt;li&gt;pentesttools - 5 assets netsec $95/month, 5 assets webnetsec $140/month&lt;/li&gt; 
 &lt;li&gt;projecdiscovery - requires work email. Free monthly discovery and vulnerability scans on sign-up email domain, enterprise $&lt;/li&gt; 
 &lt;li&gt;rocketreach - 100 email lookups/month $48, 250 email lookups/month $108&lt;/li&gt; 
 &lt;li&gt;securityscorecard&lt;/li&gt; 
 &lt;li&gt;securityTrails - 50 free queries/month, 20k queries/month $500&lt;/li&gt; 
 &lt;li&gt;shodan - Freelancer $69 month, Small Business $359 month&lt;/li&gt; 
 &lt;li&gt;tomba - 25 searches/month free, 1k searches/month $39, 5k searches/month $89&lt;/li&gt; 
 &lt;li&gt;venacus - 1 search/day free, 10 searches/day $12, 30 searches/day $36&lt;/li&gt; 
 &lt;li&gt;whoisxml - 2k queries $50, 5k queries $105&lt;/li&gt; 
 &lt;li&gt;zoomeye - 5 results/day free, 30/results/day $190/yr&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Comments, bugs, and requests&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://twitter.com/laramies"&gt;&lt;img src="https://img.shields.io/twitter/follow/laramies.svg?style=social&amp;amp;label=Follow" alt="Twitter Follow" /&gt;&lt;/a&gt; Christian Martorella @laramies &lt;a href="mailto:cmartorella@edge-security.com"&gt;cmartorella@edge-security.com&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://twitter.com/NotoriousRebel1"&gt;&lt;img src="https://img.shields.io/twitter/follow/NotoriousRebel1.svg?style=social&amp;amp;label=Follow" alt="Twitter Follow" /&gt;&lt;/a&gt; Matthew Brown @NotoriousRebel1&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://twitter.com/jay_townsend1"&gt;&lt;img src="https://img.shields.io/twitter/follow/jay_townsend1.svg?style=social&amp;amp;label=Follow" alt="Twitter Follow" /&gt;&lt;/a&gt; Jay "L1ghtn1ng" Townsend @jay_townsend1&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Main contributors&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://twitter.com/NotoriousRebel1"&gt;&lt;img src="https://img.shields.io/twitter/follow/NotoriousRebel1.svg?style=social&amp;amp;label=Follow" alt="Twitter Follow" /&gt;&lt;/a&gt; Matthew Brown @NotoriousRebel1&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://twitter.com/jay_townsend1"&gt;&lt;img src="https://img.shields.io/twitter/follow/jay_townsend1.svg?style=social&amp;amp;label=Follow" alt="Twitter Follow" /&gt;&lt;/a&gt; Jay "L1ghtn1ng" Townsend @jay_townsend1&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://twitter.com/discoverscripts"&gt;&lt;img src="https://img.shields.io/twitter/follow/discoverscripts.svg?style=social&amp;amp;label=Follow" alt="Twitter Follow" /&gt;&lt;/a&gt; Lee Baird @discoverscripts&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Thanks&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;John Matherly - Shodan project&lt;/li&gt; 
 &lt;li&gt;Ahmed Aboul Ela - subdomain names dictionaries (big and small)&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>TheAlgorithms/Python</title>
      <link>https://github.com/TheAlgorithms/Python</link>
      <description>&lt;p&gt;All Algorithms implemented in Python&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;!-- Title: --&gt; 
 &lt;a href="https://github.com/TheAlgorithms/"&gt; &lt;img src="https://raw.githubusercontent.com/TheAlgorithms/website/1cd824df116b27029f17c2d1b42d81731f28a920/public/logo.svg?sanitize=true" height="100" /&gt; &lt;/a&gt; 
 &lt;h1&gt;&lt;a href="https://github.com/TheAlgorithms/"&gt;The Algorithms&lt;/a&gt; - Python&lt;/h1&gt; 
 &lt;!-- Labels: --&gt; 
 &lt;!-- First row: --&gt; 
 &lt;a href="https://gitpod.io/#https://github.com/TheAlgorithms/Python"&gt; &lt;img src="https://img.shields.io/badge/Gitpod-Ready--to--Code-blue?logo=gitpod&amp;amp;style=flat-square" height="20" alt="Gitpod Ready-to-Code" /&gt; &lt;/a&gt; 
 &lt;a href="https://github.com/TheAlgorithms/Python/raw/master/CONTRIBUTING.md"&gt; &lt;img src="https://img.shields.io/static/v1.svg?label=Contributions&amp;amp;message=Welcome&amp;amp;color=0059b3&amp;amp;style=flat-square" height="20" alt="Contributions Welcome" /&gt; &lt;/a&gt; 
 &lt;img src="https://img.shields.io/github/repo-size/TheAlgorithms/Python.svg?label=Repo%20size&amp;amp;style=flat-square" height="20" /&gt; 
 &lt;a href="https://the-algorithms.com/discord"&gt; &lt;img src="https://img.shields.io/discord/808045925556682782.svg?logo=discord&amp;amp;colorB=7289DA&amp;amp;style=flat-square" height="20" alt="Discord chat" /&gt; &lt;/a&gt; 
 &lt;a href="https://gitter.im/TheAlgorithms/community"&gt; &lt;img src="https://img.shields.io/badge/Chat-Gitter-ff69b4.svg?label=Chat&amp;amp;logo=gitter&amp;amp;style=flat-square" height="20" alt="Gitter chat" /&gt; &lt;/a&gt; 
 &lt;!-- Second row: --&gt; 
 &lt;br /&gt; 
 &lt;a href="https://github.com/TheAlgorithms/Python/actions"&gt; &lt;img src="https://img.shields.io/github/actions/workflow/status/TheAlgorithms/Python/build.yml?branch=master&amp;amp;label=CI&amp;amp;logo=github&amp;amp;style=flat-square" height="20" alt="GitHub Workflow Status" /&gt; &lt;/a&gt; 
 &lt;a href="https://github.com/pre-commit/pre-commit"&gt; &lt;img src="https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&amp;amp;logoColor=white&amp;amp;style=flat-square" height="20" alt="pre-commit" /&gt; &lt;/a&gt; 
 &lt;a href="https://docs.astral.sh/ruff/formatter/"&gt; &lt;img src="https://img.shields.io/static/v1?label=code%20style&amp;amp;message=ruff&amp;amp;color=black&amp;amp;style=flat-square" height="20" alt="code style: black" /&gt; &lt;/a&gt; 
 &lt;!-- Short description: --&gt; 
 &lt;h3&gt;All algorithms implemented in Python - for education üìö&lt;/h3&gt; 
&lt;/div&gt; 
&lt;p&gt;Implementations are for learning purposes only. They may be less efficient than the implementations in the Python standard library. Use them at your discretion.&lt;/p&gt; 
&lt;h2&gt;üöÄ Getting Started&lt;/h2&gt; 
&lt;p&gt;üìã Read through our &lt;a href="https://raw.githubusercontent.com/TheAlgorithms/Python/master/CONTRIBUTING.md"&gt;Contribution Guidelines&lt;/a&gt; before you contribute.&lt;/p&gt; 
&lt;h2&gt;üåê Community Channels&lt;/h2&gt; 
&lt;p&gt;We are on &lt;a href="https://the-algorithms.com/discord"&gt;Discord&lt;/a&gt; and &lt;a href="https://gitter.im/TheAlgorithms/community"&gt;Gitter&lt;/a&gt;! Community channels are a great way for you to ask questions and get help. Please join us!&lt;/p&gt; 
&lt;h2&gt;üìú List of Algorithms&lt;/h2&gt; 
&lt;p&gt;See our &lt;a href="https://raw.githubusercontent.com/TheAlgorithms/Python/master/DIRECTORY.md"&gt;directory&lt;/a&gt; for easier navigation and a better overview of the project.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>microsoft/qlib</title>
      <link>https://github.com/microsoft/qlib</link>
      <description>&lt;p&gt;Qlib is an AI-oriented Quant investment platform that aims to use AI tech to empower Quant Research, from exploring ideas to implementing productions. Qlib supports diverse ML modeling paradigms, including supervised learning, market dynamics modeling, and RL, and is now equipped with https://github.com/microsoft/RD-Agent to automate R&amp;D process.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href="https://pypi.org/project/pyqlib/#files"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/pyqlib.svg?logo=python&amp;amp;logoColor=white" alt="Python Versions" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/pyqlib/#files"&gt;&lt;img src="https://img.shields.io/badge/platform-linux%20%7C%20windows%20%7C%20macos-lightgrey" alt="Platform" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/pyqlib/#history"&gt;&lt;img src="https://img.shields.io/pypi/v/pyqlib" alt="PypI Versions" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/pyqlib/"&gt;&lt;img src="https://github.com/microsoft/qlib/workflows/Upload%20Python%20Package/badge.svg?sanitize=true" alt="Upload Python Package" /&gt;&lt;/a&gt; &lt;a href="https://github.com/microsoft/qlib/actions"&gt;&lt;img src="https://github.com/microsoft/qlib/workflows/Test/badge.svg?branch=main" alt="Github Actions Test Status" /&gt;&lt;/a&gt; &lt;a href="https://qlib.readthedocs.io/en/latest/?badge=latest"&gt;&lt;img src="https://readthedocs.org/projects/qlib/badge/?version=latest" alt="Documentation Status" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/LICENSE"&gt;&lt;img src="https://img.shields.io/pypi/l/pyqlib" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://gitter.im/Microsoft/qlib?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&amp;amp;utm_content=badge"&gt;&lt;img src="https://badges.gitter.im/Microsoft/qlib.svg?sanitize=true" alt="Join the chat at https://gitter.im/Microsoft/qlib" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;&lt;span&gt;üì∞&lt;/span&gt; &lt;strong&gt;What's NEW!&lt;/strong&gt; &amp;nbsp; &lt;span&gt;üíñ&lt;/span&gt;&lt;/h2&gt; 
&lt;p&gt;Recent released features&lt;/p&gt; 
&lt;h3&gt;Introducing &lt;a href="https://github.com/microsoft/RD-Agent"&gt;&lt;img src="https://raw.githubusercontent.com/microsoft/qlib/main/docs/_static/img/rdagent_logo.png" alt="RD_Agent" style="height: 2em" /&gt;&lt;/a&gt;: LLM-Based Autonomous Evolving Agents for Industrial Data-Driven R&amp;amp;D&lt;/h3&gt; 
&lt;p&gt;We are excited to announce the release of &lt;strong&gt;RD-Agent&lt;/strong&gt;üì¢, a powerful tool that supports automated factor mining and model optimization in quant investment R&amp;amp;D.&lt;/p&gt; 
&lt;p&gt;RD-Agent is now available on &lt;a href="https://github.com/microsoft/RD-Agent"&gt;GitHub&lt;/a&gt;, and we welcome your starüåü!&lt;/p&gt; 
&lt;p&gt;To learn more, please visit our &lt;a href="https://rdagent.azurewebsites.net/"&gt;‚ôæÔ∏èDemo page&lt;/a&gt;. Here, you will find demo videos in both English and Chinese to help you better understand the scenario and usage of RD-Agent.&lt;/p&gt; 
&lt;p&gt;We have prepared several demo videos for you:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Scenario&lt;/th&gt; 
   &lt;th&gt;Demo video (English)&lt;/th&gt; 
   &lt;th&gt;Demo video (‰∏≠Êñá)&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Quant Factor Mining&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://rdagent.azurewebsites.net/factor_loop?lang=en"&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://rdagent.azurewebsites.net/factor_loop?lang=zh"&gt;Link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Quant Factor Mining from reports&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://rdagent.azurewebsites.net/report_factor?lang=en"&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://rdagent.azurewebsites.net/report_factor?lang=zh"&gt;Link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Quant Model Optimization&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://rdagent.azurewebsites.net/model_loop?lang=en"&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://rdagent.azurewebsites.net/model_loop?lang=zh"&gt;Link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìÉ&lt;strong&gt;Paper&lt;/strong&gt;: &lt;a href="https://arxiv.org/abs/2505.15155"&gt;R&amp;amp;D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üëæ&lt;strong&gt;Code&lt;/strong&gt;: &lt;a href="https://github.com/microsoft/RD-Agent/"&gt;https://github.com/microsoft/RD-Agent/&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-BibTeX"&gt;@misc{li2025rdagentquant,
    title={R\&amp;amp;D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization},
    author={Yuante Li and Xu Yang and Xiao Yang and Minrui Xu and Xisen Wang and Weiqing Liu and Jiang Bian},
    year={2025},
    eprint={2505.15155},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/3198bc10-47ba-4ee0-8a8e-46d5ce44f45d" alt="image" /&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Feature&lt;/th&gt; 
   &lt;th&gt;Status&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/abs/2505.15155"&gt;R&amp;amp;D-Agent-Quant&lt;/a&gt; Published&lt;/td&gt; 
   &lt;td&gt;Apply R&amp;amp;D-Agent to Qlib for quant trading&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BPQP for End-to-end learning&lt;/td&gt; 
   &lt;td&gt;üìàComing soon!(&lt;a href="https://github.com/microsoft/qlib/pull/1863"&gt;Under review&lt;/a&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üî•LLM-driven Auto Quant Factoryüî•&lt;/td&gt; 
   &lt;td&gt;üöÄ Released in &lt;a href="https://github.com/microsoft/RD-Agent"&gt;‚ôæÔ∏èRD-Agent&lt;/a&gt; on Aug 8, 2024&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;KRNN and Sandwich models&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;üìà&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/1414/"&gt;Released&lt;/a&gt; on May 26, 2023&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Release Qlib v0.9.0&lt;/td&gt; 
   &lt;td&gt;&lt;img alt="octocat" src="https://github.githubassets.com/images/icons/emoji/octocat.png?v8" /&gt;) &lt;a href="https://github.com/microsoft/qlib/releases/tag/v0.9.0"&gt;Released&lt;/a&gt; on Dec 9, 2022&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;RL Learning Framework&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;üî®&lt;/span&gt; &lt;span&gt;üìà&lt;/span&gt; Released on Nov 10, 2022. &lt;a href="https://github.com/microsoft/qlib/pull/1332"&gt;#1332&lt;/a&gt;, &lt;a href="https://github.com/microsoft/qlib/pull/1322"&gt;#1322&lt;/a&gt;, &lt;a href="https://github.com/microsoft/qlib/pull/1316"&gt;#1316&lt;/a&gt;,&lt;a href="https://github.com/microsoft/qlib/pull/1299"&gt;#1299&lt;/a&gt;,&lt;a href="https://github.com/microsoft/qlib/pull/1263"&gt;#1263&lt;/a&gt;, &lt;a href="https://github.com/microsoft/qlib/pull/1244"&gt;#1244&lt;/a&gt;, &lt;a href="https://github.com/microsoft/qlib/pull/1169"&gt;#1169&lt;/a&gt;, &lt;a href="https://github.com/microsoft/qlib/pull/1125"&gt;#1125&lt;/a&gt;, &lt;a href="https://github.com/microsoft/qlib/pull/1076"&gt;#1076&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;HIST and IGMTF models&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;üìà&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/1040"&gt;Released&lt;/a&gt; on Apr 10, 2022&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Qlib &lt;a href="https://github.com/microsoft/qlib/tree/main/examples/tutorial"&gt;notebook tutorial&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;üìñ &lt;a href="https://github.com/microsoft/qlib/pull/1037"&gt;Released&lt;/a&gt; on Apr 7, 2022&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ibovespa index data&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;üçö&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/990"&gt;Released&lt;/a&gt; on Apr 6, 2022&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Point-in-Time database&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;üî®&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/343"&gt;Released&lt;/a&gt; on Mar 10, 2022&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Arctic Provider Backend &amp;amp; Orderbook data example&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;üî®&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/744"&gt;Released&lt;/a&gt; on Jan 17, 2022&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Meta-Learning-based framework &amp;amp; DDG-DA&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;üìà&lt;/span&gt; &lt;span&gt;üî®&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/743"&gt;Released&lt;/a&gt; on Jan 10, 2022&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Planning-based portfolio optimization&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;üî®&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/754"&gt;Released&lt;/a&gt; on Dec 28, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Release Qlib v0.8.0&lt;/td&gt; 
   &lt;td&gt;&lt;img alt="octocat" src="https://github.githubassets.com/images/icons/emoji/octocat.png?v8" /&gt;) &lt;a href="https://github.com/microsoft/qlib/releases/tag/v0.8.0"&gt;Released&lt;/a&gt; on Dec 8, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ADD model&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;üìà&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/704"&gt;Released&lt;/a&gt; on Nov 22, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ADARNN model&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;üìà&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/689"&gt;Released&lt;/a&gt; on Nov 14, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;TCN model&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;üìà&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/668"&gt;Released&lt;/a&gt; on Nov 4, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Nested Decision Framework&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;üî®&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/438"&gt;Released&lt;/a&gt; on Oct 1, 2021. &lt;a href="https://github.com/microsoft/qlib/raw/main/examples/nested_decision_execution/workflow.py"&gt;Example&lt;/a&gt; and &lt;a href="https://qlib.readthedocs.io/en/latest/component/highfreq.html"&gt;Doc&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Temporal Routing Adaptor (TRA)&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;üìà&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/531"&gt;Released&lt;/a&gt; on July 30, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Transformer &amp;amp; Localformer&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;üìà&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/508"&gt;Released&lt;/a&gt; on July 22, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Release Qlib v0.7.0&lt;/td&gt; 
   &lt;td&gt;&lt;img alt="octocat" src="https://github.githubassets.com/images/icons/emoji/octocat.png?v8" /&gt;) &lt;a href="https://github.com/microsoft/qlib/releases/tag/v0.7.0"&gt;Released&lt;/a&gt; on July 12, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;TCTS Model&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;üìà&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/491"&gt;Released&lt;/a&gt; on July 1, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Online serving and automatic model rolling&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;üî®&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/290"&gt;Released&lt;/a&gt; on May 17, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;DoubleEnsemble Model&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;üìà&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/286"&gt;Released&lt;/a&gt; on Mar 2, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;High-frequency data processing example&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;üî®&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/257"&gt;Released&lt;/a&gt; on Feb 5, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;High-frequency trading example&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;üìà&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/227"&gt;Part of code released&lt;/a&gt; on Jan 28, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;High-frequency data(1min)&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;üçö&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/221"&gt;Released&lt;/a&gt; on Jan 27, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Tabnet Model&lt;/td&gt; 
   &lt;td&gt;&lt;span&gt;üìà&lt;/span&gt; &lt;a href="https://github.com/microsoft/qlib/pull/205"&gt;Released&lt;/a&gt; on Jan 22, 2021&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Features released before 2021 are not listed here.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/qlib/main/docs/_static/img/logo/1.png" /&gt; &lt;/p&gt; 
&lt;p&gt;Qlib is an open-source, AI-oriented quantitative investment platform that aims to realize the potential, empower research, and create value using AI technologies in quantitative investment, from exploring ideas to implementing productions. Qlib supports diverse machine learning modeling paradigms, including supervised learning, market dynamics modeling, and reinforcement learning.&lt;/p&gt; 
&lt;p&gt;An increasing number of SOTA Quant research works/papers in diverse paradigms are being released in Qlib to collaboratively solve key challenges in quantitative investment. For example, 1) using supervised learning to mine the market's complex non-linear patterns from rich and heterogeneous financial data, 2) modeling the dynamic nature of the financial market using adaptive concept drift technology, and 3) using reinforcement learning to model continuous investment decisions and assist investors in optimizing their trading strategies.&lt;/p&gt; 
&lt;p&gt;It contains the full ML pipeline of data processing, model training, back-testing; and covers the entire chain of quantitative investment: alpha seeking, risk modeling, portfolio optimization, and order execution. For more details, please refer to our paper &lt;a href="https://arxiv.org/abs/2009.11189"&gt;"Qlib: An AI-oriented Quantitative Investment Platform"&lt;/a&gt;.&lt;/p&gt; 
&lt;table&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Frameworks, Tutorial, Data &amp;amp; DevOps&lt;/th&gt; 
   &lt;th&gt;Main Challenges &amp;amp; Solutions in Quant Research&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt; &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#plans"&gt;&lt;strong&gt;Plans&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#framework-of-qlib"&gt;Framework of Qlib&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#quick-start"&gt;Quick Start&lt;/a&gt;&lt;/li&gt; 
    &lt;ul dir="auto"&gt; 
     &lt;li type="circle"&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#installation"&gt;Installation&lt;/a&gt; &lt;/li&gt; 
     &lt;li type="circle"&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#data-preparation"&gt;Data Preparation&lt;/a&gt;&lt;/li&gt; 
     &lt;li type="circle"&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#auto-quant-research-workflow"&gt;Auto Quant Research Workflow&lt;/a&gt;&lt;/li&gt; 
     &lt;li type="circle"&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#building-customized-quant-research-workflow-by-code"&gt;Building Customized Quant Research Workflow by Code&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt; &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#quant-dataset-zoo"&gt;&lt;strong&gt;Quant Dataset Zoo&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#learning-framework"&gt;Learning Framework&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#more-about-qlib"&gt;More About Qlib&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#offline-mode-and-online-mode"&gt;Offline Mode and Online Mode&lt;/a&gt; 
     &lt;ul&gt; 
      &lt;li type="circle"&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#performance-of-qlib-data-server"&gt;Performance of Qlib Data Server&lt;/a&gt;&lt;/li&gt;
     &lt;/ul&gt; &lt;/li&gt;&lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#related-reports"&gt;Related Reports&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#contact-us"&gt;Contact Us&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; &lt;/td&gt; 
   &lt;td valign="baseline"&gt; &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#main-challenges--solutions-in-quant-research"&gt;Main Challenges &amp;amp; Solutions in Quant Research&lt;/a&gt; 
     &lt;ul&gt; 
      &lt;li type="circle"&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#forecasting-finding-valuable-signalspatterns"&gt;Forecasting: Finding Valuable Signals/Patterns&lt;/a&gt; 
       &lt;ul&gt; 
        &lt;li type="disc"&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#quant-model-paper-zoo"&gt;&lt;strong&gt;Quant Model (Paper) Zoo&lt;/strong&gt;&lt;/a&gt; 
         &lt;ul&gt; 
          &lt;li type="circle"&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#run-a-single-model"&gt;Run a Single Model&lt;/a&gt;&lt;/li&gt; 
          &lt;li type="circle"&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#run-multiple-models"&gt;Run Multiple Models&lt;/a&gt;&lt;/li&gt; 
         &lt;/ul&gt; &lt;/li&gt; 
       &lt;/ul&gt; &lt;/li&gt; 
      &lt;li type="circle"&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#adapting-to-market-dynamics"&gt;Adapting to Market Dynamics&lt;/a&gt;&lt;/li&gt; 
      &lt;li type="circle"&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#reinforcement-learning-modeling-continuous-decisions"&gt;Reinforcement Learning: modeling continuous decisions&lt;/a&gt;&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;Plans&lt;/h1&gt; 
&lt;p&gt;New features under development(order by estimated release time). Your feedbacks about the features are very important.&lt;/p&gt; 
&lt;!-- | Feature                        | Status      | --&gt; 
&lt;!-- | --                      | ------    | --&gt; 
&lt;h1&gt;Framework of Qlib&lt;/h1&gt; 
&lt;div style="align: center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/microsoft/qlib/main/docs/_static/img/framework-abstract.jpg" /&gt; 
&lt;/div&gt; 
&lt;p&gt;The high-level framework of Qlib can be found above(users can find the &lt;a href="https://qlib.readthedocs.io/en/latest/introduction/introduction.html#framework"&gt;detailed framework&lt;/a&gt; of Qlib's design when getting into nitty gritty). The components are designed as loose-coupled modules, and each component could be used stand-alone.&lt;/p&gt; 
&lt;p&gt;Qlib provides a strong infrastructure to support Quant research. &lt;a href="https://qlib.readthedocs.io/en/latest/component/data.html"&gt;Data&lt;/a&gt; is always an important part. A strong learning framework is designed to support diverse learning paradigms (e.g. &lt;a href="https://qlib.readthedocs.io/en/latest/component/rl.html"&gt;reinforcement learning&lt;/a&gt;, &lt;a href="https://qlib.readthedocs.io/en/latest/component/workflow.html#model-section"&gt;supervised learning&lt;/a&gt;) and patterns at different levels(e.g. &lt;a href="https://qlib.readthedocs.io/en/latest/component/meta.html"&gt;market dynamic modeling&lt;/a&gt;). By modeling the market, &lt;a href="https://qlib.readthedocs.io/en/latest/component/strategy.html"&gt;trading strategies&lt;/a&gt; will generate trade decisions that will be executed. Multiple trading strategies and executors in different levels or granularities can be &lt;a href="https://qlib.readthedocs.io/en/latest/component/highfreq.html"&gt;nested to be optimized and run together&lt;/a&gt;. At last, a comprehensive &lt;a href="https://qlib.readthedocs.io/en/latest/component/report.html"&gt;analysis&lt;/a&gt; will be provided and the model can be &lt;a href="https://qlib.readthedocs.io/en/latest/component/online.html"&gt;served online&lt;/a&gt; in a low cost.&lt;/p&gt; 
&lt;h1&gt;Quick Start&lt;/h1&gt; 
&lt;p&gt;This quick start guide tries to demonstrate&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;It's very easy to build a complete Quant research workflow and try your ideas with &lt;em&gt;Qlib&lt;/em&gt;.&lt;/li&gt; 
 &lt;li&gt;Though with &lt;em&gt;public data&lt;/em&gt; and &lt;em&gt;simple models&lt;/em&gt;, machine learning technologies &lt;strong&gt;work very well&lt;/strong&gt; in practical Quant investment.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Here is a quick &lt;strong&gt;&lt;a href="https://terminalizer.com/view/3f24561a4470"&gt;demo&lt;/a&gt;&lt;/strong&gt; shows how to install &lt;code&gt;Qlib&lt;/code&gt;, and run LightGBM with &lt;code&gt;qrun&lt;/code&gt;. &lt;strong&gt;But&lt;/strong&gt;, please make sure you have already prepared the data following the &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/#data-preparation"&gt;instruction&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;This table demonstrates the supported Python version of &lt;code&gt;Qlib&lt;/code&gt;:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;install with pip&lt;/th&gt; 
   &lt;th align="center"&gt;install from source&lt;/th&gt; 
   &lt;th align="center"&gt;plot&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Python 3.8&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Python 3.9&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Python 3.10&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Python 3.11&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Python 3.12&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;span&gt;‚úî&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Conda&lt;/strong&gt; is suggested for managing your Python environment. In some cases, using Python outside of a &lt;code&gt;conda&lt;/code&gt; environment may result in missing header files, causing the installation failure of certain packages.&lt;/li&gt; 
 &lt;li&gt;Please pay attention that installing cython in Python 3.6 will raise some error when installing &lt;code&gt;Qlib&lt;/code&gt; from source. If users use Python 3.6 on their machines, it is recommended to &lt;em&gt;upgrade&lt;/em&gt; Python to version 3.8 or higher, or use &lt;code&gt;conda&lt;/code&gt;'s Python to install &lt;code&gt;Qlib&lt;/code&gt; from source.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Install with pip&lt;/h3&gt; 
&lt;p&gt;Users can easily install &lt;code&gt;Qlib&lt;/code&gt; by pip according to the following command.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;  pip install pyqlib
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: pip will install the latest stable qlib. However, the main branch of qlib is in active development. If you want to test the latest scripts or functions in the main branch. Please install qlib with the methods below.&lt;/p&gt; 
&lt;h3&gt;Install from source&lt;/h3&gt; 
&lt;p&gt;Also, users can install the latest dev version &lt;code&gt;Qlib&lt;/code&gt; by the source code according to the following steps:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Before installing &lt;code&gt;Qlib&lt;/code&gt; from source, users need to install some dependencies:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pip install numpy
pip install --upgrade cython
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Clone the repository and install &lt;code&gt;Qlib&lt;/code&gt; as follows.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/microsoft/qlib.git &amp;amp;&amp;amp; cd qlib
pip install .  # `pip install -e .[dev]` is recommended for development. check details in docs/developer/code_standard_and_dev_guide.rst
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Tips&lt;/strong&gt;: If you fail to install &lt;code&gt;Qlib&lt;/code&gt; or run the examples in your environment, comparing your steps and the &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/.github/workflows/test_qlib_from_source.yml"&gt;CI workflow&lt;/a&gt; may help you find the problem.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Tips for Mac&lt;/strong&gt;: If you are using Mac with M1, you might encounter issues in building the wheel for LightGBM, which is due to missing dependencies from OpenMP. To solve the problem, install openmp first with &lt;code&gt;brew install libomp&lt;/code&gt; and then run &lt;code&gt;pip install .&lt;/code&gt; to build it successfully.&lt;/p&gt; 
&lt;h2&gt;Data Preparation&lt;/h2&gt; 
&lt;p&gt;‚ùó Due to more restrict data security policy. The official dataset is disabled temporarily. You can try &lt;a href="https://github.com/chenditc/investment_data/releases"&gt;this data source&lt;/a&gt; contributed by the community. Here is an example to download the latest data.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;wget https://github.com/chenditc/investment_data/releases/latest/download/qlib_bin.tar.gz
mkdir -p ~/.qlib/qlib_data/cn_data
tar -zxvf qlib_bin.tar.gz -C ~/.qlib/qlib_data/cn_data --strip-components=1
rm -f qlib_bin.tar.gz
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The official dataset below will resume in short future.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;Load and prepare data by running the following code:&lt;/p&gt; 
&lt;h3&gt;Get with module&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# get 1d data
python -m qlib.cli.data qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn

# get 1min data
python -m qlib.cli.data qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min

&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Get from source&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# get 1d data
python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn

# get 1min data
python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This dataset is created by public data collected by &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/scripts/data_collector/"&gt;crawler scripts&lt;/a&gt;, which have been released in the same repository. Users could create the same dataset with it. &lt;a href="https://github.com/microsoft/qlib/tree/main/scripts/data_collector#description-of-dataset"&gt;Description of dataset&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Please pay &lt;strong&gt;ATTENTION&lt;/strong&gt; that the data is collected from &lt;a href="https://finance.yahoo.com/lookup"&gt;Yahoo Finance&lt;/a&gt;, and the data might not be perfect. We recommend users to prepare their own data if they have a high-quality dataset. For more information, users can refer to the &lt;a href="https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format"&gt;related document&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt; 
&lt;h3&gt;Automatic update of daily frequency data (from yahoo finance)&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;This step is &lt;em&gt;Optional&lt;/em&gt; if users only want to try their models and strategies on history data.&lt;/p&gt; 
 &lt;p&gt;It is recommended that users update the data manually once (--trading_date 2021-05-25) and then set it to update automatically.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: Users can't incrementally update data based on the offline data provided by Qlib(some fields are removed to reduce the data size). Users should use &lt;a href="https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance"&gt;yahoo collector&lt;/a&gt; to download Yahoo data from scratch and then incrementally update it.&lt;/p&gt; 
 &lt;p&gt;For more information, please refer to: &lt;a href="https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance"&gt;yahoo collector&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Automatic update of data to the "qlib" directory each trading day(Linux)&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;use &lt;em&gt;crontab&lt;/em&gt;: &lt;code&gt;crontab -e&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;set up timed tasks:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;* * * * 1-5 python &amp;lt;script path&amp;gt; update_data_to_bin --qlib_data_1d_dir &amp;lt;user data dir&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;script path&lt;/strong&gt;: &lt;em&gt;scripts/data_collector/yahoo/collector.py&lt;/em&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Manual update of data&lt;/p&gt; &lt;pre&gt;&lt;code&gt;python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir &amp;lt;user data dir&amp;gt; --trading_date &amp;lt;start date&amp;gt; --end_date &amp;lt;end date&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;em&gt;trading_date&lt;/em&gt;: start of trading day&lt;/li&gt; 
   &lt;li&gt;&lt;em&gt;end_date&lt;/em&gt;: end of trading day(not included)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Checking the health of the data&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;We provide a script to check the health of the data, you can run the following commands to check whether the data is healthy or not. &lt;pre&gt;&lt;code&gt;python scripts/check_data_health.py check_data --qlib_dir ~/.qlib/qlib_data/cn_data
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Of course, you can also add some parameters to adjust the test results, such as this. &lt;pre&gt;&lt;code&gt;python scripts/check_data_health.py check_data --qlib_dir ~/.qlib/qlib_data/cn_data --missing_data_num 30055 --large_step_threshold_volume 94485 --large_step_threshold_price 20
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;If you want more information about &lt;code&gt;check_data_health&lt;/code&gt;, please refer to the &lt;a href="https://qlib.readthedocs.io/en/latest/component/data.html#checking-the-health-of-the-data"&gt;documentation&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- 
- Run the initialization code and get stock data:

  ```python
  import qlib
  from qlib.data import D
  from qlib.constant import REG_CN

  # Initialization
  mount_path = "~/.qlib/qlib_data/cn_data"  # target_dir
  qlib.init(mount_path=mount_path, region=REG_CN)

  # Get stock data by Qlib
  # Load trading calendar with the given time range and frequency
  print(D.calendar(start_time='2010-01-01', end_time='2017-12-31', freq='day')[:2])

  # Parse a given market name into a stockpool config
  instruments = D.instruments('csi500')
  print(D.list_instruments(instruments=instruments, start_time='2010-01-01', end_time='2017-12-31', as_list=True)[:6])

  # Load features of certain instruments in given time range
  instruments = ['SH600000']
  fields = ['$close', '$volume', 'Ref($close, 1)', 'Mean($close, 3)', '$high-$low']
  print(D.features(instruments, fields, start_time='2010-01-01', end_time='2017-12-31', freq='day').head())
  ```
 --&gt; 
&lt;h2&gt;Docker images&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Pulling a docker image from a docker hub repository &lt;pre&gt;&lt;code class="language-bash"&gt;docker pull pyqlib/qlib_image_stable:stable
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Start a new Docker container &lt;pre&gt;&lt;code class="language-bash"&gt;docker run -it --name &amp;lt;container name&amp;gt; -v &amp;lt;Mounted local directory&amp;gt;:/app qlib_image_stable
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;At this point you are in the docker environment and can run the qlib scripts. An example: &lt;pre&gt;&lt;code class="language-bash"&gt;&amp;gt;&amp;gt;&amp;gt; python scripts/get_data.py qlib_data --name qlib_data_simple --target_dir ~/.qlib/qlib_data/cn_data --interval 1d --region cn
&amp;gt;&amp;gt;&amp;gt; python qlib/cli/run.py examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Exit the container &lt;pre&gt;&lt;code class="language-bash"&gt;&amp;gt;&amp;gt;&amp;gt; exit
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Restart the container &lt;pre&gt;&lt;code class="language-bash"&gt;docker start -i -a &amp;lt;container name&amp;gt;
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Stop the container &lt;pre&gt;&lt;code class="language-bash"&gt;docker stop &amp;lt;container name&amp;gt;
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Delete the container &lt;pre&gt;&lt;code class="language-bash"&gt;docker rm &amp;lt;container name&amp;gt;
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;If you want to know more information, please refer to the &lt;a href="https://qlib.readthedocs.io/en/latest/developer/how_to_build_image.html"&gt;documentation&lt;/a&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Auto Quant Research Workflow&lt;/h2&gt; 
&lt;p&gt;Qlib provides a tool named &lt;code&gt;qrun&lt;/code&gt; to run the whole workflow automatically (including building dataset, training models, backtest and evaluation). You can start an auto quant research workflow and have a graphical reports analysis according to the following steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Quant Research Workflow: Run &lt;code&gt;qrun&lt;/code&gt; with lightgbm workflow config (&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml"&gt;workflow_config_lightgbm_Alpha158.yaml&lt;/a&gt; as following.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;  cd examples  # Avoid running program under the directory contains `qlib`
  qrun benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If users want to use &lt;code&gt;qrun&lt;/code&gt; under debug mode, please use the following command:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;python -m pdb qlib/cli/run.py examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The result of &lt;code&gt;qrun&lt;/code&gt; is as follows, please refer to &lt;a href="https://qlib.readthedocs.io/en/latest/component/strategy.html#result"&gt;docs&lt;/a&gt; for more explanations about the result.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;
'The following are analysis results of the excess return without cost.'
                       risk
mean               0.000708
std                0.005626
annualized_return  0.178316
information_ratio  1.996555
max_drawdown      -0.081806
'The following are analysis results of the excess return with cost.'
                       risk
mean               0.000512
std                0.005626
annualized_return  0.128982
information_ratio  1.444287
max_drawdown      -0.091078
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here are detailed documents for &lt;code&gt;qrun&lt;/code&gt; and &lt;a href="https://qlib.readthedocs.io/en/latest/component/workflow.html"&gt;workflow&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Graphical Reports Analysis: First, run &lt;code&gt;python -m pip install .[analysis]&lt;/code&gt; to install the required dependencies. Then run &lt;code&gt;examples/workflow_by_code.ipynb&lt;/code&gt; with &lt;code&gt;jupyter notebook&lt;/code&gt; to get graphical reports.&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;Forecasting signal (model prediction) analysis&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Cumulative Return of groups &lt;img src="https://github.com/microsoft/qlib/raw/main/docs/_static/img/analysis/analysis_model_cumulative_return.png" alt="Cumulative Return" /&gt;&lt;/li&gt; 
     &lt;li&gt;Return distribution &lt;img src="https://github.com/microsoft/qlib/raw/main/docs/_static/img/analysis/analysis_model_long_short.png" alt="long_short" /&gt;&lt;/li&gt; 
     &lt;li&gt;Information Coefficient (IC) &lt;img src="https://github.com/microsoft/qlib/raw/main/docs/_static/img/analysis/analysis_model_IC.png" alt="Information Coefficient" /&gt; &lt;img src="https://github.com/microsoft/qlib/raw/main/docs/_static/img/analysis/analysis_model_monthly_IC.png" alt="Monthly IC" /&gt; &lt;img src="https://github.com/microsoft/qlib/raw/main/docs/_static/img/analysis/analysis_model_NDQ.png" alt="IC" /&gt;&lt;/li&gt; 
     &lt;li&gt;Auto Correlation of forecasting signal (model prediction) &lt;img src="https://github.com/microsoft/qlib/raw/main/docs/_static/img/analysis/analysis_model_auto_correlation.png" alt="Auto Correlation" /&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Portfolio analysis&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Backtest return &lt;img src="https://github.com/microsoft/qlib/raw/main/docs/_static/img/analysis/report.png" alt="Report" /&gt;&lt;/li&gt; 
    &lt;/ul&gt; 
    &lt;!-- 
- Score IC
![Score IC](docs/_static/img/score_ic.png)
- Cumulative Return
![Cumulative Return](docs/_static/img/cumulative_return.png)
- Risk Analysis
![Risk Analysis](docs/_static/img/risk_analysis.png)
- Rank Label
![Rank Label](docs/_static/img/rank_label.png)
--&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;a href="https://qlib.readthedocs.io/en/latest/component/report.html"&gt;Explanation&lt;/a&gt; of above results&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Building Customized Quant Research Workflow by Code&lt;/h2&gt; 
&lt;p&gt;The automatic workflow may not suit the research workflow of all Quant researchers. To support a flexible Quant research workflow, Qlib also provides a modularized interface to allow researchers to build their own workflow by code. &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/workflow_by_code.ipynb"&gt;Here&lt;/a&gt; is a demo for customized Quant research workflow by code.&lt;/p&gt; 
&lt;h1&gt;Main Challenges &amp;amp; Solutions in Quant Research&lt;/h1&gt; 
&lt;p&gt;Quant investment is a very unique scenario with lots of key challenges to be solved. Currently, Qlib provides some solutions for several of them.&lt;/p&gt; 
&lt;h2&gt;Forecasting: Finding Valuable Signals/Patterns&lt;/h2&gt; 
&lt;p&gt;Accurate forecasting of the stock price trend is a very important part to construct profitable portfolios. However, huge amount of data with various formats in the financial market which make it challenging to build forecasting models.&lt;/p&gt; 
&lt;p&gt;An increasing number of SOTA Quant research works/papers, which focus on building forecasting models to mine valuable signals/patterns in complex financial data, are released in &lt;code&gt;Qlib&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks"&gt;Quant Model (Paper) Zoo&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Here is a list of models built on &lt;code&gt;Qlib&lt;/code&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/XGBoost/"&gt;GBDT based on XGBoost (Tianqi Chen, et al. KDD 2016)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/LightGBM/"&gt;GBDT based on LightGBM (Guolin Ke, et al. NIPS 2017)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/CatBoost/"&gt;GBDT based on Catboost (Liudmila Prokhorenkova, et al. NIPS 2018)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/MLP/"&gt;MLP based on pytorch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/LSTM/"&gt;LSTM based on pytorch (Sepp Hochreiter, et al. Neural computation 1997)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/GRU/"&gt;GRU based on pytorch (Kyunghyun Cho, et al. 2014)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/ALSTM"&gt;ALSTM based on pytorch (Yao Qin, et al. IJCAI 2017)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/GATs/"&gt;GATs based on pytorch (Petar Velickovic, et al. 2017)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/SFM/"&gt;SFM based on pytorch (Liheng Zhang, et al. KDD 2017)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/TFT/"&gt;TFT based on tensorflow (Bryan Lim, et al. International Journal of Forecasting 2019)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/TabNet/"&gt;TabNet based on pytorch (Sercan O. Arik, et al. AAAI 2019)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/DoubleEnsemble/"&gt;DoubleEnsemble based on LightGBM (Chuheng Zhang, et al. ICDM 2020)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/TCTS/"&gt;TCTS based on pytorch (Xueqing Wu, et al. ICML 2021)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/Transformer/"&gt;Transformer based on pytorch (Ashish Vaswani, et al. NeurIPS 2017)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/Localformer/"&gt;Localformer based on pytorch (Juyong Jiang, et al.)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/TRA/"&gt;TRA based on pytorch (Hengxu, Dong, et al. KDD 2021)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/TCN/"&gt;TCN based on pytorch (Shaojie Bai, et al. 2018)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/ADARNN/"&gt;ADARNN based on pytorch (YunTao Du, et al. 2021)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/ADD/"&gt;ADD based on pytorch (Hongshun Tang, et al.2020)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/IGMTF/"&gt;IGMTF based on pytorch (Wentao Xu, et al.2021)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/HIST/"&gt;HIST based on pytorch (Wentao Xu, et al.2021)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/KRNN/"&gt;KRNN based on pytorch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/Sandwich/"&gt;Sandwich based on pytorch&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Your PR of new Quant models is highly welcomed.&lt;/p&gt; 
&lt;p&gt;The performance of each model on the &lt;code&gt;Alpha158&lt;/code&gt; and &lt;code&gt;Alpha360&lt;/code&gt; datasets can be found &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/README.md"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Run a single model&lt;/h3&gt; 
&lt;p&gt;All the models listed above are runnable with &lt;code&gt;Qlib&lt;/code&gt;. Users can find the config files we provide and some details about the model through the &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks"&gt;benchmarks&lt;/a&gt; folder. More information can be retrieved at the model files listed above.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;Qlib&lt;/code&gt; provides three different ways to run a single model, users can pick the one that fits their cases best:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Users can use the tool &lt;code&gt;qrun&lt;/code&gt; mentioned above to run a model's workflow based from a config file.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Users can create a &lt;code&gt;workflow_by_code&lt;/code&gt; python script based on the &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/workflow_by_code.py"&gt;one&lt;/a&gt; listed in the &lt;code&gt;examples&lt;/code&gt; folder.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Users can use the script &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/run_all_model.py"&gt;&lt;code&gt;run_all_model.py&lt;/code&gt;&lt;/a&gt; listed in the &lt;code&gt;examples&lt;/code&gt; folder to run a model. Here is an example of the specific shell command to be used: &lt;code&gt;python run_all_model.py run --models=lightgbm&lt;/code&gt;, where the &lt;code&gt;--models&lt;/code&gt; arguments can take any number of models listed above(the available models can be found in &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks/"&gt;benchmarks&lt;/a&gt;). For more use cases, please refer to the file's &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/run_all_model.py"&gt;docstrings&lt;/a&gt;.&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: Each baseline has different environment dependencies, please make sure that your python version aligns with the requirements(e.g. TFT only supports Python 3.6~3.7 due to the limitation of &lt;code&gt;tensorflow==1.15.0&lt;/code&gt;)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Run multiple models&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;Qlib&lt;/code&gt; also provides a script &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/run_all_model.py"&gt;&lt;code&gt;run_all_model.py&lt;/code&gt;&lt;/a&gt; which can run multiple models for several iterations. (&lt;strong&gt;Note&lt;/strong&gt;: the script only support &lt;em&gt;Linux&lt;/em&gt; for now. Other OS will be supported in the future. Besides, it doesn't support parallel running the same model for multiple times as well, and this will be fixed in the future development too.)&lt;/p&gt; 
&lt;p&gt;The script will create a unique virtual environment for each model, and delete the environments after training. Thus, only experiment results such as &lt;code&gt;IC&lt;/code&gt; and &lt;code&gt;backtest&lt;/code&gt; results will be generated and stored.&lt;/p&gt; 
&lt;p&gt;Here is an example of running all the models for 10 iterations:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;python run_all_model.py run 10
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;It also provides the API to run specific models at once. For more use cases, please refer to the file's &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/run_all_model.py"&gt;docstrings&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Break change&lt;/h3&gt; 
&lt;p&gt;In &lt;code&gt;pandas&lt;/code&gt;, &lt;code&gt;group_key&lt;/code&gt; is one of the parameters of the &lt;code&gt;groupby&lt;/code&gt; method. From version 1.5 to 2.0 of &lt;code&gt;pandas&lt;/code&gt;, the default value of &lt;code&gt;group_key&lt;/code&gt; has been changed from &lt;code&gt;no default&lt;/code&gt; to &lt;code&gt;True&lt;/code&gt;, which will cause qlib to report an error during operation. So we set &lt;code&gt;group_key=False&lt;/code&gt;, but it doesn't guarantee that some programmes will run correctly, including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;qlib\examples\rl_order_execution\scripts\gen_training_orders.py&lt;/li&gt; 
 &lt;li&gt;qlib\examples\benchmarks\TRA\src\dataset.MTSDatasetH.py&lt;/li&gt; 
 &lt;li&gt;qlib\examples\benchmarks\TFT\tft.py&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks_dynamic"&gt;Adapting to Market Dynamics&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;Due to the non-stationary nature of the environment of the financial market, the data distribution may change in different periods, which makes the performance of models build on training data decays in the future test data. So adapting the forecasting models/strategies to market dynamics is very important to the model/strategies' performance.&lt;/p&gt; 
&lt;p&gt;Here is a list of solutions built on &lt;code&gt;Qlib&lt;/code&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks_dynamic/baseline/"&gt;Rolling Retraining&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/benchmarks_dynamic/DDG-DA/"&gt;DDG-DA on pytorch (Wendi, et al. AAAI 2022)&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Reinforcement Learning: modeling continuous decisions&lt;/h2&gt; 
&lt;p&gt;Qlib now supports reinforcement learning, a feature designed to model continuous investment decisions. This functionality assists investors in optimizing their trading strategies by learning from interactions with the environment to maximize some notion of cumulative reward.&lt;/p&gt; 
&lt;p&gt;Here is a list of solutions built on &lt;code&gt;Qlib&lt;/code&gt; categorized by scenarios.&lt;/p&gt; 
&lt;h3&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/rl_order_execution"&gt;RL for order execution&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://qlib.readthedocs.io/en/latest/component/rl/overall.html#order-execution"&gt;Here&lt;/a&gt; is the introduction of this scenario. All the methods below are compared &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/rl_order_execution"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/rl_order_execution/exp_configs/backtest_twap.yml"&gt;TWAP&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/rl_order_execution/exp_configs/backtest_ppo.yml"&gt;PPO: "An End-to-End Optimal Trade Execution Framework based on Proximal Policy Optimization", IJCAL 2020&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/rl_order_execution/exp_configs/backtest_opds.yml"&gt;OPDS: "Universal Trading for Order Execution with Oracle Policy Distillation", AAAI 2021&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Quant Dataset Zoo&lt;/h1&gt; 
&lt;p&gt;Dataset plays a very important role in Quant. Here is a list of the datasets built on &lt;code&gt;Qlib&lt;/code&gt;:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Dataset&lt;/th&gt; 
   &lt;th&gt;US Market&lt;/th&gt; 
   &lt;th&gt;China Market&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/qlib/contrib/data/handler.py"&gt;Alpha360&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚àö&lt;/td&gt; 
   &lt;td&gt;‚àö&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/qlib/contrib/data/handler.py"&gt;Alpha158&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚àö&lt;/td&gt; 
   &lt;td&gt;‚àö&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;a href="https://qlib.readthedocs.io/en/latest/advanced/alpha.html"&gt;Here&lt;/a&gt; is a tutorial to build dataset with &lt;code&gt;Qlib&lt;/code&gt;. Your PR to build new Quant dataset is highly welcomed.&lt;/p&gt; 
&lt;h1&gt;Learning Framework&lt;/h1&gt; 
&lt;p&gt;Qlib is high customizable and a lot of its components are learnable. The learnable components are instances of &lt;code&gt;Forecast Model&lt;/code&gt; and &lt;code&gt;Trading Agent&lt;/code&gt;. They are learned based on the &lt;code&gt;Learning Framework&lt;/code&gt; layer and then applied to multiple scenarios in &lt;code&gt;Workflow&lt;/code&gt; layer. The learning framework leverages the &lt;code&gt;Workflow&lt;/code&gt; layer as well(e.g. sharing &lt;code&gt;Information Extractor&lt;/code&gt;, creating environments based on &lt;code&gt;Execution Env&lt;/code&gt;).&lt;/p&gt; 
&lt;p&gt;Based on learning paradigms, they can be categorized into reinforcement learning and supervised learning.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;For supervised learning, the detailed docs can be found &lt;a href="https://qlib.readthedocs.io/en/latest/component/model.html"&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;For reinforcement learning, the detailed docs can be found &lt;a href="https://qlib.readthedocs.io/en/latest/component/rl.html"&gt;here&lt;/a&gt;. Qlib's RL learning framework leverages &lt;code&gt;Execution Env&lt;/code&gt; in &lt;code&gt;Workflow&lt;/code&gt; layer to create environments. It's worth noting that &lt;code&gt;NestedExecutor&lt;/code&gt; is supported as well. This empowers users to optimize different level of strategies/models/agents together (e.g. optimizing an order execution strategy for a specific portfolio management strategy).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;More About Qlib&lt;/h1&gt; 
&lt;p&gt;If you want to have a quick glance at the most frequently used components of qlib, you can try notebooks &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/examples/tutorial/"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The detailed documents are organized in &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/docs/"&gt;docs&lt;/a&gt;. &lt;a href="http://www.sphinx-doc.org"&gt;Sphinx&lt;/a&gt; and the readthedocs theme is required to build the documentation in html formats.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd docs/
conda install sphinx sphinx_rtd_theme -y
# Otherwise, you can install them with pip
# pip install sphinx sphinx_rtd_theme
make html
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also view the &lt;a href="http://qlib.readthedocs.io/"&gt;latest document&lt;/a&gt; online directly.&lt;/p&gt; 
&lt;p&gt;Qlib is in active and continuing development. Our plan is in the roadmap, which is managed as a &lt;a href="https://github.com/microsoft/qlib/projects/1"&gt;github project&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Offline Mode and Online Mode&lt;/h1&gt; 
&lt;p&gt;The data server of Qlib can either deployed as &lt;code&gt;Offline&lt;/code&gt; mode or &lt;code&gt;Online&lt;/code&gt; mode. The default mode is offline mode.&lt;/p&gt; 
&lt;p&gt;Under &lt;code&gt;Offline&lt;/code&gt; mode, the data will be deployed locally.&lt;/p&gt; 
&lt;p&gt;Under &lt;code&gt;Online&lt;/code&gt; mode, the data will be deployed as a shared data service. The data and their cache will be shared by all the clients. The data retrieval performance is expected to be improved due to a higher rate of cache hits. It will consume less disk space, too. The documents of the online mode can be found in &lt;a href="https://qlib-server.readthedocs.io/"&gt;Qlib-Server&lt;/a&gt;. The online mode can be deployed automatically with &lt;a href="https://qlib-server.readthedocs.io/en/latest/build.html#one-click-deployment-in-azure"&gt;Azure CLI based scripts&lt;/a&gt;. The source code of online data server can be found in &lt;a href="https://github.com/microsoft/qlib-server"&gt;Qlib-Server repository&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Performance of Qlib Data Server&lt;/h2&gt; 
&lt;p&gt;The performance of data processing is important to data-driven methods like AI technologies. As an AI-oriented platform, Qlib provides a solution for data storage and data processing. To demonstrate the performance of Qlib data server, we compare it with several other data storage solutions.&lt;/p&gt; 
&lt;p&gt;We evaluate the performance of several storage solutions by finishing the same task, which creates a dataset (14 features/factors) from the basic OHLCV daily data of a stock market (800 stocks each day from 2007 to 2020). The task involves data queries and processing.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th&gt;HDF5&lt;/th&gt; 
   &lt;th&gt;MySQL&lt;/th&gt; 
   &lt;th&gt;MongoDB&lt;/th&gt; 
   &lt;th&gt;InfluxDB&lt;/th&gt; 
   &lt;th&gt;Qlib -E -D&lt;/th&gt; 
   &lt;th&gt;Qlib +E -D&lt;/th&gt; 
   &lt;th&gt;Qlib +E +D&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Total (1CPU) (seconds)&lt;/td&gt; 
   &lt;td&gt;184.4¬±3.7&lt;/td&gt; 
   &lt;td&gt;365.3¬±7.5&lt;/td&gt; 
   &lt;td&gt;253.6¬±6.7&lt;/td&gt; 
   &lt;td&gt;368.2¬±3.6&lt;/td&gt; 
   &lt;td&gt;147.0¬±8.8&lt;/td&gt; 
   &lt;td&gt;47.6¬±1.0&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;7.4¬±0.3&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Total (64CPU) (seconds)&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;8.8¬±0.6&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;4.2¬±0.2&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;+(-)E&lt;/code&gt; indicates with (out) &lt;code&gt;ExpressionCache&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;+(-)D&lt;/code&gt; indicates with (out) &lt;code&gt;DatasetCache&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Most general-purpose databases take too much time to load data. After looking into the underlying implementation, we find that data go through too many layers of interfaces and unnecessary format transformations in general-purpose database solutions. Such overheads greatly slow down the data loading process. Qlib data are stored in a compact format, which is efficient to be combined into arrays for scientific computation.&lt;/p&gt; 
&lt;h1&gt;Related Reports&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://analyticsindiamag.com/qlib/"&gt;Guide To Qlib: Microsoft‚Äôs AI Investment Platform&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://mp.weixin.qq.com/s/47bP5YwxfTp2uTHjUBzJQQ"&gt;ÂæÆËΩØ‰πüÊêûAIÈáèÂåñÂπ≥Âè∞ÔºüËøòÊòØÂºÄÊ∫êÁöÑÔºÅ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://mp.weixin.qq.com/s/vsJv7lsgjEi-ALYUz4CvtQ"&gt;ÂæÆÁüøQlibÔºö‰∏öÂÜÖÈ¶ñ‰∏™AIÈáèÂåñÊäïËµÑÂºÄÊ∫êÂπ≥Âè∞&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Contact Us&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;If you have any issues, please create issue &lt;a href="https://github.com/microsoft/qlib/issues/new/choose"&gt;here&lt;/a&gt; or send messages in &lt;a href="https://gitter.im/Microsoft/qlib"&gt;gitter&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;If you want to make contributions to &lt;code&gt;Qlib&lt;/code&gt;, please &lt;a href="https://github.com/microsoft/qlib/compare"&gt;create pull requests&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;For other reasons, you are welcome to contact us by email(&lt;a href="mailto:qlib@microsoft.com"&gt;qlib@microsoft.com&lt;/a&gt;). 
  &lt;ul&gt; 
   &lt;li&gt;We are recruiting new members(both FTEs and interns), your resumes are welcome!&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Join IM discussion groups:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;a href="https://gitter.im/Microsoft/qlib"&gt;Gitter&lt;/a&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src="https://github.com/microsoft/qlib/raw/main/docs/_static/img/qrcode/gitter_qr.png" alt="image" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;Contributing&lt;/h1&gt; 
&lt;p&gt;We appreciate all contributions and thank all the contributors! &lt;a href="https://github.com/microsoft/qlib/graphs/contributors"&gt;&lt;img src="https://contrib.rocks/image?repo=microsoft/qlib" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Before we released Qlib as an open-source project on Github in Sep 2020, Qlib is an internal project in our group. Unfortunately, the internal commit history is not kept. A lot of members in our group have also contributed a lot to Qlib, which includes Ruihua Wang, Yinda Zhang, Haisu Yu, Shuyu Wang, Bochen Pang, and &lt;a href="https://github.com/evanzd/evanzd"&gt;Dong Zhou&lt;/a&gt;. Especially thanks to &lt;a href="https://github.com/evanzd/evanzd"&gt;Dong Zhou&lt;/a&gt; due to his initial version of Qlib.&lt;/p&gt; 
&lt;h2&gt;Guidance&lt;/h2&gt; 
&lt;p&gt;This project welcomes contributions and suggestions.&lt;br /&gt; &lt;strong&gt;Here are some &lt;a href="https://raw.githubusercontent.com/microsoft/qlib/main/docs/developer/code_standard_and_dev_guide.rst"&gt;code standards and development guidance&lt;/a&gt; for submiting a pull request.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Making contributions is not a hard thing. Solving an issue(maybe just answering a question raised in &lt;a href="https://github.com/microsoft/qlib/issues"&gt;issues list&lt;/a&gt; or &lt;a href="https://gitter.im/Microsoft/qlib"&gt;gitter&lt;/a&gt;), fixing/issuing a bug, improving the documents and even fixing a typo are important contributions to Qlib.&lt;/p&gt; 
&lt;p&gt;For example, if you want to contribute to Qlib's document/code, you can follow the steps in the figure below.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://github.com/demon143/qlib/raw/main/docs/_static/img/change%20doc.gif" /&gt; &lt;/p&gt; 
&lt;p&gt;If you don't know how to start to contribute, you can refer to the following examples.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Examples&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Solving issues&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/qlib/issues/749"&gt;Answer a question&lt;/a&gt;; &lt;a href="https://github.com/microsoft/qlib/issues/765"&gt;issuing&lt;/a&gt; or &lt;a href="https://github.com/microsoft/qlib/pull/792"&gt;fixing&lt;/a&gt; a bug&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Docs&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/qlib/pull/797/files"&gt;Improve docs quality&lt;/a&gt; ; &lt;a href="https://github.com/microsoft/qlib/pull/774"&gt;Fix a typo&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Feature&lt;/td&gt; 
   &lt;td&gt;Implement a &lt;a href="https://github.com/microsoft/qlib/projects"&gt;requested feature&lt;/a&gt; like &lt;a href="https://github.com/microsoft/qlib/pull/754"&gt;this&lt;/a&gt;; &lt;a href="https://github.com/microsoft/qlib/pull/539/files"&gt;Refactor interfaces&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Dataset&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/qlib/pull/733"&gt;Add a dataset&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Models&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/qlib/pull/689"&gt;Implement a new model&lt;/a&gt;, &lt;a href="https://github.com/microsoft/qlib/tree/main/examples/benchmarks#contributing"&gt;some instructions to contribute models&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;a href="https://github.com/microsoft/qlib/labels/good%20first%20issue"&gt;Good first issues&lt;/a&gt; are labelled to indicate that they are easy to start your contributions.&lt;/p&gt; 
&lt;p&gt;You can find some impefect implementation in Qlib by &lt;code&gt;rg 'TODO|FIXME' qlib&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;If you would like to become one of Qlib's maintainers to contribute more (e.g. help merge PR, triage issues), please contact us by email(&lt;a href="mailto:qlib@microsoft.com"&gt;qlib@microsoft.com&lt;/a&gt;). We are glad to help to upgrade your permission.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the right to use your contribution. For details, visit &lt;a href="https://cla.opensource.microsoft.com"&gt;https://cla.opensource.microsoft.com&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; 
&lt;p&gt;This project has adopted the &lt;a href="https://opensource.microsoft.com/codeofconduct/"&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href="https://opensource.microsoft.com/codeofconduct/faq/"&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href="mailto:opencode@microsoft.com"&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>pydantic/pydantic</title>
      <link>https://github.com/pydantic/pydantic</link>
      <description>&lt;p&gt;Data validation using Python type hints&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Pydantic Validation&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/pydantic/pydantic/actions?query=event%3Apush+branch%3Amain+workflow%3ACI"&gt;&lt;img src="https://img.shields.io/github/actions/workflow/status/pydantic/pydantic/ci.yml?branch=main&amp;amp;logo=github&amp;amp;label=CI" alt="CI" /&gt;&lt;/a&gt; &lt;a href="https://coverage-badge.samuelcolvin.workers.dev/redirect/pydantic/pydantic"&gt;&lt;img src="https://coverage-badge.samuelcolvin.workers.dev/pydantic/pydantic.svg?sanitize=true" alt="Coverage" /&gt;&lt;/a&gt; &lt;a href="https://pypi.python.org/pypi/pydantic"&gt;&lt;img src="https://img.shields.io/pypi/v/pydantic.svg?sanitize=true" alt="pypi" /&gt;&lt;/a&gt; &lt;a href="https://anaconda.org/conda-forge/pydantic"&gt;&lt;img src="https://img.shields.io/conda/v/conda-forge/pydantic.svg?sanitize=true" alt="CondaForge" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/pydantic"&gt;&lt;img src="https://static.pepy.tech/badge/pydantic/month" alt="downloads" /&gt;&lt;/a&gt; &lt;a href="https://github.com/pydantic/pydantic"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/pydantic.svg?sanitize=true" alt="versions" /&gt;&lt;/a&gt; &lt;a href="https://github.com/pydantic/pydantic/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/pydantic/pydantic.svg?sanitize=true" alt="license" /&gt;&lt;/a&gt; &lt;a href="https://docs.pydantic.dev/latest/contributing/#badges"&gt;&lt;img src="https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/pydantic/pydantic/main/docs/badge/v2.json" alt="Pydantic v2" /&gt;&lt;/a&gt; &lt;a href="https://docs.pydantic.dev/latest/llms.txt"&gt;&lt;img src="https://img.shields.io/badge/llms.txt-green" alt="llms.txt" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Data validation using Python type hints.&lt;/p&gt; 
&lt;p&gt;Fast and extensible, Pydantic plays nicely with your linters/IDE/brain. Define how data should be in pure, canonical Python 3.9+; validate it with Pydantic.&lt;/p&gt; 
&lt;h2&gt;Pydantic Logfire &lt;span&gt;üî•&lt;/span&gt;&lt;/h2&gt; 
&lt;p&gt;We've recently launched Pydantic Logfire to help you monitor your applications. &lt;a href="https://pydantic.dev/articles/logfire-announcement"&gt;Learn more&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Pydantic V1.10 vs. V2&lt;/h2&gt; 
&lt;p&gt;Pydantic V2 is a ground-up rewrite that offers many new features, performance improvements, and some breaking changes compared to Pydantic V1.&lt;/p&gt; 
&lt;p&gt;If you're using Pydantic V1 you may want to look at the &lt;a href="https://docs.pydantic.dev/"&gt;pydantic V1.10 Documentation&lt;/a&gt; or, &lt;a href="https://github.com/pydantic/pydantic/tree/1.10.X-fixes"&gt;&lt;code&gt;1.10.X-fixes&lt;/code&gt; git branch&lt;/a&gt;. Pydantic V2 also ships with the latest version of Pydantic V1 built in so that you can incrementally upgrade your code base and projects: &lt;code&gt;from pydantic import v1 as pydantic_v1&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Help&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://docs.pydantic.dev/"&gt;documentation&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;Install using &lt;code&gt;pip install -U pydantic&lt;/code&gt; or &lt;code&gt;conda install pydantic -c conda-forge&lt;/code&gt;. For more installation options to make Pydantic even faster, see the &lt;a href="https://docs.pydantic.dev/install/"&gt;Install&lt;/a&gt; section in the documentation.&lt;/p&gt; 
&lt;h2&gt;A Simple Example&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from datetime import datetime
from typing import Optional
from pydantic import BaseModel

class User(BaseModel):
    id: int
    name: str = 'John Doe'
    signup_ts: Optional[datetime] = None
    friends: list[int] = []

external_data = {'id': '123', 'signup_ts': '2017-06-01 12:22', 'friends': [1, '2', b'3']}
user = User(**external_data)
print(user)
#&amp;gt; User id=123 name='John Doe' signup_ts=datetime.datetime(2017, 6, 1, 12, 22) friends=[1, 2, 3]
print(user.id)
#&amp;gt; 123
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;For guidance on setting up a development environment and how to make a contribution to Pydantic, see &lt;a href="https://docs.pydantic.dev/contributing/"&gt;Contributing to Pydantic&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Reporting a Security Vulnerability&lt;/h2&gt; 
&lt;p&gt;See our &lt;a href="https://github.com/pydantic/pydantic/security/policy"&gt;security policy&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>OpenBMB/MiniCPM-V</title>
      <link>https://github.com/OpenBMB/MiniCPM-V</link>
      <description>&lt;p&gt;MiniCPM-V 4.5: A GPT-4o Level MLLM for Single Image, Multi Image and High-FPS Video Understanding on Your Phone&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpm_v_and_minicpm_o_title.png" width="500em" /&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;A GPT-4o Level MLLM for Single Image, Multi Image and High-FPS Video Understanding on Your Phone&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/README_zh.md"&gt;‰∏≠Êñá&lt;/a&gt; | English&lt;/strong&gt;&lt;/p&gt; 
 &lt;span style="display: inline-flex; align-items: center; margin-right: 2px;"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/wechat.png" alt="WeChat" style="margin-right: 4px;" /&gt; &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/docs/wechat.md" target="_blank"&gt; WeChat&lt;/a&gt; &amp;nbsp;| &lt;/span&gt; &amp;nbsp; 
 &lt;span style="display: inline-flex; align-items: center; margin-left: -8px;"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/discord.png" alt="Discord" style="margin-right: 4px;" /&gt; &lt;a href="https://discord.gg/rftuRMbqzf" target="_blank"&gt; Discord&lt;/a&gt; &amp;nbsp; &lt;/span&gt; 
 &lt;p align="center"&gt; MiniCPM-V 4.5 &lt;a href="https://huggingface.co/openbmb/MiniCPM-V-4_5"&gt;ü§ó&lt;/a&gt; &lt;a href="http://101.126.42.235:30910/"&gt;ü§ñ&lt;/a&gt; | MiniCPM-o 2.6 &lt;a href="https://huggingface.co/openbmb/MiniCPM-o-2_6"&gt;ü§ó&lt;/a&gt; &lt;a href="https://minicpm-omni-webdemo-us.modelbest.cn/"&gt; ü§ñ&lt;/a&gt; | &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-Cookbook"&gt;üç≥ Cookbook&lt;/a&gt; | üìÑ Technical Report (Coming Soon) &lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;MiniCPM-V&lt;/strong&gt; is a series of efficient end-side multimodal LLMs (MLLMs), which accept images, videos and text as inputs and deliver high-quality text outputs. &lt;strong&gt;MiniCPM-o&lt;/strong&gt; additionally takes audio as inputs and provides high-quality speech outputs in an end-to-end fashion. Since February 2024, we have released 7 versions of the model, aiming to achieve &lt;strong&gt;strong performance and efficient deployment&lt;/strong&gt;. The most notable models in the series currently include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;MiniCPM-V 4.5&lt;/strong&gt;: üî•üî•üî• The latest and most capable model in the MiniCPM-V series. With a total of 8B parameters, this model &lt;strong&gt;outperforms GPT-4o-latest, Gemini-2.0 Pro, and Qwen2.5-VL 72B&lt;/strong&gt; in vision-language capabilities, making it the most performant on-device multimodal model in the open-source community. This version brings &lt;strong&gt;new features including efficient high-FPS and long video understanding (up to 96x compression rate for video tokens), controllable hybrid fast/deep thinking, strong handwritten OCR and complex table/document parsing&lt;/strong&gt;. It also advances MiniCPM-V's popular features such as trustworthy behavior, multilingual support and end-side deployability.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;MiniCPM-o 2.6&lt;/strong&gt;: ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è The most capable model in the MiniCPM-o series. With a total of 8B parameters, this end-to-end model &lt;strong&gt;achieves comparable performance to GPT-4o-202405 in vision, speech, and multimodal live streaming&lt;/strong&gt;, making it one of the most versatile and performant models in the open-source community. For the new voice mode, MiniCPM-o 2.6 &lt;strong&gt;supports bilingual real-time speech conversation with configurable voices&lt;/strong&gt;, and also allows for fun capabilities such as emotion/speed/style control, end-to-end voice cloning, role play, etc. Due to its superior token density, MiniCPM-o 2.6 can for the first time &lt;strong&gt;support multimodal live streaming on end-side devices&lt;/strong&gt; such as iPad.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;News 
 &lt;!-- omit in toc --&gt;&lt;/h2&gt; 
&lt;h4&gt;üìå Pinned&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;[2025.08.26] üî•üî•üî• We open-source MiniCPM-V 4.5, which outperforms GPT-4o-latest, Gemini-2.0 Pro, and Qwen2.5-VL 72B. It advances popular capabilities of MiniCPM-V, and brings useful new features. Try it now!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2025.08.01] ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è We open-sourced the &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook"&gt;MiniCPM-V &amp;amp; o Cookbook&lt;/a&gt;! It provides comprehensive guides for diverse user scenarios, paired with our new &lt;a href="https://minicpm-o.readthedocs.io/en/latest/index.html"&gt;Docs Site&lt;/a&gt; for smoother onboarding.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2025.06.20] ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è Our official &lt;a href="https://ollama.com/openbmb"&gt;Ollama repository&lt;/a&gt; is released. Try our latest models with &lt;a href="https://ollama.com/openbmb/minicpm-o2.6"&gt;one click&lt;/a&gt;ÔºÅ&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2025.03.01] üöÄüöÄüöÄ RLAIF-V, the alignment technique of MiniCPM-o, is accepted by CVPR 2025 HighlightsÔºÅThe &lt;a href="https://github.com/RLHF-V/RLAIF-V"&gt;code&lt;/a&gt;, &lt;a href="https://huggingface.co/datasets/openbmb/RLAIF-V-Dataset"&gt;dataset&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/2405.17220"&gt;paper&lt;/a&gt; are open-sourced!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2025.01.24] üì¢üì¢üì¢ MiniCPM-o 2.6 technical report is released! See &lt;a href="https://openbmb.notion.site/MiniCPM-o-2-6-A-GPT-4o-Level-MLLM-for-Vision-Speech-and-Multimodal-Live-Streaming-on-Your-Phone-185ede1b7a558042b5d5e45e6b237da9"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2025.01.19] üì¢ &lt;strong&gt;ATTENTION!&lt;/strong&gt; We are currently working on merging MiniCPM-o 2.6 into the official repositories of llama.cpp, Ollama, and vllm. Until the merge is complete, please USE OUR LOCAL FORKS of &lt;a href="https://github.com/OpenBMB/llama.cpp/raw/minicpm-omni/examples/llava/README-minicpmo2.6.md"&gt;llama.cpp&lt;/a&gt;, &lt;a href="https://github.com/OpenBMB/ollama/raw/minicpm-v2.6/examples/minicpm-v2.6/README.md"&gt;Ollama&lt;/a&gt;, and &lt;a href="https://github.com/OpenBMB/MiniCPM-o?tab=readme-ov-file#efficient-inference-with-llamacpp-ollama-vllm"&gt;vllm&lt;/a&gt;. &lt;strong&gt;Using the official repositories before the merge may lead to unexpected issues&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2025.01.19] ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è MiniCPM-o tops GitHub Trending and reaches top-2 on Hugging Face Trending!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2025.01.17] We have updated the usage of MiniCPM-o 2.6 int4 quantization version and resolved the model initialization error. Click &lt;a href="https://huggingface.co/openbmb/MiniCPM-o-2_6-int4"&gt;here&lt;/a&gt; and try it now!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2025.01.13] üî•üî•üî• We open-source MiniCPM-o 2.6, which matches GPT-4o-202405 on vision, speech and multimodal live streaming. It advances popular capabilities of MiniCPM-V 2.6, and supports various new fun features. Try it now!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2024.08.17] üöÄüöÄüöÄ MiniCPM-V 2.6 is now fully supported by &lt;a href="https://github.com/ggerganov/llama.cpp"&gt;official&lt;/a&gt; llama.cpp! GGUF models of various sizes are available &lt;a href="https://huggingface.co/openbmb/MiniCPM-V-2_6-gguf"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2024.08.06] üî•üî•üî• We open-source MiniCPM-V 2.6, which outperforms GPT-4V on single image, multi-image and video understanding. It advances popular features of MiniCPM-Llama3-V 2.5, and can support real-time video understanding on iPad. Try it now!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2024.08.03] MiniCPM-Llama3-V 2.5 technical report is released! See &lt;a href="https://arxiv.org/abs/2408.01800"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;[2024.05.23] üî•üî•üî• MiniCPM-V tops GitHub Trending and Hugging Face Trending! Our demo, recommended by Hugging Face Gradio‚Äôs official account, is available &lt;a href="https://huggingface.co/spaces/openbmb/MiniCPM-Llama3-V-2_5"&gt;here&lt;/a&gt;. Come and try it out!&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click to view more news.&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;[2025.08.02] üöÄüöÄüöÄ We open-source MiniCPM-V 4.0, which outperforms GPT-4.1-mini-20250414 in image understanding. It advances popular features of MiniCPM-V 2.6, and largely improves the efficiency. We also open-source the iOS App on iPhone and iPad. Try it now!&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2025.01.23] üí°üí°üí° MiniCPM-o 2.6 is now supported by &lt;a href="https://github.com/PKU-Alignment/align-anything"&gt;Align-Anything&lt;/a&gt;, a framework by PKU-Alignment Team for aligning any-to-any modality large models with human intentions. It supports DPO and SFT fine-tuning on both vision and audio. Try it now!&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.08.15] We now also support multi-image SFT. For more details, please refer to the &lt;a href="https://github.com/OpenBMB/MiniCPM-V/tree/main/finetune"&gt;document&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.08.14] MiniCPM-V 2.6 now also supports &lt;a href="https://github.com/modelscope/ms-swift/issues/1613"&gt;fine-tuning&lt;/a&gt; with the SWIFT framework!&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.08.10] üöÄüöÄüöÄ MiniCPM-Llama3-V 2.5 is now fully supported by &lt;a href="https://github.com/ggerganov/llama.cpp"&gt;official&lt;/a&gt; llama.cpp! GGUF models of various sizes are available &lt;a href="https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5-gguf"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.07.19] MiniCPM-Llama3-V 2.5 supports vLLM now! See &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#inference-with-vllm"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.06.03] Now, you can run MiniCPM-Llama3-V 2.5 on multiple low VRAM GPUs(12 GB or 16 GB) by distributing the model's layers across multiple GPUs. For more details, check this &lt;a href="https://github.com/OpenBMB/MiniCPM-V/raw/main/docs/inference_on_multiple_gpus.md"&gt;link&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.05.28] üöÄüöÄüöÄ MiniCPM-Llama3-V 2.5 now fully supports its feature in llama.cpp and Ollama! Please pull the latest code &lt;strong&gt;of our provided forks&lt;/strong&gt; (&lt;a href="https://github.com/OpenBMB/llama.cpp/raw/minicpm-v2.5/examples/minicpmv/README.md"&gt;llama.cpp&lt;/a&gt;, &lt;a href="https://github.com/OpenBMB/ollama/tree/minicpm-v2.5/examples/minicpm-v2.5"&gt;Ollama&lt;/a&gt;). GGUF models in various sizes are available &lt;a href="https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5-gguf/tree/main"&gt;here&lt;/a&gt;. MiniCPM-Llama3-V 2.5 series is &lt;strong&gt;not supported by the official repositories yet&lt;/strong&gt;, and we are working hard to merge PRs. Please stay tuned!&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.05.28] üí´ We now support LoRA fine-tuning for MiniCPM-Llama3-V 2.5, using only 2 V100 GPUs! See more statistics &lt;a href="https://github.com/OpenBMB/MiniCPM-V/tree/main/finetune#model-fine-tuning-memory-usage-statistics"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.05.25] MiniCPM-Llama3-V 2.5 now supports streaming outputs and customized system prompts. Try it &lt;a href="https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5#usage"&gt;here&lt;/a&gt;!&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.05.24] We release the MiniCPM-Llama3-V 2.5 &lt;a href="https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5-gguf"&gt;gguf&lt;/a&gt;, which supports &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#inference-with-llamacpp"&gt;llama.cpp&lt;/a&gt; inference and provides a 6~8 token/s smooth decoding on mobile phones. Try it now!&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.05.23] üîç We've released a comprehensive comparison between Phi-3-vision-128k-instruct and MiniCPM-Llama3-V 2.5, including benchmark evaluations, multilingual capabilities, and inference efficiency üåüüìäüåçüöÄ. Click &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/docs/compare_with_phi-3_vision.md"&gt;here&lt;/a&gt; to view more details.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.05.20] We open-soure MiniCPM-Llama3-V 2.5, it has improved OCR capability and supports 30+ languages, representing the first end-side MLLM achieving GPT-4V level performance! We provide &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#deployment-on-mobile-phone"&gt;efficient inference&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/finetune/readme.md"&gt;simple fine-tuning&lt;/a&gt;. Try it now!&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.04.23] MiniCPM-V-2.0 supports vLLM now! Click &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#inference-with-vllm"&gt;here&lt;/a&gt; to view more details.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.04.18] We create a HuggingFace Space to host the demo of MiniCPM-V 2.0 at &lt;a href="https://huggingface.co/spaces/openbmb/MiniCPM-V-2"&gt;here&lt;/a&gt;!&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.04.17] MiniCPM-V-2.0 supports deploying &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#webui-demo"&gt;WebUI Demo&lt;/a&gt; now!&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.04.15] MiniCPM-V-2.0 now also supports &lt;a href="https://github.com/modelscope/swift/raw/main/docs/source/Multi-Modal/minicpm-v-2%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5.md"&gt;fine-tuning&lt;/a&gt; with the SWIFT framework!&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.04.12] We open-source MiniCPM-V 2.0, which achieves comparable performance with Gemini Pro in understanding scene text and outperforms strong Qwen-VL-Chat 9.6B and Yi-VL 34B on &lt;a href="https://rank.opencompass.org.cn/leaderboard-multimodal"&gt;OpenCompass&lt;/a&gt;, a comprehensive evaluation over 11 popular benchmarks. Click &lt;a href="https://openbmb.vercel.app/minicpm-v-2"&gt;here&lt;/a&gt; to view the MiniCPM-V 2.0 technical blog.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.03.14] MiniCPM-V now supports &lt;a href="https://github.com/modelscope/swift/raw/main/docs/source/Multi-Modal/minicpm-v%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5.md"&gt;fine-tuning&lt;/a&gt; with the SWIFT framework. Thanks to &lt;a href="https://github.com/Jintao-Huang"&gt;Jintao&lt;/a&gt; for the contributionÔºÅ&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.03.01] MiniCPM-V can now be deployed on Mac!&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;[2024.02.01] We open-source MiniCPM-V and OmniLMM-12B, which support efficient end-side deployment and powerful multimodal capabilities correspondingly.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;Contents 
 &lt;!-- omit in toc --&gt;&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#minicpm-v-45"&gt;MiniCPM-V 4.5&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#minicpm-o-26"&gt;MiniCPM-o 2.6&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#minicpm-v--o-cookbook"&gt;MiniCPM-V &amp;amp; o Cookbook&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#chat-with-our-demo-on-gradio-"&gt;Chat with Our Demo on Gradio ü§ó&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#inference"&gt;Inference&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#model-zoo"&gt;Model Zoo&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#multi-turn-conversation"&gt;Multi-turn Conversation&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#chat-with-multiple-images"&gt;Chat with Multiple Images&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#in-context-few-shot-learning"&gt;In-context Few-shot Learning&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#chat-with-video"&gt;Chat with Video&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#speech-and-audio-mode"&gt;Speech and Audio Mode&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#multimodal-live-streaming"&gt;Multimodal Live Streaming&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#inference-on-multiple-gpus"&gt;Inference on Multiple GPUs&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#inference-on-mac"&gt;Inference on Mac&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#efficient-inference-with-llamacpp-ollama-vllm"&gt;Efficient Inference with llama.cpp, Ollama, vLLM&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#fine-tuning"&gt;Fine-tuning&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#awesome-work-using-minicpm-v--minicpm-o"&gt;Awesome work using MiniCPM-V &amp;amp; MiniCPM-o&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#faqs"&gt;FAQs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#limitations"&gt;Limitations&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;MiniCPM-V 4.5&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;MiniCPM-V 4.5&lt;/strong&gt; is the latest and most capable model in the MiniCPM-V series. The model is built on Qwen3-8B and SigLIP2-400M with a total of 8B parameters. It exhibits a significant performance improvement over previous MiniCPM-V and MiniCPM-o models, and introduces new useful features. Notable features of MiniCPM-V 4.5 include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;üî• &lt;strong&gt;State-of-the-art Vision-Language Capability.&lt;/strong&gt; MiniCPM-V 4.5 achieves an average score of 77.0 on OpenCompass, a comprehensive evaluation of 8 popular benchmarks. &lt;strong&gt;With only 8B parameters, it surpasses widely used proprietary models like GPT-4o-latest, Gemini-2.0 Pro, and strong open-source models like Qwen2.5-VL 72B&lt;/strong&gt; for vision-language capabilities, making it the most performant MLLM under 30B parameters.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üé¨ &lt;strong&gt;Efficient High-FPS and Long Video Understanding.&lt;/strong&gt; Powered by a new unified 3D-Resampler over images and videos, MiniCPM-V 4.5 can now achieve 96x compression rate for video tokens, where 6 448x448 video frames can be jointly compressed into 64 video tokens (normally 1,536 tokens for most MLLMs). This means that the model can perceive significantly more video frames without increasing the LLM inference cost. This brings state-of-the-art high-FPS (up to 10FPS) video understanding and long video understanding capabilities on Video-MME, LVBench, MLVU, MotionBench, FavorBench, etc., efficiently.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;‚öôÔ∏è &lt;strong&gt;Controllable Hybrid Fast/Deep Thinking.&lt;/strong&gt; MiniCPM-V 4.5 supports both fast thinking for efficient frequent usage with competitive performance, and deep thinking for more complex problem solving. To cover efficiency and performance trade-offs in different user scenarios, this fast/deep thinking mode can be switched in a highly controlled fashion.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üí™ &lt;strong&gt;Strong OCR, Document Parsing and Others.&lt;/strong&gt; Based on &lt;a href="https://arxiv.org/pdf/2403.11703"&gt;LLaVA-UHD&lt;/a&gt; architecture, MiniCPM-V 4.5 can process high-resolution images with any aspect ratio and up to 1.8 million pixels (e.g., 1344x1344), using 4x fewer visual tokens than most MLLMs. The model achieves &lt;strong&gt;leading performance on OCRBench, surpassing proprietary models such as GPT-4o-latest and Gemini 2.5&lt;/strong&gt;. It also achieves state-of-the-art performance for PDF document parsing capability on OmniDocBench among general MLLMs. Based on the latest &lt;a href="https://github.com/RLHF-V/RLAIF-V/"&gt;RLAIF-V&lt;/a&gt; and &lt;a href="https://github.com/OpenBMB/VisCPM"&gt;VisCPM&lt;/a&gt; techniques, it features &lt;strong&gt;trustworthy behaviors&lt;/strong&gt;, outperforming GPT-4o-latest on MMHal-Bench, and supports &lt;strong&gt;multilingual capabilities&lt;/strong&gt; in more than 30 languages.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üí´ &lt;strong&gt;Easy Usage.&lt;/strong&gt; MiniCPM-V 4.5 can be easily used in various ways: (1) &lt;a href="https://github.com/tc-mb/llama.cpp/raw/Support-MiniCPM-V-4.5/docs/multimodal/minicpmv4.5.md"&gt;llama.cpp&lt;/a&gt; and &lt;a href="https://github.com/tc-mb/ollama/tree/MIniCPM-V"&gt;ollama&lt;/a&gt; support for efficient CPU inference on local devices, (2) &lt;a href="https://huggingface.co/openbmb/MiniCPM-V-4_5-int4"&gt;int4&lt;/a&gt;, &lt;a href="https://huggingface.co/openbmb/MiniCPM-V-4_5-gguf"&gt;GGUF&lt;/a&gt; and &lt;a href="https://github.com/tc-mb/AutoAWQ"&gt;AWQ&lt;/a&gt; format quantized models in 16 sizes, (3) &lt;a href="https://github.com/tc-mb/sglang/tree/main"&gt;SGLang&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#efficient-inference-with-llamacpp-ollama-vllm"&gt;vLLM&lt;/a&gt; support for high-throughput and memory-efficient inference, (4) fine-tuning on new domains and tasks with &lt;a href="https://github.com/tc-mb/transformers/tree/main"&gt;Transformers&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/docs/llamafactory_train_and_infer.md"&gt;LLaMA-Factory&lt;/a&gt;, (5) quick &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#chat-with-our-demo-on-gradio"&gt;local WebUI demo&lt;/a&gt;, (6) optimized &lt;a href="https://github.com/tc-mb/MiniCPM-o-demo-iOS"&gt;local iOS app&lt;/a&gt; on iPhone and iPad, and (7) online web demo on &lt;a href="http://101.126.42.235:30910/"&gt;server&lt;/a&gt;. See our &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook"&gt;Cookbook&lt;/a&gt; for full usage!&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Key Techniques 
 &lt;!-- omit in toc --&gt;&lt;/h3&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpm-v-4dot5-framework.png" , width="100%" /&gt; 
&lt;/div&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Architechture: Unified 3D-Resampler for High-density Video Compression.&lt;/strong&gt; MiniCPM-V 4.5 introduces a 3D-Resampler that overcomes the performance-efficiency trade-off in video understanding. By grouping and jointly compressing up to 6 consecutive video frames into just 64 tokens (the same token count used for a single image in MiniCPM-V series), MiniCPM-V 4.5 achieves a 96√ó compression rate for video tokens. This allows the model to process more video frames without additional LLM computational cost, enabling high-FPS video and long video understanding. The architecture supports unified encoding for images, multi-image inputs, and videos, ensuring seamless capability and knowledge transfer.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Pre-training: Unified Learning for OCR and Knowledge from Documents.&lt;/strong&gt; Existing MLLMs learn OCR capability and knowledge from documents in isolated training approaches. We observe that the essential difference between these two training approaches is the visibility of the text in images. By dynamically corrupting text regions in documents with varying noise levels and asking the model to reconstruct the text, the model learns to adaptively and properly switch between accurate text recognition (when text is visible) and multimodal context-based knowledge reasoning (when text is heavily obscured). This eliminates reliance on error-prone document parsers in knowledge learning from documents, and prevents hallucinations from over-augmented OCR data, resulting in top-tier OCR and multimodal knowledge performance with minimal engineering overhead.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Post-training: Hybrid Fast/Deep Thinking with Multimodal RL.&lt;/strong&gt; MiniCPM-V 4.5 offers a balanced reasoning experience through two switchable modes: fast thinking for efficient daily use and deep thinking for complex tasks. Using a new hybrid reinforcement learning method, the model jointly optimizes both modes, significantly enhancing fast-mode performance without compromising deep-mode capability. Incorporated with &lt;a href="https://github.com/OpenBMB/RLPR"&gt;RLPR&lt;/a&gt; and &lt;a href="https://github.com/RLHF-V/RLAIF-V"&gt;RLAIF-V&lt;/a&gt;, it generalizes robust reasoning skills from broad multimodal data while effectively reducing hallucinations.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Evaluation 
 &lt;!-- omit in toc --&gt;&lt;/h3&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/radar_minicpm_v45.png" , width="60%" /&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmv_4_5_evaluation_result.png" , width="80%" /&gt; 
&lt;/div&gt; 
&lt;h3&gt;Inference Efficiency&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;OpenCompass&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="left"&gt; 
 &lt;table style="margin: 0px auto;"&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="left"&gt;Model&lt;/th&gt; 
    &lt;th&gt;Size&lt;/th&gt; 
    &lt;th&gt;Avg Score ‚Üë&lt;/th&gt; 
    &lt;th&gt;Total Inference Time ‚Üì&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody align="center"&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;GLM-4.1V-9B-Thinking&lt;/td&gt; 
    &lt;td&gt;10.3B&lt;/td&gt; 
    &lt;td&gt;76.6&lt;/td&gt; 
    &lt;td&gt;17.5h&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;MiMo-VL-7B-RL&lt;/td&gt; 
    &lt;td&gt;8.3B&lt;/td&gt; 
    &lt;td&gt;76.4&lt;/td&gt; 
    &lt;td&gt;11h&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;MiniCPM-V 4.5&lt;/td&gt; 
    &lt;td&gt;8.7B&lt;/td&gt; 
    &lt;td&gt;&lt;b&gt;77.0&lt;/b&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;b&gt;7.5h&lt;/b&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Video-MME&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="left"&gt; 
 &lt;table style="margin: 0px auto;"&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="left"&gt;Model&lt;/th&gt; 
    &lt;th&gt;Size&lt;/th&gt; 
    &lt;th&gt;Avg Score ‚Üë&lt;/th&gt; 
    &lt;th&gt;Total Inference Time ‚Üì&lt;/th&gt; 
    &lt;th&gt;GPU Mem ‚Üì&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody align="center"&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;Qwen2.5-VL-7B-Instruct&lt;/td&gt; 
    &lt;td&gt;8.3B&lt;/td&gt; 
    &lt;td&gt;71.6&lt;/td&gt; 
    &lt;td&gt;3h&lt;/td&gt; 
    &lt;td&gt;60G&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;GLM-4.1V-9B-Thinking&lt;/td&gt; 
    &lt;td&gt;10.3B&lt;/td&gt; 
    &lt;td&gt;&lt;b&gt;73.6&lt;/b&gt;&lt;/td&gt; 
    &lt;td&gt;2.63h&lt;/td&gt; 
    &lt;td&gt;32G&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;MiniCPM-V 4.5&lt;/td&gt; 
    &lt;td&gt;8.7B&lt;/td&gt; 
    &lt;td&gt;73.5&lt;/td&gt; 
    &lt;td&gt;&lt;b&gt;0.26h&lt;/b&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;b&gt;28G&lt;/b&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;p&gt;Both Video-MME and OpenCompass were evaluated using 8√óA100 GPUs for inference. The reported inference time of Video-MME excludes the cost of video frame extraction.&lt;/p&gt; 
&lt;h3&gt;Examples 
 &lt;!-- omit in toc --&gt;&lt;/h3&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://www.youtube.com/watch?v=Cn23FujYMMU"&gt;&lt;img src="./assets/minicpmv4_5/MiniCPM-V 4.5-8.26_img.jpeg" , width="70%" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;div style="display: flex; flex-direction: column; align-items: center;"&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmv4_5/en_case1.png" alt="en_case1" style="margin-bottom: 5px;" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmv4_5/en_case2.png" alt="en_case2" style="margin-bottom: 5px;" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmv4_5/en_case3.jpeg" alt="en_case3" style="margin-bottom: 5px;" /&gt; 
&lt;/div&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click to view more cases.&lt;/summary&gt; 
 &lt;div style="display: flex; flex-direction: column; align-items: center;"&gt; 
  &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmv4_5/zh_extra.jpeg" alt="zh_extra" style="margin-bottom: 5px;" /&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;p&gt;We deploy MiniCPM-V 4.5 on iPad M4 with &lt;a href="https://github.com/tc-mb/MiniCPM-o-demo-iOS"&gt;iOS demo&lt;/a&gt;. The demo video is the raw screen recording without edition.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmv4_5/v45_en_handwriting.gif" width="45%/" /&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmv4_5/v45_en_cot.gif" width="45%/" /&gt; &lt;/p&gt;
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmv4_5/v45_cn_handwriting.gif" width="45%/" /&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmv4_5/v45_cn_travel.gif" width="45%/" /&gt; &lt;/p&gt;
&lt;table align="center"&gt;   
&lt;/table&gt; 
&lt;h2&gt;MiniCPM-o 2.6&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;MiniCPM-o 2.6&lt;/strong&gt; is the latest and most capable model in the MiniCPM-o series. The model is built in an end-to-end fashion based on SigLip-400M, Whisper-medium-300M, ChatTTS-200M, and Qwen2.5-7B with a total of 8B parameters. It exhibits a significant performance improvement over MiniCPM-V 2.6, and introduces new features for real-time speech conversation and multimodal live streaming. Notable features of MiniCPM-o 2.6 include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;üî• &lt;strong&gt;Leading Visual Capability.&lt;/strong&gt; MiniCPM-o 2.6 achieves an average score of 70.2 on OpenCompass, a comprehensive evaluation of 8 popular benchmarks. &lt;strong&gt;With only 8B parameters, it surpasses widely used proprietary models like GPT-4o-202405, Gemini 1.5 Pro, and Claude 3.5 Sonnet&lt;/strong&gt; for single image understanding. It also &lt;strong&gt;outperforms GPT-4V and Claude 3.5 Sonnet&lt;/strong&gt; in multi-image and video understanding, and shows promising in-context learning capability.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üéô &lt;strong&gt;State-of-the-art Speech Capability.&lt;/strong&gt; MiniCPM-o 2.6 supports &lt;strong&gt;bilingual real-time speech conversation with configurable voices&lt;/strong&gt; in English and Chinese. It &lt;strong&gt;outperforms GPT-4o-realtime on audio understanding tasks&lt;/strong&gt; such as ASR and STT translation, and shows &lt;strong&gt;state-of-the-art performance on speech conversation in both semantic and acoustic evaluations in the open-source community&lt;/strong&gt;. It also allows for fun features such as emotion/speed/style control, end-to-end voice cloning, role play, etc.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üé¨ &lt;strong&gt;Strong Multimodal Live Streaming Capability.&lt;/strong&gt; As a new feature, MiniCPM-o 2.6 can &lt;strong&gt;accept continuous video and audio streams independent of user queries, and support real-time speech interaction&lt;/strong&gt;. It &lt;strong&gt;outperforms GPT-4o-202408 and Claude 3.5 Sonnet and shows state-of-the-art performance in the open-source community on StreamingBench&lt;/strong&gt;, a comprehensive benchmark for real-time video understanding, omni-source (video &amp;amp; audio) understanding, and multimodal contextual understanding.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üí™ &lt;strong&gt;Strong OCR Capability and Others.&lt;/strong&gt; Advancing popular visual capabilities from MiniCPM-V series, MiniCPM-o 2.6 can process images with any aspect ratio and up to 1.8 million pixels (e.g., 1344x1344). It achieves &lt;strong&gt;state-of-the-art performance on OCRBench for models under 25B, surpassing proprietary models such as GPT-4o-202405&lt;/strong&gt;. Based on the latest &lt;a href="https://github.com/RLHF-V/RLAIF-V/"&gt;RLAIF-V&lt;/a&gt; and &lt;a href="https://github.com/OpenBMB/VisCPM"&gt;VisCPM&lt;/a&gt; techniques, it features &lt;strong&gt;trustworthy behaviors&lt;/strong&gt;, outperforming GPT-4o and Claude 3.5 Sonnet on MMHal-Bench, and supports &lt;strong&gt;multilingual capabilities&lt;/strong&gt; on more than 30 languages.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üöÄ &lt;strong&gt;Superior Efficiency.&lt;/strong&gt; In addition to its friendly size, MiniCPM-o 2.6 also shows &lt;strong&gt;state-of-the-art token density&lt;/strong&gt; (i.e., the number of pixels encoded into each visual token). &lt;strong&gt;It produces only 640 tokens when processing a 1.8M pixel image, which is 75% fewer than most models&lt;/strong&gt;. This directly improves the inference speed, first-token latency, memory usage, and power consumption. As a result, MiniCPM-o 2.6 can efficiently support &lt;strong&gt;multimodal live streaming&lt;/strong&gt; on end-side devices such as iPads.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üí´ &lt;strong&gt;Easy Usage.&lt;/strong&gt; MiniCPM-o 2.6 can be easily used in various ways: (1) &lt;a href="https://github.com/OpenBMB/llama.cpp/raw/minicpm-omni/examples/llava/README-minicpmo2.6.md"&gt;llama.cpp&lt;/a&gt; support for efficient CPU inference on local devices, (2) &lt;a href="https://huggingface.co/openbmb/MiniCPM-o-2_6-int4"&gt;int4&lt;/a&gt; and &lt;a href="https://huggingface.co/openbmb/MiniCPM-o-2_6-gguf"&gt;GGUF&lt;/a&gt; format quantized models in 16 sizes, (3) &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#efficient-inference-with-llamacpp-ollama-vllm"&gt;vLLM&lt;/a&gt; support for high-throughput and memory-efficient inference, (4) fine-tuning on new domains and tasks with &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/docs/llamafactory_train_and_infer.md"&gt;LLaMA-Factory&lt;/a&gt;, (5) quick &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#chat-with-our-demo-on-gradio"&gt;local WebUI demo&lt;/a&gt;, and (6) online web demo on &lt;a href="https://minicpm-omni-webdemo-us.modelbest.cn/"&gt;server&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Model Architecture.&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;End-to-end Omni-modal Architecture.&lt;/strong&gt; Different modality encoders/decoders are connected and trained in an &lt;strong&gt;end-to-end&lt;/strong&gt; fashion to fully exploit rich multimodal knowledge. The model is trained in a fully end-to-end manner with only CE loss.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Omni-modal Live Streaming Mechanism.&lt;/strong&gt; (1) We change the offline modality encoder/decoders into online ones for &lt;strong&gt;streaming inputs/outputs.&lt;/strong&gt; (2) We devise a &lt;strong&gt;time-division multiplexing (TDM) mechanism&lt;/strong&gt; for omni-modality streaming processing in the LLM backbone. It divides parallel omni-modality streams into sequential info within small periodic time slices.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Configurable Speech Modeling Design.&lt;/strong&gt; We devise a multimodal system prompt, including traditional text system prompt, and &lt;strong&gt;a new audio system prompt to determine the assistant voice&lt;/strong&gt;. This enables flexible voice configurations in inference time, and also facilitates end-to-end voice cloning and description-based voice creation.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpm-o-26-framework-v2.png" , width="80%" /&gt; 
&lt;/div&gt; 
&lt;h3&gt;Evaluation 
 &lt;!-- omit in toc --&gt;&lt;/h3&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/radar.jpg" , width="80%" /&gt; 
&lt;/div&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click to view visual understanding results.&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;Image Understanding&lt;/strong&gt;&lt;/p&gt; 
 &lt;div align="center"&gt; 
  &lt;table style="margin: 0px auto;"&gt; 
   &lt;thead&gt; 
    &lt;tr&gt; 
     &lt;th align="left"&gt;Model&lt;/th&gt; 
     &lt;th&gt;Size&lt;/th&gt; 
     &lt;th&gt;Token Density&lt;sup&gt;+&lt;/sup&gt;&lt;/th&gt; 
     &lt;th&gt;OpenCompass&lt;/th&gt; 
     &lt;th&gt;OCRBench&lt;/th&gt; 
     &lt;th&gt;MathVista mini&lt;/th&gt; 
     &lt;th&gt;ChartQA&lt;/th&gt; 
     &lt;th&gt;MMVet&lt;/th&gt; 
     &lt;th&gt;MMStar&lt;/th&gt; 
     &lt;th&gt;MME&lt;/th&gt; 
     &lt;th&gt;MMB1.1 test&lt;/th&gt; 
     &lt;th&gt;AI2D&lt;/th&gt; 
     &lt;th&gt;MMMU val&lt;/th&gt; 
     &lt;th&gt;HallusionBench&lt;/th&gt; 
     &lt;th&gt;TextVQA val&lt;/th&gt; 
     &lt;th&gt;DocVQA test&lt;/th&gt; 
     &lt;th&gt;MathVerse mini&lt;/th&gt; 
     &lt;th&gt;MathVision&lt;/th&gt; 
     &lt;th&gt;MMHal Score&lt;/th&gt; 
    &lt;/tr&gt; 
   &lt;/thead&gt; 
   &lt;tbody align="center"&gt; 
    &lt;tr&gt; 
     &lt;td colspan="19" align="left"&gt;&lt;strong&gt;Proprietary&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;GPT-4o-20240513&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;1088&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;69.9&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;736&lt;/td&gt; 
     &lt;td&gt;61.3&lt;/td&gt; 
     &lt;td&gt;85.7&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;69.1&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;63.9&lt;/td&gt; 
     &lt;td&gt;2328.7&lt;/td&gt; 
     &lt;td&gt;82.2&lt;/td&gt; 
     &lt;td&gt;84.6&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;69.2&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;55.0&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;92.8&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;50.2&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;30.4&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;3.6&lt;/u&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;Claude3.5-Sonnet&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;750&lt;/td&gt; 
     &lt;td&gt;67.9&lt;/td&gt; 
     &lt;td&gt;788&lt;/td&gt; 
     &lt;td&gt;61.6&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;90.8&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;66.0&lt;/td&gt; 
     &lt;td&gt;62.2&lt;/td&gt; 
     &lt;td&gt;1920.0&lt;/td&gt; 
     &lt;td&gt;78.5&lt;/td&gt; 
     &lt;td&gt;80.2&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;65.9&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;49.9&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;95.2&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;3.4&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;Gemini 1.5 Pro&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;64.4&lt;/td&gt; 
     &lt;td&gt;754&lt;/td&gt; 
     &lt;td&gt;57.7&lt;/td&gt; 
     &lt;td&gt;81.3&lt;/td&gt; 
     &lt;td&gt;64.0&lt;/td&gt; 
     &lt;td&gt;59.1&lt;/td&gt; 
     &lt;td&gt;2110.6&lt;/td&gt; 
     &lt;td&gt;73.9&lt;/td&gt; 
     &lt;td&gt;79.1&lt;/td&gt; 
     &lt;td&gt;60.6&lt;/td&gt; 
     &lt;td&gt;45.6&lt;/td&gt; 
     &lt;td&gt;73.5&lt;/td&gt; 
     &lt;td&gt;86.5&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;19.2&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;GPT-4o-mini-20240718&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;1088&lt;/td&gt; 
     &lt;td&gt;64.1&lt;/td&gt; 
     &lt;td&gt;785&lt;/td&gt; 
     &lt;td&gt;52.4&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;66.9&lt;/td&gt; 
     &lt;td&gt;54.8&lt;/td&gt; 
     &lt;td&gt;2003.4&lt;/td&gt; 
     &lt;td&gt;76.0&lt;/td&gt; 
     &lt;td&gt;77.8&lt;/td&gt; 
     &lt;td&gt;60.0&lt;/td&gt; 
     &lt;td&gt;46.1&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;3.3&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td colspan="19" align="left"&gt;&lt;strong&gt;Open Source&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;Cambrian-34B&lt;/td&gt; 
     &lt;td&gt;34B&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;1820&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;58.3&lt;/td&gt; 
     &lt;td&gt;591&lt;/td&gt; 
     &lt;td&gt;50.3&lt;/td&gt; 
     &lt;td&gt;75.6&lt;/td&gt; 
     &lt;td&gt;53.2&lt;/td&gt; 
     &lt;td&gt;54.2&lt;/td&gt; 
     &lt;td&gt;2049.9&lt;/td&gt; 
     &lt;td&gt;77.8&lt;/td&gt; 
     &lt;td&gt;79.5&lt;/td&gt; 
     &lt;td&gt;50.4&lt;/td&gt; 
     &lt;td&gt;41.6&lt;/td&gt; 
     &lt;td&gt;76.7&lt;/td&gt; 
     &lt;td&gt;75.5&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;GLM-4V-9B&lt;/td&gt; 
     &lt;td&gt;13B&lt;/td&gt; 
     &lt;td&gt;784&lt;/td&gt; 
     &lt;td&gt;59.1&lt;/td&gt; 
     &lt;td&gt;776&lt;/td&gt; 
     &lt;td&gt;51.1&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;58.0&lt;/td&gt; 
     &lt;td&gt;54.8&lt;/td&gt; 
     &lt;td&gt;2018.8&lt;/td&gt; 
     &lt;td&gt;67.9&lt;/td&gt; 
     &lt;td&gt;71.2&lt;/td&gt; 
     &lt;td&gt;46.9&lt;/td&gt; 
     &lt;td&gt;45.0&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;Pixtral-12B&lt;/td&gt; 
     &lt;td&gt;12B&lt;/td&gt; 
     &lt;td&gt;256&lt;/td&gt; 
     &lt;td&gt;61.0&lt;/td&gt; 
     &lt;td&gt;685&lt;/td&gt; 
     &lt;td&gt;56.9&lt;/td&gt; 
     &lt;td&gt;81.8&lt;/td&gt; 
     &lt;td&gt;58.5&lt;/td&gt; 
     &lt;td&gt;54.5&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;72.7&lt;/td&gt; 
     &lt;td&gt;79.0&lt;/td&gt; 
     &lt;td&gt;51.1&lt;/td&gt; 
     &lt;td&gt;47.0&lt;/td&gt; 
     &lt;td&gt;75.7&lt;/td&gt; 
     &lt;td&gt;90.7&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;VITA-1.5&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;784&lt;/td&gt; 
     &lt;td&gt;63.3&lt;/td&gt; 
     &lt;td&gt;741&lt;/td&gt; 
     &lt;td&gt;66.2&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;52.7&lt;/td&gt; 
     &lt;td&gt;60.2&lt;/td&gt; 
     &lt;td&gt;2328.1&lt;/td&gt; 
     &lt;td&gt;76.8&lt;/td&gt; 
     &lt;td&gt;79.2&lt;/td&gt; 
     &lt;td&gt;52.6&lt;/td&gt; 
     &lt;td&gt;44.6&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;DeepSeek-VL2-27B (4B)&lt;/td&gt; 
     &lt;td&gt;27B&lt;/td&gt; 
     &lt;td&gt;672&lt;/td&gt; 
     &lt;td&gt;66.4&lt;/td&gt; 
     &lt;td&gt;809&lt;/td&gt; 
     &lt;td&gt;63.9&lt;/td&gt; 
     &lt;td&gt;86.0&lt;/td&gt; 
     &lt;td&gt;60.0&lt;/td&gt; 
     &lt;td&gt;61.9&lt;/td&gt; 
     &lt;td&gt;2253.0&lt;/td&gt; 
     &lt;td&gt;81.2&lt;/td&gt; 
     &lt;td&gt;83.8&lt;/td&gt; 
     &lt;td&gt;54.0&lt;/td&gt; 
     &lt;td&gt;45.3&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;84.2&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;93.3&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;3.0&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;Qwen2-VL-7B&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;784&lt;/td&gt; 
     &lt;td&gt;67.1&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;866&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;58.2&lt;/td&gt; 
     &lt;td&gt;83.0&lt;/td&gt; 
     &lt;td&gt;62.0&lt;/td&gt; 
     &lt;td&gt;60.7&lt;/td&gt; 
     &lt;td&gt;2326.0&lt;/td&gt; 
     &lt;td&gt;81.8&lt;/td&gt; 
     &lt;td&gt;83.0&lt;/td&gt; 
     &lt;td&gt;54.1&lt;/td&gt; 
     &lt;td&gt;50.6&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;84.3&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;94.5&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;31.9&lt;/td&gt; 
     &lt;td&gt;16.3&lt;/td&gt; 
     &lt;td&gt;3.2&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;LLaVA-OneVision-72B&lt;/td&gt; 
     &lt;td&gt;72B&lt;/td&gt; 
     &lt;td&gt;182&lt;/td&gt; 
     &lt;td&gt;68.1&lt;/td&gt; 
     &lt;td&gt;741&lt;/td&gt; 
     &lt;td&gt;67.5&lt;/td&gt; 
     &lt;td&gt;83.7&lt;/td&gt; 
     &lt;td&gt;60.6&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;65.8&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;2261.0&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;85.0&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;85.6&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;56.8&lt;/td&gt; 
     &lt;td&gt;49.0&lt;/td&gt; 
     &lt;td&gt;80.5&lt;/td&gt; 
     &lt;td&gt;91.3&lt;/td&gt; 
     &lt;td&gt;39.1&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;3.5&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;InternVL2.5-8B&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;706&lt;/td&gt; 
     &lt;td&gt;68.3&lt;/td&gt; 
     &lt;td&gt;822&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;64.4&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;84.8&lt;/td&gt; 
     &lt;td&gt;62.8&lt;/td&gt; 
     &lt;td&gt;62.8&lt;/td&gt; 
     &lt;td&gt;2344.0&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;83.6&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;84.5&lt;/td&gt; 
     &lt;td&gt;56.0&lt;/td&gt; 
     &lt;td&gt;50.1&lt;/td&gt; 
     &lt;td&gt;79.1&lt;/td&gt; 
     &lt;td&gt;93.0&lt;/td&gt; 
     &lt;td&gt;39.5&lt;/td&gt; 
     &lt;td&gt;19.7&lt;/td&gt; 
     &lt;td&gt;3.4&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;MiniCPM-V 2.6&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;2822&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;65.2&lt;/td&gt; 
     &lt;td&gt;852*&lt;/td&gt; 
     &lt;td&gt;60.6&lt;/td&gt; 
     &lt;td&gt;79.4&lt;/td&gt; 
     &lt;td&gt;60.0&lt;/td&gt; 
     &lt;td&gt;57.5&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;2348.4*&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;78.0&lt;/td&gt; 
     &lt;td&gt;82.1&lt;/td&gt; 
     &lt;td&gt;49.8*&lt;/td&gt; 
     &lt;td&gt;48.1*&lt;/td&gt; 
     &lt;td&gt;80.1&lt;/td&gt; 
     &lt;td&gt;90.8&lt;/td&gt; 
     &lt;td&gt;25.7&lt;/td&gt; 
     &lt;td&gt;18.3&lt;/td&gt; 
     &lt;td&gt;3.6&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;MiniCPM-o 2.6&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;2822&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;70.2&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;897*&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;71.9*&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;86.9*&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;67.5&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;64.0&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;2372.0*&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;80.5&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;85.8&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;50.4*&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;51.9&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;82.0&lt;/td&gt; 
     &lt;td&gt;93.5&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;41.4*&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;23.1*&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;3.8&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
   &lt;/tbody&gt; 
  &lt;/table&gt; 
 &lt;/div&gt; * We evaluate this benchmark using chain-of-thought prompting. Specifically, for MME, we used this technique only for the Cognition set. 
 &lt;p&gt;&lt;sup&gt;+&lt;/sup&gt; Token Density: number of pixels encoded into each visual token at maximum resolution, i.e., # pixels at maximum resolution / # visual tokens.&lt;/p&gt; 
 &lt;p&gt;Note: For proprietary models, we calculate token density based on the image encoding charging strategy defined in the official API documentation, which provides an upper-bound estimation.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Multi-image and Video Understanding&lt;/strong&gt;&lt;/p&gt; 
 &lt;div align="center"&gt; 
  &lt;table style="margin: 0px auto;"&gt; 
   &lt;thead&gt; 
    &lt;tr&gt; 
     &lt;th align="left"&gt;Model&lt;/th&gt; 
     &lt;th&gt;Size&lt;/th&gt; 
     &lt;th&gt;BLINK val&lt;/th&gt; 
     &lt;th&gt;Mantis Eval&lt;/th&gt; 
     &lt;th&gt;MIRB&lt;/th&gt; 
     &lt;th&gt;Video-MME (wo / w subs)&lt;/th&gt; 
    &lt;/tr&gt; 
   &lt;/thead&gt; 
   &lt;tbody align="center"&gt; 
    &lt;tr&gt; 
     &lt;td colspan="6" align="left"&gt;&lt;strong&gt;Proprietary&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;GPT-4o-20240513&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;68.0&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;71.9/77.2&lt;strong&gt;&lt;/strong&gt;&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;GPT4V&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;54.6&lt;/td&gt; 
     &lt;td&gt;62.7&lt;/td&gt; 
     &lt;td&gt;53.1&lt;/td&gt; 
     &lt;td&gt;59.9/63.3&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td colspan="6" align="left"&gt;&lt;strong&gt;Open-source&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;VITA-1.5&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;45.0&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;56.1/58.7&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;LLaVA-NeXT-Interleave 14B&lt;/td&gt; 
     &lt;td&gt;14B&lt;/td&gt; 
     &lt;td&gt;52.6&lt;/td&gt; 
     &lt;td&gt;66.4&lt;/td&gt; 
     &lt;td&gt;30.2&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;LLaVA-OneVision-72B&lt;/td&gt; 
     &lt;td&gt;72B&lt;/td&gt; 
     &lt;td&gt;55.4&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;77.6&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;66.2/69.5&lt;/u&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;MANTIS 8B&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;49.1&lt;/td&gt; 
     &lt;td&gt;59.5&lt;/td&gt; 
     &lt;td&gt;34.8&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;Qwen2-VL-7B&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;53.2&lt;/td&gt; 
     &lt;td&gt;69.6*&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;67.6*&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;63.3/69.0&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;InternVL2.5-8B&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;54.8&lt;/td&gt; 
     &lt;td&gt;67.7&lt;/td&gt; 
     &lt;td&gt;52.5&lt;/td&gt; 
     &lt;td&gt;64.2/66.9&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;MiniCPM-V 2.6&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;53.0&lt;/td&gt; 
     &lt;td&gt;69.1&lt;/td&gt; 
     &lt;td&gt;53.8&lt;/td&gt; 
     &lt;td&gt;60.9/63.6&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;MiniCPM-o 2.6&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;56.7&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;71.9&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;58.6&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;63.9/67.9&lt;/td&gt; 
    &lt;/tr&gt; 
   &lt;/tbody&gt; 
  &lt;/table&gt; 
 &lt;/div&gt; * We evaluate officially released checkpoints by ourselves. 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click to view audio understanding and speech conversation results.&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;Audio Understanding&lt;/strong&gt;&lt;/p&gt; 
 &lt;div align="center"&gt; 
  &lt;table style="margin: 0px auto;"&gt; 
   &lt;thead&gt; 
    &lt;tr&gt; 
     &lt;th align="left"&gt;Task&lt;/th&gt; 
     &lt;th&gt;Size&lt;/th&gt; 
     &lt;th colspan="3"&gt;ASR (zh)&lt;/th&gt; 
     &lt;th colspan="3"&gt;ASR (en)&lt;/th&gt; 
     &lt;th colspan="2"&gt;AST&lt;/th&gt; 
     &lt;th&gt;Emotion&lt;/th&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;th align="left"&gt;Metric&lt;/th&gt; 
     &lt;td&gt;&lt;/td&gt; 
     &lt;th colspan="3"&gt;CER‚Üì&lt;/th&gt; 
     &lt;th colspan="3"&gt;WER‚Üì&lt;/th&gt; 
     &lt;th colspan="2"&gt;BLEU‚Üë&lt;/th&gt; 
     &lt;th&gt;ACC‚Üë&lt;/th&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;th align="left"&gt;Dataset&lt;/th&gt; 
     &lt;td&gt;&lt;/td&gt; 
     &lt;th&gt;AISHELL-1&lt;/th&gt; 
     &lt;th&gt;Fleurs zh&lt;/th&gt; 
     &lt;th&gt;WenetSpeech test-net&lt;/th&gt; 
     &lt;th&gt;LibriSpeech test-clean&lt;/th&gt; 
     &lt;th&gt;GigaSpeech&lt;/th&gt; 
     &lt;th&gt;TED-LIUM&lt;/th&gt; 
     &lt;th&gt;CoVoST en2zh&lt;/th&gt; 
     &lt;th&gt;CoVoST zh2en&lt;/th&gt; 
     &lt;th&gt;MELD emotion&lt;/th&gt; 
    &lt;/tr&gt; 
   &lt;/thead&gt; 
   &lt;tbody align="center"&gt; 
    &lt;tr&gt; 
     &lt;td colspan="11" align="left"&gt;&lt;strong&gt;Proprietary&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;GPT-4o-Realtime&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;7.3*&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;5.4*&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;28.9*&lt;/td&gt; 
     &lt;td&gt;2.6*&lt;/td&gt; 
     &lt;td&gt;12.9*&lt;/td&gt; 
     &lt;td&gt;4.8*&lt;/td&gt; 
     &lt;td&gt;37.1*&lt;/td&gt; 
     &lt;td&gt;15.7*&lt;/td&gt; 
     &lt;td&gt;33.2*&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;Gemini 1.5 Pro&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;4.5*&lt;/td&gt; 
     &lt;td&gt;5.9*&lt;/td&gt; 
     &lt;td&gt;14.3*&lt;/td&gt; 
     &lt;td&gt;2.9*&lt;/td&gt; 
     &lt;td&gt;10.6*&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;3.0*&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;47.3*&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;22.6*&lt;/td&gt; 
     &lt;td&gt;48.4*&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td colspan="11" align="left"&gt;&lt;strong&gt;Open-Source&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;Qwen2-Audio-7B&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;7.5&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;1.6&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;45.2&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;24.4&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;55.3&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;Qwen2-Audio-7B-Instruct&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;2.6*&lt;/td&gt; 
     &lt;td&gt;6.9*&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;10.3*&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;3.1*&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;9.7&lt;/u&gt;*&lt;/td&gt; 
     &lt;td&gt;5.9*&lt;/td&gt; 
     &lt;td&gt;39.5*&lt;/td&gt; 
     &lt;td&gt;22.9*&lt;/td&gt; 
     &lt;td&gt;17.4*&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;VITA-1.5&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;2.16&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;8.4&lt;/td&gt; 
     &lt;td&gt;3.4&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;GLM-4-Voice-Base&lt;/td&gt; 
     &lt;td&gt;9B&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;2.5&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;2.8&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;MiniCPM-o 2.6&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;1.6&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;4.4&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;6.9&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;1.7&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;8.7&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;3.0&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;48.2&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;27.2&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;52.4&lt;/u&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
   &lt;/tbody&gt; 
  &lt;/table&gt; 
 &lt;/div&gt; * We evaluate officially released checkpoints by ourselves.
 &lt;br /&gt;
 &lt;br /&gt; 
 &lt;p&gt;&lt;strong&gt;Speech Generation&lt;/strong&gt;&lt;/p&gt; 
 &lt;div align="center"&gt; 
  &lt;table style="margin: 0px auto;"&gt; 
   &lt;thead&gt; 
    &lt;tr&gt; 
     &lt;th align="left"&gt;Task&lt;/th&gt; 
     &lt;th&gt;Size&lt;/th&gt; 
     &lt;th colspan="9"&gt;SpeechQA&lt;/th&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;th align="left"&gt;Metric&lt;/th&gt; 
     &lt;th&gt;&lt;/th&gt; 
     &lt;th colspan="3"&gt;ACC‚Üë&lt;/th&gt; 
     &lt;th&gt;G-Eval (10 point)‚Üë&lt;/th&gt; 
     &lt;th&gt;Semantic ELO score‚Üë&lt;/th&gt; 
     &lt;th&gt;Acoustic ELO score‚Üë&lt;/th&gt; 
     &lt;th&gt;Overall ELO score‚Üë&lt;/th&gt; 
     &lt;th&gt;UTMOS‚Üë&lt;/th&gt; 
     &lt;th&gt;ASR-WER‚Üì&lt;/th&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;th align="left"&gt;Dataset&lt;/th&gt; 
     &lt;th&gt;&lt;/th&gt; 
     &lt;th&gt;Speech Llama Q.&lt;/th&gt; 
     &lt;th&gt;Speech Web Q.&lt;/th&gt; 
     &lt;th&gt;Speech Trivia QA&lt;/th&gt; 
     &lt;th&gt;Speech AlpacaEval&lt;/th&gt; 
     &lt;th colspan="5"&gt;AudioArena&lt;/th&gt; 
    &lt;/tr&gt; 
   &lt;/thead&gt; 
   &lt;tbody align="center"&gt; 
    &lt;tr&gt; 
     &lt;td colspan="11" align="left"&gt;&lt;strong&gt;Proprietary&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;GPT-4o-Realtime&lt;/td&gt; 
     &lt;td&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;71.7&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;51.6&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;69.7&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;7.4&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;1157&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;1203&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;1200&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;4.2&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;2.3&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td colspan="11" align="left"&gt;&lt;strong&gt;Open-Source&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;GLM-4-Voice&lt;/td&gt; 
     &lt;td&gt;9B&lt;/td&gt; 
     &lt;td&gt;50.0&lt;/td&gt; 
     &lt;td&gt;32.0&lt;/td&gt; 
     &lt;td&gt;36.4&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;5.1&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;999&lt;/td&gt; 
     &lt;td&gt;1147&lt;/td&gt; 
     &lt;td&gt;1035&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;4.1&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;11.7&lt;/u&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;Llama-Omni&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;45.3&lt;/td&gt; 
     &lt;td&gt;22.9&lt;/td&gt; 
     &lt;td&gt;10.7&lt;/td&gt; 
     &lt;td&gt;3.9&lt;/td&gt; 
     &lt;td&gt;960&lt;/td&gt; 
     &lt;td&gt;878&lt;/td&gt; 
     &lt;td&gt;897&lt;/td&gt; 
     &lt;td&gt;3.2&lt;/td&gt; 
     &lt;td&gt;24.3&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;VITA-1.5&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;46.7&lt;/td&gt; 
     &lt;td&gt;28.1&lt;/td&gt; 
     &lt;td&gt;23.3&lt;/td&gt; 
     &lt;td&gt;2.0&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
     &lt;td&gt;-&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;Moshi&lt;/td&gt; 
     &lt;td&gt;7B&lt;/td&gt; 
     &lt;td&gt;43.7&lt;/td&gt; 
     &lt;td&gt;23.8&lt;/td&gt; 
     &lt;td&gt;16.7&lt;/td&gt; 
     &lt;td&gt;2.4&lt;/td&gt; 
     &lt;td&gt;871&lt;/td&gt; 
     &lt;td&gt;808&lt;/td&gt; 
     &lt;td&gt;875&lt;/td&gt; 
     &lt;td&gt;2.8&lt;/td&gt; 
     &lt;td&gt;8.2&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;Mini-Omni&lt;/td&gt; 
     &lt;td&gt;1B&lt;/td&gt; 
     &lt;td&gt;22.0&lt;/td&gt; 
     &lt;td&gt;12.8&lt;/td&gt; 
     &lt;td&gt;6.9&lt;/td&gt; 
     &lt;td&gt;2.5&lt;/td&gt; 
     &lt;td&gt;926&lt;/td&gt; 
     &lt;td&gt;803&lt;/td&gt; 
     &lt;td&gt;865&lt;/td&gt; 
     &lt;td&gt;3.4&lt;/td&gt; 
     &lt;td&gt;10.0&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;MiniCPM-o 2.6&lt;/td&gt; 
     &lt;td&gt;8B&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;61.0&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;40.0&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;40.2&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;5.1&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;1088&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;1163&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;1131&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;4.2&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;9.8&lt;/td&gt; 
    &lt;/tr&gt; 
   &lt;/tbody&gt; 
  &lt;/table&gt; 
 &lt;/div&gt; All results are from AudioEvals, and the evaluation methods along with further details can be found in 
 &lt;a href="https://github.com/OpenBMB/UltraEval-Audio" target="_blank"&gt;AudioEvals&lt;/a&gt;.
 &lt;br /&gt;
 &lt;br /&gt; 
 &lt;p&gt;&lt;strong&gt;End-to-end Voice Cloning&lt;/strong&gt;&lt;/p&gt; 
 &lt;div align="center"&gt; 
  &lt;table style="margin: 0px auto;"&gt; 
   &lt;thead&gt; 
    &lt;tr&gt; 
     &lt;th align="left"&gt;Task&lt;/th&gt; 
     &lt;th colspan="2"&gt;Voice cloning&lt;/th&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;th align="left"&gt;Metric&lt;/th&gt; 
     &lt;th&gt;SIMO‚Üë&lt;/th&gt; 
     &lt;th&gt;SIMO‚Üë&lt;/th&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;th align="left"&gt;Dataset&lt;/th&gt; 
     &lt;th&gt;Seed-TTS test-zh&lt;/th&gt; 
     &lt;th&gt;Seed-TTS test-en&lt;/th&gt; 
    &lt;/tr&gt; 
   &lt;/thead&gt; 
   &lt;tbody align="center"&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;F5-TTS&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;76&lt;/strong&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;strong&gt;67&lt;/strong&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;CosyVoice&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;75&lt;/u&gt;&lt;/td&gt; 
     &lt;td&gt;&lt;u&gt;64&lt;/u&gt;&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;FireRedTTS&lt;/td&gt; 
     &lt;td&gt;63&lt;/td&gt; 
     &lt;td&gt;46&lt;/td&gt; 
    &lt;/tr&gt; 
    &lt;tr&gt; 
     &lt;td nowrap align="left"&gt;MiniCPM-o 2.6&lt;/td&gt; 
     &lt;td&gt;57&lt;/td&gt; 
     &lt;td&gt;47&lt;/td&gt; 
    &lt;/tr&gt; 
   &lt;/tbody&gt; 
  &lt;/table&gt; 
 &lt;/div&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click to view multimodal live streaming results.&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;Multimodal Live Streaming&lt;/strong&gt;: results on StreamingBench&lt;/p&gt; 
 &lt;table style="margin: 0px auto;"&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="left"&gt;Model&lt;/th&gt; 
    &lt;th&gt;Size&lt;/th&gt; 
    &lt;th&gt;Real-Time Video Understanding&lt;/th&gt; 
    &lt;th&gt;Omni-Source Understanding&lt;/th&gt; 
    &lt;th&gt;Contextual Understanding&lt;/th&gt; 
    &lt;th&gt;Overall&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody align="center"&gt; 
   &lt;tr&gt; 
    &lt;td colspan="7" align="left"&gt;&lt;strong&gt;Proprietary&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;Gemini 1.5 Pro&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;&lt;u&gt;77.4&lt;/u&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;67.8&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;51.1&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;70.3&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;GPT-4o-202408&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;74.5&lt;/td&gt; 
    &lt;td&gt;51.0&lt;/td&gt; 
    &lt;td&gt;&lt;u&gt;48.0&lt;/u&gt;&lt;/td&gt; 
    &lt;td&gt;64.1&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;Claude-3.5-Sonnet&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;74.0&lt;/td&gt; 
    &lt;td&gt;41.4&lt;/td&gt; 
    &lt;td&gt;37.8&lt;/td&gt; 
    &lt;td&gt;59.7&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td colspan="9" align="left"&gt;&lt;strong&gt;Open-source&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;VILA-1.5&lt;/td&gt; 
    &lt;td&gt;8B&lt;/td&gt; 
    &lt;td&gt;61.5&lt;/td&gt; 
    &lt;td&gt;37.5&lt;/td&gt; 
    &lt;td&gt;26.7&lt;/td&gt; 
    &lt;td&gt;49.5&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;LongVA&lt;/td&gt; 
    &lt;td&gt;7B&lt;/td&gt; 
    &lt;td&gt;63.1&lt;/td&gt; 
    &lt;td&gt;35.9&lt;/td&gt; 
    &lt;td&gt;30.2&lt;/td&gt; 
    &lt;td&gt;50.7&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;LLaVA-Next-Video-34B&lt;/td&gt; 
    &lt;td&gt;34B&lt;/td&gt; 
    &lt;td&gt;69.8&lt;/td&gt; 
    &lt;td&gt;41.7&lt;/td&gt; 
    &lt;td&gt;34.3&lt;/td&gt; 
    &lt;td&gt;56.7&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;Qwen2-VL-7B&lt;/td&gt; 
    &lt;td&gt;8B&lt;/td&gt; 
    &lt;td&gt;71.2&lt;/td&gt; 
    &lt;td&gt;40.7&lt;/td&gt; 
    &lt;td&gt;33.1&lt;/td&gt; 
    &lt;td&gt;57.0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;InternVL2-8B&lt;/td&gt; 
    &lt;td&gt;8B&lt;/td&gt; 
    &lt;td&gt;70.1&lt;/td&gt; 
    &lt;td&gt;42.7&lt;/td&gt; 
    &lt;td&gt;34.1&lt;/td&gt; 
    &lt;td&gt;57.0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;VITA-1.5&lt;/td&gt; 
    &lt;td&gt;8B&lt;/td&gt; 
    &lt;td&gt;70.9&lt;/td&gt; 
    &lt;td&gt;40.8&lt;/td&gt; 
    &lt;td&gt;35.8&lt;/td&gt; 
    &lt;td&gt;57.4&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;LLaVA-OneVision-7B&lt;/td&gt; 
    &lt;td&gt;8B&lt;/td&gt; 
    &lt;td&gt;74.3&lt;/td&gt; 
    &lt;td&gt;40.8&lt;/td&gt; 
    &lt;td&gt;31.0&lt;/td&gt; 
    &lt;td&gt;58.4&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;InternLM-XC2.5-OL-7B&lt;/td&gt; 
    &lt;td&gt;8B&lt;/td&gt; 
    &lt;td&gt;75.4&lt;/td&gt; 
    &lt;td&gt;46.2&lt;/td&gt; 
    &lt;td&gt;33.6&lt;/td&gt; 
    &lt;td&gt;60.8&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;MiniCPM-V 2.6&lt;/td&gt; 
    &lt;td&gt;8B&lt;/td&gt; 
    &lt;td&gt;72.4&lt;/td&gt; 
    &lt;td&gt;40.2&lt;/td&gt; 
    &lt;td&gt;33.4&lt;/td&gt; 
    &lt;td&gt;57.7&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td nowrap align="left"&gt;MiniCPM-o 2.6&lt;/td&gt; 
    &lt;td&gt;8B&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;79.9&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;u&gt;53.4&lt;/u&gt;&lt;/td&gt; 
    &lt;td&gt;38.5&lt;/td&gt; 
    &lt;td&gt;&lt;u&gt;66.0&lt;/u&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h3&gt;Examples 
 &lt;!-- omit in toc --&gt;&lt;/h3&gt; 
&lt;p&gt;We deploy MiniCPM-o 2.6 on end devices. The demo video is the raw-speed recording on an iPad Pro and a Web demo.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://www.youtube.com/watch?v=vRIMbxJzStY&amp;amp;t=2s"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmo2_6/2dot6_o_demo_video_img.png" , width="70%" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;div style="display: flex; flex-direction: column; align-items: center;"&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmo2_6/minicpmo2_6_math_intersect.png" alt="math" style="margin-bottom: 5px;" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmo2_6/minicpmo2_6_diagram_train_NN.png" alt="diagram" style="margin-bottom: 5px;" /&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmo2_6/minicpmo2_6_multi-image_bike.png" alt="bike" style="margin-bottom: 5px;" /&gt; 
&lt;/div&gt; 
&lt;h2&gt;Legacy Models 
 &lt;!-- omit in toc --&gt;&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Model&lt;/th&gt; 
   &lt;th align="center"&gt;Introduction and Guidance&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;MiniCPM-V 4.0&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/docs/minicpm_v4_en.md"&gt;Document&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;MiniCPM-V 2.6&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/docs/minicpm_v2dot6_en.md"&gt;Document&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;MiniCPM-Llama3-V 2.5&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/docs/minicpm_llama3_v2dot5.md"&gt;Document&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;MiniCPM-V 2.0&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/docs/minicpm_v2.md"&gt;Document&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;MiniCPM-V 1.0&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/docs/minicpm_v1.md"&gt;Document&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;OmniLMM-12B&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/docs/omnilmm_en.md"&gt;Document&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;MiniCPM-V &amp;amp; o Cookbook&lt;/h2&gt; 
&lt;p&gt;Discover comprehensive, ready-to-deploy solutions for the MiniCPM-V and MiniCPM-o model series in our structured &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook"&gt;cookbook&lt;/a&gt;, which empowers developers to rapidly implement multimodal AI applications with integrated vision, speech, and live-streaming capabilities. Key features include:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Easy Usage Documentation&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Our comprehensive &lt;a href="https://minicpm-o.readthedocs.io/en/latest/index.html"&gt;documentation website&lt;/a&gt; presents every recipe in a clear, well-organized manner. All features are displayed at a glance, making it easy for you to quickly find exactly what you need.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Broad User Spectrum&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;We support a wide range of users, from individuals to enterprises and researchers.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Individuals&lt;/strong&gt;: Enjoy effortless inference using &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/raw/main/deployment/ollama/minicpm-v4_ollama.md"&gt;Ollama&lt;/a&gt; and &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/raw/main/deployment/llama.cpp/minicpm-v4_llamacpp.md"&gt;Llama.cpp&lt;/a&gt; with minimal setup.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Enterprises&lt;/strong&gt;: Achieve high-throughput, scalable performance with &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/raw/main/deployment/vllm/minicpm-v4_vllm.md"&gt;vLLM&lt;/a&gt; and &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/raw/main/deployment/sglang/MiniCPM-v4_sglang.md"&gt;SGLang&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Researchers&lt;/strong&gt;: Leverage advanced frameworks including &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/raw/main/finetune/finetune_full.md"&gt;Transformers&lt;/a&gt;, &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/raw/main/finetune/finetune_llamafactory.md"&gt;LLaMA-Factory&lt;/a&gt;, &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/raw/main/finetune/swift.md"&gt;SWIFT&lt;/a&gt;, and &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/raw/main/finetune/align_anything.md"&gt;Align-anything&lt;/a&gt; to enable flexible model development and cutting-edge experimentation.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Versatile Deployment Scenarios&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Our ecosystem delivers optimal solution for a variety of hardware environments and deployment demands.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Web demo&lt;/strong&gt;: Launch interactive multimodal AI web demo with &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/raw/main/demo/README.md"&gt;FastAPI&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Quantized deployment&lt;/strong&gt;: Maximize efficiency and minimize resource consumption using &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/raw/main/quantization/gguf/minicpm-v4_gguf_quantize.md"&gt;GGUF&lt;/a&gt; and &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/raw/main/quantization/bnb/minicpm-v4_bnb_quantize.md"&gt;BNB&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;End devices&lt;/strong&gt;: Bring powerful AI experiences to &lt;a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/raw/main/demo/ios_demo/ios.md"&gt;iPhone and iPad&lt;/a&gt;, supporting offline and privacy-sensitive applications.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Chat with Our Demo on Gradio ü§ó&lt;/h2&gt; 
&lt;p&gt;We provide online and local demos powered by Hugging Face Gradio &lt;a href="https://github.com/gradio-app/gradio"&gt;&lt;img src="https://img.shields.io/github/stars/gradio-app/gradio" /&gt;&lt;/a&gt;, the most popular model deployment framework nowadays. It supports streaming outputs, progress bars, queuing, alerts, and other useful features.&lt;/p&gt; 
&lt;h3&gt;Online Demo 
 &lt;!-- omit in toc --&gt;&lt;/h3&gt; 
&lt;p&gt;Click here to try out the online demo of &lt;a href="https://minicpm-omni-webdemo-us.modelbest.cn/"&gt;MiniCPM-o 2.6&lt;/a&gt; | &lt;a href="http://120.92.209.146:8887/"&gt;MiniCPM-V 2.6&lt;/a&gt; | &lt;a href="https://huggingface.co/spaces/openbmb/MiniCPM-Llama3-V-2_5"&gt;MiniCPM-Llama3-V 2.5&lt;/a&gt; | &lt;a href="https://huggingface.co/spaces/openbmb/MiniCPM-V-2"&gt;MiniCPM-V 2.0&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Local WebUI Demo 
 &lt;!-- omit in toc --&gt;&lt;/h3&gt; 
&lt;p&gt;You can easily build your own local WebUI demo using the following commands.&lt;/p&gt; 
&lt;p&gt;Please ensure that &lt;code&gt;transformers==4.44.2&lt;/code&gt; is installed, as other versions may have compatibility issues.&lt;/p&gt; 
&lt;p&gt;If you are using an older version of PyTorch, you might encounter this issue &lt;code&gt;"weight_norm_fwd_first_dim_kernel" not implemented for 'BFloat16'&lt;/code&gt;, Please add &lt;code&gt;self.minicpmo_model.tts.float()&lt;/code&gt; during the model initialization.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;For real-time voice/video call demo:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;launch model server:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip install -r requirements_o2.6.txt

python web_demos/minicpm-o_2.6/model_server.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;launch web server:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# Make sure Node and PNPM is installed.
sudo apt-get update
sudo apt-get install nodejs npm
npm install -g pnpm


cd web_demos/minicpm-o_2.6/web_server
# create ssl cert for https, https is required to request camera and microphone permissions.
bash ./make_ssl_cert.sh  # output key.pem and cert.pem

pnpm install  # install requirements
pnpm run dev  # start server
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Open &lt;code&gt;https://localhost:8088/&lt;/code&gt; in browser and enjoy the real-time voice/video call.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;For chatbot demo:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip install -r requirements_o2.6.txt

python web_demos/minicpm-o_2.6/chatbot_web_demo_o2.6.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Open &lt;code&gt;http://localhost:8000/&lt;/code&gt; in browser and enjoy the vision mode chatbot.&lt;/p&gt; 
&lt;h2&gt;Inference&lt;/h2&gt; 
&lt;h3&gt;Model Zoo&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Model&lt;/th&gt; 
   &lt;th align="center"&gt;Device&lt;/th&gt; 
   &lt;th align="center"&gt;Memory&lt;/th&gt; 
   &lt;th align="left"&gt;‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ Description&lt;/th&gt; 
   &lt;th align="center"&gt;Download&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;MiniCPM-V 4.5&lt;/td&gt; 
   &lt;td align="center"&gt;GPU&lt;/td&gt; 
   &lt;td align="center"&gt;18 GB&lt;/td&gt; 
   &lt;td align="left"&gt;The latest version, strong end-side multimodal performance for single image, multi-image and video understanding.&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/openbmb/MiniCPM-V-4_5"&gt;ü§ó&lt;/a&gt; &amp;nbsp;&amp;nbsp; &lt;a href="https://modelscope.cn/models/OpenBMB/MiniCPM-V-4_5"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/modelscope_logo.png" width="20px" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;MiniCPM-V 4.5 gguf&lt;/td&gt; 
   &lt;td align="center"&gt;CPU&lt;/td&gt; 
   &lt;td align="center"&gt;8 GB&lt;/td&gt; 
   &lt;td align="left"&gt;The gguf version, lower memory usage and faster inference.&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/openbmb/MiniCPM-V-4_5-gguf"&gt;ü§ó&lt;/a&gt; &amp;nbsp;&amp;nbsp; &lt;a href="https://modelscope.cn/models/OpenBMB/MiniCPM-V-4_5-gguf"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/modelscope_logo.png" width="20px" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;MiniCPM-V 4.5 int4&lt;/td&gt; 
   &lt;td align="center"&gt;GPU&lt;/td&gt; 
   &lt;td align="center"&gt;9 GB&lt;/td&gt; 
   &lt;td align="left"&gt;The int4 quantized version, lower GPU memory usage.&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/openbmb/MiniCPM-V-4_5-int4"&gt;ü§ó&lt;/a&gt; &amp;nbsp;&amp;nbsp; &lt;a href="https://modelscope.cn/models/OpenBMB/MiniCPM-V-4_5-int4"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/modelscope_logo.png" width="20px" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;MiniCPM-V 4.5 AWQ&lt;/td&gt; 
   &lt;td align="center"&gt;GPU&lt;/td&gt; 
   &lt;td align="center"&gt;9 GB&lt;/td&gt; 
   &lt;td align="left"&gt;The int4 quantized version, lower GPU memory usage.&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/openbmb/MiniCPM-V-4_5-AWQ"&gt;ü§ó&lt;/a&gt; &amp;nbsp;&amp;nbsp; &lt;a href="https://modelscope.cn/models/OpenBMB/MiniCPM-V-4_5-AWQ"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/modelscope_logo.png" width="20px" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;MiniCPM-o 2.6&lt;/td&gt; 
   &lt;td align="center"&gt;GPU&lt;/td&gt; 
   &lt;td align="center"&gt;18 GB&lt;/td&gt; 
   &lt;td align="left"&gt;The latest version, achieving GPT-4o level performance for vision, speech and multimodal live streaming on end-side devices.&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/openbmb/MiniCPM-o-2_6"&gt;ü§ó&lt;/a&gt; &amp;nbsp;&amp;nbsp; &lt;a href="https://modelscope.cn/models/OpenBMB/MiniCPM-o-2_6"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/modelscope_logo.png" width="20px" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;MiniCPM-o 2.6 gguf&lt;/td&gt; 
   &lt;td align="center"&gt;CPU&lt;/td&gt; 
   &lt;td align="center"&gt;8 GB&lt;/td&gt; 
   &lt;td align="left"&gt;The gguf version, lower memory usage and faster inference.&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/openbmb/MiniCPM-o-2_6-gguf"&gt;ü§ó&lt;/a&gt; &amp;nbsp;&amp;nbsp; &lt;a href="https://modelscope.cn/models/OpenBMB/MiniCPM-o-2_6-gguf"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/modelscope_logo.png" width="20px" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;MiniCPM-o 2.6 int4&lt;/td&gt; 
   &lt;td align="center"&gt;GPU&lt;/td&gt; 
   &lt;td align="center"&gt;9 GB&lt;/td&gt; 
   &lt;td align="left"&gt;The int4 quantized version, lower GPU memory usage.&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/openbmb/MiniCPM-o-2_6-int4"&gt;ü§ó&lt;/a&gt; &amp;nbsp;&amp;nbsp; &lt;a href="https://modelscope.cn/models/OpenBMB/MiniCPM-o-2_6-int4"&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/modelscope_logo.png" width="20px" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Multi-turn Conversation&lt;/h3&gt; 
&lt;p&gt;If you wish to enable long-thinking mode, provide the argument &lt;code&gt;enable_thinking=True&lt;/code&gt; to the chat function.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip install -r requirements_o2.6.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Please refer to the following codes to run.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmo2_6/show_demo.jpg" width="500px" /&gt; 
&lt;/div&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import torch
from PIL import Image
from transformers import AutoModel, AutoTokenizer

torch.manual_seed(100)

model = AutoModel.from_pretrained('openbmb/MiniCPM-V-4_5', trust_remote_code=True, # or openbmb/MiniCPM-o-2_6
    attn_implementation='sdpa', torch_dtype=torch.bfloat16) # sdpa or flash_attention_2, no eager
model = model.eval().cuda()
tokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V-4_5', trust_remote_code=True) # or openbmb/MiniCPM-o-2_6

image = Image.open('./assets/minicpmo2_6/show_demo.jpg').convert('RGB')

enable_thinking=False # If `enable_thinking=True`, the long-thinking mode is enabled.

# First round chat 
question = "What is the landform in the picture?"
msgs = [{'role': 'user', 'content': [image, question]}]

answer = model.chat(
    msgs=msgs,
    tokenizer=tokenizer,
    enable_thinking=enable_thinking
)
print(answer)

# Second round chat, pass history context of multi-turn conversation
msgs.append({"role": "assistant", "content": [answer]})
msgs.append({"role": "user", "content": ["What should I pay attention to when traveling here?"]})

answer = model.chat(
    msgs=msgs,
    tokenizer=tokenizer
)
print(answer)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You will get the following output:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# round1
The landform in the picture is karst topography. Karst landscapes are characterized by distinctive, jagged limestone hills or mountains with steep, irregular peaks and deep valleys‚Äîexactly what you see here These unique formations result from the dissolution of soluble rocks like limestone over millions of years through water erosion.

This scene closely resembles the famous karst landscape of Guilin and Yangshuo in China‚Äôs Guangxi Province. The area features dramatic, pointed limestone peaks rising dramatically above serene rivers and lush green forests, creating a breathtaking and iconic natural beauty that attracts millions of visitors each year for its picturesque views.

# round2
When traveling to a karst landscape like this, here are some important tips:

1. Wear comfortable shoes: The terrain can be uneven and hilly.
2. Bring water and snacks for energy during hikes or boat rides.
3. Protect yourself from the sun with sunscreen, hats, and sunglasses‚Äîespecially since you‚Äôll likely spend time outdoors exploring scenic spots.
4. Respect local customs and nature regulations by not littering or disturbing wildlife.

By following these guidelines, you'll have a safe and enjoyable trip while appreciating the stunning natural beauty of places such as Guilin‚Äôs karst mountains.
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Chat with Multiple Images&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt; Click to view Python code running MiniCPM-V-4_5 with multiple images input. &lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;import torch
from PIL import Image
from transformers import AutoModel, AutoTokenizer

model = AutoModel.from_pretrained('openbmb/MiniCPM-V-4_5', trust_remote_code=True,  # or openbmb/MiniCPM-o-2_6
    attn_implementation='sdpa', torch_dtype=torch.bfloat16) # sdpa or flash_attention_2, no eager
model = model.eval().cuda()
tokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V-4_5', trust_remote_code=True)  # or openbmb/MiniCPM-o-2_6

image1 = Image.open('image1.jpg').convert('RGB')
image2 = Image.open('image2.jpg').convert('RGB')
question = 'Compare image 1 and image 2, tell me about the differences between image 1 and image 2.'

msgs = [{'role': 'user', 'content': [image1, image2, question]}]

answer = model.chat(
    msgs=msgs,
    tokenizer=tokenizer
)
print(answer)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h4&gt;In-context Few-shot Learning&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt; Click to view Python code running MiniCPM-V-4_5 with few-shot input. &lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;import torch
from PIL import Image
from transformers import AutoModel, AutoTokenizer

model = AutoModel.from_pretrained('openbmb/MiniCPM-V-4_5', trust_remote_code=True,  # or openbmb/MiniCPM-o-2_6
    attn_implementation='sdpa', torch_dtype=torch.bfloat16) # sdpa or flash_attention_2, no eager
model = model.eval().cuda()
tokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V-4_5', trust_remote_code=True)  # or openbmb/MiniCPM-o-2_6

question = "production date" 
image1 = Image.open('example1.jpg').convert('RGB')
answer1 = "2023.08.04"
image2 = Image.open('example2.jpg').convert('RGB')
answer2 = "2007.04.24"
image_test = Image.open('test.jpg').convert('RGB')

msgs = [
    {'role': 'user', 'content': [image1, question]}, {'role': 'assistant', 'content': [answer1]},
    {'role': 'user', 'content': [image2, question]}, {'role': 'assistant', 'content': [answer2]},
    {'role': 'user', 'content': [image_test, question]}
]

answer = model.chat(
    msgs=msgs,
    tokenizer=tokenizer
)
print(answer)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h4&gt;Chat with Video&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt; Click to view Python code running MiniCPM-V-4_5 by with video input and 3D-Resampler. &lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;## The 3d-resampler compresses multiple frames into 64 tokens by introducing temporal_ids. 
# To achieve this, you need to organize your video data into two corresponding sequences: 
#   frames: List[Image]
#   temporal_ids: List[List[Int]].

import torch
from PIL import Image
from transformers import AutoModel, AutoTokenizer
from decord import VideoReader, cpu    # pip install decord
from scipy.spatial import cKDTree
import numpy as np
import math

model = AutoModel.from_pretrained('openbmb/MiniCPM-V-4_5', trust_remote_code=True,  # or openbmb/MiniCPM-o-2_6
    attn_implementation='sdpa', torch_dtype=torch.bfloat16) # sdpa or flash_attention_2, no eager
model = model.eval().cuda()
tokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V-4_5', trust_remote_code=True)  # or openbmb/MiniCPM-o-2_6

MAX_NUM_FRAMES=180 # Indicates the maximum number of frames received after the videos are packed. The actual maximum number of valid frames is MAX_NUM_FRAMES * MAX_NUM_PACKING.
MAX_NUM_PACKING=3  # indicates the maximum packing number of video frames. valid range: 1-6
TIME_SCALE = 0.1 

def map_to_nearest_scale(values, scale):
    tree = cKDTree(np.asarray(scale)[:, None])
    _, indices = tree.query(np.asarray(values)[:, None])
    return np.asarray(scale)[indices]


def group_array(arr, size):
    return [arr[i:i+size] for i in range(0, len(arr), size)]

def encode_video(video_path, choose_fps=3, force_packing=None):
    def uniform_sample(l, n):
        gap = len(l) / n
        idxs = [int(i * gap + gap / 2) for i in range(n)]
        return [l[i] for i in idxs]
    vr = VideoReader(video_path, ctx=cpu(0))
    fps = vr.get_avg_fps()
    video_duration = len(vr) / fps
        
    if choose_fps * int(video_duration) &amp;lt;= MAX_NUM_FRAMES:
        packing_nums = 1
        choose_frames = round(min(choose_fps, round(fps)) * min(MAX_NUM_FRAMES, video_duration))
        
    else:
        packing_nums = math.ceil(video_duration * choose_fps / MAX_NUM_FRAMES)
        if packing_nums &amp;lt;= MAX_NUM_PACKING:
            choose_frames = round(video_duration * choose_fps)
        else:
            choose_frames = round(MAX_NUM_FRAMES * MAX_NUM_PACKING)
            packing_nums = MAX_NUM_PACKING

    frame_idx = [i for i in range(0, len(vr))]      
    frame_idx =  np.array(uniform_sample(frame_idx, choose_frames))

    if force_packing:
        packing_nums = min(force_packing, MAX_NUM_PACKING)
    
    print(video_path, ' duration:', video_duration)
    print(f'get video frames={len(frame_idx)}, packing_nums={packing_nums}')
    
    frames = vr.get_batch(frame_idx).asnumpy()

    frame_idx_ts = frame_idx / fps
    scale = np.arange(0, video_duration, TIME_SCALE)

    frame_ts_id = map_to_nearest_scale(frame_idx_ts, scale) / TIME_SCALE
    frame_ts_id = frame_ts_id.astype(np.int32)

    assert len(frames) == len(frame_ts_id)

    frames = [Image.fromarray(v.astype('uint8')).convert('RGB') for v in frames]
    frame_ts_id_group = group_array(frame_ts_id, packing_nums)
    
    return frames, frame_ts_id_group


video_path="video_test.mp4"
fps = 5 # fps for video
force_packing = None # You can set force_packing to ensure that 3D packing is forcibly enabled; otherwise, encode_video will dynamically set the packing quantity based on the duration.
frames, frame_ts_id_group = encode_video(video_path, fps, force_packing=force_packing)

question = "Describe the video"
msgs = [
    {'role': 'user', 'content': frames + [question]}, 
]


answer = model.chat(
    msgs=msgs,
    tokenizer=tokenizer,
    use_image_id=False,
    max_slice_nums=1,
    temporal_ids=frame_ts_id_group
)
print(answer)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h4&gt;Speech and Audio Mode&lt;/h4&gt; 
&lt;p&gt;Model initialization&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import torch
import librosa
from transformers import AutoModel, AutoTokenizer

model = AutoModel.from_pretrained('openbmb/MiniCPM-o-2_6', trust_remote_code=True,
    attn_implementation='sdpa', torch_dtype=torch.bfloat16) # sdpa or flash_attention_2, no eager
model = model.eval().cuda()
tokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-o-2_6', trust_remote_code=True)

model.init_tts()
model.tts.float()
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h5&gt;Mimick 
 &lt;!-- omit in toc --&gt;&lt;/h5&gt; 
&lt;p&gt;&lt;code&gt;Mimick&lt;/code&gt; task reflects a model's end-to-end speech modeling capability. The model takes audio input, and outputs an ASR transcription and subsequently reconstructs the original audio with high similarity. The higher the similarity between the reconstructed audio and the original audio, the stronger the model's foundational capability in end-to-end speech modeling.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;mimick_prompt = "Please repeat each user's speech, including voice style and speech content."
audio_input, _ = librosa.load('./assets/input_examples/Trump_WEF_2018_10s.mp3', sr=16000, mono=True) # load the audio to be mimicked

# `./assets/input_examples/fast-pace.wav`, 
# `./assets/input_examples/chi-english-1.wav` 
# `./assets/input_examples/exciting-emotion.wav` 
# for different aspects of speech-centric features.

msgs = [{'role': 'user', 'content': [mimick_prompt, audio_input]}]
res = model.chat(
    msgs=msgs,
    tokenizer=tokenizer,
    sampling=True,
    max_new_tokens=128,
    use_tts_template=True,
    temperature=0.3,
    generate_audio=True,
    output_audio_path='output_mimick.wav', # save the tts result to output_audio_path
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h5&gt;General Speech Conversation with Configurable Voices 
 &lt;!-- omit in toc --&gt;&lt;/h5&gt; 
&lt;p&gt;A general usage scenario of &lt;code&gt;MiniCPM-o-2.6&lt;/code&gt; is role-playing a specific character based on the audio prompt. It will mimic the voice of the character to some extent and act like the character in text, including language style. In this mode, &lt;code&gt;MiniCPM-o-2.6&lt;/code&gt; sounds &lt;strong&gt;more natural and human-like&lt;/strong&gt;. Self-defined audio prompts can be used to customize the voice of the character in an end-to-end manner.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;ref_audio, _ = librosa.load('./assets/input_examples/icl_20.wav', sr=16000, mono=True) # load the reference audio
sys_prompt = model.get_sys_prompt(ref_audio=ref_audio, mode='audio_roleplay', language='en')

# round one
user_question = {'role': 'user', 'content': [librosa.load('xxx.wav', sr=16000, mono=True)[0]]}
msgs = [sys_prompt, user_question]
res = model.chat(
    msgs=msgs,
    tokenizer=tokenizer,
    sampling=True,
    max_new_tokens=128,
    use_tts_template=True,
    generate_audio=True,
    temperature=0.3,
    output_audio_path='result_roleplay_round_1.wav',
)

# round two
history = msgs.append({'role': 'assistant', 'content': res})
user_question = {'role': 'user', 'content': [librosa.load('xxx.wav', sr=16000, mono=True)[0]]}
msgs = history.append(user_question)
res = model.chat(
    msgs=msgs,
    tokenizer=tokenizer,
    sampling=True,
    max_new_tokens=128,
    use_tts_template=True,
    generate_audio=True,
    temperature=0.3,
    output_audio_path='result_roleplay_round_2.wav',
)
print(res)
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h5&gt;Speech Conversation as an AI Assistant 
 &lt;!-- omit in toc --&gt;&lt;/h5&gt; 
&lt;p&gt;An enhanced feature of &lt;code&gt;MiniCPM-o-2.6&lt;/code&gt; is to act as an AI assistant, but only with limited choice of voices. In this mode, &lt;code&gt;MiniCPM-o-2.6&lt;/code&gt; is &lt;strong&gt;less human-like and more like a voice assistant&lt;/strong&gt;. In this mode, the model is more instruction-following. For demo, you are suggested to use &lt;code&gt;assistant_female_voice&lt;/code&gt;, &lt;code&gt;assistant_male_voice&lt;/code&gt;, and &lt;code&gt;assistant_default_female_voice&lt;/code&gt;. Other voices may work but not as stable as the default voices.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Please note that, &lt;code&gt;assistant_female_voice&lt;/code&gt; and &lt;code&gt;assistant_male_voice&lt;/code&gt; are more stable but sounds like robots, while &lt;code&gt;assistant_default_female_voice&lt;/code&gt; is more human-alike but not stable, its voice often changes in multiple turns. We suggest you to try stable voices &lt;code&gt;assistant_female_voice&lt;/code&gt; and &lt;code&gt;assistant_male_voice&lt;/code&gt;.&lt;/em&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;ref_audio, _ = librosa.load('./assets/input_examples/assistant_female_voice.wav', sr=16000, mono=True) # or use `./assets/input_examples/assistant_male_voice.wav`
sys_prompt = model.get_sys_prompt(ref_audio=ref_audio, mode='audio_assistant', language='en') 
user_question = {'role': 'user', 'content': [librosa.load('xxx.wav', sr=16000, mono=True)[0]]} # load the user's audio question

# round one
msgs = [sys_prompt, user_question]
res = model.chat(
    msgs=msgs,
    tokenizer=tokenizer,
    sampling=True,
    max_new_tokens=128,
    use_tts_template=True,
    generate_audio=True,
    temperature=0.3,
    output_audio_path='result_assistant_round_1.wav',
)

# round two
history = msgs.append({'role': 'assistant', 'content': res})
user_question = {'role': 'user', 'content': [librosa.load('xxx.wav', sr=16000, mono=True)[0]]}
msgs = history.append(user_question)
res = model.chat(
    msgs=msgs,
    tokenizer=tokenizer,
    sampling=True,
    max_new_tokens=128,
    use_tts_template=True,
    generate_audio=True,
    temperature=0.3,
    output_audio_path='result_assistant_round_2.wav',
)
print(res)
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h5&gt;Instruction-to-Speech 
 &lt;!-- omit in toc --&gt;&lt;/h5&gt; 
&lt;p&gt;&lt;code&gt;MiniCPM-o-2.6&lt;/code&gt; can also do Instruction-to-Speech, aka &lt;strong&gt;Voice Creation&lt;/strong&gt;. You can describe a voice in detail, and the model will generate a voice that matches the description. For more Instruction-to-Speech sample instructions, you can refer to &lt;a href="https://voxinstruct.github.io/VoxInstruct/"&gt;https://voxinstruct.github.io/VoxInstruct/&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;instruction = 'Speak like a male charming superstar, radiating confidence and style in every word.'

msgs = [{'role': 'user', 'content': [instruction]}]

res = model.chat(
    msgs=msgs,
    tokenizer=tokenizer,
    sampling=True,
    max_new_tokens=128,
    use_tts_template=True,
    generate_audio=True,
    temperature=0.3,
    output_audio_path='result_voice_creation.wav',
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h5&gt;Voice Cloning 
 &lt;!-- omit in toc --&gt;&lt;/h5&gt; 
&lt;p&gt;&lt;code&gt;MiniCPM-o-2.6&lt;/code&gt; can also do zero-shot text-to-speech, aka &lt;strong&gt;Voice Cloning&lt;/strong&gt;. With this mode, model will act like a TTS model.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;ref_audio, _ = librosa.load('./assets/input_examples/icl_20.wav', sr=16000, mono=True) # load the reference audio
sys_prompt = model.get_sys_prompt(ref_audio=ref_audio, mode='voice_cloning', language='en')
text_prompt = f"Please read the text below."
user_question = {'role': 'user', 'content': [text_prompt, "content that you want to read"]}

msgs = [sys_prompt, user_question]
res = model.chat(
    msgs=msgs,
    tokenizer=tokenizer,
    sampling=True,
    max_new_tokens=128,
    use_tts_template=True,
    generate_audio=True,
    temperature=0.3,
    output_audio_path='result_voice_cloning.wav',
)

&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h5&gt;Addressing Various Audio Understanding Tasks 
 &lt;!-- omit in toc --&gt;&lt;/h5&gt; 
&lt;p&gt;&lt;code&gt;MiniCPM-o-2.6&lt;/code&gt; can also be used to address various audio understanding tasks, such as ASR, speaker analysis, general audio captioning, and sound scene tagging.&lt;/p&gt; 
&lt;p&gt;For audio-to-text tasks, you can use the following prompts:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ASR with ZH(same as AST en2zh): &lt;code&gt;ËØ∑‰ªîÁªÜÂê¨ËøôÊÆµÈü≥È¢ëÁâáÊÆµÔºåÂπ∂Â∞ÜÂÖ∂ÂÜÖÂÆπÈÄêÂ≠óËÆ∞ÂΩï„ÄÇ&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;ASR with EN(same as AST zh2en): &lt;code&gt;Please listen to the audio snippet carefully and transcribe the content.&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Speaker Analysis: &lt;code&gt;Based on the speaker's content, speculate on their gender, condition, age range, and health status.&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;General Audio Caption: &lt;code&gt;Summarize the main content of the audio.&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;General Sound Scene Tagging: &lt;code&gt;Utilize one keyword to convey the audio's content or the associated scene.&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;task_prompt = "Please listen to the audio snippet carefully and transcribe the content." + "\n" # can change to other prompts.
audio_input, _ = librosa.load('./assets/input_examples/audio_understanding.mp3', sr=16000, mono=True) # load the audio to be captioned

msgs = [{'role': 'user', 'content': [task_prompt, audio_input]}]

res = model.chat(
    msgs=msgs,
    tokenizer=tokenizer,
    sampling=True,
    max_new_tokens=128,
    use_tts_template=True,
    generate_audio=True,
    temperature=0.3,
    output_audio_path='result_audio_understanding.wav',
)
print(res)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Multimodal Live Streaming&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt; Click to view Python code running MiniCPM-o 2.6 with chat inference. &lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;import math
import numpy as np
from PIL import Image
from moviepy.editor import VideoFileClip
import tempfile
import librosa
import soundfile as sf
import torch
from transformers import AutoModel, AutoTokenizer

def get_video_chunk_content(video_path, flatten=True):
    video = VideoFileClip(video_path)
    print('video_duration:', video.duration)
    
    with tempfile.NamedTemporaryFile(suffix=".wav", delete=True) as temp_audio_file:
        temp_audio_file_path = temp_audio_file.name
        video.audio.write_audiofile(temp_audio_file_path, codec="pcm_s16le", fps=16000)
        audio_np, sr = librosa.load(temp_audio_file_path, sr=16000, mono=True)
    num_units = math.ceil(video.duration)
    
    # 1 frame + 1s audio chunk
    contents= []
    for i in range(num_units):
        frame = video.get_frame(i+1)
        image = Image.fromarray((frame).astype(np.uint8))
        audio = audio_np[sr*i:sr*(i+1)]
        if flatten:
            contents.extend(["&amp;lt;unit&amp;gt;", image, audio])
        else:
            contents.append(["&amp;lt;unit&amp;gt;", image, audio])
    
    return contents


model = AutoModel.from_pretrained('openbmb/MiniCPM-o-2_6', trust_remote_code=True,
    attn_implementation='sdpa', torch_dtype=torch.bfloat16)
model = model.eval().cuda()
tokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-o-2_6', trust_remote_code=True)

model.init_tts()

# If you are using an older version of PyTorch, you might encounter this issue "weight_norm_fwd_first_dim_kernel" not implemented for 'BFloat16', Please convert the TTS to float32 type.
# model.tts.float()

# https://huggingface.co/openbmb/MiniCPM-o-2_6/blob/main/assets/Skiing.mp4
video_path="assets/Skiing.mp4"
sys_msg = model.get_sys_prompt(mode='omni', language='en')
# if use voice clone prompt, please set ref_audio
# ref_audio_path = '/path/to/ref_audio'
# ref_audio, _ = librosa.load(ref_audio_path, sr=16000, mono=True)
# sys_msg = model.get_sys_prompt(ref_audio=ref_audio, mode='omni', language='en')

contents = get_video_chunk_content(video_path)
msg = {"role":"user", "content": contents}
msgs = [sys_msg, msg]

# please set generate_audio=True and output_audio_path to save the tts result
generate_audio = True
output_audio_path = 'output.wav'

res = model.chat(
    msgs=msgs,
    tokenizer=tokenizer,
    sampling=True,
    temperature=0.5,
    max_new_tokens=4096,
    omni_input=True, # please set omni_input=True when omni inference
    use_tts_template=True,
    generate_audio=generate_audio,
    output_audio_path=output_audio_path,
    max_slice_nums=1,
    use_image_id=False,
    return_dict=True
)
print(res)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt; Click to view Python code running MiniCPM-o 2.6 with streaming inference. &lt;/summary&gt; 
 &lt;p&gt;Note: The streaming inference has a slight performance degradation because the audio encoding is not global.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# a new conversation need reset session first, it will reset the kv-cache
model.reset_session()

contents = get_video_chunk_content(video_path, flatten=False)
session_id = '123'
generate_audio = True

# 1. prefill system prompt
res = model.streaming_prefill(
    session_id=session_id,
    msgs=[sys_msg], 
    tokenizer=tokenizer
)

# 2. prefill video/audio chunks
for content in contents:
    msgs = [{"role":"user", "content": content}]
    res = model.streaming_prefill(
        session_id=session_id,
        msgs=msgs, 
        tokenizer=tokenizer
    )

# 3. generate
res = model.streaming_generate(
    session_id=session_id,
    tokenizer=tokenizer,
    temperature=0.5,
    generate_audio=generate_audio
)

audios = []
text = ""

if generate_audio:
    for r in res:
        audio_wav = r.audio_wav
        sampling_rate = r.sampling_rate
        txt = r.text

        audios.append(audio_wav)
        text += txt
        
    res = np.concatenate(audios)
    sf.write("output.wav", res, samplerate=sampling_rate)
    print("text:", text)
    print("audio saved to output.wav")
else:
    for r in res:
        text += r['text']
    print("text:", text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;Inference on Multiple GPUs&lt;/h3&gt; 
&lt;p&gt;You can run MiniCPM-Llama3-V 2.5 on multiple low VRAM GPUs (12 GB or 16 GB) by distributing the model's layers across multiple GPUs. Please refer to this &lt;a href="https://github.com/OpenBMB/MiniCPM-V/raw/main/docs/inference_on_multiple_gpus.md"&gt;tutorial&lt;/a&gt; for detailed instructions on how to load the model and inference using multiple low VRAM GPUs.&lt;/p&gt; 
&lt;h3&gt;Inference on Mac&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click to view an example, to run MiniCPM-Llama3-V 2.5 on üíª Mac with MPS (Apple silicon or AMD GPUs). &lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;# test.py  Need more than 16GB memory.
import torch
from PIL import Image
from transformers import AutoModel, AutoTokenizer

model = AutoModel.from_pretrained('openbmb/MiniCPM-Llama3-V-2_5', trust_remote_code=True, low_cpu_mem_usage=True)
model = model.to(device='mps')

tokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-Llama3-V-2_5', trust_remote_code=True)
model.eval()

image = Image.open('./assets/hk_OCR.jpg').convert('RGB')
question = 'Where is this photo taken?'
msgs = [{'role': 'user', 'content': question}]

answer, context, _ = model.chat(
    image=image,
    msgs=msgs,
    context=None,
    tokenizer=tokenizer,
    sampling=True
)
print(answer)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Run with command:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;PYTORCH_ENABLE_MPS_FALLBACK=1 python test.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;Efficient Inference with llama.cpp, Ollama, vLLM&lt;/h3&gt; 
&lt;p&gt;See &lt;a href="https://github.com/OpenBMB/llama.cpp/tree/minicpmv-main/examples/llava/README-minicpmv2.6.md"&gt;our fork of llama.cpp&lt;/a&gt; for more detail. This implementation supports smooth inference of 16~18 token/s on iPad (test environmentÔºöiPad Pro + M4).&lt;/p&gt; 
&lt;p&gt;See &lt;a href="https://github.com/OpenBMB/ollama/raw/minicpm-v2.6/examples/minicpm-v2.6/README.md"&gt;our fork of Ollama&lt;/a&gt; for more detail. This implementation supports smooth inference of 16~18 token/s on iPad (test environmentÔºöiPad Pro + M4).&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt; vLLM now officially supports MiniCPM-V 2.6, MiniCPM-Llama3-V 2.5 and MiniCPM-V 2.0. And you can use our fork to run MiniCPM-o 2.6 for now. Click to see. &lt;/summary&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Install vLLM(&amp;gt;=0.7.1):&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;pip install vllm
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="2"&gt; 
  &lt;li&gt;Run Example:&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://docs.vllm.ai/en/latest/getting_started/examples/vision_language.html"&gt;Vision Language&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://docs.vllm.ai/en/latest/getting_started/examples/audio_language.html"&gt;Audio Language&lt;/a&gt; &lt;/li&gt;
 &lt;/ul&gt;
&lt;/details&gt;   
&lt;h2&gt;Fine-tuning&lt;/h2&gt; 
&lt;h3&gt;Simple Fine-tuning 
 &lt;!-- omit in toc --&gt;&lt;/h3&gt; 
&lt;p&gt;We support simple fine-tuning with Hugging Face for MiniCPM-o 2.6, MiniCPM-V 2.6, MiniCPM-Llama3-V 2.5 and MiniCPM-V 2.0.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/finetune/readme.md"&gt;Reference Document&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;With Align-Anything 
 &lt;!-- omit in toc --&gt;&lt;/h3&gt; 
&lt;p&gt;We support fine-tuning MiniCPM-o 2.6 by PKU-Alignment Team (both vision and audio, SFT and DPO) with the &lt;a href="https://github.com/PKU-Alignment/align-anything"&gt;Align-Anything framework&lt;/a&gt;. Align-Anything is a scalable framework that aims to align any-modality large models with human intentions, open-sourcing the &lt;a href="https://huggingface.co/datasets/PKU-Alignment/align-anything"&gt;datasets, models and benchmarks&lt;/a&gt;. Benefiting from its concise and modular design, it supports 30+ open-source benchmarks, 40+ models and algorithms including SFT, SimPO, RLHF, &lt;em&gt;etc&lt;/em&gt;. It also provides 30+ directly runnable scripts, making it suitable for beginners to quickly get started.&lt;/p&gt; 
&lt;p&gt;Best Practices: &lt;a href="https://github.com/PKU-Alignment/align-anything/tree/main/scripts"&gt;MiniCPM-o 2.6&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;With LLaMA-Factory 
 &lt;!-- omit in toc --&gt;&lt;/h3&gt; 
&lt;p&gt;We support fine-tuning MiniCPM-o 2.6 and MiniCPM-V 2.6 with the LLaMA-Factory framework. LLaMA-Factory provides a solution for flexibly customizing the fine-tuning (Lora/Full/Qlora) of 200+ LLMs without the need for coding through the built-in web UI LLaMABoard. It supports various training methods like sft/ppo/dpo/kto and advanced algorithms like Galore/BAdam/LLaMA-Pro/Pissa/LongLoRA.&lt;/p&gt; 
&lt;p&gt;Best Practices: &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/docs/llamafactory_train_and_infer.md"&gt;MiniCPM-o 2.6 | MiniCPM-V 2.6&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;With the SWIFT Framework 
 &lt;!-- omit in toc --&gt;&lt;/h3&gt; 
&lt;p&gt;We now support MiniCPM-V series fine-tuning with the SWIFT framework. SWIFT supports training, inference, evaluation and deployment of nearly 200 LLMs and MLLMs . It supports the lightweight training solutions provided by PEFT and a complete Adapters Library including techniques such as NEFTune, LoRA+ and LLaMA-PRO.&lt;/p&gt; 
&lt;p&gt;Best PracticesÔºö&lt;a href="https://github.com/modelscope/swift/raw/main/docs/source/Multi-Modal/minicpm-v%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5.md"&gt;MiniCPM-V 1.0&lt;/a&gt;, &lt;a href="https://github.com/modelscope/swift/raw/main/docs/source/Multi-Modal/minicpm-v-2%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5.md"&gt;MiniCPM-V 2.0&lt;/a&gt;, &lt;a href="https://github.com/modelscope/ms-swift/issues/1613"&gt;MiniCPM-V 2.6&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Awesome work using MiniCPM-V &amp;amp; MiniCPM-o&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/CatchTheTornado/text-extract-api"&gt;text-extract-api&lt;/a&gt;: Document extraction API using OCRs and Ollama supported models &lt;img src="https://img.shields.io/github/stars/CatchTheTornado/text-extract-api" alt="GitHub Repo stars" /&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/heshengtao/comfyui_LLM_party"&gt;comfyui_LLM_party&lt;/a&gt;: Build LLM workflows and integrate into existing image workflows &lt;img src="https://img.shields.io/github/stars/heshengtao/comfyui_LLM_party" alt="GitHub Repo stars" /&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/imanoop7/Ollama-OCR"&gt;Ollama-OCR&lt;/a&gt;: OCR package uses vlms through Ollama to extract text from images and PDF &lt;img src="https://img.shields.io/github/stars/imanoop7/Ollama-OCR" alt="GitHub Repo stars" /&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/MixLabPro/comfyui-mixlab-nodes"&gt;comfyui-mixlab-nodes&lt;/a&gt;: ComfyUI node suite supports Workflow-to-APP„ÄÅGPT&amp;amp;3D and more &lt;img src="https://img.shields.io/github/stars/MixLabPro/comfyui-mixlab-nodes" alt="GitHub Repo stars" /&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/HumanAIGC-Engineering/OpenAvatarChat"&gt;OpenAvatarChat&lt;/a&gt;: Interactive digital human conversation implementation on single PC &lt;img src="https://img.shields.io/github/stars/HumanAIGC-Engineering/OpenAvatarChat" alt="GitHub Repo stars" /&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/arkohut/pensieve"&gt;pensieve&lt;/a&gt;: A privacy-focused passive recording project by recording screen content &lt;img src="https://img.shields.io/github/stars/arkohut/pensieve" alt="GitHub Repo stars" /&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/icereed/paperless-gpt"&gt;paperless-gpt&lt;/a&gt;: Use LLMs to handle paperless-ngx, AI-powered titles, tags and OCR &lt;img src="https://img.shields.io/github/stars/icereed/paperless-gpt" alt="GitHub Repo stars" /&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/kimjammer/Neuro"&gt;Neuro&lt;/a&gt;: A recreation of Neuro-Sama, but running on local models on consumer hardware &lt;img src="https://img.shields.io/github/stars/kimjammer/Neuro" alt="GitHub Repo stars" /&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;FAQs&lt;/h2&gt; 
&lt;p&gt;Click here to view the &lt;a href="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/docs/faqs.md"&gt;FAQs&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Limitations&lt;/h2&gt; 
&lt;p&gt;As an experimental trial, we find MiniCPM-o 2.6 has notable limitations worth further investigation and improvement.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Unstable speech output.&lt;/strong&gt; The speech generation can be flawed with noisy backgrounds and unmeaningful sounds.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Repeated response.&lt;/strong&gt; The model tends to repeat its response when encountering similar consecutive user queries.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;High-latency on Web Demo.&lt;/strong&gt; Users may experience unusual high-latency when using web demo hosted on overseas servers. We recommend deploying the demo locally or with good network connections.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Model License 
 &lt;!-- omit in toc --&gt;&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;This repository is released under the &lt;a href="https://github.com/OpenBMB/MiniCPM/raw/main/LICENSE"&gt;Apache-2.0&lt;/a&gt; License.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;The usage of MiniCPM-o/V model weights must strictly follow &lt;a href="https://github.com/OpenBMB/MiniCPM/raw/main/MiniCPM%20Model%20License.md"&gt;MiniCPM Model License.md&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;The models and weights of MiniCPM are completely free for academic research. after filling out a &lt;a href="https://modelbest.feishu.cn/share/base/form/shrcnpV5ZT9EJ6xYjh3Kx0J6v8g"&gt;"questionnaire"&lt;/a&gt; for registration, are also available for free commercial use.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Statement 
 &lt;!-- omit in toc --&gt;&lt;/h2&gt; 
&lt;p&gt;As MLLMs, MiniCPM-o/V models generate content by learning a large number of multimodal corpora, but they cannot comprehend, express personal opinions, or make value judgements. Anything generated by MiniCPM-o/V models does not represent the views and positions of the model developers&lt;/p&gt; 
&lt;p&gt;We will not be liable for any problems arising from the use of MiniCPM-o/V models, including but not limited to data security issues, risk of public opinion, or any risks and problems arising from the misdirection, misuse, dissemination, or misuse of the model.&lt;/p&gt; 
&lt;h2&gt;Institutions 
 &lt;!-- omit in toc --&gt;&lt;/h2&gt; 
&lt;p&gt;This project is developed by the following institutions:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/thunlp.png" width="28px" /&gt; &lt;a href="https://nlp.csai.tsinghua.edu.cn/"&gt;THUNLP&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;img src="https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/modelbest.png" width="28px" /&gt; &lt;a href="https://modelbest.cn/"&gt;ModelBest&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üåü Star History 
 &lt;!-- omit in toc --&gt;&lt;/h2&gt; 
&lt;!-- &lt;table align="center"&gt;
    &lt;p align="center"&gt;
      &lt;img src="assets/star_history.svg"/&gt;
    &lt;/p&gt;
&lt;/table&gt; --&gt; 
&lt;picture&gt; 
 &lt;source media="(prefers-color-scheme: dark)" srcset="
      https://api.star-history.com/svg?repos=OpenBMB/MiniCPM-o&amp;amp;type=Date&amp;amp;theme=dark
    " /&gt; 
 &lt;source media="(prefers-color-scheme: light)" srcset="
      https://api.star-history.com/svg?repos=OpenBMB/MiniCPM-o&amp;amp;type=Date
    " /&gt; 
 &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=OpenBMB/MiniCPM-o&amp;amp;type=Date" /&gt; 
&lt;/picture&gt; 
&lt;h2&gt;Key Techniques and Other Multimodal Projects 
 &lt;!-- omit in toc --&gt;&lt;/h2&gt; 
&lt;p&gt;üëè Welcome to explore key techniques of MiniCPM-o/V and other multimodal projects of our team:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/OpenBMB/VisCPM/tree/main"&gt;VisCPM&lt;/a&gt; | &lt;a href="https://github.com/OpenBMB/RLPR"&gt;RLPR&lt;/a&gt; | &lt;a href="https://github.com/RLHF-V/RLHF-V"&gt;RLHF-V&lt;/a&gt; | &lt;a href="https://github.com/thunlp/LLaVA-UHD"&gt;LLaVA-UHD&lt;/a&gt; | &lt;a href="https://github.com/RLHF-V/RLAIF-V"&gt;RLAIF-V&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Citation 
 &lt;!-- omit in toc --&gt;&lt;/h2&gt; 
&lt;p&gt;If you find our model/code/paper helpful, please consider citing our papers üìù and staring us ‚≠êÔ∏èÔºÅ&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bib"&gt;@article{yao2024minicpm,
  title={MiniCPM-V: A GPT-4V Level MLLM on Your Phone},
  author={Yao, Yuan and Yu, Tianyu and Zhang, Ao and Wang, Chongyi and Cui, Junbo and Zhu, Hongji and Cai, Tianchi and Li, Haoyu and Zhao, Weilin and He, Zhihui and others},
  journal={arXiv preprint arXiv:2408.01800},
  year={2024}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>openai/openai-agents-python</title>
      <link>https://github.com/openai/openai-agents-python</link>
      <description>&lt;p&gt;A lightweight, powerful framework for multi-agent workflows&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;OpenAI Agents SDK&lt;/h1&gt; 
&lt;p&gt;The OpenAI Agents SDK is a lightweight yet powerful framework for building multi-agent workflows. It is provider-agnostic, supporting the OpenAI Responses and Chat Completions APIs, as well as 100+ other LLMs.&lt;/p&gt; 
&lt;img src="https://cdn.openai.com/API/docs/images/orchestration.png" alt="Image of the Agents Tracing UI" style="max-height: 803px;" /&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Looking for the JavaScript/TypeScript version? Check out &lt;a href="https://github.com/openai/openai-agents-js"&gt;Agents SDK JS/TS&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Core concepts:&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://openai.github.io/openai-agents-python/agents"&gt;&lt;strong&gt;Agents&lt;/strong&gt;&lt;/a&gt;: LLMs configured with instructions, tools, guardrails, and handoffs&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openai.github.io/openai-agents-python/handoffs/"&gt;&lt;strong&gt;Handoffs&lt;/strong&gt;&lt;/a&gt;: A specialized tool call used by the Agents SDK for transferring control between agents&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openai.github.io/openai-agents-python/guardrails/"&gt;&lt;strong&gt;Guardrails&lt;/strong&gt;&lt;/a&gt;: Configurable safety checks for input and output validation&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/openai/openai-agents-python/main/#sessions"&gt;&lt;strong&gt;Sessions&lt;/strong&gt;&lt;/a&gt;: Automatic conversation history management across agent runs&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openai.github.io/openai-agents-python/tracing/"&gt;&lt;strong&gt;Tracing&lt;/strong&gt;&lt;/a&gt;: Built-in tracking of agent runs, allowing you to view, debug and optimize your workflows&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Explore the &lt;a href="https://raw.githubusercontent.com/openai/openai-agents-python/main/examples"&gt;examples&lt;/a&gt; directory to see the SDK in action, and read our &lt;a href="https://openai.github.io/openai-agents-python/"&gt;documentation&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;Get started&lt;/h2&gt; 
&lt;p&gt;To get started, set up your Python environment (Python 3.9 or newer required), and then install OpenAI Agents SDK package.&lt;/p&gt; 
&lt;h3&gt;venv&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
pip install openai-agents
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For voice support, install with the optional &lt;code&gt;voice&lt;/code&gt; group: &lt;code&gt;pip install 'openai-agents[voice]'&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;uv&lt;/h3&gt; 
&lt;p&gt;If you're familiar with &lt;a href="https://docs.astral.sh/uv/"&gt;uv&lt;/a&gt;, using the tool would be even similar:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv init
uv add openai-agents
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For voice support, install with the optional &lt;code&gt;voice&lt;/code&gt; group: &lt;code&gt;uv add 'openai-agents[voice]'&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Hello world example&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from agents import Agent, Runner

agent = Agent(name="Assistant", instructions="You are a helpful assistant")

result = Runner.run_sync(agent, "Write a haiku about recursion in programming.")
print(result.final_output)

# Code within the code,
# Functions calling themselves,
# Infinite loop's dance.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;(&lt;em&gt;If running this, ensure you set the &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; environment variable&lt;/em&gt;)&lt;/p&gt; 
&lt;p&gt;(&lt;em&gt;For Jupyter notebook users, see &lt;a href="https://raw.githubusercontent.com/openai/openai-agents-python/main/examples/basic/hello_world_jupyter.ipynb"&gt;hello_world_jupyter.ipynb&lt;/a&gt;&lt;/em&gt;)&lt;/p&gt; 
&lt;h2&gt;Handoffs example&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from agents import Agent, Runner
import asyncio

spanish_agent = Agent(
    name="Spanish agent",
    instructions="You only speak Spanish.",
)

english_agent = Agent(
    name="English agent",
    instructions="You only speak English",
)

triage_agent = Agent(
    name="Triage agent",
    instructions="Handoff to the appropriate agent based on the language of the request.",
    handoffs=[spanish_agent, english_agent],
)


async def main():
    result = await Runner.run(triage_agent, input="Hola, ¬øc√≥mo est√°s?")
    print(result.final_output)
    # ¬°Hola! Estoy bien, gracias por preguntar. ¬øY t√∫, c√≥mo est√°s?


if __name__ == "__main__":
    asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Functions example&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import asyncio

from agents import Agent, Runner, function_tool


@function_tool
def get_weather(city: str) -&amp;gt; str:
    return f"The weather in {city} is sunny."


agent = Agent(
    name="Hello world",
    instructions="You are a helpful agent.",
    tools=[get_weather],
)


async def main():
    result = await Runner.run(agent, input="What's the weather in Tokyo?")
    print(result.final_output)
    # The weather in Tokyo is sunny.


if __name__ == "__main__":
    asyncio.run(main())
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;The agent loop&lt;/h2&gt; 
&lt;p&gt;When you call &lt;code&gt;Runner.run()&lt;/code&gt;, we run a loop until we get a final output.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;We call the LLM, using the model and settings on the agent, and the message history.&lt;/li&gt; 
 &lt;li&gt;The LLM returns a response, which may include tool calls.&lt;/li&gt; 
 &lt;li&gt;If the response has a final output (see below for more on this), we return it and end the loop.&lt;/li&gt; 
 &lt;li&gt;If the response has a handoff, we set the agent to the new agent and go back to step 1.&lt;/li&gt; 
 &lt;li&gt;We process the tool calls (if any) and append the tool responses messages. Then we go to step 1.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;There is a &lt;code&gt;max_turns&lt;/code&gt; parameter that you can use to limit the number of times the loop executes.&lt;/p&gt; 
&lt;h3&gt;Final output&lt;/h3&gt; 
&lt;p&gt;Final output is the last thing the agent produces in the loop.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;If you set an &lt;code&gt;output_type&lt;/code&gt; on the agent, the final output is when the LLM returns something of that type. We use &lt;a href="https://platform.openai.com/docs/guides/structured-outputs"&gt;structured outputs&lt;/a&gt; for this.&lt;/li&gt; 
 &lt;li&gt;If there's no &lt;code&gt;output_type&lt;/code&gt; (i.e. plain text responses), then the first LLM response without any tool calls or handoffs is considered as the final output.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;As a result, the mental model for the agent loop is:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;If the current agent has an &lt;code&gt;output_type&lt;/code&gt;, the loop runs until the agent produces structured output matching that type.&lt;/li&gt; 
 &lt;li&gt;If the current agent does not have an &lt;code&gt;output_type&lt;/code&gt;, the loop runs until the current agent produces a message without any tool calls/handoffs.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Common agent patterns&lt;/h2&gt; 
&lt;p&gt;The Agents SDK is designed to be highly flexible, allowing you to model a wide range of LLM workflows including deterministic flows, iterative loops, and more. See examples in &lt;a href="https://raw.githubusercontent.com/openai/openai-agents-python/main/examples/agent_patterns"&gt;&lt;code&gt;examples/agent_patterns&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Tracing&lt;/h2&gt; 
&lt;p&gt;The Agents SDK automatically traces your agent runs, making it easy to track and debug the behavior of your agents. Tracing is extensible by design, supporting custom spans and a wide variety of external destinations, including &lt;a href="https://logfire.pydantic.dev/docs/integrations/llms/openai/#openai-agents"&gt;Logfire&lt;/a&gt;, &lt;a href="https://docs.agentops.ai/v1/integrations/agentssdk"&gt;AgentOps&lt;/a&gt;, &lt;a href="https://braintrust.dev/docs/guides/traces/integrations#openai-agents-sdk"&gt;Braintrust&lt;/a&gt;, &lt;a href="https://docs.scorecard.io/docs/documentation/features/tracing#openai-agents-sdk-integration"&gt;Scorecard&lt;/a&gt;, and &lt;a href="https://docs.keywordsai.co/integration/development-frameworks/openai-agent"&gt;Keywords AI&lt;/a&gt;. For more details about how to customize or disable tracing, see &lt;a href="http://openai.github.io/openai-agents-python/tracing"&gt;Tracing&lt;/a&gt;, which also includes a larger list of &lt;a href="http://openai.github.io/openai-agents-python/tracing/#external-tracing-processors-list"&gt;external tracing processors&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Long running agents &amp;amp; human-in-the-loop&lt;/h2&gt; 
&lt;p&gt;You can use the Agents SDK &lt;a href="https://temporal.io/"&gt;Temporal&lt;/a&gt; integration to run durable, long-running workflows, including human-in-the-loop tasks. View a demo of Temporal and the Agents SDK working in action to complete long-running tasks &lt;a href="https://www.youtube.com/watch?v=fFBZqzT4DD8"&gt;in this video&lt;/a&gt;, and &lt;a href="https://github.com/temporalio/sdk-python/tree/main/temporalio/contrib/openai_agents"&gt;view docs here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Sessions&lt;/h2&gt; 
&lt;p&gt;The Agents SDK provides built-in session memory to automatically maintain conversation history across multiple agent runs, eliminating the need to manually handle &lt;code&gt;.to_input_list()&lt;/code&gt; between turns.&lt;/p&gt; 
&lt;h3&gt;Quick start&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from agents import Agent, Runner, SQLiteSession

# Create agent
agent = Agent(
    name="Assistant",
    instructions="Reply very concisely.",
)

# Create a session instance
session = SQLiteSession("conversation_123")

# First turn
result = await Runner.run(
    agent,
    "What city is the Golden Gate Bridge in?",
    session=session
)
print(result.final_output)  # "San Francisco"

# Second turn - agent automatically remembers previous context
result = await Runner.run(
    agent,
    "What state is it in?",
    session=session
)
print(result.final_output)  # "California"

# Also works with synchronous runner
result = Runner.run_sync(
    agent,
    "What's the population?",
    session=session
)
print(result.final_output)  # "Approximately 39 million"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Session options&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;No memory&lt;/strong&gt; (default): No session memory when session parameter is omitted&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;session: Session = DatabaseSession(...)&lt;/code&gt;&lt;/strong&gt;: Use a Session instance to manage conversation history&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from agents import Agent, Runner, SQLiteSession

# Custom SQLite database file
session = SQLiteSession("user_123", "conversations.db")
agent = Agent(name="Assistant")

# Different session IDs maintain separate conversation histories
result1 = await Runner.run(
    agent,
    "Hello",
    session=session
)
result2 = await Runner.run(
    agent,
    "Hello",
    session=SQLiteSession("user_456", "conversations.db")
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Custom session implementations&lt;/h3&gt; 
&lt;p&gt;You can implement your own session memory by creating a class that follows the &lt;code&gt;Session&lt;/code&gt; protocol:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from agents.memory import Session
from typing import List

class MyCustomSession:
    """Custom session implementation following the Session protocol."""

    def __init__(self, session_id: str):
        self.session_id = session_id
        # Your initialization here

    async def get_items(self, limit: int | None = None) -&amp;gt; List[dict]:
        # Retrieve conversation history for the session
        pass

    async def add_items(self, items: List[dict]) -&amp;gt; None:
        # Store new items for the session
        pass

    async def pop_item(self) -&amp;gt; dict | None:
        # Remove and return the most recent item from the session
        pass

    async def clear_session(self) -&amp;gt; None:
        # Clear all items for the session
        pass

# Use your custom session
agent = Agent(name="Assistant")
result = await Runner.run(
    agent,
    "Hello",
    session=MyCustomSession("my_session")
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Development (only needed if you need to edit the SDK/examples)&lt;/h2&gt; 
&lt;ol start="0"&gt; 
 &lt;li&gt;Ensure you have &lt;a href="https://docs.astral.sh/uv/"&gt;&lt;code&gt;uv&lt;/code&gt;&lt;/a&gt; installed.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv --version
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install dependencies&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;make sync
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;(After making changes) lint/test&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code&gt;make check # run tests linter and typechecker
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or to run them individually:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;make tests  # run tests
make mypy   # run typechecker
make lint   # run linter
make format-check # run style checker
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;p&gt;We'd like to acknowledge the excellent work of the open-source community, especially:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.pydantic.dev/latest/"&gt;Pydantic&lt;/a&gt; (data validation) and &lt;a href="https://ai.pydantic.dev/"&gt;PydanticAI&lt;/a&gt; (advanced agent framework)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/BerriAI/litellm"&gt;LiteLLM&lt;/a&gt; (unified interface for 100+ LLMs)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/squidfunk/mkdocs-material"&gt;MkDocs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/mkdocstrings/griffe"&gt;Griffe&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/astral-sh/uv"&gt;uv&lt;/a&gt; and &lt;a href="https://github.com/astral-sh/ruff"&gt;ruff&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We're committed to continuing to build the Agents SDK as an open source framework so others in the community can expand on our approach.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>chubin/cheat.sh</title>
      <link>https://github.com/chubin/cheat.sh</link>
      <description>&lt;p&gt;the only cheat sheet you need&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="http://cheat.sh/files/big-logo-v2-fixed.png" alt="cheat.sh logo" /&gt;&lt;/p&gt; 
&lt;p&gt;Unified access to the best community driven cheat sheets repositories of the world.&lt;/p&gt; 
&lt;p&gt;Let's imagine for a moment that there is such a thing as an ideal cheat sheet. What should it look like? What features should it have?&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Concise&lt;/strong&gt; ‚Äî It should only contain the things you need, and nothing else.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Fast&lt;/strong&gt; ‚Äî It should be possible to use it instantly.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Comprehensive&lt;/strong&gt; ‚Äî It should contain answers for every possible question.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Universal&lt;/strong&gt; ‚Äî It should be available everywhere, anytime, without any preparations.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Unobtrusive&lt;/strong&gt; ‚Äî It should not distract you from your main task.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Tutoring&lt;/strong&gt; ‚Äî It should help you to learn the subject.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Inconspicuous&lt;/strong&gt; ‚Äî It should be possible to use it completely unnoticed.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Such a thing exists! It's easy to &lt;a href="https://raw.githubusercontent.com/chubin/cheat.sh/master/#installation"&gt;install&lt;/a&gt; and there's even &lt;a href="https://raw.githubusercontent.com/chubin/cheat.sh/master/#tab-completion"&gt;auto-complete&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;cheat.sh&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Has a simple curl/browser/editor interface.&lt;/li&gt; 
 &lt;li&gt;Covers 56 programming languages, several DBMSes, and more than 1000 most important UNIX/Linux commands.&lt;/li&gt; 
 &lt;li&gt;Provides access to the best community driven cheat sheets repositories in the world, on par with StackOverflow.&lt;/li&gt; 
 &lt;li&gt;Available everywhere, no installation needed, but can be installed for offline usage.&lt;/li&gt; 
 &lt;li&gt;Ultrafast, returns answers within 100 ms, as a rule.&lt;/li&gt; 
 &lt;li&gt;Has a convenient command line client, &lt;code&gt;cht.sh&lt;/code&gt;, that is very advantageous and helpful, though not mandatory.&lt;/li&gt; 
 &lt;li&gt;Can be used directly from code editors, without opening a browser and not switching your mental context.&lt;/li&gt; 
 &lt;li&gt;Supports a special stealth mode where it can be used fully invisibly without ever touching a key and making sounds.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="center"&gt; &lt;img src="https://cheat.sh/files/demo-curl.gif" /&gt; &lt;/p&gt; 
&lt;h2&gt;Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/chubin/cheat.sh/master/#features"&gt;Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/chubin/cheat.sh/master/#usage"&gt;Usage&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/chubin/cheat.sh/master/#command-line-client-chtsh"&gt;Command line client, cht.sh&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/chubin/cheat.sh/master/#installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/chubin/cheat.sh/master/#client-usage"&gt;Client usage&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/chubin/cheat.sh/master/#tab-completion"&gt;Tab-completion&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/chubin/cheat.sh/master/#bash-tab-completion"&gt;Bash Tab completion&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/chubin/cheat.sh/master/#zsh-tab-completion"&gt;ZSH Tab completion&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/chubin/cheat.sh/master/#stealth-mode"&gt;Stealth mode&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/chubin/cheat.sh/master/#windows-command-line-client"&gt;Windows command line client&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/chubin/cheat.sh/master/#self-hosting"&gt;Self-Hosting&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/chubin/cheat.sh/master/#docker"&gt;Docker&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/chubin/cheat.sh/master/#editors-integration"&gt;Editors integration&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/chubin/cheat.sh/master/#vim"&gt;Vim&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/chubin/cheat.sh/master/#emacs"&gt;Emacs&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/chubin/cheat.sh/master/#visual-studio-code"&gt;Visual Studio Code&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/chubin/cheat.sh/master/#sublime"&gt;Sublime&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/chubin/cheat.sh/master/#intellij-idea"&gt;IntelliJ IDEA&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/chubin/cheat.sh/master/#qtcreator"&gt;QT Creator&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/chubin/cheat.sh/master/#special-pages"&gt;Special pages&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/chubin/cheat.sh/master/#search"&gt;Search&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/chubin/cheat.sh/master/#programming-languages-cheat-sheets"&gt;Programming languages cheat sheets&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/chubin/cheat.sh/master/#cheat-sheets-sources"&gt;Cheat sheets sources&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/chubin/cheat.sh/master/#how-to-contribute"&gt;How to contribute&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/chubin/cheat.sh/master/#how-to-edit-a-cheat-sheet"&gt;How to edit a cheat sheet&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/chubin/cheat.sh/master/#how-to-add-a-cheat-sheet"&gt;How to add a cheat sheet&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/chubin/cheat.sh/master/#how-to-add-a-cheat-sheet-repository"&gt;How to add a cheat sheet repository&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;p&gt;To get a cheat sheet for a UNIX/Linux command from a command line, query the service using &lt;code&gt;curl&lt;/code&gt; or any other HTTP/HTTPS client specifying the name of the command in the query:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    curl cheat.sh/tar
    curl cht.sh/curl
    curl https://cheat.sh/rsync
    curl https://cht.sh/tr
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;As you can see, you can use both HTTPS and HTTP to access the service, and both the long (cheat.sh) and the short (cht.sh) service names.&lt;/p&gt; 
&lt;p&gt;Here &lt;code&gt;tar&lt;/code&gt;, &lt;code&gt;curl&lt;/code&gt;, &lt;code&gt;rsync&lt;/code&gt;, and &lt;code&gt;tr&lt;/code&gt; are names of the UNIX/Linux commands you want to get cheat sheets for.&lt;/p&gt; 
&lt;p&gt;If you don't know the name of the command you need, you can search for it using the &lt;code&gt;~KEYWORD&lt;/code&gt; notation. For example, to see how you can make &lt;code&gt;snapshots&lt;/code&gt; of a filesystem/volume/something else:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    curl cht.sh/~snapshot
&lt;/code&gt;&lt;/pre&gt; 
&lt;p align="center"&gt; &lt;img src="https://cheat.sh/files/cht.sh-url-structure.png" /&gt; &lt;/p&gt; 
&lt;p&gt;The programming language cheat sheets are located in special namespaces dedicated to them.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    curl cht.sh/go/Pointers
    curl cht.sh/scala/Functions
    curl cht.sh/python/lambda
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To get the list of available programming language cheat sheets, use the special query &lt;code&gt;:list&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    curl cht.sh/go/:list
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Almost each programming language has a special page named &lt;code&gt;:learn&lt;/code&gt; that describes the language basics (that's a direct mapping from the &lt;em&gt;"Learn X in Y"&lt;/em&gt; project). It could be a good starting point if you've just started learning a language.&lt;/p&gt; 
&lt;p&gt;If there is no cheat sheet for a programming language query (and it is almost always the case), it is generated on the fly, based on available cheat sheets and answers on StackOverflow. Of course, there is no guarantee that the returned cheat sheet will be a 100% hit, but it is almost always exactly what you are looking for.&lt;/p&gt; 
&lt;p&gt;Try these (and your own) queries to get the impression of that, what the answers look like:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    curl cht.sh/go/reverse+a+list
    curl cht.sh/python/random+list+elements
    curl cht.sh/js/parse+json
    curl cht.sh/lua/merge+tables
    curl cht.sh/clojure/variadic+function
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you don't like an answer for your queries, you can pick another one. For that, repeat the query with an additional parameter &lt;code&gt;/1&lt;/code&gt;, &lt;code&gt;/2&lt;/code&gt; etc. appended:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    curl cht.sh/python/random+string
    curl cht.sh/python/random+string/1
    curl cht.sh/python/random+string/2
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Cheat sheets are formatted as code of the queried programming language (at least we are trying our best to do so) so they can be pasted into a program in this language directly. Text comments, if there are any, are formatted according to the language syntax.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-lua"&gt;    $ curl cht.sh/lua/table+keys
    -- lua: retrieve list of keys in a table

    local keyset={}
    local n=0

    for k,v in pairs(tab) do
      n=n+1
      keyset[n]=k
    end

    --[[
       [ Note that you cannot guarantee any order in keyset. If you want the
       [ keys in sorted order, then sort keyset with table.sort(keyset).
       [ 
       [ [lhf] [so/q/12674345] [cc by-sa 3.0]
       ]]

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you don't need text comments in the answer, you can eliminate them using a special option &lt;code&gt;\?Q&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-lua"&gt;    $ curl cht.sh/lua/table+keys\?Q
    local keyset={}
    local n=0

    for k,v in pairs(tab) do
      n=n+1
      keyset[n]=k
    end
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;And if you don't need syntax highlighting, switch it off using &lt;code&gt;\?T&lt;/code&gt;. You can combine the options together:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    curl cht.sh/go/reverse+a+list\?Q
    curl cht.sh/python/random+list+elements\?Q
    curl cht.sh/js/parse+json\?Q
    curl cht.sh/lua/merge+tables\?QT
    curl cht.sh/clojure/variadic+function\?QT
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Full list of all options described below and in &lt;code&gt;/:help&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Try your own queries. Follow these rules:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Try to be more specific (&lt;code&gt;/python/append+file&lt;/code&gt; is better than &lt;code&gt;/python/file&lt;/code&gt; and &lt;code&gt;/python/append&lt;/code&gt;).&lt;/li&gt; 
 &lt;li&gt;Ask practical question if possible (yet theoretical question are possible too).&lt;/li&gt; 
 &lt;li&gt;Ask programming language questions only; specify the name of the programming language as the section name.&lt;/li&gt; 
 &lt;li&gt;Separate words with &lt;code&gt;+&lt;/code&gt; instead of spaces.&lt;/li&gt; 
 &lt;li&gt;Do not use special characters, they are ignored anyway.&lt;/li&gt; 
 &lt;li&gt;If you want to eliminate cheat sheets containing some word, add it to the query with &lt;code&gt;+-&lt;/code&gt;: &lt;code&gt;python/multiply+matrices+-numpy&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Read more about the programming languages queries below.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Command line client, cht.sh&lt;/h2&gt; 
&lt;p&gt;The cheat.sh service has its own command line client (&lt;code&gt;cht.sh&lt;/code&gt;) that has several useful features compared to querying the service directly with &lt;code&gt;curl&lt;/code&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Special shell mode with a persistent queries context and readline support.&lt;/li&gt; 
 &lt;li&gt;Queries history.&lt;/li&gt; 
 &lt;li&gt;Clipboard integration.&lt;/li&gt; 
 &lt;li&gt;Tab completion support for shells (bash, fish, zsh).&lt;/li&gt; 
 &lt;li&gt;Stealth mode.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;p&gt;To install the client:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;PATH_DIR="$HOME/bin"  # or another directory on your $PATH
mkdir -p "$PATH_DIR"
curl https://cht.sh/:cht.sh &amp;gt; "$PATH_DIR/cht.sh"
chmod +x "$PATH_DIR/cht.sh"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or to install it globally (for all users):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -s https://cht.sh/:cht.sh | sudo tee /usr/local/bin/cht.sh &amp;amp;&amp;amp; sudo chmod +x /usr/local/bin/cht.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note: The package "rlwrap" is a required dependency to run in shell mode. Install this using &lt;code&gt;sudo apt install rlwrap&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Client usage&lt;/h3&gt; 
&lt;p&gt;Now, you can use &lt;code&gt;cht.sh&lt;/code&gt; instead of &lt;code&gt;curl&lt;/code&gt;, and write your queries in more natural way, with spaces instead of &lt;code&gt;+&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    $ cht.sh go reverse a list
    $ cht.sh python random list elements
    $ cht.sh js parse json
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;It is even more convenient to start the client in a special shell mode:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    $ cht.sh --shell
    cht.sh&amp;gt; go reverse a list
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If all your queries are about the same language, you can change the context and spare repeating the programming language name:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    $ cht.sh --shell
    cht.sh&amp;gt; cd go
    cht.sh/go&amp;gt; reverse a list
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or even start the client in this context:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    $ cht.sh --shell go
    cht.sh/go&amp;gt; reverse a list
    ...
    cht.sh/go&amp;gt; join a list
    ...
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you want to change the context, you can do it with the &lt;code&gt;cd&lt;/code&gt; command, or if you want do a single query for some other language, just prepend it with &lt;code&gt;/&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    $ cht.sh --shell go
    ...
    cht.sh/go&amp;gt; /python dictionary comprehension
    ...
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you want to copy the last answer into the clipboard, you can use the &lt;code&gt;c&lt;/code&gt; (&lt;code&gt;copy&lt;/code&gt;) command, or &lt;code&gt;C&lt;/code&gt; (&lt;code&gt;ccopy&lt;/code&gt;, without comments).&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    cht.sh/python&amp;gt; append file
    #  python - How do you append to a file?

    with open("test.txt", "a") as myfile:
        myfile.write("appended text")
    cht.sh/python&amp;gt; C
    copy: 2 lines copied to the selection
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Type &lt;code&gt;help&lt;/code&gt; for other internal &lt;code&gt;cht.sh&lt;/code&gt; commands.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;	cht.sh&amp;gt; help
	help    - show this help
	hush    - do not show the 'help' string at start anymore
	cd LANG - change the language context
	copy    - copy the last answer in the clipboard (aliases: yank, y, c)
	ccopy   - copy the last answer w/o comments (cut comments; aliases: cc, Y, C)
	exit    - exit the cheat shell (aliases: quit, ^D)
	id [ID] - set/show an unique session id ("reset" to reset, "remove" to remove)
	stealth - stealth mode (automatic queries for selected text)
	update  - self update (only if the scriptfile is writeable)
	version - show current cht.sh version
	/:help  - service help
	QUERY   - space separated query staring (examples are below)
				  cht.sh&amp;gt; python zip list
				  cht.sh/python&amp;gt; zip list
				  cht.sh/go&amp;gt; /python zip list
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The &lt;code&gt;cht.sh&lt;/code&gt; client has its configuration file which is located at &lt;code&gt;~/.cht.sh/cht.sh.conf&lt;/code&gt; (location of the file can be overridden by the environment variable &lt;code&gt;CHTSH_CONF&lt;/code&gt;). Use it to specify query options that you would use with each query. For example, to switch syntax highlighting off create the file with the following content:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;CHTSH_QUERY_OPTIONS="T"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or if you want to use a special syntax highlighting theme:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;CHTSH_QUERY_OPTIONS="style=native"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;(&lt;code&gt;curl cht.sh/:styles-demo&lt;/code&gt; to see all supported styles).&lt;/p&gt; 
&lt;p&gt;Other cht.sh configuration parameters:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;CHTSH_CURL_OPTIONS="-A curl"        # curl options used for cht.sh queries
CHTSH_URL=https://cht.sh            # URL of the cheat.sh server
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Tab completion&lt;/h3&gt; 
&lt;h4&gt;Bash Tab completion&lt;/h4&gt; 
&lt;p&gt;To activate tab completion support for &lt;code&gt;cht.sh&lt;/code&gt;, add the &lt;code&gt;:bash_completion&lt;/code&gt; script to your &lt;code&gt;~/.bashrc&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;    curl https://cheat.sh/:bash_completion &amp;gt; ~/.bash.d/cht.sh
    . ~/.bash.d/cht.sh
    # and add . ~/.bash.d/cht.sh to ~/.bashrc
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;ZSH Tab completion&lt;/h4&gt; 
&lt;p&gt;To activate tab completion support for &lt;code&gt;cht.sh&lt;/code&gt;, add the &lt;code&gt;:zsh&lt;/code&gt; script to the &lt;em&gt;fpath&lt;/em&gt; in your &lt;code&gt;~/.zshrc&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-zsh"&gt;    curl https://cheat.sh/:zsh &amp;gt; ~/.zsh.d/_cht
    echo 'fpath=(~/.zsh.d/ $fpath)' &amp;gt;&amp;gt; ~/.zshrc
    # Open a new shell to load the plugin
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Stealth mode&lt;/h3&gt; 
&lt;p&gt;Being used fully unnoticed is one of the most important property of any cheat sheet.&lt;/p&gt; 
&lt;p&gt;cheat.sh can be used completely unnoticed too. The cheat.sh client, &lt;code&gt;cht.sh&lt;/code&gt;, has a special mode, called &lt;strong&gt;stealth mode&lt;/strong&gt;. Using that, you don't even need to touch your keyboard to open a cheat sheet.&lt;/p&gt; 
&lt;p&gt;In this mode, as soon as you select some text with the mouse (and thus adding it into the selection buffer of X Window System or into the clipboard) it's used as a query string for cheat.sh, and the correspondent cheat sheet is automatically shown.&lt;/p&gt; 
&lt;p&gt;Let's imagine, that you are having an online interview, where your interviewer asks you some questions using a shared document (say Google Docs) and you are supposed to write your coding answers there (it's possible too that you'll type in the questions on your own, just to show to the interviewer that you've heard it right).&lt;/p&gt; 
&lt;p&gt;When using the stealth mode of &lt;code&gt;cht.sh&lt;/code&gt;, the only thing you need to do in order to see a cheat sheet for some question, is to select the question using the mouse. If you don't want any text in the answers and the only thing you need is code, use the &lt;code&gt;Q&lt;/code&gt; option when starting the stealth mode.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://cheat.sh/files/stealth-mode.gif" /&gt; &lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;You: Hi!                                            | $ cht.sh --shell python
She: Hi!                                            | cht.sh/python&amp;gt; stealth Q
She: Are you ready for a small interview?           | stealth: you are in the stealth mode; select any text
She: Just a couple of questions                     | stealth: selections longer than 5 words are ignored
She: We will talk about python                      | stealth: query arguments: ?Q
She: Let's start from something simple.             | stealth: use ^C to leave this mode
She: Do you know how to reverse a list in python?   |
You: Sure                                           |
You: (selecting "reverse a list")                   | stealth: reverse a list
                                                    | reverse_lst = lst[::-1]
You: lst[::-1]?                                     |
She: Good.                                          |
She: Do you know how to chain a list of lists?      |
You: (selecting "chain a list of lists")            | stealth: chain a list of lists
                                                    | import itertools
                                                    | a = [["a","b"], ["c"]]
                                                    | print list(itertools.chain.from_iterable(a))
You: May I use external modules?                    |
She: What module do you want to use?                |
You: itertools                                      |
She: Yes, you may use it                            |
You: Ok, then:                                      |
You: itertools.chain.from_iterable(a)               |
She: Good. Let's try something harder.              |
She: What about quicksort implementation?           |
You: (selecting "quicksort implementation")         | stealth: quicksort implementation
You: Let me think about it.                         | (some big and clumsy lowlevel implementation shown)
You: Well...(starting typing it in)                 | def sort(array=[12,4,5,6,7,3,1,15]):
                                                    |     less = []
She: (seeing your ugly pascal style)                |     equal = []
She: Could you write it more concise?               |     greater = []
                                                    |     if len(array) &amp;gt; 1:
You: What do you mean?                              |         pivot = array[0]
                                                    |         for x in array:
She: I mean,                                        |             if x &amp;lt; pivot: less.append(x)
She: do you really need all these ifs and fors?     |             if x == pivot: equal.append(x)
She: Could you maybe just use filter instead?       |             if x &amp;gt; pivot: greater.append(x)
                                                    |         return sort(less)+equal+sort(greater)
You: quicksort with filter?                         |     else:
                                                    |         return array
She: Yes                                            |
You: (selecting "quicksort with filter")            | stealth: quicksort with filter
You: Ok, I will try.                                | return qsort(filter(lt, L[1:]))+[pivot] \
You: Something like this?                           |     +qsort(filter(ge, L[1:]))
You: qsort(filter(lt, L[1:]))+[pivot] \             |
       + qsort(filter(ge, L[1:]))                   |
                                                    |
She: Yes! Perfect! Exactly what I wanted to see!    |
                                                    |

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Of course, this is just for fun, and you should never cheat in your coding interviews, because you know what happens when you do.&lt;/p&gt; 
&lt;p&gt;&lt;img src="http://cheat.sh/files/when-you-lie-katze.png" alt="when you lie in your interview" /&gt;&lt;/p&gt; 
&lt;h3&gt;Windows command line client&lt;/h3&gt; 
&lt;p&gt;You can access cheat.sh from Windows command line too.&lt;/p&gt; 
&lt;p&gt;Use cheat.sh command line client for that: &lt;a href="https://github.com/tpanj/cht.exe"&gt;&lt;code&gt;cht.exe&lt;/code&gt;&lt;/a&gt;. It supports:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;output colorization;&lt;/li&gt; 
 &lt;li&gt;command line options;&lt;/li&gt; 
 &lt;li&gt;its own configuration file.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can also use &lt;a href="https://github.com/lukesampson/scoop"&gt;&lt;code&gt;scoop&lt;/code&gt;&lt;/a&gt; command-line installer for Windows to get it:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-batch"&gt;scoop install cht
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Self-Hosting&lt;/h2&gt; 
&lt;h3&gt;Docker&lt;/h3&gt; 
&lt;p&gt;Currently, the easiest way to get a self-hosted instance running is by using the &lt;code&gt;docker-compose.yml&lt;/code&gt; file.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;docker-compose up
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This builds and runs the image with baked in cheatsheets and starts the app and a Redis instance to back it, making the service available at &lt;a href="http://localhost:8002"&gt;http://localhost:8002&lt;/a&gt; This is currently an early implementation and should probably not be used for anything outside of internal/dev/personal use right now.&lt;/p&gt; 
&lt;h2&gt;Editors integration&lt;/h2&gt; 
&lt;p&gt;You can use &lt;em&gt;cheat.sh&lt;/em&gt; directly from the editor (&lt;em&gt;Emacs&lt;/em&gt;, &lt;em&gt;Sublime&lt;/em&gt;, &lt;em&gt;Vim&lt;/em&gt;, and &lt;em&gt;Visual Studio Code&lt;/em&gt; are currently supported; not all features are supported by all plugins though; see below). Instead of opening your browser, googling, browsing Stack Overflow and eventually copying the code snippets you need into the clipboard and later pasting them into the editor, you can achieve the same instantly and without leaving the editor at all!&lt;/p&gt; 
&lt;p&gt;Here is what it looks like in Vim:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;If you have a question while editing a program, you can just type your question directly in the buffer and press &lt;code&gt;&amp;lt;leader&amp;gt;KK&lt;/code&gt;. You will get the answer to your question in pager. (with &lt;code&gt;&amp;lt;leader&amp;gt;KB&lt;/code&gt; you'll get the answer in a separate buffer).&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;If you like the answer, you can manually paste it from the buffer or the pager, or if you are lazy you can use &lt;code&gt;&amp;lt;leader&amp;gt;KP&lt;/code&gt; to paste it below/under your question (or replace you question using &lt;code&gt;&amp;lt;leader&amp;gt;KR&lt;/code&gt;). If you want the answer without the comments, &lt;code&gt;&amp;lt;leader&amp;gt;KC&lt;/code&gt; replays the last query toggling them.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;If you use some static analysis plugin such as &lt;em&gt;syntastic&lt;/em&gt; (for Vim), you can use its warning and error messages as cheat.sh queries: place the cursor on the problem line and press &lt;code&gt;&amp;lt;leader&amp;gt;KE&lt;/code&gt;: explanation for the warning will be opened in a new buffer.&lt;/p&gt; 
&lt;p&gt;Features supported by cheat.sh plugins for different editors:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Feature&lt;/th&gt; 
   &lt;th&gt;Emacs&lt;/th&gt; 
   &lt;th&gt;Sublime&lt;/th&gt; 
   &lt;th&gt;Vim&lt;/th&gt; 
   &lt;th&gt;VSCode&lt;/th&gt; 
   &lt;th&gt;IDEA&lt;/th&gt; 
   &lt;th&gt;QtCreator&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Command queries&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Queries from buffer&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Toggle comments&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Prev/next answer&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Multiple answers&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Warnings as queries&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Queries history&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Session id&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Configurable server&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Vim&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/dbeniamine/cheat.sh-vim"&gt;cheat.sh-vim&lt;/a&gt; ‚Äî Vim support&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Here is Vim configuration example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-vim"&gt;" some configuration above ...

let mapleader=" "

call vundle#begin()
Bundle 'gmarik/vundle'
Bundle 'scrooloose/syntastic'
Bundle 'dbeniamine/cheat.sh-vim'
call vundle#end()

let g:syntastic_javascript_checkers = [ 'jshint' ]
let g:syntastic_ocaml_checkers = ['merlin']
let g:syntastic_python_checkers = ['pylint']
let g:syntastic_shell_checkers = ['shellcheck']

" some configuration below ...
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;In this example, several Vim plugins are used:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/VundleVim/Vundle.vim"&gt;gmarik/vundle&lt;/a&gt; ‚Äî Vim plugin manager&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/vim-syntastic/syntastic"&gt;scrooloose/syntastic&lt;/a&gt; ‚Äî Syntax checking plugin&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/dbeniamine/cheat.sh-vim"&gt;cheat.sh-vim&lt;/a&gt; ‚Äî Vim support&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Syntastic shows warnings and errors (found by code analysis tools: &lt;code&gt;jshint&lt;/code&gt;, &lt;code&gt;merlin&lt;/code&gt;, &lt;code&gt;pylint&lt;/code&gt;, &lt;code&gt;shellcheck&lt;/code&gt; etc.), and &lt;code&gt;cheat.sh-vim&lt;/code&gt; shows you explanations for the errors and warnings and answers on programming languages queries written in the editor.&lt;/p&gt; 
&lt;p&gt;Watch a demo, where the most important features of the cheat.sh Vim plugin are shown (5 Min):&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://cheat.sh/files/vim-demo.gif" /&gt; &lt;/p&gt; 
&lt;p&gt;Or, if you want to scroll and/or pause, the same on YouTube:&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="http://www.youtube.com/watch?feature=player_embedded&amp;amp;v=xyf6MJ0y-z8
  " target="_blank"&gt;&lt;img src="http://img.youtube.com/vi/xyf6MJ0y-z8/0.jpg" alt="cheat.sh-vim: Using cheat.sh from vim" width="700" height="490" border="10" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;!-- [![asciicast](https://asciinema.org/a/c6QRIhus7np2OOQzmQ2RNXzRZ.png)](https://asciinema.org/a/c6QRIhus7np2OOQzmQ2RNXzRZ) --&gt; 
&lt;h3&gt;Emacs&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/davep/cheat-sh.el"&gt;cheat-sh.el&lt;/a&gt; ‚Äî Emacs support (available also at cheat.sh/:emacs)&lt;/li&gt; 
 &lt;li&gt;cheat.sh/:emacs-ivy ‚Äî Emacs support for ivy users&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://asciinema.org/a/3xvqwrsu9g4taj5w526sb2t35"&gt;&lt;img src="https://asciinema.org/a/3xvqwrsu9g4taj5w526sb2t35.png" alt="asciicast" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Visual Studio Code&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/mre/vscode-snippet"&gt;vscode-snippet&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Install it from &lt;a href="https://marketplace.visualstudio.com/items?itemName=vscode-snippet.Snippet"&gt;VSCode Marketplace&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Usage:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Hit &lt;kbd&gt;‚åò Command&lt;/kbd&gt; + &lt;kbd&gt;‚áß Shift&lt;/kbd&gt; + &lt;kbd&gt;p&lt;/kbd&gt;&lt;/li&gt; 
 &lt;li&gt;Run &lt;code&gt;Snippet: Find&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Type your query and hit enter.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;a href="https://github.com/mre/vscode-snippet"&gt;&lt;img src="https://cheat.sh/files/vscode-snippet-demo.gif" alt="vscode-snippet" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;(GIF courtesy: Matthias Endler, @mre)&lt;/em&gt;&lt;/p&gt; 
&lt;h3&gt;Sublime&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/gauravk-in/cheat.sh-sublime-plugin/"&gt;cheat.sh-sublime-plugin&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Usage:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Write your query string.&lt;/li&gt; 
 &lt;li&gt;Select the query string.&lt;/li&gt; 
 &lt;li&gt;Press &lt;kbd&gt;Cmd&lt;/kbd&gt; + &lt;kbd&gt;‚áß Shift&lt;/kbd&gt; + &lt;kbd&gt;B&lt;/kbd&gt; to replace the selected query string by the answer generated from &lt;code&gt;cht.sh&lt;/code&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;a href="https://github.com/gauravk-in/cheat.sh-sublime-plugin"&gt;&lt;img src="https://cheat.sh/files/demo-sublime.gif" alt="cheat.sh-sublime-plugin-demo" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;(GIF courtesy: Gaurav Kukreja, @gauravk-in)&lt;/em&gt;&lt;/p&gt; 
&lt;h3&gt;IntelliJ IDEA&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/szymonprz/idea-cheatsh-plugin"&gt;idea-cheatsh-plugin&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Install from &lt;a href="https://plugins.jetbrains.com/plugin/11942-cheat-sh-code-snippets"&gt;idea plugins marketplace&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Usage:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Write query string&lt;/li&gt; 
 &lt;li&gt;Select the query string&lt;/li&gt; 
 &lt;li&gt;Press keyboard shortcut &lt;kbd&gt;Alt&lt;/kbd&gt; + &lt;kbd&gt;C&lt;/kbd&gt; , &lt;kbd&gt;S&lt;/kbd&gt; to replace the selected query string by the answer&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;a href="https://github.com/szymonprz/idea-cheatsh-plugin"&gt;&lt;img src="https://cheat.sh/files/idea-demo.gif" alt="idea-cheatsh-plugin" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;(GIF courtesy: Szymon Przebierowski, @szymonprz)&lt;/em&gt;&lt;/p&gt; 
&lt;h3&gt;QtCreator&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pozemka/cheatsh-qtcreator"&gt;cheatsh-qtcreator&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Current features:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;search word under cursor&lt;/li&gt; 
 &lt;li&gt;search selected&lt;/li&gt; 
 &lt;li&gt;query search&lt;/li&gt; 
 &lt;li&gt;disable comments&lt;/li&gt; 
 &lt;li&gt;paste answer (?TQ version)&lt;/li&gt; 
 &lt;li&gt;custom server URL&lt;/li&gt; 
 &lt;li&gt;custom search context (default is cpp)&lt;/li&gt; 
 &lt;li&gt;hotkeys and menu&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://github.com/pozemka/cheatsh-qtcreator"&gt;&lt;img src="https://user-images.githubusercontent.com/1259724/73876361-ecce5d00-4867-11ea-9f75-c5b127a9739c.gif" alt="cheatsh-qtcreator" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;(GIF courtesy: Pozemka, @pozemka)&lt;/em&gt;&lt;/p&gt; 
&lt;h2&gt;Special pages&lt;/h2&gt; 
&lt;p&gt;There are several special pages that are not cheat sheets. Their names start with colon and have special meaning.&lt;/p&gt; 
&lt;p&gt;Getting started:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    :help               description of all special pages and options
    :intro              cheat.sh introduction, covering the most important usage questions
    :list               list all cheat sheets (can be used in a subsection too: /go/:list)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Command line client &lt;code&gt;cht.sh&lt;/code&gt; and shells support:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    :cht.sh             code of the cht.sh client
    :bash_completion    bash function for tab completion
    :bash               bash function and tab completion setup
    :fish               fish function and tab completion setup
    :zsh                zsh function and tab completion setup
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Editors support:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    :vim                cheat.sh support for Vim
    :emacs              cheat.sh function for Emacs
    :emacs-ivy          cheat.sh function for Emacs (uses ivy)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Other pages:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    :post               how to post new cheat sheet
    :styles             list of color styles
    :styles-demo        show color styles usage examples
    :random             fetches a random page (can be used in a subsection too: /go/:random)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Search&lt;/h2&gt; 
&lt;p&gt;To search for a keyword, use the query:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    /~keyword
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;In this case search is not recursive ‚Äî it is conducted only in a page of the specified level. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    /~snapshot          look for snapshot in the first level cheat sheets
    /scala/~currying     look for currying in scala cheat sheets
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For a recursive search in all cheat sheets, use double slash:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    /~snapshot/r         look for snapshot in all cheat sheets
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can use special search options after the closing slash:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    /~shot/bi           case insensitive (i), word boundaries (b)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;List of search options:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    i   case insensitive search
    b   word boundaries
    r   recursive search
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Programming languages cheat sheets&lt;/h2&gt; 
&lt;p&gt;Cheat sheets related to programming languages are organized in namespaces (subdirectories), that are named according to the programming language.&lt;/p&gt; 
&lt;p&gt;For each supported programming language there are several special cheat sheets: its own sheet, &lt;code&gt;hello&lt;/code&gt;, &lt;code&gt;:list&lt;/code&gt; and &lt;code&gt;:learn&lt;/code&gt;. Say for lua it will look like:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    lua
    lua/hello
    lua/:list
    lua/:learn
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Some languages has the one-liners-cheat sheet, &lt;code&gt;1line&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    perl/1line
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;hello&lt;/code&gt; describes how you can start with the language ‚Äî install it if needed, build and run its programs, and it shows the "Hello world" program written in the language;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;:list&lt;/code&gt; shows all topics related to the language&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;:learn&lt;/code&gt; shows a learn-x-in-minutes language cheat sheet perfect for getting started with the language.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;1line&lt;/code&gt; is a collection of one-liners in this language&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;weirdness&lt;/code&gt; is a collection of examples of weird things in this language&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="http://cheat.sh/files/supported-languages-c++.png" alt="cheat.sh usage" /&gt;&lt;/p&gt; 
&lt;p&gt;At the moment, cheat.sh covers the 58 following programming languages (alphabetically sorted):&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Prefix&lt;/th&gt; 
   &lt;th&gt;Language&lt;/th&gt; 
   &lt;th&gt;Basics&lt;/th&gt; 
   &lt;th&gt;One-liners&lt;/th&gt; 
   &lt;th&gt;Weirdness&lt;/th&gt; 
   &lt;th&gt;StackOverflow&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;arduino/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Arduino&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;assembly/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Assembly&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;awk/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;AWK&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;bash/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Bash&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;basic/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;BASIC&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;bf/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Brainfuck&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;c/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;C&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;chapel/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Chapel&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;clean/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Clean&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;clojure/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Clojure&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;coffee/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;CoffeeScript&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;cpp/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;C++&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;csharp/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;C#&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;d/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;D&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;dart/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Dart&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;delphi/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Dephi&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;dylan/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Dylan&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;eiffel/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Eiffel&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;elixir/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Elixir&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;elisp/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;ELisp&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;elm/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Elm&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;erlang/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Erlang&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;factor/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Factor&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;fortran/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Fortran&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;forth/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Forth&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;fsharp/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;F#&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;go/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Go&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;groovy/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Groovy&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;haskell/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Haskell&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;java/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Java&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;js/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;JavaScript&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;julia/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Julia&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;kotlin/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Kotlin&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;latex/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;LaTeX&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;lisp/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Lisp&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;lua/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Lua&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;matlab/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;MATLAB&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;nim/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Nim&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ocaml/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;OCaml&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;octave/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Octave&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;perl/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Perl&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;perl6/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Perl 6&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;php/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;PHP&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;pike/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Pike&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;python/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Python&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;python3/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Python 3&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;r/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;R&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;racket/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Racket&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ruby/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Ruby&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;rust/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Rust&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;scala/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Scala&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;scheme/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Scheme&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;solidity/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Solidity&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;swift/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Swift&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;tcsh/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Tcsh&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;tcl/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Tcl&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;objective-c/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Objective-C&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;vb/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;VisualBasic&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;vbnet/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;VB.Net&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;And several other topics, that are though related to programming, are not programming languages:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Prefix&lt;/th&gt; 
   &lt;th&gt;Topic&lt;/th&gt; 
   &lt;th&gt;Basics&lt;/th&gt; 
   &lt;th&gt;StackOverflow&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;cmake/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;CMake&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;django/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Django&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;flask/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Flask&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;git/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Git&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
   &lt;td&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Cheat sheets sources&lt;/h2&gt; 
&lt;p&gt;Instead of creating yet another mediocre cheat sheet repository, we are concentrating our efforts on creation of a unified mechanism to access selected existing well developed and good maintained cheat sheet repositories covering topics of our interest: programming and operating systems usage.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;cheat.sh&lt;/em&gt; uses selected community driven cheat sheet repositories and information sources, maintained by thousands of users, developers and authors all over the world (in the &lt;em&gt;Users&lt;/em&gt; column number of contributors/number of stars is shown):&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Cheat sheets&lt;/th&gt; 
   &lt;th&gt;Repository&lt;/th&gt; 
   &lt;th&gt;C/U*&lt;/th&gt; 
   &lt;th&gt;Stars&lt;/th&gt; 
   &lt;th&gt;Creation Date&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;UNIX/Linux, programming&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/chubin/cheat.sheets"&gt;cheat.sheets&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/contributors-anon/chubin/cheat.sheets?label=%F0%9F%91%A5&amp;amp;labelColor=white" alt="" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/chubin/cheat.sheets?label=%E2%AD%90&amp;amp;labelColor=white" alt="" /&gt;&lt;/td&gt; 
   &lt;td&gt;May 1, 2017&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;UNIX/Linux commands&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/tldr-pages/tldr"&gt;tldr-pages/tldr&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/contributors-anon/tldr-pages/tldr?label=%F0%9F%91%A5&amp;amp;labelColor=white" alt="" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/tldr-pages/tldr?label=%E2%AD%90&amp;amp;labelColor=white" alt="" /&gt;&lt;/td&gt; 
   &lt;td&gt;Dec 8, 2013&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;UNIX/Linux commands&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/cheat/cheat"&gt;cheat/cheat&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/contributors-anon/cheat/cheat?label=%F0%9F%91%A5&amp;amp;labelColor=white" alt="" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/cheat/cheat?label=%E2%AD%90&amp;amp;labelColor=white" alt="" /&gt;&lt;/td&gt; 
   &lt;td&gt;Jul 28, 2013&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Programming languages&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/adambard/learnxinyminutes-docs"&gt;adambard/learnxinyminutes-docs&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/contributors-anon/adambard/learnxinyminutes-docs?label=%F0%9F%91%A5&amp;amp;labelColor=white" alt="" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/adambard/learnxinyminutes-docs?label=%E2%AD%90&amp;amp;labelColor=white" alt="" /&gt;&lt;/td&gt; 
   &lt;td&gt;Jun 23, 2013&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Go&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/a8m/go-lang-cheat-sheet"&gt;a8m/go-lang-cheat-sheet&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/contributors-anon/a8m/go-lang-cheat-sheet?label=%F0%9F%91%A5&amp;amp;labelColor=white" alt="" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/a8m/go-lang-cheat-sheet?label=%E2%AD%90&amp;amp;labelColor=white" alt="" /&gt;&lt;/td&gt; 
   &lt;td&gt;Feb 9, 2014&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Perl&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/pkrumins/perl1line.txt"&gt;pkrumnis/perl1line.txt&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/contributors-anon/pkrumins/perl1line.txt?label=%F0%9F%91%A5&amp;amp;labelColor=white" alt="" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/github/stars/pkrumins/perl1line.txt?label=%E2%AD%90&amp;amp;labelColor=white" alt="" /&gt;&lt;/td&gt; 
   &lt;td&gt;Nov 4, 2011&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Programming languages&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://stackoverflow.com"&gt;StackOverflow&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://stackexchange.com/leagues/1/alltime/stackoverflow"&gt;14M&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
   &lt;td&gt;Sep 15, 2008&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;sup&gt;(*) C/U ‚Äî contributors for GitHub repositories, Users for Stackoverflow&lt;/sup&gt;&lt;/p&gt; 
&lt;p&gt;Pie diagram reflecting cheat sheets sources distribution (by number of cheat sheets on cheat.sh originating from a repository):&lt;/p&gt; 
&lt;p&gt;&lt;img src="http://cheat.sh/files/stat-2017-06-05.png" alt="cheat.sh cheat sheets repositories" /&gt;&lt;/p&gt; 
&lt;h2&gt;How to contribute&lt;/h2&gt; 
&lt;h3&gt;How to edit a cheat sheet&lt;/h3&gt; 
&lt;p&gt;If you want to edit a cheat.sh cheat sheet, you should edit it in the upstream repository. You will find the name of the source repository in a browser when you open a cheat sheet. There are two github buttons at the bottom of the page: the second one is the button of the repository, which belongs the current cheat sheet.&lt;/p&gt; 
&lt;p&gt;You can edit the cheat sheet directly in your browser (you need a github account for it). There is an edit button in the top right corner. If you click on it, an editor will be open. There you will change the cheat sheet (under the hood: the upstream repository is forked, your changes are committed in the forked repository, a pull request to the upstream repository owner is sent).&lt;/p&gt; 
&lt;p&gt;&lt;img src="http://cheat.sh/files/edit-cheat-sheet.png" alt="cheat.sh cheat sheets repositories" /&gt;&lt;/p&gt; 
&lt;h3&gt;How to add a cheat sheet&lt;/h3&gt; 
&lt;p&gt;If you want to add a cheat sheet, you have one of the following ways:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Add it to one of the external cheat sheets repositories; you should decide on your own what is the best repository for your cheat sheet;&lt;/li&gt; 
 &lt;li&gt;Add it to the local cheat.sh repository (&lt;a href="https://github.com/chubin/cheat.sheets"&gt;cheat.sheets&lt;/a&gt;) on github (fork, commit, pull request);&lt;/li&gt; 
 &lt;li&gt;Post it on cheat.sh using curl or a web browser (&lt;a href="http://cheat.sh/:post"&gt;cheat.sh/:post&lt;/a&gt;).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If you want to change an existing cheat sheet, you have to find the original repository (when you open a cheat sheet in a browser, you see the repository's github button in the bottom of the cheat sheet), the cheat sheet is coming from, and change it there. After some time the changes will be synchronized on cheat.sh.&lt;/p&gt; 
&lt;h3&gt;How to add a cheat sheet repository&lt;/h3&gt; 
&lt;p&gt;If you want to add a cheat sheet repository to cheat.sh, please open an issue:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/chubin/cheat.sh/issues/new"&gt;Add a new repository&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Please specify the name of the repository, and give its short description.&lt;/p&gt; 
&lt;h2&gt;Installation and standalone usage&lt;/h2&gt; 
&lt;p&gt;You don't need to install anything, to start using &lt;em&gt;cheat.sh&lt;/em&gt;.&lt;/p&gt; 
&lt;p&gt;There are two cases, when you want to install &lt;em&gt;cheat.sh&lt;/em&gt; locally:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;You plan to use it off-line, without Internet access;&lt;/li&gt; 
 &lt;li&gt;You want to use your own cheat sheets (additionally, or as a replacement).&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Installation process in described in details here: &lt;a href="https://raw.githubusercontent.com/chubin/cheat.sh/master/doc/standalone.md"&gt;cheat.sh standalone installation&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>QuentinFuxa/WhisperLiveKit</title>
      <link>https://github.com/QuentinFuxa/WhisperLiveKit</link>
      <description>&lt;p&gt;Real-time &amp; local speech-to-text, translation, and speaker diarization. With server &amp; web UI.&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt;WhisperLiveKit&lt;/h1&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/QuentinFuxa/WhisperLiveKit/refs/heads/main/demo.png" alt="WhisperLiveKit Demo" width="730" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt;&lt;b&gt;Real-time, Fully Local Speech-to-Text with Speaker Identification&lt;/b&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://pypi.org/project/whisperlivekit/"&gt;&lt;img alt="PyPI Version" src="https://img.shields.io/pypi/v/whisperlivekit?color=g" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/whisperlivekit"&gt;&lt;img alt="PyPI Downloads" src="https://static.pepy.tech/personalized-badge/whisperlivekit?period=total&amp;amp;units=international_system&amp;amp;left_color=grey&amp;amp;right_color=brightgreen&amp;amp;left_text=installations" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/whisperlivekit/"&gt;&lt;img alt="Python Versions" src="https://img.shields.io/badge/python-3.9--3.13-dark_green" /&gt;&lt;/a&gt; &lt;a href="https://github.com/QuentinFuxa/WhisperLiveKit/raw/main/LICENSE"&gt;&lt;img alt="License" src="https://img.shields.io/badge/License-MIT/Dual Licensed-dark_green" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;Real-time speech transcription directly to your browser, with a ready-to-use backend+server and a simple frontend. ‚ú®&lt;/p&gt; 
&lt;h4&gt;Powered by Leading Research:&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ufal/SimulStreaming"&gt;SimulStreaming&lt;/a&gt; (SOTA 2025) - Ultra-low latency transcription with AlignAtt policy&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ufal/whisper_streaming"&gt;WhisperStreaming&lt;/a&gt; (SOTA 2023) - Low latency transcription with LocalAgreement policy&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/2507.18446"&gt;Streaming Sortformer&lt;/a&gt; (SOTA 2025) - Advanced real-time speaker diarization&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/juanmc2005/diart"&gt;Diart&lt;/a&gt; (SOTA 2021) - Real-time speaker diarization&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/snakers4/silero-vad"&gt;Silero VAD&lt;/a&gt; (2024) - Enterprise-grade Voice Activity Detection&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Why not just run a simple Whisper model on every audio batch?&lt;/strong&gt; Whisper is designed for complete utterances, not real-time chunks. Processing small segments loses context, cuts off words mid-syllable, and produces poor transcription. WhisperLiveKit uses state-of-the-art simultaneous speech research for intelligent buffering and incremental processing.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Architecture&lt;/h3&gt; 
&lt;img alt="Architecture" src="https://raw.githubusercontent.com/QuentinFuxa/WhisperLiveKit/refs/heads/main/architecture.png" /&gt; 
&lt;p&gt;&lt;em&gt;The backend supports multiple concurrent users. Voice Activity Detection reduces overhead when no voice is detected.&lt;/em&gt;&lt;/p&gt; 
&lt;h3&gt;Installation &amp;amp; Quick Start&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install whisperlivekit
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;FFmpeg is required&lt;/strong&gt; and must be installed before using WhisperLiveKit&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;OS&lt;/th&gt; 
    &lt;th&gt;How to install&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Ubuntu/Debian&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;sudo apt install ffmpeg&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;MacOS&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;brew install ffmpeg&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Windows&lt;/td&gt; 
    &lt;td&gt;Download .exe from &lt;a href="https://ffmpeg.org/download.html"&gt;https://ffmpeg.org/download.html&lt;/a&gt; and add to PATH&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Quick Start&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Start the transcription server:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;whisperlivekit-server --model base --language en
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Open your browser&lt;/strong&gt; and navigate to &lt;code&gt;http://localhost:8000&lt;/code&gt;. Start speaking and watch your words appear in real-time!&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;ul&gt; 
  &lt;li&gt;See &lt;a href="https://github.com/QuentinFuxa/WhisperLiveKit/raw/main/whisperlivekit/simul_whisper/whisper/tokenizer.py"&gt;tokenizer.py&lt;/a&gt; for the list of all available languages.&lt;/li&gt; 
  &lt;li&gt;For HTTPS requirements, see the &lt;strong&gt;Parameters&lt;/strong&gt; section for SSL configuration options.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Optional Dependencies&lt;/h4&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Optional&lt;/th&gt; 
   &lt;th&gt;&lt;code&gt;pip install&lt;/code&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Speaker diarization with Sortformer&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;git+https://github.com/NVIDIA/NeMo.git@main#egg=nemo_toolkit[asr]&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Speaker diarization with Diart&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;diart&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Original Whisper backend&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;whisper&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Improved timestamps backend&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;whisper-timestamped&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Apple Silicon optimization backend&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;mlx-whisper&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenAI API backend&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;openai&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;See &lt;strong&gt;Parameters &amp;amp; Configuration&lt;/strong&gt; below on how to use them.&lt;/p&gt; 
&lt;h3&gt;Usage Examples&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Command-line Interface&lt;/strong&gt;: Start the transcription server with various options:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Use better model than default (small)
whisperlivekit-server --model large-v3

# Advanced configuration with diarization and language
whisperlivekit-server --host 0.0.0.0 --port 8000 --model medium --diarization --language fr
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Python API Integration&lt;/strong&gt;: Check &lt;a href="https://github.com/QuentinFuxa/WhisperLiveKit/raw/main/whisperlivekit/basic_server.py"&gt;basic_server&lt;/a&gt; for a more complete example of how to use the functions and classes.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from whisperlivekit import TranscriptionEngine, AudioProcessor, parse_args
from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from fastapi.responses import HTMLResponse
from contextlib import asynccontextmanager
import asyncio

transcription_engine = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    global transcription_engine
    transcription_engine = TranscriptionEngine(model="medium", diarization=True, lan="en")
    yield

app = FastAPI(lifespan=lifespan)

async def handle_websocket_results(websocket: WebSocket, results_generator):
    async for response in results_generator:
        await websocket.send_json(response)
    await websocket.send_json({"type": "ready_to_stop"})

@app.websocket("/asr")
async def websocket_endpoint(websocket: WebSocket):
    global transcription_engine

    # Create a new AudioProcessor for each connection, passing the shared engine
    audio_processor = AudioProcessor(transcription_engine=transcription_engine)    
    results_generator = await audio_processor.create_tasks()
    results_task = asyncio.create_task(handle_websocket_results(websocket, results_generator))
    await websocket.accept()
    while True:
        message = await websocket.receive_bytes()
        await audio_processor.process_audio(message)        
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Frontend Implementation&lt;/strong&gt;: The package includes an HTML/JavaScript implementation &lt;a href="https://github.com/QuentinFuxa/WhisperLiveKit/raw/main/whisperlivekit/web/live_transcription.html"&gt;here&lt;/a&gt;. You can also import it using &lt;code&gt;from whisperlivekit import get_inline_ui_html&lt;/code&gt; &amp;amp; &lt;code&gt;page = get_inline_ui_html()&lt;/code&gt;&lt;/p&gt; 
&lt;h2&gt;Parameters &amp;amp; Configuration&lt;/h2&gt; 
&lt;p&gt;An important list of parameters can be changed. But what &lt;em&gt;should&lt;/em&gt; you change?&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;the &lt;code&gt;--model&lt;/code&gt; size. List and recommandations &lt;a href="https://github.com/QuentinFuxa/WhisperLiveKit/raw/main/available_models.md"&gt;here&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;the &lt;code&gt;--language&lt;/code&gt;. List &lt;a href="https://github.com/QuentinFuxa/WhisperLiveKit/raw/main/whisperlivekit/simul_whisper/whisper/tokenizer.py"&gt;here&lt;/a&gt;. If you use &lt;code&gt;auto&lt;/code&gt;, the model attempts to detect the language automatically, but it tends to bias towards English.&lt;/li&gt; 
 &lt;li&gt;the &lt;code&gt;--backend&lt;/code&gt; ? you can switch to &lt;code&gt;--backend faster-whisper&lt;/code&gt; if &lt;code&gt;simulstreaming&lt;/code&gt; does not work correctly or if you prefer to avoid the dual-license requirements.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--warmup-file&lt;/code&gt;, if you have one&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--host&lt;/code&gt;, &lt;code&gt;--port&lt;/code&gt;, &lt;code&gt;--ssl-certfile&lt;/code&gt;, &lt;code&gt;--ssl-keyfile&lt;/code&gt;, if you set up a server&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--diarization&lt;/code&gt;, if you want to use it.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The rest I don't recommend. But below are your options.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Parameter&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Default&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--model&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Whisper model size.&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;small&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--language&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Source language code or &lt;code&gt;auto&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;auto&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--task&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;transcribe&lt;/code&gt; or &lt;code&gt;translate&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;transcribe&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--backend&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Processing backend&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;simulstreaming&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--min-chunk-size&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Minimum audio chunk size (seconds)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;1.0&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--no-vac&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Disable Voice Activity Controller&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--no-vad&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Disable Voice Activity Detection&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--warmup-file&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Audio file path for model warmup&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;jfk.wav&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--host&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Server host address&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;localhost&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--port&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Server port&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;8000&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--ssl-certfile&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Path to the SSL certificate file (for HTTPS support)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--ssl-keyfile&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Path to the SSL private key file (for HTTPS support)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;WhisperStreaming backend options&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Default&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--confidence-validation&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Use confidence scores for faster validation&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--buffer_trimming&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Buffer trimming strategy (&lt;code&gt;sentence&lt;/code&gt; or &lt;code&gt;segment&lt;/code&gt;)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;segment&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;SimulStreaming backend options&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Default&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--frame-threshold&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;AlignAtt frame threshold (lower = faster, higher = more accurate)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;25&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--beams&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Number of beams for beam search (1 = greedy decoding)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;1&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--decoder&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Force decoder type (&lt;code&gt;beam&lt;/code&gt; or &lt;code&gt;greedy&lt;/code&gt;)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;auto&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--audio-max-len&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Maximum audio buffer length (seconds)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;30.0&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--audio-min-len&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Minimum audio length to process (seconds)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;0.0&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--cif-ckpt-path&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Path to CIF model for word boundary detection&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--never-fire&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Never truncate incomplete words&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--init-prompt&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Initial prompt for the model&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--static-init-prompt&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Static prompt that doesn't scroll&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--max-context-tokens&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Maximum context tokens&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--model-path&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Direct path to .pt model file. Download it if not found&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;./base.pt&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--preloaded-model-count&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Optional. Number of models to preload in memory to speed up loading (set up to the expected number of concurrent users)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;1&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Diarization options&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Default&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--diarization&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Enable speaker identification&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--diarization-backend&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;diart&lt;/code&gt; or &lt;code&gt;sortformer&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;sortformer&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--segmentation-model&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Hugging Face model ID for Diart segmentation model. &lt;a href="https://github.com/juanmc2005/diart/tree/main?tab=readme-ov-file#pre-trained-models"&gt;Available models&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;pyannote/segmentation-3.0&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--embedding-model&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Hugging Face model ID for Diart embedding model. &lt;a href="https://github.com/juanmc2005/diart/tree/main?tab=readme-ov-file#pre-trained-models"&gt;Available models&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;speechbrain/spkrec-ecapa-voxceleb&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;For diarization using Diart, you need access to pyannote.audio models:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/pyannote/segmentation"&gt;Accept user conditions&lt;/a&gt; for the &lt;code&gt;pyannote/segmentation&lt;/code&gt; model&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/pyannote/segmentation-3.0"&gt;Accept user conditions&lt;/a&gt; for the &lt;code&gt;pyannote/segmentation-3.0&lt;/code&gt; model&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/pyannote/embedding"&gt;Accept user conditions&lt;/a&gt; for the &lt;code&gt;pyannote/embedding&lt;/code&gt; model&lt;/li&gt; 
  &lt;li&gt;Login with HuggingFace: &lt;code&gt;huggingface-cli login&lt;/code&gt;&lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;üöÄ Deployment Guide&lt;/h3&gt; 
&lt;p&gt;To deploy WhisperLiveKit in production:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Server Setup&lt;/strong&gt;: Install production ASGI server &amp;amp; launch with multiple workers&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pip install uvicorn gunicorn
gunicorn -k uvicorn.workers.UvicornWorker -w 4 your_app:app
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Frontend&lt;/strong&gt;: Host your customized version of the &lt;code&gt;html&lt;/code&gt; example &amp;amp; ensure WebSocket connection points correctly&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Nginx Configuration&lt;/strong&gt; (recommended for production):&lt;/p&gt; &lt;pre&gt;&lt;code class="language-nginx"&gt;server {
   listen 80;
   server_name your-domain.com;
    location / {
        proxy_pass http://localhost:8000;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
        proxy_set_header Host $host;
}}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;HTTPS Support&lt;/strong&gt;: For secure deployments, use "wss://" instead of "ws://" in WebSocket URL&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;üêã Docker&lt;/h2&gt; 
&lt;p&gt;Deploy the application easily using Docker with GPU or CPU support.&lt;/p&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Docker installed on your system&lt;/li&gt; 
 &lt;li&gt;For GPU support: NVIDIA Docker runtime installed&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Quick Start&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;With GPU acceleration (recommended):&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker build -t wlk .
docker run --gpus all -p 8000:8000 --name wlk wlk
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;CPU only:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker build -f Dockerfile.cpu -t wlk .
docker run -p 8000:8000 --name wlk wlk
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Advanced Usage&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Custom configuration:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Example with custom model and language
docker run --gpus all -p 8000:8000 --name wlk wlk --model large-v3 --language fr
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Memory Requirements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Large models&lt;/strong&gt;: Ensure your Docker runtime has sufficient memory allocated&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Customization&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--build-arg&lt;/code&gt; Options: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;EXTRAS="whisper-timestamped"&lt;/code&gt; - Add extras to the image's installation (no spaces). Remember to set necessary container options!&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;HF_PRECACHE_DIR="./.cache/"&lt;/code&gt; - Pre-load a model cache for faster first-time start&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;HF_TKN_FILE="./token"&lt;/code&gt; - Add your Hugging Face Hub access token to download gated models&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üîÆ Use Cases&lt;/h2&gt; 
&lt;p&gt;Capture discussions in real-time for meeting transcription, help hearing-impaired users follow conversations through accessibility tools, transcribe podcasts or videos automatically for content creation, transcribe support calls with speaker identification for customer service...&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>feder-cr/Jobs_Applier_AI_Agent_AIHawk</title>
      <link>https://github.com/feder-cr/Jobs_Applier_AI_Agent_AIHawk</link>
      <description>&lt;p&gt;AIHawk aims to easy job hunt process by automating the job application process. Utilizing artificial intelligence, it enables users to apply for multiple jobs in a tailored way.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h1&gt;AIHawk: the first Jobs Applier AI Agent&lt;/h1&gt; 
 &lt;p&gt;AIHawk's core architecture remains &lt;strong&gt;open source&lt;/strong&gt;, allowing developers to inspect and extend the codebase. However, due to copyright considerations, we have removed all third‚Äëparty provider plugins from this repository.&lt;/p&gt; 
 &lt;p&gt;For a fully integrated experience, including managed provider connections: check out &lt;strong&gt;&lt;a href="https://laboro.co/"&gt;laboro.co&lt;/a&gt;&lt;/strong&gt; an AI‚Äëdriven job board where the agent &lt;strong&gt;automatically applies to jobs&lt;/strong&gt; for you.&lt;/p&gt; 
 &lt;hr /&gt; 
 &lt;p&gt;AIHawk has been featured by major media outlets for revolutionizing how job seekers interact with the job market:&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://www.businessinsider.com/aihawk-applies-jobs-for-you-linkedin-risks-inaccuracies-mistakes-2024-11"&gt;&lt;strong&gt;Business Insider&lt;/strong&gt;&lt;/a&gt; &lt;a href="https://techcrunch.com/2024/10/10/a-reporter-used-ai-to-apply-to-2843-jobs/"&gt;&lt;strong&gt;TechCrunch&lt;/strong&gt;&lt;/a&gt; &lt;a href="https://www.semafor.com/article/09/12/2024/linkedins-have-nots-and-have-bots"&gt;&lt;strong&gt;Semafor&lt;/strong&gt;&lt;/a&gt; &lt;a href="https://devby.io/news/ya-razoslal-rezume-na-2843-vakansii-po-17-v-chas-kak-ii-boty-vytesnyaut-ludei-iz-protsessa-naima.amp"&gt;&lt;strong&gt;Dev.by&lt;/strong&gt;&lt;/a&gt; &lt;a href="https://www.wired.it/article/aihawk-come-automatizzare-ricerca-lavoro/"&gt;&lt;strong&gt;Wired&lt;/strong&gt;&lt;/a&gt; &lt;a href="https://www.theverge.com/2024/10/10/24266898/ai-is-enabling-job-seekers-to-think-like-spammers"&gt;&lt;strong&gt;The Verge&lt;/strong&gt;&lt;/a&gt; &lt;a href="https://www.vanityfair.it/article/intelligenza-artificiale-candidature-di-lavoro"&gt;&lt;strong&gt;Vanity Fair&lt;/strong&gt;&lt;/a&gt; &lt;a href="https://www.404media.co/i-applied-to-2-843-roles-the-rise-of-ai-powered-job-application-bots/"&gt;&lt;strong&gt;404 Media&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>inventree/InvenTree</title>
      <link>https://github.com/inventree/InvenTree</link>
      <description>&lt;p&gt;Open Source Inventory Management System&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/inventree/InvenTree/master/assets/images/logo/inventree.png" alt="InvenTree logo" width="200" height="auto" /&gt; 
 &lt;h1&gt;InvenTree&lt;/h1&gt; 
 &lt;p&gt;Open Source Inventory Management System &lt;/p&gt; 
 &lt;!-- Badges --&gt; 
 &lt;p&gt;&lt;a href="https://opensource.org/license/MIT"&gt;&lt;img src="https://img.shields.io/badge/License-MIT-yellow.svg?sanitize=true" alt="License: MIT" /&gt;&lt;/a&gt;&lt;img src="https://img.shields.io/github/v/tag/inventree/inventree" alt="GitHub tag (latest SemVer)" /&gt; &lt;img src="https://github.com/inventree/inventree/actions/workflows/qc_checks.yaml/badge.svg?sanitize=true" alt="CI" /&gt; &lt;a href="https://inventree.readthedocs.io/en/latest/?badge=latest"&gt;&lt;img src="https://readthedocs.org/projects/inventree/badge/?version=latest" alt="Documentation Status" /&gt;&lt;/a&gt; &lt;img src="https://github.com/inventree/inventree/actions/workflows/docker.yaml/badge.svg?sanitize=true" alt="Docker Build" /&gt; &lt;a href="https://app.netlify.com/sites/inventree/deploys"&gt;&lt;img src="https://api.netlify.com/api/v1/badges/9bbb2101-0a4d-41e7-ad56-b63fb6053094/deploy-status" alt="Netlify Status" /&gt;&lt;/a&gt; &lt;a href="https://dev.azure.com/InvenTree/InvenTree%20test%20statistics/_build/latest?definitionId=3&amp;amp;branchName=testing"&gt;&lt;img src="https://dev.azure.com/InvenTree/InvenTree%20test%20statistics/_apis/build/status%2Fmatmair.InvenTree?branchName=testing" alt="Performance Testing" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://bestpractices.coreinfrastructure.org/projects/7179"&gt;&lt;img src="https://bestpractices.coreinfrastructure.org/projects/7179/badge" alt="OpenSSF Best Practices" /&gt;&lt;/a&gt; &lt;a href="https://securityscorecards.dev/viewer/?uri=github.com/inventree/InvenTree"&gt;&lt;img src="https://api.securityscorecards.dev/projects/github.com/inventree/InvenTree/badge" alt="OpenSSF Scorecard" /&gt;&lt;/a&gt; &lt;a href="https://sonarcloud.io/summary/new_code?id=inventree_InvenTree"&gt;&lt;img src="https://sonarcloud.io/api/project_badges/measure?project=inventree_InvenTree&amp;amp;metric=sqale_rating" alt="Maintainability Rating" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://codecov.io/gh/inventree/InvenTree"&gt;&lt;img src="https://codecov.io/gh/inventree/InvenTree/graph/badge.svg?token=9DZRGUUV7B" alt="codecov" /&gt;&lt;/a&gt; &lt;a href="https://crowdin.com/project/inventree"&gt;&lt;img src="https://badges.crowdin.net/inventree/localized.svg?sanitize=true" alt="Crowdin" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/github/commit-activity/m/inventree/inventree" alt="GitHub commit activity" /&gt; &lt;a href="https://hub.docker.com/r/inventree/inventree"&gt;&lt;img src="https://img.shields.io/docker/pulls/inventree/inventree" alt="Docker Pulls" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/inventree/InvenTree/"&gt;&lt;img src="https://img.shields.io/github/stars/inventree?style=social" alt="GitHub Org's stars" /&gt;&lt;/a&gt; &lt;a href="https://twitter.com/inventreedb"&gt;&lt;img src="https://img.shields.io/twitter/follow/inventreedb?style=social" alt="Twitter Follow" /&gt;&lt;/a&gt; &lt;a href="https://www.reddit.com/r/InvenTree/"&gt;&lt;img src="https://img.shields.io/reddit/subreddit-subscribers/inventree?style=social" alt="Subreddit subscribers" /&gt;&lt;/a&gt; &lt;a href="https://chaos.social/@InvenTree"&gt;&lt;img src="https://img.shields.io/badge/dynamic/json?label=Mastodon&amp;amp;query=followers_count&amp;amp;url=https%3A%2F%2Fchaos.social%2Fapi%2Fv1%2Faccounts%2Flookup%3Facct=InvenTree&amp;amp;logo=mastodon&amp;amp;style=social" alt="Mastdon" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;h4&gt; &lt;a href="https://demo.inventree.org/"&gt;View Demo&lt;/a&gt; &lt;span&gt; ¬∑ &lt;/span&gt; &lt;a href="https://docs.inventree.org/en/latest/"&gt;Documentation&lt;/a&gt; &lt;span&gt; ¬∑ &lt;/span&gt; &lt;a href="https://github.com/inventree/InvenTree/issues/new?template=bug_report.md&amp;amp;title=%5BBUG%5D"&gt;Report Bug&lt;/a&gt; &lt;span&gt; ¬∑ &lt;/span&gt; &lt;a href="https://github.com/inventree/InvenTree/issues/new?template=feature_request.md&amp;amp;title=%5BFR%5D"&gt;Request Feature&lt;/a&gt; &lt;/h4&gt; 
&lt;/div&gt; 
&lt;!-- About the Project --&gt; 
&lt;h2&gt;&lt;span&gt;üåü&lt;/span&gt; About the Project&lt;/h2&gt; 
&lt;p&gt;InvenTree is an open-source Inventory Management System which provides powerful low-level stock control and part tracking. The core of the InvenTree system is a Python/Django database backend which provides an admin interface (web-based) and a REST API for interaction with external interfaces and applications. A powerful plugin system provides support for custom applications and extensions.&lt;/p&gt; 
&lt;p&gt;Check out &lt;a href="https://inventree.org"&gt;our website&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;!-- Roadmap --&gt; 
&lt;h3&gt;&lt;span&gt;üß≠&lt;/span&gt; Roadmap&lt;/h3&gt; 
&lt;p&gt;Want to see what we are working on? Check out the &lt;a href="https://github.com/inventree/InvenTree/issues?q=is%3Aopen+is%3Aissue+label%3Aroadmap"&gt;roadmap tag&lt;/a&gt; and &lt;a href="https://github.com/inventree/InvenTree/milestone/42"&gt;horizon milestone&lt;/a&gt;.&lt;/p&gt; 
&lt;!-- Integration --&gt; 
&lt;h3&gt;&lt;span&gt;üõ†&lt;/span&gt; Integration&lt;/h3&gt; 
&lt;p&gt;InvenTree is designed to be &lt;strong&gt;extensible&lt;/strong&gt;, and provides multiple options for &lt;strong&gt;integration&lt;/strong&gt; with external applications or addition of custom plugins:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.inventree.org/en/latest/api/"&gt;InvenTree API&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.inventree.org/en/latest/api/python/"&gt;Python module&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.inventree.org/en/latest/plugins/"&gt;Plugin interface&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.inventree.org/en/latest/plugins/integrate/"&gt;Third party tools&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- TechStack --&gt; 
&lt;h3&gt;&lt;span&gt;üëæ&lt;/span&gt; Tech Stack&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;Server&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://www.python.org/"&gt;Python&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://www.djangoproject.com/"&gt;Django&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://www.django-rest-framework.org/"&gt;DRF&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://django-q.readthedocs.io/"&gt;Django Q&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://docs.allauth.org/"&gt;Django-Allauth&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Database&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://www.postgresql.org/"&gt;PostgreSQL&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://www.mysql.com/"&gt;MySQL&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://www.sqlite.org/"&gt;SQLite&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://redis.io/"&gt;Redis&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Client&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://react.dev/"&gt;React&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://lingui.dev/"&gt;Lingui&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://reactrouter.com/"&gt;React Router&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://tanstack.com/query/"&gt;TanStack Query&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/pmndrs/zustand"&gt;Zustand&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://mantine.dev/"&gt;Mantine&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://icflorescu.github.io/mantine-datatable/"&gt;Mantine Data Table&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://codemirror.net/"&gt;CodeMirror&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;DevOps&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://hub.docker.com/r/inventree/inventree"&gt;Docker&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://crowdin.com/project/inventree"&gt;Crowdin&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://app.codecov.io/gh/inventree/InvenTree"&gt;Codecov&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://sonarcloud.io/project/overview?id=inventree_InvenTree"&gt;SonarCloud&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://packager.io/gh/inventree/InvenTree"&gt;Packager.io&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;!-- Getting Started --&gt; 
&lt;h2&gt;&lt;span&gt;üß∞&lt;/span&gt; Deployment / Getting Started&lt;/h2&gt; 
&lt;p&gt;There are several options to deploy InvenTree.&lt;/p&gt; 
&lt;div align="center"&gt;
 &lt;h4&gt; &lt;a href="https://docs.inventree.org/en/latest/start/docker/"&gt;Docker&lt;/a&gt; &lt;span&gt; ¬∑ &lt;/span&gt; &lt;a href="https://inventree.org/digitalocean"&gt;&lt;img src="https://www.deploytodo.com/do-btn-blue-ghost.svg?sanitize=true" alt="Deploy to DO" width="auto" height="40" /&gt;&lt;/a&gt; &lt;span&gt; ¬∑ &lt;/span&gt; &lt;a href="https://docs.inventree.org/en/latest/start/install/"&gt;Bare Metal&lt;/a&gt; &lt;/h4&gt;
&lt;/div&gt; 
&lt;p&gt;Single line install - read &lt;a href="https://docs.inventree.org/en/latest/start/installer/"&gt;the docs&lt;/a&gt; for supported distros and details about the function:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;wget -qO install.sh https://get.inventree.org &amp;amp;&amp;amp; bash install.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Refer to the &lt;a href="https://docs.inventree.org/en/latest/start/install/"&gt;getting started guide&lt;/a&gt; for a full set of installation and setup instructions.&lt;/p&gt; 
&lt;!-- Mobile App --&gt; 
&lt;h2&gt;&lt;span&gt;üì±&lt;/span&gt; Mobile App&lt;/h2&gt; 
&lt;p&gt;InvenTree is supported by a &lt;a href="https://docs.inventree.org/app/"&gt;companion mobile app&lt;/a&gt; which allows users access to stock control information and functionality.&lt;/p&gt; 
&lt;div align="center"&gt;
 &lt;h4&gt; &lt;a href="https://play.google.com/store/apps/details?id=inventree.inventree_app"&gt;Android Play Store&lt;/a&gt; &lt;span&gt; ¬∑ &lt;/span&gt; &lt;a href="https://apps.apple.com/au/app/inventree/id1581731101#?platform=iphone"&gt;Apple App Store&lt;/a&gt; &lt;/h4&gt;
&lt;/div&gt; 
&lt;!-- Security --&gt; 
&lt;h2&gt;&lt;span&gt;üîí&lt;/span&gt; Code of Conduct &amp;amp; Security Policy&lt;/h2&gt; 
&lt;p&gt;The InvenTree project team is committed to providing a safe and welcoming environment for all users. Please read our &lt;a href="https://raw.githubusercontent.com/inventree/InvenTree/master/CODE_OF_CONDUCT.md"&gt;Code of Conduct&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;p&gt;InvenTree is following industry best practices for security. Our security policy is included &lt;a href="https://raw.githubusercontent.com/inventree/InvenTree/master/SECURITY.md"&gt;in this repo&lt;/a&gt;. We provide dedicated security pages on &lt;a href="https://docs.inventree.org/en/latest/security/"&gt;our documentation site&lt;/a&gt;.&lt;/p&gt; 
&lt;!-- Contributing --&gt; 
&lt;h2&gt;&lt;span&gt;üëã&lt;/span&gt; Contributing&lt;/h2&gt; 
&lt;p&gt;Contributions are welcomed and encouraged. Please help to make this project even better! Refer to the &lt;a href="https://docs.inventree.org/en/latest/develop/contributing/"&gt;contribution page&lt;/a&gt;.&lt;/p&gt; 
&lt;!-- Translation --&gt; 
&lt;h2&gt;&lt;span&gt;üìú&lt;/span&gt; Translation&lt;/h2&gt; 
&lt;p&gt;Native language translation of the InvenTree web application is &lt;a href="https://crowdin.com/project/inventree"&gt;community contributed via crowdin&lt;/a&gt;. &lt;strong&gt;Contributions are welcomed and encouraged&lt;/strong&gt;.&lt;/p&gt; 
&lt;!-- Sponsor --&gt; 
&lt;h2&gt;&lt;span&gt;üí∏&lt;/span&gt; Sponsor&lt;/h2&gt; 
&lt;p&gt;If you use InvenTree and find it to be useful, please consider &lt;a href="https://github.com/sponsors/inventree"&gt;sponsoring the project&lt;/a&gt;.&lt;/p&gt; 
&lt;!-- Acknowledgments --&gt; 
&lt;h2&gt;&lt;span&gt;üíé&lt;/span&gt; Acknowledgements&lt;/h2&gt; 
&lt;p&gt;We want to acknowledge &lt;a href="https://github.com/partkeepr/PartKeepr"&gt;PartKeepr&lt;/a&gt; as a valuable predecessor and inspiration. Find a full list of used third-party libraries in the license information dialog of your instance.&lt;/p&gt; 
&lt;h2&gt;&lt;span&gt;‚ù§Ô∏è&lt;/span&gt; Support&lt;/h2&gt; 
&lt;p&gt;This project is supported by the following sponsors:&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/MartinLoeper"&gt;&lt;img src="https://github.com/MartinLoeper.png" width="60px" alt="Martin L√∂per" /&gt;&lt;/a&gt; &lt;a href="https://github.com/lippoliv"&gt;&lt;img src="https://github.com/lippoliv.png" width="60px" alt="Oliver Lippert" /&gt;&lt;/a&gt; &lt;a href="https://github.com/lfg-seth"&gt;&lt;img src="https://github.com/lfg-seth.png" width="60px" alt="Seth Smith" /&gt;&lt;/a&gt; &lt;a href="https://github.com/snorkrat"&gt;&lt;img src="https://github.com/snorkrat.png" width="60px" alt="" /&gt;&lt;/a&gt; &lt;a href="https://github.com/spacequest-ltd"&gt;&lt;img src="https://github.com/spacequest-ltd.png" width="60px" alt="SpaceQuest Ltd" /&gt;&lt;/a&gt; &lt;a href="https://github.com/appwrite"&gt;&lt;img src="https://github.com/appwrite.png" width="60px" alt="Appwrite" /&gt;&lt;/a&gt; &lt;a href="https://github.com/PricelessToolkit"&gt;&lt;img src="https://github.com/PricelessToolkit.png" width="60px" alt="" /&gt;&lt;/a&gt; &lt;a href="https://github.com/cabottech"&gt;&lt;img src="https://github.com/cabottech.png" width="60px" alt="Cabot Technologies" /&gt;&lt;/a&gt; &lt;a href="https://github.com/markus-k"&gt;&lt;img src="https://github.com/markus-k.png" width="60px" alt="Markus Kasten" /&gt;&lt;/a&gt; &lt;a href="https://github.com/jefffhaynes"&gt;&lt;img src="https://github.com/jefffhaynes.png" width="60px" alt="Jeff Haynes" /&gt;&lt;/a&gt; &lt;a href="https://github.com/dnviti"&gt;&lt;img src="https://github.com/dnviti.png" width="60px" alt="Daniele Viti" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Islendur"&gt;&lt;img src="https://github.com/Islendur.png" width="60px" alt="Islendur" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Gibeon-NL"&gt;&lt;img src="https://github.com/Gibeon-NL.png" width="60px" alt="Gibeon-NL" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Motrac-Research-Engineering"&gt;&lt;img src="https://github.com/Motrac-Research-Engineering.png" width="60px" alt="Motrac Research" /&gt;&lt;/a&gt; &lt;a href="https://github.com/trytuna"&gt;&lt;img src="https://github.com/trytuna.png" width="60px" alt="Timo Scrappe" /&gt;&lt;/a&gt; &lt;a href="https://github.com/ATLAS2246"&gt;&lt;img src="https://github.com/ATLAS2246.png" width="60px" alt="ATLAS2246" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Kedarius"&gt;&lt;img src="https://github.com/Kedarius.png" width="60px" alt="Radek Hladik" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;With ongoing resources provided by:&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://depot.dev?utm_source=inventree"&gt;&lt;img src="https://depot.dev/badges/built-with-depot.svg?sanitize=true" alt="Built with Depot" /&gt;&lt;/a&gt; &lt;a href="https://inventree.org/digitalocean"&gt; &lt;img src="https://opensource.nyc3.cdn.digitaloceanspaces.com/attribution/assets/SVG/DO_Logo_horizontal_blue.svg?sanitize=true" width="201px" alt="Servers by Digital Ocean" /&gt; &lt;/a&gt; &lt;a href="https://www.netlify.com"&gt; &lt;img src="https://www.netlify.com/v3/img/components/netlify-color-bg.svg?sanitize=true" alt="Deploys by Netlify" /&gt; &lt;/a&gt; &lt;a href="https://crowdin.com"&gt; &lt;img src="https://crowdin.com/images/crowdin-logo.svg?sanitize=true" alt="Translation by Crowdin" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;/p&gt; 
&lt;!-- License --&gt; 
&lt;h2&gt;&lt;span&gt;‚ö†&lt;/span&gt; License&lt;/h2&gt; 
&lt;p&gt;Distributed under the &lt;a href="https://choosealicense.com/licenses/mit/"&gt;MIT&lt;/a&gt; License. See &lt;a href="https://github.com/inventree/InvenTree/raw/master/LICENSE"&gt;LICENSE.txt&lt;/a&gt; for more information.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>MetaCubeX/mihomo</title>
      <link>https://github.com/MetaCubeX/mihomo</link>
      <description>&lt;p&gt;A simple Python Pydantic model for Honkai: Star Rail parsed data from the Mihomo API.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;mihomo&lt;/h1&gt; 
&lt;p&gt;A simple python pydantic model (type hint and autocompletion support) for Honkai: Star Rail parsed data from the Mihomo API.&lt;/p&gt; 
&lt;p&gt;API url: &lt;a href="https://api.mihomo.me/sr_info_parsed/%7BUID%7D?lang=%7BLANG%7D"&gt;https://api.mihomo.me/sr_info_parsed/{UID}?lang={LANG}&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;pip install -U git+https://github.com/KT-Yeh/mihomo.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;h3&gt;Basic&lt;/h3&gt; 
&lt;p&gt;There are two parsed data formats:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;V1: 
  &lt;ul&gt; 
   &lt;li&gt;URL: &lt;a href="https://api.mihomo.me/sr_info_parsed/800333171?lang=en&amp;amp;version=v1"&gt;https://api.mihomo.me/sr_info_parsed/800333171?lang=en&amp;amp;version=v1&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Fetching: use &lt;code&gt;client.fetch_user_v1(800333171)&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Data model: &lt;code&gt;mihomo.models.v1.StarrailInfoParsedV1&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;All models defined in &lt;code&gt;mihomo/models/v1&lt;/code&gt; directory.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;V2: 
  &lt;ul&gt; 
   &lt;li&gt;URL: &lt;a href="https://api.mihomo.me/sr_info_parsed/800333171?lang=en"&gt;https://api.mihomo.me/sr_info_parsed/800333171?lang=en&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Fetching: use &lt;code&gt;client.fetch_user(800333171)&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Data model: &lt;code&gt;mihomo.models.StarrailInfoParsed&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;All models defined in &lt;code&gt;mihomo/models&lt;/code&gt; directory.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If you don't want to use &lt;code&gt;client.get_icon_url&lt;/code&gt; to get the image url everytime, you can use &lt;code&gt;client.fetch_user(800333171, replace_icon_name_with_url=True)&lt;/code&gt; to get the parsed data with asset urls.&lt;/p&gt; 
&lt;h3&gt;Example&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;import asyncio

from mihomo import Language, MihomoAPI
from mihomo.models import StarrailInfoParsed
from mihomo.models.v1 import StarrailInfoParsedV1

client = MihomoAPI(language=Language.EN)


async def v1():
    data: StarrailInfoParsedV1 = await client.fetch_user_v1(800333171)

    print(f"Name: {data.player.name}")
    print(f"Level: {data.player.level}")
    print(f"Signature: {data.player.signature}")
    print(f"Achievements: {data.player_details.achievements}")
    print(f"Characters count: {data.player_details.characters}")
    print(f"Profile picture url: {client.get_icon_url(data.player.icon)}")
    for character in data.characters:
        print("-----------")
        print(f"Name: {character.name}")
        print(f"Rarity: {character.rarity}")
        print(f"Level: {character.level}")
        print(f"Avatar url: {client.get_icon_url(character.icon)}")
        print(f"Preview url: {client.get_icon_url(character.preview)}")
        print(f"Portrait url: {client.get_icon_url(character.portrait)}")


async def v2():
    data: StarrailInfoParsed = await client.fetch_user(800333171, replace_icon_name_with_url=True)

    print(f"Name: {data.player.name}")
    print(f"Level: {data.player.level}")
    print(f"Signature: {data.player.signature}")
    print(f"Profile picture url: {data.player.avatar.icon}")
    for character in data.characters:
        print("-----------")
        print(f"Name: {character.name}")
        print(f"Rarity: {character.rarity}")
        print(f"Portrait url: {character.portrait}")

asyncio.run(v1())
asyncio.run(v2())
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Tools&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;from mihomo import tools&lt;/code&gt;&lt;/p&gt; 
&lt;h4&gt;Remove Duplicate Character&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;    data = await client.fetch_user(800333171)
    data = tools.remove_duplicate_character(data)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Merge Character Data&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;    old_data = await client.fetch_user(800333171)

    # Change characters in game and wait for the API to refresh
    # ...

    new_data = await client.fetch_user(800333171)
    data = tools.merge_character_data(new_data, old_data)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Data Persistence&lt;/h3&gt; 
&lt;p&gt;Take pickle and json as an example&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;import pickle
import zlib
from mihomo import MihomoAPI, Language, StarrailInfoParsed

client = MihomoAPI(language=Language.EN)
data = await client.fetch_user(800333171)

# Save
pickle_data = zlib.compress(pickle.dumps(data))
print(len(pickle_data))
json_data = data.json(by_alias=True, ensure_ascii=False)
print(len(json_data))

# Load
data_from_pickle = pickle.loads(zlib.decompress(pickle_data))
data_from_json = StarrailInfoParsed.parse_raw(json_data)
print(type(data_from_pickle))
print(type(data_from_json))
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>mindsdb/mindsdb</title>
      <link>https://github.com/mindsdb/mindsdb</link>
      <description>&lt;p&gt;AI Analytics Engine that can answer questions over large scale data. - The only MCP Server you'll ever need&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a name="readme-top"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://pypi.org/project/MindsDB/" target="_blank"&gt;&lt;img src="https://badge.fury.io/py/MindsDB.svg?sanitize=true" alt="MindsDB Release" /&gt;&lt;/a&gt; 
 &lt;a href="https://www.python.org/downloads/" target="_blank"&gt;&lt;img src="https://img.shields.io/badge/python-3.10.x%7C%203.11.x-brightgreen.svg?sanitize=true" alt="Python supported" /&gt;&lt;/a&gt; 
 &lt;a href="https://hub.docker.com/u/mindsdb" target="_blank"&gt;&lt;img src="https://img.shields.io/docker/pulls/mindsdb/mindsdb" alt="Docker pulls" /&gt;&lt;/a&gt; 
 &lt;br /&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/3068" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/3068" alt="mindsdb%2Fmindsdb | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;a href="https://github.com/mindsdb/mindsdb"&gt; &lt;img src="https://raw.githubusercontent.com/mindsdb/mindsdb/main/docs/assets/mindsdb_logo.png" alt="MindsDB" width="300" /&gt; &lt;/a&gt; 
 &lt;p align="center"&gt; &lt;br /&gt; &lt;a href="https://www.mindsdb.com?utm_medium=community&amp;amp;utm_source=github&amp;amp;utm_campaign=mindsdb%20repo"&gt;Website&lt;/a&gt; ¬∑ &lt;a href="https://docs.mindsdb.com?utm_medium=community&amp;amp;utm_source=github&amp;amp;utm_campaign=mindsdb%20repo"&gt;Docs&lt;/a&gt; ¬∑ &lt;a href="https://mindsdb.com/contact"&gt;Contact us for a Demo&lt;/a&gt; ¬∑ &lt;a href="https://mindsdb.com/joincommunity?utm_medium=community&amp;amp;utm_source=github&amp;amp;utm_campaign=mindsdb%20repo"&gt;Community Slack&lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;p&gt;MindsDB enables humans, AI, agents, and applications to get highly accurate answers across large scale data sources.&lt;/p&gt; 
&lt;a href="https://www.youtube.com/watch?v=MX3OKpnsoLM" target="_blank"&gt; &lt;img src="https://github.com/user-attachments/assets/119e7b82-f901-4214-a26f-ff7c5ad86064" alt="MindsDB Demo" /&gt; &lt;/a&gt; 
&lt;h2&gt;Install MindsDB Server&lt;/h2&gt; 
&lt;p&gt;MindsDB is an open-source server that can be deployed anywhere - from your laptop to the cloud, and everywhere in between. And yes, you can customize it to your heart's content.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.mindsdb.com/setup/self-hosted/docker-desktop"&gt;Using Docker Desktop&lt;/a&gt;. This is the fastest and recommended way to get started and have it all running.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.mindsdb.com/setup/self-hosted/docker"&gt;Using Docker&lt;/a&gt;. This is also simple, but gives you more flexibility on how to further customize your server.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://docs.mindsdb.com/mcp/overview"&gt;MindsDB has an MCP server built in&lt;/a&gt; that enables your MCP applications to connect, unify and respond to questions over large-scale federated data‚Äîspanning databases, data warehouses, and SaaS applications.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h1&gt;Core Philosophy: Connect, Unify, Respond&lt;/h1&gt; 
&lt;p&gt;MindsDB's architecture is built around three fundamental capabilities:&lt;/p&gt; 
&lt;h2&gt;&lt;a href="https://docs.mindsdb.com/integrations/data-overview"&gt;Connect&lt;/a&gt; Your Data&lt;/h2&gt; 
&lt;p&gt;You can connect to hundreds of enterprise &lt;a href="https://docs.mindsdb.com/integrations/data-overview"&gt;data sources (learn more)&lt;/a&gt;. These integrations allow MindsDB to access data wherever it resides, forming the foundation for all other capabilities.&lt;/p&gt; 
&lt;h2&gt;&lt;a href="https://docs.mindsdb.com/mindsdb_sql/overview"&gt;Unify&lt;/a&gt; Your Data&lt;/h2&gt; 
&lt;p&gt;In many situations, it‚Äôs important to be able to prepare and unify data before generating responses from it. MindsDB SQL offers knowledge bases and views that allow indexing and organizing structured and unstructured data as if it were unified in a single system.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.mindsdb.com/mindsdb_sql/knowledge-bases"&gt;&lt;strong&gt;KNOWLEDGE BASES&lt;/strong&gt;&lt;/a&gt; ‚Äì Index and organize unstructured data for efficient Q&amp;amp;A.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.mindsdb.com/mindsdb_sql/sql/create/view"&gt;&lt;strong&gt;VIEWS&lt;/strong&gt;&lt;/a&gt; ‚Äì Simplify data access by creating unified views across different sources (no-ETL).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Unification of data can be automated using JOBs&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.mindsdb.com/mindsdb_sql/sql/create/jobs"&gt;&lt;strong&gt;JOBS&lt;/strong&gt;&lt;/a&gt; ‚Äì Schedule synchronization and transformation tasks for real-time processing.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;a href="https://docs.mindsdb.com/mindsdb_sql/agents/agent"&gt;Respond&lt;/a&gt; From Your Data&lt;/h2&gt; 
&lt;p&gt;Chat with Your Data&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.mindsdb.com/mindsdb_sql/agents/agent"&gt;&lt;strong&gt;AGENTS&lt;/strong&gt;&lt;/a&gt; ‚Äì Configure built-in agents specialized in answering questions over your connected and unified data.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.mindsdb.com/mcp/overview"&gt;&lt;strong&gt;MCP&lt;/strong&gt;&lt;/a&gt; ‚Äì Connect to MindsDB through the MCP (Model Context Protocol) for seamless interaction.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ü§ù Contribute&lt;/h2&gt; 
&lt;p&gt;Interested in contributing to MindsDB? Follow our &lt;a href="https://docs.mindsdb.com/contribute/install?utm_medium=community&amp;amp;utm_source=github&amp;amp;utm_campaign=mindsdb%20repo"&gt;installation guide for development&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You can find our &lt;a href="https://docs.mindsdb.com/contribute/contribute?utm_medium=community&amp;amp;utm_source=github&amp;amp;utm_campaign=mindsdb%20repo"&gt;contribution guide here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;We welcome suggestions! Feel free to open new issues with your ideas, and we‚Äôll guide you.&lt;/p&gt; 
&lt;p&gt;This project adheres to a &lt;a href="https://github.com/mindsdb/mindsdb/raw/main/CODE_OF_CONDUCT.md"&gt;Contributor Code of Conduct&lt;/a&gt;. By participating, you agree to follow its terms.&lt;/p&gt; 
&lt;p&gt;Also, check out our &lt;a href="https://mindsdb.com/community?utm_medium=community&amp;amp;utm_source=github&amp;amp;utm_campaign=mindsdb%20repo"&gt;community rewards and programs&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;ü§ç Support&lt;/h2&gt; 
&lt;p&gt;If you find a bug, please submit an &lt;a href="https://github.com/mindsdb/mindsdb/issues/new/choose"&gt;issue on GitHub&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Here‚Äôs how you can get community support:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Ask a question in our &lt;a href="https://mindsdb.com/joincommunity?utm_medium=community&amp;amp;utm_source=github&amp;amp;utm_campaign=mindsdb%20repo"&gt;Slack Community&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Join our &lt;a href="https://github.com/mindsdb/mindsdb/discussions"&gt;GitHub Discussions&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Post on &lt;a href="https://stackoverflow.com/questions/tagged/mindsdb"&gt;Stack Overflow&lt;/a&gt; with the MindsDB tag.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For commercial support, please &lt;a href="https://mindsdb.com/contact?utm_medium=community&amp;amp;utm_source=github&amp;amp;utm_campaign=mindsdb%20repo"&gt;contact the MindsDB team&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;üíö Current Contributors&lt;/h2&gt; 
&lt;a href="https://github.com/mindsdb/mindsdb/graphs/contributors"&gt; &lt;img src="https://contributors-img.web.app/image?repo=mindsdb/mindsdb" /&gt; &lt;/a&gt; 
&lt;p&gt;Generated with &lt;a href="https://contributors-img.web.app"&gt;contributors-img&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;üîî Subscribe for Updates&lt;/h2&gt; 
&lt;p&gt;Join our &lt;a href="https://mindsdb.com/joincommunity"&gt;Slack community&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>santinic/audiblez</title>
      <link>https://github.com/santinic/audiblez</link>
      <description>&lt;p&gt;Generate audiobooks from e-books&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Audiblez: Generate audiobooks from e-books&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/santinic/audiblez/actions/workflows/pip-install.yaml"&gt;&lt;img src="https://github.com/santinic/audiblez/actions/workflows/pip-install.yaml/badge.svg?sanitize=true" alt="Installing via pip and running" /&gt;&lt;/a&gt; &lt;a href="https://github.com/santinic/audiblez/actions/workflows/git-clone-and-run.yml"&gt;&lt;img src="https://github.com/santinic/audiblez/actions/workflows/git-clone-and-run.yml/badge.svg?sanitize=true" alt="Git clone and run" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/pypi/pyversions/audiblez" alt="PyPI - Python Version" /&gt; &lt;img src="https://img.shields.io/pypi/v/audiblez" alt="PyPI - Version" /&gt;&lt;/p&gt; 
&lt;h3&gt;v4 Now with Graphical interface, CUDA support, and many languages!&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/santinic/audiblez/main/imgs/mac.png" alt="Audiblez GUI on MacOSX" /&gt;&lt;/p&gt; 
&lt;p&gt;Audiblez generates &lt;code&gt;.m4b&lt;/code&gt; audiobooks from regular &lt;code&gt;.epub&lt;/code&gt; e-books, using Kokoro's high-quality speech synthesis.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/hexgrad/Kokoro-82M"&gt;Kokoro-82M&lt;/a&gt; is a recently published text-to-speech model with just 82M params and very natural sounding output. It's released under Apache licence and it was trained on &amp;lt; 100 hours of audio. It currently supports these languages: üá∫üá∏ üá¨üáß üá™üá∏ üá´üá∑ üáÆüá≥ üáÆüáπ üáØüáµ üáßüá∑ üá®üá≥&lt;/p&gt; 
&lt;p&gt;On a Google Colab's T4 GPU via Cuda, &lt;strong&gt;it takes about 5 minutes to convert "Animal's Farm" by Orwell&lt;/strong&gt; (which is about 160,000 characters) to audiobook, at a rate of about 600 characters per second.&lt;/p&gt; 
&lt;p&gt;On my M2 MacBook Pro, on CPU, it takes about 1 hour, at a rate of about 60 characters per second.&lt;/p&gt; 
&lt;h2&gt;How to install the Command Line tool&lt;/h2&gt; 
&lt;p&gt;If you have Python 3 on your computer, you can install it with pip. You also need &lt;code&gt;espeak-ng&lt;/code&gt; and &lt;code&gt;ffmpeg&lt;/code&gt; installed on your machine:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo apt install ffmpeg espeak-ng                   # on Ubuntu/Debian üêß
pip install audiblez
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;brew install ffmpeg espeak-ng                       # on Mac üçè
pip install audiblez
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then you can convert an .epub directly with:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;audiblez book.epub -v af_sky
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;It will first create a bunch of &lt;code&gt;book_chapter_1.wav&lt;/code&gt;, &lt;code&gt;book_chapter_2.wav&lt;/code&gt;, etc. files in the same directory, and at the end it will produce a &lt;code&gt;book.m4b&lt;/code&gt; file with the whole book you can listen with VLC or any audiobook player. It will only produce the &lt;code&gt;.m4b&lt;/code&gt; file if you have &lt;code&gt;ffmpeg&lt;/code&gt; installed on your machine.&lt;/p&gt; 
&lt;h2&gt;How to run the GUI&lt;/h2&gt; 
&lt;p&gt;The GUI is a simple graphical interface to use audiblez. You need some extra dependencies to run the GUI:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;sudo apt install ffmpeg espeak-ng 
sudo apt install libgtk-3-dev        # just for Ubuntu/Debian üêß, Windows/Mac don't need this
  
pip install audiblez pillow wxpython
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then you can run the GUI with:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;audiblez-ui
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;How to run on Windows&lt;/h2&gt; 
&lt;p&gt;After many trials, on Windows we recommend to install audiblez in a Python venv:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Open a Windows terminal&lt;/li&gt; 
 &lt;li&gt;Create anew folder: &lt;code&gt;mkdir audiblez&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Enter the folder: &lt;code&gt;cd audiblez&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Create a venv: &lt;code&gt;python -m venv venv&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Activate the venv: &lt;code&gt;.\venv\Scripts\Activate.ps1&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Install the dependencies: &lt;code&gt;pip install audiblez pillow wxpython&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Now you can run &lt;code&gt;audiblez&lt;/code&gt; or &lt;code&gt;audiblez-ui&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;For Cuda support, you need to install Pytorch accordingly: &lt;a href="https://pytorch.org/get-started/locally/"&gt;https://pytorch.org/get-started/locally/&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Speed&lt;/h2&gt; 
&lt;p&gt;By default the audio is generated using a normal speed, but you can make it up to twice slower or faster by specifying a speed argument between 0.5 to 2.0:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;audiblez book.epub -v af_sky -s 1.5
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Supported Voices&lt;/h2&gt; 
&lt;p&gt;Use &lt;code&gt;-v&lt;/code&gt; option to specify the voice to use. Available voices are listed here. The first letter is the language code and the second is the gender of the speaker e.g. &lt;code&gt;im_nicola&lt;/code&gt; is an italian male voice.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://claudio.uk/posts/audiblez-v4.html"&gt;For hearing samples of Kokoro-82M voices, go here&lt;/a&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Language&lt;/th&gt; 
   &lt;th&gt;Voices&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üá∫üá∏ American English&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;af_alloy&lt;/code&gt;, &lt;code&gt;af_aoede&lt;/code&gt;, &lt;code&gt;af_bella&lt;/code&gt;, &lt;code&gt;af_heart&lt;/code&gt;, &lt;code&gt;af_jessica&lt;/code&gt;, &lt;code&gt;af_kore&lt;/code&gt;, &lt;code&gt;af_nicole&lt;/code&gt;, &lt;code&gt;af_nova&lt;/code&gt;, &lt;code&gt;af_river&lt;/code&gt;, &lt;code&gt;af_sarah&lt;/code&gt;, &lt;code&gt;af_sky&lt;/code&gt;, &lt;code&gt;am_adam&lt;/code&gt;, &lt;code&gt;am_echo&lt;/code&gt;, &lt;code&gt;am_eric&lt;/code&gt;, &lt;code&gt;am_fenrir&lt;/code&gt;, &lt;code&gt;am_liam&lt;/code&gt;, &lt;code&gt;am_michael&lt;/code&gt;, &lt;code&gt;am_onyx&lt;/code&gt;, &lt;code&gt;am_puck&lt;/code&gt;, &lt;code&gt;am_santa&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üá¨üáß British English&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;bf_alice&lt;/code&gt;, &lt;code&gt;bf_emma&lt;/code&gt;, &lt;code&gt;bf_isabella&lt;/code&gt;, &lt;code&gt;bf_lily&lt;/code&gt;, &lt;code&gt;bm_daniel&lt;/code&gt;, &lt;code&gt;bm_fable&lt;/code&gt;, &lt;code&gt;bm_george&lt;/code&gt;, &lt;code&gt;bm_lewis&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üá™üá∏ Spanish&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ef_dora&lt;/code&gt;, &lt;code&gt;em_alex&lt;/code&gt;, &lt;code&gt;em_santa&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üá´üá∑ French&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ff_siwis&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üáÆüá≥ Hindi&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;hf_alpha&lt;/code&gt;, &lt;code&gt;hf_beta&lt;/code&gt;, &lt;code&gt;hm_omega&lt;/code&gt;, &lt;code&gt;hm_psi&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üáÆüáπ Italian&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;if_sara&lt;/code&gt;, &lt;code&gt;im_nicola&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üáØüáµ Japanese&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;jf_alpha&lt;/code&gt;, &lt;code&gt;jf_gongitsune&lt;/code&gt;, &lt;code&gt;jf_nezumi&lt;/code&gt;, &lt;code&gt;jf_tebukuro&lt;/code&gt;, &lt;code&gt;jm_kumo&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üáßüá∑ Brazilian Portuguese&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;pf_dora&lt;/code&gt;, &lt;code&gt;pm_alex&lt;/code&gt;, &lt;code&gt;pm_santa&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üá®üá≥ Mandarin Chinese&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;zf_xiaobei&lt;/code&gt;, &lt;code&gt;zf_xiaoni&lt;/code&gt;, &lt;code&gt;zf_xiaoxiao&lt;/code&gt;, &lt;code&gt;zf_xiaoyi&lt;/code&gt;, &lt;code&gt;zm_yunjian&lt;/code&gt;, &lt;code&gt;zm_yunxi&lt;/code&gt;, &lt;code&gt;zm_yunxia&lt;/code&gt;, &lt;code&gt;zm_yunyang&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;For more detaila about voice quality, check this document: &lt;a href="https://huggingface.co/hexgrad/Kokoro-82M/blob/main/VOICES.md"&gt;Kokoro-82M voices&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;How to run on GPU&lt;/h2&gt; 
&lt;p&gt;By default, audiblez runs on CPU. If you pass the option &lt;code&gt;--cuda&lt;/code&gt; it will try to use the Cuda device via Torch.&lt;/p&gt; 
&lt;p&gt;Check out this example: &lt;a href="https://colab.research.google.com/drive/164PQLowogprWQpRjKk33e-8IORAvqXKI?usp=sharing%5D"&gt;Audiblez running on a Google Colab Notebook with Cuda &lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;We don't currently support Apple Silicon, as there is not yet a Kokoro implementation in MLX. As soon as it will be available, we will support it.&lt;/p&gt; 
&lt;h2&gt;Manually pick chapters to convert&lt;/h2&gt; 
&lt;p&gt;Sometimes you want to manually select which chapters/sections in the e-book to read out loud. To do so, you can use &lt;code&gt;--pick&lt;/code&gt; to interactively choose the chapters to convert (without running the GUI).&lt;/p&gt; 
&lt;h2&gt;Help page&lt;/h2&gt; 
&lt;p&gt;For all the options available, you can check the help page &lt;code&gt;audiblez --help&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;usage: audiblez [-h] [-v VOICE] [-p] [-s SPEED] [-c] [-o FOLDER] epub_file_path

positional arguments:
  epub_file_path        Path to the epub file

options:
  -h, --help            show this help message and exit
  -v VOICE, --voice VOICE
                        Choose narrating voice: a, b, e, f, h, i, j, p, z
  -p, --pick            Interactively select which chapters to read in the audiobook
  -s SPEED, --speed SPEED
                        Set speed from 0.5 to 2.0
  -c, --cuda            Use GPU via Cuda in Torch if available
  -o FOLDER, --output FOLDER
                        Output folder for the audiobook and temporary files

example:
  audiblez book.epub -l en-us -v af_sky

to use the GUI, run:
  audiblez-ui
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Author&lt;/h2&gt; 
&lt;p&gt;by &lt;a href="https://claudio.uk"&gt;Claudio Santini&lt;/a&gt; in 2025, distributed under MIT licence.&lt;/p&gt; 
&lt;p&gt;Related Article: &lt;a href="https://claudio.uk/posts/audiblez-v4.html"&gt;Audiblez v4: Generate Audiobooks from E-books&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>sansan0/TrendRadar</title>
      <link>https://github.com/sansan0/TrendRadar</link>
      <description>&lt;p&gt;üéØ ÂëäÂà´‰ø°ÊÅØËøáËΩΩÔºåÂè™ÁúãÁúüÊ≠£ÂÖ≥ÂøÉÁöÑÊñ∞Èóª - Â§öÂπ≥Âè∞ÁÉ≠ÁÇπËÅöÂêàÂ∑•ÂÖ∑Ôºå‰∏ÄÈîÆÁõëÊéß‰ªäÊó•Â§¥Êù°„ÄÅÁôæÂ∫¶ÁÉ≠Êêú„ÄÅÂæÆÂçö„ÄÅÊäñÈü≥„ÄÅÁü•‰πé„ÄÅBÁ´ôÁ≠â35‰∏™Âπ≥Âè∞ÔºåÊô∫ËÉΩÂÖ≥ÈîÆËØçÁ≠õÈÄâÔºåËá™Âä®ÁîüÊàêÁÉ≠ÁÇπÂàÜÊûêÊä•Âëä„ÄÇÊîØÊåÅ‰ºÅ‰∏öÂæÆ‰ø°„ÄÅÈ£û‰π¶„ÄÅÈíâÈíâ„ÄÅTelegramÊé®ÈÄÅÔºå30ÁßíÁΩëÈ°µÈÉ®ÁΩ≤Ôºå1ÂàÜÈíüÊâãÊú∫ÈÄöÁü•ÔºåÊó†ÈúÄÁºñÁ®ãÂü∫Á°Ä„ÄÇ‰πüÊîØÊåÅdockerÁßÅ‰∫∫ÈÉ®ÁΩ≤‚≠ê ËÆ©ÁÆóÊ≥ï‰∏∫‰Ω†ÊúçÂä°ÔºåËÄåÈùûË¢´ÁÆóÊ≥ïÁªëÊû∂&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h1&gt;üéØTrendRadar&lt;/h1&gt; 
 &lt;p&gt;üöÄ ÊúÄÂø´&lt;strong&gt;30Áßí&lt;/strong&gt;ÈÉ®ÁΩ≤ÁöÑÁÉ≠ÁÇπÂä©Êâã ‚Äî‚Äî ÂëäÂà´Êó†ÊïàÂà∑Â±èÔºåÂè™ÁúãÁúüÊ≠£ÂÖ≥ÂøÉÁöÑÊñ∞ÈóªËµÑËÆØ&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/sansan0/TrendRadar/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/sansan0/TrendRadar?style=flat-square&amp;amp;logo=github&amp;amp;color=yellow" alt="GitHub Stars" /&gt;&lt;/a&gt; &lt;a href="https://github.com/sansan0/TrendRadar/network/members"&gt;&lt;img src="https://img.shields.io/github/forks/sansan0/TrendRadar?style=flat-square&amp;amp;logo=github&amp;amp;color=blue" alt="GitHub Forks" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/license-GPL--3.0-blue.svg?style=flat-square" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://github.com/sansan0/TrendRadar"&gt;&lt;img src="https://img.shields.io/badge/version-v2.1.0-green.svg?style=flat-square" alt="Version" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://work.weixin.qq.com/"&gt;&lt;img src="https://img.shields.io/badge/%E4%BC%81%E4%B8%9A%E5%BE%AE%E4%BF%A1-%E9%80%9A%E7%9F%A5%E6%94%AF%E6%8C%81-00D4AA?style=flat-square" alt="‰ºÅ‰∏öÂæÆ‰ø°ÈÄöÁü•" /&gt;&lt;/a&gt; &lt;a href="https://telegram.org/"&gt;&lt;img src="https://img.shields.io/badge/Telegram-%E9%80%9A%E7%9F%A5%E6%94%AF%E6%8C%81-00D4AA?style=flat-square" alt="TelegramÈÄöÁü•" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#"&gt;&lt;img src="https://img.shields.io/badge/%E9%92%89%E9%92%89-%E9%80%9A%E7%9F%A5%E6%94%AF%E6%8C%81-00D4AA?style=flat-square" alt="dingtalkÈÄöÁü•" /&gt;&lt;/a&gt; &lt;a href="https://www.feishu.cn/"&gt;&lt;img src="https://img.shields.io/badge/%E9%A3%9E%E4%B9%A6-%E9%80%9A%E7%9F%A5%E6%94%AF%E6%8C%81-00D4AA?style=flat-square" alt="È£û‰π¶ÈÄöÁü•" /&gt;&lt;/a&gt; &lt;a href="https://github.com/sansan0/TrendRadar"&gt;&lt;img src="https://img.shields.io/badge/GitHub_Actions-%E8%87%AA%E5%8A%A8%E5%8C%96-2088FF?style=flat-square&amp;amp;logo=github-actions&amp;amp;logoColor=white" alt="GitHub Actions" /&gt;&lt;/a&gt; &lt;a href="https://sansan0.github.io/TrendRadar"&gt;&lt;img src="https://img.shields.io/badge/GitHub_Pages-%E9%83%A8%E7%BD%B2-4285F4?style=flat-square&amp;amp;logo=github&amp;amp;logoColor=white" alt="GitHub Pages" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/"&gt;&lt;img src="https://img.shields.io/badge/Docker-%E9%83%A8%E7%BD%B2-2496ED?style=flat-square&amp;amp;logo=docker&amp;amp;logoColor=white" alt="Docker" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Êú¨È°πÁõÆ‰ª•ËΩªÈáèÔºåÊòìÈÉ®ÁΩ≤‰∏∫ÁõÆÊ†áÔºå‰∏ªË¶ÅÂ§ÑÁêÜ issues&lt;/p&gt; 
 &lt;p&gt;ÈÅáÂà∞ÈóÆÈ¢òÊèê issuesÔºåÊàñ„ÄêÁ°ÖÂü∫Ëå∂Ê∞¥Èó¥„ÄëÂÖ¨‰ºóÂè∑ÁïôË®Ä&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;details&gt; 
 &lt;summary&gt;üëâ ÁÇπÂáªÊü•ÁúãËá¥Ë∞¢ÂêçÂçï (ÂΩìÂâç &lt;strong&gt;13&lt;/strong&gt; ‰∏™)&lt;/summary&gt; 
 &lt;h3&gt;Êï∞ÊçÆÊîØÊåÅ&lt;/h3&gt; 
 &lt;p&gt;Êú¨È°πÁõÆ‰ΩøÁî®‰∫Ü &lt;a href="https://github.com/ourongxing/newsnow"&gt;newsnow&lt;/a&gt; È°πÁõÆÊèê‰æõÁöÑ API Êé•Âè£Ëé∑ÂèñÂ§öÂπ≥Âè∞Êï∞ÊçÆ&lt;/p&gt; 
 &lt;h3&gt;Êé®ÂπøÂä©Âäõ&lt;/h3&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;ÊÑüË∞¢‰ª•‰∏ãÂπ≥Âè∞Âíå‰∏™‰∫∫ÁöÑÊé®Ëçê(ÊåâÊó∂Èó¥ÊéíÂàó)Ôºå‰ª•ÂèäÂêÑÂæÆ‰ø°Áæ§ÔºåqqÁæ§Á≠âÁªôÂà∞Ëøô‰∏™È°πÁõÆÂ∏ÆÂä©ÁöÑ‰∫∫&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://mp.weixin.qq.com/s/fvutkJ_NPUelSW9OGK39aA"&gt;Â∞è‰ºóËΩØ‰ª∂&lt;/a&gt; - ÂºÄÊ∫êËΩØ‰ª∂Êé®ËçêÂπ≥Âè∞&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://linux.do/"&gt;LinuxDo Á§æÂå∫&lt;/a&gt; - ÊäÄÊúØÁà±Â•ΩËÄÖÁöÑËÅöÈõÜÂú∞&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/ruanyf/weekly"&gt;ÈòÆ‰∏ÄÂ≥∞Âë®Âàä&lt;/a&gt; - ÊäÄÊúØÂúàÊúâÂΩ±ÂìçÂäõÁöÑÂë®Âàä&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;ËßÇ‰ºóÊîØÊåÅ&lt;/h3&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;ÊÑüË∞¢‰ª•‰∏ãÁÉ≠ÂøÉËßÇ‰ºóÁöÑ‰ø°‰ªª‰∏éÊîØÊåÅ&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;ÁÇπËµû‰∫∫&lt;/th&gt; 
    &lt;th align="center"&gt;ÈáëÈ¢ù&lt;/th&gt; 
    &lt;th align="center"&gt;Êó•Êúü&lt;/th&gt; 
    &lt;th align="center"&gt;Â§áÊ≥®&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*‰∏ã&lt;/td&gt; 
    &lt;td align="center"&gt;1&lt;/td&gt; 
    &lt;td align="center"&gt;2025.8.30&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;2*D&lt;/td&gt; 
    &lt;td align="center"&gt;88&lt;/td&gt; 
    &lt;td align="center"&gt;2025.8.13 ‰∏ãÂçà&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;2*D&lt;/td&gt; 
    &lt;td align="center"&gt;1&lt;/td&gt; 
    &lt;td align="center"&gt;2025.8.13 ‰∏äÂçà&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;S*o&lt;/td&gt; 
    &lt;td align="center"&gt;1&lt;/td&gt; 
    &lt;td align="center"&gt;2025.8.05&lt;/td&gt; 
    &lt;td align="center"&gt;ÊîØÊåÅ‰∏Ä‰∏ã&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*‰æ†&lt;/td&gt; 
    &lt;td align="center"&gt;10&lt;/td&gt; 
    &lt;td align="center"&gt;2025.8.04&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;x*x&lt;/td&gt; 
    &lt;td align="center"&gt;2&lt;/td&gt; 
    &lt;td align="center"&gt;2025.8.03&lt;/td&gt; 
    &lt;td align="center"&gt;trendRadar Â•ΩÈ°πÁõÆ ÁÇπËµû&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*Ëøú&lt;/td&gt; 
    &lt;td align="center"&gt;1&lt;/td&gt; 
    &lt;td align="center"&gt;2025.8.01&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*ÈÇ™&lt;/td&gt; 
    &lt;td align="center"&gt;5&lt;/td&gt; 
    &lt;td align="center"&gt;2025.8.01&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;*Ê¢¶&lt;/td&gt; 
    &lt;td align="center"&gt;0.1&lt;/td&gt; 
    &lt;td align="center"&gt;2025.7.30&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;**Èæô&lt;/td&gt; 
    &lt;td align="center"&gt;10&lt;/td&gt; 
    &lt;td align="center"&gt;2025.7.29&lt;/td&gt; 
    &lt;td align="center"&gt;ÊîØÊåÅ‰∏Ä‰∏ã&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;details&gt; 
  &lt;summary&gt;&lt;strong&gt;üëâ "ÊâãÊú∫Êé®ÈÄÅÈÄöÁü•Á≥ªÂàó" ÊåñÂùë(ÊöÇÊó∂È∏Ω)&lt;/strong&gt;&lt;/summary&gt; 
  &lt;br /&gt; 
  &lt;p&gt;Êà™Âõæ‰∏≠Âè™ÊîØÊåÅ‰∏Ä‰∏™Ê∏†ÈÅìÔºåÂ§ßÂÆ∂Êúâ‰ªÄ‰πàÂ•ΩÁöÑÂª∫ËÆÆÂíåÊÉ≥Ê≥ïÂèØ‰ª•ÂÖ¨‰ºóÂè∑ÁïôË®ÄÔºåÂÆåÂñÑÂ•ΩÂêéÂºÄÊ∫ê Ëøô‰∏™ÊöÇÊó∂Ê≤°Êúâ‰∫∫Êù•ÂíåÊàëËÆ®ËÆ∫ÔºåÊàëÂÖàÈ∏Ω‰∏∫Êï¨ÂòøÂòø&lt;/p&gt; 
  &lt;img src="https://raw.githubusercontent.com/sansan0/TrendRadar/master/_image/next.jpg" width="300" title="github" /&gt; 
 &lt;/details&gt; 
&lt;/details&gt; 
&lt;h2&gt;‚ú® Ê†∏ÂøÉÂäüËÉΩ&lt;/h2&gt; 
&lt;h3&gt;&lt;strong&gt;ÂÖ®ÁΩëÁÉ≠ÁÇπËÅöÂêà&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;‰ªäÊó•Â§¥Êù°&lt;/li&gt; 
 &lt;li&gt;ÁôæÂ∫¶ÁÉ≠Êêú&lt;/li&gt; 
 &lt;li&gt;ÂçéÂ∞îË°óËßÅÈóª&lt;/li&gt; 
 &lt;li&gt;ÊæéÊπÉÊñ∞Èóª&lt;/li&gt; 
 &lt;li&gt;bilibili ÁÉ≠Êêú&lt;/li&gt; 
 &lt;li&gt;Ë¥¢ËÅîÁ§æÁÉ≠Èó®&lt;/li&gt; 
 &lt;li&gt;Âá§Âá∞ÁΩë&lt;/li&gt; 
 &lt;li&gt;Ë¥¥Âêß&lt;/li&gt; 
 &lt;li&gt;ÂæÆÂçö&lt;/li&gt; 
 &lt;li&gt;ÊäñÈü≥&lt;/li&gt; 
 &lt;li&gt;Áü•‰πé&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;ÈªòËÆ§ÁõëÊéß 11 ‰∏™‰∏ªÊµÅÂπ≥Âè∞ÔºåÂ¶ÇÊÉ≥È¢ùÂ§ñÂ¢ûÂä†ÔºåÂèØÁúãÊúÄ‰∏ãÊñπÁöÑ&lt;strong&gt;Ëá™ÂÆö‰πâÁõëÊéßÂπ≥Âè∞&lt;/strong&gt;&lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;Êô∫ËÉΩÊé®ÈÄÅÁ≠ñÁï•&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;‰∏âÁßçÊé®ÈÄÅÊ®°Âºè&lt;/strong&gt;Ôºö&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;üìà ÊäïËµÑËÄÖ/‰∫§ÊòìÂëò&lt;/strong&gt; ‚Üí ÈÄâÊã© &lt;code&gt;incremental&lt;/code&gt;ÔºåÂèäÊó∂Ëé∑ÂèñÊñ∞Â¢ûËµÑËÆØ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üì∞ Ëá™Â™í‰Ωì‰∫∫/ÂÜÖÂÆπÂàõ‰ΩúËÄÖ&lt;/strong&gt; ‚Üí ÈÄâÊã© &lt;code&gt;current&lt;/code&gt;ÔºåÊéåÊè°ÂÆûÊó∂ÁÉ≠ÁÇπË∂ãÂäø&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üìã ‰ºÅ‰∏öÁÆ°ÁêÜËÄÖ/ÊôÆÈÄöÁî®Êà∑&lt;/strong&gt; ‚Üí ÈÄâÊã© &lt;code&gt;daily&lt;/code&gt;ÔºåÂÆöÊó∂Ëé∑ÂèñÂÆåÊï¥Êó•Êä•&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;ÈùôÈªòÊé®ÈÄÅÊ®°Âºè&lt;/strong&gt;Ôºö&lt;/p&gt; 
&lt;p&gt;ÊîØÊåÅÊó∂Èó¥Á™óÂè£ÊéßÂà∂ÔºåÈÅøÂÖçÈùûÂ∑•‰ΩúÊó∂Èó¥ÁöÑÊ∂àÊÅØÊâìÊâ∞Ôºö&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Êó∂Èó¥ËåÉÂõ¥ÊéßÂà∂&lt;/strong&gt;ÔºöËÆæÂÆöÊé®ÈÄÅÊó∂Èó¥Á™óÂè£ÔºàÂ¶Ç 9:00-18:00ÔºâÔºå‰ªÖÂú®ÊåáÂÆöÊó∂Èó¥ÂÜÖÊé®ÈÄÅ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ÈÄÇÁî®Âú∫ÊôØ&lt;/strong&gt;Ôºö 
  &lt;ul&gt; 
   &lt;li&gt;Êó∂Èó¥ÂÜÖÊØèÊ¨°ÊâßË°åÈÉΩÊé®ÈÄÅ&lt;/li&gt; 
   &lt;li&gt;Êó∂Èó¥ËåÉÂõ¥ÂÜÖÂè™Êé®ÈÄÅ‰∏ÄÊ¨°&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;strong&gt;Á≤æÂáÜÂÜÖÂÆπÁ≠õÈÄâ&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;ËÆæÁΩÆ‰∏™‰∫∫ÂÖ≥ÈîÆËØçÔºàÂ¶ÇÔºöAI„ÄÅÊØî‰∫öËø™„ÄÅÊïôËÇ≤ÊîøÁ≠ñÔºâÔºåÂè™Êé®ÈÄÅÁõ∏ÂÖ≥ÁÉ≠ÁÇπÔºåËøáÊª§Êó†ÂÖ≥‰ø°ÊÅØ&lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;Â§öÊ∏†ÈÅìÂÆûÊó∂Êé®ÈÄÅ&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;ÊîØÊåÅ&lt;strong&gt;‰ºÅ‰∏öÂæÆ‰ø°&lt;/strong&gt;„ÄÅ&lt;strong&gt;È£û‰π¶&lt;/strong&gt;„ÄÅ&lt;strong&gt;ÈíâÈíâ&lt;/strong&gt;„ÄÅ&lt;strong&gt;Telegram&lt;/strong&gt;ÔºåÊ∂àÊÅØÁõ¥ËææÊâãÊú∫&lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;Èõ∂ÊäÄÊúØÈó®ÊßõÈÉ®ÁΩ≤&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;GitHub ‰∏ÄÈîÆ Fork Âç≥ÂèØ‰ΩøÁî®ÔºåÊó†ÈúÄÁºñÁ®ãÂü∫Á°Ä„ÄÇ&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;30ÁßíÈÉ®ÁΩ≤Ôºö GitHub PagesÔºàÁΩëÈ°µÊµèËßàÔºâ&lt;/p&gt; 
 &lt;p&gt;1ÂàÜÈíüÈÉ®ÁΩ≤Ôºö ‰ºÅ‰∏öÂæÆ‰ø°ÔºàÊâãÊú∫ÈÄöÁü•Ôºâ&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;üí° ÊèêÁ§∫Ôºö&lt;/strong&gt; ÊÉ≥Ë¶Å&lt;strong&gt;ÂÆûÊó∂Êõ¥Êñ∞&lt;/strong&gt;ÁöÑÁΩëÈ°µÁâàÔºüfork ÂêéÔºåËøõÂÖ•‰Ω†ÁöÑ‰ªìÂ∫ì Settings ‚Üí PagesÔºåÂêØÁî® GitHub Pages„ÄÇ&lt;a href="https://sansan0.github.io/TrendRadar/"&gt;ÊïàÊûúÈ¢ÑËßà&lt;/a&gt;„ÄÇ&lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;ÂáèÂ∞ë APP ‰æùËµñ&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;‰ªé"Ë¢´ÁÆóÊ≥ïÊé®ËçêÁªëÊû∂"ÂèòÊàê"‰∏ªÂä®Ëé∑ÂèñËá™Â∑±ÊÉ≥Ë¶ÅÁöÑ‰ø°ÊÅØ"&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;ÈÄÇÂêà‰∫∫Áæ§Ôºö&lt;/strong&gt; ÊäïËµÑËÄÖ„ÄÅËá™Â™í‰Ωì‰∫∫„ÄÅ‰ºÅ‰∏öÂÖ¨ÂÖ≥„ÄÅÂÖ≥ÂøÉÊó∂‰∫ãÁöÑÊôÆÈÄöÁî®Êà∑&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;ÂÖ∏ÂûãÂú∫ÊôØÔºö&lt;/strong&gt; ËÇ°Â∏ÇÊäïËµÑÁõëÊéß„ÄÅÂìÅÁâåËàÜÊÉÖËøΩË∏™„ÄÅË°å‰∏öÂä®ÊÄÅÂÖ≥Ê≥®„ÄÅÁîüÊ¥ªËµÑËÆØËé∑Âèñ&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;Github PagesÊïàÊûú&lt;/th&gt; 
   &lt;th align="center"&gt;È£û‰π¶Êé®ÈÄÅÊïàÊûú&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/sansan0/TrendRadar/master/_image/github-pages.png" alt="Github PagesÊïàÊûú" /&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/sansan0/TrendRadar/master/_image/feishu.jpg" alt="È£û‰π¶Êé®ÈÄÅÊïàÊûú" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üëâ Êé®ÈÄÅÊ†ºÂºèËØ¥Êòé&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h2&gt;üìä ËæìÂá∫Á§∫‰æã&lt;/h2&gt; 
 &lt;h3&gt;ÈÄöÁü•Á§∫‰æãÔºö&lt;/h3&gt; 
 &lt;pre&gt;&lt;code&gt;üìä ÁÉ≠ÁÇπËØçÊ±áÁªüËÆ°

üî• ‰∫∫Â∑•Êô∫ËÉΩ AI : 12 Êù°

  1. [ÁôæÂ∫¶ÁÉ≠Êêú] ÁßëÊäÄÂ∑®Â§¥ÂèëÂ∏ÉÊñ∞AIÊ®°Âûã [1] - 12Êó∂30ÂàÜ (4Ê¨°)

  2. [‰ªäÊó•Â§¥Êù°] AIÊäÄÊúØÊúÄÊñ∞Á™ÅÁ†¥ [2] - [13Êó∂15ÂàÜ ~ 14Êó∂30ÂàÜ] (2Ê¨°)

&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;Ê∂àÊÅØÊ†ºÂºèËØ¥Êòé&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Ê†ºÂºèÂÖÉÁ¥†&lt;/th&gt; 
    &lt;th&gt;Á§∫‰æã&lt;/th&gt; 
    &lt;th&gt;Âê´‰πâ&lt;/th&gt; 
    &lt;th&gt;ËØ¥Êòé&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;ÂÖ≥ÈîÆËØç&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;‰∫∫Â∑•Êô∫ËÉΩ AI&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;È¢ëÁéáËØçÁªÑ&lt;/td&gt; 
    &lt;td&gt;Ë°®Á§∫Êú¨ÁªÑÂåπÈÖçÁöÑÂÖ≥ÈîÆËØç&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;: N Êù°&lt;/td&gt; 
    &lt;td&gt;: 12 Êù°&lt;/td&gt; 
    &lt;td&gt;ÂåπÈÖçÊï∞Èáè&lt;/td&gt; 
    &lt;td&gt;ËØ•ÂÖ≥ÈîÆËØçÁªÑÂåπÈÖçÁöÑÊ†áÈ¢òÊÄªÊï∞&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;[Âπ≥Âè∞Âêç]&lt;/td&gt; 
    &lt;td&gt;[ÁôæÂ∫¶ÁÉ≠Êêú]&lt;/td&gt; 
    &lt;td&gt;Êù•Ê∫êÂπ≥Âè∞&lt;/td&gt; 
    &lt;td&gt;Ê†áÈ¢òÊâÄÂ±ûÁöÑÂπ≥Âè∞ÂêçÁß∞&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;[&lt;strong&gt;Êï∞Â≠ó&lt;/strong&gt;]&lt;/td&gt; 
    &lt;td&gt;[&lt;strong&gt;1&lt;/strong&gt;]&lt;/td&gt; 
    &lt;td&gt;È´òÊéíÂêçÊ†áËÆ∞&lt;/td&gt; 
    &lt;td&gt;ÊéíÂêç ‚â§ ÈòàÂÄº(ÈªòËÆ§ 5)ÁöÑÁÉ≠ÊêúÔºåÁ∫¢Ëâ≤Âä†Á≤óÊòæÁ§∫&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;[Êï∞Â≠ó]&lt;/td&gt; 
    &lt;td&gt;[7]&lt;/td&gt; 
    &lt;td&gt;ÊôÆÈÄöÊéíÂêçÊ†áËÆ∞&lt;/td&gt; 
    &lt;td&gt;ÊéíÂêç&amp;gt;ÈòàÂÄºÁöÑÁÉ≠ÊêúÔºåÊôÆÈÄöÊòæÁ§∫&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;- Êó∂Èó¥&lt;/td&gt; 
    &lt;td&gt;- 12 Êó∂ 30 ÂàÜ&lt;/td&gt; 
    &lt;td&gt;È¶ñÊ¨°ÂèëÁé∞Êó∂Èó¥&lt;/td&gt; 
    &lt;td&gt;Ê†áÈ¢òÈ¶ñÊ¨°Ë¢´ÂèëÁé∞ÁöÑÊó∂Èó¥&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;[Êó∂Èó¥ ~ Êó∂Èó¥]&lt;/td&gt; 
    &lt;td&gt;[12 Êó∂ 30 ÂàÜ ~ 14 Êó∂ 00 ÂàÜ]&lt;/td&gt; 
    &lt;td&gt;Êó∂Èó¥ËåÉÂõ¥&lt;/td&gt; 
    &lt;td&gt;Ê†áÈ¢òÂá∫Áé∞ÁöÑÊó∂Èó¥ËåÉÂõ¥(È¶ñÊ¨°~ÊúÄÂêé)&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;(N Ê¨°)&lt;/td&gt; 
    &lt;td&gt;(4 Ê¨°)&lt;/td&gt; 
    &lt;td&gt;Âá∫Áé∞Ê¨°Êï∞&lt;/td&gt; 
    &lt;td&gt;Ê†áÈ¢òÂú®ÁõëÊéßÊúüÈó¥Âá∫Áé∞ÁöÑÊÄªÊ¨°Êï∞&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h2&gt;üìù Êõ¥Êñ∞Êó•Âøó&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üëâ È°πÁõÆÁõ∏ÂÖ≥Êé®Ëçê&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;ÈôÑÈ°πÁõÆÁõ∏ÂÖ≥ÁöÑ‰∏§ÁØáÊñáÁ´†ÔºåÊ¨¢ËøéÁïôË®Ä‰∫§ÊµÅ&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://mp.weixin.qq.com/s/jzn0vLiQFX408opcfpPPxQ"&gt;2‰∏™ÊúàÁ†¥ 1000 starÔºåÊàëÁöÑGitHubÈ°πÁõÆÊé®ÂπøÂÆûÊàòÁªèÈ™å&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://mp.weixin.qq.com/s/8ghyfDAtQZjLrnWTQabYOQ"&gt;Âü∫‰∫éÊú¨È°πÁõÆÔºåÂ¶Ç‰ΩïÂºÄÂ±ïÂÖ¨‰ºóÂè∑ÊàñËÄÖÊñ∞ÈóªËµÑËÆØÁ±ªÊñáÁ´†ÂÜô‰Ωú&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;&lt;strong&gt;AI ÂºÄÂèëÔºö&lt;/strong&gt;&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Â¶ÇÊûú‰Ω†ÊúâÂ∞è‰ºóÈúÄÊ±ÇÔºåÂÆåÂÖ®ÂèØ‰ª•Âü∫‰∫éÊàëÁöÑÈ°πÁõÆËá™Ë°åÂºÄÂèëÔºåÈõ∂ÁºñÁ®ãÂü∫Á°ÄÁöÑ‰πüÂèØ‰ª•ËØïËØï&lt;/li&gt; 
  &lt;li&gt;ÊàëÊâÄÊúâÁöÑÂºÄÊ∫êÈ°πÁõÆÊàñÂ§öÊàñÂ∞ëÈÉΩ‰ΩøÁî®‰∫ÜËá™Â∑±ÂÜôÁöÑ&lt;strong&gt;AIËæÖÂä©ËΩØ‰ª∂&lt;/strong&gt;Êù•ÊèêÂçáÂºÄÂèëÊïàÁéáÔºåËøôÊ¨æÂ∑•ÂÖ∑Â∑≤ÂºÄÊ∫ê&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Ê†∏ÂøÉÂäüËÉΩ&lt;/strong&gt;ÔºöËøÖÈÄüÁ≠õÈÄâÈ°πÁõÆ‰ª£Á†ÅÂñÇÁªôAIÔºå‰Ω†Âè™ÈúÄË¶ÅË°•ÂÖÖ‰∏™‰∫∫ÈúÄÊ±ÇÂç≥ÂèØ&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;È°πÁõÆÂú∞ÂùÄ&lt;/strong&gt;Ôºö&lt;a href="https://github.com/sansan0/ai-code-context-helper"&gt;https://github.com/sansan0/ai-code-context-helper&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;ÂçáÁ∫ßËØ¥ÊòéÔºö&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Ê≥®ÊÑè&lt;/strong&gt;ÔºöËØ∑ÈÄöËøá‰ª•‰∏ãÊñπÂºèÊõ¥Êñ∞È°πÁõÆÔºå‰∏çË¶ÅÈÄöËøá Sync fork Á≠âÊñπÂºèÊõ¥Êñ∞&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Â∞èÁâàÊú¨Êõ¥Êñ∞&lt;/strong&gt;ÔºöÁõ¥Êé•Âú® GitHub ÁΩëÈ°µÁºñËæëÂô®‰∏≠ÔºåÁî®Êú¨È°πÁõÆÁöÑ &lt;code&gt;main.py&lt;/code&gt; ‰ª£Á†ÅÊõøÊç¢‰Ω† fork ‰ªìÂ∫ì‰∏≠ÁöÑÂØπÂ∫îÊñá‰ª∂&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Â§ßÁâàÊú¨ÂçáÁ∫ß&lt;/strong&gt;Ôºö‰ªé v1.x ÂçáÁ∫ßÂà∞ v2.0 Âª∫ËÆÆÂà†Èô§Áé∞Êúâ fork ÂêéÈáçÊñ∞ forkÔºåËøôÊ†∑Êõ¥ÁúÅÂäõ‰∏îÈÅøÂÖçÈÖçÁΩÆÂÜ≤Á™Å&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ÊàñËÄÖ&lt;/strong&gt;ÔºöÊ†πÊçÆÊõ¥Êñ∞Êó•ÂøóÁöÑÁâπÂà´ËØ¥ÊòéÂçáÁ∫ß&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;2025/08/30 - v2.1.0&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ÊÑüË∞¢ÂêÑ‰ΩçÊúãÂèãÁöÑÊîØÊåÅ‰∏éÂéöÁà±ÔºåÁâπÂà´ÊÑüË∞¢Ôºö&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;fork Âπ∂‰∏∫È°πÁõÆÁÇπ star&lt;/strong&gt; ÁöÑËßÇ‰ºó‰ª¨Ôºå‰Ω†‰ª¨ÁöÑËÆ§ÂèØÊòØÊàëÂâçËøõÁöÑÂä®Âäõ&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;ÂÖ≥Ê≥®ÂÖ¨‰ºóÂè∑Âπ∂ÁßØÊûÅ‰∫íÂä®&lt;/strong&gt; ÁöÑËØªËÄÖ‰ª¨Ôºå‰Ω†‰ª¨ÁöÑÁïôË®ÄÂíåÁÇπËµûËÆ©ÂÜÖÂÆπÊõ¥ÊúâÊ∏©Â∫¶&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Áªô‰∫àËµÑÈáëÁÇπËµûÊîØÊåÅ&lt;/strong&gt; ÁöÑÊúãÂèã‰ª¨Ôºå‰Ω†‰ª¨ÁöÑÊÖ∑ÊÖ®ËÆ©È°πÁõÆÂæó‰ª•ÊåÅÁª≠ÂèëÂ±ï&lt;/p&gt; 
 &lt;p&gt;‰∏ã‰∏ÄÊ¨°&lt;strong&gt;Êñ∞ÂäüËÉΩ&lt;/strong&gt;ÔºåÂ§ßÊ¶Ç‰ºöÊòØ ai ÂàÜÊûêÂäüËÉΩ(Â§ßÊ¶Ç(‚óè'‚ó°'‚óè)&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;Ê†∏ÂøÉÊîπËøõ&lt;/strong&gt;Ôºö&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Êé®ÈÄÅÈÄªËæë‰ºòÂåñ&lt;/strong&gt;Ôºö‰ªé"ÊØèÊ¨°ÊâßË°åÈÉΩÊé®ÈÄÅ"Êîπ‰∏∫"Êó∂Èó¥Á™óÂè£ÂÜÖÂèØÊéßÊé®ÈÄÅ"&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Êó∂Èó¥Á™óÂè£ÊéßÂà∂&lt;/strong&gt;ÔºöÂèØËÆæÂÆöÊé®ÈÄÅÊó∂Èó¥ËåÉÂõ¥ÔºåÈÅøÂÖçÈùûÂ∑•‰ΩúÊó∂Èó¥ÊâìÊâ∞&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Êé®ÈÄÅÈ¢ëÁéáÂèØÈÄâ&lt;/strong&gt;ÔºöÊó∂Èó¥ÊÆµÂÜÖÊîØÊåÅÂçïÊ¨°Êé®ÈÄÅÊàñÂ§öÊ¨°Êé®ÈÄÅ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Êõ¥Êñ∞ÊèêÁ§∫&lt;/strong&gt;Ôºö&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Êú¨ÂäüËÉΩÈªòËÆ§ÂÖ≥Èó≠ÔºåÈúÄÊâãÂä®ÂºÄÂêØ&lt;/li&gt; 
 &lt;li&gt;ÂêåÊó∂Êõ¥Êñ∞ main.py Âíå config.yaml&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üëâ ÂéÜÂè≤Êõ¥Êñ∞&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h3&gt;2025/08/27 - v2.0.4&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Êú¨Ê¨°ÁâàÊú¨‰∏çÊòØÂäüËÉΩ‰øÆÂ§çÔºåËÄåÊòØÈáçË¶ÅÊèêÈÜí&lt;/li&gt; 
  &lt;li&gt;ËØ∑Âä°ÂøÖÂ¶•ÂñÑ‰øùÁÆ°Â•Ω webhooksÔºå‰∏çË¶ÅÂÖ¨ÂºÄÔºå‰∏çË¶ÅÂÖ¨ÂºÄÔºå‰∏çË¶ÅÂÖ¨ÂºÄ&lt;/li&gt; 
  &lt;li&gt;Â¶ÇÊûú‰Ω†‰ª• fork ÁöÑÊñπÂºèÂ∞ÜÊú¨È°πÁõÆÈÉ®ÁΩ≤Âú® GitHub ‰∏äÔºåËØ∑Â∞Ü webhooks Â°´ÂÖ• GitHub SecretÔºåËÄåÈùû config.yaml&lt;/li&gt; 
  &lt;li&gt;Â¶ÇÊûú‰Ω†Â∑≤ÁªèÊö¥Èú≤‰∫Ü webhooks ÊàñÂ∞ÜÂÖ∂Â°´ÂÖ•‰∫Ü config.yamlÔºåÂª∫ËÆÆÂà†Èô§ÂêéÈáçÊñ∞ÁîüÊàê&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/08/06 - v2.0.3&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;‰ºòÂåñ github page ÁöÑÁΩëÈ°µÁâàÊïàÊûúÔºåÊñπ‰æøÁßªÂä®Á´Ø‰ΩøÁî®&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/07/28 - v2.0.2&lt;/h3&gt; 
 &lt;ul&gt; 
  &lt;li&gt;ÈáçÊûÑ‰ª£Á†Å&lt;/li&gt; 
  &lt;li&gt;Ëß£ÂÜ≥ÁâàÊú¨Âè∑ÂÆπÊòìË¢´ÈÅóÊºè‰øÆÊîπÁöÑÈóÆÈ¢ò&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/07/27 - v2.0.1&lt;/h3&gt; 
 &lt;p&gt;&lt;strong&gt;‰øÆÂ§çÈóÆÈ¢ò&lt;/strong&gt;:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;docker ÁöÑ shell ËÑöÊú¨ÁöÑÊç¢Ë°åÁ¨¶‰∏∫ CRLF ÂØºËá¥ÁöÑÊâßË°åÂºÇÂ∏∏ÈóÆÈ¢ò&lt;/li&gt; 
  &lt;li&gt;frequency_words.txt ‰∏∫Á©∫Êó∂ÔºåÂØºËá¥Êñ∞ÈóªÂèëÈÄÅ‰πü‰∏∫Á©∫ÁöÑÈÄªËæëÈóÆÈ¢ò&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;ul&gt; 
  &lt;li&gt;‰øÆÂ§çÂêéÔºåÂΩì‰Ω†ÈÄâÊã© frequency_words.txt ‰∏∫Á©∫Êó∂ÔºåÂ∞Ü&lt;strong&gt;Êé®ÈÄÅÊâÄÊúâÊñ∞Èóª&lt;/strong&gt;Ôºå‰ΩÜÂèóÈôê‰∫éÊ∂àÊÅØÊé®ÈÄÅÂ§ßÂ∞èÈôêÂà∂ÔºåËØ∑ÂÅöÂ¶Ç‰∏ãË∞ÉÊï¥ 
   &lt;ul&gt; 
    &lt;li&gt;ÊñπÊ°à‰∏ÄÔºöÂÖ≥Èó≠ÊâãÊú∫Êé®ÈÄÅÔºåÂè™ÈÄâÊã© Github Pages Â∏ÉÁΩÆ(ËøôÊòØËÉΩËé∑ÂæóÊúÄÂÆåÊï¥‰ø°ÊÅØÁöÑÊñπÊ°àÔºåÂ∞ÜÊääÊâÄÊúâÂπ≥Âè∞ÁöÑÁÉ≠ÁÇπÊåâÁÖß‰Ω†&lt;strong&gt;Ëá™ÂÆö‰πâÁöÑÁÉ≠ÊêúÁÆóÊ≥ï&lt;/strong&gt;ËøõË°åÈáçÊñ∞ÊéíÂ∫è)&lt;/li&gt; 
    &lt;li&gt;ÊñπÊ°à‰∫åÔºöÂáèÂ∞ëÊé®ÈÄÅÂπ≥Âè∞Ôºå‰ºòÂÖàÈÄâÊã©&lt;strong&gt;‰ºÅ‰∏öÂæÆ‰ø°&lt;/strong&gt;Êàñ&lt;strong&gt;Telegram&lt;/strong&gt;ÔºåËøô‰∏§‰∏™Êé®ÈÄÅÊàëÂÅö‰∫ÜÂàÜÊâπÊé®ÈÄÅÂäüËÉΩ(Âõ†‰∏∫ÂàÜÊâπÊé®ÈÄÅÂΩ±ÂìçÊé®ÈÄÅ‰ΩìÈ™åÔºå‰∏îÂè™ÊúâËøô‰∏§‰∏™Âπ≥Âè∞Âè™Áªô‰∏ÄÁÇπÁÇπÊé®ÈÄÅÂÆπÈáèÔºåÊâÄ‰ª•Êâç‰∏çÂæóÂ∑≤ÂÅö‰∫ÜÂàÜÊâπÊé®ÈÄÅÂäüËÉΩÔºå‰ΩÜËá≥Â∞ëËÉΩ‰øùËØÅËé∑ÂæóÁöÑ‰ø°ÊÅØÂÆåÊï¥)&lt;/li&gt; 
    &lt;li&gt;ÊñπÊ°à‰∏âÔºöÂèØ‰∏éÊñπÊ°à‰∫åÁªìÂêàÔºåÊ®°ÂºèÈÄâÊã© current Êàñ incremental ÂèØÊúâÊïàÂáèÂ∞ë‰∏ÄÊ¨°ÊÄßÊé®ÈÄÅÁöÑÂÜÖÂÆπ&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/07/17 - v2.0.0&lt;/h3&gt; 
 &lt;p&gt;&lt;strong&gt;ÈáçÂ§ßÈáçÊûÑ&lt;/strong&gt;Ôºö&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;ÈÖçÁΩÆÁÆ°ÁêÜÈáçÊûÑÔºöÊâÄÊúâÈÖçÁΩÆÁé∞Âú®ÈÄöËøá &lt;code&gt;config/config.yaml&lt;/code&gt; Êñá‰ª∂ÁÆ°ÁêÜÔºàmain.py Êàë‰æùÊóßÊ≤°ÊãÜÂàÜÔºåÊñπ‰æø‰Ω†‰ª¨Â§çÂà∂ÂçáÁ∫ßÔºâ&lt;/li&gt; 
  &lt;li&gt;ËøêË°åÊ®°ÂºèÂçáÁ∫ßÔºöÊîØÊåÅ‰∏âÁßçÊ®°Âºè - &lt;code&gt;daily&lt;/code&gt;ÔºàÂΩìÊó•Ê±áÊÄªÔºâ„ÄÅ&lt;code&gt;current&lt;/code&gt;ÔºàÂΩìÂâçÊ¶úÂçïÔºâ„ÄÅ&lt;code&gt;incremental&lt;/code&gt;ÔºàÂ¢ûÈáèÁõëÊéßÔºâ&lt;/li&gt; 
  &lt;li&gt;Docker ÊîØÊåÅÔºöÂÆåÊï¥ÁöÑ Docker ÈÉ®ÁΩ≤ÊñπÊ°àÔºåÊîØÊåÅÂÆπÂô®ÂåñËøêË°å&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;ÈÖçÁΩÆÊñá‰ª∂ËØ¥Êòé&lt;/strong&gt;Ôºö&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;config/config.yaml&lt;/code&gt; - ‰∏ªÈÖçÁΩÆÊñá‰ª∂ÔºàÂ∫îÁî®ËÆæÁΩÆ„ÄÅÁà¨Ëô´ÈÖçÁΩÆ„ÄÅÈÄöÁü•ÈÖçÁΩÆ„ÄÅÂπ≥Âè∞ÈÖçÁΩÆÁ≠âÔºâ&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;config/frequency_words.txt&lt;/code&gt; - ÂÖ≥ÈîÆËØçÈÖçÁΩÆÔºàÁõëÊéßËØçÊ±áËÆæÁΩÆÔºâ&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;2025/07/09 - v1.4.1&lt;/h3&gt; 
 &lt;p&gt;&lt;strong&gt;ÂäüËÉΩÊñ∞Â¢û&lt;/strong&gt;ÔºöÂ¢ûÂä†Â¢ûÈáèÊé®ÈÄÅ(Âú® main.py Â§¥ÈÉ®ÈÖçÁΩÆ FOCUS_NEW_ONLY)ÔºåËØ•ÂºÄÂÖ≥Âè™ÂÖ≥ÂøÉÊñ∞ËØùÈ¢òËÄåÈùûÊåÅÁª≠ÁÉ≠Â∫¶ÔºåÂè™Âú®ÊúâÊñ∞ÂÜÖÂÆπÊó∂ÊâçÂèëÈÄöÁü•„ÄÇ&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;‰øÆÂ§çÈóÆÈ¢ò&lt;/strong&gt;: Êüê‰∫õÊÉÖÂÜµ‰∏ãÔºåÁî±‰∫éÊñ∞ÈóªÊú¨Ë∫´Âê´ÊúâÁâπÊÆäÁ¨¶Âè∑ÂØºËá¥ÁöÑÂÅ∂ÂèëÊÄßÊéíÁâàÂºÇÂ∏∏„ÄÇ&lt;/p&gt; 
 &lt;h3&gt;2025/06/23 - v1.3.0&lt;/h3&gt; 
 &lt;p&gt;‰ºÅ‰∏öÂæÆ‰ø° Âíå Telegram ÁöÑÊé®ÈÄÅÊ∂àÊÅØÊúâÈïøÂ∫¶ÈôêÂà∂ÔºåÂØπÊ≠§ÊàëÈááÁî®Â∞ÜÊ∂àÊÅØÊãÜÂàÜÊé®ÈÄÅÁöÑÊñπÂºè„ÄÇÂºÄÂèëÊñáÊ°£ËØ¶ËßÅ&lt;a href="https://developer.work.weixin.qq.com/document/path/91770"&gt;‰ºÅ‰∏öÂæÆ‰ø°&lt;/a&gt; Âíå &lt;a href="https://core.telegram.org/bots/api"&gt;Telegram&lt;/a&gt;&lt;/p&gt; 
 &lt;h3&gt;2025/06/21 - v1.2.1&lt;/h3&gt; 
 &lt;p&gt;Âú®Êú¨ÁâàÊú¨‰πãÂâçÁöÑÊóßÁâàÊú¨Ôºå‰∏ç‰ªÖ main.py ÈúÄË¶ÅÂ§çÂà∂ÊõøÊç¢Ôºå crawler.yml ‰πüÈúÄË¶Å‰Ω†Â§çÂà∂ÊõøÊç¢ &lt;a href="https://github.com/sansan0/TrendRadar/raw/master/.github/workflows/crawler.yml"&gt;https://github.com/sansan0/TrendRadar/blob/master/.github/workflows/crawler.yml&lt;/a&gt;&lt;/p&gt; 
 &lt;h3&gt;2025/06/19 - v1.2.0&lt;/h3&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;ÊÑüË∞¢ claude research Êï¥ÁêÜÁöÑÂêÑÂπ≥Âè∞ api ,ËÆ©ÊàëÂø´ÈÄüÂÆåÊàêÂêÑÂπ≥Âè∞ÈÄÇÈÖçÔºàËôΩÁÑ∂‰ª£Á†ÅÊõ¥Â§öÂÜó‰Ωô‰∫Ü~&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;ol&gt; 
  &lt;li&gt;ÊîØÊåÅ telegram Ôºå‰ºÅ‰∏öÂæÆ‰ø°ÔºåÈíâÈíâÊé®ÈÄÅÊ∏†ÈÅì, ÊîØÊåÅÂ§öÊ∏†ÈÅìÈÖçÁΩÆÂíåÂêåÊó∂Êé®ÈÄÅ&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;h3&gt;2025/06/18 - v1.1.0&lt;/h3&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;&lt;strong&gt;200 star‚≠ê&lt;/strong&gt; ‰∫Ü, ÁªßÁª≠ÁªôÂ§ß‰ºôÂÑøÂä©ÂÖ¥~ËøëÊúüÔºåÂú®ÊàëÁöÑ"ÊÄÇÊÅø"‰∏ãÔºåÊå∫Â§ö‰∫∫Âú®ÊàëÂÖ¨‰ºóÂè∑ÁÇπËµûÂàÜ‰∫´Êé®ËçêÂä©Âäõ‰∫ÜÊàëÔºåÊàëÈÉΩÂú®ÂêéÂè∞ÁúãËßÅ‰∫ÜÂÖ∑‰ΩìË¥¶Âè∑ÁöÑÈºìÂä±Êï∞ÊçÆÔºåÂæàÂ§öÈÉΩÊàê‰∫ÜÂ§©‰ΩøËΩÆËÄÅÁ≤âÔºàÊàëÁé©ÂÖ¨‰ºóÂè∑Êâç‰∏Ä‰∏™Â§öÊúàÔºåËôΩÁÑ∂Ê≥®ÂÜåÊòØ‰∏ÉÂÖ´Âπ¥ÂâçÁöÑ‰∫ã‰∫ÜÂìàÂìàÔºåÂ±û‰∫é‰∏äËΩ¶Êó©ÔºåÂèëËΩ¶ÊôöÔºâÔºå‰ΩÜÂõ†‰∏∫‰Ω†‰ª¨Ê≤°ÊúâÁïôË®ÄÊàñÁßÅ‰ø°ÊàëÔºåÊâÄ‰ª•Êàë‰πüÊó†Ê≥ï‰∏Ä‰∏ÄÂõûÂ∫îÂπ∂ÊÑüË∞¢ÊîØÊåÅÔºåÂú®Ê≠§‰∏ÄÂπ∂Ë∞¢Ë∞¢ÔºÅ&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;ol&gt; 
  &lt;li&gt;ÈáçË¶ÅÁöÑÊõ¥Êñ∞ÔºåÂä†‰∫ÜÊùÉÈáçÔºå‰Ω†Áé∞Âú®ÁúãÂà∞ÁöÑÊñ∞ÈóªÈÉΩÊòØÊúÄÁÉ≠ÁÇπÊúÄÊúâÂÖ≥Ê≥®Â∫¶ÁöÑÂá∫Áé∞Âú®ÊúÄ‰∏äÈù¢&lt;/li&gt; 
  &lt;li&gt;Êõ¥Êñ∞ÊñáÊ°£‰ΩøÁî®ÔºåÂõ†‰∏∫ËøëÊúüÊõ¥Êñ∞‰∫ÜÂæàÂ§öÂäüËÉΩÔºåËÄå‰∏î‰πãÂâçÁöÑ‰ΩøÁî®ÊñáÊ°£ÊàëÂÅ∑ÊáíÂÜôÁöÑÁÆÄÂçïÔºàËßÅ‰∏ãÈù¢ÁöÑ ‚öôÔ∏è frequency_words.txt ÈÖçÁΩÆÂÆåÊï¥ÊïôÁ®ãÔºâ&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;h3&gt;2025/06/16 - v1.0.0&lt;/h3&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Â¢ûÂä†‰∫Ü‰∏Ä‰∏™È°πÁõÆÊñ∞ÁâàÊú¨Êõ¥Êñ∞ÊèêÁ§∫ÔºåÈªòËÆ§ÊâìÂºÄÔºåÂ¶ÇË¶ÅÂÖ≥ÊéâÔºåÂèØ‰ª•Âú® main.py ‰∏≠Êää "FEISHU_SHOW_VERSION_UPDATE": True ‰∏≠ÁöÑ True ÊîπÊàê False Âç≥ÂèØ&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;h3&gt;2025/06/13+14&lt;/h3&gt; 
 &lt;ol&gt; 
  &lt;li&gt;ÂéªÊéâ‰∫ÜÂÖºÂÆπ‰ª£Á†ÅÔºå‰πãÂâç fork ÁöÑÂêåÂ≠¶ÔºåÁõ¥Êé•Â§çÂà∂‰ª£Á†Å‰ºöÂú®ÂΩìÂ§©ÊòæÁ§∫ÂºÇÂ∏∏ÔºàÁ¨¨‰∫åÂ§©‰ºöÊÅ¢Â§çÊ≠£Â∏∏Ôºâ&lt;/li&gt; 
  &lt;li&gt;feishu Âíå html Â∫ïÈÉ®Â¢ûÂä†‰∏Ä‰∏™Êñ∞Â¢ûÊñ∞ÈóªÊòæÁ§∫&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;h3&gt;2025/06/09&lt;/h3&gt; 
 &lt;p&gt;&lt;strong&gt;100 star‚≠ê&lt;/strong&gt; ‰∫ÜÔºåÂÜô‰∏™Â∞èÂäüËÉΩÁªôÂ§ß‰ºôÂÑøÂä©Âä©ÂÖ¥ frequency_words.txt Êñá‰ª∂Â¢ûÂä†‰∫Ü‰∏Ä‰∏™„ÄêÂøÖÈ°ªËØç„ÄëÂäüËÉΩÔºå‰ΩøÁî® + Âè∑&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;ÂøÖÈ°ªËØçËØ≠Ê≥ïÂ¶Ç‰∏ãÔºö&lt;br /&gt; ÂîêÂÉßÊàñËÄÖÁå™ÂÖ´ÊàíÂøÖÈ°ªÂú®Ê†áÈ¢òÈáåÂêåÊó∂Âá∫Áé∞ÔºåÊâç‰ºöÊî∂ÂΩïÂà∞Êé®ÈÄÅÊñ∞Èóª‰∏≠&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code&gt;+ÂîêÂÉß
+Áå™ÂÖ´Êàí
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ol start="2"&gt; 
  &lt;li&gt;ËøáÊª§ËØçÁöÑ‰ºòÂÖàÁ∫ßÊõ¥È´òÔºö&lt;br /&gt; Â¶ÇÊûúÊ†áÈ¢ò‰∏≠ËøáÊª§ËØçÂåπÈÖçÂà∞ÂîêÂÉßÂøµÁªèÔºåÈÇ£‰πàÂç≥‰ΩøÂøÖÈ°ªËØçÈáåÊúâÂîêÂÉßÔºå‰πü‰∏çÊòæÁ§∫&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;pre&gt;&lt;code&gt;+ÂîêÂÉß
!ÂîêÂÉßÂøµÁªè
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;2025/06/02&lt;/h3&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;strong&gt;ÁΩëÈ°µ&lt;/strong&gt;Âíå&lt;strong&gt;È£û‰π¶Ê∂àÊÅØ&lt;/strong&gt;ÊîØÊåÅÊâãÊú∫Áõ¥Êé•Ë∑≥ËΩ¨ËØ¶ÊÉÖÊñ∞Èóª&lt;/li&gt; 
  &lt;li&gt;‰ºòÂåñÊòæÁ§∫ÊïàÊûú + 1&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;h3&gt;2025/05/26&lt;/h3&gt; 
 &lt;ol&gt; 
  &lt;li&gt;È£û‰π¶Ê∂àÊÅØÊòæÁ§∫ÊïàÊûú‰ºòÂåñ&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;table&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td align="center"&gt; ‰ºòÂåñÂâç&lt;br /&gt; &lt;img src="https://raw.githubusercontent.com/sansan0/TrendRadar/master/_image/before.jpg" alt="È£û‰π¶Ê∂àÊÅØÁïåÈù¢ - ‰ºòÂåñÂâç" width="400" /&gt; &lt;/td&gt; 
    &lt;td align="center"&gt; ‰ºòÂåñÂêé&lt;br /&gt; &lt;img src="https://raw.githubusercontent.com/sansan0/TrendRadar/master/_image/after.jpg" alt="È£û‰π¶Ê∂àÊÅØÁïåÈù¢ - ‰ºòÂåñÂêé" width="400" /&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h2&gt;üöÄ ‰ΩøÁî®ÊñπÂºè&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Fork Êú¨È°πÁõÆ&lt;/strong&gt;Âà∞‰Ω†ÁöÑ GitHub Ë¥¶Êà∑&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;ÁÇπÂáªÊú¨È°µÈù¢Âè≥‰∏äËßíÁöÑ"Fork"ÊåâÈíÆ&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ËÆæÁΩÆ GitHub SecretsÔºàÈÄâÊã©‰Ω†ÈúÄË¶ÅÁöÑÂπ≥Âè∞Ôºâ&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;Âú®‰Ω† Fork ÂêéÁöÑ‰ªìÂ∫ì‰∏≠ÔºåËøõÂÖ• &lt;code&gt;Settings&lt;/code&gt; &amp;gt; &lt;code&gt;Secrets and variables&lt;/code&gt; &amp;gt; &lt;code&gt;Actions&lt;/code&gt; &amp;gt; &lt;code&gt;New repository secret&lt;/code&gt;ÔºåÁÑ∂ÂêéÊ†πÊçÆÈúÄË¶ÅÈÖçÁΩÆ‰ª•‰∏ã‰ªª‰∏ÄÊàñÂ§ö‰∏™ÈÄöÁü•Âπ≥Âè∞Ôºö&lt;/p&gt; &lt;p&gt;ÂèØ‰ª•ÂêåÊó∂ÈÖçÁΩÆÂ§ö‰∏™Âπ≥Âè∞ÔºåÁ≥ªÁªü‰ºöÂêëÊâÄÊúâÈÖçÁΩÆÁöÑÂπ≥Âè∞ÂèëÈÄÅÈÄöÁü•„ÄÇ&lt;/p&gt; 
  &lt;details&gt; 
   &lt;summary&gt; &lt;strong&gt;üëâ ‰ºÅ‰∏öÂæÆ‰ø°Êú∫Âô®‰∫∫&lt;/strong&gt;ÔºàÈÖçÁΩÆÊúÄÁÆÄÂçïÊúÄËøÖÈÄüÔºâ&lt;/summary&gt; 
   &lt;br /&gt; 
   &lt;p&gt;&lt;strong&gt;GitHub Secret ÈÖçÁΩÆÔºö&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;ÂêçÁß∞Ôºö&lt;code&gt;WEWORK_WEBHOOK_URL&lt;/code&gt;&lt;/li&gt; 
    &lt;li&gt;ÂÄºÔºö‰Ω†ÁöÑ‰ºÅ‰∏öÂæÆ‰ø°Êú∫Âô®‰∫∫ Webhook Âú∞ÂùÄ&lt;/li&gt; 
   &lt;/ul&gt; 
   &lt;p&gt;&lt;strong&gt;Êú∫Âô®‰∫∫ËÆæÁΩÆÊ≠•È™§Ôºö&lt;/strong&gt;&lt;/p&gt; 
   &lt;h4&gt;ÊâãÊú∫Á´ØËÆæÁΩÆÔºö&lt;/h4&gt; 
   &lt;ol&gt; 
    &lt;li&gt;ÊâìÂºÄ‰ºÅ‰∏öÂæÆ‰ø° App ‚Üí ËøõÂÖ•ÁõÆÊ†áÂÜÖÈÉ®Áæ§ËÅä&lt;/li&gt; 
    &lt;li&gt;ÁÇπÂáªÂè≥‰∏äËßí"‚Ä¶"ÊåâÈíÆ ‚Üí ÈÄâÊã©"Áæ§Êú∫Âô®‰∫∫"&lt;/li&gt; 
    &lt;li&gt;ÁÇπÂáª"Ê∑ªÂä†" ‚Üí ÁÇπÂáª"Êñ∞Âª∫" ‚Üí ËÆæÁΩÆÊú∫Âô®‰∫∫ÊòµÁß∞&lt;/li&gt; 
    &lt;li&gt;Â§çÂà∂ Webhook Âú∞ÂùÄÔºåÈÖçÁΩÆÂà∞‰∏äÊñπÁöÑ GitHub Secret ‰∏≠&lt;/li&gt; 
   &lt;/ol&gt; 
   &lt;h4&gt;PC Á´ØËÆæÁΩÆÊµÅÁ®ãÁ±ª‰ºº&lt;/h4&gt; 
  &lt;/details&gt; 
  &lt;details&gt; 
   &lt;summary&gt; &lt;strong&gt;üëâ È£û‰π¶Êú∫Âô®‰∫∫&lt;/strong&gt;ÔºàÊ∂àÊÅØÊòæÁ§∫ÊúÄÂèãÂ•ΩÔºâ&lt;/summary&gt; 
   &lt;br /&gt; 
   &lt;p&gt;&lt;strong&gt;GitHub Secret ÈÖçÁΩÆÔºö&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;ÂêçÁß∞Ôºö&lt;code&gt;FEISHU_WEBHOOK_URL&lt;/code&gt;&lt;/li&gt; 
    &lt;li&gt;ÂÄºÔºö‰Ω†ÁöÑÈ£û‰π¶Êú∫Âô®‰∫∫ Webhook Âú∞ÂùÄ&lt;/li&gt; 
   &lt;/ul&gt; 
   &lt;p&gt;&lt;strong&gt;Êú∫Âô®‰∫∫ËÆæÁΩÆÊ≠•È™§Ôºö&lt;/strong&gt;&lt;/p&gt; 
   &lt;ol&gt; 
    &lt;li&gt; &lt;p&gt;ÁîµËÑëÊµèËßàÂô®ÊâìÂºÄ &lt;a href="https://botbuilder.feishu.cn/home/my-app"&gt;https://botbuilder.feishu.cn/home/my-app&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;ÁÇπÂáª"Êñ∞Âª∫Êú∫Âô®‰∫∫Â∫îÁî®"&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;ËøõÂÖ•ÂàõÂª∫ÁöÑÂ∫îÁî®ÂêéÔºåÁÇπÂáª"ÊµÅÁ®ãÊ∂âÂèä" &amp;gt; "ÂàõÂª∫ÊµÅÁ®ã" &amp;gt; "ÈÄâÊã©Ëß¶ÂèëÂô®"&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;ÂæÄ‰∏ãÊªëÂä®ÔºåÁÇπÂáª"Webhook Ëß¶Âèë"&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;Ê≠§Êó∂‰Ω†‰ºöÁúãÂà∞"Webhook Âú∞ÂùÄ"ÔºåÊääËøô‰∏™ÈìæÊé•ÂÖàÂ§çÂà∂Âà∞Êú¨Âú∞ËÆ∞‰∫ãÊú¨ÊöÇÂ≠òÔºåÁªßÁª≠Êé•‰∏ãÊù•ÁöÑÊìç‰Ωú&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;"ÂèÇÊï∞"ÈáåÈù¢Êîæ‰∏ä‰∏ãÈù¢ÁöÑÂÜÖÂÆπÔºåÁÑ∂ÂêéÁÇπÂáª"ÂÆåÊàê"&lt;/p&gt; &lt;/li&gt; 
   &lt;/ol&gt; 
   &lt;pre&gt;&lt;code class="language-json"&gt;{
  "message_type": "text",
  "content": {
    "total_titles": "{{ÂÜÖÂÆπ}}",
    "timestamp": "{{ÂÜÖÂÆπ}}",
    "report_type": "{{ÂÜÖÂÆπ}}",
    "text": "{{ÂÜÖÂÆπ}}"
  }
}
&lt;/code&gt;&lt;/pre&gt; 
   &lt;ol start="7"&gt; 
    &lt;li&gt; &lt;p&gt;ÁÇπÂáª"ÈÄâÊã©Êìç‰Ωú" &amp;gt; "ÂèëÈÄÅÈ£û‰π¶Ê∂àÊÅØ"ÔºåÂãæÈÄâ "Áæ§Ê∂àÊÅØ"ÔºåÁÑ∂ÂêéÁÇπÂáª‰∏ãÈù¢ÁöÑËæìÂÖ•Ê°ÜÔºåÁÇπÂáª"ÊàëÁÆ°ÁêÜÁöÑÁæ§ÁªÑ"ÔºàÂ¶ÇÊûúÊ≤°ÊúâÁæ§ÁªÑÔºå‰Ω†ÂèØ‰ª•Âú®È£û‰π¶ app ‰∏äÂàõÂª∫Áæ§ÁªÑÔºâ&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;Ê∂àÊÅØÊ†áÈ¢òÂ°´ÂÜô"TrendRadar ÁÉ≠ÁÇπÁõëÊéß"&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;ÊúÄÂÖ≥ÈîÆÁöÑÈÉ®ÂàÜÊù•‰∫ÜÔºåÁÇπÂáª + ÊåâÈíÆÔºåÈÄâÊã©"Webhook Ëß¶Âèë"ÔºåÁÑ∂ÂêéÊåâÁÖß‰∏ãÈù¢ÁöÑÂõæÁâáÊëÜÊîæ&lt;/p&gt; &lt;/li&gt; 
   &lt;/ol&gt; 
   &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/sansan0/TrendRadar/master/_image/image.png" alt="È£û‰π¶Êú∫Âô®‰∫∫ÈÖçÁΩÆÁ§∫‰æã" /&gt;&lt;/p&gt; 
   &lt;ol start="10"&gt; 
    &lt;li&gt;ÈÖçÁΩÆÂÆåÊàêÂêéÔºåÂ∞ÜÁ¨¨ 5 Ê≠•Â§çÂà∂ÁöÑ Webhook Âú∞ÂùÄÈÖçÁΩÆÂà∞ GitHub Secrets ‰∏≠ÁöÑ &lt;code&gt;FEISHU_WEBHOOK_URL&lt;/code&gt;&lt;/li&gt; 
   &lt;/ol&gt; 
  &lt;/details&gt; 
  &lt;details&gt; 
   &lt;summary&gt; &lt;strong&gt;üëâ ÈíâÈíâÊú∫Âô®‰∫∫&lt;/strong&gt;&lt;/summary&gt; 
   &lt;br /&gt; 
   &lt;p&gt;&lt;strong&gt;GitHub Secret ÈÖçÁΩÆÔºö&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;ÂêçÁß∞Ôºö&lt;code&gt;DINGTALK_WEBHOOK_URL&lt;/code&gt;&lt;/li&gt; 
    &lt;li&gt;ÂÄºÔºö‰Ω†ÁöÑÈíâÈíâÊú∫Âô®‰∫∫ Webhook Âú∞ÂùÄ&lt;/li&gt; 
   &lt;/ul&gt; 
   &lt;p&gt;&lt;strong&gt;Êú∫Âô®‰∫∫ËÆæÁΩÆÊ≠•È™§Ôºö&lt;/strong&gt;&lt;/p&gt; 
   &lt;ol&gt; 
    &lt;li&gt; &lt;p&gt;&lt;strong&gt;ÂàõÂª∫Êú∫Âô®‰∫∫Ôºà‰ªÖ PC Á´ØÊîØÊåÅÔºâ&lt;/strong&gt;Ôºö&lt;/p&gt; 
     &lt;ul&gt; 
      &lt;li&gt;ÊâìÂºÄÈíâÈíâ PC ÂÆ¢Êà∑Á´ØÔºåËøõÂÖ•ÁõÆÊ†áÁæ§ËÅä&lt;/li&gt; 
      &lt;li&gt;ÁÇπÂáªÁæ§ËÆæÁΩÆÂõæÊ†áÔºà‚öôÔ∏èÔºâ‚Üí ÂæÄ‰∏ãÁøªÊâæÂà∞"Êú∫Âô®‰∫∫"ÁÇπÂºÄ&lt;/li&gt; 
      &lt;li&gt;ÈÄâÊã©"Ê∑ªÂä†Êú∫Âô®‰∫∫" ‚Üí "Ëá™ÂÆö‰πâ"&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;&lt;strong&gt;ÈÖçÁΩÆÊú∫Âô®‰∫∫&lt;/strong&gt;Ôºö&lt;/p&gt; 
     &lt;ul&gt; 
      &lt;li&gt;ËÆæÁΩÆÊú∫Âô®‰∫∫ÂêçÁß∞&lt;/li&gt; 
      &lt;li&gt;&lt;strong&gt;ÂÆâÂÖ®ËÆæÁΩÆ&lt;/strong&gt;Ôºö 
       &lt;ul&gt; 
        &lt;li&gt;&lt;strong&gt;Ëá™ÂÆö‰πâÂÖ≥ÈîÆËØç&lt;/strong&gt;ÔºöËÆæÁΩÆ "ÁÉ≠ÁÇπ"&lt;/li&gt; 
       &lt;/ul&gt; &lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;&lt;strong&gt;ÂÆåÊàêËÆæÁΩÆ&lt;/strong&gt;Ôºö&lt;/p&gt; 
     &lt;ul&gt; 
      &lt;li&gt;ÂãæÈÄâÊúçÂä°Êù°Ê¨æÂçèËÆÆ ‚Üí ÁÇπÂáª"ÂÆåÊàê"&lt;/li&gt; 
      &lt;li&gt;Â§çÂà∂Ëé∑ÂæóÁöÑ Webhook URL&lt;/li&gt; 
      &lt;li&gt;Â∞Ü URL ÈÖçÁΩÆÂà∞ GitHub Secrets ‰∏≠ÁöÑ &lt;code&gt;DINGTALK_WEBHOOK_URL&lt;/code&gt;&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
   &lt;/ol&gt; 
   &lt;p&gt;&lt;strong&gt;Ê≥®ÊÑè&lt;/strong&gt;ÔºöÁßªÂä®Á´ØÂè™ËÉΩÊé•Êî∂Ê∂àÊÅØÔºåÊó†Ê≥ïÂàõÂª∫Êñ∞Êú∫Âô®‰∫∫„ÄÇ&lt;/p&gt; 
  &lt;/details&gt; 
  &lt;details&gt; 
   &lt;summary&gt; &lt;strong&gt;üëâ Telegram Bot&lt;/strong&gt;&lt;/summary&gt; 
   &lt;br /&gt; 
   &lt;p&gt;&lt;strong&gt;GitHub Secret ÈÖçÁΩÆÔºö&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;ÂêçÁß∞Ôºö&lt;code&gt;TELEGRAM_BOT_TOKEN&lt;/code&gt; - ‰Ω†ÁöÑ Telegram Bot Token&lt;/li&gt; 
    &lt;li&gt;ÂêçÁß∞Ôºö&lt;code&gt;TELEGRAM_CHAT_ID&lt;/code&gt; - ‰Ω†ÁöÑ Telegram Chat ID&lt;/li&gt; 
   &lt;/ul&gt; 
   &lt;p&gt;&lt;strong&gt;Êú∫Âô®‰∫∫ËÆæÁΩÆÊ≠•È™§Ôºö&lt;/strong&gt;&lt;/p&gt; 
   &lt;ol&gt; 
    &lt;li&gt; &lt;p&gt;&lt;strong&gt;ÂàõÂª∫Êú∫Âô®‰∫∫&lt;/strong&gt;Ôºö&lt;/p&gt; 
     &lt;ul&gt; 
      &lt;li&gt;Âú® Telegram ‰∏≠ÊêúÁ¥¢ &lt;code&gt;@BotFather&lt;/code&gt;ÔºàÂ§ßÂ∞èÂÜôÊ≥®ÊÑèÔºåÊúâËìùËâ≤ÂæΩÁ´†ÂãæÂãæÔºåÊúâÁ±ª‰ºº 37849827 monthly usersÔºåËøô‰∏™ÊâçÊòØÂÆòÊñπÁöÑÔºåÊúâ‰∏Ä‰∫õ‰ªøÂÆòÊñπÁöÑË¥¶Âè∑Ê≥®ÊÑèËæ®Âà´Ôºâ&lt;/li&gt; 
      &lt;li&gt;ÂèëÈÄÅ &lt;code&gt;/newbot&lt;/code&gt; ÂëΩ‰ª§ÂàõÂª∫Êñ∞Êú∫Âô®‰∫∫&lt;/li&gt; 
      &lt;li&gt;ËÆæÁΩÆÊú∫Âô®‰∫∫ÂêçÁß∞ÔºàÂøÖÈ°ª‰ª•"bot"ÁªìÂ∞æÔºåÂæàÂÆπÊòìÈÅáÂà∞ÈáçÂ§çÂêçÂ≠óÔºåÊâÄ‰ª•‰Ω†Ë¶ÅÁªûÂ∞ΩËÑëÊ±ÅÊÉ≥‰∏çÂêåÁöÑÂêçÂ≠óÔºâ&lt;/li&gt; 
      &lt;li&gt;Ëé∑Âèñ Bot TokenÔºàÊ†ºÂºèÂ¶ÇÔºö&lt;code&gt;123456789:AAHfiqksKZ8WmR2zSjiQ7_v4TMAKdiHm9T0&lt;/code&gt;Ôºâ&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;&lt;strong&gt;Ëé∑Âèñ Chat ID&lt;/strong&gt;Ôºö&lt;/p&gt; &lt;p&gt;&lt;strong&gt;ÊñπÊ≥ï‰∏ÄÔºöÈÄöËøáÂÆòÊñπ API Ëé∑Âèñ&lt;/strong&gt;&lt;/p&gt; 
     &lt;ul&gt; 
      &lt;li&gt;ÂÖàÂêë‰Ω†ÁöÑÊú∫Âô®‰∫∫ÂèëÈÄÅ‰∏ÄÊù°Ê∂àÊÅØ&lt;/li&gt; 
      &lt;li&gt;ËÆøÈóÆÔºö&lt;code&gt;https://api.telegram.org/bot&amp;lt;‰Ω†ÁöÑBot Token&amp;gt;/getUpdates&lt;/code&gt;&lt;/li&gt; 
      &lt;li&gt;Âú®ËøîÂõûÁöÑ JSON ‰∏≠ÊâæÂà∞ &lt;code&gt;"chat":{"id":Êï∞Â≠ó}&lt;/code&gt; ‰∏≠ÁöÑÊï∞Â≠ó&lt;/li&gt; 
     &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;ÊñπÊ≥ï‰∫åÔºö‰ΩøÁî®Á¨¨‰∏âÊñπÂ∑•ÂÖ∑&lt;/strong&gt;&lt;/p&gt; 
     &lt;ul&gt; 
      &lt;li&gt;ÊêúÁ¥¢ &lt;code&gt;@userinfobot&lt;/code&gt; Âπ∂ÂèëÈÄÅ &lt;code&gt;/start&lt;/code&gt;&lt;/li&gt; 
      &lt;li&gt;Ëé∑Âèñ‰Ω†ÁöÑÁî®Êà∑ ID ‰Ωú‰∏∫ Chat ID&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;&lt;strong&gt;ÈÖçÁΩÆÂà∞ GitHub&lt;/strong&gt;Ôºö&lt;/p&gt; 
     &lt;ul&gt; 
      &lt;li&gt;&lt;code&gt;TELEGRAM_BOT_TOKEN&lt;/code&gt;ÔºöÂ°´ÂÖ•Á¨¨ 1 Ê≠•Ëé∑ÂæóÁöÑ Bot Token&lt;/li&gt; 
      &lt;li&gt;&lt;code&gt;TELEGRAM_CHAT_ID&lt;/code&gt;ÔºöÂ°´ÂÖ•Á¨¨ 2 Ê≠•Ëé∑ÂæóÁöÑ Chat ID&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
   &lt;/ol&gt; 
  &lt;/details&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ÈÖçÁΩÆÂÖ≥ÈîÆËØçÂíåËÆæÁΩÆ&lt;/strong&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;ÂÖ≥ÈîÆËØçÈÖçÁΩÆ&lt;/strong&gt;: ‰øÆÊîπ &lt;code&gt;config/frequency_words.txt&lt;/code&gt; Êñá‰ª∂ÔºåÊ∑ªÂä†‰Ω†ÂÖ≥ÂøÉÁöÑÂÖ≥ÈîÆËØç&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;ËøêË°åÊ®°ÂºèÈÖçÁΩÆ&lt;/strong&gt;: Âú® &lt;code&gt;config/config.yaml&lt;/code&gt; ‰∏≠‰øÆÊîπ &lt;code&gt;report.mode&lt;/code&gt; ËÆæÁΩÆÔºö 
    &lt;table&gt; 
     &lt;thead&gt; 
      &lt;tr&gt; 
       &lt;th&gt;Ê®°Âºè&lt;/th&gt; 
       &lt;th&gt;Êé®ÈÄÅÊó∂Êú∫&lt;/th&gt; 
       &lt;th&gt;ÊòæÁ§∫ÂÜÖÂÆπ&lt;/th&gt; 
       &lt;th&gt;ÈÄÇÁî®Âú∫ÊôØ&lt;/th&gt; 
      &lt;/tr&gt; 
     &lt;/thead&gt; 
     &lt;tbody&gt; 
      &lt;tr&gt; 
       &lt;td&gt;&lt;strong&gt;ÂΩìÊó•Ê±áÊÄªÊ®°Âºè&lt;/strong&gt;&lt;br /&gt;&lt;code&gt;daily&lt;/code&gt;&lt;/td&gt; 
       &lt;td&gt;ÊåâÊó∂Êé®ÈÄÅ&lt;/td&gt; 
       &lt;td&gt;ÂΩìÊó•ÊâÄÊúâÂåπÈÖçÊñ∞Èóª&lt;br /&gt;+ Êñ∞Â¢ûÊñ∞ÈóªÂå∫Âüü&lt;/td&gt; 
       &lt;td&gt;Êó•Êä•ÊÄªÁªì&lt;br /&gt;ÂÖ®Èù¢‰∫ÜËß£ÂΩìÊó•ÁÉ≠ÁÇπË∂ãÂäø&lt;/td&gt; 
      &lt;/tr&gt; 
      &lt;tr&gt; 
       &lt;td&gt;&lt;strong&gt;ÂΩìÂâçÊ¶úÂçïÊ®°Âºè&lt;/strong&gt;&lt;br /&gt;&lt;code&gt;current&lt;/code&gt;&lt;/td&gt; 
       &lt;td&gt;ÊåâÊó∂Êé®ÈÄÅ&lt;/td&gt; 
       &lt;td&gt;ÂΩìÂâçÊ¶úÂçïÂåπÈÖçÊñ∞Èóª&lt;br /&gt;+ Êñ∞Â¢ûÊñ∞ÈóªÂå∫Âüü&lt;/td&gt; 
       &lt;td&gt;ÂÆûÊó∂ÁÉ≠ÁÇπËøΩË∏™&lt;br /&gt;‰∫ÜËß£ÂΩìÂâçÊúÄÁÅ´ÁöÑÂÜÖÂÆπ&lt;/td&gt; 
      &lt;/tr&gt; 
      &lt;tr&gt; 
       &lt;td&gt;&lt;strong&gt;Â¢ûÈáèÁõëÊéßÊ®°Âºè&lt;/strong&gt;&lt;br /&gt;&lt;code&gt;incremental&lt;/code&gt;&lt;/td&gt; 
       &lt;td&gt;ÊúâÊñ∞Â¢ûÊâçÊé®ÈÄÅ&lt;/td&gt; 
       &lt;td&gt;Êñ∞Âá∫Áé∞ÁöÑÂåπÈÖçÈ¢ëÁéáËØçÊñ∞Èóª&lt;/td&gt; 
       &lt;td&gt;ÈÅøÂÖçÈáçÂ§ç‰ø°ÊÅØÂπ≤Êâ∞&lt;br /&gt;È´òÈ¢ëÁõëÊéßÂú∫ÊôØ&lt;/td&gt; 
      &lt;/tr&gt; 
     &lt;/tbody&gt; 
    &lt;/table&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;details&gt; 
   &lt;summary&gt;&lt;strong&gt;üëâ frequency_words.txt ÈÖçÁΩÆÊïôÁ®ã&lt;/strong&gt;&lt;/summary&gt; 
   &lt;br /&gt; 
   &lt;p&gt;Âú® &lt;code&gt;frequency_words.txt&lt;/code&gt; Êñá‰ª∂‰∏≠ÈÖçÁΩÆÁõëÊéßÁöÑÂÖ≥ÈîÆËØçÔºåÊîØÊåÅ‰∏âÁßçËØ≠Ê≥ïÂíåËØçÁªÑÂäüËÉΩ„ÄÇ&lt;/p&gt; 
   &lt;h3&gt;üìã Âü∫Á°ÄËØ≠Ê≥ïËØ¥Êòé&lt;/h3&gt; 
   &lt;h4&gt;1. &lt;strong&gt;ÊôÆÈÄöÂÖ≥ÈîÆËØç&lt;/strong&gt; - Âü∫Á°ÄÂåπÈÖç&lt;/h4&gt; 
   &lt;pre&gt;&lt;code class="language-txt"&gt;Âçé‰∏∫
OPPO
ËãπÊûú
&lt;/code&gt;&lt;/pre&gt; 
   &lt;p&gt;&lt;strong&gt;‰ΩúÁî®Ôºö&lt;/strong&gt; Êñ∞ÈóªÊ†áÈ¢òÂåÖÂê´ÂÖ∂‰∏≠&lt;strong&gt;‰ªªÊÑè‰∏Ä‰∏™ËØç&lt;/strong&gt;Â∞±‰ºöË¢´ÊçïËé∑&lt;/p&gt; 
   &lt;h4&gt;2. &lt;strong&gt;ÂøÖÈ°ªËØç&lt;/strong&gt; &lt;code&gt;+ËØçÊ±á&lt;/code&gt; - ÈôêÂÆöËåÉÂõ¥&lt;/h4&gt; 
   &lt;pre&gt;&lt;code class="language-txt"&gt;Âçé‰∏∫
OPPO
+ÊâãÊú∫
&lt;/code&gt;&lt;/pre&gt; 
   &lt;p&gt;&lt;strong&gt;‰ΩúÁî®Ôºö&lt;/strong&gt; ÂøÖÈ°ªÂêåÊó∂ÂåÖÂê´ÊôÆÈÄöËØç&lt;strong&gt;Âíå&lt;/strong&gt;ÂøÖÈ°ªËØçÊâç‰ºöË¢´ÊçïËé∑&lt;/p&gt; 
   &lt;h4&gt;3. &lt;strong&gt;ËøáÊª§ËØç&lt;/strong&gt; &lt;code&gt;!ËØçÊ±á&lt;/code&gt; - ÊéíÈô§Âπ≤Êâ∞&lt;/h4&gt; 
   &lt;pre&gt;&lt;code class="language-txt"&gt;ËãπÊûú
Âçé‰∏∫
!Ê∞¥Êûú
!‰ª∑Ê†º
&lt;/code&gt;&lt;/pre&gt; 
   &lt;p&gt;&lt;strong&gt;‰ΩúÁî®Ôºö&lt;/strong&gt; ÂåÖÂê´ËøáÊª§ËØçÁöÑÊñ∞Èóª‰ºöË¢´&lt;strong&gt;Áõ¥Êé•ÊéíÈô§&lt;/strong&gt;ÔºåÂç≥‰ΩøÂåÖÂê´ÂÖ≥ÈîÆËØç&lt;/p&gt; 
   &lt;h3&gt;üîó ËØçÁªÑÂäüËÉΩ - Á©∫Ë°åÂàÜÈöîÁöÑÈáçË¶Å‰ΩúÁî®&lt;/h3&gt; 
   &lt;p&gt;&lt;strong&gt;Ê†∏ÂøÉËßÑÂàôÔºö&lt;/strong&gt; Áî®&lt;strong&gt;Á©∫Ë°å&lt;/strong&gt;ÂàÜÈöî‰∏çÂêåÁöÑËØçÁªÑÔºåÊØè‰∏™ËØçÁªÑÁã¨Á´ãÁªüËÆ°&lt;/p&gt; 
   &lt;h4&gt;Á§∫‰æãÈÖçÁΩÆÔºö&lt;/h4&gt; 
   &lt;pre&gt;&lt;code class="language-txt"&gt;iPhone
Âçé‰∏∫
OPPO
+ÂèëÂ∏É

AËÇ°
‰∏äËØÅ
Ê∑±ËØÅ
+Ê∂®Ë∑å
!È¢ÑÊµã

‰∏ñÁïåÊùØ
Ê¨ßÊ¥≤ÊùØ
‰∫öÊ¥≤ÊùØ
+ÊØîËµõ
&lt;/code&gt;&lt;/pre&gt; 
   &lt;h4&gt;ËØçÁªÑËß£ÈáäÂèäÂåπÈÖçÊïàÊûúÔºö&lt;/h4&gt; 
   &lt;p&gt;&lt;strong&gt;Á¨¨1ÁªÑ - ÊâãÊú∫Êñ∞ÂìÅÁ±ªÔºö&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;ÂÖ≥ÈîÆËØçÔºöiPhone„ÄÅÂçé‰∏∫„ÄÅOPPO&lt;/li&gt; 
    &lt;li&gt;ÂøÖÈ°ªËØçÔºöÂèëÂ∏É&lt;/li&gt; 
    &lt;li&gt;ÊïàÊûúÔºöÂøÖÈ°ªÂåÖÂê´ÊâãÊú∫ÂìÅÁâåÂêçÔºåÂêåÊó∂ÂåÖÂê´"ÂèëÂ∏É"&lt;/li&gt; 
   &lt;/ul&gt; 
   &lt;p&gt;&lt;strong&gt;ÂåπÈÖçÁ§∫‰æãÔºö&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;‚úÖ "iPhone 15Ê≠£ÂºèÂèëÂ∏ÉÂîÆ‰ª∑ÂÖ¨Â∏É" ‚Üê Êúâ"iPhone"+"ÂèëÂ∏É"&lt;/li&gt; 
    &lt;li&gt;‚úÖ "Âçé‰∏∫Mate60Á≥ªÂàóÂèëÂ∏É‰ºöÁõ¥Êí≠" ‚Üê Êúâ"Âçé‰∏∫"+"ÂèëÂ∏É"&lt;/li&gt; 
    &lt;li&gt;‚úÖ "OPPO Find X7ÂèëÂ∏ÉÊó∂Èó¥Á°ÆÂÆö" ‚Üê Êúâ"OPPO"+"ÂèëÂ∏É"&lt;/li&gt; 
    &lt;li&gt;‚ùå "iPhoneÈîÄÈáèÂàõÊñ∞È´ò" ‚Üê Êúâ"iPhone"‰ΩÜÁº∫Â∞ë"ÂèëÂ∏É"&lt;/li&gt; 
   &lt;/ul&gt; 
   &lt;p&gt;&lt;strong&gt;Á¨¨2ÁªÑ - ËÇ°Â∏ÇË°åÊÉÖÁ±ªÔºö&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;ÂÖ≥ÈîÆËØçÔºöAËÇ°„ÄÅ‰∏äËØÅ„ÄÅÊ∑±ËØÅ&lt;/li&gt; 
    &lt;li&gt;ÂøÖÈ°ªËØçÔºöÊ∂®Ë∑å&lt;/li&gt; 
    &lt;li&gt;ËøáÊª§ËØçÔºöÈ¢ÑÊµã&lt;/li&gt; 
    &lt;li&gt;ÊïàÊûúÔºöÂåÖÂê´ËÇ°Â∏ÇÁõ∏ÂÖ≥ËØçÔºåÂêåÊó∂ÂåÖÂê´"Ê∂®Ë∑å"Ôºå‰ΩÜÊéíÈô§ÂåÖÂê´"È¢ÑÊµã"ÁöÑÂÜÖÂÆπ&lt;/li&gt; 
   &lt;/ul&gt; 
   &lt;p&gt;&lt;strong&gt;ÂåπÈÖçÁ§∫‰æãÔºö&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;‚úÖ "AËÇ°‰ªäÊó•Â§ßÂπÖÊ∂®Ë∑åÂàÜÊûê" ‚Üê Êúâ"AËÇ°"+"Ê∂®Ë∑å"&lt;/li&gt; 
    &lt;li&gt;‚úÖ "‰∏äËØÅÊåáÊï∞Ê∂®Ë∑åÂéüÂõ†Ëß£ËØª" ‚Üê Êúâ"‰∏äËØÅ"+"Ê∂®Ë∑å"&lt;/li&gt; 
    &lt;li&gt;‚ùå "‰∏ìÂÆ∂È¢ÑÊµãAËÇ°Ê∂®Ë∑åË∂ãÂäø" ‚Üê Êúâ"AËÇ°"+"Ê∂®Ë∑å"‰ΩÜÂåÖÂê´"È¢ÑÊµã"&lt;/li&gt; 
    &lt;li&gt;‚ùå "AËÇ°Êàê‰∫§ÈáèÂàõÊñ∞È´ò" ‚Üê Êúâ"AËÇ°"‰ΩÜÁº∫Â∞ë"Ê∂®Ë∑å"&lt;/li&gt; 
   &lt;/ul&gt; 
   &lt;p&gt;&lt;strong&gt;Á¨¨3ÁªÑ - Ë∂≥ÁêÉËµõ‰∫ãÁ±ªÔºö&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;ÂÖ≥ÈîÆËØçÔºö‰∏ñÁïåÊùØ„ÄÅÊ¨ßÊ¥≤ÊùØ„ÄÅ‰∫öÊ¥≤ÊùØ&lt;/li&gt; 
    &lt;li&gt;ÂøÖÈ°ªËØçÔºöÊØîËµõ&lt;/li&gt; 
    &lt;li&gt;ÊïàÊûúÔºöÂøÖÈ°ªÂåÖÂê´ÊùØËµõÂêçÁß∞ÔºåÂêåÊó∂ÂåÖÂê´"ÊØîËµõ"&lt;/li&gt; 
   &lt;/ul&gt; 
   &lt;p&gt;&lt;strong&gt;ÂåπÈÖçÁ§∫‰æãÔºö&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;‚úÖ "‰∏ñÁïåÊùØÂ∞èÁªÑËµõÊØîËµõÁªìÊûú" ‚Üê Êúâ"‰∏ñÁïåÊùØ"+"ÊØîËµõ"&lt;/li&gt; 
    &lt;li&gt;‚úÖ "Ê¨ßÊ¥≤ÊùØÂÜ≥ËµõÊØîËµõÊó∂Èó¥" ‚Üê Êúâ"Ê¨ßÊ¥≤ÊùØ"+"ÊØîËµõ"&lt;/li&gt; 
    &lt;li&gt;‚ùå "‰∏ñÁïåÊùØÈó®Á•®ÂºÄÂîÆ" ‚Üê Êúâ"‰∏ñÁïåÊùØ"‰ΩÜÁº∫Â∞ë"ÊØîËµõ"&lt;/li&gt; 
   &lt;/ul&gt; 
   &lt;h3&gt;üéØ ÈÖçÁΩÆÊäÄÂ∑ß&lt;/h3&gt; 
   &lt;h4&gt;1. &lt;strong&gt;‰ªéÂÆΩÂà∞‰∏•ÁöÑÈÖçÁΩÆÁ≠ñÁï•&lt;/strong&gt;&lt;/h4&gt; 
   &lt;pre&gt;&lt;code class="language-txt"&gt;# Á¨¨‰∏ÄÊ≠•ÔºöÂÖàÁî®ÂÆΩÊ≥õÂÖ≥ÈîÆËØçÊµãËØï
‰∫∫Â∑•Êô∫ËÉΩ
AI
ChatGPT

# Á¨¨‰∫åÊ≠•ÔºöÂèëÁé∞ËØØÂåπÈÖçÂêéÔºåÂä†ÂÖ•ÂøÖÈ°ªËØçÈôêÂÆö
‰∫∫Â∑•Êô∫ËÉΩ  
AI
ChatGPT
+ÊäÄÊúØ

# Á¨¨‰∏âÊ≠•ÔºöÂèëÁé∞Âπ≤Êâ∞ÂÜÖÂÆπÂêéÔºåÂä†ÂÖ•ËøáÊª§ËØç
‰∫∫Â∑•Êô∫ËÉΩ
AI  
ChatGPT
+ÊäÄÊúØ
!ÂπøÂëä
!ÂüπËÆ≠
&lt;/code&gt;&lt;/pre&gt; 
   &lt;h4&gt;2. &lt;strong&gt;ÈÅøÂÖçËøáÂ∫¶Â§çÊùÇ&lt;/strong&gt;&lt;/h4&gt; 
   &lt;p&gt;‚ùå &lt;strong&gt;‰∏çÊé®ËçêÔºö&lt;/strong&gt; ‰∏Ä‰∏™ËØçÁªÑÂåÖÂê´Â§™Â§öËØçÊ±á&lt;/p&gt; 
   &lt;pre&gt;&lt;code class="language-txt"&gt;Âçé‰∏∫
OPPO
ËãπÊûú
‰∏âÊòü
vivo
‰∏ÄÂä†
È≠ÖÊóè
+ÊâãÊú∫
+ÂèëÂ∏É
+ÈîÄÈáè
!ÂÅáË¥ß
!Áª¥‰øÆ
!‰∫åÊâã
&lt;/code&gt;&lt;/pre&gt; 
   &lt;p&gt;‚úÖ &lt;strong&gt;Êé®ËçêÔºö&lt;/strong&gt; ÊãÜÂàÜÊàêÂ§ö‰∏™Á≤æÁ°ÆÁöÑËØçÁªÑ&lt;/p&gt; 
   &lt;pre&gt;&lt;code class="language-txt"&gt;Âçé‰∏∫
OPPO
+Êñ∞ÂìÅ

ËãπÊûú
‰∏âÊòü  
+ÂèëÂ∏É

ÊâãÊú∫
ÈîÄÈáè
+Â∏ÇÂú∫
&lt;/code&gt;&lt;/pre&gt; 
  &lt;/details&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Ëá™Âä®ËøêË°å&lt;/strong&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;È°πÁõÆÂ∑≤ÂåÖÂê´&lt;code&gt;.github/workflows/crawler.yml&lt;/code&gt;ÈÖçÁΩÆÊñá‰ª∂ÔºåÈªòËÆ§ÊØèÂ∞èÊó∂ËøêË°å‰∏ÄÊ¨°&lt;/li&gt; 
   &lt;li&gt;‰Ω†‰πüÂèØ‰ª•Âú® GitHub ‰ªìÂ∫ìÁöÑ Actions È°µÈù¢ÊâãÂä®Ëß¶ÂèëËøêË°å&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Êü•ÁúãÁªìÊûú&lt;/strong&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;ËøêË°åÁªìÊûúÂ∞ÜËá™Âä®‰øùÂ≠òÂú®‰ªìÂ∫ìÁöÑ&lt;code&gt;output&lt;/code&gt;ÁõÆÂΩï‰∏≠&lt;/li&gt; 
   &lt;li&gt;ÂêåÊó∂ÈÄöËøáÈÖçÁΩÆÁöÑÊú∫Âô®‰∫∫ÂèëÈÄÅÈÄöÁü•Âà∞‰Ω†ÁöÑÁæ§ÁªÑ&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üëâ Ëá™ÂÆö‰πâÁõëÊéßÂπ≥Âè∞&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h3&gt;üîß Ëá™ÂÆö‰πâÁõëÊéßÂπ≥Âè∞&lt;/h3&gt; 
 &lt;p&gt;Êú¨È°πÁõÆÁöÑËµÑËÆØÊï∞ÊçÆÊù•Ê∫ê‰∫é &lt;a href="https://github.com/ourongxing/newsnow"&gt;newsnow&lt;/a&gt; Ôºå‰Ω†ÂèØ‰ª•ÁÇπÂáª&lt;a href="https://newsnow.busiyi.world/"&gt;ÁΩëÁ´ô&lt;/a&gt;ÔºåÁÇπÂáª[Êõ¥Â§ö]ÔºåÊü•ÁúãÊòØÂê¶Êúâ‰Ω†ÊÉ≥Ë¶ÅÁöÑÂπ≥Âè∞„ÄÇ&lt;/p&gt; 
 &lt;p&gt;ÂÖ∑‰ΩìÊ∑ªÂä†ÂèØËÆøÈóÆ &lt;a href="https://github.com/ourongxing/newsnow/tree/main/server/sources"&gt;È°πÁõÆÊ∫ê‰ª£Á†Å&lt;/a&gt;ÔºåÊ†πÊçÆÈáåÈù¢ÁöÑÊñá‰ª∂ÂêçÔºåÂú® &lt;code&gt;config/config.yaml&lt;/code&gt; Êñá‰ª∂‰∏≠‰øÆÊîπ &lt;code&gt;platforms&lt;/code&gt; ÈÖçÁΩÆÔºö&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-yaml"&gt;platforms:
  - id: "toutiao"
    name: "‰ªäÊó•Â§¥Êù°"
  - id: "baidu"  
    name: "ÁôæÂ∫¶ÁÉ≠Êêú"
  - id: "wallstreetcn-hot"
    name: "ÂçéÂ∞îË°óËßÅÈóª"
  # Ê∑ªÂä†Êõ¥Â§öÂπ≥Âè∞...
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üëâ Docker ÈÉ®ÁΩ≤&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h3&gt;üê≥ Docker ÈÉ®ÁΩ≤&lt;/h3&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;ÂÖãÈöÜÈ°πÁõÆÂπ∂ËøõÂÖ•ÁõÆÂΩï&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/sansan0/TrendRadar.git
cd TrendRadar
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;ÈÖçÁΩÆÊñá‰ª∂&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;‰øÆÊîπ &lt;code&gt;config/config.yaml&lt;/code&gt; Âíå &lt;code&gt;config/frequency_words.txt&lt;/code&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;strong&gt;Êé®ÈÄÅÈìæÊé•Â°´ÂÜô&lt;/strong&gt;Ôºå&lt;strong&gt;ËÆæÁΩÆÊé®ÈÄÅÂÆöÊó∂&lt;/strong&gt;ÂèØÈÄöËøá .env ËøõË°åÈÖçÁΩÆ&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;ÂêØÂä®ÊúçÂä°&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cd docker
docker-compose up -d
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;ÁÆ°ÁêÜÊúçÂä°&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# Êü•ÁúãËøêË°åÁä∂ÊÄÅ
docker exec -it trend-radar python manage.py status

# ÊâãÂä®ÊâßË°å‰∏ÄÊ¨°Áà¨Ëô´
docker exec -it trend-radar python manage.py run

# Êü•ÁúãÂÆûÊó∂Êó•Âøó
docker exec -it trend-radar python manage.py logs

# ÊòæÁ§∫ÂΩìÂâçÈÖçÁΩÆ
docker exec -it trend-radar python manage.py config

# ÊòæÁ§∫ËæìÂá∫Êñá‰ª∂
docker exec -it trend-radar python manage.py files

# ÈáçÂêØÂÆöÊó∂ÊúçÂä°
docker exec -it trend-radar python manage.py restart

# ÊòæÁ§∫Â∏ÆÂä©‰ø°ÊÅØ
docker exec -it trend-radar python manage.py help
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/details&gt; 
&lt;h2&gt;‚òï Â≠¶‰π†‰∫§ÊµÅ‰∏é1ÂÖÉÁÇπËµû&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ÂøÉÊÑèÂà∞Â∞±Ë°åÔºåÊî∂Âà∞ÁöÑÁÇπËµûÁî®‰∫éÊèêÈ´òÂºÄÂèëËÄÖÂºÄÊ∫êÁöÑÁßØÊûÅÊÄß&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;ÂÖ¨‰ºóÂè∑ÂÖ≥Ê≥®&lt;/th&gt; 
    &lt;th align="center"&gt;ÂæÆ‰ø°ÁÇπËµû&lt;/th&gt; 
    &lt;th align="center"&gt;ÊîØ‰ªòÂÆùÁÇπËµû&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/sansan0/TrendRadar/master/_image/weixin.png" width="300" title="Á°ÖÂü∫Ëå∂Ê∞¥Èó¥" /&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;img src="https://cdn-1258574687.cos.ap-shanghai.myqcloud.com/img/%2F2025%2F07%2F17%2F2ae0a88d98079f7e876c2b4dc85233c6-9e8025.JPG" width="300" title="ÂæÆ‰ø°ÊîØ‰ªò" /&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;img src="https://cdn-1258574687.cos.ap-shanghai.myqcloud.com/img/%2F2025%2F07%2F17%2Fed4f20ab8e35be51f8e84c94e6e239b4-fe4947.JPG" width="300" title="ÊîØ‰ªòÂÆùÊîØ‰ªò" /&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üëâ ÂæÆ‰ø°Êé®ÈÄÅÈÄöÁü•ÁöÑÊäò‰∏≠ÊñπÊ°à&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;Áî±‰∫éËØ•ÊñπÊ°àÊòØÂü∫‰∫é‰ºÅ‰∏öÂæÆ‰ø°ÁöÑÊèí‰ª∂Êú∫Âà∂ÔºåÊé®ÈÄÅÊ†∑Âºè‰πüÂçÅÂàÜ‰∏çÂêåÔºåÊâÄ‰ª•Áõ∏ÂÖ≥ÂÆûÁé∞ÊàëÊöÇÊó∂‰∏çÂáÜÂ§áÁ∫≥ÂÖ•ÂΩìÂâçÈ°πÁõÆ&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;ul&gt; 
  &lt;li&gt;fork Ëøô‰ΩçÂÖÑÂè∞ÁöÑÈ°πÁõÆ &lt;a href="https://github.com/jayzqj/TrendRadar"&gt;https://github.com/jayzqj/TrendRadar&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;ÂÆåÊàê‰∏äÊñπÁöÑ‰ºÅ‰∏öÂæÆ‰ø°Êé®ÈÄÅËÆæÁΩÆ&lt;/li&gt; 
  &lt;li&gt;ÊåâÁÖß‰∏ãÈù¢ÂõæÁâáÊìç‰Ωú&lt;/li&gt; 
  &lt;li&gt;ÈÖçÁΩÆÂ•ΩÂêéÔºåÊâãÊú∫‰∏äÁöÑ‰ºÅ‰∏öÂæÆ‰ø° app Âà†Èô§Êéâ‰πüÊ≤°‰∫ã&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;img src="https://raw.githubusercontent.com/sansan0/TrendRadar/master/_image/wework.png" title="github" /&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üëâ Êú¨È°πÁõÆÊµÅÁ®ãÂõæ&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-mermaid"&gt;flowchart TD
    A[üë§ Áî®Êà∑ÂºÄÂßã] --&amp;gt; B[üç¥ Fork È°πÁõÆ]
    B --&amp;gt; C[‚öôÔ∏è ÈÄâÊã©ÈÄöÁü•ÊñπÂºè]
    
    C --&amp;gt; D1[üì± ‰ºÅ‰∏öÂæÆ‰ø°Áæ§Êú∫Âô®‰∫∫&amp;lt;br/&amp;gt;ÊúÄÁÆÄÂçïÂø´ÈÄü]
    C --&amp;gt; D2[üí¨ È£û‰π¶Êú∫Âô®‰∫∫&amp;lt;br/&amp;gt;ÊòæÁ§∫ÊïàÊûúÊúÄ‰Ω≥]
    C --&amp;gt; D3[üîî ÈíâÈíâÊú∫Âô®‰∫∫&amp;lt;br/&amp;gt;]
    C --&amp;gt; D4[üìü Telegram Bot&amp;lt;br/&amp;gt;]
    
    D1 --&amp;gt; E[üîë ÈÖçÁΩÆ GitHub Secrets&amp;lt;br/&amp;gt;Â°´ÂÖ•Êú∫Âô®‰∫∫ Webhook Âú∞ÂùÄ]
    D2 --&amp;gt; E
    D3 --&amp;gt; E  
    D4 --&amp;gt; E
    
    E --&amp;gt; F[üìù ÁºñËæëÂÖ≥ÈîÆËØçÈÖçÁΩÆ&amp;lt;br/&amp;gt;config/frequency_words.txt&amp;lt;br/&amp;gt;Ê∑ªÂä†‰Ω†ÂÖ≥ÂøÉÁöÑËØçÊ±á]
    F --&amp;gt; G[üéØ ÈÄâÊã©ËøêË°åÊ®°Âºè&amp;lt;br/&amp;gt;config/config.yaml&amp;lt;br/&amp;gt;daily/current/incremental]
    
    G --&amp;gt; H[‚úÖ ÈÖçÁΩÆÂÆåÊàê]
    H --&amp;gt; I[ü§ñ Á≥ªÁªüÊ†πÊçÆËÆæÂÆöÊó∂Èó¥Ëá™Âä®ËøêË°å]
    
    I --&amp;gt; J[üìä Áà¨ÂèñÂêÑÂ§ßÂπ≥Âè∞ÁÉ≠ÁÇπ]
    J --&amp;gt; K[üîç Ê†πÊçÆÂÖ≥ÈîÆËØçÁ≠õÈÄâ]
    K --&amp;gt; L[üì± Êé®ÈÄÅÂà∞‰Ω†ÁöÑÊâãÊú∫]
    
    L --&amp;gt; M[üìà Êü•ÁúãÊé®ÈÄÅÁªìÊûú]
    M --&amp;gt; N{Êª°ÊÑèÊïàÊûú?}
    N --&amp;gt;|‰∏çÊª°ÊÑè| F
    N --&amp;gt;|Êª°ÊÑè| O[üéâ ÊåÅÁª≠Êé•Êî∂Á≤æÂáÜÊé®ÈÄÅ]
    
    style A fill:#e1f5fe
    style B fill:#f3e5f5
    style E fill:#fff3e0
    style F fill:#e8f5e8
    style G fill:#e8f5e8
    style L fill:#ffebee
    style O fill:#e8f5e8
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p&gt;&lt;a href="https://www.star-history.com/#sansan0/TrendRadar&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=sansan0/TrendRadar&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;üìÑ ËÆ∏ÂèØËØÅ&lt;/h2&gt; 
&lt;p&gt;GPL-3.0 License&lt;/p&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;strong&gt;‚≠ê Â¶ÇÊûúËøô‰∏™Â∑•ÂÖ∑ÂØπ‰Ω†ÊúâÂ∏ÆÂä©ÔºåËØ∑ÁªôÈ°πÁõÆÁÇπ‰∏™ Star ÊîØÊåÅÂºÄÂèëÔºÅ&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/sansan0/TrendRadar/master/#trendradar"&gt;üîù ÂõûÂà∞È°∂ÈÉ®&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt;</description>
    </item>
    
  </channel>
</rss>