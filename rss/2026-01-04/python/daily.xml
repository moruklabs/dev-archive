<rss version="2.0">
  <channel>
    <title>GitHub Python Daily Trending</title>
    <description>Daily Trending of Python in GitHub</description>
    <pubDate>Sat, 03 Jan 2026 01:39:30 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>yichuan-w/LEANN</title>
      <link>https://github.com/yichuan-w/LEANN</link>
      <description>&lt;p&gt;RAG on Everything with LEANN. Enjoy 97% storage savings while running a fast, accurate, and 100% private RAG application on your personal device.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/yichuan-w/LEANN/main/assets/logo-text.png" alt="LEANN Logo" width="400" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://img.shields.io/badge/Python-3.9%20%7C%203.10%20%7C%203.11%20%7C%203.12%20%7C%203.13-blue.svg?sanitize=true" alt="Python Versions" /&gt; &lt;img src="https://github.com/yichuan-w/LEANN/actions/workflows/build-and-publish.yml/badge.svg?sanitize=true" alt="CI Status" /&gt; &lt;img src="https://img.shields.io/badge/Platform-Ubuntu%20%26%20Arch%20%26%20WSL%20%7C%20macOS%20(ARM64%2FIntel)-lightgrey" alt="Platform" /&gt; &lt;img src="https://img.shields.io/badge/License-MIT-green.svg?sanitize=true" alt="MIT License" /&gt; &lt;img src="https://img.shields.io/badge/MCP-Native%20Integration-blue" alt="MCP Integration" /&gt; &lt;a href="https://join.slack.com/t/leann-e2u9779/shared_invite/zt-3ckd2f6w1-OX08~NN4gkWhh10PRVBj1Q"&gt; &lt;img src="https://img.shields.io/badge/Slack-Join-4A154B?logo=slack&amp;amp;logoColor=white" alt="Join Slack" /&gt; &lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/assets/wechat_user_group.JPG" title="Join WeChat group"&gt; &lt;img src="https://img.shields.io/badge/WeChat-Join-2DC100?logo=wechat&amp;amp;logoColor=white" alt="Join WeChat group" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://forms.gle/rDbZf864gMNxhpTq8"&gt; &lt;img src="https://img.shields.io/badge/üì£_Community_Survey-Help_Shape_v0.4-007ec6?style=for-the-badge&amp;amp;logo=google-forms&amp;amp;logoColor=white" alt="Take Survey" /&gt; &lt;/a&gt; 
 &lt;p&gt; We track &lt;b&gt;zero telemetry&lt;/b&gt;. This survey is the ONLY way to tell us if you want &lt;br /&gt; &lt;b&gt;GPU Acceleration&lt;/b&gt; or &lt;b&gt;More Integrations&lt;/b&gt; next.&lt;br /&gt; üëâ &lt;a href="https://forms.gle/rDbZf864gMNxhpTq8"&gt;&lt;b&gt;Click here to cast your vote (2 mins)&lt;/b&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2 align="center" tabindex="-1" class="heading-element" dir="auto"&gt; The smallest vector index in the world. RAG Everything with LEANN! &lt;/h2&gt; 
&lt;p&gt;LEANN is an innovative vector database that democratizes personal AI. Transform your laptop into a powerful RAG system that can index and search through millions of documents while using &lt;strong&gt;97% less storage&lt;/strong&gt; than traditional solutions &lt;strong&gt;without accuracy loss&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;LEANN achieves this through &lt;em&gt;graph-based selective recomputation&lt;/em&gt; with &lt;em&gt;high-degree preserving pruning&lt;/em&gt;, computing embeddings on-demand instead of storing them all. &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#%EF%B8%8F-architecture--how-it-works"&gt;Illustration Fig ‚Üí&lt;/a&gt; | &lt;a href="https://arxiv.org/abs/2506.08276"&gt;Paper ‚Üí&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Ready to RAG Everything?&lt;/strong&gt; Transform your laptop into a personal AI assistant that can semantic search your &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#-personal-data-manager-process-any-documents-pdf-txt-md"&gt;file system&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#-your-personal-email-secretary-rag-on-apple-mail"&gt;emails&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#-time-machine-for-the-web-rag-your-entire-browser-history"&gt;browser history&lt;/a&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#-wechat-detective-unlock-your-golden-memories"&gt;chat history&lt;/a&gt;&lt;/strong&gt; (&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#-wechat-detective-unlock-your-golden-memories"&gt;WeChat&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#-imessage-history-your-personal-conversation-archive"&gt;iMessage&lt;/a&gt;), &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#-chatgpt-chat-history-your-personal-ai-conversation-archive"&gt;agent memory&lt;/a&gt;&lt;/strong&gt; (&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#-chatgpt-chat-history-your-personal-ai-conversation-archive"&gt;ChatGPT&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#-claude-chat-history-your-personal-ai-conversation-archive"&gt;Claude&lt;/a&gt;), &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#mcp-integration-rag-on-live-data-from-any-platform"&gt;live data&lt;/a&gt;&lt;/strong&gt; (&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#slack-messages-search-your-team-conversations"&gt;Slack&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#-twitter-bookmarks-your-personal-tweet-library"&gt;Twitter&lt;/a&gt;), &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#-claude-code-integration-transform-your-development-workflow"&gt;codebase&lt;/a&gt;&lt;/strong&gt;* , or external knowledge bases (i.e., 60M documents) - all on your laptop, with zero cloud costs and complete privacy.&lt;/p&gt; 
&lt;p&gt;* Claude Code only supports basic &lt;code&gt;grep&lt;/code&gt;-style keyword search. &lt;strong&gt;LEANN&lt;/strong&gt; is a drop-in &lt;strong&gt;semantic search MCP service fully compatible with Claude Code&lt;/strong&gt;, unlocking intelligent retrieval without changing your workflow. üî• Check out &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/packages/leann-mcp/README.md"&gt;the easy setup ‚Üí&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Why LEANN?&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/yichuan-w/LEANN/main/assets/effects.png" alt="LEANN vs Traditional Vector DB Storage Comparison" width="70%" /&gt; &lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;The numbers speak for themselves:&lt;/strong&gt; Index 60 million text chunks in just 6GB instead of 201GB. From emails to browser history, everything fits on your laptop. &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/#-storage-comparison"&gt;See detailed benchmarks for different applications below ‚Üì&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;üîí &lt;strong&gt;Privacy:&lt;/strong&gt; Your data never leaves your laptop. No OpenAI, no cloud, no "terms of service".&lt;/p&gt; 
&lt;p&gt;ü™∂ &lt;strong&gt;Lightweight:&lt;/strong&gt; Graph-based recomputation eliminates heavy embedding storage, while smart graph pruning and CSR format minimize graph storage overhead. Always less storage, less memory usage!&lt;/p&gt; 
&lt;p&gt;üì¶ &lt;strong&gt;Portable:&lt;/strong&gt; Transfer your entire knowledge base between devices (even with others) with minimal cost - your personal AI memory travels with you.&lt;/p&gt; 
&lt;p&gt;üìà &lt;strong&gt;Scalability:&lt;/strong&gt; Handle messy personal data that would crash traditional vector DBs, easily managing your growing personalized data and agent generated memory!&lt;/p&gt; 
&lt;p&gt;‚ú® &lt;strong&gt;No Accuracy Loss:&lt;/strong&gt; Maintain the same search quality as heavyweight solutions while using 97% less storage.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;üì¶ Prerequisites: Install uv&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://docs.astral.sh/uv/getting-started/installation/#installation-methods"&gt;Install uv&lt;/a&gt; first if you don't have it. Typically, you can install it with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -LsSf https://astral.sh/uv/install.sh | sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;üöÄ Quick Install&lt;/h3&gt; 
&lt;p&gt;Clone the repository to access all examples and try amazing applications,&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/yichuan-w/LEANN.git leann
cd leann
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;and install LEANN from &lt;a href="https://pypi.org/project/leann/"&gt;PyPI&lt;/a&gt; to run them immediately:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv venv
source .venv/bin/activate
uv pip install leann
&lt;/code&gt;&lt;/pre&gt; 
&lt;!--
&gt; Low-resource? See "Low-resource setups" in the [Configuration Guide](docs/configuration-guide.md#low-resource-setups). --&gt; 
&lt;details&gt; 
 &lt;summary&gt; &lt;strong&gt;üîß Build from Source (Recommended for development)&lt;/strong&gt; &lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/yichuan-w/LEANN.git leann
cd leann
git submodule update --init --recursive
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;macOS:&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;Note: DiskANN requires MacOS 13.3 or later.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;brew install libomp boost protobuf zeromq pkgconf
uv sync --extra diskann
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Linux (Ubuntu/Debian):&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;Note: On Ubuntu 20.04, you may need to build a newer Abseil and pin Protobuf (e.g., v3.20.x) for building DiskANN. See &lt;a href="https://github.com/yichuan-w/LEANN/issues/30"&gt;Issue #30&lt;/a&gt; for a step-by-step note.&lt;/p&gt; 
 &lt;p&gt;You can manually install &lt;a href="https://www.intel.com/content/www/us/en/developer/tools/oneapi/onemkl.html"&gt;Intel oneAPI MKL&lt;/a&gt; instead of &lt;code&gt;libmkl-full-dev&lt;/code&gt; for DiskANN. You can also use &lt;code&gt;libopenblas-dev&lt;/code&gt; for building HNSW only, by removing &lt;code&gt;--extra diskann&lt;/code&gt; in the command below.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;sudo apt-get update &amp;amp;&amp;amp; sudo apt-get install -y \
  libomp-dev libboost-all-dev protobuf-compiler libzmq3-dev \
  pkg-config libabsl-dev libaio-dev libprotobuf-dev \
  libmkl-full-dev

uv sync --extra diskann
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Linux (Arch Linux):&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;sudo pacman -Syu &amp;amp;&amp;amp; sudo pacman -S --needed base-devel cmake pkgconf git gcc \
  boost boost-libs protobuf abseil-cpp libaio zeromq

# For MKL in DiskANN
sudo pacman -S --needed base-devel git
git clone https://aur.archlinux.org/paru-bin.git
cd paru-bin &amp;amp;&amp;amp; makepkg -si
paru -S intel-oneapi-mkl intel-oneapi-compiler
source /opt/intel/oneapi/setvars.sh

uv sync --extra diskann
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Linux (RHEL / CentOS Stream / Oracle / Rocky / AlmaLinux):&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;See &lt;a href="https://github.com/yichuan-w/LEANN/issues/50"&gt;Issue #50&lt;/a&gt; for more details.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;sudo dnf groupinstall -y "Development Tools"
sudo dnf install -y libomp-devel boost-devel protobuf-compiler protobuf-devel \
  abseil-cpp-devel libaio-devel zeromq-devel pkgconf-pkg-config

# For MKL in DiskANN
sudo dnf install -y intel-oneapi-mkl intel-oneapi-mkl-devel \
  intel-oneapi-openmp || sudo dnf install -y intel-oneapi-compiler
source /opt/intel/oneapi/setvars.sh

uv sync --extra diskann
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;p&gt;Our declarative API makes RAG as easy as writing a config file.&lt;/p&gt; 
&lt;p&gt;Check out &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/demo.ipynb"&gt;demo.ipynb&lt;/a&gt; or &lt;a href="https://colab.research.google.com/github/yichuan-w/LEANN/blob/main/demo.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from leann import LeannBuilder, LeannSearcher, LeannChat
from pathlib import Path
INDEX_PATH = str(Path("./").resolve() / "demo.leann")

# Build an index
builder = LeannBuilder(backend_name="hnsw")
builder.add_text("LEANN saves 97% storage compared to traditional vector databases.")
builder.add_text("Tung Tung Tung Sahur called‚Äîthey need their banana‚Äëcrocodile hybrid back")
builder.build_index(INDEX_PATH)

# Search
searcher = LeannSearcher(INDEX_PATH)
results = searcher.search("fantastical AI-generated creatures", top_k=1)

# Chat with your data
chat = LeannChat(INDEX_PATH, llm_config={"type": "hf", "model": "Qwen/Qwen3-0.6B"})
response = chat.ask("How much storage does LEANN save?", top_k=1)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Performance Optimization: Task-Specific Prompt Templates&lt;/h2&gt; 
&lt;p&gt;LEANN now supports prompt templates for task-specific embedding models like Google's EmbeddingGemma. This feature enables &lt;strong&gt;significant performance gains&lt;/strong&gt; by using smaller, faster models without sacrificing search quality.&lt;/p&gt; 
&lt;h3&gt;Real-World Performance&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Benchmark (MacBook M1 Pro, LM Studio):&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;EmbeddingGemma 300M (QAT)&lt;/strong&gt; with templates: &lt;strong&gt;4-5x faster&lt;/strong&gt; than Qwen 600M&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Search quality:&lt;/strong&gt; Identical ranking to larger models&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Use case:&lt;/strong&gt; Ideal for real-time workflows (e.g., pre-commit hooks in Claude Code; ~7min for whole LEANN's code + doc files on MacBook M1 Pro)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Quick Example&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Build index with task-specific templates
leann build my-index ./docs \
  --embedding-mode ollama \
  --embedding-model embeddinggemma \
  --embedding-prompt-template "title: none | text: " \
  --query-prompt-template "task: search result | query: "

# Search automatically applies query template
leann search my-index "How does LEANN optimize vector search?"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Templates are automatically persisted and applied during searches (CLI, MCP, API). No manual configuration needed after indexing.&lt;/p&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/docs/configuration-guide.md#task-specific-prompt-templates"&gt;Configuration Guide&lt;/a&gt; for detailed usage and model recommendations.&lt;/p&gt; 
&lt;h2&gt;RAG on Everything!&lt;/h2&gt; 
&lt;p&gt;LEANN supports RAG on various data sources including documents (&lt;code&gt;.pdf&lt;/code&gt;, &lt;code&gt;.txt&lt;/code&gt;, &lt;code&gt;.md&lt;/code&gt;), Apple Mail, Google Search History, WeChat, ChatGPT conversations, Claude conversations, iMessage conversations, and &lt;strong&gt;live data from any platform through MCP (Model Context Protocol) servers&lt;/strong&gt; - including Slack, Twitter, and more.&lt;/p&gt; 
&lt;h3&gt;Generation Model Setup&lt;/h3&gt; 
&lt;h4&gt;LLM Backend&lt;/h4&gt; 
&lt;p&gt;LEANN supports many LLM providers for text generation (HuggingFace, Ollama, Anthropic, and Any OpenAI compatible API).&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üîë OpenAI API Setup (Default)&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;Set your OpenAI API key as an environment variable:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;export OPENAI_API_KEY="your-api-key-here"
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Make sure to use &lt;code&gt;--llm openai&lt;/code&gt; flag when using the CLI. You can also specify the model name with &lt;code&gt;--llm-model &amp;lt;model-name&amp;gt;&lt;/code&gt; flag.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üõ†Ô∏è Supported LLM &amp;amp; Embedding Providers (via OpenAI Compatibility)&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;Thanks to the widespread adoption of the OpenAI API format, LEANN is compatible out-of-the-box with a vast array of LLM and embedding providers. Simply set the &lt;code&gt;OPENAI_BASE_URL&lt;/code&gt; and &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; environment variables to connect to your preferred service.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-sh"&gt;export OPENAI_API_KEY="xxx"
export OPENAI_BASE_URL="http://localhost:1234/v1" # base url of the provider
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;To use OpenAI compatible endpoint with the CLI interface:&lt;/p&gt; 
 &lt;p&gt;If you are using it for text generation, make sure to use &lt;code&gt;--llm openai&lt;/code&gt; flag and specify the model name with &lt;code&gt;--llm-model &amp;lt;model-name&amp;gt;&lt;/code&gt; flag.&lt;/p&gt; 
 &lt;p&gt;If you are using it for embedding, set the &lt;code&gt;--embedding-mode openai&lt;/code&gt; flag and specify the model name with &lt;code&gt;--embedding-model &amp;lt;MODEL&amp;gt;&lt;/code&gt;.&lt;/p&gt; 
 &lt;hr /&gt; 
 &lt;p&gt;Below is a list of base URLs for common providers to get you started.&lt;/p&gt; 
 &lt;h3&gt;üñ•Ô∏è Local Inference Engines (Recommended for full privacy)&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Provider&lt;/th&gt; 
    &lt;th&gt;Sample Base URL&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Ollama&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;http://localhost:11434/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;LM Studio&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;http://localhost:1234/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;vLLM&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;http://localhost:8000/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;llama.cpp&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;http://localhost:8080/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;SGLang&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;http://localhost:30000/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;LiteLLM&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;http://localhost:4000&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;hr /&gt; 
 &lt;h3&gt;‚òÅÔ∏è Cloud Providers&lt;/h3&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;&lt;strong&gt;üö® A Note on Privacy:&lt;/strong&gt; Before choosing a cloud provider, carefully review their privacy and data retention policies. Depending on their terms, your data may be used for their own purposes, including but not limited to human reviews and model training, which can lead to serious consequences if not handled properly.&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Provider&lt;/th&gt; 
    &lt;th&gt;Base URL&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;OpenAI&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;https://api.openai.com/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;OpenRouter&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;https://openrouter.ai/api/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Gemini&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;https://generativelanguage.googleapis.com/v1beta/openai/&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;x.AI (Grok)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;https://api.x.ai/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Groq AI&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;https://api.groq.com/openai/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;DeepSeek&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;https://api.deepseek.com/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;SiliconFlow&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;https://api.siliconflow.cn/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Zhipu (BigModel)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;https://open.bigmodel.cn/api/paas/v4/&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Mistral AI&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;https://api.mistral.ai/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Anthropic&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;https://api.anthropic.com/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Jina AI&lt;/strong&gt; (Embeddings)&lt;/td&gt; 
    &lt;td&gt;&lt;code&gt;https://api.jina.ai/v1&lt;/code&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;&lt;strong&gt;üí° Tip: Separate Embedding Provider&lt;/strong&gt;&lt;/p&gt; 
  &lt;p&gt;To use a different provider for embeddings (e.g., Jina AI) while using another for LLM, use &lt;code&gt;--embedding-api-base&lt;/code&gt; and &lt;code&gt;--embedding-api-key&lt;/code&gt;:&lt;/p&gt; 
  &lt;pre&gt;&lt;code class="language-bash"&gt;leann build my-index --docs ./docs \
  --embedding-mode openai \
  --embedding-model jina-embeddings-v3 \
  --embedding-api-base https://api.jina.ai/v1 \
  --embedding-api-key $JINA_API_KEY
&lt;/code&gt;&lt;/pre&gt; 
 &lt;/blockquote&gt; 
 &lt;p&gt;If your provider isn't on this list, don't worry! Check their documentation for an OpenAI-compatible endpoint‚Äîchances are, it's OpenAI Compatible too!&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üîß Ollama Setup (Recommended for full privacy)&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;macOS:&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;First, &lt;a href="https://ollama.com/download/mac"&gt;download Ollama for macOS&lt;/a&gt;.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Pull a lightweight model (recommended for consumer hardware)
ollama pull llama3.2:1b
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Linux:&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Install Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Start Ollama service manually
ollama serve &amp;amp;

# Pull a lightweight model (recommended for consumer hardware)
ollama pull llama3.2:1b
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;‚≠ê Flexible Configuration&lt;/h2&gt; 
&lt;p&gt;LEANN provides flexible parameters for embedding models, search strategies, and data processing to fit your specific needs.&lt;/p&gt; 
&lt;p&gt;üìö &lt;strong&gt;Need configuration best practices?&lt;/strong&gt; Check our &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/docs/configuration-guide.md"&gt;Configuration Guide&lt;/a&gt; for detailed optimization tips, model selection advice, and solutions to common issues like slow embeddings or poor search quality.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìã Click to expand: Common Parameters (Available in All Examples)&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;All RAG examples share these common parameters. &lt;strong&gt;Interactive mode&lt;/strong&gt; is available in all examples - simply run without &lt;code&gt;--query&lt;/code&gt; to start a continuous Q&amp;amp;A session where you can ask multiple questions. Type 'quit' to exit.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Environment Variables (GPU Device Selection)
LEANN_EMBEDDING_DEVICE       # GPU for embedding model (e.g., cuda:0, cuda:1, cpu)
LEANN_LLM_DEVICE             # GPU for HFChat LLM (e.g., cuda:1, or "cuda" for multi-GPU auto)

# Core Parameters (General preprocessing for all examples)
--index-dir DIR              # Directory to store the index (default: current directory)
--query "YOUR QUESTION"      # Single query mode. Omit for interactive chat (type 'quit' to exit), and now you can play with your index interactively
--max-items N                # Limit data preprocessing (default: -1, process all data)
--force-rebuild              # Force rebuild index even if it exists

# Embedding Parameters
--embedding-model MODEL      # e.g., facebook/contriever, text-embedding-3-small, mlx-community/Qwen3-Embedding-0.6B-8bit or nomic-embed-text
--embedding-mode MODE        # sentence-transformers, openai, mlx, or ollama

# LLM Parameters (Text generation models)
--llm TYPE                   # LLM backend: openai, ollama, hf, or anthropic (default: openai)
--llm-model MODEL            # Model name (default: gpt-4o) e.g., gpt-4o-mini, llama3.2:1b, Qwen/Qwen2.5-1.5B-Instruct
--thinking-budget LEVEL      # Thinking budget for reasoning models: low/medium/high (supported by o3, o3-mini, GPT-Oss:20b, and other reasoning models)

# Search Parameters
--top-k N                    # Number of results to retrieve (default: 20)
--search-complexity N        # Search complexity for graph traversal (default: 32)

# Chunking Parameters
--chunk-size N               # Size of text chunks (default varies by source: 256 for most, 192 for WeChat)
--chunk-overlap N            # Overlap between chunks (default varies: 25-128 depending on source)

# Index Building Parameters
--backend-name NAME          # Backend to use: hnsw or diskann (default: hnsw)
--graph-degree N             # Graph degree for index construction (default: 32)
--build-complexity N         # Build complexity for index construction (default: 64)
--compact / --no-compact     # Use compact storage (default: true). Must be `no-compact` for `no-recompute` build.
--recompute / --no-recompute # Enable/disable embedding recomputation (default: enabled). Should not do a `no-recompute` search in a `recompute` build.
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;üìÑ Personal Data Manager: Process Any Documents (&lt;code&gt;.pdf&lt;/code&gt;, &lt;code&gt;.txt&lt;/code&gt;, &lt;code&gt;.md&lt;/code&gt;)!&lt;/h3&gt; 
&lt;p&gt;Ask questions directly about your personal PDFs, documents, and any directory containing your files!&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/yichuan-w/LEANN/main/videos/paper_clear.gif" alt="LEANN Document Search Demo" width="600" /&gt; &lt;/p&gt; 
&lt;p&gt;The example below asks a question about summarizing our paper (uses default data in &lt;code&gt;data/&lt;/code&gt;, which is a directory with diverse data sources: two papers, Pride and Prejudice, and a Technical report about LLM in Huawei in Chinese), and this is the &lt;strong&gt;easiest example&lt;/strong&gt; to run here:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;source .venv/bin/activate # Don't forget to activate the virtual environment
python -m apps.document_rag --query "What are the main techniques LEANN explores?"
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìã Click to expand: Document-Specific Arguments&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h4&gt;Parameters&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;--data-dir DIR           # Directory containing documents to process (default: data)
--file-types .ext .ext   # Filter by specific file types (optional - all LlamaIndex supported types if omitted)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Example Commands&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Process all documents with larger chunks for academic papers
python -m apps.document_rag --data-dir "~/Documents/Papers" --chunk-size 1024

# Filter only markdown and Python files with smaller chunks
python -m apps.document_rag --data-dir "./docs" --chunk-size 256 --file-types .md .py

# Enable AST-aware chunking for code files
python -m apps.document_rag --enable-code-chunking --data-dir "./my_project"

# Or use the specialized code RAG for better code understanding
python -m apps.code_rag --repo-dir "./my_codebase" --query "How does authentication work?"
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;üé® ColQwen: Multimodal PDF Retrieval with Vision-Language Models&lt;/h3&gt; 
&lt;p&gt;Search through PDFs using both text and visual understanding with ColQwen2/ColPali models. Perfect for research papers, technical documents, and any PDFs with complex layouts, figures, or diagrams.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;üçé Mac Users&lt;/strong&gt;: ColQwen is optimized for Apple Silicon with MPS acceleration for faster inference!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Build index from PDFs
python -m apps.colqwen_rag build --pdfs ./my_papers/ --index research_papers

# Search with text queries
python -m apps.colqwen_rag search research_papers "How does attention mechanism work?"

# Interactive Q&amp;amp;A
python -m apps.colqwen_rag ask research_papers --interactive
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìã Click to expand: ColQwen Setup &amp;amp; Usage&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h4&gt;Prerequisites&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Install dependencies
uv pip install colpali_engine pdf2image pillow matplotlib qwen_vl_utils einops seaborn
brew install poppler  # macOS only, for PDF processing
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Build Index&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;python -m apps.colqwen_rag build \
  --pdfs ./pdf_directory/ \
  --index my_index \
  --model colqwen2  # or colpali
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Search&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;python -m apps.colqwen_rag search my_index "your question here" --top-k 5
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Models&lt;/h4&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;ColQwen2&lt;/strong&gt; (&lt;code&gt;colqwen2&lt;/code&gt;): Latest vision-language model with improved performance&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;ColPali&lt;/strong&gt; (&lt;code&gt;colpali&lt;/code&gt;): Proven multimodal retriever&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;For detailed usage, see the &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/docs/COLQWEN_GUIDE.md"&gt;ColQwen Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h3&gt;üìß Your Personal Email Secretary: RAG on Apple Mail!&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; The examples below currently support macOS only. Windows support coming soon.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/yichuan-w/LEANN/main/videos/mail_clear.gif" alt="LEANN Email Search Demo" width="600" /&gt; &lt;/p&gt; 
&lt;p&gt;Before running the example below, you need to grant full disk access to your terminal/VS Code in System Preferences ‚Üí Privacy &amp;amp; Security ‚Üí Full Disk Access.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m apps.email_rag --query "What's the food I ordered by DoorDash or Uber Eats mostly?"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;780K email chunks ‚Üí 78MB storage.&lt;/strong&gt; Finally, search your email like you search Google.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìã Click to expand: Email-Specific Arguments&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h4&gt;Parameters&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;--mail-path PATH         # Path to specific mail directory (auto-detects if omitted)
--include-html          # Include HTML content in processing (useful for newsletters)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Example Commands&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Search work emails from a specific account
python -m apps.email_rag --mail-path "~/Library/Mail/V10/WORK_ACCOUNT"

# Find all receipts and order confirmations (includes HTML)
python -m apps.email_rag --query "receipt order confirmation invoice" --include-html
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìã Click to expand: Example queries you can try&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;Once the index is built, you can ask questions like:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;"Find emails from my boss about deadlines"&lt;/li&gt; 
  &lt;li&gt;"What did John say about the project timeline?"&lt;/li&gt; 
  &lt;li&gt;"Show me emails about travel expenses"&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;üîç Time Machine for the Web: RAG Your Entire Chrome Browser History!&lt;/h3&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/yichuan-w/LEANN/main/videos/google_clear.gif" alt="LEANN Browser History Search Demo" width="600" /&gt; &lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m apps.browser_rag --query "Tell me my browser history about machine learning?"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;38K browser entries ‚Üí 6MB storage.&lt;/strong&gt; Your browser history becomes your personal search engine.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìã Click to expand: Browser-Specific Arguments&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h4&gt;Parameters&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;--chrome-profile PATH    # Path to Chrome profile directory (auto-detects if omitted)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Example Commands&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Search academic research from your browsing history
python -m apps.browser_rag --query "arxiv papers machine learning transformer architecture"

# Track competitor analysis across work profile
python -m apps.browser_rag --chrome-profile "~/Library/Application Support/Google/Chrome/Work Profile" --max-items 5000
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìã Click to expand: How to find your Chrome profile&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;The default Chrome profile path is configured for a typical macOS setup. If you need to find your specific Chrome profile:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Open Terminal&lt;/li&gt; 
  &lt;li&gt;Run: &lt;code&gt;ls ~/Library/Application\ Support/Google/Chrome/&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;Look for folders like "Default", "Profile 1", "Profile 2", etc.&lt;/li&gt; 
  &lt;li&gt;Use the full path as your &lt;code&gt;--chrome-profile&lt;/code&gt; argument&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;&lt;strong&gt;Common Chrome profile locations:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;macOS: &lt;code&gt;~/Library/Application Support/Google/Chrome/Default&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;Linux: &lt;code&gt;~/.config/google-chrome/Default&lt;/code&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üí¨ Click to expand: Example queries you can try&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;Once the index is built, you can ask questions like:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;"What websites did I visit about machine learning?"&lt;/li&gt; 
  &lt;li&gt;"Find my search history about programming"&lt;/li&gt; 
  &lt;li&gt;"What YouTube videos did I watch recently?"&lt;/li&gt; 
  &lt;li&gt;"Show me websites I visited about travel planning"&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;üí¨ WeChat Detective: Unlock Your Golden Memories!&lt;/h3&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/yichuan-w/LEANN/main/videos/wechat_clear.gif" alt="LEANN WeChat Search Demo" width="600" /&gt; &lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m apps.wechat_rag --query "Show me all group chats about weekend plans"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;400K messages ‚Üí 64MB storage&lt;/strong&gt; Search years of chat history in any language.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üîß Click to expand: Installation Requirements&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;First, you need to install the &lt;a href="https://github.com/sunnyyoung/WeChatTweak-CLI"&gt;WeChat exporter&lt;/a&gt;,&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;brew install sunnyyoung/repo/wechattweak-cli
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;or install it manually (if you have issues with Homebrew):&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;sudo packages/wechat-exporter/wechattweak-cli install
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Troubleshooting:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Installation issues&lt;/strong&gt;: Check the &lt;a href="https://github.com/sunnyyoung/WeChatTweak-CLI/issues/41"&gt;WeChatTweak-CLI issues page&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Export errors&lt;/strong&gt;: If you encounter the error below, try restarting WeChat &lt;pre&gt;&lt;code class="language-bash"&gt;Failed to export WeChat data. Please ensure WeChat is running and WeChatTweak is installed.
Failed to find or export WeChat data. Exiting.
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìã Click to expand: WeChat-Specific Arguments&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h4&gt;Parameters&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;--export-dir DIR         # Directory to store exported WeChat data (default: wechat_export_direct)
--force-export          # Force re-export even if data exists
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Example Commands&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Search for travel plans discussed in group chats
python -m apps.wechat_rag --query "travel plans" --max-items 10000

# Re-export and search recent chats (useful after new messages)
python -m apps.wechat_rag --force-export --query "work schedule"
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üí¨ Click to expand: Example queries you can try&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;Once the index is built, you can ask questions like:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;"ÊàëÊÉ≥‰π∞È≠îÊúØÂ∏àÁ∫¶Áø∞ÈÄäÁöÑÁêÉË°£ÔºåÁªôÊàë‰∏Ä‰∫õÂØπÂ∫îËÅäÂ§©ËÆ∞ÂΩï?" (Chinese: Show me chat records about buying Magic Johnson's jersey)&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;ü§ñ ChatGPT Chat History: Your Personal AI Conversation Archive!&lt;/h3&gt; 
&lt;p&gt;Transform your ChatGPT conversations into a searchable knowledge base! Search through all your ChatGPT discussions about coding, research, brainstorming, and more.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m apps.chatgpt_rag --export-path chatgpt_export.html --query "How do I create a list in Python?"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Unlock your AI conversation history.&lt;/strong&gt; Never lose track of valuable insights from your ChatGPT discussions again.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìã Click to expand: How to Export ChatGPT Data&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;Step-by-step export process:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;strong&gt;Sign in to ChatGPT&lt;/strong&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Click your profile icon&lt;/strong&gt; in the top right corner&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Navigate to Settings&lt;/strong&gt; ‚Üí &lt;strong&gt;Data Controls&lt;/strong&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Click "Export"&lt;/strong&gt; under Export Data&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Confirm the export&lt;/strong&gt; request&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Download the ZIP file&lt;/strong&gt; from the email link (expires in 24 hours)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Extract or use directly&lt;/strong&gt; with LEANN&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;&lt;strong&gt;Supported formats:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;.html&lt;/code&gt; files from ChatGPT exports&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;.zip&lt;/code&gt; archives from ChatGPT&lt;/li&gt; 
  &lt;li&gt;Directories with multiple export files&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìã Click to expand: ChatGPT-Specific Arguments&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h4&gt;Parameters&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;--export-path PATH           # Path to ChatGPT export file (.html/.zip) or directory (default: ./chatgpt_export)
--separate-messages         # Process each message separately instead of concatenated conversations
--chunk-size N              # Text chunk size (default: 512)
--chunk-overlap N           # Overlap between chunks (default: 128)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Example Commands&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Basic usage with HTML export
python -m apps.chatgpt_rag --export-path conversations.html

# Process ZIP archive from ChatGPT
python -m apps.chatgpt_rag --export-path chatgpt_export.zip

# Search with specific query
python -m apps.chatgpt_rag --export-path chatgpt_data.html --query "Python programming help"

# Process individual messages for fine-grained search
python -m apps.chatgpt_rag --separate-messages --export-path chatgpt_export.html

# Process directory containing multiple exports
python -m apps.chatgpt_rag --export-path ./chatgpt_exports/ --max-items 1000
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üí° Click to expand: Example queries you can try&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;Once your ChatGPT conversations are indexed, you can search with queries like:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;"What did I ask ChatGPT about Python programming?"&lt;/li&gt; 
  &lt;li&gt;"Show me conversations about machine learning algorithms"&lt;/li&gt; 
  &lt;li&gt;"Find discussions about web development frameworks"&lt;/li&gt; 
  &lt;li&gt;"What coding advice did ChatGPT give me?"&lt;/li&gt; 
  &lt;li&gt;"Search for conversations about debugging techniques"&lt;/li&gt; 
  &lt;li&gt;"Find ChatGPT's recommendations for learning resources"&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;ü§ñ Claude Chat History: Your Personal AI Conversation Archive!&lt;/h3&gt; 
&lt;p&gt;Transform your Claude conversations into a searchable knowledge base! Search through all your Claude discussions about coding, research, brainstorming, and more.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m apps.claude_rag --export-path claude_export.json --query "What did I ask about Python dictionaries?"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Unlock your AI conversation history.&lt;/strong&gt; Never lose track of valuable insights from your Claude discussions again.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìã Click to expand: How to Export Claude Data&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;Step-by-step export process:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;strong&gt;Open Claude&lt;/strong&gt; in your browser&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Navigate to Settings&lt;/strong&gt; (look for gear icon or settings menu)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Find Export/Download&lt;/strong&gt; options in your account settings&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Download conversation data&lt;/strong&gt; (usually in JSON format)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Place the file&lt;/strong&gt; in your project directory&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;&lt;em&gt;Note: Claude export methods may vary depending on the interface you're using. Check Claude's help documentation for the most current export instructions.&lt;/em&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Supported formats:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;.json&lt;/code&gt; files (recommended)&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;.zip&lt;/code&gt; archives containing JSON data&lt;/li&gt; 
  &lt;li&gt;Directories with multiple export files&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìã Click to expand: Claude-Specific Arguments&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h4&gt;Parameters&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;--export-path PATH           # Path to Claude export file (.json/.zip) or directory (default: ./claude_export)
--separate-messages         # Process each message separately instead of concatenated conversations
--chunk-size N              # Text chunk size (default: 512)
--chunk-overlap N           # Overlap between chunks (default: 128)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Example Commands&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Basic usage with JSON export
python -m apps.claude_rag --export-path my_claude_conversations.json

# Process ZIP archive from Claude
python -m apps.claude_rag --export-path claude_export.zip

# Search with specific query
python -m apps.claude_rag --export-path claude_data.json --query "machine learning advice"

# Process individual messages for fine-grained search
python -m apps.claude_rag --separate-messages --export-path claude_export.json

# Process directory containing multiple exports
python -m apps.claude_rag --export-path ./claude_exports/ --max-items 1000
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üí° Click to expand: Example queries you can try&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;Once your Claude conversations are indexed, you can search with queries like:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;"What did I ask Claude about Python programming?"&lt;/li&gt; 
  &lt;li&gt;"Show me conversations about machine learning algorithms"&lt;/li&gt; 
  &lt;li&gt;"Find discussions about software architecture patterns"&lt;/li&gt; 
  &lt;li&gt;"What debugging advice did Claude give me?"&lt;/li&gt; 
  &lt;li&gt;"Search for conversations about data structures"&lt;/li&gt; 
  &lt;li&gt;"Find Claude's recommendations for learning resources"&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;üí¨ iMessage History: Your Personal Conversation Archive!&lt;/h3&gt; 
&lt;p&gt;Transform your iMessage conversations into a searchable knowledge base! Search through all your text messages, group chats, and conversations with friends, family, and colleagues.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m apps.imessage_rag --query "What did we discuss about the weekend plans?"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Unlock your message history.&lt;/strong&gt; Never lose track of important conversations, shared links, or memorable moments from your iMessage history.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìã Click to expand: How to Access iMessage Data&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;iMessage data location:&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;iMessage conversations are stored in a SQLite database on your Mac at:&lt;/p&gt; 
 &lt;pre&gt;&lt;code&gt;~/Library/Messages/chat.db
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Important setup requirements:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Grant Full Disk Access&lt;/strong&gt; to your terminal or IDE:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Open &lt;strong&gt;System Preferences&lt;/strong&gt; ‚Üí &lt;strong&gt;Security &amp;amp; Privacy&lt;/strong&gt; ‚Üí &lt;strong&gt;Privacy&lt;/strong&gt;&lt;/li&gt; 
    &lt;li&gt;Select &lt;strong&gt;Full Disk Access&lt;/strong&gt; from the left sidebar&lt;/li&gt; 
    &lt;li&gt;Click the &lt;strong&gt;+&lt;/strong&gt; button and add your terminal app (Terminal, iTerm2) or IDE (VS Code, etc.)&lt;/li&gt; 
    &lt;li&gt;Restart your terminal/IDE after granting access&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Alternative: Use a backup database&lt;/strong&gt;&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;If you have Time Machine backups or manual copies of the database&lt;/li&gt; 
    &lt;li&gt;Use &lt;code&gt;--db-path&lt;/code&gt; to specify a custom location&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;&lt;strong&gt;Supported formats:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Direct access to &lt;code&gt;~/Library/Messages/chat.db&lt;/code&gt; (default)&lt;/li&gt; 
  &lt;li&gt;Custom database path with &lt;code&gt;--db-path&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;Works with backup copies of the database&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìã Click to expand: iMessage-Specific Arguments&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h4&gt;Parameters&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;--db-path PATH                    # Path to chat.db file (default: ~/Library/Messages/chat.db)
--concatenate-conversations       # Group messages by conversation (default: True)
--no-concatenate-conversations    # Process each message individually
--chunk-size N                    # Text chunk size (default: 1000)
--chunk-overlap N                 # Overlap between chunks (default: 200)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Example Commands&lt;/h4&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Basic usage (requires Full Disk Access)
python -m apps.imessage_rag

# Search with specific query
python -m apps.imessage_rag --query "family dinner plans"

# Use custom database path
python -m apps.imessage_rag --db-path /path/to/backup/chat.db

# Process individual messages instead of conversations
python -m apps.imessage_rag --no-concatenate-conversations

# Limit processing for testing
python -m apps.imessage_rag --max-items 100 --query "weekend"
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üí° Click to expand: Example queries you can try&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;Once your iMessage conversations are indexed, you can search with queries like:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;"What did we discuss about vacation plans?"&lt;/li&gt; 
  &lt;li&gt;"Find messages about restaurant recommendations"&lt;/li&gt; 
  &lt;li&gt;"Show me conversations with John about the project"&lt;/li&gt; 
  &lt;li&gt;"Search for shared links about technology"&lt;/li&gt; 
  &lt;li&gt;"Find group chat discussions about weekend events"&lt;/li&gt; 
  &lt;li&gt;"What did mom say about the family gathering?"&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;MCP Integration: RAG on Live Data from Any Platform&lt;/h3&gt; 
&lt;p&gt;Connect to live data sources through the Model Context Protocol (MCP). LEANN now supports real-time RAG on platforms like Slack, Twitter, and more through standardized MCP servers.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Key Benefits:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Live Data Access&lt;/strong&gt;: Fetch real-time data without manual exports&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Standardized Protocol&lt;/strong&gt;: Use any MCP-compatible server&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Easy Extension&lt;/strong&gt;: Add new platforms with minimal code&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Secure Access&lt;/strong&gt;: MCP servers handle authentication&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;üí¨ Slack Messages: Search Your Team Conversations&lt;/h4&gt; 
&lt;p&gt;Transform your Slack workspace into a searchable knowledge base! Find discussions, decisions, and shared knowledge across all your channels.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Test MCP server connection
python -m apps.slack_rag --mcp-server "slack-mcp-server" --test-connection

# Index and search Slack messages
python -m apps.slack_rag \
  --mcp-server "slack-mcp-server" \
  --workspace-name "my-team" \
  --channels general dev-team random \
  --query "What did we decide about the product launch?"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;üìñ Comprehensive Setup Guide&lt;/strong&gt;: For detailed setup instructions, troubleshooting common issues (like "users cache is not ready yet"), and advanced configuration options, see our &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/docs/slack-setup-guide.md"&gt;&lt;strong&gt;Slack Setup Guide&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Quick Setup:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install a Slack MCP server (e.g., &lt;code&gt;npm install -g slack-mcp-server&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Create a Slack App and get API credentials (see detailed guide above)&lt;/li&gt; 
 &lt;li&gt;Set environment variables: &lt;pre&gt;&lt;code class="language-bash"&gt;export SLACK_BOT_TOKEN="xoxb-your-bot-token"
export SLACK_APP_TOKEN="xapp-your-app-token"  # Optional
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Test connection with &lt;code&gt;--test-connection&lt;/code&gt; flag&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Arguments:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--mcp-server&lt;/code&gt;: Command to start the Slack MCP server&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--workspace-name&lt;/code&gt;: Slack workspace name for organization&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--channels&lt;/code&gt;: Specific channels to index (optional)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--concatenate-conversations&lt;/code&gt;: Group messages by channel (default: true)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--max-messages-per-channel&lt;/code&gt;: Limit messages per channel (default: 100)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--max-retries&lt;/code&gt;: Maximum retries for cache sync issues (default: 5)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--retry-delay&lt;/code&gt;: Initial delay between retries in seconds (default: 2.0)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;üê¶ Twitter Bookmarks: Your Personal Tweet Library&lt;/h4&gt; 
&lt;p&gt;Search through your Twitter bookmarks! Find that perfect article, thread, or insight you saved for later.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Test MCP server connection
python -m apps.twitter_rag --mcp-server "twitter-mcp-server" --test-connection

# Index and search Twitter bookmarks
python -m apps.twitter_rag \
  --mcp-server "twitter-mcp-server" \
  --max-bookmarks 1000 \
  --query "What AI articles did I bookmark about machine learning?"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Setup Requirements:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install a Twitter MCP server (e.g., &lt;code&gt;npm install -g twitter-mcp-server&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Get Twitter API credentials: 
  &lt;ul&gt; 
   &lt;li&gt;Apply for a Twitter Developer Account at &lt;a href="https://developer.twitter.com"&gt;developer.twitter.com&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Create a new app in the Twitter Developer Portal&lt;/li&gt; 
   &lt;li&gt;Generate API keys and access tokens with "Read" permissions&lt;/li&gt; 
   &lt;li&gt;For bookmarks access, you may need Twitter API v2 with appropriate scopes&lt;/li&gt; 
  &lt;/ul&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;export TWITTER_API_KEY="your-api-key"
export TWITTER_API_SECRET="your-api-secret"
export TWITTER_ACCESS_TOKEN="your-access-token"
export TWITTER_ACCESS_TOKEN_SECRET="your-access-token-secret"
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Test connection with &lt;code&gt;--test-connection&lt;/code&gt; flag&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Arguments:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--mcp-server&lt;/code&gt;: Command to start the Twitter MCP server&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--username&lt;/code&gt;: Filter bookmarks by username (optional)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--max-bookmarks&lt;/code&gt;: Maximum bookmarks to fetch (default: 1000)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--no-tweet-content&lt;/code&gt;: Exclude tweet content, only metadata&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--no-metadata&lt;/code&gt;: Exclude engagement metadata&lt;/li&gt; 
&lt;/ul&gt;  
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üí° Click to expand: Example queries you can try&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;Slack Queries:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;"What did the team discuss about the project deadline?"&lt;/li&gt; 
  &lt;li&gt;"Find messages about the new feature launch"&lt;/li&gt; 
  &lt;li&gt;"Show me conversations about budget planning"&lt;/li&gt; 
  &lt;li&gt;"What decisions were made in the dev-team channel?"&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;Twitter Queries:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;"What AI articles did I bookmark last month?"&lt;/li&gt; 
  &lt;li&gt;"Find tweets about machine learning techniques"&lt;/li&gt; 
  &lt;li&gt;"Show me bookmarked threads about startup advice"&lt;/li&gt; 
  &lt;li&gt;"What Python tutorials did I save?"&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;summary&gt;&lt;strong&gt;üîß Using MCP with CLI Commands&lt;/strong&gt;&lt;/summary&gt; 
&lt;p&gt;&lt;strong&gt;Want to use MCP data with regular LEANN CLI?&lt;/strong&gt; You can combine MCP apps with CLI commands:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Step 1: Use MCP app to fetch and index data
python -m apps.slack_rag --mcp-server "slack-mcp-server" --workspace-name "my-team"

# Step 2: The data is now indexed and available via CLI
leann search slack_messages "project deadline"
leann ask slack_messages "What decisions were made about the product launch?"

# Same for Twitter bookmarks
python -m apps.twitter_rag --mcp-server "twitter-mcp-server"
leann search twitter_bookmarks "machine learning articles"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;MCP vs Manual Export:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;MCP&lt;/strong&gt;: Live data, automatic updates, requires server setup&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Manual Export&lt;/strong&gt;: One-time setup, works offline, requires manual data export&lt;/li&gt; 
&lt;/ul&gt;  
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üîß Adding New MCP Platforms&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;Want to add support for other platforms? LEANN's MCP integration is designed for easy extension:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;strong&gt;Find or create an MCP server&lt;/strong&gt; for your platform&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Create a reader class&lt;/strong&gt; following the pattern in &lt;code&gt;apps/slack_data/slack_mcp_reader.py&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Create a RAG application&lt;/strong&gt; following the pattern in &lt;code&gt;apps/slack_rag.py&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Test and contribute&lt;/strong&gt; back to the community!&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;&lt;strong&gt;Popular MCP servers to explore:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;GitHub repositories and issues&lt;/li&gt; 
  &lt;li&gt;Discord messages&lt;/li&gt; 
  &lt;li&gt;Notion pages&lt;/li&gt; 
  &lt;li&gt;Google Drive documents&lt;/li&gt; 
  &lt;li&gt;And many more in the MCP ecosystem!&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;üöÄ Claude Code Integration: Transform Your Development Workflow!&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;AST‚ÄëAware Code Chunking&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;LEANN features intelligent code chunking that preserves semantic boundaries (functions, classes, methods) for Python, Java, C#, and TypeScript, improving code understanding compared to text-based chunking.&lt;/p&gt; 
 &lt;p&gt;üìñ Read the &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/docs/ast_chunking_guide.md"&gt;AST Chunking Guide ‚Üí&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;p&gt;&lt;strong&gt;The future of code assistance is here.&lt;/strong&gt; Transform your development workflow with LEANN's native MCP integration for Claude Code. Index your entire codebase and get intelligent code assistance directly in your IDE.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Key features:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üîç &lt;strong&gt;Semantic code search&lt;/strong&gt; across your entire project, fully local index and lightweight&lt;/li&gt; 
 &lt;li&gt;üß† &lt;strong&gt;AST-aware chunking&lt;/strong&gt; preserves code structure (functions, classes)&lt;/li&gt; 
 &lt;li&gt;üìö &lt;strong&gt;Context-aware assistance&lt;/strong&gt; for debugging and development&lt;/li&gt; 
 &lt;li&gt;üöÄ &lt;strong&gt;Zero-config setup&lt;/strong&gt; with automatic language detection&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install LEANN globally for MCP integration
uv tool install leann-core --with leann
claude mcp add --scope user leann-server -- leann_mcp
# Setup is automatic - just start using Claude Code!
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Try our fully agentic pipeline with auto query rewriting, semantic search planning, and more:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/yichuan-w/LEANN/main/assets/mcp_leann.png" alt="LEANN MCP Integration" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;üî• Ready to supercharge your coding?&lt;/strong&gt; &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/packages/leann-mcp/README.md"&gt;Complete Setup Guide ‚Üí&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Command Line Interface&lt;/h2&gt; 
&lt;p&gt;LEANN includes a powerful CLI for document processing and search. Perfect for quick document indexing and interactive chat.&lt;/p&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;p&gt;If you followed the Quick Start, &lt;code&gt;leann&lt;/code&gt; is already installed in your virtual environment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;source .venv/bin/activate
leann --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;To make it globally available:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install the LEANN CLI globally using uv tool
uv tool install leann-core --with leann


# Now you can use leann from anywhere without activating venv
leann --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Global installation is required for Claude Code integration. The &lt;code&gt;leann_mcp&lt;/code&gt; server depends on the globally available &lt;code&gt;leann&lt;/code&gt; command.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Usage Examples&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# build from a specific directory, and my_docs is the index name(Here you can also build from multiple dict or multiple files)
leann build my-docs --docs ./your_documents

# Search your documents
leann search my-docs "machine learning concepts"

# Interactive chat with your documents
leann ask my-docs --interactive

# Ask a single question (non-interactive)
leann ask my-docs "Where are prompts configured?"

# List all your indexes
leann list

# Remove an index
leann remove my-docs
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Key CLI features:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Auto-detects document formats (PDF, TXT, MD, DOCX, PPTX + code files)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üß† AST-aware chunking&lt;/strong&gt; for Python, Java, C#, TypeScript files&lt;/li&gt; 
 &lt;li&gt;Smart text chunking with overlap for all other content&lt;/li&gt; 
 &lt;li&gt;Multiple LLM providers (Ollama, OpenAI, HuggingFace)&lt;/li&gt; 
 &lt;li&gt;Organized index storage in &lt;code&gt;.leann/indexes/&lt;/code&gt; (project-local)&lt;/li&gt; 
 &lt;li&gt;Support for advanced search parameters&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìã Click to expand: Complete CLI Reference&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;You can use &lt;code&gt;leann --help&lt;/code&gt;, or &lt;code&gt;leann build --help&lt;/code&gt;, &lt;code&gt;leann search --help&lt;/code&gt;, &lt;code&gt;leann ask --help&lt;/code&gt;, &lt;code&gt;leann list --help&lt;/code&gt;, &lt;code&gt;leann remove --help&lt;/code&gt; to get the complete CLI reference.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Build Command:&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;leann build INDEX_NAME --docs DIRECTORY|FILE [DIRECTORY|FILE ...] [OPTIONS]

Options:
  --backend {hnsw,diskann}     Backend to use (default: hnsw)
  --embedding-model MODEL      Embedding model (default: facebook/contriever)
  --graph-degree N             Graph degree (default: 32)
  --complexity N               Build complexity (default: 64)
  --force                      Force rebuild existing index
  --compact / --no-compact     Use compact storage (default: true). Must be `no-compact` for `no-recompute` build.
  --recompute / --no-recompute Enable recomputation (default: true)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Search Command:&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;leann search INDEX_NAME QUERY [OPTIONS]

Options:
  --top-k N                     Number of results (default: 5)
  --complexity N                Search complexity (default: 64)
  --recompute / --no-recompute  Enable/disable embedding recomputation (default: enabled). Should not do a `no-recompute` search in a `recompute` build.
  --pruning-strategy {global,local,proportional}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Ask Command:&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;leann ask INDEX_NAME [OPTIONS]

Options:
  --llm {ollama,openai,hf,anthropic}    LLM provider (default: ollama)
  --model MODEL                         Model name (default: qwen3:8b)
  --interactive                         Interactive chat mode
  --top-k N                             Retrieval count (default: 20)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;List Command:&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;leann list

# Lists all indexes across all projects with status indicators:
# ‚úÖ - Index is complete and ready to use
# ‚ùå - Index is incomplete or corrupted
# üìÅ - CLI-created index (in .leann/indexes/)
# üìÑ - App-created index (*.leann.meta.json files)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Remove Command:&lt;/strong&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;leann remove INDEX_NAME [OPTIONS]

Options:
  --force, -f    Force removal without confirmation

# Smart removal: automatically finds and safely removes indexes
# - Shows all matching indexes across projects
# - Requires confirmation for cross-project removal
# - Interactive selection when multiple matches found
# - Supports both CLI and app-created indexes
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;üöÄ Advanced Features&lt;/h2&gt; 
&lt;h3&gt;üéØ Metadata Filtering&lt;/h3&gt; 
&lt;p&gt;LEANN supports a simple metadata filtering system to enable sophisticated use cases like document filtering by date/type, code search by file extension, and content management based on custom criteria.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Add metadata during indexing
builder.add_text(
    "def authenticate_user(token): ...",
    metadata={"file_extension": ".py", "lines_of_code": 25}
)

# Search with filters
results = searcher.search(
    query="authentication function",
    metadata_filters={
        "file_extension": {"==": ".py"},
        "lines_of_code": {"&amp;lt;": 100}
    }
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Supported operators&lt;/strong&gt;: &lt;code&gt;==&lt;/code&gt;, &lt;code&gt;!=&lt;/code&gt;, &lt;code&gt;&amp;lt;&lt;/code&gt;, &lt;code&gt;&amp;lt;=&lt;/code&gt;, &lt;code&gt;&amp;gt;&lt;/code&gt;, &lt;code&gt;&amp;gt;=&lt;/code&gt;, &lt;code&gt;in&lt;/code&gt;, &lt;code&gt;not_in&lt;/code&gt;, &lt;code&gt;contains&lt;/code&gt;, &lt;code&gt;starts_with&lt;/code&gt;, &lt;code&gt;ends_with&lt;/code&gt;, &lt;code&gt;is_true&lt;/code&gt;, &lt;code&gt;is_false&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;üìñ &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/docs/metadata_filtering.md"&gt;Complete Metadata filtering guide ‚Üí&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h3&gt;üîç Grep Search&lt;/h3&gt; 
&lt;p&gt;For exact text matching instead of semantic search, use the &lt;code&gt;use_grep&lt;/code&gt; parameter:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Exact text search
results = searcher.search("banana‚Äëcrocodile", use_grep=True, top_k=1)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Use cases&lt;/strong&gt;: Finding specific code patterns, error messages, function names, or exact phrases where semantic similarity isn't needed.&lt;/p&gt; 
&lt;p&gt;üìñ &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/docs/grep_search.md"&gt;Complete grep search guide ‚Üí&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;üèóÔ∏è Architecture &amp;amp; How It Works&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/yichuan-w/LEANN/main/assets/arch.png" alt="LEANN Architecture" width="800" /&gt; &lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;The magic:&lt;/strong&gt; Most vector DBs store every single embedding (expensive). LEANN stores a pruned graph structure (cheap) and recomputes embeddings only when needed (fast).&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Core techniques:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Graph-based selective recomputation:&lt;/strong&gt; Only compute embeddings for nodes in the search path&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;High-degree preserving pruning:&lt;/strong&gt; Keep important "hub" nodes while removing redundant connections&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Dynamic batching:&lt;/strong&gt; Efficiently batch embedding computations for GPU utilization&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Two-level search:&lt;/strong&gt; Smart graph traversal that prioritizes promising nodes&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Backends:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;HNSW&lt;/strong&gt; (default): Ideal for most datasets with maximum storage savings through full recomputation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;DiskANN&lt;/strong&gt;: Advanced option with superior search performance, using PQ-based graph traversal with real-time reranking for the best speed-accuracy trade-off&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Benchmarks&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/benchmarks/diskann_vs_hnsw_speed_comparison.py"&gt;DiskANN vs HNSW Performance Comparison ‚Üí&lt;/a&gt;&lt;/strong&gt; - Compare search performance between both backends&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/benchmarks/compare_faiss_vs_leann.py"&gt;Simple Example: Compare LEANN vs FAISS ‚Üí&lt;/a&gt;&lt;/strong&gt; - See storage savings in action&lt;/p&gt; 
&lt;h3&gt;üìä Storage Comparison&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;System&lt;/th&gt; 
   &lt;th&gt;DPR (2.1M)&lt;/th&gt; 
   &lt;th&gt;Wiki (60M)&lt;/th&gt; 
   &lt;th&gt;Chat (400K)&lt;/th&gt; 
   &lt;th&gt;Email (780K)&lt;/th&gt; 
   &lt;th&gt;Browser (38K)&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Traditional vector database (e.g., FAISS)&lt;/td&gt; 
   &lt;td&gt;3.8 GB&lt;/td&gt; 
   &lt;td&gt;201 GB&lt;/td&gt; 
   &lt;td&gt;1.8 GB&lt;/td&gt; 
   &lt;td&gt;2.4 GB&lt;/td&gt; 
   &lt;td&gt;130 MB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;LEANN&lt;/td&gt; 
   &lt;td&gt;324 MB&lt;/td&gt; 
   &lt;td&gt;6 GB&lt;/td&gt; 
   &lt;td&gt;64 MB&lt;/td&gt; 
   &lt;td&gt;79 MB&lt;/td&gt; 
   &lt;td&gt;6.4 MB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Savings&lt;/td&gt; 
   &lt;td&gt;91%&lt;/td&gt; 
   &lt;td&gt;97%&lt;/td&gt; 
   &lt;td&gt;97%&lt;/td&gt; 
   &lt;td&gt;97%&lt;/td&gt; 
   &lt;td&gt;95%&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Reproduce Our Results&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run benchmarks/run_evaluation.py    # Will auto-download evaluation data and run benchmarks
uv run benchmarks/run_evaluation.py benchmarks/data/indices/rpj_wiki/rpj_wiki --num-queries 2000    # After downloading data, you can run the benchmark with our biggest index
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The evaluation script downloads data automatically on first run. The last three results were tested with partial personal data, and you can reproduce them with your own data!&lt;/p&gt; 
&lt;h2&gt;üî¨ Paper&lt;/h2&gt; 
&lt;p&gt;If you find Leann useful, please cite:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://arxiv.org/abs/2506.08276"&gt;LEANN: A Low-Storage Vector Index&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{wang2025leannlowstoragevectorindex,
      title={LEANN: A Low-Storage Vector Index},
      author={Yichuan Wang and Shu Liu and Zhifei Li and Yongji Wu and Ziming Mao and Yilong Zhao and Xiao Yan and Zhiying Xu and Yang Zhou and Ion Stoica and Sewon Min and Matei Zaharia and Joseph E. Gonzalez},
      year={2025},
      eprint={2506.08276},
      archivePrefix={arXiv},
      primaryClass={cs.DB},
      url={https://arxiv.org/abs/2506.08276},
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;‚ú® &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/docs/features.md"&gt;Detailed Features ‚Üí&lt;/a&gt;&lt;/h2&gt; 
&lt;h2&gt;ü§ù &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/docs/CONTRIBUTING.md"&gt;CONTRIBUTING ‚Üí&lt;/a&gt;&lt;/h2&gt; 
&lt;h2&gt;‚ùì &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/docs/faq.md"&gt;FAQ ‚Üí&lt;/a&gt;&lt;/h2&gt; 
&lt;h2&gt;üìà &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/docs/roadmap.md"&gt;Roadmap ‚Üí&lt;/a&gt;&lt;/h2&gt; 
&lt;h2&gt;üìÑ License&lt;/h2&gt; 
&lt;p&gt;MIT License - see &lt;a href="https://raw.githubusercontent.com/yichuan-w/LEANN/main/LICENSE"&gt;LICENSE&lt;/a&gt; for details.&lt;/p&gt; 
&lt;h2&gt;üôè Acknowledgments&lt;/h2&gt; 
&lt;p&gt;Core Contributors: &lt;a href="https://yichuan-w.github.io/"&gt;Yichuan Wang&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/andylizf"&gt;Zhifei Li&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Active Contributors: &lt;a href="https://github.com/gabriel-dehan"&gt;Gabriel Dehan&lt;/a&gt;, &lt;a href="https://github.com/ASuresh0524"&gt;Aakash Suresh&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;We welcome more contributors! Feel free to open issues or submit PRs.&lt;/p&gt; 
&lt;p&gt;This work is done at &lt;a href="https://sky.cs.berkeley.edu/"&gt;&lt;strong&gt;Berkeley Sky Computing Lab&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.star-history.com/#yichuan-w/LEANN&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=yichuan-w/LEANN&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;strong&gt;‚≠ê Star us on GitHub if Leann is useful for your research or applications!&lt;/strong&gt; &lt;/p&gt; 
&lt;p align="center"&gt; Made with ‚ù§Ô∏è by the Leann team &lt;/p&gt; 
&lt;h2&gt;ü§ñ Explore LEANN with AI&lt;/h2&gt; 
&lt;p&gt;LEANN is indexed on &lt;a href="https://deepwiki.com/yichuan-w/LEANN"&gt;DeepWiki&lt;/a&gt;, so you can ask questions to LLMs using Deep Research to explore the codebase and get help to add new features.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>SYSTRAN/faster-whisper</title>
      <link>https://github.com/SYSTRAN/faster-whisper</link>
      <description>&lt;p&gt;Faster Whisper transcription with CTranslate2&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href="https://github.com/SYSTRAN/faster-whisper/actions?query=workflow%3ACI"&gt;&lt;img src="https://github.com/SYSTRAN/faster-whisper/workflows/CI/badge.svg?sanitize=true" alt="CI" /&gt;&lt;/a&gt; &lt;a href="https://badge.fury.io/py/faster-whisper"&gt;&lt;img src="https://badge.fury.io/py/faster-whisper.svg?sanitize=true" alt="PyPI version" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Faster Whisper transcription with CTranslate2&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;faster-whisper&lt;/strong&gt; is a reimplementation of OpenAI's Whisper model using &lt;a href="https://github.com/OpenNMT/CTranslate2/"&gt;CTranslate2&lt;/a&gt;, which is a fast inference engine for Transformer models.&lt;/p&gt; 
&lt;p&gt;This implementation is up to 4 times faster than &lt;a href="https://github.com/openai/whisper"&gt;openai/whisper&lt;/a&gt; for the same accuracy while using less memory. The efficiency can be further improved with 8-bit quantization on both CPU and GPU.&lt;/p&gt; 
&lt;h2&gt;Benchmark&lt;/h2&gt; 
&lt;h3&gt;Whisper&lt;/h3&gt; 
&lt;p&gt;For reference, here's the time and memory usage that are required to transcribe &lt;a href="https://www.youtube.com/watch?v=0u7tTptBo9I"&gt;&lt;strong&gt;13 minutes&lt;/strong&gt;&lt;/a&gt; of audio using different implementations:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/openai/whisper"&gt;openai/whisper&lt;/a&gt;@&lt;a href="https://github.com/openai/whisper/tree/v20240930"&gt;v20240930&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ggerganov/whisper.cpp"&gt;whisper.cpp&lt;/a&gt;@&lt;a href="https://github.com/ggerganov/whisper.cpp/tree/v1.7.2"&gt;v1.7.2&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/huggingface/transformers"&gt;transformers&lt;/a&gt;@&lt;a href="https://github.com/huggingface/transformers/tree/v4.46.3"&gt;v4.46.3&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/SYSTRAN/faster-whisper"&gt;faster-whisper&lt;/a&gt;@&lt;a href="https://github.com/SYSTRAN/faster-whisper/tree/v1.1.0"&gt;v1.1.0&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Large-v2 model on GPU&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Implementation&lt;/th&gt; 
   &lt;th&gt;Precision&lt;/th&gt; 
   &lt;th&gt;Beam size&lt;/th&gt; 
   &lt;th&gt;Time&lt;/th&gt; 
   &lt;th&gt;VRAM Usage&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;openai/whisper&lt;/td&gt; 
   &lt;td&gt;fp16&lt;/td&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;2m23s&lt;/td&gt; 
   &lt;td&gt;4708MB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;whisper.cpp (Flash Attention)&lt;/td&gt; 
   &lt;td&gt;fp16&lt;/td&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;1m05s&lt;/td&gt; 
   &lt;td&gt;4127MB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;transformers (SDPA)[^1]&lt;/td&gt; 
   &lt;td&gt;fp16&lt;/td&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;1m52s&lt;/td&gt; 
   &lt;td&gt;4960MB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;faster-whisper&lt;/td&gt; 
   &lt;td&gt;fp16&lt;/td&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;1m03s&lt;/td&gt; 
   &lt;td&gt;4525MB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;faster-whisper (&lt;code&gt;batch_size=8&lt;/code&gt;)&lt;/td&gt; 
   &lt;td&gt;fp16&lt;/td&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;17s&lt;/td&gt; 
   &lt;td&gt;6090MB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;faster-whisper&lt;/td&gt; 
   &lt;td&gt;int8&lt;/td&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;59s&lt;/td&gt; 
   &lt;td&gt;2926MB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;faster-whisper (&lt;code&gt;batch_size=8&lt;/code&gt;)&lt;/td&gt; 
   &lt;td&gt;int8&lt;/td&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;16s&lt;/td&gt; 
   &lt;td&gt;4500MB&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;distil-whisper-large-v3 model on GPU&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Implementation&lt;/th&gt; 
   &lt;th&gt;Precision&lt;/th&gt; 
   &lt;th&gt;Beam size&lt;/th&gt; 
   &lt;th&gt;Time&lt;/th&gt; 
   &lt;th&gt;YT Commons WER&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;transformers (SDPA) (&lt;code&gt;batch_size=16&lt;/code&gt;)&lt;/td&gt; 
   &lt;td&gt;fp16&lt;/td&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;46m12s&lt;/td&gt; 
   &lt;td&gt;14.801&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;faster-whisper (&lt;code&gt;batch_size=16&lt;/code&gt;)&lt;/td&gt; 
   &lt;td&gt;fp16&lt;/td&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;25m50s&lt;/td&gt; 
   &lt;td&gt;13.527&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;em&gt;GPU Benchmarks are Executed with CUDA 12.4 on a NVIDIA RTX 3070 Ti 8GB.&lt;/em&gt; [^1]: transformers OOM for any batch size &amp;gt; 1&lt;/p&gt; 
&lt;h3&gt;Small model on CPU&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Implementation&lt;/th&gt; 
   &lt;th&gt;Precision&lt;/th&gt; 
   &lt;th&gt;Beam size&lt;/th&gt; 
   &lt;th&gt;Time&lt;/th&gt; 
   &lt;th&gt;RAM Usage&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;openai/whisper&lt;/td&gt; 
   &lt;td&gt;fp32&lt;/td&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;6m58s&lt;/td&gt; 
   &lt;td&gt;2335MB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;whisper.cpp&lt;/td&gt; 
   &lt;td&gt;fp32&lt;/td&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;2m05s&lt;/td&gt; 
   &lt;td&gt;1049MB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;whisper.cpp (OpenVINO)&lt;/td&gt; 
   &lt;td&gt;fp32&lt;/td&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;1m45s&lt;/td&gt; 
   &lt;td&gt;1642MB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;faster-whisper&lt;/td&gt; 
   &lt;td&gt;fp32&lt;/td&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;2m37s&lt;/td&gt; 
   &lt;td&gt;2257MB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;faster-whisper (&lt;code&gt;batch_size=8&lt;/code&gt;)&lt;/td&gt; 
   &lt;td&gt;fp32&lt;/td&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;1m06s&lt;/td&gt; 
   &lt;td&gt;4230MB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;faster-whisper&lt;/td&gt; 
   &lt;td&gt;int8&lt;/td&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;1m42s&lt;/td&gt; 
   &lt;td&gt;1477MB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;faster-whisper (&lt;code&gt;batch_size=8&lt;/code&gt;)&lt;/td&gt; 
   &lt;td&gt;int8&lt;/td&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;51s&lt;/td&gt; 
   &lt;td&gt;3608MB&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;em&gt;Executed with 8 threads on an Intel Core i7-12700K.&lt;/em&gt;&lt;/p&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.9 or greater&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Unlike openai-whisper, FFmpeg does &lt;strong&gt;not&lt;/strong&gt; need to be installed on the system. The audio is decoded with the Python library &lt;a href="https://github.com/PyAV-Org/PyAV"&gt;PyAV&lt;/a&gt; which bundles the FFmpeg libraries in its package.&lt;/p&gt; 
&lt;h3&gt;GPU&lt;/h3&gt; 
&lt;p&gt;GPU execution requires the following NVIDIA libraries to be installed:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://developer.nvidia.com/cublas"&gt;cuBLAS for CUDA 12&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://developer.nvidia.com/cudnn"&gt;cuDNN 9 for CUDA 12&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: The latest versions of &lt;code&gt;ctranslate2&lt;/code&gt; only support CUDA 12 and cuDNN 9. For CUDA 11 and cuDNN 8, the current workaround is downgrading to the &lt;code&gt;3.24.0&lt;/code&gt; version of &lt;code&gt;ctranslate2&lt;/code&gt;, for CUDA 12 and cuDNN 8, downgrade to the &lt;code&gt;4.4.0&lt;/code&gt; version of &lt;code&gt;ctranslate2&lt;/code&gt;, (This can be done with &lt;code&gt;pip install --force-reinstall ctranslate2==4.4.0&lt;/code&gt; or specifying the version in a &lt;code&gt;requirements.txt&lt;/code&gt;).&lt;/p&gt; 
&lt;p&gt;There are multiple ways to install the NVIDIA libraries mentioned above. The recommended way is described in the official NVIDIA documentation, but we also suggest other installation methods below.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Other installation methods (click to expand)&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; For all these methods below, keep in mind the above note regarding CUDA versions. Depending on your setup, you may need to install the &lt;em&gt;CUDA 11&lt;/em&gt; versions of libraries that correspond to the CUDA 12 libraries listed in the instructions below.&lt;/p&gt; 
 &lt;h4&gt;Use Docker&lt;/h4&gt; 
 &lt;p&gt;The libraries (cuBLAS, cuDNN) are installed in this official NVIDIA CUDA Docker images: &lt;code&gt;nvidia/cuda:12.3.2-cudnn9-runtime-ubuntu22.04&lt;/code&gt;.&lt;/p&gt; 
 &lt;h4&gt;Install with &lt;code&gt;pip&lt;/code&gt; (Linux only)&lt;/h4&gt; 
 &lt;p&gt;On Linux these libraries can be installed with &lt;code&gt;pip&lt;/code&gt;. Note that &lt;code&gt;LD_LIBRARY_PATH&lt;/code&gt; must be set before launching Python.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;pip install nvidia-cublas-cu12 nvidia-cudnn-cu12==9.*

export LD_LIBRARY_PATH=`python3 -c 'import os; import nvidia.cublas.lib; import nvidia.cudnn.lib; print(os.path.dirname(nvidia.cublas.lib.__file__) + ":" + os.path.dirname(nvidia.cudnn.lib.__file__))'`
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Download the libraries from Purfview's repository (Windows &amp;amp; Linux)&lt;/h4&gt; 
 &lt;p&gt;Purfview's &lt;a href="https://github.com/Purfview/whisper-standalone-win"&gt;whisper-standalone-win&lt;/a&gt; provides the required NVIDIA libraries for Windows &amp;amp; Linux in a &lt;a href="https://github.com/Purfview/whisper-standalone-win/releases/tag/libs"&gt;single archive&lt;/a&gt;. Decompress the archive and place the libraries in a directory included in the &lt;code&gt;PATH&lt;/code&gt;.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;The module can be installed from &lt;a href="https://pypi.org/project/faster-whisper/"&gt;PyPI&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install faster-whisper
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;Other installation methods (click to expand)&lt;/summary&gt; 
 &lt;h3&gt;Install the master branch&lt;/h3&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;pip install --force-reinstall "faster-whisper @ https://github.com/SYSTRAN/faster-whisper/archive/refs/heads/master.tar.gz"
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;Install a specific commit&lt;/h3&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;pip install --force-reinstall "faster-whisper @ https://github.com/SYSTRAN/faster-whisper/archive/a4f1cc8f11433e454c3934442b5e1a4ed5e865c3.tar.gz"
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;h3&gt;Faster-whisper&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from faster_whisper import WhisperModel

model_size = "large-v3"

# Run on GPU with FP16
model = WhisperModel(model_size, device="cuda", compute_type="float16")

# or run on GPU with INT8
# model = WhisperModel(model_size, device="cuda", compute_type="int8_float16")
# or run on CPU with INT8
# model = WhisperModel(model_size, device="cpu", compute_type="int8")

segments, info = model.transcribe("audio.mp3", beam_size=5)

print("Detected language '%s' with probability %f" % (info.language, info.language_probability))

for segment in segments:
    print("[%.2fs -&amp;gt; %.2fs] %s" % (segment.start, segment.end, segment.text))
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Warning:&lt;/strong&gt; &lt;code&gt;segments&lt;/code&gt; is a &lt;em&gt;generator&lt;/em&gt; so the transcription only starts when you iterate over it. The transcription can be run to completion by gathering the segments in a list or a &lt;code&gt;for&lt;/code&gt; loop:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;segments, _ = model.transcribe("audio.mp3")
segments = list(segments)  # The transcription will actually run here.
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Batched Transcription&lt;/h3&gt; 
&lt;p&gt;The following code snippet illustrates how to run batched transcription on an example audio file. &lt;code&gt;BatchedInferencePipeline.transcribe&lt;/code&gt; is a drop-in replacement for &lt;code&gt;WhisperModel.transcribe&lt;/code&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from faster_whisper import WhisperModel, BatchedInferencePipeline

model = WhisperModel("turbo", device="cuda", compute_type="float16")
batched_model = BatchedInferencePipeline(model=model)
segments, info = batched_model.transcribe("audio.mp3", batch_size=16)

for segment in segments:
    print("[%.2fs -&amp;gt; %.2fs] %s" % (segment.start, segment.end, segment.text))
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Faster Distil-Whisper&lt;/h3&gt; 
&lt;p&gt;The Distil-Whisper checkpoints are compatible with the Faster-Whisper package. In particular, the latest &lt;a href="https://huggingface.co/distil-whisper/distil-large-v3"&gt;distil-large-v3&lt;/a&gt; checkpoint is intrinsically designed to work with the Faster-Whisper transcription algorithm. The following code snippet demonstrates how to run inference with distil-large-v3 on a specified audio file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from faster_whisper import WhisperModel

model_size = "distil-large-v3"

model = WhisperModel(model_size, device="cuda", compute_type="float16")
segments, info = model.transcribe("audio.mp3", beam_size=5, language="en", condition_on_previous_text=False)

for segment in segments:
    print("[%.2fs -&amp;gt; %.2fs] %s" % (segment.start, segment.end, segment.text))
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more information about the distil-large-v3 model, refer to the original &lt;a href="https://huggingface.co/distil-whisper/distil-large-v3"&gt;model card&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Word-level timestamps&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;segments, _ = model.transcribe("audio.mp3", word_timestamps=True)

for segment in segments:
    for word in segment.words:
        print("[%.2fs -&amp;gt; %.2fs] %s" % (word.start, word.end, word.word))
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;VAD filter&lt;/h3&gt; 
&lt;p&gt;The library integrates the &lt;a href="https://github.com/snakers4/silero-vad"&gt;Silero VAD&lt;/a&gt; model to filter out parts of the audio without speech:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;segments, _ = model.transcribe("audio.mp3", vad_filter=True)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The default behavior is conservative and only removes silence longer than 2 seconds. See the available VAD parameters and default values in the &lt;a href="https://github.com/SYSTRAN/faster-whisper/raw/master/faster_whisper/vad.py"&gt;source code&lt;/a&gt;. They can be customized with the dictionary argument &lt;code&gt;vad_parameters&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;segments, _ = model.transcribe(
    "audio.mp3",
    vad_filter=True,
    vad_parameters=dict(min_silence_duration_ms=500),
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Vad filter is enabled by default for batched transcription.&lt;/p&gt; 
&lt;h3&gt;Logging&lt;/h3&gt; 
&lt;p&gt;The library logging level can be configured like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import logging

logging.basicConfig()
logging.getLogger("faster_whisper").setLevel(logging.DEBUG)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Going further&lt;/h3&gt; 
&lt;p&gt;See more model and transcription options in the &lt;a href="https://github.com/SYSTRAN/faster-whisper/raw/master/faster_whisper/transcribe.py"&gt;&lt;code&gt;WhisperModel&lt;/code&gt;&lt;/a&gt; class implementation.&lt;/p&gt; 
&lt;h2&gt;Community integrations&lt;/h2&gt; 
&lt;p&gt;Here is a non exhaustive list of open-source projects using faster-whisper. Feel free to add your project to the list!&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/speaches-ai/speaches"&gt;speaches&lt;/a&gt; is an OpenAI compatible server using &lt;code&gt;faster-whisper&lt;/code&gt;. It's easily deployable with Docker, works with OpenAI SDKs/CLI, supports streaming, and live transcription.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/m-bain/whisperX"&gt;WhisperX&lt;/a&gt; is an award-winning Python library that offers speaker diarization and accurate word-level timestamps using wav2vec2 alignment&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Softcatala/whisper-ctranslate2"&gt;whisper-ctranslate2&lt;/a&gt; is a command line client based on faster-whisper and compatible with the original client from openai/whisper.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/MahmoudAshraf97/whisper-diarization"&gt;whisper-diarize&lt;/a&gt; is a speaker diarization tool that is based on faster-whisper and NVIDIA NeMo.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Purfview/whisper-standalone-win"&gt;whisper-standalone-win&lt;/a&gt; Standalone CLI executables of faster-whisper for Windows, Linux &amp;amp; macOS.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/hedrergudene/asr-sd-pipeline"&gt;asr-sd-pipeline&lt;/a&gt; provides a scalable, modular, end to end multi-speaker speech to text solution implemented using AzureML pipelines.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/zh-plus/Open-Lyrics"&gt;Open-Lyrics&lt;/a&gt; is a Python library that transcribes voice files using faster-whisper, and translates/polishes the resulting text into &lt;code&gt;.lrc&lt;/code&gt; files in the desired language using OpenAI-GPT.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/geekodour/wscribe"&gt;wscribe&lt;/a&gt; is a flexible transcript generation tool supporting faster-whisper, it can export word level transcript and the exported transcript then can be edited with &lt;a href="https://github.com/geekodour/wscribe-editor"&gt;wscribe-editor&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/BANDAS-Center/aTrain"&gt;aTrain&lt;/a&gt; is a graphical user interface implementation of faster-whisper developed at the BANDAS-Center at the University of Graz for transcription and diarization in Windows (&lt;a href="https://apps.microsoft.com/detail/atrain/9N15Q44SZNS2"&gt;Windows Store App&lt;/a&gt;) and Linux.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ufal/whisper_streaming"&gt;Whisper-Streaming&lt;/a&gt; implements real-time mode for offline Whisper-like speech-to-text models with faster-whisper as the most recommended back-end. It implements a streaming policy with self-adaptive latency based on the actual source complexity, and demonstrates the state of the art.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/collabora/WhisperLive"&gt;WhisperLive&lt;/a&gt; is a nearly-live implementation of OpenAI's Whisper which uses faster-whisper as the backend to transcribe audio in real-time.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/BBC-Esq/ctranslate2-faster-whisper-transcriber"&gt;Faster-Whisper-Transcriber&lt;/a&gt; is a simple but reliable voice transcriber that provides a user-friendly interface.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/softcatala/open-dubbing"&gt;Open-dubbing&lt;/a&gt; is open dubbing is an AI dubbing system which uses machine learning models to automatically translate and synchronize audio dialogue into different languages.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/heimoshuiyu/whisper-fastapi"&gt;Whisper-FastAPI&lt;/a&gt; whisper-fastapi is a very simple script that provides an API backend compatible with OpenAI, HomeAssistant, and Konele (Android voice typing) formats.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Model conversion&lt;/h2&gt; 
&lt;p&gt;When loading a model from its size such as &lt;code&gt;WhisperModel("large-v3")&lt;/code&gt;, the corresponding CTranslate2 model is automatically downloaded from the &lt;a href="https://huggingface.co/Systran"&gt;Hugging Face Hub&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;We also provide a script to convert any Whisper models compatible with the Transformers library. They could be the original OpenAI models or user fine-tuned models.&lt;/p&gt; 
&lt;p&gt;For example the command below converts the &lt;a href="https://huggingface.co/openai/whisper-large-v3"&gt;original "large-v3" Whisper model&lt;/a&gt; and saves the weights in FP16:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install transformers[torch]&amp;gt;=4.23

ct2-transformers-converter --model openai/whisper-large-v3 --output_dir whisper-large-v3-ct2
--copy_files tokenizer.json preprocessor_config.json --quantization float16
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;The option &lt;code&gt;--model&lt;/code&gt; accepts a model name on the Hub or a path to a model directory.&lt;/li&gt; 
 &lt;li&gt;If the option &lt;code&gt;--copy_files tokenizer.json&lt;/code&gt; is not used, the tokenizer configuration is automatically downloaded when the model is loaded later.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Models can also be converted from the code. See the &lt;a href="https://opennmt.net/CTranslate2/python/ctranslate2.converters.TransformersConverter.html"&gt;conversion API&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Load a converted model&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Directly load the model from a local directory:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;model = faster_whisper.WhisperModel("whisper-large-v3-ct2")
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/docs/transformers/model_sharing#upload-with-the-web-interface"&gt;Upload your model to the Hugging Face Hub&lt;/a&gt; and load it from its name:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;model = faster_whisper.WhisperModel("username/whisper-large-v3-ct2")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Comparing performance against other implementations&lt;/h2&gt; 
&lt;p&gt;If you are comparing the performance against other Whisper implementations, you should make sure to run the comparison with similar settings. In particular:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Verify that the same transcription options are used, especially the same beam size. For example in openai/whisper, &lt;code&gt;model.transcribe&lt;/code&gt; uses a default beam size of 1 but here we use a default beam size of 5.&lt;/li&gt; 
 &lt;li&gt;Transcription speed is closely affected by the number of words in the transcript, so ensure that other implementations have a similar WER (Word Error Rate) to this one.&lt;/li&gt; 
 &lt;li&gt;When running on CPU, make sure to set the same number of threads. Many frameworks will read the environment variable &lt;code&gt;OMP_NUM_THREADS&lt;/code&gt;, which can be set when running your script:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;OMP_NUM_THREADS=4 python3 my_script.py
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>kijai/ComfyUI-KJNodes</title>
      <link>https://github.com/kijai/ComfyUI-KJNodes</link>
      <description>&lt;p&gt;Various custom nodes for ComfyUI&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;KJNodes for ComfyUI&lt;/h1&gt; 
&lt;p&gt;Various quality of life and masking related -nodes and scripts made by combining functionality of existing nodes for ComfyUI.&lt;/p&gt; 
&lt;p&gt;I know I'm bad at documentation, especially this project that has grown from random practice nodes to... too many lines in one file. I have however started to add descriptions to the nodes themselves, there's a small ? you can click for info what the node does. This is still work in progress, like everything else.&lt;/p&gt; 
&lt;h1&gt;Installation&lt;/h1&gt; 
&lt;ol&gt; 
 &lt;li&gt;Clone this repo into &lt;code&gt;custom_nodes&lt;/code&gt; folder.&lt;/li&gt; 
 &lt;li&gt;Install dependencies: &lt;code&gt;pip install -r requirements.txt&lt;/code&gt; or if you use the portable install, run this in ComfyUI_windows_portable -folder:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;code&gt;python_embeded\python.exe -m pip install -r ComfyUI\custom_nodes\ComfyUI-KJNodes\requirements.txt&lt;/code&gt;&lt;/p&gt; 
&lt;h2&gt;Javascript&lt;/h2&gt; 
&lt;h3&gt;browserstatus.js&lt;/h3&gt; 
&lt;p&gt;Sets the favicon to green circle when not processing anything, sets it to red when processing and shows progress percentage and the length of your queue. Default off, needs to be enabled from options, overrides Custom-Scripts favicon when enabled.&lt;/p&gt; 
&lt;h2&gt;Nodes:&lt;/h2&gt; 
&lt;h3&gt;Set/Get&lt;/h3&gt; 
&lt;p&gt;Javascript nodes to set and get constants to reduce unnecessary lines. Takes in and returns anything, purely visual nodes. On the right click menu of these nodes there's now an options to visualize the paths, as well as option to jump to the corresponding node on the other end.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Known limitations&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Will not work with any node that dynamically sets it's outpute, such as reroute or other Set/Get node&lt;/li&gt; 
 &lt;li&gt;Will not work when directly connected to a bypassed node&lt;/li&gt; 
 &lt;li&gt;Other possible conflicts with javascript based nodes.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ColorToMask&lt;/h3&gt; 
&lt;p&gt;RBG color value to mask, works with batches and AnimateDiff.&lt;/p&gt; 
&lt;h3&gt;ConditioningMultiCombine&lt;/h3&gt; 
&lt;p&gt;Combine any number of conditions, saves space.&lt;/p&gt; 
&lt;h3&gt;ConditioningSetMaskAndCombine&lt;/h3&gt; 
&lt;p&gt;Mask and combine two sets of conditions, saves space.&lt;/p&gt; 
&lt;h3&gt;GrowMaskWithBlur&lt;/h3&gt; 
&lt;p&gt;Grows or shrinks (with negative values) mask, option to invert input, returns mask and inverted mask. Additionally Blurs the mask, this is a slow operation especially with big batches.&lt;/p&gt; 
&lt;h3&gt;RoundMask&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://github.com/kijai/ComfyUI-KJNodes/assets/40791699/52c85202-f74e-4b96-9dac-c8bda5ddcc40" alt="image" /&gt;&lt;/p&gt; 
&lt;h3&gt;WidgetToString&lt;/h3&gt; 
&lt;p&gt;Outputs the value of a widget on any node as a string &lt;img src="https://raw.githubusercontent.com/kijai/ComfyUI-KJNodes/main/docs/images/2024-04-03_20_49_29-ComfyUI.png" alt="example of use" /&gt;&lt;/p&gt; 
&lt;p&gt;Enable node id display from Manager menu, to get the ID of the node you want to read a widget from: &lt;img src="https://raw.githubusercontent.com/kijai/ComfyUI-KJNodes/main/docs/images/319121636-706b5081-9120-4a29-bd76-901691ada688.png" alt="enable node id display" /&gt;&lt;/p&gt; 
&lt;p&gt;Use the node id of the target node, and add the name of the widget to read from &lt;img src="https://raw.githubusercontent.com/kijai/ComfyUI-KJNodes/main/docs/images/319121566-05f66385-7568-4b1f-8bbc-11053660b02f.png" alt="use node id and widget name" /&gt;&lt;/p&gt; 
&lt;p&gt;Recreating or reloading the target node will change its id, and the WidgetToString node will no longer be able to find it until you update the node id value with the new id.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>shiyu-coder/Kronos</title>
      <link>https://github.com/shiyu-coder/Kronos</link>
      <description>&lt;p&gt;Kronos: A Foundation Model for the Language of Financial Markets&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h2&gt;&lt;b&gt;Kronos: A Foundation Model for the Language of Financial Markets &lt;/b&gt;&lt;/h2&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt;  
 &lt;a href="https://huggingface.co/NeoQuasar"&gt; &lt;img src="https://img.shields.io/badge/ü§ó-Hugging_Face-yellow" alt="Hugging Face" /&gt; &lt;/a&gt; 
 &lt;a href="https://shiyu-coder.github.io/Kronos-demo/"&gt; &lt;img src="https://img.shields.io/badge/üöÄ-Live_Demo-brightgreen" alt="Live Demo" /&gt; &lt;/a&gt; 
 &lt;a href="https://github.com/shiyu-coder/Kronos/graphs/commit-activity"&gt; &lt;img src="https://img.shields.io/github/last-commit/shiyu-coder/Kronos?color=blue" alt="Last Commit" /&gt; &lt;/a&gt; 
 &lt;a href="https://github.com/shiyu-coder/Kronos/stargazers"&gt; &lt;img src="https://img.shields.io/github/stars/shiyu-coder/Kronos?color=lightblue" alt="GitHub Stars" /&gt; &lt;/a&gt; 
 &lt;a href="https://github.com/shiyu-coder/Kronos/network/members"&gt; &lt;img src="https://img.shields.io/github/forks/shiyu-coder/Kronos?color=yellow" alt="GitHub Forks" /&gt; &lt;/a&gt; 
 &lt;a href="https://raw.githubusercontent.com/shiyu-coder/Kronos/master/LICENSE"&gt; &lt;img src="https://img.shields.io/github/license/shiyu-coder/Kronos?color=green" alt="License" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;!-- Keep these links. Translations will automatically update with the README. --&gt; 
 &lt;a href="https://zdoc.app/de/shiyu-coder/Kronos"&gt;Deutsch&lt;/a&gt; | 
 &lt;a href="https://zdoc.app/es/shiyu-coder/Kronos"&gt;Espa√±ol&lt;/a&gt; | 
 &lt;a href="https://zdoc.app/fr/shiyu-coder/Kronos"&gt;Fran√ßais&lt;/a&gt; | 
 &lt;a href="https://zdoc.app/ja/shiyu-coder/Kronos"&gt;Êó•Êú¨Ë™û&lt;/a&gt; | 
 &lt;a href="https://zdoc.app/ko/shiyu-coder/Kronos"&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt; | 
 &lt;a href="https://zdoc.app/pt/shiyu-coder/Kronos"&gt;Portugu√™s&lt;/a&gt; | 
 &lt;a href="https://zdoc.app/ru/shiyu-coder/Kronos"&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt; | 
 &lt;a href="https://zdoc.app/zh/shiyu-coder/Kronos"&gt;‰∏≠Êñá&lt;/a&gt; 
&lt;/div&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/shiyu-coder/Kronos/master/figures/logo.png" width="100" /&gt; &lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Kronos is the &lt;strong&gt;first open-source foundation model&lt;/strong&gt; for financial candlesticks (K-lines), trained on data from over &lt;strong&gt;45 global exchanges&lt;/strong&gt;.&lt;/p&gt; 
&lt;/blockquote&gt;  
&lt;h2&gt;üì∞ News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;üö© &lt;strong&gt;[2025.11.10]&lt;/strong&gt; Kronos has been accpeted by AAAI 2026.&lt;/li&gt; 
 &lt;li&gt;üö© &lt;strong&gt;[2025.08.17]&lt;/strong&gt; We have released the scripts for fine-tuning! Check them out to adapt Kronos to your own tasks.&lt;/li&gt; 
 &lt;li&gt;üö© &lt;strong&gt;[2025.08.02]&lt;/strong&gt; Our paper is now available on &lt;a href="https://arxiv.org/abs/2508.02739"&gt;arXiv&lt;/a&gt;!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="center"&gt; &lt;/p&gt;
&lt;h2&gt;üìú Introduction&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Kronos&lt;/strong&gt; is a family of decoder-only foundation models, pre-trained specifically for the "language" of financial markets‚ÄîK-line sequences. Unlike general-purpose TSFMs, Kronos is designed to handle the unique, high-noise characteristics of financial data. It leverages a novel two-stage framework:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;A specialized tokenizer first quantizes continuous, multi-dimensional K-line data (OHLCV) into &lt;strong&gt;hierarchical discrete tokens&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;A large, autoregressive Transformer is then pre-trained on these tokens, enabling it to serve as a unified model for diverse quantitative tasks.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/shiyu-coder/Kronos/master/figures/overview.png" alt="" align="center" width="700px" /&gt; &lt;/p&gt; 
&lt;h2&gt;‚ú® Live Demo&lt;/h2&gt; 
&lt;p&gt;We have set up a live demo to visualize Kronos's forecasting results. The webpage showcases a forecast for the &lt;strong&gt;BTC/USDT&lt;/strong&gt; trading pair over the next 24 hours.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;üëâ &lt;a href="https://shiyu-coder.github.io/Kronos-demo/"&gt;Access the Live Demo Here&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;üì¶ Model Zoo&lt;/h2&gt; 
&lt;p&gt;We release a family of pre-trained models with varying capacities to suit different computational and application needs. All models are readily accessible from the Hugging Face Hub.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Tokenizer&lt;/th&gt; 
   &lt;th&gt;Context length&lt;/th&gt; 
   &lt;th&gt;Params&lt;/th&gt; 
   &lt;th&gt;Open-source&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Kronos-mini&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/NeoQuasar/Kronos-Tokenizer-2k"&gt;Kronos-Tokenizer-2k&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;2048&lt;/td&gt; 
   &lt;td&gt;4.1M&lt;/td&gt; 
   &lt;td&gt;‚úÖ &lt;a href="https://huggingface.co/NeoQuasar/Kronos-mini"&gt;NeoQuasar/Kronos-mini&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Kronos-small&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/NeoQuasar/Kronos-Tokenizer-base"&gt;Kronos-Tokenizer-base&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;512&lt;/td&gt; 
   &lt;td&gt;24.7M&lt;/td&gt; 
   &lt;td&gt;‚úÖ &lt;a href="https://huggingface.co/NeoQuasar/Kronos-small"&gt;NeoQuasar/Kronos-small&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Kronos-base&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/NeoQuasar/Kronos-Tokenizer-base"&gt;Kronos-Tokenizer-base&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;512&lt;/td&gt; 
   &lt;td&gt;102.3M&lt;/td&gt; 
   &lt;td&gt;‚úÖ &lt;a href="https://huggingface.co/NeoQuasar/Kronos-base"&gt;NeoQuasar/Kronos-base&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Kronos-large&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/NeoQuasar/Kronos-Tokenizer-base"&gt;Kronos-Tokenizer-base&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;512&lt;/td&gt; 
   &lt;td&gt;499.2M&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;üöÄ Getting Started&lt;/h2&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install Python 3.10+, and then install the dependencies:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;üìà Making Forecasts&lt;/h3&gt; 
&lt;p&gt;Forecasting with Kronos is straightforward using the &lt;code&gt;KronosPredictor&lt;/code&gt; class. It handles data preprocessing, normalization, prediction, and inverse normalization, allowing you to get from raw data to forecasts in just a few lines of code.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Important Note&lt;/strong&gt;: The &lt;code&gt;max_context&lt;/code&gt; for &lt;code&gt;Kronos-small&lt;/code&gt; and &lt;code&gt;Kronos-base&lt;/code&gt; is &lt;strong&gt;512&lt;/strong&gt;. This is the maximum sequence length the model can process. For optimal performance, it is recommended that your input data length (i.e., &lt;code&gt;lookback&lt;/code&gt;) does not exceed this limit. The &lt;code&gt;KronosPredictor&lt;/code&gt; will automatically handle truncation for longer contexts.&lt;/p&gt; 
&lt;p&gt;Here is a step-by-step guide to making your first forecast.&lt;/p&gt; 
&lt;h4&gt;1. Load the Tokenizer and Model&lt;/h4&gt; 
&lt;p&gt;First, load a pre-trained Kronos model and its corresponding tokenizer from the Hugging Face Hub.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from model import Kronos, KronosTokenizer, KronosPredictor

# Load from Hugging Face Hub
tokenizer = KronosTokenizer.from_pretrained("NeoQuasar/Kronos-Tokenizer-base")
model = Kronos.from_pretrained("NeoQuasar/Kronos-small")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;2. Instantiate the Predictor&lt;/h4&gt; 
&lt;p&gt;Create an instance of &lt;code&gt;KronosPredictor&lt;/code&gt;, passing the model, tokenizer, and desired device.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Initialize the predictor
predictor = KronosPredictor(model, tokenizer, max_context=512)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;3. Prepare Input Data&lt;/h4&gt; 
&lt;p&gt;The &lt;code&gt;predict&lt;/code&gt; method requires three main inputs:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;df&lt;/code&gt;: A pandas DataFrame containing the historical K-line data. It must include columns &lt;code&gt;['open', 'high', 'low', 'close']&lt;/code&gt;. &lt;code&gt;volume&lt;/code&gt; and &lt;code&gt;amount&lt;/code&gt; are optional.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;x_timestamp&lt;/code&gt;: A pandas Series of timestamps corresponding to the historical data in &lt;code&gt;df&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;y_timestamp&lt;/code&gt;: A pandas Series of timestamps for the future periods you want to predict.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import pandas as pd

# Load your data
df = pd.read_csv("./data/XSHG_5min_600977.csv")
df['timestamps'] = pd.to_datetime(df['timestamps'])

# Define context window and prediction length
lookback = 400
pred_len = 120

# Prepare inputs for the predictor
x_df = df.loc[:lookback-1, ['open', 'high', 'low', 'close', 'volume', 'amount']]
x_timestamp = df.loc[:lookback-1, 'timestamps']
y_timestamp = df.loc[lookback:lookback+pred_len-1, 'timestamps']
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;4. Generate Forecasts&lt;/h4&gt; 
&lt;p&gt;Call the &lt;code&gt;predict&lt;/code&gt; method to generate forecasts. You can control the sampling process with parameters like &lt;code&gt;T&lt;/code&gt;, &lt;code&gt;top_p&lt;/code&gt;, and &lt;code&gt;sample_count&lt;/code&gt; for probabilistic forecasting.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Generate predictions
pred_df = predictor.predict(
    df=x_df,
    x_timestamp=x_timestamp,
    y_timestamp=y_timestamp,
    pred_len=pred_len,
    T=1.0,          # Temperature for sampling
    top_p=0.9,      # Nucleus sampling probability
    sample_count=1  # Number of forecast paths to generate and average
)

print("Forecasted Data Head:")
print(pred_df.head())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The &lt;code&gt;predict&lt;/code&gt; method returns a pandas DataFrame containing the forecasted values for &lt;code&gt;open&lt;/code&gt;, &lt;code&gt;high&lt;/code&gt;, &lt;code&gt;low&lt;/code&gt;, &lt;code&gt;close&lt;/code&gt;, &lt;code&gt;volume&lt;/code&gt;, and &lt;code&gt;amount&lt;/code&gt;, indexed by the &lt;code&gt;y_timestamp&lt;/code&gt; you provided.&lt;/p&gt; 
&lt;p&gt;For efficient processing of multiple time series, Kronos provides a &lt;code&gt;predict_batch&lt;/code&gt; method that enables parallel prediction on multiple datasets simultaneously. This is particularly useful when you need to forecast multiple assets or time periods at once.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Prepare multiple datasets for batch prediction
df_list = [df1, df2, df3]  # List of DataFrames
x_timestamp_list = [x_ts1, x_ts2, x_ts3]  # List of historical timestamps
y_timestamp_list = [y_ts1, y_ts2, y_ts3]  # List of future timestamps

# Generate batch predictions
pred_df_list = predictor.predict_batch(
    df_list=df_list,
    x_timestamp_list=x_timestamp_list,
    y_timestamp_list=y_timestamp_list,
    pred_len=pred_len,
    T=1.0,
    top_p=0.9,
    sample_count=1,
    verbose=True
)

# pred_df_list contains prediction results in the same order as input
for i, pred_df in enumerate(pred_df_list):
    print(f"Predictions for series {i}:")
    print(pred_df.head())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Important Requirements for Batch Prediction:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;All series must have the same historical length (lookback window)&lt;/li&gt; 
 &lt;li&gt;All series must have the same prediction length (&lt;code&gt;pred_len&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Each DataFrame must contain the required columns: &lt;code&gt;['open', 'high', 'low', 'close']&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;volume&lt;/code&gt; and &lt;code&gt;amount&lt;/code&gt; columns are optional and will be filled with zeros if missing&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The &lt;code&gt;predict_batch&lt;/code&gt; method leverages GPU parallelism for efficient processing and automatically handles normalization and denormalization for each series independently.&lt;/p&gt; 
&lt;h4&gt;5. Example and Visualization&lt;/h4&gt; 
&lt;p&gt;For a complete, runnable script that includes data loading, prediction, and plotting, please see &lt;a href="https://raw.githubusercontent.com/shiyu-coder/Kronos/master/examples/prediction_example.py"&gt;&lt;code&gt;examples/prediction_example.py&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Running this script will generate a plot comparing the ground truth data against the model's forecast, similar to the one shown below:&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/shiyu-coder/Kronos/master/figures/prediction_example.png" alt="Forecast Example" align="center" width="600px" /&gt; &lt;/p&gt; 
&lt;p&gt;Additionally, we provide a script that makes predictions without Volume and Amount data, which can be found in &lt;a href="https://raw.githubusercontent.com/shiyu-coder/Kronos/master/examples/prediction_wo_vol_example.py"&gt;&lt;code&gt;examples/prediction_wo_vol_example.py&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;üîß Finetuning on Your Own Data (A-Share Market Example)&lt;/h2&gt; 
&lt;p&gt;We provide a complete pipeline for finetuning Kronos on your own datasets. As an example, we demonstrate how to use &lt;a href="https://github.com/microsoft/qlib"&gt;Qlib&lt;/a&gt; to prepare data from the Chinese A-share market and conduct a simple backtest.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Disclaimer:&lt;/strong&gt; This pipeline is intended as a demonstration to illustrate the finetuning process. It is a simplified example and not a production-ready quantitative trading system. A robust quantitative strategy requires more sophisticated techniques, such as portfolio optimization and risk factor neutralization, to achieve stable alpha.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;The finetuning process is divided into four main steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Configuration&lt;/strong&gt;: Set up paths and hyperparameters.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Data Preparation&lt;/strong&gt;: Process and split your data using Qlib.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Model Finetuning&lt;/strong&gt;: Finetune the Tokenizer and the Predictor models.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Backtesting&lt;/strong&gt;: Evaluate the finetuned model's performance.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;First, ensure you have all dependencies from &lt;code&gt;requirements.txt&lt;/code&gt; installed.&lt;/li&gt; 
 &lt;li&gt;This pipeline relies on &lt;code&gt;qlib&lt;/code&gt;. Please install it: &lt;pre&gt;&lt;code class="language-shell"&gt;  pip install pyqlib
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;You will need to prepare your Qlib data. Follow the &lt;a href="https://github.com/microsoft/qlib"&gt;official Qlib guide&lt;/a&gt; to download and set up your data locally. The example scripts assume you are using daily frequency data.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Step 1: Configure Your Experiment&lt;/h3&gt; 
&lt;p&gt;All settings for data, training, and model paths are centralized in &lt;code&gt;finetune/config.py&lt;/code&gt;. Before running any scripts, please &lt;strong&gt;modify the following paths&lt;/strong&gt; according to your environment:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;qlib_data_path&lt;/code&gt;: Path to your local Qlib data directory.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;dataset_path&lt;/code&gt;: Directory where the processed train/validation/test pickle files will be saved.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;save_path&lt;/code&gt;: Base directory for saving model checkpoints.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;backtest_result_path&lt;/code&gt;: Directory for saving backtesting results.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;pretrained_tokenizer_path&lt;/code&gt; and &lt;code&gt;pretrained_predictor_path&lt;/code&gt;: Paths to the pre-trained models you want to start from (can be local paths or Hugging Face model names).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can also adjust other parameters like &lt;code&gt;instrument&lt;/code&gt;, &lt;code&gt;train_time_range&lt;/code&gt;, &lt;code&gt;epochs&lt;/code&gt;, and &lt;code&gt;batch_size&lt;/code&gt; to fit your specific task. If you don't use &lt;a href="https://www.comet.com/"&gt;Comet.ml&lt;/a&gt;, set &lt;code&gt;use_comet = False&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Step 2: Prepare the Dataset&lt;/h3&gt; 
&lt;p&gt;Run the data preprocessing script. This script will load raw market data from your Qlib directory, process it, split it into training, validation, and test sets, and save them as pickle files.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;python finetune/qlib_data_preprocess.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After running, you will find &lt;code&gt;train_data.pkl&lt;/code&gt;, &lt;code&gt;val_data.pkl&lt;/code&gt;, and &lt;code&gt;test_data.pkl&lt;/code&gt; in the directory specified by &lt;code&gt;dataset_path&lt;/code&gt; in your config.&lt;/p&gt; 
&lt;h3&gt;Step 3: Run the Finetuning&lt;/h3&gt; 
&lt;p&gt;The finetuning process consists of two stages: finetuning the tokenizer and then the predictor. Both training scripts are designed for multi-GPU training using &lt;code&gt;torchrun&lt;/code&gt;.&lt;/p&gt; 
&lt;h4&gt;3.1 Finetune the Tokenizer&lt;/h4&gt; 
&lt;p&gt;This step adjusts the tokenizer to the data distribution of your specific domain.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# Replace NUM_GPUS with the number of GPUs you want to use (e.g., 2)
torchrun --standalone --nproc_per_node=NUM_GPUS finetune/train_tokenizer.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The best tokenizer checkpoint will be saved to the path configured in &lt;code&gt;config.py&lt;/code&gt; (derived from &lt;code&gt;save_path&lt;/code&gt; and &lt;code&gt;tokenizer_save_folder_name&lt;/code&gt;).&lt;/p&gt; 
&lt;h4&gt;3.2 Finetune the Predictor&lt;/h4&gt; 
&lt;p&gt;This step finetunes the main Kronos model for the forecasting task.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# Replace NUM_GPUS with the number of GPUs you want to use (e.g., 2)
torchrun --standalone --nproc_per_node=NUM_GPUS finetune/train_predictor.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The best predictor checkpoint will be saved to the path configured in &lt;code&gt;config.py&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Step 4: Evaluate with Backtesting&lt;/h3&gt; 
&lt;p&gt;Finally, run the backtesting script to evaluate your finetuned model. This script loads the models, performs inference on the test set, generates prediction signals (e.g., forecasted price change), and runs a simple top-K strategy backtest.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# Specify the GPU for inference
python finetune/qlib_test.py --device cuda:0
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The script will output a detailed performance analysis in your console and generate a plot showing the cumulative return curves of your strategy against the benchmark, similar to the one below:&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/shiyu-coder/Kronos/master/figures/backtest_result_example.png" alt="Backtest Example" align="center" width="700px" /&gt; &lt;/p&gt; 
&lt;h3&gt;üí° From Demo to Production: Important Considerations&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Raw Signals vs. Pure Alpha&lt;/strong&gt;: The signals generated by the model in this demo are raw predictions. In a real-world quantitative workflow, these signals would typically be fed into a portfolio optimization model. This model would apply constraints to neutralize exposure to common risk factors (e.g., market beta, style factors like size and value), thereby isolating the &lt;strong&gt;"pure alpha"&lt;/strong&gt; and improving the strategy's robustness.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Data Handling&lt;/strong&gt;: The provided &lt;code&gt;QlibDataset&lt;/code&gt; is an example. For different data sources or formats, you will need to adapt the data loading and preprocessing logic.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Strategy and Backtesting Complexity&lt;/strong&gt;: The simple top-K strategy used here is a basic starting point. Production-level strategies often incorporate more complex logic for portfolio construction, dynamic position sizing, and risk management (e.g., stop-loss/take-profit rules). Furthermore, a high-fidelity backtest should meticulously model transaction costs, slippage, and market impact to provide a more accurate estimate of real-world performance.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;üìù AI-Generated Comments&lt;/strong&gt;: Please note that many of the code comments within the &lt;code&gt;finetune/&lt;/code&gt; directory were generated by an AI assistant (Gemini 2.5 Pro) for explanatory purposes. While they aim to be helpful, they may contain inaccuracies. We recommend treating the code itself as the definitive source of logic.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;üìñ Citation&lt;/h2&gt; 
&lt;p&gt;If you use Kronos in your research, we would appreciate a citation to our &lt;a href="https://arxiv.org/abs/2508.02739"&gt;paper&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@misc{shi2025kronos,
      title={Kronos: A Foundation Model for the Language of Financial Markets}, 
      author={Yu Shi and Zongliang Fu and Shuo Chen and Bohan Zhao and Wei Xu and Changshui Zhang and Jian Li},
      year={2025},
      eprint={2508.02739},
      archivePrefix={arXiv},
      primaryClass={q-fin.ST},
      url={https://arxiv.org/abs/2508.02739}, 
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;üìú License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the &lt;a href="https://raw.githubusercontent.com/shiyu-coder/Kronos/master/LICENSE"&gt;MIT License&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>rossant/awesome-math</title>
      <link>https://github.com/rossant/awesome-math</link>
      <description>&lt;p&gt;A curated list of awesome mathematics resources&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Awesome Math &lt;a href="https://github.com/sindresorhus/awesome"&gt;&lt;img src="https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg?sanitize=true" alt="Awesome" /&gt;&lt;/a&gt;&lt;/h1&gt; 
&lt;p&gt;A curated list of awesome mathematics resources.&lt;/p&gt; 
&lt;p&gt;All resources are freely available except those with a üí≤ icon.&lt;/p&gt; 
&lt;h1&gt;Contents&lt;/h1&gt; 
&lt;!-- START_TOC --&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#contents"&gt;Contents&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#general-resources"&gt;General Resources&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#learning-platforms"&gt;Learning Platforms&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#learn-to-learn"&gt;Learn to Learn&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#youtube-series"&gt;Youtube Series&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#tools"&gt;Tools&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#questions-and-answers"&gt;Questions and Answers&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#encyclopedia"&gt;Encyclopedia&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#books"&gt;Books&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#magazines"&gt;Magazines&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#blogs"&gt;Blogs&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#meetings-and-conferences"&gt;Meetings and Conferences&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#misc"&gt;Misc&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#branches-of-mathematics"&gt;Branches of Mathematics&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#foundations-of-mathematics"&gt;Foundations of Mathematics&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#transition-to-pure-rigour-math"&gt;Transition To Pure Rigour Math&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#set-theory"&gt;Set Theory&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#logic"&gt;Logic&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#category-theory"&gt;Category Theory&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#type-theory"&gt;Type Theory&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#homotopy-type-theory"&gt;Homotopy Type Theory&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#surreal-numbers"&gt;Surreal Numbers&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#number-theory"&gt;Number Theory&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#algebraic-number-theory"&gt;Algebraic Number Theory&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#analytic-number-theory"&gt;Analytic Number Theory&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#algebra"&gt;Algebra&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#abstract-algebra"&gt;Abstract Algebra&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#group-theory"&gt;Group Theory&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#linear-algebra"&gt;Linear Algebra&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#ring-theory"&gt;Ring Theory&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#galois-theory"&gt;Galois Theory&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#lie-algebras"&gt;Lie Algebras&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#combinatorics"&gt;Combinatorics&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#graph-theory"&gt;Graph Theory&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#geometry-and-topology"&gt;Geometry and Topology&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#differential-geometry"&gt;Differential Geometry&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#algebraic-geometry"&gt;Algebraic Geometry&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#algebraic-statistics"&gt;Algebraic Statistics&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#topology"&gt;Topology&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#algebraic-topology"&gt;Algebraic Topology&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#analysis"&gt;Analysis&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#real-analysis"&gt;Real Analysis&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#harmonic-analysis"&gt;Harmonic Analysis&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#complex-analysis"&gt;Complex Analysis&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#functional-analysis"&gt;Functional Analysis&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#measure-theory"&gt;Measure Theory&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#ordinary-differential-equations"&gt;Ordinary Differential Equations&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#partial-differential-equations"&gt;Partial Differential Equations&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#chaos-theory"&gt;Chaos Theory&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#probability-and-statistics"&gt;Probability and Statistics&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#probability-theory"&gt;Probability Theory&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#statistics"&gt;Statistics&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#statistical-learning"&gt;Statistical Learning&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#stochastic-processes"&gt;Stochastic processes&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#numerical-analysis"&gt;Numerical Analysis&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#signal-processing"&gt;Signal processing&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#mathematics-for-computer-science"&gt;Mathematics for Computer Science&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#mathematical-biology"&gt;Mathematical Biology&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#mathematical-physics"&gt;Mathematical Physics&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#students-lecture-notes"&gt;Students Lecture Notes&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#related-awesome-lists"&gt;Related Awesome Lists&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/rossant/awesome-math/master/#license"&gt;License&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- END_TOC --&gt; 
&lt;h1&gt;General Resources&lt;/h1&gt; 
&lt;h2&gt;Learning Platforms&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.khanacademy.org/math"&gt;Khan Academy&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.coursera.org/courses?query=mathematics&amp;amp;languages=en"&gt;Coursera&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://ocw.mit.edu/courses/mathematics/"&gt;MIT OpenCourseWare&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.edx.org/course/subject/math"&gt;edX&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://brilliant.org/courses/#math-foundational"&gt;Brilliant&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://misterwootube.com/"&gt;WooTube&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://mathigon.org/"&gt;Mathigon&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://calculus.org/"&gt;Calculus.org&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ximera.osu.edu/"&gt;Ximera&lt;/a&gt; : free interactive mathematics textbooks (Ohio State University)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.almostfun.org/lessons/"&gt;Almost Fun&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/c/OxfordMathematics"&gt;Oxford Mathematics&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://mathacademy.com/"&gt;Math Academy&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Learn to Learn&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/nelson-brochado/understanding-math"&gt;Understanding Mathematics&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Youtube Series&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/@BrandonFoltz"&gt;Brandon Foltz&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/channel/UCtYLUTtgS3k1Fg4y5tAhLbw"&gt;StatQuest&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/@3blue1brown"&gt;3Blue1Brown&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/@iit"&gt;NPTEL&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/@patrickjmt"&gt;PatrickJMT&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/@ProfessorLeonard"&gt;Professor Leonard&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://www.youtube.com/playlist?list=PLDesaqWTN6ESsmwELdrzhcGiRhk5DjwLP"&gt;Precalculus - College Algebra/Trigonometry&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.youtube.com/playlist?list=PLF797E961509B4EB5"&gt;Calculus 1&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.youtube.com/playlist?list=PLDesaqWTN6EQ2J4vgsN1HyBeRADEh4Cw-"&gt;Calculus 2&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.youtube.com/playlist?list=PLDesaqWTN6ESk16YRmzuJ8f6-rnuy0Ry7"&gt;Calculus 3&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.youtube.com/playlist?list=PLDesaqWTN6ESPaHy2QUKVaXNZuQNxkYQ_"&gt;Differential Equations&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.youtube.com/playlist?list=PLDesaqWTN6ETc1ZwHWijCBcZ2gOvS2tTN"&gt;To The Point Math&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/@crashcourse"&gt;Crash Course&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/@harvard"&gt;Harvard&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/@mitocw"&gt;MIT OpenCourseWare&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/@Mathologer"&gt;Mathologer&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/@TheMathDistrict"&gt;The Math District&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/@mathematicalmonk"&gt;Mathematical Monk&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/@TheMathSorcerer"&gt;The Math Sorcerer&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Tools&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.symbolab.com/"&gt;Symbolab&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.desmos.com/calculator"&gt;Desmos&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://www.mathwords.com/"&gt;Math Words&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://www.wolframalpha.com/"&gt;Wolfram Alpha&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://maxima.sourceforge.io/"&gt;Maxima&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.sympy.org/"&gt;Sympy&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://www.sagemath.org/"&gt;Sagemath&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Nonanti/MathFlow"&gt;MathFlow&lt;/a&gt; - C# math expression library with symbolic computation (differentiation, simplification, equation solving)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://unitconverters.net"&gt;Unit Converter&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.geogebra.org/?lang=en"&gt;GeoGebra&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://www2.macaulay2.com/Macaulay2/"&gt;Macaulay2&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.singular.uni-kl.de/"&gt;Singular&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.gnu.org/software/octave/"&gt;GNU Octave&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://magma.maths.usyd.edu.au/magma/"&gt;Magma&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.maplesoft.com/products/Maple/"&gt;Maple&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.mathworks.com/products/matlab.html"&gt;Matlab&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.wolfram.com/mathematica/"&gt;Wolfram Mathematica&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://freemathapp.org"&gt;Free Math&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://chrome.google.com/webstore/detail/xhub/anidddebgkllnnnnjfkmjcaallemhjee"&gt;xhub&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.copypastemathjax.com/"&gt;CopyPasteMathjax&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.financecharts.com/pages/5724-retirement-calculators-and-stock-market-tips"&gt;Finance calculators&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://mathcheap.xyz"&gt;Mathcheap&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://midpointcalculator.co"&gt;Midpoint Calculator&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://quartilecalculator.net"&gt;Quartiles Calculator&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://corca.io/"&gt;Corca Editor&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/runmat-org/runmat"&gt;RunMat&lt;/a&gt; - Runtime for MATLAB-syntax array math with automatic CPU/GPU execution.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/sepcostructural/structural-engineering-tools"&gt;Structural Engineering Tools (SEPCO Engineering)&lt;/a&gt; - Free online calculators for beam diagrams, Canadian steel section properties, and pressure conversions.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Questions and Answers&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="http://math.stackexchange.com/"&gt;Mathematics Stack Exchange&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://mathoverflow.net/"&gt;MathOverflow&lt;/a&gt; - for professional mathematicians&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Encyclopedia&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.encyclopediaofmath.org"&gt;Encyclopedia of Mathematics&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://planetmath.org/"&gt;Planetmath&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://proofwiki.org/wiki/Main_Page"&gt;ProofWiki&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://mathworld.wolfram.com/"&gt;Wolfram Mathworld&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://oeis.org"&gt;The On-Line Encyclopedia of Integer Sequences&lt;/a&gt; - Great compendium of many different integer sequences. Founded 1964 by N. J. A. Sloane.&lt;/li&gt; 
 &lt;li&gt;üí≤ &lt;a href="https://press.princeton.edu/books/hardcover/9780691118802/the-princeton-companion-to-mathematics"&gt;The Princeton Companion to Mathematics&lt;/a&gt; - Timothy Gowers (Professor, Fields medallist), June Barrow-Green (Professor), and Imre Leader (Professor).&lt;/li&gt; 
 &lt;li&gt;üí≤ &lt;a href="https://link.springer.com/book/10.1007/978-3-662-52844-0"&gt;Encyclopedia of Distances (4th Edition)&lt;/a&gt; - Michel Marie Deza, Elena Deza.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Books&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://archive.org/details/TarasovCalculus"&gt;Calculus: Basic Concepts for High Schools&lt;/a&gt; - L.V. Tarasov&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://www.cis.upenn.edu/~jean/math-basics.pdf"&gt;Basics of Algebra, Topology, and Differential Calculus&lt;/a&gt; - Jean Gallier (University of Pennsylvania)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://people.math.gatech.edu/%7Ecain/notes/calculus.html"&gt;Multivariable Calculus&lt;/a&gt; - G. Cain, J. Herod (Georgia Tech)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://en.wikibooks.org/wiki/Wikibooks:Mathematics_bookshelf"&gt;Wikibooks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://people.math.gatech.edu/~cain/textbooks/onlinebooks.html"&gt;Online Mathematics Textbooks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://www.wallace.ccfaculty.org/book/Beginning_and_Intermediate_Algebra.pdf"&gt;Beginning and Intermediate Algebra&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/EbookFoundation/free-programming-books/raw/master/books/free-programming-books-subjects.md#mathematics"&gt;Free Mathematics Books&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://www.mecmath.net/trig/trigbook.pdf"&gt;Trigonometry&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.manning.com/books/math-for-frontend-web-dev"&gt;Math for Frontend Web Dev&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.manning.com/books/grokking-statistics"&gt;Grokking Statistics&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Magazines&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.quantamagazine.org/mathematics/"&gt;Quanta Magazine&lt;/a&gt; - Features latest research breakthroughs in an accessible style for non-experts.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.ams.org/journals/bull/all_issues.html"&gt;Bulletin of the American Mathematical Society&lt;/a&gt; - Expository articles on contemporary mathematical research, written in a way that gives insight to mathematicians who may not be experts in the particular topic.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://www.ams.org/cgi-bin/notices/amsnotices.pl?article_id=fullissue&amp;amp;article_type=gallery&amp;amp;gallery_type=fullissue"&gt;Notices of the American Mathematical Society&lt;/a&gt; - Publicizes activities of the Society and features surveys, reports, news, announcements, and opinions on industry trends, academia, and research.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://euromathsoc.org/magazine"&gt;European Mathematical Society Magazine&lt;/a&gt; - The Magazine features announcements about meetings and conferences, articles outlining current trends in scientific development, reports on member societies, and many other informational items.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ima.org.uk/publications/mathematics-today/"&gt;Mathematics Today by Institute of Mathematics and its Applications&lt;/a&gt; - News, opinions, and articles related to mathematics, so the reader stays updated.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://cms.math.ca/publications/crux/"&gt;Crux Mathematicorum by Canadian Mathematical Society&lt;/a&gt; - source of unique and challenging mathematical problems designed for the secondary and undergraduate levels. It includes an Olympiad Corner which is helpful for math competitions.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Blogs&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://betterexplained.com/"&gt;BetterExplained&lt;/a&gt; - Maintained by Kalid Azad&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://ilovemaths.com/"&gt;ILoveMaths&lt;/a&gt; - For grades 6 thru 12 in K-12 system&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.3blue1brown.com/"&gt;3blue1brown&lt;/a&gt; - Animated Maths&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.mathsisfun.com"&gt;Mathsisfun&lt;/a&gt; simple text lightweight site for students up to highschool&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://calculus123.com/wiki/Peter_Saveliev"&gt;MathematicsIsAScience&lt;/a&gt; - Peter Saveliev (Professor of mathematics at Marshall University, Huntington WV, USA)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Meetings and Conferences&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://mathsjam.com/"&gt;MathsJam&lt;/a&gt; - monthly local recreational maths/puzzle meetups and an annual gathering in Staffordshire, England&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://talkingmathsinpublic.uk/"&gt;Talking Maths in Public&lt;/a&gt; - a conference for maths communicators, running every two years, usually in the UK&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.bridgesmathart.org/"&gt;Bridges&lt;/a&gt; - an annual conference on mathematical connections in art, music, architecture, and culture. The 2025 meeting is in Eindhoven, Netherlands.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Misc&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Areas_of_mathematics"&gt;Areas of mathematics on Wikipedia&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://tutorial.math.lamar.edu/"&gt;Paul's Online Math Notes&lt;/a&gt; - Paul Dawkins (Lamar University)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://faculty.atu.edu/mfinan/nnotes.html"&gt;List of electronic textbooks&lt;/a&gt; - Marcel B. Finan (Arkansas Tech University)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://at.yorku.ca/topology/"&gt;Topology Atlas&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://djm.cc/library/Recreations_in_Mathematics_Licks_edited.pdf"&gt;Recreations in Math&lt;/a&gt; - H. E. Licks (1917)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://djm.cc/library/Magic_Squares_Cubes_Andrews_edited.pdf"&gt;Magic Squares and Cubes&lt;/a&gt; - W. S. Andrews (1917)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://web.stanford.edu/~boyd/cvxbook/"&gt;Convex Optimization&lt;/a&gt; - Stephen Boyd and Lieven Vandenberghe&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://fabricebaudoin.wordpress.com/"&gt;Fabrice Baudoin's Notes&lt;/a&gt; - Both research and lecture notes on many topics, Including Diffusions on foliated manifold, Stochastic Calculus, Global analysis in Dirichlet spaces, and more.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Branches of Mathematics&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;Content Format&lt;/strong&gt; &lt;br /&gt; üìñ Books &lt;br /&gt; üé• Videos &lt;br /&gt; üìù Lecture notes, slides, articles, papers&lt;/p&gt; 
&lt;h2&gt;Foundations of Mathematics&lt;/h2&gt; 
&lt;h3&gt;Transition To Pure Rigour Math&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.trillia.com/zakon1.html"&gt;Basic Concepts of Mathematics&lt;/a&gt; - Elias Zakon&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://richardhammack.github.io/BookOfProof/"&gt;Book of Proof&lt;/a&gt; - Richard Hammak (Virginia Commonwealth University)&lt;/li&gt; 
 &lt;li&gt;üìñ &lt;a href="https://ia800501.us.archive.org/7/items/how-to-prove-it-a-structured-approach-daniel-j.-velleman/How%20to%20Prove%20It%20A%20Structured%20Approach%20%28Daniel%20J.%20Velleman%29.pdf"&gt;How to Prove It: A Structured Approach (3rd Edition)&lt;/a&gt; - Daniel J. Velleman (Professor).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Set Theory&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.cosc.brocku.ca/~duentsch/papers/methprimer1.html"&gt;Sets, Relations, Functions&lt;/a&gt; - Ivo D√ºntsch, G√ºnther Gediga&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.math.toronto.edu/weiss/set_theory.pdf"&gt;An Introduction to Set Theory&lt;/a&gt; - William A. R. Weiss&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.settheory.net/"&gt;Set Theory and Foundations of Mathematics&lt;/a&gt; - Sylvain Poirier&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://plato.stanford.edu/entries/set-theory/"&gt;Set Theory on the Stanford Encyclopedia of Philosophy&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Logic&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìù &lt;a href="https://pdfs.semanticscholar.org/6967/f52773d9c2ccfc94658657a5761e0f00e95a.pdf"&gt;Introduction to Logic&lt;/a&gt; - Michael Genesereth, Eric Kao (Stanford University)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://www.fecundity.com/codex/forallx.pdf"&gt;An Introduction to Formal Logic&lt;/a&gt; - P.D. Magnus (University at Albany)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://euclid.trentu.ca/math/sb/pcml/pcml-16.pdf"&gt;A Problem Course in Mathematical Logic&lt;/a&gt; - Stefan Bilaniuk (Trent University)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://poincare.matf.bg.ac.rs/~zarkom/Book_Math__Cutland_Computability.pdf"&gt;Computability - An introduction to recursive function theory&lt;/a&gt; - Nigel Cutland (University of Hull)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://homepages.uc.edu/~martinj/Symbolic_Logic/341%20Syllabus,%20Textbook,%20Handouts,%20Notes/LPL%20textbook.pdf"&gt;Language, Proof, and Logic&lt;/a&gt; - Jon Barwise, John Etchemendy&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.mathematik.uni-muenchen.de/~schwicht/lectures/logic/ws03/ml.pdf"&gt;Mathematical Logic&lt;/a&gt; - Helmut Schwichtenberg&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.personal.psu.edu/t20/notes/logic.pdf"&gt;Mathematical Logic&lt;/a&gt; - Stephen G. Simpson (Pennsylvania State University)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://maude.sip.ucm.es/~miguelpt/papers/flogic.pdf"&gt;Formal Logic&lt;/a&gt; - Miguel Palomino&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://web.math.princeton.edu/~nelson/books/pa.pdf"&gt;Predictive Arithmetic&lt;/a&gt; - Edward Nelson&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://people.uleth.ca/~dave.morris/books/proofs+concepts.html"&gt;Proofs and Concepts: the fundamentals of abstract mathematics&lt;/a&gt; - Joy Morris, Dave Morris&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://www.tedsundstrom.com/mathreasoning"&gt;Mathematical Reasoning: Writing and Proof&lt;/a&gt; - Ted Sundstrom&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://leanprover.github.io/logic_and_proof/"&gt;Logic and Proof&lt;/a&gt; - Jeremy Avigad, Robert Y. Lewis, and Floris van Doorn&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://teorth.github.io/QED"&gt;QED - an interactive textbook&lt;/a&gt; - Terence Tao&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://builds.openlogicproject.org/"&gt;Open Logic Textbook&lt;/a&gt; - collaborative effort, main contributors listed &lt;a href="https://openlogicproject.org/people/"&gt;here&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Category Theory&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.mathematik.tu-darmstadt.de/~streicher/CTCL.pdf"&gt;Introduction to Category Theory and Categorical Logic&lt;/a&gt; - Thomas Streicher&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.cs.man.ac.uk/~hsimmons/zCATS.pdf"&gt;An Introduction to Category Theory&lt;/a&gt; - Harold Simmons&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.211.4754&amp;amp;rep=rep1&amp;amp;type=pdf"&gt;Category Theory&lt;/a&gt; - Steve Awodey (Carnegie Mellon University)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.mathematik.uni-muenchen.de/~pareigis/Vorlesungen/04SS/Cats1.pdf"&gt;Category Theory&lt;/a&gt; - B. Pareigis&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://web.archive.org/web/20181221233252/http://www.math.mcgill.ca/triples/Barr-Wells-ctcs.pdf"&gt;Category Theory for Computing Science&lt;/a&gt; - Michael Barr, Charles Wells&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.tac.mta.ca/tac/reprints/articles/12/tr12.pdf"&gt;Toposes, Triples and Theories&lt;/a&gt; - Michael Barr, Charles Wells&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.tac.mta.ca/tac/reprints/articles/3/tr3abs.html"&gt;Abelian Categories&lt;/a&gt; - Peter Freyd&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.tac.mta.ca/tac/reprints/articles/7/tr7abs.html"&gt;Categories and Groupoids&lt;/a&gt; - P. J. Higgins&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.tac.mta.ca/tac/reprints/articles/10/tr10abs.html"&gt;Basic Concepts of Enriched Category Theory&lt;/a&gt; - G. M. Kelley&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.tac.mta.ca/tac/reprints/articles/17/tr17abs.html"&gt;Abstract and Concrete Categories: The Joy of Cats&lt;/a&gt; - Jiri Adamek, Horst Herrlich, George Strecker&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://math.mit.edu/~dspivak/teaching/sp18/7Sketches.pdf"&gt;Seven Sketches in Compositionality: An Invitation to Applied Category Theory&lt;/a&gt; - Brendan Fong and David I. Spivak (MIT)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.math.jhu.edu/~eriehl/context/"&gt;Category Theory in Context&lt;/a&gt; - Emily Riehl (John Hopkins University)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Type Theory&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.paultaylor.eu/stable/prot.pdf"&gt;Proofs and Types&lt;/a&gt; - Jean-Yves Girard&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://archive-pml.github.io/martin-lof/pdfs/Bibliopolis-Book-retypeset-1984.pdf"&gt;Intuitionistic Type Theory&lt;/a&gt; - Per Martin-Lof&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://www.cs.kent.ac.uk/people/staff/sjt/TTFP/"&gt;Type Theory and Functional Programming&lt;/a&gt; - Simon Thompson&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.cse.chalmers.se/research/group/logic/book/book.pdf"&gt;Programming in Martin-Lof‚Äôs Type Theory&lt;/a&gt; - Bengt Nordstrom, Kent Petersson, Jan M. Smith&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Homotopy Type Theory&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìù &lt;a href="https://hottheory.files.wordpress.com/2013/03/hott-online-611-ga1a258c.pdf"&gt;Homotopy Type Theory&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Surreal Numbers&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.math.harvard.edu/~knill/teaching/mathe320_2015_fall/blog15/surreal1.pdf"&gt;Surreal Numbers - How two ex-students turned on to pure mathematics and found total happiness&lt;/a&gt; - D. E. Knuth&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://web.mit.edu/sp.268/www/2010/surreal.pdf"&gt;Surreal Numbers and Games&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.ohio.edu/people/ehrlich/ConwayNames.pdf"&gt;Conway names, the simplicity hierarchy and the surreal number tree&lt;/a&gt; - Philip Ehrlich&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Number Theory&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìù &lt;a href="http://wstein.org/ent/ent.pdf"&gt;Elementary Number Theory: Primes, Congruences, and Secrets&lt;/a&gt; - William Stein&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://math.utoledo.edu/~codenth/Spring_13/3200/ENT-books/Elementary_Number_Theory-Clark.pdf"&gt;Elementary Number Theory&lt;/a&gt; - W. Edwin Clark (University of South Florida)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.maths.qmul.ac.uk/~pjc/notes/nt.pdf"&gt;A Course on Number Theory&lt;/a&gt; - Peter J. Cameron&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://shoup.net/ntb/ntb-v2.pdf"&gt;A Computational Introduction to Number Theory and Algebra&lt;/a&gt; - Victor Shoup&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://alpha.math.uga.edu/~pete/4400FULL.pdf"&gt;Number Theory: A Contemporary Introduction&lt;/a&gt; - Pete L. Clark&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.trillia.com/moser-number.html"&gt;An Introduction to the Theory of Numbers&lt;/a&gt; - Leo Moser&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://www.poritz.net/jonathan/share/yaintt/"&gt;Yet Another Introductory Number Theory Textbook&lt;/a&gt; - Jonathan A. Poritz&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Algebraic Number Theory&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìù &lt;a href="https://feog.github.io/ANT10.pdf"&gt;Introduction to Algebraic Number Theory&lt;/a&gt; - F. Oggier&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.jmilne.org/math/CourseNotes/ANT.pdf"&gt;Algebraic Number Theory&lt;/a&gt; - J.S. Milne&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://people.math.gatech.edu/~mbaker/pdf/ANTBook.pdf"&gt;Algebraic Number Theory Course Notes&lt;/a&gt; - Matthew Baker (Georgia Tech)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.math.uiuc.edu/~r-ash/ANT.html"&gt;A Course In Algebraic Number Theory&lt;/a&gt; - Robert Ash&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Analytic Number Theory&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.math.uiuc.edu/~hildebr/ant/main.pdf"&gt;Introduction to Analytic Number Theory&lt;/a&gt; - A.J. Hildebrand (University of Illinois)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://math.nsc.ru/~vdovin/lectures/numth_eng.pdf"&gt;Elements of Analytic Number Theory&lt;/a&gt; - P. S. Kolesnikov, E. P. Vdovin (Novosibirsk)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.mathematik.uni-muenchen.de/~forster/v/ann/annth_all.pdf"&gt;Analytic Number Theory&lt;/a&gt; - Otto Forster (LMU Munich)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www2.math.uu.se/~astrombe/analtalt08/www_notes.pdf"&gt;Analytic Number Theory - Lecture Notes based on Davenport‚Äôs book&lt;/a&gt; - Andreas Str√∂mbergsson&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Algebra&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.math.uwaterloo.ca/~snburris/htdocs/ualg.html"&gt;A Course in Universal Algebra&lt;/a&gt; - S. Burris, H.P. Sankappanavar&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://faculty.math.illinois.edu/~r-ash/ComAlg.html"&gt;A Course in Commutative Algebra&lt;/a&gt; - Robert Ash&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://djm.cc/library/First_Algebra_Hawkes_Luby_Touton_edited.pdf"&gt;First Course in Algebra&lt;/a&gt; - Herbert E. Hawkes, William A. Luby, Frank C. Touton (1910)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://djm.cc/library/Second_Algebra_Hawkes_Luby_Touton_edited.pdf"&gt;Second Course in Algebra&lt;/a&gt; - Herbert E. Hawkes, William A. Luby, Frank C. Touton (1911)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://djm.cc/library/Algebra_Elementary_Text-Book_Part_I_Chrystal_edited.pdf"&gt;Algebra: An Elementary Text-Book, Part I&lt;/a&gt; - G. Chrystal (1904)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://djm.cc/library/Algebra_Elementary_Text-Book_Part_II_Chrystal_edited02.pdf"&gt;Algebra: An Elementary Text-Book, Part II&lt;/a&gt; - G. Chrystal (1900)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://jamesbrennan.org/algebra"&gt;Understanding Algebra&lt;/a&gt; - James W. Brennan&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Abstract Algebra&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìù &lt;a href="https://zodml.org/sites/default/files/Introduction_to_Abstract_Algebra_0.pdf"&gt;Introduction to Abstract Algebra&lt;/a&gt; - D. S. Malik, John N. Mordeson, M.K. Sen (Creighton University)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://aleph0.clarku.edu/~djoyce/ma225/algebra.pdf"&gt;Introduction to Modern Algebra&lt;/a&gt; - David Joyce (Clark University)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://feog.github.io/AA11.pdf"&gt;Algebraic Methods&lt;/a&gt; - F. Oggier&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://abstract.ups.edu/download/aata-20150812.pdf"&gt;Abstract Algebra : Theory and Applications&lt;/a&gt; - Thomas W. Judson, Robert A. Beezer (Austin State University)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.maths.usyd.edu.au/u/bobh/UoS/rfwhole.pdf"&gt;An Undergraduate Course in Abstract Algebra&lt;/a&gt; - Robert Howlett&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.math.miami.edu/~ec/book"&gt;Elements of Abstract and Linear Algebra&lt;/a&gt; - E.H. Connell (University of Miami)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.math.uiuc.edu/~r-ash/Algebra.html"&gt;Abstract Algebra: The Basic Graduate Year&lt;/a&gt; - Robert Ash&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://web.archive.org/web/20150528171650/extension.harvard.edu/open-learning-initiative/abstract-algebra"&gt;Abstract Algebra: Harvard Extension (Archived)&lt;/a&gt; - Benedict Gross&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://www.youtube.com/playlist?list=PLA58AC5CABC1321A3"&gt;Abstract Algebra: Harvard Extension Videos&lt;/a&gt; - Benedict Gross&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Group Theory&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìù &lt;a href="https://www2.bc.edu/mark-reeder/Groups.pdf"&gt;Notes on Group Theory&lt;/a&gt; - Mark Reeder&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.jmilne.org/math/CourseNotes/GT.pdf"&gt;Group Theory&lt;/a&gt; - J.S. Milne&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.maths.qmul.ac.uk/~pjc/notes/gt.pdf"&gt;Notes on Finite Group Theory&lt;/a&gt; - Peter J. Cameron&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.cns.gatech.edu/GroupTheory/index.html"&gt;Group Theory&lt;/a&gt; - Pedrag Civitanovic&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Linear Algebra&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.math.ubc.ca/~carrell/NB.pdf"&gt;Fundamentals of Linear Algebra&lt;/a&gt; - James B. Carrell&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://web.archive.org/web/20140824074655/http://mathstat.helsinki.fi/~fluch/linear_algebra_1-sp07/la1.pdf"&gt;Linear Algebra and Matrices&lt;/a&gt; - Martin Fluch&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.maths.usyd.edu.au/u/bobh/UoS/MATH2902/vswhole.pdf"&gt;Vector Space Theory&lt;/a&gt; - Robert Howlett&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://joshua.smcvt.edu/linearalgebra"&gt;Linear Algebra&lt;/a&gt; - Jim Hefferon&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://github.com/juanklopper/MIT_OCW_Linear_Algebra_18_06"&gt;MIT OpenCourseWare Lectures on Linear Algebra (18.06) as Jupyter Notebooks&lt;/a&gt; - Juan Klopper&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.numbertheory.org/book/"&gt;Elementary Linear Algebra&lt;/a&gt; - Keith Matthews&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://linear.ups.edu/"&gt;A First Courses in Linear Algebra&lt;/a&gt; - Rob Breezer&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://www.math.ucdavis.edu/~linear/"&gt;Linear Algebra&lt;/a&gt; - David Cherney, Tom Denton, Andrew Waldron&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://oaktrust.library.tamu.edu/handle/1969.1/2502"&gt;Introduction to vectors and tensors, Vol 1: linear and multilinear algebra&lt;/a&gt; - Ray M Bowen, C. C. Wang&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://oaktrust.library.tamu.edu/handle/1969.1/3609"&gt;Introduction to vectors and tensors, Vol 2: vector and tensor analysis&lt;/a&gt; - Ray M Bowen, C. C. Wang&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://web.stanford.edu/~boyd/vmls/vmls.pdf"&gt;Introduction to Applied Linear Algebra&lt;/a&gt; - Stephen Boyd (Stanford University), Lieven Vandenberghe (UCLA)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://www.math.brown.edu/~treil/papers/LADW/LADW_2017-09-04.pdf"&gt;Linear Algebra Done Wrong&lt;/a&gt; - Sergei Treil&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://immersivemath.com/ila/index.html"&gt;Immersive Linear Algebra&lt;/a&gt; - J. Str√∂m, K. √Östr√∂m, and T. Akenine-M√∂ller&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://textbooks.math.gatech.edu/ila/"&gt;Interactive Linear Algebra&lt;/a&gt; - Dan Margalit and Joseph Rabinoff&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://people.math.gatech.edu/~herod/Hspace/Hspace.html"&gt;Linear Algebra, Infinite Dimensions, and Maple&lt;/a&gt; - James Herod&lt;/li&gt; 
 &lt;li&gt;üìñ &lt;a href="https://linear.axler.net/"&gt;Linear Algebra Done Right&lt;/a&gt; - Sheldon Axler&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Ring Theory&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.math.uni-duesseldorf.de/~wisbauer/book.pdf"&gt;Foundations of Module and Ring Theory&lt;/a&gt; - Robert Wisbauer (University of D√ºsseldorf)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Galois Theory&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.maths.gla.ac.uk/~ajb/dvi-ps/Galois.pdf"&gt;An Introduction to Galois Theory&lt;/a&gt; - Andrew Baker (University of Glasgow)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.jmilne.org/math/CourseNotes/FT.pdf"&gt;Fields and Galois Theory&lt;/a&gt; - J.S. Milne&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://homepages.warwick.ac.uk/~masda/MA3D5/Galois.pdf"&gt;Galois theory&lt;/a&gt; - Miles Reid&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://eclass.uoa.gr/modules/document/file.php/MATH594/Stewart%20Galois%204th%20edition.pdf"&gt;Galois Theory&lt;/a&gt; - Ian Stewart&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://arxiv.org/pdf/2408.07499"&gt;Galois Theory&lt;/a&gt; ‚Äî Tom Leinster (University of Edinburgh)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Lie Algebras&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.math.harvard.edu/~shlomo/docs/lie_algebras.pdf"&gt;Lie Algebras&lt;/a&gt; - Shlomo Sternberg&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Combinatorics&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìù &lt;a href="http://web.math.utk.edu/~wagner/papers/comb.pdf"&gt;Basic Combinatorics&lt;/a&gt; - Carl G. Wagner (University of Tennessee)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://people.math.gatech.edu/~trotter/book.pdf"&gt;Applied Combinatorics&lt;/a&gt; - Mitchel T. Keller, William T. Trotter&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.maths.qmul.ac.uk/~pjc/notes/comb.pdf"&gt;Notes on Combinatorics&lt;/a&gt; - Peter J. Cameron&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://algo.inria.fr/flajolet/Publications/book.pdf"&gt;Analytic Combinatorics&lt;/a&gt; - Philippe Flajolet, Robert Sedgewick&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.math.upenn.edu/~wilf/DownldGF.html"&gt;generatingfunctionology&lt;/a&gt; - Herbert Wilf&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Graph Theory&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.personal.psu.edu/cxg286/Math485.pdf"&gt;Graph Theory: Lecture Notes&lt;/a&gt; - Christopher Griffin&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.cs.unibo.it/babaoglu/courses/cas00-01/tutorials/GraphTheory.pdf"&gt;Graph Theory&lt;/a&gt; - Reinhard Diestel&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://learngraphtheory.org/"&gt;Graph Theory : Interactive Algorithm Visualizer | Graph Theory Learning Platform&lt;/a&gt; - Hadjoudj Mohammed Islam&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Geometry and Topology&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìù &lt;a href="http://polly.phys.msu.ru/~belyaev/geometry.pdf"&gt;Fundamentals of Geometry&lt;/a&gt; - Oleg A. Belyaev&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://www.math.upenn.edu/~wilf/AeqB.html"&gt;A=B&lt;/a&gt; - M. Petkovsek, H. Wilf, D. Zeilberger&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://aleph0.clarku.edu/~djoyce/java/elements/toc.html"&gt;Elements&lt;/a&gt; - Euclid&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://starrhorse.com/euclid/"&gt;Euclid's Elements Redux&lt;/a&gt; - Daniel Callahan&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.math.ubc.ca/~cass/graphics/manual/"&gt;Mathematical Illustrations&lt;/a&gt; - Bill Casselman&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://www.c82.net/euclid/"&gt;Byrne's Euclid&lt;/a&gt; - Oliver Byrne&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://djm.cc/library/Plane_Geometry_Wentworth_Smith_edited.pdf"&gt;Plane Geometry&lt;/a&gt; - George Wentworth and David Eugene Smith (1913)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://djm.cc/library/Plane_Spherical_Trigonometry_Wentworth_Smith_edited_2.pdf"&gt;Planes and Spherical Trigonometry&lt;/a&gt; - George Wentworth and David Eugene Smith (1915)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://djm.cc/library/Coordinate_Geometry_Fine_Thompson_edited03.pdf"&gt;Coordinate Geometry&lt;/a&gt; - Henry Buchard Fine and Henry Dallas Thompson (1911)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://djm.cc/library/Analytic_Geometry_Siceloff_Wentworth_Smith_edited.pdf"&gt;Analytic Geometry&lt;/a&gt; - Lewis Parker Siceloff, George Wentworth, David Eugene Smith (1922)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Differential Geometry&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìù &lt;a href="https://people.math.ethz.ch/~salamon/PREPRINTS/diffgeo.pdf"&gt;Introduction to Differential Geometry&lt;/a&gt; - Joel W. Robbin, Dietmar A. Salamon&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://www.cis.upenn.edu/~jean/gbooks/manif.html"&gt;Notes on Differential Geometry and Lie Groups&lt;/a&gt; - Jean Gallier (University of Pennsylvania)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.mat.univie.ac.at/~michor/dgbook.pdf"&gt;Topics in Differential Geometry&lt;/a&gt; - Peter W. Michor&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://mysite.science.uottawa.ca/rossmann/Differential%20Geometry%20book_files/Diffgeo.pdf"&gt;Lectures on Differential Geometry&lt;/a&gt; - Wulf Rossmann&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.matematik.lu.se/matematiklu/personal/sigma/Riemann.pdf"&gt;An Introduction to Riemannian Geometry&lt;/a&gt; - Sigmundur Gudmundsson (Lund University)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://msri.org/publications/books/gt3m/"&gt;The Geometry and Topology of Three-Manifolds&lt;/a&gt; - W. Thurston&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.math.harvard.edu/~shlomo/docs/semi_riemannian_geometry.pdf"&gt;Semi-Riemann Geometry and General Relativity&lt;/a&gt; - Shlomo Sternberg&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.cs.cmu.edu/~kmcrane/Projects/DDG/paper.pdf"&gt;Discrete Differential Geometry&lt;/a&gt; - Keenan Crane&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Algebraic Geometry&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìù &lt;a href="https://ksda.ccny.cuny.edu/PostedPapers/rickksda1107.pdf"&gt;A Brief Introduction to Algebraic Geometry&lt;/a&gt; - R.C. Churchill&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.math.lsa.umich.edu/~idolga/631.pdf"&gt;Introduction to Algebraic Geometry&lt;/a&gt; - Igor V. Dolgachev&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://math.stanford.edu/~vakil/216blog/FOAGjun1113public.pdf"&gt;Foundations of Algebraic Geometry&lt;/a&gt; - Ravi Vakil&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.cis.upenn.edu/~jean/algeoms.pdf"&gt;Algebraic Geometry&lt;/a&gt; - Jean Gallier, Stephen S. Shatz (University of Pennsylvania)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.jmilne.org/math/CourseNotes/AG.pdf"&gt;Algebraic Geometry&lt;/a&gt; - J.S. Milne&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.mathematik.uni-kl.de/~gathmann/class/alggeom-2002/main.pdf"&gt;Algebraic Geometry&lt;/a&gt; - Andreas Gathmann (University of Kaiserslautern)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://stacks.math.columbia.edu/"&gt;The Stacks Project&lt;/a&gt; - Maintained by Aise Johan de Jong (Columbia)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Algebraic Statistics&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìù &lt;a href="https://math.berkeley.edu/~bernd/owl.pdf"&gt;Lectures on Algebraic Statistics&lt;/a&gt; - Mathias Drton, Bernd Sturmfels, Seth Sullivant&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://www3.diism.unisi.it/~chiantini/did/00Book.pdf"&gt;An Introduction to Algebraic Statistics&lt;/a&gt; - Cristiano Bocci, Luca Chiantini and Anthony V. Geramita&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://tore.tuhh.de/dspace-cris-server/api/core/bitstreams/a0c378d5-ce8e-442a-8891-9e7f763b4279/content"&gt;Algebraic Statistics&lt;/a&gt; - Karl-Heinz Zimmermann&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://yaroslavvb.com/papers/pachter-algebraic.pdf"&gt;Algebraic Statistics for Computational Biology&lt;/a&gt; - Pachter, and Sturmfels.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Topology&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìù &lt;a href="https://www.math.upenn.edu/~ghrist/notes.html"&gt;Elementary Applied Topology&lt;/a&gt; - Robert Ghrist (UPenn)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.math.colostate.edu/~renzo/teaching/Topology10/Notes.pdf"&gt;Introduction to Topology&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.math.bme.hu/~kalex/Teaching/Spring10/Topology/TopNotes_Spring10.pdf"&gt;Introduction to Topology&lt;/a&gt; - Alex K√ºronya&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.math.clemson.edu/~jimlb/Teaching/2009-10/Math986/Topology.pdf"&gt;Introductory Topology&lt;/a&gt; - Jim L. Brown&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://webusers.imj-prg.fr/~pierre.schapira/lectnotes/Topo.pdf"&gt;General Topology&lt;/a&gt; - Pierre Schapira (Paris VI University)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.pdmi.ras.ru/~olegviro/topoman/eng-book-nopfs.pdf"&gt;Elementary Topology Problem Textbook&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.math.ku.dk/~moller/e03/3gt/notes/gtnotes.pdf"&gt;General Topology&lt;/a&gt; - Jesper M. M√∏ller&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://mathonline.wikidot.com/topology"&gt;Topology Topics&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Algebraic Topology&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.math.cornell.edu/~hatcher/AT/AT.pdf"&gt;Algebraic Topology&lt;/a&gt; - Allen Hatcher&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.math.uchicago.edu/~may/CONCISE/ConciseRevised.pdf"&gt;A Concise Course in Algebraic Topology&lt;/a&gt; - J. P. May&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.math.muni.cz/~cadek/at/at.pdf"&gt;Introduction to Algebraic Topology&lt;/a&gt; - Martin Cadek&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://webusers.imj-prg.fr/~pierre.schapira/lectnotes/AlTo.pdf"&gt;Algebra and Topology&lt;/a&gt; - Pierre Schapira (Paris VI University)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.indiana.edu/~jfdavis/teaching/m623/book.pdf"&gt;Lecture Notes in Algebraic Topology&lt;/a&gt; - James F. Davis, Paul Kirk (Indiana University)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://www.ma.utexas.edu/ibl1/courses/resources/12_15_07_grad_alg_top_mooremethod.pdf"&gt;Algebraic Topology&lt;/a&gt; - Michael Starbird&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.math.nus.edu.sg/~matwujie/ma5209.pdf"&gt;Lecture Notes on Algebraic Topology&lt;/a&gt; - Jie Wu&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Analysis&lt;/h2&gt; 
&lt;h3&gt;Real Analysis&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìù &lt;a href="https://ocw.mit.edu/resources/res-18-001-calculus-online-textbook-spring-2005/textbook/"&gt;MIT OpenCourseWare Lectures on Calculus&lt;/a&gt; - G. Strang&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.math.wisc.edu/~keisler/calc.html"&gt;Elementary Calculus: An Approach Using Infinitesimals&lt;/a&gt; - Professor H. Jerome Keisler&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://www.math.ucdavis.edu/~hunter/intro_analysis_pdf/intro_analysis.pdf"&gt;An Introduction to Real Analysis&lt;/a&gt; - John K. Hunter (University of California at Davis)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://ramanujan.math.trinity.edu/wtrench/texts/TRENCH_REAL_ANALYSIS.PDF"&gt;Introduction to Real Analysis&lt;/a&gt; - William F. Trench (Trinity University, Texas)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.jirka.org/ra/realanal.pdf"&gt;Basic Analysis: Introduction to Real Analysis&lt;/a&gt; - Ji≈ô√≠ Lebl&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://prac.im.pwr.wroc.pl/~kwasnicki/pl/stuff/tbb-hyper.pdf"&gt;Elementary Real Analysis&lt;/a&gt; - Thomson, Bruckner&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://ms.mcmaster.ca/~sawyer/Publications/Real_Analysis.pdf"&gt;Lecture Notes in Real Analysis&lt;/a&gt; - Eric T. Sawyer (McMaster University)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://math.harvard.edu/~ctm/papers/home/text/class/harvard/212a/course/course.pdf"&gt;Real Analysis&lt;/a&gt; - C. McMullen&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://bass.math.uconn.edu/3rd.pdf"&gt;Real Analysis for Graduate Students&lt;/a&gt; - Richard F. Bass&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.math.purdue.edu/~torres/pubs/Modern-real-analysis.pdf"&gt;Modern Real Analysis&lt;/a&gt; - William P. Ziemer (Indiana University)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.trillia.com/zakon-analysisI.html"&gt;Mathematical Analysis Vol I&lt;/a&gt; - Elias Zakon&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.trillia.com/zakon-analysisII.html"&gt;Mathematical Analysis Vol II&lt;/a&gt; - Elias Zakon&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.math.harvard.edu/~shlomo/docs/Advanced_Calculus.pdf"&gt;Advanced Calculus&lt;/a&gt; - Lynn Loomis, Schlomo Sternberg&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://spot.colorado.edu/~baggett/analysis.html"&gt; Analysis of Functions of a Single Variable&lt;/a&gt; - Lawerence Baggett&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.synechism.org/wp/the-calculus-of-functions-of-several-variables/"&gt;The Calculus of Functions of Several Variables&lt;/a&gt; - Dan Sloughter&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://web.pdx.edu/~erdman/PTAC/problemtext_pdf.pdf"&gt;A ProblemText in Advanced Calculus&lt;/a&gt; - John M. Erdman&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://hdl.handle.net/2027/spo.5597602.0001.001"&gt;Calculus and Linear Algebra. Vol. 1&lt;/a&gt; - Wilfred Kaplan, Donald J. Lewis&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://quod.lib.umich.edu/s/spobooks/5597602.0002.001"&gt;Calculus and Linear Algebra. Vol. 2&lt;/a&gt; - Wilfred Kaplan, Donald J. Lewis&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.math.odu.edu/~jhh/counter10.html"&gt;Introduction to Calculus I and II&lt;/a&gt; - J.H. Heinbockel&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://faculty.gvsu.edu/boelkinm/Home/Active_Calculus.html"&gt;Active Calculus&lt;/a&gt; - Matt Boelkins&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://math.berkeley.edu/~gbergman/ug.hndts/#Rudin"&gt;Supplements to the Exercises in Chapters 1-7 of Walter Rudin's "Principles of Mathematical Analysis"&lt;/a&gt; - George M. Bergman&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://calculusmadeeasy.org/"&gt;Calculus Made Easy&lt;/a&gt; - Silvanus P. Thompson (1910)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://djm.cc/library/Elements_Differential_Integral_Calculus_Granville_edited_2.pdf"&gt;Elements of Differential and Integral Calculus&lt;/a&gt; - William Anthony Granville (1911)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://stitz-zeager.com/szprecalculus07042013.pdf"&gt;Precalculus&lt;/a&gt; - Carl Stitz, Jeff Zeager&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Harmonic Analysis&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.math.uiuc.edu/~laugesen/545/545Lectures.pdf"&gt;Harmonic Analysis Lecture Notes&lt;/a&gt; - Richard S. Laugesen (University of Illinois at Urbana‚ÄìChampaign)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.math.uchicago.edu/~schlag/harmonicnotes.pdf"&gt;Harmonic Analysis&lt;/a&gt; - W. Schlag&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://see.stanford.edu/materials/lsoftaee261/book-fall-07.pdf"&gt;Lecture Notes: Fourier Transform and its Applications&lt;/a&gt; - Brad Osgood&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.reed.edu/physics/courses/Physics331.f08/pdf/Fourier.pdf"&gt;Fourier Analysis&lt;/a&gt; - Lucas Illing&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://ccrma.stanford.edu/~jos/mdft"&gt;Mathematics of the Discrete Fourier Transform (DFT) with Audio Applications&lt;/a&gt; - Julius O. Smith III (Stanford University)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Complex Analysis&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìù &lt;a href="https://mtaylor.web.unc.edu/wp-content/uploads/sites/16915/2018/04/complex.pdf"&gt;Introduction to Complex Analysis&lt;/a&gt; - Michael Taylor&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.math.uiuc.edu/~jpda/jpd-complex-geometry-book-5-refs-bip.pdf"&gt;An Introduction to Complex Analysis and Geometry&lt;/a&gt; - John P. D'Angelo (University of Illinois)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://math.sfsu.edu/beck/papers/complex.pdf"&gt;A First Course in Complex Analysis&lt;/a&gt; - Matthias Beck, Gerald Marchesi, Dennis Pixton, Lucas Sabalka&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.math.wustl.edu/~sk/books/guide.pdf"&gt;A Guide to Complex Variables&lt;/a&gt; - Steven G. Krantz&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.maths.manchester.ac.uk/~cwalkden/complex-analysis/complex_analysis.pdf"&gt;Complex Analysis&lt;/a&gt; - Charles Walkden&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.math.ku.dk/noter/filer/koman-12.pdf"&gt;Complex Analysis&lt;/a&gt; - Christian Berg&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://people.math.sc.edu/girardi/m7034/book/AshComplexVariablesWithHyperlinks.pdf"&gt;Complex Variables&lt;/a&gt; - R. B. Ash, W.P. Novinger&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.maths.lth.se/matematiklu/personal/olofsson/CompHT06.pdf"&gt;Complex Analysis&lt;/a&gt; - Christer Bennewitz&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://web.archive.org/web/20150620124453/https://www.math.washington.edu/~marshall/math_536/Notes.pdf"&gt;Complex Analysis&lt;/a&gt; - Donald E. Marshall&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://gauss.math.yale.edu/~ws442/complex.pdf"&gt;A Concise Course in Complex Analysis and Riemann Surfaces&lt;/a&gt; - Wilhelm Schlag&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://people.math.gatech.edu/%7Ecain/winter99/complex.html"&gt;Complex Analysis&lt;/a&gt; - G. Cain (Georgia Tech)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://complex-analysis.com/"&gt;Complex Analysis&lt;/a&gt; - Juan Carlos Ponce Campuzano&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Functional Analysis&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìù &lt;a href="https://www.math.uwaterloo.ca/~lwmarcou/notes/pmath453.pdf"&gt;An Introduction to Functional Analysis&lt;/a&gt; - Laurent W. Marcoux (University of Waterloo)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://users.math.msu.edu/users/jeffrey/920/920notes.pdf"&gt;Functional Analysis: Lecture Notes&lt;/a&gt; - Jeff Schenker (Michigan State University)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://archive.org/details/TB_Ward___Functional_analysis_lecture_notes"&gt;Functional Analysis Lecture Notes&lt;/a&gt; - T.B. Ward (University of East Anglia)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.maths.lancs.ac.uk/~belton/www/notes/fa_notes.pdf"&gt;Functional Analysis&lt;/a&gt; - Alexander C. R. Belton&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://www.mat.univie.ac.at/~gerald/ftp/book-fa/fa.pdf"&gt;Topics in Real and Functional Analysis&lt;/a&gt; - Gerald Teschl&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www2.math.ou.edu/~cremling/teaching/lecturenotes/fa-new/LN-I.pdf"&gt;Functional Analysis&lt;/a&gt; - Christian Remling&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.math.harvard.edu/~shlomo/docs/Real_Variables.pdf"&gt;Theory of Functions of a Real Variable&lt;/a&gt; - Shlomo Sternberg&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://spot.colorado.edu/~baggett/functional.html"&gt;Functional Analysis&lt;/a&gt; - Lawerence Baggett&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Measure Theory&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìù &lt;a href="https://terrytao.files.wordpress.com/2012/12/gsm-126-tao5-measure-book.pdf"&gt;An Introduction to Measure Theory&lt;/a&gt; - Terence Tao (UCLA)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.mat.uniroma2.it/~cannarsa/cam_0607.pdf"&gt;Lecture Notes on Measure Theory and Functional Analysis&lt;/a&gt; - P. Cannarsa, T. D‚ÄôAprile&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.math.chalmers.se/~borell/MeasureTheory.pdf"&gt;Lecture Notes in Measure Theory&lt;/a&gt; - Christer Borell&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.gold-saucer.org/math/lebesgue/lebesgue.pdf"&gt;A Crash Course on the Lebesgue Integral and Measure Theory&lt;/a&gt; - Steve Cheng&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://www.math.ucdavis.edu/~hunter/measure_theory/measure_notes.pdf"&gt;Measure Theory&lt;/a&gt; - John K. Hunter (University of California at Davis)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://people.math.ethz.ch/~salamon/PREPRINTS/measure.pdf"&gt;Measure and Integration&lt;/a&gt; - Dietmar A. Salamon (ETH Z√ºrich)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.math.ucsd.edu/~bdriver/240-00-01/Lecture_Notes/measurep.pdf"&gt;Lecture notes: Measure Theory&lt;/a&gt; - Bruce K. Driver&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Ordinary Differential Equations&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.synechism.org/wp/difference-equations-to-differential-equations/"&gt;Difference Equations To Differential Equations&lt;/a&gt; - Dan Sloughter&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://www.math.uni-bielefeld.de/~grigor/odelec2008.pdf"&gt;Ordinary Differential Equation&lt;/a&gt; - Alexander Grigorian (University of Bielefeld)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.cs.bgu.ac.il/~leonid/ode_bio_files/Ionascu_LectNotes.pdf"&gt;Ordinary Differential Equations: Lecture Notes&lt;/a&gt; - Eugen J. Ionascu&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.math.lmu.de/~philip/publications/lectureNotes/ODE.pdf"&gt;Ordinary Differential Equations&lt;/a&gt; - Peter Philip&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://users.math.msu.edu/users/gnagy/teaching/ode.pdf"&gt;Ordinary Differential Equations&lt;/a&gt; - Gabriel Nagy&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.mat.univie.ac.at/~gerald/ftp/book-ode/ode.pdf"&gt;Ordinary Differential Equations and Dynamical Systems&lt;/a&gt; - Gerald Teschl&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://leipper.org/manuals/zip-fill/dn-difeq-notes.pdf"&gt;Notes on Differential Equations&lt;/a&gt; - Bob Terrell&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://digitalcommons.trinity.edu/mono/8/"&gt;Elementary Differential Equations&lt;/a&gt; - William F. Trench&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://digitalcommons.trinity.edu/mono/9/"&gt;Elementary Differential Equations With Boundary Value Problems&lt;/a&gt; - William F. Trench&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.jirka.org/diffyqs/"&gt;Notes on Diffy Qs: Differential Equations for Engineers&lt;/a&gt; - Ji≈ô√≠ Lebl&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://djm.cc/library/Differential_Equations_Phillips_edited.pdf"&gt;Differential Equations&lt;/a&gt; - H. B. Phillips (1922)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Partial Differential Equations&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìù &lt;a href="https://www.math.ucdavis.edu/~hunter/pdes/pde_notes.pdf"&gt;Notes on Partial Differential Equations&lt;/a&gt; - John K. Hunter (University of California at Davis)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.math.uni-leipzig.de/~miersemann/pdebook.pdf"&gt;Partial Differential Equations: Lecture Notes&lt;/a&gt; - Erich Miersemann (Leipzig University)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.mathphysics.com/pde/"&gt;Linear Methods of Applied Mathematics&lt;/a&gt; - E. Harrell, J. Herod (Georgia Tech)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Chaos Theory&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìù &lt;a href="https://archive.org/details/chaosmakingnewsc0000unse"&gt;Chaos: Making a New Science&lt;/a&gt; - James Gleick&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://archive.org/details/complexityguided0000mitc?utm_source=chatgpt.com"&gt;Complexity: A Guided Tour&lt;/a&gt; - Melanie Mitchell (Oxford University)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Probability and Statistics&lt;/h2&gt; 
&lt;h3&gt;Probability Theory&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìù &lt;a href="https://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/amsbook.mac.pdf"&gt;Introduction to Probability&lt;/a&gt; - Charles M. Grinstead, J. Laurie Snell&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://vfu.bg/en/e-Learning/Math--Bertsekas_Tsitsiklis_Introduction_to_probability.pdf"&gt;Introduction to Probability&lt;/a&gt; - Dimitri P. Bertsekas, John N. Tsitsiklis (MIT)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.maths.uq.edu.au/~kroese/asitp.pdf"&gt;A Short Introduction to Probability&lt;/a&gt; - Dirk P. Kroese (University of Queensland)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://www.math.duke.edu/~rtd/PTE/PTE4_1.pdf"&gt;Probability: Theory and Examples&lt;/a&gt; - Rick Durrett&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://github.com/mavam/stat-cookbook/releases/download/0.2.3/stat-cookbook.pdf"&gt;Probability and Statistics Cookbook&lt;/a&gt; - Matthias Vallentin (UC Berkeley)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.wzchen.com/probability-cheatsheet/"&gt;The Only Probability Cheatsheet You'll Ever Need&lt;/a&gt; - William Chen&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.ellerman.org/Davids-Stuff/Maths/Rota-Baclawski-Prob-Theory-79.pdf"&gt;An Introduction to Probability and Random Processes&lt;/a&gt; - Gian-Carlo Rota, Kenneth Baclawski&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://arxiv.org/pdf/1906.01803.pdf"&gt;Foundations of Constructive Probability Theory&lt;/a&gt; - Yuen-Kwok Chan&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Statistics&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìù &lt;a href="http://homepages.math.uic.edu/~rgmartin/Teaching/Stat411/Notes/411notes.pdf"&gt;Lecture Notes on Statistical Theory&lt;/a&gt; - Ryan Martin (University of Illinois)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www-library.desy.de/preparch/books/vstatmp_engl.pdf"&gt;Introduction to Statistics and Data Analysis for Physicists&lt;/a&gt; - Gerhard Bohm, G√ºnter Zech&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.iiserpune.ac.in/~ayan/MTH201/Sahoo_textbook.pdf"&gt;Probability and Mathematical Statistics&lt;/a&gt; - Prasanna Sahoo (University of Louisville)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://math.arizona.edu/~faris/stat.pdf"&gt;Lectures on Statistics&lt;/a&gt; - William G. Faris&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://pages.pomona.edu/~ajr04747/Fall2009/Math152/Notes/Math152NotesFall09.pdf"&gt;Statistical Theory&lt;/a&gt; - Adolfo J. Rumbos&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://mason.gmu.edu/~jgentle/books/MathStat.pdf"&gt;Theory of Statistics&lt;/a&gt; - James E. Gentle (George Mason University)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://math.arizona.edu/~jwatkins/notests.pdf"&gt;Theory of Statistics&lt;/a&gt; - Joseph C. Watkins (University of Arizona)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://web.archive.org/web/20130523134625/http://www.aiaccess.net/e_gm.htm"&gt;Glossary of Data Modeling&lt;/a&gt; - AI Access&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.ats.ucla.edu/stat/papers/"&gt;Statistics Papers&lt;/a&gt; - List of statistics papers curated by the Institute for Digital Research and Education (IDRE) at UCLA on methods such as bootstrap and factor invariance.&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://itl.nist.gov/div898/handbook/index.htm"&gt;NIST Handbook of Statistical Methods&lt;/a&gt; - Resource on practical statistics directed towards scientists and engineers.&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://vassarstats.net/textbook/"&gt;Concepts and Applications of Inferential Statistics&lt;/a&gt; - Richard Lowry&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.cosc.brocku.ca/~duentsch/papers/methprimer2.html"&gt;Rough set data analysis: A road to non-invasive knowledge discovery&lt;/a&gt; - Ivo D√ºntsch, G√ºnther Gediga&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://statsthinking21.org/"&gt;Statistical Thinking for the 21st Century&lt;/a&gt; - Russell A. Poldrack&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://jonathanweisberg.org/vip/"&gt;Odds and Ends: Introducing Probability &amp;amp; Decision with a Visual Emphasis&lt;/a&gt; - Jonathan Weisberg&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://seeing-theory.brown.edu/"&gt;Seeing Theory&lt;/a&gt; - Daniel Kunin, Jingru Guo, Tyler Dae Devlin, and Daniel Xiang&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://www.statisticsdonewrong.com/"&gt;Statistics Done Wrong&lt;/a&gt; - Alex Reinhart&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://link.springer.com/book/10.1007/978-0-387-21736-9"&gt;All of Statistics: A Concise Course in Statistical Inference&lt;/a&gt; - Larry Wasserman&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Statistical Learning&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìù &lt;a href="http://www-bcf.usc.edu/~gareth/ISL/ISLR%20First%20Printing.pdf"&gt;An Introduction to Statistical Learning with Applications in R&lt;/a&gt; - Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://web.stanford.edu/~hastie/Papers/ESLII.pdf"&gt;The Elements of Statistical Learning&lt;/a&gt; - Trevor Hastie, Robert Tibshirani, Jerome Friedman&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://web.stanford.edu/class/cs229t/notes.pdf"&gt;Statistical Learning Theory&lt;/a&gt; - Percy Liang&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://webdocs.cs.ualberta.ca/~sutton/book/bookdraft2016sep.pdf"&gt;Reinforcement Learning: An Introduction&lt;/a&gt; - Richard S. Sutton, Andrew G. Barto&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Stochastic processes&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.math.tifr.res.in/~publ/ln/tifr24.pdf"&gt;Lectures on Stochastic Processes&lt;/a&gt; - K. Ito (Tata Institute of Fundamental Research, Bombay)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.math.harvard.edu/~knill/teaching/math144_1994/probability.pdf"&gt;Probability and Stochastic Processes with Applications&lt;/a&gt; - Oliver Knill (Harvard University)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://statweb.stanford.edu/~adembo/math-136/nnotes.pdf"&gt;Stochastic Processes&lt;/a&gt; - Amir Dembo (Stanford University)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.mi.fu-berlin.de/wiki/pub/CompMolBio/MarkovKetten15/stochastic_processes_2011.pdf"&gt;Lecture Notes on Stochastic Processes&lt;/a&gt; - Frank No√©, Bettina Keller and Jan-Hendrik Prinz (Freie Universit√§t Berlin)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://www.ma.utexas.edu/users/gordanz/notes/introduction_to_stochastic_processes.pdf"&gt;Introduction to Stochastic Processes - Lecture Notes&lt;/a&gt; - Gordan ≈Ωitkoviƒá (University of Texas)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://www.math.uwaterloo.ca/~mscott/Little_Notes.pdf"&gt;Applied Stochastic Processes in science and engineering&lt;/a&gt; - Matt Scott (University of Waterloo)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.math.leidenuniv.nl/~spieksma/colleges/sp-master/sp-hvz1.pdf"&gt;An Introduction to Stochastic Processes in Continuous Time&lt;/a&gt; - Flora Spieksma (Leiden University)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://pages.uoregon.edu/dlevin/MARKOV/markovmixing.pdf"&gt;Markov Chains and Mixing Times&lt;/a&gt; - David A. Levin, Yuval Peres, Elizabeth L. Wilmer&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.stat.yale.edu/~pollard/Books/1984book/pollard1984.pdf"&gt;Convergence of Stochastic Processes&lt;/a&gt; - David Pollard&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Numerical Analysis&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.math.umd.edu/~dlevy/resources/notes.pdf"&gt;Introduction to Numerical Analysis&lt;/a&gt; - Doron Levy (University of Maryland)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.ima.umn.edu/~arnold/597.00-01/nabook.pdf"&gt;A Concise Introduction to Numerical Analysis&lt;/a&gt; - Douglas N. Arnold (University of Minnesota)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://people.cs.uchicago.edu/~ridg/newna/nalrs.pdf"&gt;Numerical Analysis&lt;/a&gt; - L. Ridgway Scott&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://uknowledge.uky.edu/cgi/viewcontent.cgi?article=1000&amp;amp;context=math_textbooks"&gt;Lectures In Basic Computational Numerical Analysis&lt;/a&gt; - J. M. McDonough (University of Kentucky)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://user.math.uni-bremen.de/schmi/SS04/YSU_Notes.pdf"&gt;Advanced Numerical Methods and Their Applications to Industrial Problems: Adaptive Finite Element Methods&lt;/a&gt; - Alfred Schmidt, Arsen Narimanyan&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://ece.uwaterloo.ca/~dwharder/nm/"&gt;Numerical Analysis for Engineers&lt;/a&gt; - Douglas Wilhelm Harder&lt;/li&gt; 
 &lt;li&gt;üìùüé• &lt;a href="https://www.cs.utexas.edu/users/flame/laff/alaff/frontmatter.html"&gt;Advanced Linear Algebra: Foundations to Frontiers&lt;/a&gt; - Robert van de Geijn, Margaret Myers (University of Texas at Austin)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Signal processing&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.ece.rutgers.edu/~orfanidi/intro2sp/orfanidis-i2sp.pdf"&gt;Introduction to Signal Processing&lt;/a&gt; - Sophocles J. Orfanidis (Rutgers University)&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.fourierandwavelets.org/FSP_v1.1_2014.pdf"&gt;Foundations of Signal Processing&lt;/a&gt; - Martin Vetterli, Jelena Kovacevic, Vivek K Goyal&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://ee.stanford.edu/~gray/sp.pdf"&gt;An Introduction to Statistical Signal Processing&lt;/a&gt; - Robert M. Gray, Lee D. Davisson&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://greenteapress.com/wp/think-dsp/"&gt;Think DSP&lt;/a&gt; - Allen B. Downey&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://www.uio.no/studier/emner/matnat/math/MAT-INF2360/v15/kompendium/applinalgpython.pdf"&gt;Linear algebra, signal processing, and wavelets. A unified approach.&lt;/a&gt; - √òyvind Ryan (University of Oslo)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Mathematics for Computer Science&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìù &lt;a href="https://people.csail.mit.edu/meyer/mcs.pdf"&gt;Mathematics for Computer Science&lt;/a&gt; - Eric Lehman, F. Thomson Leighton, Albert R. Meyer&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.math.upenn.edu/%7Ewilf/AlgComp3.html"&gt;Algorithms and Complexity&lt;/a&gt; - H. Wilf&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://people.eecs.berkeley.edu/~varaiya/papers_ps.dir/NOO.pdf"&gt;Lecture Notes on Optimization&lt;/a&gt; - Pravin Varaiya&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.inference.org.uk/mackay/itila/book.html"&gt;Information Theory, Inference, and Learning Algorithms&lt;/a&gt; - David J. C. MacKay&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://hypertextbook.com/chaos/"&gt;The Chaos Textbook: Mathematics in the age of the computer&lt;/a&gt; - Glenn Elert&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Mathematical Biology&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.math.ust.hk/~machas/mathematical-biology.pdf"&gt;Mathematical Biology&lt;/a&gt; - Jeffrey Chasnov&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Mathematical Physics&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìù &lt;a href="http://oaktrust.library.tamu.edu/handle/1969.1/2501"&gt;Introduction to Continuum Mechanics&lt;/a&gt; - Ray. M. Bowen&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.physics.miami.edu/nearing/mathmethods/"&gt;Mathematical Tools for Physics&lt;/a&gt; - James Nearing&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="http://www.malaspina.com/etext/heavens.htm"&gt;Mechanism of the Heavens (1831)&lt;/a&gt; - Mary Somerville&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Students Lecture Notes&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://web.evanchen.cc/coursework.html"&gt;Evan Chen&lt;/a&gt; - MIT. 2012 ~ 2018. Covers Combinatorics, Number Theory, Honors Algebra, Set Theory, Real Analysis, Graph Theory, and more.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://dec41.user.srcf.net/notes/"&gt;Dexter Chua&lt;/a&gt; - Harvard. 2013 ~ 2018. Covers Analysis, Probability, Linear Algebra, Complex Analysis, Numerical Analysis, Statistics, Optimization, Algebraic Topology, Quantum Field Theory, and more.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Related Awesome Lists&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/mostafatouny/awesome-theoretical-computer-science"&gt;Theoretical Computer Science&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;License&lt;/h1&gt; 
&lt;p&gt;&lt;a href="http://creativecommons.org/publicdomain/zero/1.0/"&gt;&lt;img src="http://i.creativecommons.org/p/zero/1.0/88x31.png" alt="CC0" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;To the extent possible under law, &lt;a href="http://cyrille.rossant.net"&gt;Cyrille Rossant&lt;/a&gt; has waived all copyright and related or neighboring rights to this work.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>subframe7536/maple-font</title>
      <link>https://github.com/subframe7536/maple-font</link>
      <description>&lt;p&gt;Maple Mono: Open source monospace font with round corner, ligatures and Nerd-Font icons for IDE and terminal, fine-grained customization options. Â∏¶ËøûÂ≠óÂíåÊéßÂà∂Âè∞ÂõæÊ†áÁöÑÂúÜËßíÁ≠âÂÆΩÂ≠ó‰ΩìÔºå‰∏≠Ëã±ÊñáÂÆΩÂ∫¶ÂÆåÁæé2:1ÔºåÁªÜÁ≤íÂ∫¶ÁöÑËá™ÂÆö‰πâÈÄâÈ°π&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/subframe7536/maple-font/variable/resources/header.png" alt="Cover" /&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/13165" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/13165" alt="subframe7536%2Fmaple-font | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; &lt;a href="https://hellogithub.com/repository/0601f355bd824d88b58f1af3066c486a" target="_blank"&gt;&lt;img src="https://api.hellogithub.com/v1/widgets/recommend.svg?rid=0601f355bd824d88b58f1af3066c486a&amp;amp;claim_uid=AO0yWRQ48ITGNqK" alt="FeaturedÔΩúHelloGitHub" style="width: 250px; height: 54px;" width="250" height="54" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;img alt="GitHub Repo Stars" src="https://img.shields.io/github/stars/subframe7536/maple-font" /&gt; &lt;img alt="GitHub Repo Forks" src="https://img.shields.io/github/forks/subframe7536/maple-font" /&gt; &lt;img alt="X (formerly Twitter) Follow" src="https://img.shields.io/twitter/follow/subframe7536" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;img alt="GitHub Release" src="https://img.shields.io/github/v/release/subframe7536/maple-font" /&gt; &lt;img alt="GitHub Downloads (all assets, all releases)" src="https://img.shields.io/github/downloads/subframe7536/maple-font/total" /&gt; &lt;img alt="GitHub Repo License" src="https://img.shields.io/github/license/subframe7536/maple-font" /&gt; &lt;img alt="GitHub Repo Issues" src="https://img.shields.io/github/issues/subframe7536/maple-font" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://raw.githubusercontent.com/subframe7536/maple-font/variable/#download"&gt;Download&lt;/a&gt; | &lt;a href="https://font.subf.dev"&gt;Website&lt;/a&gt; | English | &lt;a href="https://raw.githubusercontent.com/subframe7536/maple-font/variable/README_CN.md"&gt;‰∏≠Êñá&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/subframe7536/maple-font/variable/README_JA.md"&gt;Êó•Êú¨Ë™û&lt;/a&gt; &lt;/p&gt; 
&lt;h1&gt;Maple Mono&lt;/h1&gt; 
&lt;p&gt;Maple Mono is an open source monospace font focused on smoothing your coding flow.&lt;/p&gt; 
&lt;p&gt;I create it to enhance my working experience, and hope that it can be useful to others.&lt;/p&gt; 
&lt;p&gt;V7 is a completely remade version, providing variable font format and source files of font project, redesigning more than half of the glyphs and offering smarter ligatures. You can checkout V6 &lt;a href="https://github.com/subframe7536/maple-font/tree/main"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;‚ú® Variable - Infinity font weights with fine-grained italic glyphs.&lt;/li&gt; 
 &lt;li&gt;‚òÅÔ∏è Smooth - Round corner, brand-new glyph of &lt;code&gt;@ $ % &amp;amp; Q -&amp;gt;&lt;/code&gt; and cursive &lt;code&gt;f i j k l x y&lt;/code&gt; in italic style.&lt;/li&gt; 
 &lt;li&gt;üí™ Useful - Large amount of smart ligatures, see in &lt;a href="https://raw.githubusercontent.com/subframe7536/maple-font/variable/source/features/README.md"&gt;&lt;code&gt;features/&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üé® Icon - First-Class &lt;a href="https://github.com/ryanoasis/nerd-fonts"&gt;Nerd-Font&lt;/a&gt; support, make your terminal more vivid.&lt;/li&gt; 
 &lt;li&gt;üî® Customize - Enable or disable font features as you want, just make your own font.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Simpified Chinese, Traditional Chinese and Japanese&lt;/h3&gt; 
&lt;p&gt;CN version based on &lt;a href="https://github.com/CyanoHao/Resource-Han-Rounded"&gt;Resource Han Rounded&lt;/a&gt; provides complete character set support for Chinese development environments, including Simplified Chinese, Traditional Chinese, and Japanese. Meanwhile, the characteristic of perfect 2:1 alignment between Chinese and English allows this font to achieve a neat, uniform, beautiful, and comfortable appearance in scenarios such as multilingual display and Markdown tables. However, the spacing of Chinese characters is larger compared to other popular Chinese fonts. See details in &lt;a href="https://github.com/subframe7536/maple-font/releases/tag/cn-base"&gt;release notes&lt;/a&gt; and &lt;a href="https://github.com/subframe7536/maple-font/issues/211"&gt;this issue&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/subframe7536/maple-font/variable/resources/2-1.png" alt="2-1.png" /&gt;&lt;/p&gt; 
&lt;h2&gt;ScreenShots&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/subframe7536/maple-font/variable/resources/showcase.png" alt="showcase.png" /&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Pictured by &lt;a href="https://github.com/subframe7536/vscode-codeimg"&gt;CodeImg&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Theme: &lt;a href="https://github.com/subframe7536/vscode-theme-maple"&gt;Maple&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Config: font size 16px, line height 1.8, default letter spacing&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Download&lt;/h2&gt; 
&lt;p&gt;You can download all the font archives from &lt;a href="https://github.com/subframe7536/maple-font/releases"&gt;Releases&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Scoop (Windows)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Add bucket
scoop bucket add nerd-fonts
# Maple Mono (ttf format)
scoop install Maple-Mono
# Maple Mono NF
scoop install Maple-Mono-NF
# Maple Mono NF CN
scoop install Maple-Mono-NF-CN
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;All packages (Click to expand)&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-sh"&gt;# Add bucket
scoop bucket add nerd-fonts
# Maple Mono (ttf format)
scoop install Maple-Mono
# Maple Mono (hinted ttf format)
scoop install Maple-Mono-autohint
# Maple Mono (otf format)
scoop install Maple-Mono-otf
# Maple Mono NF
scoop install Maple-Mono-NF
# Maple Mono NF CN
scoop install Maple-Mono-NF-CN
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;Homebrew (MacOS, Linux)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Maple Mono
brew install --cask font-maple-mono
# Maple Mono NF
brew install --cask font-maple-mono-nf
# Maple Mono NF CN
brew install --cask font-maple-mono-nf-cn
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;All packages (Click to expand)&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-sh"&gt;# Maple Mono
brew install --cask font-maple-mono
# Maple Mono NF
brew install --cask font-maple-mono-nf
# Maple Mono CN
brew install --cask font-maple-mono-cn
# Maple Mono NF CN
brew install --cask font-maple-mono-nf-cn

# Maple Mono Normal
brew install --cask font-maple-mono-normal
# Maple Mono Normal NF
brew install --cask font-maple-mono-normal-nf
# Maple Mono Normal CN
brew install --cask font-maple-mono-normal-cn
# Maple Mono Normal NF CN
brew install --cask font-maple-mono-normal-nf-cn
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;Arch Linux&lt;/h3&gt; 
&lt;p&gt;ArchLinuxCN repository allows downloading a single package zip file without downloading all the package zip files in pkgbase, but AUR does not. (If you have a good solution, please contact Cyberczy(&lt;a href="mailto:czysheep@gmail.com"&gt;czysheep@gmail.com&lt;/a&gt;))&lt;/p&gt; 
&lt;h4&gt;ArchLinuxCN (Recommended)&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Maple Mono (Ligature TTF unhinted)
paru -S ttf-maplemono
# Maple Mono NF (Ligature unhinted)
paru -S ttf-maplemono-nf-unhinted
# Maple Mono NF CN (Ligature unhinted)
paru -S ttf-maplemono-nf-cn-unhinted
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;All packages (Click to expand)&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-sh"&gt;# Maple Mono (Ligature Variable)
paru -S ttf-maplemono-variable
# Maple Mono (Ligature TTF hinted)
paru -S ttf-maplemono-autohint
# Maple Mono (Ligature TTF unhinted)
paru -S ttf-maplemono
# Maple Mono (Ligature OTF)
paru -S otf-maplemono
# Maple Mono (Ligature WOFF2)
paru -S woff2-maplemono
# Maple Mono NF (Ligature hinted)
paru -S ttf-maplemono-nf
# Maple Mono NF (Ligature unhinted)
paru -S ttf-maplemono-nf-unhinted
# Maple Mono CN (Ligature hinted)
paru -S ttf-maplemono-cn
# Maple Mono CN (Ligature unhinted)
paru -S ttf-maplemono-cn-unhinted
# Maple Mono NF CN (Ligature hinted)
paru -S ttf-maplemono-nf-cn
# Maple Mono NF CN (Ligature unhinted)
paru -S ttf-maplemono-nf-cn-unhinted

# Maple Mono (No-Ligature Variable)
paru -S ttf-maplemononl-variable
# Maple Mono (No-Ligature TTF hinted)
paru -S ttf-maplemononl-autohint
# Maple Mono (No-Ligature TTF unhinted)
paru -S ttf-maplemononl
# Maple Mono (No-Ligature OTF)
paru -S otf-maplemononl
# Maple Mono (No-Ligature WOFF2)
paru -S woff2-maplemononl
# Maple Mono NF (No-Ligature hinted)
paru -S ttf-maplemononl-nf
# Maple Mono NF (No-Ligature unhinted)
paru -S ttf-maplemononl-nf-unhinted
# Maple Mono CN (No-Ligature hinted)
paru -S ttf-maplemononl-cn
# Maple Mono CN (No-Ligature unhinted)
paru -S ttf-maplemononl-cn-unhinted
# Maple Mono NF CN (No-Ligature hinted)
paru -S ttf-maplemononl-nf-cn
# Maple Mono NF CN (No-Ligature unhinted)
paru -S ttf-maplemononl-nf-cn-unhinted

# Maple Mono Normal (Ligature Variable)
paru -S ttf-maplemononormal-variable
# Maple Mono Normal (Ligature TTF hinted)
paru -S ttf-maplemononormal-autohint
# Maple Mono Normal (Ligature TTF unhinted)
paru -S ttf-maplemononormal
# Maple Mono Normal (Ligature OTF)
paru -S otf-maplemononormal
# Maple Mono Normal (Ligature WOFF2)
paru -S woff2-maplemononormal
# Maple Mono Normal NF (Ligature hinted)
paru -S ttf-maplemononormal-nf
# Maple Mono Normal NF (Ligature unhinted)
paru -S ttf-maplemononormal-nf-unhinted
# Maple Mono Normal CN (Ligature hinted)
paru -S ttf-maplemononormal-cn
# Maple Mono Normal CN (Ligature unhinted)
paru -S ttf-maplemononormal-cn-unhinted
# Maple Mono Normal NF CN (Ligature hinted)
paru -S ttf-maplemononormal-nf-cn
# Maple Mono Normal NF CN (Ligature unhinted)
paru -S ttf-maplemononormal-nf-cn-unhinted

# Maple Mono Normal (No-Ligature Variable)
paru -S ttf-maplemononormalnl-variable
# Maple Mono Normal (No-Ligature TTF hinted)
paru -S ttf-maplemononormalnl-autohint
# Maple Mono Normal (No-Ligature TTF unhinted)
paru -S ttf-maplemononormalnl
# Maple Mono Normal (No-Ligature OTF)
paru -S otf-maplemononormalnl
# Maple Mono Normal (No-Ligature WOFF2)
paru -S woff2-maplemononormalnl
# Maple Mono Normal NF (No-Ligature hinted)
paru -S ttf-maplemononormalnl-nf
# Maple Mono Normal NF (No-Ligature unhinted)
paru -S ttf-maplemononormalnl-nf-unhinted
# Maple Mono Normal CN (No-Ligature hinted)
paru -S ttf-maplemononormalnl-cn
# Maple Mono Normal CN (No-Ligature unhinted)
paru -S ttf-maplemononormalnl-cn-unhinted
# Maple Mono Normal NF CN (No-Ligature hinted)
paru -S ttf-maplemononormalnl-nf-cn
# Maple Mono Normal NF CN (No-Ligature unhinted)
paru -S ttf-maplemononormalnl-nf-cn-unhinted
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h4&gt;AUR (Not Recommended)&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Maple Mono (Ligature TTF unhinted)
paru -S maplemono-ttf
# Maple Mono NF (Ligature unhinted)
paru -S maplemono-nf-unhinted
# Maple Mono NF CN (Ligature unhinted)
paru -S maplemono-nf-cn-unhinted
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;All packages (Click to expand)&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-sh"&gt;# Maple Mono (Ligature Variable)
paru -S maplemono-variable
# Maple Mono (Ligature TTF hinted)
paru -S maplemono-ttf-autohint
# Maple Mono (Ligature TTF unhinted)
paru -S maplemono-ttf
# Maple Mono (Ligature OTF)
paru -S maplemono-otf
# Maple Mono (Ligature WOFF2)
paru -S maplemono-woff2
# Maple Mono NF (Ligature hinted)
paru -S maplemono-nf
# Maple Mono NF (Ligature unhinted)
paru -S maplemono-nf-unhinted
# Maple Mono CN (Ligature hinted)
paru -S maplemono-cn
# Maple Mono CN (Ligature unhinted)
paru -S maplemono-cn-unhinted
# Maple Mono NF CN (Ligature hinted)
paru -S maplemono-nf-cn
# Maple Mono NF CN (Ligature unhinted)
paru -S maplemono-nf-cn-unhinted

# Maple Mono (No-Ligature Variable)
paru -S maplemononl-variable
# Maple Mono (No-Ligature TTF hinted)
paru -S maplemononl-ttf-autohint
# Maple Mono (No-Ligature TTF unhinted)
paru -S maplemononl-ttf
# Maple Mono (No-Ligature OTF)
paru -S maplemononl-otf
# Maple Mono (No-Ligature WOFF2)
paru -S maplemononl-woff2
# Maple Mono NF (No-Ligature hinted)
paru -S maplemononl-nf
# Maple Mono NF (No-Ligature unhinted)
paru -S maplemononl-nf-unhinted
# Maple Mono CN (No-Ligature hinted)
paru -S maplemononl-cn
# Maple Mono CN (No-Ligature unhinted)
paru -S maplemononl-cn-unhinted
# Maple Mono NF CN (No-Ligature hinted)
paru -S maplemononl-nf-cn
# Maple Mono NF CN (No-Ligature unhinted)
paru -S maplemononl-nf-cn-unhinted

# Maple Mono Normal (Ligature Variable)
paru -S maplemononormal-variable
# Maple Mono Normal (Ligature TTF hinted)
paru -S maplemononormal-ttf-autohint
# Maple Mono Normal (Ligature TTF unhinted)
paru -S maplemononormal-ttf
# Maple Mono Normal (Ligature OTF)
paru -S maplemononormal-otf
# Maple Mono Normal (Ligature WOFF2)
paru -S maplemononormal-woff2
# Maple Mono Normal NF (Ligature hinted)
paru -S maplemononormal-nf
# Maple Mono Normal NF (Ligature unhinted)
paru -S maplemononormal-nf-unhinted
# Maple Mono Normal CN (Ligature hinted)
paru -S maplemononormal-cn
# Maple Mono Normal CN (Ligature unhinted)
paru -S maplemononormal-cn-unhinted
# Maple Mono Normal NF CN (Ligature hinted)
paru -S maplemononormal-nf-cn
# Maple Mono Normal NF CN (Ligature unhinted)
paru -S maplemononormal-nf-cn-unhinted

# Maple Mono Normal (No-Ligature Variable)
paru -S maplemononormalnl-variable
# Maple Mono Normal (No-Ligature TTF hinted)
paru -S maplemononormalnl-ttf-autohint
# Maple Mono Normal (No-Ligature TTF unhinted)
paru -S maplemononormalnl-ttf
# Maple Mono Normal (No-Ligature OTF)
paru -S maplemononormalnl-otf
# Maple Mono Normal (No-Ligature WOFF2)
paru -S maplemononormalnl-woff2
# Maple Mono Normal NF (No-Ligature hinted)
paru -S maplemononormalnl-nf
# Maple Mono Normal NF (No-Ligature unhinted)
paru -S maplemononormalnl-nf-unhinted
# Maple Mono Normal CN (No-Ligature hinted)
paru -S maplemononormalnl-cn
# Maple Mono Normal CN (No-Ligature unhinted)
paru -S maplemononormalnl-cn-unhinted
# Maple Mono Normal NF CN (No-Ligature hinted)
paru -S maplemononormalnl-nf-cn
# Maple Mono Normal NF CN (No-Ligature unhinted)
paru -S maplemononormalnl-nf-cn-unhinted
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;Nixpkgs (NixOS, Linux, MacOS)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-nix"&gt;fonts.packages = with pkgs; [
  # Maple Mono (Ligature TTF unhinted)
  maple-mono.truetype
  # Maple Mono NF (Ligature unhinted)
  maple-mono.NF-unhinted
  # Maple Mono NF CN (Ligature unhinted)
  maple-mono.NF-CN-unhinted
];
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;All packages (Click to expand)&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-nix"&gt;fonts.packages = with pkgs; [
  # Maple Mono (Ligature Variable)
  maple-mono.variable
  # Maple Mono (Ligature TTF hinted)
  maple-mono.truetype-autohint
  # Maple Mono (Ligature TTF unhinted)
  maple-mono.truetype
  # Maple Mono (Ligature OTF)
  maple-mono.opentype
  # Maple Mono (Ligature WOFF2)
  maple-mono.woff2
  # Maple Mono NF (Ligature hinted)
  maple-mono.NF
  # Maple Mono NF (Ligature unhinted)
  maple-mono.NF-unhinted
  # Maple Mono CN (Ligature hinted)
  maple-mono.CN
  # Maple Mono CN (Ligature unhinted)
  maple-mono.CN-unhinted
  # Maple Mono NF CN (Ligature hinted)
  maple-mono.NF-CN
  # Maple Mono NF CN (Ligature unhinted)
  maple-mono.NF-CN-unhinted

  # Maple Mono (No-Ligature Variable)
  maple-mono.NL-Variable
  # Maple Mono (No-Ligature TTF hinted)
  maple-mono.NL-TTF-AutoHint
  # Maple Mono (No-Ligature TTF unhinted)
  maple-mono.NL-TTF
  # Maple Mono (No-Ligature OTF)
  maple-mono.NL-OTF
  # Maple Mono (No-Ligature WOFF2)
  maple-mono.NL-Woff2
  # Maple Mono NF (No-Ligature hinted)
  maple-mono.NL-NF
  # Maple Mono NF (No-Ligature unhinted)
  maple-mono.NL-NF-unhinted
  # Maple Mono CN (No-Ligature hinted)
  maple-mono.NL-CN
  # Maple Mono CN (No-Ligature unhinted)
  maple-mono.NL-CN-unhinted
  # Maple Mono NF CN (No-Ligature hinted)
  maple-mono.NL-NF-CN
  # Maple Mono NF CN (No-Ligature unhinted)
  maple-mono.NL-NF-CN-unhinted

  # Maple Mono Normal (Ligature Variable)
  maple-mono.Normal-Variable
  # Maple Mono Normal (Ligature TTF hinted)
  maple-mono.Normal-TTF-AutoHint
  # Maple Mono Normal (Ligature TTF unhinted)
  maple-mono.Normal-TTF
  # Maple Mono Normal (Ligature OTF)
  maple-mono.Normal-OTF
  # Maple Mono Normal (Ligature WOFF2)
  maple-mono.Normal-Woff2
  # Maple Mono Normal NF (Ligature hinted)
  maple-mono.Normal-NF
  # Maple Mono Normal NF (Ligature unhinted)
  maple-mono.Normal-NF-unhinted
  # Maple Mono Normal CN (Ligature hinted)
  maple-mono.Normal-CN
  # Maple Mono Normal CN (Ligature unhinted)
  maple-mono.Normal-CN-unhinted
  # Maple Mono Normal NF CN (Ligature hinted)
  maple-mono.Normal-NF-CN
  # Maple Mono Normal NF CN (Ligature unhinted)
  maple-mono.Normal-NF-CN-unhinted

  # Maple Mono Normal (No-Ligature Variable)
  maple-mono.NormalNL-Variable
  # Maple Mono Normal (No-Ligature TTF hinted)
  maple-mono.NormalNL-TTF-AutoHint
  # Maple Mono Normal (No-Ligature TTF unhinted)
  maple-mono.NormalNL-TTF
  # Maple Mono Normal (No-Ligature OTF)
  maple-mono.NormalNL-OTF
  # Maple Mono Normal (No-Ligature WOFF2)
  maple-mono.NormalNL-Woff2
  # Maple Mono Normal NF (No-Ligature hinted)
  maple-mono.NormalNL-NF
  # Maple Mono Normal NF (No-Ligature unhinted)
  maple-mono.NormalNL-NF-unhinted
  # Maple Mono Normal CN (No-Ligature hinted)
  maple-mono.NormalNL-CN
  # Maple Mono Normal CN (No-Ligature unhinted)
  maple-mono.NormalNL-CN-unhinted
  # Maple Mono Normal NF CN (No-Ligature hinted)
  maple-mono.NormalNL-NF-CN
  # Maple Mono Normal NF CN (No-Ligature unhinted)
  maple-mono.NormalNL-NF-CN-unhinted
];
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;CDN&lt;/h2&gt; 
&lt;h3&gt;Maple Mono&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://fontsource.org/fonts/maple-mono"&gt;fontsource&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://fonts.zeoseven.com/items/443/"&gt;ZeoSeven Fonts&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Maple Mono CN&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://chinese-font.netlify.app/zh-cn/fonts/maple-mono-cn/MapleMono-CN-Regular"&gt;The Chinese Web Fonts Plan (‰∏≠ÊñáÁΩëÂ≠óËÆ°Âàí)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://fonts.zeoseven.com/items/442/"&gt;ZeoSeven Fonts&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Usage &amp;amp; Feature Configurations&lt;/h2&gt; 
&lt;p&gt;See in &lt;a href="https://raw.githubusercontent.com/subframe7536/maple-font/variable/source/features/README.md"&gt;document&lt;/a&gt; or try it in &lt;a href="https://font.subf.dev/en/playground"&gt;Playground&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Naming FAQ&lt;/h2&gt; 
&lt;h3&gt;Features&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Ligature&lt;/strong&gt;: Default version with ligatures (&lt;code&gt;Maple Mono&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;No-Ligature&lt;/strong&gt;: Default version without ligatures (&lt;code&gt;Maple Mono NL&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Normal-Ligature&lt;/strong&gt;: &lt;a href="https://raw.githubusercontent.com/subframe7536/maple-font/variable/#preset"&gt;&lt;code&gt;--normal&lt;/code&gt; preset&lt;/a&gt; with ligatures (&lt;code&gt;Maple Mono Normal&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Normal-No-Ligature&lt;/strong&gt;: &lt;a href="https://raw.githubusercontent.com/subframe7536/maple-font/variable/#preset"&gt;&lt;code&gt;--normal&lt;/code&gt; preset&lt;/a&gt; without ligatures (&lt;code&gt;Maple Mono Normal NL&lt;/code&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Format and Glyph Set&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Variable&lt;/strong&gt;: Minimal version, smoothly change font weight by variable&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;TTF&lt;/strong&gt;: Minimal version, ttf format [Recommend!]&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;OTF&lt;/strong&gt;: Minimal version, otf format&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;WOFF2&lt;/strong&gt;: Minimal version, woff2 format, for small size on web pages&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;NF&lt;/strong&gt;: Nerd-Font patched version, add icons for terminal (With &lt;code&gt;-NF&lt;/code&gt; suffix)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;CN&lt;/strong&gt;: Chinese version, embed with Chinese and Japanese glyphs (With &lt;code&gt;-CN&lt;/code&gt; suffix)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;NF-CN&lt;/strong&gt;: Full version, embed with icons, Chinese and Japanese glyphs (With &lt;code&gt;-NF-CN&lt;/code&gt; suffix)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Font Hint&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Hinted font&lt;/strong&gt; is used for low resolution screen to have better render effect. From my experience, if your screen resolution is lower or equal than 1080P, it is recommended to use "hinted font". Using "unhinted font" will lead to misalignment or uneven thickness on your text. 
  &lt;ul&gt; 
   &lt;li&gt;In this case, you can choose &lt;code&gt;MapleMono-TTF-AutoHint&lt;/code&gt; / &lt;code&gt;MapleMono-NF&lt;/code&gt; / &lt;code&gt;MapleMono-NF-CN&lt;/code&gt;, etc.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Unhinted font&lt;/strong&gt; is used for high resolution screen (e.g. for MacBook). Using "hinted font" will blur your text or make it looks weird. 
  &lt;ul&gt; 
   &lt;li&gt;In this case, you can choose &lt;code&gt;MapleMono-OTF&lt;/code&gt; / &lt;code&gt;MapleMono-TTF&lt;/code&gt; / &lt;code&gt;MapleMono-NF-unhinted&lt;/code&gt; / &lt;code&gt;MapleMono-NF-CN-unhinted&lt;/code&gt;, etc.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Why there exists &lt;code&gt;-AutoHint&lt;/code&gt; and &lt;code&gt;-unhinted&lt;/code&gt; suffix? 
  &lt;ul&gt; 
   &lt;li&gt;for backward compatibility, I keep the original naming scheme. &lt;code&gt;-AutoHint&lt;/code&gt; is only used for &lt;code&gt;TTF&lt;/code&gt; format.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Custom Build&lt;/h2&gt; 
&lt;p&gt;The &lt;a href="https://raw.githubusercontent.com/subframe7536/maple-font/variable/config.json"&gt;&lt;code&gt;config.json&lt;/code&gt;&lt;/a&gt; file is used to configure the build process. Checkout the &lt;a href="https://raw.githubusercontent.com/subframe7536/maple-font/variable/source/schema.json"&gt;schema&lt;/a&gt; or &lt;a href="https://raw.githubusercontent.com/subframe7536/maple-font/variable/source/features/README.md"&gt;document&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;p&gt;There also have some &lt;a href="https://raw.githubusercontent.com/subframe7536/maple-font/variable/#build-script-usage"&gt;command line options&lt;/a&gt; for customizing the build process. Cli options have higher priority than options in &lt;code&gt;config.json&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Build Methods&lt;/h3&gt; 
&lt;h4&gt;1. Build In Browser&lt;/h4&gt; 
&lt;p&gt;Go to &lt;a href="https://font.subf.dev/en/playground"&gt;Playground&lt;/a&gt;, and click "Custom Build" button in the bottom left corner&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Only support freezing OpenType features currently.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;2. Use Github Actions&lt;/h4&gt; 
&lt;p&gt;You can use &lt;a href="https://github.com/subframe7536/maple-font/actions/workflows/custom.yml"&gt;Github Actions&lt;/a&gt; to build the font.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Fork the repo&lt;/li&gt; 
 &lt;li&gt;(Optional) Change the content in &lt;code&gt;config.json&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Go to Actions tab&lt;/li&gt; 
 &lt;li&gt;Click on &lt;code&gt;Custom Build&lt;/code&gt; menu item on the left&lt;/li&gt; 
 &lt;li&gt;Click on &lt;code&gt;Run workflow&lt;/code&gt; button with options setup&lt;/li&gt; 
 &lt;li&gt;Wait for the build to finish&lt;/li&gt; 
 &lt;li&gt;Download the font archives from Releases&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;3. Use Docker&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;git clone https://github.com/subframe7536/maple-font --depth 1 -b variable
docker build -t maple-font .
docker run -v "$(pwd)/fonts:/app/fonts" -e BUILD_ARGS="--normal" maple-font
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;4. Local Build&lt;/h4&gt; 
&lt;p&gt;Clone the repo and run on your local machine. Make sure you have &lt;code&gt;python3&lt;/code&gt; and &lt;code&gt;pip&lt;/code&gt; installed&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;git clone https://github.com/subframe7536/maple-font --depth 1 -b variable
pip install -r requirements.txt
python build.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] For &lt;code&gt;Ubuntu&lt;/code&gt; or &lt;code&gt;Debian&lt;/code&gt;, maybe &lt;code&gt;python-is-python3&lt;/code&gt; is needed as well.&lt;/p&gt; 
 &lt;p&gt;If you have trouble installing the dependencies, just create a new GitHub Codespace and run the commands there.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Narrow Glyph Width&lt;/h3&gt; 
&lt;p&gt;You can setup &lt;code&gt;"width": "narrow"&lt;/code&gt; in &lt;code&gt;config.json&lt;/code&gt; or add &lt;code&gt;--width slim&lt;/code&gt; in cli flag to change glyph width at build time.&lt;/p&gt; 
&lt;p&gt;There are 3 options:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;default: 600&lt;/li&gt; 
 &lt;li&gt;narrow: 550&lt;/li&gt; 
 &lt;li&gt;slim: 500&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Preview: &lt;a href="https://github.com/subframe7536/maple-font/issues/131#issuecomment-3678666194"&gt;#131&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Custom Nerd-Font&lt;/h3&gt; 
&lt;p&gt;If you want to get fixed width icons, setup &lt;code&gt;"nerd_font.mono": true&lt;/code&gt; in &lt;code&gt;config.json&lt;/code&gt; or add &lt;code&gt;--nf-mono&lt;/code&gt; flag to build script args.&lt;/p&gt; 
&lt;p&gt;If you want to get variable width icons, setup &lt;code&gt;"nerd_font.propo": true&lt;/code&gt; in &lt;code&gt;config.json&lt;/code&gt; or add &lt;code&gt;--nf-propo&lt;/code&gt; flag to build script args.&lt;/p&gt; 
&lt;p&gt;For custom &lt;code&gt;font-patcher&lt;/code&gt; args, &lt;code&gt;font-forge&lt;/code&gt; (and maybe &lt;code&gt;python3-fontforge&lt;/code&gt; as well) is required.&lt;/p&gt; 
&lt;p&gt;Maybe you should also change &lt;code&gt;"nerd_font.extra_args"&lt;/code&gt; in &lt;a href="https://raw.githubusercontent.com/subframe7536/maple-font/variable/config.json"&gt;config.json&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Default args: &lt;code&gt;-l --careful --outputdir dir&lt;/code&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;if &lt;code&gt;"nerd_font.propo"&lt;/code&gt; is &lt;code&gt;true&lt;/code&gt;, then add &lt;code&gt;--variable-width-glyphs&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;else if &lt;code&gt;"nerd_font.mono"&lt;/code&gt; is &lt;code&gt;true&lt;/code&gt;, then add &lt;code&gt;--mono&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Preset&lt;/h3&gt; 
&lt;p&gt;Run &lt;code&gt;build.py&lt;/code&gt; with &lt;code&gt;--normal&lt;/code&gt; flag, make the font looks not such "Opinioned" , just like &lt;code&gt;JetBrains Mono&lt;/code&gt; (with slashed zero).&lt;/p&gt; 
&lt;p&gt;If you are using variable font (NOT recommended), please enable &lt;code&gt;calt&lt;/code&gt; to make all features work.&lt;/p&gt; 
&lt;p&gt;Enabled features:&lt;/p&gt; 
&lt;!-- NORMAL --&gt; 
&lt;pre&gt;&lt;code&gt;cv01, cv02, cv33, cv34, cv35, cv36, cv61, cv62, ss05, ss06, ss07, ss08
&lt;/code&gt;&lt;/pre&gt; 
&lt;!-- NORMAL --&gt; 
&lt;p&gt;&lt;a href="https://font.subf.dev/en/playground?normal"&gt;Online Preview&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Freeze OpenType Feature&lt;/h3&gt; 
&lt;p&gt;There are three kinds of options for feature freeze (&lt;a href="https://github.com/subframe7536/maple-font/issues/233#issuecomment-2410170270"&gt;Why&lt;/a&gt;):&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;code&gt;enable&lt;/code&gt;: Forcely enable the features without setting up &lt;code&gt;cvXX&lt;/code&gt; / &lt;code&gt;ssXX&lt;/code&gt; / &lt;code&gt;zero&lt;/code&gt; in font features config, just as default glyphs / ligatures&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;disable&lt;/code&gt;: Remove the features in &lt;code&gt;cvXX&lt;/code&gt; / &lt;code&gt;ssXX&lt;/code&gt; / &lt;code&gt;zero&lt;/code&gt;, which will no longer effect, even if you enable it manually&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ignore&lt;/code&gt;: Do nothing&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Custom OpenType Feature&lt;/h4&gt; 
&lt;p&gt;OpenType Feature is used to control the font's built-in variants and ligatures. You can remove some ligatures or features you don't want to, change feature's trigger rule or add some new rules by modifying OpenType Feature.&lt;/p&gt; 
&lt;p&gt;By default, the Python module in &lt;a href="https://raw.githubusercontent.com/subframe7536/maple-font/variable/source/py/feature"&gt;&lt;code&gt;source/py/feature/&lt;/code&gt;&lt;/a&gt; will generate feature rule string and load it at build time. You can modify the features or customize tags there.&lt;/p&gt; 
&lt;p&gt;If you would like to modify the feature file instead, run &lt;code&gt;build.py&lt;/code&gt; with &lt;code&gt;--apply-fea-file&lt;/code&gt; flag, the feature file from &lt;a href="https://raw.githubusercontent.com/subframe7536/maple-font/variable/source/features"&gt;&lt;code&gt;source/features/{regular,italic}{_cn,}.fea&lt;/code&gt;&lt;/a&gt; will be loaded.&lt;/p&gt; 
&lt;h3&gt;Infinite Arrow Ligatures&lt;/h3&gt; 
&lt;p&gt;Inspired by Fira Code, the font enables infinite arrow ligatures by default from v7.3. For some reason, the ligatures are misaligned when using hinted font, so they are removed in hinted version by default from v7.4.&lt;/p&gt; 
&lt;p&gt;You can setup &lt;code&gt;"infinite_arrow": true&lt;/code&gt; in &lt;code&gt;config.json&lt;/code&gt; or add &lt;code&gt;--infinite-arrow&lt;/code&gt; in cli flag to force enabling the feature. See more details in &lt;a href="https://github.com/subframe7536/maple-font/issues/508"&gt;#508&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Custom Font Weight Mapping&lt;/h3&gt; 
&lt;p&gt;You can modify the static font weight through &lt;code&gt;"weight_mapping"&lt;/code&gt; item in &lt;code&gt;config.json&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;For example, if you want to make regular font weight a little bit lighter, just decrease the number of &lt;code&gt;"weight_mapping.regular"&lt;/code&gt; (from 400 to 350 in this example) :&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "weight_mapping": {
    "thin": 100,
    "extralight": 200,
    "light": 300,
    "regular": 350,
    "semibold": 500,
    "medium": 600,
    "bold": 700,
    "extrabold": 800
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Chinese version&lt;/h3&gt; 
&lt;p&gt;CN version is disabled by default. Run &lt;code&gt;python build.py&lt;/code&gt; with &lt;code&gt;--cn&lt;/code&gt; flag, the CN base fonts (about 111 MB) will download from GitHub.&lt;/p&gt; 
&lt;p&gt;If you want to build CN base fonts from variable (about 27 MB), setup &lt;code&gt;"cn.use_static_base_font": false&lt;/code&gt; in &lt;a href="https://raw.githubusercontent.com/subframe7536/maple-font/variable/config.json"&gt;config.json&lt;/a&gt; and &lt;strong&gt;BE PATIENT&lt;/strong&gt;, instantiation will take about 10-30 minutes.&lt;/p&gt; 
&lt;h4&gt;Narrow spacing in CN glyphs&lt;/h4&gt; 
&lt;p&gt;If you think that &lt;strong&gt;CN glyphs spacing is TOOOOOO large&lt;/strong&gt;, there is a build option &lt;code&gt;cn.narrow&lt;/code&gt; or cli flag &lt;code&gt;--cn-narrow&lt;/code&gt; to narrow spacing in CN glyphs, but this will make the font cannot be recogized as monospaced font. You can see effect in &lt;a href="https://github.com/subframe7536/maple-font/issues/249#issuecomment-2871260476"&gt;#249&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;And if you want to change the Latin letters' width as well, use &lt;a href="https://raw.githubusercontent.com/subframe7536/maple-font/variable/#narrow-glyph-width"&gt;&lt;code&gt;--width&lt;/code&gt; option&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;GitHub Mirror&lt;/h4&gt; 
&lt;p&gt;The build script will auto download required assets from GitHub. If you have trouble downloading, please setup &lt;code&gt;github_mirror&lt;/code&gt; in &lt;a href="https://raw.githubusercontent.com/subframe7536/maple-font/variable/config.json"&gt;config.json&lt;/a&gt; or &lt;code&gt;$GITHUB&lt;/code&gt; to your environment variable. (Target URL will be &lt;code&gt;https://&amp;lt;github_mirror&amp;gt;/&amp;lt;user&amp;gt;/&amp;lt;repo&amp;gt;/releases/download/&amp;lt;tag&amp;gt;/&amp;lt;file&amp;gt;&lt;/code&gt;), or just download the target &lt;code&gt;.zip&lt;/code&gt; file and put it in the same directory as &lt;code&gt;build.py&lt;/code&gt;.&lt;/p&gt; 
&lt;h4&gt;Traditional Chinese Punctuation Support&lt;/h4&gt; 
&lt;p&gt;By enabling &lt;code&gt;cv99&lt;/code&gt;, all Chinese punctuation marks will be centred. See more details in &lt;a href="https://github.com/subframe7536/maple-font/issues/150"&gt;#150&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Build Script Usage&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;usage: build.py [-h] [-v] [-d] [--debug] [-n] [--feat FEAT] [--apply-fea-file]
                [--hinted | --no-hinted] [--liga | --no-liga] [--keep-infinite-arrow]
                [--infinite-arrow] [--remove-tag-liga] [--line-height LINE_HEIGHT]
                [--width {default,narrow,slim}] [--nf-mono] [--nf-propo]
                [--cn-narrow] [--cn-scale-factor CN_SCALE_FACTOR] [--nf | --no-nf]
                [--cn | --no-cn] [--cn-both] [--ttf-only] [--least-styles]
                [--font-patcher] [--cache] [--cn-rebuild] [--archive]

‚ú® Builder and optimizer for Maple Mono

options:
  -h, --help            show this help message and exit
  -v, --version         show program's version number and exit
  -d, --dry             Output config and exit
  --debug               Add `Debug` suffix to family name and faster build

Feature Options:
  -n, --normal          Use normal preset, just like `JetBrains Mono` with slashed
                        zero
  --feat FEAT           Freeze font features, splited by `,` (e.g. `--feat
                        zero,cv01,ss07,ss08`). No effect on variable format
  --apply-fea-file      Load feature file from `source/features/{regular,italic}.fea`
                        to variable font
  --hinted              Use hinted font as base font in NF / CN / NF-CN (default)
  --no-hinted           Use unhinted font as base font in NF / CN / NF-CN
  --liga                Preserve all the ligatures (default)
  --no-liga             Remove all the ligatures
  --infinite-arrow      Enable infinite arrow ligatures (Disabled in hinted font by
                        default)
  --remove-tag-liga     Remove plain text tag ligatures like `[TODO]`
  --line-height LINE_HEIGHT
                        Scale factor for line height (e.g. 1.1)
  --width {default,narrow,slim}
                        Set glyph width: default (600), narrow (550), slim (500)
  --nf-mono             Make Nerd Font icons' width fixed
  --nf-propo            Make Nerd Font icons' width variable, override `--nf-mono`
  --cn-narrow           Make CN / JP characters narrow (And the font cannot be
                        recogized as monospaced font)
  --cn-scale-factor CN_SCALE_FACTOR
                        Scale factor for CN / JP glyphs. Format: &amp;lt;factor&amp;gt; or
                        &amp;lt;width_factor&amp;gt;,&amp;lt;height_factor&amp;gt; (e.g. 1.1 or 1.2,1.1)

Build Options:
  --nf, --nerd-font     Build Nerd-Font version (default)
  --no-nf, --no-nerd-font
                        Do not build Nerd-Font version
  --cn                  Build Chinese version
  --no-cn               Do not build Chinese version (default)
  --cn-both             Build both `Maple Mono CN` and `Maple Mono NF CN`. Nerd-Font
                        version must be enabled
  --ttf-only            Only build TTF format
  --least-styles        Only build Regular / Bold / Italic / BoldItalic style
  --font-patcher        Force the use of Nerd Font Patcher to build NF format
  --cache               Reuse font cache of TTF, OTF and Woff2 formats
  --cn-rebuild          Reinstantiate variable CN base font
  --archive             Build font archives with config and license. If has `--cache`
                        flag, only archive NF and CN formats
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Development&lt;/h2&gt; 
&lt;h3&gt;Design&lt;/h3&gt; 
&lt;p&gt;Using &lt;a href="https://www.fontlab.com/"&gt;FontLab&lt;/a&gt; or &lt;a href="https://glyphs.app"&gt;Glyphs&lt;/a&gt;, generate variable TTF into &lt;code&gt;source/&lt;/code&gt; folder.&lt;/p&gt; 
&lt;h3&gt;Build&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Init project
uv sync
# Dev
uv run build.py --ttf-only --cn --debug
# Update nerd font
uv run task.py nerd-font
# Update fea file
uv run task.py fea
# Update landing page info
uv run task.py page
# Release
uv run task.py release 7.0
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Credit&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/JetBrains/JetBrainsMono"&gt;JetBrains Mono&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/googlefonts/RobotoMono"&gt;Roboto Mono&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/tonsky/FiraCode"&gt;Fira Code&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/rubjo/victor-mono"&gt;Victor Mono&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/eigilnikolajsen/commit-mono"&gt;Commit Mono&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/TheRenegadeCoder/sample-programs-website"&gt;Code Sample&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ryanoasis/nerd-fonts"&gt;Nerd Font&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/MuTsunTsai/fontfreeze/"&gt;Font Freeze&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://tophix.com/font-tools/font-viewer"&gt;Font Viewer&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.monolisa.dev/"&gt;Monolisa&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.recursive.design/"&gt;Recursive&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Sponser&lt;/h2&gt; 
&lt;p&gt;If this font is helpful to you, please feel free to buy me a coffee&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.buymeacoffee.com/subframe753"&gt;&lt;img src="https://img.buymeacoffee.com/button-api/?text=Buy me a coffee&amp;amp;emoji=&amp;amp;slug=subframe753&amp;amp;button_colour=5F7FFF&amp;amp;font_colour=ffffff&amp;amp;font_family=Lato&amp;amp;outline_colour=000000&amp;amp;coffee_colour=FFDD00" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;or sponser me through &lt;a href="https://afdian.com/a/subframe7536"&gt;Afdian&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;a href="https://www.star-history.com/#subframe7536/maple-font&amp;amp;type=date&amp;amp;legend=top-left"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=subframe7536/maple-font&amp;amp;type=date&amp;amp;theme=dark&amp;amp;legend=top-left" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=subframe7536/maple-font&amp;amp;type=date&amp;amp;legend=top-left" /&gt; 
  &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=subframe7536/maple-font&amp;amp;type=date&amp;amp;legend=top-left" /&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;SIL Open Font License 1.1&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>google-gemini/computer-use-preview</title>
      <link>https://github.com/google-gemini/computer-use-preview</link>
      <description>&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Computer Use Preview&lt;/h1&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;p&gt;This section will guide you through setting up and running the Computer Use Preview model, either the Gemini Developer API or Vertex AI. Follow these steps to get started.&lt;/p&gt; 
&lt;h3&gt;1. Installation&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Clone the Repository&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/google/computer-use-preview.git
cd computer-use-preview
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Set up Python Virtual Environment and Install Dependencies&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Install Playwright and Browser Dependencies&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install system dependencies required by Playwright for Chrome
playwright install-deps chrome

# Install the Chrome browser for Playwright
playwright install chrome
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. Configuration&lt;/h3&gt; 
&lt;p&gt;You can get started using either the Gemini Developer API or Vertex AI.&lt;/p&gt; 
&lt;h4&gt;A. If using the Gemini Developer API:&lt;/h4&gt; 
&lt;p&gt;You need a Gemini API key to use the agent:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export GEMINI_API_KEY="YOUR_GEMINI_API_KEY"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or to add this to your virtual environment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;echo 'export GEMINI_API_KEY="YOUR_GEMINI_API_KEY"' &amp;gt;&amp;gt; .venv/bin/activate
# After editing, you'll need to deactivate and reactivate your virtual
# environment if it's already active:
deactivate
source .venv/bin/activate
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Replace &lt;code&gt;YOUR_GEMINI_API_KEY&lt;/code&gt; with your actual key.&lt;/p&gt; 
&lt;h4&gt;B. If using the Vertex AI Client:&lt;/h4&gt; 
&lt;p&gt;You need to explicitly use Vertex AI, then provide project and location to use the agent:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export USE_VERTEXAI=true
export VERTEXAI_PROJECT="YOUR_PROJECT_ID"
export VERTEXAI_LOCATION="YOUR_LOCATION"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or to add this to your virtual environment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;echo 'export USE_VERTEXAI=true' &amp;gt;&amp;gt; .venv/bin/activate
echo 'export VERTEXAI_PROJECT="your-project-id"' &amp;gt;&amp;gt; .venv/bin/activate
echo 'export VERTEXAI_LOCATION="your-location"' &amp;gt;&amp;gt; .venv/bin/activate
# After editing, you'll need to deactivate and reactivate your virtual
# environment if it's already active:
deactivate
source .venv/bin/activate
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Replace &lt;code&gt;YOUR_PROJECT_ID&lt;/code&gt; and &lt;code&gt;YOUR_LOCATION&lt;/code&gt; with your actual project and location.&lt;/p&gt; 
&lt;h3&gt;3. Running the Tool&lt;/h3&gt; 
&lt;p&gt;The primary way to use the tool is via the &lt;code&gt;main.py&lt;/code&gt; script.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;General Command Structure:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python main.py --query "Go to Google and type 'Hello World' into the search bar"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Available Environments:&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;You can specify a particular environment with the &lt;code&gt;--env &amp;lt;environment&amp;gt;&lt;/code&gt; flag. Available options:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;playwright&lt;/code&gt;: Runs the browser locally using Playwright.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;browserbase&lt;/code&gt;: Connects to a Browserbase instance.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Local Playwright&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Runs the agent using a Chrome browser instance controlled locally by Playwright.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python main.py --query="Go to Google and type 'Hello World' into the search bar" --env="playwright"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also specify an initial URL for the Playwright environment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python main.py --query="Go to Google and type 'Hello World' into the search bar" --env="playwright" --initial_url="https://www.google.com/search?q=latest+AI+news"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Browserbase&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Runs the agent using Browserbase as the browser backend. Ensure the proper Browserbase environment variables are set:&lt;code&gt;BROWSERBASE_API_KEY&lt;/code&gt; and &lt;code&gt;BROWSERBASE_PROJECT_ID&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python main.py --query="Go to Google and type 'Hello World' into the search bar" --env="browserbase"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Agent CLI&lt;/h2&gt; 
&lt;p&gt;The &lt;code&gt;main.py&lt;/code&gt; script is the command-line interface (CLI) for running the browser agent.&lt;/p&gt; 
&lt;h3&gt;Command-Line Arguments&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Argument&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Required&lt;/th&gt; 
   &lt;th&gt;Default&lt;/th&gt; 
   &lt;th&gt;Supported Environment(s)&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--query&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;The natural language query for the browser agent to execute.&lt;/td&gt; 
   &lt;td&gt;Yes&lt;/td&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
   &lt;td&gt;All&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--env&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;The computer use environment to use. Must be one of the following: &lt;code&gt;playwright&lt;/code&gt;, or &lt;code&gt;browserbase&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;No&lt;/td&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
   &lt;td&gt;All&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--initial_url&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;The initial URL to load when the browser starts.&lt;/td&gt; 
   &lt;td&gt;No&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.google.com"&gt;https://www.google.com&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;All&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--highlight_mouse&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;If specified, the agent will attempt to highlight the mouse cursor's position in the screenshots. This is useful for visual debugging.&lt;/td&gt; 
   &lt;td&gt;No&lt;/td&gt; 
   &lt;td&gt;False (not highlighted)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;playwright&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Environment Variables&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Required&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;GEMINI_API_KEY&lt;/td&gt; 
   &lt;td&gt;Your API key for the Gemini model.&lt;/td&gt; 
   &lt;td&gt;Yes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BROWSERBASE_API_KEY&lt;/td&gt; 
   &lt;td&gt;Your API key for Browserbase.&lt;/td&gt; 
   &lt;td&gt;Yes (when using the browserbase environment)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BROWSERBASE_PROJECT_ID&lt;/td&gt; 
   &lt;td&gt;Your Project ID for Browserbase.&lt;/td&gt; 
   &lt;td&gt;Yes (when using the browserbase environment)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Known Issues&lt;/h2&gt; 
&lt;h3&gt;Playwright Dropdown Menu&lt;/h3&gt; 
&lt;p&gt;On certain operating systems, the Playwright browser is unable to capture &lt;code&gt;&amp;lt;select&amp;gt;&lt;/code&gt; elements because they are rendered by the operating system. As a result, the agent is unable to send the correct screenshot to the model.&lt;/p&gt; 
&lt;p&gt;There are several ways to mitigate this.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Use the Browserbase option instead of Playwright.&lt;/li&gt; 
 &lt;li&gt;Inject a script like &lt;a href="https://github.com/amitamb/proxy-select"&gt;proxy-select&lt;/a&gt; to render a custom &lt;code&gt;&amp;lt;select&amp;gt;&lt;/code&gt; element. You must inject &lt;code&gt;proxy-select.css&lt;/code&gt; and &lt;code&gt;proxy-select.js&lt;/code&gt; into each page that has a non-custom &lt;code&gt;&amp;lt;select&amp;gt;&lt;/code&gt; element. You can do this in the &lt;a href="https://github.com/google-gemini/computer-use-preview/raw/main/computers/playwright/playwright.py#L100"&gt;&lt;code&gt;Playwright.__enter__&lt;/code&gt;&lt;/a&gt; method by adding a few lines of code, like the following (replacing &lt;code&gt;PROXY_SELECT_JS&lt;/code&gt; and &lt;code&gt;PROXY_SELECT_CSS&lt;/code&gt; with the appropriate variables):&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;self._page.add_init_script(PROXY_SELECT_JS)
def inject_style(page):
    try:
        page.add_style_tag(content=PROXY_SELECT_CSS)
    except Exception as e:
        print(f"Error injecting style: {e}")

self._page.on('domcontentloaded', inject_style)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note, option 2 does not work 100% of the time, but is a temporary workaround for certain websites. The better option is to use Browserbase.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>OpenMind/OM1</title>
      <link>https://github.com/OpenMind/OM1</link>
      <description>&lt;p&gt;Modular AI runtime for robots&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/853153b7-351a-433d-9e1a-d257b781f93c" alt="OM_Banner_X2 (1)" /&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://arxiv.org/abs/2412.18588"&gt;Technical Paper&lt;/a&gt; | &lt;a href="https://docs.openmind.org/"&gt;Documentation&lt;/a&gt; | &lt;a href="https://x.com/openmind_agi"&gt;X&lt;/a&gt; | &lt;a href="https://discord.gg/openmind"&gt;Discord&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;OpenMind's OM1 is a modular AI runtime that empowers developers to create and deploy multimodal AI agents across digital environments and physical robots&lt;/strong&gt;, including Humanoids, Phone Apps, websites, Quadrupeds, and educational robots such as TurtleBot 4. OM1 agents can process diverse inputs like web data, social media, camera feeds, and LIDAR, while enabling physical actions including motion, autonomous navigation, and natural conversations. The goal of OM1 is to make it easy to create highly capable human-focused robots, that are easy to upgrade and (re)configure to accommodate different physical form factors.&lt;/p&gt; 
&lt;h2&gt;Capabilities of OM1&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Modular Architecture&lt;/strong&gt;: Designed with Python for simplicity and seamless integration.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Data Input&lt;/strong&gt;: Easily handles new data and sensors.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Hardware Support via Plugins&lt;/strong&gt;: Supports new hardware through plugins for API endpoints and specific robot hardware connections to &lt;code&gt;ROS2&lt;/code&gt;, &lt;code&gt;Zenoh&lt;/code&gt;, and &lt;code&gt;CycloneDDS&lt;/code&gt;. (We recommend &lt;code&gt;Zenoh&lt;/code&gt; for all new development).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Web-Based Debugging Display&lt;/strong&gt;: Monitor the system in action with WebSim (available at &lt;a href="http://localhost:8000/"&gt;http://localhost:8000/&lt;/a&gt;) for easy visual debugging.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Pre-configured Endpoints&lt;/strong&gt;: Supports Text-to-Speech, multiple LLMs from OpenAI, xAI, DeepSeek, Anthropic, Meta, Gemini, NearAI and multiple Visual Language Models (VLMs) with pre-configured endpoints for each service.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Architecture Overview&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/dd91457d-010f-43d8-960e-d1165834aa58" alt="Artboard 1@4x 1 (1)" /&gt;&lt;/p&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;To get started with OM1, let's run the Spot agent. Spot uses your webcam to capture and label objects. These text captions are then sent to the LLM, which returns &lt;code&gt;movement&lt;/code&gt;, &lt;code&gt;speech&lt;/code&gt; and &lt;code&gt;face&lt;/code&gt; action commands. These commands are displayed on WebSim along with basic timing and other debugging information.&lt;/p&gt; 
&lt;h3&gt;Package Management and VENV&lt;/h3&gt; 
&lt;p&gt;You will need the &lt;a href="https://docs.astral.sh/uv/getting-started/installation/"&gt;&lt;code&gt;uv&lt;/code&gt; package manager&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Clone the Repo&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/OpenMind/OM1.git
cd OM1
git submodule update --init
uv venv
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Install Dependencies&lt;/h3&gt; 
&lt;p&gt;For MacOS&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;brew install portaudio ffmpeg
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For Linux&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo apt-get update
sudo apt-get install portaudio19-dev python-dev ffmpeg
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Obtain an OpenMind API Key&lt;/h3&gt; 
&lt;p&gt;Obtain your API Key at &lt;a href="https://portal.openmind.org/"&gt;OpenMind Portal&lt;/a&gt;. Copy it to &lt;code&gt;config/spot.json5&lt;/code&gt;, replacing the &lt;code&gt;openmind_free&lt;/code&gt; placeholder. Or, &lt;code&gt;cp env.example .env&lt;/code&gt; and add your key to the &lt;code&gt;.env&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Launching OM1&lt;/h3&gt; 
&lt;p&gt;Run&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run src/run.py spot
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After launching OM1, the Spot agent will interact with you and perform (simulated) actions. For more help connecting OM1 to your robot hardware, see &lt;a href="https://docs.openmind.org/developing/1_get-started"&gt;getting started&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Note: This is just an example agent configuration. If you want to interact with the agent and see how it works, make sure ASR and TTS are configured in spot.json5.&lt;/p&gt; 
&lt;h2&gt;What's Next?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Try out some &lt;a href="https://docs.openmind.org/examples"&gt;examples&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Add new &lt;code&gt;inputs&lt;/code&gt; and &lt;code&gt;actions&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Design custom agents and robots by creating your own &lt;code&gt;json5&lt;/code&gt; config files with custom combinations of inputs and actions.&lt;/li&gt; 
 &lt;li&gt;Change the system prompts in the configuration files (located in &lt;code&gt;/config/&lt;/code&gt;) to create new behaviors.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Interfacing with New Robot Hardware&lt;/h2&gt; 
&lt;p&gt;OM1 assumes that robot hardware provides a high-level SDK that accepts elemental movement and action commands such as &lt;code&gt;backflip&lt;/code&gt;, &lt;code&gt;run&lt;/code&gt;, &lt;code&gt;gently pick up the red apple&lt;/code&gt;, &lt;code&gt;move(0.37, 0, 0)&lt;/code&gt;, and &lt;code&gt;smile&lt;/code&gt;. An example is provided in &lt;code&gt;src/actions/move/connector/ros2.py&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;...
elif output_interface.action == "shake paw":
    if self.sport_client:
        self.sport_client.Hello()
...
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If your robot hardware does not yet provide a suitable HAL (hardware abstraction layer), traditional robotics approaches such as RL (reinforcement learning) in concert with suitable simulation environments (Unity, Gazebo), sensors (such as hand mounted ZED depth cameras), and custom VLAs will be needed for you to create one. It is further assumed that your HAL accepts motion trajectories, provides battery and thermal management/monitoring, and calibrates and tunes sensors such as IMUs, LIDARs, and magnetometers.&lt;/p&gt; 
&lt;p&gt;OM1 can interface with your HAL via USB, serial, ROS2, CycloneDDS, Zenoh, or websockets. For an example of an advanced humanoid HAL, please see &lt;a href="https://github.com/unitreerobotics/unitree_sdk2/raw/adee312b081c656ecd0bb4e936eed96325546296/example/g1/high_level/g1_loco_client_example.cpp#L159"&gt;Unitree's C++ SDK&lt;/a&gt;. Frequently, a HAL, especially ROS2 code, will be dockerized and can then interface with OM1 through DDS middleware or websockets.&lt;/p&gt; 
&lt;h2&gt;Recommended Development Platforms&lt;/h2&gt; 
&lt;p&gt;OM1 is developed on:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Nvidia Thor (running JetPak 7.0) - full support&lt;/li&gt; 
 &lt;li&gt;Jetson AGX Orin 64GB (running Ubuntu 22.04 and JetPack 6.1) - limited support&lt;/li&gt; 
 &lt;li&gt;Mac Studio with Apple M2 Ultra with 48 GB unified memory (running MacOS Sequoia)&lt;/li&gt; 
 &lt;li&gt;Mac Mini with Apple M4 Pro with 48 GB unified memory (running MacOS Sequoia)&lt;/li&gt; 
 &lt;li&gt;Generic Linux machines (running Ubuntu 22.04)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;OM1 &lt;em&gt;should&lt;/em&gt; run on other platforms (such as Windows) and microcontrollers such as the Raspberry Pi 5 16GB.&lt;/p&gt; 
&lt;h2&gt;Full Autonomy Guidance&lt;/h2&gt; 
&lt;p&gt;We're excited to introduce &lt;strong&gt;full autonomy&lt;/strong&gt; for Unitree Go2 and G1. Full autonomy has four services that work together in a loop without manual intervention:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;om1&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;unitree_sdk&lt;/strong&gt; ‚Äì A ROS 2 package that provides SLAM (Simultaneous Localization and Mapping) capabilities for the Unitree Go2 robot using an RPLiDAR sensor, the SLAM Toolbox and the Nav2 stack.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;om1-avatar&lt;/strong&gt; ‚Äì A modern React-based frontend application that provides the user interface and avatar display system for OM1 robotics software.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;om1-video-processor&lt;/strong&gt; - The OM1 Video Processor is a Docker-based solution that enables real-time video streaming, face recognition, and audio capture for OM1 robots.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Intro to BrainPack?&lt;/h2&gt; 
&lt;p&gt;From research to real-world autonomy, a platform that learns, moves, and builds with you. We'll shortly be releasing the &lt;strong&gt;BOM&lt;/strong&gt; and details on &lt;strong&gt;DIY&lt;/strong&gt; for it. Stay tuned!&lt;/p&gt; 
&lt;p&gt;Clone the following repos -&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenMind/OM1.git"&gt;https://github.com/OpenMind/OM1.git&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenMind/unitree-sdk.git"&gt;https://github.com/OpenMind/unitree-sdk.git&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenMind/OM1-avatar.git"&gt;https://github.com/OpenMind/OM1-avatar.git&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenMind/OM1-video-processor.git"&gt;https://github.com/OpenMind/OM1-video-processor.git&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Starting the system&lt;/h2&gt; 
&lt;p&gt;To start all services, run the following commands:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;For OM1&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Setup the API key&lt;/p&gt; 
&lt;p&gt;For Bash: vim ~/.bashrc or ~/.bash_profile.&lt;/p&gt; 
&lt;p&gt;For Zsh: vim ~/.zshrc.&lt;/p&gt; 
&lt;p&gt;Add&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export OM_API_KEY="your_api_key"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Update the docker-compose file. Replace "unitree_go2_autonomy_advance" with the agent you want to run.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;command: ["unitree_go2_autonomy_advance"]
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd OM1
docker compose up om1 -d --no-build
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;For unitree_sdk&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd unitree_sdk
docker compose up orchestrator -d --no-build
docker compose up om1_sensor -d --no-build
docker compose up watchdog -d --no-build
docker compose up zenoh_bridge -d --no-build
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;For OM1-avatar&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd OM1-avatar
docker compose up om1_avatar -d --no-build
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;For OM1-video-processor&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd OM1-video-processor
docker compose up -d
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Detailed Documentation&lt;/h2&gt; 
&lt;p&gt;More detailed documentation can be accessed at &lt;a href="https://docs.openmind.org/"&gt;docs.openmind.org&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Please make sure to read the &lt;a href="https://raw.githubusercontent.com/OpenMind/OM1/main/CONTRIBUTING.md"&gt;Contributing Guide&lt;/a&gt; before making a pull request.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the terms of the MIT License, which is a permissive free software license that allows users to freely use, modify, and distribute the software. The MIT License is a widely used and well-established license that is known for its simplicity and flexibility. By using the MIT License, this project aims to encourage collaboration, modification, and distribution of the software.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Bambu-Research-Group/RFID-Tag-Guide</title>
      <link>https://github.com/Bambu-Research-Group/RFID-Tag-Guide</link>
      <description>&lt;p&gt;Instructions on how to read out the bambulab nfc tags&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Bambu Lab RFID Tag Guide&lt;/h1&gt; 
&lt;p&gt;This guide gives you a basic overview how you can decrypt and read your tags.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/queengooborg/Bambu-Lab-RFID-Library"&gt;View Collection of Tags&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://discord.gg/zVfCVubwr7"&gt;&lt;img src="https://img.shields.io/badge/Discord-join_now-blue?style=flat-square&amp;amp;logo=discord&amp;amp;logoColor=white&amp;amp;label=Discord&amp;amp;color=blue" alt="Link to Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Table of contents&lt;/h1&gt; 
&lt;!--ts--&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Bambu-Research-Group/RFID-Tag-Guide/main/#project-summary"&gt;Project Summary&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Bambu-Research-Group/RFID-Tag-Guide/main/#faqs"&gt;FAQs&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Bambu-Research-Group/RFID-Tag-Guide/main/#how-to-contribute"&gt;How to contribute&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Bambu-Research-Group/RFID-Tag-Guide/main/#todostimelinenext-steps"&gt;Todos/Timeline/Next steps&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Bambu-Research-Group/RFID-Tag-Guide/main/#requirements"&gt;Requirements&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Bambu-Research-Group/RFID-Tag-Guide/main/#proxmark3-compatible-readers"&gt;Proxmark3 compatible readers&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Bambu-Research-Group/RFID-Tag-Guide/main/#proxmark3-easy"&gt;Proxmark3 Easy&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Bambu-Research-Group/RFID-Tag-Guide/main/#hacking-a-bambu-lab-tag-and-readout-of-its-data"&gt;Hacking a Bambu Lab Tag and readout of its data&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Bambu-Research-Group/RFID-Tag-Guide/main/#deriving-the-keys"&gt;Deriving the keys&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Bambu-Research-Group/RFID-Tag-Guide/main/#proxmark3-fm11rf08s-recovery-script"&gt;Proxmark3 fm11rf08s recovery script&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Bambu-Research-Group/RFID-Tag-Guide/main/#sniffing-the-tag-data-with-a-proxmark3-legacy-method"&gt;Sniffing the tag data with a Proxmark3 (legacy method)&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Bambu-Research-Group/RFID-Tag-Guide/main/#tag-documentation"&gt;Tag Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Bambu-Research-Group/RFID-Tag-Guide/main/#how-do-rfid-tags-work"&gt;How do RFID tags work?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Bambu-Research-Group/RFID-Tag-Guide/main/#compatible-rfid-tags---by-generation"&gt;Compatible RFID tags - By generation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Bambu-Research-Group/RFID-Tag-Guide/main/#reverse-engineering-rfid-board"&gt;Reverse engineering RFID Board&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!--te--&gt; 
&lt;h2&gt;Project Summary&lt;/h2&gt; 
&lt;p&gt;This is a research group dedicated to documenting the data structures used by Bambu Lab 3D printers to identify filament data.&lt;/p&gt; 
&lt;h3&gt;FAQs&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Can I create custom tags?&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;No, tags are digitally signed. Even if you modify the contents, the printer will reject any tags without a valid RSA signature&lt;/li&gt; 
   &lt;li&gt;An &lt;a href="https://raw.githubusercontent.com/Bambu-Research-Group/RFID-Tag-Guide/main/OpenSourceRfid.md"&gt;Open Source RFID Tag&lt;/a&gt; has been proposed to allow anyone to create / modify their own tags. This must be adopted by printer manufacturers, or you can mod your own printer for support&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Can I clone tags?&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Yes, you can read and clone tags using a tool such as a Proxmark3. Check out our &lt;a href="https://github.com/queengooborg/Bambu-Lab-RFID-Library"&gt;collection of scanned tags&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;What are the next steps for this project?&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Decyphering the rest of the unknown tag content&lt;/li&gt; 
   &lt;li&gt;Custom AMS firmware that allows custom tags to be read while ignoring the signature&lt;/li&gt; 
   &lt;li&gt;See &lt;a href="https://raw.githubusercontent.com/Bambu-Research-Group/RFID-Tag-Guide/main/#todostimelinenext-steps"&gt;Todos/Timeline/Next steps&lt;/a&gt; for more info&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;How to contribute&lt;/h3&gt; 
&lt;p&gt;If you have a Proxmark3 (or other RFID debugging tool), you can decrypt the contents of your Bambu Lab RFID tags and submit them via &lt;a href="https://discord.gg/zVfCVubwr7"&gt;Discord&lt;/a&gt;. A lot of the contents have been deciphered, but the more data we have, the easier it is to compare differences to learn what each byte represents and double-check our answers.&lt;/p&gt; 
&lt;h3&gt;Todos/Timeline/Next steps&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Tool for automatic trace analysis&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Tag content analysis&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Generate keys based on an arbitrary UID&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;A computer running macOS or Linux, or a Windows computer with a WSL installation&lt;/li&gt; 
 &lt;li&gt;Python 3.6 or higher&lt;/li&gt; 
 &lt;li&gt;Bambu Lab Filament spool &lt;strong&gt;or&lt;/strong&gt; the related tags&lt;/li&gt; 
 &lt;li&gt;An NFC/RFID reader that can read encrypted tags, such as... 
  &lt;ul&gt; 
   &lt;li&gt;A Proxmark3-compatible RFID reader (recommended) 
    &lt;ul&gt; 
     &lt;li&gt;The &lt;a href="https://github.com/RfidResearchGroup/proxmark3"&gt;proxmark3 (Iceman fork) software&lt;/a&gt; 
      &lt;ul&gt; 
       &lt;li&gt;Requires v4.18994 (codename "Backdoor") or higher&lt;/li&gt; 
       &lt;li&gt;You MUST use the Iceman fork as the original version of the software is unmaintained; all instructions and scripts are written for the Iceman fork and will not work on the original version&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;A Flipper Zero&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Proxmark3 compatible readers&lt;/h3&gt; 
&lt;h4&gt;Proxmark3 Easy&lt;/h4&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Bambu-Research-Group/RFID-Tag-Guide/main/images/Proxmark3_easy.png" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;A Proxmark3 Easy is sufficient for all the tasks that need to be done. You can buy a clone from Alixepress, Amazon or Dangerous Things.&lt;/p&gt; 
&lt;h2&gt;Hacking a Bambu Lab Tag and readout of its data&lt;/h2&gt; 
&lt;p&gt;We document here the most simple approach to get all required A-Keys and the data of the tag. The easiest way is to derive the keys using the Python script in this repository.&lt;/p&gt; 
&lt;h3&gt;Deriving the keys&lt;/h3&gt; 
&lt;p&gt;A way to derive the keys from the UID of an RFID tag was discovered, which unlocked the ability to scan and scrape RFID tag data without sniffing, as well as with other devices like the Flipper Zero. A script is included in the repository to derive the keys from the UID of a tag.&lt;/p&gt; 
&lt;p&gt;First, obtain the tag's UID:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Proxmark3 
  &lt;ol&gt; 
   &lt;li&gt;Run the Proxmark3 software by running &lt;code&gt;pm3&lt;/code&gt; in the terminal&lt;/li&gt; 
   &lt;li&gt;Place the Proxmark3 device on the RFID tag of the spool&lt;/li&gt; 
   &lt;li&gt;Run &lt;code&gt;hf mf info&lt;/code&gt; and look for the UID line item&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
 &lt;li&gt;Flipper Zero 
  &lt;ol&gt; 
   &lt;li&gt;Open the NFC app and scan the tag&lt;/li&gt; 
   &lt;li&gt;The Flipper will attempt to decrypt the tag, but you can skip the "Nested Dictionary (Backdoor)" step for speed&lt;/li&gt; 
   &lt;li&gt;The UID of the tag will appear on-screen&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
 &lt;li&gt;Bambu Lab AMS 
  &lt;ol&gt; 
   &lt;li&gt;Load the spool into an AMS slot and wait for it to finish loading&lt;/li&gt; 
   &lt;li&gt;View the spool's details on the printer's touchscreen, Bambu Studio or Bambu Handy&lt;/li&gt; 
   &lt;li&gt;The UID is the first eight characters of the spool's serial number&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Next, run the key derivation script and pipe its output to a file by running &lt;code&gt;python3 deriveKeys.py [UID] &amp;gt; ./keys.dic&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Then, use the keys file to extract the data from the RFID tag:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Proxmark3 
  &lt;ol&gt; 
   &lt;li&gt;Run the Proxmark3 software by running &lt;code&gt;pm3&lt;/code&gt; in the terminal&lt;/li&gt; 
   &lt;li&gt;Place the Proxmark3 device on the RFID tag of the spool&lt;/li&gt; 
   &lt;li&gt;Run &lt;code&gt;hf mf dump -k ./keys.dic&lt;/code&gt; to dump the RFID tag's contents&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
 &lt;li&gt;Flipper Zero 
  &lt;ol&gt; 
   &lt;li&gt;Open the qFlipper program and connect your Flipper to your computer&lt;/li&gt; 
  &lt;/ol&gt; 
  &lt;ul&gt; 
   &lt;li&gt;You may also connect the SD card directly to your computer&lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;ol start="2"&gt; 
   &lt;li&gt;Navigate to &lt;code&gt;SD Card/nfc/assets/&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Copy the &lt;code&gt;mf_classic_dict_user.nfc&lt;/code&gt; file to your computer&lt;/li&gt; 
   &lt;li&gt;Copy the contents of &lt;code&gt;keys.dic&lt;/code&gt; to &lt;code&gt;mf_classic_dict_user.nfc&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Copy &lt;code&gt;mf_classic_dict_user.nfc&lt;/code&gt; back onto your Flipper&lt;/li&gt; 
   &lt;li&gt;Use the NFC app to scan your tag&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Proxmark3 fm11rf08s recovery script&lt;/h3&gt; 
&lt;p&gt;In 2024, a new backdoor&lt;a href="https://eprint.iacr.org/2024/1275.pdf"&gt;^rfid-backdoor&lt;/a&gt; was found that makes it much easier to obtain the data from the RFID tags. A script is included in the proxmark3 software since v4.18994 (nicknamed "Backdoor"), which allows us to utilize this backdoor. Before this script was implemented, the tag had to be sniffed by placing the spool in the AMS and sniffing the packets transferred between the tag and the AMS.&lt;/p&gt; 
&lt;p&gt;Place your reader on the tag, start proxmark3 (run &lt;code&gt;pm3&lt;/code&gt;) and run the following command:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;script run fm11rf08s_recovery&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;This script takes about 15-20 minutes to complete. Once it has finished, you will receive a binary key file and a dump.&lt;/p&gt; 
&lt;p&gt;To visualize the data on the tag, run the following:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;script run fm11rf08s_full -b&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Sniffing the tag data with a Proxmark3 (legacy method)&lt;/h3&gt; 
&lt;p&gt;Before the above methods were developed, tag data had to be obtained by sniffing the data between the RFID tag and the AMS using a Proxmark3-compatible device.&lt;/p&gt; 
&lt;p&gt;To read how to obtain the tag data using the legacy sniffing method, see the &lt;a href="https://raw.githubusercontent.com/Bambu-Research-Group/RFID-Tag-Guide/main/TagSniffing.md"&gt;TagSniffing.md&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Tag Documentation&lt;/h2&gt; 
&lt;p&gt;For a description of the blocks of a Bambu Lab RFID tag, see &lt;a href="https://raw.githubusercontent.com/Bambu-Research-Group/RFID-Tag-Guide/main/BambuLabRfid.md"&gt;BambuLabRfid.md&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For a description of the blocks of a Creality RFID tag, see &lt;a href="https://raw.githubusercontent.com/Bambu-Research-Group/RFID-Tag-Guide/main/CrealityRfid.md"&gt;CrealityRfid.md&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;An open-source standard proposal, OpenTag3D, was previously being incubated in this repository. It has since moved to its own repository. For a description of the standard, see &lt;a href="https://opentag3d.info/"&gt;https://opentag3d.info/&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;How do RFID tags work?&lt;/h2&gt; 
&lt;p&gt;Here's a high-level summary of how everything works:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Bambu Lab printers use MiFare 13.56MHZ RFID tags 
  &lt;ul&gt; 
   &lt;li&gt;These tags contain a unique ID that is not encrypted (called the UID)&lt;/li&gt; 
   &lt;li&gt;In most cases UID is fixed (not-changable). Some "hackable" RFID tags allow you to set the UID to anything you want&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Blocks (Encrypted) 
  &lt;ul&gt; 
   &lt;li&gt;MiFare tags also contain "Blocks" of data. Each block contains info about the spool, such as Material, Color, Manufacturing Date, etc.&lt;/li&gt; 
   &lt;li&gt;The blocks are encrypted, meaning that you need to have a KEY to decipher them&lt;/li&gt; 
   &lt;li&gt;Each block is encrypted with a different key&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Encryption Keys 
  &lt;ul&gt; 
   &lt;li&gt;Keys are unique to each RFID tag. Even if you discover the key for one tag, that doesn't mean you can use that same key to unlock a different tag.&lt;/li&gt; 
   &lt;li&gt;As of 11/19/24, keys can be derived from the UID. After reading the UID from the tag, the KDF (key derivation function) can be used to derive the 16 keys.&lt;/li&gt; 
   &lt;li&gt;(Outdated, sniffing is no longer required now that the KDF is known) Keys can be sniffed by using a device (such as a ProxMark 3) to listen in on the communication between the AMS and the rfid tag.&lt;/li&gt; 
   &lt;li&gt;Once the keys have been sniffed, they can be saved and used to read the contents of the tag directly (without an AMS). (Reminder, the saved keys will ONLY work for the tag they were sniffed from)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;RSA Signature 
  &lt;ul&gt; 
   &lt;li&gt;One of the blocks contains a 2048-bit RSA Signature&lt;/li&gt; 
   &lt;li&gt;RSA signatures are a way to digitally sign / certify authenticity of content, and they are effectively un-breakable (this is how things like cryptocurrency remain secure)&lt;/li&gt; 
   &lt;li&gt;RSA signatures encompass all of the data of the RFID tag. Changing a single byte somewhere else in the tag would require a completely different signature to be considered genuine&lt;/li&gt; 
   &lt;li&gt;Bambu printers check the content of the tag and then check if the signature is valid. If the signature is invalid, it rejects the tag&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Cloning Tags 
  &lt;ul&gt; 
   &lt;li&gt;Even though there is a signature, a tag can be cloned&lt;/li&gt; 
   &lt;li&gt;To clone a tag, it must have the same UID, identical content from the data blocks, and the identical RSA signature&lt;/li&gt; 
   &lt;li&gt;Changing even one byte will cause the signature to be invalid, and the tag will be rejected&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Custom Tags 
  &lt;ul&gt; 
   &lt;li&gt;This is very unlikely to happen, mostly due to the RSA signature. Only Bambu Lab has their "Private Key" which is used to digitally sign these tags.&lt;/li&gt; 
   &lt;li&gt;To create a custom key, you need to know the following info: 
    &lt;ul&gt; 
     &lt;li&gt;RSA Signature Private Key. You'd have to get this from bambu, good luck&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Since Bambu Lab will likely not remove the signature requirement, you would need custom AMS firmware to read tags and ignore the signature&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Compatible RFID tags - By generation&lt;/h2&gt; 
&lt;p&gt;There are tags known as "Magic Tags" which allow functionality that's not part of the classic MIFARE spec. One example is that most Magic Tags allow the UID to be changed, which is normally read-only on MIFARE tags. Magic tags are often refered to by their "generation", eg "Magic Gen 1". Each newer generation increases the functionality, but tends to also be more expensive)&lt;/p&gt; 
&lt;p&gt;Gen 1 --&amp;gt; &lt;strong&gt;Not compatible&lt;/strong&gt;(due to AMS checking if tag is unlockable with command 0x40)&lt;/p&gt; 
&lt;p&gt;Gen 2 --&amp;gt; &lt;strong&gt;Works&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Gen 2 OTW --&amp;gt; &lt;strong&gt;Not tested&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Gen 3 --&amp;gt; &lt;strong&gt;Not tested&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Gen 4 --&amp;gt; &lt;strong&gt;Not tested&lt;/strong&gt;(The best option but pricey and hard to source in small chip formfactor)&lt;/p&gt; 
&lt;p&gt;FUID --&amp;gt; &lt;strong&gt;Works&lt;/strong&gt; "Fused UID" aka "write-once UID". Once a UID is written, it cannot be changed&lt;/p&gt; 
&lt;h2&gt;Reverse engineering RFID Board&lt;/h2&gt; 
&lt;p&gt;For ease of debugging and lowering the cost of failures, the RFID board is reverse-engineered. You can find complete production-ready gerber files and a bill of materials in &lt;a href="https://raw.githubusercontent.com/Bambu-Research-Group/RFID-Tag-Guide/main/rfid-board"&gt;rfid-board&lt;/a&gt; folder.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>sherlock-project/sherlock</title>
      <link>https://github.com/sherlock-project/sherlock</link>
      <description>&lt;p&gt;Hunt down social media accounts by username across social networks&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;br /&gt; &lt;a href="https://sherlock-project.github.io/" target="_blank"&gt;&lt;img src="https://raw.githubusercontent.com/sherlock-project/sherlock/master/images/sherlock-logo.png" alt="sherlock" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;span&gt;Hunt down social media accounts by username across &lt;a href="https://sherlockproject.xyz/sites"&gt;400+ social networks&lt;/a&gt;&lt;/span&gt; &lt;br /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://sherlockproject.xyz/installation"&gt;Installation&lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;‚Ä¢&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;a href="https://sherlockproject.xyz/usage"&gt;Usage&lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;‚Ä¢&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;a href="https://sherlockproject.xyz/contribute"&gt;Contributing&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;img width="70%" height="70%" src="https://raw.githubusercontent.com/sherlock-project/sherlock/master/images/demo.png" alt="demo" /&gt; &lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING]&lt;br /&gt; Packages for ParrotOS and Ubuntu 24.04, maintained by a third party, appear to be &lt;strong&gt;broken&lt;/strong&gt;.&lt;br /&gt; Users of these systems should defer to pipx/pip or Docker.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Method&lt;/th&gt; 
   &lt;th&gt;Notes&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;pipx install sherlock-project&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;pip&lt;/code&gt; may be used in place of &lt;code&gt;pipx&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;docker run -it --rm sherlock/sherlock&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;dnf install sherlock-project&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Community-maintained packages are available for Debian (&amp;gt;= 13), Ubuntu (&amp;gt;= 22.10), Homebrew, Kali, and BlackArch. These packages are not directly supported or maintained by the Sherlock Project.&lt;/p&gt; 
&lt;p&gt;See all alternative installation methods &lt;a href="https://sherlockproject.xyz/installation"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;General usage&lt;/h2&gt; 
&lt;p&gt;To search for only one user:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sherlock user123
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To search for more than one user:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sherlock user1 user2 user3
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Accounts found will be stored in an individual text file with the corresponding username (e.g &lt;code&gt;user123.txt&lt;/code&gt;).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;$ sherlock --help
usage: sherlock [-h] [--version] [--verbose] [--folderoutput FOLDEROUTPUT]
                [--output OUTPUT] [--tor] [--unique-tor] [--csv] [--xlsx]
                [--site SITE_NAME] [--proxy PROXY_URL] [--json JSON_FILE]
                [--timeout TIMEOUT] [--print-all] [--print-found] [--no-color]
                [--browse] [--local] [--nsfw]
                USERNAMES [USERNAMES ...]

Sherlock: Find Usernames Across Social Networks (Version 0.14.3)

positional arguments:
  USERNAMES             One or more usernames to check with social networks.
                        Check similar usernames using {?} (replace to '_', '-', '.').

optional arguments:
  -h, --help            show this help message and exit
  --version             Display version information and dependencies.
  --verbose, -v, -d, --debug
                        Display extra debugging information and metrics.
  --folderoutput FOLDEROUTPUT, -fo FOLDEROUTPUT
                        If using multiple usernames, the output of the results will be
                        saved to this folder.
  --output OUTPUT, -o OUTPUT
                        If using single username, the output of the result will be saved
                        to this file.
  --tor, -t             Make requests over Tor; increases runtime; requires Tor to be
                        installed and in system path.
  --unique-tor, -u      Make requests over Tor with new Tor circuit after each request;
                        increases runtime; requires Tor to be installed and in system
                        path.
  --csv                 Create Comma-Separated Values (CSV) File.
  --xlsx                Create the standard file for the modern Microsoft Excel
                        spreadsheet (xlsx).
  --site SITE_NAME      Limit analysis to just the listed sites. Add multiple options to
                        specify more than one site.
  --proxy PROXY_URL, -p PROXY_URL
                        Make requests over a proxy. e.g. socks5://127.0.0.1:1080
  --json JSON_FILE, -j JSON_FILE
                        Load data from a JSON file or an online, valid, JSON file.
  --timeout TIMEOUT     Time (in seconds) to wait for response to requests (Default: 60)
  --print-all           Output sites where the username was not found.
  --print-found         Output sites where the username was found.
  --no-color            Don't color terminal output
  --browse, -b          Browse to all results on default browser.
  --local, -l           Force the use of the local data.json file.
  --nsfw                Include checking of NSFW sites from default list.
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Apify Actor Usage &lt;a href="https://apify.com/netmilk/sherlock?fpr=sherlock"&gt;&lt;img src="https://apify.com/actor-badge?actor=netmilk/sherlock" alt="Sherlock Actor" /&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://apify.com/netmilk/sherlock?fpr=sherlock"&gt;&lt;img src="https://apify.com/ext/run-on-apify.png" alt="Run Sherlock Actor on Apify" width="176" height="39" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;You can run Sherlock in the cloud without installation using the &lt;a href="https://apify.com/netmilk/sherlock?fpr=sherlock"&gt;Sherlock Actor&lt;/a&gt; on &lt;a href="https://apify.com?fpr=sherlock"&gt;Apify&lt;/a&gt; free of charge.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;$ echo '{"usernames":["user123"]}' | apify call -so netmilk/sherlock
[{
  "username": "user123",
  "links": [
    "https://www.1337x.to/user/user123/",
    ...
  ]
}]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Read more about the &lt;a href="https://raw.githubusercontent.com/sherlock-project/sherlock/.actor/README.md"&gt;Sherlock Actor&lt;/a&gt;, including how to use it programmatically via the Apify &lt;a href="https://apify.com/netmilk/sherlock/api?fpr=sherlock"&gt;API&lt;/a&gt;, &lt;a href="https://docs.apify.com/cli/?fpr=sherlock"&gt;CLI&lt;/a&gt; and &lt;a href="https://docs.apify.com/sdk?fpr=sherlock"&gt;JS/TS and Python SDKs&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Credits&lt;/h2&gt; 
&lt;p&gt;Thank you to everyone who has contributed to Sherlock! ‚ù§Ô∏è&lt;/p&gt; 
&lt;a href="https://github.com/sherlock-project/sherlock/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?&amp;amp;columns=25&amp;amp;max=10000&amp;amp;&amp;amp;repo=sherlock-project/sherlock" alt="contributors" /&gt; &lt;/a&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;picture&gt; 
 &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=sherlock-project/sherlock&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
 &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=sherlock-project/sherlock&amp;amp;type=Date" /&gt; 
 &lt;img alt="Sherlock Project Star History Chart" src="https://api.star-history.com/svg?repos=sherlock-project/sherlock&amp;amp;type=Date" /&gt; 
&lt;/picture&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;MIT ¬© Sherlock Project&lt;br /&gt; Original Creator - &lt;a href="https://github.com/sdushantha"&gt;Siddharth Dushantha&lt;/a&gt;&lt;/p&gt; 
&lt;!-- Reference Links --&gt;</description>
    </item>
    
    <item>
      <title>QwenLM/Qwen-Image</title>
      <link>https://github.com/QwenLM/Qwen-Image</link>
      <description>&lt;p&gt;Qwen-Image is a powerful image generation foundation model capable of complex text rendering and precise image editing.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/qwen_image_logo.png" width="400" /&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p align="center"&gt;&amp;nbsp;&amp;nbsp;üíú &lt;a href="https://chat.qwen.ai/"&gt;Qwen Chat&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ü§ó &lt;a href="https://huggingface.co/Qwen/Qwen-Image"&gt;HuggingFace(T2I)&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ü§ó &lt;a href="https://huggingface.co/Qwen/Qwen-Image-Edit-2511"&gt;HuggingFace(Edit)&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ü§ñ &lt;a href="https://modelscope.cn/models/Qwen/Qwen-Image"&gt;ModelScope-T2I&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ü§ñ &lt;a href="https://modelscope.cn/models/Qwen/Qwen-Image-Edit-2511"&gt;ModelScope-Edit&lt;/a&gt;&amp;nbsp;&amp;nbsp;| &amp;nbsp;&amp;nbsp; üìë &lt;a href="https://arxiv.org/abs/2508.02324"&gt;Tech Report&lt;/a&gt; &amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp; üìë &lt;a href="https://qwenlm.github.io/blog/qwen-image/"&gt;Blog(T2I)&lt;/a&gt; &amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp; üìë &lt;a href="https://qwenlm.github.io/blog/qwen-image-edit-2511/"&gt;Blog(Edit)&lt;/a&gt; &amp;nbsp;&amp;nbsp; &lt;br /&gt; üñ•Ô∏è &lt;a href="https://huggingface.co/spaces/Qwen/Qwen-Image"&gt;T2I Demo&lt;/a&gt;&amp;nbsp;&amp;nbsp; | üñ•Ô∏è &lt;a href="https://huggingface.co/spaces/Qwen/Qwen-Image-Edit-2511"&gt;Edit Demo&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;üí¨ &lt;a href="https://github.com/QwenLM/Qwen-Image/raw/main/assets/wechat.png"&gt;WeChat (ÂæÆ‰ø°)&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ü´® &lt;a href="https://discord.gg/CV4E9rpNSD"&gt;Discord&lt;/a&gt;&amp;nbsp;&amp;nbsp; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/merge3.jpg" width="1024" /&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;We are thrilled to release &lt;strong&gt;Qwen-Image&lt;/strong&gt;, a 20B MMDiT image foundation model that achieves significant advances in &lt;strong&gt;complex text rendering&lt;/strong&gt; and &lt;strong&gt;precise image editing&lt;/strong&gt;. Experiments show strong general capabilities in both image generation and editing, with exceptional performance in text rendering, especially for Chinese.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/bench.png#center" alt="" /&gt;&lt;/p&gt; 
&lt;h2&gt;News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;2025.12.31: We released Qwen-Image-2512 weights! Check at &lt;a href="https://huggingface.co/Qwen/Qwen-Image-2512"&gt;Huggingface&lt;/a&gt; and &lt;a href="https://modelscope.cn/models/Qwen/Qwen-Image-2512"&gt;ModelScope&lt;/a&gt;!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.12.31: We released Qwen-Image-2512! Check our &lt;a href="https://qwen.ai/blog?id=qwen-image-2512"&gt;Blog&lt;/a&gt; for more details! üöÄ Our December upgrade to Qwen-Image, just in time for the New Year.&lt;/p&gt; &lt;p&gt;‚ú® What‚Äôs new: ‚Ä¢ More realistic humans ‚Äî dramatically reduced ‚ÄúAI look,‚Äù richer facial &amp;amp; age details ‚Ä¢ Finer natural textures ‚Äî sharper landscapes, water, fur, and materials ‚Ä¢ Stronger text rendering ‚Äî better layout, higher accuracy in text‚Äìimage composition&lt;/p&gt; &lt;p&gt;üèÜ Tested in 10,000+ blind rounds on AI Arena, Qwen-Image-2512 ranks as the strongest open-source image model, while staying competitive with closed-source systems. &lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/image2512/arena.png#center" alt="" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.12.31: &lt;a href="https://github.com/ModelTC/Qwen-Image-Lightning"&gt;Qwen-Image-Lightning&lt;/a&gt;, developed by &lt;a href="https://github.com/ModelTC/LightX2V"&gt;Lightx2v&lt;/a&gt;, provides &lt;a href="https://huggingface.co/lightx2v/Qwen-Image-2512-Lightning"&gt;Day 0 acceleration support for Qwen-Image-2512&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.12.31:vLLM-Omni supports high performance Qwen-Image-2512 inference from Day-0, with long sequence parallelism, cache acceleration and fast kernels, please check &lt;a href="https://github.com/vllm-project/vllm-omni/tree/main/examples/offline_inference/text_to_image"&gt;here&lt;/a&gt; for details.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.12.23: We released Qwen-Image-Edit-2511 weights! Check at &lt;a href="https://huggingface.co/Qwen/Qwen-Image-Edit-2511"&gt;Huggingface&lt;/a&gt; and &lt;a href="https://modelscope.cn/models/Qwen/Qwen-Image-Edit-2511"&gt;ModelScope&lt;/a&gt;!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.12.23: We released Qwen-Image-Edit-2511! Check our &lt;a href="https://qwen.ai/blog?id=qwen-image-edit-2511"&gt;Blog&lt;/a&gt; for more details!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.12.23: &lt;strong&gt;&lt;a href="https://github.com/ModelTC/LightX2V/"&gt;LightX2V&lt;/a&gt;&lt;/strong&gt; delivers Day 0 acceleration for Qwen-Image-Edit-2511, with native support for a wide range of hardware, including &lt;strong&gt;NVIDIA, Hygon, Metax, Ascend, and Cambricon&lt;/strong&gt;. By combining &lt;strong&gt;&lt;a href="https://github.com/ModelTC/Qwen-Image-Lightning"&gt;diffusion distillation&lt;/a&gt;&lt;/strong&gt; with cutting-edge inference optimizations, LightX2V achieves a &lt;strong&gt;25x reduction in DiT NFEs&lt;/strong&gt; and &lt;strong&gt;an order-of-magnitude 42.55x overall speedup&lt;/strong&gt;, enabling real-time image editing across diverse AI accelerators.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.12.23: &lt;strong&gt;vLLM-Omni&lt;/strong&gt; supports high performance &lt;code&gt;Qwen-Image-Edit-2511&lt;/code&gt;, &lt;code&gt;Qwen-Image-Layered&lt;/code&gt; inference from Day-0, with long sequence parallelism, cache acceleration and fast kernels, please check &lt;a href="https://github.com/vllm-project/vllm-omni/tree/main/examples/offline_inference/image_to_image"&gt;here&lt;/a&gt; for details.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.12.23: &lt;strong&gt;SGLang-Diffusion&lt;/strong&gt; provides day-0 support for Qwen-Image models. To play with &lt;code&gt;Qwen-Image-Edit-2511&lt;/code&gt; in SGlang, please check community supports section for details.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.12.19: We released Qwen-Image-Layered weights! Check at &lt;a href="https://huggingface.co/Qwen/Qwen-Image-Layered"&gt;Huggingface&lt;/a&gt; and &lt;a href="https://modelscope.cn/models/Qwen/Qwen-Image-Layered"&gt;ModelScope&lt;/a&gt;!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.12.19: We released Qwen-Image-Layered! Check our &lt;a href="https://qwenlm.github.io/blog/qwen-image-layered"&gt;Blog&lt;/a&gt; for more details!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.12.18: We released our &lt;a href="https://arxiv.org/abs/2512.15603"&gt;Research Paper&lt;/a&gt; on Arxiv!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.11.11: &lt;strong&gt;&lt;a href="https://t2i-corebench.github.io/"&gt;T2I-CoreBench&lt;/a&gt;&lt;/strong&gt; offers a comprehensive and complex evaluation of T2I models in real-world scenarios. On this benchmark, Qwen-Image achieves state-of-the-art performance under real-world complexities in both composition and reasoning T2I tasks, surpassing other open-source models and showing comparable results to closed-source ones.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.11.07: LeMiCa is a diffusion model inference acceleration solution developed by China Unicom Data Science and Artificial Intelligence Research Institute. By leveraging cache-based techniques and global denoising path optimization, LeMiCa provides efficient inference support for Qwen-Image, achieving nearly 3x lossless acceleration while maintaining visual consistency and quality. For more details, please visit the homepage: &lt;a href="https://unicomai.github.io/LeMiCa/"&gt;https://unicomai.github.io/LeMiCa/&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.09.22: This September, we are pleased to introduce Qwen-Image-Edit-2509, the monthly iteration of Qwen-Image-Edit. To experience the latest model, please visit &lt;a href="https://qwen.ai"&gt;Qwen Chat&lt;/a&gt; and select the "Image Editing" feature. Compared with Qwen-Image-Edit released in August, the main improvements of Qwen-Image-Edit-2509 include:&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.08.19: We have observed performance misalignments of Qwen-Image-Edit. To ensure optimal results, please update to the latest diffusers commit. Improvements are expected, especially in identity preservation and instruction following.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.08.18: We‚Äôre excited to announce the open-sourcing of Qwen-Image-Edit! üéâ Try it out in your local environment with the quick start guide below, or head over to &lt;a href="https://chat.qwen.ai/"&gt;Qwen Chat&lt;/a&gt; or &lt;a href="https://huggingface.co/spaces/Qwen/Qwen-Image-Edit"&gt;Huggingface Demo&lt;/a&gt; to experience the online demo right away! If you enjoy our work, please show your support by giving our repository a star. Your encouragement means a lot to us!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.08.09: Qwen-Image now supports a variety of LoRA models, such as MajicBeauty LoRA, enabling the generation of highly realistic beauty images. Check out the available weights on &lt;a href="https://modelscope.cn/models/merjic/majicbeauty-qwen1/summary"&gt;ModelScope&lt;/a&gt;. &lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/magicbeauty.png#center" alt="" /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.08.05: Qwen-Image is now natively supported in ComfyUI, see &lt;a href="https://blog.comfy.org/p/qwen-image-in-comfyui-new-era-of"&gt;Qwen-Image in ComfyUI: New Era of Text Generation in Images!&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.08.05: Qwen-Image is now on Qwen Chat. Click &lt;a href="https://chat.qwen.ai/"&gt;Qwen Chat&lt;/a&gt; and choose "Image Generation".&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.08.05: We released our &lt;a href="https://arxiv.org/abs/2508.02324"&gt;Technical Report&lt;/a&gt; on Arxiv!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.08.04: We released Qwen-Image weights! Check at &lt;a href="https://huggingface.co/Qwen/Qwen-Image"&gt;Huggingface&lt;/a&gt; and &lt;a href="https://modelscope.cn/models/Qwen/Qwen-Image"&gt;ModelScope&lt;/a&gt;!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;2025.08.04: We released Qwen-Image! Check our &lt;a href="https://qwenlm.github.io/blog/qwen-image"&gt;Blog&lt;/a&gt; for more details!&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Due to heavy traffic, if you'd like to experience our demo online, we also recommend visiting DashScope, WaveSpeed, and LibLib. Please find the links below in the community support.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Make sure your transformers&amp;gt;=4.51.3 (Supporting Qwen2.5-VL)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install the latest version of diffusers&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code&gt;pip install git+https://github.com/huggingface/diffusers
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Qwen-Image-2512 (for Text to Image generation, better character realism/texture quality)&lt;/h3&gt; 
&lt;p&gt;We recommand use the latest prompt enhancing tools for Qwen-Image-2512, please check &lt;code&gt;src/examples/tools/prompt_utils_2512.py&lt;/code&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from diffusers import QwenImagePipeline
import torch
# Load the pipeline
if torch.cuda.is_available():
    torch_dtype = torch.bfloat16
    device = "cuda"
else:
    torch_dtype = torch.float32
    device = "cpu"

pipe = QwenImagePipeline.from_pretrained("Qwen/Qwen-Image-2512", torch_dtype=torch_dtype).to(device)

# Generate image
prompt = '''A 20-year-old East Asian girl with delicate, charming features and large, bright brown eyes‚Äîexpressive and lively, with a cheerful or subtly smiling expression. Her naturally wavy long hair is either loose or tied in twin ponytails. She has fair skin and light makeup accentuating her youthful freshness. She wears a modern, cute dress or relaxed outfit in bright, soft colors‚Äîlightweight fabric, minimalist cut. She stands indoors at an anime convention, surrounded by banners, posters, or stalls. Lighting is typical indoor illumination‚Äîno staged lighting‚Äîand the image resembles a casual iPhone snapshot: unpretentious composition, yet brimming with vivid, fresh, youthful charm.'''

negative_prompt = "‰ΩéÂàÜËæ®ÁéáÔºå‰ΩéÁîªË¥®ÔºåËÇ¢‰ΩìÁï∏ÂΩ¢ÔºåÊâãÊåáÁï∏ÂΩ¢ÔºåÁîªÈù¢ËøáÈ•±ÂíåÔºåËú°ÂÉèÊÑüÔºå‰∫∫ËÑ∏Êó†ÁªÜËäÇÔºåËøáÂ∫¶ÂÖâÊªëÔºåÁîªÈù¢ÂÖ∑ÊúâAIÊÑü„ÄÇÊûÑÂõæÊ∑∑‰π±„ÄÇÊñáÂ≠óÊ®°Á≥äÔºåÊâ≠Êõ≤„ÄÇ"


# Generate with different aspect ratios
aspect_ratios = {
    "1:1": (1328, 1328),
    "16:9": (1664, 928),
    "9:16": (928, 1664),
    "4:3": (1472, 1104),
    "3:4": (1104, 1472),
    "3:2": (1584, 1056),
    "2:3": (1056, 1584),
}

width, height = aspect_ratios["16:9"]

image = pipe(
    prompt=prompt,
    negative_prompt=negative_prompt,
    width=width,
    height=height,
    num_inference_steps=50,
    true_cfg_scale=4.0,
    generator=torch.Generator(device="cuda").manual_seed(42)
).images[0]

image.save("example.png")

&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Qwen-Image-Edit-2511 (for Image Editing, Multiple Image Support and Improved Consistency)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import os
import torch
from PIL import Image
from diffusers import QwenImageEditPlusPipeline
from io import BytesIO
import requests

pipeline = QwenImageEditPlusPipeline.from_pretrained("Qwen/Qwen-Image-Edit-2511", torch_dtype=torch.bfloat16)
print("pipeline loaded")

pipeline.to('cuda')
pipeline.set_progress_bar_config(disable=None)
image1 = Image.open(BytesIO(requests.get("https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen-Image/edit2511/edit2511input.png").content))
prompt = "Ëøô‰∏™Â•≥ÁîüÁúãÁùÄÈù¢ÂâçÁöÑÁîµËßÜÂ±èÂπïÔºåÂ±èÂπï‰∏äÈù¢ÂÜôÁùÄ‚ÄúÈòøÈáåÂ∑¥Â∑¥‚Äù"
inputs = {
    "image": [image1],
    "prompt": prompt,
    "generator": torch.manual_seed(0),
    "true_cfg_scale": 4.0,
    "negative_prompt": " ",
    "num_inference_steps": 40,
    "guidance_scale": 1.0,
    "num_images_per_prompt": 1,
}
with torch.inference_mode():
    output = pipeline(**inputs)
    output_image = output.images[0]
    output_image.save("output_image_edit_2511.png")
    print("image saved at", os.path.abspath("output_image_edit_2511.png"))
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt; Previous Version &lt;/summary&gt; 
 &lt;h3&gt;Qwen-Image (for Text-to-Image)&lt;/h3&gt; 
 &lt;p&gt;The following contains a code snippet illustrating how to use the model to generate images based on text prompts:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from diffusers import DiffusionPipeline
import torch

model_name = "Qwen/Qwen-Image"

# Load the pipeline
if torch.cuda.is_available():
    torch_dtype = torch.bfloat16
    device = "cuda"
else:
    torch_dtype = torch.float32
    device = "cpu"

pipe = DiffusionPipeline.from_pretrained(model_name, torch_dtype=torch_dtype).to(device)

positive_magic = {
    "en": ", Ultra HD, 4K, cinematic composition.", # for english prompt
    "zh": ", Ë∂ÖÊ∏ÖÔºå4KÔºåÁîµÂΩ±Á∫ßÊûÑÂõæ." # for chinese prompt
}

# Generate image
prompt = '''A coffee shop entrance features a chalkboard sign reading "Qwen Coffee üòä $2 per cup," with a neon light beside it displaying "ÈÄö‰πâÂçÉÈóÆ". Next to it hangs a poster showing a beautiful Chinese woman, and beneath the poster is written "œÄ‚âà3.1415926-53589793-23846264-33832795-02384197".'''

negative_prompt = " " # Recommended if you don't use a negative prompt.


# Generate with different aspect ratios
aspect_ratios = {
    "1:1": (1328, 1328),
    "16:9": (1664, 928),
    "9:16": (928, 1664),
    "4:3": (1472, 1104),
    "3:4": (1104, 1472),
    "3:2": (1584, 1056),
    "2:3": (1056, 1584),
}

width, height = aspect_ratios["16:9"]

image = pipe(
    prompt=prompt + positive_magic["en"],
    negative_prompt=negative_prompt,
    width=width,
    height=height,
    num_inference_steps=50,
    true_cfg_scale=4.0,
    generator=torch.Generator(device="cuda").manual_seed(42)
).images[0]

image.save("example.png")
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;Qwen-Image-Edit (for Image Editing, Only Support Single Image Input)&lt;/h3&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;[!NOTE] Qwen-Image-Edit-2509 has better consistency than Qwen-Image-Edit; it is recommended to use Qwen-Image-Edit-2509 directlyÔºåfor both single image input and multiple image inputs.&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;import os
from PIL import Image
import torch

from diffusers import QwenImageEditPipeline

pipeline = QwenImageEditPipeline.from_pretrained("Qwen/Qwen-Image-Edit")
print("pipeline loaded")
pipeline.to(torch.bfloat16)
pipeline.to("cuda")
pipeline.set_progress_bar_config(disable=None)

image = Image.open("./input.png").convert("RGB")
prompt = "Change the rabbit's color to purple, with a flash light background."


inputs = {
    "image": image,
    "prompt": prompt,
    "generator": torch.manual_seed(0),
    "true_cfg_scale": 4.0,
    "negative_prompt": " ",
    "num_inference_steps": 50,
}

with torch.inference_mode():
    output = pipeline(**inputs)
    output_image = output.images[0]
    output_image.save("output_image_edit.png")
    print("image saved at", os.path.abspath("output_image_edit.png"))
&lt;/code&gt;&lt;/pre&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;[!NOTE] We have observed that editing results may become unstable if prompt rewriting is not used. Therefore, we strongly recommend applying prompt rewriting to improve the stability of editing tasks. For reference, please see our official &lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen-Image/main/src/examples/tools/prompt_utils.py"&gt;demo script&lt;/a&gt; or Advanced Usage below, which includes example system prompts. Qwen-Image-Edit is actively evolving with ongoing development. Stay tuned for future enhancements!&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;h3&gt;Qwen-Image-Edit-2509 (for Image Editing, Multiple Image Support and Improved Consistency)&lt;/h3&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;import os
import torch
from PIL import Image
from diffusers import QwenImageEditPlusPipeline
from io import BytesIO
import requests

pipeline = QwenImageEditPlusPipeline.from_pretrained("Qwen/Qwen-Image-Edit-2509", torch_dtype=torch.bfloat16)
print("pipeline loaded")

pipeline.to('cuda')
pipeline.set_progress_bar_config(disable=None)
image1 = Image.open(BytesIO(requests.get("https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/edit2509_1.jpg").content))
image2 = Image.open(BytesIO(requests.get("https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/edit2509_2.jpg").content))
prompt = "The magician bear is on the left, the alchemist bear is on the right, facing each other in the central park square."
inputs = {
    "image": [image1, image2],
    "prompt": prompt,
    "generator": torch.manual_seed(0),
    "true_cfg_scale": 4.0,
    "negative_prompt": " ",
    "num_inference_steps": 40,
    "guidance_scale": 1.0,
    "num_images_per_prompt": 1,
}
with torch.inference_mode():
    output = pipeline(**inputs)
    output_image = output.images[0]
    output_image.save("output_image_edit_plus.png")
    print("image saved at", os.path.abspath("output_image_edit_plus.png"))
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;Advanced Usage&lt;/h3&gt; 
&lt;h4&gt;Prompt Enhance for Text-to-Image&lt;/h4&gt; 
&lt;p&gt;For enhanced prompt optimization and multi-language support, we recommend using our official Prompt Enhancement Tool powered by Qwen-Plus .&lt;/p&gt; 
&lt;p&gt;You can integrate it directly into your code:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from tools.prompt_utils import rewrite
prompt = rewrite(prompt)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, run the example script from the command line:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd src
DASHSCOPE_API_KEY=sk-xxxxxxxxxxxxxxxxxxxx python examples/generate_w_prompt_enhance.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Prompt Enhance for Image Edit&lt;/h4&gt; 
&lt;p&gt;For enhanced stability, we recommend using our official Prompt Enhancement Tool powered by Qwen-VL-Max.&lt;/p&gt; 
&lt;p&gt;You can integrate it directly into your code:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from tools.prompt_utils import polish_edit_prompt
prompt = polish_edit_prompt(prompt, pil_image)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Deploy Qwen-Image&lt;/h2&gt; 
&lt;p&gt;Qwen-Image supports Multi-GPU API Server for local deployment:&lt;/p&gt; 
&lt;h3&gt;Multi-GPU API Server Pipeline &amp;amp; Usage&lt;/h3&gt; 
&lt;p&gt;The Multi-GPU API Server will start a Gradio-based web interface with:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Multi-GPU parallel processing&lt;/li&gt; 
 &lt;li&gt;Queue management for high concurrency&lt;/li&gt; 
 &lt;li&gt;Automatic prompt optimization&lt;/li&gt; 
 &lt;li&gt;Support for multiple aspect ratios&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Configuration via environment variables:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export NUM_GPUS_TO_USE=4          # Number of GPUs to use
export TASK_QUEUE_SIZE=100        # Task queue size
export TASK_TIMEOUT=300           # Task timeout in seconds
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Start the gradio demo server, api key for prompt enhance
cd src
DASHSCOPE_API_KEY=sk-xxxxxxxxxxxxxxxxx python examples/demo.py 
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Showcase&lt;/h2&gt; 
&lt;p&gt;For previous showcases, click the following links:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen-Image/main/Qwen-Image.md"&gt;Qwen-Image&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen-Image/main/Qwen-Image-Edit.md"&gt;Qwen-Image-Edit&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen-Image/main/Qwen-Image-Edit-2509.md"&gt;Qwen-Image-Edit-2509&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Showcase of Qwen-Image-2512&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Enhanced Huamn Realism&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;In Qwen-Image-2512, human depiction has been substantially refined. Compared to the August release, Qwen-Image-2512 adds significantly richer facial details and better environmental context. For example:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;A Chinese female college student, around 20 years old, with a very short haircut that conveys a gentle, artistic vibe. Her hair naturally falls to partially cover her cheeks, projecting a tomboyish yet charming demeanor. She has cool-toned fair skin and delicate features, with a slightly shy yet subtly confident expression‚Äîher mouth crooked in a playful, youthful smirk. She wears an off-shoulder top, revealing one shoulder, with a well-proportioned figure. The image is framed as a close-up selfie: she dominates the foreground, while the background clearly shows her dormitory‚Äîa neatly made bed with white linens on the top bunk, a tidy study desk with organized stationery, and wooden cabinets and drawers. The photo is captured on a smartphone under soft, even ambient lighting, with natural tones, high clarity, and a bright, lively atmosphere full of youthful, everyday energy.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/image2512/%E5%B9%BB%E7%81%AF%E7%89%871.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;For the same prompt, Qwen-Image-2512 yields notably more lifelike facial features, and background objects‚Äîe.g., the desk, stationery, and bedding‚Äîare rendered with significantly greater clarity than in Qwen-Image.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;A 20-year-old East Asian girl with delicate, charming features and large, bright brown eyes‚Äîexpressive and lively, with a cheerful or subtly smiling expression. Her naturally wavy long hair is either loose or tied in twin ponytails. She has fair skin and light makeup accentuating her youthful freshness. She wears a modern, cute dress or relaxed outfit in bright, soft colors‚Äîlightweight fabric, minimalist cut. She stands indoors at an anime convention, surrounded by banners, posters, or stalls. Lighting is typical indoor illumination‚Äîno staged lighting‚Äîand the image resembles a casual iPhone snapshot: unpretentious composition, yet brimming with vivid, fresh, youthful charm.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/image2512/%E5%B9%BB%E7%81%AF%E7%89%872.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;Here, hair strands serve as a key differentiator: Qwen-Image‚Äôs August version tends to blur them together, losing fine detail, whereas Qwen-Image-2512 renders individual strands with precision, resulting in a more natural and realistic appearance.&lt;/p&gt; 
&lt;p&gt;Another case:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;An East Asian teenage boy, aged 15‚Äì18, with soft, fluffy black short hair and refined facial contours. His large, warm brown eyes sparkle with energy. His fair skin and sunny, open smile convey an approachable, friendly demeanor‚Äîno makeup or blemishes. He wears a blue-and-white summer uniform shirt, slightly unbuttoned, made of thin breathable fabric, with black headphones hanging around his neck. His hands are in his pockets, body leaning slightly forward in a relaxed pose, as if engaged in conversation. Behind him lies a summer school playground: lush green grass and a red rubber track in the foreground, blurred school buildings in the distance, a clear blue sky with fluffy white clouds. The bright, airy lighting evokes a joyful, carefree adolescent atmosphere.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/image2512/%E5%B9%BB%E7%81%AF%E7%89%873.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;In this example, Qwen-Image-2512 better adheres to semantic instructions‚Äîfor instance, the prompt specifies ‚Äúbody leaning slightly forward,‚Äù and Qwen-Image-2512 accurately captures this posture, unlike its predecessor.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;An elderly Chinese couple in their 70s in a clean, organized home kitchen. The woman has a kind face and a warm smile, wearing a patterned apron; the man stands behind her, also smiling, as they both gaze at a steaming pot of buns on the stove. The kitchen is bright and tidy, exuding warmth and harmony. The scene is captured with a wide-angle lens to fully show the subjects and their surroundings.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/image2512/%E5%B9%BB%E7%81%AF%E7%89%874.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;This comparison starkly highlights the gap between the August and December models. The original Qwen-Image struggles to accurately render aged facial features (e.g., wrinkles), resulting in an artificial ‚ÄúAI look.‚Äù In contrast, Qwen-Image-2512 precisely captures age cues, dramatically boosting realism.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Finer Natural Detail&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Qwen-Image-2512‚Äôs enhanced detail rendering extends beyond humans‚Äîto landscapes, wildlife, and more. For instance:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;A turquoise river winds through a lush canyon. Thick moss and dense ferns blanket the rocky walls; multiple waterfalls cascade from above, enveloped in mist. At noon, sunlight filters through the dense canopy, dappling the river surface with shimmering light. The atmosphere is humid and fresh, pulsing with primal jungle vitality. No humans, text, or artificial traces present.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/image2512/%E5%B9%BB%E7%81%AF%E7%89%875.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;Side-by-side, Qwen-Image-2512 exhibits superior fidelity in water flow, foliage, and waterfall mist‚Äîand renders richer gradation in greens. Another example (wave rendering):&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;At dawn, a thin mist veils the sea. An ancient stone lighthouse stands at the cliff‚Äôs edge, its beacon faintly visible through the fog. Black rocks are pounded by waves, sending up bursts of white spray. The sky glows in soft blue-purple hues under cool, hazy light‚Äîevoking solitude and solemn grandeur.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/image2512/%E5%B9%BB%E7%81%AF%E7%89%876.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;Fur detail is another highlight‚Äîhere, a golden retriever portrait:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;An ultra-realistic close-up of a golden retriever outdoors under soft daylight. Hair is exquisitely detailed: strands distinct, color transitioning naturally from warm gold to light cream, light glinting delicately at the tips; a gentle breeze adds subtle volume. Undercoat is soft and dense; guard hairs are long and well-defined, with visible layering. Eyes are moist, expressive; nose is slightly damp with fine specular highlights. Background is softly blurred to emphasize the dog‚Äôs tangible texture and vivid expression.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/image2512/%E5%B9%BB%E7%81%AF%E7%89%877.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;Similarly, texture quality improves in depictions of rugged wildlife‚Äîfor example, a male argali sheep:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;A male argali stands atop a barren, rocky mountainside. Its coarse, dense grey-brown coat covers a powerful, muscular body. Most striking are its massive, thick, outward-spiraling horns‚Äîa symbol of wild strength. Its gaze is alert and sharp. The background reveals steep alpine terrain: jagged peaks, sparse low vegetation, and abundant sunlight‚Äîconveying the harsh yet majestic wilderness and the animal‚Äôs resilient vitality.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/image2512/%E5%B9%BB%E7%81%AF%E7%89%878.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Improved Text Rendering&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Qwen-Image-2512 further elevates text rendering‚Äîalready a strength of the original‚Äîby improving accuracy, layout, and multimodal integration.&lt;/p&gt; 
&lt;p&gt;For instance, this prompt requests a complete PPT slide illustrating Qwen-Image‚Äôs development roadmap (generation and editing tracks):&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ËøôÊòØ‰∏ÄÂº†Áé∞‰ª£È£éÊ†ºÁöÑÁßëÊäÄÊÑüÂπªÁÅØÁâáÔºåÊï¥‰ΩìÈááÁî®Ê∑±ËìùËâ≤Ê∏êÂèòËÉåÊôØ„ÄÇÊ†áÈ¢òÊòØ‚ÄúQwen-ImageÂèëÂ±ïÂéÜÁ®ã‚Äù„ÄÇ‰∏ãÊñπ‰∏ÄÊù°Ê∞¥Âπ≥Âª∂‰º∏ÁöÑÂèëÂÖâÊó∂Èó¥ËΩ¥ÔºåËΩ¥Á∫ø‰∏≠Èó¥ÂÜôÁùÄ‚ÄúÁîüÂõæË∑ØÁ∫ø‚Äù„ÄÇÁî±Â∑¶‰æßÊ∑°ËìùËâ≤Ê∏êÂèò‰∏∫Âè≥‰æßÊ∑±Á¥´Ëâ≤ÔºåÂπ∂‰ª•Á≤æËá¥ÁöÑÁÆ≠Â§¥Êî∂Â∞æ„ÄÇÊó∂Èó¥ËΩ¥‰∏äÊØè‰∏™ËäÇÁÇπÈÄöËøáËôöÁ∫øËøûÊé•Ëá≥‰∏ãÊñπÈÜíÁõÆÁöÑËìùËâ≤ÂúÜËßíÁü©ÂΩ¢Êó•ÊúüÊ†áÁ≠æÔºåÊ†áÁ≠æÂÜÖ‰∏∫Ê∏ÖÊô∞ÁôΩËâ≤Â≠ó‰ΩìÔºå‰ªéÂ∑¶ÂêëÂè≥‰æùÊ¨°ÂÜôÁùÄÔºö‚Äú2025Âπ¥5Êúà6Êó• Qwen-Image È°πÁõÆÂêØÂä®‚Äù‚Äú2025Âπ¥8Êúà4Êó• Qwen-Image ÂºÄÊ∫êÂèëÂ∏É‚Äù‚Äú2025Âπ¥12Êúà31Êó• Qwen-Image-2512 ÂºÄÊ∫êÂèëÂ∏É‚Äù ÔºàÂë®Âõ¥ÂÖâÊôïÊòæËëóÔºâÂú®‰∏ãÊñπ‰∏ÄÊù°Ê∞¥Âπ≥Âª∂‰º∏ÁöÑÂèëÂÖâÊó∂Èó¥ËΩ¥ÔºåËΩ¥Á∫ø‰∏≠Èó¥ÂÜôÁùÄ‚ÄúÁºñËæëË∑ØÁ∫ø‚Äù„ÄÇÁî±Â∑¶‰æßÊ∑°ËìùËâ≤Ê∏êÂèò‰∏∫Âè≥‰æßÊ∑±Á¥´Ëâ≤ÔºåÂπ∂‰ª•Á≤æËá¥ÁöÑÁÆ≠Â§¥Êî∂Â∞æ„ÄÇÊó∂Èó¥ËΩ¥‰∏äÊØè‰∏™ËäÇÁÇπÈÄöËøáËôöÁ∫øËøûÊé•Ëá≥‰∏ãÊñπÈÜíÁõÆÁöÑËìùËâ≤ÂúÜËßíÁü©ÂΩ¢Êó•ÊúüÊ†áÁ≠æÔºåÊ†áÁ≠æÂÜÖ‰∏∫Ê∏ÖÊô∞ÁôΩËâ≤Â≠ó‰ΩìÔºå‰ªéÂ∑¶ÂêëÂè≥‰æùÊ¨°ÂÜôÁùÄÔºö‚Äú2025Âπ¥8Êúà18Êó• Qwen-Image-Edit ÂºÄÊ∫êÂèëÂ∏É‚Äù‚Äú2025Âπ¥9Êúà22Êó• Qwen-Image-Edit-2509 ÂºÄÊ∫êÂèëÂ∏É‚Äù‚Äú2025Âπ¥12Êúà19Êó• Qwen-Image-Layered ÂºÄÊ∫êÂèëÂ∏É‚Äù‚Äú2025Âπ¥12Êúà23Êó• Qwen-Image-Edit-2511 ÂºÄÊ∫êÂèëÂ∏É‚Äù&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/image2512/%E5%B9%BB%E7%81%AF%E7%89%879.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;We can even generate a before-and-after comparison slide to highlight the leap from ‚ÄúAI-blurry‚Äù to ‚Äúphotorealistic‚Äù:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ËøôÊòØ‰∏ÄÂº†Áé∞‰ª£È£éÊ†ºÁöÑÁßëÊäÄÊÑüÂπªÁÅØÁâáÔºåÊï¥‰ΩìÈááÁî®Ê∑±ËìùËâ≤Ê∏êÂèòËÉåÊôØ„ÄÇÈ°∂ÈÉ®‰∏≠Â§Æ‰∏∫ÁôΩËâ≤Êó†Ë°¨Á∫øÁ≤ó‰ΩìÂ§ßÂ≠óÊ†áÈ¢ò‚ÄúQwen-Image-2512ÈáçÁ£ÖÂèëÂ∏É‚Äù„ÄÇÁîªÈù¢‰∏ª‰Ωì‰∏∫Ê®™ÂêëÂØπÊØîÂõæÔºåËßÜËßâÁÑ¶ÁÇπÈõÜ‰∏≠‰∫é‰∏≠Èó¥ÁöÑÂçáÁ∫ßÂØπÊØîÂå∫Âüü„ÄÇÂ∑¶‰æß‰∏∫Èù¢ÈÉ®ÂÖâÊªëÊ≤°Êúâ‰ªª‰ΩïÁªÜËäÇÁöÑÂ•≥ÊÄß‰∫∫ÂÉèÔºåË¥®ÊÑüÂ∑ÆÔºõÂè≥‰æß‰∏∫È´òÂ∫¶ÂÜôÂÆûÁöÑÂπ¥ËΩªÂ•≥ÊÄßËÇñÂÉèÔºåÁöÆËÇ§ÂëàÁé∞ÁúüÂÆûÊØõÂ≠îÁ∫πÁêÜ‰∏éÁªÜÂæÆÂÖâÂΩ±ÂèòÂåñÔºåÂèë‰∏ùÊ†πÊ†πÂàÜÊòéÔºåÁúºÁú∏ÈÄè‰∫ÆÔºåË°®ÊÉÖËá™ÁÑ∂ÔºåÊï¥‰ΩìË¥®ÊÑüÊé•ËøëÂÜôÂÆûÊëÑÂΩ±„ÄÇ‰∏§ÂõæÂÉè‰πãÈó¥‰ª•‰∏Ä‰∏™ÁªøËâ≤ÊµÅÁ∫øÂûãÁÆ≠Â§¥ÈìæÊé•„ÄÇÈÄ†ÂûãÁßëÊäÄÊÑüÂçÅË∂≥Ôºå‰∏≠ÈÉ®Ê†áÊ≥®‚Äú2512Ë¥®ÊÑüÂçáÁ∫ß‚ÄùÔºå‰ΩøÁî®ÁôΩËâ≤Âä†Á≤óÂ≠ó‰ΩìÔºåÂ±Ö‰∏≠ÊòæÁ§∫„ÄÇÁÆ≠Â§¥‰∏§‰æßÊúâÂæÆÂº±ÂÖâÊôïÊïàÊûúÔºåÂ¢ûÂº∫Âä®ÊÄÅÊÑü„ÄÇÂú®ÂõæÂÉè‰∏ãÊñπÔºå‰ª•ÁôΩËâ≤ÊñáÂ≠óÂëàÁé∞‰∏âË°åËØ¥ÊòéÔºö‚Äú‚óè Êõ¥ÁúüÂÆûÁöÑ‰∫∫Áâ©Ë¥®ÊÑü„ÄÇÂ§ßÂπÖÂ∫¶Èôç‰Ωé‰∫ÜÁîüÊàêÂõæÁâáÁöÑAIÊÑüÔºåÊèêÂçá‰∫ÜÂõæÂÉèÁúüÂÆûÊÄß ‚óè Êõ¥ÁªÜËÖªÁöÑËá™ÁÑ∂Á∫πÁêÜ„ÄÇÂ§ßÂπÖÂ∫¶ÊèêÂçá‰∫ÜÁîüÊàêÂõæÁâáÁöÑÁ∫πÁêÜÁªÜËäÇ„ÄÇÈ£éÊôØÂõæÔºåÂä®Áâ©ÊØõÂèëÂàªÁîªÊõ¥ÁªÜËÖª„ÄÇ‚óè Êõ¥Â§çÊùÇÁöÑÊñáÂ≠óÊ∏≤Êüì„ÄÇÂ§ßÂπÖÊèêÂçá‰∫ÜÊñáÂ≠óÊ∏≤ÊüìÁöÑË¥®Èáè„ÄÇÂõæÊñáÊ∑∑ÂêàÊ∏≤ÊüìÊõ¥ÂáÜÁ°ÆÔºåÊéíÁâàÊõ¥Â•Ω‚Äù&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/image2512/%E5%B9%BB%E7%81%AF%E7%89%8710.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;A more complex infographic example:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ËøôÊòØ‰∏ÄÂπÖ‰∏ì‰∏öÁ∫ßÂ∑•‰∏öÊäÄÊúØ‰ø°ÊÅØÂõæË°®ÔºåÊï¥‰ΩìÈááÁî®Ê∑±ËìùËâ≤ÁßëÊäÄÊÑüËÉåÊôØÔºåÂÖâÁ∫øÂùáÂåÄÊüîÂíåÔºåËê•ÈÄ†Âá∫ÂÜ∑Èùô„ÄÅÁ≤æÂáÜÁöÑÁé∞‰ª£Â∑•‰∏öÊ∞õÂõ¥„ÄÇÁîªÈù¢ÂàÜ‰∏∫Â∑¶Âè≥‰∏§Â§ßÊùøÂùóÔºåÂ∏ÉÂ±ÄÊ∏ÖÊô∞ÔºåËßÜËßâÂ±ÇÊ¨°ÂàÜÊòé„ÄÇÂ∑¶‰æßÊùøÂùóÊ†áÈ¢ò‰∏∫‚ÄúÂÆûÈôÖÂèëÁîüÁöÑÁé∞Ë±°‚ÄùÔºå‰ª•ÊµÖËìùËâ≤ÂúÜËßíÁü©ÂΩ¢Ê°ÜÁ™ÅÂá∫ÊòæÁ§∫ÔºåÂÜÖÈÉ®ÊéíÂàó‰∏â‰∏™Ê∑±ËìùËâ≤ÊåâÈíÆÂºèÊù°ÁõÆÔºåÁ¨¨‰∏Ä‰∏™Êù°ÁõÆÂ±ïÁ§∫‰∏ÄÂ†ÜÊ£ïËâ≤Á≤âÊú´Áä∂ÂéüÊñô‰∏äÊª¥ËêΩÊ∞¥Êª¥ÁöÑÂõæÊ†áÔºåÊñáÂ≠ó‰∏∫‚ÄúÂõ¢ËÅö/ÁªìÂùó‚ÄùÔºåÂêéÈù¢ÈÖçÊúâÁªøËâ≤ÂØπÈí©ÔºõÁ¨¨‰∫å‰∏™Êù°ÁõÆ‰∏∫‰∏Ä‰∏™Ë£ÖÊúâËìùËâ≤Ê∂≤‰ΩìÂπ∂ÂÜíÂá∫Ê∞îÊ≥°ÁöÑÈî•ÂΩ¢Áì∂ÔºåÊñáÂ≠ó‰∏∫‚Äú‰∫ßÁîüÊ∞îÊ≥°/Áº∫Èô∑‚ÄùÔºåÂêéÈù¢ÈÖçÊúâÁªøËâ≤ÂØπÈí©ÔºõÁ¨¨‰∏â‰∏™Êù°ÁõÆ‰∏∫‰∏§‰∏™ÁîüÈîàÁöÑÈΩøËΩÆÔºåÊñáÂ≠ó‰∏∫‚ÄúËÆæÂ§áËÖêËöÄ/ÂÇ¨ÂåñÂâÇÂ§±Ê¥ª‚ÄùÔºåÂêéÈù¢ÈÖçÊúâÁªøËâ≤ÂØπÈí©„ÄÇÂè≥‰æßÊùøÂùóÊ†áÈ¢ò‰∏∫‚Äú„Äê‰∏ç‰ºö„ÄëÂèëÁîüÁöÑÁé∞Ë±°‚ÄùÔºå‰ΩøÁî®Á±≥ÈªÑËâ≤ÂúÜËßíÁü©ÂΩ¢Ê°ÜÂëàÁé∞ÔºåÂÜÖÈÉ®Âõõ‰∏™Êù°ÁõÆÂùáÁΩÆ‰∫éÊ∑±ÁÅ∞Ëâ≤ËÉåÊôØÊñπÊ°Ü‰∏≠„ÄÇÂõæÊ†áÂàÜÂà´‰∏∫Ôºö‰∏ÄÁªÑÁ≤æÂØÜÂïÆÂêàÁöÑÈáëÂ±ûÈΩøËΩÆÔºåÊñáÂ≠ó‰∏∫‚ÄúÂèçÂ∫îÊïàÁéá„ÄêÊòæËëóÊèêÈ´ò„Äë‚ÄùÔºå‰∏äÊñπË¶ÜÁõñÈÜíÁõÆÁöÑÁ∫¢Ëâ≤ÂèâÂè∑Ôºõ‰∏ÄÊçÜÊï¥ÈΩêÊéíÂàóÁöÑÈáëÂ±ûÁÆ°ÊùêÔºåÊñáÂ≠ó‰∏∫‚ÄúÊàêÂìÅÂÜÖÈÉ®„ÄêÁªùÂØπÊó†Ê∞îÊ≥°/Â≠îÈöô„Äë‚ÄùÔºå‰∏äÊñπË¶ÜÁõñÈÜíÁõÆÁöÑÁ∫¢Ëâ≤ÂèâÂè∑Ôºõ‰∏ÄÊù°ÂùöÂõ∫ÁöÑÈáëÂ±ûÈìæÊù°Ê≠£Âú®ÊâøÂèóÊãâÂäõÔºåÊñáÂ≠ó‰∏∫‚ÄúÊùêÊñôÂº∫Â∫¶‰∏éËÄê‰πÖÊÄß„ÄêÂæóÂà∞Â¢ûÂº∫„Äë‚ÄùÔºå‰∏äÊñπË¶ÜÁõñÈÜíÁõÆÁöÑÁ∫¢Ëâ≤ÂèâÂè∑Ôºõ‰∏ÄÂ†ÜËÖêËöÄÁöÑÊâ≥ÊâãÔºåÊñáÂ≠ó‰∏∫‚ÄúÂä†Â∑•ËøáÁ®ã„ÄêÈõ∂ËÖêËöÄ/Èõ∂ÂâØÂèçÂ∫îÈ£éÈô©„Äë‚ÄùÔºå‰∏äÊñπË¶ÜÁõñÈÜíÁõÆÁöÑÁ∫¢Ëâ≤ÂèâÂè∑„ÄÇÂ∫ïÈÉ®‰∏≠Â§ÆÊúâ‰∏ÄË°åÂ∞èÂ≠óÊ≥®ÈáäÔºö‚ÄúÊ≥®ÔºöÊ∞¥ÂàÜÁöÑÂ≠òÂú®ÈÄöÂ∏∏‰ºöÂØºËá¥Ë¥üÈù¢ÊàñÂπ≤Êâ∞ÊÄßÁöÑÁªìÊûúÔºåËÄåÈùûÁêÜÊÉ≥ÊàñÂ¢ûÂº∫ÁöÑÁä∂ÊÄÅ‚ÄùÔºåÂ≠ó‰Ωì‰∏∫ÁôΩËâ≤ÔºåÊ∏ÖÊô∞ÂèØËØª„ÄÇÊï¥‰ΩìÈ£éÊ†ºÁé∞‰ª£ÁÆÄÁ∫¶ÔºåÈÖçËâ≤ÂØπÊØîÂº∫ÁÉàÔºåÂõæÂΩ¢Á¨¶Âè∑ÂáÜÁ°Æ‰º†ËææÊäÄÊúØÈÄªËæëÔºåÈÄÇÂêàÁî®‰∫éÂ∑•‰∏öÂüπËÆ≠ÊàñÁßëÊôÆÊºîÁ§∫Âú∫ÊôØ„ÄÇ&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/image2512/%E5%B9%BB%E7%81%AF%E7%89%8711.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;Or even a full educational poster:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ËøôÊòØ‰∏ÄÂπÖÁî±ÂçÅ‰∫å‰∏™ÂàÜÊ†ºÁªÑÊàêÁöÑ3√ó4ÁΩëÊ†ºÂ∏ÉÂ±ÄÁöÑÂÜôÂÆûÊëÑÂΩ±‰ΩúÂìÅÔºåÊï¥‰ΩìÂëàÁé∞‚ÄúÂÅ•Â∫∑ÁöÑ‰∏ÄÂ§©‚Äù‰∏ªÈ¢òÔºåÁîªÈù¢È£éÊ†ºÁÆÄÊ¥ÅÊ∏ÖÊô∞ÔºåÊØè‰∏ÄÂàÜÊ†ºÁã¨Á´ãÊàêÊôØÂèàÁªü‰∏Ä‰∫éÁîüÊ¥ªËäÇÂ•èÁöÑÂèô‰∫ãËÑâÁªú„ÄÇÁ¨¨‰∏ÄË°åÂàÜÂà´ÊòØ‚Äú06:00 Êô®Ë∑ëÂî§ÈÜíË∫´‰Ωì‚ÄùÔºöÈù¢ÈÉ®ÁâπÂÜôÔºå‰∏Ä‰ΩçÂ•≥ÊÄßË∫´Á©øÁÅ∞Ëâ≤ËøêÂä®Â•óË£ÖÔºåËÉåÊôØÊòØÂàùÂçáÁöÑÊúùÈò≥‰∏éËë±ÈÉÅÁªøÊ†ëÔºõ‚Äú06:30 Âä®ÊÄÅÊãâ‰º∏ÊøÄÊ¥ªÂÖ≥ËäÇ‚ÄùÔºöÂ•≥ÊÄßË∫´ÁùÄÁëú‰ºΩÊúçÂú®Èò≥Âè∞ÂÅöÊô®Èó¥Êãâ‰º∏ÔºåË∫´‰ΩìËàíÂ±ïÔºåËÉåÊôØ‰∏∫Ê∑°Á≤âËâ≤Â§©Á©∫‰∏éËøúÂ±±ËΩÆÂªìÔºõ‚Äú07:30 ÂùáË°°Ëê•ÂÖªÊó©È§ê‚ÄùÔºöÊ°å‰∏äÊëÜÊîæÂÖ®È∫¶Èù¢ÂåÖ„ÄÅÁâõÊ≤πÊûúÂíå‰∏ÄÊùØÊ©ôÊ±ÅÔºåÂ•≥ÊÄßÂæÆÁ¨ëÁùÄÂáÜÂ§áÁî®È§êÔºõ‚Äú08:00 Ë°•Ê∞¥Ê∂¶Áá•‚ÄùÔºöÈÄèÊòéÁéªÁíÉÊ∞¥ÊùØ‰∏≠ÊµÆÊúâÊü†Ê™¨ÁâáÔºåÂ•≥ÊÄßÊâãÊåÅÊ∞¥ÊùØËΩªÂïúÔºåÈò≥ÂÖâ‰ªéÂ∑¶‰æßÊñúÁÖßÂÖ•ÂÆ§ÔºåÊùØÂ£ÅÊ∞¥Áè†ÊªëËêΩÔºõÁ¨¨‰∫åË°åÂàÜÂà´ÊòØÔºö‚Äú09:00 ‰∏ìÊ≥®È´òÊïàÂ∑•‰Ωú‚ÄùÔºöÂ•≥ÊÄß‰∏ìÊ≥®Êï≤ÂáªÈîÆÁõòÔºåÂ±èÂπïÊòæÁ§∫ÁÆÄÊ¥ÅÁïåÈù¢ÔºåË∫´ÊóÅÊîæÊúâ‰∏ÄÊùØÂíñÂï°‰∏é‰∏ÄÁõÜÁªøÊ§çÔºõ‚Äú12:00 ÈùôÂøÉÈòÖËØªÊó∂ÂÖâ‚ÄùÔºöÂ•≥ÊÄßÂùêÂú®‰π¶Ê°åÂâçÁøªÈòÖÁ∫∏Ë¥®‰π¶Á±çÔºåÂè∞ÁÅØÊï£ÂèëÊöñÂÖâÔºå‰π¶È°µÊ≥õÈªÑÔºåÊóÅÊîæÂçäÊùØÁ∫¢Ëå∂Ôºõ‚Äú12:30 ÂçàÂêéËΩªÊùæÊº´Ê≠•‚ÄùÔºöÂ•≥ÊÄßÂú®ÊûóËç´ÈÅì‰∏äÊº´Ê≠•ÔºåËÑ∏ÈÉ®ÁâπÂÜôÔºõ‚Äú15:00 Ëå∂È¶ô‰º¥ÂçàÂêé‚ÄùÔºöÂ•≥ÊÄßÁ´ØÁùÄÈ™®Áì∑Ëå∂ÊùØÁ´ôÂú®Á™óËæπÔºåÁ™óÂ§ñÊòØÂüéÂ∏ÇË°óÊôØ‰∏éÈ£òÂä®‰∫ëÊúµÔºåËå∂È¶ôË¢ÖË¢ÖÔºõÁ¨¨‰∏âË°åÂàÜÂà´ÊòØÔºö‚Äú18:00 ËøêÂä®ÈáäÊîæÂéãÂäõ‚ÄùÔºöÂÅ•Ë∫´ÊàøÂÜÖÔºåÂ•≥ÊÄßÊ≠£Âú®ÁªÉ‰π†Áëú‰ºΩÔºõ‚Äú19:00 ÁæéÂë≥ÊôöÈ§ê‚ÄùÔºöÂ•≥ÊÄßÂú®ÂºÄÊîæÂºèÂé®Êàø‰∏≠ÂàáËèúÔºåÁ†ßÊùø‰∏äÊúâÁï™ËåÑ‰∏éÈùíÊ§íÔºåÈîÖ‰∏≠ÁÉ≠Ê∞îÂçáËÖæÔºåÁÅØÂÖâÊ∏©ÊöñÔºõ‚Äú21:00 ÂÜ•ÊÉ≥Âä©Áú†‚ÄùÔºöÂ•≥ÊÄßÁõòËÖøÂùêÂú®ÊüîËΩØÂú∞ÊØØ‰∏äÂÜ•ÊÉ≥ÔºåÂèåÊâãËΩªÊîæËÜù‰∏äÔºåÈó≠ÁõÆÂÆÅÈùôÔºõ‚Äú21:30 ËøõÂÖ•Áù°Áú†‚ÄùÔºöÂ•≥ÊÄßË∫∫Âú®Â∫ä‰∏ä‰ºëÊÅØ„ÄÇÊï¥‰ΩìÈááÁî®Ëá™ÁÑ∂ÂÖâÁ∫ø‰∏∫‰∏ªÔºåËâ≤Ë∞É‰ª•ÊöñÁôΩ‰∏éÁ±≥ÁÅ∞‰∏∫Âü∫Ë∞ÉÔºåÂÖâÂΩ±Â±ÇÊ¨°ÂàÜÊòéÔºåÁîªÈù¢ÂÖÖÊª°Ê∏©È¶®ÁöÑÁîüÊ¥ªÊ∞îÊÅØ‰∏éËßÑÂæãÁöÑËäÇÂ•èÊÑü„ÄÇ&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/image2512/%E5%B9%BB%E7%81%AF%E7%89%8712.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;These are the core enhancements in this update. We hope you enjoy using Qwen-Image-2512!&lt;/p&gt; 
&lt;h3&gt;Showcase of Qwen-Image-Edit-2511&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Qwen-Image-Edit-2511 Enhances Character Consistency&lt;/strong&gt; In Qwen-Image-Edit-2511, character consistency has been significantly improved. The model can perform imaginative edits based on an input portrait while preserving the identity and visual characteristics of the subject.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/%E5%B9%BB%E7%81%AF%E7%89%871.JPG#center" alt="" /&gt; &lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/%E5%B9%BB%E7%81%AF%E7%89%872.JPG#center" alt="" /&gt; &lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/%E5%B9%BB%E7%81%AF%E7%89%873.JPG#center" alt="" /&gt; &lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/%E5%B9%BB%E7%81%AF%E7%89%874.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Improved Multi-Person Consistency&lt;/strong&gt; While Qwen-Image-Edit-2509 already improved consistency for single-subject editing, Qwen-Image-Edit-2511 further enhances consistency in multi-person group photos‚Äîenabling high-fidelity fusion of two separate person images into a coherent group shot: &lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/%E5%B9%BB%E7%81%AF%E7%89%875.JPG#center" alt="" /&gt; &lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/%E5%B9%BB%E7%81%AF%E7%89%876.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Built-in Support for Community-Created LoRAs&lt;/strong&gt; Since Qwen-Image-Edit‚Äôs release, the community has developed many creative and high-quality LoRAs‚Äîgreatly expanding its expressive potential. Qwen-Image-Edit-2511 integrates selected popular LoRAs directly into the base model, unlocking their effects without extra tuning.&lt;/p&gt; 
&lt;p&gt;For example, Lighting Enhancement LoRA Realistic lighting control is now achievable out-of-the-box: &lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/%E5%B9%BB%E7%81%AF%E7%89%877.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/%E5%B9%BB%E7%81%AF%E7%89%878.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;Another example, generating new viewpoints can now be done directly with the base model:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/%E5%B9%BB%E7%81%AF%E7%89%879.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/%E5%B9%BB%E7%81%AF%E7%89%8710.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Industrial Design Applications&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;We‚Äôve paid special attention to practical engineering scenarios‚Äîfor instance, batch industrial product design:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/%E5%B9%BB%E7%81%AF%E7%89%8711.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/%E5%B9%BB%E7%81%AF%E7%89%8712.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;‚Ä¶and material replacement for industrial components: &lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/%E5%B9%BB%E7%81%AF%E7%89%8713.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/%E5%B9%BB%E7%81%AF%E7%89%8714.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Enhanced Geometric Reasoning&lt;/strong&gt; Qwen-Image-Edit-2511 introduces stronger geometric reasoning capability‚Äîe.g., directly generating auxiliary construction lines for design or annotation purposes:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/%E5%B9%BB%E7%81%AF%E7%89%8715.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2511/%E5%B9%BB%E7%81%AF%E7%89%8716.JPG#center" alt="" /&gt;&lt;/p&gt; 
&lt;h2&gt;AI Arena&lt;/h2&gt; 
&lt;p&gt;To comprehensively evaluate the general image generation capabilities of Qwen-Image and objectively compare it with state-of-the-art closed-source APIs, we introduce &lt;a href="https://aiarena.alibaba-inc.com"&gt;AI Arena&lt;/a&gt;, an open benchmarking platform built on the Elo rating system. AI Arena provides a fair, transparent, and dynamic environment for model evaluation.&lt;/p&gt; 
&lt;p&gt;In each round, two images‚Äîgenerated by randomly selected models from the same prompt‚Äîare anonymously presented to users for pairwise comparison. Users vote for the better image, and the results are used to update both personal and global leaderboards via the Elo algorithm, enabling developers, researchers, and the public to assess model performance in a robust and data-driven way. AI Arena is now publicly available, welcoming everyone to participate in model evaluations.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/figure_aiarena_website.png" alt="AI Arena" /&gt;&lt;/p&gt; 
&lt;p&gt;The latest leaderboard rankings can be viewed at &lt;a href="https://aiarena.alibaba-inc.com/corpora/arena/leaderboard?arenaType=text2image"&gt;AI Arena Learboard&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you wish to deploy your model on AI Arena and participate in the evaluation, please contact &lt;a href="mailto:weiyue.wy@alibaba-inc.com"&gt;weiyue.wy@alibaba-inc.com&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Community Support&lt;/h2&gt; 
&lt;h3&gt;Huggingface&lt;/h3&gt; 
&lt;p&gt;Diffusers has supported Qwen-Image since day 0. Support for LoRA and finetuning workflows is currently in development and will be available soon.&lt;/p&gt; 
&lt;h3&gt;ModelScope&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/modelscope/DiffSynth-Studio"&gt;DiffSynth-Studio&lt;/a&gt;&lt;/strong&gt; provides comprehensive support for Qwen-Image, including low-GPU-memory layer-by-layer offload (inference within 4GB VRAM), FP8 quantization, LoRA / full training.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/modelscope/DiffSynth-Engine"&gt;DiffSynth-Engine&lt;/a&gt;&lt;/strong&gt; delivers advanced optimizations for Qwen-Image inference and deployment, including FBCache-based acceleration, classifier-free guidance (CFG) parallel, and more.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://www.modelscope.cn/aigc"&gt;ModelScope AIGC Central&lt;/a&gt;&lt;/strong&gt; provides hands-on experiences on Qwen Image, including: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://www.modelscope.cn/aigc/imageGeneration"&gt;Image Generation&lt;/a&gt;: Generate high fidelity images using the Qwen Image model.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://www.modelscope.cn/aigc/modelTraining"&gt;LoRA Training&lt;/a&gt;: Easily train Qwen Image LoRAs for personalized concepts.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;SGLang&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;SGLang-Diffusion&lt;/strong&gt; provides day-0 support for Qwen-Image models. To play with &lt;code&gt;Qwen-Image-Edit-2511&lt;/code&gt;, use the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;sglang generate --model-path Qwen/Qwen-Image-Edit-2511 --prompt "make the girl in Figure 1 dance with the capybara in Figure 2."  --image-path "https://github.com/lm-sys/lm-sys.github.io/releases/download/test/TI2I_Qwen_Image_Edit_Input.jpg" "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/edit2509_2.jpg"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The output should be like &lt;img src="https://github.com/lm-sys/lm-sys.github.io/releases/download/test/SGLang_Diffusion_Qwen_Image_Edit_2511_example_output.jpg" alt="" /&gt;&lt;/p&gt; 
&lt;h3&gt;WaveSpeedAI&lt;/h3&gt; 
&lt;p&gt;WaveSpeed has deployed Qwen-Image on their platform from day 0, visit their &lt;a href="https://wavespeed.ai/models/wavespeed-ai/qwen-image/text-to-image"&gt;model page&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h3&gt;LiblibAI&lt;/h3&gt; 
&lt;p&gt;LiblibAI offers native support for Qwen-Image from day 0. Visit their &lt;a href="https://www.liblib.art/modelinfo/c62a103bd98a4246a2334e2d952f7b21?from=sd&amp;amp;versionUuid=75e0be0c93b34dd8baeec9c968013e0c"&gt;community&lt;/a&gt; page for more details and discussions.&lt;/p&gt; 
&lt;h3&gt;Inference Acceleration Method: cache-dit&lt;/h3&gt; 
&lt;p&gt;cache-dit offers cache acceleration support for Qwen-Image with DBCache, TaylorSeer and Cache CFG. Visit their &lt;a href="https://github.com/vipshop/cache-dit/raw/main/examples/pipeline/run_qwen_image.py"&gt;example&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;License Agreement&lt;/h2&gt; 
&lt;p&gt;Qwen-Image is licensed under Apache 2.0.&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;We kindly encourage citation of our work if you find it useful.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{wu2025qwenimagetechnicalreport,
      title={Qwen-Image Technical Report}, 
      author={Chenfei Wu and Jiahao Li and Jingren Zhou and Junyang Lin and Kaiyuan Gao and Kun Yan and Sheng-ming Yin and Shuai Bai and Xiao Xu and Yilei Chen and Yuxiang Chen and Zecheng Tang and Zekai Zhang and Zhengyi Wang and An Yang and Bowen Yu and Chen Cheng and Dayiheng Liu and Deqing Li and Hang Zhang and Hao Meng and Hu Wei and Jingyuan Ni and Kai Chen and Kuan Cao and Liang Peng and Lin Qu and Minggang Wu and Peng Wang and Shuting Yu and Tingkun Wen and Wensen Feng and Xiaoxiao Xu and Yi Wang and Yichang Zhang and Yongqiang Zhu and Yujia Wu and Yuxuan Cai and Zenan Liu},
      year={2025},
      eprint={2508.02324},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2508.02324}, 
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Contact and Join Us&lt;/h2&gt; 
&lt;p&gt;If you'd like to get in touch with our research team, we'd love to hear from you! Join our &lt;a href="https://discord.gg/z3GAxXZ9Ce"&gt;Discord&lt;/a&gt; or scan the QR code to connect via our &lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen-Image/main/assets/wechat.png"&gt;WeChat groups&lt;/a&gt; ‚Äî we're always open to discussion and collaboration.&lt;/p&gt; 
&lt;p&gt;If you have questions about this repository, feedback to share, or want to contribute directly, we welcome your issues and pull requests on GitHub. Your contributions help make Qwen-Image better for everyone.&lt;/p&gt; 
&lt;p&gt;If you're passionate about fundamental research, we're hiring full-time employees (FTEs) and research interns. Don't wait ‚Äî reach out to us at &lt;a href="mailto:fulai.hr@alibaba-inc.com"&gt;fulai.hr@alibaba-inc.com&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.star-history.com/#QwenLM/Qwen-Image&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=QwenLM/Qwen-Image&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>