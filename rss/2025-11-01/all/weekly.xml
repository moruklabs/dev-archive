<rss version="2.0">
  <channel>
    <title>GitHub All Languages Weekly Trending</title>
    <description>Weekly Trending of All Languages in GitHub</description>
    <pubDate>Fri, 31 Oct 2025 01:38:43 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>toeverything/AFFiNE</title>
      <link>https://github.com/toeverything/AFFiNE</link>
      <description>&lt;p&gt;There can be more than Notion and Miro. AFFiNE(pronounced […ô‚Äòfain]) is a next-gen knowledge base that brings planning, sorting and creating all together. Privacy first, open-source, customizable and ready to use.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h1 style="border-bottom: none"&gt; &lt;b&gt;&lt;a href="https://affine.pro"&gt;AFFiNE.Pro&lt;/a&gt;&lt;/b&gt;&lt;br /&gt; Write, Draw and Plan All at Once &lt;br /&gt; &lt;/h1&gt; 
 &lt;a href="https://affine.pro/download"&gt; &lt;img alt="affine logo" src="https://cdn.affine.pro/Github_hero_image2.png" style="width: 100%" /&gt; &lt;/a&gt; 
 &lt;br /&gt; 
 &lt;p align="center"&gt; A privacy-focused, local-first, open-source, and ready-to-use alternative for Notion &amp;amp; Miro. &lt;br /&gt; One hyper-fused platform for wildly creative minds. &lt;/p&gt; 
 &lt;br /&gt; 
 &lt;br /&gt; 
 &lt;a href="https://www.producthunt.com/posts/affine-3?utm_source=badge-featured&amp;amp;utm_medium=badge&amp;amp;utm_souce=badge-affine-3" target="_blank"&gt;&lt;img src="https://api.producthunt.com/widgets/embed-image/v1/featured.svg?post_id=440671&amp;amp;theme=light" alt="AFFiNE - One app for all - Where Notion meets Miro | Product Hunt" style="width: 250px; height: 54px;" width="250" height="54" /&gt;&lt;/a&gt; 
 &lt;br /&gt; 
 &lt;br /&gt; 
 &lt;div align="left" valign="middle"&gt; 
  &lt;a href="https://runblaze.dev"&gt; 
   &lt;picture&gt; 
    &lt;source media="(prefers-color-scheme: dark)" srcset="https://www.runblaze.dev/logo_dark.png" /&gt; 
    &lt;img align="right" src="https://www.runblaze.dev/logo_light.png" height="102px" /&gt; 
   &lt;/picture&gt; &lt;/a&gt; 
  &lt;br style="display: none;" /&gt; 
  &lt;p&gt;&lt;em&gt;Special thanks to &lt;a href="https://runblaze.dev"&gt;Blaze&lt;/a&gt; for their support of this project. They provide high-performance Apple Silicon macOS and Linux (AMD64 &amp;amp; ARM64) runners for GitHub Actions, greatly reducing our automated build times.&lt;/em&gt;&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;br /&gt; 
 &lt;br /&gt; 
 &lt;div align="center"&gt; 
  &lt;a href="https://affine.pro"&gt;Home Page&lt;/a&gt; | 
  &lt;a href="https://affine.pro/redirect/discord"&gt;Discord&lt;/a&gt; | 
  &lt;a href="https://app.affine.pro"&gt;Live Demo&lt;/a&gt; | 
  &lt;a href="https://affine.pro/blog/"&gt;Blog&lt;/a&gt; | 
  &lt;a href="https://docs.affine.pro/"&gt;Documentation&lt;/a&gt; 
 &lt;/div&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://github.com/toeverything/AFFiNE/releases/latest"&gt;&lt;img src="https://img.shields.io/github/downloads/toeverything/AFFiNE/total" alt="Releases" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/toeverything/AFFiNE/canary/#contributors"&gt;&lt;img src="https://img.shields.io/github/contributors/toeverything/AFFiNE" alt="All Contributors" /&gt;&lt;/a&gt; &lt;a href="https://www.typescriptlang.org/"&gt;&lt;img src="https://img.shields.io/github/package-json/dependency-version/toeverything/affine/dev/typescript" alt="TypeScript-version-icon" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt; 
 &lt;em&gt;Docs, canvas and tables are hyper-merged with AFFiNE - just like the word affine (…ôÀàf å…™n | a-fine).&lt;/em&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://github.com/toeverything/AFFiNE/assets/79301703/49a426bb-8d2b-4216-891a-fa5993642253" style="width: 100%" /&gt; 
&lt;/div&gt; 
&lt;h2&gt;Getting started &amp;amp; staying tuned with us.&lt;/h2&gt; 
&lt;p&gt;Star us, and you will receive all release notifications from GitHub without any delay!&lt;/p&gt; 
&lt;img src="https://user-images.githubusercontent.com/79301703/230891830-0110681e-8c7e-483b-b6d9-9e42b291b9ef.gif" style="width: 100%" /&gt; 
&lt;h2&gt;What is AFFiNE&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://affine.pro"&gt;AFFiNE&lt;/a&gt; is an open-source, all-in-one workspace and an operating system for all the building blocks that assemble your knowledge base and much more -- wiki, knowledge management, presentation and digital assets. It's a better alternative to Notion and Miro.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;A true canvas for blocks in any form. Docs and whiteboard are now fully merged.&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Many editor apps claim to be a canvas for productivity, but AFFiNE is one of the very few which allows you to put any building block on an edgeless canvas -- rich text, sticky notes, any embedded web pages, multi-view databases, linked pages, shapes and even slides. We have it all.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Multimodal AI partner ready to kick in any work&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Write up professional work report? Turn an outline into expressive and presentable slides? Summary an article into a well-structured mindmap? Sorting your job plan and backlog for tasks? Or... draw and code prototype apps and web pages directly all with one prompt? With you, &lt;a href="https://affine.pro/ai"&gt;AFFiNE AI&lt;/a&gt; pushes your creativity to the edge of your imagination, just like &lt;a href="https://affine.pro/blog/best-canvas-ai"&gt;Canvas AI&lt;/a&gt; to generate mind map for brainstorming.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Local-first &amp;amp; Real-time collaborative&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;We love the idea of local-first that you always own your data on your disk, in spite of the cloud. Furthermore, AFFiNE supports real-time sync and collaborations on web and cross-platform clients.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Self-host &amp;amp; Shape your own AFFiNE&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;You have the freedom to manage, self-host, fork and build your own AFFiNE. Plugin community and third-party blocks are coming soon. More tractions on &lt;a href="https://blocksuite.io"&gt;Blocksuite&lt;/a&gt;. Check there to learn how to &lt;a href="https://docs.affine.pro/self-host-affine"&gt;self-host AFFiNE&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Acknowledgement&lt;/h2&gt; 
&lt;p&gt;‚ÄúWe shape our tools and thereafter our tools shape us‚Äù. A lot of pioneers have inspired us along the way, e.g.:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Quip &amp;amp; Notion with their great concept of ‚Äúeverything is a block‚Äù&lt;/li&gt; 
 &lt;li&gt;Trello with their Kanban&lt;/li&gt; 
 &lt;li&gt;Airtable &amp;amp; Miro with their no-code programmable datasheets&lt;/li&gt; 
 &lt;li&gt;Miro &amp;amp; Whimiscal with their edgeless visual whiteboard&lt;/li&gt; 
 &lt;li&gt;Remote &amp;amp; Capacities with their object-based tag system&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;There is a large overlap of their atomic ‚Äúbuilding blocks‚Äù between these apps. They are not open source, nor do they have a plugin system like Vscode for contributors to customize. We want to have something that contains all the features we love and also goes one step even further.&lt;/p&gt; 
&lt;p&gt;Thanks for checking us out, we appreciate your interest and sincerely hope that AFFiNE resonates with you! üéµ Checking &lt;a href="https://affine.pro/"&gt;https://affine.pro/&lt;/a&gt; for more details ions.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Bug Reports&lt;/th&gt; 
   &lt;th&gt;Feature Requests&lt;/th&gt; 
   &lt;th&gt;Questions/Discussions&lt;/th&gt; 
   &lt;th&gt;AFFiNE Community&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/toeverything/AFFiNE/issues/new?assignees=&amp;amp;labels=bug%2Cproduct-review&amp;amp;template=BUG-REPORT.yml&amp;amp;title=TITLE"&gt;Create a bug report&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/toeverything/AFFiNE/issues/new?assignees=&amp;amp;labels=feat%2Cproduct-review&amp;amp;template=FEATURE-REQUEST.yml&amp;amp;title=TITLE"&gt;Submit a feature request&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/toeverything/AFFiNE/discussions"&gt;Check GitHub Discussion&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://community.affine.pro"&gt;Vist the AFFiNE Community&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Something isn't working as expected&lt;/td&gt; 
   &lt;td&gt;An idea for a new feature, or improvements&lt;/td&gt; 
   &lt;td&gt;Discuss and ask questions&lt;/td&gt; 
   &lt;td&gt;A place to ask, learn and engage with others&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Calling all developers, testers, tech writers and more! Contributions of all types are more than welcome, you can read more in &lt;a href="https://raw.githubusercontent.com/toeverything/AFFiNE/canary/docs/types-of-contributions.md"&gt;docs/types-of-contributions.md&lt;/a&gt;. If you are interested in contributing code, read our &lt;a href="https://raw.githubusercontent.com/toeverything/AFFiNE/canary/docs/CONTRIBUTING.md"&gt;docs/CONTRIBUTING.md&lt;/a&gt; and feel free to check out our GitHub issues to get stuck in to show us what you‚Äôre made of.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Before you start contributing, please make sure you have read and accepted our &lt;a href="https://github.com/toeverything/affine/edit/canary/.github/CLA.md"&gt;Contributor License Agreement&lt;/a&gt;. To indicate your agreement, simply edit this file and submit a pull request.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;For &lt;strong&gt;bug reports&lt;/strong&gt;, &lt;strong&gt;feature requests&lt;/strong&gt; and other &lt;strong&gt;suggestions&lt;/strong&gt; you can also &lt;a href="https://github.com/toeverything/AFFiNE/issues/new/choose"&gt;create a new issue&lt;/a&gt; and choose the most appropriate template for your feedback.&lt;/p&gt; 
&lt;p&gt;For &lt;strong&gt;translation&lt;/strong&gt; and &lt;strong&gt;language support&lt;/strong&gt; you can visit our &lt;a href="https://community.affine.pro/c/i18n-general"&gt;i18n General Space&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Looking for &lt;strong&gt;other ways to contribute&lt;/strong&gt; and wondering where to start? Check out the &lt;a href="https://community.affine.pro/c/start-here/affine-ambassador"&gt;AFFiNE Ambassador program&lt;/a&gt;, we work closely with passionate community members and provide them with a wide range of support and resources.&lt;/p&gt; 
&lt;p&gt;If you have questions, you are welcome to contact us. One of the best places to get more info and learn more is in the &lt;a href="https://community.affine.pro"&gt;AFFiNE Community&lt;/a&gt; where you can engage with other like-minded individuals.&lt;/p&gt; 
&lt;h2&gt;Templates&lt;/h2&gt; 
&lt;p&gt;AFFiNE now provides pre-built &lt;a href="https://affine.pro/templates"&gt;templates&lt;/a&gt; from our team. Following are the Top 10 most popular templates among AFFiNE users,if you want to contribute, you can contribute your own template so other people can use it too.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://affine.pro/templates/category-vision-board-template"&gt;vision board template&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://affine.pro/templates/category-one-pager-template-free"&gt;one pager template&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://affine.pro/templates/sample-lesson-plan-math-template"&gt;sample lesson plan math template&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://affine.pro/templates/grr-lesson-plan-template-free"&gt;grr lesson plan template free&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://affine.pro/templates/free-editable-lesson-plan-template-for-pre-k"&gt;free editable lesson plan template for pre k&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://affine.pro/templates/high-note-collection-planners"&gt;high note collection planners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://affine.pro/templates/category-digital-planner"&gt;digital planner&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://affine.pro/templates/adhd-planner"&gt;ADHD Planner&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://affine.pro/templates/reading-log"&gt;Reading Log&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://affine.pro/templates/category-cornell-notes-template"&gt;Cornell Notes Template&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Blog&lt;/h2&gt; 
&lt;p&gt;Welcome to the AFFiNE blog section! Here, you‚Äôll find the latest insights, tips, and guides on how to maximize your experience with AFFiNE and AFFiNE AI, the leading Canvas AI tool for flexible note-taking and creative organization.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://affine.pro/blog/8-free-printable-vision-board-templates-examples-2023"&gt;vision board template&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://affine.pro/blog/ai-homework-helper"&gt;ai homework helper&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://affine.pro/blog/vision-board-maker"&gt;vision board maker&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://affine.pro/blog/free-customized-travel-itinerary-planner-templates"&gt;itinerary template&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://affine.pro/blog/top-12-one-pager-examples-how-to-create-your-own"&gt;one pager template&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://affine.pro/blog/the-cornell-notes-template-and-system-learning-tips"&gt;cornell notes template&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://affine.pro/blog/top-10-free-editable-swot-analysis-template-examples"&gt;swot chart template&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://affine.pro/blog/apps-like-luna-task"&gt;apps like luna task&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://affine.pro/blog/dynamic-AI-notes"&gt;note taking ai from rough notes to mind map&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://affine.pro/blog/best-canvas-ai"&gt;canvas ai&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://affine.pro/blog/top-12-one-pager-examples-how-to-create-your-own"&gt;one pager&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://affine.pro/blog/how-to-write-sop-step-by-step-guide-5-best-free-tools-templates"&gt;SOP Template&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://affine.pro/blog/10-best-free-chore-chart-templates-kids-adults"&gt;Chore Chart&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Ecosystem&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Name&lt;/th&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/toeverything/AFFiNE/canary/packages/frontend/component"&gt;@affine/component&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;AFFiNE Component Resources&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://img.shields.io/codecov/c/github/toeverything/affine?style=flat-square" alt="" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/toeverything/AFFiNE/canary/packages/common/theme"&gt;@toeverything/theme&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;AFFiNE theme&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.npmjs.com/package/@toeverything/theme"&gt;&lt;img src="https://img.shields.io/npm/dm/@toeverything/theme?style=flat-square&amp;amp;color=eee" alt="" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Upstreams&lt;/h2&gt; 
&lt;p&gt;We would also like to give thanks to open-source projects that make AFFiNE possible:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/toeverything/BlockSuite"&gt;Blocksuite&lt;/a&gt; - üí† BlockSuite is the open-source collaborative editor project behind AFFiNE.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/toeverything/OctoBase"&gt;OctoBase&lt;/a&gt; - üêô OctoBase is the open-source database behind AFFiNE, local-first, yet collaborative. A light-weight, scalable, data engine written in Rust.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/yjs/yjs"&gt;yjs&lt;/a&gt; - Fundamental support of CRDTs for our implementation on state management and data sync.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/electron/electron"&gt;electron&lt;/a&gt; - Build cross-platform desktop apps with JavaScript, HTML, and CSS.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/facebook/react"&gt;React&lt;/a&gt; - The library for web and native user interfaces.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/napi-rs/napi-rs"&gt;napi-rs&lt;/a&gt; - A framework for building compiled Node.js add-ons in Rust via Node-API.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pmndrs/jotai"&gt;Jotai&lt;/a&gt; - Primitive and flexible state management for React.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Jack-Works/async-call-rpc"&gt;async-call-rpc&lt;/a&gt; - A lightweight JSON RPC client &amp;amp; server.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/vitejs/vite"&gt;Vite&lt;/a&gt; - Next generation frontend tooling.&lt;/li&gt; 
 &lt;li&gt;Other upstream &lt;a href="https://github.com/toeverything/AFFiNE/network/dependencies"&gt;dependencies&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Thanks a lot to the community for providing such powerful and simple libraries, so that we can focus more on the implementation of the product logic, and we hope that in the future our projects will also provide a more easy-to-use knowledge base for everyone.&lt;/p&gt; 
&lt;h2&gt;Contributors&lt;/h2&gt; 
&lt;p&gt;We would like to express our gratitude to all the individuals who have already contributed to AFFiNE! If you have any AFFiNE-related project, documentation, tool or template, please feel free to contribute it by submitting a pull request to our curated list on GitHub: &lt;a href="https://github.com/toeverything/awesome-affine"&gt;awesome-affine&lt;/a&gt;.&lt;/p&gt; 
&lt;a href="https://github.com/toeverything/affine/graphs/contributors"&gt; &lt;img alt="contributors" src="https://opencollective.com/affine/contributors.svg?width=890&amp;amp;button=false" /&gt; &lt;/a&gt; 
&lt;h2&gt;Self-Host&lt;/h2&gt; 
&lt;p&gt;Begin with Docker to deploy your own feature-rich, unrestricted version of AFFiNE. Our team is diligently updating to the latest version. For more information on how to self-host AFFiNE, please refer to our &lt;a href="https://docs.affine.pro/self-host-affine"&gt;documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://template.run.claw.cloud/?openapp=system-fastdeploy%3FtemplateName%3Daffine"&gt;&lt;img src="https://raw.githubusercontent.com/ClawCloud/Run-Template/refs/heads/main/Run-on-ClawCloud.svg?sanitize=true" alt="Run on ClawCloud" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Hiring&lt;/h2&gt; 
&lt;p&gt;Some amazing companies, including AFFiNE, are looking for developers! Are you interested in joining AFFiNE or its partners? Check out our &lt;a href="https://affine.pro/redirect/discord"&gt;Discord channel&lt;/a&gt; for some of the latest jobs available.&lt;/p&gt; 
&lt;h2&gt;Feature Request&lt;/h2&gt; 
&lt;p&gt;For feature requests, please see &lt;a href="https://community.affine.pro/c/feature-requests/"&gt;community.affine.pro&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Building&lt;/h2&gt; 
&lt;h3&gt;Codespaces&lt;/h3&gt; 
&lt;p&gt;From the GitHub repo main page, click the green "Code" button and select "Create codespace on master". This will open a new Codespace with the (supposedly auto-forked AFFiNE repo cloned, built, and ready to go.&lt;/p&gt; 
&lt;h3&gt;Local&lt;/h3&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/toeverything/AFFiNE/canary/docs/BUILDING.md"&gt;BUILDING.md&lt;/a&gt; for instructions on how to build AFFiNE from source code.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions from everyone. See &lt;a href="https://raw.githubusercontent.com/toeverything/AFFiNE/canary/docs/contributing/tutorial.md"&gt;docs/contributing/tutorial.md&lt;/a&gt; for details.&lt;/p&gt; 
&lt;h2&gt;Thanks&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.chromatic.com/"&gt;&lt;img src="https://user-images.githubusercontent.com/321738/84662277-e3db4f80-af1b-11ea-88f5-91d67a5e59f6.png" width="153" height="30" alt="Chromatic" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Thanks to &lt;a href="https://www.chromatic.com/"&gt;Chromatic&lt;/a&gt; for providing the visual testing platform that helps us review UI changes and catch visual regressions.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;h3&gt;Editions&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;AFFiNE Community Edition (CE) is the current available version, it's free for self-host under the MIT license.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;AFFiNE Enterprise Edition (EE) is yet to be published, it will have more advanced features and enterprise-oriented offerings, including but not exclusive to rebranding and SSO, advanced admin and audit, etc., you may refer to &lt;a href="https://affine.pro/pricing"&gt;https://affine.pro/pricing&lt;/a&gt; for more information&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/toeverything/AFFiNE/canary/LICENSE"&gt;LICENSE&lt;/a&gt; for details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>cjpais/Handy</title>
      <link>https://github.com/cjpais/Handy</link>
      <description>&lt;p&gt;A free, open source, and extensible speech-to-text application that works completely offline.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Handy&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://discord.com/invite/WVBeWsNXK4"&gt;&lt;img src="https://img.shields.io/badge/Discord-%235865F2.svg?style=for-the-badge&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;A free, open source, and extensible speech-to-text application that works completely offline.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Handy is a cross-platform desktop application built with Tauri (Rust + React/TypeScript) that provides simple, privacy-focused speech transcription. Press a shortcut, speak, and have your words appear in any text field‚Äîall without sending your voice to the cloud.&lt;/p&gt; 
&lt;h2&gt;Why Handy?&lt;/h2&gt; 
&lt;p&gt;Handy was created to fill the gap for a truly open source, extensible speech-to-text tool. As stated on &lt;a href="https://handy.computer"&gt;handy.computer&lt;/a&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Free&lt;/strong&gt;: Accessibility tooling belongs in everyone's hands, not behind a paywall&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Open Source&lt;/strong&gt;: Together we can build further. Extend Handy for yourself and contribute to something bigger&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Private&lt;/strong&gt;: Your voice stays on your computer. Get transcriptions without sending audio to the cloud&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Simple&lt;/strong&gt;: One tool, one job. Transcribe what you say and put it into a text box&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Handy isn't trying to be the best speech-to-text app‚Äîit's trying to be the most forkable one.&lt;/p&gt; 
&lt;h2&gt;How It Works&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Press&lt;/strong&gt; a configurable keyboard shortcut to start/stop recording (or use push-to-talk mode)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Speak&lt;/strong&gt; your words while the shortcut is active&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Release&lt;/strong&gt; and Handy processes your speech using Whisper&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Get&lt;/strong&gt; your transcribed text pasted directly into whatever app you're using&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;The process is entirely local:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Silence is filtered using VAD (Voice Activity Detection) with Silero&lt;/li&gt; 
 &lt;li&gt;Transcription uses your choice of models: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Whisper models&lt;/strong&gt; (Small/Medium/Turbo/Large) with GPU acceleration when available&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Parakeet V3&lt;/strong&gt; - CPU-optimized model with excellent performance and automatic language detection&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Works on Windows, macOS, and Linux&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Download the latest release from the &lt;a href="https://github.com/cjpais/Handy/releases"&gt;releases page&lt;/a&gt; or the &lt;a href="https://handy.computer"&gt;website&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Install the application following platform-specific instructions&lt;/li&gt; 
 &lt;li&gt;Launch Handy and grant necessary system permissions (microphone, accessibility)&lt;/li&gt; 
 &lt;li&gt;Configure your preferred keyboard shortcuts in Settings&lt;/li&gt; 
 &lt;li&gt;Start transcribing!&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Development Setup&lt;/h3&gt; 
&lt;p&gt;For detailed build instructions including platform-specific requirements, see &lt;a href="https://raw.githubusercontent.com/cjpais/Handy/main/BUILD.md"&gt;BUILD.md&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Architecture&lt;/h2&gt; 
&lt;p&gt;Handy is built as a Tauri application combining:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Frontend&lt;/strong&gt;: React + TypeScript with Tailwind CSS for the settings UI&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Backend&lt;/strong&gt;: Rust for system integration, audio processing, and ML inference&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Core Libraries&lt;/strong&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;whisper-rs&lt;/code&gt;: Local speech recognition with Whisper models&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;transcription-rs&lt;/code&gt;: CPU-optimized speech recognition with Parakeet models&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;cpal&lt;/code&gt;: Cross-platform audio I/O&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;vad-rs&lt;/code&gt;: Voice Activity Detection&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;rdev&lt;/code&gt;: Global keyboard shortcuts and system events&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;rubato&lt;/code&gt;: Audio resampling&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Debug Mode&lt;/h3&gt; 
&lt;p&gt;Handy includes an advanced debug mode for development and troubleshooting. Access it by pressing:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;macOS&lt;/strong&gt;: &lt;code&gt;Cmd+Shift+D&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Windows/Linux&lt;/strong&gt;: &lt;code&gt;Ctrl+Shift+D&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Known Issues &amp;amp; Current Limitations&lt;/h2&gt; 
&lt;p&gt;This project is actively being developed and has some &lt;a href="https://github.com/cjpais/Handy/issues"&gt;known issues&lt;/a&gt;. We believe in transparency about the current state:&lt;/p&gt; 
&lt;h3&gt;Platform Support&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;macOS (both Intel and Apple Silicon)&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;x64 Windows&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;x64 Linux&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;System Requirements/Recommendations&lt;/h3&gt; 
&lt;p&gt;The following are recommendations for running Handy on your own machine. If you don't meet the system requirements, the performance of the application may be degraded. We are working on improving the performance across all kinds of computers and hardware.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;For Whisper Models:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;macOS&lt;/strong&gt;: M series Mac, Intel Mac&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt;: Intel, AMD, or NVIDIA GPU&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Linux&lt;/strong&gt;: Intel, AMD, or NVIDIA GPU 
  &lt;ul&gt; 
   &lt;li&gt;Ubuntu 22.04, 24.04&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;For Parakeet V3 Model:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;CPU-only operation&lt;/strong&gt; - runs on a wide variety of hardware&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Minimum&lt;/strong&gt;: Intel Skylake (6th gen) or equivalent AMD processors&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Performance&lt;/strong&gt;: ~5x real-time speed on mid-range hardware (tested on i5)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Automatic language detection&lt;/strong&gt; - no manual language selection required&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;How to Contribute&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Check existing issues&lt;/strong&gt; at &lt;a href="https://github.com/cjpais/Handy/issues"&gt;github.com/cjpais/Handy/issues&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Fork the repository&lt;/strong&gt; and create a feature branch&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Test thoroughly&lt;/strong&gt; on your target platform&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Submit a pull request&lt;/strong&gt; with clear description of changes&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Join the discussion&lt;/strong&gt; - reach out at &lt;a href="mailto:contact@handy.computer"&gt;contact@handy.computer&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;The goal is to create both a useful tool and a foundation for others to build upon‚Äîa well-patterned, simple codebase that serves the community.&lt;/p&gt; 
&lt;h2&gt;Sponsors&lt;/h2&gt; 
&lt;div align="center"&gt;
  We're grateful for the support of our sponsors who help make Handy possible: 
 &lt;br /&gt;
 &lt;br /&gt; 
 &lt;a href="https://wordcab.com"&gt; &lt;img src="https://raw.githubusercontent.com/cjpais/Handy/main/sponsor-images/wordcab.png" alt="Wordcab" width="120" height="120" /&gt; &lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 
 &lt;a href="https://github.com/epicenter-so/epicenter"&gt; &lt;img src="https://raw.githubusercontent.com/cjpais/Handy/main/sponsor-images/epicenter.png" alt="Epicenter" width="120" height="120" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;Related Projects&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/cjpais/handy-cli"&gt;Handy CLI&lt;/a&gt;&lt;/strong&gt; - The original Python command-line version&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://handy.computer"&gt;handy.computer&lt;/a&gt;&lt;/strong&gt; - Project website with demos and documentation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;MIT License - see &lt;a href="https://raw.githubusercontent.com/cjpais/Handy/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;h2&gt;Acknowledgments&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Whisper&lt;/strong&gt; by OpenAI for the speech recognition model&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;whisper.cpp and ggml&lt;/strong&gt; for amazing cross-platform whisper inference/acceleration&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Silero&lt;/strong&gt; for great lightweight VAD&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Tauri&lt;/strong&gt; team for the excellent Rust-based app framework&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Community contributors&lt;/strong&gt; helping make Handy better&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;em&gt;"Your search for the right speech-to-text tool can end here‚Äînot because Handy is perfect, but because you can make it perfect for you."&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>microsoft/agent-lightning</title>
      <link>https://github.com/microsoft/agent-lightning</link>
      <description>&lt;p&gt;The absolute trainer to light up AI agents.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/agent-lightning/main/docs/assets/readme-banner.svg?sanitize=true" alt="Agent-lightning-banner" style="width:600px" /&gt; &lt;/p&gt; 
&lt;h1&gt;Agent Lightning‚ö°&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/microsoft/agent-lightning/actions/workflows/tests-full.yml"&gt;&lt;img src="https://github.com/microsoft/agent-lightning/actions/workflows/tests-full.yml/badge.svg?sanitize=true" alt="Test" /&gt;&lt;/a&gt; &lt;a href="https://microsoft.github.io/agent-lightning/"&gt;&lt;img src="https://img.shields.io/badge/GitHub%20Pages-Documentation-blue" alt="Documentation" /&gt;&lt;/a&gt; &lt;a href="https://badge.fury.io/py/agentlightning"&gt;&lt;img src="https://badge.fury.io/py/agentlightning.svg?sanitize=true" alt="PyPI version" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/microsoft/agent-lightning/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/RYk7CdvDR7"&gt;&lt;img src="https://img.shields.io/badge/Discord-Join-5865F2?logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;The absolute trainer to light up AI agents.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Join our &lt;a href="https://discord.gg/RYk7CdvDR7"&gt;Discord community&lt;/a&gt; to connect with other users and contributors.&lt;/p&gt; 
&lt;h2&gt;‚ö° Core Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Turn your agent into an optimizable beast with &lt;strong&gt;ZERO CODE CHANGE&lt;/strong&gt; (almost)! üí§&lt;/li&gt; 
 &lt;li&gt;Build with &lt;strong&gt;ANY&lt;/strong&gt; agent framework (LangChain, OpenAI Agent SDK, AutoGen, CrewAI, Microsoft Agent Framework...); or even WITHOUT agent framework (Python OpenAI). You name it! ü§ñ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Selectively&lt;/strong&gt; optimize one or more agents in a multi-agent system. üéØ&lt;/li&gt; 
 &lt;li&gt;Embraces &lt;strong&gt;Algorithms&lt;/strong&gt; like Reinforcement Learning, Automatic Prompt Optimization, Supervised Fine-tuning and more. ü§ó&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Read more on our &lt;a href="https://microsoft.github.io/agent-lightning/"&gt;documentation website&lt;/a&gt;.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/agent-lightning/main/docs/assets/readme-diff.svg?sanitize=true" alt="Agent-Lightning Core Quickstart" style="width:100%" /&gt; &lt;/p&gt; 
&lt;h2&gt;‚ö° Installation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install agentlightning
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Please refer to our &lt;a href="https://microsoft.github.io/agent-lightning/stable/tutorials/installation/"&gt;installation guide&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;p&gt;To start using Agent-lightning, check out our &lt;a href="https://microsoft.github.io/agent-lightning/"&gt;documentation&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/microsoft/agent-lightning/main/examples"&gt;examples&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;‚ö° Articles&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;10/22/2025 &lt;a href="https://blog.vllm.ai/2025/10/22/agent-lightning.html"&gt;No More Retokenization Drift: Returning Token IDs via the OpenAI Compatible API Matters in Agent RL&lt;/a&gt; vLLM blog. See also &lt;a href="https://zhuanlan.zhihu.com/p/1965067274642785725"&gt;Zhihu writeup&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;8/11/2025 &lt;a href="https://medium.com/@yugez/training-ai-agents-to-write-and-self-correct-sql-with-reinforcement-learning-571ed31281ad"&gt;Training AI Agents to Write and Self-correct SQL with Reinforcement Learning&lt;/a&gt; Medium.&lt;/li&gt; 
 &lt;li&gt;8/5/2025 &lt;a href="https://arxiv.org/abs/2508.03680"&gt;Agent Lightning: Train ANY AI Agents with Reinforcement Learning&lt;/a&gt; arXiv paper.&lt;/li&gt; 
 &lt;li&gt;7/26/2025 &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1m9m670/we_discovered_an_approach_to_train_any_ai_agent/"&gt;We discovered an approach to train any AI agent with RL, with (almost) zero code changes.&lt;/a&gt; Reddit.&lt;/li&gt; 
 &lt;li&gt;6/6/2025 &lt;a href="https://www.microsoft.com/en-us/research/project/agent-lightning/"&gt;Agent Lightning - Microsoft Research&lt;/a&gt; Project page.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;‚ö° Community Projects&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/af-74413592/DeepWerewolf"&gt;DeepWerewolf&lt;/a&gt; ‚Äî A case study of agent RL training for the Chinese Werewolf game built with AgentScope and Agent Lightning.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://agentflow.stanford.edu/"&gt;AgentFlow&lt;/a&gt; ‚Äî A modular multi-agent framework that combines planner, executor, verifier, and generator agents with the Flow-GRPO algorithm to tackle long-horizon, sparse-reward tasks.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;‚ö° Architecture&lt;/h2&gt; 
&lt;p&gt;Agent Lightning keeps the moving parts to a minimum so you can focus on your idea, not the plumbing. Your agent continues to run as usual; you can still use any agent framework you like; you drop in the lightweight &lt;code&gt;agl.emit_xxx()&lt;/code&gt; helper, or let the tracer collect every prompt, tool call, and reward. Those events become structured spans that flow into the LightningStore, a central hub that keeps tasks, resources, and traces in sync.&lt;/p&gt; 
&lt;p&gt;On the other side of the store sits the algorithm you choose, or write yourself. The algorithm reads spans, learns from them, and posts updated resources such as refined prompt templates or new policy weights. The Trainer ties it all together: it streams datasets to runners, ferries resources between the store and the algorithm, and updates the inference engine when improvements land. You can either stop there, or simply let the same loop keep turning.&lt;/p&gt; 
&lt;p&gt;No rewrites, no lock-in, just a clear path from first rollout to steady improvement.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/agent-lightning/main/docs/assets/readme-architecture.svg?sanitize=true" alt="Agent-lightning Architecture" style="width:100%" /&gt; &lt;/p&gt; 
&lt;h2&gt;‚ö° CI Status&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Workflow&lt;/th&gt; 
   &lt;th&gt;Status&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;CPU Tests&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/agent-lightning/actions/workflows/tests.yml"&gt;&lt;img src="https://github.com/microsoft/agent-lightning/actions/workflows/tests.yml/badge.svg?sanitize=true" alt="tests workflow status" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;GPU Tests&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/agent-lightning/actions/workflows/tests-full.yml"&gt;&lt;img src="https://github.com/microsoft/agent-lightning/actions/workflows/tests-full.yml/badge.svg?sanitize=true" alt="tests-full workflow status" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Examples Integration&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/agent-lightning/actions/workflows/badge-examples.yml"&gt;&lt;img src="https://github.com/microsoft/agent-lightning/actions/workflows/badge-examples.yml/badge.svg?sanitize=true" alt="examples summary workflow status" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Latest Dependency Compatibility&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/agent-lightning/actions/workflows/badge-latest.yml"&gt;&lt;img src="https://github.com/microsoft/agent-lightning/actions/workflows/badge-latest.yml/badge.svg?sanitize=true" alt="latest summary workflow status" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Legacy Examples Compatibility&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/agent-lightning/actions/workflows/examples-compat.yml"&gt;&lt;img src="https://github.com/microsoft/agent-lightning/actions/workflows/examples-compat.yml/badge.svg?sanitize=true" alt="examples compatibility workflow status" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;‚ö° Citation&lt;/h2&gt; 
&lt;p&gt;If you find Agent Lightning useful in your research or projects, please cite our paper:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{luo2025agentlightningtrainai,
      title={Agent Lightning: Train ANY AI Agents with Reinforcement Learning},
      author={Xufang Luo and Yuge Zhang and Zhiyuan He and Zilong Wang and Siyun Zhao and Dongsheng Li and Luna K. Qiu and Yuqing Yang},
      year={2025},
      eprint={2508.03680},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2508.03680},
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;‚ö° Contributing&lt;/h2&gt; 
&lt;p&gt;This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href="https://cla.opensource.microsoft.com"&gt;https://cla.opensource.microsoft.com&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; 
&lt;p&gt;This project has adopted the &lt;a href="https://opensource.microsoft.com/codeofconduct/"&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href="https://opensource.microsoft.com/codeofconduct/faq/"&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href="mailto:opencode@microsoft.com"&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; 
&lt;h2&gt;‚ö° Trademarks&lt;/h2&gt; 
&lt;p&gt;This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow &lt;a href="https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general"&gt;Microsoft's Trademark &amp;amp; Brand Guidelines&lt;/a&gt;. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.&lt;/p&gt; 
&lt;h2&gt;‚ö° Responsible AI&lt;/h2&gt; 
&lt;p&gt;This project has been evaluated and certified to comply with the Microsoft Responsible AI Standard. The team will continue to monitor and maintain the repository, addressing any severe issues, including potential harms, if they arise.&lt;/p&gt; 
&lt;h2&gt;‚ö° License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the MIT License. See the &lt;a href="https://raw.githubusercontent.com/microsoft/agent-lightning/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>hoppscotch/hoppscotch</title>
      <link>https://github.com/hoppscotch/hoppscotch</link>
      <description>&lt;p&gt;Open-Source API Development Ecosystem ‚Ä¢ https://hoppscotch.io ‚Ä¢ Offline, On-Prem &amp; Cloud ‚Ä¢ Web, Desktop &amp; CLI ‚Ä¢ Open-Source Alternative to Postman, Insomnia&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;a href="https://hoppscotch.io"&gt; &lt;img src="https://avatars.githubusercontent.com/u/56705483" alt="Hoppscotch" height="64" /&gt; &lt;/a&gt; 
 &lt;h3&gt; &lt;b&gt; Hoppscotch &lt;/b&gt; &lt;/h3&gt; 
 &lt;b&gt; Open Source API Development Ecosystem &lt;/b&gt; 
 &lt;p&gt; &lt;/p&gt;
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/hoppscotch/hoppscotch/main/CODE_OF_CONDUCT.md"&gt;&lt;img src="https://img.shields.io/badge/contributions-welcome-brightgreen?logo=github" alt="contributions welcome" /&gt;&lt;/a&gt; &lt;a href="https://hoppscotch.io"&gt;&lt;img src="https://img.shields.io/website?url=https%3A%2F%2Fhoppscotch.io&amp;amp;logo=hoppscotch" alt="Website" /&gt;&lt;/a&gt; &lt;a href="https://github.com/hoppscotch/hoppscotch/actions"&gt;&lt;img src="https://github.com/hoppscotch/hoppscotch/actions/workflows/tests.yml/badge.svg?sanitize=true" alt="Tests" /&gt;&lt;/a&gt; &lt;a href="https://twitter.com/share?text=%F0%9F%91%BD%20Hoppscotch%20%E2%80%A2%20Open%20source%20API%20development%20ecosystem%20-%20Helps%20you%20create%20requests%20faster,%20saving%20precious%20time%20on%20development.&amp;amp;url=https://hoppscotch.io&amp;amp;hashtags=hoppscotch&amp;amp;via=hoppscotch_io"&gt;&lt;img src="https://img.shields.io/twitter/url?url=https%3A%2F%2Fhoppscotch.io%2F" alt="Tweet" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;/p&gt; 
 &lt;p&gt; &lt;sub&gt; Built with ‚ù§Ô∏é by &lt;a href="https://github.com/hoppscotch/hoppscotch/graphs/contributors"&gt; contributors &lt;/a&gt; &lt;/sub&gt; &lt;/p&gt; 
 &lt;br /&gt; 
 &lt;p&gt; &lt;a href="https://hoppscotch.io"&gt; 
   &lt;picture&gt; 
    &lt;source media="(prefers-color-scheme: dark)" srcset="./packages/hoppscotch-common/public/images/banner-dark.png" /&gt; 
    &lt;source media="(prefers-color-scheme: light)" srcset="./packages/hoppscotch-common/public/images/banner-light.png" /&gt; 
    &lt;img alt="Hoppscotch" src="https://raw.githubusercontent.com/hoppscotch/hoppscotch/main/packages/hoppscotch-common/public/images/banner-dark.png" /&gt; 
   &lt;/picture&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;em&gt;We highly recommend you take a look at the &lt;a href="https://docs.hoppscotch.io"&gt;&lt;strong&gt;Hoppscotch Documentation&lt;/strong&gt;&lt;/a&gt; to learn more about the app.&lt;/em&gt;&lt;/p&gt; 
&lt;h4&gt;&lt;strong&gt;Support&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://hoppscotch.io/discord"&gt;&lt;img src="https://img.shields.io/badge/chat-Discord-7289DA?logo=discord" alt="Chat on Discord" /&gt;&lt;/a&gt; &lt;a href="https://hoppscotch.io/telegram"&gt;&lt;img src="https://img.shields.io/badge/chat-Telegram-2CA5E0?logo=telegram" alt="Chat on Telegram" /&gt;&lt;/a&gt; &lt;a href="https://github.com/hoppscotch/hoppscotch/discussions"&gt;&lt;img src="https://img.shields.io/badge/discussions-GitHub-333333?logo=github" alt="Discuss on GitHub" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;‚ù§Ô∏è &lt;strong&gt;Lightweight:&lt;/strong&gt; Crafted with minimalistic UI design.&lt;/p&gt; 
&lt;p&gt;‚ö°Ô∏è &lt;strong&gt;Fast:&lt;/strong&gt; Send requests and get responses in real time.&lt;/p&gt; 
&lt;p&gt;üóÑÔ∏è &lt;strong&gt;HTTP Methods:&lt;/strong&gt; Request methods define the type of action you are requesting to be performed.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;GET&lt;/code&gt; - Requests retrieve resource information&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;POST&lt;/code&gt; - The server creates a new entry in a database&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;PUT&lt;/code&gt; - Updates an existing resource&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;PATCH&lt;/code&gt; - Very similar to &lt;code&gt;PUT&lt;/code&gt; but makes a partial update on a resource&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;DELETE&lt;/code&gt; - Deletes resource or related component&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;HEAD&lt;/code&gt; - Retrieve response headers identical to those of a GET request, but without the response body.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;CONNECT&lt;/code&gt; - Establishes a tunnel to the server identified by the target resource&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;OPTIONS&lt;/code&gt; - Describe the communication options for the target resource&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;TRACE&lt;/code&gt; - Performs a message loop-back test along the path to the target resource&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;&amp;lt;custom&amp;gt;&lt;/code&gt; - Some APIs use custom request methods such as &lt;code&gt;LIST&lt;/code&gt;. Type in your custom methods.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;üåà &lt;strong&gt;Theming:&lt;/strong&gt; Customizable combinations for background, foreground, and accent colors ‚Äî &lt;a href="https://hoppscotch.io/settings"&gt;customize now&lt;/a&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Choose a theme: System preference, Light, Dark, and Black&lt;/li&gt; 
 &lt;li&gt;Choose accent colors: Green, Teal, Blue, Indigo, Purple, Yellow, Orange, Red, and Pink&lt;/li&gt; 
 &lt;li&gt;Distraction-free Zen mode&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;em&gt;Customized themes are synced with your cloud/local session.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;üî• &lt;strong&gt;PWA:&lt;/strong&gt; Install as a &lt;a href="https://web.dev/progressive-web-apps"&gt;Progressive Web App&lt;/a&gt; on your device.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Instant loading with Service Workers&lt;/li&gt; 
 &lt;li&gt;Offline support&lt;/li&gt; 
 &lt;li&gt;Low RAM/memory and CPU usage&lt;/li&gt; 
 &lt;li&gt;Add to Home Screen&lt;/li&gt; 
 &lt;li&gt;Desktop PWA&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;üöÄ &lt;strong&gt;Request:&lt;/strong&gt; Retrieve response from endpoint instantly.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Choose &lt;code&gt;method&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Enter &lt;code&gt;URL&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Send&lt;/li&gt; 
&lt;/ol&gt; 
&lt;ul&gt; 
 &lt;li&gt;Copy/share public "Share URL"&lt;/li&gt; 
 &lt;li&gt;Generate/copy request code snippets for 10+ languages and frameworks&lt;/li&gt; 
 &lt;li&gt;Import &lt;code&gt;cURL&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Label requests&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;üîå &lt;strong&gt;WebSocket:&lt;/strong&gt; Establish full-duplex communication channels over a single TCP connection.&lt;/p&gt; 
&lt;p&gt;üì° &lt;strong&gt;Server-Sent Events:&lt;/strong&gt; Receive a stream of updates from a server over an HTTP connection without resorting to polling.&lt;/p&gt; 
&lt;p&gt;üå© &lt;strong&gt;Socket.IO:&lt;/strong&gt; Send and Receive data with the SocketIO server.&lt;/p&gt; 
&lt;p&gt;ü¶ü &lt;strong&gt;MQTT:&lt;/strong&gt; Subscribe and Publish to topics of an MQTT Broker.&lt;/p&gt; 
&lt;p&gt;üîÆ &lt;strong&gt;GraphQL:&lt;/strong&gt; GraphQL is a query language for APIs and a runtime for fulfilling those queries with your existing data.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Set endpoint and get schema&lt;/li&gt; 
 &lt;li&gt;Multi-column docs&lt;/li&gt; 
 &lt;li&gt;Set custom request headers&lt;/li&gt; 
 &lt;li&gt;Query schema&lt;/li&gt; 
 &lt;li&gt;Get query response&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;üîê &lt;strong&gt;Authorization:&lt;/strong&gt; Allows to identify the end-user.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;None&lt;/li&gt; 
 &lt;li&gt;Basic&lt;/li&gt; 
 &lt;li&gt;Bearer Token&lt;/li&gt; 
 &lt;li&gt;OAuth 2.0&lt;/li&gt; 
 &lt;li&gt;OIDC Access Token/PKCE&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;üì¢ &lt;strong&gt;Headers:&lt;/strong&gt; Describes the format the body of your request is being sent in.&lt;/p&gt; 
&lt;p&gt;üì´ &lt;strong&gt;Parameters:&lt;/strong&gt; Use request parameters to set varying parts in simulated requests.&lt;/p&gt; 
&lt;p&gt;üìÉ &lt;strong&gt;Request Body:&lt;/strong&gt; Used to send and receive data via the REST API.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Set &lt;code&gt;Content Type&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;FormData, JSON, and many more&lt;/li&gt; 
 &lt;li&gt;Toggle between key-value and RAW input parameter list&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;üìÆ &lt;strong&gt;Response:&lt;/strong&gt; Contains the status line, headers, and the message/response body.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Copy the response to the clipboard&lt;/li&gt; 
 &lt;li&gt;Download the response as a file&lt;/li&gt; 
 &lt;li&gt;View response headers&lt;/li&gt; 
 &lt;li&gt;View raw and preview HTML, image, JSON, and XML responses&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;‚è∞ &lt;strong&gt;History:&lt;/strong&gt; Request entries are synced with your cloud/local session storage.&lt;/p&gt; 
&lt;p&gt;üìÅ &lt;strong&gt;Collections:&lt;/strong&gt; Keep your API requests organized with collections and folders. Reuse them with a single click.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Unlimited collections, folders, and requests&lt;/li&gt; 
 &lt;li&gt;Nested folders&lt;/li&gt; 
 &lt;li&gt;Export and import as a file or GitHub gist&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;em&gt;Collections are synced with your cloud/local session storage.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;üìú &lt;strong&gt;Pre-Request Scripts:&lt;/strong&gt; Snippets of code associated with a request that is executed before the request is sent.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Set environment variables&lt;/li&gt; 
 &lt;li&gt;Include timestamp in the request headers&lt;/li&gt; 
 &lt;li&gt;Send a random alphanumeric string in the URL parameters&lt;/li&gt; 
 &lt;li&gt;Any JavaScript functions&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;üë®‚Äçüë©‚Äçüëß‚Äçüë¶ &lt;strong&gt;Teams:&lt;/strong&gt; Helps you collaborate across your teams to design, develop, and test APIs faster.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Create unlimited teams&lt;/li&gt; 
 &lt;li&gt;Create unlimited shared collections&lt;/li&gt; 
 &lt;li&gt;Create unlimited team members&lt;/li&gt; 
 &lt;li&gt;Role-based access control&lt;/li&gt; 
 &lt;li&gt;Cloud sync&lt;/li&gt; 
 &lt;li&gt;Multiple devices&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;üë• &lt;strong&gt;Workspaces:&lt;/strong&gt; Organize your personal and team collections environments into workspaces. Easily switch between workspaces to manage multiple projects.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Create unlimited workspaces&lt;/li&gt; 
 &lt;li&gt;Switch between personal and team workspaces&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;‚å®Ô∏è &lt;strong&gt;Keyboard Shortcuts:&lt;/strong&gt; Optimized for efficiency.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;&lt;a href="https://docs.hoppscotch.io/documentation/features/shortcuts"&gt;Read our documentation on Keyboard Shortcuts&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;üåê &lt;strong&gt;Proxy:&lt;/strong&gt; Enable Proxy Mode from Settings to access blocked APIs.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Hide your IP address&lt;/li&gt; 
 &lt;li&gt;Fixes &lt;a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS"&gt;&lt;code&gt;CORS&lt;/code&gt;&lt;/a&gt; (Cross-Origin Resource Sharing) issues&lt;/li&gt; 
 &lt;li&gt;Access APIs served in non-HTTPS (&lt;code&gt;http://&lt;/code&gt;) endpoints&lt;/li&gt; 
 &lt;li&gt;Use your Proxy URL&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;em&gt;Official proxy server is hosted by Hoppscotch - &lt;strong&gt;&lt;a href="https://github.com/hoppscotch/proxyscotch"&gt;GitHub&lt;/a&gt;&lt;/strong&gt; - &lt;strong&gt;&lt;a href="https://docs.hoppscotch.io/support/privacy"&gt;Privacy Policy&lt;/a&gt;&lt;/strong&gt;.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;üåé &lt;strong&gt;i18n:&lt;/strong&gt; Experience the app in your language.&lt;/p&gt; 
&lt;p&gt;Help us to translate Hoppscotch. Please read &lt;a href="https://raw.githubusercontent.com/hoppscotch/hoppscotch/main/TRANSLATIONS.md"&gt;&lt;code&gt;TRANSLATIONS&lt;/code&gt;&lt;/a&gt; for details on our &lt;a href="https://raw.githubusercontent.com/hoppscotch/hoppscotch/main/CODE_OF_CONDUCT.md"&gt;&lt;code&gt;CODE OF CONDUCT&lt;/code&gt;&lt;/a&gt; and the process for submitting pull requests to us.&lt;/p&gt; 
&lt;p&gt;‚òÅÔ∏è &lt;strong&gt;Auth + Sync:&lt;/strong&gt; Sign in and sync your data in real-time across all your devices.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Sign in with:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;GitHub&lt;/li&gt; 
 &lt;li&gt;Google&lt;/li&gt; 
 &lt;li&gt;Microsoft&lt;/li&gt; 
 &lt;li&gt;Email&lt;/li&gt; 
 &lt;li&gt;SSO (Single Sign-On)[^EE]&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;üîÑ Synchronize your data:&lt;/strong&gt; Handoff to continue tasks on your other devices.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Workspaces&lt;/li&gt; 
 &lt;li&gt;History&lt;/li&gt; 
 &lt;li&gt;Collections&lt;/li&gt; 
 &lt;li&gt;Environments&lt;/li&gt; 
 &lt;li&gt;Settings&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;‚úÖ &lt;strong&gt;Post-Request Tests:&lt;/strong&gt; Write tests associated with a request that is executed after the request's response.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Check the status code as an integer&lt;/li&gt; 
 &lt;li&gt;Filter response headers&lt;/li&gt; 
 &lt;li&gt;Parse the response data&lt;/li&gt; 
 &lt;li&gt;Set environment variables&lt;/li&gt; 
 &lt;li&gt;Write JavaScript code&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;üå± &lt;strong&gt;Environments:&lt;/strong&gt; Environment variables allow you to store and reuse values in your requests and scripts.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Unlimited environments and variables&lt;/li&gt; 
 &lt;li&gt;Initialize through the pre-request script&lt;/li&gt; 
 &lt;li&gt;Export as / import from GitHub gist&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;i&gt;Use-cases&lt;/i&gt;&lt;/summary&gt; 
 &lt;hr /&gt; 
 &lt;ul&gt; 
  &lt;li&gt;By storing a value in a variable, you can reference it throughout your request section&lt;/li&gt; 
  &lt;li&gt;If you need to update the value, you only have to change it in one place&lt;/li&gt; 
  &lt;li&gt;Using variables increases your ability to work efficiently and minimizes the likelihood of error&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;hr /&gt; 
&lt;/details&gt; 
&lt;p&gt;üöö &lt;strong&gt;Bulk Edit:&lt;/strong&gt; Edit key-value pairs in bulk.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Entries are separated by newline&lt;/li&gt; 
 &lt;li&gt;Keys and values are separated by &lt;code&gt;:&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Prepend &lt;code&gt;#&lt;/code&gt; to any row you want to add but keep disabled&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;üéõÔ∏è &lt;strong&gt;Admin dashboard:&lt;/strong&gt; Manage your team and invite members.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Insights&lt;/li&gt; 
 &lt;li&gt;Manage users&lt;/li&gt; 
 &lt;li&gt;Manage teams&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;üì¶ &lt;strong&gt;Add-ons:&lt;/strong&gt; Official add-ons for hoppscotch.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/hoppscotch/hoppscotch/tree/main/packages/hoppscotch-cli"&gt;Hoppscotch CLI&lt;/a&gt;&lt;/strong&gt; - Command-line interface for Hoppscotch.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/hoppscotch/proxyscotch"&gt;Proxy&lt;/a&gt;&lt;/strong&gt; - A simple proxy server created for Hoppscotch.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/hoppscotch/hoppscotch-extension"&gt;Browser Extensions&lt;/a&gt;&lt;/strong&gt; - Browser extensions that enhance your Hoppscotch experience.&lt;/p&gt; &lt;p&gt;&lt;a href="https://addons.mozilla.org/en-US/firefox/addon/hoppscotch"&gt;&lt;img src="https://raw.github.com/alrra/browser-logos/master/src/firefox/firefox_16x16.png" alt="Firefox" /&gt; &lt;strong&gt;Firefox&lt;/strong&gt;&lt;/a&gt; &amp;nbsp;|&amp;nbsp; &lt;a href="https://chrome.google.com/webstore/detail/hoppscotch-extension-for-c/amknoiejhlmhancpahfcfcfhllgkpbld"&gt;&lt;img src="https://raw.github.com/alrra/browser-logos/master/src/chrome/chrome_16x16.png" alt="Chrome" /&gt; &lt;strong&gt;Chrome&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
  &lt;blockquote&gt; 
   &lt;p&gt;&lt;strong&gt;Extensions fix &lt;code&gt;CORS&lt;/code&gt; issues.&lt;/strong&gt;&lt;/p&gt; 
  &lt;/blockquote&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;em&gt;Add-ons are developed and maintained under &lt;strong&gt;&lt;a href="https://github.com/hoppscotch"&gt;Hoppscotch Organization&lt;/a&gt;&lt;/strong&gt;.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;For a complete list of features, please read our &lt;a href="https://docs.hoppscotch.io"&gt;documentation&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;&lt;strong&gt;Demo&lt;/strong&gt;&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Web : &lt;a href="https://hoppscotch.io"&gt;hoppscotch.io&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Windows/Linux/macOS : &lt;a href="https://docs.hoppscotch.io/documentation/clients/desktop#download-hoppscotch-desktop-app"&gt;Desktop Apps&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Provide your API endpoint in the URL field&lt;/li&gt; 
 &lt;li&gt;Click "Send" to simulate the request&lt;/li&gt; 
 &lt;li&gt;View the response&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Developing&lt;/h2&gt; 
&lt;p&gt;Follow our &lt;a href="https://docs.hoppscotch.io/documentation/self-host/getting-started"&gt;self-hosting documentation&lt;/a&gt; to get started with the development environment.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Please contribute using &lt;a href="https://guides.github.com/introduction/flow"&gt;GitHub Flow&lt;/a&gt;. Create a branch, add commits, and &lt;a href="https://github.com/hoppscotch/hoppscotch/compare"&gt;open a pull request&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Please read &lt;a href="https://raw.githubusercontent.com/hoppscotch/hoppscotch/main/CONTRIBUTING.md"&gt;&lt;code&gt;CONTRIBUTING&lt;/code&gt;&lt;/a&gt; for details on our &lt;a href="https://raw.githubusercontent.com/hoppscotch/hoppscotch/main/CODE_OF_CONDUCT.md"&gt;&lt;code&gt;CODE OF CONDUCT&lt;/code&gt;&lt;/a&gt;, and the process for submitting pull requests to us.&lt;/p&gt; 
&lt;h2&gt;Continuous Integration&lt;/h2&gt; 
&lt;p&gt;We use &lt;a href="https://github.com/features/actions"&gt;GitHub Actions&lt;/a&gt; for continuous integration. Check out our &lt;a href="https://github.com/hoppscotch/hoppscotch/actions"&gt;build workflows&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Changelog&lt;/h2&gt; 
&lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/hoppscotch/hoppscotch/main/CHANGELOG.md"&gt;&lt;code&gt;CHANGELOG&lt;/code&gt;&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;h2&gt;Authors&lt;/h2&gt; 
&lt;p&gt;This project owes its existence to the collective efforts of all those who contribute ‚Äî &lt;a href="https://raw.githubusercontent.com/hoppscotch/hoppscotch/main/CONTRIBUTING.md"&gt;contribute now&lt;/a&gt;.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/hoppscotch/hoppscotch/graphs/contributors"&gt; &lt;img src="https://opencollective.com/hoppscotch/contributors.svg?width=840&amp;amp;button=false" alt="Contributors" width="100%" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the &lt;a href="https://opensource.org/licenses/MIT"&gt;MIT License&lt;/a&gt; ‚Äî see the &lt;a href="https://raw.githubusercontent.com/hoppscotch/hoppscotch/main/LICENSE"&gt;&lt;code&gt;LICENSE&lt;/code&gt;&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;p&gt;[^EE]: Enterprise edition feature. &lt;a href="https://docs.hoppscotch.io/documentation/self-host/getting-started"&gt;Learn more&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>allenai/olmocr</title>
      <link>https://github.com/allenai/olmocr</link>
      <description>&lt;p&gt;Toolkit for linearizing PDFs for LLM datasets/training&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img width="350" alt="olmocr-2-full@2x" src="https://github.com/user-attachments/assets/24f1b596-4059-46f1-8130-5d72dcc0b02e" /&gt; 
 &lt;hr /&gt; 
&lt;/div&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/allenai/OLMo/raw/main/LICENSE"&gt; &lt;img alt="GitHub License" src="https://img.shields.io/github/license/allenai/OLMo" /&gt; &lt;/a&gt; &lt;a href="https://github.com/allenai/olmocr/releases"&gt; &lt;img alt="GitHub release" src="https://img.shields.io/github/release/allenai/olmocr.svg?sanitize=true" /&gt; &lt;/a&gt; &lt;a href="https://arxiv.org/abs/2502.18443"&gt; &lt;img alt="Tech Report v1" src="https://img.shields.io/badge/Paper_v1-olmOCR-blue" /&gt; &lt;/a&gt; &lt;a href="https://arxiv.org/abs/2510.19817"&gt; &lt;img alt="Tech Report v2" src="https://img.shields.io/badge/Paper_v2-olmOCR-blue" /&gt; &lt;/a&gt; &lt;a href="https://olmocr.allenai.org"&gt; &lt;img alt="Demo" src="https://img.shields.io/badge/Ai2-Demo-F0529C" /&gt; &lt;/a&gt; &lt;a href="https://discord.gg/sZq3jTNVNG"&gt; &lt;img alt="Discord" src="https://img.shields.io/badge/Discord%20-%20blue?style=flat&amp;amp;logo=discord&amp;amp;label=Ai2&amp;amp;color=%235B65E9" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;A toolkit for converting PDFs and other image-based document formats into clean, readable, plain text format.&lt;/p&gt; 
&lt;p&gt;Try the online demo: &lt;a href="https://olmocr.allenai.org/"&gt;https://olmocr.allenai.org/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Features:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Convert PDF, PNG, and JPEG based documents into clean Markdown&lt;/li&gt; 
 &lt;li&gt;Support for equations, tables, handwriting, and complex formatting&lt;/li&gt; 
 &lt;li&gt;Automatically removes headers and footers&lt;/li&gt; 
 &lt;li&gt;Convert into text with a natural reading order, even in the presence of figures, multi-column layouts, and insets&lt;/li&gt; 
 &lt;li&gt;Efficient, less than $200 USD per million pages converted&lt;/li&gt; 
 &lt;li&gt;(Based on a 7B parameter VLM, so it requires a GPU)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;News&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;October 21, 2025 - v0.4.0 - &lt;a href="https://huggingface.co/allenai/olmOCR-2-7B-1025-FP8"&gt;New model release&lt;/a&gt;, boosts olmOCR-bench score by ~4 points using synthetic data and introduces RL training.&lt;/li&gt; 
 &lt;li&gt;August 13, 2025 - v0.3.0 - &lt;a href="https://huggingface.co/allenai/olmOCR-7B-0825-FP8"&gt;New model release&lt;/a&gt;, fixes auto-rotation detection, and hallucinations on blank documents.&lt;/li&gt; 
 &lt;li&gt;July 24, 2025 - v0.2.1 - &lt;a href="https://huggingface.co/allenai/olmOCR-7B-0725-FP8"&gt;New model release&lt;/a&gt;, scores 3 points higher on &lt;a href="https://github.com/allenai/olmocr/tree/main/olmocr/bench"&gt;olmOCR-Bench&lt;/a&gt;, also runs significantly faster because it's default FP8, and needs much fewer retries per document.&lt;/li&gt; 
 &lt;li&gt;July 23, 2025 - v0.2.0 - New cleaned up &lt;a href="https://github.com/allenai/olmocr/tree/main/olmocr/train"&gt;trainer code&lt;/a&gt;, makes it much simpler to train olmOCR models yourself.&lt;/li&gt; 
 &lt;li&gt;June 17, 2025 - v0.1.75 - Switch from sglang to vllm based inference pipeline, updated docker image to CUDA 12.8.&lt;/li&gt; 
 &lt;li&gt;May 23, 2025 - v0.1.70 - Official docker support and images are now available! &lt;a href="https://raw.githubusercontent.com/allenai/olmocr/main/#using-docker"&gt;See Docker usage&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;May 19, 2025 - v0.1.68 - &lt;a href="https://github.com/allenai/olmocr/tree/main/olmocr/bench"&gt;olmOCR-Bench&lt;/a&gt; launch, scoring 77.4. Launch includes 2 point performance boost in olmOCR pipeline due to bug fixes with prompts.&lt;/li&gt; 
 &lt;li&gt;Mar 17, 2025 - v0.1.60 - Performance improvements due to better temperature selection in sampling.&lt;/li&gt; 
 &lt;li&gt;Feb 25, 2025 - v0.1.58 - Initial public launch and demo.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Benchmark&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/allenai/olmocr/tree/main/olmocr/bench"&gt;&lt;strong&gt;olmOCR-Bench&lt;/strong&gt;&lt;/a&gt;: We also ship a comprehensive benchmark suite covering over 7,000 test cases across 1,400 documents to help measure performance of OCR systems.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th&gt;ArXiv&lt;/th&gt; 
   &lt;th&gt;Old&lt;br /&gt;scans&lt;br /&gt;math&lt;/th&gt; 
   &lt;th&gt;Tables&lt;/th&gt; 
   &lt;th&gt;Old&lt;br /&gt;scans&lt;/th&gt; 
   &lt;th&gt;Headers&lt;br /&gt;&amp;amp;&lt;br /&gt;footers&lt;/th&gt; 
   &lt;th&gt;Multi&lt;br /&gt;column&lt;/th&gt; 
   &lt;th&gt;Long&lt;br /&gt;tiny&lt;br /&gt;text&lt;/th&gt; 
   &lt;th&gt;Base&lt;/th&gt; 
   &lt;th&gt;Overall&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Mistral OCR API&lt;/td&gt; 
   &lt;td&gt;77.2&lt;/td&gt; 
   &lt;td&gt;67.5&lt;/td&gt; 
   &lt;td&gt;60.6&lt;/td&gt; 
   &lt;td&gt;29.3&lt;/td&gt; 
   &lt;td&gt;93.6&lt;/td&gt; 
   &lt;td&gt;71.3&lt;/td&gt; 
   &lt;td&gt;77.1&lt;/td&gt; 
   &lt;td&gt;99.4&lt;/td&gt; 
   &lt;td&gt;72.0¬±1.1&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Marker 1.10.1&lt;/td&gt; 
   &lt;td&gt;83.8&lt;/td&gt; 
   &lt;td&gt;66.8&lt;/td&gt; 
   &lt;td&gt;72.9&lt;/td&gt; 
   &lt;td&gt;33.5&lt;/td&gt; 
   &lt;td&gt;86.6&lt;/td&gt; 
   &lt;td&gt;80.0&lt;/td&gt; 
   &lt;td&gt;85.7&lt;/td&gt; 
   &lt;td&gt;99.3&lt;/td&gt; 
   &lt;td&gt;76.1¬±1.1&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MinerU 2.5.4*&lt;/td&gt; 
   &lt;td&gt;76.6&lt;/td&gt; 
   &lt;td&gt;54.6&lt;/td&gt; 
   &lt;td&gt;84.9&lt;/td&gt; 
   &lt;td&gt;33.7&lt;/td&gt; 
   &lt;td&gt;96.6&lt;/td&gt; 
   &lt;td&gt;78.2&lt;/td&gt; 
   &lt;td&gt;83.5&lt;/td&gt; 
   &lt;td&gt;93.7&lt;/td&gt; 
   &lt;td&gt;75.2¬±1.1&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;DeepSeek-OCR&lt;/td&gt; 
   &lt;td&gt;77.2&lt;/td&gt; 
   &lt;td&gt;73.6&lt;/td&gt; 
   &lt;td&gt;80.2&lt;/td&gt; 
   &lt;td&gt;33.3&lt;/td&gt; 
   &lt;td&gt;96.1&lt;/td&gt; 
   &lt;td&gt;66.4&lt;/td&gt; 
   &lt;td&gt;79.4&lt;/td&gt; 
   &lt;td&gt;99.8&lt;/td&gt; 
   &lt;td&gt;75.7¬±1.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Nanonets-OCR2-3B&lt;/td&gt; 
   &lt;td&gt;75.4&lt;/td&gt; 
   &lt;td&gt;46.1&lt;/td&gt; 
   &lt;td&gt;86.8&lt;/td&gt; 
   &lt;td&gt;40.9&lt;/td&gt; 
   &lt;td&gt;32.1&lt;/td&gt; 
   &lt;td&gt;81.9&lt;/td&gt; 
   &lt;td&gt;93.0&lt;/td&gt; 
   &lt;td&gt;99.6&lt;/td&gt; 
   &lt;td&gt;69.5¬±1.1&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;PaddleOCR-VL*&lt;/td&gt; 
   &lt;td&gt;85.7&lt;/td&gt; 
   &lt;td&gt;71.0&lt;/td&gt; 
   &lt;td&gt;84.1&lt;/td&gt; 
   &lt;td&gt;37.8&lt;/td&gt; 
   &lt;td&gt;97.0&lt;/td&gt; 
   &lt;td&gt;79.9&lt;/td&gt; 
   &lt;td&gt;85.7&lt;/td&gt; 
   &lt;td&gt;98.5&lt;/td&gt; 
   &lt;td&gt;80.0¬±1.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Infinity-Parser 7B*&lt;/td&gt; 
   &lt;td&gt;84.4&lt;/td&gt; 
   &lt;td&gt;83.8&lt;/td&gt; 
   &lt;td&gt;85.0&lt;/td&gt; 
   &lt;td&gt;47.9&lt;/td&gt; 
   &lt;td&gt;88.7&lt;/td&gt; 
   &lt;td&gt;84.2&lt;/td&gt; 
   &lt;td&gt;86.4&lt;/td&gt; 
   &lt;td&gt;99.8&lt;/td&gt; 
   &lt;td&gt;82.5¬±?&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Chandra OCR 0.1.0*&lt;/td&gt; 
   &lt;td&gt;82.2&lt;/td&gt; 
   &lt;td&gt;80.3&lt;/td&gt; 
   &lt;td&gt;88.0&lt;/td&gt; 
   &lt;td&gt;50.4&lt;/td&gt; 
   &lt;td&gt;90.8&lt;/td&gt; 
   &lt;td&gt;81.2&lt;/td&gt; 
   &lt;td&gt;92.3&lt;/td&gt; 
   &lt;td&gt;99.9&lt;/td&gt; 
   &lt;td&gt;83.1¬±0.9&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td colspan="10"&gt;
    &lt;hr /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;olmOCR v0.4.0&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;83.0&lt;/td&gt; 
   &lt;td&gt;82.3&lt;/td&gt; 
   &lt;td&gt;84.9&lt;/td&gt; 
   &lt;td&gt;47.7&lt;/td&gt; 
   &lt;td&gt;96.1&lt;/td&gt; 
   &lt;td&gt;83.7&lt;/td&gt; 
   &lt;td&gt;81.9&lt;/td&gt; 
   &lt;td&gt;99.7&lt;/td&gt; 
   &lt;td&gt;82.4¬±1.1&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;p&gt;Requirements:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Recent NVIDIA GPU (tested on RTX 4090, L40S, A100, H100) with at least 15 GB of GPU RAM&lt;/li&gt; 
 &lt;li&gt;30GB of free disk space&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You will need to install poppler-utils and additional fonts for rendering PDF images.&lt;/p&gt; 
&lt;p&gt;Install dependencies (Ubuntu/Debian)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo apt-get update
sudo apt-get install poppler-utils ttf-mscorefonts-installer msttcorefonts fonts-crosextra-caladea fonts-crosextra-carlito gsfonts lcdf-typetools
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Set up a conda environment and install olmocr. The requirements for running olmOCR are difficult to install in an existing python environment, so please do make a clean python environment to install into.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;conda create -n olmocr python=3.11
conda activate olmocr

# For CPU-only operations, ex running the benchmark
pip install olmocr[bench]

# For actually converting the files with your own GPU
pip install olmocr[gpu]  --extra-index-url https://download.pytorch.org/whl/cu128

# Recommended: Install flash infer for faster inference on GPU
pip install https://download.pytorch.org/whl/cu128/flashinfer/flashinfer_python-0.2.5%2Bcu128torch2.7-cp38-abi3-linux_x86_64.whl
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Local Usage Example&lt;/h3&gt; 
&lt;p&gt;For quick testing, try the &lt;a href="https://olmocr.allen.ai/"&gt;web demo&lt;/a&gt;. To run locally, a GPU is required, as inference is powered by &lt;a href="https://github.com/sgl-project/sglang"&gt;sglang&lt;/a&gt; under the hood.&lt;/p&gt; 
&lt;p&gt;Convert a Single PDF:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Download a sample PDF
curl -o olmocr-sample.pdf https://olmocr.allenai.org/papers/olmocr_3pg_sample.pdf

# Convert it to markdown
python -m olmocr.pipeline ./localworkspace --markdown --pdfs olmocr-sample.pdf
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Convert an Image file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m olmocr.pipeline ./localworkspace --markdown --pdfs random_page.png
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Convert Multiple PDFs:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m olmocr.pipeline ./localworkspace --markdown --pdfs tests/gnarly_pdfs/*.pdf
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;With the addition of the &lt;code&gt;--markdown&lt;/code&gt; flag, results will be stored as markdown files inside of &lt;code&gt;./localworkspace/markdown/&lt;/code&gt;.&lt;/p&gt; 
&lt;h4&gt;Viewing Results&lt;/h4&gt; 
&lt;p&gt;The &lt;code&gt;./localworkspace/&lt;/code&gt; workspace folder will then have both &lt;a href="https://github.com/allenai/dolma"&gt;Dolma&lt;/a&gt; and markdown files (if using &lt;code&gt;--markdown&lt;/code&gt;).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cat localworkspace/markdown/olmocr-sample.md 
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code&gt;olmOCR: Unlocking Trillions of Tokens in PDFs with Vision Language Models
...
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Using an Inference Provider or External Server&lt;/h3&gt; 
&lt;p&gt;If you have a vLLM server already running elsewhere (or any inference platform implementing the OpenAI API), you can point olmOCR to use it instead of spawning a local instance:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Use external vLLM server instead of local one
python -m olmocr.pipeline ./localworkspace --server http://remote-server:8000/v1 --markdown --pdfs tests/gnarly_pdfs/*.pdf
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The served model name should be &lt;code&gt;olmocr&lt;/code&gt;. An example vLLM launch command would be:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;vllm serve allenai/olmOCR-2-7B-1025-FP8 --served-model-name olmocr --max-model-len 16384
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Verified External Providers&lt;/h4&gt; 
&lt;p&gt;We have tested &lt;code&gt;olmOCR-2-7B-1025-FP8&lt;/code&gt; on these external model providers and confirmed that they work&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th&gt;$/1M Input tokens&lt;/th&gt; 
   &lt;th&gt;$/1M Output tokens&lt;/th&gt; 
   &lt;th&gt;Example Command&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://ai2endpoints.cirrascale.ai/models/overview"&gt;Cirrascale&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;$0.07&lt;/td&gt; 
   &lt;td&gt;$0.15&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;python -m olmocr.pipeline ./localworkspace1 --server https://ai2endpoints.cirrascale.ai/api --api_key sk-XXXXXXX --model olmOCR-2-7B-1025 --pdfs tests/gnarly_pdfs/*.pdf&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://deepinfra.com/"&gt;DeepInfra&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;$0.09&lt;/td&gt; 
   &lt;td&gt;$0.19&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;python -m olmocr.pipeline ./localworkspace1 --server https://api.deepinfra.com/v1/openai --api_key DfXXXXXXX --model allenai/olmOCR-2-7B-1025 --pdfs tests/gnarly_pdfs/*.pdf&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.saas.parasail.io/serverless?name=olmocr-7b-1025-fp8"&gt;Parasail&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;$0.10&lt;/td&gt; 
   &lt;td&gt;$0.20&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;python -m olmocr.pipeline ./localworkspace1 --server https://api.parasail.io/v1 --api_key psk-XXXXX --model allenai/olmOCR-2-7B-1025 --pdfs tests/gnarly_pdfs/*.pdf&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Notes on arguments&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--server&lt;/code&gt;: Defines the OpenAI-compatible endpoint: ex &lt;code&gt;https://api.deepinfra.com/v1/openai&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--api_key&lt;/code&gt;: Your API key, bassed in via Authorization Bearer HTTP header&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--pages_per_group&lt;/code&gt;: You may want a smaller number of pages per group as many external provides have lower concurrent request limits&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--model&lt;/code&gt;: The model identifier, ex. &lt;code&gt;allenai/olmOCR-2-7B-1025&lt;/code&gt;, different providers have different names, and if you run locally, you can use &lt;code&gt;olmocr&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Other arguments work the same as with local inference&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Multi-node / Cluster Usage&lt;/h3&gt; 
&lt;p&gt;If you want to convert millions of PDFs, using multiple nodes running in parallel, then olmOCR supports reading your PDFs from AWS S3, and coordinating work using an AWS S3 output bucket.&lt;/p&gt; 
&lt;p&gt;For example, you can start this command on your first worker node, and it will set up a simple work queue in your AWS bucket and start converting PDFs.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m olmocr.pipeline s3://my_s3_bucket/pdfworkspaces/exampleworkspace --pdfs s3://my_s3_bucket/jakep/gnarly_pdfs/*.pdf
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Now on any subsequent nodes, just run this and they will start grabbing items from the same workspace queue.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m olmocr.pipeline s3://my_s3_bucket/pdfworkspaces/exampleworkspace
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you are at Ai2 and want to linearize millions of PDFs efficiently using &lt;a href="https://www.beaker.org"&gt;beaker&lt;/a&gt;, just add the &lt;code&gt;--beaker&lt;/code&gt; flag. This will prepare the workspace on your local machine, and then launch N GPU workers in the cluster to start converting PDFs.&lt;/p&gt; 
&lt;p&gt;For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m olmocr.pipeline s3://my_s3_bucket/pdfworkspaces/exampleworkspace --pdfs s3://my_s3_bucket/jakep/gnarly_pdfs/*.pdf --beaker --beaker_gpus 4
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Using Docker&lt;/h3&gt; 
&lt;p&gt;Pull the Docker image.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker pull alleninstituteforai/olmocr:latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To run the container interactively:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run -it --gpus all --name olmocr_container alleninstituteforai/olmocr:latest /bin/bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you want to access your local files inside the container, use volume mounting:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run -it --gpus all \
  -v /path/to/your/local/files:/local_files \
  --name olmocr_container \
  alleninstituteforai/olmocr:latest /bin/bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;All dependencies are already installed. Once you‚Äôre inside the container, you can run olmOCR commands. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -o olmocr-sample.pdf https://olmocr.allenai.org/papers/olmocr_3pg_sample.pdf

python -m olmocr.pipeline ./localworkspace --markdown --pdfs olmocr-sample.pdf
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;You can also visit our Docker repository on &lt;a href="https://hub.docker.com/r/alleninstituteforai/olmocr"&gt;Docker Hub&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Full documentation for the pipeline&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m olmocr.pipeline --help
usage: pipeline.py [-h] [--pdfs [PDFS ...]] [--model MODEL] [--workspace_profile WORKSPACE_PROFILE] [--pdf_profile PDF_PROFILE] [--pages_per_group PAGES_PER_GROUP] [--max_page_retries MAX_PAGE_RETRIES] [--max_page_error_rate MAX_PAGE_ERROR_RATE] [--workers WORKERS]
                   [--apply_filter] [--stats] [--markdown] [--target_longest_image_dim TARGET_LONGEST_IMAGE_DIM] [--target_anchor_text_len TARGET_ANCHOR_TEXT_LEN] [--guided_decoding] [--gpu-memory-utilization GPU_MEMORY_UTILIZATION] [--max_model_len MAX_MODEL_LEN]
                   [--tensor-parallel-size TENSOR_PARALLEL_SIZE] [--data-parallel-size DATA_PARALLEL_SIZE] [--port PORT] [--server SERVER] [--beaker] [--beaker_workspace BEAKER_WORKSPACE] [--beaker_cluster BEAKER_CLUSTER] [--beaker_gpus BEAKER_GPUS] [--beaker_priority BEAKER_PRIORITY]
                   workspace

Manager for running millions of PDFs through a batch inference pipeline

positional arguments:
  workspace             The filesystem path where work will be stored, can be a local folder, or an s3 path if coordinating work with many workers, s3://bucket/prefix/

options:
  -h, --help            show this help message and exit
  --pdfs [PDFS ...]     Path to add pdfs stored in s3 to the workspace, can be a glob path s3://bucket/prefix/*.pdf or path to file containing list of pdf paths
  --model MODEL         Path where the model is located, allenai/olmOCR-7B-0725-FP8 is the default, can be local, s3, or hugging face.
  --workspace_profile WORKSPACE_PROFILE
                        S3 configuration profile for accessing the workspace
  --pdf_profile PDF_PROFILE
                        S3 configuration profile for accessing the raw pdf documents
  --pages_per_group PAGES_PER_GROUP
                        Aiming for this many pdf pages per work item group
  --max_page_retries MAX_PAGE_RETRIES
                        Max number of times we will retry rendering a page
  --max_page_error_rate MAX_PAGE_ERROR_RATE
                        Rate of allowable failed pages in a document, 1/250 by default
  --workers WORKERS     Number of workers to run at a time
  --apply_filter        Apply basic filtering to English pdfs which are not forms, and not likely seo spam
  --stats               Instead of running any job, reports some statistics about the current workspace
  --markdown            Also write natural text to markdown files preserving the folder structure of the input pdfs
  --target_longest_image_dim TARGET_LONGEST_IMAGE_DIM
                        Dimension on longest side to use for rendering the pdf pages
  --target_anchor_text_len TARGET_ANCHOR_TEXT_LEN
                        Maximum amount of anchor text to use (characters), not used for new models
  --guided_decoding     Enable guided decoding for model YAML type outputs

VLLM arguments:
  --gpu-memory-utilization GPU_MEMORY_UTILIZATION
                        Fraction of VRAM vLLM may pre-allocate for KV-cache (passed through to vllm serve).
  --max_model_len MAX_MODEL_LEN
                        Upper bound (tokens) vLLM will allocate KV-cache for, lower if VLLM won't start
  --tensor-parallel-size TENSOR_PARALLEL_SIZE, -tp TENSOR_PARALLEL_SIZE
                        Tensor parallel size for vLLM
  --data-parallel-size DATA_PARALLEL_SIZE, -dp DATA_PARALLEL_SIZE
                        Data parallel size for vLLM
  --port PORT           Port to use for the VLLM server
  --server SERVER       URL of external vLLM (or other compatible provider)
                        server (e.g., http://hostname:port). If provided,
                        skips spawning local vLLM instance

beaker/cluster execution:
  --beaker              Submit this job to beaker instead of running locally
  --beaker_workspace BEAKER_WORKSPACE
                        Beaker workspace to submit to
  --beaker_cluster BEAKER_CLUSTER
                        Beaker clusters you want to run on
  --beaker_gpus BEAKER_GPUS
                        Number of gpu replicas to run
  --beaker_priority BEAKER_PRIORITY
                        Beaker priority level for the job
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Code overview&lt;/h2&gt; 
&lt;p&gt;There are some nice reusable pieces of the code that may be useful for your own projects:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;A prompting strategy to get really good natural text parsing using ChatGPT 4o - &lt;a href="https://github.com/allenai/olmocr/raw/main/olmocr/data/buildsilver.py"&gt;buildsilver.py&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Basic filtering by language and SEO spam removal - &lt;a href="https://github.com/allenai/olmocr/raw/main/olmocr/filter/filter.py"&gt;filter.py&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;SFT Finetuning code for Qwen2.5-VL - &lt;a href="https://github.com/allenai/olmocr/raw/main/olmocr/train/train.py"&gt;train.py&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;GRPO RL Trainer - &lt;a href="https://github.com/allenai/olmocr/raw/main/olmocr/train/grpo_train.py"&gt;grpo_train.py&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Synthetic data generation - &lt;a href="https://github.com/allenai/olmocr/raw/main/olmocr/bench/synth/mine_html_templates.py"&gt;mine_html_templates.py&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Processing millions of PDFs through a finetuned model using VLLM - &lt;a href="https://github.com/allenai/olmocr/raw/main/olmocr/pipeline.py"&gt;pipeline.py&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Viewing &lt;a href="https://github.com/allenai/dolma"&gt;Dolma docs&lt;/a&gt; created from PDFs - &lt;a href="https://github.com/allenai/olmocr/raw/main/olmocr/viewer/dolmaviewer.py"&gt;dolmaviewer.py&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Team&lt;/h2&gt; 
&lt;!-- start team --&gt; 
&lt;p&gt;&lt;strong&gt;olmOCR&lt;/strong&gt; is developed and maintained by the AllenNLP team, backed by &lt;a href="https://allenai.org/"&gt;the Allen Institute for Artificial Intelligence (AI2)&lt;/a&gt;. AI2 is a non-profit institute with the mission to contribute to humanity through high-impact AI research and engineering. To learn more about who specifically contributed to this codebase, see &lt;a href="https://github.com/allenai/olmocr/graphs/contributors"&gt;our contributors&lt;/a&gt; page.&lt;/p&gt; 
&lt;!-- end team --&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;!-- start license --&gt; 
&lt;p&gt;&lt;strong&gt;olmOCR&lt;/strong&gt; is licensed under &lt;a href="https://www.apache.org/licenses/LICENSE-2.0"&gt;Apache 2.0&lt;/a&gt;. A full copy of the license can be found &lt;a href="https://github.com/allenai/olmocr/raw/main/LICENSE"&gt;on GitHub&lt;/a&gt;.&lt;/p&gt; 
&lt;!-- end license --&gt; 
&lt;h2&gt;Citing&lt;/h2&gt; 
&lt;p&gt;For olmOCR v1 and OlmOCR-bench:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{olmocrbench,
      title={{olmOCR: Unlocking Trillions of Tokens in PDFs with Vision Language Models}},
      author={Jake Poznanski and Jon Borchardt and Jason Dunkelberger and Regan Huff and Daniel Lin and Aman Rangapur and Christopher Wilhelm and Kyle Lo and Luca Soldaini},
      year={2025},
      eprint={2502.18443},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2502.18443},
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For olmOCR v2 Unit Testing Rewards with RL:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{olmocr2,
      title={olmOCR 2: Unit Test Rewards for Document OCR}, 
      author={Jake Poznanski and Luca Soldaini and Kyle Lo},
      year={2025},
      eprint={2510.19817},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2510.19817}, 
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>mountain-loop/yaak</title>
      <link>https://github.com/mountain-loop/yaak</link>
      <description>&lt;p&gt;The most intuitive desktop API client. Organize and execute REST, GraphQL, WebSockets, Server Sent Events, and gRPC ü¶¨&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="https://github.com/JamesIves/github-sponsors-readme-action"&gt; &lt;img width="200px" src="https://github.com/mountain-loop/yaak/raw/main/src-tauri/icons/icon.png" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h1 align="center"&gt; üí´ Yaak ‚ûü Desktop API Client üí´ &lt;/h1&gt; 
&lt;p align="center"&gt; A fast, privacy-first API client for REST, GraphQL, SSE, WebSocket, and gRPC ‚Äì built with Tauri, Rust, and React. &lt;/p&gt; 
&lt;p align="center"&gt; Development is funded by community-purchased &lt;a href="https://yaak.app/pricing"&gt;licenses&lt;/a&gt;. You can also &lt;a href="https://github.com/sponsors/gschier"&gt;become a sponsor&lt;/a&gt; to have your logo appear below. üíñ &lt;/p&gt; 
&lt;br /&gt; 
&lt;p align="center"&gt; 
 &lt;!-- sponsors-premium --&gt;&lt;a href="https://github.com/MVST-Solutions"&gt;&lt;img src="https://github.com/MVST-Solutions.png" width="80px" alt="User avatar: MVST-Solutions" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;&lt;a href="https://github.com/dharsanb"&gt;&lt;img src="https://github.com/dharsanb.png" width="80px" alt="User avatar: dharsanb" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;&lt;a href="https://github.com/railwayapp"&gt;&lt;img src="https://github.com/railwayapp.png" width="80px" alt="User avatar: railwayapp" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;&lt;a href="https://github.com/caseyamcl"&gt;&lt;img src="https://github.com/caseyamcl.png" width="80px" alt="User avatar: caseyamcl" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;&lt;a href="https://github.com/andriyor"&gt;&lt;img src="https://github.com/andriyor.png" width="80px" alt="User avatar: andriyor" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;&lt;a href="https://github.com/"&gt;&lt;img src="https://raw.githubusercontent.com/JamesIves/github-sponsors-readme-action/dev/.github/assets/placeholder.png" width="80px" alt="User avatar: " /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;
 &lt;!-- sponsors-premium --&gt; &lt;/p&gt; 
&lt;p align="center"&gt; 
 &lt;!-- sponsors-base --&gt;&lt;a href="https://github.com/seanwash"&gt;&lt;img src="https://github.com/seanwash.png" width="50px" alt="User avatar: seanwash" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;&lt;a href="https://github.com/jerath"&gt;&lt;img src="https://github.com/jerath.png" width="50px" alt="User avatar: jerath" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;&lt;a href="https://github.com/itsa-sh"&gt;&lt;img src="https://github.com/itsa-sh.png" width="50px" alt="User avatar: itsa-sh" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;&lt;a href="https://github.com/dmmulroy"&gt;&lt;img src="https://github.com/dmmulroy.png" width="50px" alt="User avatar: dmmulroy" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;&lt;a href="https://github.com/timcole"&gt;&lt;img src="https://github.com/timcole.png" width="50px" alt="User avatar: timcole" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;&lt;a href="https://github.com/VLZH"&gt;&lt;img src="https://github.com/VLZH.png" width="50px" alt="User avatar: VLZH" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;&lt;a href="https://github.com/terasaka2k"&gt;&lt;img src="https://github.com/terasaka2k.png" width="50px" alt="User avatar: terasaka2k" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;&lt;a href="https://github.com/majudhu"&gt;&lt;img src="https://github.com/majudhu.png" width="50px" alt="User avatar: majudhu" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;&lt;a href="https://github.com/axelrindle"&gt;&lt;img src="https://github.com/axelrindle.png" width="50px" alt="User avatar: axelrindle" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;&lt;a href="https://github.com/jirizverina"&gt;&lt;img src="https://github.com/jirizverina.png" width="50px" alt="User avatar: jirizverina" /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;
 &lt;!-- sponsors-base --&gt; &lt;/p&gt; 
&lt;p&gt;&lt;img src="https://yaak.app/static/screenshot.png" alt="Yaak API Client" /&gt;&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;p&gt;Yaak is an offline-first API client designed to stay out of your way while giving you everything you need when you need it. Built with &lt;a href="https://tauri.app"&gt;Tauri&lt;/a&gt;, Rust, and React, it‚Äôs fast, lightweight, and private. No telemetry, no VC funding, and no cloud lock-in.&lt;/p&gt; 
&lt;h3&gt;üåê Work with any API&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Import collections from Postman, Insomnia, OpenAPI, Swagger, or Curl.&lt;/li&gt; 
 &lt;li&gt;Send requests via REST, GraphQL, gRPC, WebSocket, or Server-Sent Events.&lt;/li&gt; 
 &lt;li&gt;Filter and inspect responses with JSONPath or XPath.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üîê Stay secure&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Use OAuth 2.0, JWT, Basic Auth, or custom plugins for authentication.&lt;/li&gt; 
 &lt;li&gt;Secure sensitive values with encrypted secrets.&lt;/li&gt; 
 &lt;li&gt;Store secrets in your OS keychain.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;‚òÅÔ∏è Organize &amp;amp; collaborate&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Group requests into workspaces and nested folders.&lt;/li&gt; 
 &lt;li&gt;Use environment variables to switch between dev, staging, and prod.&lt;/li&gt; 
 &lt;li&gt;Mirror workspaces to your filesystem for versioning in Git or syncing with Dropbox.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üß© Extend &amp;amp; customize&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Insert dynamic values like UUIDs or timestamps with template tags.&lt;/li&gt; 
 &lt;li&gt;Pick from built-in themes or build your own.&lt;/li&gt; 
 &lt;li&gt;Create plugins to extend authentication, template tags, or the UI.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contribution Policy&lt;/h2&gt; 
&lt;p&gt;Yaak is open source but only accepting contributions for bug fixes. To get started, visit &lt;a href="https://raw.githubusercontent.com/mountain-loop/yaak/main/DEVELOPMENT.md"&gt;&lt;code&gt;DEVELOPMENT.md&lt;/code&gt;&lt;/a&gt; for tips on setting up your environment.&lt;/p&gt; 
&lt;h2&gt;Useful Resources&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://feedback.yaak.app"&gt;Feedback and Bug Reports&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://feedback.yaak.app/help"&gt;Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://yaak.app/alternatives/postman"&gt;Yaak vs Postman&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://yaak.app/alternatives/bruno"&gt;Yaak vs Bruno&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://yaak.app/alternatives/insomnia"&gt;Yaak vs Insomnia&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>LadybirdBrowser/ladybird</title>
      <link>https://github.com/LadybirdBrowser/ladybird</link>
      <description>&lt;p&gt;Truly independent web browser&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Ladybird&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://ladybird.org"&gt;Ladybird&lt;/a&gt; is a truly independent web browser, using a novel engine based on web standards.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] Ladybird is in a pre-alpha state, and only suitable for use by developers&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;p&gt;We aim to build a complete, usable browser for the modern web.&lt;/p&gt; 
&lt;p&gt;Ladybird uses a multi-process architecture with a main UI process, several WebContent renderer processes, an ImageDecoder process, and a RequestServer process.&lt;/p&gt; 
&lt;p&gt;Image decoding and network connections are done out of process to be more robust against malicious content. Each tab has its own renderer process, which is sandboxed from the rest of the system.&lt;/p&gt; 
&lt;p&gt;At the moment, many core library support components are inherited from SerenityOS:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;LibWeb: Web rendering engine&lt;/li&gt; 
 &lt;li&gt;LibJS: JavaScript engine&lt;/li&gt; 
 &lt;li&gt;LibWasm: WebAssembly implementation&lt;/li&gt; 
 &lt;li&gt;LibCrypto/LibTLS: Cryptography primitives and Transport Layer Security&lt;/li&gt; 
 &lt;li&gt;LibHTTP: HTTP/1.1 client&lt;/li&gt; 
 &lt;li&gt;LibGfx: 2D Graphics Library, Image Decoding and Rendering&lt;/li&gt; 
 &lt;li&gt;LibUnicode: Unicode and locale support&lt;/li&gt; 
 &lt;li&gt;LibMedia: Audio and video playback&lt;/li&gt; 
 &lt;li&gt;LibCore: Event loop, OS abstraction layer&lt;/li&gt; 
 &lt;li&gt;LibIPC: Inter-process communication&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;How do I build and run this?&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/LadybirdBrowser/ladybird/master/Documentation/BuildInstructionsLadybird.md"&gt;build instructions&lt;/a&gt; for information on how to build Ladybird.&lt;/p&gt; 
&lt;p&gt;Ladybird runs on Linux, macOS, Windows (with WSL2), and many other *Nixes.&lt;/p&gt; 
&lt;h2&gt;How do I read the documentation?&lt;/h2&gt; 
&lt;p&gt;Code-related documentation can be found in the &lt;a href="https://raw.githubusercontent.com/LadybirdBrowser/ladybird/master/Documentation/"&gt;documentation&lt;/a&gt; folder.&lt;/p&gt; 
&lt;h2&gt;Get in touch and participate!&lt;/h2&gt; 
&lt;p&gt;Join &lt;a href="https://discord.gg/nvfjVJ4Svh"&gt;our Discord server&lt;/a&gt; to participate in development discussion.&lt;/p&gt; 
&lt;p&gt;Please read &lt;a href="https://raw.githubusercontent.com/LadybirdBrowser/ladybird/master/Documentation/GettingStartedContributing.md"&gt;Getting started contributing&lt;/a&gt; if you plan to contribute to Ladybird for the first time.&lt;/p&gt; 
&lt;p&gt;Before opening an issue, please see the &lt;a href="https://raw.githubusercontent.com/LadybirdBrowser/ladybird/master/CONTRIBUTING.md#issue-policy"&gt;issue policy&lt;/a&gt; and the &lt;a href="https://raw.githubusercontent.com/LadybirdBrowser/ladybird/master/ISSUES.md"&gt;detailed issue-reporting guidelines&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The full contribution guidelines can be found in &lt;a href="https://raw.githubusercontent.com/LadybirdBrowser/ladybird/master/CONTRIBUTING.md"&gt;&lt;code&gt;CONTRIBUTING.md&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Ladybird is licensed under a 2-clause BSD license.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>yeongpin/cursor-free-vip</title>
      <link>https://github.com/yeongpin/cursor-free-vip</link>
      <description>&lt;p&gt;[Support 0.49.x]ÔºàReset Cursor AI MachineID &amp; Bypass Higher Token LimitÔºâ Cursor Ai ÔºåËá™Âä®ÈáçÁΩÆÊú∫Âô®ID Ôºå ÂÖçË¥πÂçáÁ∫ß‰ΩøÁî®ProÂäüËÉΩ: You've reached your trial request limit. / Too many free trial accounts used on this machine. Please upgrade to pro. We have this limit in place to prevent abuse. Please let us know if you believe this is a mistake.&lt;/p&gt;&lt;hr&gt;&lt;div align="center" markdown="1"&gt; 
 &lt;sup&gt;Special thanks to:&lt;/sup&gt; 
 &lt;br /&gt; 
 &lt;br /&gt; 
 &lt;a href="http://go.warp.dev/cursor-free-vip"&gt; &lt;img alt="Warp sponsorship" width="400" src="https://github.com/user-attachments/assets/ab8dd143-b0fd-4904-bdc5-dd7ecac94eae" /&gt; &lt;/a&gt; 
 &lt;h3&gt;&lt;a href="http://go.warp.dev/cursor-free-vip"&gt;Warp, built for coding with multiple agents.&lt;/a&gt;&lt;/h3&gt; 
 &lt;p&gt;&lt;a href="http://go.warp.dev/cursor-free-vip"&gt;Available for MacOS, Linux, &amp;amp; Windows&lt;/a&gt;&lt;br /&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h1&gt;‚û§ Cursor Free VIP&lt;/h1&gt; 
&lt;div align="center"&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/yeongpin/cursor-free-vip/main/images/logo.png" alt="Cursor Pro Logo" width="200" style="border-radius: 6px;" /&gt; &lt;/p&gt; 
 &lt;p align="center"&gt; &lt;/p&gt;
 &lt;p&gt;&lt;a href="https://github.com/yeongpin/cursor-free-vip/releases/latest"&gt;&lt;img src="https://img.shields.io/endpoint?url=https://api.pinstudios.net/api/badges/release/yeongpin/cursor-free-vip" alt="Release" /&gt;&lt;/a&gt; &lt;a href="https://creativecommons.org/licenses/by-nc-nd/4.0/"&gt;&lt;img src="https://img.shields.io/badge/License-CC_BY--NC--ND_4.0-lightgrey.svg?sanitize=true" alt="License: CC BY-NC-ND 4.0" /&gt;&lt;/a&gt; &lt;a href="https://github.com/yeongpin/cursor-free-vip/stargazers"&gt;&lt;img src="https://img.shields.io/endpoint?url=https://api.pinstudios.net/api/badges/stars/yeongpin/cursor-free-vip" alt="Stars" /&gt;&lt;/a&gt; &lt;a href="https://github.com/yeongpin/cursor-free-vip/releases/latest"&gt;&lt;img src="https://img.shields.io/endpoint?url=https://api.pinstudios.net/api/badges/downloads/yeongpin/cursor-free-vip/total" alt="Downloads" /&gt;&lt;/a&gt; &lt;a href="https://buymeacoffee.com/yeongpin" target="_blank"&gt;&lt;img alt="Buy Me a Coffee" src="https://img.shields.io/badge/Buy%20Me%20a%20Coffee-Support%20Me-FFDA33" /&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/yeongpin/cursor-free-vip"&gt;&lt;img src="https://devin.ai/assets/deepwiki-badge.png" alt="Ask DeepWiki.com" height="20" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/13425" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/13425" alt="yeongpin%2Fcursor-free-vip | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; &lt;br /&gt;&lt;/p&gt; 
 &lt;h4&gt;Support Latest 0.49.x Version | ÊîØÊåÅÊúÄÊñ∞ 0.49.x ÁâàÊú¨&lt;/h4&gt; 
 &lt;p&gt;This tool is for educational purposes, currently the repo does not violate any laws. Please support the original project. This tool will not generate any fake email accounts and OAuth access.&lt;/p&gt; 
 &lt;p&gt;Supports Windows, macOS and Linux.&lt;/p&gt; 
 &lt;p&gt;For optimal performance, run with privileges and always stay up to date.&lt;/p&gt; 
 &lt;p&gt;ÈÄôÊòØ‰∏ÄÊ¨æÁî®ÊñºÂ≠∏ÁøíÂíåÁ†îÁ©∂ÁöÑÂ∑•ÂÖ∑ÔºåÁõÆÂâç repo Ê≤íÊúâÈÅïÂèç‰ªª‰ΩïÊ≥ïÂæã„ÄÇË´ãÊîØÊåÅÂéü‰ΩúËÄÖ„ÄÇ ÈÄôÊ¨æÂ∑•ÂÖ∑‰∏çÊúÉÁîüÊàê‰ªª‰ΩïÂÅáÁöÑÈõªÂ≠êÈÉµ‰ª∂Â∏≥Êà∂Âíå OAuth Ë®™Âïè„ÄÇ&lt;/p&gt; 
 &lt;p&gt;ÊîØÊåÅ Windows„ÄÅmacOS Âíå Linux„ÄÇ&lt;/p&gt; 
 &lt;p&gt;Â∞çÊñºÊúÄ‰Ω≥ÊÄßËÉΩÔºåË´ã‰ª•ÁÆ°ÁêÜÂì°Ë∫´‰ªΩÈÅãË°å‰∏¶ÂßãÁµÇ‰øùÊåÅÊúÄÊñ∞„ÄÇ&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/yeongpin/cursor-free-vip/main/images/product_2025-04-16_10-40-21.png" alt="new" width="800" style="border-radius: 6px;" /&gt;&lt;br /&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;üîÑ Change Log | Êõ¥Êñ∞Êó•Âøó&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/yeongpin/cursor-free-vip/main/CHANGELOG.md"&gt;Watch Change Log | Êü•ÁúãÊõ¥Êñ∞Êó•Âøó&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;‚ú® Features | ÂäüËÉΩÁâπÈªû&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Support Windows macOS and Linux systems&lt;br /&gt;ÊîØÊåÅ Windows„ÄÅmacOS Âíå Linux Á≥ªÁµ±&lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Reset Cursor's configuration&lt;br /&gt;ÈáçÁΩÆ Cursor ÁöÑÈÖçÁΩÆ&lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Multi-language support (English, ÁÆÄ‰Ωì‰∏≠Êñá, ÁπÅÈ´î‰∏≠Êñá, Vietnamese)&lt;br /&gt;Â§öË™ûË®ÄÊîØÊåÅÔºàËã±Êñá„ÄÅÁÆÄ‰Ωì‰∏≠Êñá„ÄÅÁπÅÈ´î‰∏≠Êñá„ÄÅË∂äÂçóË™ûÔºâ&lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üíª System Support | Á≥ªÁµ±ÊîØÊåÅ&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Operating System&lt;/th&gt; 
   &lt;th&gt;Architecture&lt;/th&gt; 
   &lt;th&gt;Supported&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Windows&lt;/td&gt; 
   &lt;td&gt;x64, x86&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;macOS&lt;/td&gt; 
   &lt;td&gt;Intel, Apple Silicon&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Linux&lt;/td&gt; 
   &lt;td&gt;x64, x86, ARM64&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;üëÄ How to use | Â¶Ç‰Ωï‰ΩøÁî®&lt;/h2&gt; 
&lt;details open&gt; 
 &lt;summary&gt;&lt;b&gt;‚≠ê Auto Run Script | ËÖ≥Êú¨Ëá™ÂãïÂåñÈÅãË°å&lt;/b&gt;&lt;/summary&gt; 
 &lt;h3&gt;&lt;strong&gt;Linux/macOS&lt;/strong&gt;&lt;/h3&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;curl -fsSL https://raw.githubusercontent.com/yeongpin/cursor-free-vip/main/scripts/install.sh -o install.sh &amp;amp;&amp;amp; chmod +x install.sh &amp;amp;&amp;amp; ./install.sh
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;&lt;strong&gt;Archlinux&lt;/strong&gt;&lt;/h3&gt; 
 &lt;p&gt;Install via &lt;a href="https://aur.archlinux.org/packages/cursor-free-vip-git"&gt;AUR&lt;/a&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;yay -S cursor-free-vip-git
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;&lt;strong&gt;Windows&lt;/strong&gt;&lt;/h3&gt; 
 &lt;pre&gt;&lt;code class="language-powershell"&gt;irm https://raw.githubusercontent.com/yeongpin/cursor-free-vip/main/scripts/install.ps1 | iex
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p&gt;If you want to stop the script, please press Ctrl+C&lt;br /&gt;Ë¶ÅÂÅúÊ≠¢ËÖ≥Êú¨ÔºåË´ãÊåâ Ctrl+C&lt;/p&gt; 
&lt;h2&gt;‚ùó Note | Ê≥®ÊÑè‰∫ãÈ†Ö&lt;/h2&gt; 
&lt;p&gt;üìù Config | Êñá‰ª∂ÈÖçÁΩÆ &lt;code&gt;Win / Macos / Linux Path | Ë∑ØÂæë [Documents/.cursor-free-vip/config.ini]&lt;/code&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;‚≠ê Config | Êñá‰ª∂ÈÖçÁΩÆ&lt;/b&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code&gt;[Chrome]
# Default Google Chrome Path | ÈªòË™çGoogle Chrome ÈÅäË¶ΩÂô®Ë∑ØÂæë
chromepath = C:\Program Files\Google/Chrome/Application/chrome.exe

[Turnstile]
# Handle Turnstile Wait Time | Á≠âÂæÖ‰∫∫Ê©üÈ©óË≠âÊôÇÈñì
handle_turnstile_time = 2
# Handle Turnstile Wait Random Time (must merge 1-3 or 1,3) | Á≠âÂæÖ‰∫∫Ê©üÈ©óË≠âÈö®Ê©üÊôÇÈñìÔºàÂøÖÈ†àÊòØ 1-3 ÊàñËÄÖ 1,3 ÈÄôÊ®£ÁöÑÁµÑÂêàÔºâ
handle_turnstile_random_time = 1-3

[OSPaths]
# Storage Path | Â≠òÂÑ≤Ë∑ØÂæë
storage_path = /Users/username/Library/Application Support/Cursor/User/globalStorage/storage.json
# SQLite Path | SQLiteË∑ØÂæë
sqlite_path = /Users/username/Library/Application Support/Cursor/User/globalStorage/state.vscdb
# Machine ID Path | Ê©üÂô®IDË∑ØÂæë
machine_id_path = /Users/username/Library/Application Support/Cursor/machineId
# For Linux users: ~/.config/cursor/machineid

[Timing]
# Min Random Time | ÊúÄÂ∞èÈö®Ê©üÊôÇÈñì
min_random_time = 0.1
# Max Random Time | ÊúÄÂ§ßÈö®Ê©üÊôÇÈñì
max_random_time = 0.8
# Page Load Wait | È†ÅÈù¢Âä†ËºâÁ≠âÂæÖÊôÇÈñì
page_load_wait = 0.1-0.8
# Input Wait | Ëº∏ÂÖ•Á≠âÂæÖÊôÇÈñì
input_wait = 0.3-0.8
# Submit Wait | Êèê‰∫§Á≠âÂæÖÊôÇÈñì
submit_wait = 0.5-1.5
# Verification Code Input | È©óË≠âÁ¢ºËº∏ÂÖ•Á≠âÂæÖÊôÇÈñì
verification_code_input = 0.1-0.3
# Verification Success Wait | È©óË≠âÊàêÂäüÁ≠âÂæÖÊôÇÈñì
verification_success_wait = 2-3
# Verification Retry Wait | È©óË≠âÈáçË©¶Á≠âÂæÖÊôÇÈñì
verification_retry_wait = 2-3
# Email Check Initial Wait | ÈÉµ‰ª∂Ê™¢Êü•ÂàùÂßãÁ≠âÂæÖÊôÇÈñì
email_check_initial_wait = 4-6
# Email Refresh Wait | ÈÉµ‰ª∂Âà∑Êñ∞Á≠âÂæÖÊôÇÈñì
email_refresh_wait = 2-4
# Settings Page Load Wait | Ë®≠ÁΩÆÈ†ÅÈù¢Âä†ËºâÁ≠âÂæÖÊôÇÈñì
settings_page_load_wait = 1-2
# Failed Retry Time | Â§±ÊïóÈáçË©¶ÊôÇÈñì
failed_retry_time = 0.5-1
# Retry Interval | ÈáçË©¶ÈñìÈöî
retry_interval = 8-12
# Max Timeout | ÊúÄÂ§ßË∂ÖÊôÇÊôÇÈñì
max_timeout = 160

[Utils]
# Check Update | Ê™¢Êü•Êõ¥Êñ∞
check_update = True
# Show Account Info | È°ØÁ§∫Ë≥¨Ëôü‰ø°ÊÅØ
show_account_info = True

[TempMailPlus]
# Enable TempMailPlus | ÂïìÁî® TempMailPlusÔºà‰ªª‰ΩïËΩâÁôºÂà∞TempMailPlusÁöÑÈÉµ‰ª∂ÈÉΩÊîØÊåÅÁç≤ÂèñÈ©óË≠âÁ¢ºÔºå‰æãÂ¶ÇcloudflareÈÉµ‰ª∂Catch-allÔºâ
enabled = false
# TempMailPlus Email | TempMailPlus ÈõªÂ≠êÈÉµ‰ª∂
email = xxxxx@mailto.plus
# TempMailPlus pin | TempMailPlus pinÁ¢º
epin = 

[WindowsPaths]
storage_path = C:\Users\yeongpin\AppData\Roaming\Cursor\User\globalStorage\storage.json
sqlite_path = C:\Users\yeongpin\AppData\Roaming\Cursor\User\globalStorage\state.vscdb
machine_id_path = C:\Users\yeongpin\AppData\Roaming\Cursor\machineId
cursor_path = C:\Users\yeongpin\AppData\Local\Programs\Cursor\resources\app
updater_path = C:\Users\yeongpin\AppData\Local\cursor-updater
update_yml_path = C:\Users\yeongpin\AppData\Local\Programs\Cursor\resources\app-update.yml
product_json_path = C:\Users\yeongpin\AppData\Local\Programs\Cursor\resources\app\product.json

[Browser]
default_browser = opera
chrome_path = C:\Program Files\Google\Chrome\Application\chrome.exe
edge_path = C:\Program Files (x86)\Microsoft\Edge\Application\msedge.exe
firefox_path = C:\Program Files\Mozilla Firefox\firefox.exe
brave_path = C:\Program Files\BraveSoftware/Brave-Browser/Application/brave.exe
chrome_driver_path = D:\VisualCode\cursor-free-vip-new\drivers\chromedriver.exe
edge_driver_path = D:\VisualCode\cursor-free-vip-new\drivers\msedgedriver.exe
firefox_driver_path = D:\VisualCode\cursor-free-vip-new\drivers\geckodriver.exe
brave_driver_path = D:\VisualCode\cursor-free-vip-new\drivers\chromedriver.exe
opera_path = C:\Users\yeongpin\AppData\Local\Programs\Opera\opera.exe
opera_driver_path = D:\VisualCode\cursor-free-vip-new\drivers\chromedriver.exe

[OAuth]
show_selection_alert = False
timeout = 120
max_attempts = 3
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Use administrator privileges to run the script &lt;br /&gt;Ë´ã‰ΩøÁî®ÁÆ°ÁêÜÂì°Ë∫´‰ªΩÈÅãË°åËÖ≥Êú¨&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Confirm that Cursor is closed before running the script &lt;br /&gt;Ë´ãÁ¢∫‰øùÂú®ÈÅãË°åËÖ≥Êú¨ÂâçÂ∑≤Á∂ìÈóúÈñâ Cursor&lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;This tool is only for learning and research purposes &lt;br /&gt;Ê≠§Â∑•ÂÖ∑ÂÉÖ‰æõÂ≠∏ÁøíÂíåÁ†îÁ©∂‰ΩøÁî®&lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Please comply with the relevant software usage terms when using this tool &lt;br /&gt;‰ΩøÁî®Êú¨Â∑•ÂÖ∑ÊôÇË´ãÈÅµÂÆàÁõ∏ÈóúËªü‰ª∂‰ΩøÁî®Ê¢ùÊ¨æ&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üö® Common Issues | Â∏∏Ë¶ãÂïèÈ°å&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;Â¶ÇÊûúÈÅáÂà∞Ê¨äÈôêÂïèÈ°åÔºåË´ãÁ¢∫‰øùÔºö&lt;/th&gt; 
   &lt;th align="center"&gt;Ê≠§ËÖ≥Êú¨‰ª•ÁÆ°ÁêÜÂì°Ë∫´‰ªΩÈÅãË°å&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;If you encounter permission issues, please ensure:&lt;/td&gt; 
   &lt;td align="center"&gt;This script is run with administrator privileges&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Error 'User is not authorized'&lt;/td&gt; 
   &lt;td align="center"&gt;This means your account was banned for using temporary (disposal) mail. Ensure using a non-temporary mail service&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;ü§© Contribution | Ë≤¢Áçª&lt;/h2&gt; 
&lt;p&gt;Ê≠°ËøéÊèê‰∫§ Issue Âíå Pull RequestÔºÅ&lt;/p&gt; 
&lt;a href="https://github.com/yeongpin/cursor-free-vip/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=yeongpin/cursor-free-vip&amp;amp;preview=true&amp;amp;max=&amp;amp;columns=" /&gt; &lt;/a&gt; 
&lt;br /&gt;
&lt;br /&gt; 
&lt;h2&gt;üì© Disclaimer | ÂÖçË≤¨ËÅ≤Êòé&lt;/h2&gt; 
&lt;p&gt;Êú¨Â∑•ÂÖ∑ÂÉÖ‰æõÂ≠∏ÁøíÂíåÁ†îÁ©∂‰ΩøÁî®Ôºå‰ΩøÁî®Êú¨Â∑•ÂÖ∑ÊâÄÁî¢ÁîüÁöÑ‰ªª‰ΩïÂæåÊûúÁî±‰ΩøÁî®ËÄÖËá™Ë°åÊâøÊìî„ÄÇ &lt;br /&gt;&lt;/p&gt; 
&lt;p&gt;This tool is only for learning and research purposes, and any consequences arising from the use of this tool are borne by the user.&lt;/p&gt; 
&lt;h2&gt;üí∞ Buy Me a Coffee | Ë´ãÊàëÂñùÊùØÂíñÂï°&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td&gt; &lt;img src="https://raw.githubusercontent.com/yeongpin/cursor-free-vip/main/images/provi-code.jpg" alt="buy_me_a_coffee" width="280" /&gt;&lt;br /&gt; &lt;/td&gt; 
    &lt;td&gt; &lt;img src="https://raw.githubusercontent.com/yeongpin/cursor-free-vip/main/images/paypal.png" alt="buy_me_a_coffee" width="280" /&gt;&lt;br /&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h2&gt;‚≠ê Star History | ÊòüÊòüÊï∏&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://star-history.com/#yeongpin/cursor-free-vip&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=yeongpin/cursor-free-vip&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;üìù License | ÊéàÊ¨ä&lt;/h2&gt; 
&lt;p&gt;Êú¨È†ÖÁõÆÊé°Áî® &lt;a href="https://creativecommons.org/licenses/by-nc-nd/4.0/"&gt;CC BY-NC-ND 4.0&lt;/a&gt; ÊéàÊ¨ä„ÄÇ Please refer to the &lt;a href="https://raw.githubusercontent.com/yeongpin/cursor-free-vip/main/LICENSE.md"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>minio/minio</title>
      <link>https://github.com/minio/minio</link>
      <description>&lt;p&gt;MinIO is a high-performance, S3 compatible object store, open sourced under GNU AGPLv3 license.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MinIO Quickstart Guide&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://slack.min.io"&gt;&lt;img src="https://slack.min.io/slack?type=svg" alt="Slack" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/minio/minio/"&gt;&lt;img src="https://img.shields.io/docker/pulls/minio/minio.svg?maxAge=604800" alt="Docker Pulls" /&gt;&lt;/a&gt; &lt;a href="https://github.com/minio/minio/raw/master/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/license-AGPL%20V3-blue" alt="license" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://min.io"&gt;&lt;img src="https://raw.githubusercontent.com/minio/minio/master/.github/logo.svg?sanitize=true" alt="MinIO" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;MinIO is a high-performance, S3-compatible object storage solution released under the GNU AGPL v3.0 license. Designed for speed and scalability, it powers AI/ML, analytics, and data-intensive workloads with industry-leading performance.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;S3 API Compatible ‚Äì Seamless integration with existing S3 tools&lt;/li&gt; 
 &lt;li&gt;Built for AI &amp;amp; Analytics ‚Äì Optimized for large-scale data pipelines&lt;/li&gt; 
 &lt;li&gt;High Performance ‚Äì Ideal for demanding storage workloads.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This README provides instructions for building MinIO from source and deploying onto baremetal hardware. Use the &lt;a href="https://github.com/minio/docs"&gt;MinIO Documentation&lt;/a&gt; project to build and host a local copy of the documentation.&lt;/p&gt; 
&lt;h2&gt;MinIO is Open Source Software&lt;/h2&gt; 
&lt;p&gt;We designed MinIO as Open Source software for the Open Source software community. We encourage the community to remix, redesign, and reshare MinIO under the terms of the AGPLv3 license.&lt;/p&gt; 
&lt;p&gt;All usage of MinIO in your application stack requires validation against AGPLv3 obligations, which include but are not limited to the release of modified code to the community from which you have benefited. Any commercial/proprietary usage of the AGPLv3 software, including repackaging or reselling services/features, is done at your own risk.&lt;/p&gt; 
&lt;p&gt;The AGPLv3 provides no obligation by any party to support, maintain, or warranty the original or any modified work. All support is provided on a best-effort basis through Github and our &lt;a href="https://raw.githubusercontent.com/minio/minio/master/https//slack.min.io"&gt;Slack&lt;/a&gt; channel, and any member of the community is welcome to contribute and assist others in their usage of the software.&lt;/p&gt; 
&lt;p&gt;MinIO &lt;a href="https://www.min.io/product/aistor"&gt;AIStor&lt;/a&gt; includes enterprise-grade support and licensing for workloads which require commercial or proprietary usage and production-level SLA/SLO-backed support. For more information, &lt;a href="https://min.io/pricing"&gt;reach out for a quote&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Source-Only Distribution&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Important:&lt;/strong&gt; The MinIO community edition is now distributed as source code only. We will no longer provide pre-compiled binary releases for the community version.&lt;/p&gt; 
&lt;h3&gt;Installing Latest MinIO Community Edition&lt;/h3&gt; 
&lt;p&gt;To use MinIO community edition, you have two options:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Install from source&lt;/strong&gt; using &lt;code&gt;go install github.com/minio/minio@latest&lt;/code&gt; (recommended)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Build a Docker image&lt;/strong&gt; from the provided Dockerfile&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;See the sections below for detailed instructions on each method.&lt;/p&gt; 
&lt;h3&gt;Legacy Binary Releases&lt;/h3&gt; 
&lt;p&gt;Historical pre-compiled binary releases remain available for reference but are no longer maintained:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;GitHub Releases: &lt;a href="https://github.com/minio/minio/releases"&gt;https://github.com/minio/minio/releases&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Direct downloads: &lt;a href="https://dl.min.io/server/minio/release/"&gt;https://dl.min.io/server/minio/release/&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;These legacy binaries will not receive updates.&lt;/strong&gt; We strongly recommend using source builds for access to the latest features, bug fixes, and security updates.&lt;/p&gt; 
&lt;h2&gt;Install from Source&lt;/h2&gt; 
&lt;p&gt;Use the following commands to compile and run a standalone MinIO server from source. If you do not have a working Golang environment, please follow &lt;a href="https://golang.org/doc/install"&gt;How to install Golang&lt;/a&gt;. Minimum version required is &lt;a href="https://golang.org/dl/#stable"&gt;go1.24&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;go install github.com/minio/minio@latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can alternatively run &lt;code&gt;go build&lt;/code&gt; and use the &lt;code&gt;GOOS&lt;/code&gt; and &lt;code&gt;GOARCH&lt;/code&gt; environment variables to control the OS and architecture target. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;env GOOS=linux GOARCh=arm64 go build
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Start MinIO by running &lt;code&gt;minio server PATH&lt;/code&gt; where &lt;code&gt;PATH&lt;/code&gt; is any empty folder on your local filesystem.&lt;/p&gt; 
&lt;p&gt;The MinIO deployment starts using default root credentials &lt;code&gt;minioadmin:minioadmin&lt;/code&gt;. You can test the deployment using the MinIO Console, an embedded web-based object browser built into MinIO Server. Point a web browser running on the host machine to &lt;a href="http://127.0.0.1:9000"&gt;http://127.0.0.1:9000&lt;/a&gt; and log in with the root credentials. You can use the Browser to create buckets, upload objects, and browse the contents of the MinIO server.&lt;/p&gt; 
&lt;p&gt;You can also connect using any S3-compatible tool, such as the MinIO Client &lt;code&gt;mc&lt;/code&gt; commandline tool:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;mc alias set local http://localhost:9000 minioadmin minioadmin
mc admin info local
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/minio/minio/master/#test-using-minio-client-mc"&gt;Test using MinIO Client &lt;code&gt;mc&lt;/code&gt;&lt;/a&gt; for more information on using the &lt;code&gt;mc&lt;/code&gt; commandline tool. For application developers, see &lt;a href="https://docs.min.io/enterprise/aistor-object-store/developers/sdk/"&gt;https://docs.min.io/enterprise/aistor-object-store/developers/sdk/&lt;/a&gt; to view MinIO SDKs for supported languages.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Production environments using compiled-from-source MinIO binaries do so at their own risk. The AGPLv3 license provides no warranties nor liabilites for any such usage.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Build Docker Image&lt;/h2&gt; 
&lt;p&gt;You can use the &lt;code&gt;docker build .&lt;/code&gt; command to build a Docker image on your local host machine. You must first &lt;a href="https://raw.githubusercontent.com/minio/minio/master/#install-from-source"&gt;build MinIO&lt;/a&gt; and ensure the &lt;code&gt;minio&lt;/code&gt; binary exists in the project root.&lt;/p&gt; 
&lt;p&gt;The following command builds the Docker image using the default &lt;code&gt;Dockerfile&lt;/code&gt; in the root project directory with the repository and image tag &lt;code&gt;myminio:minio&lt;/code&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;docker build -t myminio:minio .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Use &lt;code&gt;docker image ls&lt;/code&gt; to confirm the image exists in your local repository. You can run the server using standard Docker invocation:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;docker run -p 9000:9000 -p 9001:9001 myminio:minio server /tmp/minio --console-address :9001
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Complete documentation for building Docker containers, managing custom images, or loading images into orchestration platforms is out of scope for this documentation. You can modify the &lt;code&gt;Dockerfile&lt;/code&gt; and &lt;code&gt;dockerscripts/docker-entrypoint.sh&lt;/code&gt; as-needed to reflect your specific image requirements.&lt;/p&gt; 
&lt;p&gt;See the &lt;a href="https://docs.min.io/community/minio-object-store/operations/deployments/baremetal-deploy-minio-as-a-container.html#deploy-minio-container"&gt;MinIO Container&lt;/a&gt; documentation for more guidance on running MinIO within a Container image.&lt;/p&gt; 
&lt;h2&gt;Install using Helm Charts&lt;/h2&gt; 
&lt;p&gt;There are two paths for installing MinIO onto Kubernetes infrastructure:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Use the &lt;a href="https://github.com/minio/operator"&gt;MinIO Operator&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Use the community-maintained &lt;a href="https://github.com/minio/minio/tree/master/helm/minio"&gt;Helm charts&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See the &lt;a href="https://docs.min.io/community/minio-object-store/operations/deployments/kubernetes.html"&gt;MinIO Documentation&lt;/a&gt; for guidance on deploying using the Operator. The Community Helm chart has instructions in the folder-level README.&lt;/p&gt; 
&lt;h2&gt;Test MinIO Connectivity&lt;/h2&gt; 
&lt;h3&gt;Test using MinIO Console&lt;/h3&gt; 
&lt;p&gt;MinIO Server comes with an embedded web based object browser. Point your web browser to &lt;a href="http://127.0.0.1:9000"&gt;http://127.0.0.1:9000&lt;/a&gt; to ensure your server has started successfully.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] MinIO runs console on random port by default, if you wish to choose a specific port use &lt;code&gt;--console-address&lt;/code&gt; to pick a specific interface and port.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Test using MinIO Client &lt;code&gt;mc&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;mc&lt;/code&gt; provides a modern alternative to UNIX commands like ls, cat, cp, mirror, diff etc. It supports filesystems and Amazon S3 compatible cloud storage services.&lt;/p&gt; 
&lt;p&gt;The following commands set a local alias, validate the server information, create a bucket, copy data to that bucket, and list the contents of the bucket.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;mc alias set local http://localhost:9000 minioadmin minioadmin
mc admin info
mc mb data
mc cp ~/Downloads/mydata data/
mc ls data/
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Follow the MinIO Client &lt;a href="https://docs.min.io/community/minio-object-store/reference/minio-mc.html#quickstart"&gt;Quickstart Guide&lt;/a&gt; for further instructions.&lt;/p&gt; 
&lt;h2&gt;Explore Further&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.min.io/community/minio-object-store/index.html"&gt;The MinIO documentation website&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.min.io/community/minio-object-store/operations/concepts/erasure-coding.html"&gt;MinIO Erasure Code Overview&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.min.io/community/minio-object-store/reference/minio-mc.html"&gt;Use &lt;code&gt;mc&lt;/code&gt; with MinIO Server&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.min.io/enterprise/aistor-object-store/developers/sdk/go/"&gt;Use &lt;code&gt;minio-go&lt;/code&gt; SDK with MinIO Server&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contribute to MinIO Project&lt;/h2&gt; 
&lt;p&gt;Please follow MinIO &lt;a href="https://github.com/minio/minio/raw/master/CONTRIBUTING.md"&gt;Contributor's Guide&lt;/a&gt; for guidance on making new contributions to the repository.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;MinIO source is licensed under the &lt;a href="https://github.com/minio/minio/raw/master/LICENSE"&gt;GNU AGPLv3&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;MinIO &lt;a href="https://github.com/minio/minio/tree/master/docs"&gt;documentation&lt;/a&gt; is licensed under &lt;a href="https://creativecommons.org/licenses/by/4.0/"&gt;CC BY 4.0&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/minio/minio/raw/master/COMPLIANCE.md"&gt;License Compliance&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>DrewThomasson/ebook2audiobook</title>
      <link>https://github.com/DrewThomasson/ebook2audiobook</link>
      <description>&lt;p&gt;Generate audiobooks from e-books, voice cloning &amp; 1107+ languages!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;üìö ebook2audiobook&lt;/h1&gt; 
&lt;p&gt;CPU/GPU Converter from eBooks to audiobooks with chapters and metadata&lt;br /&gt; using XTTSv2, Bark, Vits, Fairseq, YourTTS, Tacotron and more. Supports voice cloning and +1110 languages!&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] &lt;strong&gt;This tool is intended for use with non-DRM, legally acquired eBooks only.&lt;/strong&gt; &lt;br /&gt; The authors are not responsible for any misuse of this software or any resulting legal consequences. &lt;br /&gt; Use this tool responsibly and in accordance with all applicable laws.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;a href="https://discord.gg/63Tv3F65k6"&gt;&lt;img src="https://dcbadge.limes.pink/api/server/https://discord.gg/63Tv3F65k6" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Thanks to support ebook2audiobook developers!&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://ko-fi.com/athomasson2"&gt;&lt;img src="https://img.shields.io/badge/Ko--fi-F16061?style=for-the-badge&amp;amp;logo=ko-fi&amp;amp;logoColor=white" alt="Ko-Fi" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Run locally&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#launching-gradio-web-interface"&gt;&lt;img src="https://img.shields.io/badge/Quick%20Start-blue?style=for-the-badge" alt="Quick Start" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/DrewThomasson/ebook2audiobook/actions/workflows/Docker-Build.yml"&gt;&lt;img src="https://github.com/DrewThomasson/ebook2audiobook/actions/workflows/Docker-Build.yml/badge.svg?sanitize=true" alt="Docker Build" /&gt;&lt;/a&gt; &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/releases/latest"&gt;&lt;img src="https://img.shields.io/badge/Download-Now-blue.svg?sanitize=true" alt="Download" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;a href="https://github.com/DrewThomasson/ebook2audiobook"&gt; &lt;img src="https://img.shields.io/badge/Platform-mac%20|%20linux%20|%20windows-lightgrey" alt="Platform" /&gt; &lt;/a&gt;
&lt;a href="https://hub.docker.com/r/athomasson2/ebook2audiobook"&gt; &lt;img alt="Docker Pull Count" src="https://img.shields.io/docker/pulls/athomasson2/ebook2audiobook.svg?sanitize=true" /&gt; &lt;/a&gt; 
&lt;h3&gt;Run Remotely&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/spaces/drewThomasson/ebook2audiobook"&gt;&lt;img src="https://img.shields.io/badge/Hugging%20Face-Spaces-yellow?style=flat&amp;amp;logo=huggingface" alt="Hugging Face" /&gt;&lt;/a&gt; &lt;a href="https://colab.research.google.com/github/DrewThomasson/ebook2audiobook/blob/main/Notebooks/colab_ebook2audiobook.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Free Google Colab" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Rihcus/ebook2audiobookXTTS/raw/main/Notebooks/kaggle-ebook2audiobook.ipynb"&gt;&lt;img src="https://img.shields.io/badge/Kaggle-035a7d?style=flat&amp;amp;logo=kaggle&amp;amp;logoColor=white" alt="Kaggle" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;GUI Interface&lt;/h4&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/assets/demo_web_gui.gif" alt="demo_web_gui" /&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click to see images of Web GUI&lt;/summary&gt; 
 &lt;img width="1728" alt="GUI Screen 1" src="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/assets/gui_1.png" /&gt; 
 &lt;img width="1728" alt="GUI Screen 2" src="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/assets/gui_2.png" /&gt; 
 &lt;img width="1728" alt="GUI Screen 3" src="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/assets/gui_3.png" /&gt; 
&lt;/details&gt; 
&lt;h2&gt;Demos&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;New Default Voice Demo&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/750035dc-e355-46f1-9286-05c1d9e88cea"&gt;https://github.com/user-attachments/assets/750035dc-e355-46f1-9286-05c1d9e88cea&lt;/a&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;More Demos&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;ASMR Voice&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/68eee9a1-6f71-4903-aacd-47397e47e422"&gt;https://github.com/user-attachments/assets/68eee9a1-6f71-4903-aacd-47397e47e422&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Rainy Day Voice&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/d25034d9-c77f-43a9-8f14-0d167172b080"&gt;https://github.com/user-attachments/assets/d25034d9-c77f-43a9-8f14-0d167172b080&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Scarlett Voice&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/b12009ee-ec0d-45ce-a1ef-b3a52b9f8693"&gt;https://github.com/user-attachments/assets/b12009ee-ec0d-45ce-a1ef-b3a52b9f8693&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;David Attenborough Voice&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/81c4baad-117e-4db5-ac86-efc2b7fea921"&gt;https://github.com/user-attachments/assets/81c4baad-117e-4db5-ac86-efc2b7fea921&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://github.com/DrewThomasson/VoxNovel/raw/dc5197dff97252fa44c391dc0596902d71278a88/readme_files/example_in_app.jpeg" alt="Example" /&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;README.md&lt;/h2&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#-ebook2audiobook"&gt;ebook2audiobook&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#features"&gt;Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#gui-interface"&gt;GUI Interface&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#demos"&gt;Demos&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#supported-languages"&gt;Supported Languages&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#hardware-requirements"&gt;Minimum Requirements&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#launching-gradio-web-interface"&gt;Usage&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#launching-gradio-web-interface"&gt;Run Locally&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#launching-gradio-web-interface"&gt;Launching Gradio Web Interface&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#basic--usage"&gt;Basic Headless Usage&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#example-of-custom-model-zip-upload"&gt;Headless Custom XTTS Model Usage&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#help-command-output"&gt;Help command output&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#run-remotely"&gt;Run Remotely&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#fine-tuned-tts-models"&gt;Fine Tuned TTS models&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#fine-tuned-tts-collection"&gt;Collection of Fine-Tuned TTS Models&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#fine-tune-your-own-xttsv2-model"&gt;Train XTTSv2&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#docker-gpu-options"&gt;Docker&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#docker-gpu-options"&gt;GPU options&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#running-the-pre-built-docker-container"&gt;Docker Run&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#building-the-docker-container"&gt;Docker Build&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#docker-compose"&gt;Docker Compose&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#docker-headless-guide"&gt;Docker headless guide&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#docker-container-file-locations"&gt;Docker container file locations&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#common-docker-issues"&gt;Common Docker issues&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#supported-ebook-formats"&gt;Supported eBook Formats&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#output-formats"&gt;Output Formats&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#updating-to-latest-version"&gt;Updating to Latest Version&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#reverting-to-older-versions"&gt;Revert to older Version&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#common-issues"&gt;Common Issues&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#special-thanks"&gt;Special Thanks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#table-of-contents"&gt;Table of Contents&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìö Splits eBook into chapters for organized audio.&lt;/li&gt; 
 &lt;li&gt;üéôÔ∏è High-quality text-to-speech with &lt;a href="https://huggingface.co/coqui/XTTS-v2"&gt;Coqui XTTSv2&lt;/a&gt; and &lt;a href="https://github.com/facebookresearch/fairseq/tree/main/examples/mms"&gt;Fairseq&lt;/a&gt; (and more).&lt;/li&gt; 
 &lt;li&gt;üó£Ô∏è Optional voice cloning with your own voice file.&lt;/li&gt; 
 &lt;li&gt;üåç Supports +1110 languages (English by default). &lt;a href="https://dl.fbaipublicfiles.com/mms/tts/all-tts-languages.html"&gt;List of Supported languages&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üñ•Ô∏è Designed to run on 4GB RAM.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Supported Languages&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;&lt;strong&gt;Arabic (ar)&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;strong&gt;Chinese (zh)&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;strong&gt;English (en)&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;strong&gt;Spanish (es)&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;French (fr)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;German (de)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Italian (it)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Portuguese (pt)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Polish (pl)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Turkish (tr)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Russian (ru)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Dutch (nl)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Czech (cs)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Japanese (ja)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Hindi (hi)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Bengali (bn)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Hungarian (hu)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Korean (ko)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Vietnamese (vi)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Swedish (sv)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Persian (fa)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Yoruba (yo)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Swahili (sw)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Indonesian (id)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Slovak (sk)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Croatian (hr)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Tamil (ta)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;strong&gt;Danish (da)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://dl.fbaipublicfiles.com/mms/tts/all-tts-languages.html"&gt;&lt;strong&gt;+1100 languages and dialects here&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Hardware Requirements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;4gb RAM minimum, 8GB recommended&lt;/li&gt; 
 &lt;li&gt;Virtualization enabled if running on windows (Docker only)&lt;/li&gt; 
 &lt;li&gt;CPU (intel, AMD, ARM), GPU (Nvidia, AMD*, Intel*) (Recommended), MPS (Apple Silicon CPU) *available very soon&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] &lt;strong&gt;Before to post an install or bug issue search carefully to the opened and closed issues TAB&lt;br /&gt; to be sure your issue does not exist already.&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] &lt;strong&gt;Lacking of any standards structure like what is a chapter, paragraph, preface etc.&lt;br /&gt; you should first remove manually any text you don't want to be converted in audio.&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Installation Instructions&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Clone repo&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/DrewThomasson/ebook2audiobook.git
cd ebook2audiobook
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Launching Gradio Web Interface&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Run ebook2audiobook&lt;/strong&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Linux/MacOS&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;./ebook2audiobook.sh  # Run launch script
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Mac Launcher&lt;/strong&gt;&lt;br /&gt; Double click &lt;code&gt;Mac Ebook2Audiobook Launcher.command&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Windows&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;ebook2audiobook.cmd  # Run launch script or double click on it
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Windows Launcher&lt;/strong&gt;&lt;br /&gt; Double click &lt;code&gt;ebook2audiobook.cmd&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Manual Python Install&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# (for experts only!)
REQUIRED_PROGRAMS=("calibre" "ffmpeg" "nodejs" "mecab" "espeak-ng" "rust" "sox")
REQUIRED_PYTHON_VERSION="3.12"
pip install -r requirements.txt  # Install Python Requirements
python app.py  # Run Ebook2Audiobook
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Open the Web App&lt;/strong&gt;: Click the URL provided in the terminal to access the web app and convert eBooks. &lt;code&gt;http://localhost:7860/&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;For Public Link&lt;/strong&gt;: &lt;code&gt;python app.py --share&lt;/code&gt; (all OS) &lt;code&gt;./ebook2audiobook.sh --share&lt;/code&gt; (Linux/MacOS) &lt;code&gt;ebook2audiobook.cmd --share&lt;/code&gt; (Windows)&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] &lt;strong&gt;If the script is stopped and run again, you need to refresh your gradio GUI interface&lt;br /&gt; to let the web page reconnect to the new connection socket.&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Basic Usage&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Linux/MacOS&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;./ebook2audiobook.sh --headless --ebook &amp;lt;path_to_ebook_file&amp;gt; \
    --voice [path_to_voice_file] --language [language_code]
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Windows&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;ebook2audiobook.cmd --headless --ebook &amp;lt;path_to_ebook_file&amp;gt;
    --voice [path_to_voice_file] --language [language_code]
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;[--ebook]&lt;/strong&gt;: Path to your eBook file&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;[--voice]&lt;/strong&gt;: Voice cloning file path (optional)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;[--language]&lt;/strong&gt;: Language code in ISO-639-3 (i.e.: ita for italian, eng for english, deu for german...).&lt;br /&gt; Default language is eng and --language is optional for default language set in ./lib/lang.py.&lt;br /&gt; The ISO-639-1 2 letters codes are also supported.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Example of Custom Model Zip Upload&lt;/h3&gt; 
&lt;p&gt;(must be a .zip file containing the mandatory model files. Example for XTTSv2: config.json, model.pth, vocab.json and ref.wav)&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Linux/MacOS&lt;/strong&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;./ebook2audiobook.sh --headless --ebook &amp;lt;ebook_file_path&amp;gt; \
    --voice &amp;lt;target_voice_file_path&amp;gt; --language &amp;lt;language&amp;gt; --custom_model &amp;lt;custom_model_path&amp;gt;
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;ebook2audiobook.cmd --headless --ebook &amp;lt;ebook_file_path&amp;gt; \
    --voice &amp;lt;target_voice_file_path&amp;gt; --language &amp;lt;language&amp;gt; --custom_model &amp;lt;custom_model_path&amp;gt;
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&amp;lt;custom_model_path&amp;gt;&lt;/strong&gt;: Path to &lt;code&gt;model_name.zip&lt;/code&gt; file, which must contain (according to the tts engine) all the mandatory files&lt;br /&gt; (see ./lib/models.py).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;For Detailed Guide with list of all Parameters to use&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Linux/MacOS&lt;/strong&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;./ebook2audiobook.sh --help
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Windows&lt;/strong&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;ebook2audiobook.cmd --help
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Or for all OS&lt;/strong&gt; &lt;code&gt;python app.py --help &lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a id="help-command-output"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;usage: app.py [-h] [--session SESSION] [--share] [--headless] [--ebook EBOOK]
              [--ebooks_dir EBOOKS_DIR] [--language LANGUAGE] [--voice VOICE]
              [--device {cpu,gpu,mps}]
              [--tts_engine {XTTSv2,BARK,VITS,FAIRSEQ,TACOTRON2,YOURTTS,xtts,bark,vits,fairseq,tacotron,yourtts}]
              [--custom_model CUSTOM_MODEL] [--fine_tuned FINE_TUNED]
              [--output_format OUTPUT_FORMAT] [--temperature TEMPERATURE]
              [--length_penalty LENGTH_PENALTY] [--num_beams NUM_BEAMS]
              [--repetition_penalty REPETITION_PENALTY] [--top_k TOP_K]
              [--top_p TOP_P] [--speed SPEED] [--enable_text_splitting]
              [--text_temp TEXT_TEMP] [--waveform_temp WAVEFORM_TEMP]
              [--output_dir OUTPUT_DIR] [--version]

Convert eBooks to Audiobooks using a Text-to-Speech model. You can either launch the Gradio interface or run the script in headless mode for direct conversion.

options:
  -h, --help            show this help message and exit
  --session SESSION     Session to resume the conversion in case of interruption, crash, 
                            or reuse of custom models and custom cloning voices.

**** The following options are for all modes:
  Optional

**** The following option are for gradio/gui mode only:
  Optional

  --share               Enable a public shareable Gradio link.

**** The following options are for --headless mode only:
  --headless            Run the script in headless mode
  --ebook EBOOK         Path to the ebook file for conversion. Cannot be used when --ebooks_dir is present.
  --ebooks_dir EBOOKS_DIR
                        Relative or absolute path of the directory containing the files to convert. 
                            Cannot be used when --ebook is present.
  --language LANGUAGE   Language of the e-book. Default language is set 
                            in ./lib/lang.py sed as default if not present. All compatible language codes are in ./lib/lang.py

optional parameters:
  --voice VOICE         (Optional) Path to the voice cloning file for TTS engine. 
                            Uses the default voice if not present.
  --device {cpu,gpu,mps}
                        (Optional) Pprocessor unit type for the conversion. 
                            Default is set in ./lib/conf.py if not present. Fall back to CPU if GPU not available.
  --tts_engine {XTTSv2,BARK,VITS,FAIRSEQ,TACOTRON2,YOURTTS,xtts,bark,vits,fairseq,tacotron,yourtts}
                        (Optional) Preferred TTS engine (available are: ['XTTSv2', 'BARK', 'VITS', 'FAIRSEQ', 'TACOTRON2', 'YOURTTS', 'xtts', 'bark', 'vits', 'fairseq', 'tacotron', 'yourtts'].
                            Default depends on the selected language. The tts engine should be compatible with the chosen language
  --custom_model CUSTOM_MODEL
                        (Optional) Path to the custom model zip file cntaining mandatory model files. 
                            Please refer to ./lib/models.py
  --fine_tuned FINE_TUNED
                        (Optional) Fine tuned model path. Default is builtin model.
  --output_format OUTPUT_FORMAT
                        (Optional) Output audio format. Default is set in ./lib/conf.py
  --temperature TEMPERATURE
                        (xtts only, optional) Temperature for the model. 
                            Default to config.json model. Higher temperatures lead to more creative outputs.
  --length_penalty LENGTH_PENALTY
                        (xtts only, optional) A length penalty applied to the autoregressive decoder. 
                            Default to config.json model. Not applied to custom models.
  --num_beams NUM_BEAMS
                        (xtts only, optional) Controls how many alternative sequences the model explores. Must be equal or greater than length penalty. 
                            Default to config.json model.
  --repetition_penalty REPETITION_PENALTY
                        (xtts only, optional) A penalty that prevents the autoregressive decoder from repeating itself. 
                            Default to config.json model.
  --top_k TOP_K         (xtts only, optional) Top-k sampling. 
                            Lower values mean more likely outputs and increased audio generation speed. 
                            Default to config.json model.
  --top_p TOP_P         (xtts only, optional) Top-p sampling. 
                            Lower values mean more likely outputs and increased audio generation speed. Default to config.json model.
  --speed SPEED         (xtts only, optional) Speed factor for the speech generation. 
                            Default to config.json model.
  --enable_text_splitting
                        (xtts only, optional) Enable TTS text splitting. This option is known to not be very efficient. 
                            Default to config.json model.
  --text_temp TEXT_TEMP
                        (bark only, optional) Text Temperature for the model. 
                            Default to 0.85. Higher temperatures lead to more creative outputs.
  --waveform_temp WAVEFORM_TEMP
                        (bark only, optional) Waveform Temperature for the model. 
                            Default to 0.5. Higher temperatures lead to more creative outputs.
  --output_dir OUTPUT_DIR
                        (Optional) Path to the output directory. Default is set in ./lib/conf.py
  --version             Show the version of the script and exit

Example usage:    
Windows:
    Gradio/GUI:
    ebook2audiobook.cmd
    Headless mode:
    ebook2audiobook.cmd --headless --ebook '/path/to/file'
Linux/Mac:
    Gradio/GUI:
    ./ebook2audiobook.sh
    Headless mode:
    ./ebook2audiobook.sh --headless --ebook '/path/to/file'
    
Tip: to add of silence (1.4 seconds) into your text just use "###" or "[pause]".

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;NOTE: in gradio/gui mode, to cancel a running conversion, just click on the [X] from the ebook upload component.&lt;/p&gt; 
&lt;p&gt;TIP: if it needs some more pauses, just add '###' or '[pause]' between the words you wish more pause. one [pause] equals to 1.4 seconds&lt;/p&gt; 
&lt;h4&gt;Docker GPU Options&lt;/h4&gt; 
&lt;p&gt;Available pre-build tags: &lt;code&gt;latest&lt;/code&gt; (CUDA 11.8)&lt;/p&gt; 
&lt;h4&gt;Edit: IF GPU isn't detected then you'll have to build the image -&amp;gt; &lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#building-the-docker-container"&gt;Building the Docker Container&lt;/a&gt;&lt;/h4&gt; 
&lt;h4&gt;Running the pre-built Docker Container&lt;/h4&gt; 
&lt;p&gt;-Run with CPU only&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-powershell"&gt;docker run --pull always --rm -p 7860:7860 athomasson2/ebook2audiobook
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;-Run with GPU Speedup (NVIDIA compatible only)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-powershell"&gt;docker run --pull always --rm --gpus all -p 7860:7860 athomasson2/ebook2audiobook
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This command will start the Gradio interface on port 7860.(localhost:7860)&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;For more options add the parameter &lt;code&gt;--help&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Building the Docker Container&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;You can build the docker image with the command:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-powershell"&gt;docker build -t athomasson2/ebook2audiobook .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Avalible Docker Build Arguments&lt;/h4&gt; 
&lt;p&gt;&lt;code&gt;--build-arg TORCH_VERSION=cuda118&lt;/code&gt; Available tags: [cuda121, cuda118, cuda128, rocm, xpu, cpu]&lt;/p&gt; 
&lt;p&gt;All CUDA version numbers should work, Ex: CUDA 11.6-&amp;gt; cuda116&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;--build-arg SKIP_XTTS_TEST=true&lt;/code&gt; (Saves space by not baking XTTSv2 model into docker image)&lt;/p&gt; 
&lt;h2&gt;Docker container file locations&lt;/h2&gt; 
&lt;p&gt;All ebook2audiobooks will have the base dir of &lt;code&gt;/app/&lt;/code&gt; For example: &lt;code&gt;tmp&lt;/code&gt; = &lt;code&gt;/app/tmp&lt;/code&gt; &lt;code&gt;audiobooks&lt;/code&gt; = &lt;code&gt;/app/audiobooks&lt;/code&gt;&lt;/p&gt; 
&lt;h2&gt;Docker headless guide&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Before you do run this you need to create a dir named "input-folder" in your current dir which will be linked, This is where you can put your input files for the docker image to see&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;mkdir input-folder &amp;amp;&amp;amp; mkdir Audiobooks
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;In the command below swap out &lt;strong&gt;YOUR_INPUT_FILE.TXT&lt;/strong&gt; with the name of your input file&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run --pull always --rm \
    -v $(pwd)/input-folder:/app/input_folder \
    -v $(pwd)/audiobooks:/app/audiobooks \
    athomasson2/ebook2audiobook \
    --headless --ebook /input_folder/YOUR_EBOOK_FILE
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;The output Audiobooks will be found in the Audiobook folder which will also be located in your local dir you ran this docker command in&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;To get the help command for the other parameters this program has you can run this&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run --pull always --rm athomasson2/ebook2audiobook --help

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;That will output this &lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/#help-command-output"&gt;Help command output&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Docker Compose&lt;/h3&gt; 
&lt;p&gt;This project uses Docker Compose to run locally. You can enable or disable GPU support by setting either &lt;code&gt;*gpu-enabled&lt;/code&gt; or &lt;code&gt;*gpu-disabled&lt;/code&gt; in &lt;code&gt;docker-compose.yml&lt;/code&gt;&lt;/p&gt; 
&lt;h4&gt;Steps to Run&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Clone the Repository&lt;/strong&gt; (if you haven't already): &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/DrewThomasson/ebook2audiobook.git
cd ebook2audiobook
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Set GPU Support (disabled by default)&lt;/strong&gt; To enable GPU support, modify &lt;code&gt;docker-compose.yml&lt;/code&gt; and change &lt;code&gt;*gpu-disabled&lt;/code&gt; to &lt;code&gt;*gpu-enabled&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Start the service:&lt;/strong&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# Docker
docker-compose up -d # To update add --build

# Podman
podman compose -f podman-compose.yml up -d # To update add --build
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Access the service:&lt;/strong&gt; The service will be available at &lt;a href="http://localhost:7860"&gt;http://localhost:7860&lt;/a&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Common Docker Issues&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;My NVIDIA GPU isnt being detected?? -&amp;gt; &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/wiki/GPU-ISSUES"&gt;GPU ISSUES Wiki Page&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;python: can't open file '/home/user/app/app.py': [Errno 2] No such file or directory&lt;/code&gt; (Just remove all post arguments as I replaced the &lt;code&gt;CMD&lt;/code&gt; with &lt;code&gt;ENTRYPOINT&lt;/code&gt; in the &lt;a href="https://raw.githubusercontent.com/DrewThomasson/ebook2audiobook/main/Dockerfile"&gt;Dockerfile&lt;/a&gt;)&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Example: &lt;code&gt;docker run --pull always athomasson2/ebook2audiobook app.py --script_mode full_docker&lt;/code&gt; - &amp;gt; corrected - &amp;gt; &lt;code&gt;docker run --pull always athomasson2/ebook2audiobook&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Arguments can be easily added like this now &lt;code&gt;docker run --pull always athomasson2/ebook2audiobook --share&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Docker gets stuck downloading Fine-Tuned models. (This does not happen for every computer but some appear to run into this issue) Disabling the progress bar appears to fix the issue, as discussed &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/issues/191"&gt;here in #191&lt;/a&gt; Example of adding this fix in the &lt;code&gt;docker run&lt;/code&gt; command&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-Dockerfile"&gt;docker run --pull always --rm --gpus all -e HF_HUB_DISABLE_PROGRESS_BARS=1 -e HF_HUB_ENABLE_HF_TRANSFER=0 \
    -p 7860:7860 athomasson2/ebook2audiobook
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Fine Tuned TTS models&lt;/h2&gt; 
&lt;h4&gt;Fine Tune your own XTTSv2 model&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/spaces/drewThomasson/xtts-finetune-webui-gpu"&gt;&lt;img src="https://img.shields.io/badge/Hugging%20Face-Spaces-yellow?style=flat&amp;amp;logo=huggingface" alt="Hugging Face" /&gt;&lt;/a&gt; &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/raw/v25/Notebooks/finetune/xtts/kaggle-xtts-finetune-webui-gradio-gui.ipynb"&gt;&lt;img src="https://img.shields.io/badge/Kaggle-035a7d?style=flat&amp;amp;logo=kaggle&amp;amp;logoColor=white" alt="Kaggle" /&gt;&lt;/a&gt; &lt;a href="https://colab.research.google.com/github/DrewThomasson/ebook2audiobook/blob/v25/Notebooks/finetune/xtts/colab_xtts_finetune_webui.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;De-noise training data&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/spaces/drewThomasson/DeepFilterNet2_no_limit"&gt;&lt;img src="https://img.shields.io/badge/Hugging%20Face-Spaces-yellow?style=flat&amp;amp;logo=huggingface" alt="Hugging Face" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Rikorose/DeepFilterNet"&gt;&lt;img src="https://img.shields.io/badge/DeepFilterNet-181717?logo=github" alt="GitHub Repo" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Fine Tuned TTS Collection&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/drewThomasson/fineTunedTTSModels/tree/main"&gt;&lt;img src="https://img.shields.io/badge/Hugging%20Face-Models-yellow?style=flat&amp;amp;logo=huggingface" alt="Hugging Face" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;For an XTTSv2 custom model a ref audio clip of the voice reference is mandatory:&lt;/p&gt; 
&lt;h2&gt;Supported eBook Formats&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;.epub&lt;/code&gt;, &lt;code&gt;.pdf&lt;/code&gt;, &lt;code&gt;.mobi&lt;/code&gt;, &lt;code&gt;.txt&lt;/code&gt;, &lt;code&gt;.html&lt;/code&gt;, &lt;code&gt;.rtf&lt;/code&gt;, &lt;code&gt;.chm&lt;/code&gt;, &lt;code&gt;.lit&lt;/code&gt;, &lt;code&gt;.pdb&lt;/code&gt;, &lt;code&gt;.fb2&lt;/code&gt;, &lt;code&gt;.odt&lt;/code&gt;, &lt;code&gt;.cbr&lt;/code&gt;, &lt;code&gt;.cbz&lt;/code&gt;, &lt;code&gt;.prc&lt;/code&gt;, &lt;code&gt;.lrf&lt;/code&gt;, &lt;code&gt;.pml&lt;/code&gt;, &lt;code&gt;.snb&lt;/code&gt;, &lt;code&gt;.cbc&lt;/code&gt;, &lt;code&gt;.rb&lt;/code&gt;, &lt;code&gt;.tcr&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Best results&lt;/strong&gt;: &lt;code&gt;.epub&lt;/code&gt; or &lt;code&gt;.mobi&lt;/code&gt; for automatic chapter detection&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Output Formats&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Creates a &lt;code&gt;['m4b', 'm4a', 'mp4', 'webm', 'mov', 'mp3', 'flac', 'wav', 'ogg', 'aac']&lt;/code&gt; (set in ./lib/conf.py) file with metadata and chapters.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Updating to Latest Version&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git pull # Locally/Compose

docker pull athomasson2/ebook2audiobook:latest # For Pre-build docker images
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Reverting to older Versions&lt;/h2&gt; 
&lt;p&gt;Releases can be found -&amp;gt; &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/releases"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git checkout tags/VERSION_NUM # Locally/Compose -&amp;gt; Example: git checkout tags/v25.7.7

athomasson2/ebook2audiobook:VERSION_NUM # For Pre-build docker images -&amp;gt; Example: athomasson2/ebook2audiobook:v25.7.7
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Common Issues:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;My NVIDIA GPU isnt being detected?? -&amp;gt; &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/wiki/GPU-ISSUES"&gt;GPU ISSUES Wiki Page&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;CPU is slow (better on server smp CPU) while NVIDIA GPU can have almost real time conversion. &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/discussions/19#discussioncomment-10879846"&gt;Discussion about this&lt;/a&gt; For faster multilingual generation I would suggest my other &lt;a href="https://github.com/DrewThomasson/ebook2audiobookpiper-tts"&gt;project that uses piper-tts&lt;/a&gt; instead (It doesn't have zero-shot voice cloning though, and is Siri quality voices, but it is much faster on cpu).&lt;/li&gt; 
 &lt;li&gt;"I'm having dependency issues" - Just use the docker, its fully self contained and has a headless mode, add &lt;code&gt;--help&lt;/code&gt; parameter at the end of the docker run command for more information.&lt;/li&gt; 
 &lt;li&gt;"Im getting a truncated audio issue!" - PLEASE MAKE AN ISSUE OF THIS, we don't speak every language and need advise from users to fine tune the sentence splitting logic.üòä&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;What we need help with! üôå&lt;/h2&gt; 
&lt;h2&gt;&lt;a href="https://github.com/DrewThomasson/ebook2audiobook/issues/32"&gt;Full list of things can be found here&lt;/a&gt;&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Any help from people speaking any of the supported languages to help us improve the models&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Do you need to rent a GPU to boost service from us?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;A poll is open here &lt;a href="https://github.com/DrewThomasson/ebook2audiobook/discussions/889"&gt;https://github.com/DrewThomasson/ebook2audiobook/discussions/889&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Special Thanks&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Coqui TTS&lt;/strong&gt;: &lt;a href="https://github.com/idiap/coqui-ai-TTS"&gt;Coqui TTS GitHub&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Calibre&lt;/strong&gt;: &lt;a href="https://calibre-ebook.com"&gt;Calibre Website&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;FFmpeg&lt;/strong&gt;: &lt;a href="https://ffmpeg.org"&gt;FFmpeg Website&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/DrewThomasson/ebook2audiobook/issues/8"&gt;@shakenbake15 for better chapter saving method&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>guofei9987/blind_watermark</title>
      <link>https://github.com/guofei9987/blind_watermark</link>
      <description>&lt;p&gt;Blind&amp;Invisible Watermark ÔºåÂõæÁâáÁõ≤Ê∞¥Âç∞ÔºåÊèêÂèñÊ∞¥Âç∞Êó†È°ªÂéüÂõæÔºÅ&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;blind-watermark&lt;/h1&gt; 
&lt;p&gt;Blind watermark based on DWT-DCT-SVD.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://pypi.org/project/blind_watermark/"&gt;&lt;img src="https://img.shields.io/pypi/v/blind_watermark" alt="PyPI" /&gt;&lt;/a&gt; &lt;a href="https://travis-ci.com/guofei9987/blind_watermark"&gt;&lt;img src="https://travis-ci.com/guofei9987/blind_watermark.svg?branch=master" alt="Build Status" /&gt;&lt;/a&gt; &lt;a href="https://codecov.io/gh/guofei9987/blind_watermark"&gt;&lt;img src="https://codecov.io/gh/guofei9987/blind_watermark/branch/master/graph/badge.svg?sanitize=true" alt="codecov" /&gt;&lt;/a&gt; &lt;a href="https://github.com/guofei9987/blind_watermark/raw/master/LICENSE"&gt;&lt;img src="https://img.shields.io/pypi/l/blind_watermark.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/badge/python-%3E=3.5-green.svg?sanitize=true" alt="Python" /&gt; &lt;img src="https://img.shields.io/badge/platform-windows%20%7C%20linux%20%7C%20macos-green.svg?sanitize=true" alt="Platform" /&gt; &lt;a href="https://github.com/guofei9987/blind_watermark/"&gt;&lt;img src="https://img.shields.io/github/stars/guofei9987/blind_watermark.svg?style=social" alt="stars" /&gt;&lt;/a&gt; &lt;a href="https://github.com/guofei9987/blind_watermark/fork"&gt;&lt;img src="https://img.shields.io/github/forks/guofei9987/blind_watermark?style=social" alt="fork" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/project/blind-watermark"&gt;&lt;img src="https://pepy.tech/badge/blind-watermark" alt="Downloads" /&gt;&lt;/a&gt; &lt;a href="https://github.com/guofei9987/blind_watermark/discussions"&gt;&lt;img src="https://img.shields.io/badge/discussions-green.svg?sanitize=true" alt="Discussions" /&gt;&lt;/a&gt; &lt;a href="https://hellogithub.com/repository/guofei9987/blind_watermark" target="_blank"&gt;&lt;img src="https://abroad.hellogithub.com/v1/widgets/recommend.svg?rid=3834302ff46a40f188a651ef8bd26ff5&amp;amp;claim_uid=se0WHo8cbiLv2w1&amp;amp;theme=small" alt="FeaturedÔΩúHelloGitHub" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Documentation:&lt;/strong&gt; &lt;a href="https://BlindWatermark.github.io/blind_watermark/#/en/"&gt;https://BlindWatermark.github.io/blind_watermark/#/en/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ÊñáÊ°£Ôºö&lt;/strong&gt; &lt;a href="https://BlindWatermark.github.io/blind_watermark/#/zh/"&gt;https://BlindWatermark.github.io/blind_watermark/#/zh/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;‰∏≠Êñá readme&lt;/strong&gt; &lt;a href="https://raw.githubusercontent.com/guofei9987/blind_watermark/master/README_cn.md"&gt;README_cn.md&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Source code:&lt;/strong&gt; &lt;a href="https://github.com/guofei9987/blind_watermark"&gt;https://github.com/guofei9987/blind_watermark&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;install&lt;/h1&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install blind-watermark
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For the current developer version:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bach"&gt;git clone git@github.com:guofei9987/blind_watermark.git
cd blind_watermark
pip install .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;How to use&lt;/h1&gt; 
&lt;h2&gt;Use in bash&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# embed watermark into image:
blind_watermark --embed --pwd 1234 examples/pic/ori_img.jpeg "watermark text" examples/output/embedded.png
# extract watermark from image:
blind_watermark --extract --pwd 1234 --wm_shape 111 examples/output/embedded.png
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Use in Python&lt;/h2&gt; 
&lt;p&gt;Original Image + Watermark = Watermarked Image&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/guofei9987/blind_watermark/master/docs/%E5%8E%9F%E5%9B%BE.jpeg" alt="origin_image" /&gt; + '@guofei9987 ÂºÄÊ∫ê‰∏áÂ≤ÅÔºÅ' = &lt;img src="https://raw.githubusercontent.com/guofei9987/blind_watermark/master/docs/%E6%89%93%E4%B8%8A%E6%B0%B4%E5%8D%B0%E7%9A%84%E5%9B%BE.jpg" alt="Êâì‰∏äÊ∞¥Âç∞ÁöÑÂõæ" /&gt;&lt;/p&gt; 
&lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/guofei9987/blind_watermark/master/examples/example_str.py"&gt;codes&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Embed watermark:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from blind_watermark import WaterMark

bwm1 = WaterMark(password_img=1, password_wm=1)
bwm1.read_img('pic/ori_img.jpg')
wm = '@guofei9987 ÂºÄÊ∫ê‰∏áÂ≤ÅÔºÅ'
bwm1.read_wm(wm, mode='str')
bwm1.embed('output/embedded.png')
len_wm = len(bwm1.wm_bit)
print('Put down the length of wm_bit {len_wm}'.format(len_wm=len_wm))
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Extract watermark:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;bwm1 = WaterMark(password_img=1, password_wm=1)
wm_extract = bwm1.extract('output/embedded.png', wm_shape=len_wm, mode='str')
print(wm_extract)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Output:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;@guofei9987 ÂºÄÊ∫ê‰∏áÂ≤ÅÔºÅ&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;attacks on Watermarked Image&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;attack method&lt;/th&gt; 
   &lt;th&gt;image after attack&lt;/th&gt; 
   &lt;th&gt;extracted watermark&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Rotate 45 Degrees&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/guofei9987/blind_watermark/master/docs/%E6%97%8B%E8%BD%AC%E6%94%BB%E5%87%BB.jpg" alt="ÊóãËΩ¨ÊîªÂáª" /&gt;&lt;/td&gt; 
   &lt;td&gt;'@guofei9987 ÂºÄÊ∫ê‰∏áÂ≤ÅÔºÅ'&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Random crop&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/guofei9987/blind_watermark/master/docs/%E6%88%AA%E5%B1%8F%E6%94%BB%E5%87%BB2_%E8%BF%98%E5%8E%9F.jpg" alt="Êà™Â±èÊîªÂáª" /&gt;&lt;/td&gt; 
   &lt;td&gt;'@guofei9987 ÂºÄÊ∫ê‰∏áÂ≤ÅÔºÅ'&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Masks&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/guofei9987/blind_watermark/master/docs/%E5%A4%9A%E9%81%AE%E6%8C%A1%E6%94%BB%E5%87%BB.jpg" alt="Â§öÈÅÆÊå°ÊîªÂáª" /&gt;&lt;/td&gt; 
   &lt;td&gt;'@guofei9987 ÂºÄÊ∫ê‰∏áÂ≤ÅÔºÅ'&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Vertical cut&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/guofei9987/blind_watermark/master/docs/%E6%A8%AA%E5%90%91%E8%A3%81%E5%89%AA%E6%94%BB%E5%87%BB_%E5%A1%AB%E8%A1%A5.jpg" alt="Ê®™ÂêëË£ÅÂâ™ÊîªÂáª" /&gt;&lt;/td&gt; 
   &lt;td&gt;'@guofei9987 ÂºÄÊ∫ê‰∏áÂ≤ÅÔºÅ'&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Horizontal cut&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/guofei9987/blind_watermark/master/docs/%E7%BA%B5%E5%90%91%E8%A3%81%E5%89%AA%E6%94%BB%E5%87%BB_%E5%A1%AB%E8%A1%A5.jpg" alt="Á∫µÂêëË£ÅÂâ™ÊîªÂáª" /&gt;&lt;/td&gt; 
   &lt;td&gt;'@guofei9987 ÂºÄÊ∫ê‰∏áÂ≤ÅÔºÅ'&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Resize&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/guofei9987/blind_watermark/master/docs/%E7%BC%A9%E6%94%BE%E6%94%BB%E5%87%BB.jpg" alt="Áº©ÊîæÊîªÂáª" /&gt;&lt;/td&gt; 
   &lt;td&gt;'@guofei9987 ÂºÄÊ∫ê‰∏áÂ≤ÅÔºÅ'&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Pepper Noise&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/guofei9987/blind_watermark/master/docs/%E6%A4%92%E7%9B%90%E6%94%BB%E5%87%BB.jpg" alt="Ê§íÁõêÊîªÂáª" /&gt;&lt;/td&gt; 
   &lt;td&gt;'@guofei9987 ÂºÄÊ∫ê‰∏áÂ≤ÅÔºÅ'&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Brightness 10% Down&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/guofei9987/blind_watermark/master/docs/%E4%BA%AE%E5%BA%A6%E6%94%BB%E5%87%BB.jpg" alt="‰∫ÆÂ∫¶ÊîªÂáª" /&gt;&lt;/td&gt; 
   &lt;td&gt;'@guofei9987 ÂºÄÊ∫ê‰∏áÂ≤ÅÔºÅ'&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;embed images&lt;/h3&gt; 
&lt;p&gt;embed watermark:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from blind_watermark import WaterMark

bwm1 = WaterMark(password_wm=1, password_img=1)
# read original image
bwm1.read_img('pic/ori_img.jpg')
# read watermark
bwm1.read_wm('pic/watermark.png')
# embed
bwm1.embed('output/embedded.png')
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Extract watermark:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;bwm1 = WaterMark(password_wm=1, password_img=1)
# notice that wm_shape is necessary
bwm1.extract(filename='output/embedded.png', wm_shape=(128, 128), out_wm_name='output/extracted.png', )
&lt;/code&gt;&lt;/pre&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;attack method&lt;/th&gt; 
   &lt;th&gt;image after attack&lt;/th&gt; 
   &lt;th&gt;extracted watermark&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Rotate 45 Degrees&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/guofei9987/blind_watermark/master/docs/%E6%97%8B%E8%BD%AC%E6%94%BB%E5%87%BB.jpg" alt="ÊóãËΩ¨ÊîªÂáª" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/guofei9987/blind_watermark/master/docs/%E6%97%8B%E8%BD%AC%E6%94%BB%E5%87%BB_%E6%8F%90%E5%8F%96%E6%B0%B4%E5%8D%B0.png" alt="" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Random crop&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/guofei9987/blind_watermark/master/docs/%E6%88%AA%E5%B1%8F%E6%94%BB%E5%87%BB2_%E8%BF%98%E5%8E%9F.jpg" alt="Êà™Â±èÊîªÂáª" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/guofei9987/blind_watermark/master/docs/%E5%A4%9A%E9%81%AE%E6%8C%A1%E6%94%BB%E5%87%BB_%E6%8F%90%E5%8F%96%E6%B0%B4%E5%8D%B0.png" alt="Â§öÈÅÆÊå°_ÊèêÂèñÊ∞¥Âç∞" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Mask&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/guofei9987/blind_watermark/master/docs/%E5%A4%9A%E9%81%AE%E6%8C%A1%E6%94%BB%E5%87%BB.jpg" alt="Â§öÈÅÆÊå°ÊîªÂáª" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/guofei9987/blind_watermark/master/docs/%E5%A4%9A%E9%81%AE%E6%8C%A1%E6%94%BB%E5%87%BB_%E6%8F%90%E5%8F%96%E6%B0%B4%E5%8D%B0.png" alt="Â§öÈÅÆÊå°_ÊèêÂèñÊ∞¥Âç∞" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;embed array of bits&lt;/h3&gt; 
&lt;p&gt;See it &lt;a href="https://raw.githubusercontent.com/guofei9987/blind_watermark/master/examples/example_bit.py"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;As demo, we embed 6 bytes data:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;wm = [True, False, True, True, True, False]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Embed:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from blind_watermark import WaterMark

bwm1 = WaterMark(password_img=1, password_wm=1)
bwm1.read_ori_img('pic/ori_img.jpg')
bwm1.read_wm([True, False, True, True, True, False], mode='bit')
bwm1.embed('output/embedded.png')
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Extract:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;bwm1 = WaterMark(password_img=1, password_wm=1, wm_shape=6)
wm_extract = bwm1.extract('output/Êâì‰∏äÊ∞¥Âç∞ÁöÑÂõæ.png', mode='bit')
print(wm_extract)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Notice that &lt;code&gt;wm_shape&lt;/code&gt; (shape of watermark) is necessary&lt;/p&gt; 
&lt;p&gt;The output &lt;code&gt;wm_extract&lt;/code&gt; is an array of float. set a threshold such as 0.5.&lt;/p&gt; 
&lt;h1&gt;Concurrency&lt;/h1&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;WaterMark(..., processes=None)
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;processes&lt;/code&gt; number of processes, can be integer. Default &lt;code&gt;None&lt;/code&gt;, which means using all processes.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Related Project&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;text_blind_watermark (Embed message into text): &lt;a href="https://github.com/guofei9987/text_blind_watermark"&gt;https://github.com/guofei9987/text_blind_watermark&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;HideInfoÔºàhide as image, hide as sounds, hide as textÔºâÔºö&lt;a href="https://github.com/guofei9987/HideInfo"&gt;https://github.com/guofei9987/HideInfo&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>moondevonyt/moon-dev-ai-agents</title>
      <link>https://github.com/moondevonyt/moon-dev-ai-agents</link>
      <description>&lt;p&gt;autonomous ai agents for trading in python&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ü§ñ AI AGENTS FOR TRADING&lt;/h1&gt; 
&lt;p align="center"&gt; &lt;a href="https://www.moondev.com/"&gt;&lt;img src="https://raw.githubusercontent.com/moondevonyt/moon-dev-ai-agents/main/moondev.png" width="300" alt="Moon Dev" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;üéØ Vision&lt;/h2&gt; 
&lt;p&gt;ai agents are clearly the future and the entire workforce will be replaced or atleast using ai agents. while i am a quant and building agents for algo trading i will be contributing to all different types of ai agent flows and placing all of the agents here for free, 100% open sourced because i believe code is the great equalizer and we have never seen a regime shift like this so i need to get this code to the people&lt;/p&gt; 
&lt;p&gt;feel free to join &lt;a href="https://discord.gg/8UPuVZ53bh"&gt;our discord&lt;/a&gt; if you beleive ai agents will be integrated into the workforce&lt;/p&gt; 
&lt;h2&gt;Video Updates &amp;amp; Training&lt;/h2&gt; 
&lt;p&gt;‚≠êÔ∏è &lt;a href="https://youtu.be/RlqzkSgDKDc"&gt;first full concise documentation video (watch here)&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;‚≠êÔ∏è &lt;a href="https://youtu.be/tjY24JR8Cso?si=Za-PQ2L79US6cu2T"&gt;second full walkthrough video(watch here)&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;‚≠êÔ∏è &lt;a href="https://youtu.be/qZv6IFIkk6I"&gt;third full walkthrough w/ big updates, new models, new agents(watch here)&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;üìÄ follow all updates here on youtube in this playlist: &lt;a href="https://www.youtube.com/playlist?list=PLXrNVMjRZUJg4M4uz52iGd1LhXXGVbIFz"&gt;https://www.youtube.com/playlist?list=PLXrNVMjRZUJg4M4uz52iGd1LhXXGVbIFz&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ü§ñ All Available Agents&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;‚ö†Ô∏è For live trading agents: Only use these AFTER thoroughly backtesting your strategies!&lt;/strong&gt;&lt;/p&gt; 
&lt;h3&gt;Backtesting &amp;amp; Research Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;RBI Agent&lt;/strong&gt; (&lt;code&gt;rbi_agent.py&lt;/code&gt;): Uses DeepSeek to research trading strategies based on YouTube videos, PDFs, or text you provide, then codes out the backtest automatically&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;RBI Parallel Agent&lt;/strong&gt; (&lt;code&gt;rbi_agent_pp_multi.py&lt;/code&gt;): Parallel version with 18 threads, tests across 20+ data sources, web dashboard included&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Research Agent&lt;/strong&gt; (&lt;code&gt;research_agent.py&lt;/code&gt;): Fills the ideas.txt file so the RBI agent can run forever&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Websearch Agent&lt;/strong&gt; (&lt;code&gt;websearch_agent.py&lt;/code&gt;): This agent searches the web, in my use case for trading strategy resources and then uses other ai's to split the website ideas into strategy files i can have my &lt;code&gt;rbi_agent_pp_multi.py&lt;/code&gt; process and build out backtests&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Live Trading Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Trading Agent&lt;/strong&gt; (&lt;code&gt;trading_agent.py&lt;/code&gt;): &lt;strong&gt;DUAL-MODE AI trading system&lt;/strong&gt; - Toggle between single model (fast ~10s) or swarm mode (6-model consensus ~45-60s). Swarm mode queries Claude 4.5, GPT-5, Gemini 2.5, Grok-4, DeepSeek, and DeepSeek-R1 local for majority vote trading decisions. Configure via &lt;code&gt;USE_SWARM_MODE&lt;/code&gt; in config.py&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Strategy Agent&lt;/strong&gt; (&lt;code&gt;strategy_agent.py&lt;/code&gt;): Manages and executes trading strategies placed in the strategies folder&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Risk Agent&lt;/strong&gt; (&lt;code&gt;risk_agent.py&lt;/code&gt;): Monitors and manages portfolio risk, enforcing position limits and PnL thresholds&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Copy Agent&lt;/strong&gt; (&lt;code&gt;copy_agent.py&lt;/code&gt;): Monitors copy bot for potential trades&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Swarm Agent&lt;/strong&gt; (&lt;code&gt;swarm_agent.py&lt;/code&gt;): Queries 6 AI models in parallel (Claude 4.5, GPT-5, Gemini 2.5, Grok-4, DeepSeek, DeepSeek-R1 local), generates AI consensus summary, returns clean JSON with model mapping for easy parsing üêù&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Market Analysis Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Whale Agent&lt;/strong&gt; (&lt;code&gt;whale_agent.py&lt;/code&gt;): Monitors whale activity and announces when a whale enters the market&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Sentiment Agent&lt;/strong&gt; (&lt;code&gt;sentiment_agent.py&lt;/code&gt;): Analyzes Twitter sentiment for crypto tokens with voice announcements&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Chart Agent&lt;/strong&gt; (&lt;code&gt;chartanalysis_agent.py&lt;/code&gt;): Looks at any crypto chart and analyzes it with AI to make a buy/sell/nothing recommendation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Funding Agent&lt;/strong&gt; (&lt;code&gt;funding_agent.py&lt;/code&gt;): Monitors funding rates across exchanges and uses AI to analyze opportunities, providing voice alerts for extreme funding situations with technical context üåô&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Liquidation Agent&lt;/strong&gt; (&lt;code&gt;liquidation_agent.py&lt;/code&gt;): Tracks liquidation events with configurable time windows (15min/1hr/4hr), providing AI analysis and voice alerts for significant liquidation spikes üí¶&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Listing Arbitrage Agent&lt;/strong&gt; (&lt;code&gt;listingarb_agent.py&lt;/code&gt;): Identifies promising Solana tokens on CoinGecko before they reach major exchanges like Binance and Coinbase, using parallel AI analysis for technical and fundamental insights&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Funding Arbitrage Agent&lt;/strong&gt; (&lt;code&gt;fundingarb_agent.py&lt;/code&gt;): Tracks the funding rate on HyperLiquid to find funding rate arbitrage opportunities between HL and Solana&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;New or Top Tokens Agent&lt;/strong&gt; (&lt;code&gt;new_or_top_agent.py&lt;/code&gt;): Looks at the new tokens and the top tokens from CoinGecko API&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Solana-Specific Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Sniper Agent&lt;/strong&gt; (&lt;code&gt;sniper_agent.py&lt;/code&gt;): Watches for new Solana token launches, analyzes them, and maybe snipes&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;TX Agent&lt;/strong&gt; (&lt;code&gt;tx_agent.py&lt;/code&gt;): Watches transactions made by your copy list and prints them out with optional auto tab open&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Solana Agent&lt;/strong&gt; (&lt;code&gt;solana_agent.py&lt;/code&gt;): Looks at the sniper agent and the TX agent to select which memes may be interesting&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Content Creation Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Chat Agent&lt;/strong&gt; (&lt;code&gt;chat_agent.py&lt;/code&gt;): Monitors YouTube live stream chat, moderates &amp;amp; responds to known questions. Absolute fire.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Twitter Agent&lt;/strong&gt; (&lt;code&gt;tweet_agent.py&lt;/code&gt;): Takes in text and creates tweets using DeepSeek or other models&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Video Agent&lt;/strong&gt; (&lt;code&gt;video_agent.py&lt;/code&gt;): üé¨ Parallel AI video generation using OpenAI's Sora 2 API - create videos directly from text prompts with 9 concurrent workers, configurable resolutions (720p/1080p), durations (4/8/12s), and aspect ratios (9:16 for TikTok/Reels, 16:9 for YouTube, 1:1 for Instagram). &lt;a href="https://raw.githubusercontent.com/moondevonyt/moon-dev-ai-agents/main/docs/video_agent.md"&gt;See full docs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Clips Agent&lt;/strong&gt; (&lt;code&gt;clips_agent.py&lt;/code&gt;): Helps clip long videos into shorter ones so you can upload to your YouTube and get paid. More info: &lt;a href="https://discord.gg/XAw8US9aHT"&gt;https://discord.gg/XAw8US9aHT&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Real-Time Clips Agent&lt;/strong&gt; (&lt;code&gt;realtime_clips_agent.py&lt;/code&gt;): Makes real-time clips of streamers using OBS&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Phone Agent&lt;/strong&gt; (&lt;code&gt;phone_agent.py&lt;/code&gt;): An AI agent that can take phone calls for you&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Specialized Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Prompt Agent&lt;/strong&gt; (&lt;code&gt;prompt_agent.py&lt;/code&gt;): üéØ Interactive prompt enhancement tool that transforms basic prompts into professional, production-ready prompts using best practices from Parahelp &amp;amp; Cursor. Stays open in terminal, continuously ready to enhance your prompts with expert design principles (role-based prompting, structured formatting, explicit thinking order). Auto-saves and copies enhanced prompts. Perfect for improving prompts for any AI task. &lt;a href="https://raw.githubusercontent.com/moondevonyt/moon-dev-ai-agents/main/docs/prompt_agent.md"&gt;See full docs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Focus Agent&lt;/strong&gt; (&lt;code&gt;focus_agent.py&lt;/code&gt;): Randomly samples audio during coding sessions to maintain productivity, providing focus scores and voice alerts when focus drops (~$10/month, perfect for voice-to-code workflows)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Million Agent&lt;/strong&gt; (&lt;code&gt;million_agent.py&lt;/code&gt;): Uses million context window from Gemini to pull in a knowledge base&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;TikTok Agent&lt;/strong&gt; (&lt;code&gt;tiktok_agent.py&lt;/code&gt;): Scrolls TikTok and gets screenshots of the video + comments to extract consumer data to feed into algos. Sometimes called social arbitrage&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Compliance Agent&lt;/strong&gt; (&lt;code&gt;compliance_agent.py&lt;/code&gt;): Analyzes TikTok ads for Facebook advertising compliance, extracting frames and transcribing audio to check against FB guidelines&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Housecoin Agent&lt;/strong&gt; (&lt;code&gt;housecoin_agent.py&lt;/code&gt;): DCA (dollar cost average) agent with AI confirmation layer using Grok-4 for the thesis: 1 House = 1 Housecoin üè†&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Polymarket Agent&lt;/strong&gt; (&lt;code&gt;polymarket_agent.py&lt;/code&gt;): Connects to the live trades feed via WebSocket and analyzes with the swarm agent to see which markets could be interesting to trade&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;‚ö†Ô∏è Critical Disclaimers&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;There is no token associated with this project and there never will be. any token launched is not affiliated with this project, moon dev will never dm you. be careful. don't send funds anywhere&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;PLEASE READ CAREFULLY:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;This is an experimental research project, NOT a trading system&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;There are NO plug-and-play solutions for guaranteed profits&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;We do NOT provide trading strategies&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Success depends entirely on YOUR:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Trading strategy&lt;/li&gt; 
   &lt;li&gt;Risk management&lt;/li&gt; 
   &lt;li&gt;Market research&lt;/li&gt; 
   &lt;li&gt;Testing and validation&lt;/li&gt; 
   &lt;li&gt;Overall trading approach&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;NO AI agent can guarantee profitable trading&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;You MUST develop and validate your own trading approach&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Trading involves substantial risk of loss&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Past performance does not indicate future results&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;‚ö†Ô∏è IMPORTANT: This is an experimental project. There are NO guarantees of profitability. Trading involves substantial risk of loss.&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;üëÇ Looking for Updates?&lt;/h2&gt; 
&lt;p&gt;Project updates will be posted in Discord, join here: &lt;a href="https://discord.gg/8UPuVZ53bh"&gt;discord.gg/8UPuVZ53bh&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;üîó Links&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Free Algo Trading Roadmap: &lt;a href="https://moondev.com"&gt;moondev.com&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Algo Trading Education: &lt;a href="https://algotradecamp.com"&gt;algotradecamp.com&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Business Contact &lt;a href="mailto:moon@algotradecamp.com"&gt;moon@algotradecamp.com&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üöÄ Quick Start Guide - RBI Backtesting Agent&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Why Start with Backtesting?&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Before running ANY trading algorithm or AI agent with real money, you MUST backtest your strategies. Backtesting shows you how a strategy would have performed on historical data. The RBI (Research-Based Inference) Agent automates this entire process for you.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;What is the RBI Agent?&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;The RBI Agent takes your trading ideas (from YouTube videos, PDFs, or plain text) and:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;üß† Uses AI to understand the trading strategy&lt;/li&gt; 
 &lt;li&gt;üíª Codes a complete backtest using the &lt;code&gt;backtesting.py&lt;/code&gt; library&lt;/li&gt; 
 &lt;li&gt;üìä Tests across 20+ different market data sources&lt;/li&gt; 
 &lt;li&gt;‚úÖ Only saves strategies that pass a 1% return threshold&lt;/li&gt; 
 &lt;li&gt;üéØ Tries to optimize strategies to hit a 50% target return&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Python Version:&lt;/strong&gt; 3.10.9 was used during development&lt;/p&gt; 
&lt;h3&gt;Step 1: ‚≠ê Star &amp;amp; Fork the Repo&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Click the star button to save it to your GitHub favorites&lt;/li&gt; 
 &lt;li&gt;Fork to your GitHub account to get your own copy&lt;/li&gt; 
 &lt;li&gt;This lets you make changes and track updates&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Step 2: üíª Clone to Your Machine&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/YOUR_USERNAME/moon-dev-ai-agents-for-trading.git
cd moon-dev-ai-agents-for-trading
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Recommended IDEs:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.cursor.com/"&gt;Cursor&lt;/a&gt; - AI-enabled coding&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://codeium.com/"&gt;Windsurfer&lt;/a&gt; - AI-enabled coding&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Step 3: üîë Set Up Environment Variables&lt;/h3&gt; 
&lt;p&gt;The RBI Agent needs API keys to function. Create a &lt;code&gt;.env&lt;/code&gt; file in the root directory:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Copy the example file
cp .env.example .env
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Required API Keys for RBI Agent:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# AI Model APIs (you need at least ONE of these)
ANTHROPIC_KEY=your_anthropic_api_key_here          # Claude models (recommended)
OPENAI_KEY=your_openai_api_key_here                # GPT models
DEEPSEEK_KEY=your_deepseek_api_key_here            # DeepSeek models (cheap!)
GROQ_API_KEY=your_groq_api_key_here                # Groq (fast inference)
GEMINI_KEY=your_gemini_api_key_here                # Google Gemini
XAI_API_KEY=your_xai_api_key_here                  # Grok models
OPENROUTER_API_KEY=your_openrouter_api_key_here    # OpenRouter (200+ models!)

# Market Data APIs (for downloading price data)
BIRDEYE_API_KEY=your_birdeye_api_key_here          # Solana token data
COINGECKO_API_KEY=your_coingecko_api_key_here      # Crypto market data
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Where to Get API Keys:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Anthropic Claude&lt;/strong&gt;: &lt;a href="https://console.anthropic.com/"&gt;https://console.anthropic.com/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;OpenAI GPT&lt;/strong&gt;: &lt;a href="https://platform.openai.com/api-keys"&gt;https://platform.openai.com/api-keys&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;DeepSeek&lt;/strong&gt;: &lt;a href="https://platform.deepseek.com/"&gt;https://platform.deepseek.com/&lt;/a&gt; (very cheap, great for backtesting)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Groq&lt;/strong&gt;: &lt;a href="https://console.groq.com/"&gt;https://console.groq.com/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Google Gemini&lt;/strong&gt;: &lt;a href="https://aistudio.google.com/app/apikey"&gt;https://aistudio.google.com/app/apikey&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;xAI Grok&lt;/strong&gt;: &lt;a href="https://console.x.ai/"&gt;https://console.x.ai/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;OpenRouter&lt;/strong&gt;: &lt;a href="https://openrouter.ai/keys"&gt;https://openrouter.ai/keys&lt;/a&gt; (access 200+ models including Qwen, GLM, and more!)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;BirdEye&lt;/strong&gt;: &lt;a href="https://birdeye.so/"&gt;https://birdeye.so/&lt;/a&gt; (Solana data)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;CoinGecko&lt;/strong&gt;: &lt;a href="https://www.coingecko.com/en/api"&gt;https://www.coingecko.com/en/api&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;‚ö†Ô∏è &lt;strong&gt;Never commit or share your &lt;code&gt;.env&lt;/code&gt; file! It's in .gitignore for your safety.&lt;/strong&gt;&lt;/p&gt; 
&lt;h3&gt;Step 4: üì¶ Install Dependencies&lt;/h3&gt; 
&lt;p&gt;Using conda (recommended):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;conda create -n tflow python=3.10.9
conda activate tflow
pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or using pip directly:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Step 5: üß™ Run Your First Backtest&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Option A: Single Strategy Test&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Create a file called &lt;code&gt;ideas.txt&lt;/code&gt; in &lt;code&gt;src/data/rbi_pp_multi/&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Buy when RSI &amp;lt; 30 and sell when RSI &amp;gt; 70
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python src/agents/rbi_agent_pp_multi.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Option B: Use the Web Dashboard&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Start the dashboard:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd src/data/rbi_pp_multi
python app.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Open browser to: &lt;code&gt;http://localhost:8001&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Click "New Backtests" and enter your strategy ideas!&lt;/p&gt; 
&lt;h3&gt;Step 6: üìä Understanding Results&lt;/h3&gt; 
&lt;p&gt;The agent will:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Process your strategy idea&lt;/li&gt; 
 &lt;li&gt;Generate backtest code&lt;/li&gt; 
 &lt;li&gt;Test across 20+ market datasets (BTC, ETH, SOL, etc.)&lt;/li&gt; 
 &lt;li&gt;Show results in a table with: 
  &lt;ul&gt; 
   &lt;li&gt;Return %&lt;/li&gt; 
   &lt;li&gt;Buy &amp;amp; Hold %&lt;/li&gt; 
   &lt;li&gt;Max Drawdown&lt;/li&gt; 
   &lt;li&gt;Sharpe Ratio&lt;/li&gt; 
   &lt;li&gt;Sortino Ratio&lt;/li&gt; 
   &lt;li&gt;Number of Trades&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Only strategies returning &amp;gt; 1% are saved to the CSV.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Results are saved to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;src/data/rbi_pp_multi/backtest_stats.csv&lt;/code&gt; - All passing backtests&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;src/data/rbi_pp_multi/user_folders/&lt;/code&gt; - Organized by run name&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Step 7: üîç Analyze Backtest Code&lt;/h3&gt; 
&lt;p&gt;Find your strategy files in:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;src/data/rbi_pp_multi/10_25_2025_09_08/
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Each successful backtest has:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Python file&lt;/strong&gt;: The actual backtest code you can review and modify&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Results&lt;/strong&gt;: Performance metrics&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Read the code!&lt;/strong&gt; This is how you learn what works and what doesn't.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üéØ Configuration - RBI Agent&lt;/h2&gt; 
&lt;p&gt;All settings are in &lt;code&gt;src/agents/rbi_agent_pp_multi.py&lt;/code&gt; (lines 130-132):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# üéØ PROFIT TARGET CONFIGURATION
TARGET_RETURN = 50  # Target return in % (AI tries to optimize to this)
SAVE_IF_OVER_RETURN = 1.0  # Save backtest to CSV if return &amp;gt; this %
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;AI tries to optimize strategies to hit &lt;strong&gt;50% return&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;But ANY backtest returning &lt;strong&gt;&amp;gt; 1%&lt;/strong&gt; gets saved to CSV&lt;/li&gt; 
 &lt;li&gt;This way you can review all decent strategies, not just perfect ones&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Other Settings:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;MAX_WORKERS = 18  # Number of parallel threads (adjust based on your CPU)
DEBUG_BACKTEST_ERRORS = True  # Auto-fix coding errors with AI
MAX_DEBUG_ITERATIONS = 10  # How many times to try fixing errors
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üìö Advanced: Adding Custom Data Sources&lt;/h2&gt; 
&lt;p&gt;Want to test on your own tokens? Edit the data list in &lt;code&gt;rbi_agent_pp_multi.py&lt;/code&gt; (lines 157-178):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;ALL_DATA_CONFIGS = [
    # Crypto data from CoinGecko/BirdEye
    {'symbol': 'BTC-USD', 'timeframe': '15m', 'days_back': 90},
    {'symbol': 'ETH-USD', 'timeframe': '15m', 'days_back': 90},
    {'symbol': 'SOL-USD', 'timeframe': '15m', 'days_back': 90},

    # Add your own token (Solana contract address)
    {'symbol': 'YOUR_TOKEN_ADDRESS', 'timeframe': '1H', 'days_back': 30},
]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The agent will automatically download and cache the data.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üó∫Ô∏è ROADMAP&lt;/h2&gt; 
&lt;h3&gt;In Progress&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;HyperLiquid Perps Integration&lt;/strong&gt; ‚úÖ&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;Swarm Consensus Trading&lt;/strong&gt; ‚úÖ&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;strong&gt;RBI Parallel Backtesting&lt;/strong&gt; ‚úÖ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Coming Soon&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;strong&gt;Polymarket Integration&lt;/strong&gt; - Prediction market trading&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;strong&gt;Base Chain Integration&lt;/strong&gt; - L2 network support&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;strong&gt;Extended Integration&lt;/strong&gt; - Additional exchange support&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;strong&gt;HyperLiquid Spot Trading&lt;/strong&gt; - Spot market support&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;strong&gt;Trending Agent&lt;/strong&gt; - Spots leaders on HyperLiquid&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;strong&gt;Position Sizing Agent&lt;/strong&gt; - Volume/liquidation-based sizing&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;strong&gt;Regime Agents&lt;/strong&gt; - Adaptive strategy switching&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;strong&gt;Polymarket Sweeper Agent&lt;/strong&gt; - Follow successful prediction traders&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Future Ideas&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;strong&gt;Lighter Integration&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;strong&gt;Pacifica Integration&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;strong&gt;Hibachi Integration&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;strong&gt;Aster Integration&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;strong&gt;HyperEVM Support&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;em&gt;Built with love by Moon Dev - Pioneering the future of AI-powered trading&lt;/em&gt;&lt;/p&gt; 
&lt;h2&gt;üìú Detailed Disclaimer&lt;/h2&gt; 
&lt;p&gt;The content presented is for educational and informational purposes only and does not constitute financial advice. All trading involves risk and may not be suitable for all investors. You should carefully consider your investment objectives, level of experience, and risk appetite before investing.&lt;/p&gt; 
&lt;p&gt;Past performance is not indicative of future results. There is no guarantee that any trading strategy or algorithm discussed will result in profits or will not incur losses.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;CFTC Disclaimer:&lt;/strong&gt; Commodity Futures Trading Commission (CFTC) regulations require disclosure of the risks associated with trading commodities and derivatives. There is a substantial risk of loss in trading and investing.&lt;/p&gt; 
&lt;p&gt;I am not a licensed financial advisor or a registered broker-dealer. Content &amp;amp; code is based on personal research perspectives and should not be relied upon as a guarantee of success in trading.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>coinbase/x402</title>
      <link>https://github.com/coinbase/x402</link>
      <description>&lt;p&gt;A payments protocol for the internet. Built on HTTP.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;x402 payments protocol&lt;/h1&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;"1 line of code to accept digital dollars. No fee, 2 second settlement, $0.001 minimum payment."&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-typescript"&gt;app.use(
  // How much you want to charge, and where you want the funds to land
  paymentMiddleware("0xYourAddress", { "/your-endpoint": "$0.01" })
);
// That's it! See examples/typescript/servers/express.ts for a complete example. Instruction below for running on base-sepolia.
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Philosophy&lt;/h2&gt; 
&lt;p&gt;Payments on the internet are fundamentally flawed. Credit Cards are high friction, hard to accept, have minimum payments that are far too high, and don't fit into the programmatic nature of the internet. It's time for an open, internet-native form of payments. A payment rail that doesn't have high minimums + % based fee. Payments that are amazing for humans and AI agents.&lt;/p&gt; 
&lt;h2&gt;Principles&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Open standard:&lt;/strong&gt; the x402 protocol will never force reliance on a single party&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;HTTP Native:&lt;/strong&gt; x402 is meant to seamlessly complement the existing HTTP request made by traditional web services, it should not mandate additional requests outside the scope of a typical client / server flow.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Chain and token agnostic:&lt;/strong&gt; we welcome contributions that add support for new chains, signing standards, or schemes, so long as they meet our acceptance criteria laid out in &lt;a href="https://github.com/coinbase/x402/raw/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Trust minimizing:&lt;/strong&gt; all payment schemes must not allow for the facilitator or resource server to move funds, other than in accordance with client intentions&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Easy to use:&lt;/strong&gt; x402 needs to be 10x better than existing ways to pay on the internet. This means abstracting as many details of crypto as possible away from the client and resource server, and into the facilitator. This means the client/server should not need to think about gas, rpc, etc.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Ecosystem&lt;/h2&gt; 
&lt;p&gt;The x402 ecosystem is growing! Check out our &lt;a href="https://x402.org/ecosystem"&gt;ecosystem page&lt;/a&gt; to see projects building with x402, including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Client-side integrations&lt;/li&gt; 
 &lt;li&gt;Services and endpoints&lt;/li&gt; 
 &lt;li&gt;Ecosystem infrastructure and tooling&lt;/li&gt; 
 &lt;li&gt;Learning and community resources&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Want to add your project to the ecosystem? See our &lt;a href="https://github.com/coinbase/x402/tree/main/typescript/site#adding-your-project-to-the-ecosystem"&gt;demo site README&lt;/a&gt; for detailed instructions on how to submit your project.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Roadmap:&lt;/strong&gt; see &lt;a href="https://github.com/coinbase/x402/raw/main/ROADMAP.md"&gt;ROADMAP.md&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Terms:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;resource&lt;/code&gt;: Something on the internet. This could be a webpage, file server, RPC service, API, any resource on the internet that accepts HTTP / HTTPS requests.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;client&lt;/code&gt;: An entity wanting to pay for a resource.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;facilitator server&lt;/code&gt;: A server that facilitates verification and execution of on-chain payments.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;resource server&lt;/code&gt;: An HTTP server that provides an API or other resource for a client.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Technical Goals:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Permissionless and secure for clients and servers&lt;/li&gt; 
 &lt;li&gt;Gasless for client and resource servers&lt;/li&gt; 
 &lt;li&gt;Minimal integration for the resource server and client (1 line for the server, 1 function for the client)&lt;/li&gt; 
 &lt;li&gt;Ability to trade off speed of response for guarantee of payment&lt;/li&gt; 
 &lt;li&gt;Extensible to different payment flows and chains&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;V1 Protocol&lt;/h2&gt; 
&lt;p&gt;The &lt;code&gt;x402&lt;/code&gt; protocol is a chain agnostic standard for payments on top of HTTP, leverage the existing &lt;code&gt;402 Payment Required&lt;/code&gt; HTTP status code to indicate that a payment is required for access to the resource.&lt;/p&gt; 
&lt;p&gt;It specifies:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;A schema for how servers can respond to clients to facilitate payment for a resource (&lt;code&gt;PaymentRequirements&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;A standard header &lt;code&gt;X-PAYMENT&lt;/code&gt; that is set by clients paying for resources&lt;/li&gt; 
 &lt;li&gt;A standard schema and encoding method for data in the &lt;code&gt;X-PAYMENT&lt;/code&gt; header&lt;/li&gt; 
 &lt;li&gt;A recommended flow for how payments should be verified and settled by a resource server&lt;/li&gt; 
 &lt;li&gt;A REST specification for how a resource server can perform verification and settlement against a remote 3rd party server (&lt;code&gt;facilitator&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;A specification for a &lt;code&gt;X-PAYMENT-RESPONSE&lt;/code&gt; header that can be used by resource servers to communicate blockchain transactions details to the client in their HTTP response&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;V1 Protocol Sequencing&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/coinbase/x402/main/static/x402-protocol-flow.png" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;The following outlines the flow of a payment using the &lt;code&gt;x402&lt;/code&gt; protocol. Note that steps (1) and (2) are optional if the client already knows the payment details accepted for a resource.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;Client&lt;/code&gt; makes an HTTP request to a &lt;code&gt;resource server&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;Resource server&lt;/code&gt; responds with a &lt;code&gt;402 Payment Required&lt;/code&gt; status and a &lt;code&gt;Payment Required Response&lt;/code&gt; JSON object in the response body.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;Client&lt;/code&gt; selects one of the &lt;code&gt;paymentRequirements&lt;/code&gt; returned by the server response and creates a &lt;code&gt;Payment Payload&lt;/code&gt; based on the &lt;code&gt;scheme&lt;/code&gt; of the &lt;code&gt;paymentRequirements&lt;/code&gt; they have selected.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;Client&lt;/code&gt; sends the HTTP request with the &lt;code&gt;X-PAYMENT&lt;/code&gt; header containing the &lt;code&gt;Payment Payload&lt;/code&gt; to the resource server.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;Resource server&lt;/code&gt; verifies the &lt;code&gt;Payment Payload&lt;/code&gt; is valid either via local verification or by POSTing the &lt;code&gt;Payment Payload&lt;/code&gt; and &lt;code&gt;Payment Requirements&lt;/code&gt; to the &lt;code&gt;/verify&lt;/code&gt; endpoint of a &lt;code&gt;facilitator server&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;Facilitator server&lt;/code&gt; performs verification of the object based on the &lt;code&gt;scheme&lt;/code&gt; and &lt;code&gt;network&lt;/code&gt; of the &lt;code&gt;Payment Payload&lt;/code&gt; and returns a &lt;code&gt;Verification Response&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;If the &lt;code&gt;Verification Response&lt;/code&gt; is valid, the resource server performs the work to fulfill the request. If the &lt;code&gt;Verification Response&lt;/code&gt; is invalid, the resource server returns a &lt;code&gt;402 Payment Required&lt;/code&gt; status and a &lt;code&gt;Payment Required Response&lt;/code&gt; JSON object in the response body.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;Resource server&lt;/code&gt; either settles the payment by interacting with a blockchain directly, or by POSTing the &lt;code&gt;Payment Payload&lt;/code&gt; and &lt;code&gt;Payment PaymentRequirements&lt;/code&gt; to the &lt;code&gt;/settle&lt;/code&gt; endpoint of a &lt;code&gt;facilitator server&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;Facilitator server&lt;/code&gt; submits the payment to the blockchain based on the &lt;code&gt;scheme&lt;/code&gt; and &lt;code&gt;network&lt;/code&gt; of the &lt;code&gt;Payment Payload&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;Facilitator server&lt;/code&gt; waits for the payment to be confirmed on the blockchain.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;Facilitator server&lt;/code&gt; returns a &lt;code&gt;Payment Execution Response&lt;/code&gt; to the resource server.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;Resource server&lt;/code&gt; returns a &lt;code&gt;200 OK&lt;/code&gt; response to the &lt;code&gt;Client&lt;/code&gt; with the resource they requested as the body of the HTTP response, and a &lt;code&gt;X-PAYMENT-RESPONSE&lt;/code&gt; header containing the &lt;code&gt;Settlement Response&lt;/code&gt; as Base64 encoded JSON if the payment was executed successfully.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Type Specifications&lt;/h3&gt; 
&lt;h4&gt;Data types&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;Payment Required Response&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json5"&gt;{
  // Version of the x402 payment protocol
  x402Version: int,

  // List of payment requirements that the resource server accepts. A resource server may accept on multiple chains, or in multiple currencies.
  accepts: [paymentRequirements]

  // Message from the resource server to the client to communicate errors in processing payment
  error: string
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;paymentRequirements&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json5"&gt;{
  // Scheme of the payment protocol to use
  scheme: string;

  // Network of the blockchain to send payment on
  network: string;

  // Maximum amount required to pay for the resource in atomic units of the asset
  maxAmountRequired: uint256 as string;

  // URL of resource to pay for
  resource: string;

  // Description of the resource
  description: string;

  // MIME type of the resource response
  mimeType: string;

  // Output schema of the resource response
  outputSchema?: object | null;

  // Address to pay value to
  payTo: string;

  // Maximum time in seconds for the resource server to respond
  maxTimeoutSeconds: number;

  // Address of the EIP-3009 compliant ERC20 contract
  asset: string;

  // Extra information about the payment details specific to the scheme
  // For `exact` scheme on a EVM network, expects extra to contain the records `name` and `version` pertaining to asset
  extra: object | null;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;&lt;code&gt;Payment Payload&lt;/code&gt;&lt;/strong&gt; (included as the &lt;code&gt;X-PAYMENT&lt;/code&gt; header in base64 encoded json)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json5"&gt;{
  // Version of the x402 payment protocol
  x402Version: number;

  // scheme is the scheme value of the accepted `paymentRequirements` the client is using to pay
  scheme: string;

  // network is the network id of the accepted `paymentRequirements` the client is using to pay
  network: string;

  // payload is scheme dependent
  payload: &amp;lt;scheme dependent&amp;gt;;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Facilitator Types &amp;amp; Interface&lt;/h4&gt; 
&lt;p&gt;A &lt;code&gt;facilitator server&lt;/code&gt; is a 3rd party service that can be used by a &lt;code&gt;resource server&lt;/code&gt; to verify and settle payments, without the &lt;code&gt;resource server&lt;/code&gt; needing to have access to a blockchain node or wallet.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;POST /verify&lt;/strong&gt;. Verify a payment with a supported scheme and network:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Request body JSON: &lt;pre&gt;&lt;code class="language-json5"&gt;{
  x402Version: number;
  paymentHeader: string;
  paymentRequirements: paymentRequirements;
}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Response: &lt;pre&gt;&lt;code class="language-json5"&gt;{
  isValid: boolean;
  invalidReason: string | null;
}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;POST /settle&lt;/strong&gt;. Settle a payment with a supported scheme and network:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Request body JSON:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-json5"&gt;{
  x402Version: number;
  paymentHeader: string;
  paymentRequirements: paymentRequirements;
}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Response:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-json5"&gt;{
  // Whether the payment was successful
  success: boolean;

  // Error message from the facilitator server
  error: string | null;

  // Transaction hash of the settled payment
  txHash: string | null;

  // Network id of the blockchain the payment was settled on
  networkId: string | null;
}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;GET /supported&lt;/strong&gt;. Get supported payment schemes and networks:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Response: &lt;pre&gt;&lt;code class="language-json5"&gt;{
  kinds: [
    {
      "scheme": string,
      "network": string,
    }
  ]
}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Schemes&lt;/h3&gt; 
&lt;p&gt;A scheme is a logical way of moving money.&lt;/p&gt; 
&lt;p&gt;Blockchains allow for a large number of flexible ways to move money. To help facilitate an expanding number of payment use cases, the &lt;code&gt;x402&lt;/code&gt; protocol is extensible to different ways of settling payments via its &lt;code&gt;scheme&lt;/code&gt; field.&lt;/p&gt; 
&lt;p&gt;Each payment scheme may have different operational functionality depending on what actions are necessary to fulfill the payment. For example &lt;code&gt;exact&lt;/code&gt;, the first scheme shipping as part of the protocol, would have different behavior than &lt;code&gt;upto&lt;/code&gt;. &lt;code&gt;exact&lt;/code&gt; transfers a specific amount (ex: pay $1 to read an article), while a theoretical &lt;code&gt;upto&lt;/code&gt; would transfer up to an amount, based on the resources consumed during a request (ex: generating tokens from an LLM).&lt;/p&gt; 
&lt;p&gt;See &lt;code&gt;specs/schemes&lt;/code&gt; for more details on schemes, and see &lt;code&gt;specs/schemes/exact/scheme_exact_evm.md&lt;/code&gt; to see the first proposed scheme for exact payment on EVM chains.&lt;/p&gt; 
&lt;h3&gt;Schemes vs Networks&lt;/h3&gt; 
&lt;p&gt;Because a scheme is a logical way of moving money, the way a scheme is implemented can be different for different blockchains. (ex: the way you need to implement &lt;code&gt;exact&lt;/code&gt; on Ethereum is very different from the way you need to implement &lt;code&gt;exact&lt;/code&gt; on Solana).&lt;/p&gt; 
&lt;p&gt;Clients and facilitators must explicitly support different &lt;code&gt;(scheme, network)&lt;/code&gt; pairs in order to be able to create proper payloads and verify / settle payments.&lt;/p&gt; 
&lt;h2&gt;Running example&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Requirements:&lt;/strong&gt; Node.js v24 or higher&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;From &lt;code&gt;examples/typescript&lt;/code&gt; run &lt;code&gt;pnpm install&lt;/code&gt; and &lt;code&gt;pnpm build&lt;/code&gt; to ensure all dependent packages and examples are setup.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Select a server, i.e. express, and &lt;code&gt;cd&lt;/code&gt; into that example. Add your server's ethereum address to get paid to into the &lt;code&gt;.env&lt;/code&gt; file, and then run &lt;code&gt;pnpm dev&lt;/code&gt; in that directory.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Select a client, i.e. axios, and &lt;code&gt;cd&lt;/code&gt; into that example. Add your private key for the account making payments into the &lt;code&gt;.env&lt;/code&gt; file, and then run &lt;code&gt;pnpm dev&lt;/code&gt; in that directory.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;You should see activities in the client terminal, which will display a weather report.&lt;/p&gt; 
&lt;h2&gt;Running tests&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Navigate to the typescript directory: &lt;code&gt;cd typescript&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Install dependencies: &lt;code&gt;pnpm install&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Run the unit tests: &lt;code&gt;pnpm test&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;This will run the unit tests for the x402 packages.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>amazon-science/chronos-forecasting</title>
      <link>https://github.com/amazon-science/chronos-forecasting</link>
      <description>&lt;p&gt;Chronos: Pretrained Models for Time Series Forecasting&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/amazon-science/chronos-forecasting/main/figures/chronos-logo.png" width="60%" /&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;h1&gt;Chronos: Pretrained Models for Time Series Forecasting&lt;/h1&gt; 
 &lt;p&gt;&lt;a href="https://arxiv.org/abs/2403.07815"&gt;&lt;img src="https://img.shields.io/static/v1?label=Chronos-Paper&amp;amp;message=2403.07815&amp;amp;color=B31B1B&amp;amp;logo=arXiv" alt="preprint" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2510.15821"&gt;&lt;img src="https://img.shields.io/static/v1?label=Chronos-2-Report&amp;amp;message=2510.15821&amp;amp;color=B31B1B&amp;amp;logo=arXiv" alt="preprint" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/datasets/autogluon/chronos_datasets"&gt;&lt;img src="https://img.shields.io/badge/%F0%9F%A4%97%20HF-Datasets-FFD21E" alt="huggingface" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/collections/amazon/chronos-models-65f1791d630a8d57cb718444"&gt;&lt;img src="https://img.shields.io/badge/%F0%9F%A4%97%20HF-Models-FFD21E" alt="huggingface" /&gt;&lt;/a&gt; &lt;a href="https://github.com/autogluon/fev"&gt;&lt;img src="https://img.shields.io/static/v1?label=fev&amp;amp;message=Benchmark&amp;amp;color=B31B1B&amp;amp;logo=github" alt="fev" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/amazon-science/chronos-forecasting/main/notebooks/deploy-chronos-bolt-to-amazon-sagemaker.ipynb"&gt;&lt;img src="https://img.shields.io/static/v1?label=SageMaker&amp;amp;message=Deploy&amp;amp;color=FF9900&amp;amp;logo=amazon-web-services" alt="aws" /&gt;&lt;/a&gt; &lt;a href="https://github.com/amazon-science/chronos-forecasting/issues?q=is%3Aissue+label%3AFAQ"&gt;&lt;img src="https://img.shields.io/badge/FAQ-Questions%3F-blue" alt="faq" /&gt;&lt;/a&gt; &lt;a href="https://opensource.org/licenses/Apache-2.0"&gt;&lt;img src="https://img.shields.io/badge/License-Apache--2.0-green.svg?sanitize=true" alt="License: MIT" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;üöÄ News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;20 Oct 2025&lt;/strong&gt;: üöÄ &lt;a href="https://huggingface.co/amazon/chronos-2"&gt;Chronos-2&lt;/a&gt; released. It offers &lt;em&gt;zero-shot&lt;/em&gt; support for univariate, multivariate, and covariate-informed forecasting tasks. Chronos-2 achieves the best performance on fev-bench, GIFT-Eval and Chronos Benchmark II amongst pretrained models. Check out &lt;a href="https://raw.githubusercontent.com/amazon-science/chronos-forecasting/main/notebooks/chronos-2-quickstart.ipynb"&gt;this notebook&lt;/a&gt; to get started with Chronos-2.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;14 Feb 2025&lt;/strong&gt;: üöÄ Chronos-Bolt is now available on Amazon SageMaker JumpStart! Check out the &lt;a href="https://raw.githubusercontent.com/amazon-science/chronos-forecasting/main/notebooks/deploy-chronos-bolt-to-amazon-sagemaker.ipynb"&gt;tutorial notebook&lt;/a&gt; to learn how to deploy Chronos endpoints for production use in 3 lines of code.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;12 Dec 2024&lt;/strong&gt;: üìä We released &lt;a href="https://github.com/autogluon/fev"&gt;&lt;code&gt;fev&lt;/code&gt;&lt;/a&gt;, a lightweight package for benchmarking time series forecasting models based on the &lt;a href="https://huggingface.co/docs/datasets/en/index"&gt;Hugging Face &lt;code&gt;datasets&lt;/code&gt;&lt;/a&gt; library.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;26 Nov 2024&lt;/strong&gt;: ‚ö°Ô∏è Chronos-Bolt models released &lt;a href="https://huggingface.co/collections/amazon/chronos-models-65f1791d630a8d57cb718444"&gt;on HuggingFace&lt;/a&gt;. Chronos-Bolt models are more accurate (5% lower error), up to 250x faster and 20x more memory efficient than the original Chronos models of the same size!&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;13 Mar 2024&lt;/strong&gt;: üöÄ Chronos &lt;a href="https://arxiv.org/abs/2403.07815"&gt;paper&lt;/a&gt; and inference code released.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;‚ú® Introduction&lt;/h2&gt; 
&lt;p&gt;This package provides an interface to the Chronos family of &lt;strong&gt;pretrained time series forecasting models&lt;/strong&gt;. The following model types are supported.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Chronos-2&lt;/strong&gt;: Our latest model with significantly enhanced capabilities. It offers zero-shot support for univariate, multivariate, and covariate-informed forecasting tasks. Chronos-2 delivers state-of-the-art zero-shot performance across multiple benchmarks (including fev-bench and GIFT-Eval), with the largest improvements observed on tasks that include exogenous features. It also achieves a win rate of over 90% against Chronos-Bolt in head-to-head comparisons. To learn more about Chronos, check out the &lt;a href="https://arxiv.org/abs/2510.15821"&gt;technical report&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Chronos-Bolt&lt;/strong&gt;: A patch-based variant of Chronos. It chunks the historical time series context into patches of multiple observations, which are then input into the encoder. The decoder then uses these representations to directly generate quantile forecasts across multiple future steps‚Äîa method known as direct multi-step forecasting. Chronos-Bolt models are up to 250 times faster and 20 times more memory-efficient than the original Chronos models of the same size. To learn more about Chronos-Bolt, check out this &lt;a href="https://aws.amazon.com/blogs/machine-learning/fast-and-accurate-zero-shot-forecasting-with-chronos-bolt-and-autogluon/"&gt;blog post&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Chronos&lt;/strong&gt;: The original Chronos family which is based on language model architectures. A time series is transformed into a sequence of tokens via scaling and quantization, and a language model is trained on these tokens using the cross-entropy loss. Once trained, probabilistic forecasts are obtained by sampling multiple future trajectories given the historical context. To learn more about Chronos, check out the &lt;a href="https://openreview.net/forum?id=gerNCVqqtR"&gt;publication&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Available Models&lt;/h3&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model ID&lt;/th&gt; 
    &lt;th&gt;Parameters&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://huggingface.co/amazon/chronos-2"&gt;&lt;code&gt;amazon/chronos-2&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;120M&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://huggingface.co/amazon/chronos-bolt-tiny"&gt;&lt;code&gt;amazon/chronos-bolt-tiny&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;9M&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://huggingface.co/amazon/chronos-bolt-mini"&gt;&lt;code&gt;amazon/chronos-bolt-mini&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;21M&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://huggingface.co/amazon/chronos-bolt-small"&gt;&lt;code&gt;amazon/chronos-bolt-small&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;48M&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://huggingface.co/amazon/chronos-bolt-base"&gt;&lt;code&gt;amazon/chronos-bolt-base&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;205M&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://huggingface.co/amazon/chronos-t5-tiny"&gt;&lt;code&gt;amazon/chronos-t5-tiny&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;8M&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://huggingface.co/amazon/chronos-t5-mini"&gt;&lt;code&gt;amazon/chronos-t5-mini&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;20M&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://huggingface.co/amazon/chronos-t5-small"&gt;&lt;code&gt;amazon/chronos-t5-small&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;46M&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://huggingface.co/amazon/chronos-t5-base"&gt;&lt;code&gt;amazon/chronos-t5-base&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;200M&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://huggingface.co/amazon/chronos-t5-large"&gt;&lt;code&gt;amazon/chronos-t5-large&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;710M&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h2&gt;üìà Usage&lt;/h2&gt; 
&lt;p&gt;To perform inference with Chronos, the easiest way is to install this package through &lt;code&gt;pip&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;pip install chronos-forecasting
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Forecasting&lt;/h3&gt; 
&lt;p&gt;A minimal example showing how to perform forecasting using Chronos-2:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import pandas as pd  # requires: pip install 'pandas[pyarrow]'
from chronos import Chronos2Pipeline

pipeline = Chronos2Pipeline.from_pretrained("amazon/chronos-2", device_map="cuda")

# Load historical target values and past values of covariates
context_df = pd.read_parquet("https://autogluon.s3.amazonaws.com/datasets/timeseries/electricity_price/train.parquet")

# (Optional) Load future values of covariates
test_df = pd.read_parquet("https://autogluon.s3.amazonaws.com/datasets/timeseries/electricity_price/test.parquet")
future_df = test_df.drop(columns="target")

# Generate predictions with covariates
pred_df = pipeline.predict_df(
    context_df,
    future_df=future_df,
    prediction_length=24,  # Number of steps to forecast
    quantile_levels=[0.1, 0.5, 0.9],  # Quantile for probabilistic forecast
    id_column="id",  # Column identifying different time series
    timestamp_column="timestamp",  # Column with datetime information
    target="target",  # Column(s) with time series values to predict
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;We can now visualize the forecast:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import matplotlib.pyplot as plt  # requires: pip install matplotlib

ts_context = context_df.set_index("timestamp")["target"].tail(256)
ts_pred = pred_df.set_index("timestamp")
ts_ground_truth = test_df.set_index("timestamp")["target"]

ts_context.plot(label="historical data", color="xkcd:azure", figsize=(12, 3))
ts_ground_truth.plot(label="future data (ground truth)", color="xkcd:grass green")
ts_pred["predictions"].plot(label="forecast", color="xkcd:violet")
plt.fill_between(
    ts_pred.index,
    ts_pred["0.1"],
    ts_pred["0.9"],
    alpha=0.7,
    label="prediction interval",
    color="xkcd:light lavender",
)
plt.legend()
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Example Notebooks&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/amazon-science/chronos-forecasting/main/notebooks/chronos-2-quickstart.ipynb"&gt;Chronos-2 Quick Start&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/amazon-science/chronos-forecasting/main/notebooks/deploy-chronos-bolt-to-amazon-sagemaker.ipynb"&gt;Deploy Chronos-Bolt on Amazon SageMaker&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Deploy Chronos-2 on Amazon SageMaker (coming soon!)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üìù Citation&lt;/h2&gt; 
&lt;p&gt;If you find Chronos models useful for your research, please consider citing the associated papers:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@article{ansari2024chronos,
  title={Chronos: Learning the Language of Time Series},
  author={Ansari, Abdul Fatir and Stella, Lorenzo and Turkmen, Caner and Zhang, Xiyuan, and Mercado, Pedro and Shen, Huibin and Shchur, Oleksandr and Rangapuram, Syama Syndar and Pineda Arango, Sebastian and Kapoor, Shubham and Zschiegner, Jasper and Maddix, Danielle C. and Mahoney, Michael W. and Torkkola, Kari and Gordon Wilson, Andrew and Bohlke-Schneider, Michael and Wang, Yuyang},
  journal={Transactions on Machine Learning Research},
  issn={2835-8856},
  year={2024},
  url={https://openreview.net/forum?id=gerNCVqqtR}
}

@article{ansari2025chronos2,
  title        = {Chronos-2: From Univariate to Universal Forecasting},
  author       = {Abdul Fatir Ansari and Oleksandr Shchur and Jaris K√ºken and Andreas Auer and Boran Han and Pedro Mercado and Syama Sundar Rangapuram and Huibin Shen and Lorenzo Stella and Xiyuan Zhang and Mononito Goswami and Shubham Kapoor and Danielle C. Maddix and Pablo Guerron and Tony Hu and Junming Yin and Nick Erickson and Prateek Mutalik Desai and Hao Wang and Huzefa Rangwala and George Karypis and Yuyang Wang and Michael Bohlke-Schneider},
  journal      = {arXiv preprint arXiv:2510.15821},
  year         = {2025},
  url          = {https://arxiv.org/abs/2510.15821}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;üõ°Ô∏è Security&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/amazon-science/chronos-forecasting/main/CONTRIBUTING.md#security-issue-notifications"&gt;CONTRIBUTING&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h2&gt;üìÉ License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the Apache-2.0 License.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Canner/WrenAI</title>
      <link>https://github.com/Canner/WrenAI</link>
      <description>&lt;p&gt;‚ö°Ô∏è GenBI (Generative BI) queries any database in natural language, generates accurate SQL (Text-to-SQL), charts (Text-to-Chart), and AI-powered business intelligence in seconds.&lt;/p&gt;&lt;hr&gt;&lt;p align="center" id="top"&gt; &lt;a href="https://getwren.ai/?utm_source=github&amp;amp;utm_medium=title&amp;amp;utm_campaign=readme"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: light)" srcset="./misc/wrenai_logo.png" /&gt; 
   &lt;img src="https://raw.githubusercontent.com/Canner/WrenAI/main/misc/wrenai_logo_white.png" width="300px" /&gt; 
  &lt;/picture&gt; &lt;/a&gt;&lt;/p&gt;
&lt;h1 align="center"&gt;&lt;a href="https://getwren.ai/?utm_source=github&amp;amp;utm_medium=title&amp;amp;utm_campaign=readme"&gt;Wren AI - Open-Source GenBI Agent&lt;/a&gt;&lt;/h1&gt;
&lt;a href="https://getwren.ai/?utm_source=github&amp;amp;utm_medium=title&amp;amp;utm_campaign=readme"&gt; &lt;/a&gt; 
&lt;p&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a aria-label="Follow us on X" href="https://x.com/getwrenai"&gt; &lt;img alt="" src="https://img.shields.io/badge/-@getwrenai-blue?style=for-the-badge&amp;amp;logo=x&amp;amp;logoColor=white&amp;amp;labelColor=gray&amp;amp;logoWidth=20" /&gt; &lt;/a&gt; &lt;a aria-label="Releases" href="https://github.com/canner/WrenAI/releases"&gt; &lt;img alt="" src="https://img.shields.io/github/v/release/canner/WrenAI?logo=github&amp;amp;label=GitHub%20Release&amp;amp;color=blue&amp;amp;style=for-the-badge" /&gt; &lt;/a&gt; &lt;a aria-label="License" href="https://github.com/Canner/WrenAI/raw/main/LICENSE"&gt; &lt;img alt="" src="https://img.shields.io/github/license/canner/WrenAI?color=blue&amp;amp;style=for-the-badge" /&gt; &lt;/a&gt; &lt;a href="https://docs.getwren.ai"&gt; &lt;img src="https://img.shields.io/badge/docs-online-brightgreen?style=for-the-badge" alt="Docs" /&gt; &lt;/a&gt; &lt;a aria-label="Join the community on GitHub" href="https://discord.gg/5DvshJqG8Z"&gt; &lt;img alt="" src="https://img.shields.io/badge/-JOIN%20THE%20COMMUNITY-blue?style=for-the-badge&amp;amp;logo=discord&amp;amp;logoColor=white&amp;amp;labelColor=grey&amp;amp;logoWidth=20" /&gt; &lt;/a&gt; &lt;a aria-label="Canner" href="https://cannerdata.com/?utm_source=github&amp;amp;utm_medium=badge&amp;amp;utm_campaign=readme"&gt; &lt;img src="https://img.shields.io/badge/%F0%9F%A7%A1-Made%20by%20Canner-blue?style=for-the-badge" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/9263" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/9263" alt="Canner%2FWrenAI | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;‚ö° GenBI (Generative BI) queries any database in natural language, generates accurate SQL (Text-to-SQL), charts (Text-to-Chart), and AI-powered business intelligence in seconds. Ô∏è&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p align="center"&gt; &lt;img width="1920" height="1080" alt="1" src="https://github.com/user-attachments/assets/bba9d37a-33e3-49ab-b7cb-32fd6dddc8d1" /&gt; &lt;/p&gt; 
&lt;h2&gt;üòç Demos&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/f9c1cb34-5a95-4580-8890-ec9644da4160"&gt;https://github.com/user-attachments/assets/f9c1cb34-5a95-4580-8890-ec9644da4160&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/90ad1d35-bb1e-490b-9676-b29863ff090b"&gt;Watch GenBI Demo&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;ü§ñ Features&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th&gt;What you get&lt;/th&gt; 
   &lt;th&gt;Why it matters&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Talk to Your Data&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Ask in any language ‚Üí precise SQL &amp;amp; answers&lt;/td&gt; 
   &lt;td&gt;Slash the SQL learning curveÔªø&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;GenBI Insights&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;AI-written summaries, charts &amp;amp; reports&lt;/td&gt; 
   &lt;td&gt;Decision-ready context in one clickÔªø&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Semantic Layer&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;MDL models encode schema, metrics, joins&lt;/td&gt; 
   &lt;td&gt;Keeps LLM outputs accurate &amp;amp; governedÔªø&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Embed via API&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Generate queries &amp;amp; charts inside your apps (&lt;a href="https://wrenai.readme.io/reference/cloud-getting-started"&gt;API Docs&lt;/a&gt;)&lt;/td&gt; 
   &lt;td&gt;Build custom agents, SaaS features, chatbotsÔªø (&lt;a href="https://huggingface.co/spaces/getWrenAI/wrenai-cloud-api-demo"&gt;Streamlit Live Demo&lt;/a&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;ü§© &lt;a href="https://getwren.ai/genbi?utm_source=github&amp;amp;utm_medium=content&amp;amp;utm_campaign=readme"&gt;Learn more about GenBI&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;üöÄ Getting Started&lt;/h2&gt; 
&lt;p&gt;Using Wren AI is super simple, you can set it up within 3 minutes, and start to interact with your data!&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img width="1920" height="1080" alt="2" src="https://github.com/user-attachments/assets/6555f539-9ef2-485d-9135-0071741fda96" /&gt; &lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Visit our &lt;a href="http://docs.getwren.ai/oss/installation?utm_source=github&amp;amp;utm_medium=content&amp;amp;utm_campaign=readme"&gt;Install in your local environment&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Visit the &lt;a href="https://docs.getwren.ai/oss/guide/connect/overview?utm_source=github&amp;amp;utm_medium=content&amp;amp;utm_campaign=readme"&gt;Usage Guides&lt;/a&gt; to learn more about how to use Wren AI.&lt;/li&gt; 
 &lt;li&gt;Or just start with &lt;a href="https://getwren.ai/?utm_source=github&amp;amp;utm_medium=content&amp;amp;utm_campaign=readme"&gt;Wren AI Cloud&lt;/a&gt; our Managed Cloud Service. (&lt;a href="https://docs.getwren.ai/oss/overview/cloud_vs_self_host"&gt;OSS vs. Commercial Plans&lt;/a&gt;).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üèóÔ∏è Architecture&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;img width="1011" height="682" alt="wrenai-architecture" src="https://github.com/user-attachments/assets/e99b999f-9912-4fa7-921a-9c86b6b83354" /&gt; &lt;/p&gt; 
&lt;p&gt;üëâ &lt;a href="https://getwren.ai/post/how-we-design-our-semantic-engine-for-llms-the-backbone-of-the-semantic-layer-for-llm-architecture?utm_source=github&amp;amp;utm_medium=content&amp;amp;utm_campaign=readme"&gt;Learn more about our Design&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;üîå Data Sources&lt;/h2&gt; 
&lt;p&gt;If your data source is not listed here, vote for it in our &lt;a href="https://github.com/Canner/WrenAI/discussions/327"&gt;GitHub discussion thread&lt;/a&gt;. It will be a valuable input for us to decide on the next supported data sources.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Athena (Trino)&lt;/li&gt; 
 &lt;li&gt;Redshift&lt;/li&gt; 
 &lt;li&gt;BigQuery&lt;/li&gt; 
 &lt;li&gt;DuckDB&lt;/li&gt; 
 &lt;li&gt;PostgreSQL&lt;/li&gt; 
 &lt;li&gt;MySQL&lt;/li&gt; 
 &lt;li&gt;Microsoft SQL Server&lt;/li&gt; 
 &lt;li&gt;ClickHouse&lt;/li&gt; 
 &lt;li&gt;Oracle&lt;/li&gt; 
 &lt;li&gt;Trino&lt;/li&gt; 
 &lt;li&gt;Snowflake&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ü§ñ LLM Models&lt;/h2&gt; 
&lt;p&gt;Wren AI supports integration with various Large Language Models (LLMs), including but not limited to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;OpenAI Models&lt;/li&gt; 
 &lt;li&gt;Azure OpenAI Models&lt;/li&gt; 
 &lt;li&gt;DeepSeek Models&lt;/li&gt; 
 &lt;li&gt;Google AI Studio ‚Äì Gemini Models&lt;/li&gt; 
 &lt;li&gt;Vertex AI Models (Gemini + Anthropic)&lt;/li&gt; 
 &lt;li&gt;Bedrock Models&lt;/li&gt; 
 &lt;li&gt;Anthropic API Models&lt;/li&gt; 
 &lt;li&gt;Groq Models&lt;/li&gt; 
 &lt;li&gt;Ollama Models&lt;/li&gt; 
 &lt;li&gt;Databricks Models&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Check &lt;a href="https://github.com/Canner/WrenAI/tree/main/wren-ai-service/docs/config_examples"&gt;configuration examples here&lt;/a&gt;!&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!CAUTION] The performance of Wren AI depends significantly on the capabilities of the LLM you choose. We strongly recommend using the most powerful model available for optimal results. Using less capable models may lead to reduced performance, slower response times, or inaccurate outputs.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;üìö Documentation&lt;/h2&gt; 
&lt;p&gt;Visit &lt;a href="https://docs.getwren.ai/oss/overview/introduction?utm_source=github&amp;amp;utm_medium=content&amp;amp;utm_campaign=readme"&gt;Wren AI documentation&lt;/a&gt; to view the full documentation.&lt;/p&gt; 
&lt;h2&gt;üì™ Keep Posted?&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.getwren.ai/blog/?utm_source=github&amp;amp;utm_medium=content&amp;amp;utm_campaign=readme"&gt;Subscribe our blog&lt;/a&gt; and &lt;a href="https://www.linkedin.com/company/wrenai"&gt;Follow our LinkedIn&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;üõ†Ô∏è Contribution&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Star ‚≠ê the repo to show support (it really helps).&lt;/li&gt; 
 &lt;li&gt;Open an issue for bugs, ideas, or discussions.&lt;/li&gt; 
 &lt;li&gt;Read &lt;a href="https://github.com/Canner/WrenAI/raw/main/CONTRIBUTING.md"&gt;Contribution Guidelines&lt;/a&gt; for setup &amp;amp; PR guidelines.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;‚≠êÔ∏è Community&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Join 1.3k+ developers in our &lt;a href="https://discord.gg/5DvshJqG8Z"&gt;Discord&lt;/a&gt; for real-time help and roadmap previews.&lt;/li&gt; 
 &lt;li&gt;If there are any issues, please visit &lt;a href="https://github.com/Canner/WrenAI/issues"&gt;GitHub Issues&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Explore our &lt;a href="https://wrenai.notion.site/"&gt;public roadmap&lt;/a&gt; to stay updated on upcoming features and improvements!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Please note that our &lt;a href="https://raw.githubusercontent.com/Canner/WrenAI/main/CODE_OF_CONDUCT.md"&gt;Code of Conduct&lt;/a&gt; applies to all Wren AI community channels. Users are &lt;strong&gt;highly encouraged&lt;/strong&gt; to read and adhere to them to avoid repercussions.&lt;/p&gt; 
&lt;h2&gt;üéâ Our Contributors&lt;/h2&gt; 
&lt;a href="https://github.com/canner/wrenAI/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=Canner/WrenAI" /&gt; &lt;/a&gt; 
&lt;p align="right"&gt; &lt;a href="https://raw.githubusercontent.com/Canner/WrenAI/main/#top"&gt;‚¨ÜÔ∏è Back to Top&lt;/a&gt; &lt;/p&gt;</description>
    </item>
    
    <item>
      <title>slidevjs/slidev</title>
      <link>https://github.com/slidevjs/slidev</link>
      <description>&lt;p&gt;Presentation Slides for Developers&lt;/p&gt;&lt;hr&gt;&lt;br /&gt; 
&lt;p align="center"&gt; &lt;a href="https://sli.dev" target="_blank"&gt; &lt;img src="https://sli.dev/logo-title.png" alt="Slidev" height="250" width="250" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; Presentation &lt;b&gt;slide&lt;/b&gt;s for &lt;b&gt;dev&lt;/b&gt;elopers üßë‚Äçüíªüë©‚Äçüíªüë®‚Äçüíª &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://www.npmjs.com/package/@slidev/cli" target="__blank"&gt;&lt;img src="https://img.shields.io/npm/v/@slidev/cli?color=2B90B6&amp;amp;label=" alt="NPM version" /&gt;&lt;/a&gt; &lt;a href="https://www.npmjs.com/package/@slidev/cli" target="__blank"&gt;&lt;img alt="NPM Downloads" src="https://img.shields.io/npm/dm/@slidev/cli?color=349dbe&amp;amp;label=" /&gt;&lt;/a&gt; &lt;a href="https://sli.dev/" target="__blank"&gt;&lt;img src="https://img.shields.io/static/v1?label=&amp;amp;message=docs%20%26%20demos&amp;amp;color=45b8cd" alt="Docs &amp;amp; Demos" /&gt;&lt;/a&gt; &lt;a href="https://sli.dev/resources/theme-gallery" target="__blank"&gt;&lt;img src="https://img.shields.io/static/v1?label=&amp;amp;message=themes&amp;amp;color=4ec5d4" alt="Themes" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;a href="https://github.com/slidevjs/slidev/stargazers" target="__blank"&gt;&lt;img alt="GitHub stars" src="https://img.shields.io/github/stars/slidevjs/slidev?style=social" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://twitter.com/antfu7/status/1389604687502995457"&gt;Video Preview&lt;/a&gt; | &lt;a href="https://sli.dev"&gt;Documentation&lt;/a&gt; &lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt;
    &lt;td align="center"&gt; &lt;img width="2000" height="0" alt="" aria-hidden /&gt;&lt;br /&gt; &lt;sub&gt;Made possible by my &lt;a href="https://github.com/sponsors/antfu"&gt;Sponsor Program üíñ&lt;/a&gt;&lt;/sub&gt;&lt;br /&gt; &lt;img width="2000" height="0" alt="" aria-hidden /&gt; &lt;/td&gt; 
   &lt;/tr&gt;
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìù &lt;a href="https://sli.dev/guide/syntax"&gt;&lt;strong&gt;Markdown-based&lt;/strong&gt;&lt;/a&gt; - focus on content and use your favorite editor&lt;/li&gt; 
 &lt;li&gt;üßë‚Äçüíª &lt;a href="https://sli.dev/guide/syntax#code-blocks"&gt;&lt;strong&gt;Developer Friendly&lt;/strong&gt;&lt;/a&gt; - built-in code highlighting, live coding, etc.&lt;/li&gt; 
 &lt;li&gt;üé® &lt;a href="https://sli.dev/resources/theme-gallery"&gt;&lt;strong&gt;Themable&lt;/strong&gt;&lt;/a&gt; - theme can be shared and used with npm packages&lt;/li&gt; 
 &lt;li&gt;üåà &lt;a href="https://sli.dev/guide/syntax#embedded-styles"&gt;&lt;strong&gt;Stylish&lt;/strong&gt;&lt;/a&gt; - on-demand utilities via &lt;a href="https://github.com/unocss/unocss"&gt;UnoCSS&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;ü§π &lt;a href="https://sli.dev/custom/directory-structure#components"&gt;&lt;strong&gt;Interactive&lt;/strong&gt;&lt;/a&gt; - embedding Vue components seamlessly&lt;/li&gt; 
 &lt;li&gt;üéô &lt;a href="https://sli.dev/guide/ui#presenter-mode"&gt;&lt;strong&gt;Presenter Mode&lt;/strong&gt;&lt;/a&gt; - use another window, or even your phone to control your slides&lt;/li&gt; 
 &lt;li&gt;üé® &lt;a href="https://sli.dev/features/drawing"&gt;&lt;strong&gt;Drawing&lt;/strong&gt;&lt;/a&gt; - draw and annotate on your slides&lt;/li&gt; 
 &lt;li&gt;üßÆ &lt;a href="https://sli.dev/features/latex"&gt;&lt;strong&gt;LaTeX&lt;/strong&gt;&lt;/a&gt; - built-in LaTeX math equations support&lt;/li&gt; 
 &lt;li&gt;üì∞ &lt;a href="https://sli.dev/guide/syntax#diagrams"&gt;&lt;strong&gt;Diagrams&lt;/strong&gt;&lt;/a&gt; - creates diagrams using textual descriptions with &lt;a href="https://mermaid.js.org/"&gt;Mermaid&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üåü &lt;a href="https://sli.dev/features/icons"&gt;&lt;strong&gt;Icons&lt;/strong&gt;&lt;/a&gt; - access to icons from any icon set directly&lt;/li&gt; 
 &lt;li&gt;üíª &lt;a href="https://sli.dev/guide/index#editor"&gt;&lt;strong&gt;Editor&lt;/strong&gt;&lt;/a&gt; - integrated editor, or the &lt;a href="https://sli.dev/features/vscode-extension"&gt;VSCode extension&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üé• &lt;a href="https://sli.dev/features/recording"&gt;&lt;strong&gt;Recording&lt;/strong&gt;&lt;/a&gt; - built-in recording and camera view&lt;/li&gt; 
 &lt;li&gt;üì§ &lt;a href="https://sli.dev/guide/exporting"&gt;&lt;strong&gt;Portable&lt;/strong&gt;&lt;/a&gt; - export into PDF, PNGs, or PPTX&lt;/li&gt; 
 &lt;li&gt;‚ö°Ô∏è &lt;a href="https://vitejs.dev"&gt;&lt;strong&gt;Fast&lt;/strong&gt;&lt;/a&gt; - instant reloading powered by &lt;a href="https://vitejs.dev"&gt;Vite&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üõ† &lt;a href="https://sli.dev/custom/"&gt;&lt;strong&gt;Hackable&lt;/strong&gt;&lt;/a&gt; - using Vite plugins, Vue components, or any npm packages&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;h3&gt;Try it Online ‚ö°Ô∏è&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://sli.dev/new"&gt;sli.dev/new&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://sli.dev/new"&gt;&lt;img src="https://developer.stackblitz.com/img/open_in_stackblitz.svg?sanitize=true" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Init Project Locally&lt;/h3&gt; 
&lt;p&gt;Install &lt;a href="https://nodejs.org/"&gt;Node.js &amp;gt;=18&lt;/a&gt; and run the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npm init slidev
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Documentation: &lt;strong&gt;&lt;a href="https://sli.dev"&gt;English&lt;/a&gt;&lt;/strong&gt; | &lt;a href="https://cn.sli.dev"&gt;‰∏≠ÊñáÊñáÊ°£&lt;/a&gt; | &lt;a href="https://fr.sli.dev"&gt;Fran√ßais&lt;/a&gt; | &lt;a href="https://es.sli.dev"&gt;Espa√±ol&lt;/a&gt; | &lt;a href="https://ru.sli.dev"&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt; | &lt;a href="https://br.sli.dev"&gt;Portugu√™s-BR&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Discord: &lt;a href="https://chat.sli.dev"&gt;chat.sli.dev&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;For a full example, you can check the &lt;a href="https://github.com/slidevjs/slidev/raw/main/demo"&gt;demo&lt;/a&gt; folder, which is also the source file for &lt;a href="https://antfu.me/posts/composable-vue-vueday-2021"&gt;my previous talk&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Tech Stack&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://vitejs.dev"&gt;Vite&lt;/a&gt; - An extremely fast frontend tooling&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://v3.vuejs.org/"&gt;Vue 3&lt;/a&gt; powered &lt;a href="https://daringfireball.net/projects/markdown/syntax"&gt;Markdown&lt;/a&gt; - Focus on the content while having the power of HTML and Vue components whenever needed&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/unocss/unocss"&gt;UnoCSS&lt;/a&gt; - On-demand utility-first CSS engine, style your slides at ease&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/shikijs/shiki"&gt;Shiki&lt;/a&gt;, &lt;a href="https://github.com/Microsoft/monaco-editor"&gt;Monaco Editor&lt;/a&gt; - First-class code snippets support with live coding capability&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://recordrtc.org"&gt;RecordRTC&lt;/a&gt; - Built-in recording and camera view&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://vueuse.org"&gt;VueUse&lt;/a&gt; family - &lt;a href="https://github.com/vueuse/vueuse"&gt;&lt;code&gt;@vueuse/core&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://github.com/vueuse/motion"&gt;&lt;code&gt;@vueuse/motion&lt;/code&gt;&lt;/a&gt;, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://iconify.design/"&gt;Iconify&lt;/a&gt; - Icon sets collection.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/antfu/drauu"&gt;Drauu&lt;/a&gt; - Drawing and annotations support&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://katex.org/"&gt;KaTeX&lt;/a&gt; - LaTeX math rendering.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://mermaid-js.github.io/mermaid"&gt;Mermaid&lt;/a&gt; - Textual Diagrams.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Sponsors&lt;/h2&gt; 
&lt;p&gt;This project is made possible by all the sponsors supporting my work:&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/sponsors/antfu"&gt; &lt;img src="https://cdn.jsdelivr.net/gh/antfu/static/sponsors.svg?sanitize=true" alt="Logos from Sponsors" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;MIT License ¬© 2021 &lt;a href="https://github.com/antfu"&gt;Anthony Fu&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>lfnovo/open-notebook</title>
      <link>https://github.com/lfnovo/open-notebook</link>
      <description>&lt;p&gt;An Open Source implementation of Notebook LM with more flexibility and features&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a id="readme-top"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;!-- [![Contributors][contributors-shield]][contributors-url] --&gt; 
&lt;p&gt;&lt;a href="https://github.com/lfnovo/open-notebook/network/members"&gt;&lt;img src="https://img.shields.io/github/forks/lfnovo/open-notebook.svg?style=for-the-badge" alt="Forks" /&gt;&lt;/a&gt; &lt;a href="https://github.com/lfnovo/open-notebook/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/lfnovo/open-notebook.svg?style=for-the-badge" alt="Stargazers" /&gt;&lt;/a&gt; &lt;a href="https://github.com/lfnovo/open-notebook/issues"&gt;&lt;img src="https://img.shields.io/github/issues/lfnovo/open-notebook.svg?style=for-the-badge" alt="Issues" /&gt;&lt;/a&gt; &lt;a href="https://github.com/lfnovo/open-notebook/raw/master/LICENSE.txt"&gt;&lt;img src="https://img.shields.io/github/license/lfnovo/open-notebook.svg?style=for-the-badge" alt="MIT License" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;!-- [![LinkedIn][linkedin-shield]][linkedin-url] --&gt; 
&lt;!-- PROJECT LOGO --&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/lfnovo/open-notebook"&gt; &lt;img src="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/assets/hero.svg?sanitize=true" alt="Logo" /&gt; &lt;/a&gt; 
 &lt;h3 align="center"&gt;Open Notebook&lt;/h3&gt; 
 &lt;p align="center"&gt; An open source, privacy-focused alternative to Google's Notebook LM! &lt;br /&gt;&lt;strong&gt;Join our &lt;a href="https://discord.gg/37XJPXfz2w"&gt;Discord server&lt;/a&gt; for help, to share workflow ideas, and suggest features!&lt;/strong&gt; &lt;br /&gt; &lt;a href="https://www.open-notebook.ai"&gt;&lt;strong&gt;Checkout our website ¬ª&lt;/strong&gt;&lt;/a&gt; &lt;br /&gt; &lt;br /&gt; &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/index.md"&gt;üìö Get Started&lt;/a&gt; ¬∑ &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/index.md"&gt;üìñ User Guide&lt;/a&gt; ¬∑ &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/features/index.md"&gt;‚ú® Features&lt;/a&gt; ¬∑ &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/deployment/index.md"&gt;üöÄ Deploy&lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;!-- Keep these links. Translations will automatically update with the README. --&gt; 
 &lt;a href="https://zdoc.app/de/lfnovo/open-notebook"&gt;Deutsch&lt;/a&gt; | 
 &lt;a href="https://zdoc.app/es/lfnovo/open-notebook"&gt;Espa√±ol&lt;/a&gt; | 
 &lt;a href="https://zdoc.app/fr/lfnovo/open-notebook"&gt;fran√ßais&lt;/a&gt; | 
 &lt;a href="https://zdoc.app/ja/lfnovo/open-notebook"&gt;Êó•Êú¨Ë™û&lt;/a&gt; | 
 &lt;a href="https://zdoc.app/ko/lfnovo/open-notebook"&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt; | 
 &lt;a href="https://zdoc.app/pt/lfnovo/open-notebook"&gt;Portugu√™s&lt;/a&gt; | 
 &lt;a href="https://zdoc.app/ru/lfnovo/open-notebook"&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt; | 
 &lt;a href="https://zdoc.app/zh/lfnovo/open-notebook"&gt;‰∏≠Êñá&lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;A private, multi-model, 100% local, full-featured alternative to Notebook LM&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/assets/asset_list.png" alt="New Notebook" /&gt;&lt;/p&gt; 
&lt;p&gt;In a world dominated by Artificial Intelligence, having the ability to think üß† and acquire new knowledge üí°, is a skill that should not be a privilege for a few, nor restricted to a single provider.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Open Notebook empowers you to:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üîí &lt;strong&gt;Control your data&lt;/strong&gt; - Keep your research private and secure&lt;/li&gt; 
 &lt;li&gt;ü§ñ &lt;strong&gt;Choose your AI models&lt;/strong&gt; - Support for 16+ providers including OpenAI, Anthropic, Ollama, LM Studio, and more&lt;/li&gt; 
 &lt;li&gt;üìö &lt;strong&gt;Organize multi-modal content&lt;/strong&gt; - PDFs, videos, audio, web pages, and more&lt;/li&gt; 
 &lt;li&gt;üéôÔ∏è &lt;strong&gt;Generate professional podcasts&lt;/strong&gt; - Advanced multi-speaker podcast generation&lt;/li&gt; 
 &lt;li&gt;üîç &lt;strong&gt;Search intelligently&lt;/strong&gt; - Full-text and vector search across all your content&lt;/li&gt; 
 &lt;li&gt;üí¨ &lt;strong&gt;Chat with context&lt;/strong&gt; - AI conversations powered by your research&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Learn more about our project at &lt;a href="https://www.open-notebook.ai"&gt;https://www.open-notebook.ai&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;‚ö†Ô∏è IMPORTANT: v1.0 Breaking Changes&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;If you're upgrading from a previous version&lt;/strong&gt;, please note:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üè∑Ô∏è &lt;strong&gt;Docker tags have changed&lt;/strong&gt;: The &lt;code&gt;latest&lt;/code&gt; tag is now &lt;strong&gt;frozen&lt;/strong&gt; at the last Streamlit version&lt;/li&gt; 
 &lt;li&gt;üÜï &lt;strong&gt;Use &lt;code&gt;v1-latest&lt;/code&gt; tag&lt;/strong&gt; for the new React/Next.js version (recommended)&lt;/li&gt; 
 &lt;li&gt;üîå &lt;strong&gt;Port 5055 required&lt;/strong&gt;: You must expose port 5055 for the API to work&lt;/li&gt; 
 &lt;li&gt;üìñ &lt;strong&gt;Read the migration guide&lt;/strong&gt;: See &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/MIGRATION.md"&gt;MIGRATION.md&lt;/a&gt; for detailed upgrade instructions&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;New users&lt;/strong&gt;: You can ignore this notice and proceed with the Quick Start below using the &lt;code&gt;v1-latest-single&lt;/code&gt; tag.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üÜö Open Notebook vs Google Notebook LM&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Feature&lt;/th&gt; 
   &lt;th&gt;Open Notebook&lt;/th&gt; 
   &lt;th&gt;Google Notebook LM&lt;/th&gt; 
   &lt;th&gt;Advantage&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Privacy &amp;amp; Control&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Self-hosted, your data&lt;/td&gt; 
   &lt;td&gt;Google cloud only&lt;/td&gt; 
   &lt;td&gt;Complete data sovereignty&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;AI Provider Choice&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;16+ providers (OpenAI, Anthropic, Ollama, LM Studio, etc.)&lt;/td&gt; 
   &lt;td&gt;Google models only&lt;/td&gt; 
   &lt;td&gt;Flexibility and cost optimization&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Podcast Speakers&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;1-4 speakers with custom profiles&lt;/td&gt; 
   &lt;td&gt;2 speakers only&lt;/td&gt; 
   &lt;td&gt;Extreme flexibility&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Context Control&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;3 granular levels&lt;/td&gt; 
   &lt;td&gt;All-or-nothing&lt;/td&gt; 
   &lt;td&gt;Privacy and performance tuning&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Content Transformations&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Custom and built-in&lt;/td&gt; 
   &lt;td&gt;Limited options&lt;/td&gt; 
   &lt;td&gt;Unlimited processing power&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;API Access&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Full REST API&lt;/td&gt; 
   &lt;td&gt;No API&lt;/td&gt; 
   &lt;td&gt;Complete automation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Deployment&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Docker, cloud, or local&lt;/td&gt; 
   &lt;td&gt;Google hosted only&lt;/td&gt; 
   &lt;td&gt;Deploy anywhere&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Citations&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Comprehensive with sources&lt;/td&gt; 
   &lt;td&gt;Basic references&lt;/td&gt; 
   &lt;td&gt;Research integrity&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Customization&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Open source, fully customizable&lt;/td&gt; 
   &lt;td&gt;Closed system&lt;/td&gt; 
   &lt;td&gt;Unlimited extensibility&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Cost&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Pay only for AI usage&lt;/td&gt; 
   &lt;td&gt;Monthly subscription + usage&lt;/td&gt; 
   &lt;td&gt;Transparent and controllable&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Why Choose Open Notebook?&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üîí &lt;strong&gt;Privacy First&lt;/strong&gt;: Your sensitive research stays completely private&lt;/li&gt; 
 &lt;li&gt;üí∞ &lt;strong&gt;Cost Control&lt;/strong&gt;: Choose cheaper AI providers or run locally with Ollama&lt;/li&gt; 
 &lt;li&gt;üéôÔ∏è &lt;strong&gt;Better Podcasts&lt;/strong&gt;: Full script control and multi-speaker flexibility vs limited 2-speaker deep-dive format&lt;/li&gt; 
 &lt;li&gt;üîß &lt;strong&gt;Unlimited Customization&lt;/strong&gt;: Modify, extend, and integrate as needed&lt;/li&gt; 
 &lt;li&gt;üåê &lt;strong&gt;No Vendor Lock-in&lt;/strong&gt;: Switch providers, deploy anywhere, own your data&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Built With&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://www.python.org/"&gt;&lt;img src="https://img.shields.io/badge/Python-3776AB?style=for-the-badge&amp;amp;logo=python&amp;amp;logoColor=white" alt="Python" /&gt;&lt;/a&gt; &lt;a href="https://nextjs.org/"&gt;&lt;img src="https://img.shields.io/badge/Next.js-000000?style=for-the-badge&amp;amp;logo=next.js&amp;amp;logoColor=white" alt="Next.js" /&gt;&lt;/a&gt; &lt;a href="https://reactjs.org/"&gt;&lt;img src="https://img.shields.io/badge/React-61DAFB?style=for-the-badge&amp;amp;logo=react&amp;amp;logoColor=black" alt="React" /&gt;&lt;/a&gt; &lt;a href="https://surrealdb.com/"&gt;&lt;img src="https://img.shields.io/badge/SurrealDB-FF5E00?style=for-the-badge&amp;amp;logo=databricks&amp;amp;logoColor=white" alt="SurrealDB" /&gt;&lt;/a&gt; &lt;a href="https://www.langchain.com/"&gt;&lt;img src="https://img.shields.io/badge/LangChain-3A3A3A?style=for-the-badge&amp;amp;logo=chainlink&amp;amp;logoColor=white" alt="LangChain" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;üöÄ Quick Start&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Docker Images Available:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Docker Hub&lt;/strong&gt;: &lt;code&gt;lfnovo/open_notebook:v1-latest-single&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;GitHub Container Registry&lt;/strong&gt;: &lt;code&gt;ghcr.io/lfnovo/open-notebook:v1-latest-single&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Both registries contain identical images - choose whichever you prefer!&lt;/p&gt; 
&lt;h3&gt;Choose Your Setup:&lt;/h3&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td width="50%"&gt; &lt;h4&gt;üè† &lt;strong&gt;Local Machine Setup&lt;/strong&gt;&lt;/h4&gt; &lt;p&gt;Perfect if Docker runs on the &lt;strong&gt;same computer&lt;/strong&gt; where you'll access Open Notebook.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;mkdir open-notebook &amp;amp;&amp;amp; cd open-notebook

docker run -d \
  --name open-notebook \
  -p 8502:8502 -p 5055:5055 \
  -v ./notebook_data:/app/data \
  -v ./surreal_data:/mydata \
  -e OPENAI_API_KEY=your_key_here \
  -e SURREAL_URL="ws://localhost:8000/rpc" \
  -e SURREAL_USER="root" \
  -e SURREAL_PASSWORD="root" \
  -e SURREAL_NAMESPACE="open_notebook" \
  -e SURREAL_DATABASE="production" \
  lfnovo/open_notebook:v1-latest-single
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Access at:&lt;/strong&gt; &lt;a href="http://localhost:8502"&gt;http://localhost:8502&lt;/a&gt;&lt;/p&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;h4&gt;üåê &lt;strong&gt;Remote Server Setup&lt;/strong&gt;&lt;/h4&gt; &lt;p&gt;Use this for servers, Raspberry Pi, NAS, Proxmox, or any remote machine.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;mkdir open-notebook &amp;amp;&amp;amp; cd open-notebook

docker run -d \
  --name open-notebook \
  -p 8502:8502 -p 5055:5055 \
  -v ./notebook_data:/app/data \
  -v ./surreal_data:/mydata \
  -e OPENAI_API_KEY=your_key_here \
  -e API_URL=http://YOUR_SERVER_IP:5055 \
  -e SURREAL_URL="ws://localhost:8000/rpc" \
  -e SURREAL_USER="root" \
  -e SURREAL_PASSWORD="root" \
  -e SURREAL_NAMESPACE="open_notebook" \
  -e SURREAL_DATABASE="production" \
  lfnovo/open_notebook:v1-latest-single
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Replace &lt;code&gt;YOUR_SERVER_IP&lt;/code&gt;&lt;/strong&gt; with your server's IP (e.g., &lt;code&gt;192.168.1.100&lt;/code&gt;) or domain&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Access at:&lt;/strong&gt; http://YOUR_SERVER_IP:8502&lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;‚ö†Ô∏è Critical Setup Notes:&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Both ports are required:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Port 8502&lt;/strong&gt;: Web interface (what you see in your browser)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Port 5055&lt;/strong&gt;: API backend (required for the app to function)&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;API_URL must match how YOU access the server:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;‚úÖ Access via &lt;code&gt;http://192.168.1.100:8502&lt;/code&gt; ‚Üí set &lt;code&gt;API_URL=http://192.168.1.100:5055&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;‚úÖ Access via &lt;code&gt;http://myserver.local:8502&lt;/code&gt; ‚Üí set &lt;code&gt;API_URL=http://myserver.local:5055&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;‚ùå Don't use &lt;code&gt;localhost&lt;/code&gt; for remote servers - it won't work from other devices!&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Using Docker Compose (Recommended for Easy Management)&lt;/h3&gt; 
&lt;p&gt;Create a &lt;code&gt;docker-compose.yml&lt;/code&gt; file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;services:
  open_notebook:
    image: lfnovo/open_notebook:v1-latest-single
    # Or use: ghcr.io/lfnovo/open-notebook:v1-latest-single
    ports:
      - "8502:8502"  # Web UI
      - "5055:5055"  # API (required!)
    environment:
      - OPENAI_API_KEY=your_key_here
      # For remote access, uncomment and set your server IP/domain:
      # - API_URL=http://192.168.1.100:5055
      # Database connection (required for single-container)
      - SURREAL_URL=ws://localhost:8000/rpc
      - SURREAL_USER=root
      - SURREAL_PASSWORD=root
      - SURREAL_NAMESPACE=open_notebook
      - SURREAL_DATABASE=production
    volumes:
      - ./notebook_data:/app/data
      - ./surreal_data:/mydata
    restart: always
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Start with: &lt;code&gt;docker compose up -d&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;What gets created:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;open-notebook/
‚îú‚îÄ‚îÄ docker-compose.yml # Your configuration
‚îú‚îÄ‚îÄ notebook_data/     # Your notebooks and research content
‚îî‚îÄ‚îÄ surreal_data/      # Database files
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;üÜò Quick Troubleshooting&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Problem&lt;/th&gt; 
   &lt;th&gt;Solution&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;"Unable to connect to server"&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Set &lt;code&gt;API_URL&lt;/code&gt; environment variable to match how you access the server (see remote setup above)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Blank page or errors&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Ensure BOTH ports (8502 and 5055) are exposed in your docker command&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Works on server but not from other computers&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Don't use &lt;code&gt;localhost&lt;/code&gt; in &lt;code&gt;API_URL&lt;/code&gt; - use your server's actual IP address&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;"404" or "config endpoint" errors&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Don't add &lt;code&gt;/api&lt;/code&gt; to &lt;code&gt;API_URL&lt;/code&gt; - use just &lt;code&gt;http://your-ip:5055&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Still having issues?&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Check our &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/troubleshooting/quick-fixes.md"&gt;5-minute troubleshooting guide&lt;/a&gt; or &lt;a href="https://discord.gg/37XJPXfz2w"&gt;join Discord&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;How Open Notebook Works&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Your Browser                                           ‚îÇ
‚îÇ  Access: http://your-server-ip:8502                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
                 ‚ñº
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ   Port 8502   ‚îÇ  ‚Üê Next.js Frontend (what you see)
         ‚îÇ   Frontend    ‚îÇ    Also proxies API requests internally!
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ proxies /api/* requests ‚Üì
                 ‚ñº
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ   Port 5055   ‚îÇ  ‚Üê FastAPI Backend (handles requests)
         ‚îÇ     API       ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
                 ‚ñº
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ   SurrealDB   ‚îÇ  ‚Üê Database (internal, auto-configured)
         ‚îÇ   (Port 8000) ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Key Points:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;v1.1+&lt;/strong&gt;: Next.js automatically proxies &lt;code&gt;/api/*&lt;/code&gt; requests to the backend, simplifying reverse proxy setup&lt;/li&gt; 
 &lt;li&gt;Your browser loads the frontend from port 8502&lt;/li&gt; 
 &lt;li&gt;The frontend needs to know where to find the API - when accessing remotely, set: &lt;code&gt;API_URL=http://your-server-ip:5055&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Behind reverse proxy?&lt;/strong&gt; You only need to proxy to port 8502 now! See &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/deployment/reverse-proxy.md"&gt;Reverse Proxy Guide&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.star-history.com/#lfnovo/open-notebook&amp;amp;type=date&amp;amp;legend=top-left"&gt;&lt;img src="https://api.star-history.com/svg?repos=lfnovo/open-notebook&amp;amp;type=date&amp;amp;legend=top-left" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;üõ†Ô∏è Full Installation&lt;/h3&gt; 
&lt;p&gt;For development or customization:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/lfnovo/open-notebook
cd open-notebook
make start-all
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;üìñ Need Help?&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ü§ñ AI Installation Assistant&lt;/strong&gt;: We have a &lt;a href="https://chatgpt.com/g/g-68776e2765b48191bd1bae3f30212631-open-notebook-installation-assistant"&gt;CustomGPT built to help you install Open Notebook&lt;/a&gt; - it will guide you through each step!&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;New to Open Notebook?&lt;/strong&gt; Start with our &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/index.md"&gt;Getting Started Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Need installation help?&lt;/strong&gt; Check our &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/installation.md"&gt;Installation Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Want to see it in action?&lt;/strong&gt; Try our &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/quick-start.md"&gt;Quick Start Tutorial&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Provider Support Matrix&lt;/h2&gt; 
&lt;p&gt;Thanks to the &lt;a href="https://github.com/lfnovo/esperanto"&gt;Esperanto&lt;/a&gt; library, we support this providers out of the box!&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Provider&lt;/th&gt; 
   &lt;th&gt;LLM Support&lt;/th&gt; 
   &lt;th&gt;Embedding Support&lt;/th&gt; 
   &lt;th&gt;Speech-to-Text&lt;/th&gt; 
   &lt;th&gt;Text-to-Speech&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenAI&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Anthropic&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Groq&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Google (GenAI)&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Vertex AI&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ollama&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Perplexity&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ElevenLabs&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Azure OpenAI&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Mistral&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;DeepSeek&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Voyage&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;xAI&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenRouter&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenAI Compatible*&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;*Supports LM Studio and any OpenAI-compatible endpoint&lt;/p&gt; 
&lt;h2&gt;‚ú® Key Features&lt;/h2&gt; 
&lt;h3&gt;Core Capabilities&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;üîí Privacy-First&lt;/strong&gt;: Your data stays under your control - no cloud dependencies&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üéØ Multi-Notebook Organization&lt;/strong&gt;: Manage multiple research projects seamlessly&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üìö Universal Content Support&lt;/strong&gt;: PDFs, videos, audio, web pages, Office docs, and more&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ü§ñ Multi-Model AI Support&lt;/strong&gt;: 16+ providers including OpenAI, Anthropic, Ollama, Google, LM Studio, and more&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üéôÔ∏è Professional Podcast Generation&lt;/strong&gt;: Advanced multi-speaker podcasts with Episode Profiles&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üîç Intelligent Search&lt;/strong&gt;: Full-text and vector search across all your content&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üí¨ Context-Aware Chat&lt;/strong&gt;: AI conversations powered by your research materials&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üìù AI-Assisted Notes&lt;/strong&gt;: Generate insights or write notes manually&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Advanced Features&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;‚ö° Reasoning Model Support&lt;/strong&gt;: Full support for thinking models like DeepSeek-R1 and Qwen3&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üîß Content Transformations&lt;/strong&gt;: Powerful customizable actions to summarize and extract insights&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üåê Comprehensive REST API&lt;/strong&gt;: Full programmatic access for custom integrations &lt;a href="http://localhost:5055/docs"&gt;&lt;img src="https://img.shields.io/badge/API-Documentation-blue?style=flat-square" alt="API Docs" /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üîê Optional Password Protection&lt;/strong&gt;: Secure public deployments with authentication&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üìä Fine-Grained Context Control&lt;/strong&gt;: Choose exactly what to share with AI models&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üìé Citations&lt;/strong&gt;: Get answers with proper source citations&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Three-Column Interface&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Sources&lt;/strong&gt;: Manage all your research materials&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Notes&lt;/strong&gt;: Create manual or AI-generated notes&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Chat&lt;/strong&gt;: Converse with AI using your content as context&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=D-760MlGwaI"&gt;&lt;img src="https://img.youtube.com/vi/D-760MlGwaI/0.jpg" alt="Check out our podcast sample" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;üìö Documentation&lt;/h2&gt; 
&lt;h3&gt;Getting Started&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/introduction.md"&gt;üìñ Introduction&lt;/a&gt;&lt;/strong&gt; - Learn what Open Notebook offers&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/quick-start.md"&gt;‚ö° Quick Start&lt;/a&gt;&lt;/strong&gt; - Get up and running in 5 minutes&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/installation.md"&gt;üîß Installation&lt;/a&gt;&lt;/strong&gt; - Comprehensive setup guide&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/getting-started/first-notebook.md"&gt;üéØ Your First Notebook&lt;/a&gt;&lt;/strong&gt; - Step-by-step tutorial&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;User Guide&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/interface-overview.md"&gt;üì± Interface Overview&lt;/a&gt;&lt;/strong&gt; - Understanding the layout&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/notebooks.md"&gt;üìö Notebooks&lt;/a&gt;&lt;/strong&gt; - Organizing your research&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/sources.md"&gt;üìÑ Sources&lt;/a&gt;&lt;/strong&gt; - Managing content types&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/notes.md"&gt;üìù Notes&lt;/a&gt;&lt;/strong&gt; - Creating and managing notes&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/chat.md"&gt;üí¨ Chat&lt;/a&gt;&lt;/strong&gt; - AI conversations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/user-guide/search.md"&gt;üîç Search&lt;/a&gt;&lt;/strong&gt; - Finding information&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Advanced Topics&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/features/podcasts.md"&gt;üéôÔ∏è Podcast Generation&lt;/a&gt;&lt;/strong&gt; - Create professional podcasts&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/features/transformations.md"&gt;üîß Content Transformations&lt;/a&gt;&lt;/strong&gt; - Customize content processing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/features/ai-models.md"&gt;ü§ñ AI Models&lt;/a&gt;&lt;/strong&gt; - AI model configuration&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/development/api-reference.md"&gt;üîß REST API Reference&lt;/a&gt;&lt;/strong&gt; - Complete API documentation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/deployment/security.md"&gt;üîê Security&lt;/a&gt;&lt;/strong&gt; - Password protection and privacy&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/docs/deployment/index.md"&gt;üöÄ Deployment&lt;/a&gt;&lt;/strong&gt; - Complete deployment guides for all scenarios&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="right"&gt;(&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/#readme-top"&gt;back to top&lt;/a&gt;)&lt;/p&gt; 
&lt;h2&gt;üó∫Ô∏è Roadmap&lt;/h2&gt; 
&lt;h3&gt;Upcoming Features&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Live Front-End Updates&lt;/strong&gt;: Real-time UI updates for smoother experience&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Async Processing&lt;/strong&gt;: Faster UI through asynchronous content processing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Cross-Notebook Sources&lt;/strong&gt;: Reuse research materials across projects&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Bookmark Integration&lt;/strong&gt;: Connect with your favorite bookmarking apps&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Recently Completed ‚úÖ&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Next.js Frontend&lt;/strong&gt;: Modern React-based frontend with improved performance&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Comprehensive REST API&lt;/strong&gt;: Full programmatic access to all functionality&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-Model Support&lt;/strong&gt;: 16+ AI providers including OpenAI, Anthropic, Ollama, LM Studio&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Advanced Podcast Generator&lt;/strong&gt;: Professional multi-speaker podcasts with Episode Profiles&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Content Transformations&lt;/strong&gt;: Powerful customizable actions for content processing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Enhanced Citations&lt;/strong&gt;: Improved layout and finer control for source citations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multiple Chat Sessions&lt;/strong&gt;: Manage different conversations within notebooks&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See the &lt;a href="https://github.com/lfnovo/open-notebook/issues"&gt;open issues&lt;/a&gt; for a full list of proposed features and known issues.&lt;/p&gt; 
&lt;p align="right"&gt;(&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/#readme-top"&gt;back to top&lt;/a&gt;)&lt;/p&gt; 
&lt;h2&gt;ü§ù Community &amp;amp; Contributing&lt;/h2&gt; 
&lt;h3&gt;Join the Community&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;üí¨ &lt;strong&gt;&lt;a href="https://discord.gg/37XJPXfz2w"&gt;Discord Server&lt;/a&gt;&lt;/strong&gt; - Get help, share ideas, and connect with other users&lt;/li&gt; 
 &lt;li&gt;üêõ &lt;strong&gt;&lt;a href="https://github.com/lfnovo/open-notebook/issues"&gt;GitHub Issues&lt;/a&gt;&lt;/strong&gt; - Report bugs and request features&lt;/li&gt; 
 &lt;li&gt;‚≠ê &lt;strong&gt;Star this repo&lt;/strong&gt; - Show your support and help others discover Open Notebook&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Contributing&lt;/h3&gt; 
&lt;p&gt;We welcome contributions! We're especially looking for help with:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Frontend Development&lt;/strong&gt;: Help improve our modern Next.js/React UI&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Testing &amp;amp; Bug Fixes&lt;/strong&gt;: Make Open Notebook more robust&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Feature Development&lt;/strong&gt;: Build the coolest research tool together&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Documentation&lt;/strong&gt;: Improve guides and tutorials&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Current Tech Stack&lt;/strong&gt;: Python, FastAPI, Next.js, React, SurrealDB &lt;strong&gt;Future Roadmap&lt;/strong&gt;: Real-time updates, enhanced async processing&lt;/p&gt; 
&lt;p&gt;See our &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/CONTRIBUTING.md"&gt;Contributing Guide&lt;/a&gt; for detailed information on how to get started.&lt;/p&gt; 
&lt;p align="right"&gt;(&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/#readme-top"&gt;back to top&lt;/a&gt;)&lt;/p&gt; 
&lt;h2&gt;üìÑ License&lt;/h2&gt; 
&lt;p&gt;Open Notebook is MIT licensed. See the &lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;h2&gt;üìû Contact&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Luis Novo&lt;/strong&gt; - &lt;a href="https://twitter.com/lfnovo"&gt;@lfnovo&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Community Support&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üí¨ &lt;a href="https://discord.gg/37XJPXfz2w"&gt;Discord Server&lt;/a&gt; - Get help, share ideas, and connect with users&lt;/li&gt; 
 &lt;li&gt;üêõ &lt;a href="https://github.com/lfnovo/open-notebook/issues"&gt;GitHub Issues&lt;/a&gt; - Report bugs and request features&lt;/li&gt; 
 &lt;li&gt;üåê &lt;a href="https://www.open-notebook.ai"&gt;Website&lt;/a&gt; - Learn more about the project&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üôè Acknowledgments&lt;/h2&gt; 
&lt;p&gt;Open Notebook is built on the shoulders of amazing open-source projects:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/lfnovo/podcast-creator"&gt;Podcast Creator&lt;/a&gt;&lt;/strong&gt; - Advanced podcast generation capabilities&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/lfnovo/surreal-commands"&gt;Surreal Commands&lt;/a&gt;&lt;/strong&gt; - Background job processing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/lfnovo/content-core"&gt;Content Core&lt;/a&gt;&lt;/strong&gt; - Content processing and management&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/lfnovo/esperanto"&gt;Esperanto&lt;/a&gt;&lt;/strong&gt; - Multi-provider AI model abstraction&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/docling-project/docling"&gt;Docling&lt;/a&gt;&lt;/strong&gt; - Document processing and parsing&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="right"&gt;(&lt;a href="https://raw.githubusercontent.com/lfnovo/open-notebook/main/#readme-top"&gt;back to top&lt;/a&gt;)&lt;/p&gt; 
&lt;!-- MARKDOWN LINKS &amp; IMAGES --&gt; 
&lt;!-- https://www.markdownguide.org/basic-syntax/#reference-style-links --&gt;</description>
    </item>
    
    <item>
      <title>seaweedfs/seaweedfs</title>
      <link>https://github.com/seaweedfs/seaweedfs</link>
      <description>&lt;p&gt;SeaweedFS is a fast distributed storage system for blobs, objects, files, and data lake, for billions of files! Blob store has O(1) disk seek, cloud tiering. Filer supports Cloud Drive, xDC replication, Kubernetes, POSIX FUSE mount, S3 API, S3 Gateway, Hadoop, WebDAV, encryption, Erasure Coding. Enterprise version is at seaweedfs.com.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;SeaweedFS&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://join.slack.com/t/seaweedfs/shared_invite/enQtMzI4MTMwMjU2MzA3LTEyYzZmZWYzOGQ3MDJlZWMzYmI0OTE4OTJiZjJjODBmMzUxNmYwODg0YjY3MTNlMjBmZDQ1NzQ5NDJhZWI2ZmY"&gt;&lt;img src="https://img.shields.io/badge/slack-purple" alt="Slack" /&gt;&lt;/a&gt; &lt;a href="https://twitter.com/intent/follow?screen_name=seaweedfs"&gt;&lt;img src="https://img.shields.io/twitter/follow/seaweedfs.svg?style=social&amp;amp;label=Follow" alt="Twitter" /&gt;&lt;/a&gt; &lt;a href="https://github.com/seaweedfs/seaweedfs/actions/workflows/go.yml"&gt;&lt;img src="https://img.shields.io/github/actions/workflow/status/seaweedfs/seaweedfs/go.yml" alt="Build Status" /&gt;&lt;/a&gt; &lt;a href="https://godoc.org/github.com/seaweedfs/seaweedfs/weed"&gt;&lt;img src="https://godoc.org/github.com/seaweedfs/seaweedfs/weed?status.svg?sanitize=true" alt="GoDoc" /&gt;&lt;/a&gt; &lt;a href="https://github.com/seaweedfs/seaweedfs/wiki"&gt;&lt;img src="https://img.shields.io/badge/docs-wiki-blue.svg?sanitize=true" alt="Wiki" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/chrislusf/seaweedfs/"&gt;&lt;img src="https://img.shields.io/docker/pulls/chrislusf/seaweedfs?maxAge=4800" alt="Docker Pulls" /&gt;&lt;/a&gt; &lt;a href="https://search.maven.org/search?q=g:com.github.chrislusf"&gt;&lt;img src="https://img.shields.io/maven-central/v/com.github.chrislusf/seaweedfs-client" alt="SeaweedFS on Maven Central" /&gt;&lt;/a&gt; &lt;a href="https://artifacthub.io/packages/search?repo=seaweedfs"&gt;&lt;img src="https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/seaweedfs" alt="Artifact Hub" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/note/seaweedfs.png" alt="SeaweedFS Logo" /&gt;&lt;/p&gt; 
&lt;h2 align="center"&gt;&lt;a href="https://www.patreon.com/seaweedfs"&gt;Sponsor SeaweedFS via Patreon&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;SeaweedFS is an independent Apache-licensed open source project with its ongoing development made possible entirely thanks to the support of these awesome &lt;a href="https://github.com/seaweedfs/seaweedfs/raw/master/backers.md"&gt;backers&lt;/a&gt;. If you'd like to grow SeaweedFS even stronger, please consider joining our &lt;a href="https://www.patreon.com/seaweedfs"&gt;sponsors on Patreon&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Your support will be really appreciated by me and other supporters!&lt;/p&gt; 
&lt;!--
&lt;h4 align="center"&gt;Platinum&lt;/h4&gt;

&lt;p align="center"&gt;
  &lt;a href="" target="_blank"&gt;
    Add your name or icon here
  &lt;/a&gt;
&lt;/p&gt;
--&gt; 
&lt;h3&gt;Gold Sponsors&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://www.nodion.com"&gt;&lt;img src="https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/note/sponsor_nodion.png" alt="nodion" /&gt;&lt;/a&gt; &lt;a href="https://www.piknik.com"&gt;&lt;img src="https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/note/piknik.png" alt="piknik" /&gt;&lt;/a&gt; &lt;a href="https://www.keepsec.ca"&gt;&lt;img src="https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/note/keepsec.png" alt="keepsec" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/seaweedfs/seaweedfs/releases/latest"&gt;Download Binaries for different platforms&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://join.slack.com/t/seaweedfs/shared_invite/enQtMzI4MTMwMjU2MzA3LTEyYzZmZWYzOGQ3MDJlZWMzYmI0OTE4OTJiZjJjODBmMzUxNmYwODg0YjY3MTNlMjBmZDQ1NzQ5NDJhZWI2ZmY"&gt;SeaweedFS on Slack&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://twitter.com/SeaweedFS"&gt;SeaweedFS on Twitter&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://t.me/Seaweedfs"&gt;SeaweedFS on Telegram&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.reddit.com/r/SeaweedFS/"&gt;SeaweedFS on Reddit&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://groups.google.com/d/forum/seaweedfs"&gt;SeaweedFS Mailing List&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/seaweedfs/seaweedfs/wiki"&gt;Wiki Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/seaweedfs/seaweedfs/wiki/SeaweedFS_Architecture.pdf"&gt;SeaweedFS White Paper&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.google.com/presentation/d/1tdkp45J01oRV68dIm4yoTXKJDof-EhainlA0LMXexQE/edit?usp=sharing"&gt;SeaweedFS Introduction Slides 2025.5&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.google.com/presentation/d/1DcxKWlINc-HNCjhYeERkpGXXm6nTCES8mi2W5G0Z4Ts/edit?usp=sharing"&gt;SeaweedFS Introduction Slides 2021.5&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.slideshare.net/chrislusf/seaweedfs-introduction"&gt;SeaweedFS Introduction Slides 2019.3&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Table of Contents&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#quick-start"&gt;Quick Start&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#quick-start-for-s3-api-on-docker"&gt;Quick Start for S3 API on Docker&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#quick-start-with-single-binary"&gt;Quick Start with Single Binary&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#quick-start-seaweedfs-s3-on-aws"&gt;Quick Start SeaweedFS S3 on AWS&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#introduction"&gt;Introduction&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#features"&gt;Features&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#additional-features"&gt;Additional Features&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#filer-features"&gt;Filer Features&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#example-using-seaweed-object-store"&gt;Example: Using Seaweed Object Store&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#object-store-architecture"&gt;Architecture&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#compared-to-other-file-systems"&gt;Compared to Other File Systems&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#compared-to-hdfs"&gt;Compared to HDFS&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#compared-to-glusterfs-ceph"&gt;Compared to GlusterFS, Ceph&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#compared-to-glusterfs"&gt;Compared to GlusterFS&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#compared-to-ceph"&gt;Compared to Ceph&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#compared-to-minio"&gt;Compared to Minio&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#dev-plan"&gt;Dev Plan&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#installation-guide"&gt;Installation Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#disk-related-topics"&gt;Disk Related Topics&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#benchmark"&gt;Benchmark&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#enterprise"&gt;Enterprise&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#license"&gt;License&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Quick Start&lt;/h1&gt; 
&lt;h2&gt;Quick Start for S3 API on Docker&lt;/h2&gt; 
&lt;p&gt;&lt;code&gt;docker run -p 8333:8333 chrislusf/seaweedfs server -s3&lt;/code&gt;&lt;/p&gt; 
&lt;h2&gt;Quick Start with Single Binary&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Download the latest binary from &lt;a href="https://github.com/seaweedfs/seaweedfs/releases"&gt;https://github.com/seaweedfs/seaweedfs/releases&lt;/a&gt; and unzip a single binary file &lt;code&gt;weed&lt;/code&gt; or &lt;code&gt;weed.exe&lt;/code&gt;. Or run &lt;code&gt;go install github.com/seaweedfs/seaweedfs/weed@latest&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;export AWS_ACCESS_KEY_ID=admin ; export AWS_SECRET_ACCESS_KEY=key&lt;/code&gt; as the admin credentials to access the object store.&lt;/li&gt; 
 &lt;li&gt;Run &lt;code&gt;weed server -dir=/some/data/dir -s3&lt;/code&gt; to start one master, one volume server, one filer, and one S3 gateway.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Also, to increase capacity, just add more volume servers by running &lt;code&gt;weed volume -dir="/some/data/dir2" -mserver="&amp;lt;master_host&amp;gt;:9333" -port=8081&lt;/code&gt; locally, or on a different machine, or on thousands of machines. That is it!&lt;/p&gt; 
&lt;h2&gt;Quick Start SeaweedFS S3 on AWS&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Setup fast production-ready &lt;a href="https://aws.amazon.com/marketplace/pp/prodview-nzelz5gprlrjc"&gt;SeaweedFS S3 on AWS with cloudformation&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Introduction&lt;/h1&gt; 
&lt;p&gt;SeaweedFS is a simple and highly scalable distributed file system. There are two objectives:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;to store billions of files!&lt;/li&gt; 
 &lt;li&gt;to serve the files fast!&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;SeaweedFS started as an Object Store to handle small files efficiently. Instead of managing all file metadata in a central master, the central master only manages volumes on volume servers, and these volume servers manage files and their metadata. This relieves concurrency pressure from the central master and spreads file metadata into volume servers, allowing faster file access (O(1), usually just one disk read operation).&lt;/p&gt; 
&lt;p&gt;There is only 40 bytes of disk storage overhead for each file's metadata. It is so simple with O(1) disk reads that you are welcome to challenge the performance with your actual use cases.&lt;/p&gt; 
&lt;p&gt;SeaweedFS started by implementing &lt;a href="http://www.usenix.org/event/osdi10/tech/full_papers/Beaver.pdf"&gt;Facebook's Haystack design paper&lt;/a&gt;. Also, SeaweedFS implements erasure coding with ideas from &lt;a href="https://www.usenix.org/system/files/conference/osdi14/osdi14-paper-muralidhar.pdf"&gt;f4: Facebook‚Äôs Warm BLOB Storage System&lt;/a&gt;, and has a lot of similarities with &lt;a href="https://www.usenix.org/system/files/fast21-pan.pdf"&gt;Facebook‚Äôs Tectonic Filesystem&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;On top of the object store, optional &lt;a href="https://github.com/seaweedfs/seaweedfs/wiki/Directories-and-Files"&gt;Filer&lt;/a&gt; can support directories and POSIX attributes. Filer is a separate linearly-scalable stateless server with customizable metadata stores, e.g., MySql, Postgres, Redis, Cassandra, HBase, Mongodb, Elastic Search, LevelDB, RocksDB, Sqlite, MemSql, TiDB, Etcd, CockroachDB, YDB, etc.&lt;/p&gt; 
&lt;p&gt;For any distributed key value stores, the large values can be offloaded to SeaweedFS. With the fast access speed and linearly scalable capacity, SeaweedFS can work as a distributed &lt;a href="https://github.com/seaweedfs/seaweedfs/wiki/Filer-as-a-Key-Large-Value-Store"&gt;Key-Large-Value store&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;SeaweedFS can transparently integrate with the cloud. With hot data on local cluster, and warm data on the cloud with O(1) access time, SeaweedFS can achieve both fast local access time and elastic cloud storage capacity. What's more, the cloud storage access API cost is minimized. Faster and cheaper than direct cloud storage!&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#table-of-contents"&gt;Back to TOC&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Features&lt;/h1&gt; 
&lt;h2&gt;Additional Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Can choose no replication or different replication levels, rack and data center aware.&lt;/li&gt; 
 &lt;li&gt;Automatic master servers failover - no single point of failure (SPOF).&lt;/li&gt; 
 &lt;li&gt;Automatic Gzip compression depending on file MIME type.&lt;/li&gt; 
 &lt;li&gt;Automatic compaction to reclaim disk space after deletion or update.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/seaweedfs/seaweedfs/wiki/Store-file-with-a-Time-To-Live"&gt;Automatic entry TTL expiration&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Any server with some disk space can add to the total storage space.&lt;/li&gt; 
 &lt;li&gt;Adding/Removing servers does &lt;strong&gt;not&lt;/strong&gt; cause any data re-balancing unless triggered by admin commands.&lt;/li&gt; 
 &lt;li&gt;Optional picture resizing.&lt;/li&gt; 
 &lt;li&gt;Support ETag, Accept-Range, Last-Modified, etc.&lt;/li&gt; 
 &lt;li&gt;Support in-memory/leveldb/readonly mode tuning for memory/performance balance.&lt;/li&gt; 
 &lt;li&gt;Support rebalancing the writable and readonly volumes.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/seaweedfs/seaweedfs/wiki/Tiered-Storage"&gt;Customizable Multiple Storage Tiers&lt;/a&gt;: Customizable storage disk types to balance performance and cost.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/seaweedfs/seaweedfs/wiki/Cloud-Tier"&gt;Transparent cloud integration&lt;/a&gt;: unlimited capacity via tiered cloud storage for warm data.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/seaweedfs/seaweedfs/wiki/Erasure-coding-for-warm-storage"&gt;Erasure Coding for warm storage&lt;/a&gt; Rack-Aware 10.4 erasure coding reduces storage cost and increases availability.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#table-of-contents"&gt;Back to TOC&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Filer Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/seaweedfs/seaweedfs/wiki/Directories-and-Files"&gt;Filer server&lt;/a&gt; provides "normal" directories and files via HTTP.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/seaweedfs/seaweedfs/wiki/Filer-Stores"&gt;File TTL&lt;/a&gt; automatically expires file metadata and actual file data.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/seaweedfs/seaweedfs/wiki/FUSE-Mount"&gt;Mount filer&lt;/a&gt; reads and writes files directly as a local directory via FUSE.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/seaweedfs/seaweedfs/wiki/Filer-Store-Replication"&gt;Filer Store Replication&lt;/a&gt; enables HA for filer meta data stores.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/seaweedfs/seaweedfs/wiki/Filer-Active-Active-cross-cluster-continuous-synchronization"&gt;Active-Active Replication&lt;/a&gt; enables asynchronous one-way or two-way cross cluster continuous replication.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/seaweedfs/seaweedfs/wiki/Amazon-S3-API"&gt;Amazon S3 compatible API&lt;/a&gt; accesses files with S3 tooling.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/seaweedfs/seaweedfs/wiki/Hadoop-Compatible-File-System"&gt;Hadoop Compatible File System&lt;/a&gt; accesses files from Hadoop/Spark/Flink/etc or even runs HBase.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/seaweedfs/seaweedfs/wiki/Async-Replication-to-Cloud"&gt;Async Replication To Cloud&lt;/a&gt; has extremely fast local access and backups to Amazon S3, Google Cloud Storage, Azure, BackBlaze.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/seaweedfs/seaweedfs/wiki/WebDAV"&gt;WebDAV&lt;/a&gt; accesses as a mapped drive on Mac and Windows, or from mobile devices.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/seaweedfs/seaweedfs/wiki/Filer-Data-Encryption"&gt;AES256-GCM Encrypted Storage&lt;/a&gt; safely stores the encrypted data.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/seaweedfs/seaweedfs/wiki/Data-Structure-for-Large-Files"&gt;Super Large Files&lt;/a&gt; stores large or super large files in tens of TB.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/seaweedfs/seaweedfs/wiki/Cloud-Drive-Architecture"&gt;Cloud Drive&lt;/a&gt; mounts cloud storage to local cluster, cached for fast read and write with asynchronous write back.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/seaweedfs/seaweedfs/wiki/Gateway-to-Remote-Object-Storage"&gt;Gateway to Remote Object Store&lt;/a&gt; mirrors bucket operations to remote object storage, in addition to &lt;a href="https://github.com/seaweedfs/seaweedfs/wiki/Cloud-Drive-Architecture"&gt;Cloud Drive&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Kubernetes&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/seaweedfs/seaweedfs-csi-driver"&gt;Kubernetes CSI Driver&lt;/a&gt; A Container Storage Interface (CSI) Driver. &lt;a href="https://hub.docker.com/r/chrislusf/seaweedfs-csi-driver/"&gt;&lt;img src="https://img.shields.io/docker/pulls/chrislusf/seaweedfs-csi-driver.svg?maxAge=4800" alt="Docker Pulls" /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/seaweedfs/seaweedfs-operator"&gt;SeaweedFS Operator&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#table-of-contents"&gt;Back to TOC&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Example: Using Seaweed Object Store&lt;/h2&gt; 
&lt;p&gt;By default, the master node runs on port 9333, and the volume nodes run on port 8080. Let's start one master node, and two volume nodes on port 8080 and 8081. Ideally, they should be started from different machines. We'll use localhost as an example.&lt;/p&gt; 
&lt;p&gt;SeaweedFS uses HTTP REST operations to read, write, and delete. The responses are in JSON or JSONP format.&lt;/p&gt; 
&lt;h3&gt;Start Master Server&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;&amp;gt; ./weed master
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Start Volume Servers&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;&amp;gt; weed volume -dir="/tmp/data1" -max=5  -mserver="localhost:9333" -port=8080 &amp;amp;
&amp;gt; weed volume -dir="/tmp/data2" -max=10 -mserver="localhost:9333" -port=8081 &amp;amp;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Write File&lt;/h3&gt; 
&lt;p&gt;To upload a file: first, send a HTTP POST, PUT, or GET request to &lt;code&gt;/dir/assign&lt;/code&gt; to get an &lt;code&gt;fid&lt;/code&gt; and a volume server URL:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;gt; curl http://localhost:9333/dir/assign
{"count":1,"fid":"3,01637037d6","url":"127.0.0.1:8080","publicUrl":"localhost:8080"}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Second, to store the file content, send a HTTP multi-part POST request to &lt;code&gt;url + '/' + fid&lt;/code&gt; from the response:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;gt; curl -F file=@/home/chris/myphoto.jpg http://127.0.0.1:8080/3,01637037d6
{"name":"myphoto.jpg","size":43234,"eTag":"1cc0118e"}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To update, send another POST request with updated file content.&lt;/p&gt; 
&lt;p&gt;For deletion, send an HTTP DELETE request to the same &lt;code&gt;url + '/' + fid&lt;/code&gt; URL:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;gt; curl -X DELETE http://127.0.0.1:8080/3,01637037d6
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Save File Id&lt;/h3&gt; 
&lt;p&gt;Now, you can save the &lt;code&gt;fid&lt;/code&gt;, 3,01637037d6 in this case, to a database field.&lt;/p&gt; 
&lt;p&gt;The number 3 at the start represents a volume id. After the comma, it's one file key, 01, and a file cookie, 637037d6.&lt;/p&gt; 
&lt;p&gt;The volume id is an unsigned 32-bit integer. The file key is an unsigned 64-bit integer. The file cookie is an unsigned 32-bit integer, used to prevent URL guessing.&lt;/p&gt; 
&lt;p&gt;The file key and file cookie are both coded in hex. You can store the &amp;lt;volume id, file key, file cookie&amp;gt; tuple in your own format, or simply store the &lt;code&gt;fid&lt;/code&gt; as a string.&lt;/p&gt; 
&lt;p&gt;If stored as a string, in theory, you would need 8+1+16+8=33 bytes. A char(33) would be enough, if not more than enough, since most uses will not need 2^32 volumes.&lt;/p&gt; 
&lt;p&gt;If space is really a concern, you can store the file id in your own format. You would need one 4-byte integer for volume id, 8-byte long number for file key, and a 4-byte integer for the file cookie. So 16 bytes are more than enough.&lt;/p&gt; 
&lt;h3&gt;Read File&lt;/h3&gt; 
&lt;p&gt;Here is an example of how to render the URL.&lt;/p&gt; 
&lt;p&gt;First look up the volume server's URLs by the file's volumeId:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;gt; curl http://localhost:9333/dir/lookup?volumeId=3
{"volumeId":"3","locations":[{"publicUrl":"localhost:8080","url":"localhost:8080"}]}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Since (usually) there are not too many volume servers, and volumes don't move often, you can cache the results most of the time. Depending on the replication type, one volume can have multiple replica locations. Just randomly pick one location to read.&lt;/p&gt; 
&lt;p&gt;Now you can take the public URL, render the URL or directly read from the volume server via URL:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt; http://localhost:8080/3,01637037d6.jpg
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Notice we add a file extension ".jpg" here. It's optional and just one way for the client to specify the file content type.&lt;/p&gt; 
&lt;p&gt;If you want a nicer URL, you can use one of these alternative URL formats:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt; http://localhost:8080/3/01637037d6/my_preferred_name.jpg
 http://localhost:8080/3/01637037d6.jpg
 http://localhost:8080/3,01637037d6.jpg
 http://localhost:8080/3/01637037d6
 http://localhost:8080/3,01637037d6
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you want to get a scaled version of an image, you can add some params:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;http://localhost:8080/3/01637037d6.jpg?height=200&amp;amp;width=200
http://localhost:8080/3/01637037d6.jpg?height=200&amp;amp;width=200&amp;amp;mode=fit
http://localhost:8080/3/01637037d6.jpg?height=200&amp;amp;width=200&amp;amp;mode=fill
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Rack-Aware and Data Center-Aware Replication&lt;/h3&gt; 
&lt;p&gt;SeaweedFS applies the replication strategy at a volume level. So, when you are getting a file id, you can specify the replication strategy. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;curl http://localhost:9333/dir/assign?replication=001
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The replication parameter options are:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;000: no replication
001: replicate once on the same rack
010: replicate once on a different rack, but same data center
100: replicate once on a different data center
200: replicate twice on two different data center
110: replicate once on a different rack, and once on a different data center
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;More details about replication can be found &lt;a href="https://github.com/seaweedfs/seaweedfs/wiki/Replication"&gt;on the wiki&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You can also set the default replication strategy when starting the master server.&lt;/p&gt; 
&lt;h3&gt;Allocate File Key on Specific Data Center&lt;/h3&gt; 
&lt;p&gt;Volume servers can be started with a specific data center name:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt; weed volume -dir=/tmp/1 -port=8080 -dataCenter=dc1
 weed volume -dir=/tmp/2 -port=8081 -dataCenter=dc2
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;When requesting a file key, an optional "dataCenter" parameter can limit the assigned volume to the specific data center. For example, this specifies that the assigned volume should be limited to 'dc1':&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt; http://localhost:9333/dir/assign?dataCenter=dc1
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Other Features&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/seaweedfs/seaweedfs/wiki/Failover-Master-Server"&gt;No Single Point of Failure&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/seaweedfs/seaweedfs/wiki/Optimization#insert-with-your-own-keys"&gt;Insert with your own keys&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/seaweedfs/seaweedfs/wiki/Optimization#upload-large-files"&gt;Chunking large files&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/seaweedfs/seaweedfs/wiki/Optimization#collection-as-a-simple-name-space"&gt;Collection as a Simple Name Space&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#table-of-contents"&gt;Back to TOC&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Object Store Architecture&lt;/h2&gt; 
&lt;p&gt;Usually distributed file systems split each file into chunks, a central master keeps a mapping of filenames, chunk indices to chunk handles, and also which chunks each chunk server has.&lt;/p&gt; 
&lt;p&gt;The main drawback is that the central master can't handle many small files efficiently, and since all read requests need to go through the chunk master, so it might not scale well for many concurrent users.&lt;/p&gt; 
&lt;p&gt;Instead of managing chunks, SeaweedFS manages data volumes in the master server. Each data volume is 32GB in size, and can hold a lot of files. And each storage node can have many data volumes. So the master node only needs to store the metadata about the volumes, which is a fairly small amount of data and is generally stable.&lt;/p&gt; 
&lt;p&gt;The actual file metadata is stored in each volume on volume servers. Since each volume server only manages metadata of files on its own disk, with only 16 bytes for each file, all file access can read file metadata just from memory and only needs one disk operation to actually read file data.&lt;/p&gt; 
&lt;p&gt;For comparison, consider that an xfs inode structure in Linux is 536 bytes.&lt;/p&gt; 
&lt;h3&gt;Master Server and Volume Server&lt;/h3&gt; 
&lt;p&gt;The architecture is fairly simple. The actual data is stored in volumes on storage nodes. One volume server can have multiple volumes, and can both support read and write access with basic authentication.&lt;/p&gt; 
&lt;p&gt;All volumes are managed by a master server. The master server contains the volume id to volume server mapping. This is fairly static information, and can be easily cached.&lt;/p&gt; 
&lt;p&gt;On each write request, the master server also generates a file key, which is a growing 64-bit unsigned integer. Since write requests are not generally as frequent as read requests, one master server should be able to handle the concurrency well.&lt;/p&gt; 
&lt;h3&gt;Write and Read files&lt;/h3&gt; 
&lt;p&gt;When a client sends a write request, the master server returns (volume id, file key, file cookie, volume node URL) for the file. The client then contacts the volume node and POSTs the file content.&lt;/p&gt; 
&lt;p&gt;When a client needs to read a file based on (volume id, file key, file cookie), it asks the master server by the volume id for the (volume node URL, volume node public URL), or retrieves this from a cache. Then the client can GET the content, or just render the URL on web pages and let browsers fetch the content.&lt;/p&gt; 
&lt;p&gt;Please see the example for details on the write-read process.&lt;/p&gt; 
&lt;h3&gt;Storage Size&lt;/h3&gt; 
&lt;p&gt;In the current implementation, each volume can hold 32 gibibytes (32GiB or 8x2^32 bytes). This is because we align content to 8 bytes. We can easily increase this to 64GiB, or 128GiB, or more, by changing 2 lines of code, at the cost of some wasted padding space due to alignment.&lt;/p&gt; 
&lt;p&gt;There can be 4 gibibytes (4GiB or 2^32 bytes) of volumes. So the total system size is 8 x 4GiB x 4GiB which is 128 exbibytes (128EiB or 2^67 bytes).&lt;/p&gt; 
&lt;p&gt;Each individual file size is limited to the volume size.&lt;/p&gt; 
&lt;h3&gt;Saving memory&lt;/h3&gt; 
&lt;p&gt;All file meta information stored on a volume server is readable from memory without disk access. Each file takes just a 16-byte map entry of &amp;lt;64bit key, 32bit offset, 32bit size&amp;gt;. Of course, each map entry has its own space cost for the map. But usually the disk space runs out before the memory does.&lt;/p&gt; 
&lt;h3&gt;Tiered Storage to the cloud&lt;/h3&gt; 
&lt;p&gt;The local volume servers are much faster, while cloud storages have elastic capacity and are actually more cost-efficient if not accessed often (usually free to upload, but relatively costly to access). With the append-only structure and O(1) access time, SeaweedFS can take advantage of both local and cloud storage by offloading the warm data to the cloud.&lt;/p&gt; 
&lt;p&gt;Usually hot data are fresh and warm data are old. SeaweedFS puts the newly created volumes on local servers, and optionally upload the older volumes on the cloud. If the older data are accessed less often, this literally gives you unlimited capacity with limited local servers, and still fast for new data.&lt;/p&gt; 
&lt;p&gt;With the O(1) access time, the network latency cost is kept at minimum.&lt;/p&gt; 
&lt;p&gt;If the hot/warm data is split as 20/80, with 20 servers, you can achieve storage capacity of 100 servers. That's a cost saving of 80%! Or you can repurpose the 80 servers to store new data also, and get 5X storage throughput.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#table-of-contents"&gt;Back to TOC&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Compared to Other File Systems&lt;/h2&gt; 
&lt;p&gt;Most other distributed file systems seem more complicated than necessary.&lt;/p&gt; 
&lt;p&gt;SeaweedFS is meant to be fast and simple, in both setup and operation. If you do not understand how it works when you reach here, we've failed! Please raise an issue with any questions or update this file with clarifications.&lt;/p&gt; 
&lt;p&gt;SeaweedFS is constantly moving forward. Same with other systems. These comparisons can be outdated quickly. Please help to keep them updated.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#table-of-contents"&gt;Back to TOC&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Compared to HDFS&lt;/h3&gt; 
&lt;p&gt;HDFS uses the chunk approach for each file, and is ideal for storing large files.&lt;/p&gt; 
&lt;p&gt;SeaweedFS is ideal for serving relatively smaller files quickly and concurrently.&lt;/p&gt; 
&lt;p&gt;SeaweedFS can also store extra large files by splitting them into manageable data chunks, and store the file ids of the data chunks into a meta chunk. This is managed by "weed upload/download" tool, and the weed master or volume servers are agnostic about it.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#table-of-contents"&gt;Back to TOC&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Compared to GlusterFS, Ceph&lt;/h3&gt; 
&lt;p&gt;The architectures are mostly the same. SeaweedFS aims to store and read files fast, with a simple and flat architecture. The main differences are&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;SeaweedFS optimizes for small files, ensuring O(1) disk seek operation, and can also handle large files.&lt;/li&gt; 
 &lt;li&gt;SeaweedFS statically assigns a volume id for a file. Locating file content becomes just a lookup of the volume id, which can be easily cached.&lt;/li&gt; 
 &lt;li&gt;SeaweedFS Filer metadata store can be any well-known and proven data store, e.g., Redis, Cassandra, HBase, Mongodb, Elastic Search, MySql, Postgres, Sqlite, MemSql, TiDB, CockroachDB, Etcd, YDB etc, and is easy to customize.&lt;/li&gt; 
 &lt;li&gt;SeaweedFS Volume server also communicates directly with clients via HTTP, supporting range queries, direct uploads, etc.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;System&lt;/th&gt; 
   &lt;th&gt;File Metadata&lt;/th&gt; 
   &lt;th&gt;File Content Read&lt;/th&gt; 
   &lt;th&gt;POSIX&lt;/th&gt; 
   &lt;th&gt;REST API&lt;/th&gt; 
   &lt;th&gt;Optimized for large number of small files&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;SeaweedFS&lt;/td&gt; 
   &lt;td&gt;lookup volume id, cacheable&lt;/td&gt; 
   &lt;td&gt;O(1) disk seek&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;Yes&lt;/td&gt; 
   &lt;td&gt;Yes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;SeaweedFS Filer&lt;/td&gt; 
   &lt;td&gt;Linearly Scalable, Customizable&lt;/td&gt; 
   &lt;td&gt;O(1) disk seek&lt;/td&gt; 
   &lt;td&gt;FUSE&lt;/td&gt; 
   &lt;td&gt;Yes&lt;/td&gt; 
   &lt;td&gt;Yes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;GlusterFS&lt;/td&gt; 
   &lt;td&gt;hashing&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;FUSE, NFS&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ceph&lt;/td&gt; 
   &lt;td&gt;hashing + rules&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;FUSE&lt;/td&gt; 
   &lt;td&gt;Yes&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MooseFS&lt;/td&gt; 
   &lt;td&gt;in memory&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;FUSE&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;No&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MinIO&lt;/td&gt; 
   &lt;td&gt;separate meta file for each file&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;Yes&lt;/td&gt; 
   &lt;td&gt;No&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#table-of-contents"&gt;Back to TOC&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Compared to GlusterFS&lt;/h3&gt; 
&lt;p&gt;GlusterFS stores files, both directories and content, in configurable volumes called "bricks".&lt;/p&gt; 
&lt;p&gt;GlusterFS hashes the path and filename into ids, and assigned to virtual volumes, and then mapped to "bricks".&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#table-of-contents"&gt;Back to TOC&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Compared to MooseFS&lt;/h3&gt; 
&lt;p&gt;MooseFS chooses to neglect small file issue. From moosefs 3.0 manual, "even a small file will occupy 64KiB plus additionally 4KiB of checksums and 1KiB for the header", because it "was initially designed for keeping large amounts (like several thousands) of very big files"&lt;/p&gt; 
&lt;p&gt;MooseFS Master Server keeps all meta data in memory. Same issue as HDFS namenode.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#table-of-contents"&gt;Back to TOC&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Compared to Ceph&lt;/h3&gt; 
&lt;p&gt;Ceph can be setup similar to SeaweedFS as a key-&amp;gt;blob store. It is much more complicated, with the need to support layers on top of it. &lt;a href="https://github.com/seaweedfs/seaweedfs/issues/120"&gt;Here is a more detailed comparison&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;SeaweedFS has a centralized master group to look up free volumes, while Ceph uses hashing and metadata servers to locate its objects. Having a centralized master makes it easy to code and manage.&lt;/p&gt; 
&lt;p&gt;Ceph, like SeaweedFS, is based on the object store RADOS. Ceph is rather complicated with mixed reviews.&lt;/p&gt; 
&lt;p&gt;Ceph uses CRUSH hashing to automatically manage data placement, which is efficient to locate the data. But the data has to be placed according to the CRUSH algorithm. Any wrong configuration would cause data loss. Topology changes, such as adding new servers to increase capacity, will cause data migration with high IO cost to fit the CRUSH algorithm. SeaweedFS places data by assigning them to any writable volumes. If writes to one volume failed, just pick another volume to write. Adding more volumes is also as simple as it can be.&lt;/p&gt; 
&lt;p&gt;SeaweedFS is optimized for small files. Small files are stored as one continuous block of content, with at most 8 unused bytes between files. Small file access is O(1) disk read.&lt;/p&gt; 
&lt;p&gt;SeaweedFS Filer uses off-the-shelf stores, such as MySql, Postgres, Sqlite, Mongodb, Redis, Elastic Search, Cassandra, HBase, MemSql, TiDB, CockroachCB, Etcd, YDB, to manage file directories. These stores are proven, scalable, and easier to manage.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;SeaweedFS&lt;/th&gt; 
   &lt;th&gt;comparable to Ceph&lt;/th&gt; 
   &lt;th&gt;advantage&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Master&lt;/td&gt; 
   &lt;td&gt;MDS&lt;/td&gt; 
   &lt;td&gt;simpler&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Volume&lt;/td&gt; 
   &lt;td&gt;OSD&lt;/td&gt; 
   &lt;td&gt;optimized for small files&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Filer&lt;/td&gt; 
   &lt;td&gt;Ceph FS&lt;/td&gt; 
   &lt;td&gt;linearly scalable, Customizable, O(1) or O(logN)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#table-of-contents"&gt;Back to TOC&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Compared to MinIO&lt;/h3&gt; 
&lt;p&gt;MinIO follows AWS S3 closely and is ideal for testing for S3 API. It has good UI, policies, versionings, etc. SeaweedFS is trying to catch up here. It is also possible to put MinIO as a gateway in front of SeaweedFS later.&lt;/p&gt; 
&lt;p&gt;MinIO metadata are in simple files. Each file write will incur extra writes to corresponding meta file.&lt;/p&gt; 
&lt;p&gt;MinIO does not have optimization for lots of small files. The files are simply stored as is to local disks. Plus the extra meta file and shards for erasure coding, it only amplifies the LOSF problem.&lt;/p&gt; 
&lt;p&gt;MinIO has multiple disk IO to read one file. SeaweedFS has O(1) disk reads, even for erasure coded files.&lt;/p&gt; 
&lt;p&gt;MinIO has full-time erasure coding. SeaweedFS uses replication on hot data for faster speed and optionally applies erasure coding on warm data.&lt;/p&gt; 
&lt;p&gt;MinIO does not have POSIX-like API support.&lt;/p&gt; 
&lt;p&gt;MinIO has specific requirements on storage layout. It is not flexible to adjust capacity. In SeaweedFS, just start one volume server pointing to the master. That's all.&lt;/p&gt; 
&lt;h2&gt;Dev Plan&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;More tools and documentation, on how to manage and scale the system.&lt;/li&gt; 
 &lt;li&gt;Read and write stream data.&lt;/li&gt; 
 &lt;li&gt;Support structured data.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This is a super exciting project! And we need helpers and &lt;a href="https://www.patreon.com/seaweedfs"&gt;support&lt;/a&gt;!&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#table-of-contents"&gt;Back to TOC&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Installation Guide&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Installation guide for users who are not familiar with golang&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Step 1: install go on your machine and setup the environment by following the instructions at:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://golang.org/doc/install"&gt;https://golang.org/doc/install&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;make sure to define your $GOPATH&lt;/p&gt; 
&lt;p&gt;Step 2: checkout this repo:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/seaweedfs/seaweedfs.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Step 3: download, compile, and install the project by executing the following command&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd seaweedfs/weed &amp;amp;&amp;amp; make install
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Once this is done, you will find the executable "weed" in your &lt;code&gt;$GOPATH/bin&lt;/code&gt; directory&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#table-of-contents"&gt;Back to TOC&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Disk Related Topics&lt;/h2&gt; 
&lt;h3&gt;Hard Drive Performance&lt;/h3&gt; 
&lt;p&gt;When testing read performance on SeaweedFS, it basically becomes a performance test of your hard drive's random read speed. Hard drives usually get 100MB/s~200MB/s.&lt;/p&gt; 
&lt;h3&gt;Solid State Disk&lt;/h3&gt; 
&lt;p&gt;To modify or delete small files, SSD must delete a whole block at a time, and move content in existing blocks to a new block. SSD is fast when brand new, but will get fragmented over time and you have to garbage collect, compacting blocks. SeaweedFS is friendly to SSD since it is append-only. Deletion and compaction are done on volume level in the background, not slowing reading and not causing fragmentation.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#table-of-contents"&gt;Back to TOC&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Benchmark&lt;/h2&gt; 
&lt;p&gt;My Own Unscientific Single Machine Results on Mac Book with Solid State Disk, CPU: 1 Intel Core i7 2.6GHz.&lt;/p&gt; 
&lt;p&gt;Write 1 million 1KB file:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Concurrency Level:      16
Time taken for tests:   66.753 seconds
Completed requests:      1048576
Failed requests:        0
Total transferred:      1106789009 bytes
Requests per second:    15708.23 [#/sec]
Transfer rate:          16191.69 [Kbytes/sec]

Connection Times (ms)
              min      avg        max      std
Total:        0.3      1.0       84.3      0.9

Percentage of the requests served within a certain time (ms)
   50%      0.8 ms
   66%      1.0 ms
   75%      1.1 ms
   80%      1.2 ms
   90%      1.4 ms
   95%      1.7 ms
   98%      2.1 ms
   99%      2.6 ms
  100%     84.3 ms
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Randomly read 1 million files:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Concurrency Level:      16
Time taken for tests:   22.301 seconds
Completed requests:      1048576
Failed requests:        0
Total transferred:      1106812873 bytes
Requests per second:    47019.38 [#/sec]
Transfer rate:          48467.57 [Kbytes/sec]

Connection Times (ms)
              min      avg        max      std
Total:        0.0      0.3       54.1      0.2

Percentage of the requests served within a certain time (ms)
   50%      0.3 ms
   90%      0.4 ms
   98%      0.6 ms
   99%      0.7 ms
  100%     54.1 ms
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Run WARP and launch a mixed benchmark.&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;make benchmark
warp: Benchmark data written to "warp-mixed-2023-10-16[102354]-l70a.csv.zst"                                                                                                                                                                                               
Mixed operations.
Operation: DELETE, 10%, Concurrency: 20, Ran 4m59s.
 * Throughput: 6.19 obj/s

Operation: GET, 45%, Concurrency: 20, Ran 5m0s.
 * Throughput: 279.85 MiB/s, 27.99 obj/s

Operation: PUT, 15%, Concurrency: 20, Ran 5m0s.
 * Throughput: 89.86 MiB/s, 8.99 obj/s

Operation: STAT, 30%, Concurrency: 20, Ran 5m0s.
 * Throughput: 18.63 obj/s

Cluster Total: 369.74 MiB/s, 61.79 obj/s, 0 errors over 5m0s.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To see segmented request statistics, use the --analyze.v parameter.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;warp analyze --analyze.v warp-mixed-2023-10-16[102354]-l70a.csv.zst
18642 operations loaded... Done!
Mixed operations.
----------------------------------------
Operation: DELETE - total: 1854, 10.0%, Concurrency: 20, Ran 5m0s, starting 2023-10-16 10:23:57.115 +0500 +05
 * Throughput: 6.19 obj/s

Requests considered: 1855:
 * Avg: 104ms, 50%: 30ms, 90%: 207ms, 99%: 1.355s, Fastest: 1ms, Slowest: 4.613s, StdDev: 320ms

----------------------------------------
Operation: GET - total: 8388, 45.3%, Size: 10485760 bytes. Concurrency: 20, Ran 5m0s, starting 2023-10-16 10:23:57.12 +0500 +05
 * Throughput: 279.77 MiB/s, 27.98 obj/s

Requests considered: 8389:
 * Avg: 221ms, 50%: 106ms, 90%: 492ms, 99%: 1.739s, Fastest: 8ms, Slowest: 8.633s, StdDev: 383ms
 * TTFB: Avg: 81ms, Best: 2ms, 25th: 24ms, Median: 39ms, 75th: 65ms, 90th: 171ms, 99th: 669ms, Worst: 4.783s StdDev: 163ms
 * First Access: Avg: 240ms, 50%: 105ms, 90%: 511ms, 99%: 2.08s, Fastest: 12ms, Slowest: 8.633s, StdDev: 480ms
 * First Access TTFB: Avg: 88ms, Best: 2ms, 25th: 24ms, Median: 38ms, 75th: 64ms, 90th: 179ms, 99th: 919ms, Worst: 4.783s StdDev: 199ms
 * Last Access: Avg: 219ms, 50%: 106ms, 90%: 463ms, 99%: 1.782s, Fastest: 9ms, Slowest: 8.633s, StdDev: 416ms
 * Last Access TTFB: Avg: 81ms, Best: 2ms, 25th: 24ms, Median: 39ms, 75th: 65ms, 90th: 161ms, 99th: 657ms, Worst: 4.783s StdDev: 176ms

----------------------------------------
Operation: PUT - total: 2688, 14.5%, Size: 10485760 bytes. Concurrency: 20, Ran 5m0s, starting 2023-10-16 10:23:57.115 +0500 +05
 * Throughput: 89.83 MiB/s, 8.98 obj/s

Requests considered: 2689:
 * Avg: 1.165s, 50%: 878ms, 90%: 2.015s, 99%: 5.74s, Fastest: 99ms, Slowest: 8.264s, StdDev: 968ms

----------------------------------------
Operation: STAT - total: 5586, 30.2%, Concurrency: 20, Ran 5m0s, starting 2023-10-16 10:23:57.113 +0500 +05
 * Throughput: 18.63 obj/s

Requests considered: 5587:
 * Avg: 15ms, 50%: 11ms, 90%: 34ms, 99%: 80ms, Fastest: 0s, Slowest: 245ms, StdDev: 17ms
 * First Access: Avg: 14ms, 50%: 10ms, 90%: 33ms, 99%: 69ms, Fastest: 0s, Slowest: 203ms, StdDev: 16ms
 * Last Access: Avg: 15ms, 50%: 11ms, 90%: 34ms, 99%: 74ms, Fastest: 0s, Slowest: 203ms, StdDev: 17ms

Cluster Total: 369.64 MiB/s, 61.77 obj/s, 0 errors over 5m0s.
Total Errors:0.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#table-of-contents"&gt;Back to TOC&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Enterprise&lt;/h2&gt; 
&lt;p&gt;For enterprise users, please visit &lt;a href="https://seaweedfs.com"&gt;seaweedfs.com&lt;/a&gt; for the SeaweedFS Enterprise Edition, which has a self-healing storage format with better data protection.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#table-of-contents"&gt;Back to TOC&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;http://www.apache.org/licenses/LICENSE-2.0
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.&lt;/p&gt; 
&lt;p&gt;The text of this page is available for modification and reuse under the terms of the Creative Commons Attribution-Sharealike 3.0 Unported License and the GNU Free Documentation License (unversioned, with no invariant sections, front-cover texts, or back-cover texts).&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/seaweedfs/seaweedfs/master/#table-of-contents"&gt;Back to TOC&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Stargazers over time&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://starchart.cc/chrislusf/seaweedfs"&gt;&lt;img src="https://starchart.cc/chrislusf/seaweedfs.svg?sanitize=true" alt="Stargazers over time" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>TheRobotStudio/SO-ARM100</title>
      <link>https://github.com/TheRobotStudio/SO-ARM100</link>
      <description>&lt;p&gt;Standard Open Arm 100&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h1&gt;Standard Open SO-100 &amp;amp; SO-101 Arms&lt;/h1&gt; 
 &lt;div style="display: flex; gap: 1rem; justify-content: center; align-items: center;"&gt; 
  &lt;img src="https://raw.githubusercontent.com/TheRobotStudio/SO-ARM100/main/media/SO101_Follower.webp?raw=true" alt="SO-101 follower arm" title="SO-101 follower arm" style="width: 40%;" /&gt; 
  &lt;img src="https://raw.githubusercontent.com/TheRobotStudio/SO-ARM100/main/media/SO101_Leader.webp?raw=true" alt="SO-101 leader arm" title="SO-101 leader arm" style="width: 40%;" /&gt; 
 &lt;/div&gt; 
 &lt;h2&gt; &lt;p&gt;Build Your Own SO-101 Robot!&lt;/p&gt; &lt;/h2&gt; 
 &lt;p&gt;The SO‚Äë101 is the next‚Äëgeneration version of the SO‚Äë100 robot arm, originally designed by the &lt;a href="https://www.therobotstudio.com"&gt;RobotStudio&lt;/a&gt; in collaboration with &lt;a href="https://huggingface.co/lerobot"&gt;Hugging Face&lt;/a&gt;. It has improved wiring, is easier to assemble (no gear removal) and uses updated motors for the leader arm.&lt;/p&gt; 
 &lt;p&gt;These arms are designed to work seamlessly with the open‚Äësource ü§ó LeRobot library. Join our community on &lt;a href="https://discord.gg/ggrqhPTsMe"&gt;Discord&lt;/a&gt; to collaborate on both hardware and software, and help make end‚Äëto‚Äëend AI for robotics more accessible.&lt;/p&gt; 
&lt;/div&gt; 
&lt;h3&gt;Documentation üìñ&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;For the SO‚Äë101 docs, follow this page further.&lt;/li&gt; 
 &lt;li&gt;Here you can find the &lt;a href="https://raw.githubusercontent.com/TheRobotStudio/SO-ARM100/main/SO100.md"&gt;SO‚Äë100 docs&lt;/a&gt; which is deprecated.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Getting Your Own SO‚Äë101&lt;/h3&gt; 
&lt;p&gt;You have two options:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Build it Yourself&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Source the components from the &lt;a href="https://raw.githubusercontent.com/TheRobotStudio/SO-ARM100/main/#sourcing-parts"&gt;Bill of Materials&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;3D print the parts (or order the 3D printed parts), explained in &lt;a href="https://raw.githubusercontent.com/TheRobotStudio/SO-ARM100/main/#printing-the-parts"&gt;Printing the Parts&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;Follow our &lt;a href="https://huggingface.co/docs/lerobot/so101"&gt;Assembly Guide&lt;/a&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Buy a Kit&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Buy assembled arms or a parts kit from one of the vendors &lt;a href="https://raw.githubusercontent.com/TheRobotStudio/SO-ARM100/main/#kits"&gt;here&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;Optionally follow our &lt;a href="https://huggingface.co/docs/lerobot/so101"&gt;Assembly Guide&lt;/a&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Setup with LeRobot ü§ó&lt;/h3&gt; 
&lt;p&gt;After sourcing all parts you can setup your SO-101 with LeRobot &lt;a href="https://huggingface.co/docs/lerobot/so101"&gt;tutorial&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Optional Hardware üîß&lt;/h3&gt; 
&lt;p&gt;This repository also includes a range of optional hardware designs such as a raised leader base and different camera mounts. Here you can explore the &lt;a href="https://raw.githubusercontent.com/TheRobotStudio/SO-ARM100/main/#optional-hardware"&gt;full list&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Kits&lt;/h2&gt; 
&lt;p&gt;You can find all optional for SO-100/SO-101 kits here:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;from PartaBot &lt;span&gt;üá∫üá∏&lt;/span&gt; &lt;a href="https://partabot.com"&gt;US&lt;/a&gt; (They include &lt;strong&gt;assembled&lt;/strong&gt; versions, and also sell LeKiwi and Koch robots)&lt;/li&gt; 
 &lt;li&gt;from Seeed studio &lt;span&gt;üåç&lt;/span&gt; &lt;a href="https://www.seeedstudio.com/SO-ARM100-Low-Cost-AI-Arm-Kit.html"&gt;International&lt;/a&gt; or &lt;span&gt;üá®üá≥&lt;/span&gt; &lt;a href="https://item.taobao.com/item.htm?id=878010637397&amp;amp;skuId=5915703371829&amp;amp;spm=a213gs.v2success.0.0.4cbf4831mkqWLn"&gt;China&lt;/a&gt; or &lt;span&gt;üáØüáµ&lt;/span&gt; &lt;a href="https://akizukidenshi.com/catalog/g/g131169/"&gt;Akizuki Denshi&lt;/a&gt; or &lt;a href="https://www.aliexpress.com/item/3256808696884714.html?gatewayAdapt=4itemAdapt"&gt;Aliexpress&lt;/a&gt; (They include &lt;strong&gt;3d printed kits&lt;/strong&gt;)&lt;/li&gt; 
 &lt;li&gt;from WowRobo &lt;span&gt;üåç&lt;/span&gt; &lt;a href="https://shop.wowrobo.com/products/so-arm101-diy-kit-assembled-version-1"&gt;International&lt;/a&gt; or &lt;span&gt;üá®üá≥&lt;/span&gt; &lt;a href="https://item.taobao.com/item.htm?ft=t&amp;amp;id=860171734711"&gt;China&lt;/a&gt; (They include &lt;strong&gt;assembled&lt;/strong&gt; versions)&lt;/li&gt; 
 &lt;li&gt;from Sudoremove &lt;span&gt;üá∞üá∑&lt;/span&gt; &lt;a href="https://smartstore.naver.com/sudoremove/products/12022333117"&gt;South Korea&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;from NeoBot &lt;span&gt;üá®üá≥&lt;/span&gt; &lt;a href="https://item.taobao.com/item.htm?ft=t&amp;amp;id=957685951340"&gt;China&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Additionally you can find SO-100 follower arm kit (without leader arm) on &lt;a href="https://robots.phospho.ai"&gt;Phospho&lt;/a&gt;. It can be especially useful if you own a VR headset.&lt;/p&gt; 
&lt;h2&gt;Sourcing Parts&lt;/h2&gt; 
&lt;p&gt;The follower and leader arm for this teleoperation setup will almost the same off the shelf parts (except for the motors). If you plan on creating the classic teleoperation set up to be used with the &lt;code&gt;LeRobot&lt;/code&gt; library please buy from the Parts for Two Arms below.&lt;/p&gt; 
&lt;p&gt;We only have links for US, EU, CN, and JP for now. If you find links for other countries, please create an issue or PR so that we add them to the list. Note that prices and items may vary depending on geographic location.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT]&lt;br /&gt; The STS3215 motors for the follower arm comes in two sizes. The 7.4V has a stall torque of 16.5kg.cm at 6V (and likely slightly less for a 5V power supply). The 12V version has a stall torque of 30kg.cm. While we found the 7.4V to be sufficient, if you would like more powerful motors you can buy the 12V version &lt;a href="https://www.alibaba.com/product-detail/6PCS-12V-30KG-STS3215-High-Torque_1601216757543.html"&gt;here&lt;/a&gt;. Note if you do this, you will also have to buy a 12V 5A+ power supply instead of a 5V one. The leader arm is always 7.4V for the SO101.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Parts For Two Arms (Follower and Leader Setup):&lt;/h4&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Part&lt;/th&gt; 
   &lt;th&gt;Amount&lt;/th&gt; 
   &lt;th&gt;Unit Cost (US)&lt;/th&gt; 
   &lt;th&gt;Buy US&lt;/th&gt; 
   &lt;th&gt;Unit Cost (EU)&lt;/th&gt; 
   &lt;th&gt;Buy EU&lt;/th&gt; 
   &lt;th&gt;Unit Cost (RMB)&lt;/th&gt; 
   &lt;th&gt;Buy CN&lt;/th&gt; 
   &lt;th&gt;Unit Cost (JPY)&lt;/th&gt; 
   &lt;th&gt;Buy JP&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;STS3215 Servo 7.4V, 1/345 gear (C001) **&lt;sup&gt;&lt;a href="https://raw.githubusercontent.com/TheRobotStudio/SO-ARM100/main/#leaderbundle"&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/td&gt; 
   &lt;td&gt;7&lt;/td&gt; 
   &lt;td&gt;$13.89&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.alibaba.com/product-detail/Top-Seller-Low-Cost-Feetech-STS3215_1600999461525.html"&gt;Alibaba&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚Ç¨12.2&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.alibaba.com/product-detail/Top-Seller-Low-Cost-Feetech-STS3215_1600999461525.html"&gt;Alibaba&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Ôø•97.72&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://item.taobao.com/item.htm?id=712179366565&amp;amp;skuId=5268252241438"&gt;TaoBao&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Ôø•2,980&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://akizukidenshi.com/catalog/g/g116312/"&gt;Akizuki Denshi&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;STS3215 Servo 7.4V, 1/191 gear (C044) **&lt;sup&gt;&lt;a href="https://raw.githubusercontent.com/TheRobotStudio/SO-ARM100/main/#leaderbundle"&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/td&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;$13.89&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.alibaba.com/product-detail/Feetech-STS3215-SO-ARM101-Servo-7_1601430747897.html?spm=a2747.product_manager.0.0.59a371d2W4e0SR"&gt;Alibaba&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚Ç¨12.2&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.alibaba.com/product-detail/Feetech-STS3215-SO-ARM101-Servo-7_1601430747897.html?spm=a2747.product_manager.0.0.59a371d2W4e0SR"&gt;Alibaba&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Ôø•97.72&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;Ôø•2,980&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://akizukidenshi.com/catalog/g/g131131/"&gt;Akizuki Denshi&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;STS3215 Servo 7.4V, 1/147 gear (C046) **&lt;sup&gt;&lt;a href="https://raw.githubusercontent.com/TheRobotStudio/SO-ARM100/main/#leaderbundle"&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/td&gt; 
   &lt;td&gt;3&lt;/td&gt; 
   &lt;td&gt;$13.89&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.alibaba.com/product-detail/Feetech-STS3215-SO-ARM101-Servo-7_1601430760797.html?spm=a2747.product_manager.0.0.167371d25QeX3F"&gt;Alibaba&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚Ç¨12.2&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.alibaba.com/product-detail/Feetech-STS3215-SO-ARM101-Servo-7_1601430760797.html?spm=a2747.product_manager.0.0.167371d25QeX3F"&gt;Alibaba&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Ôø•97.72&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;Ôø•2,980&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://akizukidenshi.com/catalog/g/g131132/"&gt;Akizuki Denshi&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Motor Control Board&lt;/td&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;$10.6&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.amazon.com/Waveshare-Integrates-Control-Circuit-Supports/dp/B0CTMM4LWK/"&gt;Amazon&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚Ç¨11.4&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.amazon.fr/-/en/dp/B0CJ6TP3TP/"&gt;Amazon&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Ôø•27&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://detail.tmall.com/item.htm?id=738817173460&amp;amp;skuId=5096283384143"&gt;TaoBao&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Ôø•980&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://akizukidenshi.com/catalog/g/g131227/"&gt;Akizuki Denshi&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;USB-C Cable 2 pcs&lt;/td&gt; 
   &lt;td&gt;1&lt;/td&gt; 
   &lt;td&gt;$7&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.amazon.com/Charging-etguuds-Charger-Braided-Compatible/dp/B0B8NWLLW2/?th=1"&gt;Amazon&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚Ç¨7&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.amazon.fr/dp/B07BNF842T/"&gt;Amazon&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Ôø•23.9*2&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://detail.tmall.com/item.htm?id=44425281296&amp;amp;skuId=5611379016222"&gt;TaoBao&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Ôø•1,498&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.amazon.co.jp/dp/B0C3H9L6KZ"&gt;Amazon&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Power Supply&lt;/td&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;$10&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.amazon.com/Facmogu-Switching-Transformer-Compatible-5-5x2-1mm/dp/B087LY41PV/"&gt;Amazon&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚Ç¨15.7&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.amazon.fr/-/en/dp/B01HRR9GY4/"&gt;Amazon&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Ôø•22.31&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://item.taobao.com/item.htm?id=544824248494&amp;amp;skuId=4974994129990"&gt;TaoBao&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Ôø•1,550&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://akizukidenshi.com/catalog/g/g106238/"&gt;Akizuki Denshi&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Table Clamp 4pcs&lt;/td&gt; 
   &lt;td&gt;1&lt;/td&gt; 
   &lt;td&gt;$9&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.amazon.com/TAODAN-Trigger-Ratchet-Woodworking-Processes/dp/B0DJNXF8WH?rps=1&amp;amp;sr=1-18"&gt;Amazon&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚Ç¨9.7&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.amazon.fr/Connex-COXT865210-Lot-Serre-joints-bricolage/dp/B00NA3T2CQ"&gt;Amazon&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Ôø•5.2*4&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://detail.tmall.com/item.htm?id=801399113134&amp;amp;skuId=5633627126649"&gt;TaoBao&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Ôø•2,200&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.amazon.co.jp/dp/B0DJNXF8WH"&gt;Amazon&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Screwdriver Set&lt;sup&gt;&lt;a href="https://raw.githubusercontent.com/TheRobotStudio/SO-ARM100/main/#myfootnote1"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/td&gt; 
   &lt;td&gt;1&lt;/td&gt; 
   &lt;td&gt;$6&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.amazon.com/Precision-Phillips-Screwdriver-Electronics-Computer/dp/B0DB227RTH"&gt;Amazon&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚Ç¨9&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.amazon.fr/Vinabo-Magn%C3%A9tique-Electronique-R%C3%A9paration-Informatique/dp/B0BNQBNFFJ"&gt;Amazon&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Ôø•14.9&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://detail.tmall.com/item.htm?id=675684600845&amp;amp;skuId=4856851392176"&gt;TaoBao&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Ôø•500&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.amazon.co.jp/dp/B01MDNJVMN"&gt;Amazon&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Total&lt;/td&gt; 
   &lt;td&gt;---&lt;/td&gt; 
   &lt;td&gt;$229.88&lt;/td&gt; 
   &lt;td&gt;---&lt;/td&gt; 
   &lt;td&gt;‚Ç¨226.3&lt;/td&gt; 
   &lt;td&gt;---&lt;/td&gt; 
   &lt;td&gt;Ôø•1343.16&lt;/td&gt; 
   &lt;td&gt;---&lt;/td&gt; 
   &lt;td&gt;Ôø•44,530&lt;/td&gt; 
   &lt;td&gt;---&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;a name="leaderbundle"&gt;2&lt;/a&gt;: You can buy &lt;strong&gt;all six STS3215 servos needed for the SO-101 leader arm&lt;/strong&gt;&lt;br /&gt; (3 √ó 1/147 gear (C046), 2 √ó 1/191 gear (C044), 1 √ó 1/345 gear (C001)) in a single bundle on &lt;a href="https://www.alibaba.com/product-detail/6PCS-7-4V-STS3215-Servos-for_1601428584027.html?spm=a2747.product_manager.0.0.757c2c3clU7uH3"&gt;Alibaba&lt;/a&gt;.&lt;/p&gt; 
&lt;h4&gt;Parts for One Follower Arm:&lt;/h4&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Part&lt;/th&gt; 
   &lt;th&gt;Amount&lt;/th&gt; 
   &lt;th&gt;Unit Cost (US)&lt;/th&gt; 
   &lt;th&gt;Buy US&lt;/th&gt; 
   &lt;th&gt;Unit Cost (EU)&lt;/th&gt; 
   &lt;th&gt;Buy EU&lt;/th&gt; 
   &lt;th&gt;Unit Cost (RMB)&lt;/th&gt; 
   &lt;th&gt;Buy CN&lt;/th&gt; 
   &lt;th&gt;Unit Cost (JPY)&lt;/th&gt; 
   &lt;th&gt;Buy JP&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;STS3215 Servo 7.4V, 1/345 gear (C001)&lt;/td&gt; 
   &lt;td&gt;6&lt;/td&gt; 
   &lt;td&gt;$13.89&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.alibaba.com/product-detail/Top-Seller-Low-Cost-Feetech-STS3215_1600999461525.html?spm=a2747.product_manager.0.0.11be71d2ARQb82"&gt;Alibaba&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚Ç¨12.2&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.alibaba.com/product-detail/Top-Seller-Low-Cost-Feetech-STS3215_1600999461525.html?spm=a2747.product_manager.0.0.11be71d2ARQb82"&gt;Alibaba&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Ôø•97.72&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://item.taobao.com/item.htm?id=712179366565&amp;amp;skuId=5268252241438"&gt;TaoBao&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Ôø•2,980&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://akizukidenshi.com/catalog/g/g116312/"&gt;Akizuki Denshi&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Motor Control Board&lt;/td&gt; 
   &lt;td&gt;1&lt;/td&gt; 
   &lt;td&gt;$10.6&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.amazon.com/Waveshare-Integrates-Control-Circuit-Supports/dp/B0CTMM4LWK/"&gt;Amazon&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚Ç¨11.4&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.amazon.fr/-/en/dp/B0CJ6TP3TP/"&gt;Amazon&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Ôø•27&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://detail.tmall.com/item.htm?id=738817173460&amp;amp;skuId=5096283384143"&gt;TaoBao&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Ôø•980&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://akizukidenshi.com/catalog/g/g131227/"&gt;Akizuki Denshi&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;USB-C Cable 2 pcs&lt;/td&gt; 
   &lt;td&gt;1&lt;/td&gt; 
   &lt;td&gt;$7&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.amazon.com/Charging-etguuds-Charger-Braided-Compatible/dp/B0B8NWLLW2/?th=1"&gt;Amazon&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚Ç¨7&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.amazon.fr/dp/B07BNF842T/"&gt;Amazon&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Ôø•23.9&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://detail.tmall.com/item.htm?id=44425281296&amp;amp;skuId=5611379016222"&gt;TaoBao&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Ôø•1,498&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.amazon.co.jp/dp/B0C3H9L6KZ"&gt;Amazon&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Power Supply&lt;/td&gt; 
   &lt;td&gt;1&lt;/td&gt; 
   &lt;td&gt;$10&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.amazon.com/Facmogu-Switching-Transformer-Compatible-5-5x2-1mm/dp/B087LY41PV/"&gt;Amazon&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚Ç¨15.7&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.amazon.fr/-/en/dp/B01HRR9GY4/"&gt;Amazon&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Ôø•22.31&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://item.taobao.com/item.htm?id=544824248494&amp;amp;skuId=4974994129990"&gt;TaoBao&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Ôø•1,550&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://akizukidenshi.com/catalog/g/g106238/"&gt;Akizuki Denshi&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Table Clamp 2pcs&lt;/td&gt; 
   &lt;td&gt;1&lt;/td&gt; 
   &lt;td&gt;$5&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.amazon.com/Mr-Pen-Carpenter-Clamp-6inch/dp/B092L925J4/"&gt;Amazon&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚Ç¨8&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.amazon.fr/-/en/dp/B08HZ1QRBF/"&gt;Amazon&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Ôø•7.8&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://detail.tmall.com/item.htm?id=738636473238&amp;amp;skuId=5505939904942"&gt;TaoBao&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Ôø•2,200&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.amazon.co.jp/dp/B0DJNXF8WH"&gt;Amazon&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Screwdriver Set&lt;sup&gt;&lt;a href="https://raw.githubusercontent.com/TheRobotStudio/SO-ARM100/main/#myfootnote1"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/td&gt; 
   &lt;td&gt;1&lt;/td&gt; 
   &lt;td&gt;$6&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.amazon.com/Precision-Phillips-Screwdriver-Electronics-Computer/dp/B0DB227RTH"&gt;Amazon&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚Ç¨9&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.amazon.fr/Vinabo-Magn%C3%A9tique-Electronique-R%C3%A9paration-Informatique/dp/B0BNQBNFFJ"&gt;Amazon&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Ôø•14.9&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://detail.tmall.com/item.htm?id=675684600845&amp;amp;skuId=4856851392176"&gt;TaoBao&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Ôø•500&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.amazon.co.jp/dp/B01MDNJVMN"&gt;Amazon&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Total&lt;/td&gt; 
   &lt;td&gt;---&lt;/td&gt; 
   &lt;td&gt;$121.94&lt;/td&gt; 
   &lt;td&gt;---&lt;/td&gt; 
   &lt;td&gt;‚Ç¨124.3&lt;/td&gt; 
   &lt;td&gt;---&lt;/td&gt; 
   &lt;td&gt;Ôø•682.23&lt;/td&gt; 
   &lt;td&gt;---&lt;/td&gt; 
   &lt;td&gt;Ôø•24,414&lt;/td&gt; 
   &lt;td&gt;---&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;a name="myfootnote1"&gt;1&lt;/a&gt;: You do not need to use this exact screwdriver set, but it is highly recommended to have phillips head screw driver sizes #0 and #1 for easiest screw installation and removal. These are both standard sizes which will likely appear in most small screwdriver sets.&lt;/p&gt; 
&lt;h2&gt;Printing the Parts&lt;/h2&gt; 
&lt;p&gt;A variety of 3D printers are acceptable to print the parts necessary of the follower and leader arm. Follow the steps below to ensure a good print.&lt;/p&gt; 
&lt;h3&gt;Step 1: Choose a Printer&lt;/h3&gt; 
&lt;p&gt;The STL files provided are ready to print on many FDM printers. Below are the tested and suggested settings though others may work.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Material: PLA+&lt;/li&gt; 
 &lt;li&gt;Nozzle Diameter and Precision: 0.4mm nozzle diameter at 0.2mm layer height or 0.6mm nozzle at 0.4mm layer height.&lt;/li&gt; 
 &lt;li&gt;Infill Density: 15%&lt;/li&gt; 
 &lt;li&gt;Sample Printers: &lt;a href="https://www.prusa3d.com/product/original-prusa-mini-semi-assembled-3d-printer-4/"&gt;Prusa MINI+&lt;/a&gt;, &lt;a href="https://shop.tiertime.com/product/tiertime-up-plus-2-3d-printer/"&gt;UP Plus 2&lt;/a&gt;, &lt;a href="https://www.amazon.com/Comgrow-Creality-Ender-Aluminum-220x220x250mm/dp/B07BR3F9N6/"&gt;Creality Ender 3&lt;/a&gt;, &lt;a href="https://bambulab.com"&gt;Bambu Lab A/P/X-series&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Step 2: Set up the Printer&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Ensure that the printer is calibrated and the bed level is correctly set using the printer specific instructions.&lt;/li&gt; 
 &lt;li&gt;Clean the print bed, making sure it is free from dust, or grease. If cleaning the bed using water, or other liquid, dry the bed.&lt;/li&gt; 
 &lt;li&gt;If your printer recommends it, use a standard glue stick and apply a thin, even layer of glue across the print area of the bed. Avoid clumping or uneven application.&lt;/li&gt; 
 &lt;li&gt;Load the printer filament using printer specific instructions.&lt;/li&gt; 
 &lt;li&gt;Ensure the printer settings match the ones suggested above (most printers have multiple settings so choose the ones that most closely match).&lt;/li&gt; 
 &lt;li&gt;Set for supports everywhere but ignore slopes greater than 45 degrees to the horizontal.&lt;/li&gt; 
 &lt;li&gt;There should be no supports in the screw holes with horizontal axes.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Step 3: Check Printer Accuracy&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;In the &lt;a href="https://raw.githubusercontent.com/TheRobotStudio/SO-ARM100/main/STL/Gauges"&gt;Gauges&lt;/a&gt; folder, there are two types of gauges, one to check the size of print against a standard 4x2 lego block and one against a STS3215 servo. 
  &lt;ol&gt; 
   &lt;li&gt;If you have a STS3215 servo, print: 
    &lt;ol&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/TheRobotStudio/SO-ARM100/main/STL/Gauges/Gauge_0.STL"&gt;Gauge Zero&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/TheRobotStudio/SO-ARM100/main/STL/Gauges/Gauge_tight_1.STL"&gt;Gauge Tight&lt;/a&gt;&lt;/li&gt; 
    &lt;/ol&gt; &lt;/li&gt; 
   &lt;li&gt;If you have a standard lego block, print: 
    &lt;ol&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/TheRobotStudio/SO-ARM100/main/STL/Gauges/Lego_Size_Test_02_zero.STL"&gt;Gauge Zero&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/TheRobotStudio/SO-ARM100/main/STL/Gauges/Lego_Size_Test_02_minuspoint1.STL"&gt;Gauge -0.1&lt;/a&gt;&lt;/li&gt; 
    &lt;/ol&gt; &lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
 &lt;li&gt;Test the gauge 0 against your given object (Lego or Servo). The fit should be similar to this &lt;a href="https://youtu.be/dss8E3DG2rA"&gt;tutorial&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;If the fit is appropriate, go onto Step 4, otherwise, change your printer settings and try again or create an issue.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Step 4: Print the Parts&lt;/h3&gt; 
&lt;p&gt;All the parts for the leader or follower are for easy 3D printing already contained in a &lt;strong&gt;single file&lt;/strong&gt;, correctly orientated for z upwards to minimize supports.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;For printer bed sizes of 220mmx220mm (such as the Ender), print these files: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/TheRobotStudio/SO-ARM100/main/STL/SO101/Follower/Ender_Follower_SO101.stl"&gt;Follower&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/TheRobotStudio/SO-ARM100/main/STL/SO101/Leader/Ender_Leader_SO101.stl"&gt;Leader&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;For printer bed sizes of 205mm x 250mm (such as the Prusa/Up): 
  &lt;ol&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/TheRobotStudio/SO-ARM100/main/STL/SO101/Follower/Prusa_Follower_SO101.stl"&gt;Follower&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/TheRobotStudio/SO-ARM100/main/STL/SO101/Leader/Prusa_Leader_SO101.stl"&gt;Leader&lt;/a&gt;&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;This table contains all individual files:&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Individual Part Files&lt;/summary&gt; 
 &lt;h4&gt;Common Parts&lt;/h4&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Part&lt;/th&gt; 
    &lt;th&gt;Link&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Base_motor_holder_SO101.stl&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/TheRobotStudio/SO-ARM100/main/STL/SO101/Individual/Base_motor_holder_SO101.stl"&gt;Base_motor_holder_SO101.stl&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Base_SO101.stl&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/TheRobotStudio/SO-ARM100/main/STL/SO101/Individual/Base_SO101.stl"&gt;Base_SO101.stl&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Motor_holder_SO101_Base.stl&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/TheRobotStudio/SO-ARM100/main/STL/SO101/Individual/Motor_holder_SO101_Base.stl"&gt;Motor_holder_SO101_Base.stl&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Motor_holder_SO101_Wrist.stl&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/TheRobotStudio/SO-ARM100/main/STL/SO101/Individual/Motor_holder_SO101_Wrist.stl"&gt;Motor_holder_SO101_Wrist.stl&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Under_arm_SO101.stl&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/TheRobotStudio/SO-ARM100/main/STL/SO101/Individual/Under_arm_SO101.stl"&gt;Under_arm_SO101.stl&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Upper_arm_SO101.stl&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/TheRobotStudio/SO-ARM100/main/STL/SO101/Individual/Upper_arm_SO101.stl"&gt;Upper_arm_SO101.stl&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Rotation_Pitch_SO101.stl&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/TheRobotStudio/SO-ARM100/main/STL/SO101/Individual/Rotation_Pitch_SO101.stl"&gt;Rotation_Pitch_SO101.stl&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Wrist_Roll_Pitch_SO101.stl&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/TheRobotStudio/SO-ARM100/main/STL/SO101/Individual/Wrist_Roll_Pitch_SO101.stl"&gt;Wrist_Roll_Pitch_SO101.stl&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;WaveShare_Mounting_Plate_SO101.stl&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/TheRobotStudio/SO-ARM100/main/STL/SO101/Individual/WaveShare_Mounting_Plate_SO101.stl"&gt;WaveShare_Mounting_Plate_SO101.stl&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;h4&gt;Leader‚ÄëSpecific Parts&lt;/h4&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Part&lt;/th&gt; 
    &lt;th&gt;Link&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Handle_SO101.stl&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/TheRobotStudio/SO-ARM100/main/STL/SO101/Individual/Handle_SO101.stl"&gt;Handle_SO101.stl&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Trigger_SO101.stl&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/TheRobotStudio/SO-ARM100/main/STL/SO101/Individual/Trigger_SO101.stl"&gt;Trigger_SO101.stl&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Wrist_Roll_SO101.stl&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/TheRobotStudio/SO-ARM100/main/STL/SO101/Individual/Wrist_Roll_SO101.stl"&gt;Wrist_Roll_SO101.stl&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;h4&gt;Follower‚ÄëSpecific Parts&lt;/h4&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Part&lt;/th&gt; 
    &lt;th&gt;Link&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Moving_Jaw_SO101.stl&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/TheRobotStudio/SO-ARM100/main/STL/SO101/Individual/Moving_Jaw_SO101.stl"&gt;Moving_Jaw_SO101.stl&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Wrist_Roll_Follower_SO101.stl&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/TheRobotStudio/SO-ARM100/main/STL/SO101/Individual/Wrist_Roll_Follower_SO101.stl"&gt;Wrist_Roll_Follower_SO101.stl&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h3&gt;Step 5: Remove Supports&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;After the print is done, use a putty knife to scrape the the parts off the print bed.&lt;/li&gt; 
 &lt;li&gt;Remove any support material from parts.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Don't Own a 3D printer?&lt;/h3&gt; 
&lt;p&gt;Go here: &lt;a href="https://raw.githubusercontent.com/TheRobotStudio/SO-ARM100/main/3DPRINT.md"&gt;Printing services&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Optional Hardware&lt;/h2&gt; 
&lt;!--Note: no hardware emoji here so links work correctly--&gt; 
&lt;p&gt;Extend your SO‚Äë100/SO‚Äë101 with these add-ons.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Add‚Äëons&lt;/summary&gt; 
 &lt;h4&gt;0. XLeRobot&lt;/h4&gt; 
 &lt;p&gt;A dual-arm mobile robot for daily use, with 2x SO101 arms, 1x Lekiwi base, 1x 300Wh Anker Battery, 2x wrist RGB cameras, 1x head depth camera (with a 2-dof neck). Total cost 660$.&lt;/p&gt; 
 &lt;img width="1725" height="1140" alt="82c2b72eab7c716b90fafac0a9caf895" src="https://github.com/user-attachments/assets/10819ef0-80a2-4cfe-be81-7daa8918cca1" /&gt; 
 &lt;p&gt;&lt;a href="https://xlerobot.readthedocs.io/en/latest/index.html"&gt;‚Üí Full Documentation&lt;/a&gt;, with detailed BOM, 3D printing models, assembly guide, simulation, and teleop guide.&lt;/p&gt; 
 &lt;h4&gt;1. Mount Helper&lt;/h4&gt; 
 &lt;p&gt;Print the handy mount jig for easier alignment during assembly.&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/TheRobotStudio/SO-ARM100/main/Optional/Mount_Helper/README.md"&gt;‚Üí View README&lt;/a&gt;&lt;/p&gt; 
 &lt;img src="https://raw.githubusercontent.com/TheRobotStudio/SO-ARM100/main/media/mount_helper.png" alt="Mount Helper" width="150" /&gt; 
 &lt;h4&gt;2. Overhead Camera Mount&lt;/h4&gt; 
 &lt;p&gt;For bird‚Äôs‚Äëeye views in single or bi‚Äëmanual setups.&lt;br /&gt; (SO100/101)&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;Webcam&lt;/th&gt; 
    &lt;th align="center"&gt;32√ó32&amp;nbsp;Cam Module&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;&lt;img src="https://github.com/user-attachments/assets/a652e133-8672-448d-baa0-bdd494a0a515" height="200" /&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/TheRobotStudio/SO-ARM100/main/media/overhead_cam_two_followers.png" height="200" /&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/TheRobotStudio/SO-ARM100/main/Optional/Overhead_Cam_Mount_Webcam/README.md"&gt;Instructions&lt;/a&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/TheRobotStudio/SO-ARM100/main/Optional/Overhead_Cam_Mount_32x32_UVC_Module/README.md"&gt;Instructions&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;h4&gt;3. Base Mounts&lt;/h4&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;Raised Leader Base&lt;/th&gt; 
    &lt;th align="center"&gt;4040 Aluminum Profile Mount&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/TheRobotStudio/SO-ARM100/main/media/Raised_Base.jpeg" height="150" /&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/TheRobotStudio/SO-ARM100/main/media/4040_base_mount.jpg" height="150" /&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/TheRobotStudio/SO-ARM100/main/Optional/Raised_Base/Raised_Base_Extension.stl"&gt;Download STL&lt;/a&gt;&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://raw.githubusercontent.com/TheRobotStudio/SO-ARM100/main/Optional/4040_Base_Mount/README.md"&gt;Instructions&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;h4&gt;4. Tactile Sensing (AnySkin)&lt;/h4&gt; 
 &lt;p&gt;Add touch sensing to your gripper.&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://shop.wowrobo.com/products/enhanced-anyskin-premium-crafted-editionwowskin"&gt;‚Üí Find on WOWROBO&lt;/a&gt;&lt;/p&gt; 
 &lt;img src="https://raw.githubusercontent.com/TheRobotStudio/SO-ARM100/main/media/tactile_sensor_anyskin.png" alt="AnySkin Sensor" width="150" /&gt; 
 &lt;h4&gt;5. Wrist‚ÄëMount Cameras&lt;/h4&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;32√ó32&amp;nbsp;UVC Hex Nut (SO101)&lt;/th&gt; 
    &lt;th&gt;32√ó32&amp;nbsp;UVC Integrated (SO100/101)&lt;/th&gt; 
    &lt;th&gt;32√ó32&amp;nbsp;UVC Plug‚Äëon&lt;/th&gt; 
    &lt;th&gt;RealSense D405&lt;/th&gt; 
    &lt;th&gt;RealSense D435/D435I&lt;/th&gt; 
    &lt;th&gt;Webcam (Vinmooog)&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/TheRobotStudio/SO-ARM100/main/media/UVC_cam_mount_so101.jpg" height="100" /&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/TheRobotStudio/SO-ARM100/main/media/Wrist_Cam_Mount_32x32_UVC_module_1.jpg" height="100" /&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/TheRobotStudio/SO-ARM100/main/media/UVC_cam_mount_plugin.jpg" height="100" /&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/TheRobotStudio/SO-ARM100/main/media/d405_mount.jpg" height="100" /&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/TheRobotStudio/SO-ARM100/main/media/d435_mount.jpg" height="100" /&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/TheRobotStudio/SO-ARM100/main/media/cam_mount2.jpg" height="100" /&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/TheRobotStudio/SO-ARM100/main/Optional/SO101_Wrist_Cam_Hex-Nut_Mount_32x32_UVC_Module"&gt;Instructions&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/TheRobotStudio/SO-ARM100/main/Optional/Wrist_Cam_Mount_32x32_UVC_Module/README.md"&gt;Instructions&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/TheRobotStudio/SO-ARM100/main/Optional/Wrist_Cam_Plug_Mount_32x32_UVC_Module"&gt;Instructions&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/TheRobotStudio/SO-ARM100/main/Optional/Wrist_Cam_Mount_RealSense_D405"&gt;Instructions&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/TheRobotStudio/SO-ARM100/main/Optional/Wrist_Cam_Mount_RealSense_D435"&gt;Instructions&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/TheRobotStudio/SO-ARM100/main/Optional/Wrist_Cam_Mount_Vinmooog_Webcam"&gt;Instructions&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;h4&gt;6. Compliant Gripper&lt;/h4&gt; 
 &lt;p&gt;Add some flexibility to your gripper by downloading the compliant gripper parts and printing them using a flexible filament like TPU 95A.&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/TheRobotStudio/SO-ARM100/main/Optional/Compliant_Gripper/README.md"&gt;‚Üí View README&lt;/a&gt;.&lt;/p&gt; 
 &lt;img src="https://github.com/user-attachments/assets/26de0b8c-8bd6-4651-867f-1358532e2cc6" width="150" /&gt; 
 &lt;h4&gt;7. Compliant Gripper (new)&lt;/h4&gt; 
 &lt;p&gt;Printed with TPU 95A for the finger and PLA for the base. Better structure and better grasp (both precision and power). No need to print support for the TPU finger. Requires 2 additional M3 screws, optional 3M gripper tape for higher friction. &lt;img src="https://github.com/user-attachments/assets/e814ed0a-72ce-43ad-80bf-5f03b7f16b90" alt="d2e57534a9e55a3d4dc0d644242cd044" /&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/Vector-Wangel/XLeRobot/tree/main/hardware"&gt;‚Üí Find on XLeRobot&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;Debugging Motors&lt;/h2&gt; 
&lt;p&gt;For debugging, any Windows PC can connect over USB to program the servos and to debug or do tests. To do so download &lt;a href="https://www.feetechrc.com/software.html"&gt;Feetech Software&lt;/a&gt;. For Ubuntu, you can use &lt;a href="https://github.com/Kotakku/FT_SCServo_Debug_Qt"&gt;FT_SCServo_Debug_Qt&lt;/a&gt;. Note: This step is not necessary as motors can be configured using the LeRobot Library, but this can be helpful for debugging.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>jj-vcs/jj</title>
      <link>https://github.com/jj-vcs/jj</link>
      <description>&lt;p&gt;A Git-compatible VCS that is both simple and powerful&lt;/p&gt;&lt;hr&gt;&lt;div class="title-block" style="text-align: center;" align="center"&gt; 
 &lt;h1&gt;Jujutsu‚Äîa version control system&lt;/h1&gt; 
 &lt;p&gt;&lt;img title="jj logo" src="https://raw.githubusercontent.com/jj-vcs/jj/main/docs/images/jj-logo.svg?sanitize=true" width="320" height="320" /&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/jj-vcs/jj/releases"&gt;&lt;img src="https://img.shields.io/github/v/release/martinvonz/jj" alt="Release" /&gt;&lt;/a&gt; &lt;a href="https://github.com/jj-vcs/jj/releases"&gt;&lt;img src="https://img.shields.io/github/release-date/martinvonz/jj" alt="Release date" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;a href="https://github.com/jj-vcs/jj/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/martinvonz/jj" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/dkmfj3aGQN"&gt;&lt;img src="https://img.shields.io/discord/968932220549103686.svg?label=&amp;amp;logo=discord&amp;amp;logoColor=ffffff&amp;amp;color=7389D8&amp;amp;labelColor=6A7EC2" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://web.libera.chat/?channel=#jujutsu"&gt;&lt;img src="https://img.shields.io/badge/irc-%23jujutsu-blue.svg?sanitize=true" alt="IRC" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;&lt;a href="https://jj-vcs.github.io/jj"&gt;Homepage&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚Ä¢&amp;nbsp;&amp;nbsp;&lt;/strong&gt; &lt;strong&gt;&lt;a href="https://jj-vcs.github.io/jj/latest/install-and-setup"&gt;Installation&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚Ä¢&amp;nbsp;&amp;nbsp;&lt;/strong&gt; &lt;strong&gt;&lt;a href="https://jj-vcs.github.io/jj/latest/tutorial"&gt;Getting Started&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚Ä¢&amp;nbsp;&amp;nbsp;&lt;/strong&gt; &lt;strong&gt;&lt;a href="https://jj-vcs.github.io/jj/latest/roadmap"&gt;Development Roadmap&lt;/a&gt; &amp;nbsp;&amp;nbsp;‚Ä¢&amp;nbsp;&amp;nbsp;&lt;/strong&gt; &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/jj-vcs/jj/main/#contributing"&gt;Contributing&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;Jujutsu is a powerful &lt;a href="https://en.wikipedia.org/wiki/Version_control"&gt;version control system&lt;/a&gt; for software projects. You use it to get a copy of your code, track changes to the code, and finally publish those changes for others to see and use. It is designed from the ground up to be easy to use‚Äîwhether you're new or experienced, working on brand new projects alone, or large scale software projects with large histories and teams.&lt;/p&gt; 
&lt;p&gt;Jujutsu is unlike most other systems, because internally it abstracts the user interface and version control algorithms from the &lt;em&gt;storage systems&lt;/em&gt; used to serve your content. This allows it to serve as a VCS with many possible physical backends, that may have their own data or networking models‚Äîlike &lt;a href="https://www.mercurial-scm.org/"&gt;Mercurial&lt;/a&gt; or &lt;a href="https://www.breezy-vcs.org/"&gt;Breezy&lt;/a&gt;, or hybrid systems like Google's cloud-based design, &lt;a href="https://youtu.be/W71BTkUbdqE?t=645"&gt;Piper/CitC&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Today, we use Git repositories as a storage layer to serve and track content, making it &lt;strong&gt;compatible with many of your favorite Git-based tools, right now!&lt;/strong&gt; All core developers use Jujutsu to develop Jujutsu, right here on GitHub. But it should hopefully work with your favorite Git forges, too.&lt;/p&gt; 
&lt;p&gt;We combine many distinct design choices and concepts from other version control systems into a single tool. Some of those sources of inspiration include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Git&lt;/strong&gt;: We make an effort to &lt;a href="https://github.com/jj-vcs/jj/discussions/49"&gt;be fast&lt;/a&gt;‚Äîwith a snappy UX, efficient algorithms, correct data structures, and good-old-fashioned attention to detail. The default storage backend uses Git repositories for "physical storage", for wide interoperability and ease of onboarding.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Mercurial &amp;amp; Sapling&lt;/strong&gt;: There are many Mercurial-inspired features, such as the &lt;a href="https://jj-vcs.github.io/jj/latest/revsets/"&gt;revset&lt;/a&gt; language to select commits. There is &lt;a href="https://jj-vcs.github.io/jj/latest/git-comparison/#the-index"&gt;no explicit index&lt;/a&gt; or staging area. Branches are "anonymous" like Mercurial, so you don't need to make up a name for each small change. Primitives for rewriting history are powerful and simple. Formatting output is done with a robust template language that can be configured by the user.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Darcs&lt;/strong&gt;: Jujutsu keeps track of conflicts as &lt;a href="https://jj-vcs.github.io/jj/latest/conflicts/"&gt;first-class objects&lt;/a&gt; in its model; they are first-class in the same way commits are, while alternatives like Git simply think of conflicts as textual diffs. While not as rigorous as systems like Darcs (which is based on a formalized theory of patches, as opposed to snapshots), the effect is that many forms of conflict resolution can be performed and propagated automatically.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;And it adds several innovative, useful features of its own:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Working-copy-as-a-commit&lt;/strong&gt;: Changes to files are &lt;a href="https://jj-vcs.github.io/jj/latest/working-copy/"&gt;recorded automatically&lt;/a&gt; as normal commits, and amended on every subsequent change. This "snapshot" design simplifies the user-facing data model (commits are the only visible object), simplifies internal algorithms, and completely subsumes features like Git's stashes or the index/staging-area.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Operation log &amp;amp; undo&lt;/strong&gt;: Jujutsu records every operation that is performed on the repository, from commits, to pulls, to pushes. This makes debugging problems like "what just happened?" or "how did I end up here?" easier, &lt;em&gt;especially&lt;/em&gt; when you're helping your coworker answer those questions about their repository! And because everything is recorded, you can undo that mistake you just made with ease. Version control has finally entered &lt;a href="https://en.wikipedia.org/wiki/Undo#History"&gt;the 1960s&lt;/a&gt;!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Automatic rebase and conflict resolution&lt;/strong&gt;: When you modify a commit, every descendent is automatically rebased on top of the freshly-modified one. This makes "patch-based" workflows a breeze. If you resolve a conflict in a commit, the &lt;em&gt;resolution&lt;/em&gt; of that conflict is also propagated through descendants as well. In effect, this is a completely transparent version of &lt;code&gt;git rebase --update-refs&lt;/code&gt; combined with &lt;code&gt;git rerere&lt;/code&gt;, supported by design.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING] The following features are available for use, but experimental; they may have bugs, backwards incompatible storage changes, and user-interface changes!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Safe, concurrent replication&lt;/strong&gt;: Have you ever wanted to store your version controlled repositories inside a Dropbox folder? Or continuously backup repositories to S3? No? Well, now you can!&lt;/p&gt; &lt;p&gt;The fundamental problem with using filesystems like Dropbox and backup tools like &lt;code&gt;rsync&lt;/code&gt; on your typical Git/Mercurial repositories is that they rely on &lt;em&gt;local filesystem operations&lt;/em&gt; being atomic, serialized, and non-concurrent with respect to other reads and writes‚Äîwhich is &lt;em&gt;not&lt;/em&gt; true when operating on distributed file systems, or when operations like concurrent file copies (for backup) happen while lock files are being held.&lt;/p&gt; &lt;p&gt;Jujutsu is instead designed to be &lt;a href="https://jj-vcs.github.io/jj/latest/technical/concurrency/"&gt;safe under concurrent scenarios&lt;/a&gt;; simply using rsync or Dropbox and then using that resulting repository should never result in a repository in a &lt;em&gt;corrupt state&lt;/em&gt;. The worst that &lt;em&gt;should&lt;/em&gt; happen is that it will expose conflicts between the local and remote state, leaving you to resolve them.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The command-line tool is called &lt;code&gt;jj&lt;/code&gt; for now because it's easy to type and easy to replace (rare in English). The project is called "Jujutsu" because it matches "jj".&lt;/p&gt; 
&lt;p&gt;Jujutsu is relatively young, with lots of work to still be done. If you have any questions, or want to talk about future plans, please join us on Discord &lt;a href="https://discord.gg/dkmfj3aGQN"&gt;&lt;img src="https://img.shields.io/discord/968932220549103686.svg?label=&amp;amp;logo=discord&amp;amp;logoColor=ffffff&amp;amp;color=7389D8&amp;amp;labelColor=6A7EC2" alt="Discord" /&gt;&lt;/a&gt;, start a &lt;a href="https://github.com/jj-vcs/jj/discussions"&gt;GitHub Discussion&lt;/a&gt;, or send an IRC message to &lt;a href="https://web.libera.chat/?channel=#jujutsu"&gt;&lt;code&gt;#jujutsu&lt;/code&gt; on Libera Chat&lt;/a&gt;. The developers monitor all of these channels[^bridge].&lt;/p&gt; 
&lt;p&gt;[^bridge]: To be more precise, the &lt;code&gt;#jujutsu&lt;/code&gt; Libera IRC channel is bridged to one of the channels on jj's Discord. Some of the developers stay on Discord and use the bridge to follow IRC.&lt;/p&gt; 
&lt;h3&gt;News and Updates üì£&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;December 2024&lt;/strong&gt;: The &lt;code&gt;jj&lt;/code&gt; Repository has moved to the &lt;code&gt;jj-vcs&lt;/code&gt; GitHub organization.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;November 2024&lt;/strong&gt;: Version 0.24 is released which adds &lt;code&gt;jj file annotate&lt;/code&gt;, which is equivalent to &lt;code&gt;git blame&lt;/code&gt; or &lt;code&gt;hg annotate&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;September 2024&lt;/strong&gt;: Martin gave a &lt;a href="https://www.youtube.com/watch?v=LV0JzI8IcCY"&gt;presentation about Jujutsu&lt;/a&gt; at Git Merge 2024.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Feb 2024&lt;/strong&gt;: Version 0.14 is released, which deprecates &lt;a href="https://raw.githubusercontent.com/jj-vcs/jj/main/CHANGELOG.md#0140---2024-02-07"&gt;"jj checkout" and "jj merge"&lt;/a&gt;, as well as &lt;code&gt;jj init --git&lt;/code&gt;, which is now just called &lt;code&gt;jj git init&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Oct 2023&lt;/strong&gt;: Version 0.10.0 is released! Now includes a bundled merge and diff editor for all platforms, "immutable revsets" to avoid accidentally &lt;code&gt;edit&lt;/code&gt;-ing the wrong revisions, and lots of polish.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Jan 2023&lt;/strong&gt;: Martin gave a presentation about Google's plans for Jujutsu at Git Merge 2022! See the &lt;a href="https://docs.google.com/presentation/d/1F8j9_UOOSGUN9MvHxPZX_L4bQ9NMcYOp1isn17kTC_M/view"&gt;slides&lt;/a&gt; or the &lt;a href="https://www.youtube.com/watch?v=bx_LGilOuE4"&gt;recording&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Related Media&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Mar 2024&lt;/strong&gt;: Chris Krycho started &lt;a href="https://www.youtube.com/playlist?list=PLelyiwKWHHAq01Pvmpf6x7J0y-yQpmtxp"&gt;a YouTube series about Jujutsu&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Feb 2024&lt;/strong&gt;: Chris Krycho published an article about Jujutsu called &lt;a href="https://v5.chriskrycho.com/essays/jj-init/"&gt;jj init&lt;/a&gt; and Steve Klabnik followed up with the &lt;a href="https://steveklabnik.github.io/jujutsu-tutorial/"&gt;Jujutsu Tutorial&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Jan 2024&lt;/strong&gt;: Jujutsu was featured in an LWN.net article called &lt;a href="https://lwn.net/Articles/958468/"&gt;Jujutsu: a new, Git-compatible version control system&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Jan 2023&lt;/strong&gt;: Martin's Talk about Jujutsu at Git Merge 2022, &lt;a href="https://www.youtube.com/watch?v=bx_LGilOuE4"&gt;video&lt;/a&gt; and the associated &lt;a href="https://docs.google.com/presentation/d/1F8j9_UOOSGUN9MvHxPZX_L4bQ9NMcYOp1isn17kTC_M/view"&gt;slides&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The wiki also contains a more extensive list of &lt;a href="https://github.com/jj-vcs/jj/wiki/Media"&gt;media references&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Getting started&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] Jujutsu is an &lt;strong&gt;experimental version control system&lt;/strong&gt;. While Git compatibility is stable, and most developers use it daily for all their needs, there may still be work-in-progress features, suboptimal UX, and workflow gaps that make it unusable for your particular use.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Follow the &lt;a href="https://jj-vcs.github.io/jj/latest/install-and-setup"&gt;installation instructions&lt;/a&gt; to obtain and configure &lt;code&gt;jj&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;The best way to get started is probably to go through &lt;a href="https://jj-vcs.github.io/jj/latest/tutorial"&gt;the tutorial&lt;/a&gt;. Also see the &lt;a href="https://jj-vcs.github.io/jj/latest/git-comparison"&gt;Git comparison&lt;/a&gt;, which includes a table of &lt;code&gt;jj&lt;/code&gt; vs. &lt;code&gt;git&lt;/code&gt; commands.&lt;/p&gt; 
&lt;p&gt;As you become more familiar with Jujutsu, the following resources may be helpful:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;The &lt;a href="https://jj-vcs.github.io/jj/latest/FAQ"&gt;FAQ&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;The &lt;a href="https://jj-vcs.github.io/jj/latest/glossary"&gt;Glossary&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;The &lt;code&gt;jj help&lt;/code&gt; command (e.g. &lt;code&gt;jj help rebase&lt;/code&gt;).&lt;/li&gt; 
 &lt;li&gt;The &lt;code&gt;jj help -k &amp;lt;keyword&amp;gt;&lt;/code&gt; command (e.g. &lt;code&gt;jj help -k config&lt;/code&gt;). Use &lt;code&gt;jj help --help&lt;/code&gt; to see what keywords are available.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If you are using a &lt;strong&gt;prerelease&lt;/strong&gt; version of &lt;code&gt;jj&lt;/code&gt;, you would want to consult &lt;a href="https://jj-vcs.github.io/jj/prerelease/"&gt;the docs for the prerelease (main branch) version&lt;/a&gt;. You can also get there from the docs for the latest release by using the website's version switcher. The version switcher is visible in the header of the website when you scroll to the top of any page.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;h3&gt;Compatible with Git&lt;/h3&gt; 
&lt;p&gt;Jujutsu is designed so that the underlying data and storage model is abstract. Today, only the Git backend is production-ready. The Git backend uses the &lt;a href="https://github.com/Byron/gitoxide"&gt;gitoxide&lt;/a&gt; Rust library.&lt;/p&gt; 
&lt;p&gt;The Git backend is fully featured and maintained, and allows you to use Jujutsu with any Git remote. The commits you create will look like regular Git commits. You can fetch branches from a regular Git remote and push branches to the remote. You can always switch back to Git.&lt;/p&gt; 
&lt;p&gt;Here is how you can explore a GitHub repository with &lt;code&gt;jj&lt;/code&gt;.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/jj-vcs/jj/main/demos/git_compat.png" /&gt; 
&lt;p&gt;You can even have a &lt;a href="https://jj-vcs.github.io/jj/latest/git-compatibility#colocated-jujutsugit-repos"&gt;colocated local repository&lt;/a&gt; where you can use both &lt;code&gt;jj&lt;/code&gt; and &lt;code&gt;git&lt;/code&gt; commands interchangeably.&lt;/p&gt; 
&lt;h3&gt;The working copy is automatically committed&lt;/h3&gt; 
&lt;p&gt;Jujutsu uses a real commit to represent the working copy. Checking out a commit results a new working-copy commit on top of the target commit. Almost all commands automatically amend the working-copy commit.&lt;/p&gt; 
&lt;p&gt;The working-copy being a commit means that commands never fail because the working copy is dirty (no "error: Your local changes to the following files..."), and there is no need for &lt;code&gt;git stash&lt;/code&gt;. Also, because the working copy is a commit, commands work the same way on the working-copy commit as on any other commit, so you can set the commit message before you're done with the changes.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/jj-vcs/jj/main/demos/working_copy.png" /&gt; 
&lt;h3&gt;The repo is the source of truth&lt;/h3&gt; 
&lt;p&gt;With Jujutsu, the working copy plays a smaller role than with Git. Commands snapshot the working copy before they start, then they update the repo, and then the working copy is updated (if the working-copy commit was modified). Almost all commands (even checkout!) operate on the commits in the repo, leaving the common functionality of snapshotting and updating of the working copy to centralized code. For example, &lt;code&gt;jj restore&lt;/code&gt; (similar to &lt;code&gt;git restore&lt;/code&gt;) can restore from any commit and into any commit, and &lt;code&gt;jj describe&lt;/code&gt; can set the commit message of any commit (defaults to the working-copy commit).&lt;/p&gt; 
&lt;h3&gt;Entire repo is under version control&lt;/h3&gt; 
&lt;p&gt;All operations you perform in the repo are recorded, along with a snapshot of the repo state after the operation. This means that you can easily restore to an earlier repo state, simply undo your operations one-by-one or even &lt;em&gt;revert&lt;/em&gt; a particular operation which does not have to be the most recent one.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/jj-vcs/jj/main/demos/operation_log.png" /&gt; 
&lt;h3&gt;Conflicts can be recorded in commits&lt;/h3&gt; 
&lt;p&gt;If an operation results in &lt;a href="https://jj-vcs.github.io/jj/latest/glossary#conflict"&gt;conflicts&lt;/a&gt;, information about those conflicts will be recorded in the commit(s). The operation will succeed. You can then resolve the conflicts later. One consequence of this design is that there's no need to continue interrupted operations. Instead, you get a single workflow for resolving conflicts, regardless of which command caused them. This design also lets Jujutsu rebase merge commits correctly (unlike both Git and Mercurial).&lt;/p&gt; 
&lt;p&gt;Basic conflict resolution:&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/jj-vcs/jj/main/demos/resolve_conflicts.png" /&gt; 
&lt;p&gt;Juggling conflicts:&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/jj-vcs/jj/main/demos/juggle_conflicts.png" /&gt; 
&lt;h3&gt;Automatic rebase&lt;/h3&gt; 
&lt;p&gt;Whenever you modify a commit, any descendants of the old commit will be rebased onto the new commit. Thanks to the conflict design described above, that can be done even if there are conflicts. Bookmarks pointing to rebased commits will be updated. So will the working copy if it points to a rebased commit.&lt;/p&gt; 
&lt;h3&gt;Comprehensive support for rewriting history&lt;/h3&gt; 
&lt;p&gt;Besides the usual rebase command, there's &lt;code&gt;jj describe&lt;/code&gt; for editing the description (commit message) of an arbitrary commit. There's also &lt;code&gt;jj diffedit&lt;/code&gt;, which lets you edit the changes in a commit without checking it out. To split a commit into two, use &lt;code&gt;jj split&lt;/code&gt;. You can even move part of the changes in a commit to any other commit using &lt;code&gt;jj squash -i --from X --into Y&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Status&lt;/h2&gt; 
&lt;p&gt;The tool is fairly feature-complete, but some important features like support for Git submodules are not yet completed. There are also several performance bugs. It's likely that workflows and setups different from what the core developers use are not well supported, e.g. there is no native support for email-based workflows.&lt;/p&gt; 
&lt;p&gt;Today, all core developers use &lt;code&gt;jj&lt;/code&gt; to work on &lt;code&gt;jj&lt;/code&gt;. I (Martin von Zweigbergk) have almost exclusively used &lt;code&gt;jj&lt;/code&gt; to develop the project itself since early January 2021. I haven't had to re-clone from source (I don't think I've even had to restore from backup).&lt;/p&gt; 
&lt;p&gt;There &lt;em&gt;will&lt;/em&gt; be changes to workflows and backward-incompatible changes to the on-disk formats before version 1.0.0. For any format changes, we'll try to implement transparent upgrades (as we've done with recent changes), or provide upgrade commands or scripts if requested.&lt;/p&gt; 
&lt;h2&gt;Related work&lt;/h2&gt; 
&lt;p&gt;There are several tools trying to solve similar problems as Jujutsu. See &lt;a href="https://jj-vcs.github.io/jj/latest/related-work"&gt;related work&lt;/a&gt; for details.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome outside contributions, and there's plenty of things to do, so don't be shy. Please ask if you want a pointer on something you can help with, and hopefully we can all figure something out.&lt;/p&gt; 
&lt;p&gt;We do have &lt;a href="https://jj-vcs.github.io/jj/prerelease/contributing/"&gt;a few policies and suggestions&lt;/a&gt; for contributors. The broad TL;DR:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Bug reports are very welcome!&lt;/li&gt; 
 &lt;li&gt;Every commit that lands in the &lt;code&gt;main&lt;/code&gt; branch is code reviewed.&lt;/li&gt; 
 &lt;li&gt;Please behave yourself, and obey the Community Guidelines.&lt;/li&gt; 
 &lt;li&gt;There &lt;strong&gt;is&lt;/strong&gt; a mandatory CLA you must agree to. Importantly, it &lt;strong&gt;does not&lt;/strong&gt; transfer copyright ownership to Google or anyone else; it simply gives us the right to safely redistribute and use your changes.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Mandatory Google Disclaimer&lt;/h3&gt; 
&lt;p&gt;I (Martin von Zweigbergk, &lt;a href="mailto:martinvonz@google.com"&gt;martinvonz@google.com&lt;/a&gt;) started Jujutsu as a hobby project in late 2019, and it has evolved into my full-time project at Google, with several other Googlers (now) assisting development in various capacities. That said, &lt;strong&gt;this is not a Google product&lt;/strong&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Jujutsu is available as Open Source Software, under the Apache 2.0 license. See &lt;a href="https://raw.githubusercontent.com/jj-vcs/jj/main/LICENSE"&gt;&lt;code&gt;LICENSE&lt;/code&gt;&lt;/a&gt; for details about copyright and redistribution.&lt;/p&gt; 
&lt;p&gt;The &lt;code&gt;jj&lt;/code&gt; logo was contributed by J. Jennings and is licensed under a Creative Commons License, see &lt;a href="https://raw.githubusercontent.com/jj-vcs/jj/main/docs/images/LICENSE"&gt;&lt;code&gt;docs/images/LICENSE&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>