<rss version="2.0">
  <channel>
    <title>GitHub Python Daily Trending</title>
    <description>Daily Trending of Python in GitHub</description>
    <pubDate>Fri, 08 Aug 2025 01:35:07 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>openai/tiktoken</title>
      <link>https://github.com/openai/tiktoken</link>
      <description>&lt;p&gt;tiktoken is a fast BPE tokeniser for use with OpenAI's models.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;‚è≥ tiktoken&lt;/h1&gt; 
&lt;p&gt;tiktoken is a fast &lt;a href="https://en.wikipedia.org/wiki/Byte_pair_encoding"&gt;BPE&lt;/a&gt; tokeniser for use with OpenAI's models.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import tiktoken
enc = tiktoken.get_encoding("o200k_base")
assert enc.decode(enc.encode("hello world")) == "hello world"

# To get the tokeniser corresponding to a specific model in the OpenAI API:
enc = tiktoken.encoding_for_model("gpt-4o")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The open source version of &lt;code&gt;tiktoken&lt;/code&gt; can be installed from &lt;a href="https://pypi.org/project/tiktoken"&gt;PyPI&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install tiktoken
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The tokeniser API is documented in &lt;code&gt;tiktoken/core.py&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Example code using &lt;code&gt;tiktoken&lt;/code&gt; can be found in the &lt;a href="https://github.com/openai/openai-cookbook/raw/main/examples/How_to_count_tokens_with_tiktoken.ipynb"&gt;OpenAI Cookbook&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Performance&lt;/h2&gt; 
&lt;p&gt;&lt;code&gt;tiktoken&lt;/code&gt; is between 3-6x faster than a comparable open source tokeniser:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/openai/tiktoken/main/perf.svg?sanitize=true" alt="image"&gt;&lt;/p&gt; 
&lt;p&gt;Performance measured on 1GB of text using the GPT-2 tokeniser, using &lt;code&gt;GPT2TokenizerFast&lt;/code&gt; from &lt;code&gt;tokenizers==0.13.2&lt;/code&gt;, &lt;code&gt;transformers==4.24.0&lt;/code&gt; and &lt;code&gt;tiktoken==0.2.0&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Getting help&lt;/h2&gt; 
&lt;p&gt;Please post questions in the &lt;a href="https://github.com/openai/tiktoken/issues"&gt;issue tracker&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you work at OpenAI, make sure to check the internal documentation or feel free to contact @shantanu.&lt;/p&gt; 
&lt;h2&gt;What is BPE anyway?&lt;/h2&gt; 
&lt;p&gt;Language models don't see text like you and I, instead they see a sequence of numbers (known as tokens). Byte pair encoding (BPE) is a way of converting text into tokens. It has a couple desirable properties:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;It's reversible and lossless, so you can convert tokens back into the original text&lt;/li&gt; 
 &lt;li&gt;It works on arbitrary text, even text that is not in the tokeniser's training data&lt;/li&gt; 
 &lt;li&gt;It compresses the text: the token sequence is shorter than the bytes corresponding to the original text. On average, in practice, each token corresponds to about 4 bytes.&lt;/li&gt; 
 &lt;li&gt;It attempts to let the model see common subwords. For instance, "ing" is a common subword in English, so BPE encodings will often split "encoding" into tokens like "encod" and "ing" (instead of e.g. "enc" and "oding"). Because the model will then see the "ing" token again and again in different contexts, it helps models generalise and better understand grammar.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;code&gt;tiktoken&lt;/code&gt; contains an educational submodule that is friendlier if you want to learn more about the details of BPE, including code that helps visualise the BPE procedure:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from tiktoken._educational import *

# Train a BPE tokeniser on a small amount of text
enc = train_simple_encoding()

# Visualise how the GPT-4 encoder encodes text
enc = SimpleBytePairEncoding.from_tiktoken("cl100k_base")
enc.encode("hello world aaaaaaaaaaaa")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Extending tiktoken&lt;/h2&gt; 
&lt;p&gt;You may wish to extend &lt;code&gt;tiktoken&lt;/code&gt; to support new encodings. There are two ways to do this.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Create your &lt;code&gt;Encoding&lt;/code&gt; object exactly the way you want and simply pass it around.&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;cl100k_base = tiktoken.get_encoding("cl100k_base")

# In production, load the arguments directly instead of accessing private attributes
# See openai_public.py for examples of arguments for specific encodings
enc = tiktoken.Encoding(
    # If you're changing the set of special tokens, make sure to use a different name
    # It should be clear from the name what behaviour to expect.
    name="cl100k_im",
    pat_str=cl100k_base._pat_str,
    mergeable_ranks=cl100k_base._mergeable_ranks,
    special_tokens={
        **cl100k_base._special_tokens,
        "&amp;lt;|im_start|&amp;gt;": 100264,
        "&amp;lt;|im_end|&amp;gt;": 100265,
    }
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Use the &lt;code&gt;tiktoken_ext&lt;/code&gt; plugin mechanism to register your &lt;code&gt;Encoding&lt;/code&gt; objects with &lt;code&gt;tiktoken&lt;/code&gt;.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;This is only useful if you need &lt;code&gt;tiktoken.get_encoding&lt;/code&gt; to find your encoding, otherwise prefer option 1.&lt;/p&gt; 
&lt;p&gt;To do this, you'll need to create a namespace package under &lt;code&gt;tiktoken_ext&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Layout your project like this, making sure to omit the &lt;code&gt;tiktoken_ext/__init__.py&lt;/code&gt; file:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;my_tiktoken_extension
‚îú‚îÄ‚îÄ tiktoken_ext
‚îÇ&amp;nbsp;&amp;nbsp; ‚îî‚îÄ‚îÄ my_encodings.py
‚îî‚îÄ‚îÄ setup.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;code&gt;my_encodings.py&lt;/code&gt; should be a module that contains a variable named &lt;code&gt;ENCODING_CONSTRUCTORS&lt;/code&gt;. This is a dictionary from an encoding name to a function that takes no arguments and returns arguments that can be passed to &lt;code&gt;tiktoken.Encoding&lt;/code&gt; to construct that encoding. For an example, see &lt;code&gt;tiktoken_ext/openai_public.py&lt;/code&gt;. For precise details, see &lt;code&gt;tiktoken/registry.py&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Your &lt;code&gt;setup.py&lt;/code&gt; should look something like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from setuptools import setup, find_namespace_packages

setup(
    name="my_tiktoken_extension",
    packages=find_namespace_packages(include=['tiktoken_ext*']),
    install_requires=["tiktoken"],
    ...
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then simply &lt;code&gt;pip install ./my_tiktoken_extension&lt;/code&gt; and you should be able to use your custom encodings! Make sure &lt;strong&gt;not&lt;/strong&gt; to use an editable install.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>jumpserver/jumpserver</title>
      <link>https://github.com/jumpserver/jumpserver</link>
      <description>&lt;p&gt;JumpServer is an open-source Privileged Access Management (PAM) tool that provides DevOps and IT teams with on-demand and secure access to SSH, RDP, Kubernetes, Database and RemoteApp endpoints through a web browser.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;a name="readme-top"&gt;&lt;/a&gt; 
 &lt;a href="https://jumpserver.com" target="_blank"&gt;&lt;img src="https://download.jumpserver.org/images/jumpserver-logo.svg?sanitize=true" alt="JumpServer" width="300"&gt;&lt;/a&gt; 
 &lt;h2&gt;An open-source PAM tool (Bastion Host)&lt;/h2&gt; 
 &lt;p&gt;&lt;a href="https://www.gnu.org/licenses/gpl-3.0.html"&gt;&lt;img src="https://img.shields.io/github/license/jumpserver/jumpserver" alt=""&gt;&lt;/a&gt; &lt;a href="https://jumpserver.com/docs"&gt;&lt;img src="https://img.shields.io/badge/documentation-148F76" alt=""&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/jumpserver/jumpserver/"&gt;&lt;img src="https://img.shields.io/badge/deepwiki-devin?color=blue" alt=""&gt;&lt;/a&gt; &lt;a href="https://discord.com/invite/W6vYXmAQG2"&gt;&lt;img src="https://img.shields.io/discord/1194233267294052363?style=flat&amp;amp;logo=discord&amp;amp;logoColor=%23f5f5f5&amp;amp;labelColor=%235462eb&amp;amp;color=%235462eb" alt=""&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/u/jumpserver"&gt;&lt;img src="https://img.shields.io/docker/pulls/jumpserver/jms_all.svg?sanitize=true" alt=""&gt;&lt;/a&gt; &lt;a href="https://github.com/jumpserver/jumpserver/releases/latest"&gt;&lt;img src="https://img.shields.io/github/v/release/jumpserver/jumpserver" alt=""&gt;&lt;/a&gt; &lt;a href="https://github.com/jumpserver/jumpserver"&gt;&lt;img src="https://img.shields.io/github/stars/jumpserver/jumpserver?color=%231890FF&amp;amp;style=flat-square%C2%A0%C2%A0%C2%A0" alt=""&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/jumpserver/jumpserver/dev/README.md"&gt;English&lt;/a&gt; ¬∑ &lt;a href="https://raw.githubusercontent.com/jumpserver/jumpserver/dev/readmes/README.zh-hans.md"&gt;‰∏≠Êñá(ÁÆÄ‰Ωì)&lt;/a&gt; ¬∑ &lt;a href="https://raw.githubusercontent.com/jumpserver/jumpserver/dev/readmes/README.zh-hant.md"&gt;‰∏≠Êñá(ÁπÅÈ´î)&lt;/a&gt; ¬∑ &lt;a href="https://raw.githubusercontent.com/jumpserver/jumpserver/dev/readmes/README.ja.md"&gt;Êó•Êú¨Ë™û&lt;/a&gt; ¬∑ &lt;a href="https://raw.githubusercontent.com/jumpserver/jumpserver/dev/readmes/README.pt-br.md"&gt;Portugu√™s (Brasil)&lt;/a&gt; ¬∑ &lt;a href="https://raw.githubusercontent.com/jumpserver/jumpserver/dev/readmes/README.es.md"&gt;Espa√±ol&lt;/a&gt; ¬∑ &lt;a href="https://raw.githubusercontent.com/jumpserver/jumpserver/dev/readmes/README.ru.md"&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt; ¬∑ &lt;a href="https://raw.githubusercontent.com/jumpserver/jumpserver/dev/readmes/README.ko.md"&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;br&gt; 
&lt;h2&gt;What is JumpServer?&lt;/h2&gt; 
&lt;p&gt;JumpServer is an open-source Privileged Access Management (PAM) tool that provides DevOps and IT teams with on-demand and secure access to SSH, RDP, Kubernetes, Database and RemoteApp endpoints through a web browser.&lt;/p&gt; 
&lt;picture&gt; 
 &lt;source media="(prefers-color-scheme: light)" srcset="https://www.jumpserver.com/images/jumpserver-arch-light.png"&gt; 
 &lt;source media="(prefers-color-scheme: dark)" srcset="https://www.jumpserver.com/images/jumpserver-arch-dark.png"&gt; 
 &lt;img src="https://github.com/user-attachments/assets/dd612f3d-c958-4f84-b164-f31b75454d7f" alt="Theme-based Image"&gt; 
&lt;/picture&gt; 
&lt;h2&gt;Quickstart&lt;/h2&gt; 
&lt;p&gt;Prepare a clean Linux Server ( 64 bit, &amp;gt;= 4c8g )&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;curl -sSL https://github.com/jumpserver/jumpserver/releases/latest/download/quick_start.sh | bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Access JumpServer in your browser at &lt;code&gt;http://your-jumpserver-ip/&lt;/code&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Username: &lt;code&gt;admin&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Password: &lt;code&gt;ChangeMe&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=UlGYRbKrpgY" title="JumpServer Quickstart"&gt;&lt;img src="https://github.com/user-attachments/assets/0f32f52b-9935-485e-8534-336c63389612" alt="JumpServer Quickstart"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Screenshots&lt;/h2&gt; 
&lt;table style="border-collapse: collapse; border: 1px solid black;"&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td style="padding: 5px;background-color:#fff;"&gt;&lt;img src="https://github.com/jumpserver/jumpserver/assets/32935519/99fabe5b-0475-4a53-9116-4c370a1426c4" alt="JumpServer Console"&gt;&lt;/td&gt; 
   &lt;td style="padding: 5px;background-color:#fff;"&gt;&lt;img src="https://github.com/user-attachments/assets/7c1f81af-37e8-4f07-8ac9-182895e1062e" alt="JumpServer PAM"&gt;&lt;/td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td style="padding: 5px;background-color:#fff;"&gt;&lt;img src="https://github.com/jumpserver/jumpserver/assets/32935519/a424d731-1c70-4108-a7d8-5bbf387dda9a" alt="JumpServer Audits"&gt;&lt;/td&gt; 
   &lt;td style="padding: 5px;background-color:#fff;"&gt;&lt;img src="https://github.com/jumpserver/jumpserver/assets/32935519/393d2c27-a2d0-4dea-882d-00ed509e00c9" alt="JumpServer Workbench"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td style="padding: 5px;background-color:#fff;"&gt;&lt;img src="https://github.com/user-attachments/assets/eaa41f66-8cc8-4f01-a001-0d258501f1c9" alt="JumpServer RBAC"&gt;&lt;/td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 
   &lt;td style="padding: 5px;background-color:#fff;"&gt;&lt;img src="https://github.com/jumpserver/jumpserver/assets/32935519/3a2611cd-8902-49b8-b82b-2a6dac851f3e" alt="JumpServer Settings"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td style="padding: 5px;background-color:#fff;"&gt;&lt;img src="https://github.com/jumpserver/jumpserver/assets/32935519/1e236093-31f7-4563-8eb1-e36d865f1568" alt="JumpServer SSH"&gt;&lt;/td&gt; 
   &lt;td style="padding: 5px;background-color:#fff;"&gt;&lt;img src="https://github.com/jumpserver/jumpserver/assets/32935519/69373a82-f7ab-41e8-b763-bbad2ba52167" alt="JumpServer RDP"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td style="padding: 5px;background-color:#fff;"&gt;&lt;img src="https://github.com/jumpserver/jumpserver/assets/32935519/5bed98c6-cbe8-4073-9597-d53c69dc3957" alt="JumpServer K8s"&gt;&lt;/td&gt; 
   &lt;td style="padding: 5px;background-color:#fff;"&gt;&lt;img src="https://github.com/jumpserver/jumpserver/assets/32935519/b80ad654-548f-42bc-ba3d-c1cfdf1b46d6" alt="JumpServer DB"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;Components&lt;/h2&gt; 
&lt;p&gt;JumpServer consists of multiple key components, which collectively form the functional framework of JumpServer, providing users with comprehensive capabilities for operations management and security control.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Project&lt;/th&gt; 
   &lt;th&gt;Status&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/jumpserver/lina"&gt;Lina&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/jumpserver/lina/releases"&gt;&lt;img alt="Lina release" src="https://img.shields.io/github/release/jumpserver/lina.svg?sanitize=true"&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;JumpServer Web UI&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/jumpserver/luna"&gt;Luna&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/jumpserver/luna/releases"&gt;&lt;img alt="Luna release" src="https://img.shields.io/github/release/jumpserver/luna.svg?sanitize=true"&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;JumpServer Web Terminal&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/jumpserver/koko"&gt;KoKo&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/jumpserver/koko/releases"&gt;&lt;img alt="Koko release" src="https://img.shields.io/github/release/jumpserver/koko.svg?sanitize=true"&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;JumpServer Character Protocol Connector&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/jumpserver/lion"&gt;Lion&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/jumpserver/lion/releases"&gt;&lt;img alt="Lion release" src="https://img.shields.io/github/release/jumpserver/lion.svg?sanitize=true"&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;JumpServer Graphical Protocol Connector&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/jumpserver/chen"&gt;Chen&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/jumpserver/chen/releases"&gt;&lt;img alt="Chen release" src="https://img.shields.io/github/release/jumpserver/chen.svg?sanitize=true"&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;JumpServer Web DB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/jumpserver/tinker"&gt;Tinker&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img alt="Tinker" src="https://img.shields.io/badge/release-private-red"&gt;&lt;/td&gt; 
   &lt;td&gt;JumpServer Remote Application Connector (Windows)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/jumpserver/Panda"&gt;Panda&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img alt="Panda" src="https://img.shields.io/badge/release-private-red"&gt;&lt;/td&gt; 
   &lt;td&gt;JumpServer EE Remote Application Connector (Linux)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/jumpserver/razor"&gt;Razor&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img alt="Chen" src="https://img.shields.io/badge/release-private-red"&gt;&lt;/td&gt; 
   &lt;td&gt;JumpServer EE RDP Proxy Connector&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/jumpserver/magnus"&gt;Magnus&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img alt="Magnus" src="https://img.shields.io/badge/release-private-red"&gt;&lt;/td&gt; 
   &lt;td&gt;JumpServer EE Database Proxy Connector&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/jumpserver/nec"&gt;Nec&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img alt="Nec" src="https://img.shields.io/badge/release-private-red"&gt;&lt;/td&gt; 
   &lt;td&gt;JumpServer EE VNC Proxy Connector&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/jumpserver/facelive"&gt;Facelive&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img alt="Facelive" src="https://img.shields.io/badge/release-private-red"&gt;&lt;/td&gt; 
   &lt;td&gt;JumpServer EE Facial Recognition&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Third-party projects&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/acerrah/jumpserver-grafana-dashboard"&gt;jumpserver-grafana-dashboard&lt;/a&gt; JumpServer with grafana dashboard&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Welcome to submit PR to contribute. Please refer to &lt;a href="https://github.com/jumpserver/jumpserver/raw/dev/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for guidelines.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Copyright (c) 2014-2025 FIT2CLOUD, All rights reserved.&lt;/p&gt; 
&lt;p&gt;Licensed under The GNU General Public License version 3 (GPLv3) (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.gnu.org/licenses/gpl-3.0.html"&gt;https://www.gnu.org/licenses/gpl-3.0.html&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an " AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.&lt;/p&gt; 
&lt;!-- JumpServer official link --&gt; 
&lt;!-- JumpServer Other link--&gt; 
&lt;!-- Shield link--&gt;</description>
    </item>
    
    <item>
      <title>reflex-dev/reflex</title>
      <link>https://github.com/reflex-dev/reflex</link>
      <description>&lt;p&gt;üï∏Ô∏è Web apps in pure Python üêç&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/reflex-dev/reflex/main/docs/images/reflex.svg?sanitize=true" alt="Reflex Logo" width="300px"&gt; 
 &lt;hr&gt; 
 &lt;h3&gt;&lt;strong&gt;‚ú® Performant, customizable web apps in pure Python. Deploy in seconds. ‚ú®&lt;/strong&gt;&lt;/h3&gt; 
 &lt;p&gt;&lt;a href="https://badge.fury.io/py/reflex"&gt;&lt;img src="https://badge.fury.io/py/reflex.svg?sanitize=true" alt="PyPI version"&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/pypi/pyversions/reflex.svg?sanitize=true" alt="versions"&gt; &lt;a href="https://reflex.dev/docs/getting-started/introduction"&gt;&lt;img src="https://img.shields.io/badge/Documentation%20-Introduction%20-%20%23007ec6" alt="Documentation"&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/projects/reflex"&gt;&lt;img src="https://static.pepy.tech/badge/reflex" alt="PyPI Downloads"&gt;&lt;/a&gt; &lt;a href="https://discord.gg/T5WSbC2YtQ"&gt;&lt;img src="https://img.shields.io/discord/1029853095527727165?color=%237289da&amp;amp;label=Discord" alt="Discord"&gt;&lt;/a&gt; &lt;a href="https://x.com/getreflex"&gt;&lt;img src="https://img.shields.io/twitter/follow/getreflex" alt="Twitter"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr&gt; 
&lt;p&gt;&lt;a href="https://github.com/reflex-dev/reflex/raw/main/README.md"&gt;English&lt;/a&gt; | &lt;a href="https://github.com/reflex-dev/reflex/raw/main/docs/zh/zh_cn/README.md"&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt; | &lt;a href="https://github.com/reflex-dev/reflex/raw/main/docs/zh/zh_tw/README.md"&gt;ÁπÅÈ´î‰∏≠Êñá&lt;/a&gt; | &lt;a href="https://github.com/reflex-dev/reflex/raw/main/docs/tr/README.md"&gt;T√ºrk√ße&lt;/a&gt; | &lt;a href="https://github.com/reflex-dev/reflex/raw/main/docs/in/README.md"&gt;‡§π‡§ø‡§Ç‡§¶‡•Ä&lt;/a&gt; | &lt;a href="https://github.com/reflex-dev/reflex/raw/main/docs/pt/pt_br/README.md"&gt;Portugu√™s (Brasil)&lt;/a&gt; | &lt;a href="https://github.com/reflex-dev/reflex/raw/main/docs/it/README.md"&gt;Italiano&lt;/a&gt; | &lt;a href="https://github.com/reflex-dev/reflex/raw/main/docs/es/README.md"&gt;Espa√±ol&lt;/a&gt; | &lt;a href="https://github.com/reflex-dev/reflex/raw/main/docs/kr/README.md"&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt; | &lt;a href="https://github.com/reflex-dev/reflex/raw/main/docs/ja/README.md"&gt;Êó•Êú¨Ë™û&lt;/a&gt; | &lt;a href="https://github.com/reflex-dev/reflex/raw/main/docs/de/README.md"&gt;Deutsch&lt;/a&gt; | &lt;a href="https://github.com/reflex-dev/reflex/raw/main/docs/pe/README.md"&gt;Persian (Ÿæÿßÿ±ÿ≥€å)&lt;/a&gt; | &lt;a href="https://github.com/reflex-dev/reflex/raw/main/docs/vi/README.md"&gt;Ti·∫øng Vi·ªát&lt;/a&gt;&lt;/p&gt; 
&lt;hr&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] üöÄ &lt;strong&gt;Try &lt;a href="https://build.reflex.dev/"&gt;Reflex Build&lt;/a&gt;&lt;/strong&gt; ‚Äì our AI-powered app builder that generates full-stack Reflex applications in seconds.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr&gt; 
&lt;h1&gt;Introduction&lt;/h1&gt; 
&lt;p&gt;Reflex is a library to build full-stack web apps in pure Python.&lt;/p&gt; 
&lt;p&gt;Key features:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Pure Python&lt;/strong&gt; - Write your app's frontend and backend all in Python, no need to learn Javascript.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Full Flexibility&lt;/strong&gt; - Reflex is easy to get started with, but can also scale to complex apps.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Deploy Instantly&lt;/strong&gt; - After building, deploy your app with a &lt;a href="https://reflex.dev/docs/hosting/deploy-quick-start/"&gt;single command&lt;/a&gt; or host it on your own server.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See our &lt;a href="https://reflex.dev/blog/2024-03-21-reflex-architecture/#the-reflex-architecture"&gt;architecture page&lt;/a&gt; to learn how Reflex works under the hood.&lt;/p&gt; 
&lt;h2&gt;‚öôÔ∏è Installation&lt;/h2&gt; 
&lt;p&gt;Open a terminal and run (Requires Python 3.10+):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install reflex
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ü•≥ Create your first app&lt;/h2&gt; 
&lt;p&gt;Installing &lt;code&gt;reflex&lt;/code&gt; also installs the &lt;code&gt;reflex&lt;/code&gt; command line tool.&lt;/p&gt; 
&lt;p&gt;Test that the install was successful by creating a new project. (Replace &lt;code&gt;my_app_name&lt;/code&gt; with your project name):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;mkdir my_app_name
cd my_app_name
reflex init
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This command initializes a template app in your new directory.&lt;/p&gt; 
&lt;p&gt;You can run this app in development mode:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;reflex run
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You should see your app running at &lt;a href="http://localhost:3000"&gt;http://localhost:3000&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Now you can modify the source code in &lt;code&gt;my_app_name/my_app_name.py&lt;/code&gt;. Reflex has fast refreshes so you can see your changes instantly when you save your code.&lt;/p&gt; 
&lt;h2&gt;ü´ß Example App&lt;/h2&gt; 
&lt;p&gt;Let's go over an example: creating an image generation UI around &lt;a href="https://platform.openai.com/docs/guides/images/image-generation?context=node"&gt;DALL¬∑E&lt;/a&gt;. For simplicity, we just call the &lt;a href="https://platform.openai.com/docs/api-reference/authentication"&gt;OpenAI API&lt;/a&gt;, but you could replace this with an ML model run locally.&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/reflex-dev/reflex/main/docs/images/dalle.gif" alt="A frontend wrapper for DALL¬∑E, shown in the process of generating an image." width="550"&gt; 
&lt;/div&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;Here is the complete code to create this. This is all done in one Python file!&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import reflex as rx
import openai

openai_client = openai.OpenAI()


class State(rx.State):
    """The app state."""

    prompt = ""
    image_url = ""
    processing = False
    complete = False

    def get_image(self):
        """Get the image from the prompt."""
        if self.prompt == "":
            return rx.window_alert("Prompt Empty")

        self.processing, self.complete = True, False
        yield
        response = openai_client.images.generate(
            prompt=self.prompt, n=1, size="1024x1024"
        )
        self.image_url = response.data[0].url
        self.processing, self.complete = False, True


def index():
    return rx.center(
        rx.vstack(
            rx.heading("DALL-E", font_size="1.5em"),
            rx.input(
                placeholder="Enter a prompt..",
                on_blur=State.set_prompt,
                width="25em",
            ),
            rx.button(
                "Generate Image",
                on_click=State.get_image,
                width="25em",
                loading=State.processing
            ),
            rx.cond(
                State.complete,
                rx.image(src=State.image_url, width="20em"),
            ),
            align="center",
        ),
        width="100%",
        height="100vh",
    )

# Add state and page to the app.
app = rx.App()
app.add_page(index, title="Reflex:DALL-E")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Let's break this down.&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/reflex-dev/reflex/main/docs/images/dalle_colored_code_example.png" alt="Explaining the differences between backend and frontend parts of the DALL-E app." width="900"&gt; 
&lt;/div&gt; 
&lt;h3&gt;&lt;strong&gt;Reflex UI&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;Let's start with the UI.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;def index():
    return rx.center(
        ...
    )
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This &lt;code&gt;index&lt;/code&gt; function defines the frontend of the app.&lt;/p&gt; 
&lt;p&gt;We use different components such as &lt;code&gt;center&lt;/code&gt;, &lt;code&gt;vstack&lt;/code&gt;, &lt;code&gt;input&lt;/code&gt;, and &lt;code&gt;button&lt;/code&gt; to build the frontend. Components can be nested within each other to create complex layouts. And you can use keyword args to style them with the full power of CSS.&lt;/p&gt; 
&lt;p&gt;Reflex comes with &lt;a href="https://reflex.dev/docs/library"&gt;60+ built-in components&lt;/a&gt; to help you get started. We are actively adding more components, and it's easy to &lt;a href="https://reflex.dev/docs/wrapping-react/overview/"&gt;create your own components&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;State&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;Reflex represents your UI as a function of your state.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;class State(rx.State):
    """The app state."""
    prompt = ""
    image_url = ""
    processing = False
    complete = False

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The state defines all the variables (called vars) in an app that can change and the functions that change them.&lt;/p&gt; 
&lt;p&gt;Here the state is comprised of a &lt;code&gt;prompt&lt;/code&gt; and &lt;code&gt;image_url&lt;/code&gt;. There are also the booleans &lt;code&gt;processing&lt;/code&gt; and &lt;code&gt;complete&lt;/code&gt; to indicate when to disable the button (during image generation) and when to show the resulting image.&lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;Event Handlers&lt;/strong&gt;&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;def get_image(self):
    """Get the image from the prompt."""
    if self.prompt == "":
        return rx.window_alert("Prompt Empty")

    self.processing, self.complete = True, False
    yield
    response = openai_client.images.generate(
        prompt=self.prompt, n=1, size="1024x1024"
    )
    self.image_url = response.data[0].url
    self.processing, self.complete = False, True
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Within the state, we define functions called event handlers that change the state vars. Event handlers are the way that we can modify the state in Reflex. They can be called in response to user actions, such as clicking a button or typing in a text box. These actions are called events.&lt;/p&gt; 
&lt;p&gt;Our DALL¬∑E app has an event handler, &lt;code&gt;get_image&lt;/code&gt; which gets this image from the OpenAI API. Using &lt;code&gt;yield&lt;/code&gt; in the middle of an event handler will cause the UI to update. Otherwise the UI will update at the end of the event handler.&lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;Routing&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;Finally, we define our app.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;app = rx.App()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;We add a page from the root of the app to the index component. We also add a title that will show up in the page preview/browser tab.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;app.add_page(index, title="DALL-E")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can create a multi-page app by adding more pages.&lt;/p&gt; 
&lt;h2&gt;üìë Resources&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;üìë &lt;a href="https://reflex.dev/docs/getting-started/introduction"&gt;Docs&lt;/a&gt; &amp;nbsp; | &amp;nbsp; üóûÔ∏è &lt;a href="https://reflex.dev/blog"&gt;Blog&lt;/a&gt; &amp;nbsp; | &amp;nbsp; üì± &lt;a href="https://reflex.dev/docs/library"&gt;Component Library&lt;/a&gt; &amp;nbsp; | &amp;nbsp; üñºÔ∏è &lt;a href="https://reflex.dev/templates/"&gt;Templates&lt;/a&gt; &amp;nbsp; | &amp;nbsp; üõ∏ &lt;a href="https://reflex.dev/docs/hosting/deploy-quick-start"&gt;Deployment&lt;/a&gt; &amp;nbsp;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;‚úÖ Status&lt;/h2&gt; 
&lt;p&gt;Reflex launched in December 2022 with the name Pynecone.&lt;/p&gt; 
&lt;p&gt;üöÄ Introducing &lt;a href="https://build.reflex.dev/"&gt;Reflex Build&lt;/a&gt; ‚Äî Our AI-Powered Builder Reflex Build uses AI to generate complete full-stack Python applications. It helps you quickly create, customize, and refine your Reflex apps ‚Äî from frontend components to backend logic ‚Äî so you can focus on your ideas instead of boilerplate code. Whether you‚Äôre prototyping or scaling, Reflex Build accelerates development by intelligently scaffolding and optimizing your app‚Äôs entire stack.&lt;/p&gt; 
&lt;p&gt;Alongside this, &lt;a href="https://cloud.reflex.dev"&gt;Reflex Cloud&lt;/a&gt; launched in 2025 to offer the best hosting experience for your Reflex apps. We‚Äôre continuously improving the platform with new features and capabilities.&lt;/p&gt; 
&lt;p&gt;Reflex has new releases and features coming every week! Make sure to &lt;span&gt;‚≠ê&lt;/span&gt; star and &lt;span&gt;üëÄ&lt;/span&gt; watch this repository to stay up to date.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions of any size! Below are some good ways to get started in the Reflex community.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Join Our Discord&lt;/strong&gt;: Our &lt;a href="https://discord.gg/T5WSbC2YtQ"&gt;Discord&lt;/a&gt; is the best place to get help on your Reflex project and to discuss how you can contribute.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;GitHub Discussions&lt;/strong&gt;: A great way to talk about features you want added or things that are confusing/need clarification.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;GitHub Issues&lt;/strong&gt;: &lt;a href="https://github.com/reflex-dev/reflex/issues"&gt;Issues&lt;/a&gt; are an excellent way to report bugs. Additionally, you can try and solve an existing issue and submit a PR.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We are actively looking for contributors, no matter your skill level or experience. To contribute check out &lt;a href="https://github.com/reflex-dev/reflex/raw/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;All Thanks To Our Contributors:&lt;/h2&gt; 
&lt;a href="https://github.com/reflex-dev/reflex/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=reflex-dev/reflex"&gt; &lt;/a&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Reflex is open-source and licensed under the &lt;a href="https://raw.githubusercontent.com/reflex-dev/reflex/main/LICENSE"&gt;Apache License 2.0&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>crewAIInc/crewAI</title>
      <link>https://github.com/crewAIInc/crewAI</link>
      <description>&lt;p&gt;Framework for orchestrating role-playing, autonomous AI agents. By fostering collaborative intelligence, CrewAI empowers agents to work together seamlessly, tackling complex tasks.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="https://github.com/crewAIInc/crewAI"&gt; &lt;img src="https://raw.githubusercontent.com/crewAIInc/crewAI/main/docs/images/crewai_logo.png" width="600px" alt="Open source Multi-AI Agent orchestration framework"&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center" style="display: flex; justify-content: center; gap: 20px; align-items: center;"&gt; &lt;a href="https://trendshift.io/repositories/11239" target="_blank"&gt; &lt;img src="https://trendshift.io/api/badge/repositories/11239" alt="crewAIInc%2FcrewAI | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://crewai.com"&gt;Homepage&lt;/a&gt; ¬∑ &lt;a href="https://docs.crewai.com"&gt;Docs&lt;/a&gt; ¬∑ &lt;a href="https://app.crewai.com"&gt;Start Cloud Trial&lt;/a&gt; ¬∑ &lt;a href="https://blog.crewai.com"&gt;Blog&lt;/a&gt; ¬∑ &lt;a href="https://community.crewai.com"&gt;Forum&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/crewAIInc/crewAI"&gt; &lt;img src="https://img.shields.io/github/stars/crewAIInc/crewAI" alt="GitHub Repo stars"&gt; &lt;/a&gt; &lt;a href="https://github.com/crewAIInc/crewAI/network/members"&gt; &lt;img src="https://img.shields.io/github/forks/crewAIInc/crewAI" alt="GitHub forks"&gt; &lt;/a&gt; &lt;a href="https://github.com/crewAIInc/crewAI/issues"&gt; &lt;img src="https://img.shields.io/github/issues/crewAIInc/crewAI" alt="GitHub issues"&gt; &lt;/a&gt; &lt;a href="https://github.com/crewAIInc/crewAI/pulls"&gt; &lt;img src="https://img.shields.io/github/issues-pr/crewAIInc/crewAI" alt="GitHub pull requests"&gt; &lt;/a&gt; &lt;a href="https://opensource.org/licenses/MIT"&gt; &lt;img src="https://img.shields.io/badge/License-MIT-green.svg?sanitize=true" alt="License: MIT"&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://pypi.org/project/crewai/"&gt; &lt;img src="https://img.shields.io/pypi/v/crewai" alt="PyPI version"&gt; &lt;/a&gt; &lt;a href="https://pypi.org/project/crewai/"&gt; &lt;img src="https://img.shields.io/pypi/dm/crewai" alt="PyPI downloads"&gt; &lt;/a&gt; &lt;a href="https://twitter.com/crewAIInc"&gt; &lt;img src="https://img.shields.io/twitter/follow/crewAIInc?style=social" alt="Twitter Follow"&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h3&gt;Fast and Flexible Multi-Agent Automation Framework&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;CrewAI is a lean, lightning-fast Python framework built entirely from scratch‚Äîcompletely &lt;strong&gt;independent of LangChain or other agent frameworks&lt;/strong&gt;. It empowers developers with both high-level simplicity and precise low-level control, ideal for creating autonomous AI agents tailored to any scenario.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;CrewAI Crews&lt;/strong&gt;: Optimize for autonomy and collaborative intelligence.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;CrewAI Flows&lt;/strong&gt;: Enable granular, event-driven control, single LLM calls for precise task orchestration and supports Crews natively&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;With over 100,000 developers certified through our community courses at &lt;a href="https://learn.crewai.com"&gt;learn.crewai.com&lt;/a&gt;, CrewAI is rapidly becoming the standard for enterprise-ready AI automation.&lt;/p&gt; 
&lt;h1&gt;CrewAI Enterprise Suite&lt;/h1&gt; 
&lt;p&gt;CrewAI Enterprise Suite is a comprehensive bundle tailored for organizations that require secure, scalable, and easy-to-manage agent-driven automation.&lt;/p&gt; 
&lt;p&gt;You can try one part of the suite the &lt;a href="https://app.crewai.com"&gt;Crew Control Plane for free&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Crew Control Plane Key Features:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Tracing &amp;amp; Observability&lt;/strong&gt;: Monitor and track your AI agents and workflows in real-time, including metrics, logs, and traces.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Unified Control Plane&lt;/strong&gt;: A centralized platform for managing, monitoring, and scaling your AI agents and workflows.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Seamless Integrations&lt;/strong&gt;: Easily connect with existing enterprise systems, data sources, and cloud infrastructure.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Advanced Security&lt;/strong&gt;: Built-in robust security and compliance measures ensuring safe deployment and management.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Actionable Insights&lt;/strong&gt;: Real-time analytics and reporting to optimize performance and decision-making.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;24/7 Support&lt;/strong&gt;: Dedicated enterprise support to ensure uninterrupted operation and quick resolution of issues.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;On-premise and Cloud Deployment Options&lt;/strong&gt;: Deploy CrewAI Enterprise on-premise or in the cloud, depending on your security and compliance requirements.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;CrewAI Enterprise is designed for enterprises seeking a powerful, reliable solution to transform complex business processes into efficient, intelligent automations.&lt;/p&gt; 
&lt;h2&gt;Table of contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#why-crewai"&gt;Why CrewAI?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#getting-started"&gt;Getting Started&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#key-features"&gt;Key Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#understanding-flows-and-crews"&gt;Understanding Flows and Crews&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#how-crewai-compares"&gt;CrewAI vs LangGraph&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#examples"&gt;Examples&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#quick-tutorial"&gt;Quick Tutorial&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#write-job-descriptions"&gt;Write Job Descriptions&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#trip-planner"&gt;Trip Planner&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#stock-analysis"&gt;Stock Analysis&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#using-crews-and-flows-together"&gt;Using Crews and Flows Together&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#connecting-your-crew-to-a-model"&gt;Connecting Your Crew to a Model&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#how-crewai-compares"&gt;How CrewAI Compares&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#frequently-asked-questions-faq"&gt;Frequently Asked Questions (FAQ)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#contribution"&gt;Contribution&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#telemetry"&gt;Telemetry&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#license"&gt;License&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Why CrewAI?&lt;/h2&gt; 
&lt;div align="center" style="margin-bottom: 30px;"&gt; 
 &lt;img src="https://raw.githubusercontent.com/crewAIInc/crewAI/main/docs/images/asset.png" alt="CrewAI Logo" width="100%"&gt; 
&lt;/div&gt; 
&lt;p&gt;CrewAI unlocks the true potential of multi-agent automation, delivering the best-in-class combination of speed, flexibility, and control with either Crews of AI Agents or Flows of Events:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Standalone Framework&lt;/strong&gt;: Built from scratch, independent of LangChain or any other agent framework.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;High Performance&lt;/strong&gt;: Optimized for speed and minimal resource usage, enabling faster execution.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Flexible Low Level Customization&lt;/strong&gt;: Complete freedom to customize at both high and low levels - from overall workflows and system architecture to granular agent behaviors, internal prompts, and execution logic.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Ideal for Every Use Case&lt;/strong&gt;: Proven effective for both simple tasks and highly complex, real-world, enterprise-grade scenarios.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Robust Community&lt;/strong&gt;: Backed by a rapidly growing community of over &lt;strong&gt;100,000 certified&lt;/strong&gt; developers offering comprehensive support and resources.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;CrewAI empowers developers and enterprises to confidently build intelligent automations, bridging the gap between simplicity, flexibility, and performance.&lt;/p&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;Setup and run your first CrewAI agents by following this tutorial.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=-kSOTtYzgEw" title="CrewAI Getting Started Tutorial"&gt;&lt;img src="https://img.youtube.com/vi/-kSOTtYzgEw/hqdefault.jpg" alt="CrewAI Getting Started Tutorial"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;&lt;/h3&gt; 
&lt;p&gt;Learning Resources&lt;/p&gt; 
&lt;p&gt;Learn CrewAI through our comprehensive courses:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.deeplearning.ai/short-courses/multi-ai-agent-systems-with-crewai/"&gt;Multi AI Agent Systems with CrewAI&lt;/a&gt; - Master the fundamentals of multi-agent systems&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.deeplearning.ai/short-courses/practical-multi-ai-agents-and-advanced-use-cases-with-crewai/"&gt;Practical Multi AI Agents and Advanced Use Cases&lt;/a&gt; - Deep dive into advanced implementations&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Understanding Flows and Crews&lt;/h3&gt; 
&lt;p&gt;CrewAI offers two powerful, complementary approaches that work seamlessly together to build sophisticated AI applications:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Crews&lt;/strong&gt;: Teams of AI agents with true autonomy and agency, working together to accomplish complex tasks through role-based collaboration. Crews enable:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Natural, autonomous decision-making between agents&lt;/li&gt; 
   &lt;li&gt;Dynamic task delegation and collaboration&lt;/li&gt; 
   &lt;li&gt;Specialized roles with defined goals and expertise&lt;/li&gt; 
   &lt;li&gt;Flexible problem-solving approaches&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Flows&lt;/strong&gt;: Production-ready, event-driven workflows that deliver precise control over complex automations. Flows provide:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Fine-grained control over execution paths for real-world scenarios&lt;/li&gt; 
   &lt;li&gt;Secure, consistent state management between tasks&lt;/li&gt; 
   &lt;li&gt;Clean integration of AI agents with production Python code&lt;/li&gt; 
   &lt;li&gt;Conditional branching for complex business logic&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;The true power of CrewAI emerges when combining Crews and Flows. This synergy allows you to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Build complex, production-grade applications&lt;/li&gt; 
 &lt;li&gt;Balance autonomy with precise control&lt;/li&gt; 
 &lt;li&gt;Handle sophisticated real-world scenarios&lt;/li&gt; 
 &lt;li&gt;Maintain clean, maintainable code structure&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Getting Started with Installation&lt;/h3&gt; 
&lt;p&gt;To get started with CrewAI, follow these simple steps:&lt;/p&gt; 
&lt;h3&gt;1. Installation&lt;/h3&gt; 
&lt;p&gt;Ensure you have Python &amp;gt;=3.10 &amp;lt;3.14 installed on your system. CrewAI uses &lt;a href="https://docs.astral.sh/uv/"&gt;UV&lt;/a&gt; for dependency management and package handling, offering a seamless setup and execution experience.&lt;/p&gt; 
&lt;p&gt;First, install CrewAI:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip install crewai
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you want to install the 'crewai' package along with its optional features that include additional tools for agents, you can do so by using the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip install 'crewai[tools]'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The command above installs the basic package and also adds extra components which require more dependencies to function.&lt;/p&gt; 
&lt;h3&gt;Troubleshooting Dependencies&lt;/h3&gt; 
&lt;p&gt;If you encounter issues during installation or usage, here are some common solutions:&lt;/p&gt; 
&lt;h4&gt;Common Issues&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;ModuleNotFoundError: No module named 'tiktoken'&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Install tiktoken explicitly: &lt;code&gt;pip install 'crewai[embeddings]'&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;If using embedchain or other tools: &lt;code&gt;pip install 'crewai[tools]'&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Failed building wheel for tiktoken&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Ensure Rust compiler is installed (see installation steps above)&lt;/li&gt; 
   &lt;li&gt;For Windows: Verify Visual C++ Build Tools are installed&lt;/li&gt; 
   &lt;li&gt;Try upgrading pip: &lt;code&gt;pip install --upgrade pip&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;If issues persist, use a pre-built wheel: &lt;code&gt;pip install tiktoken --prefer-binary&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;2. Setting Up Your Crew with the YAML Configuration&lt;/h3&gt; 
&lt;p&gt;To create a new CrewAI project, run the following CLI (Command Line Interface) command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;crewai create crew &amp;lt;project_name&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This command creates a new project folder with the following structure:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;my_project/
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ pyproject.toml
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ .env
‚îî‚îÄ‚îÄ src/
    ‚îî‚îÄ‚îÄ my_project/
        ‚îú‚îÄ‚îÄ __init__.py
        ‚îú‚îÄ‚îÄ main.py
        ‚îú‚îÄ‚îÄ crew.py
        ‚îú‚îÄ‚îÄ tools/
        ‚îÇ   ‚îú‚îÄ‚îÄ custom_tool.py
        ‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
        ‚îî‚îÄ‚îÄ config/
            ‚îú‚îÄ‚îÄ agents.yaml
            ‚îî‚îÄ‚îÄ tasks.yaml
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can now start developing your crew by editing the files in the &lt;code&gt;src/my_project&lt;/code&gt; folder. The &lt;code&gt;main.py&lt;/code&gt; file is the entry point of the project, the &lt;code&gt;crew.py&lt;/code&gt; file is where you define your crew, the &lt;code&gt;agents.yaml&lt;/code&gt; file is where you define your agents, and the &lt;code&gt;tasks.yaml&lt;/code&gt; file is where you define your tasks.&lt;/p&gt; 
&lt;h4&gt;To customize your project, you can:&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Modify &lt;code&gt;src/my_project/config/agents.yaml&lt;/code&gt; to define your agents.&lt;/li&gt; 
 &lt;li&gt;Modify &lt;code&gt;src/my_project/config/tasks.yaml&lt;/code&gt; to define your tasks.&lt;/li&gt; 
 &lt;li&gt;Modify &lt;code&gt;src/my_project/crew.py&lt;/code&gt; to add your own logic, tools, and specific arguments.&lt;/li&gt; 
 &lt;li&gt;Modify &lt;code&gt;src/my_project/main.py&lt;/code&gt; to add custom inputs for your agents and tasks.&lt;/li&gt; 
 &lt;li&gt;Add your environment variables into the &lt;code&gt;.env&lt;/code&gt; file.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Example of a simple crew with a sequential process:&lt;/h4&gt; 
&lt;p&gt;Instantiate your crew:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;crewai create crew latest-ai-development
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Modify the files as needed to fit your use case:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;agents.yaml&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;# src/my_project/config/agents.yaml
researcher:
  role: &amp;gt;
    {topic} Senior Data Researcher
  goal: &amp;gt;
    Uncover cutting-edge developments in {topic}
  backstory: &amp;gt;
    You're a seasoned researcher with a knack for uncovering the latest
    developments in {topic}. Known for your ability to find the most relevant
    information and present it in a clear and concise manner.

reporting_analyst:
  role: &amp;gt;
    {topic} Reporting Analyst
  goal: &amp;gt;
    Create detailed reports based on {topic} data analysis and research findings
  backstory: &amp;gt;
    You're a meticulous analyst with a keen eye for detail. You're known for
    your ability to turn complex data into clear and concise reports, making
    it easy for others to understand and act on the information you provide.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;tasks.yaml&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;# src/my_project/config/tasks.yaml
research_task:
  description: &amp;gt;
    Conduct a thorough research about {topic}
    Make sure you find any interesting and relevant information given
    the current year is 2025.
  expected_output: &amp;gt;
    A list with 10 bullet points of the most relevant information about {topic}
  agent: researcher

reporting_task:
  description: &amp;gt;
    Review the context you got and expand each topic into a full section for a report.
    Make sure the report is detailed and contains any and all relevant information.
  expected_output: &amp;gt;
    A fully fledge reports with the mains topics, each with a full section of information.
    Formatted as markdown without '```'
  agent: reporting_analyst
  output_file: report.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;crew.py&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# src/my_project/crew.py
from crewai import Agent, Crew, Process, Task
from crewai.project import CrewBase, agent, crew, task
from crewai_tools import SerperDevTool
from crewai.agents.agent_builder.base_agent import BaseAgent
from typing import List

@CrewBase
class LatestAiDevelopmentCrew():
	"""LatestAiDevelopment crew"""
	agents: List[BaseAgent]
	tasks: List[Task]

	@agent
	def researcher(self) -&amp;gt; Agent:
		return Agent(
			config=self.agents_config['researcher'],
			verbose=True,
			tools=[SerperDevTool()]
		)

	@agent
	def reporting_analyst(self) -&amp;gt; Agent:
		return Agent(
			config=self.agents_config['reporting_analyst'],
			verbose=True
		)

	@task
	def research_task(self) -&amp;gt; Task:
		return Task(
			config=self.tasks_config['research_task'],
		)

	@task
	def reporting_task(self) -&amp;gt; Task:
		return Task(
			config=self.tasks_config['reporting_task'],
			output_file='report.md'
		)

	@crew
	def crew(self) -&amp;gt; Crew:
		"""Creates the LatestAiDevelopment crew"""
		return Crew(
			agents=self.agents, # Automatically created by the @agent decorator
			tasks=self.tasks, # Automatically created by the @task decorator
			process=Process.sequential,
			verbose=True,
		)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;main.py&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;#!/usr/bin/env python
# src/my_project/main.py
import sys
from latest_ai_development.crew import LatestAiDevelopmentCrew

def run():
    """
    Run the crew.
    """
    inputs = {
        'topic': 'AI Agents'
    }
    LatestAiDevelopmentCrew().crew().kickoff(inputs=inputs)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;3. Running Your Crew&lt;/h3&gt; 
&lt;p&gt;Before running your crew, make sure you have the following keys set as environment variables in your &lt;code&gt;.env&lt;/code&gt; file:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;An &lt;a href="https://platform.openai.com/account/api-keys"&gt;OpenAI API key&lt;/a&gt; (or other LLM API key): &lt;code&gt;OPENAI_API_KEY=sk-...&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;A &lt;a href="https://serper.dev/"&gt;Serper.dev&lt;/a&gt; API key: &lt;code&gt;SERPER_API_KEY=YOUR_KEY_HERE&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Lock the dependencies and install them by using the CLI command but first, navigate to your project directory:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;cd my_project
crewai install (Optional)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To run your crew, execute the following command in the root of your project:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;crewai run
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python src/my_project/main.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If an error happens due to the usage of poetry, please run the following command to update your crewai package:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;crewai update
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You should see the output in the console and the &lt;code&gt;report.md&lt;/code&gt; file should be created in the root of your project with the full final report.&lt;/p&gt; 
&lt;p&gt;In addition to the sequential process, you can use the hierarchical process, which automatically assigns a manager to the defined crew to properly coordinate the planning and execution of tasks through delegation and validation of results. &lt;a href="https://docs.crewai.com/core-concepts/Processes/"&gt;See more about the processes here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;p&gt;CrewAI stands apart as a lean, standalone, high-performance multi-AI Agent framework delivering simplicity, flexibility, and precise control‚Äîfree from the complexity and limitations found in other agent frameworks.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Standalone &amp;amp; Lean&lt;/strong&gt;: Completely independent from other frameworks like LangChain, offering faster execution and lighter resource demands.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Flexible &amp;amp; Precise&lt;/strong&gt;: Easily orchestrate autonomous agents through intuitive &lt;a href="https://docs.crewai.com/concepts/crews"&gt;Crews&lt;/a&gt; or precise &lt;a href="https://docs.crewai.com/concepts/flows"&gt;Flows&lt;/a&gt;, achieving perfect balance for your needs.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Seamless Integration&lt;/strong&gt;: Effortlessly combine Crews (autonomy) and Flows (precision) to create complex, real-world automations.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Deep Customization&lt;/strong&gt;: Tailor every aspect‚Äîfrom high-level workflows down to low-level internal prompts and agent behaviors.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Reliable Performance&lt;/strong&gt;: Consistent results across simple tasks and complex, enterprise-level automations.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Thriving Community&lt;/strong&gt;: Backed by robust documentation and over 100,000 certified developers, providing exceptional support and guidance.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Choose CrewAI to easily build powerful, adaptable, and production-ready AI automations.&lt;/p&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;p&gt;You can test different real life examples of AI crews in the &lt;a href="https://github.com/crewAIInc/crewAI-examples?tab=readme-ov-file"&gt;CrewAI-examples repo&lt;/a&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/crewAIInc/crewAI-examples/tree/main/landing_page_generator"&gt;Landing Page Generator&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.crewai.com/how-to/Human-Input-on-Execution"&gt;Having Human input on the execution&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/crewAIInc/crewAI-examples/tree/main/trip_planner"&gt;Trip Planner&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/crewAIInc/crewAI-examples/tree/main/stock_analysis"&gt;Stock Analysis&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Quick Tutorial&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=tnejrr-0a94" title="CrewAI Tutorial"&gt;&lt;img src="https://img.youtube.com/vi/tnejrr-0a94/maxresdefault.jpg" alt="CrewAI Tutorial"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Write Job Descriptions&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/crewAIInc/crewAI-examples/tree/main/job-posting"&gt;Check out code for this example&lt;/a&gt; or watch a video below:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=u98wEMz-9to" title="Jobs postings"&gt;&lt;img src="https://img.youtube.com/vi/u98wEMz-9to/maxresdefault.jpg" alt="Jobs postings"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Trip Planner&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/crewAIInc/crewAI-examples/tree/main/trip_planner"&gt;Check out code for this example&lt;/a&gt; or watch a video below:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=xis7rWp-hjs" title="Trip Planner"&gt;&lt;img src="https://img.youtube.com/vi/xis7rWp-hjs/maxresdefault.jpg" alt="Trip Planner"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Stock Analysis&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/crewAIInc/crewAI-examples/tree/main/stock_analysis"&gt;Check out code for this example&lt;/a&gt; or watch a video below:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=e0Uj4yWdaAg" title="Stock Analysis"&gt;&lt;img src="https://img.youtube.com/vi/e0Uj4yWdaAg/maxresdefault.jpg" alt="Stock Analysis"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Using Crews and Flows Together&lt;/h3&gt; 
&lt;p&gt;CrewAI's power truly shines when combining Crews with Flows to create sophisticated automation pipelines. CrewAI flows support logical operators like &lt;code&gt;or_&lt;/code&gt; and &lt;code&gt;and_&lt;/code&gt; to combine multiple conditions. This can be used with &lt;code&gt;@start&lt;/code&gt;, &lt;code&gt;@listen&lt;/code&gt;, or &lt;code&gt;@router&lt;/code&gt; decorators to create complex triggering conditions.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;or_&lt;/code&gt;: Triggers when any of the specified conditions are met.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;and_&lt;/code&gt;Triggers when all of the specified conditions are met.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Here's how you can orchestrate multiple Crews within a Flow:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from crewai.flow.flow import Flow, listen, start, router, or_
from crewai import Crew, Agent, Task, Process
from pydantic import BaseModel

# Define structured state for precise control
class MarketState(BaseModel):
    sentiment: str = "neutral"
    confidence: float = 0.0
    recommendations: list = []

class AdvancedAnalysisFlow(Flow[MarketState]):
    @start()
    def fetch_market_data(self):
        # Demonstrate low-level control with structured state
        self.state.sentiment = "analyzing"
        return {"sector": "tech", "timeframe": "1W"}  # These parameters match the task description template

    @listen(fetch_market_data)
    def analyze_with_crew(self, market_data):
        # Show crew agency through specialized roles
        analyst = Agent(
            role="Senior Market Analyst",
            goal="Conduct deep market analysis with expert insight",
            backstory="You're a veteran analyst known for identifying subtle market patterns"
        )
        researcher = Agent(
            role="Data Researcher",
            goal="Gather and validate supporting market data",
            backstory="You excel at finding and correlating multiple data sources"
        )

        analysis_task = Task(
            description="Analyze {sector} sector data for the past {timeframe}",
            expected_output="Detailed market analysis with confidence score",
            agent=analyst
        )
        research_task = Task(
            description="Find supporting data to validate the analysis",
            expected_output="Corroborating evidence and potential contradictions",
            agent=researcher
        )

        # Demonstrate crew autonomy
        analysis_crew = Crew(
            agents=[analyst, researcher],
            tasks=[analysis_task, research_task],
            process=Process.sequential,
            verbose=True
        )
        return analysis_crew.kickoff(inputs=market_data)  # Pass market_data as named inputs

    @router(analyze_with_crew)
    def determine_next_steps(self):
        # Show flow control with conditional routing
        if self.state.confidence &amp;gt; 0.8:
            return "high_confidence"
        elif self.state.confidence &amp;gt; 0.5:
            return "medium_confidence"
        return "low_confidence"

    @listen("high_confidence")
    def execute_strategy(self):
        # Demonstrate complex decision making
        strategy_crew = Crew(
            agents=[
                Agent(role="Strategy Expert",
                      goal="Develop optimal market strategy")
            ],
            tasks=[
                Task(description="Create detailed strategy based on analysis",
                     expected_output="Step-by-step action plan")
            ]
        )
        return strategy_crew.kickoff()

    @listen(or_("medium_confidence", "low_confidence"))
    def request_additional_analysis(self):
        self.state.recommendations.append("Gather more data")
        return "Additional analysis required"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This example demonstrates how to:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Use Python code for basic data operations&lt;/li&gt; 
 &lt;li&gt;Create and execute Crews as steps in your workflow&lt;/li&gt; 
 &lt;li&gt;Use Flow decorators to manage the sequence of operations&lt;/li&gt; 
 &lt;li&gt;Implement conditional branching based on Crew results&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Connecting Your Crew to a Model&lt;/h2&gt; 
&lt;p&gt;CrewAI supports using various LLMs through a variety of connection options. By default your agents will use the OpenAI API when querying the model. However, there are several other ways to allow your agents to connect to models. For example, you can configure your agents to use a local model via the Ollama tool.&lt;/p&gt; 
&lt;p&gt;Please refer to the &lt;a href="https://docs.crewai.com/how-to/LLM-Connections/"&gt;Connect CrewAI to LLMs&lt;/a&gt; page for details on configuring your agents' connections to models.&lt;/p&gt; 
&lt;h2&gt;How CrewAI Compares&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;CrewAI's Advantage&lt;/strong&gt;: CrewAI combines autonomous agent intelligence with precise workflow control through its unique Crews and Flows architecture. The framework excels at both high-level orchestration and low-level customization, enabling complex, production-grade systems with granular control.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;LangGraph&lt;/strong&gt;: While LangGraph provides a foundation for building agent workflows, its approach requires significant boilerplate code and complex state management patterns. The framework's tight coupling with LangChain can limit flexibility when implementing custom agent behaviors or integrating with external systems.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;em&gt;P.S. CrewAI demonstrates significant performance advantages over LangGraph, executing 5.76x faster in certain cases like this QA task example (&lt;a href="https://github.com/crewAIInc/crewAI-examples/tree/main/Notebooks/CrewAI%20Flows%20%26%20Langgraph/QA%20Agent"&gt;see comparison&lt;/a&gt;) while achieving higher evaluation scores with faster completion times in certain coding tasks, like in this example (&lt;a href="https://github.com/crewAIInc/crewAI-examples/raw/main/Notebooks/CrewAI%20Flows%20%26%20Langgraph/Coding%20Assistant/coding_assistant_eval.ipynb"&gt;detailed analysis&lt;/a&gt;).&lt;/em&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Autogen&lt;/strong&gt;: While Autogen excels at creating conversational agents capable of working together, it lacks an inherent concept of process. In Autogen, orchestrating agents' interactions requires additional programming, which can become complex and cumbersome as the scale of tasks grows.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ChatDev&lt;/strong&gt;: ChatDev introduced the idea of processes into the realm of AI agents, but its implementation is quite rigid. Customizations in ChatDev are limited and not geared towards production environments, which can hinder scalability and flexibility in real-world applications.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contribution&lt;/h2&gt; 
&lt;p&gt;CrewAI is open-source and we welcome contributions. If you're looking to contribute, please:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Fork the repository.&lt;/li&gt; 
 &lt;li&gt;Create a new branch for your feature.&lt;/li&gt; 
 &lt;li&gt;Add your feature or improvement.&lt;/li&gt; 
 &lt;li&gt;Send a pull request.&lt;/li&gt; 
 &lt;li&gt;We appreciate your input!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Installing Dependencies&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv lock
uv sync
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Virtual Env&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv venv
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Pre-commit hooks&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pre-commit install
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Running Tests&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv run pytest .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Running static type checks&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uvx mypy src
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Packaging&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv build
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Installing Locally&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install dist/*.tar.gz
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Telemetry&lt;/h2&gt; 
&lt;p&gt;CrewAI uses anonymous telemetry to collect usage data with the main purpose of helping us improve the library by focusing our efforts on the most used features, integrations and tools.&lt;/p&gt; 
&lt;p&gt;It's pivotal to understand that &lt;strong&gt;NO data is collected&lt;/strong&gt; concerning prompts, task descriptions, agents' backstories or goals, usage of tools, API calls, responses, any data processed by the agents, or secrets and environment variables, with the exception of the conditions mentioned. When the &lt;code&gt;share_crew&lt;/code&gt; feature is enabled, detailed data including task descriptions, agents' backstories or goals, and other specific attributes are collected to provide deeper insights while respecting user privacy. Users can disable telemetry by setting the environment variable OTEL_SDK_DISABLED to true.&lt;/p&gt; 
&lt;p&gt;Data collected includes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Version of CrewAI 
  &lt;ul&gt; 
   &lt;li&gt;So we can understand how many users are using the latest version&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Version of Python 
  &lt;ul&gt; 
   &lt;li&gt;So we can decide on what versions to better support&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;General OS (e.g. number of CPUs, macOS/Windows/Linux) 
  &lt;ul&gt; 
   &lt;li&gt;So we know what OS we should focus on and if we could build specific OS related features&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Number of agents and tasks in a crew 
  &lt;ul&gt; 
   &lt;li&gt;So we make sure we are testing internally with similar use cases and educate people on the best practices&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Crew Process being used 
  &lt;ul&gt; 
   &lt;li&gt;Understand where we should focus our efforts&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;If Agents are using memory or allowing delegation 
  &lt;ul&gt; 
   &lt;li&gt;Understand if we improved the features or maybe even drop them&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;If Tasks are being executed in parallel or sequentially 
  &lt;ul&gt; 
   &lt;li&gt;Understand if we should focus more on parallel execution&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Language model being used 
  &lt;ul&gt; 
   &lt;li&gt;Improved support on most used languages&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Roles of agents in a crew 
  &lt;ul&gt; 
   &lt;li&gt;Understand high level use cases so we can build better tools, integrations and examples about it&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Tools names available 
  &lt;ul&gt; 
   &lt;li&gt;Understand out of the publicly available tools, which ones are being used the most so we can improve them&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Users can opt-in to Further Telemetry, sharing the complete telemetry data by setting the &lt;code&gt;share_crew&lt;/code&gt; attribute to &lt;code&gt;True&lt;/code&gt; on their Crews. Enabling &lt;code&gt;share_crew&lt;/code&gt; results in the collection of detailed crew and task execution data, including &lt;code&gt;goal&lt;/code&gt;, &lt;code&gt;backstory&lt;/code&gt;, &lt;code&gt;context&lt;/code&gt;, and &lt;code&gt;output&lt;/code&gt; of tasks. This enables a deeper insight into usage patterns while respecting the user's choice to share.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;CrewAI is released under the &lt;a href="https://github.com/crewAIInc/crewAI/raw/main/LICENSE"&gt;MIT License&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Frequently Asked Questions (FAQ)&lt;/h2&gt; 
&lt;h3&gt;General&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-what-exactly-is-crewai"&gt;What exactly is CrewAI?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-how-do-i-install-crewai"&gt;How do I install CrewAI?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-does-crewai-depend-on-langchain"&gt;Does CrewAI depend on LangChain?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-is-crewai-open-source"&gt;Is CrewAI open-source?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-does-crewai-collect-data-from-users"&gt;Does CrewAI collect data from users?&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Features and Capabilities&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-can-crewai-handle-complex-use-cases"&gt;Can CrewAI handle complex use cases?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-can-i-use-crewai-with-local-ai-models"&gt;Can I use CrewAI with local AI models?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-what-makes-crews-different-from-flows"&gt;What makes Crews different from Flows?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-how-is-crewai-better-than-langchain"&gt;How is CrewAI better than LangChain?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-does-crewai-support-fine-tuning-or-training-custom-models"&gt;Does CrewAI support fine-tuning or training custom models?&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Resources and Community&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-where-can-i-find-real-world-crewai-examples"&gt;Where can I find real-world CrewAI examples?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-how-can-i-contribute-to-crewai"&gt;How can I contribute to CrewAI?&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Enterprise Features&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-what-additional-features-does-crewai-enterprise-offer"&gt;What additional features does CrewAI Enterprise offer?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-is-crewai-enterprise-available-for-cloud-and-on-premise-deployments"&gt;Is CrewAI Enterprise available for cloud and on-premise deployments?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/crewAIInc/crewAI/main/#q-can-i-try-crewai-enterprise-for-free"&gt;Can I try CrewAI Enterprise for free?&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Q: What exactly is CrewAI?&lt;/h3&gt; 
&lt;p&gt;A: CrewAI is a standalone, lean, and fast Python framework built specifically for orchestrating autonomous AI agents. Unlike frameworks like LangChain, CrewAI does not rely on external dependencies, making it leaner, faster, and simpler.&lt;/p&gt; 
&lt;h3&gt;Q: How do I install CrewAI?&lt;/h3&gt; 
&lt;p&gt;A: Install CrewAI using pip:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip install crewai
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For additional tools, use:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip install 'crewai[tools]'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Q: Does CrewAI depend on LangChain?&lt;/h3&gt; 
&lt;p&gt;A: No. CrewAI is built entirely from the ground up, with no dependencies on LangChain or other agent frameworks. This ensures a lean, fast, and flexible experience.&lt;/p&gt; 
&lt;h3&gt;Q: Can CrewAI handle complex use cases?&lt;/h3&gt; 
&lt;p&gt;A: Yes. CrewAI excels at both simple and highly complex real-world scenarios, offering deep customization options at both high and low levels, from internal prompts to sophisticated workflow orchestration.&lt;/p&gt; 
&lt;h3&gt;Q: Can I use CrewAI with local AI models?&lt;/h3&gt; 
&lt;p&gt;A: Absolutely! CrewAI supports various language models, including local ones. Tools like Ollama and LM Studio allow seamless integration. Check the &lt;a href="https://docs.crewai.com/how-to/LLM-Connections/"&gt;LLM Connections documentation&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h3&gt;Q: What makes Crews different from Flows?&lt;/h3&gt; 
&lt;p&gt;A: Crews provide autonomous agent collaboration, ideal for tasks requiring flexible decision-making and dynamic interaction. Flows offer precise, event-driven control, ideal for managing detailed execution paths and secure state management. You can seamlessly combine both for maximum effectiveness.&lt;/p&gt; 
&lt;h3&gt;Q: How is CrewAI better than LangChain?&lt;/h3&gt; 
&lt;p&gt;A: CrewAI provides simpler, more intuitive APIs, faster execution speeds, more reliable and consistent results, robust documentation, and an active community‚Äîaddressing common criticisms and limitations associated with LangChain.&lt;/p&gt; 
&lt;h3&gt;Q: Is CrewAI open-source?&lt;/h3&gt; 
&lt;p&gt;A: Yes, CrewAI is open-source and actively encourages community contributions and collaboration.&lt;/p&gt; 
&lt;h3&gt;Q: Does CrewAI collect data from users?&lt;/h3&gt; 
&lt;p&gt;A: CrewAI collects anonymous telemetry data strictly for improvement purposes. Sensitive data such as prompts, tasks, or API responses are never collected unless explicitly enabled by the user.&lt;/p&gt; 
&lt;h3&gt;Q: Where can I find real-world CrewAI examples?&lt;/h3&gt; 
&lt;p&gt;A: Check out practical examples in the &lt;a href="https://github.com/crewAIInc/crewAI-examples"&gt;CrewAI-examples repository&lt;/a&gt;, covering use cases like trip planners, stock analysis, and job postings.&lt;/p&gt; 
&lt;h3&gt;Q: How can I contribute to CrewAI?&lt;/h3&gt; 
&lt;p&gt;A: Contributions are warmly welcomed! Fork the repository, create your branch, implement your changes, and submit a pull request. See the Contribution section of the README for detailed guidelines.&lt;/p&gt; 
&lt;h3&gt;Q: What additional features does CrewAI Enterprise offer?&lt;/h3&gt; 
&lt;p&gt;A: CrewAI Enterprise provides advanced features such as a unified control plane, real-time observability, secure integrations, advanced security, actionable insights, and dedicated 24/7 enterprise support.&lt;/p&gt; 
&lt;h3&gt;Q: Is CrewAI Enterprise available for cloud and on-premise deployments?&lt;/h3&gt; 
&lt;p&gt;A: Yes, CrewAI Enterprise supports both cloud-based and on-premise deployment options, allowing enterprises to meet their specific security and compliance requirements.&lt;/p&gt; 
&lt;h3&gt;Q: Can I try CrewAI Enterprise for free?&lt;/h3&gt; 
&lt;p&gt;A: Yes, you can explore part of the CrewAI Enterprise Suite by accessing the &lt;a href="https://app.crewai.com"&gt;Crew Control Plane&lt;/a&gt; for free.&lt;/p&gt; 
&lt;h3&gt;Q: Does CrewAI support fine-tuning or training custom models?&lt;/h3&gt; 
&lt;p&gt;A: Yes, CrewAI can integrate with custom-trained or fine-tuned models, allowing you to enhance your agents with domain-specific knowledge and accuracy.&lt;/p&gt; 
&lt;h3&gt;Q: Can CrewAI agents interact with external tools and APIs?&lt;/h3&gt; 
&lt;p&gt;A: Absolutely! CrewAI agents can easily integrate with external tools, APIs, and databases, empowering them to leverage real-world data and resources.&lt;/p&gt; 
&lt;h3&gt;Q: Is CrewAI suitable for production environments?&lt;/h3&gt; 
&lt;p&gt;A: Yes, CrewAI is explicitly designed with production-grade standards, ensuring reliability, stability, and scalability for enterprise deployments.&lt;/p&gt; 
&lt;h3&gt;Q: How scalable is CrewAI?&lt;/h3&gt; 
&lt;p&gt;A: CrewAI is highly scalable, supporting simple automations and large-scale enterprise workflows involving numerous agents and complex tasks simultaneously.&lt;/p&gt; 
&lt;h3&gt;Q: Does CrewAI offer debugging and monitoring tools?&lt;/h3&gt; 
&lt;p&gt;A: Yes, CrewAI Enterprise includes advanced debugging, tracing, and real-time observability features, simplifying the management and troubleshooting of your automations.&lt;/p&gt; 
&lt;h3&gt;Q: What programming languages does CrewAI support?&lt;/h3&gt; 
&lt;p&gt;A: CrewAI is primarily Python-based but easily integrates with services and APIs written in any programming language through its flexible API integration capabilities.&lt;/p&gt; 
&lt;h3&gt;Q: Does CrewAI offer educational resources for beginners?&lt;/h3&gt; 
&lt;p&gt;A: Yes, CrewAI provides extensive beginner-friendly tutorials, courses, and documentation through learn.crewai.com, supporting developers at all skill levels.&lt;/p&gt; 
&lt;h3&gt;Q: Can CrewAI automate human-in-the-loop workflows?&lt;/h3&gt; 
&lt;p&gt;A: Yes, CrewAI fully supports human-in-the-loop workflows, allowing seamless collaboration between human experts and AI agents for enhanced decision-making.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>python/cpython</title>
      <link>https://github.com/python/cpython</link>
      <description>&lt;p&gt;The Python programming language&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;This is Python version 3.15.0 alpha 0&lt;/h1&gt; 
&lt;p&gt;.. image:: &lt;a href="https://github.com/python/cpython/actions/workflows/build.yml/badge.svg?branch=main&amp;amp;event=push"&gt;https://github.com/python/cpython/actions/workflows/build.yml/badge.svg?branch=main&amp;amp;event=push&lt;/a&gt; :alt: CPython build status on GitHub Actions :target: &lt;a href="https://github.com/python/cpython/actions"&gt;https://github.com/python/cpython/actions&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;.. image:: &lt;a href="https://dev.azure.com/python/cpython/_apis/build/status/Azure%20Pipelines%20CI?branchName=main"&gt;https://dev.azure.com/python/cpython/_apis/build/status/Azure%20Pipelines%20CI?branchName=main&lt;/a&gt; :alt: CPython build status on Azure DevOps :target: &lt;a href="https://dev.azure.com/python/cpython/_build/latest?definitionId=4&amp;amp;branchName=main"&gt;https://dev.azure.com/python/cpython/_build/latest?definitionId=4&amp;amp;branchName=main&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;.. image:: &lt;a href="https://img.shields.io/badge/discourse-join_chat-brightgreen.svg"&gt;https://img.shields.io/badge/discourse-join_chat-brightgreen.svg&lt;/a&gt; :alt: Python Discourse chat :target: &lt;a href="https://discuss.python.org/"&gt;https://discuss.python.org/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Copyright ¬© 2001 Python Software Foundation. All rights reserved.&lt;/p&gt; 
&lt;p&gt;See the end of this file for further copyright and license information.&lt;/p&gt; 
&lt;p&gt;.. contents::&lt;/p&gt; 
&lt;h2&gt;General Information&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Website: &lt;a href="https://www.python.org"&gt;https://www.python.org&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Source code: &lt;a href="https://github.com/python/cpython"&gt;https://github.com/python/cpython&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Issue tracker: &lt;a href="https://github.com/python/cpython/issues"&gt;https://github.com/python/cpython/issues&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Documentation: &lt;a href="https://docs.python.org"&gt;https://docs.python.org&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Developer's Guide: &lt;a href="https://devguide.python.org/"&gt;https://devguide.python.org/&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing to CPython&lt;/h2&gt; 
&lt;p&gt;For more complete instructions on contributing to CPython development, see the &lt;code&gt;Developer Guide&lt;/code&gt;_.&lt;/p&gt; 
&lt;p&gt;.. _Developer Guide: &lt;a href="https://devguide.python.org/"&gt;https://devguide.python.org/&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Using Python&lt;/h2&gt; 
&lt;p&gt;Installable Python kits, and information about using Python, are available at &lt;code&gt;python.org&lt;/code&gt;_.&lt;/p&gt; 
&lt;p&gt;.. _python.org: &lt;a href="https://www.python.org/"&gt;https://www.python.org/&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Build Instructions&lt;/h2&gt; 
&lt;p&gt;On Unix, Linux, BSD, macOS, and Cygwin::&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;./configure
make
make test
sudo make install
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will install Python as &lt;code&gt;python3&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;You can pass many options to the configure script; run &lt;code&gt;./configure --help&lt;/code&gt; to find out more. On macOS case-insensitive file systems and on Cygwin, the executable is called &lt;code&gt;python.exe&lt;/code&gt;; elsewhere it's just &lt;code&gt;python&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Building a complete Python installation requires the use of various additional third-party libraries, depending on your build platform and configure options. Not all standard library modules are buildable or usable on all platforms. Refer to the &lt;code&gt;Install dependencies &amp;lt;https://devguide.python.org/getting-started/setup-building.html#build-dependencies&amp;gt;&lt;/code&gt;_ section of the &lt;code&gt;Developer Guide&lt;/code&gt;_ for current detailed information on dependencies for various Linux distributions and macOS.&lt;/p&gt; 
&lt;p&gt;On macOS, there are additional configure and build options related to macOS framework and universal builds. Refer to &lt;code&gt;Mac/README.rst &amp;lt;https://github.com/python/cpython/blob/main/Mac/README.rst&amp;gt;&lt;/code&gt;_.&lt;/p&gt; 
&lt;p&gt;On Windows, see &lt;code&gt;PCbuild/readme.txt &amp;lt;https://github.com/python/cpython/blob/main/PCbuild/readme.txt&amp;gt;&lt;/code&gt;_.&lt;/p&gt; 
&lt;p&gt;To build Windows installer, see &lt;code&gt;Tools/msi/README.txt &amp;lt;https://github.com/python/cpython/blob/main/Tools/msi/README.txt&amp;gt;&lt;/code&gt;_.&lt;/p&gt; 
&lt;p&gt;If you wish, you can create a subdirectory and invoke configure from there. For example::&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;mkdir debug
cd debug
../configure --with-pydebug
make
make test
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;(This will fail if you &lt;em&gt;also&lt;/em&gt; built at the top-level directory. You should do a &lt;code&gt;make clean&lt;/code&gt; at the top-level first.)&lt;/p&gt; 
&lt;p&gt;To get an optimized build of Python, &lt;code&gt;configure --enable-optimizations&lt;/code&gt; before you run &lt;code&gt;make&lt;/code&gt;. This sets the default make targets up to enable Profile Guided Optimization (PGO) and may be used to auto-enable Link Time Optimization (LTO) on some platforms. For more details, see the sections below.&lt;/p&gt; 
&lt;p&gt;Profile Guided Optimization ^^^^^^^^^^^^^^^^^^^^^^^^^^^&lt;/p&gt; 
&lt;p&gt;PGO takes advantage of recent versions of the GCC or Clang compilers. If used, either via &lt;code&gt;configure --enable-optimizations&lt;/code&gt; or by manually running &lt;code&gt;make profile-opt&lt;/code&gt; regardless of configure flags, the optimized build process will perform the following steps:&lt;/p&gt; 
&lt;p&gt;The entire Python directory is cleaned of temporary files that may have resulted from a previous compilation.&lt;/p&gt; 
&lt;p&gt;An instrumented version of the interpreter is built, using suitable compiler flags for each flavor. Note that this is just an intermediary step. The binary resulting from this step is not good for real-life workloads as it has profiling instructions embedded inside.&lt;/p&gt; 
&lt;p&gt;After the instrumented interpreter is built, the Makefile will run a training workload. This is necessary in order to profile the interpreter's execution. Note also that any output, both stdout and stderr, that may appear at this step is suppressed.&lt;/p&gt; 
&lt;p&gt;The final step is to build the actual interpreter, using the information collected from the instrumented one. The end result will be a Python binary that is optimized; suitable for distribution or production installation.&lt;/p&gt; 
&lt;p&gt;Link Time Optimization ^^^^^^^^^^^^^^^^^^^^^^&lt;/p&gt; 
&lt;p&gt;Enabled via configure's &lt;code&gt;--with-lto&lt;/code&gt; flag. LTO takes advantage of the ability of recent compiler toolchains to optimize across the otherwise arbitrary &lt;code&gt;.o&lt;/code&gt; file boundary when building final executables or shared libraries for additional performance gains.&lt;/p&gt; 
&lt;h2&gt;What's New&lt;/h2&gt; 
&lt;p&gt;We have a comprehensive overview of the changes in the &lt;code&gt;What's new in Python 3.15 &amp;lt;https://docs.python.org/3.15/whatsnew/3.15.html&amp;gt;&lt;/code&gt;_ document. For a more detailed change log, read &lt;code&gt;Misc/NEWS &amp;lt;https://github.com/python/cpython/tree/main/Misc/NEWS.d&amp;gt;&lt;/code&gt;&lt;em&gt;, but a full accounting of changes can only be gleaned from the &lt;code&gt;commit history &amp;lt;https://github.com/python/cpython/commits/main&amp;gt;&lt;/code&gt;&lt;/em&gt;.&lt;/p&gt; 
&lt;p&gt;If you want to install multiple versions of Python, see the section below entitled "Installing multiple versions".&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;&lt;code&gt;Documentation for Python 3.15 &amp;lt;https://docs.python.org/3.15/&amp;gt;&lt;/code&gt;_ is online, updated daily.&lt;/p&gt; 
&lt;p&gt;It can also be downloaded in many formats for faster access. The documentation is downloadable in HTML, PDF, and reStructuredText formats; the latter version is primarily for documentation authors, translators, and people with special formatting requirements.&lt;/p&gt; 
&lt;p&gt;For information about building Python's documentation, refer to &lt;code&gt;Doc/README.rst &amp;lt;https://github.com/python/cpython/blob/main/Doc/README.rst&amp;gt;&lt;/code&gt;_.&lt;/p&gt; 
&lt;h2&gt;Testing&lt;/h2&gt; 
&lt;p&gt;To test the interpreter, type &lt;code&gt;make test&lt;/code&gt; in the top-level directory. The test set produces some output. You can generally ignore the messages about skipped tests due to optional features which can't be imported. If a message is printed about a failed test or a traceback or core dump is produced, something is wrong.&lt;/p&gt; 
&lt;p&gt;By default, tests are prevented from overusing resources like disk space and memory. To enable these tests, run &lt;code&gt;make buildbottest&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;If any tests fail, you can re-run the failing test(s) in verbose mode. For example, if &lt;code&gt;test_os&lt;/code&gt; and &lt;code&gt;test_gdb&lt;/code&gt; failed, you can run::&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;make test TESTOPTS="-v test_os test_gdb"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If the failure persists and appears to be a problem with Python rather than your environment, you can &lt;code&gt;file a bug report &amp;lt;https://github.com/python/cpython/issues&amp;gt;&lt;/code&gt;_ and include relevant output from that command to show the issue.&lt;/p&gt; 
&lt;p&gt;See &lt;code&gt;Running &amp;amp; Writing Tests &amp;lt;https://devguide.python.org/testing/run-write-tests.html&amp;gt;&lt;/code&gt;_ for more on running tests.&lt;/p&gt; 
&lt;h2&gt;Installing multiple versions&lt;/h2&gt; 
&lt;p&gt;On Unix and Mac systems if you intend to install multiple versions of Python using the same installation prefix (&lt;code&gt;--prefix&lt;/code&gt; argument to the configure script) you must take care that your primary python executable is not overwritten by the installation of a different version. All files and directories installed using &lt;code&gt;make altinstall&lt;/code&gt; contain the major and minor version and can thus live side-by-side. &lt;code&gt;make install&lt;/code&gt; also creates &lt;code&gt;${prefix}/bin/python3&lt;/code&gt; which refers to &lt;code&gt;${prefix}/bin/python3.X&lt;/code&gt;. If you intend to install multiple versions using the same prefix you must decide which version (if any) is your "primary" version. Install that version using &lt;code&gt;make install&lt;/code&gt;. Install all other versions using &lt;code&gt;make altinstall&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;For example, if you want to install Python 2.7, 3.6, and 3.15 with 3.15 being the primary version, you would execute &lt;code&gt;make install&lt;/code&gt; in your 3.15 build directory and &lt;code&gt;make altinstall&lt;/code&gt; in the others.&lt;/p&gt; 
&lt;h2&gt;Release Schedule&lt;/h2&gt; 
&lt;p&gt;See &lt;code&gt;PEP 790 &amp;lt;https://peps.python.org/pep-0790/&amp;gt;&lt;/code&gt;__ for Python 3.15 release details.&lt;/p&gt; 
&lt;h2&gt;Copyright and License Information&lt;/h2&gt; 
&lt;p&gt;Copyright ¬© 2001 Python Software Foundation. All rights reserved.&lt;/p&gt; 
&lt;p&gt;Copyright ¬© 2000 BeOpen.com. All rights reserved.&lt;/p&gt; 
&lt;p&gt;Copyright ¬© 1995-2001 Corporation for National Research Initiatives. All rights reserved.&lt;/p&gt; 
&lt;p&gt;Copyright ¬© 1991-1995 Stichting Mathematisch Centrum. All rights reserved.&lt;/p&gt; 
&lt;p&gt;See the &lt;code&gt;LICENSE &amp;lt;https://github.com/python/cpython/blob/main/LICENSE&amp;gt;&lt;/code&gt;_ for information on the history of this software, terms &amp;amp; conditions for usage, and a DISCLAIMER OF ALL WARRANTIES.&lt;/p&gt; 
&lt;p&gt;This Python distribution contains &lt;em&gt;no&lt;/em&gt; GNU General Public License (GPL) code, so it may be used in proprietary projects. There are interfaces to some GNU code but these are entirely optional.&lt;/p&gt; 
&lt;p&gt;All trademarks referenced herein are property of their respective holders.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Shubhamsaboo/awesome-llm-apps</title>
      <link>https://github.com/Shubhamsaboo/awesome-llm-apps</link>
      <description>&lt;p&gt;Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="http://www.theunwindai.com"&gt; &lt;img src="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/docs/banner/unwind_black.png" width="900px" alt="Unwind AI"&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://www.linkedin.com/in/shubhamsaboo/"&gt; &lt;img src="https://img.shields.io/badge/-Follow%20Shubham%20Saboo-blue?logo=linkedin&amp;amp;style=flat-square" alt="LinkedIn"&gt; &lt;/a&gt; &lt;a href="https://twitter.com/Saboo_Shubham_"&gt; &lt;img src="https://img.shields.io/twitter/follow/Shubham_Saboo" alt="Twitter"&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; 
 &lt;!-- Keep these links. Translations will automatically update with the README. --&gt; &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=de"&gt;Deutsch&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=es"&gt;Espa√±ol&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=fr"&gt;fran√ßais&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ja"&gt;Êó•Êú¨Ë™û&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ko"&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=pt"&gt;Portugu√™s&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ru"&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=zh"&gt;‰∏≠Êñá&lt;/a&gt; &lt;/p&gt; 
&lt;hr&gt; 
&lt;h1&gt;üåü Awesome LLM Apps&lt;/h1&gt; 
&lt;p&gt;A curated collection of &lt;strong&gt;Awesome LLM apps built with RAG, AI Agents, Multi-agent Teams, MCP, Voice Agents, and more.&lt;/strong&gt; This repository features LLM apps that use models from OpenAI, Anthropic, Google, and open-source models like DeepSeek, Qwen or Llama that you can run locally on your computer.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/9876" target="_blank"&gt; &lt;img src="https://trendshift.io/api/badge/repositories/9876" alt="Shubhamsaboo%2Fawesome-llm-apps | Trendshift" style="width: 250px; height: 55px;"&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;ü§î Why Awesome LLM Apps?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;üí° Discover practical and creative ways LLMs can be applied across different domains, from code repositories to email inboxes and more.&lt;/li&gt; 
 &lt;li&gt;üî• Explore apps that combine LLMs from OpenAI, Anthropic, Gemini, and open-source alternatives with AI Agents, Agent Teams, MCP &amp;amp; RAG.&lt;/li&gt; 
 &lt;li&gt;üéì Learn from well-documented projects and contribute to the growing open-source ecosystem of LLM-powered applications.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üìÇ Featured AI Projects&lt;/h2&gt; 
&lt;h3&gt;AI Agents&lt;/h3&gt; 
&lt;h3&gt;üå± Starter AI Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_blog_to_podcast_agent/"&gt;üéôÔ∏è AI Blog to Podcast Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_breakup_recovery_agent/"&gt;‚ù§Ô∏è‚Äçü©π AI Breakup Recovery Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_data_analysis_agent/"&gt;üìä AI Data Analysis Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_medical_imaging_agent/"&gt;ü©ª AI Medical Imaging Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_meme_generator_agent_browseruse/"&gt;üòÇ AI Meme Generator Agent (Browser)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_music_generator_agent/"&gt;üéµ AI Music Generator Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_travel_agent/"&gt;üõ´ AI Travel Agent (Local &amp;amp; Cloud)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/gemini_multimodal_agent_demo/"&gt;‚ú® Gemini Multimodal Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/local_news_agent_openai_swarm/"&gt;üåê Local News Agent (OpenAI Swarm)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/mixture_of_agents/"&gt;üîÑ Mixture of Agents&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/xai_finance_agent/"&gt;üìä xAI Finance Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/opeani_research_agent/"&gt;üîç OpenAI Research Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/web_scrapping_ai_agent/"&gt;üï∏Ô∏è Web Scrapping AI Agent (Local &amp;amp; Cloud)&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üöÄ Advanced AI Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_deep_research_agent/"&gt;üîç AI Deep Research Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_consultant_agent"&gt;ü§ù AI Consultant Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_system_architect_r1/"&gt;üèóÔ∏è AI System Architect Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_lead_generation_agent/"&gt;üéØ AI Lead Generation Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/ai_financial_coach_agent/"&gt;üí∞ AI Financial Coach Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_movie_production_agent/"&gt;üé¨ AI Movie Production Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_investment_agent/"&gt;üìà AI Investment Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_health_fitness_agent/"&gt;üèãÔ∏è‚Äç‚ôÇÔ∏è AI Health &amp;amp; Fitness Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/product_launch_intelligence_agent"&gt;üöÄ AI Product Launch Intelligence Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_journalist_agent/"&gt;üóûÔ∏è AI Journalist Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/ai_mental_wellbeing_agent/"&gt;üß† AI Mental Wellbeing Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_meeting_agent/"&gt;üìë AI Meeting Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/ai_Self-Evolving_agent/"&gt;üß¨ AI Self-Evolving Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/ai_news_and_podcast_agents/"&gt;üéß AI Social Media News and Podcast Agent&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üéÆ Autonomous Game Playing Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/autonomous_game_playing_agent_apps/ai_3dpygame_r1/"&gt;üéÆ AI 3D Pygame Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/autonomous_game_playing_agent_apps/ai_chess_agent/"&gt;‚ôú AI Chess Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/autonomous_game_playing_agent_apps/ai_tic_tac_toe_agent/"&gt;üé≤ AI Tic-Tac-Toe Agent&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ü§ù Multi-agent Teams&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_competitor_intelligence_agent_team/"&gt;üß≤ AI Competitor Intelligence Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_finance_agent_team/"&gt;üí≤ AI Finance Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_game_design_agent_team/"&gt;üé® AI Game Design Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_legal_agent_team/"&gt;üë®‚Äç‚öñÔ∏è AI Legal Agent Team (Cloud &amp;amp; Local)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_recruitment_agent_team/"&gt;üíº AI Recruitment Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_real_estate_agent_team"&gt;üè† AI Real Estate Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_services_agency/"&gt;üë®‚Äçüíº AI Services Agency (CrewAI)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_teaching_agent_team/"&gt;üë®‚Äçüè´ AI Teaching Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_coding_agent_team/"&gt;üíª Multimodal Coding Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_design_agent_team/"&gt;‚ú® Multimodal Design Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_travel_planner_agent_team/"&gt;üåè AI Travel Planner Agent Team&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üó£Ô∏è Voice AI Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/voice_ai_agents/ai_audio_tour_agent/"&gt;üó£Ô∏è AI Audio Tour Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/voice_ai_agents/customer_support_voice_agent/"&gt;üìû Customer Support Voice Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/voice_ai_agents/voice_rag_openaisdk/"&gt;üîä Voice RAG Agent (OpenAI SDK)&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üåê MCP AI Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/mcp_ai_agents/browser_mcp_agent/"&gt;‚ôæÔ∏è Browser MCP Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/mcp_ai_agents/github_mcp_agent/"&gt;üêô GitHub MCP Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/mcp_ai_agents/notion_mcp_agent"&gt;üìë Notion MCP Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/mcp_ai_agents/ai_travel_planner_mcp_agent_team"&gt;üåç AI Travel Planner MCP Agent&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üìÄ RAG (Retrieval Augmented Generation)&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/agentic_rag/"&gt;üîó Agentic RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/agentic_rag_with_reasoning/"&gt;üßê Agentic RAG with Reasoning&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/ai_blog_search/"&gt;üì∞ AI Blog Search (RAG)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/autonomous_rag/"&gt;üîç Autonomous RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/corrective_rag/"&gt;üîÑ Corrective RAG (CRAG)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/deepseek_local_rag_agent/"&gt;üêã Deepseek Local RAG Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/gemini_agentic_rag/"&gt;ü§î Gemini Agentic RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/hybrid_search_rag/"&gt;üëÄ Hybrid Search RAG (Cloud)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/llama3.1_local_rag/"&gt;üîÑ Llama 3.1 Local RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/local_hybrid_search_rag/"&gt;üñ•Ô∏è Local Hybrid Search RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/local_rag_agent/"&gt;ü¶ô Local RAG Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/rag-as-a-service/"&gt;üß© RAG-as-a-Service&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/rag_agent_cohere/"&gt;‚ú® RAG Agent with Cohere&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/rag_chain/"&gt;‚õìÔ∏è Basic RAG Chain&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/rag_database_routing/"&gt;üì† RAG with Database Routing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/vision_rag/"&gt;üñºÔ∏è Vision RAG&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üíæ LLM Apps with Memory Tutorials&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/ai_arxiv_agent_memory/"&gt;üíæ AI ArXiv Agent with Memory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/ai_travel_agent_memory/"&gt;üõ©Ô∏è AI Travel Agent with Memory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/llama3_stateful_chat/"&gt;üí¨ Llama3 Stateful Chat&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/llm_app_personalized_memory/"&gt;üìù LLM App with Personalized Memory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/local_chatgpt_with_memory/"&gt;üóÑÔ∏è Local ChatGPT Clone with Memory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/multi_llm_memory/"&gt;üß† Multi-LLM Application with Shared Memory&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üí¨ Chat with X Tutorials&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_github/"&gt;üí¨ Chat with GitHub (GPT &amp;amp; Llama3)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_gmail/"&gt;üì® Chat with Gmail&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_pdf/"&gt;üìÑ Chat with PDF (GPT &amp;amp; Llama3)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_research_papers/"&gt;üìö Chat with Research Papers (ArXiv) (GPT &amp;amp; Llama3)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_substack/"&gt;üìù Chat with Substack&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_youtube_videos/"&gt;üìΩÔ∏è Chat with YouTube Videos&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üîß LLM Fine-tuning Tutorials&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_finetuning_tutorials/llama3.2_finetuning/"&gt;üîß Llama 3.2 Fine-tuning&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üöÄ Getting Started&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Clone the repository&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/Shubhamsaboo/awesome-llm-apps.git 
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Navigate to the desired project directory&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cd awesome-llm-apps/starter_ai_agents/ai_travel_agent
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install the required dependencies&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Follow the project-specific instructions&lt;/strong&gt; in each project's &lt;code&gt;README.md&lt;/code&gt; file to set up and run the app.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;ü§ù Contributing to Open Source&lt;/h2&gt; 
&lt;p&gt;Contributions are welcome! If you have any ideas, improvements, or new apps to add, please create a new &lt;a href="https://github.com/Shubhamsaboo/awesome-llm-apps/issues"&gt;GitHub Issue&lt;/a&gt; or submit a pull request. Make sure to follow the existing project structure and include a detailed &lt;code&gt;README.md&lt;/code&gt; for each new app.&lt;/p&gt; 
&lt;h3&gt;Thank You, Community, for the Support! üôè&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#Shubhamsaboo/awesome-llm-apps&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=Shubhamsaboo/awesome-llm-apps&amp;amp;type=Date" alt="Star History Chart"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;üåü &lt;strong&gt;Don‚Äôt miss out on future updates! Star the repo now and be the first to know about new and exciting LLM apps with RAG and AI Agents.&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>google/adk-python</title>
      <link>https://github.com/google/adk-python</link>
      <description>&lt;p&gt;An open-source, code-first Python toolkit for building, evaluating, and deploying sophisticated AI agents with flexibility and control.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Agent Development Kit (ADK)&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/google/adk-python/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/License-Apache_2.0-blue.svg?sanitize=true" alt="License"&gt;&lt;/a&gt; &lt;a href="https://github.com/google/adk-python/actions/workflows/python-unit-tests.yml"&gt;&lt;img src="https://github.com/google/adk-python/actions/workflows/python-unit-tests.yml/badge.svg?sanitize=true" alt="Python Unit Tests"&gt;&lt;/a&gt; &lt;a href="https://www.reddit.com/r/agentdevelopmentkit/"&gt;&lt;img src="https://img.shields.io/badge/Reddit-r%2Fagentdevelopmentkit-FF4500?style=flat&amp;amp;logo=reddit&amp;amp;logoColor=white" alt="r/agentdevelopmentkit"&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/google/adk-python"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki"&gt;&lt;/a&gt;&lt;/p&gt;  
&lt;h2 align="center"&gt; &lt;img src="https://raw.githubusercontent.com/google/adk-python/main/assets/agent-development-kit.png" width="256"&gt; &lt;/h2&gt; 
&lt;h3 align="center"&gt; An open-source, code-first Python toolkit for building, evaluating, and deploying sophisticated AI agents with flexibility and control. &lt;/h3&gt; 
&lt;h3 align="center"&gt; Important Links: &lt;a href="https://google.github.io/adk-docs/"&gt;Docs&lt;/a&gt;, &lt;a href="https://github.com/google/adk-samples"&gt;Samples&lt;/a&gt;, &lt;a href="https://github.com/google/adk-java"&gt;Java ADK&lt;/a&gt; &amp;amp; &lt;a href="https://github.com/google/adk-web"&gt;ADK Web&lt;/a&gt;. &lt;/h3&gt;  
&lt;p&gt;Agent Development Kit (ADK) is a flexible and modular framework for developing and deploying AI agents. While optimized for Gemini and the Google ecosystem, ADK is model-agnostic, deployment-agnostic, and is built for compatibility with other frameworks. ADK was designed to make agent development feel more like software development, to make it easier for developers to create, deploy, and orchestrate agentic architectures that range from simple tasks to complex workflows.&lt;/p&gt; 
&lt;hr&gt; 
&lt;h2&gt;‚ú® Key Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Rich Tool Ecosystem&lt;/strong&gt;: Utilize pre-built tools, custom functions, OpenAPI specs, or integrate existing tools to give agents diverse capabilities, all for tight integration with the Google ecosystem.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Code-First Development&lt;/strong&gt;: Define agent logic, tools, and orchestration directly in Python for ultimate flexibility, testability, and versioning.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Modular Multi-Agent Systems&lt;/strong&gt;: Design scalable applications by composing multiple specialized agents into flexible hierarchies.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Deploy Anywhere&lt;/strong&gt;: Easily containerize and deploy agents on Cloud Run or scale seamlessly with Vertex AI Agent Engine.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ü§ñ Agent2Agent (A2A) Protocol and ADK Integration&lt;/h2&gt; 
&lt;p&gt;For remote agent-to-agent communication, ADK integrates with the &lt;a href="https://github.com/google-a2a/A2A/"&gt;A2A protocol&lt;/a&gt;. See this &lt;a href="https://github.com/a2aproject/a2a-samples/tree/main/samples/python/agents"&gt;example&lt;/a&gt; for how they can work together.&lt;/p&gt; 
&lt;h2&gt;üöÄ Installation&lt;/h2&gt; 
&lt;h3&gt;Stable Release (Recommended)&lt;/h3&gt; 
&lt;p&gt;You can install the latest stable version of ADK using &lt;code&gt;pip&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install google-adk
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The release cadence is weekly.&lt;/p&gt; 
&lt;p&gt;This version is recommended for most users as it represents the most recent official release.&lt;/p&gt; 
&lt;h3&gt;Development Version&lt;/h3&gt; 
&lt;p&gt;Bug fixes and new features are merged into the main branch on GitHub first. If you need access to changes that haven't been included in an official PyPI release yet, you can install directly from the main branch:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install git+https://github.com/google/adk-python.git@main
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note: The development version is built directly from the latest code commits. While it includes the newest fixes and features, it may also contain experimental changes or bugs not present in the stable release. Use it primarily for testing upcoming changes or accessing critical fixes before they are officially released.&lt;/p&gt; 
&lt;h2&gt;üìö Documentation&lt;/h2&gt; 
&lt;p&gt;Explore the full documentation for detailed guides on building, evaluating, and deploying agents:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://google.github.io/adk-docs"&gt;Documentation&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üèÅ Feature Highlight&lt;/h2&gt; 
&lt;h3&gt;Define a single agent:&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from google.adk.agents import Agent
from google.adk.tools import google_search

root_agent = Agent(
    name="search_assistant",
    model="gemini-2.0-flash", # Or your preferred Gemini model
    instruction="You are a helpful assistant. Answer user questions using Google Search when needed.",
    description="An assistant that can search the web.",
    tools=[google_search]
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Define a multi-agent system:&lt;/h3&gt; 
&lt;p&gt;Define a multi-agent system with coordinator agent, greeter agent, and task execution agent. Then ADK engine and the model will guide the agents works together to accomplish the task.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from google.adk.agents import LlmAgent, BaseAgent

# Define individual agents
greeter = LlmAgent(name="greeter", model="gemini-2.0-flash", ...)
task_executor = LlmAgent(name="task_executor", model="gemini-2.0-flash", ...)

# Create parent agent and assign children via sub_agents
coordinator = LlmAgent(
    name="Coordinator",
    model="gemini-2.0-flash",
    description="I coordinate greetings and tasks.",
    sub_agents=[ # Assign sub_agents here
        greeter,
        task_executor
    ]
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Development UI&lt;/h3&gt; 
&lt;p&gt;A built-in development UI to help you test, evaluate, debug, and showcase your agent(s).&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/google/adk-python/main/assets/adk-web-dev-ui-function-call.png"&gt; 
&lt;h3&gt;Evaluate Agents&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;adk eval \
    samples_for_testing/hello_world \
    samples_for_testing/hello_world/hello_world_eval_set_001.evalset.json
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ü§ù Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions from the community! Whether it's bug reports, feature requests, documentation improvements, or code contributions, please see our&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://google.github.io/adk-docs/contributing-guide/"&gt;General contribution guideline and flow&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Then if you want to contribute code, please read &lt;a href="https://raw.githubusercontent.com/google/adk-python/main/CONTRIBUTING.md"&gt;Code Contributing Guidelines&lt;/a&gt; to get started.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Vibe Coding&lt;/h2&gt; 
&lt;p&gt;If you are to develop agent via vibe coding the &lt;a href="https://raw.githubusercontent.com/google/adk-python/main/llms.txt"&gt;llms.txt&lt;/a&gt; and the &lt;a href="https://raw.githubusercontent.com/google/adk-python/main/llms-full.txt"&gt;llms-full.txt&lt;/a&gt; can be used as context to LLM. While the former one is a summarized one and the later one has the full information in case your LLM has big enough context window.&lt;/p&gt; 
&lt;h2&gt;üìÑ License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the Apache 2.0 License - see the &lt;a href="https://raw.githubusercontent.com/google/adk-python/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;hr&gt; 
&lt;p&gt;&lt;em&gt;Happy Agent Building!&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>microsoft/mcp-for-beginners</title>
      <link>https://github.com/microsoft/mcp-for-beginners</link>
      <description>&lt;p&gt;This open-source curriculum introduces the fundamentals of Model Context Protocol (MCP) through real-world, cross-language examples in .NET, Java, TypeScript, JavaScript, and Python. Designed for developers, it focuses on practical techniques for building modular, scalable, and secure AI workflows from session setup to service orchestration.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/images/mcp-beginners.png" alt="MCP-for-beginners"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://GitHub.com/microsoft/mcp-for-beginners/graphs/contributors"&gt;&lt;img src="https://img.shields.io/github/contributors/microsoft/mcp-for-beginners.svg?sanitize=true" alt="GitHub contributors"&gt;&lt;/a&gt; &lt;a href="https://GitHub.com/microsoft/mcp-for-beginners/issues"&gt;&lt;img src="https://img.shields.io/github/issues/microsoft/mcp-for-beginners.svg?sanitize=true" alt="GitHub issues"&gt;&lt;/a&gt; &lt;a href="https://GitHub.com/microsoft/mcp-for-beginners/pulls"&gt;&lt;img src="https://img.shields.io/github/issues-pr/microsoft/mcp-for-beginners.svg?sanitize=true" alt="GitHub pull-requests"&gt;&lt;/a&gt; &lt;a href="http://makeapullrequest.com"&gt;&lt;img src="https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square" alt="PRs Welcome"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://GitHub.com/microsoft/mcp-for-beginners/watchers"&gt;&lt;img src="https://img.shields.io/github/watchers/microsoft/mcp-for-beginners.svg?style=social&amp;amp;label=Watch" alt="GitHub watchers"&gt;&lt;/a&gt; &lt;a href="https://GitHub.com/microsoft/mcp-for-beginners/fork"&gt;&lt;img src="https://img.shields.io/github/forks/microsoft/mcp-for-beginners.svg?style=social&amp;amp;label=Fork" alt="GitHub forks"&gt;&lt;/a&gt; &lt;a href="https://GitHub.com/microsoft/mcp-for-beginners/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/microsoft/mcp-for-beginners?style=social&amp;amp;label=Star" alt="GitHub stars"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://discord.com/invite/ByRwuEEgH4"&gt;&lt;img src="https://dcbadge.limes.pink/api/server/ByRwuEEgH4" alt="Microsoft Azure AI Foundry Discord"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Follow these steps to get started using these resources:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Fork the Repository&lt;/strong&gt;: Click &lt;a href="https://GitHub.com/microsoft/mcp-for-beginners/fork"&gt;&lt;img src="https://img.shields.io/github/forks/microsoft/mcp-for-beginners.svg?style=social&amp;amp;label=Fork" alt="GitHub forks"&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Clone the Repository&lt;/strong&gt;: &lt;code&gt;git clone https://github.com/microsoft/mcp-for-beginners.git&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://discord.com/invite/ByRwuEEgH4"&gt;&lt;strong&gt;Join The Azure AI Foundry Discord and meet experts and fellow developers&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;üåê Multi-Language Support&lt;/h3&gt; 
&lt;h4&gt;Supported via GitHub Action (Automated &amp;amp; Always Up-to-Date)&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/translations/fr/README.md"&gt;French&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/translations/es/README.md"&gt;Spanish&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/translations/de/README.md"&gt;German&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/translations/ru/README.md"&gt;Russian&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/translations/ar/README.md"&gt;Arabic&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/translations/fa/README.md"&gt;Persian (Farsi)&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/translations/ur/README.md"&gt;Urdu&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/translations/zh/README.md"&gt;Chinese (Simplified)&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/translations/mo/README.md"&gt;Chinese (Traditional, Macau)&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/translations/hk/README.md"&gt;Chinese (Traditional, Hong Kong)&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/translations/tw/README.md"&gt;Chinese (Traditional, Taiwan)&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/translations/ja/README.md"&gt;Japanese&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/translations/ko/README.md"&gt;Korean&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/translations/hi/README.md"&gt;Hindi&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/translations/bn/README.md"&gt;Bengali&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/translations/mr/README.md"&gt;Marathi&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/translations/ne/README.md"&gt;Nepali&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/translations/pa/README.md"&gt;Punjabi (Gurmukhi)&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/translations/pt/README.md"&gt;Portuguese (Portugal)&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/translations/br/README.md"&gt;Portuguese (Brazil)&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/translations/it/README.md"&gt;Italian&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/translations/pl/README.md"&gt;Polish&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/translations/tr/README.md"&gt;Turkish&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/translations/el/README.md"&gt;Greek&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/translations/th/README.md"&gt;Thai&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/translations/sv/README.md"&gt;Swedish&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/translations/da/README.md"&gt;Danish&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/translations/no/README.md"&gt;Norwegian&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/translations/fi/README.md"&gt;Finnish&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/translations/nl/README.md"&gt;Dutch&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/translations/he/README.md"&gt;Hebrew&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/translations/vi/README.md"&gt;Vietnamese&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/translations/id/README.md"&gt;Indonesian&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/translations/ms/README.md"&gt;Malay&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/translations/tl/README.md"&gt;Tagalog (Filipino)&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/translations/sw/README.md"&gt;Swahili&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/translations/hu/README.md"&gt;Hungarian&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/translations/cs/README.md"&gt;Czech&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/translations/sk/README.md"&gt;Slovak&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/translations/ro/README.md"&gt;Romanian&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/translations/bg/README.md"&gt;Bulgarian&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/translations/sr/README.md"&gt;Serbian (Cyrillic)&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/translations/hr/README.md"&gt;Croatian&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/translations/sl/README.md"&gt;Slovenian&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/translations/uk/README.md"&gt;Ukrainian&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/translations/my/README.md"&gt;Burmese (Myanmar)&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;üöÄ Model Context Protocol (MCP) Curriculum for Beginners&lt;/h1&gt; 
&lt;h2&gt;&lt;strong&gt;Learn MCP with Hands-on Code Examples in C#, Java, JavaScript, Python, and TypeScript&lt;/strong&gt;&lt;/h2&gt; 
&lt;h2&gt;üß† Overview of the Model Context Protocol Curriculum&lt;/h2&gt; 
&lt;p&gt;The &lt;strong&gt;Model Context Protocol (MCP)&lt;/strong&gt; is a cutting-edge framework designed to standardize interactions between AI models and client applications. This open-source curriculum offers a structured learning path, complete with practical coding examples and real-world use cases, across popular programming languages including C#, Java, JavaScript, TypeScript, and Python.&lt;/p&gt; 
&lt;p&gt;Whether you're an AI developer, system architect, or software engineer, this guide is your comprehensive resource for mastering MCP fundamentals and implementation strategies.&lt;/p&gt; 
&lt;h2&gt;üîó Official MCP Resources&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìò &lt;a href="https://modelcontextprotocol.io/"&gt;MCP Documentation&lt;/a&gt; ‚Äì Detailed tutorials and user guides&lt;/li&gt; 
 &lt;li&gt;üìú &lt;a href="https://modelcontextprotocol.io/docs/"&gt;MCP Specification&lt;/a&gt; ‚Äì Protocol architecture and technical references&lt;/li&gt; 
 &lt;li&gt;üìú &lt;a href="https://spec.modelcontextprotocol.io/"&gt;Original MCP Specification&lt;/a&gt; ‚Äì Legacy technical references (may contain additional details)&lt;/li&gt; 
 &lt;li&gt;üßë‚Äçüíª &lt;a href="https://github.com/modelcontextprotocol"&gt;MCP GitHub Repository&lt;/a&gt; ‚Äì Open-source SDKs, tools, and code samples&lt;/li&gt; 
 &lt;li&gt;üåê &lt;a href="https://github.com/orgs/modelcontextprotocol/discussions"&gt;MCP Community&lt;/a&gt; ‚Äì Join discussions and contribute to the community&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Join us for MCP Dev Days 29-30th July 2025&lt;/h2&gt; 
&lt;p&gt;Get ready for two days of deep technical insight, community connection, and hands-on learning at MCP Dev Days, a virtual event dedicated to the Model Context Protocol (MCP) ‚Äî the emerging standard that bridges AI models and the tools they rely on.&lt;/p&gt; 
&lt;p&gt;‚û°Ô∏è &lt;a href="https://developer.microsoft.com/en-us/reactor/series/S-1563/"&gt;Register for MCP Dev Days&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;You can watch MCP Dev Days by registering on our event page: &lt;a href="https://aka.ms/mcpdevdays"&gt;https://aka.ms/mcpdevdays&lt;/a&gt;. From there, you‚Äôll be able to join a live stream on YouTube or Twitch. All of the content is recorded and will be available afterwards on the Microsoft Developer YouTube channel. Source code for the demos will be available on GitHub too.&lt;/p&gt; 
&lt;h3&gt;Event Details&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Dates: July 29 (Day 1) &amp;amp; July 30 (Day 2)&lt;/li&gt; 
 &lt;li&gt;Time: 9:00 AM PST daily&lt;/li&gt; 
 &lt;li&gt;Where: Online ‚Äì join from anywhere!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Day 1: MCP Productivity, DevTools, &amp;amp; Community:&lt;/h4&gt; 
&lt;p&gt;Is all about empowering developers to use MCP in their developer workflow and celebrating the amazing MCP community. We‚Äôll be joined with community members and partners such as Arcade, Block, Okta, and Neon to see how they are collaborating with Microsoft to shape an open, extensible MCP ecosystem. Real-world demos across VS Code, Visual Studio, GitHub Copilot, and popular community tools Practical, context-driven dev workflows Community-led sessions and insights Whether you‚Äôre just getting started with MCP or already building with it, Day 1 will set the stage with inspiration and actionable takeaways.&lt;/p&gt; 
&lt;h4&gt;Day 2: Build MCP Servers with Confidence&lt;/h4&gt; 
&lt;p&gt;Is for MCP builders. We‚Äôll go deep into implementation strategies and best practices for creating MCP servers and integrating MCP into your AI workflows.&lt;/p&gt; 
&lt;h3&gt;Topics include:&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Building MCP Servers and integrating them into agent experiences&lt;/li&gt; 
 &lt;li&gt;Prompt-driven development&lt;/li&gt; 
 &lt;li&gt;Security best practices&lt;/li&gt; 
 &lt;li&gt;Using building blocks like Functions, ACA, and API Management&lt;/li&gt; 
 &lt;li&gt;Registry alignment and tooling (1P + 3P)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If you‚Äôre a developer, tool builder, or AI product strategist, this day is packed with the insights you need to build scalable, secure, and future-ready MCP solutions.&lt;/p&gt; 
&lt;h2&gt;üß≠ MCP Curriculum Overview&lt;/h2&gt; 
&lt;h3&gt;üìö Complete Curriculum Structure&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Module&lt;/th&gt; 
   &lt;th&gt;Topic&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Link&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Module 1-3: Fundamentals&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;00&lt;/td&gt; 
   &lt;td&gt;Introduction to MCP&lt;/td&gt; 
   &lt;td&gt;Overview of the Model Context Protocol and its significance in AI pipelines&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/00-Introduction/README.md"&gt;Read more&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;01&lt;/td&gt; 
   &lt;td&gt;Core Concepts Explained&lt;/td&gt; 
   &lt;td&gt;In-depth exploration of core MCP concepts&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/01-CoreConcepts/README.md"&gt;Read more&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;02&lt;/td&gt; 
   &lt;td&gt;Security in MCP&lt;/td&gt; 
   &lt;td&gt;Security threats and best practices&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/02-Security/README.md"&gt;Read more&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;03&lt;/td&gt; 
   &lt;td&gt;Getting Started with MCP&lt;/td&gt; 
   &lt;td&gt;Environment setup, basic servers/clients, integration&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/03-GettingStarted/README.md"&gt;Read more&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Module 3: Building Your First Server &amp;amp; Client&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;3.1&lt;/td&gt; 
   &lt;td&gt;First Server&lt;/td&gt; 
   &lt;td&gt;Create your first MCP server&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/03-GettingStarted/01-first-server/README.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;3.2&lt;/td&gt; 
   &lt;td&gt;First Client&lt;/td&gt; 
   &lt;td&gt;Develop a basic MCP client&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/03-GettingStarted/02-client/README.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;3.3&lt;/td&gt; 
   &lt;td&gt;Client with LLM&lt;/td&gt; 
   &lt;td&gt;Integrate large language models&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/03-GettingStarted/03-llm-client/README.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;3.4&lt;/td&gt; 
   &lt;td&gt;VS Code Integration&lt;/td&gt; 
   &lt;td&gt;Consume MCP servers in VS Code&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/03-GettingStarted/04-vscode/README.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;3.5&lt;/td&gt; 
   &lt;td&gt;SSE Server&lt;/td&gt; 
   &lt;td&gt;Create servers using Server-Sent Events&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/03-GettingStarted/05-sse-server/README.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;3.6&lt;/td&gt; 
   &lt;td&gt;HTTP Streaming&lt;/td&gt; 
   &lt;td&gt;Implement HTTP streaming in MCP&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/03-GettingStarted/06-http-streaming/README.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;3.7&lt;/td&gt; 
   &lt;td&gt;AI Toolkit&lt;/td&gt; 
   &lt;td&gt;Use AI Toolkit with MCP&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/03-GettingStarted/07-aitk/README.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;3.8&lt;/td&gt; 
   &lt;td&gt;Testing&lt;/td&gt; 
   &lt;td&gt;Test your MCP server implementation&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/03-GettingStarted/08-testing/README.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;3.9&lt;/td&gt; 
   &lt;td&gt;Deployment&lt;/td&gt; 
   &lt;td&gt;Deploy MCP servers to production&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/03-GettingStarted/09-deployment/README.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Module 4-5: Practical &amp;amp; Advanced&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;04&lt;/td&gt; 
   &lt;td&gt;Practical Implementation&lt;/td&gt; 
   &lt;td&gt;SDKs, debugging, testing, reusable prompt templates&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/04-PracticalImplementation/README.md"&gt;Read more&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;05&lt;/td&gt; 
   &lt;td&gt;Advanced Topics in MCP&lt;/td&gt; 
   &lt;td&gt;Multi-modal AI, scaling, enterprise use&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/05-AdvancedTopics/README.md"&gt;Read more&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5.1&lt;/td&gt; 
   &lt;td&gt;Azure Integration&lt;/td&gt; 
   &lt;td&gt;MCP Integration with Azure&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/05-AdvancedTopics/mcp-integration/README.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5.2&lt;/td&gt; 
   &lt;td&gt;Multi-modality&lt;/td&gt; 
   &lt;td&gt;Working with multiple modalities&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/05-AdvancedTopics/mcp-multi-modality/README.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5.3&lt;/td&gt; 
   &lt;td&gt;OAuth2 Demo&lt;/td&gt; 
   &lt;td&gt;Implement OAuth2 authentication&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/05-AdvancedTopics/mcp-oauth2-demo/README.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5.4&lt;/td&gt; 
   &lt;td&gt;Root Contexts&lt;/td&gt; 
   &lt;td&gt;Understand and implement root contexts&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/05-AdvancedTopics/mcp-root-contexts/README.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5.5&lt;/td&gt; 
   &lt;td&gt;Routing&lt;/td&gt; 
   &lt;td&gt;MCP routing strategies&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/05-AdvancedTopics/mcp-routing/README.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5.6&lt;/td&gt; 
   &lt;td&gt;Sampling&lt;/td&gt; 
   &lt;td&gt;Sampling techniques in MCP&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/05-AdvancedTopics/mcp-sampling/README.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5.7&lt;/td&gt; 
   &lt;td&gt;Scaling&lt;/td&gt; 
   &lt;td&gt;Scale MCP implementations&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/05-AdvancedTopics/mcp-scaling/README.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5.8&lt;/td&gt; 
   &lt;td&gt;Security&lt;/td&gt; 
   &lt;td&gt;Advanced security considerations&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/05-AdvancedTopics/mcp-security/README.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5.9&lt;/td&gt; 
   &lt;td&gt;Web Search&lt;/td&gt; 
   &lt;td&gt;Implement web search capabilities&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/05-AdvancedTopics/web-search-mcp/README.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5.10&lt;/td&gt; 
   &lt;td&gt;Realtime Streaming&lt;/td&gt; 
   &lt;td&gt;Build realtime streaming functionality&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/05-AdvancedTopics/mcp-realtimestreaming/README.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5.11&lt;/td&gt; 
   &lt;td&gt;Realtime Search&lt;/td&gt; 
   &lt;td&gt;Implement realtime search&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/05-AdvancedTopics/mcp-realtimesearch/README.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5.12&lt;/td&gt; 
   &lt;td&gt;Entra ID Auth&lt;/td&gt; 
   &lt;td&gt;Authentication with Microsoft Entra ID&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/05-AdvancedTopics/mcp-security-entra/README.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5.13&lt;/td&gt; 
   &lt;td&gt;Foundry Integration&lt;/td&gt; 
   &lt;td&gt;Integrate with Azure AI Foundry&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/05-AdvancedTopics/mcp-foundry-agent-integration/README.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5.14&lt;/td&gt; 
   &lt;td&gt;Context Engineering&lt;/td&gt; 
   &lt;td&gt;Techniques for effective context engineering&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/05-AdvancedTopics/mcp-contextengineering/README.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Module 6-10: Community &amp;amp; Best Practices&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;06&lt;/td&gt; 
   &lt;td&gt;Community Contributions&lt;/td&gt; 
   &lt;td&gt;How to contribute to the MCP ecosystem&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/06-CommunityContributions/README.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;07&lt;/td&gt; 
   &lt;td&gt;Insights from Early Adoption&lt;/td&gt; 
   &lt;td&gt;Real-world implementation stories&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/07-LessonsFromEarlyAdoption/README.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;08&lt;/td&gt; 
   &lt;td&gt;Best Practices for MCP&lt;/td&gt; 
   &lt;td&gt;Performance, fault-tolerance, resilience&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/08-BestPractices/README.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;09&lt;/td&gt; 
   &lt;td&gt;MCP Case Studies&lt;/td&gt; 
   &lt;td&gt;Practical implementation examples&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/09-CaseStudy/README.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;10&lt;/td&gt; 
   &lt;td&gt;Hands-on Workshop&lt;/td&gt; 
   &lt;td&gt;Building an MCP Server with AI Toolkit&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/10-StreamliningAIWorkflowsBuildingAnMCPServerWithAIToolkit/README.md"&gt;Lab&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;üíª Sample Code Projects&lt;/h3&gt; 
&lt;h4&gt;Basic MCP Calculator Samples&lt;/h4&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Language&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Link&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;C#&lt;/td&gt; 
   &lt;td&gt;MCP Server Example&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/03-GettingStarted/samples/csharp/README.md"&gt;View Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Java&lt;/td&gt; 
   &lt;td&gt;MCP Calculator&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/03-GettingStarted/samples/java/calculator/README.md"&gt;View Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;JavaScript&lt;/td&gt; 
   &lt;td&gt;MCP Demo&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/03-GettingStarted/samples/javascript/README.md"&gt;View Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Python&lt;/td&gt; 
   &lt;td&gt;MCP Server&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/03-GettingStarted/samples/python/mcp_calculator_server.py"&gt;View Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;TypeScript&lt;/td&gt; 
   &lt;td&gt;MCP Example&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/03-GettingStarted/samples/typescript/README.md"&gt;View Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h4&gt;Advanced MCP Implementations&lt;/h4&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Language&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Link&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;C#&lt;/td&gt; 
   &lt;td&gt;Advanced Sample&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/04-PracticalImplementation/samples/csharp/README.md"&gt;View Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Java&lt;/td&gt; 
   &lt;td&gt;Container App Example&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/04-PracticalImplementation/samples/java/containerapp/README.md"&gt;View Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;JavaScript&lt;/td&gt; 
   &lt;td&gt;Advanced Sample&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/04-PracticalImplementation/samples/javascript/README.md"&gt;View Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Python&lt;/td&gt; 
   &lt;td&gt;Complex Implementation&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/04-PracticalImplementation/samples/python/mcp_sample.py"&gt;View Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;TypeScript&lt;/td&gt; 
   &lt;td&gt;Container Sample&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/04-PracticalImplementation/samples/typescript/README.md"&gt;View Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;üéØ Prerequisites for Learning MCP&lt;/h2&gt; 
&lt;p&gt;To get the most out of this curriculum, you should have:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Basic knowledge of programming in at least one of the following languages: C#, Java, JavaScript, Python, or TypeScript&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Understanding of client-server model and APIs&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Familiarity with REST and HTTP concepts&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;(Optional) Background in AI/ML concepts&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Joining our community discussions for support&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üìö Study Guide &amp;amp; Resources&lt;/h2&gt; 
&lt;p&gt;This repository includes several resources to help you navigate and learn effectively:&lt;/p&gt; 
&lt;h3&gt;Study Guide&lt;/h3&gt; 
&lt;p&gt;A comprehensive &lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/study_guide.md"&gt;Study Guide&lt;/a&gt; is available to help you navigate this repository effectively. The guide includes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;A visual curriculum map showing all topics covered&lt;/li&gt; 
 &lt;li&gt;Detailed breakdown of each repository section&lt;/li&gt; 
 &lt;li&gt;Guidance on how to use sample projects&lt;/li&gt; 
 &lt;li&gt;Recommended learning paths for different skill levels&lt;/li&gt; 
 &lt;li&gt;Additional resources to complement your learning journey&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Changelog&lt;/h3&gt; 
&lt;p&gt;We maintain a detailed &lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/changelog.md"&gt;Changelog&lt;/a&gt; that tracks all significant updates to the curriculum materials, including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;New content additions&lt;/li&gt; 
 &lt;li&gt;Structural changes&lt;/li&gt; 
 &lt;li&gt;Feature improvements&lt;/li&gt; 
 &lt;li&gt;Documentation updates&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üõ†Ô∏è How to Use This Curriculum Effectively&lt;/h2&gt; 
&lt;p&gt;Each lesson in this guide includes:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Clear explanations of MCP concepts&lt;/li&gt; 
 &lt;li&gt;Live code examples in multiple languages&lt;/li&gt; 
 &lt;li&gt;Exercises to build real MCP applications&lt;/li&gt; 
 &lt;li&gt;Extra resources for advanced learners&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;üåü Community Thanks&lt;/h2&gt; 
&lt;p&gt;Thanks to Microsoft Valued Professional &lt;a href="https://www.linkedin.com/in/shivam2003/"&gt;Shivam Goyal&lt;/a&gt; for contributing important code samples.&lt;/p&gt; 
&lt;h2&gt;üìú License Information&lt;/h2&gt; 
&lt;p&gt;This content is licensed under the &lt;strong&gt;MIT License&lt;/strong&gt;. For terms and conditions, see the &lt;a href="https://raw.githubusercontent.com/microsoft/mcp-for-beginners/main/LICENSE"&gt;LICENSE&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;ü§ù Contribution Guidelines&lt;/h2&gt; 
&lt;p&gt;This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href="https://cla.opensource.microsoft.com"&gt;https://cla.opensource.microsoft.com&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; 
&lt;p&gt;This project has adopted the &lt;a href="https://opensource.microsoft.com/codeofconduct/"&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href="https://opensource.microsoft.com/codeofconduct/faq/"&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href="mailto:opencode@microsoft.com"&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; 
&lt;h2&gt;üìÇ Repository Structure&lt;/h2&gt; 
&lt;p&gt;The repository is organized as follows:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Core Curriculum (00-10)&lt;/strong&gt;: The main content organized in ten sequential modules&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;images/&lt;/strong&gt;: Diagrams and illustrations used throughout the curriculum&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;translations/&lt;/strong&gt;: Multi-language support with automated translations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;translated_images/&lt;/strong&gt;: Localized versions of diagrams and illustrations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;study_guide.md&lt;/strong&gt;: Comprehensive guide to navigating the repository&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;changelog.md&lt;/strong&gt;: Record of all significant changes to the curriculum materials&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;mcp.json&lt;/strong&gt;: Configuration file for MCP specification&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;CODE_OF_CONDUCT.md, LICENSE, SECURITY.md, SUPPORT.md&lt;/strong&gt;: Project governance documents&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üéí Other Courses&lt;/h2&gt; 
&lt;p&gt;Our team produces other courses! Check out:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/microsoft/ai-agents-for-beginners?WT.mc_id=academic-105485-koreyst"&gt;AI Agents For Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/microsoft/Generative-AI-for-beginners-dotnet?WT.mc_id=academic-105485-koreyst"&gt;Generative AI for Beginners using .NET&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/microsoft/generative-ai-with-javascript?WT.mc_id=academic-105485-koreyst"&gt;Generative AI for Beginners using JavaScript&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/microsoft/generative-ai-for-beginners?WT.mc_id=academic-105485-koreyst"&gt;Generative AI for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/microsoft/generative-ai-for-beginners-java?WT.mc_id=academic-105485-koreyst"&gt;Generative AI for Beginners using Java&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aka.ms/ml-beginners?WT.mc_id=academic-105485-koreyst"&gt;ML for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aka.ms/datascience-beginners?WT.mc_id=academic-105485-koreyst"&gt;Data Science for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aka.ms/ai-beginners?WT.mc_id=academic-105485-koreyst"&gt;AI for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/microsoft/Security-101??WT.mc_id=academic-96948-sayoung"&gt;Cybersecurity for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aka.ms/webdev-beginners?WT.mc_id=academic-105485-koreyst"&gt;Web Dev for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aka.ms/iot-beginners?WT.mc_id=academic-105485-koreyst"&gt;IoT for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/microsoft/xr-development-for-beginners?WT.mc_id=academic-105485-koreyst"&gt;XR Development for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aka.ms/GitHubCopilotAI?WT.mc_id=academic-105485-koreyst"&gt;Mastering GitHub Copilot for AI Paired Programming&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/microsoft/mastering-github-copilot-for-dotnet-csharp-developers?WT.mc_id=academic-105485-koreyst"&gt;Mastering GitHub Copilot for C#/.NET Developers&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/microsoft/CopilotAdventures?WT.mc_id=academic-105485-koreyst"&gt;Choose Your Own Copilot Adventure&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;‚Ñ¢Ô∏è Trademark Notice&lt;/h2&gt; 
&lt;p&gt;This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow &lt;a href="https://www.microsoft.com/legal/intellectualproperty/trademarks/usage/general"&gt;Microsoft's Trademark &amp;amp; Brand Guidelines&lt;/a&gt;. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos is subject to those third-parties' policies.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>jingyaogong/minimind-v</title>
      <link>https://github.com/jingyaogong/minimind-v</link>
      <description>&lt;p&gt;üöÄ „ÄåÂ§ßÊ®°Âûã„Äç1Â∞èÊó∂‰ªé0ËÆ≠ÁªÉ26MÂèÇÊï∞ÁöÑËßÜËßâÂ§öÊ®°ÊÄÅVLMÔºÅüåè Train a 26M-parameter VLM from scratch in just 1 hours!&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind-v/master/images/logo.png" alt="logo"&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;img src="https://visitor-badge.laobi.icu/badge?page_id=jingyaogong/minimind-v" alt="visitors"&gt; &lt;a href="https://github.com/jingyaogong/minimind-v/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/jingyaogong/minimind-v?style=social" alt="GitHub Repo stars"&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/jingyaogong/minimind-v/master/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/jingyaogong/minimind-v?v=1" alt="GitHub Code License"&gt;&lt;/a&gt; &lt;a href="https://github.com/jingyaogong/minimind-v/commits/master"&gt;&lt;img src="https://img.shields.io/github/last-commit/jingyaogong/minimind-v" alt="GitHub last commit"&gt;&lt;/a&gt; &lt;a href="https://github.com/jingyaogong/minimind-v/pulls"&gt;&lt;img src="https://img.shields.io/badge/PRs-welcome-blue" alt="GitHub pull request"&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/collections/jingyaogong/minimind-v-67000833fb60b3a2e1f3597d"&gt;&lt;img src="https://img.shields.io/badge/%F0%9F%A4%97-MiniMindV%20%20Collection-blue" alt="Collection"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;h3&gt;"Â§ßÈÅìËá≥ÁÆÄ"&lt;/h3&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;‰∏≠Êñá | &lt;a href="https://raw.githubusercontent.com/jingyaogong/minimind-v/master/README_en.md"&gt;English&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;ul&gt; 
 &lt;li&gt;Ê≠§È°πÁõÆÊó®Âú®‰ªé0ÂºÄÂßãÔºå‰ªÖÁî®1.3ÂùóÈí±ÊàêÊú¨ + 1Â∞èÊó∂ÔºÅÂç≥ÂèØËÆ≠ÁªÉÂá∫26MÂèÇÊï∞ÁöÑË∂ÖÂ∞èÂ§öÊ®°ÊÄÅËßÜËßâËØ≠Ë®ÄÊ®°Âûã&lt;strong&gt;MiniMind-V&lt;/strong&gt;„ÄÇ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MiniMind-V&lt;/strong&gt;ÊúÄÂ∞èÁâàÊú¨‰ΩìÁßØ‰ªÖ‰∏∫ GPT3 ÁöÑÁ∫¶ $\frac{1}{7000}$ÔºåÂäõÊ±ÇÂÅöÂà∞‰∏™‰∫∫GPU‰πüÂèØÂø´ÈÄüÊé®ÁêÜÁîöËá≥ËÆ≠ÁªÉ„ÄÇ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MiniMind-V&lt;/strong&gt;ÊòØ&lt;a href="https://github.com/jingyaogong/minimind"&gt;MiniMind&lt;/a&gt;Á∫ØËØ≠Ë®ÄÊ®°ÂûãÁöÑËßÜËßâËÉΩÂäõÈ¢ùÂ§ñÊãìÂ±ï„ÄÇ&lt;/li&gt; 
 &lt;li&gt;È°πÁõÆÂêåÊó∂ÂåÖÂê´‰∫ÜVLMÂ§ßÊ®°ÂûãÁöÑÊûÅÁÆÄÁªìÊûÑ„ÄÅÊï∞ÊçÆÈõÜÊ∏ÖÊ¥ó„ÄÅÈ¢ÑËÆ≠ÁªÉ(Pretrain)„ÄÅÁõëÁù£ÂæÆË∞É(SFT)Á≠âÂÖ®ËøáÁ®ã‰ª£Á†Å„ÄÇ&lt;/li&gt; 
 &lt;li&gt;Ëøô‰∏ç‰ªÖÊòØ‰∏Ä‰∏™ÂºÄÊ∫êVLMÊ®°ÂûãÁöÑÊúÄÂ∞èÂÆûÁé∞Ôºå‰πüÊòØÂÖ•Èó®ËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁöÑÁÆÄÊòéÊïôÁ®ã„ÄÇ&lt;/li&gt; 
 &lt;li&gt;Â∏åÊúõÊ≠§È°πÁõÆËÉΩ‰∏∫ÊâÄÊúâ‰∫∫Êèê‰æõ‰∏Ä‰∏™ÊäõÁ†ñÂºïÁéâÁöÑÁ§∫‰æãÔºå‰∏ÄËµ∑ÊÑüÂèóÂàõÈÄ†ÁöÑ‰πêË∂£ÔºÅÊé®Âä®Êõ¥ÂπøÊ≥õAIÁ§æÂå∫ÁöÑËøõÊ≠•ÔºÅ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;‰∏∫Èò≤Ê≠¢ËØØËß£Ôºå‚Äú1Â∞èÊó∂‚Äù Âü∫‰∫éNVIDIA 3090Á°¨‰ª∂ËÆæÂ§áÔºàÂçïÂç°ÔºâÊµãËØï&lt;code&gt;1 epoch&lt;/code&gt;Ôºå‚Äú1.3ÂùóÈí±‚Äù ÊåáGPUÊúçÂä°Âô®ÁßüÁî®ÊàêÊú¨„ÄÇ&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind-v/master/images/minimind2-v.gif" alt="minimind2-v"&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://www.modelscope.cn/studios/gongjy/MiniMind-V"&gt;üîóü§ñÂú®Á∫ø‰ΩìÈ™å&lt;/a&gt; | &lt;a href="https://www.bilibili.com/video/BV1Sh1vYBEzY"&gt;üîóüéûÔ∏èËßÜÈ¢ë‰ªãÁªç&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h1&gt;üìå Introduction&lt;/h1&gt; 
&lt;p&gt;‚ÄúÁî®‰πêÈ´òÊãºÂá∫‰∏ÄÊû∂È£ûÊú∫ÔºåËøúÊØîÂùêÂú®Â§¥Á≠âËà±ÈáåÈ£ûË°åÊõ¥ËÆ©‰∫∫ÂÖ¥Â•ãÔºÅ‚Äù ÊûÑÂª∫VLMËåÉÂºèÁöÑÂ§öÊ®°ÊÄÅÂ§ßÊ®°ÂûãÊòØÂê¶ÁúüÁöÑÂ¶ÇÊÉ≥Ë±°‰∏≠ÈÇ£Ê†∑Â§çÊùÇÔºüÂÆÉÁöÑ‰ª£Á†ÅÂÆûÁé∞Âà∞Â∫ïÂ¶Ç‰ΩïÔºü ËÆ≠ÁªÉËøáÁ®ãÁ©∂Á´üÈöæ‰∏çÈöæÔºüÈÇ£‰πàÁé∞Âú®ÔºåÊé¢Á¥¢ÂÆÉ‰ª¨ÁöÑÁ≠îÊ°àÔºå‰∏ÄËµ∑ÊÑüÂèóÂàõÈÄ†ÁöÑ‰πêË∂£ÂêßÔºÅ&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] ÔºàÊà™Ëá≥2025-02-20ÔºâMiniMind-V Á≥ªÂàóÂ∑≤ÂÆåÊàê‰∫Ü‰ª•‰∏ãÂûãÂè∑Ê®°ÂûãËÆ≠ÁªÉÔºåÊúÄÂ∞è‰ªÖÈúÄ26M (0.026B)ÔºåÂç≥ÂèØÂÖ∑Â§áËØÜÂõæÂíåÂØπËØùÁöÑËÉΩÂäõÔºÅ&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Ê®°Âûã (Â§ßÂ∞è)&lt;/th&gt; 
   &lt;th&gt;Êé®ÁêÜÂç†Áî®&lt;/th&gt; 
   &lt;th&gt;release&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniMind2-V (104M)&lt;/td&gt; 
   &lt;td&gt;0.6 GB&lt;/td&gt; 
   &lt;td&gt;2025.02.20&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MiniMind2-Small-V (26M)&lt;/td&gt; 
   &lt;td&gt;1.1 GB&lt;/td&gt; 
   &lt;td&gt;2025.02.20&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;minimind-v-v1-small (27M)&lt;/td&gt; 
   &lt;td&gt;0.6 GB&lt;/td&gt; 
   &lt;td&gt;2024.10.04&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;minimind-v-v1 (109M)&lt;/td&gt; 
   &lt;td&gt;1.1 GB&lt;/td&gt; 
   &lt;td&gt;2024.10.04&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;üëâ&lt;strong&gt;ÊúÄËøëÊõ¥Êñ∞&lt;/strong&gt;&lt;/h3&gt; 
&lt;details close&gt; 
 &lt;summary&gt; &lt;b&gt;2025-04-27 (newest üéâ)&lt;/b&gt; &lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;ÂÖºÂÆπÊÄßÊõ¥Êñ∞&lt;/li&gt; 
  &lt;li&gt;ÈÄÇÈÖç&lt;a href="https://github.com/jingyaogong/minimind/issues/370"&gt;„Äåminimind‰ªìÂ∫ìÊñ∞ÁâπÊÄß„Äç&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;ËßÑËåÉÂåñÈÉ®ÂàÜ‰ª£Á†Å&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details close&gt; 
 &lt;summary&gt; &lt;b&gt;2025-02-20&lt;/b&gt; &lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;MiniMind2-V‰º¥ÈöèMiniMind2ÂêåÊ≠•Êõ¥Êñ∞&lt;/li&gt; 
  &lt;li&gt;Â§ßÂπÖÂáèÂ∞ëÊâÄÊúâÂÜó‰Ωô‰ª£Á†ÅÔºåËßÑËåÉ‰ª£Á†ÅÊ†ºÂºè&lt;/li&gt; 
  &lt;li&gt;Â§ßÂπÖÁ≤æÁÆÄÊ®°ÂûãÂÜó‰ΩôÁªìÊûÑ&lt;/li&gt; 
  &lt;li&gt;Êõ¥Êñ∞Êï∞ÊçÆÈõÜÊ†ºÂºèÔºåÊãìÂ±ïÊñ∞ÁöÑSFTÊï∞ÊçÆÈõÜ&lt;/li&gt; 
  &lt;li&gt;ÊØîÂâç‰ª£VLMÊõ¥‰ºòÁßÄÁöÑÊïàÊûúÔºÅ&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details close&gt; 
 &lt;summary&gt; &lt;b&gt;2024-10-05&lt;/b&gt; &lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;MiniMind-VÂ¶ÇÊúüËÄåËá≥ÔºåÈ¶ñÊ¨°ÂºÄÊ∫ê&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h1&gt;üìå Âø´ÈÄüÂºÄÂßã&lt;/h1&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;ÂàÜ‰∫´Êú¨‰∫∫ÁöÑËΩØÁ°¨‰ª∂ÈÖçÁΩÆÔºà‰ªÖ‰æõÂèÇËÄÉÔºâ&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;CPU: Intel(R) Core(TM) i9-10980XE CPU @ 3.00GHz&lt;/li&gt; 
  &lt;li&gt;RAM: 128 GB&lt;/li&gt; 
  &lt;li&gt;GPU: NVIDIA GeForce RTX 3090(24GB) * 8&lt;/li&gt; 
  &lt;li&gt;Ubuntu==20.04&lt;/li&gt; 
  &lt;li&gt;CUDA==12.2&lt;/li&gt; 
  &lt;li&gt;Python==3.10.16&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/jingyaogong/minimind-v/master/requirements.txt"&gt;requirements.txt&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;Á¨¨0Ê≠•&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# ÂÖãÈöÜ‰ª£Á†Å‰ªìÂ∫ì
git clone https://github.com/jingyaogong/minimind-v
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# ‰∏ãËΩΩclipÊ®°ÂûãÂà∞ ./model/vision_model ÁõÆÂΩï‰∏ã
git clone https://huggingface.co/openai/clip-vit-base-patch16
# or
git clone https://www.modelscope.cn/models/openai-mirror/clip-vit-base-patch16
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# ‰∏ãËΩΩÁ∫ØËØ≠Ë®ÄÊ®°ÂûãÊùÉÈáçÂà∞ ./out ÁõÆÂΩï‰∏ãÔºà‰Ωú‰∏∫ËÆ≠ÁªÉVLMÁöÑÂü∫Â∫ßËØ≠Ë®ÄÊ®°ÂûãÔºâ
https://huggingface.co/jingyaogong/MiniMind2-V-PyTorch/blob/main/lm_512.pth
# or
https://huggingface.co/jingyaogong/MiniMind2-V-PyTorch/blob/main/lm_768.pth
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;‚Ö† ÊµãËØïÂ∑≤ÊúâÊ®°ÂûãÊïàÊûú&lt;/h2&gt; 
&lt;h3&gt;1.ÁéØÂ¢ÉÂáÜÂ§á&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2.‰∏ãËΩΩÊ®°Âûã&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://huggingface.co/jingyaogong/MiniMind2-V
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;3.ÂëΩ‰ª§Ë°åÈóÆÁ≠î&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# load=0: load from pytorch model, load=1: load from transformers-hf model
python eval_vlm.py --load 1
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;4.ÊàñÂêØÂä®WebUI&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python web_demo_vlm.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;‚Ö° ‰ªé0ÂºÄÂßãËá™Â∑±ËÆ≠ÁªÉ&lt;/h2&gt; 
&lt;h3&gt;1.ÁéØÂ¢ÉÂáÜÂ§á&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple
&lt;/code&gt;&lt;/pre&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;Ê≥®ÔºöÊèêÂâçÊµãËØïTorchÊòØÂê¶ÂèØÁî®cuda&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;import torch
print(torch.cuda.is_available())
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Â¶ÇÊûú‰∏çÂèØÁî®ÔºåËØ∑Ëá™Ë°åÂéª&lt;a href="https://download.pytorch.org/whl/torch_stable.html"&gt;torch_stable&lt;/a&gt; ‰∏ãËΩΩwhlÊñá‰ª∂ÂÆâË£Ö„ÄÇÂèÇËÄÉ&lt;a href="https://blog.csdn.net/weixin_45456738/article/details/141029610?ops_request_misc=&amp;amp;request_id=&amp;amp;biz_id=102&amp;amp;utm_term=%E5%AE%89%E8%A3%85torch&amp;amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-2-141029610.nonecase&amp;amp;spm=1018.2226.3001.4187"&gt;ÈìæÊé•&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;h3&gt;2.Êï∞ÊçÆ‰∏ãËΩΩ&lt;/h3&gt; 
&lt;p&gt;‰ªé‰∏ãÊñáÊèê‰æõÁöÑ&lt;a href="https://huggingface.co/datasets/jingyaogong/minimind-v_dataset"&gt;Êï∞ÊçÆÈõÜ‰∏ãËΩΩÈìæÊé•&lt;/a&gt; ‰∏ãËΩΩÈúÄË¶ÅÁöÑÊï∞ÊçÆÊñá‰ª∂ÔºàÂàõÂª∫&lt;code&gt;./dataset&lt;/code&gt;ÁõÆÂΩïÔºâÂπ∂ÊîæÂà∞&lt;code&gt;./dataset&lt;/code&gt;‰∏ã„ÄÇ&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;*.jsonl&lt;/code&gt;‰∏∫ÈóÆÁ≠îÊï∞ÊçÆÈõÜÔºå&lt;code&gt;*images&lt;/code&gt;‰∏∫ÈÖçÂ•óÁöÑÂõæÁâáÊï∞ÊçÆÔºå‰∏ãËΩΩÂÆåÊàêÂêéÈúÄË¶ÅËß£ÂéãÂõæÂÉèÊï∞ÊçÆ„ÄÇ&lt;/p&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;Ê≥®ÔºöÊï∞ÊçÆÈõÜÈ°ªÁü•&lt;/summary&gt; 
 &lt;p&gt;ËØ∑È¢ÑÁïô~5GBÁ©∫Èó¥Â≠òÊîæÊï∞ÊçÆÈõÜÔºåËã•Êó†Â§ö‰ΩôÁ©∫Èó¥Â≠òÊîæpretrainÊï∞ÊçÆÔºå ÂèØÂ∞ùËØïË∑≥ËøápretrainËÆ≠ÁªÉÊ≠•È™§Áõ¥Êé•ËøõË°åsftËÆ≠ÁªÉ„ÄÇ&lt;/p&gt; 
&lt;/details&gt; 
&lt;h3&gt;3.ÂºÄÂßãËÆ≠ÁªÉ&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;3.1 È¢ÑËÆ≠ÁªÉÔºàÂ≠¶ÂõæÂÉèÊèèËø∞Ôºâ&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python train_pretrain_vlm.py --epochs 4
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ÊâßË°åÈ¢ÑËÆ≠ÁªÉÔºåÂæóÂà∞ &lt;code&gt;pretrain_vlm_*.pth&lt;/code&gt; ‰Ωú‰∏∫È¢ÑËÆ≠ÁªÉÁöÑËæìÂá∫ÊùÉÈáçÔºàÂÖ∂‰∏≠*‰∏∫Ê®°ÂûãÁöÑdimensionÔºåÈªòËÆ§‰∏∫512Ôºâ&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;3.2 ÁõëÁù£ÂæÆË∞ÉÔºàÂ≠¶ÁúãÂõæÂØπËØùÊñπÂºèÔºâ&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python train_sft_vlm.py --epochs 4
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ÊâßË°åÁõëÁù£ÂæÆË∞ÉÔºåÂæóÂà∞ &lt;code&gt;sft_vlm_*.pth&lt;/code&gt; ‰Ωú‰∏∫Êåá‰ª§ÂæÆË∞ÉÁöÑËæìÂá∫ÊùÉÈáç&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;Ê≥®ÔºöËÆ≠ÁªÉÈ°ªÁü•&lt;/summary&gt; 
 &lt;p&gt;ÊâÄÊúâËÆ≠ÁªÉËøáÁ®ãÈªòËÆ§ÊØèÈöî100Ê≠•‰øùÂ≠ò1Ê¨°ÂèÇÊï∞Âà∞Êñá‰ª∂&lt;code&gt;./out/***.pth&lt;/code&gt;ÔºàÊØèÊ¨°‰ºöË¶ÜÁõñÊéâÊóßÊùÉÈáçÊñá‰ª∂Ôºâ„ÄÇ&lt;/p&gt; 
&lt;/details&gt; 
&lt;hr&gt; 
&lt;h3&gt;4.ÊµãËØïÊ®°ÂûãÊïàÊûú&lt;/h3&gt; 
&lt;p&gt;Á°Æ‰øùÈúÄË¶ÅÊµãËØïÁöÑÊ®°Âûã&lt;code&gt;*.pth&lt;/code&gt;Êñá‰ª∂‰Ωç‰∫é&lt;code&gt;./out/&lt;/code&gt;ÁõÆÂΩï‰∏ã„ÄÇ ‰πüÂèØ‰ª•Áõ¥Êé•Âéª&lt;a href="https://huggingface.co/jingyaogong/MiniMind2-V-PyTorch"&gt;Ê≠§Â§Ñ&lt;/a&gt;‰∏ãËΩΩ‰ΩøÁî®ÊàëËÆ≠ÁªÉÁöÑ&lt;code&gt;*.pth&lt;/code&gt;Êñá‰ª∂„ÄÇ&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python eval_vlm.py --model_mode 1 # ÈªòËÆ§‰∏∫0ÔºöÊµãËØïpretrainÊ®°ÂûãÊïàÊûúÔºåËÆæÁΩÆ‰∏∫1ÔºöÊµãËØïsftÊ®°ÂûãÊïàÊûú
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] ËÆ≠ÁªÉËÑöÊú¨Âùá‰∏∫PytorchÂéüÁîüÊ°ÜÊû∂ÔºåÂùáÊîØÊåÅÂ§öÂç°Âä†ÈÄüÔºåÂÅáËÆæ‰Ω†ÁöÑËÆæÂ§áÊúâN (NÔºû1) Âº†ÊòæÂç°Ôºö&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;ÂçïÊú∫NÂç°ÂêØÂä®ËÆ≠ÁªÉÊñπÂºè (DDP, ÊîØÊåÅÂ§öÊú∫Â§öÂç°ÈõÜÁæ§)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;torchrun --nproc_per_node N train_xxx.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;details style="color:rgb(128,128,128)"&gt; 
 &lt;summary&gt;Ê≥®ÔºöÂÖ∂ÂÆÉÈ°ªÁü•&lt;/summary&gt; 
 &lt;p&gt;ÂçïÊú∫NÂç°ÂêØÂä®ËÆ≠ÁªÉ (DeepSpeed)&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;deepspeed --master_port 29500 --num_gpus=N train_xxx.py
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;ÂèØÊ†πÊçÆÈúÄË¶ÅÂºÄÂêØwandbËÆ∞ÂΩïËÆ≠ÁªÉËøáÁ®ã&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# ÈúÄË¶ÅÁôªÂΩï: wandb login
torchrun --nproc_per_node N train_xxx.py --use_wandb
# and
python train_xxx.py --use_wandb
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;ÈÄöËøáÊ∑ªÂä†&lt;code&gt;--use_wandb&lt;/code&gt;ÂèÇÊï∞ÔºåÂèØ‰ª•ËÆ∞ÂΩïËÆ≠ÁªÉËøáÁ®ãÔºåËÆ≠ÁªÉÂÆåÊàêÂêéÔºåÂèØ‰ª•Âú®wandbÁΩëÁ´ô‰∏äÊü•ÁúãËÆ≠ÁªÉËøáÁ®ã„ÄÇÈÄöËøá‰øÆÊîπ&lt;code&gt;wandb_project&lt;/code&gt; Âíå&lt;code&gt;wandb_run_name&lt;/code&gt;ÂèÇÊï∞ÔºåÂèØ‰ª•ÊåáÂÆöÈ°πÁõÆÂêçÁß∞ÂíåËøêË°åÂêçÁß∞„ÄÇ&lt;/p&gt; 
&lt;/details&gt; 
&lt;h1&gt;üìå VLM Detail&lt;/h1&gt; 
&lt;p&gt;MiniMind-V (VLM)ÁöÑÂü∫Â∫ßËØ≠Ë®ÄÊ®°ÂûãMiniMind (LLM)Êù•Ëá™Â≠™ÁîüÈ°πÁõÆ&lt;a href="https://github.com/jingyaogong/minimind"&gt;minimind&lt;/a&gt;Ôºå ÂÖ∑‰ΩìÁöÑÊ®°ÂûãÁªìÊûÑ„ÄÅËÆ≠ÁªÉÁªÜËäÇ„ÄÅÂéüÁêÜ„ÄÅÊµãËØïÊïàÊûúÁ≠âÂùáÂèØÁßªÊ≠•&lt;a href="https://github.com/jingyaogong/minimind"&gt;minimind&lt;/a&gt;È°πÁõÆÊü•ÈòÖ„ÄÇ Ê≠§Â§Ñ‰∏∫ÂáèÂ∞ëÂÜó‰ΩôÔºåÁúÅÁï•ËÆ®ËÆ∫LLMÁöÑÁõ∏ÂÖ≥ÈÉ®ÂàÜÔºåÈªòËÆ§ÊÇ®Â∑≤ÂØπMiniMind (LLM)ÁöÑÁªÜËäÇÊúâÂü∫Êú¨ÁöÑ‰∫ÜËß£„ÄÇ&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Âç≥‰ΩøÊÇ®‰∏çÂ§™‰∫ÜËß£LLMÁöÑÁªÜËäÇÔºå‰πüÂèØÂèÇËÄÉ‚ÄúÂø´ÈÄüÂºÄÂßã‚ÄùÊµÅÁ®ãËÆ≠ÁªÉ‰∏Ä‰∏™MiniMind-VÔºå ËøôÂπ∂‰∏çÂèóÂà∞ÂΩ±ÂìçÔºå‰ªìÂ∫ìËá¥Âäõ‰∫éÊúÄ‰ΩéÊàêÊú¨ÁöÑÂºÄÁÆ±Âç≥Áî®ÔºÅ&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;MiniMind-VÁöÑÁªìÊûÑ‰ªÖÂ¢ûÂä†Visual EncoderÂíåÁâπÂæÅÊäïÂΩ±‰∏§‰∏™Â≠êÊ®°ÂùóÔºåÂ¢ûÂä†Ê®°ÊÄÅÊ∑∑ÂêàÂàÜÊîØÔºå‰ª•ÊîØÊåÅÂ§öÁßçÊ®°ÊÄÅ‰ø°ÊÅØÁöÑËæìÂÖ•Ôºö &lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind-v/master/images/VLM-structure.png" alt="LLM-structure"&gt; &lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind-v/master/images/VLM-structure-moe.png" alt="LLM-structure"&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt; „ÄêÈáçË¶Å„Äë‰∏Ä‰∫õÊúâË∂£ÁöÑÊÄùËÄÉ &lt;/summary&gt; 
 &lt;p&gt;Ê≠§Â§Ñ‰∏çÂ¶®Â±ïÂºÄÊÉ≥‰∏ÄÊÉ≥‰∏§‰∏™ÈóÆÈ¢òÔºö&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;‰ªÄ‰πàÂè´ÂÅö&lt;strong&gt;L&lt;/strong&gt;arge &lt;strong&gt;L&lt;/strong&gt;anguage &lt;strong&gt;M&lt;/strong&gt;odel (LLM)Ôºü&lt;/li&gt; 
  &lt;li&gt;‰ªÄ‰πàÂè´ÂÅöÂ§öÊ®°ÊÄÅÊ®°ÂûãÔºü&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;a href="https://www.jiqizhixin.com/articles/2024-09-15-3"&gt;ËøôÁØáÊñáÁ´†&lt;/a&gt;ÂÆåÁæéÂêªÂêàÊú¨‰∫∫ÁöÑÊÉ≥Ê≥ïÔºö Â§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÂêçÂ≠óËôΩÁÑ∂Â∏¶ÊúâËØ≠Ë®Ä‰∫åÂ≠óÔºå‰ΩÜÂÆÉ‰ª¨ÂÖ∂ÂÆû‰∏éËØ≠Ë®ÄÂÖ≥Á≥ª‰∏çÂ§ßÔºåËøôÂè™ÊòØÂéÜÂè≤ÈóÆÈ¢òÔºåÊõ¥Á°ÆÂàáÁöÑÂêçÂ≠óÂ∫îËØ•ÊòØËá™ÂõûÂΩí Transformer ÊàñËÄÖÂÖ∂‰ªñ„ÄÇLLM Êõ¥Â§öÊòØ‰∏ÄÁßçÁªüËÆ°Âª∫Ê®°ÁöÑÈÄöÁî®ÊäÄÊúØÔºåÂÆÉ‰ª¨‰∏ªË¶ÅÈÄöËøáËá™ÂõûÂΩí Transformer Êù•Ê®°Êãü token ÊµÅÔºåËÄåËøô‰∫õ token ÂèØ‰ª•‰ª£Ë°®ÊñáÊú¨„ÄÅÂõæÁâá„ÄÅÈü≥È¢ë„ÄÅÂä®‰ΩúÈÄâÊã©„ÄÅÁîöËá≥ÊòØÂàÜÂ≠êÁ≠â‰ªª‰Ωï‰∏úË•ø„ÄÇ Âõ†Ê≠§ÔºåÂè™Ë¶ÅËÉΩÂ∞ÜÈóÆÈ¢òËΩ¨Âåñ‰∏∫Ê®°Êãü‰∏ÄÁ≥ªÂàóÁ¶ªÊï£ token ÁöÑÊµÅÁ®ãÔºåÁêÜËÆ∫‰∏äÈÉΩÂèØ‰ª•Â∫îÁî® LLM Êù•Ëß£ÂÜ≥„ÄÇ ÂÆûÈôÖ‰∏äÔºåÈöèÁùÄÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÊäÄÊúØÊ†àÁöÑÊó•ÁõäÊàêÁÜüÔºåÊàë‰ª¨ÂèØËÉΩ‰ºöÁúãÂà∞Ë∂äÊù•Ë∂äÂ§öÁöÑÈóÆÈ¢òË¢´Á∫≥ÂÖ•ËøôÁßçÂª∫Ê®°ËåÉÂºè„ÄÇ‰πüÂ∞±ÊòØËØ¥ÔºåÈóÆÈ¢òÂõ∫ÂÆöÂú®‰ΩøÁî® LLM ËøõË°å„Äé‰∏ã‰∏Ä‰∏™ token ÁöÑÈ¢ÑÊµã„ÄèÔºåÂè™ÊòØÊØè‰∏™È¢ÜÂüü‰∏≠ token ÁöÑÁî®ÈÄîÂíåÂê´‰πâÊúâÊâÄ‰∏çÂêå„ÄÇ&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://person.zju.edu.cn/xilics#694283"&gt;ZJU-LiXiËÄÅÂ∏à&lt;/a&gt;ÂêåÊ†∑Ë∞àÂèäËøáÁ±ª‰ººËßÇÁÇπÔºàÂéüËØùÂ§ßÊÑèÂ¶Ç‰∏ãÔºâÔºö ÊñáÊú¨„ÄÅËßÜÈ¢ë„ÄÅËØ≠Èü≥„ÄÅÂä®‰ΩúÁ≠âÂú®‰∫∫Á±ªÁúãÊù•Â±û‰∫é„ÄåÂ§öÊ®°ÊÄÅ„Äç‰ø°Âè∑Ôºå‰ΩÜÊâÄË∞ìÁöÑ„ÄåÊ®°ÊÄÅ„ÄçÂÖ∂ÂÆûÂè™ÊòØ‰∫∫Á±ªÂú®‰ø°ÊÅØÂ≠òÂÇ®ÊñπÂºè‰∏äÁöÑ‰∏ÄÁßçÂàÜÁ±ªÊ¶ÇÂøµ„ÄÇ Â∞±ÂÉè&lt;code&gt;.txt&lt;/code&gt;Âíå&lt;code&gt;.png&lt;/code&gt;Êñá‰ª∂ÔºåËôΩÁÑ∂Âú®ËßÜËßâÂëàÁé∞ÂíåÈ´òÁ∫ßË°®Áé∞ÂΩ¢Âºè‰∏äÊúâÊâÄ‰∏çÂêåÔºå‰ΩÜÂÆÉ‰ª¨Êú¨Ë¥®‰∏äÂπ∂Ê≤°ÊúâÊ†πÊú¨Âå∫Âà´„ÄÇ ‰πãÊâÄ‰ª•Âá∫Áé∞„ÄåÂ§öÊ®°ÊÄÅ„ÄçËøô‰∏™Ê¶ÇÂøµÔºå‰ªÖ‰ªÖÊòØÂõ†‰∏∫‰∫∫Á±ªÂú®‰∏çÂêåÁöÑÊÑüÁü•Â±ÇÈù¢‰∏äÂØπËøô‰∫õ‰ø°Âè∑ÁöÑÂàÜÁ±ªÈúÄÊ±Ç„ÄÇ ÁÑ∂ËÄåÔºåÂØπ‰∫éÊú∫Âô®Êù•ËØ¥ÔºåÊó†ËÆ∫‰ø°Âè∑Êù•Ëá™‰ΩïÁßç„ÄåÊ®°ÊÄÅ„ÄçÔºåÊúÄÁªàÂÆÉ‰ª¨ÈÉΩÂè™ÊòØ‰ª•‰∏Ä‰∏≤‰∫åËøõÂà∂ÁöÑ„ÄåÂçïÊ®°ÊÄÅ„ÄçÊï∞Â≠óÂ∫èÂàóÊù•ÂëàÁé∞„ÄÇ Êú∫Âô®Âπ∂‰∏ç‰ºöÂå∫ÂàÜËøô‰∫õ‰ø°Âè∑ÁöÑÊ®°ÊÄÅÊù•Ê∫êÔºåËÄåÂè™ÊòØÂ§ÑÁêÜÂíåÂàÜÊûêËøô‰∫õÂ∫èÂàóËÉåÂêéÊâÄÊâøËΩΩÁöÑ‰ø°ÊÅØÂÜÖÂÆπ„ÄÇ&lt;/p&gt; 
 &lt;p&gt;‰∏™‰∫∫ËÆ§‰∏∫&lt;strong&gt;G&lt;/strong&gt;enerative &lt;strong&gt;P&lt;/strong&gt;retrained &lt;strong&gt;T&lt;/strong&gt;ransformer (GPT) ÊØî &lt;strong&gt;L&lt;/strong&gt;arge &lt;strong&gt;L&lt;/strong&gt;anguage &lt;strong&gt;M&lt;/strong&gt;odel (LLM)Êõ¥‰∏∫Ë¥¥ÂàáÔºå Âõ†Ê≠§Êú¨‰∫∫Ë°®Ëææ‰∏äÊõ¥‰π†ÊÉØÁî®"GPT"Âéª‰ª£Ë°®LLM/VLM/Á±ªGPTÊû∂ÊûÑÁöÑÁ≥ªÂàóÊ®°ÂûãÔºåËÄåÈùû‰∏∫‰∫ÜËπ≠OpenAIÁöÑÁÉ≠Â∫¶„ÄÇ&lt;/p&gt; 
 &lt;p&gt;Ëá≥Ê≠§ÔºåÊàë‰ª¨ÂèØ‰ª•Áî®‰∏ÄÂè•ËØùÊÄªÁªìGPTÁöÑÊâÄ‰ΩúÊâÄ‰∏∫Ôºö&lt;/p&gt; 
 &lt;p&gt;GPTÊ®°ÂûãÊ†πÊçÆÁé∞ÊúâtokenÈ¢ÑÊµãËæìÂá∫‰∏ã‰∏Ä‰∏™‰∏ã‰∏ã‰∏Ä‰∏™‰∏ã‰∏ã‰∏ã‰∏Ä‰∏™token ...ÔºåÁõ¥Âà∞Ê®°ÂûãËæìÂá∫ÁªìÊùüÁ¨¶ÔºõÊ≠§Â§ÑÁöÑ"token"ÂÖ∂ÂÆûÂπ∂‰∏çÈúÄË¶Å‰∏ÄÂÆöÊòØÊñáÊú¨ÔºÅ&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-text"&gt;&amp;gt; ÂØπ‰∫éLLMÊ®°ÂûãÔºåÂ¶ÇÊûúÈúÄË¶ÅÁêÜËß£"ÂõæÁâá"ÔºåÊàë‰ª¨Âè™Ë¶ÅÊää"ÂõæÁâá"‰Ωú‰∏∫ÂØπ‰∏ÄÁßçÁâπÊÆäÁöÑ‰ªéÊù•Ê≤°ËßÅËøáÁöÑ"Â§ñÂõΩËØ≠Ë®Ä"ÔºåÈÄöËøá"Â§ñËØ≠ËØçÂÖ∏"ÁøªËØëÂêéÂç≥ÂèØ‰Ωú‰∏∫ÁâπÊÆäÁöÑËØ≠Ë®ÄËæìÂÖ•LLM
&amp;gt; ÂØπ‰∫éLLMÊ®°ÂûãÔºåÂ¶ÇÊûúÈúÄË¶ÅÁêÜËß£"Èü≥È¢ë"ÔºåÊàë‰ª¨Âè™Ë¶ÅÊää"Èü≥È¢ë"‰Ωú‰∏∫ÂØπ‰∏ÄÁßçÁâπÊÆäÁöÑ‰ªéÊù•Ê≤°ËßÅËøáÁöÑ"Â§ñÂõΩËØ≠Ë®Ä"ÔºåÈÄöËøá"Â§ñËØ≠ËØçÂÖ∏"ÁøªËØëÂêéÂç≥ÂèØ‰Ωú‰∏∫ÁâπÊÆäÁöÑËØ≠Ë®ÄËæìÂÖ•LLM
&amp;gt; ...
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;u&gt;&lt;strong&gt;‰∏∫‰∫ÜÂæóÂà∞MiniMind-VÔºåÊàë‰ª¨Âè™ÈúÄË¶ÅÂÆåÊàêËøô2‰ª∂‰∫ãÂç≥ÂèØÔºö&lt;/strong&gt;&lt;/u&gt;&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;ÂÄüÂä©ÊìÖÈïøÁøªËØëÂõæÁâáÁöÑ &lt;strong&gt;"Â§ñËØ≠ËØçÂÖ∏"&lt;/strong&gt; ÔºåÊääÂõæÁâá‰ªé &lt;strong&gt;"Â§ñÂõΩËØ≠Ë®Ä"&lt;/strong&gt; ÁøªËØë‰∏∫Ê®°Âûã‰æø‰∫éÁêÜËß£ÁöÑ &lt;strong&gt;"LLMËØ≠Ë®Ä"&lt;/strong&gt;&lt;/li&gt; 
  &lt;li&gt;ËÆ≠ÁªÉÂæÆË∞ÉLLMÔºå‰ΩøÂÖ∂Âíå &lt;strong&gt;"Â§ñËØ≠ËØçÂÖ∏"&lt;/strong&gt; Â∫¶ËøáÁ£®ÂêàÊúüÔºå‰ªéËÄåÊõ¥Â•ΩÁöÑÁêÜËß£ÂõæÁâá&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;"Â§ñËØ≠ËØçÂÖ∏" Áß∞‰πã‰∏∫Visual EncoderÊ®°Âûã„ÄÇ ÂíåLlaVA„ÄÅQwen-VLÁ≠âËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÁ±ª‰ººÔºåMiniMind-VÂêåÊ†∑ÈÄâÁî®ÂºÄÊ∫êClipÁ≥ªÂàóÊ®°Âûã‰Ωú‰∏∫Visual Encoder„ÄÇ ÂÖ∑‰Ωì‰ΩøÁî®&lt;a href="https://huggingface.co/openai/clip-vit-base-patch16"&gt;clip-vit-base-patch16&lt;/a&gt;Ôºå ‰∏ÄÁßçÂü∫‰∫é ViT-B/16 Êû∂ÊûÑÁöÑÁªèÂÖ∏Visual EncoderÁî®‰∫éÊèèËø∞ÂõæÂÉèÊñáÊú¨‰ø°ÊÅØ„ÄÇ ËæìÂÖ•ÁöÑÂõæÂÉèÂ∞∫ÂØ∏‰∏∫224x224ÔºåÂõ†‰∏∫ÂàíÂàÜÁöÑPatchÊòØ16√ó16ÔºåÊâÄ‰ª•‰ºö‰∫ßÁîü14*14=196‰∏™token‰Ωú‰∏∫encoderÁºñÁ†ÅÂ±ÇÁöÑËæìÂÖ•Ôºå ÊúÄÁªà‰∫ßÁîü1√ó768Áª¥ÁöÑÂµåÂÖ•ÂêëÈáèÁî®‰∫éÂíåÊñáÊú¨ÂØπËÆ°ÁÆóËØØÂ∑Æ„ÄÇ Êàë‰ª¨Âπ∂‰∏çÈúÄË¶ÅÊúÄÁªàÂµåÂÖ•Ë°®Á§∫ÔºåÂõ†Ê≠§Âè™ÂèñencoderÂ±ÇÁöÑËæìÂá∫Ôºå‰πüÂ∞±ÊòØVITÊ†∏ÂøÉ‰∏ªÂπ≤ÁöÑËæìÂá∫ÁâπÂæÅÂç≥ÂèØ„ÄÇ ÂÆÉÊãøÂà∞Ââç‰∏ÄÂ±ÇÁª¥Â∫¶196√ó768Â§ßÂ∞èÁöÑÁâπÂæÅÔºåÊàë‰ª¨ÊääÂÆÉ‰Ωú‰∏∫196‰∏™visual tokenËæìÂÖ•MiniMind-V„ÄÇ ‰∏éLLMÁöÑÁªìÂêàÂú®Ëé∑ÂèñÂõæÂÉèencoderÁâπÂæÅÂêéÔºå‰∏ÄÊñπÈù¢ÈúÄË¶ÅÊää768Áª¥Â∫¶ÁöÑvisual tokenÂØπÈΩêÂà∞LLMÁöÑÊñáÊú¨tokenÔºå Âè¶‰∏ÄÊñπÈù¢ÔºåË¶ÅÂ∞ÜÂõæÂÉèÁâπÂæÅÊò†Â∞ÑÂà∞‰∏éÊñáÊú¨embeddingÁõ∏ÂêåÁöÑÁ©∫Èó¥ÔºåÂç≥ÊñáÊú¨tokenÂíåÂéüÁîüÁöÑËßÜËßâtokenÈúÄË¶ÅÁ£®ÂêàÂπ∂‰∏çËÉΩÁõ¥Êé•Âú∞‰∏ÄËßÜÂêå‰ªÅÔºå ÂèØ‰ª•Áß∞‰πã‰∏∫Ë∑®Ê®°ÊÄÅÁöÑÁâπÂæÅÂØπÈΩê„ÄÇ &lt;a href="https://arxiv.org/pdf/2304.08485"&gt;LlaVA-1&lt;/a&gt;‰ΩøÁî®ÁÆÄÂçïÁöÑÊó†ÂÅèÁ∫øÊÄßÂèòÊç¢ÂÆåÊàê‰∫ÜËøô‰∏ÄÊìç‰ΩúÔºåÊïàÊûúÂæà‰∏çÈîôÔºåMiniMind-VÂêåÊ†∑Â¶ÇÊ≠§„ÄÇ&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind-v/master/images/llava-structure.png" alt="llava-structure"&gt;&lt;/p&gt; 
 &lt;p&gt;Ëá≥Ê≠§ÔºåMiniMind-VÁöÑÂÜÖÈÉ®ÁªìÊûÑÂèòÂåñÂ∑≤ÁªèÂëàÁé∞ÂÆåÊØï„ÄÇ&lt;/p&gt; 
&lt;/details&gt; 
&lt;hr&gt; 
&lt;p&gt;‰∏ãÈù¢ÔºåÊàë‰ª¨ÁÆÄÂçïËÆ®ËÆ∫MiniMind-VÁöÑÂ§ñÈÉ®ËæìÂÖ•ËæìÂá∫ÁöÑÂèòÂåñ„ÄÇ&lt;/p&gt; 
&lt;p&gt;VLMÁöÑËæìÂÖ•‰æùÁÑ∂ÊòØ‰∏ÄÊÆµÊñáÊú¨ÔºåÂÖ∂‰∏≠ÂåÖÂê´ÁâπÊÆäÁöÑ&lt;img&gt;Âç†‰ΩçÁ¨¶„ÄÇ Âú®ËÆ°ÁÆóÊñáÊú¨ÂµåÂÖ•ÂêéÔºåÂèØ‰ª•Â∞ÜÂõæÂÉèÁºñÁ†ÅÂô®ÁîüÊàêÁöÑÂêëÈáèÊäïÂΩ±Âà∞ËØ•Âç†‰ΩçÁ¨¶ÂØπÂ∫îÁöÑÂµåÂÖ•ÈÉ®ÂàÜÔºåÊõøÊç¢ÊéâÂéüÂÖàÁöÑÂç†‰ΩçÁ¨¶embedding„ÄÇ ‰æãÂ¶ÇÔºö&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;&amp;lt;image&amp;gt;\nËøô‰∏™ÂõæÂÉè‰∏≠Êúâ‰ªÄ‰πàÂÜÖÂÆπÔºü
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Âú®&lt;code&gt;minimind-v&lt;/code&gt;‰∏≠Ôºå‰ΩøÁî®196‰∏™Â≠óÁ¨¶ÁªÑÊàêÁöÑ &lt;code&gt;@@@...@@@&lt;/code&gt; Âç†‰ΩçÁ¨¶‰ª£ÊõøÂõæÂÉèÔºå‰πãÊâÄ‰ª•ÊòØ196‰∏™Â≠óÁ¨¶ÔºåÂâçÈù¢ÊúâÊâÄÊèêÂèäÔºö ‰ªª‰ΩïÂõæÂÉèÈÉΩË¢´clipÊ®°Âûãencoder‰∏∫196√ó768Áª¥ÁöÑtokenÔºå Âõ†Ê≠§&lt;code&gt;minimind-v&lt;/code&gt;ÁöÑprompt‰∏∫Ôºö&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;@@@......@@@\nËøô‰∏™ÂõæÁâáÊèèËø∞ÁöÑÊòØ‰ªÄ‰πàÂÜÖÂÆπÔºü
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;ËÆ°ÁÆóÂÆåembeddingÂíåprojectionÔºåÂπ∂ÂØπÂõæÂÉèÈÉ®ÂàÜtokenÊõøÊç¢ÂêéÊï¥‰∏™ËÆ°ÁÆóËøáÁ®ãÂà∞ËæìÂá∫ÂàôÂíåLLMÈÉ®ÂàÜÊ≤°Êúâ‰ªª‰ΩïÂå∫Âà´„ÄÇ&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind-v/master/images/minimind-v-input.png" alt="input"&gt;&lt;/p&gt; 
&lt;p&gt;‰∏ÄÊ¨°ÊÄßÂ§öÂõæÁöÑÂÆûÁé∞ÊñπÊ≥ïÂ∞±ÊòØÈÄöËøáÊ≥®ÂÖ•Â§ö‰∏™&lt;code&gt;&amp;lt;image&amp;gt;&lt;/code&gt;ÂõæÂÉèÂç†‰ΩçÁ¨¶ËøõË°åÂÆûÁé∞Ôºå‰∏çÈúÄË¶Å‰øÆÊîπ‰ªª‰ΩïÊ°ÜÊû∂„ÄÇ&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt; ËßÜÈ¢ëÁêÜËß£ÁöÑÊãìÂ±ïÊÄùË∑Ø &lt;/summary&gt; 
 &lt;p&gt;write by &lt;a href="https://github.com/xinyanghuang7"&gt;@xinyanghuang7&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;ÂØπ‰∫éÂ§öÊ®°ÊÄÅÂ§ßÊ®°ÂûãÁöÑËßÜÈ¢ëÁêÜËß£ËÉΩÂäõÔºå‰∏Ä‰∏™ÂèØË°åÁöÑÊÄùË∑ØÊòØÂèÇËÄÉÁé∞ÊúâMiniCPM-V 2.6 ËøõË°åËßÜÈ¢ëÁêÜËß£ÁöÑPythonÁ§∫‰æã„ÄÇ ‰∏ªË¶ÅÊÄùÊÉ≥ÊòØÈÄöËøáÊèêÂèñËßÜÈ¢ëÂÖ≥ÈîÆÂ∏ßÔºåËÄåÂêéËøõË°åÂ§öÂõæÊé®ÁêÜ„ÄÇ Âõ†Ê≠§ÔºåÂ¶ÇÊûúÂ∏åÊúõÂú®MiniMind-V‰∏≠Ê∑ªÂä†ËßÜÈ¢ëÁêÜËß£ËÉΩÂäõÔºåÂèØ‰ª•Âú®Áé∞ÊúâÂ§öÂõæËÆ≠ÁªÉÁöÑÂü∫Á°Ä‰∏äÔºåÂèÇËÄÉÊ≠§pythonËÑöÊú¨‰∏≠ÂØπ‰∫éÂÖ≥ÈîÆÂ∏ßÁöÑÊèêÂèñÊñπÊ≥ïÔºåËÄåÂêéÂä†Â§ßËÆ≠ÁªÉÊñá‰ª∂‰∏≠ÊîØÊåÅÂõæÁâáÁöÑÊï∞Èáè„ÄÇ ÊâÄÊîØÊåÅÁöÑMAX_NUM_FRAMESË∂äÂ§öÔºåÊâÄÊ∂àËÄóÁöÑÊòæÂ≠òË∂äÂ§ß„ÄÇ&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-text"&gt;import torch
from PIL import Image
from transformers import AutoModel, AutoTokenizer
from decord import VideoReader, cpu  # pip install decord

model = AutoModel.from_pretrained('openbmb/MiniCPM-V-2_6', trust_remote_code=True,
                                  attn_implementation='sdpa',
                                  torch_dtype=torch.bfloat16)  # sdpa or flash_attention_2, no eager
model = model.eval().cuda()
tokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V-2_6', trust_remote_code=True)

MAX_NUM_FRAMES = 64  # if cuda OOM set a smaller number


def encode_video(video_path):
    def uniform_sample(l, n):
        gap = len(l) / n
        idxs = [int(i * gap + gap / 2) for i in range(n)]
        return [l[i] for i in idxs]

    vr = VideoReader(video_path, ctx=cpu(0))
    sample_fps = round(vr.get_avg_fps() / 1)  # FPS
    frame_idx = [i for i in range(0, len(vr), sample_fps)]
    if len(frame_idx) &amp;gt; MAX_NUM_FRAMES:
        frame_idx = uniform_sample(frame_idx, MAX_NUM_FRAMES)
    frames = vr.get_batch(frame_idx).asnumpy()
    frames = [Image.fromarray(v.astype('uint8')) for v in frames]
    print('num frames:', len(frames))
    return frames


video_path = "video_test.mp4"
frames = encode_video(video_path)
question = "Describe the video"
msgs = [
    {'role': 'user', 'content': frames + [question]},
]

# Set decode params for video
params = {}
params["use_image_id"] = False
params["max_slice_nums"] = 2  # Â¶ÇÊûúcuda OOM‰∏îËßÜÈ¢ëÂàÜËæ®ÁéáÂ§ß‰∫é448*448ÂèØËÆæ‰∏∫1

answer = model.chat(
    image=None,
    msgs=msgs,
    tokenizer=tokenizer,
    **params
)
print(answer)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p&gt;Ëá≥Ê≠§Ôºå&lt;code&gt;MiniMind-V&lt;/code&gt;ÁöÑÊâÄÊúâÁªÜËäÇÂ∑≤ÁªèÂëàÁé∞ÂÆåÊØï„ÄÇ &lt;code&gt;MiniMind-V&lt;/code&gt;ÁöÑÊ®°ÂûãÂ≠êÁ±ªÂÆåÂÖ®ÁªßÊâøËá™&lt;code&gt;MiniMind&lt;/code&gt;Ôºå ‰ªÖÂü∫‰∫éÂêéËÄÖÂÅö&lt;strong&gt;ÊúÄÂ∞è&lt;/strong&gt;ÂèòÊõ¥ËÄå‰∫ßÁîüÔºå ÂÖ∂Ê†∏ÂøÉÁÆóÊ≥ïÊîπÂä®&lt;code&gt;&amp;lt; 50Ë°å&lt;/code&gt;ÔºåËøÅÁßªÈöæÂ∫¶ÊûÅ‰Ωé„ÄÇ Âõ†Ê≠§ÂèØËÉΩÂíå&lt;code&gt;LlAVA&lt;/code&gt;Á≠âÊ®°ÂûãÁªÜËäÇÂèØËÉΩÂ≠òÂú®Âå∫Âà´Ôºå‰ΩÜÊÄùË∑ØÂÆåÂÖ®Áªü‰∏Ä„ÄÇ&lt;/p&gt; 
&lt;h1&gt;üìå Experiment&lt;/h1&gt; 
&lt;h2&gt;‚Ö† Êï∞ÊçÆÈõÜ&lt;/h2&gt; 
&lt;p&gt;Êù•Ê∫êÔºö&lt;a href="https://huggingface.co/datasets/LinkSoul/Chinese-LLaVA-Vision-Instructions"&gt;Chinese-LLaVA-Vision&lt;/a&gt; ÂåÖÂê´Á∫¶57‰∏áÂº†È¢ÑËÆ≠ÁªÉÂõæÂÉèÔºåÊù•Ëá™CC-3MÂíåCOCO 2014Ôºõ &lt;a href="https://huggingface.co/datasets/BUAADreamer/llava-en-zh-300k"&gt;llava-en-zh-300k&lt;/a&gt; ÂåÖÂê´300kÊù°Êåá‰ª§ÂæÆË∞ÉÊï∞ÊçÆÂíå15‰∏áÂº†ÂõæÂÉè„ÄÇ ÈóÆÁ≠îÂÜÖÂÆπÁªèËøáÁøªËØëÔºå ÂØπ‰∏≠ÊñáÊîØÊåÅÊõ¥ÂèãÂ•ΩÔºåËøõ‰∏ÄÊ≠•ÁªèËøáÊï¥ÁêÜÂπ∂&lt;code&gt;resize&lt;/code&gt;„ÄÇ&lt;/p&gt; 
&lt;p&gt;(pretrain_vlm_data.jsonl) È¢ÑËÆ≠ÁªÉÊï∞ÊçÆÈõÜÊ†ºÂºèÔºö&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "conversations": [
    {
      "role": "user",
      "content": "Êèê‰æõÁªôÂÆöÂõæÂÉèÁöÑÁÆÄË¶ÅÊèèËø∞„ÄÇ\n&amp;lt;image&amp;gt;"
    },
    {
      "role": "assistant",
      "content": "Ê©ÑÊ¶ÑÊ≤πÊòØËá™Áî±‰ΩøÁî®ÁöÑÂÅ•Â∫∑ÊàêÂàÜ„ÄÇ"
    }
  ],
  "image": "GCC_train_002582585.jpg"
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;(sft_vlm_data.jsonl) ÂçïÂõæÊåá‰ª§ÂæÆË∞ÉÊï∞ÊçÆÈõÜÊ†ºÂºèÔºö&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "conversations": [
    {
      "role": "user",
      "content": "ÈóπÈíüÁöÑ‰ΩçÁΩÆÂØπÁù°Áú†Ë¥®ÈáèÊúâ‰ªÄ‰πàÂΩ±ÂìçÔºü&amp;lt;image&amp;gt;"
    },
    {
      "role": "assistant",
      "content": "ÊääÊï∞Â≠óÈóπÈíüÊîæÂú®Â∫äÂ§¥Êüú..."
    }
  ],
  "image": "train-00000-of-00001_image_0_0.jpg"
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;(sft_vlm_data_multi.jsonl) Â§öÂõæÊåá‰ª§ÂæÆË∞ÉÊï∞ÊçÆÈõÜÊ†ºÂºèÔºö&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "conversations": [
    {
      "role": "user",
      "content": "context: Source Image: &amp;lt;image&amp;gt; Target Image: &amp;lt;image&amp;gt; Instruction: What is the correct image edit instruction that can transfrom the source image to target image?&amp;lt;image&amp;gt;"
    },
    {
      "role": "assistant",
      "content": "take the people out of the back in the photo. Remove the two people behind the woman in the white dress and the man in the blue suit. remove people behind the couple in the centre"
    }
  ],
  "image": "0.jpg, 1.jpg"
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt; Êï∞ÊçÆËØ¥Êòé &lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;Â§öÂõæÊï∞ÊçÆÈõÜËßÑÊ®°Áõ∏ÂØπËæÉÂ∞è‰∏î‰∏∫Ëã±ÊñáÂØπËØùÔºåÊï∞ÊçÆÈõÜ‰ªÖÂåÖÂê´‰∏§ÂõæÂØπÊØîÁöÑÂú∫ÊôØÔºåÂõ†Ê≠§ÂæÆË∞ÉÊïàÊûúÊúâÈôêÔºåËøôÈáåÂè™Êèê‰æõ‰∏ÄÁßçÂèÇËÄÉÊÄùË∑Ø„ÄÇ&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;code&gt;jsonl&lt;/code&gt;Âùá‰∏∫ÊñáÊú¨Êåá‰ª§Ôºå&lt;code&gt;images.zip&lt;/code&gt;Âùá‰∏∫ÈÖçÂ•óÁöÑÂõæÂÉèÊï∞ÊçÆÔºà‰∏ãËΩΩÂêéÈúÄË¶ÅËß£ÂéãÔºâ&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;p&gt;Êï∞ÊçÆÈõÜ‰∏ãËΩΩÂú∞ÂùÄÔºö(&lt;a href="https://www.modelscope.cn/datasets/gongjy/minimind-v_dataset"&gt;ModelScope&lt;/a&gt; | &lt;a href="https://huggingface.co/datasets/jingyaogong/minimind-v_dataset"&gt;HuggingFace&lt;/a&gt;)&lt;/p&gt; 
&lt;h2&gt;‚Ö° ËÆ≠ÁªÉ&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;train_pretrain_vlm&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;È¢ÑËÆ≠ÁªÉ‰ªé595KÊù°Êï∞ÊçÆÈõÜ‰∏≠Â≠¶‰π†ÂõæÁâáÁöÑÈÄöÁî®Áü•ËØÜÔºåÊØîÂ¶ÇÈπøÊòØÈπøÔºåÁãóÊòØÁãó„ÄÇ&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;train_sft_vlm&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Êåá‰ª§ÂæÆË∞É‰ªé300KÊù°ÁúüÂÆûÂØπËØùÊï∞ÊçÆÈõÜ‰∏≠Â≠¶‰π†ÂØπÂõæÁâáÊèêÈóÆÁöÑÁúüÂÆûÈóÆÁ≠îÊ†ºÂºèÔºåÊõ¥Á¨¶Âêà‰∏é‰∫∫Á±ªÁöÑ‰∫§ÊµÅ‰π†ÊÉØ„ÄÇ&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;train_sft_vlm&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Â§öÂõæÂæÆË∞ÉÊèê‰æõdemoÔºöÈ∏üÁ±ªÂØπÊØîÊï∞ÊçÆÈõÜÔºåÈïøÂ∫¶‰∏∫13.6kÁöÑÁúüÂÆûÈóÆÁ≠îÊ†ºÂºè„ÄÇ&lt;/p&gt; 
&lt;p&gt;ËÆ≠ÁªÉÊó∂ÂùáÂÜªÁªìvisual encoder‰πüÂ∞±ÊòØclipÊ®°ÂûãÊ¢ØÂ∫¶Ôºå Âè™ËÆ≠ÁªÉProjectionÂíåLLM‰∏§ÈÉ®ÂàÜ„ÄÇ È¢ÑËÆ≠ÁªÉ‰∏≠ÔºåÂè™ËÆæÁΩÆProjectionÂíåLLMÁöÑÊúÄÂêé‰∏ÄÂ±ÇÂèÇÊï∞ÂèØÂ≠¶‰π†„ÄÇ Êåá‰ª§ÂæÆË∞É‰∏≠ÔºåËÆæÁΩÆProjectionÂíåLLMÁöÑÂÖ®ÈÉ®ÂèÇÊï∞ÂèØÂ≠¶‰π†„ÄÇ&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ËÆ≠ÁªÉÊó∂Èó¥ÂíåLossËµ∞ÂäøÔºà‰ªÖ‰æõÂèÇËÄÉÔºâ&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Pretrain [512+8] &amp;amp; [768+16] &lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind-v/master/images/pretrain_loss.png" alt="input"&gt;&lt;/p&gt; 
&lt;p&gt;SFT [512+8] &amp;amp; [768+16] &lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind-v/master/images/sft_loss.png" alt="input"&gt;&lt;/p&gt; 
&lt;h2&gt;‚Ö¢ Ê®°ÂûãÊùÉÈáç&lt;/h2&gt; 
&lt;p&gt;(ÂéüÁîüPyTorch&lt;code&gt;*.pth&lt;/code&gt;ÊùÉÈáçÊñá‰ª∂) ‰∏ãËΩΩÂú∞ÂùÄÔºö (&lt;a href="https://www.modelscope.cn/models/gongjy/MiniMind2-V-PyTorch"&gt;ModelScope&lt;/a&gt; | &lt;a href="https://huggingface.co/jingyaogong/MiniMind2-V-PyTorch"&gt;HuggingFace&lt;/a&gt;)&lt;/p&gt; 
&lt;p&gt;(&lt;code&gt;Transformers&lt;/code&gt;Ê†ºÂºèÊ®°Âûã) ‰∏ãËΩΩÂú∞ÂùÄÔºö (&lt;a href="https://www.modelscope.cn/profile/gongjy"&gt;ModelScope&lt;/a&gt; | &lt;a href="https://huggingface.co/collections/jingyaogong/minimind-v-67000833fb60b3a2e1f3597d"&gt;HuggingFace&lt;/a&gt;)&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Ê≥®ÔºöTransformersÁâàÊú¨Âùá‰∏∫ÂçïÂõæÊåá‰ª§ÂæÆË∞ÉÂêéÁöÑ&lt;code&gt;MiniMind-V&lt;/code&gt;Ê®°Âûã&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h1&gt;üìå Test&lt;/h1&gt; 
&lt;h3&gt;ÊïàÊûúÊµãËØï&lt;/h3&gt; 
&lt;h4&gt;ÂçïÂõæÂØπËØù&lt;/h4&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;ÂõæÁâá&lt;/th&gt; 
   &lt;th&gt;MiniMind2-V&lt;/th&gt; 
   &lt;th&gt;MiniMind2-V-Small&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt; &lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind-v/master/dataset/eval_images/%E5%9F%8E%E5%B8%82%E8%BD%A6%E6%B0%B4%E9%A9%AC%E9%BE%99-city-traffic.jpg" alt="city-traffic"&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;/td&gt; 
   &lt;td&gt;Âõæ‰∏≠ÊòØ‰∏Ä‰∏™ÁπÅÂøôÁöÑÂüéÂ∏ÇË°óÈÅìÔºå‰∏ÄÊù°ÈïøÈïøÁöÑË°óÈÅì‰∏§ÊóÅÈÉΩÊòØÈ´òÊ•ºÂ§ßÂé¶„ÄÇËøôÊù°Ë°ó‰∏äÊå§Êª°‰∫ÜÊ±ΩËΩ¶„ÄÅÂç°ËΩ¶ÂíåÂÖ¨ÂÖ±Ê±ΩËΩ¶ÔºåËøòÊúâËÆ∏Â§öÂÖ∂‰ªñËΩ¶ËæÜÂú®Ë∑Ø‰∏äË°åÈ©∂„ÄÇÂú®Ë°óÈÅì‰∏äÔºåÂèØ‰ª•ÁúãÂà∞ËÆ∏Â§öÊ±ΩËΩ¶ÔºåÊúâÁöÑÂú®È´òÈÄüË°åÈ©∂ÔºåËÄåÂÖ∂‰ªñÁöÑÂàôÂÅúÂú®Ë°óÈÅì‰∏Ä‰æß„ÄÇÊ≠§Â§ñËøòÊúâ‰∏ÄËæÜÂÖ¨‰∫§ËΩ¶‰πüÂÅúÂú®Ë°óÈÅìÁöÑÂè≥‰æß„ÄÇË°óÈÅì‰∏äÂèØ‰ª•ÁúãÂà∞‰∫§ÈÄöÁÅØÔºåË°®ÊòéËøôÊòØ‰∏Ä‰∏™ÁπÅÂøôÁöÑÂüéÂ∏ÇÁéØÂ¢É„ÄÇ&lt;/td&gt; 
   &lt;td&gt;Âõæ‰∏≠ÊòØ‰∏Ä‰∏™ÁπÅÂøôÁöÑÂüéÂ∏ÇÊôØË±°ÔºåÊúâÂá†ËæÜÊ±ΩËΩ¶Âíå‰∏ÄËæÜÂç°ËΩ¶Ë°åÈ©∂Âú®ÂüéÂ∏ÇË°óÈÅì‰∏ä„ÄÇÂèØ‰ª•ÁúãÂà∞ËÆ∏Â§ö‰∫§ÈÄö‰ø°Âè∑ÁÅØÔºåÂÖ∂‰∏≠‰∏Ä‰∫õ‰Ωç‰∫éË°óÈÅìÂ∑¶‰æßÔºåÂè¶‰∏Ä‰∫õÂàôÂú®Âè≥‰æß„ÄÇÂèØ‰ª•ÁúãÂà∞ÊúâÂá†‰∏™‰∫∫Âú®Ë°ó‰∏äË°åËµ∞ÔºåÂÖ∂‰∏≠‰∏Ä‰∫õ‰∫∫Á´ôÂæóÁ¶ªË°óÈÅìÊõ¥Ëøë‰∏Ä‰∫õÔºåËÄåÂè¶‰∏Ä‰∫õÂàôË∑ùÁ¶ªËæÉËøú„ÄÇËøòÊúâ‰∏Ä‰∏™ÂÅúËΩ¶Ê†áÂøó‰Ωç‰∫éÁîªÈù¢ÁöÑÂ∑¶‰æßÔºåÊöóÁ§∫ÁùÄÂüéÂ∏ÇÁéØÂ¢É„ÄÇÂèØ‰ª•ÁúãÂà∞Ë°óÈÅì‰∏äÊúâ‰∏§ËæÜÊ±ΩËΩ¶Ôºå‰∏ÄËæÜÂú®Âè≥ËæπÔºåÂè¶‰∏ÄËæÜÂú®Â∑¶ËæπÔºåËøòÊúâ‰∏ÄËæÜÂú®Â∑¶Ëæπ„ÄÇËøôÂπÖÂõæÂÉèÊçïÊçâÂà∞‰∫ÜÈÉΩÂ∏ÇÁéØÂ¢É‰∏≠ÂÖ∏ÂûãÁöÑ‰∏ÄÂ§©„ÄÇ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt; &lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind-v/master/dataset/eval_images/%E5%A4%AA%E7%A9%BA%E5%AE%87%E8%88%AA%E5%91%98-Astronaut-Space.jpg" alt="astronaut"&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;/td&gt; 
   &lt;td&gt;ÂõæÁâáÊòæÁ§∫‰∫Ü‰∏Ä‰∏™ÂÆáËà™ÂëòÁöÑÂÆáËà™ÂëòË∫´Á©øÂÆáËà™ÊúçÔºåÂùêÂú®‰∏ÄÊû∂Â§ßÂûãËà™Â§©È£ûÊú∫‰∏ä„ÄÇ‰ªñ‰ª¨‰ºº‰πéÊ≠£Âú®ËøõË°å‰∏ÄÊ¨°ÂÆáËà™ÂëòÁôªÊú∫Êàñ‰∏ãÊú∫ÁöÑÊóÖÁ®ã„ÄÇÂú®ÂÆáËà™ÂëòÁöÑË∫´ÂêéÔºåÊúâ‰∏Ä‰∏™ÁÅ´ÁÆ≠ÂèëÂ∞ÑÊû∂ÔºåÂèØËÉΩÊòØÁî®Êù•ÊîØÊíëÂÆáËà™ÂëòÂú®ÊóÖÁ®ã‰∏≠ÁöÑ‰ªªÂä°„ÄÇÊ≠§Â§ñÔºåËøòÊúâ‰∏ÄÊû∂È£ûÊú∫ÂÅúÂú®Êú∫Â∫ìÈôÑËøëÔºåËøõ‰∏ÄÊ≠•Ë°®ÊòéËøôÊòØ‰∏ÄÊ¨°Ëà™Á©∫Â±ï„ÄÇÂú®È£ûÊú∫ÁöÑÂë®Âõ¥ÔºåËøòÊúâ‰∏Ä‰∫õ‰∫∫Ôºå‰ΩÜ‰ªñ‰ª¨ÁúãËµ∑Êù•Á¶ªÈ£ûÊú∫ÂæàËøë„ÄÇÂèØ‰ª•ÁúãÂà∞‰∏Ä‰∏™‰∫∫Á´ôÂú®È£ûÊú∫ÈôÑËøëÔºåÂèØËÉΩÊ≠£Âú®ËßÇÂØüÊàñÁ≠âÂæÖËà™Â§©È£ûÊú∫ÂáÜÂ§áËµ∑È£û„ÄÇ&lt;/td&gt; 
   &lt;td&gt;Âú∫ÊôØ‰∏≠Ôºå‰∏ÄÂêçÂ£´ÂÖµÊà¥ÁùÄÂ§¥ÁõîÁ´ôÂú®‰∏ÄÊû∂Â§ßÂûãÈ£ûÊú∫‰∏ä„ÄÇËøôÊû∂È£ûÊú∫‰ºº‰πéÊòØ‰∏ÄÊû∂ÂÜõÁî®ÂÜõÁî®È£ûÊú∫Ôºå‰ºº‰πéÊ≠£ÂáÜÂ§áÁôª‰∏ä‰∏ÄÊû∂È£ûÊú∫„ÄÇÂè¶‰∏Ä‰∏™‰∫∫ÂàôÁ´ôÂú®ÂâçÈù¢ÔºåÂèØËÉΩÊ≠£Âú®ËßÇÂØüÈ£ûË°åËøáÁ®ã„ÄÇÂú®È£ûÊú∫Âë®Âõ¥ÔºåÊúâÂá†‰∏™‰∫∫ÔºåÂÖ∂‰∏≠‰∏Ä‰∫õÁ´ôÂú®Â∑¶‰æßÔºåÂè¶‰∏Ä‰∫õÂàôÁ´ôÂú®Âè≥‰æß„ÄÇ‰ªñ‰ª¨‰ºº‰πéÊ≠£Âú®ËßÇÁúãÈ£ûË°åÂëòÁöÑË°®Áé∞„ÄÇÊ≠§Â§ñÔºåËøòÊúâ‰∏ÄËæÜÂç°ËΩ¶ÂÅúÂú®Èù†ËøëÂ∑¶‰æßÁöÑ‰ΩçÁΩÆÔºåÂèØËÉΩÊòØ‰∏∫‰∫ÜÊõ¥ÂÖ∑‰ΩìÂú∞ËßÇÂØüÈ£ûË°åËøáÁ®ã„ÄÇ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt; &lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind-v/master/dataset/eval_images/%E5%B0%8F%E7%8B%97%E7%BE%8E%E5%A5%B3%E6%B5%B7%E8%BE%B9-Dog-Woman-Sea.jpg" alt="dog-woman-sea"&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;/td&gt; 
   &lt;td&gt;ÂõæÁâá‰∏≠Ôºå‰∏Ä‰∏™Â•≥‰∫∫ÂùêÂú®Ê≤ôÊª©‰∏äÔºåÊâãÈáåÊãøÁùÄ‰∏ÄÂè™ÁôΩËâ≤ÁöÑÁãó„ÄÇÂ•πÁúãËµ∑Êù•ÂÉèÊòØ‰∏™Â•≥‰∫∫ÔºåÂùêÂú®Ê≤ôÂú∞‰∏äÔºåÁúãÁùÄÂ•π„ÄÇ‰∏ÄÂè™Áãó‰πüÂùêÂú®Â•πÊóÅËæπÔºåÁúãËµ∑Êù•ÂæàÊîæÊùæÂíåËàíÈÄÇ„ÄÇÊµ∑Êª©‰∏äÊï£Â∏ÉÁùÄÂÖ∂‰ªñÊ≤ôÊª©Ê∏∏ÂÆ¢ÔºåÊúâ‰∫õ‰∫∫ÂùêÁùÄÔºåËÄåÂè¶‰∏Ä‰∫õ‰∫∫ÂàôÂùêÂú®Êõ¥ËøúÁöÑÂú∞Êñπ„ÄÇËÉåÊôØ‰∏≠ÂèØ‰ª•ÁúãÂà∞‰∏ÄËâòËàπÔºåËøôË°®ÊòéËøôÊòØ‰∏Ä‰∏™ÂèóÊ¨¢ËøéÁöÑÊµ∑Êª©ÊóÖÊ∏∏ÁõÆÁöÑÂú∞„ÄÇ&lt;/td&gt; 
   &lt;td&gt;‰∏§‰∏™‰∫∫ÂùêÂú®Êµ∑Êª©‰∏äÔºå‰∏ÄËæπÊáíÊ¥ãÊ¥ãÂú∞Ë∫∫Âú®Ê≤ôÊª©‰∏äÔºåÂè¶‰∏ÄËæπÂàôÂùêÁùÄ„ÄÇ‰ªñ‰ª¨‰ºº‰πéÊ≠£Âú®‰∫´ÂèóÊµ∑ËæπÊó∂ÂÖâ„ÄÇÊµ∑Êª©‰∏äÊúâÂá†ÊääÊ§ÖÂ≠êÔºåÂÖ∂‰∏≠‰∏ÄÊääÈù†ËøëÊ≤ôÊª©ÁöÑÂ∑¶‰æßÔºåÂè¶‰∏ÄÊääÂú®‰∏≠Èó¥„ÄÇÊ≠§Â§ñÔºåËøòÊúâ‰∏ÄÂè™ÁãóË∫∫Âú®Ê≤ôÂú∞‰∏äÔºå‰∏∫Ëøô‰∏™Âú∫ÊôØÂ¢ûÊ∑ª‰∫Ü‰∏ÄÁßçÊîæÊùæÁöÑÊ∞îÊ∞õ„ÄÇ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt; &lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind-v/master/dataset/eval_images/%E5%BD%A9%E8%99%B9%E7%80%91%E5%B8%83-Rainbow-Falls.jpg" alt="rainbow-falls"&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;/td&gt; 
   &lt;td&gt;ÁÖßÁâáÊçïÊçâÂà∞‰∏ÄÂπÖÁæé‰∏ΩÂ¶ÇÁîªÁöÑÂ§ßËá™ÁÑ∂Âú∫ÊôØÔºåËÉåÊôØÊòØÈ´òÂ±±Â≥¶Â¥ñ„ÄÇÂú®Ê∞¥ËæπÔºå‰∏ÄÂ∫ßÂ∑®Â§ßÁöÑÂñ∑Ê≥âÊ®™Ë∑®ÁùÄÊ∞¥Èù¢ÔºåÂê∏ÂºïÁùÄËÆ∏Â§öÊ∏∏ÂÆ¢„ÄÇÊ∞¥Èù¢‰∏äÊúâÂá†‰∏™‰∫∫Ôºå‰ªñ‰ª¨ÊàñÁ´ôÊàñÂùêÂú®Âñ∑Ê≥âÂë®Âõ¥ÔºåÊàñÁ´ôÊàñÂùê„ÄÇÊúâ‰∫õ‰∫∫ÂèØ‰ª•ÁúãÂà∞‰ªñ‰ª¨Âú®Ê∞¥‰∏≠Ë°åËµ∞ÔºåËÄåÂÖ∂‰ªñ‰∫∫ÂàôÁ´ôÂú®Ê∞¥Ëæπ„ÄÇÊÄª‰ΩìËÄåË®ÄÔºåËøôÂπÖÁîªÊèèÁªòÁöÑÊòØ‰∏Ä‰∏™Áæé‰∏ΩËÄåÂÆÅÈùôÁöÑÁéØÂ¢ÉÔºåÂú®ÈÇ£Èáå‰∫∫‰ª¨ÂèØ‰ª•Ê¨£ËµèÂà∞Â¶ÇÁîªËà¨ÁöÑÁæéÊôØ„ÄÇ&lt;/td&gt; 
   &lt;td&gt;Âú®‰∏Ä‰∏™Áæé‰∏ΩÁöÑËìùËâ≤Â§©Á©∫‰∏ãÔºå‰∏ÄÂ∫ßÂ∑®Â§ßËÄåÂ∑®Â§ßÁöÑÁôΩËâ≤ÁÄëÂ∏É‰∏äÊñπÊÇ¨ÊåÇÁùÄ‰∏ÄÂè™Â∑®Â§ßÁöÑÊπøÊµÅÊ∞¥„ÄÇËøôÂè™ÁÄëÂ∏É‰Ωç‰∫é‰∏ÄÂ∫ßÂ±±‰∏äÔºå‰∏∫Êï¥‰∏™Âú∫ÊôØÂ¢ûÊ∑ª‰∫Ü‰∏ÄÁßçËø∑‰∫∫ËÄåÂèàÂÆÅÈùôÁöÑÊ∞îÊ∞õ„ÄÇÂú®ËøôÂπÖÂõæÂÉèÁöÑËÉåÊôØ‰∏≠ÔºåÂèØ‰ª•ÁúãÂà∞Âá†ËâòËàπÔºåÂÖ∂‰∏≠‰∏Ä‰∫õÈù†ËøëÊ∞¥ËæπÔºåÂÖ∂‰ªñÁöÑÂàôÁ¶ªÂæóËæÉËøú„ÄÇËøô‰∫õËàπÂè™‰ºº‰πéÊ≠£Âú®‰∏∫È£éÊôØÊàñÊà∑Â§ñÊ¥ªÂä®ÂÅöÂáÜÂ§á„ÄÇ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt; &lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind-v/master/dataset/eval_images/%E6%A4%85%E5%AD%90%E8%80%81%E4%BA%BA%E7%9C%8B%E4%B9%A6-Chair-Elderly-Reading.jpg" alt="elderly-reading"&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;/td&gt; 
   &lt;td&gt;Âõæ‰∏≠Ôºå‰∏Ä‰∏™Áî∑‰∫∫ÂùêÂú®ÂÖ¨Âõ≠ÁöÑÈïøÊ§Ö‰∏äÔºåÊóÅËæπÊòØ‰∏ÄÊääÁªøËâ≤Ê§ÖÂ≠ê„ÄÇ‰ªñË∫´ËæπÊúâ‰∏ÄÊú¨ÊâìÂºÄÁöÑ‰π¶Ôºå‰∏äÈù¢ÂÜôÁùÄ"ËØª‰π¶"‰∏ÄÂè•ËØùÔºåÊöóÁ§∫‰ªñÂèØËÉΩÊ≠£Âú®ÈòÖËØª„ÄÇÂÖ¨Âõ≠ÈáåÊúâ‰∏ÄÂº†ÈïøÊ§ÖÂíå‰∏ÄÂº†ÂÖ¨Âõ≠ÈïøÊ§ÖÔºå‰∏∫Âë®Âõ¥ÁöÑÁéØÂ¢ÉÂ¢ûÊ∑ª‰∫ÜÂá†ÂàÜÁîüÊ∞î„ÄÇÂú®ÂÖ¨Âõ≠ÁöÑÂë®Âõ¥ÔºåÊúâÂá†ËæÜÊ±ΩËΩ¶Âíå‰∏ÄËæÜÂç°ËΩ¶ÔºåË°®ÊòéËøôÊòØ‰∏Ä‰∏™ÂÖ¨ÂÖ±Âå∫Âüü„ÄÇÊ≠§Â§ñÔºåËøòÂèØ‰ª•ÁúãÂà∞‰∏Ä‰∏™‰∫∫Á´ôÂú®ÂÖ¨Âõ≠ÁöÑ‰∏çÂêå‰ΩçÁΩÆ‰∏äÔºåÂèØËÉΩÊòØÁ≠âÁùÄ‰∏äË∑ØÊàñËøáÈ©¨Ë∑Ø„ÄÇ&lt;/td&gt; 
   &lt;td&gt;‰∏Ä‰∏™Á©øÁùÄÁü≠Ë£§ÁöÑËÄÅ‰∫∫ÂùêÂú®ÂÖ¨Âõ≠ÈïøÊ§Ö‰∏äÔºåÂë®Âõ¥ÊòØÊ†ëÊú®„ÄÇ‰ªñ‰ºº‰πéÊ≠£Âú®ËØª‰∏ÄÊú¨‰π¶ÔºåÂèØËÉΩÊòØÂú®ËØª‰π¶„ÄÇËÉåÊôØ‰∏≠Êúâ‰∏ÄÂ∫ßÈïøÂá≥Ôºå‰∏∫Ëøô‰∏™Âú∫ÊôØÊèê‰æõ‰∫ÜÂÖÖË∂≥ÁöÑÂ∫ß‰Ωç„ÄÇÂú®ËÉåÊôØ‰∏≠ÔºåÂèØ‰ª•ÁúãÂà∞‰∏ÄÊääÊ§ÖÂ≠êÂíå‰∏ÄÂº†È§êÊ°åÔºåËøôËØ¥ÊòéËøô‰∏™Âú∫ÊôØÂèØËÉΩÊòØÂú®‰∏Ä‰∏™Êà∑Â§ñÂ∫ß‰ΩçÂå∫ÔºåÈÇ£ÈáåÊúâÊ§ÖÂ≠ê‰æõ‰∫∫‰ª¨Âùê‰∏ãÊù•ÊîæÊùæ„ÄÇ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt; &lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind-v/master/dataset/eval_images/%E7%86%8A%E7%8C%AB%E8%8D%89%E5%9C%B0-Panda-Grassland.jpg" alt="panda-grassland"&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;/td&gt; 
   &lt;td&gt;Âõæ‰∏≠Ôºå‰∏ÄÂè™ÁôΩËâ≤ÁöÑÊ£ïÁÜäÂùêÂú®ËçâÂú∞‰∏äÔºåÊóÅËæπÊòØ‰∏ÄÂè™ÈïøÁùÄÊ£ïËâ≤ÊñëÁÇπÁöÑÂ§ßÁÜä„ÄÇËøôÂè™ÁÜäÁúãËµ∑Êù•ÂæàÂÆ≥ÁæûÊàñÈ°ΩÁöÆÔºåÂõ†‰∏∫ÂÆÉÊ≠£Ë∫∫Âú®ËçâÂú∞‰∏ä‰ºëÊÅØÔºåÁúã‰∏äÂéªÂæàÊîæÊùæ„ÄÇ&lt;/td&gt; 
   &lt;td&gt;Âú®ËøôÂπÖÂõæÂÉè‰∏≠Ôºå‰∏ÄÂè™Ê£ïËâ≤ÁöÑÁÜäÊ≠£Âú®ËçâÂú∞‰∏äÊº´Ê≠•„ÄÇËøôÂè™ÁÜäË¢´ÊîæÁΩÆÂú®ËçâÂú∞‰∏äÔºåÂç†ÊçÆ‰∫ÜÁîªÈù¢ÁöÑÂ§ßÈÉ®ÂàÜÁ©∫Èó¥„ÄÇÂÆÉ‰ºº‰πéÊ≠£Âú®Ëá™ÁÑ∂ÁéØÂ¢É‰∏≠Ë°åËµ∞ÔºåÂèØËÉΩÊòØÂú®ËçâÂú∞‰∏ä„ÄÇÂú®ËÉåÊôØ‰∏≠ÔºåÊúâÂá†Ê£µÊ†ëÔºå‰∏∫ÁîªÈù¢Â¢ûÊ∑ª‰∫ÜËá™ÁÑ∂ÂÖÉÁ¥†„ÄÇ‰∏ÄÂè™È∏üÂú®Âú∫ÊôØÁöÑ‰∏≠Èó¥ÈôÑËøëÈ£ûÁøîÔºå‰∏∫ÁîªÈù¢Â¢ûÊ∑ª‰∫ÜÁîüÊ∞îÂãÉÂãÉÁöÑÊ∞îÊ∞õ„ÄÇ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt; &lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind-v/master/dataset/eval_images/%E8%87%AA%E8%A1%8C%E8%BD%A6%E9%B2%9C%E8%8A%B1-Bicycle-Flowers.jpg" alt="bicycle-flowers"&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;/td&gt; 
   &lt;td&gt;ÂõæÁâáÂ±ïÁ§∫‰∫Ü‰∏Ä‰∏™ÊºÇ‰∫ÆÁöÑËä±Áì∂ÔºåÈáåÈù¢ÊèíÊª°‰∫Ü‰∫îÈ¢úÂÖ≠Ëâ≤ÁöÑÈ≤úËä±ÂíåËä±Êùü„ÄÇËøô‰∫õËä±ÊùüÊï£ËêΩÂú®Êï¥‰∏™Ëä±Áì∂‰∏≠ÔºåÁªô‰∫∫‰∏ÄÁßçËµèÂøÉÊÇ¶ÁõÆÁöÑÊÑüËßâ„ÄÇËä±Áì∂ÈáåÊèíÁùÄ‰∫îÈ¢úÂÖ≠Ëâ≤È≤úËä±ÔºåÂàõÈÄ†Âá∫‰∏ÄÁßç‰ª§‰∫∫ËµèÂøÉÊÇ¶ÁõÆÁöÑÊôØË±°„ÄÇËøô‰∫õÈ≤úËä±Ë¢´ÊëÜÊîæÂú®‰∏ÄÂº†Ê°åÂ≠ê‰∏äÔºåÂæàÂèØËÉΩÊòØ‰∏∫‰∫ÜÂ±ïÁ§∫ÂÆÉ‰ª¨ÁöÑÁæé‰∏ΩËÄåÊëÜÊîæÁöÑ„ÄÇ&lt;/td&gt; 
   &lt;td&gt;Âú∫ÊôØ‰∏≠Ôºå‰∏ÄËæÜÁªøËâ≤ÂíåÁ¥´Ëâ≤Áõ∏Èó¥ÁöÑËá™Ë°åËΩ¶ÂÅúÂú®‰∏ÄÊ†ãÂª∫Á≠ëÊóÅËæπÔºåÂÆÉË¢´ÊîæÁΩÆÂú®‰∏ÄÊ£µÂ§ßÊ†ëÊóÅ„ÄÇËøôËæÜËá™Ë°åËΩ¶Ë¢´ÊëÜÊîæÂú®ÈôÑËøëÔºå‰∏∫Ëøô‰∏™Âú∫ÊôØÂ¢ûÊ∑ª‰∫ÜÂá†ÂàÜËâ≤ÂΩ©„ÄÇÈô§‰∫ÜËá™Ë°åËΩ¶Â§ñÔºåËøòÊúâ‰∏Ä‰∫õÂÖ∂‰ªñÁöÑËá™Ë°åËΩ¶ÔºåÂåÖÊã¨‰∏§‰∏™‰Ωç‰∫éÂâçÊôØ‰∏≠ÁöÑ‰∏Ä‰∏™Âíå‰Ωç‰∫éËÉåÊôØ‰∏≠Èù†Ëøë‰∏≠ÂøÉ‰ΩçÁΩÆÁöÑÂè¶‰∏Ä‰∏™„ÄÇËá™Ë°åËΩ¶ÁöÑÂ≠òÂú®Ë°®ÊòéÂÆÉÂèØËÉΩÊòØÂÅúÂú®ÈÇ£ÈáåÁöÑ„ÄÇ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt; &lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind-v/master/dataset/eval_images/%E8%88%9E%E8%B9%88-dance.jpg" alt="dance"&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;/td&gt; 
   &lt;td&gt;ÂõæÁâá‰∏≠ÁöÑÂ•≥‰∫∫Á©øÁùÄ‰∏Ä‰ª∂ÁôΩËâ≤ËøûË°£Ë£ôÔºåËÑö‰∏äËøòÁ≥ªÁùÄ‰∏ÄÊù°ÈªëËâ≤ÁΩëÁêÉË£ô„ÄÇÂ•πÊ≠£Âú®Ë°®Êºî‰∏Ä‰∏™ÁΩëÁêÉÊØîËµõÔºåÂæàÂèØËÉΩÊòØÂú®ÊØîËµõ‰∏≠„ÄÇÂú®ËÉåÊôØ‰∏≠ÂèØ‰ª•ÁúãÂà∞Âá†ÊääÊ§ÖÂ≠êÔºåÂèØËÉΩÊòØ‰∏∫‰∫ÜËßÇ‰ºóÊàñÂÖ∂‰ªñËßÇ‰ºóÁöÑÂ∫ß‰ΩçÂÆâÊéíËÄåÊëÜÊîæÁöÑ„ÄÇÊ≠§Â§ñÔºåËøòÊúâ‰∏Ä‰∏™ÈïøÂá≥ÊîæÂú®Âú∫ÊôØÂ∑¶‰æßÔºå‰∏∫‰∫∫‰ª¨Êèê‰æõ‰∫Ü‰∏Ä‰∏™‰ºëÊÅØÁöÑÂú∞Êñπ„ÄÇ&lt;/td&gt; 
   &lt;td&gt;‰∏ÄÂêçË∫´Á©øÁôΩËâ≤Ë°£ÊúçÁöÑÂ•≥Â≠êÁ´ôÂú®ËàûÂè∞‰∏äÔºåÊâãÈáåÊãøÁùÄ‰∏ÄÂè™ÊâãÊãøÁùÄÁôΩËâ≤È£ûÁõò„ÄÇÂ•π‰ºº‰πéÊ≠£Âú®ÂèÇÂä†‰∏Ä‰∏™ËàûÂè∞Ëàû‰ºöÊàñÊØîËµõ„ÄÇÂú∫ÊôØ‰∏≠ËøòÊúâÂÖ∂‰ªñÂá†‰∏™‰∫∫ÔºåÂÖ∂‰∏≠‰∏Ä‰∏™Á´ôÂú®ËàûÂè∞Â∑¶‰æßÔºåÂè¶‰∏Ä‰∏™Á´ôÂú®Âè≥‰æßÔºåÁ¨¨‰∏â‰∏™‰∫∫ÂàôÁ´ôÂú®Âú∫Âú∞Âè≥‰æß„ÄÇËàûÂè∞‰∏äÊúâÂá†‰∏™ËßÇ‰ºóÔºåÊúâÁöÑÁ´ôÁùÄÔºåÊúâÁöÑÂùêÁùÄÔºåËøòÊúâ‰∏Ä‰∫õÁ´ôÁùÄ„ÄÇËøôÁúãËµ∑Êù•ÂÉèÊòØ‰∏ÄÂú∫Ê¨¢‰πêÁöÑËäÇÊó•ÊàñÊ¥ªÂä®„ÄÇ&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h4&gt;Â§öÂõæÂØπËØùÔºàÊïàÊûúÂçÅÂàÜÊúâÈôêÔºâ&lt;/h4&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;ÂõæÁâá1&lt;/th&gt; 
   &lt;th&gt;ÂõæÁâá2&lt;/th&gt; 
   &lt;th&gt;512_sft_multi&lt;/th&gt; 
   &lt;th&gt;768_sft_multi&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind-v/master/dataset/eval_multi_images/bird/0.jpg" alt="a-bird.png"&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/jingyaogong/minimind-v/master/dataset/eval_multi_images/bird/1.jpg" alt="a-bird.png"&gt;&lt;/td&gt; 
   &lt;td&gt;ËøôÂπÖÂõæÂÉèÊòæÁ§∫‰∫Ü‰∏ÄÁßçÈ∏üÁ∞∏ÊàÆÁöÑÂú∫ÊôØÔºö‰∏Ä‰∏™Â•≥‰∫∫Á´ôÂú®Á∫¢ÁªøÁõ∏Èó¥ÁöÑÁ∫¢ÁªøÁõ∏Èó¥ÁöÑÁ¥´Ëâ≤È∏üÁ∞∏Êà¥Âú®Â•πË∫´‰∏ä„ÄÇÂ•≥‰∫∫Á´ôÂú®Á∫¢Ëâ≤ÁöÑÈ∏üÁ∞∏Êà¥Âú®Â•πË∫´‰∏äÔºåËÄåÂ•πÁöÑÁøªÈ¢Ü‰∏äÁöÑÈÇ£Âè™Á∫¢È∏üÂàôÁ´ôÂú®Â•πË∫´Âêé„ÄÇ&lt;/td&gt; 
   &lt;td&gt;Ëøô‰∏§Âè™È∏üÂú®Âêå‰∏ÄÁâáÊ†ëÊûó‰∏≠È£ûÁøîÔºåÊúâÁöÑ‰Ωç‰∫éÁîªÈù¢‰∏≠ÂøÉÔºåËÄåÂè¶‰∏Ä‰∫õÂàôËæÉÂ∞èÔºåÂΩ¢Êàê‰∫ÜÈ≤úÊòéÂØπÊØî„ÄÇËøôÁßçÈ∏üÁ±ªÁöÑÂá∫Áé∞Á™ÅÂá∫‰∫ÜÂÖ∂È£ûË°åËÉΩÂäõÂíåÈÄÇÂ∫îÊÄßÔºåÂõ†‰∏∫ÂÆÉ‰ª¨ËÉΩÂ§üÂú®Ê†ëÊûó‰∏≠Âø´ÈÄüËøÖÈÄüÁßªÂä®„ÄÇÊ≠§Â§ñÔºå‰∏§Âè™È∏üÁöÑ‰ΩçÁΩÆ‰∏çÂêåÔºå‰∏Ä‰∏™Âú®ÂõæÂÉèÁöÑÂ∑¶ËæπÔºåÂè¶‰∏Ä‰∏™Âú®Âè≥ËæπÔºåËøôË°®ÊòéÂÆÉ‰ª¨Âú®Âêå‰∏ÄÁâáÊ†ëÊûó‰∏≠ÁßªÂä®ÂæóÂæàËøë„ÄÇËøôÁßçÈ∏üÁ±ªÁöÑËá™ÁÑ∂Ë°å‰∏∫‰πüÊúâÂä©‰∫éÂå∫ÂàÜËøô‰∏§ÁßçÈ∏üÁ±ªÁâ©Áßç„ÄÇ&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;ÊïàÊûúÂ∞èÁªìÔºö&lt;/h3&gt; 
&lt;p&gt;ËßÜËßâ‰ø°Âè∑ÂØπ‰∫éLLMËßÜ‰Ωú‰∏ÄÁßçÁâπÊÆäÁöÑÂ§ñËØ≠Ôºå Âõ†Ê≠§‚ÄúÂ≠¶‰π†Â§ñËØ≠‚ÄùÁöÑËÉΩÂäõÈ´ò‰ΩéÔºå ÂæàÂ§ßÁ®ãÂ∫¶‰∏äÂèñÂÜ≥‰∫éLLMÁöÑËÉΩÂäõ„ÄÇ LLMÊÄßËÉΩË∂äÂº∫ÔºåÂØπÂ∫îÁöÑVLMÂøÖÁÑ∂Ë∂äÂº∫ÔºåÊ≠§Êó∂ÊïàÊûúÂ¢ûÁõä‰ºöÂæàÊòéÊòæ„ÄÇ&lt;/p&gt; 
&lt;h4&gt;Êú™Êù•ÂÄºÂæóÊîπËøõÁöÑÊñπÈù¢Ôºö&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;&amp;gt; Êõ¥ÁÆÄÂçïÁöÑProjectionÁöÑË∑®Ê®°ÊÄÅÁâπÂæÅÂØπÈΩêÊñπÂºèÔºåÁõ∏ËæÉ‰∫éCross-AttentionÂèØËÉΩÂ§Ñ‰∫éÂä£Âäø„ÄÇ
&amp;gt; ClipÊ®°ÂûãÂèØ‰ª•Â∞ùËØïÊõ¥Â§ßÊÄßËÉΩÊõ¥Âº∫ÁöÑlargeÁ≥ªÂàóÔºåÁî®Êõ¥ÂÖ∑ÁªÜÁ≤íÂ∫¶ÁöÑtokenË°®ÂæÅÂõæÂÉèÁâπÂæÅÔºåÁõÆÂâç‰ªçÁ≤óÁ≥ô„ÄÇ
&amp;gt; ÂàÜËæ®Áéá‰∏çÈ´òÔºåÁêÜËÆ∫‰∏äÂè™Êúâ224√ó224Ôºàminimind-vÊï∞ÊçÆÈõÜ‰∏∫ËäÇÁúÅÁ©∫Èó¥Ôºå‰ªÖËÆæÂÆö‰∏∫128√ó128Ôºâ„ÄÇ
&amp;gt; ...
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;üìå Acknowledge&lt;/h1&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] Â¶ÇÊûúÊÇ®ËßâÂæó &lt;code&gt;MiniMind-V&lt;/code&gt;ÂØπÊÇ®ÊúâÊâÄÂ∏ÆÂä©ÔºåÂèØ‰ª•Âú® GitHub ‰∏äÂä†‰∏Ä‰∏™‚≠ê&lt;br&gt; Ê∞¥Âπ≥ÊúâÈôêÈöæÂÖçÂ≠òÂú®Êú™Áü•ÁöÑÁ∫∞ÊºèÔºåÊ¨¢ËøéÊâÄÊúâ‰∫∫Âú®Issues‰∫§ÊµÅÊåáÊ≠£ÊàñÊèê‰∫§PRÊîπËøõÈ°πÁõÆ&lt;br&gt; ÊÇ®ÁöÑÊîØÊåÅÂ∞±ÊòØÊåÅÁª≠ÊîπËøõÈ°πÁõÆÁöÑÂä®ÂäõÔºåË∞¢Ë∞¢ÔºÅ&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;ü§ù&lt;a href="https://github.com/jingyaogong/minimind/graphs/contributors"&gt;Ë¥°ÁåÆËÄÖ&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/jingyaogong"&gt;&lt;img src="https://avatars.githubusercontent.com/u/62287848" width="70px" height="70px"&gt;&lt;/a&gt; &amp;nbsp; &lt;a href="https://github.com/xinyanghuang7"&gt;&lt;img src="https://avatars.githubusercontent.com/u/7503252" width="70px" height="70px"&gt;&lt;/a&gt; &amp;nbsp; &lt;a href="https://github.com/chuanzhubin"&gt;&lt;img src="https://avatars.githubusercontent.com/u/2813798" width="70px" height="70px"&gt;&lt;/a&gt; &amp;nbsp;&lt;/p&gt; 
&lt;h2&gt;üòäÈ∏£Ë∞¢&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/xinyanghuang7"&gt;&lt;b&gt;@xinyanghuang7&lt;/b&gt;&lt;/a&gt;: &lt;a href="https://github.com/xinyanghuang7/minimind-v/tree/hxy"&gt;üîóÂÆûÁé∞‰∫ÜÂÆåÊï¥ÁöÑÂ§öÂõæÂàÜÊîØ&lt;/a&gt;&lt;/p&gt; 
&lt;details close&gt; 
 &lt;summary&gt; &lt;b&gt;ÂèÇËÄÉÈìæÊé• &amp;amp; ÊÑüË∞¢‰ª•‰∏ã‰ºòÁßÄÁöÑËÆ∫ÊñáÊàñÈ°πÁõÆ&lt;/b&gt; &lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;ÊéíÂêç‰∏çÂàÜ‰ªª‰ΩïÂÖàÂêéÈ°∫Â∫è&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://arxiv.org/pdf/2304.08485"&gt;LlaVA&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://arxiv.org/pdf/2310.03744"&gt;LlaVA-VL&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://huggingface.co/datasets/LinkSoul/Chinese-LLaVA-Vision-Instructions"&gt;Chinese-LLaVA-Vision-Instructions&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;ü´∂ÊîØÊåÅËÄÖ&lt;/h2&gt; 
&lt;a href="https://github.com/jingyaogong/minimind-v/stargazers"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://reporoster.com/stars/dark/jingyaogong/minimind-v"&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://reporoster.com/stars/jingyaogong/minimind-v"&gt; 
  &lt;img alt="github contribution grid snake animation" src="https://reporoster.com/stars/jingyaogong/minimind-v"&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;a href="https://github.com/jingyaogong/minimind-v/network/members"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://reporoster.com/forks/dark/jingyaogong/minimind-v"&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://reporoster.com/forks/jingyaogong/minimind-v"&gt; 
  &lt;img alt="github contribution grid snake animation" src="https://reporoster.com/forks/jingyaogong/minimind-v"&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;picture&gt; 
 &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=jingyaogong/minimind-v&amp;amp;type=Date&amp;amp;theme=dark"&gt; 
 &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=jingyaogong/minimind-v&amp;amp;type=Date"&gt; 
 &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=jingyaogong/minimind-v&amp;amp;type=Date"&gt; 
&lt;/picture&gt; 
&lt;h1&gt;License&lt;/h1&gt; 
&lt;p&gt;This repository is licensed under the &lt;a href="https://raw.githubusercontent.com/jingyaogong/minimind-v/master/LICENSE"&gt;Apache-2.0 License&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>vllm-project/vllm</title>
      <link>https://github.com/vllm-project/vllm</link>
      <description>&lt;p&gt;A high-throughput and memory-efficient inference and serving engine for LLMs&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://raw.githubusercontent.com/vllm-project/vllm/main/docs/assets/logos/vllm-logo-text-dark.png"&gt; 
  &lt;img alt="vLLM" src="https://raw.githubusercontent.com/vllm-project/vllm/main/docs/assets/logos/vllm-logo-text-light.png" width="55%"&gt; 
 &lt;/picture&gt; &lt;/p&gt; 
&lt;h3 align="center"&gt; Easy, fast, and cheap LLM serving for everyone &lt;/h3&gt; 
&lt;p align="center"&gt; | &lt;a href="https://docs.vllm.ai"&gt;&lt;b&gt;Documentation&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://blog.vllm.ai/"&gt;&lt;b&gt;Blog&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://arxiv.org/abs/2309.06180"&gt;&lt;b&gt;Paper&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://x.com/vllm_project"&gt;&lt;b&gt;Twitter/X&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://discuss.vllm.ai"&gt;&lt;b&gt;User Forum&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://slack.vllm.ai"&gt;&lt;b&gt;Developer Slack&lt;/b&gt;&lt;/a&gt; | &lt;/p&gt; 
&lt;hr&gt; 
&lt;p&gt;&lt;em&gt;Latest News&lt;/em&gt; üî•&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;[2025/05] We hosted &lt;a href="https://lu.ma/c1rqyf1f"&gt;NYC vLLM Meetup&lt;/a&gt;! Please find the meetup slides &lt;a href="https://docs.google.com/presentation/d/1_q_aW_ioMJWUImf1s1YM-ZhjXz8cUeL0IJvaquOYBeA/edit?usp=sharing"&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;[2025/05] vLLM is now a hosted project under PyTorch Foundation! Please find the announcement &lt;a href="https://pytorch.org/blog/pytorch-foundation-welcomes-vllm/"&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;[2025/04] We hosted &lt;a href="https://www.sginnovate.com/event/limited-availability-morning-evening-slots-remaining-inaugural-vllm-asia-developer-day"&gt;Asia Developer Day&lt;/a&gt;! Please find the meetup slides from the vLLM team &lt;a href="https://docs.google.com/presentation/d/19cp6Qu8u48ihB91A064XfaXruNYiBOUKrBxAmDOllOo/edit?usp=sharing"&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;[2025/01] We are excited to announce the alpha release of vLLM V1: A major architectural upgrade with 1.7x speedup! Clean code, optimized execution loop, zero-overhead prefix caching, enhanced multimodal support, and more. Please check out our blog post &lt;a href="https://blog.vllm.ai/2025/01/27/v1-alpha-release.html"&gt;here&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;Previous News&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;[2025/03] We hosted &lt;a href="https://lu.ma/vllm-ollama"&gt;vLLM x Ollama Inference Night&lt;/a&gt;! Please find the meetup slides from the vLLM team &lt;a href="https://docs.google.com/presentation/d/16T2PDD1YwRnZ4Tu8Q5r6n53c5Lr5c73UV9Vd2_eBo4U/edit?usp=sharing"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2025/03] We hosted &lt;a href="https://mp.weixin.qq.com/s/n77GibL2corAtQHtVEAzfg"&gt;the first vLLM China Meetup&lt;/a&gt;! Please find the meetup slides from vLLM team &lt;a href="https://docs.google.com/presentation/d/1REHvfQMKGnvz6p3Fd23HhSO4c8j5WPGZV0bKYLwnHyQ/edit?usp=sharing"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2025/03] We hosted &lt;a href="https://lu.ma/7mu4k4xx"&gt;the East Coast vLLM Meetup&lt;/a&gt;! Please find the meetup slides &lt;a href="https://docs.google.com/presentation/d/1NHiv8EUFF1NLd3fEYODm56nDmL26lEeXCaDgyDlTsRs/edit#slide=id.g31441846c39_0_0"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2025/02] We hosted &lt;a href="https://lu.ma/h7g3kuj9"&gt;the ninth vLLM meetup&lt;/a&gt; with Meta! Please find the meetup slides from vLLM team &lt;a href="https://docs.google.com/presentation/d/1jzC_PZVXrVNSFVCW-V4cFXb6pn7zZ2CyP_Flwo05aqg/edit?usp=sharing"&gt;here&lt;/a&gt; and AMD &lt;a href="https://drive.google.com/file/d/1Zk5qEJIkTmlQ2eQcXQZlljAx3m9s7nwn/view?usp=sharing"&gt;here&lt;/a&gt;. The slides from Meta will not be posted.&lt;/li&gt; 
  &lt;li&gt;[2025/01] We hosted &lt;a href="https://lu.ma/zep56hui"&gt;the eighth vLLM meetup&lt;/a&gt; with Google Cloud! Please find the meetup slides from vLLM team &lt;a href="https://docs.google.com/presentation/d/1epVkt4Zu8Jz_S5OhEHPc798emsYh2BwYfRuDDVEF7u4/edit?usp=sharing"&gt;here&lt;/a&gt;, and Google Cloud team &lt;a href="https://drive.google.com/file/d/1h24pHewANyRL11xy5dXUbvRC9F9Kkjix/view?usp=sharing"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2024/12] vLLM joins &lt;a href="https://pytorch.org/blog/vllm-joins-pytorch"&gt;pytorch ecosystem&lt;/a&gt;! Easy, Fast, and Cheap LLM Serving for Everyone!&lt;/li&gt; 
  &lt;li&gt;[2024/11] We hosted &lt;a href="https://lu.ma/h0qvrajz"&gt;the seventh vLLM meetup&lt;/a&gt; with Snowflake! Please find the meetup slides from vLLM team &lt;a href="https://docs.google.com/presentation/d/1e3CxQBV3JsfGp30SwyvS3eM_tW-ghOhJ9PAJGK6KR54/edit?usp=sharing"&gt;here&lt;/a&gt;, and Snowflake team &lt;a href="https://docs.google.com/presentation/d/1qF3RkDAbOULwz9WK5TOltt2fE9t6uIc_hVNLFAaQX6A/edit?usp=sharing"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2024/10] We have just created a developer slack (&lt;a href="https://slack.vllm.ai"&gt;slack.vllm.ai&lt;/a&gt;) focusing on coordinating contributions and discussing features. Please feel free to join us there!&lt;/li&gt; 
  &lt;li&gt;[2024/10] Ray Summit 2024 held a special track for vLLM! Please find the opening talk slides from the vLLM team &lt;a href="https://docs.google.com/presentation/d/1B_KQxpHBTRa_mDF-tR6i8rWdOU5QoTZNcEg2MKZxEHM/edit?usp=sharing"&gt;here&lt;/a&gt;. Learn more from the &lt;a href="https://www.youtube.com/playlist?list=PLzTswPQNepXl6AQwifuwUImLPFRVpksjR"&gt;talks&lt;/a&gt; from other vLLM contributors and users!&lt;/li&gt; 
  &lt;li&gt;[2024/09] We hosted &lt;a href="https://lu.ma/87q3nvnh"&gt;the sixth vLLM meetup&lt;/a&gt; with NVIDIA! Please find the meetup slides &lt;a href="https://docs.google.com/presentation/d/1wrLGwytQfaOTd5wCGSPNhoaW3nq0E-9wqyP7ny93xRs/edit?usp=sharing"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2024/07] We hosted &lt;a href="https://lu.ma/lp0gyjqr"&gt;the fifth vLLM meetup&lt;/a&gt; with AWS! Please find the meetup slides &lt;a href="https://docs.google.com/presentation/d/1RgUD8aCfcHocghoP3zmXzck9vX3RCI9yfUAB2Bbcl4Y/edit?usp=sharing"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2024/07] In partnership with Meta, vLLM officially supports Llama 3.1 with FP8 quantization and pipeline parallelism! Please check out our blog post &lt;a href="https://blog.vllm.ai/2024/07/23/llama31.html"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2024/06] We hosted &lt;a href="https://lu.ma/agivllm"&gt;the fourth vLLM meetup&lt;/a&gt; with Cloudflare and BentoML! Please find the meetup slides &lt;a href="https://docs.google.com/presentation/d/1iJ8o7V2bQEi0BFEljLTwc5G1S10_Rhv3beed5oB0NJ4/edit?usp=sharing"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2024/04] We hosted &lt;a href="https://robloxandvllmmeetup2024.splashthat.com/"&gt;the third vLLM meetup&lt;/a&gt; with Roblox! Please find the meetup slides &lt;a href="https://docs.google.com/presentation/d/1A--47JAK4BJ39t954HyTkvtfwn0fkqtsL8NGFuslReM/edit?usp=sharing"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2024/01] We hosted &lt;a href="https://lu.ma/ygxbpzhl"&gt;the second vLLM meetup&lt;/a&gt; with IBM! Please find the meetup slides &lt;a href="https://docs.google.com/presentation/d/12mI2sKABnUw5RBWXDYY-HtHth4iMSNcEoQ10jDQbxgA/edit?usp=sharing"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2023/10] We hosted &lt;a href="https://lu.ma/first-vllm-meetup"&gt;the first vLLM meetup&lt;/a&gt; with a16z! Please find the meetup slides &lt;a href="https://docs.google.com/presentation/d/1QL-XPFXiFpDBh86DbEegFXBXFXjix4v032GhShbKf3s/edit?usp=sharing"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2023/08] We would like to express our sincere gratitude to &lt;a href="https://a16z.com/2023/08/30/supporting-the-open-source-ai-community/"&gt;Andreessen Horowitz&lt;/a&gt; (a16z) for providing a generous grant to support the open-source development and research of vLLM.&lt;/li&gt; 
  &lt;li&gt;[2023/06] We officially released vLLM! FastChat-vLLM integration has powered &lt;a href="https://chat.lmsys.org"&gt;LMSYS Vicuna and Chatbot Arena&lt;/a&gt; since mid-April. Check out our &lt;a href="https://vllm.ai"&gt;blog post&lt;/a&gt;.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;hr&gt; 
&lt;h2&gt;About&lt;/h2&gt; 
&lt;p&gt;vLLM is a fast and easy-to-use library for LLM inference and serving.&lt;/p&gt; 
&lt;p&gt;Originally developed in the &lt;a href="https://sky.cs.berkeley.edu"&gt;Sky Computing Lab&lt;/a&gt; at UC Berkeley, vLLM has evolved into a community-driven project with contributions from both academia and industry.&lt;/p&gt; 
&lt;p&gt;vLLM is fast with:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;State-of-the-art serving throughput&lt;/li&gt; 
 &lt;li&gt;Efficient management of attention key and value memory with &lt;a href="https://blog.vllm.ai/2023/06/20/vllm.html"&gt;&lt;strong&gt;PagedAttention&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Continuous batching of incoming requests&lt;/li&gt; 
 &lt;li&gt;Fast model execution with CUDA/HIP graph&lt;/li&gt; 
 &lt;li&gt;Quantizations: &lt;a href="https://arxiv.org/abs/2210.17323"&gt;GPTQ&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/2306.00978"&gt;AWQ&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/2309.05516"&gt;AutoRound&lt;/a&gt;, INT4, INT8, and FP8&lt;/li&gt; 
 &lt;li&gt;Optimized CUDA kernels, including integration with FlashAttention and FlashInfer&lt;/li&gt; 
 &lt;li&gt;Speculative decoding&lt;/li&gt; 
 &lt;li&gt;Chunked prefill&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;vLLM is flexible and easy to use with:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Seamless integration with popular Hugging Face models&lt;/li&gt; 
 &lt;li&gt;High-throughput serving with various decoding algorithms, including &lt;em&gt;parallel sampling&lt;/em&gt;, &lt;em&gt;beam search&lt;/em&gt;, and more&lt;/li&gt; 
 &lt;li&gt;Tensor, pipeline, data and expert parallelism support for distributed inference&lt;/li&gt; 
 &lt;li&gt;Streaming outputs&lt;/li&gt; 
 &lt;li&gt;OpenAI-compatible API server&lt;/li&gt; 
 &lt;li&gt;Support NVIDIA GPUs, AMD CPUs and GPUs, Intel CPUs and GPUs, PowerPC CPUs, TPU, and AWS Neuron&lt;/li&gt; 
 &lt;li&gt;Prefix caching support&lt;/li&gt; 
 &lt;li&gt;Multi-LoRA support&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;vLLM seamlessly supports most popular open-source models on HuggingFace, including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Transformer-like LLMs (e.g., Llama)&lt;/li&gt; 
 &lt;li&gt;Mixture-of-Expert LLMs (e.g., Mixtral, Deepseek-V2 and V3)&lt;/li&gt; 
 &lt;li&gt;Embedding Models (e.g., E5-Mistral)&lt;/li&gt; 
 &lt;li&gt;Multi-modal LLMs (e.g., LLaVA)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Find the full list of supported models &lt;a href="https://docs.vllm.ai/en/latest/models/supported_models.html"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;Install vLLM with &lt;code&gt;pip&lt;/code&gt; or &lt;a href="https://docs.vllm.ai/en/latest/getting_started/installation/gpu/index.html#build-wheel-from-source"&gt;from source&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install vllm
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Visit our &lt;a href="https://docs.vllm.ai/en/latest/"&gt;documentation&lt;/a&gt; to learn more.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.vllm.ai/en/latest/getting_started/installation.html"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.vllm.ai/en/latest/getting_started/quickstart.html"&gt;Quickstart&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.vllm.ai/en/latest/models/supported_models.html"&gt;List of Supported Models&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome and value any contributions and collaborations. Please check out &lt;a href="https://docs.vllm.ai/en/latest/contributing/index.html"&gt;Contributing to vLLM&lt;/a&gt; for how to get involved.&lt;/p&gt; 
&lt;h2&gt;Sponsors&lt;/h2&gt; 
&lt;p&gt;vLLM is a community project. Our compute resources for development and testing are supported by the following organizations. Thank you for your support!&lt;/p&gt; 
&lt;!-- Note: Please sort them in alphabetical order. --&gt; 
&lt;!-- Note: Please keep these consistent with docs/community/sponsors.md --&gt; 
&lt;p&gt;Cash Donations:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;a16z&lt;/li&gt; 
 &lt;li&gt;Dropbox&lt;/li&gt; 
 &lt;li&gt;Sequoia Capital&lt;/li&gt; 
 &lt;li&gt;Skywork AI&lt;/li&gt; 
 &lt;li&gt;ZhenFund&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Compute Resources:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;AMD&lt;/li&gt; 
 &lt;li&gt;Anyscale&lt;/li&gt; 
 &lt;li&gt;AWS&lt;/li&gt; 
 &lt;li&gt;Crusoe Cloud&lt;/li&gt; 
 &lt;li&gt;Databricks&lt;/li&gt; 
 &lt;li&gt;DeepInfra&lt;/li&gt; 
 &lt;li&gt;Google Cloud&lt;/li&gt; 
 &lt;li&gt;Intel&lt;/li&gt; 
 &lt;li&gt;Lambda Lab&lt;/li&gt; 
 &lt;li&gt;Nebius&lt;/li&gt; 
 &lt;li&gt;Novita AI&lt;/li&gt; 
 &lt;li&gt;NVIDIA&lt;/li&gt; 
 &lt;li&gt;Replicate&lt;/li&gt; 
 &lt;li&gt;Roblox&lt;/li&gt; 
 &lt;li&gt;RunPod&lt;/li&gt; 
 &lt;li&gt;Trainy&lt;/li&gt; 
 &lt;li&gt;UC Berkeley&lt;/li&gt; 
 &lt;li&gt;UC San Diego&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Slack Sponsor: Anyscale&lt;/p&gt; 
&lt;p&gt;We also have an official fundraising venue through &lt;a href="https://opencollective.com/vllm"&gt;OpenCollective&lt;/a&gt;. We plan to use the fund to support the development, maintenance, and adoption of vLLM.&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you use vLLM for your research, please cite our &lt;a href="https://arxiv.org/abs/2309.06180"&gt;paper&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@inproceedings{kwon2023efficient,
  title={Efficient Memory Management for Large Language Model Serving with PagedAttention},
  author={Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica},
  booktitle={Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles},
  year={2023}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Contact Us&lt;/h2&gt; 
&lt;!-- --8&lt;-- [start:contact-us] --&gt; 
&lt;ul&gt; 
 &lt;li&gt;For technical questions and feature requests, please use GitHub &lt;a href="https://github.com/vllm-project/vllm/issues"&gt;Issues&lt;/a&gt; or &lt;a href="https://github.com/vllm-project/vllm/discussions"&gt;Discussions&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;For discussing with fellow users, please use the &lt;a href="https://discuss.vllm.ai"&gt;vLLM Forum&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;For coordinating contributions and development, please use &lt;a href="https://slack.vllm.ai"&gt;Slack&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;For security disclosures, please use GitHub's &lt;a href="https://github.com/vllm-project/vllm/security/advisories"&gt;Security Advisories&lt;/a&gt; feature&lt;/li&gt; 
 &lt;li&gt;For collaborations and partnerships, please contact us at &lt;a href="mailto:vllm-questions@lists.berkeley.edu"&gt;vllm-questions@lists.berkeley.edu&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- --8&lt;-- [end:contact-us] --&gt; 
&lt;h2&gt;Media Kit&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;If you wish to use vLLM's logo, please refer to &lt;a href="https://github.com/vllm-project/media-kit"&gt;our media kit repo&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>snap-stanford/Biomni</title>
      <link>https://github.com/snap-stanford/Biomni</link>
      <description>&lt;p&gt;Biomni: a general-purpose biomedical AI agent&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/snap-stanford/Biomni/main/figs/biomni_logo.png" alt="Biomni Logo" width="600px"&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://join.slack.com/t/biomnigroup/shared_invite/zt-38dat07mc-mmDIYzyCrNtV4atULTHRiw"&gt; &lt;img src="https://img.shields.io/badge/Join-Slack-4A154B?style=for-the-badge&amp;amp;logo=slack" alt="Join Slack"&gt; &lt;/a&gt; &lt;a href="https://biomni.stanford.edu"&gt; &lt;img src="https://img.shields.io/badge/Try-Web%20UI-blue?style=for-the-badge" alt="Web UI"&gt; &lt;/a&gt; &lt;a href="https://x.com/ProjectBiomni"&gt; &lt;img src="https://img.shields.io/badge/Follow-on%20X-black?style=for-the-badge&amp;amp;logo=x" alt="Follow on X"&gt; &lt;/a&gt; &lt;a href="https://www.linkedin.com/company/project-biomni"&gt; &lt;img src="https://img.shields.io/badge/Follow-LinkedIn-0077B5?style=for-the-badge&amp;amp;logo=linkedin" alt="Follow on LinkedIn"&gt; &lt;/a&gt; &lt;a href="https://www.biorxiv.org/content/10.1101/2025.05.30.656746v1"&gt; &lt;img src="https://img.shields.io/badge/Read-Paper-green?style=for-the-badge" alt="Paper"&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h1&gt;Biomni: A General-Purpose Biomedical AI Agent&lt;/h1&gt; 
&lt;h2&gt;Overview&lt;/h2&gt; 
&lt;p&gt;Biomni is a general-purpose biomedical AI agent designed to autonomously execute a wide range of research tasks across diverse biomedical subfields. By integrating cutting-edge large language model (LLM) reasoning with retrieval-augmented planning and code-based execution, Biomni helps scientists dramatically enhance research productivity and generate testable hypotheses.&lt;/p&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;p&gt;Our software environment is massive and we provide a single setup.sh script to setup. Follow this &lt;a href="https://raw.githubusercontent.com/snap-stanford/Biomni/main/biomni_env/README.md"&gt;file&lt;/a&gt; to setup the env first.&lt;/p&gt; 
&lt;p&gt;Then activate the environment E1:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;conda activate biomni_e1
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;then install the biomni official pip package:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install biomni --upgrade
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For the latest update, install from the github source version, or do:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install git+https://github.com/snap-stanford/Biomni.git@main
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Lastly, configure your API keys using one of the following methods:&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click to expand&lt;/summary&gt; 
 &lt;h4&gt;Option 1: Using .env file (Recommended)&lt;/h4&gt; 
 &lt;p&gt;Create a &lt;code&gt;.env&lt;/code&gt; file in your project directory:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Copy the example file
cp .env.example .env

# Edit the .env file with your actual API keys
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Your &lt;code&gt;.env&lt;/code&gt; file should look like:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-env"&gt;# Required: Anthropic API Key for Claude models
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# Optional: OpenAI API Key (if using OpenAI models)
OPENAI_API_KEY=your_openai_api_key_here

# Optional: Azure OpenAI API Key (if using Azure OpenAI models)
OPENAI_API_KEY=your_azure_openai_api_key
OPENAI_ENDPOINT=https://your-resource-name.openai.azure.com/

# Optional: AI Studio Gemini API Key (if using Gemini models)
GEMINI_API_KEY=your_gemini_api_key_here

# Optional: groq API Key (if using groq as model provider)
GROQ_API_KEY=your_groq_api_key_here

# Optional: Set the source of your LLM for example:
#"OpenAI", "AzureOpenAI", "Anthropic", "Ollama", "Gemini", "Bedrock", "Groq", "Custom"
LLM_SOURCE=your_LLM_source_here

# Optional: AWS Bedrock Configuration (if using AWS Bedrock models)
AWS_BEARER_TOKEN_BEDROCK=your_bedrock_api_key_here
AWS_REGION=us-east-1

# Optional: Custom model serving configuration
# CUSTOM_MODEL_BASE_URL=http://localhost:8000/v1
# CUSTOM_MODEL_API_KEY=your_custom_api_key_here

# Optional: Biomni data path (defaults to ./data)
# BIOMNI_DATA_PATH=/path/to/your/data

# Optional: Timeout settings (defaults to 600 seconds)
# BIOMNI_TIMEOUT_SECONDS=600
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h4&gt;Option 2: Using shell environment variables&lt;/h4&gt; 
 &lt;p&gt;Alternatively, configure your API keys in bash profile &lt;code&gt;~/.bashrc&lt;/code&gt;:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;export ANTHROPIC_API_KEY="YOUR_API_KEY"
export OPENAI_API_KEY="YOUR_API_KEY" # optional if you just use Claude
export OPENAI_ENDPOINT="https://your-resource-name.openai.azure.com/" # optional unless you are using Azure
export AWS_BEARER_TOKEN_BEDROCK="YOUR_BEDROCK_API_KEY" # optional for AWS Bedrock models
export AWS_REGION="us-east-1" # optional, defaults to us-east-1 for Bedrock
export GEMINI_API_KEY="YOUR_GEMINI_API_KEY" #optional if you want to use a gemini model
export GROQ_API_KEY="YOUR_GROQ_API_KEY" # Optional: set this to use models served by Groq
export LLM_SOURCE="Groq" # Optional: set this to use models served by Groq


&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h4&gt;‚ö†Ô∏è Known Package Conflicts&lt;/h4&gt; 
&lt;p&gt;Some Python packages are not installed by default in the Biomni environment due to dependency conflicts. If you need these features, you must install the packages manually and may need to uncomment relevant code in the codebase. See the up-to-date list and details in &lt;a href="https://raw.githubusercontent.com/snap-stanford/Biomni/main/docs/known_conflicts.md"&gt;docs/known_conflicts.md&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Basic Usage&lt;/h3&gt; 
&lt;p&gt;Once inside the environment, you can start using Biomni:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from biomni.agent import A1

# Initialize the agent with data path, Data lake will be automatically downloaded on first run (~11GB)
agent = A1(path='./data', llm='claude-sonnet-4-20250514')

# Execute biomedical tasks using natural language
agent.go("Plan a CRISPR screen to identify genes that regulate T cell exhaustion, generate 32 genes that maximize the perturbation effect.")
agent.go("Perform scRNA-seq annotation at [PATH] and generate meaningful hypothesis")
agent.go("Predict ADMET properties for this compound: CC(C)CC1=CC=C(C=C1)C(C)C(=O)O")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you plan on using Azure for your model, always prefix the model name with azure- (e.g. llm='azure-gpt-4o').&lt;/p&gt; 
&lt;h2&gt;MCP (Model Context Protocol) Support&lt;/h2&gt; 
&lt;p&gt;Biomni supports MCP servers for external tool integration:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from biomni.agent import A1

agent = A1()
agent.add_mcp(config_path="./mcp_config.yaml")
agent.go("Find FDA active ingredient information for ibuprofen")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Built-in MCP Servers:&lt;/strong&gt; For usage and implementation details, see the &lt;a href="https://raw.githubusercontent.com/snap-stanford/Biomni/main/docs/mcp_integration.md"&gt;MCP Integration Documentation&lt;/a&gt; and examples in &lt;a href="https://raw.githubusercontent.com/snap-stanford/Biomni/main/tutorials/examples/add_mcp_server/"&gt;&lt;code&gt;tutorials/examples/add_mcp_server/&lt;/code&gt;&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/snap-stanford/Biomni/main/tutorials/examples/expose_biomni_server/"&gt;&lt;code&gt;tutorials/examples/expose_biomni_server/&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;ü§ù Contributing to Biomni&lt;/h2&gt; 
&lt;p&gt;Biomni is an open-science initiative that thrives on community contributions. We welcome:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;üîß New Tools&lt;/strong&gt;: Specialized analysis functions and algorithms&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üìä Datasets&lt;/strong&gt;: Curated biomedical data and knowledge bases&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üíª Software&lt;/strong&gt;: Integration of existing biomedical software packages&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üìã Benchmarks&lt;/strong&gt;: Evaluation datasets and performance metrics&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üìö Misc&lt;/strong&gt;: Tutorials, examples, and use cases&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üîß Update existing tools&lt;/strong&gt;: many current tools are not optimized - fix and replacements are welcome!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Check out this &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/snap-stanford/Biomni/main/CONTRIBUTION.md"&gt;Contributing Guide&lt;/a&gt;&lt;/strong&gt; on how to contribute to the Biomni ecosystem.&lt;/p&gt; 
&lt;p&gt;If you have particular tool/database/software in mind that you want to add, you can also submit to &lt;a href="https://forms.gle/nu2n1unzAYodTLVj6"&gt;this form&lt;/a&gt; and the biomni team will implement them.&lt;/p&gt; 
&lt;h2&gt;üî¨ Call for Contributors: Help Build Biomni-E2&lt;/h2&gt; 
&lt;p&gt;Biomni-E1 only scratches the surface of what‚Äôs possible in the biomedical action space.&lt;/p&gt; 
&lt;p&gt;Now, we‚Äôre building &lt;strong&gt;Biomni-E2&lt;/strong&gt; ‚Äî a next-generation environment developed &lt;strong&gt;with and for the community&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;We believe that by collaboratively defining and curating a shared library of standard biomedical actions, we can accelerate science for everyone.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Join us in shaping the future of biomedical AI agent.&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Contributors with significant impact&lt;/strong&gt; (e.g., 10+ significant &amp;amp; integrated tool contributions or equivalent) will be &lt;strong&gt;invited as co-authors&lt;/strong&gt; on our upcoming paper in a top-tier journal or conference.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;All contributors&lt;/strong&gt; will be acknowledged in our publications.&lt;/li&gt; 
 &lt;li&gt;More contributor perks...&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Let‚Äôs build it together.&lt;/p&gt; 
&lt;h2&gt;Tutorials and Examples&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/snap-stanford/Biomni/main/tutorials/biomni_101.ipynb"&gt;Biomni 101&lt;/a&gt;&lt;/strong&gt; - Basic concepts and first steps&lt;/p&gt; 
&lt;p&gt;More to come!&lt;/p&gt; 
&lt;h2&gt;üåê Web Interface&lt;/h2&gt; 
&lt;p&gt;Experience Biomni through our no-code web interface at &lt;strong&gt;&lt;a href="https://biomni.stanford.edu"&gt;biomni.stanford.edu&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://youtu.be/E0BRvl23hLs"&gt;&lt;img src="https://img.youtube.com/vi/E0BRvl23hLs/maxresdefault.jpg" alt="Watch the video"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Release schedule&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled&gt; 8 Real-world research task benchmark/leaderboard release&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled&gt; A tutorial on how to contribute to Biomni&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled&gt; A tutorial on baseline agents&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; MCP support&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; Biomni A1+E1 release&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Important Note&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Security warning: Currently, Biomni executes LLM-generated code with full system privileges. If you want to use it in production, please use in isolated/sandboxed environments. The agent can access files, network, and system commands. Be careful with sensitive data or credentials.&lt;/li&gt; 
 &lt;li&gt;This release was frozen as of April 15 2025, so it differs from the current web platform.&lt;/li&gt; 
 &lt;li&gt;Biomni itself is Apache 2.0-licensed, but certain integrated tools, databases, or software may carry more restrictive commercial licenses. Review each component carefully before any commercial use.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Cite Us&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;@article{huang2025biomni,
  title={Biomni: A General-Purpose Biomedical AI Agent},
  author={Huang, Kexin and Zhang, Serena and Wang, Hanchen and Qu, Yuanhao and Lu, Yingzhou and Roohani, Yusuf and Li, Ryan and Qiu, Lin and Zhang, Junze and Di, Yin and others},
  journal={bioRxiv},
  pages={2025--05},
  year={2025},
  publisher={Cold Spring Harbor Laboratory}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>modelscope/FunASR</title>
      <link>https://github.com/modelscope/FunASR</link>
      <description>&lt;p&gt;A Fundamental End-to-End Speech Recognition Toolkit and Open Source SOTA Pretrained Models, Supporting Speech Recognition, Voice Activity Detection, Text Post-processing etc.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;(&lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/README_zh.md"&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt;|English)&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/Akshay090/svg-banners"&gt;&lt;img src="https://svg-banners.vercel.app/api?type=origin&amp;amp;text1=FunASR%F0%9F%A4%A0&amp;amp;text2=%F0%9F%92%96%20A%20Fundamental%20End-to-End%20Speech%20Recognition%20Toolkit&amp;amp;width=800&amp;amp;height=210" alt="SVG Banners"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://pypi.org/project/funasr/"&gt;&lt;img src="https://img.shields.io/pypi/v/funasr" alt="PyPI"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/3839" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/3839" alt="alibaba-damo-academy%2FFunASR | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;FunASR&lt;/strong&gt; hopes to build a bridge between academic research and industrial applications on speech recognition. By supporting the training &amp;amp; finetuning of the industrial-grade speech recognition model, researchers and developers can conduct research and production of speech recognition models more conveniently, and promote the development of speech recognition ecology. ASR for FunÔºÅ&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/#highlights"&gt;&lt;strong&gt;Highlights&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://github.com/alibaba-damo-academy/FunASR#whats-new"&gt;&lt;strong&gt;News&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/#installation"&gt;&lt;strong&gt;Installation&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/#quick-start"&gt;&lt;strong&gt;Quick Start&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://github.com/alibaba-damo-academy/FunASR/raw/main/docs/tutorial/README.md"&gt;&lt;strong&gt;Tutorial&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/runtime/readme.md"&gt;&lt;strong&gt;Runtime&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/#model-zoo"&gt;&lt;strong&gt;Model Zoo&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/#contact"&gt;&lt;strong&gt;Contact&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a name="highlights"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Highlights&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;FunASR is a fundamental speech recognition toolkit that offers a variety of features, including speech recognition (ASR), Voice Activity Detection (VAD), Punctuation Restoration, Language Models, Speaker Verification, Speaker Diarization and multi-talker ASR. FunASR provides convenient scripts and tutorials, supporting inference and fine-tuning of pre-trained models.&lt;/li&gt; 
 &lt;li&gt;We have released a vast collection of academic and industrial pretrained models on the &lt;a href="https://www.modelscope.cn/models?page=1&amp;amp;tasks=auto-speech-recognition"&gt;ModelScope&lt;/a&gt; and &lt;a href="https://huggingface.co/FunASR"&gt;huggingface&lt;/a&gt;, which can be accessed through our &lt;a href="https://github.com/alibaba-damo-academy/FunASR/raw/main/docs/model_zoo/modelscope_models.md"&gt;Model Zoo&lt;/a&gt;. The representative &lt;a href="https://www.modelscope.cn/models/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/summary"&gt;Paraformer-large&lt;/a&gt;, a non-autoregressive end-to-end speech recognition model, has the advantages of high accuracy, high efficiency, and convenient deployment, supporting the rapid construction of speech recognition services. For more details on service deployment, please refer to the &lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/runtime/readme_cn.md"&gt;service deployment document&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a name="whats-new"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;What's new:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;2024/10/29: Real-time Transcription Service 1.12 released, The 2pass-offline mode supports the SensevoiceSmal modelÔºõ(&lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/runtime/readme.md"&gt;docs&lt;/a&gt;);&lt;/li&gt; 
 &lt;li&gt;2024/10/10ÔºöAdded support for the Whisper-large-v3-turbo model, a multitasking model that can perform multilingual speech recognition, speech translation, and language identification. It can be downloaded from the &lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/examples/industrial_data_pretraining/whisper/demo.py"&gt;modelscope&lt;/a&gt;, and &lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/examples/industrial_data_pretraining/whisper/demo_from_openai.py"&gt;openai&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;2024/09/26: Offline File Transcription Service 4.6, Offline File Transcription Service of English 1.7, Real-time Transcription Service 1.11 released, fix memory leak &amp;amp; Support the SensevoiceSmall onnx modelÔºõFile Transcription Service 2.0 GPU released, Fix GPU memory leak; (&lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/runtime/readme.md"&gt;docs&lt;/a&gt;);&lt;/li&gt; 
 &lt;li&gt;2024/09/25Ôºökeyword spotting models are new supported. Supports fine-tuning and inference for four models: &lt;a href="https://modelscope.cn/models/iic/speech_sanm_kws_phone-xiaoyun-commands-online"&gt;fsmn_kws&lt;/a&gt;, &lt;a href="https://modelscope.cn/models/iic/speech_sanm_kws_phone-xiaoyun-commands-online"&gt;fsmn_kws_mt&lt;/a&gt;, &lt;a href="https://modelscope.cn/models/iic/speech_sanm_kws_phone-xiaoyun-commands-offline"&gt;sanm_kws&lt;/a&gt;, &lt;a href="https://modelscope.cn/models/iic/speech_sanm_kws_phone-xiaoyun-commands-online"&gt;sanm_kws_streaming&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;2024/07/04Ôºö&lt;a href="https://github.com/FunAudioLLM/SenseVoice"&gt;SenseVoice&lt;/a&gt; is a speech foundation model with multiple speech understanding capabilities, including ASR, LID, SER, and AED.&lt;/li&gt; 
 &lt;li&gt;2024/07/01: Offline File Transcription Service GPU 1.1 released, optimize BladeDISC model compatibility issues; ref to (&lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/runtime/readme.md"&gt;docs&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;2024/06/27: Offline File Transcription Service GPU 1.0 released, supporting dynamic batch processing and multi-threading concurrency. In the long audio test set, the single-thread RTF is 0.0076, and multi-threads' speedup is 1200+ (compared to 330+ on CPU); ref to (&lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/runtime/readme.md"&gt;docs&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;2024/05/15Ôºöemotion recognition models are new supported. &lt;a href="https://modelscope.cn/models/iic/emotion2vec_plus_large/summary"&gt;emotion2vec+large&lt;/a&gt;Ôºå&lt;a href="https://modelscope.cn/models/iic/emotion2vec_plus_base/summary"&gt;emotion2vec+base&lt;/a&gt;Ôºå&lt;a href="https://modelscope.cn/models/iic/emotion2vec_plus_seed/summary"&gt;emotion2vec+seed&lt;/a&gt;. currently supports the following categories: 0: angry 1: happy 2: neutral 3: sad 4: unknown.&lt;/li&gt; 
 &lt;li&gt;2024/05/15: Offline File Transcription Service 4.5, Offline File Transcription Service of English 1.6, Real-time Transcription Service 1.10 released, adapting to FunASR 1.0 model structureÔºõ(&lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/runtime/readme.md"&gt;docs&lt;/a&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt;
 &lt;summary&gt;Full Changelog&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;2024/03/05ÔºöAdded the Qwen-Audio and Qwen-Audio-Chat large-scale audio-text multimodal models, which have topped multiple audio domain leaderboards. These models support speech dialogue, &lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/examples/industrial_data_pretraining/qwen_audio"&gt;usage&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;2024/03/05ÔºöAdded support for the Whisper-large-v3 model, a multitasking model that can perform multilingual speech recognition, speech translation, and language identification. It can be downloaded from the&lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/examples/industrial_data_pretraining/whisper/demo.py"&gt;modelscope&lt;/a&gt;, and &lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/examples/industrial_data_pretraining/whisper/demo_from_openai.py"&gt;openai&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;2024/03/05: Offline File Transcription Service 4.4, Offline File Transcription Service of English 1.5ÔºåReal-time Transcription Service 1.9 releasedÔºådocker image supports ARM64 platform, update modelscopeÔºõ(&lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/runtime/readme.md"&gt;docs&lt;/a&gt;)&lt;/li&gt; 
  &lt;li&gt;2024/01/30Ôºöfunasr-1.0 has been released (&lt;a href="https://github.com/alibaba-damo-academy/FunASR/discussions/1319"&gt;docs&lt;/a&gt;)&lt;/li&gt; 
  &lt;li&gt;2024/01/30Ôºöemotion recognition models are new supported. &lt;a href="https://www.modelscope.cn/models/iic/emotion2vec_base_finetuned/summary"&gt;model link&lt;/a&gt;, modified from &lt;a href="https://github.com/ddlBoJack/emotion2vec"&gt;repo&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;2024/01/25: Offline File Transcription Service 4.2, Offline File Transcription Service of English 1.3 releasedÔºåoptimized the VAD (Voice Activity Detection) data processing method, significantly reducing peak memory usage, memory leak optimization; Real-time Transcription Service 1.7 releasedÔºåoptimizatized the client-sideÔºõ(&lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/runtime/readme.md"&gt;docs&lt;/a&gt;)&lt;/li&gt; 
  &lt;li&gt;2024/01/09: The Funasr SDK for Windows version 2.0 has been released, featuring support for The offline file transcription service (CPU) of Mandarin 4.1, The offline file transcription service (CPU) of English 1.2, The real-time transcription service (CPU) of Mandarin 1.6. For more details, please refer to the official documentation or release notes(&lt;a href="https://www.modelscope.cn/models/damo/funasr-runtime-win-cpu-x64/summary"&gt;FunASR-Runtime-Windows&lt;/a&gt;)&lt;/li&gt; 
  &lt;li&gt;2024/01/03: File Transcription Service 4.0 released, Added support for 8k models, optimized timestamp mismatch issues and added sentence-level timestamps, improved the effectiveness of English word FST hotwords, supported automated configuration of thread parameters, and fixed known crash issues as well as memory leak problems, refer to (&lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/runtime/readme.md#file-transcription-service-mandarin-cpu"&gt;docs&lt;/a&gt;).&lt;/li&gt; 
  &lt;li&gt;2024/01/03: Real-time Transcription Service 1.6 releasedÔºåThe 2pass-offline mode supports Ngram language model decoding and WFST hotwords, while also addressing known crash issues and memory leak problems, (&lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/runtime/readme.md#the-real-time-transcription-service-mandarin-cpu"&gt;docs&lt;/a&gt;)&lt;/li&gt; 
  &lt;li&gt;2024/01/03: Fixed known crash issues as well as memory leak problems, (&lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/runtime/readme.md#file-transcription-service-english-cpu"&gt;docs&lt;/a&gt;).&lt;/li&gt; 
  &lt;li&gt;2023/12/04: The Funasr SDK for Windows version 1.0 has been released, featuring support for The offline file transcription service (CPU) of Mandarin, The offline file transcription service (CPU) of English, The real-time transcription service (CPU) of Mandarin. For more details, please refer to the official documentation or release notes(&lt;a href="https://www.modelscope.cn/models/damo/funasr-runtime-win-cpu-x64/summary"&gt;FunASR-Runtime-Windows&lt;/a&gt;)&lt;/li&gt; 
  &lt;li&gt;2023/11/08: The offline file transcription service 3.0 (CPU) of Mandarin has been released, adding punctuation large model, Ngram language model, and wfst hot words. For detailed information, please refer to &lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/runtime#file-transcription-service-mandarin-cpu"&gt;docs&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;2023/10/17: The offline file transcription service (CPU) of English has been released. For more details, please refer to (&lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/runtime#file-transcription-service-english-cpu"&gt;docs&lt;/a&gt;).&lt;/li&gt; 
  &lt;li&gt;2023/10/13: &lt;a href="https://slidespeech.github.io/"&gt;SlideSpeech&lt;/a&gt;: A large scale multi-modal audio-visual corpus with a significant amount of real-time synchronized slides.&lt;/li&gt; 
  &lt;li&gt;2023/10/10: The ASR-SpeakersDiarization combined pipeline &lt;a href="https://github.com/alibaba-damo-academy/FunASR/raw/main/egs_modelscope/asr_vad_spk/speech_paraformer-large-vad-punc-spk_asr_nat-zh-cn/demo.py"&gt;Paraformer-VAD-SPK&lt;/a&gt; is now released. Experience the model to get recognition results with speaker information.&lt;/li&gt; 
  &lt;li&gt;2023/10/07: &lt;a href="https://github.com/alibaba-damo-academy/FunCodec"&gt;FunCodec&lt;/a&gt;: A Fundamental, Reproducible and Integrable Open-source Toolkit for Neural Speech Codec.&lt;/li&gt; 
  &lt;li&gt;2023/09/01: The offline file transcription service 2.0 (CPU) of Mandarin has been released, with added support for ffmpeg, timestamp, and hotword models. For more details, please refer to (&lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/runtime#file-transcription-service-mandarin-cpu"&gt;docs&lt;/a&gt;).&lt;/li&gt; 
  &lt;li&gt;2023/08/07: The real-time transcription service (CPU) of Mandarin has been released. For more details, please refer to (&lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/runtime#the-real-time-transcription-service-mandarin-cpu"&gt;docs&lt;/a&gt;).&lt;/li&gt; 
  &lt;li&gt;2023/07/17: BAT is released, which is a low-latency and low-memory-consumption RNN-T model. For more details, please refer to (&lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/egs/aishell/bat"&gt;BAT&lt;/a&gt;).&lt;/li&gt; 
  &lt;li&gt;2023/06/26: ASRU2023 Multi-Channel Multi-Party Meeting Transcription Challenge 2.0 completed the competition and announced the results. For more details, please refer to (&lt;a href="https://alibaba-damo-academy.github.io/FunASR/m2met2/index.html"&gt;M2MeT2.0&lt;/a&gt;).&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;p&gt;&lt;a name="Installation"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Requirements&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;python&amp;gt;=3.8
torch&amp;gt;=1.13
torchaudio
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Install for pypi&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip3 install -U funasr
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Or install from source code&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;git clone https://github.com/alibaba/FunASR.git &amp;amp;&amp;amp; cd FunASR
pip3 install -e ./
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Install modelscope or huggingface_hub for the pretrained models (Optional)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pip3 install -U modelscope huggingface_hub
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Model Zoo&lt;/h2&gt; 
&lt;p&gt;FunASR has open-sourced a large number of pre-trained models on industrial data. You are free to use, copy, modify, and share FunASR models under the &lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/MODEL_LICENSE"&gt;Model License Agreement&lt;/a&gt;. Below are some representative models, for more models please refer to the &lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/model_zoo"&gt;Model Zoo&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;(Note: ‚≠ê represents the ModelScope model zoo, ü§ó represents the Huggingface model zoo, üçÄ represents the OpenAI model zoo)&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;Model Name&lt;/th&gt; 
   &lt;th align="center"&gt;Task Details&lt;/th&gt; 
   &lt;th align="center"&gt;Training Data&lt;/th&gt; 
   &lt;th align="center"&gt;Parameters&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;SenseVoiceSmall &lt;br&gt; (&lt;a href="https://www.modelscope.cn/models/iic/SenseVoiceSmall"&gt;‚≠ê&lt;/a&gt; &lt;a href="https://huggingface.co/FunAudioLLM/SenseVoiceSmall"&gt;ü§ó&lt;/a&gt; )&lt;/td&gt; 
   &lt;td align="center"&gt;multiple speech understanding capabilities, including ASR, ITN, LID, SER, and AED, support languages such as zh, yue, en, ja, ko&lt;/td&gt; 
   &lt;td align="center"&gt;300000 hours&lt;/td&gt; 
   &lt;td align="center"&gt;234M&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;paraformer-zh &lt;br&gt; (&lt;a href="https://www.modelscope.cn/models/damo/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch/summary"&gt;‚≠ê&lt;/a&gt; &lt;a href="https://huggingface.co/funasr/paraformer-zh"&gt;ü§ó&lt;/a&gt; )&lt;/td&gt; 
   &lt;td align="center"&gt;speech recognition, with timestamps, non-streaming&lt;/td&gt; 
   &lt;td align="center"&gt;60000 hours, Mandarin&lt;/td&gt; 
   &lt;td align="center"&gt;220M&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;
    &lt;nobr&gt;
     paraformer-zh-streaming 
     &lt;br&gt; ( 
     &lt;a href="https://modelscope.cn/models/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-online/summary"&gt;‚≠ê&lt;/a&gt; 
     &lt;a href="https://huggingface.co/funasr/paraformer-zh-streaming"&gt;ü§ó&lt;/a&gt; )
    &lt;/nobr&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;speech recognition, streaming&lt;/td&gt; 
   &lt;td align="center"&gt;60000 hours, Mandarin&lt;/td&gt; 
   &lt;td align="center"&gt;220M&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;paraformer-en &lt;br&gt; ( &lt;a href="https://www.modelscope.cn/models/damo/speech_paraformer-large-vad-punc_asr_nat-en-16k-common-vocab10020/summary"&gt;‚≠ê&lt;/a&gt; &lt;a href="https://huggingface.co/funasr/paraformer-en"&gt;ü§ó&lt;/a&gt; )&lt;/td&gt; 
   &lt;td align="center"&gt;speech recognition, without timestamps, non-streaming&lt;/td&gt; 
   &lt;td align="center"&gt;50000 hours, English&lt;/td&gt; 
   &lt;td align="center"&gt;220M&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;conformer-en &lt;br&gt; ( &lt;a href="https://modelscope.cn/models/damo/speech_conformer_asr-en-16k-vocab4199-pytorch/summary"&gt;‚≠ê&lt;/a&gt; &lt;a href="https://huggingface.co/funasr/conformer-en"&gt;ü§ó&lt;/a&gt; )&lt;/td&gt; 
   &lt;td align="center"&gt;speech recognition, non-streaming&lt;/td&gt; 
   &lt;td align="center"&gt;50000 hours, English&lt;/td&gt; 
   &lt;td align="center"&gt;220M&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;ct-punc &lt;br&gt; ( &lt;a href="https://modelscope.cn/models/damo/punc_ct-transformer_cn-en-common-vocab471067-large/summary"&gt;‚≠ê&lt;/a&gt; &lt;a href="https://huggingface.co/funasr/ct-punc"&gt;ü§ó&lt;/a&gt; )&lt;/td&gt; 
   &lt;td align="center"&gt;punctuation restoration&lt;/td&gt; 
   &lt;td align="center"&gt;100M, Mandarin and English&lt;/td&gt; 
   &lt;td align="center"&gt;290M&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;fsmn-vad &lt;br&gt; ( &lt;a href="https://modelscope.cn/models/damo/speech_fsmn_vad_zh-cn-16k-common-pytorch/summary"&gt;‚≠ê&lt;/a&gt; &lt;a href="https://huggingface.co/funasr/fsmn-vad"&gt;ü§ó&lt;/a&gt; )&lt;/td&gt; 
   &lt;td align="center"&gt;voice activity detection&lt;/td&gt; 
   &lt;td align="center"&gt;5000 hours, Mandarin and English&lt;/td&gt; 
   &lt;td align="center"&gt;0.4M&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;fsmn-kws &lt;br&gt; ( &lt;a href="https://modelscope.cn/models/iic/speech_charctc_kws_phone-xiaoyun/summary"&gt;‚≠ê&lt;/a&gt; )&lt;/td&gt; 
   &lt;td align="center"&gt;keyword spottingÔºåstreaming&lt;/td&gt; 
   &lt;td align="center"&gt;5000 hours, Mandarin&lt;/td&gt; 
   &lt;td align="center"&gt;0.7M&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;fa-zh &lt;br&gt; ( &lt;a href="https://modelscope.cn/models/damo/speech_timestamp_prediction-v1-16k-offline/summary"&gt;‚≠ê&lt;/a&gt; &lt;a href="https://huggingface.co/funasr/fa-zh"&gt;ü§ó&lt;/a&gt; )&lt;/td&gt; 
   &lt;td align="center"&gt;timestamp prediction&lt;/td&gt; 
   &lt;td align="center"&gt;5000 hours, Mandarin&lt;/td&gt; 
   &lt;td align="center"&gt;38M&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;cam++ &lt;br&gt; ( &lt;a href="https://modelscope.cn/models/iic/speech_campplus_sv_zh-cn_16k-common/summary"&gt;‚≠ê&lt;/a&gt; &lt;a href="https://huggingface.co/funasr/campplus"&gt;ü§ó&lt;/a&gt; )&lt;/td&gt; 
   &lt;td align="center"&gt;speaker verification/diarization&lt;/td&gt; 
   &lt;td align="center"&gt;5000 hours&lt;/td&gt; 
   &lt;td align="center"&gt;7.2M&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Whisper-large-v3 &lt;br&gt; (&lt;a href="https://www.modelscope.cn/models/iic/Whisper-large-v3/summary"&gt;‚≠ê&lt;/a&gt; &lt;a href="https://github.com/openai/whisper"&gt;üçÄ&lt;/a&gt; )&lt;/td&gt; 
   &lt;td align="center"&gt;speech recognition, with timestamps, non-streaming&lt;/td&gt; 
   &lt;td align="center"&gt;multilingual&lt;/td&gt; 
   &lt;td align="center"&gt;1550 M&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Whisper-large-v3-turbo &lt;br&gt; (&lt;a href="https://www.modelscope.cn/models/iic/Whisper-large-v3-turbo/summary"&gt;‚≠ê&lt;/a&gt; &lt;a href="https://github.com/openai/whisper"&gt;üçÄ&lt;/a&gt; )&lt;/td&gt; 
   &lt;td align="center"&gt;speech recognition, with timestamps, non-streaming&lt;/td&gt; 
   &lt;td align="center"&gt;multilingual&lt;/td&gt; 
   &lt;td align="center"&gt;809 M&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Qwen-Audio &lt;br&gt; (&lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/examples/industrial_data_pretraining/qwen_audio/demo.py"&gt;‚≠ê&lt;/a&gt; &lt;a href="https://huggingface.co/Qwen/Qwen-Audio"&gt;ü§ó&lt;/a&gt; )&lt;/td&gt; 
   &lt;td align="center"&gt;audio-text multimodal models (pretraining)&lt;/td&gt; 
   &lt;td align="center"&gt;multilingual&lt;/td&gt; 
   &lt;td align="center"&gt;8B&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Qwen-Audio-Chat &lt;br&gt; (&lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/examples/industrial_data_pretraining/qwen_audio/demo_chat.py"&gt;‚≠ê&lt;/a&gt; &lt;a href="https://huggingface.co/Qwen/Qwen-Audio-Chat"&gt;ü§ó&lt;/a&gt; )&lt;/td&gt; 
   &lt;td align="center"&gt;audio-text multimodal models (chat)&lt;/td&gt; 
   &lt;td align="center"&gt;multilingual&lt;/td&gt; 
   &lt;td align="center"&gt;8B&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;emotion2vec+large &lt;br&gt; (&lt;a href="https://modelscope.cn/models/iic/emotion2vec_plus_large/summary"&gt;‚≠ê&lt;/a&gt; &lt;a href="https://huggingface.co/emotion2vec/emotion2vec_plus_large"&gt;ü§ó&lt;/a&gt; )&lt;/td&gt; 
   &lt;td align="center"&gt;speech emotion recongintion&lt;/td&gt; 
   &lt;td align="center"&gt;40000 hours&lt;/td&gt; 
   &lt;td align="center"&gt;300M&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;a name="quick-start"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;p&gt;Below is a quick start tutorial. Test audio files (&lt;a href="https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/vad_example.wav"&gt;Mandarin&lt;/a&gt;, &lt;a href="https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/asr_example_en.wav"&gt;English&lt;/a&gt;).&lt;/p&gt; 
&lt;h3&gt;Command-line usage&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;funasr ++model=paraformer-zh ++vad_model="fsmn-vad" ++punc_model="ct-punc" ++input=asr_example_zh.wav
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Notes: Support recognition of single audio file, as well as file list in Kaldi-style wav.scp format: &lt;code&gt;wav_id wav_pat&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Speech Recognition (Non-streaming)&lt;/h3&gt; 
&lt;h4&gt;SenseVoice&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from funasr import AutoModel
from funasr.utils.postprocess_utils import rich_transcription_postprocess

model_dir = "iic/SenseVoiceSmall"

model = AutoModel(
    model=model_dir,
    vad_model="fsmn-vad",
    vad_kwargs={"max_single_segment_time": 30000},
    device="cuda:0",
)

# en
res = model.generate(
    input=f"{model.model_path}/example/en.mp3",
    cache={},
    language="auto",  # "zn", "en", "yue", "ja", "ko", "nospeech"
    use_itn=True,
    batch_size_s=60,
    merge_vad=True,  #
    merge_length_s=15,
)
text = rich_transcription_postprocess(res[0]["text"])
print(text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Parameter Description:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;model_dir&lt;/code&gt;: The name of the model, or the path to the model on the local disk.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;vad_model&lt;/code&gt;: This indicates the activation of VAD (Voice Activity Detection). The purpose of VAD is to split long audio into shorter clips. In this case, the inference time includes both VAD and SenseVoice total consumption, and represents the end-to-end latency. If you wish to test the SenseVoice model's inference time separately, the VAD model can be disabled.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;vad_kwargs&lt;/code&gt;: Specifies the configurations for the VAD model. &lt;code&gt;max_single_segment_time&lt;/code&gt;: denotes the maximum duration for audio segmentation by the &lt;code&gt;vad_model&lt;/code&gt;, with the unit being milliseconds (ms).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;use_itn&lt;/code&gt;: Whether the output result includes punctuation and inverse text normalization.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;batch_size_s&lt;/code&gt;: Indicates the use of dynamic batching, where the total duration of audio in the batch is measured in seconds (s).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;merge_vad&lt;/code&gt;: Whether to merge short audio fragments segmented by the VAD model, with the merged length being &lt;code&gt;merge_length_s&lt;/code&gt;, in seconds (s).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ban_emo_unk&lt;/code&gt;: Whether to ban the output of the &lt;code&gt;emo_unk&lt;/code&gt; token.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Paraformer&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from funasr import AutoModel
# paraformer-zh is a multi-functional asr model
# use vad, punc, spk or not as you need
model = AutoModel(model="paraformer-zh",  vad_model="fsmn-vad",  punc_model="ct-punc", 
                  # spk_model="cam++", 
                  )
res = model.generate(input=f"{model.model_path}/example/asr_example.wav", 
                     batch_size_s=300, 
                     hotword='È≠îÊê≠')
print(res)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note: &lt;code&gt;hub&lt;/code&gt;: represents the model repository, &lt;code&gt;ms&lt;/code&gt; stands for selecting ModelScope download, &lt;code&gt;hf&lt;/code&gt; stands for selecting Huggingface download.&lt;/p&gt; 
&lt;h3&gt;Speech Recognition (Streaming)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from funasr import AutoModel

chunk_size = [0, 10, 5] #[0, 10, 5] 600ms, [0, 8, 4] 480ms
encoder_chunk_look_back = 4 #number of chunks to lookback for encoder self-attention
decoder_chunk_look_back = 1 #number of encoder chunks to lookback for decoder cross-attention

model = AutoModel(model="paraformer-zh-streaming")

import soundfile
import os

wav_file = os.path.join(model.model_path, "example/asr_example.wav")
speech, sample_rate = soundfile.read(wav_file)
chunk_stride = chunk_size[1] * 960 # 600ms

cache = {}
total_chunk_num = int(len((speech)-1)/chunk_stride+1)
for i in range(total_chunk_num):
    speech_chunk = speech[i*chunk_stride:(i+1)*chunk_stride]
    is_final = i == total_chunk_num - 1
    res = model.generate(input=speech_chunk, cache=cache, is_final=is_final, chunk_size=chunk_size, encoder_chunk_look_back=encoder_chunk_look_back, decoder_chunk_look_back=decoder_chunk_look_back)
    print(res)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note: &lt;code&gt;chunk_size&lt;/code&gt; is the configuration for streaming latency.&lt;code&gt; [0,10,5]&lt;/code&gt; indicates that the real-time display granularity is &lt;code&gt;10*60=600ms&lt;/code&gt;, and the lookahead information is &lt;code&gt;5*60=300ms&lt;/code&gt;. Each inference input is &lt;code&gt;600ms&lt;/code&gt; (sample points are &lt;code&gt;16000*0.6=960&lt;/code&gt;), and the output is the corresponding text. For the last speech segment input, &lt;code&gt;is_final=True&lt;/code&gt; needs to be set to force the output of the last word.&lt;/p&gt; 
&lt;details&gt;
 &lt;summary&gt;More Examples&lt;/summary&gt; 
 &lt;h3&gt;Voice Activity Detection (Non-Streaming)&lt;/h3&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from funasr import AutoModel

model = AutoModel(model="fsmn-vad")
wav_file = f"{model.model_path}/example/vad_example.wav"
res = model.generate(input=wav_file)
print(res)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Note: The output format of the VAD model is: &lt;code&gt;[[beg1, end1], [beg2, end2], ..., [begN, endN]]&lt;/code&gt;, where &lt;code&gt;begN/endN&lt;/code&gt; indicates the starting/ending point of the &lt;code&gt;N-th&lt;/code&gt; valid audio segment, measured in milliseconds.&lt;/p&gt; 
 &lt;h3&gt;Voice Activity Detection (Streaming)&lt;/h3&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from funasr import AutoModel

chunk_size = 200 # ms
model = AutoModel(model="fsmn-vad")

import soundfile

wav_file = f"{model.model_path}/example/vad_example.wav"
speech, sample_rate = soundfile.read(wav_file)
chunk_stride = int(chunk_size * sample_rate / 1000)

cache = {}
total_chunk_num = int(len((speech)-1)/chunk_stride+1)
for i in range(total_chunk_num):
    speech_chunk = speech[i*chunk_stride:(i+1)*chunk_stride]
    is_final = i == total_chunk_num - 1
    res = model.generate(input=speech_chunk, cache=cache, is_final=is_final, chunk_size=chunk_size)
    if len(res[0]["value"]):
        print(res)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Note: The output format for the streaming VAD model can be one of four scenarios:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;[[beg1, end1], [beg2, end2], .., [begN, endN]]&lt;/code&gt;ÔºöThe same as the offline VAD output result mentioned above.&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;[[beg, -1]]&lt;/code&gt;ÔºöIndicates that only a starting point has been detected.&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;[[-1, end]]&lt;/code&gt;ÔºöIndicates that only an ending point has been detected.&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;[]&lt;/code&gt;ÔºöIndicates that neither a starting point nor an ending point has been detected.&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;The output is measured in milliseconds and represents the absolute time from the starting point.&lt;/p&gt; 
 &lt;h3&gt;Punctuation Restoration&lt;/h3&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from funasr import AutoModel

model = AutoModel(model="ct-punc")
res = model.generate(input="ÈÇ£‰ªäÂ§©ÁöÑ‰ºöÂ∞±Âà∞ËøôÈáåÂêß happy new year ÊòéÂπ¥ËßÅ")
print(res)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;Timestamp Prediction&lt;/h3&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from funasr import AutoModel

model = AutoModel(model="fa-zh")
wav_file = f"{model.model_path}/example/asr_example.wav"
text_file = f"{model.model_path}/example/text.txt"
res = model.generate(input=(wav_file, text_file), data_type=("sound", "text"))
print(res)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;Speech Emotion Recognition&lt;/h3&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from funasr import AutoModel

model = AutoModel(model="emotion2vec_plus_large")

wav_file = f"{model.model_path}/example/test.wav"

res = model.generate(wav_file, output_dir="./outputs", granularity="utterance", extract_embedding=False)
print(res)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;More usages ref to &lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/docs/tutorial/README_zh.md"&gt;docs&lt;/a&gt;, more examples ref to &lt;a href="https://github.com/alibaba-damo-academy/FunASR/tree/main/examples/industrial_data_pretraining"&gt;demo&lt;/a&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;Export ONNX&lt;/h2&gt; 
&lt;h3&gt;Command-line usage&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;funasr-export ++model=paraformer ++quantize=false ++device=cpu
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Python&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from funasr import AutoModel

model = AutoModel(model="paraformer", device="cpu")

res = model.export(quantize=False)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Test ONNX&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# pip3 install -U funasr-onnx
from pathlib import Path
from runtime.python.onnxruntime.funasr_onnx.paraformer_bin import Paraformer


home_dir = Path.home()

model_dir = "damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch"
model = Paraformer(model_dir, batch_size=1, quantize=True)

wav_path = [f"{home_dir}/.cache/modelscope/hub/models/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/example/asr_example.wav"]

result = model(wav_path)
print(result)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;More examples ref to &lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/runtime/python/onnxruntime"&gt;demo&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Deployment Service&lt;/h2&gt; 
&lt;p&gt;FunASR supports deploying pre-trained or further fine-tuned models for service. Currently, it supports the following types of service deployment:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;File transcription service, Mandarin, CPU version, done&lt;/li&gt; 
 &lt;li&gt;The real-time transcription service, Mandarin (CPU), done&lt;/li&gt; 
 &lt;li&gt;File transcription service, English, CPU version, done&lt;/li&gt; 
 &lt;li&gt;File transcription service, Mandarin, GPU version, in progress&lt;/li&gt; 
 &lt;li&gt;and more.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For more detailed information, please refer to the &lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/runtime/readme.md"&gt;service deployment documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a name="contact"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Community Communication&lt;/h2&gt; 
&lt;p&gt;If you encounter problems in use, you can directly raise Issues on the github page.&lt;/p&gt; 
&lt;p&gt;You can also scan the following DingTalk group to join the community group for communication and discussion.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;DingTalk group&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;
    &lt;div align="left"&gt;
     &lt;img src="https://raw.githubusercontent.com/modelscope/FunASR/main/docs/images/dingding.png" width="250"&gt;
    &lt;/div&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Contributors&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;
    &lt;div align="left"&gt;
     &lt;img src="https://raw.githubusercontent.com/modelscope/FunASR/main/docs/images/alibaba.png" width="260"&gt;
    &lt;/div&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;
    &lt;div align="left"&gt;
     &lt;img src="https://raw.githubusercontent.com/modelscope/FunASR/main/docs/images/nwpu.png" width="260"&gt;
    &lt;/div&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;img src="https://raw.githubusercontent.com/modelscope/FunASR/main/docs/images/China_Telecom.png" width="200"&gt; &lt;/th&gt; 
   &lt;th align="center"&gt;&lt;img src="https://raw.githubusercontent.com/modelscope/FunASR/main/docs/images/RapidAI.png" width="200"&gt; &lt;/th&gt; 
   &lt;th align="center"&gt;&lt;img src="https://raw.githubusercontent.com/modelscope/FunASR/main/docs/images/aihealthx.png" width="200"&gt; &lt;/th&gt; 
   &lt;th align="center"&gt;&lt;img src="https://raw.githubusercontent.com/modelscope/FunASR/main/docs/images/XVERSE.png" width="250"&gt; &lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
&lt;/table&gt; 
&lt;p&gt;The contributors can be found in &lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/Acknowledge.md"&gt;contributors list&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under &lt;a href="https://opensource.org/licenses/MIT"&gt;The MIT License&lt;/a&gt;. FunASR also contains various third-party components and some code modified from other repos under other open source licenses. The use of pretraining model is subject to &lt;a href="https://raw.githubusercontent.com/modelscope/FunASR/main/MODEL_LICENSE"&gt;model license&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Citations&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@inproceedings{gao2023funasr,
  author={Zhifu Gao and Zerui Li and Jiaming Wang and Haoneng Luo and Xian Shi and Mengzhe Chen and Yabin Li and Lingyun Zuo and Zhihao Du and Zhangyu Xiao and Shiliang Zhang},
  title={FunASR: A Fundamental End-to-End Speech Recognition Toolkit},
  year={2023},
  booktitle={INTERSPEECH},
}
@inproceedings{An2023bat,
  author={Keyu An and Xian Shi and Shiliang Zhang},
  title={BAT: Boundary aware transducer for memory-efficient and low-latency ASR},
  year={2023},
  booktitle={INTERSPEECH},
}
@inproceedings{gao22b_interspeech,
  author={Zhifu Gao and ShiLiang Zhang and Ian McLoughlin and Zhijie Yan},
  title={Paraformer: Fast and Accurate Parallel Transformer for Non-autoregressive End-to-End Speech Recognition},
  year=2022,
  booktitle={Proc. Interspeech 2022},
  pages={2063--2067},
  doi={10.21437/Interspeech.2022-9996}
}
@inproceedings{shi2023seaco,
  author={Xian Shi and Yexin Yang and Zerui Li and Yanni Chen and Zhifu Gao and Shiliang Zhang},
  title={SeACo-Paraformer: A Non-Autoregressive ASR System with Flexible and Effective Hotword Customization Ability},
  year={2023},
  booktitle={ICASSP2024}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>yeongpin/cursor-free-vip</title>
      <link>https://github.com/yeongpin/cursor-free-vip</link>
      <description>&lt;p&gt;[Support 0.49.x]ÔºàReset Cursor AI MachineID &amp; Bypass Higher Token LimitÔºâ Cursor Ai ÔºåËá™Âä®ÈáçÁΩÆÊú∫Âô®ID Ôºå ÂÖçË¥πÂçáÁ∫ß‰ΩøÁî®ProÂäüËÉΩ: You've reached your trial request limit. / Too many free trial accounts used on this machine. Please upgrade to pro. We have this limit in place to prevent abuse. Please let us know if you believe this is a mistake.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;‚û§ Cursor Free VIP&lt;/h1&gt; 
&lt;div align="center"&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/yeongpin/cursor-free-vip/main/images/logo.png" alt="Cursor Pro Logo" width="200" style="border-radius: 6px;"&gt; &lt;/p&gt; 
 &lt;p align="center"&gt; &lt;/p&gt;
 &lt;p&gt;&lt;a href="https://github.com/yeongpin/cursor-free-vip/releases/latest"&gt;&lt;img src="https://img.shields.io/endpoint?url=https://api.pinstudios.net/api/badges/release/yeongpin/cursor-free-vip" alt="Release"&gt;&lt;/a&gt; &lt;a href="https://creativecommons.org/licenses/by-nc-nd/4.0/"&gt;&lt;img src="https://img.shields.io/badge/License-CC_BY--NC--ND_4.0-lightgrey.svg?sanitize=true" alt="License: CC BY-NC-ND 4.0"&gt;&lt;/a&gt; &lt;a href="https://github.com/yeongpin/cursor-free-vip/stargazers"&gt;&lt;img src="https://img.shields.io/endpoint?url=https://api.pinstudios.net/api/badges/stars/yeongpin/cursor-free-vip" alt="Stars"&gt;&lt;/a&gt; &lt;a href="https://github.com/yeongpin/cursor-free-vip/releases/latest"&gt;&lt;img src="https://img.shields.io/endpoint?url=https://api.pinstudios.net/api/badges/downloads/yeongpin/cursor-free-vip/total" alt="Downloads"&gt;&lt;/a&gt; &lt;a href="https://buymeacoffee.com/yeongpin" target="_blank"&gt;&lt;img alt="Buy Me a Coffee" src="https://img.shields.io/badge/Buy%20Me%20a%20Coffee-Support%20Me-FFDA33"&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/yeongpin/cursor-free-vip"&gt;&lt;img src="https://devin.ai/assets/deepwiki-badge.png" alt="Ask DeepWiki.com" height="20"&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/13425" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/13425" alt="yeongpin%2Fcursor-free-vip | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"&gt;&lt;/a&gt; &lt;br&gt; &lt;a href="https://www.buymeacoffee.com/yeongpin" target="_blank"&gt; &lt;img src="https://img.buymeacoffee.com/button-api/?text=buy%20me%20a%20coffee&amp;amp;emoji=%E2%98%95&amp;amp;slug=yeongpin&amp;amp;button_colour=ffda33&amp;amp;font_colour=000000&amp;amp;font_family=Bree&amp;amp;outline_colour=000000&amp;amp;coffee_colour=FFDD00&amp;amp;latest=2" width="160" height="55" alt="Buy Me a Coffee"&gt; &lt;/a&gt;&lt;/p&gt; 
 &lt;h4&gt;Support Latest 0.49.x Version | ÊîØÊåÅÊúÄÊñ∞ 0.49.x ÁâàÊú¨&lt;/h4&gt; 
 &lt;p&gt;This tool is for educational purposes, currently the repo does not violate any laws. Please support the original project. This tool will not generate any fake email accounts and OAuth access.&lt;/p&gt; 
 &lt;p&gt;Supports Windows, macOS and Linux.&lt;/p&gt; 
 &lt;p&gt;For optimal performance, run with privileges and always stay up to date.&lt;/p&gt; 
 &lt;p&gt;ÈÄôÊòØ‰∏ÄÊ¨æÁî®ÊñºÂ≠∏ÁøíÂíåÁ†îÁ©∂ÁöÑÂ∑•ÂÖ∑ÔºåÁõÆÂâç repo Ê≤íÊúâÈÅïÂèç‰ªª‰ΩïÊ≥ïÂæã„ÄÇË´ãÊîØÊåÅÂéü‰ΩúËÄÖ„ÄÇ ÈÄôÊ¨æÂ∑•ÂÖ∑‰∏çÊúÉÁîüÊàê‰ªª‰ΩïÂÅáÁöÑÈõªÂ≠êÈÉµ‰ª∂Â∏≥Êà∂Âíå OAuth Ë®™Âïè„ÄÇ&lt;/p&gt; 
 &lt;p&gt;ÊîØÊåÅ Windows„ÄÅmacOS Âíå Linux„ÄÇ&lt;/p&gt; 
 &lt;p&gt;Â∞çÊñºÊúÄ‰Ω≥ÊÄßËÉΩÔºåË´ã‰ª•ÁÆ°ÁêÜÂì°Ë∫´‰ªΩÈÅãË°å‰∏¶ÂßãÁµÇ‰øùÊåÅÊúÄÊñ∞„ÄÇ&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/yeongpin/cursor-free-vip/main/images/product_2025-04-16_10-40-21.png" alt="new" width="800" style="border-radius: 6px;"&gt;&lt;br&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;üîÑ Change Log | Êõ¥Êñ∞Êó•Âøó&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/yeongpin/cursor-free-vip/main/CHANGELOG.md"&gt;Watch Change Log | Êü•ÁúãÊõ¥Êñ∞Êó•Âøó&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;‚ú® Features | ÂäüËÉΩÁâπÈªû&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Support Windows macOS and Linux systems&lt;br&gt;ÊîØÊåÅ Windows„ÄÅmacOS Âíå Linux Á≥ªÁµ±&lt;br&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Reset Cursor's configuration&lt;br&gt;ÈáçÁΩÆ Cursor ÁöÑÈÖçÁΩÆ&lt;br&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Multi-language support (English, ÁÆÄ‰Ωì‰∏≠Êñá, ÁπÅÈ´î‰∏≠Êñá, Vietnamese)&lt;br&gt;Â§öË™ûË®ÄÊîØÊåÅÔºàËã±Êñá„ÄÅÁÆÄ‰Ωì‰∏≠Êñá„ÄÅÁπÅÈ´î‰∏≠Êñá„ÄÅË∂äÂçóË™ûÔºâ&lt;br&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üíª System Support | Á≥ªÁµ±ÊîØÊåÅ&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Operating System&lt;/th&gt; 
   &lt;th&gt;Architecture&lt;/th&gt; 
   &lt;th&gt;Supported&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Windows&lt;/td&gt; 
   &lt;td&gt;x64, x86&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;macOS&lt;/td&gt; 
   &lt;td&gt;Intel, Apple Silicon&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Linux&lt;/td&gt; 
   &lt;td&gt;x64, x86, ARM64&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;üëÄ How to use | Â¶Ç‰Ωï‰ΩøÁî®&lt;/h2&gt; 
&lt;details open&gt; 
 &lt;summary&gt;&lt;b&gt;‚≠ê Auto Run Script | ËÖ≥Êú¨Ëá™ÂãïÂåñÈÅãË°å&lt;/b&gt;&lt;/summary&gt; 
 &lt;h3&gt;&lt;strong&gt;Linux/macOS&lt;/strong&gt;&lt;/h3&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;curl -fsSL https://raw.githubusercontent.com/yeongpin/cursor-free-vip/main/scripts/install.sh -o install.sh &amp;amp;&amp;amp; chmod +x install.sh &amp;amp;&amp;amp; ./install.sh
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;&lt;strong&gt;Archlinux&lt;/strong&gt;&lt;/h3&gt; 
 &lt;p&gt;Install via &lt;a href="https://aur.archlinux.org/packages/cursor-free-vip-git"&gt;AUR&lt;/a&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;yay -S cursor-free-vip-git
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;&lt;strong&gt;Windows&lt;/strong&gt;&lt;/h3&gt; 
 &lt;pre&gt;&lt;code class="language-powershell"&gt;irm https://raw.githubusercontent.com/yeongpin/cursor-free-vip/main/scripts/install.ps1 | iex
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p&gt;If you want to stop the script, please press Ctrl+C&lt;br&gt;Ë¶ÅÂÅúÊ≠¢ËÖ≥Êú¨ÔºåË´ãÊåâ Ctrl+C&lt;/p&gt; 
&lt;h2&gt;‚ùó Note | Ê≥®ÊÑè‰∫ãÈ†Ö&lt;/h2&gt; 
&lt;p&gt;üìù Config | Êñá‰ª∂ÈÖçÁΩÆ &lt;code&gt;Win / Macos / Linux Path | Ë∑ØÂæë [Documents/.cursor-free-vip/config.ini]&lt;/code&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;‚≠ê Config | Êñá‰ª∂ÈÖçÁΩÆ&lt;/b&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code&gt;[Chrome]
# Default Google Chrome Path | ÈªòË™çGoogle Chrome ÈÅäË¶ΩÂô®Ë∑ØÂæë
chromepath = C:\Program Files\Google/Chrome/Application/chrome.exe

[Turnstile]
# Handle Turnstile Wait Time | Á≠âÂæÖ‰∫∫Ê©üÈ©óË≠âÊôÇÈñì
handle_turnstile_time = 2
# Handle Turnstile Wait Random Time (must merge 1-3 or 1,3) | Á≠âÂæÖ‰∫∫Ê©üÈ©óË≠âÈö®Ê©üÊôÇÈñìÔºàÂøÖÈ†àÊòØ 1-3 ÊàñËÄÖ 1,3 ÈÄôÊ®£ÁöÑÁµÑÂêàÔºâ
handle_turnstile_random_time = 1-3

[OSPaths]
# Storage Path | Â≠òÂÑ≤Ë∑ØÂæë
storage_path = /Users/username/Library/Application Support/Cursor/User/globalStorage/storage.json
# SQLite Path | SQLiteË∑ØÂæë
sqlite_path = /Users/username/Library/Application Support/Cursor/User/globalStorage/state.vscdb
# Machine ID Path | Ê©üÂô®IDË∑ØÂæë
machine_id_path = /Users/username/Library/Application Support/Cursor/machineId
# For Linux users: ~/.config/cursor/machineid

[Timing]
# Min Random Time | ÊúÄÂ∞èÈö®Ê©üÊôÇÈñì
min_random_time = 0.1
# Max Random Time | ÊúÄÂ§ßÈö®Ê©üÊôÇÈñì
max_random_time = 0.8
# Page Load Wait | È†ÅÈù¢Âä†ËºâÁ≠âÂæÖÊôÇÈñì
page_load_wait = 0.1-0.8
# Input Wait | Ëº∏ÂÖ•Á≠âÂæÖÊôÇÈñì
input_wait = 0.3-0.8
# Submit Wait | Êèê‰∫§Á≠âÂæÖÊôÇÈñì
submit_wait = 0.5-1.5
# Verification Code Input | È©óË≠âÁ¢ºËº∏ÂÖ•Á≠âÂæÖÊôÇÈñì
verification_code_input = 0.1-0.3
# Verification Success Wait | È©óË≠âÊàêÂäüÁ≠âÂæÖÊôÇÈñì
verification_success_wait = 2-3
# Verification Retry Wait | È©óË≠âÈáçË©¶Á≠âÂæÖÊôÇÈñì
verification_retry_wait = 2-3
# Email Check Initial Wait | ÈÉµ‰ª∂Ê™¢Êü•ÂàùÂßãÁ≠âÂæÖÊôÇÈñì
email_check_initial_wait = 4-6
# Email Refresh Wait | ÈÉµ‰ª∂Âà∑Êñ∞Á≠âÂæÖÊôÇÈñì
email_refresh_wait = 2-4
# Settings Page Load Wait | Ë®≠ÁΩÆÈ†ÅÈù¢Âä†ËºâÁ≠âÂæÖÊôÇÈñì
settings_page_load_wait = 1-2
# Failed Retry Time | Â§±ÊïóÈáçË©¶ÊôÇÈñì
failed_retry_time = 0.5-1
# Retry Interval | ÈáçË©¶ÈñìÈöî
retry_interval = 8-12
# Max Timeout | ÊúÄÂ§ßË∂ÖÊôÇÊôÇÈñì
max_timeout = 160

[Utils]
# Check Update | Ê™¢Êü•Êõ¥Êñ∞
check_update = True
# Show Account Info | È°ØÁ§∫Ë≥¨Ëôü‰ø°ÊÅØ
show_account_info = True

[TempMailPlus]
# Enable TempMailPlus | ÂïìÁî® TempMailPlusÔºà‰ªª‰ΩïËΩâÁôºÂà∞TempMailPlusÁöÑÈÉµ‰ª∂ÈÉΩÊîØÊåÅÁç≤ÂèñÈ©óË≠âÁ¢ºÔºå‰æãÂ¶ÇcloudflareÈÉµ‰ª∂Catch-allÔºâ
enabled = false
# TempMailPlus Email | TempMailPlus ÈõªÂ≠êÈÉµ‰ª∂
email = xxxxx@mailto.plus
# TempMailPlus pin | TempMailPlus pinÁ¢º
epin = 

[WindowsPaths]
storage_path = C:\Users\yeongpin\AppData\Roaming\Cursor\User\globalStorage\storage.json
sqlite_path = C:\Users\yeongpin\AppData\Roaming\Cursor\User\globalStorage\state.vscdb
machine_id_path = C:\Users\yeongpin\AppData\Roaming\Cursor\machineId
cursor_path = C:\Users\yeongpin\AppData\Local\Programs\Cursor\resources\app
updater_path = C:\Users\yeongpin\AppData\Local\cursor-updater
update_yml_path = C:\Users\yeongpin\AppData\Local\Programs\Cursor\resources\app-update.yml
product_json_path = C:\Users\yeongpin\AppData\Local\Programs\Cursor\resources\app\product.json

[Browser]
default_browser = opera
chrome_path = C:\Program Files\Google\Chrome\Application\chrome.exe
edge_path = C:\Program Files (x86)\Microsoft\Edge\Application\msedge.exe
firefox_path = C:\Program Files\Mozilla Firefox\firefox.exe
brave_path = C:\Program Files\BraveSoftware/Brave-Browser/Application/brave.exe
chrome_driver_path = D:\VisualCode\cursor-free-vip-new\drivers\chromedriver.exe
edge_driver_path = D:\VisualCode\cursor-free-vip-new\drivers\msedgedriver.exe
firefox_driver_path = D:\VisualCode\cursor-free-vip-new\drivers\geckodriver.exe
brave_driver_path = D:\VisualCode\cursor-free-vip-new\drivers\chromedriver.exe
opera_path = C:\Users\yeongpin\AppData\Local\Programs\Opera\opera.exe
opera_driver_path = D:\VisualCode\cursor-free-vip-new\drivers\chromedriver.exe

[OAuth]
show_selection_alert = False
timeout = 120
max_attempts = 3
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Use administrator privileges to run the script &lt;br&gt;Ë´ã‰ΩøÁî®ÁÆ°ÁêÜÂì°Ë∫´‰ªΩÈÅãË°åËÖ≥Êú¨&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Confirm that Cursor is closed before running the script &lt;br&gt;Ë´ãÁ¢∫‰øùÂú®ÈÅãË°åËÖ≥Êú¨ÂâçÂ∑≤Á∂ìÈóúÈñâ Cursor&lt;br&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;This tool is only for learning and research purposes &lt;br&gt;Ê≠§Â∑•ÂÖ∑ÂÉÖ‰æõÂ≠∏ÁøíÂíåÁ†îÁ©∂‰ΩøÁî®&lt;br&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Please comply with the relevant software usage terms when using this tool &lt;br&gt;‰ΩøÁî®Êú¨Â∑•ÂÖ∑ÊôÇË´ãÈÅµÂÆàÁõ∏ÈóúËªü‰ª∂‰ΩøÁî®Ê¢ùÊ¨æ&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üö® Common Issues | Â∏∏Ë¶ãÂïèÈ°å&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;Â¶ÇÊûúÈÅáÂà∞Ê¨äÈôêÂïèÈ°åÔºåË´ãÁ¢∫‰øùÔºö&lt;/th&gt; 
   &lt;th align="center"&gt;Ê≠§ËÖ≥Êú¨‰ª•ÁÆ°ÁêÜÂì°Ë∫´‰ªΩÈÅãË°å&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;If you encounter permission issues, please ensure:&lt;/td&gt; 
   &lt;td align="center"&gt;This script is run with administrator privileges&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;Error 'User is not authorized'&lt;/td&gt; 
   &lt;td align="center"&gt;This means your account was banned for using temporary (disposal) mail. Ensure using a non-temporary mail service&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;ü§© Contribution | Ë≤¢Áçª&lt;/h2&gt; 
&lt;p&gt;Ê≠°ËøéÊèê‰∫§ Issue Âíå Pull RequestÔºÅ&lt;/p&gt; 
&lt;a href="https://github.com/yeongpin/cursor-free-vip/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=yeongpin/cursor-free-vip&amp;amp;preview=true&amp;amp;max=&amp;amp;columns="&gt; &lt;/a&gt; 
&lt;br&gt;
&lt;br&gt; 
&lt;h2&gt;üì© Disclaimer | ÂÖçË≤¨ËÅ≤Êòé&lt;/h2&gt; 
&lt;p&gt;Êú¨Â∑•ÂÖ∑ÂÉÖ‰æõÂ≠∏ÁøíÂíåÁ†îÁ©∂‰ΩøÁî®Ôºå‰ΩøÁî®Êú¨Â∑•ÂÖ∑ÊâÄÁî¢ÁîüÁöÑ‰ªª‰ΩïÂæåÊûúÁî±‰ΩøÁî®ËÄÖËá™Ë°åÊâøÊìî„ÄÇ &lt;br&gt;&lt;/p&gt; 
&lt;p&gt;This tool is only for learning and research purposes, and any consequences arising from the use of this tool are borne by the user.&lt;/p&gt; 
&lt;h2&gt;üí∞ Buy Me a Coffee | Ë´ãÊàëÂñùÊùØÂíñÂï°&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td&gt; &lt;img src="https://raw.githubusercontent.com/yeongpin/cursor-free-vip/main/images/provi-code.jpg" alt="buy_me_a_coffee" width="280"&gt;&lt;br&gt; &lt;/td&gt; 
    &lt;td&gt; &lt;img src="https://raw.githubusercontent.com/yeongpin/cursor-free-vip/main/images/paypal.png" alt="buy_me_a_coffee" width="280"&gt;&lt;br&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h2&gt;‚≠ê Star History | ÊòüÊòüÊï∏&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://star-history.com/#yeongpin/cursor-free-vip&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=yeongpin/cursor-free-vip&amp;amp;type=Date" alt="Star History Chart"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;üìù License | ÊéàÊ¨ä&lt;/h2&gt; 
&lt;p&gt;Êú¨È†ÖÁõÆÊé°Áî® &lt;a href="https://creativecommons.org/licenses/by-nc-nd/4.0/"&gt;CC BY-NC-ND 4.0&lt;/a&gt; ÊéàÊ¨ä„ÄÇ Please refer to the &lt;a href="https://raw.githubusercontent.com/yeongpin/cursor-free-vip/main/LICENSE.md"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>geekcomputers/Python</title>
      <link>https://github.com/geekcomputers/Python</link>
      <description>&lt;p&gt;My Python Examples&lt;/p&gt;&lt;hr&gt;&lt;p&gt;#This is a new repo&lt;/p&gt; 
&lt;h1&gt;My Python Eggs üêç üòÑ&lt;/h1&gt; 
&lt;hr&gt; 
&lt;p&gt;I do not consider myself as a programmer. I create these little programs as experiments to play with Python, or to solve problems for myself. I would gladly accept pointers from others to improve, simplify, or make the code more efficient. If you would like to make any comments then please feel free to email me: &lt;a href="mailto:craig@geekcomputers.co.uk"&gt;craig@geekcomputers.co.uk&lt;/a&gt;.&lt;/p&gt; 
&lt;hr&gt; 
&lt;p&gt;This repository contains a collection of Python scripts that are designed to reduce human workload and serve as educational examples for beginners to get started with Python. The code documentation is aligned correctly for viewing in &lt;a href="https://notepad-plus-plus.org/"&gt;Notepad++&lt;/a&gt; &lt;span&gt;üóí&lt;/span&gt;&lt;/p&gt; 
&lt;p&gt;Feel free to explore the scripts and use them for your learning and automation needs!&lt;/p&gt; 
&lt;h2&gt;List of Scripts:&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://github.com/geekcomputers/Python/raw/master/batch_file_rename.py"&gt;batch_file_rename.py&lt;/a&gt; - Batch rename a group of files in a specified directory, changing their extensions.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/geekcomputers/Python/raw/master/create_dir_if_not_there.py"&gt;create_dir_if_not_there.py&lt;/a&gt; - Check if a directory exists in the user's home directory. Create it if it doesn't exist.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/geekcomputers/Python/raw/master/youtubedownloader.py"&gt;Fast Youtube Downloader&lt;/a&gt; - Download YouTube videos quickly with parallel threads using aria2c.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/geekcomputers/Python/tree/master/Google_Image_Downloader"&gt;Google Image Downloader&lt;/a&gt; - Query a given term and retrieve images from the Google Image database.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/geekcomputers/Python/raw/master/dir_test.py"&gt;dir_test.py&lt;/a&gt; - Test if the directory &lt;code&gt;testdir&lt;/code&gt; exists. If not, create it.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/geekcomputers/Python/raw/master/env_check.py"&gt;env_check.py&lt;/a&gt; - Check if all the required environment variables are set.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Ratna04priya/Python/raw/master/BlackJack_game/blackjack.py"&gt;blackjack.py&lt;/a&gt; - Casino Blackjack-21 game in Python.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/geekcomputers/Python/raw/master/fileinfo.py"&gt;fileinfo.py&lt;/a&gt; - Show file information for a given file.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/geekcomputers/Python/raw/master/folder_size.py"&gt;folder_size.py&lt;/a&gt; - Scan the current directory and all subdirectories and display their sizes.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/geekcomputers/Python/raw/master/logs.py"&gt;logs.py&lt;/a&gt; - Search for all &lt;code&gt;*.log&lt;/code&gt; files in a directory, zip them using the specified program, and date stamp them.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/geekcomputers/Python/raw/master/move_files_over_x_days.py"&gt;move_files_over_x_days.py&lt;/a&gt; - Move all files over a specified age (in days) from the source directory to the destination directory.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/geekcomputers/Python/raw/master/nslookup_check.py"&gt;nslookup_check.py&lt;/a&gt; - Open the file &lt;code&gt;server_list.txt&lt;/code&gt; and perform nslookup for each server to check the DNS entry.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/geekcomputers/Python/raw/master/osinfo.py"&gt;osinfo.py&lt;/a&gt; - Display information about the operating system on which the script is running.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/geekcomputers/Python/raw/master/ping_servers.py"&gt;ping_servers.py&lt;/a&gt; - Ping the servers associated with the specified application group.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/geekcomputers/Python/raw/master/ping_subnet.py"&gt;ping_subnet.py&lt;/a&gt; - Scan the final range of a given IP subnet for available addresses.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/geekcomputers/Python/raw/master/powerdown_startup.py"&gt;powerdown_startup.py&lt;/a&gt; - Ping machines in the server list. Load the putty session if the machine is up, or notify if it is not.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/geekcomputers/Python/raw/master/puttylogs.py"&gt;puttylogs.py&lt;/a&gt; - Zip all the logs in the given directory.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/geekcomputers/Python/raw/master/script_count.py"&gt;script_count.py&lt;/a&gt; - Scan the scripts directory and count the different types of scripts.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/geekcomputers/Python/raw/master/get_youtube_view.py"&gt;get_youtube_view.py&lt;/a&gt; - Get more views for YouTube videos and repeat songs on YouTube.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/geekcomputers/Python/raw/master/script_listing.py"&gt;script_listing.py&lt;/a&gt; - List all files in a given directory and its subdirectories.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/geekcomputers/Python/raw/master/testlines.py"&gt;testlines.py&lt;/a&gt; - Open a file and print out 100 lines of the set line variable.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/geekcomputers/Python/raw/master/tweeter.py"&gt;tweeter.py&lt;/a&gt; - Tweet text or a picture from the terminal.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/geekcomputers/Python/raw/master/serial_scanner.py"&gt;serial_scanner.py&lt;/a&gt; - List available serial ports in use on Linux and Windows systems.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/geekcomputers/Python/raw/master/get_youtube_view.py"&gt;get_youtube_view.py&lt;/a&gt; - Get more views for YouTube videos and repeat songs on YouTube.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/geekcomputers/Python/raw/master/CountMillionCharacter.py"&gt;CountMillionCharacter.py&lt;/a&gt; and &lt;a href="https://github.com/geekcomputers/Python/raw/master/CountMillionCharacters-2.0.py"&gt;CountMillionCharacter2.0&lt;/a&gt; - Get character count of a text file.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/geekcomputers/Python/raw/master/xkcd_downloader.py"&gt;xkcd_downloader.py&lt;/a&gt; - Download the latest XKCD comic and place them in a new folder called "comics".&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/geekcomputers/Python/raw/master/timymodule.py"&gt;timymodule.py&lt;/a&gt; - An alternative to Python's 'timeit' module and easier to use.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/geekcomputers/Python/raw/master/calculator.py"&gt;calculator.py&lt;/a&gt; - Implement a calculator using Python's eval() function.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/geekcomputers/Python/raw/master/Google_News.py"&gt;Google_News.py&lt;/a&gt; - Use BeautifulSoup to provide latest news headlines along with news links.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/geekcomputers/Python/raw/master/Cricket_score.py"&gt;cricket_live_score&lt;/a&gt; - Use BeautifulSoup to provide live cricket scores.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/geekcomputers/Python/raw/master/youtube.py"&gt;youtube.py&lt;/a&gt; - Take a song name as input and fetch the YouTube URL of the best matching song and play it.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/geekcomputers/Python/raw/master/site_health.py"&gt;site_health.py&lt;/a&gt; - Check the health of a remote server.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/geekcomputers/Python/raw/master/SimpleStopWatch.py"&gt;SimpleStopWatch.py&lt;/a&gt; - Simple stop watch implementation using Python's time module.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/geekcomputers/Python/raw/master/changemac.py"&gt;Changemac.py&lt;/a&gt; - Change your MAC address, generate a random MAC address, or enter input as a new MAC address on Linux (Successfully Tested in Ubuntu 18.04).&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/geekcomputers/Python/raw/master/whatsapp-monitor.py"&gt;whatsapp-monitor.py&lt;/a&gt; - Use Selenium to give online status updates about your contacts in WhatsApp on the terminal.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/subahanii/whatsapp-Chat-Analyzer"&gt;whatsapp-chat-analyzer.py&lt;/a&gt; - WhatsApp group/individual chat analyzer that visualizes chat activity using matplotlib.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://git.io/fjH8m"&gt;JARVIS.py&lt;/a&gt; - Control Windows programs with your voice.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://git.io/JvnJh"&gt;Images Downloader&lt;/a&gt; - Download images from webpages on Unix-based systems.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/meezan-mallick/space_invader_game"&gt;space_invader.py.py&lt;/a&gt; - Classical 2D space invader game to recall your childhood memories.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Tanmay-901/test-case-generator/raw/master/test_case.py"&gt;Test Case Generator&lt;/a&gt; - Generate different types of test cases with a clean and friendly UI, used in competitive programming and software testing.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/geekcomputers/Python/tree/ExtractThumbnailFromVideo"&gt;Extract Thumbnail From Video&lt;/a&gt; - Extract Thumbnail from video files&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=v2X51AVgl3o"&gt;How to begin the journey of open source (first contribution)&lt;/a&gt; - First Contribution of open source&lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr&gt; 
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Note&lt;/strong&gt;: The content in this repository belongs to the respective authors and creators. I'm just providing a formatted README.md for better presentation.&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>