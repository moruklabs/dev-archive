<rss version="2.0">
  <channel>
    <title>GitHub Go Weekly Trending</title>
    <description>Weekly Trending of Go in GitHub</description>
    <pubDate>Sun, 18 Jan 2026 01:46:27 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>gotenberg/gotenberg</title>
      <link>https://github.com/gotenberg/gotenberg</link>
      <description>&lt;p&gt;A developer-friendly API for converting numerous document formats into PDF files, and more!&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://user-images.githubusercontent.com/8983173/130322857-185831e2-f041-46eb-a17f-0a69d066c4e5.png" alt="Gotenberg Logo" width="150" height="150" /&gt; &lt;/p&gt;
&lt;h3 align="center"&gt;Gotenberg&lt;/h3&gt; 
&lt;p align="center"&gt;A containerized API for seamless PDF conversion&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://hub.docker.com/r/gotenberg/gotenberg"&gt;&lt;img alt="Total downloads (gotenberg/gotenberg)" src="https://img.shields.io/docker/pulls/gotenberg/gotenberg" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/thecodingmachine/gotenberg"&gt;&lt;img alt="Total downloads (thecodingmachine/gotenberg)" src="https://img.shields.io/docker/pulls/thecodingmachine/gotenberg" /&gt;&lt;/a&gt; &lt;a href="https://github.com/gotenberg/gotenberg/actions/workflows/continuous-integration.yml"&gt;&lt;img alt="Continuous Integration" src="https://github.com/gotenberg/gotenberg/actions/workflows/continuous-integration.yml/badge.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://pkg.go.dev/github.com/gotenberg/gotenberg/v8"&gt;&lt;img alt="Go Reference" src="https://pkg.go.dev/badge/github.com/gotenberg/gotenberg.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/2996"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/2996" alt="gotenberg%2Fgotenberg | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt;&lt;a href="https://gotenberg.dev/docs/getting-started/introduction"&gt;Documentation&lt;/a&gt; ¬∑ &lt;a href="https://gotenberg.dev/docs/getting-started/installation#live-demo-"&gt;Live Demo&lt;/a&gt; üî•&lt;/p&gt; 
&lt;p&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;strong&gt;Gotenberg&lt;/strong&gt; provides a developer-friendly API to interact with powerful tools like Chromium and LibreOffice for converting numerous document formats (HTML, Markdown, Word, Excel, etc.) into PDF files, and more!&lt;/p&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;p&gt;Open a terminal and run the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;docker run --rm -p 3000:3000 gotenberg/gotenberg:8
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, using the historic Docker repository from our sponsor &lt;a href="https://www.thecodingmachine.com"&gt;TheCodingMachine&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;docker run --rm -p 3000:3000 thecodingmachine/gotenberg:8
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The API is now available on your host at &lt;a href="http://localhost:3000"&gt;http://localhost:3000&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Head to the &lt;a href="https://gotenberg.dev/docs/getting-started/introduction"&gt;documentation&lt;/a&gt; to learn how to interact with it üöÄ&lt;/p&gt; 
&lt;h2&gt;Sponsors&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;a href="https://thecodingmachine.com"&gt; &lt;img src="https://user-images.githubusercontent.com/8983173/130324668-9d6e7b35-53a3-49c7-a574-38190d2bd6b0.png" alt="TheCodingMachine Logo" width="333" height="163" /&gt; &lt;/a&gt; &lt;a href="https://pdfme.com?utm_source=gotenberg_github&amp;amp;utm_medium=website" target="_blank"&gt; &lt;img src="https://github.com/user-attachments/assets/2a75dd40-ca18-4d34-acd5-5dd474595168" alt="pdfme Logo" width="333" height="163" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;Sponsorships help maintain and improve Gotenberg - &lt;a href="https://github.com/sponsors/gulien"&gt;become a sponsor&lt;/a&gt; ‚ù§Ô∏è&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p align="center"&gt; &lt;strong&gt;Powered by&lt;/strong&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://jb.gg/OpenSource"&gt; &lt;img src="https://resources.jetbrains.com/storage/products/company/brand/logos/jetbrains.svg?sanitize=true" alt="JetBrains logo" width="200" /&gt; &lt;/a&gt; &lt;/p&gt;</description>
    </item>
    
    <item>
      <title>mudler/LocalAI</title>
      <link>https://github.com/mudler/LocalAI</link>
      <description>&lt;p&gt;ü§ñ The free, Open Source alternative to OpenAI, Claude and others. Self-hosted and local-first. Drop-in replacement for OpenAI, running on consumer-grade hardware. No GPU required. Runs gguf, transformers, diffusers and many more. Features: Generate Text, MCP, Audio, Video, Images, Voice Cloning, Distributed, P2P and decentralized inference&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt; &lt;br /&gt; &lt;img width="300" src="https://raw.githubusercontent.com/mudler/LocalAI/master/core/http/static/logo.png" /&gt; &lt;br /&gt; &lt;br /&gt; &lt;/h1&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/go-skynet/LocalAI/fork" target="blank"&gt; &lt;img src="https://img.shields.io/github/forks/go-skynet/LocalAI?style=for-the-badge" alt="LocalAI forks" /&gt; &lt;/a&gt; &lt;a href="https://github.com/go-skynet/LocalAI/stargazers" target="blank"&gt; &lt;img src="https://img.shields.io/github/stars/go-skynet/LocalAI?style=for-the-badge" alt="LocalAI stars" /&gt; &lt;/a&gt; &lt;a href="https://github.com/go-skynet/LocalAI/pulls" target="blank"&gt; &lt;img src="https://img.shields.io/github/issues-pr/go-skynet/LocalAI?style=for-the-badge" alt="LocalAI pull-requests" /&gt; &lt;/a&gt; &lt;a href="https://github.com/go-skynet/LocalAI/releases"&gt; &lt;img src="https://img.shields.io/github/release/go-skynet/LocalAI?&amp;amp;label=Latest&amp;amp;style=for-the-badge" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://hub.docker.com/r/localai/localai" target="blank"&gt; &lt;img src="https://img.shields.io/badge/dockerhub-images-important.svg?logo=Docker" alt="LocalAI Docker hub" /&gt; &lt;/a&gt; &lt;a href="https://quay.io/repository/go-skynet/local-ai?tab=tags&amp;amp;tag=latest" target="blank"&gt; &lt;img src="https://img.shields.io/badge/quay.io-images-important.svg?" alt="LocalAI Quay.io" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://twitter.com/LocalAI_API" target="blank"&gt; &lt;img src="https://img.shields.io/badge/X-%23000000.svg?style=for-the-badge&amp;amp;logo=X&amp;amp;logoColor=white&amp;amp;label=LocalAI_API" alt="Follow LocalAI_API" /&gt; &lt;/a&gt; &lt;a href="https://discord.gg/uJAeKSAGDy" target="blank"&gt; &lt;img src="https://img.shields.io/badge/dynamic/json?color=blue&amp;amp;label=Discord&amp;amp;style=for-the-badge&amp;amp;query=approximate_member_count&amp;amp;url=https%3A%2F%2Fdiscordapp.com%2Fapi%2Finvites%2FuJAeKSAGDy%3Fwith_counts%3Dtrue&amp;amp;logo=discord" alt="Join LocalAI Discord Community" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/5539" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/5539" alt="mudler%2FLocalAI | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;span&gt;üí°&lt;/span&gt; Get help - &lt;a href="https://localai.io/faq/"&gt;‚ùìFAQ&lt;/a&gt; &lt;a href="https://github.com/go-skynet/LocalAI/discussions"&gt;üí≠Discussions&lt;/a&gt; &lt;a href="https://discord.gg/uJAeKSAGDy"&gt;&lt;span&gt;üí¨&lt;/span&gt; Discord&lt;/a&gt; &lt;a href="https://localai.io/"&gt;&lt;span&gt;üìñ&lt;/span&gt; Documentation website&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://localai.io/basics/getting_started/"&gt;üíª Quickstart&lt;/a&gt; &lt;a href="https://models.localai.io/"&gt;üñºÔ∏è Models&lt;/a&gt; &lt;a href="https://github.com/mudler/LocalAI/issues?q=is%3Aissue+is%3Aopen+label%3Aroadmap"&gt;üöÄ Roadmap&lt;/a&gt; &lt;a href="https://github.com/mudler/LocalAI-examples"&gt;üõ´ Examples&lt;/a&gt; Try on &lt;a href="https://t.me/localaiofficial_bot"&gt;&lt;img src="https://img.shields.io/badge/Telegram-2CA5E0?style=for-the-badge&amp;amp;logo=telegram&amp;amp;logoColor=white" alt="Telegram" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;a href="https://github.com/go-skynet/LocalAI/actions/workflows/test.yml"&gt;&lt;img src="https://github.com/go-skynet/LocalAI/actions/workflows/test.yml/badge.svg?sanitize=true" alt="tests" /&gt;&lt;/a&gt;&lt;a href="https://github.com/go-skynet/LocalAI/actions/workflows/release.yaml"&gt;&lt;img src="https://github.com/go-skynet/LocalAI/actions/workflows/release.yaml/badge.svg?sanitize=true" alt="Build and Release" /&gt;&lt;/a&gt;&lt;a href="https://github.com/go-skynet/LocalAI/actions/workflows/image.yml"&gt;&lt;img src="https://github.com/go-skynet/LocalAI/actions/workflows/image.yml/badge.svg?sanitize=true" alt="build container images" /&gt;&lt;/a&gt;&lt;a href="https://github.com/go-skynet/LocalAI/actions/workflows/bump_deps.yaml"&gt;&lt;img src="https://github.com/go-skynet/LocalAI/actions/workflows/bump_deps.yaml/badge.svg?sanitize=true" alt="Bump dependencies" /&gt;&lt;/a&gt;&lt;a href="https://artifacthub.io/packages/search?repo=localai"&gt;&lt;img src="https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/localai" alt="Artifact Hub" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;LocalAI&lt;/strong&gt; is the free, Open Source OpenAI alternative. LocalAI act as a drop-in replacement REST API that's compatible with OpenAI (Elevenlabs, Anthropic... ) API specifications for local AI inferencing. It allows you to run LLMs, generate images, audio (and not only) locally or on-prem with consumer grade hardware, supporting multiple model families. Does not require GPU. It is created and maintained by &lt;a href="https://github.com/mudler"&gt;Ettore Di Giacinto&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;üìöüÜï Local Stack Family&lt;/h2&gt; 
&lt;p&gt;üÜï LocalAI is now part of a comprehensive suite of AI tools designed to work together:&lt;/p&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td width="50%" valign="top"&gt; &lt;a href="https://github.com/mudler/LocalAGI"&gt; &lt;img src="https://raw.githubusercontent.com/mudler/LocalAGI/refs/heads/main/webui/react-ui/public/logo_2.png" width="300" alt="LocalAGI Logo" /&gt; &lt;/a&gt; &lt;/td&gt; 
   &lt;td width="50%" valign="top"&gt; &lt;h3&gt;&lt;a href="https://github.com/mudler/LocalAGI"&gt;LocalAGI&lt;/a&gt;&lt;/h3&gt; &lt;p&gt;A powerful Local AI agent management platform that serves as a drop-in replacement for OpenAI's Responses API, enhanced with advanced agentic capabilities.&lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%" valign="top"&gt; &lt;a href="https://github.com/mudler/LocalRecall"&gt; &lt;img src="https://raw.githubusercontent.com/mudler/LocalRecall/refs/heads/main/static/localrecall_horizontal.png" width="300" alt="LocalRecall Logo" /&gt; &lt;/a&gt; &lt;/td&gt; 
   &lt;td width="50%" valign="top"&gt; &lt;h3&gt;&lt;a href="https://github.com/mudler/LocalRecall"&gt;LocalRecall&lt;/a&gt;&lt;/h3&gt; &lt;p&gt;A REST-ful API and knowledge base management system that provides persistent memory and storage capabilities for AI agents.&lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;Screenshots / Video&lt;/h2&gt; 
&lt;h3&gt;Youtube video&lt;/h3&gt; 
&lt;h1 align="center"&gt; &lt;br /&gt; &lt;a href="https://www.youtube.com/watch?v=PDqYhB9nNHA" target="_blank"&gt; &lt;img width="300" src="https://img.youtube.com/vi/PDqYhB9nNHA/0.jpg" /&gt; &lt;/a&gt;&lt;br /&gt; &lt;br /&gt; &lt;/h1&gt; 
&lt;h3&gt;Screenshots&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Talk Interface&lt;/th&gt; 
   &lt;th&gt;Generate Audio&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/mudler/LocalAI/master/docs/assets/images/screenshots/screenshot_tts.png" alt="Screenshot 2025-03-31 at 12-01-36 LocalAI - Talk" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/mudler/LocalAI/master/docs/assets/images/screenshots/screenshot_tts.png" alt="Screenshot 2025-03-31 at 12-01-29 LocalAI - Generate audio with voice-en-us-ryan-low" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Models Overview&lt;/th&gt; 
   &lt;th&gt;Generate Images&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/mudler/LocalAI/master/docs/assets/images/screenshots/screenshot_gallery.png" alt="Screenshot 2025-03-31 at 12-01-20 LocalAI - Models" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/mudler/LocalAI/master/docs/assets/images/screenshots/screenshot_image.png" alt="Screenshot 2025-03-31 at 12-31-41 LocalAI - Generate images with flux 1-dev" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Chat Interface&lt;/th&gt; 
   &lt;th&gt;Home&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/mudler/LocalAI/master/docs/assets/images/screenshots/screenshot_chat.png" alt="Screenshot 2025-03-31 at 11-57-44 LocalAI - Chat with localai-functioncall-qwen2 5-7b-v0 5" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/mudler/LocalAI/master/docs/assets/images/screenshots/screenshot_home.png" alt="Screenshot 2025-03-31 at 11-57-23 LocalAI API - c2a39e3 (c2a39e3639227cfd94ffffe9f5691239acc275a8)" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Login&lt;/th&gt; 
   &lt;th&gt;Swarm&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/mudler/LocalAI/master/docs/assets/images/screenshots/screenshot_login.png" alt="Screenshot 2025-03-31 at 12-09-59 " /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/mudler/LocalAI/master/docs/assets/images/screenshots/screenshot_p2p.png" alt="Screenshot 2025-03-31 at 12-10-39 LocalAI - P2P dashboard" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;üíª Quickstart&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;‚ö†Ô∏è &lt;strong&gt;Note:&lt;/strong&gt; The &lt;code&gt;install.sh&lt;/code&gt; script is currently experiencing issues due to the heavy changes currently undergoing in LocalAI and may produce broken or misconfigured installations. Please use Docker installation (see below) or manual binary installation until &lt;a href="https://github.com/mudler/LocalAI/issues/8032"&gt;issue #8032&lt;/a&gt; is resolved.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Run the installer script:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Basic installation
curl https://localai.io/install.sh | sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more installation options, see &lt;a href="https://localai.io/installation/"&gt;Installer Options&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;macOS Download:&lt;/h3&gt; 
&lt;a href="https://github.com/mudler/LocalAI/releases/latest/download/LocalAI.dmg"&gt; &lt;img src="https://img.shields.io/badge/Download-macOS-blue?style=for-the-badge&amp;amp;logo=apple&amp;amp;logoColor=white" alt="Download LocalAI for macOS" /&gt; &lt;/a&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Note: the DMGs are not signed by Apple as quarantined. See &lt;a href="https://github.com/mudler/LocalAI/issues/6268"&gt;https://github.com/mudler/LocalAI/issues/6268&lt;/a&gt; for a workaround, fix is tracked here: &lt;a href="https://github.com/mudler/LocalAI/issues/6244"&gt;https://github.com/mudler/LocalAI/issues/6244&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Containers (Docker, podman, ...)&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;üí° Docker Run vs Docker Start&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;docker run&lt;/code&gt; creates and starts a new container. If a container with the same name already exists, this command will fail.&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;docker start&lt;/code&gt; starts an existing container that was previously created with &lt;code&gt;docker run&lt;/code&gt;.&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;If you've already run LocalAI before and want to start it again, use: &lt;code&gt;docker start -i local-ai&lt;/code&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;CPU only image:&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run -ti --name local-ai -p 8080:8080 localai/localai:latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;NVIDIA GPU Images:&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# CUDA 13.0
docker run -ti --name local-ai -p 8080:8080 --gpus all localai/localai:latest-gpu-nvidia-cuda-13

# CUDA 12.0
docker run -ti --name local-ai -p 8080:8080 --gpus all localai/localai:latest-gpu-nvidia-cuda-12

# NVIDIA Jetson (L4T) ARM64
# CUDA 12 (for Nvidia AGX Orin and similar platforms)
docker run -ti --name local-ai -p 8080:8080 --gpus all localai/localai:latest-nvidia-l4t-arm64

# CUDA 13 (for Nvidia DGX Spark)
docker run -ti --name local-ai -p 8080:8080 --gpus all localai/localai:latest-nvidia-l4t-arm64-cuda-13
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;AMD GPU Images (ROCm):&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run -ti --name local-ai -p 8080:8080 --device=/dev/kfd --device=/dev/dri --group-add=video localai/localai:latest-gpu-hipblas
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Intel GPU Images (oneAPI):&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run -ti --name local-ai -p 8080:8080 --device=/dev/dri/card1 --device=/dev/dri/renderD128 localai/localai:latest-gpu-intel
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Vulkan GPU Images:&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run -ti --name local-ai -p 8080:8080 localai/localai:latest-gpu-vulkan
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;AIO Images (pre-downloaded models):&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# CPU version
docker run -ti --name local-ai -p 8080:8080 localai/localai:latest-aio-cpu

# NVIDIA CUDA 13 version
docker run -ti --name local-ai -p 8080:8080 --gpus all localai/localai:latest-aio-gpu-nvidia-cuda-13

# NVIDIA CUDA 12 version
docker run -ti --name local-ai -p 8080:8080 --gpus all localai/localai:latest-aio-gpu-nvidia-cuda-12

# Intel GPU version
docker run -ti --name local-ai -p 8080:8080 localai/localai:latest-aio-gpu-intel

# AMD GPU version
docker run -ti --name local-ai -p 8080:8080 --device=/dev/kfd --device=/dev/dri --group-add=video localai/localai:latest-aio-gpu-hipblas
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more information about the AIO images and pre-downloaded models, see &lt;a href="https://localai.io/basics/container/"&gt;Container Documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;To load models:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# From the model gallery (see available models with `local-ai models list`, in the WebUI from the model tab, or visiting https://models.localai.io)
local-ai run llama-3.2-1b-instruct:q4_k_m
# Start LocalAI with the phi-2 model directly from huggingface
local-ai run huggingface://TheBloke/phi-2-GGUF/phi-2.Q8_0.gguf
# Install and run a model from the Ollama OCI registry
local-ai run ollama://gemma:2b
# Run a model from a configuration file
local-ai run https://gist.githubusercontent.com/.../phi-2.yaml
# Install and run a model from a standard OCI registry (e.g., Docker Hub)
local-ai run oci://localai/phi-2:latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;‚ö° &lt;strong&gt;Automatic Backend Detection&lt;/strong&gt;: When you install models from the gallery or YAML files, LocalAI automatically detects your system's GPU capabilities (NVIDIA, AMD, Intel) and downloads the appropriate backend. For advanced configuration options, see &lt;a href="https://localai.io/features/gpu-acceleration/#automatic-backend-detection"&gt;GPU Acceleration&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;For more information, see &lt;a href="https://localai.io/basics/getting_started/index.html"&gt;üíª Getting started&lt;/a&gt;, if you are interested in our roadmap items and future enhancements, you can see the &lt;a href="https://github.com/mudler/LocalAI/issues?q=is%3Aissue+is%3Aopen+label%3Aroadmap"&gt;Issues labeled as Roadmap here&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;üì∞ Latest project news&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;December 2025: &lt;a href="https://github.com/mudler/LocalAI/pull/7583"&gt;Dynamic Memory Resource reclaimer&lt;/a&gt;, &lt;a href="https://github.com/mudler/LocalAI/pull/7584"&gt;Automatic fitting of models to multiple GPUS(llama.cpp)&lt;/a&gt;, &lt;a href="https://github.com/mudler/LocalAI/pull/7494"&gt;Added Vibevoice backend&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;November 2025: Major improvements to the UX. Among these: &lt;a href="https://github.com/mudler/LocalAI/pull/7245"&gt;Import models via URL&lt;/a&gt; and &lt;a href="https://github.com/mudler/LocalAI/pull/7325"&gt;Multiple chats and history&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;October 2025: üîå &lt;a href="https://localai.io/docs/features/mcp/"&gt;Model Context Protocol (MCP)&lt;/a&gt; support added for agentic capabilities with external tools&lt;/li&gt; 
 &lt;li&gt;September 2025: New Launcher application for MacOS and Linux, extended support to many backends for Mac and Nvidia L4T devices. Models: Added MLX-Audio, WAN 2.2. WebUI improvements and Python-based backends now ships portable python environments.&lt;/li&gt; 
 &lt;li&gt;August 2025: MLX, MLX-VLM, Diffusers and llama.cpp are now supported on Mac M1/M2/M3+ chips ( with &lt;code&gt;development&lt;/code&gt; suffix in the gallery ): &lt;a href="https://github.com/mudler/LocalAI/pull/6049"&gt;https://github.com/mudler/LocalAI/pull/6049&lt;/a&gt; &lt;a href="https://github.com/mudler/LocalAI/pull/6119"&gt;https://github.com/mudler/LocalAI/pull/6119&lt;/a&gt; &lt;a href="https://github.com/mudler/LocalAI/pull/6121"&gt;https://github.com/mudler/LocalAI/pull/6121&lt;/a&gt; &lt;a href="https://github.com/mudler/LocalAI/pull/6060"&gt;https://github.com/mudler/LocalAI/pull/6060&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;July/August 2025: üîç &lt;a href="https://localai.io/features/object-detection/"&gt;Object Detection&lt;/a&gt; added to the API featuring &lt;a href="https://github.com/roboflow/rf-detr"&gt;rf-detr&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;July 2025: All backends migrated outside of the main binary. LocalAI is now more lightweight, small, and automatically downloads the required backend to run the model. &lt;a href="https://github.com/mudler/LocalAI/releases/tag/v3.2.0"&gt;Read the release notes&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;June 2025: &lt;a href="https://github.com/mudler/LocalAI/pull/5607"&gt;Backend management&lt;/a&gt; has been added. Attention: extras images are going to be deprecated from the next release! Read &lt;a href="https://github.com/mudler/LocalAI/pull/5607"&gt;the backend management PR&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;May 2025: &lt;a href="https://github.com/mudler/LocalAI/pull/5466"&gt;Audio input&lt;/a&gt; and &lt;a href="https://github.com/mudler/LocalAI/pull/5396"&gt;Reranking&lt;/a&gt; in llama.cpp backend, &lt;a href="https://github.com/mudler/LocalAI/pull/5392"&gt;Realtime API&lt;/a&gt;, Support to Gemma, SmollVLM, and more multimodal models (available in the gallery).&lt;/li&gt; 
 &lt;li&gt;May 2025: Important: image name changes &lt;a href="https://github.com/mudler/LocalAI/releases/tag/v2.29.0"&gt;See release&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Apr 2025: Rebrand, WebUI enhancements&lt;/li&gt; 
 &lt;li&gt;Apr 2025: &lt;a href="https://github.com/mudler/LocalAGI"&gt;LocalAGI&lt;/a&gt; and &lt;a href="https://github.com/mudler/LocalRecall"&gt;LocalRecall&lt;/a&gt; join the LocalAI family stack.&lt;/li&gt; 
 &lt;li&gt;Apr 2025: WebUI overhaul, AIO images updates&lt;/li&gt; 
 &lt;li&gt;Feb 2025: Backend cleanup, Breaking changes, new backends (kokoro, OutelTTS, faster-whisper), Nvidia L4T images&lt;/li&gt; 
 &lt;li&gt;Jan 2025: LocalAI model release: &lt;a href="https://huggingface.co/mudler/LocalAI-functioncall-phi-4-v0.3"&gt;https://huggingface.co/mudler/LocalAI-functioncall-phi-4-v0.3&lt;/a&gt;, SANA support in diffusers: &lt;a href="https://github.com/mudler/LocalAI/pull/4603"&gt;https://github.com/mudler/LocalAI/pull/4603&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Dec 2024: stablediffusion.cpp backend (ggml) added ( &lt;a href="https://github.com/mudler/LocalAI/pull/4289"&gt;https://github.com/mudler/LocalAI/pull/4289&lt;/a&gt; )&lt;/li&gt; 
 &lt;li&gt;Nov 2024: Bark.cpp backend added ( &lt;a href="https://github.com/mudler/LocalAI/pull/4287"&gt;https://github.com/mudler/LocalAI/pull/4287&lt;/a&gt; )&lt;/li&gt; 
 &lt;li&gt;Nov 2024: Voice activity detection models (&lt;strong&gt;VAD&lt;/strong&gt;) added to the API: &lt;a href="https://github.com/mudler/LocalAI/pull/4204"&gt;https://github.com/mudler/LocalAI/pull/4204&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Oct 2024: examples moved to &lt;a href="https://github.com/mudler/LocalAI-examples"&gt;LocalAI-examples&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Aug 2024: üÜï FLUX-1, &lt;a href="https://explorer.localai.io"&gt;P2P Explorer&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;July 2024: üî•üî• üÜï P2P Dashboard, LocalAI Federated mode and AI Swarms: &lt;a href="https://github.com/mudler/LocalAI/pull/2723"&gt;https://github.com/mudler/LocalAI/pull/2723&lt;/a&gt;. P2P Global community pools: &lt;a href="https://github.com/mudler/LocalAI/issues/3113"&gt;https://github.com/mudler/LocalAI/issues/3113&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;May 2024: üî•üî• Decentralized P2P llama.cpp: &lt;a href="https://github.com/mudler/LocalAI/pull/2343"&gt;https://github.com/mudler/LocalAI/pull/2343&lt;/a&gt; (peer2peer llama.cpp!) üëâ Docs &lt;a href="https://localai.io/features/distribute/"&gt;https://localai.io/features/distribute/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;May 2024: üî•üî• Distributed inferencing: &lt;a href="https://github.com/mudler/LocalAI/pull/2324"&gt;https://github.com/mudler/LocalAI/pull/2324&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;April 2024: Reranker API: &lt;a href="https://github.com/mudler/LocalAI/pull/2121"&gt;https://github.com/mudler/LocalAI/pull/2121&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Roadmap items: &lt;a href="https://github.com/mudler/LocalAI/issues?q=is%3Aissue+is%3Aopen+label%3Aroadmap"&gt;List of issues&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;üöÄ &lt;a href="https://localai.io/features/"&gt;Features&lt;/a&gt;&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;üß© &lt;a href="https://localai.io/backends/"&gt;Backend Gallery&lt;/a&gt;: Install/remove backends on the fly, powered by OCI images ‚Äî fully customizable and API-driven.&lt;/li&gt; 
 &lt;li&gt;üìñ &lt;a href="https://localai.io/features/text-generation/"&gt;Text generation with GPTs&lt;/a&gt; (&lt;code&gt;llama.cpp&lt;/code&gt;, &lt;code&gt;transformers&lt;/code&gt;, &lt;code&gt;vllm&lt;/code&gt; ... &lt;a href="https://localai.io/model-compatibility/index.html#model-compatibility-table"&gt;&lt;span&gt;üìñ&lt;/span&gt; and more&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;üó£ &lt;a href="https://localai.io/features/text-to-audio/"&gt;Text to Audio&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üîà &lt;a href="https://localai.io/features/audio-to-text/"&gt;Audio to Text&lt;/a&gt; (Audio transcription with &lt;code&gt;whisper.cpp&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;üé® &lt;a href="https://localai.io/features/image-generation"&gt;Image generation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üî• &lt;a href="https://localai.io/features/openai-functions/"&gt;OpenAI-alike tools API&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üß† &lt;a href="https://localai.io/features/embeddings/"&gt;Embeddings generation for vector databases&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;‚úçÔ∏è &lt;a href="https://localai.io/features/constrained_grammars/"&gt;Constrained grammars&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üñºÔ∏è &lt;a href="https://localai.io/models/"&gt;Download Models directly from Huggingface &lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ü•Ω &lt;a href="https://localai.io/features/gpt-vision/"&gt;Vision API&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üîç &lt;a href="https://localai.io/features/object-detection/"&gt;Object Detection&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üìà &lt;a href="https://localai.io/features/reranker/"&gt;Reranker API&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üÜïüñß &lt;a href="https://localai.io/features/distribute/"&gt;P2P Inferencing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üÜïüîå &lt;a href="https://localai.io/docs/features/mcp/"&gt;Model Context Protocol (MCP)&lt;/a&gt; - Agentic capabilities with external tools and &lt;a href="https://github.com/mudler/LocalAGI"&gt;LocalAGI's Agentic capabilities&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üîä Voice activity detection (Silero-VAD support)&lt;/li&gt; 
 &lt;li&gt;üåç Integrated WebUI!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üß© Supported Backends &amp;amp; Acceleration&lt;/h2&gt; 
&lt;p&gt;LocalAI supports a comprehensive range of AI backends with multiple acceleration options:&lt;/p&gt; 
&lt;h3&gt;Text Generation &amp;amp; Language Models&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Backend&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Acceleration Support&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;llama.cpp&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;LLM inference in C/C++&lt;/td&gt; 
   &lt;td&gt;CUDA 12/13, ROCm, Intel SYCL, Vulkan, Metal, CPU&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;vLLM&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Fast LLM inference with PagedAttention&lt;/td&gt; 
   &lt;td&gt;CUDA 12/13, ROCm, Intel&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;transformers&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;HuggingFace transformers framework&lt;/td&gt; 
   &lt;td&gt;CUDA 12/13, ROCm, Intel, CPU&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;exllama2&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;GPTQ inference library&lt;/td&gt; 
   &lt;td&gt;CUDA 12/13&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;MLX&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Apple Silicon LLM inference&lt;/td&gt; 
   &lt;td&gt;Metal (M1/M2/M3+)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;MLX-VLM&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Apple Silicon Vision-Language Models&lt;/td&gt; 
   &lt;td&gt;Metal (M1/M2/M3+)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Audio &amp;amp; Speech Processing&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Backend&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Acceleration Support&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;whisper.cpp&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;OpenAI Whisper in C/C++&lt;/td&gt; 
   &lt;td&gt;CUDA 12/13, ROCm, Intel SYCL, Vulkan, CPU&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;faster-whisper&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Fast Whisper with CTranslate2&lt;/td&gt; 
   &lt;td&gt;CUDA 12/13, ROCm, Intel, CPU&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;bark&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Text-to-audio generation&lt;/td&gt; 
   &lt;td&gt;CUDA 12/13, ROCm, Intel&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;bark-cpp&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;C++ implementation of Bark&lt;/td&gt; 
   &lt;td&gt;CUDA, Metal, CPU&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;coqui&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Advanced TTS with 1100+ languages&lt;/td&gt; 
   &lt;td&gt;CUDA 12/13, ROCm, Intel, CPU&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;kokoro&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Lightweight TTS model&lt;/td&gt; 
   &lt;td&gt;CUDA 12/13, ROCm, Intel, CPU&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;chatterbox&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Production-grade TTS&lt;/td&gt; 
   &lt;td&gt;CUDA 12/13, CPU&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;piper&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Fast neural TTS system&lt;/td&gt; 
   &lt;td&gt;CPU&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;kitten-tts&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Kitten TTS models&lt;/td&gt; 
   &lt;td&gt;CPU&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;silero-vad&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Voice Activity Detection&lt;/td&gt; 
   &lt;td&gt;CPU&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;neutts&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Text-to-speech with voice cloning&lt;/td&gt; 
   &lt;td&gt;CUDA 12/13, ROCm, CPU&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;vibevoice&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Real-time TTS with voice cloning&lt;/td&gt; 
   &lt;td&gt;CUDA 12/13, ROCm, Intel, CPU&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;pocket-tts&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Lightweight CPU-based TTS&lt;/td&gt; 
   &lt;td&gt;CUDA 12/13, ROCm, Intel, CPU&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Image &amp;amp; Video Generation&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Backend&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Acceleration Support&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;stablediffusion.cpp&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Stable Diffusion in C/C++&lt;/td&gt; 
   &lt;td&gt;CUDA 12/13, Intel SYCL, Vulkan, CPU&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;diffusers&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;HuggingFace diffusion models&lt;/td&gt; 
   &lt;td&gt;CUDA 12/13, ROCm, Intel, Metal, CPU&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Specialized AI Tasks&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Backend&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Acceleration Support&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;rfdetr&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Real-time object detection&lt;/td&gt; 
   &lt;td&gt;CUDA 12/13, Intel, CPU&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;rerankers&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Document reranking API&lt;/td&gt; 
   &lt;td&gt;CUDA 12/13, ROCm, Intel, CPU&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;local-store&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Vector database&lt;/td&gt; 
   &lt;td&gt;CPU&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;huggingface&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;HuggingFace API integration&lt;/td&gt; 
   &lt;td&gt;API-based&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Hardware Acceleration Matrix&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Acceleration Type&lt;/th&gt; 
   &lt;th&gt;Supported Backends&lt;/th&gt; 
   &lt;th&gt;Hardware Support&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;NVIDIA CUDA 12&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;All CUDA-compatible backends&lt;/td&gt; 
   &lt;td&gt;Nvidia hardware&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;NVIDIA CUDA 13&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;All CUDA-compatible backends&lt;/td&gt; 
   &lt;td&gt;Nvidia hardware&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;AMD ROCm&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;llama.cpp, whisper, vllm, transformers, diffusers, rerankers, coqui, kokoro, bark, neutts, vibevoice, pocket-tts&lt;/td&gt; 
   &lt;td&gt;AMD Graphics&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Intel oneAPI&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;llama.cpp, whisper, stablediffusion, vllm, transformers, diffusers, rfdetr, rerankers, exllama2, coqui, kokoro, bark, vibevoice, pocket-tts&lt;/td&gt; 
   &lt;td&gt;Intel Arc, Intel iGPUs&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Apple Metal&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;llama.cpp, whisper, diffusers, MLX, MLX-VLM, bark-cpp&lt;/td&gt; 
   &lt;td&gt;Apple M1/M2/M3+&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Vulkan&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;llama.cpp, whisper, stablediffusion&lt;/td&gt; 
   &lt;td&gt;Cross-platform GPUs&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;NVIDIA Jetson (CUDA 12)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;llama.cpp, whisper, stablediffusion, diffusers, rfdetr&lt;/td&gt; 
   &lt;td&gt;ARM64 embedded AI (AGX Orin, etc.)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;NVIDIA Jetson (CUDA 13)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;llama.cpp, whisper, stablediffusion, diffusers, rfdetr&lt;/td&gt; 
   &lt;td&gt;ARM64 embedded AI (DGX Spark)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;CPU Optimized&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;All backends&lt;/td&gt; 
   &lt;td&gt;AVX/AVX2/AVX512, quantization support&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;üîó Community and integrations&lt;/h3&gt; 
&lt;p&gt;Build and deploy custom containers:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/sozercan/aikit"&gt;https://github.com/sozercan/aikit&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;WebUIs:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Jirubizu/localai-admin"&gt;https://github.com/Jirubizu/localai-admin&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/go-skynet/LocalAI-frontend"&gt;https://github.com/go-skynet/LocalAI-frontend&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;QA-Pilot(An interactive chat project that leverages LocalAI LLMs for rapid understanding and navigation of GitHub code repository) &lt;a href="https://github.com/reid41/QA-Pilot"&gt;https://github.com/reid41/QA-Pilot&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Agentic Libraries:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/mudler/cogito"&gt;https://github.com/mudler/cogito&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;MCPs:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/mudler/MCPs"&gt;https://github.com/mudler/MCPs&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Model galleries&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/go-skynet/model-gallery"&gt;https://github.com/go-skynet/model-gallery&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Voice:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/richiejp/VoxInput"&gt;https://github.com/richiejp/VoxInput&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Other:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Helm chart &lt;a href="https://github.com/go-skynet/helm-charts"&gt;https://github.com/go-skynet/helm-charts&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;VSCode extension &lt;a href="https://github.com/badgooooor/localai-vscode-plugin"&gt;https://github.com/badgooooor/localai-vscode-plugin&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Langchain: &lt;a href="https://python.langchain.com/docs/integrations/providers/localai/"&gt;https://python.langchain.com/docs/integrations/providers/localai/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Terminal utility &lt;a href="https://github.com/djcopley/ShellOracle"&gt;https://github.com/djcopley/ShellOracle&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Local Smart assistant &lt;a href="https://github.com/mudler/LocalAGI"&gt;https://github.com/mudler/LocalAGI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Home Assistant &lt;a href="https://github.com/sammcj/homeassistant-localai"&gt;https://github.com/sammcj/homeassistant-localai&lt;/a&gt; / &lt;a href="https://github.com/drndos/hass-openai-custom-conversation"&gt;https://github.com/drndos/hass-openai-custom-conversation&lt;/a&gt; / &lt;a href="https://github.com/valentinfrlch/ha-gpt4vision"&gt;https://github.com/valentinfrlch/ha-gpt4vision&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Discord bot &lt;a href="https://github.com/mudler/LocalAGI/tree/main/examples/discord"&gt;https://github.com/mudler/LocalAGI/tree/main/examples/discord&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Slack bot &lt;a href="https://github.com/mudler/LocalAGI/tree/main/examples/slack"&gt;https://github.com/mudler/LocalAGI/tree/main/examples/slack&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Shell-Pilot(Interact with LLM using LocalAI models via pure shell scripts on your Linux or MacOS system) &lt;a href="https://github.com/reid41/shell-pilot"&gt;https://github.com/reid41/shell-pilot&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Telegram bot &lt;a href="https://github.com/mudler/LocalAI/tree/master/examples/telegram-bot"&gt;https://github.com/mudler/LocalAI/tree/master/examples/telegram-bot&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Another Telegram Bot &lt;a href="https://github.com/JackBekket/Hellper"&gt;https://github.com/JackBekket/Hellper&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Auto-documentation &lt;a href="https://github.com/JackBekket/Reflexia"&gt;https://github.com/JackBekket/Reflexia&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Github bot which answer on issues, with code and documentation as context &lt;a href="https://github.com/JackBekket/GitHelper"&gt;https://github.com/JackBekket/GitHelper&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Github Actions: &lt;a href="https://github.com/marketplace/actions/start-localai"&gt;https://github.com/marketplace/actions/start-localai&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Examples: &lt;a href="https://github.com/mudler/LocalAI/tree/master/examples/"&gt;https://github.com/mudler/LocalAI/tree/master/examples/&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üîó Resources&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://localai.io/docs/advanced/fine-tuning/"&gt;LLM finetuning guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://localai.io/basics/build/index.html"&gt;How to build locally&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://localai.io/basics/getting_started/index.html#run-localai-in-kubernetes"&gt;How to install in Kubernetes&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://localai.io/docs/integrations/"&gt;Projects integrating LocalAI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://io.midori-ai.xyz/howtos/"&gt;How tos section&lt;/a&gt; (curated by our community)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;span&gt;üìñ&lt;/span&gt; üé• &lt;a href="https://localai.io/basics/news/#media-blogs-social"&gt;Media, Blogs, Social&lt;/a&gt;&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.suse.com/c/running-ai-locally/"&gt;Run Visual studio code with LocalAI (SUSE)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üÜï &lt;a href="https://mudler.pm/posts/local-ai-jetson-nano-devkit/"&gt;Run LocalAI on Jetson Nano Devkit&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.pulumi.com/blog/low-code-llm-apps-with-local-ai-flowise-and-pulumi/"&gt;Run LocalAI on AWS EKS with Pulumi&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://staleks.hashnode.dev/installing-localai-on-aws-ec2-instance"&gt;Run LocalAI on AWS&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://mudler.pm/posts/smart-slackbot-for-teams/"&gt;Create a slackbot for teams and OSS projects that answer to documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=PKrDNuJ_dfE"&gt;LocalAI meets k8sgpt&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://mudler.pm/posts/localai-question-answering/"&gt;Question Answering on Documents locally with LangChain, LocalAI, Chroma, and GPT4All&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://medium.com/@tyler_97636/k8sgpt-localai-unlock-kubernetes-superpowers-for-free-584790de9b65"&gt;Tutorial to use k8sgpt with LocalAI&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you utilize this repository, data in a downstream project, please consider citing it with:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@misc{localai,
  author = {Ettore Di Giacinto},
  title = {LocalAI: The free, Open source OpenAI alternative},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/go-skynet/LocalAI}},
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;‚ù§Ô∏è Sponsors&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Do you find LocalAI useful?&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Support the project by becoming &lt;a href="https://github.com/sponsors/mudler"&gt;a backer or sponsor&lt;/a&gt;. Your logo will show up here with a link to your website.&lt;/p&gt; 
&lt;p&gt;A huge thank you to our generous sponsors who support this project covering CI expenses, and our &lt;a href="https://github.com/sponsors/mudler"&gt;Sponsor list&lt;/a&gt;:&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://www.spectrocloud.com/" target="blank"&gt; &lt;img height="200" src="https://github.com/user-attachments/assets/72eab1dd-8b93-4fc0-9ade-84db49f24962" /&gt; &lt;/a&gt; &lt;a href="https://www.premai.io/" target="blank"&gt; &lt;img height="200" src="https://github.com/mudler/LocalAI/assets/2420543/42e4ca83-661e-4f79-8e46-ae43689683d6" /&gt; &lt;br /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h3&gt;Individual sponsors&lt;/h3&gt; 
&lt;p&gt;A special thanks to individual sponsors that contributed to the project, a full list is in &lt;a href="https://github.com/sponsors/mudler"&gt;Github&lt;/a&gt; and &lt;a href="https://buymeacoffee.com/mudler"&gt;buymeacoffee&lt;/a&gt;, a special shout out goes to &lt;a href="https://github.com/drikster80"&gt;drikster80&lt;/a&gt; for being generous. Thank you everyone!&lt;/p&gt; 
&lt;h2&gt;üåü Star history&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#go-skynet/LocalAI&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=go-skynet/LocalAI&amp;amp;type=Date" alt="LocalAI Star history Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;üìñ License&lt;/h2&gt; 
&lt;p&gt;LocalAI is a community-driven project created by &lt;a href="https://github.com/mudler/"&gt;Ettore Di Giacinto&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;MIT - Author Ettore Di Giacinto &lt;a href="mailto:mudler@localai.io"&gt;mudler@localai.io&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;üôá Acknowledgements&lt;/h2&gt; 
&lt;p&gt;LocalAI couldn't have been built without the help of great software already available from the community. Thank you!&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ggerganov/llama.cpp"&gt;llama.cpp&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/tatsu-lab/stanford_alpaca"&gt;https://github.com/tatsu-lab/stanford_alpaca&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/cornelk/llama-go"&gt;https://github.com/cornelk/llama-go&lt;/a&gt; for the initial ideas&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/antimatter15/alpaca.cpp"&gt;https://github.com/antimatter15/alpaca.cpp&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/EdVince/Stable-Diffusion-NCNN"&gt;https://github.com/EdVince/Stable-Diffusion-NCNN&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ggerganov/whisper.cpp"&gt;https://github.com/ggerganov/whisper.cpp&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/rhasspy/piper"&gt;https://github.com/rhasspy/piper&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ü§ó Contributors&lt;/h2&gt; 
&lt;p&gt;This is a community project, a special thanks to our contributors! ü§ó &lt;a href="https://github.com/go-skynet/LocalAI/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=go-skynet/LocalAI" /&gt; &lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>abiosoft/colima</title>
      <link>https://github.com/abiosoft/colima</link>
      <description>&lt;p&gt;Container runtimes on macOS (and Linux) with minimal setup&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/abiosoft/colima/main/colima.png" alt="colima-logo" /&gt;&lt;/p&gt; 
&lt;h2&gt;Colima - container runtimes on macOS (and Linux) with minimal setup.&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/abiosoft/colima/actions/workflows/go.yml"&gt;&lt;img src="https://github.com/abiosoft/colima/actions/workflows/go.yml/badge.svg?sanitize=true" alt="Go" /&gt;&lt;/a&gt; &lt;a href="https://github.com/abiosoft/colima/actions/workflows/integration.yml"&gt;&lt;img src="https://github.com/abiosoft/colima/actions/workflows/integration.yml/badge.svg?sanitize=true" alt="Integration" /&gt;&lt;/a&gt; &lt;a href="https://goreportcard.com/report/github.com/abiosoft/colima"&gt;&lt;img src="https://goreportcard.com/badge/github.com/abiosoft/colima" alt="Go Report Card" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/abiosoft/colima/main/colima.gif" alt="Demonstration" /&gt;&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;p&gt;Support for Intel and Apple Silicon macOS, and Linux&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Simple CLI interface with sensible defaults&lt;/li&gt; 
 &lt;li&gt;Automatic Port Forwarding&lt;/li&gt; 
 &lt;li&gt;Volume mounts&lt;/li&gt; 
 &lt;li&gt;Multiple instances&lt;/li&gt; 
 &lt;li&gt;Support for multiple container runtimes 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://docker.com"&gt;Docker&lt;/a&gt; (with optional Kubernetes)&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://containerd.io"&gt;Containerd&lt;/a&gt; (with optional Kubernetes)&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://linuxcontainers.org/incus"&gt;Incus&lt;/a&gt; (containers and virtual machines)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;p&gt;Colima is available on Homebrew, MacPorts, Nix and &lt;a href="http://github.com/jdx/mise"&gt;mise&lt;/a&gt;. Check &lt;a href="https://raw.githubusercontent.com/abiosoft/colima/main/docs/INSTALL.md"&gt;here&lt;/a&gt; for other installation options.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;# Homebrew
brew install colima

# MacPorts
sudo port install colima

# Nix
nix-env -iA nixpkgs.colima

# Mise
mise use -g colima@latest

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or stay on the bleeding edge (only Homebrew)&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;brew install --HEAD colima
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Upgrading&lt;/h3&gt; 
&lt;p&gt;If upgrading from v0.5.6 or lower, it is required to start afresh by deleting existing instance.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;colima delete # delete existing instance
colima start
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;p&gt;Start Colima with defaults&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;colima start
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more usage options&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;colima --help
colima start --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or use a config file&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;colima start --edit
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Using Templates&lt;/h2&gt; 
&lt;p&gt;When you run the &lt;code&gt;colima template&lt;/code&gt; command, Colima opens the default configuration in a temporary file using your editor (VS Code by default, if installed).&lt;/p&gt; 
&lt;p&gt;For example, you might see something like:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;/var/folders/hm/xmq4vxs13dl2hx2jyct65r080000gn/T/colima-2758922589.yaml

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can edit this temporary file as needed. Once you save and close the file in the editor, Colima automatically overwrites the default template config located at:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;~/.colima/_templates/default.yaml

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To see more options for working with templates, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;colima template --help

&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Runtimes&lt;/h2&gt; 
&lt;p&gt;On initial startup, Colima initiates with a user specified runtime that defaults to Docker.&lt;/p&gt; 
&lt;h3&gt;Docker&lt;/h3&gt; 
&lt;p&gt;Docker client is required for Docker runtime. Installable with brew &lt;code&gt;brew install docker&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;You can use the &lt;code&gt;docker&lt;/code&gt; client on macOS after &lt;code&gt;colima start&lt;/code&gt; with no additional setup.&lt;/p&gt; 
&lt;h3&gt;Containerd&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;colima start --runtime containerd&lt;/code&gt; starts and setup Containerd. You can use &lt;code&gt;colima nerdctl&lt;/code&gt; to interact with Containerd using &lt;a href="https://github.com/containerd/nerdctl"&gt;nerdctl&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;It is recommended to run &lt;code&gt;colima nerdctl install&lt;/code&gt; to install &lt;code&gt;nerdctl&lt;/code&gt; alias script in $PATH.&lt;/p&gt; 
&lt;h3&gt;Kubernetes&lt;/h3&gt; 
&lt;p&gt;kubectl is required for Kubernetes. Installable with &lt;code&gt;brew install kubectl&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;To enable Kubernetes, start Colima with &lt;code&gt;--kubernetes&lt;/code&gt; flag.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;colima start --kubernetes
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Interacting with Image Registry&lt;/h4&gt; 
&lt;p&gt;For Docker runtime, images built or pulled with Docker are accessible to Kubernetes.&lt;/p&gt; 
&lt;p&gt;For Containerd runtime, images built or pulled in the &lt;code&gt;k8s.io&lt;/code&gt; namespace are accessible to Kubernetes.&lt;/p&gt; 
&lt;h3&gt;Incus&lt;/h3&gt; 
&lt;p&gt;&lt;small&gt;&lt;strong&gt;Requires v0.7.0&lt;/strong&gt;&lt;/small&gt;&lt;/p&gt; 
&lt;p&gt;Incus client is required for Incus runtime. Installable with brew &lt;code&gt;brew install incus&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;colima start --runtime incus&lt;/code&gt; starts and setup Incus.&lt;/p&gt; 
&lt;p&gt;You can use the &lt;code&gt;incus&lt;/code&gt; client on macOS after &lt;code&gt;colima start&lt;/code&gt; with no additional setup.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Running virtual machines on Incus is only supported on m3 or newer Apple Silicon devices.&lt;/p&gt; 
&lt;h3&gt;None&lt;/h3&gt; 
&lt;p&gt;&lt;small&gt;&lt;strong&gt;Requires v0.7.0&lt;/strong&gt;&lt;/small&gt;&lt;/p&gt; 
&lt;p&gt;Colima can also be utilised solely as a headless virtual machine manager by specifying &lt;code&gt;none&lt;/code&gt; runtime.&lt;/p&gt; 
&lt;h3&gt;Customizing the VM&lt;/h3&gt; 
&lt;p&gt;The default VM created by Colima has 2 CPUs, 2GiB memory and 100GiB storage.&lt;/p&gt; 
&lt;p&gt;The VM can be customized either by passing additional flags to &lt;code&gt;colima start&lt;/code&gt;. e.g. &lt;code&gt;--cpu&lt;/code&gt;, &lt;code&gt;--memory&lt;/code&gt;, &lt;code&gt;--disk&lt;/code&gt;, &lt;code&gt;--runtime&lt;/code&gt;. Or by editing the config file with &lt;code&gt;colima start --edit&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: &lt;del&gt;disk size cannot be changed after the VM is created.&lt;/del&gt; From v0.5.3, disk size can be increased.&lt;/p&gt; 
&lt;h4&gt;Customization Examples&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;create VM with 1CPU, 2GiB memory and 10GiB storage.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;colima start --cpu 1 --memory 2 --disk 10
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;modify an existing VM to 4CPUs and 8GiB memory.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;colima stop
colima start --cpu 4 --memory 8
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;create VM with Rosetta 2 emulation. Requires v0.5.3 and macOS &amp;gt;= 13 (Ventura) on Apple Silicon.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;colima start --vm-type=vz --vz-rosetta
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Project Goal&lt;/h2&gt; 
&lt;p&gt;To provide container runtimes on macOS with minimal setup.&lt;/p&gt; 
&lt;h2&gt;What is with the name?&lt;/h2&gt; 
&lt;p&gt;Colima means Containers on &lt;a href="https://github.com/lima-vm/lima"&gt;Lima&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Since Lima is aka Linux Machines. By transitivity, Colima can also mean Containers on Linux Machines.&lt;/p&gt; 
&lt;h2&gt;And the Logo?&lt;/h2&gt; 
&lt;p&gt;The logo was contributed by &lt;a href="https://github.com/dhodvogner"&gt;Daniel Hodvogner&lt;/a&gt;. Check &lt;a href="https://github.com/abiosoft/colima/issues/781"&gt;this issue&lt;/a&gt; for more.&lt;/p&gt; 
&lt;h2&gt;Troubleshooting and FAQs&lt;/h2&gt; 
&lt;p&gt;Check &lt;a href="https://raw.githubusercontent.com/abiosoft/colima/main/docs/FAQ.md"&gt;here&lt;/a&gt; for Frequently Asked Questions.&lt;/p&gt; 
&lt;h2&gt;How to Contribute?&lt;/h2&gt; 
&lt;p&gt;Check &lt;a href="https://raw.githubusercontent.com/abiosoft/colima/main/docs/CONTRIBUTE.md"&gt;here&lt;/a&gt; for the instructions on contributing to the project.&lt;/p&gt; 
&lt;h2&gt;Community&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/abiosoft/colima/discussions"&gt;GitHub Discussions&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/abiosoft/colima/issues"&gt;GitHub Issues&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;#colima&lt;/code&gt; channel in the CNCF Slack 
  &lt;ul&gt; 
   &lt;li&gt;New account: &lt;a href="https://slack.cncf.io/"&gt;https://slack.cncf.io/&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Login: &lt;a href="https://cloud-native.slack.com/"&gt;https://cloud-native.slack.com/&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Help Wanted&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Documentation and project website&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;MIT&lt;/p&gt; 
&lt;h2&gt;Sponsoring the Project&lt;/h2&gt; 
&lt;p&gt;If you (or your company) are benefiting from the project and would like to support the contributors, kindly sponsor.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/sponsors/abiosoft"&gt;Github Sponsors&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.buymeacoffee.com/abiosoft"&gt;Buy me a coffee&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://patreon.com/colima"&gt;Patreon&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;a href="https://macstadium.com"&gt;&lt;img src="https://uploads-ssl.webflow.com/5ac3c046c82724970fc60918/5c019d917bba312af7553b49_MacStadium-developerlogo.png" style="max-height: 150px" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>juicedata/juicefs</title>
      <link>https://github.com/juicedata/juicefs</link>
      <description>&lt;p&gt;JuiceFS is a distributed POSIX file system built on top of Redis and S3.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt;&lt;a href="https://github.com/juicedata/juicefs"&gt;&lt;img alt="JuiceFS Logo" src="https://raw.githubusercontent.com/juicedata/juicefs/main/docs/en/images/juicefs-logo-new.svg?sanitize=true" width="50%" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/juicedata/juicefs/releases/latest"&gt;&lt;img alt="Latest Stable Release" src="https://img.shields.io/github/v/release/juicedata/juicefs" /&gt;&lt;/a&gt; &lt;a href="https://github.com/juicedata/juicefs/actions/workflows/unittests.yml"&gt;&lt;img alt="GitHub Workflow Status" src="https://img.shields.io/github/actions/workflow/status/juicedata/juicefs/unittests.yml?branch=main&amp;amp;label=Unit%20Testing" /&gt;&lt;/a&gt; &lt;a href="https://github.com/juicedata/juicefs/actions/workflows/integrationtests.yml"&gt;&lt;img alt="GitHub Workflow Status" src="https://img.shields.io/github/actions/workflow/status/juicedata/juicefs/integrationtests.yml?branch=main&amp;amp;label=Integration%20Testing" /&gt;&lt;/a&gt; &lt;a href="https://goreportcard.com/report/github.com/juicedata/juicefs"&gt;&lt;img alt="Go Report" src="https://goreportcard.com/badge/github.com/juicedata/juicefs" /&gt;&lt;/a&gt; &lt;a href="https://juicefs.com/docs/community/introduction"&gt;&lt;img alt="English doc" src="https://img.shields.io/badge/docs-Doc%20Center-brightgreen" /&gt;&lt;/a&gt; &lt;a href="https://go.juicefs.com/slack"&gt;&lt;img alt="Join Slack" src="https://badgen.net/badge/Slack/Join%20JuiceFS/0abd59?icon=slack" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;JuiceFS&lt;/strong&gt; is a high-performance &lt;a href="https://en.wikipedia.org/wiki/POSIX"&gt;POSIX&lt;/a&gt; file system released under Apache License 2.0, particularly designed for the cloud-native environment. The data, stored via JuiceFS, will be persisted in Object Storage &lt;em&gt;(e.g. Amazon S3)&lt;/em&gt;, and the corresponding metadata can be persisted in various compatible database engines such as Redis, MySQL, and TiKV based on the scenarios and requirements.&lt;/p&gt; 
&lt;p&gt;With JuiceFS, massive cloud storage can be directly connected to big data, machine learning, artificial intelligence, and various application platforms in production environments. Without modifying code, the massive cloud storage can be used as efficiently as local storage.&lt;/p&gt; 
&lt;p&gt;üìñ &lt;strong&gt;Document&lt;/strong&gt;: &lt;a href="https://juicefs.com/docs/community/quick_start_guide"&gt;Quick Start Guide&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Highlighted Features&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Fully POSIX-compatible&lt;/strong&gt;: Use as a local file system, seamlessly docking with existing applications without breaking business workflow.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Fully Hadoop-compatible&lt;/strong&gt;: JuiceFS' &lt;a href="https://juicefs.com/docs/community/hadoop_java_sdk"&gt;Hadoop Java SDK&lt;/a&gt; is compatible with Hadoop 2.x and Hadoop 3.x as well as a variety of components in the Hadoop ecosystems.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;S3-compatible&lt;/strong&gt;: JuiceFS' &lt;a href="https://juicefs.com/docs/community/s3_gateway"&gt;S3 Gateway&lt;/a&gt; provides an S3-compatible interface.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Cloud Native&lt;/strong&gt;: A &lt;a href="https://juicefs.com/docs/community/how_to_use_on_kubernetes"&gt;Kubernetes CSI Driver&lt;/a&gt; is provided for easily using JuiceFS in Kubernetes.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Shareable&lt;/strong&gt;: JuiceFS is a shared file storage that can be read and written by thousands of clients.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Strong Consistency&lt;/strong&gt;: The confirmed modification will be immediately visible on all the servers mounted with the same file system.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Outstanding Performance&lt;/strong&gt;: The latency can be as low as a few milliseconds, and the throughput can be expanded nearly unlimitedly &lt;em&gt;(depending on the size of the Object Storage)&lt;/em&gt;. &lt;a href="https://juicefs.com/docs/community/benchmark"&gt;Test results&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Data Encryption&lt;/strong&gt;: Supports data encryption in transit and at rest (please refer to &lt;a href="https://juicefs.com/docs/community/security/encrypt"&gt;the guide&lt;/a&gt; for more information).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Global File Locks&lt;/strong&gt;: JuiceFS supports both BSD locks (flock) and POSIX record locks (fcntl).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Data Compression&lt;/strong&gt;: JuiceFS supports &lt;a href="https://lz4.github.io/lz4"&gt;LZ4&lt;/a&gt; or &lt;a href="https://facebook.github.io/zstd"&gt;Zstandard&lt;/a&gt; to compress all your data.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/juicedata/juicefs/main/#architecture"&gt;Architecture&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/juicedata/juicefs/main/#getting-started"&gt;Getting Started&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/juicedata/juicefs/main/#advanced-topics"&gt;Advanced Topics&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/juicedata/juicefs/main/#posix-compatibility"&gt;POSIX Compatibility&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/juicedata/juicefs/main/#performance-benchmark"&gt;Performance Benchmark&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/juicedata/juicefs/main/#supported-object-storage"&gt;Supported Object Storage&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/juicedata/juicefs/main/#who-is-using"&gt;Who is using&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/juicedata/juicefs/main/#roadmap"&gt;Roadmap&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/juicedata/juicefs/main/#reporting-issues"&gt;Reporting Issues&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/juicedata/juicefs/main/#contributing"&gt;Contributing&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/juicedata/juicefs/main/#community"&gt;Community&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/juicedata/juicefs/main/#usage-tracking"&gt;Usage Tracking&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/juicedata/juicefs/main/#license"&gt;License&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/juicedata/juicefs/main/#credits"&gt;Credits&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/juicedata/juicefs/main/#faq"&gt;FAQ&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Architecture&lt;/h2&gt; 
&lt;p&gt;JuiceFS consists of three parts:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;JuiceFS Client&lt;/strong&gt;: Coordinates Object Storage and metadata storage engine as well as implementation of file system interfaces such as POSIX, Hadoop, Kubernetes, and S3 gateway.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Data Storage&lt;/strong&gt;: Stores data, with supports of a variety of data storage media, e.g., local disk, public or private cloud Object Storage, and HDFS.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Metadata Engine&lt;/strong&gt;: Stores the corresponding metadata that contains information of file name, file size, permission group, creation and modification time and directory structure, etc., with supports of different metadata engines, e.g., Redis, MySQL, SQLite and TiKV.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/juicedata/juicefs/main/docs/en/images/juicefs-arch-new.png" alt="JuiceFS Architecture" /&gt;&lt;/p&gt; 
&lt;p&gt;JuiceFS can store the metadata of file system on different metadata engines, like Redis, which is a fast, open-source, in-memory key-value data storage, particularly suitable for storing metadata; meanwhile, all the data will be stored in Object Storage through JuiceFS client. &lt;a href="https://juicefs.com/docs/community/architecture"&gt;Learn more&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/juicedata/juicefs/main/docs/en/images/data-structure-diagram.svg?sanitize=true" alt="data-structure-diagram" /&gt;&lt;/p&gt; 
&lt;p&gt;Each file stored in JuiceFS is split into &lt;strong&gt;"Chunk"&lt;/strong&gt; s at a fixed size with the default upper limit of 64 MiB. Each Chunk is composed of one or more &lt;strong&gt;"Slice"&lt;/strong&gt;(s), and the length of the slice varies depending on how the file is written. Each slice is composed of size-fixed &lt;strong&gt;"Block"&lt;/strong&gt; s, which are 4 MiB by default. These blocks will be stored in Object Storage in the end; at the same time, the metadata information of the file and its Chunks, Slices, and Blocks will be stored in metadata engines via JuiceFS. &lt;a href="https://juicefs.com/docs/community/architecture/#how-juicefs-store-files"&gt;Learn more&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/juicedata/juicefs/main/docs/en/images/how-juicefs-stores-files.svg?sanitize=true" alt="How JuiceFS stores your files" /&gt;&lt;/p&gt; 
&lt;p&gt;When using JuiceFS, files will eventually be split into Chunks, Slices and Blocks and stored in Object Storage. Therefore, the source files stored in JuiceFS cannot be found in the file browser of the Object Storage platform; instead, there are only a chunks directory and a bunch of digitally numbered directories and files in the bucket. Don't panic! This is just the secret of the high-performance operation of JuiceFS!&lt;/p&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;Before you begin, make sure you have:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;One supported metadata engine, see &lt;a href="https://juicefs.com/docs/community/databases_for_metadata"&gt;How to Set Up Metadata Engine&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;One supported Object Storage for storing data blocks, see &lt;a href="https://juicefs.com/docs/community/how_to_setup_object_storage"&gt;Supported Object Storage&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://juicefs.com/docs/community/installation"&gt;JuiceFS Client&lt;/a&gt; downloaded and installed&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Please refer to &lt;a href="https://juicefs.com/docs/community/quick_start_guide"&gt;Quick Start Guide&lt;/a&gt; to start using JuiceFS right away!&lt;/p&gt; 
&lt;h3&gt;Command Reference&lt;/h3&gt; 
&lt;p&gt;Check out all the command line options in &lt;a href="https://juicefs.com/docs/community/command_reference"&gt;command reference&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Containers&lt;/h3&gt; 
&lt;p&gt;JuiceFS can be used as a persistent volume for Docker and Podman, please check &lt;a href="https://juicefs.com/docs/community/juicefs_on_docker"&gt;here&lt;/a&gt; for details.&lt;/p&gt; 
&lt;h3&gt;Kubernetes&lt;/h3&gt; 
&lt;p&gt;It is also very easy to use JuiceFS on Kubernetes. Please find more information &lt;a href="https://juicefs.com/docs/community/how_to_use_on_kubernetes"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Hadoop Java SDK&lt;/h3&gt; 
&lt;p&gt;If you wanna use JuiceFS in Hadoop, check &lt;a href="https://juicefs.com/docs/community/hadoop_java_sdk"&gt;Hadoop Java SDK&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Advanced Topics&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://juicefs.com/docs/community/redis_best_practices"&gt;Redis Best Practices&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://juicefs.com/docs/community/how_to_setup_object_storage"&gt;How to Setup Object Storage&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://juicefs.com/docs/community/cache"&gt;Cache&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://juicefs.com/docs/community/fault_diagnosis_and_analysis"&gt;Fault Diagnosis and Analysis&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://juicefs.com/docs/community/fuse_mount_options"&gt;FUSE Mount Options&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://juicefs.com/docs/community/installation#windows"&gt;Using JuiceFS on Windows&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://juicefs.com/docs/community/s3_gateway"&gt;S3 Gateway&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Please refer to &lt;a href="https://juicefs.com/docs/community/introduction"&gt;JuiceFS Document Center&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h2&gt;POSIX Compatibility&lt;/h2&gt; 
&lt;p&gt;JuiceFS has passed all of the compatibility tests (8813 in total) in the latest &lt;a href="https://github.com/pjd/pjdfstest"&gt;pjdfstest&lt;/a&gt; .&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;All tests successful.

Test Summary Report
-------------------
/root/soft/pjdfstest/tests/chown/00.t          (Wstat: 0 Tests: 1323 Failed: 0)
  TODO passed:   693, 697, 708-709, 714-715, 729, 733
Files=235, Tests=8813, 233 wallclock secs ( 2.77 usr  0.38 sys +  2.57 cusr  3.93 csys =  9.65 CPU)
Result: PASS
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Aside from the POSIX features covered by pjdfstest, JuiceFS also provides:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Close-to-open consistency&lt;/strong&gt;. Once a file is written &lt;em&gt;and&lt;/em&gt; closed, it is guaranteed to view the written data in the following opens and reads from any client. Within the same mount point, all the written data can be read immediately.&lt;/li&gt; 
 &lt;li&gt;Rename and all other metadata operations are atomic, which are guaranteed by supported metadata engine transaction.&lt;/li&gt; 
 &lt;li&gt;Opened files remain accessible after unlink from same mount point.&lt;/li&gt; 
 &lt;li&gt;Mmap (tested with FSx).&lt;/li&gt; 
 &lt;li&gt;Fallocate with punch hole support.&lt;/li&gt; 
 &lt;li&gt;Extended attributes (xattr).&lt;/li&gt; 
 &lt;li&gt;BSD locks (flock).&lt;/li&gt; 
 &lt;li&gt;POSIX record locks (fcntl).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Performance Benchmark&lt;/h2&gt; 
&lt;h3&gt;Basic benchmark&lt;/h3&gt; 
&lt;p&gt;JuiceFS provides a subcommand that can run a few basic benchmarks to help you understand how it works in your environment:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/juicedata/juicefs/main/docs/en/images/juicefs-bench.png" alt="JuiceFS Bench" /&gt;&lt;/p&gt; 
&lt;h3&gt;Throughput&lt;/h3&gt; 
&lt;p&gt;A sequential read/write benchmark has also been performed on JuiceFS, &lt;a href="https://aws.amazon.com/efs"&gt;EFS&lt;/a&gt; and &lt;a href="https://github.com/s3fs-fuse/s3fs-fuse"&gt;S3FS&lt;/a&gt; by &lt;a href="https://github.com/axboe/fio"&gt;fio&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/juicedata/juicefs/main/docs/en/images/sequential-read-write-benchmark.svg?sanitize=true" alt="Sequential Read Write Benchmark" /&gt;&lt;/p&gt; 
&lt;p&gt;Above result figure shows that JuiceFS can provide 10X more throughput than the other two (see &lt;a href="https://juicefs.com/docs/community/fio"&gt;more details&lt;/a&gt;).&lt;/p&gt; 
&lt;h3&gt;Metadata IOPS&lt;/h3&gt; 
&lt;p&gt;A simple mdtest benchmark has been performed on JuiceFS, &lt;a href="https://aws.amazon.com/efs"&gt;EFS&lt;/a&gt; and &lt;a href="https://github.com/s3fs-fuse/s3fs-fuse"&gt;S3FS&lt;/a&gt; by &lt;a href="https://github.com/hpc/ior"&gt;mdtest&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/juicedata/juicefs/main/docs/en/images/metadata-benchmark.svg?sanitize=true" alt="Metadata Benchmark" /&gt;&lt;/p&gt; 
&lt;p&gt;The result shows that JuiceFS can provide significantly more metadata IOPS than the other two (see &lt;a href="https://juicefs.com/docs/community/mdtest"&gt;more details&lt;/a&gt;).&lt;/p&gt; 
&lt;h3&gt;Analyze performance&lt;/h3&gt; 
&lt;p&gt;See &lt;a href="https://juicefs.com/docs/community/fault_diagnosis_and_analysis#performance-monitor"&gt;Real-Time Performance Monitoring&lt;/a&gt; if you encountered performance issues.&lt;/p&gt; 
&lt;h2&gt;Supported Object Storage&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Amazon S3 &lt;em&gt;(and other S3 compatible Object Storage services)&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;Google Cloud Storage&lt;/li&gt; 
 &lt;li&gt;Azure Blob Storage&lt;/li&gt; 
 &lt;li&gt;Alibaba Cloud Object Storage Service (OSS)&lt;/li&gt; 
 &lt;li&gt;Tencent Cloud Object Storage (COS)&lt;/li&gt; 
 &lt;li&gt;Qiniu Cloud Object Storage (Kodo)&lt;/li&gt; 
 &lt;li&gt;QingStor Object Storage&lt;/li&gt; 
 &lt;li&gt;Ceph RGW&lt;/li&gt; 
 &lt;li&gt;MinIO&lt;/li&gt; 
 &lt;li&gt;Local disk&lt;/li&gt; 
 &lt;li&gt;Redis&lt;/li&gt; 
 &lt;li&gt;...&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;JuiceFS supports numerous Object Storage services. &lt;a href="https://juicefs.com/docs/community/how_to_setup_object_storage#supported-object-storage"&gt;Learn more&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Who is using&lt;/h2&gt; 
&lt;p&gt;JuiceFS is production ready and used by thousands of machines in production. A list of users has been assembled and documented &lt;a href="https://juicefs.com/docs/community/adopters"&gt;here&lt;/a&gt;. In addition JuiceFS has several collaborative projects that integrate with other open source projects, which we have documented &lt;a href="https://juicefs.com/docs/community/integrations"&gt;here&lt;/a&gt;. If you are also using JuiceFS, please feel free to let us know, and you are welcome to share your specific experience with everyone.&lt;/p&gt; 
&lt;p&gt;The storage format is stable, and will be supported by all future releases.&lt;/p&gt; 
&lt;h2&gt;Roadmap&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Gateway Optimization&lt;/li&gt; 
 &lt;li&gt;Resumable Sync&lt;/li&gt; 
 &lt;li&gt;Read-ahead Optimization&lt;/li&gt; 
 &lt;li&gt;Optimization for Large-scale Scenarios&lt;/li&gt; 
 &lt;li&gt;Snapshots&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Reporting Issues&lt;/h2&gt; 
&lt;p&gt;We use &lt;a href="https://github.com/juicedata/juicefs/issues"&gt;GitHub Issues&lt;/a&gt; to track community reported issues. You can also &lt;a href="https://raw.githubusercontent.com/juicedata/juicefs/main/#community"&gt;contact&lt;/a&gt; the community for any questions.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Thank you for your contribution! Please refer to the &lt;a href="https://juicefs.com/docs/community/development/contributing_guide"&gt;JuiceFS Contributing Guide&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h2&gt;Community&lt;/h2&gt; 
&lt;p&gt;Welcome to join the &lt;a href="https://github.com/juicedata/juicefs/discussions"&gt;Discussions&lt;/a&gt; and the &lt;a href="https://go.juicefs.com/slack"&gt;Slack channel&lt;/a&gt; to connect with JuiceFS team members and other users.&lt;/p&gt; 
&lt;h2&gt;Usage Tracking&lt;/h2&gt; 
&lt;p&gt;JuiceFS collects &lt;strong&gt;anonymous&lt;/strong&gt; usage data by default to help us better understand how the community is using JuiceFS. Only core metrics (e.g. version number) will be reported, and user data and any other sensitive data will not be included. The related code can be viewed &lt;a href="https://raw.githubusercontent.com/juicedata/juicefs/main/pkg/usage/usage.go"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You could also disable reporting easily by command line option &lt;code&gt;--no-usage-report&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;juicefs mount --no-usage-report
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;JuiceFS is open-sourced under Apache License 2.0, see &lt;a href="https://raw.githubusercontent.com/juicedata/juicefs/main/LICENSE"&gt;LICENSE&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Credits&lt;/h2&gt; 
&lt;p&gt;The design of JuiceFS was inspired by &lt;a href="https://research.google/pubs/pub51"&gt;Google File System&lt;/a&gt;, &lt;a href="https://hadoop.apache.org"&gt;HDFS&lt;/a&gt; and &lt;a href="https://moosefs.com"&gt;MooseFS&lt;/a&gt;. Thanks for their great work!&lt;/p&gt; 
&lt;h2&gt;FAQ&lt;/h2&gt; 
&lt;h3&gt;Why doesn't JuiceFS support XXX Object Storage?&lt;/h3&gt; 
&lt;p&gt;JuiceFS supports many Object Storage services. Please check out &lt;a href="https://juicefs.com/docs/community/how_to_setup_object_storage#supported-object-storage"&gt;this list&lt;/a&gt; first. If the Object Storage you want to use is compatible with S3, you could treat it as S3. Otherwise, try reporting any issue.&lt;/p&gt; 
&lt;h3&gt;Can I use Redis Cluster as metadata engine?&lt;/h3&gt; 
&lt;p&gt;Yes. Since &lt;a href="https://github.com/juicedata/juicefs/releases/tag/v1.0.0-beta3"&gt;v1.0.0 Beta3&lt;/a&gt; JuiceFS supports the use of &lt;a href="https://redis.io/docs/manual/scaling"&gt;Redis Cluster&lt;/a&gt; as the metadata engine, but it should be noted that Redis Cluster requires that the keys of all operations in a transaction must be in the same hash slot, so a JuiceFS file system can only use one hash slot.&lt;/p&gt; 
&lt;p&gt;See &lt;a href="https://juicefs.com/docs/community/redis_best_practices"&gt;"Redis Best Practices"&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h3&gt;What's the difference between JuiceFS and XXX?&lt;/h3&gt; 
&lt;p&gt;See &lt;a href="https://juicefs.com/docs/community/comparison/juicefs_vs_alluxio"&gt;"Comparison with Others"&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;p&gt;For more FAQs, please see the &lt;a href="https://juicefs.com/docs/community/faq"&gt;full list&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Stargazers over time&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#juicedata/juicefs&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=juicedata/juicefs&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>actions/actions-runner-controller</title>
      <link>https://github.com/actions/actions-runner-controller</link>
      <description>&lt;p&gt;Kubernetes controller for GitHub Actions self-hosted runners&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Actions Runner Controller (ARC)&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://bestpractices.coreinfrastructure.org/projects/6061"&gt;&lt;img src="https://bestpractices.coreinfrastructure.org/projects/6061/badge" alt="CII Best Practices" /&gt;&lt;/a&gt; &lt;a href="https://github.com/jonico/awesome-runners"&gt;&lt;img src="https://img.shields.io/badge/listed%20on-awesome--runners-blue.svg?sanitize=true" alt="awesome-runners" /&gt;&lt;/a&gt; &lt;a href="https://artifacthub.io/packages/search?repo=actions-runner-controller"&gt;&lt;img src="https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/actions-runner-controller" alt="Artifact Hub" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;About&lt;/h2&gt; 
&lt;p&gt;Actions Runner Controller (ARC) is a Kubernetes operator that orchestrates and scales self-hosted runners for GitHub Actions.&lt;/p&gt; 
&lt;p&gt;With ARC, you can create runner scale sets that automatically scale based on the number of workflows running in your repository, organization, or enterprise. Because controlled runners can be ephemeral and based on containers, new runner instances can scale up or down rapidly and cleanly. For more information about autoscaling, see &lt;a href="https://docs.github.com/en/actions/hosting-your-own-runners/managing-self-hosted-runners/autoscaling-with-self-hosted-runners"&gt;"Autoscaling with self-hosted runners."&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;You can set up ARC on Kubernetes using Helm, then create and run a workflow that uses runner scale sets. For more information about runner scale sets, see &lt;a href="https://docs.github.com/en/actions/hosting-your-own-runners/managing-self-hosted-runners-with-actions-runner-controller/deploying-runner-scale-sets-with-actions-runner-controller#runner-scale-set"&gt;"Deploying runner scale sets with Actions Runner Controller."&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;People&lt;/h2&gt; 
&lt;p&gt;Actions Runner Controller (ARC) is an open-source project currently developed and maintained in collaboration with the GitHub Actions team, external maintainers @mumoshu and @toast-gear, various &lt;a href="https://github.com/actions/actions-runner-controller/graphs/contributors"&gt;contributors&lt;/a&gt;, and the &lt;a href="https://github.com/actions/actions-runner-controller/discussions"&gt;awesome community&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you think the project is awesome and is adding value to your business, please consider directly sponsoring &lt;a href="https://github.com/sponsors/actions-runner-controller"&gt;community maintainers&lt;/a&gt; and individual contributors via GitHub Sponsors.&lt;/p&gt; 
&lt;p&gt;If you are already the employer of one of the contributors, sponsoring via GitHub Sponsors might not be an option. Just support them by other means!&lt;/p&gt; 
&lt;p&gt;See &lt;a href="https://github.com/sponsors/actions-runner-controller"&gt;the sponsorship dashboard&lt;/a&gt; for the former and the current sponsors.&lt;/p&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;To give ARC a try with just a handful of commands, please refer to the &lt;a href="https://docs.github.com/en/actions/hosting-your-own-runners/managing-self-hosted-runners-with-actions-runner-controller/quickstart-for-actions-runner-controller"&gt;Quickstart guide&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For an overview of ARC, please refer to &lt;a href="https://docs.github.com/en/actions/hosting-your-own-runners/managing-self-hosted-runners-with-actions-runner-controller/about-actions-runner-controller"&gt;About ARC&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;With the introduction of &lt;a href="https://github.com/actions/actions-runner-controller/discussions/2775"&gt;autoscaling runner scale sets&lt;/a&gt;, the existing &lt;a href="https://raw.githubusercontent.com/actions/actions-runner-controller/master/docs/automatically-scaling-runners.md"&gt;autoscaling modes&lt;/a&gt; are now legacy. The legacy modes have certain use cases and will continue to be maintained by the community only.&lt;/p&gt; 
&lt;p&gt;For further information on what is supported by GitHub and what's managed by the community, please refer to &lt;a href="https://github.com/actions/actions-runner-controller/discussions/2775"&gt;this announcement discussion.&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Documentation&lt;/h3&gt; 
&lt;p&gt;ARC documentation is available on &lt;a href="https://docs.github.com/en/actions/hosting-your-own-runners/managing-self-hosted-runners-with-actions-runner-controller/quickstart-for-actions-runner-controller"&gt;docs.github.com&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Legacy documentation&lt;/h3&gt; 
&lt;p&gt;The following documentation is for the legacy autoscaling modes that continue to be maintained by the community:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/actions/actions-runner-controller/master/docs/quickstart.md"&gt;Quickstart guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/actions/actions-runner-controller/master/docs/about-arc.md"&gt;About ARC&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/actions/actions-runner-controller/master/docs/installing-arc.md"&gt;Installing ARC&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/actions/actions-runner-controller/master/docs/authenticating-to-the-github-api.md"&gt;Authenticating to the GitHub API&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/actions/actions-runner-controller/master/docs/deploying-arc-runners.md"&gt;Deploying ARC runners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/actions/actions-runner-controller/master/docs/choosing-runner-destination.md"&gt;Adding ARC runners to a repository, organization, or enterprise&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/actions/actions-runner-controller/master/docs/automatically-scaling-runners.md"&gt;Automatically scaling runners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/actions/actions-runner-controller/master/docs/using-custom-volumes.md"&gt;Using custom volumes&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/actions/actions-runner-controller/master/docs/using-arc-runners-in-a-workflow.md"&gt;Using ARC runners in a workflow&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/actions/actions-runner-controller/master/docs/managing-access-with-runner-groups.md"&gt;Managing access with runner groups&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/actions/actions-runner-controller/master/docs/configuring-windows-runners.md"&gt;Configuring Windows runners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/actions/actions-runner-controller/master/docs/using-arc-across-organizations.md"&gt;Using ARC across organizations&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/actions/actions-runner-controller/master/docs/using-entrypoint-features.md"&gt;Using entrypoint features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/actions/actions-runner-controller/master/docs/deploying-alternative-runners.md"&gt;Deploying alternative runners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/actions/actions-runner-controller/master/docs/monitoring-and-troubleshooting.md"&gt;Monitoring and troubleshooting&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions from the community. For more details on contributing to the project (including requirements), please refer to "&lt;a href="https://github.com/actions/actions-runner-controller/raw/master/CONTRIBUTING.md"&gt;Getting Started with Contributing&lt;/a&gt;."&lt;/p&gt; 
&lt;h2&gt;Troubleshooting&lt;/h2&gt; 
&lt;p&gt;We are very happy to help you with any issues you have. Please refer to the "&lt;a href="https://github.com/actions/actions-runner-controller/raw/master/TROUBLESHOOTING.md"&gt;Troubleshooting&lt;/a&gt;" section for common issues.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ArvinLovegood/go-stock</title>
      <link>https://github.com/ArvinLovegood/go-stock</link>
      <description>&lt;p&gt;ü¶Ñü¶Ñü¶ÑAIËµãËÉΩËÇ°Á•®ÂàÜÊûêÔºöAIÂä†ÊåÅÁöÑËÇ°Á•®ÂàÜÊûê/ÈÄâËÇ°Â∑•ÂÖ∑„ÄÇËÇ°Á•®Ë°åÊÉÖËé∑ÂèñÔºåAIÁÉ≠ÁÇπËµÑËÆØÂàÜÊûêÔºåAIËµÑÈáë/Ë¥¢Âä°ÂàÜÊûêÔºåÊ∂®Ë∑åÊä•Ë≠¶Êé®ÈÄÅ„ÄÇÊîØÊåÅAËÇ°ÔºåÊ∏ØËÇ°ÔºåÁæéËÇ°„ÄÇÊîØÊåÅÂ∏ÇÂú∫Êï¥‰Ωì/‰∏™ËÇ°ÊÉÖÁª™ÂàÜÊûêÔºåAIËæÖÂä©ÈÄâËÇ°Á≠â„ÄÇÊï∞ÊçÆÂÖ®ÈÉ®‰øùÁïôÂú®Êú¨Âú∞„ÄÇÊîØÊåÅDeepSeekÔºåOpenAIÔºå OllamaÔºåLMStudioÔºåAnythingLLMÔºåÁ°ÖÂü∫ÊµÅÂä®ÔºåÁÅ´Â±±ÊñπËàüÔºåÈòøÈáå‰∫ëÁôæÁÇºÁ≠âÂπ≥Âè∞ÊàñÊ®°Âûã„ÄÇ&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;go-stock : Âü∫‰∫éÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑAIËµãËÉΩËÇ°Á•®ÂàÜÊûêÂ∑•ÂÖ∑&lt;/h1&gt; 
&lt;h2&gt;&lt;img src="https://raw.githubusercontent.com/ArvinLovegood/go-stock/dev/build/appicon.png" alt="go-stock" /&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://img.shields.io/github/v/release/ArvinLovegood/go-stock?link=https%3A%2F%2Fgithub.com%2FArvinLovegood%2Fgo-stock%2Freleases&amp;amp;link=https%3A%2F%2Fgithub.com%2FArvinLovegood%2Fgo-stock%2Freleases" alt="GitHub Release" /&gt; &lt;a href="https://github.com/ArvinLovegood/go-stock"&gt;&lt;img src="https://img.shields.io/github/stars/ArvinLovegood/go-stock?link=https%3A%2F%2Fgithub.com%2FArvinLovegood%2Fgo-stock" alt="GitHub Repo stars" /&gt;&lt;/a&gt; &lt;a href="https://gitee.com/arvinlovegood_admin/go-stock"&gt;&lt;img src="https://gitee.com/arvinlovegood_admin/go-stock/badge/star.svg?theme=dark" alt="star" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;üåüÂÖ¨‰ºóÂè∑&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/ArvinLovegood/go-stock/dev/build/screenshot/%E6%89%AB%E7%A0%81_%E6%90%9C%E7%B4%A2%E8%81%94%E5%90%88%E4%BC%A0%E6%92%AD%E6%A0%B7%E5%BC%8F-%E7%99%BD%E8%89%B2%E7%89%88.png" alt="Êâ´Á†Å_ÊêúÁ¥¢ËÅîÂêà‰º†Êí≠Ê†∑Âºè-ÁôΩËâ≤Áâà.png" /&gt;&lt;/p&gt; 
&lt;h3&gt;üìà ‰∫§ÊµÅÁæ§&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;QQ‰∫§ÊµÅÁæ§Ôºö&lt;a href="http://qm.qq.com/cgi-bin/qm/qr?_wv=1027&amp;amp;k=0YQ8qD3exahsD4YLNhzQTWe5ssstWC89&amp;amp;authKey=usOMMRFtIQDC%2FYcatHYapcxQbJ7PwXPHK9OypTXWzNjAq%2FRVvQu9bj2lRgb%2BSZ3p&amp;amp;noverify=0&amp;amp;group_code=491605333"&gt;ÁÇπÂáªÈìæÊé•Âä†ÂÖ•Áæ§ËÅä„Äêgo-stock‰∫§ÊµÅÁæ§„ÄëÔºö491605333(ÂÆöÊúüÊ∏ÖÁêÜÔºåÈöèÁºòÂÖ•Áæ§)&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;‚ú® ÁÆÄ‰ªã&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Êú¨È°πÁõÆÂü∫‰∫éWailsÂíåNaiveUIÂºÄÂèëÔºåÁªìÂêàAIÂ§ßÊ®°ÂûãÊûÑÂª∫ÁöÑËÇ°Á•®ÂàÜÊûêÂ∑•ÂÖ∑„ÄÇ&lt;/li&gt; 
 &lt;li&gt;ÁõÆÂâçÂ∑≤ÊîØÊåÅAËÇ°ÔºåÊ∏ØËÇ°ÔºåÁæéËÇ°ÔºåÊú™Êù•ËÆ°ÂàíÂä†ÂÖ•Âü∫ÈáëÔºåETFÁ≠âÊîØÊåÅ„ÄÇ&lt;/li&gt; 
 &lt;li&gt;ÊîØÊåÅÂ∏ÇÂú∫Êï¥‰Ωì/‰∏™ËÇ°ÊÉÖÁª™ÂàÜÊûêÔºåKÁ∫øÊäÄÊúØÊåáÊ†áÂàÜÊûêÁ≠âÂäüËÉΩ„ÄÇ&lt;/li&gt; 
 &lt;li&gt;Êú¨È°πÁõÆ‰ªÖ‰æõÂ®±‰πêÔºå‰∏çÂñúÂãøÂñ∑ÔºåAIÂàÜÊûêËÇ°Á•®ÁªìÊûú‰ªÖ‰æõÂ≠¶‰π†Á†îÁ©∂ÔºåÊäïËµÑÊúâÈ£éÈô©ÔºåËØ∑Ë∞®ÊÖé‰ΩøÁî®„ÄÇ&lt;/li&gt; 
 &lt;li&gt;ÂºÄÂèëÁéØÂ¢É‰∏ªË¶ÅÂü∫‰∫éWindows10+ÔºåÂÖ∂‰ªñÂπ≥Âè∞Êú™ÊµãËØïÊàñÂäüËÉΩÂèóÈôê„ÄÇ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üì¶ Á´ãÂç≥‰ΩìÈ™å&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ÁªøËâ≤ÁâàÔºö&lt;a href="https://github.com/ArvinLovegood/go-stock/releases"&gt;go-stock-windows-amd64.exe&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;MACOSÁªøËâ≤ÁâàÔºö&lt;a href="https://github.com/ArvinLovegood/go-stock/releases"&gt;go-stock-darwin-universal&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üí¨ ÊîØÊåÅÂ§ßÊ®°Âûã/Âπ≥Âè∞&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Ê®°Âûã&lt;/th&gt; 
   &lt;th&gt;Áä∂ÊÄÅ&lt;/th&gt; 
   &lt;th&gt;Â§áÊ≥®&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://platform.openai.com/"&gt;OpenAI&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;ÂèØÊé•ÂÖ•‰ªª‰Ωï OpenAI Êé•Âè£Ê†ºÂºèÊ®°Âûã&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://ollama.com/"&gt;Ollama&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;Êú¨Âú∞Â§ßÊ®°ÂûãËøêË°åÂπ≥Âè∞&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://lmstudio.ai/"&gt;LMStudio&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;Êú¨Âú∞Â§ßÊ®°ÂûãËøêË°åÂπ≥Âè∞&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://anythingllm.com/"&gt;AnythingLLM&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;Êú¨Âú∞Áü•ËØÜÂ∫ì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://www.deepseek.com/"&gt;DeepSeek&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;deepseek-reasoner,deepseek-chat&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://cloud.siliconflow.cn/i/foufCerk"&gt;Â§ßÊ®°ÂûãËÅöÂêàÂπ≥Âè∞&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;Â¶ÇÔºö&lt;a href="https://cloud.siliconflow.cn/i/foufCerk"&gt;Á°ÖÂü∫ÊµÅÂä®&lt;/a&gt;Ôºå&lt;a href="https://www.volcengine.com/experience/ark?utm_term=202502dsinvite&amp;amp;ac=DSASUQY5&amp;amp;rc=IJSE43PZ"&gt;ÁÅ´Â±±ÊñπËàü&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;&lt;span style="color: #568DF4;"&gt;ÂêÑ‰Ωç‰∫≤Áà±ÁöÑÊúãÂèã‰ª¨ÔºåÂ¶ÇÊûúÊÇ®ÂØπËøô‰∏™È°πÁõÆÊÑüÂÖ¥Ë∂£ÔºåËØ∑ÂÖàÁªôÊàë‰∏Ä‰∏™&lt;i style="color: #EA2626;"&gt;star&lt;/i&gt;ÂêßÔºåË∞¢Ë∞¢ÔºÅ&lt;/span&gt;üíï&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ÁÅ´Â±±ÊñπËàüÔºöÊñ∞Áî®Êà∑ÊØè‰∏™Ê®°ÂûãÊ≥®ÂÜåÂç≥ÈÄÅ50‰∏átokensÔºå&lt;a href="https://www.volcengine.com/experience/ark?utm_term=202502dsinvite&amp;amp;ac=DSASUQY5&amp;amp;rc=IJSE43PZ"&gt;Ê≥®ÂÜåÈìæÊé•&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Á°ÖÂü∫ÊµÅÂä®(siliconflow)ÔºåÊ≥®ÂÜåÂç≥ÈÄÅ2000‰∏áTokensÔºå&lt;a href="https://cloud.siliconflow.cn/i/foufCerk"&gt;Ê≥®ÂÜåÈìæÊé•&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;TushareÂ§ßÊï∞ÊçÆÂºÄÊîæÁ§æÂå∫,ÂÖçË¥πÊèê‰æõÂêÑÁ±ªÈáëËûçÊï∞ÊçÆ,Âä©ÂäõË°å‰∏öÂíåÈáèÂåñÁ†îÁ©∂(Ê≥®ÊÑèÔºöTushareÂè™ÈúÄË¶Å120ÁßØÂàÜÂç≥ÂèØÔºåÊ≥®ÂÜåÂÆåÊàê‰∏™‰∫∫ËµÑÊñôË°•ÂÖÖÂç≥ÂèØÂæó120ÁßØÂàÜÔºÅÔºÅÔºÅ)Ôºå&lt;a href="https://tushare.pro/register?reg=701944"&gt;Ê≥®ÂÜåÈìæÊé•&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ËΩØ‰ª∂Âø´ÈÄüËø≠‰ª£ÂºÄÂèë‰∏≠,ËØ∑Â§ßÂÆ∂‰ºòÂÖàÊµãËØïÂíå‰ΩøÁî®ÊúÄÊñ∞ÂèëÂ∏ÉÁöÑÁâàÊú¨„ÄÇ&lt;/li&gt; 
 &lt;li&gt;Ê¨¢ËøéÂ§ßÂÆ∂ÊèêÂá∫ÂÆùË¥µÁöÑÂª∫ËÆÆÔºåÊ¨¢ËøéÊèêissue,PR„ÄÇÂΩìÁÑ∂Êõ¥Ê¨¢Ëøé&lt;a href="https://raw.githubusercontent.com/ArvinLovegood/go-stock/dev/#%E9%83%BD%E5%88%92%E5%88%B0%E8%BF%99%E4%BA%86%E5%A6%82%E6%9E%9C%E6%88%91%E7%9A%84%E9%A1%B9%E7%9B%AE%E5%AF%B9%E6%82%A8%E6%9C%89%E5%B8%AE%E5%8A%A9%E8%AF%B7%E8%B5%9E%E5%8A%A9%E6%88%91%E5%90%A7"&gt;ËµûÂä©Êàë&lt;/a&gt;„ÄÇüíï&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ÊîØÊåÅÂºÄÊ∫êüíïËÆ°Âàí&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;ËµûÂä©ËÆ°Âàí&lt;/th&gt; 
   &lt;th&gt;ËµûÂä©Á≠âÁ∫ß&lt;/th&gt; 
   &lt;th align="left"&gt;ÊùÉÁõäËØ¥Êòé&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;ÊØèÊúà 0 RMB&lt;/td&gt; 
   &lt;td&gt;vip0&lt;/td&gt; 
   &lt;td align="left"&gt;üåü ÂÖ®ÈÉ®ÂäüËÉΩ,ËΩØ‰ª∂Ëá™Âä®Êõ¥Êñ∞(‰ªéGitHub‰∏ãËΩΩ),Ëá™Ë°åËß£ÂÜ≥githubÂπ≥Âè∞ÁΩëÁªúÈóÆÈ¢ò„ÄÇ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;ÊØèÊúàËµûÂä© 18.8 RMB&lt;br /&gt;ÊØèÂπ¥ËµûÂä© 120 RMB&lt;/td&gt; 
   &lt;td&gt;vip1&lt;/td&gt; 
   &lt;td align="left"&gt;üíï ÂÖ®ÈÉ®ÂäüËÉΩ,ËΩØ‰ª∂Ëá™Âä®Êõ¥Êñ∞(‰ªéCDN‰∏ãËΩΩ),Êõ¥Êñ∞Âø´ÈÄü‰æøÊç∑„ÄÇAIÈÖçÁΩÆÊåáÂØºÔºåÊèêÁ§∫ËØçÂèÇËÄÉÁ≠â&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;ÊØèÊúàËµûÂä© 28.8 RMB&lt;br /&gt;ÊØèÂπ¥ËµûÂä© 240 RMB&lt;/td&gt; 
   &lt;td&gt;vip2&lt;/td&gt; 
   &lt;td align="left"&gt;üíï üíï vip1ÂÖ®ÈÉ®ÂäüËÉΩ,Ëµ†ÈÄÅÁ°ÖÂü∫ÊµÅÂä®AIÂàÜÊûêÊúçÂä°,ÂêØÂä®Êó∂Ëá™Âä®ÂêåÊ≠•ÊúÄËøë24Â∞èÊó∂Â∏ÇÂú∫ËµÑËÆØ(ÂåÖÊã¨Â§ñÂ™íÁÆÄËÆØ)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;ÊØèÊúàËµûÂä© X RMB&lt;/td&gt; 
   &lt;td&gt;vipX&lt;/td&gt; 
   &lt;td align="left"&gt;üß© Êõ¥Â§öËÆ°ÂàíÔºåËßÜgo-stockÂºÄÊ∫êÈ°πÁõÆÂèëÂ±ïÊÉÖÂÜµËÄåÂÆö...(ÊâøÊé•GitHubÈ°πÁõÆREADMEÂπøÂëäÊé®Âπøüíñ)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;üß© ÈáçÂ§ßÂäüËÉΩÂºÄÂèëËÆ°Âàí&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;ÂäüËÉΩËØ¥Êòé&lt;/th&gt; 
   &lt;th&gt;Áä∂ÊÄÅ&lt;/th&gt; 
   &lt;th&gt;Â§áÊ≥®&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ËÇ°Á•®ÂàÜÊûêÁü•ËØÜÂ∫ì&lt;/td&gt; 
   &lt;td&gt;üöß&lt;/td&gt; 
   &lt;td&gt;Êú™Êù•ËÆ°Âàí&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;AiÊô∫ËÉΩÈÄâËÇ°&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;AiÊô∫ËÉΩÈÄâËÇ°ÂäüËÉΩ(Â∏ÇÂú∫Ë°åÊÉÖ-„ÄãAIÊÄªÁªì/AIÊô∫ËÉΩ‰ΩìÂäüËÉΩ)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ETFÊîØÊåÅ&lt;/td&gt; 
   &lt;td&gt;üöß&lt;/td&gt; 
   &lt;td&gt;ETFÊï∞ÊçÆÊîØÊåÅ (ÁõÆÂâçÂèØ‰ª•Êü•ÁúãÂáÄÂÄºÂíå‰º∞ÂÄº)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ÁæéËÇ°ÊîØÊåÅ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;ÁæéËÇ°Êï∞ÊçÆÊîØÊåÅ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ê∏ØËÇ°ÊîØÊåÅ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;Ê∏ØËÇ°Êï∞ÊçÆÊîØÊåÅ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Â§öËΩÆÂØπËØù&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;AIÂàÜÊûêÂêéÂèØÁªßÁª≠ÂØπËØùÊèêÈóÆ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ëá™ÂÆö‰πâAIÂàÜÊûêÊèêÈóÆÊ®°Êùø&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;ÂèØÈÖçÁΩÆÁöÑÊèêÈóÆÊ®°Êùø &lt;a href="https://github.com/ArvinLovegood/go-stock/releases/tag/v2025.2.12.7-alpha"&gt;v2025.2.12.7-alpha&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;‰∏çÂÜçÂº∫Âà∂‰æùËµñChromeÊµèËßàÂô®&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;ÈªòËÆ§‰ΩøÁî®edgeÊµèËßàÂô®ÊäìÂèñÊñ∞ÈóªËµÑËÆØ&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;üëÄ Êõ¥Êñ∞Êó•Âøó&lt;/h2&gt; 
&lt;h3&gt;2025.12.16 Êñ∞Â¢ûAIÊÄùËÄÉÊ®°Âºè‰∏éÁÉ≠Èó®ÈÄâËÇ°Á≠ñÁï•ÂäüËÉΩ&lt;/h3&gt; 
&lt;h3&gt;2025.11.21 Êñ∞Â¢ûÂ∏¶È¢ëÁéáÊùÉÈáçÁöÑÊÉÖÊÑüÂàÜÊûêÂäüËÉΩ&lt;/h3&gt; 
&lt;h3&gt;2025.10.30 Ê∑ªÂä†AIÊô∫ËÉΩ‰ΩìÂäüËÉΩÂºÄÂÖ≥(ÈªòËÆ§ÂÖ≥Èó≠ÔºåÂõ†‰∏∫‰ΩøÁî®‰ΩìÈ™å‰∏çÁêÜÊÉ≥)ÔºåÁßªÈô§È°µÈù¢Ê∞¥Âç∞&lt;/h3&gt; 
&lt;h3&gt;2025.09.27 Ê∑ªÂä†Êú∫ÊûÑ/Âà∏ÂïÜÁöÑÁ†îÁ©∂Êä•ÂëäAIÂ∑•ÂÖ∑ÂáΩÊï∞&lt;/h3&gt; 
&lt;h3&gt;2025.08.09 Ê∑ªÂä†AIÊô∫ËÉΩ‰ΩìËÅäÂ§©ÂäüËÉΩ&lt;/h3&gt; 
&lt;h3&gt;2025.07.08 ÂÆûÁé∞ËΩØ‰ª∂Ëá™Âä®Êõ¥Êñ∞ÂäüËÉΩ&lt;/h3&gt; 
&lt;h3&gt;2025.07.07 Âç°ÁâáÊ∑ªÂä†Ëø∑‰Ω†ÂàÜÊó∂Âõæ&lt;/h3&gt; 
&lt;h3&gt;2025.07.05 MacOsÊîØÊåÅ&lt;/h3&gt; 
&lt;h3&gt;2025.07.01 AIÂàÜÊûêÈõÜÊàêÂ∑•ÂÖ∑ÂáΩÊï∞ÔºåAIÂàÜÊûêÂ∞ÜÊõ¥Âä†Êô∫ËÉΩ&lt;/h3&gt; 
&lt;h3&gt;2025.06.30 Ê∑ªÂä†ÊåáÊ†áÈÄâËÇ°ÂäüËÉΩ&lt;/h3&gt; 
&lt;h3&gt;2025.06.27 Ê∑ªÂä†Ë¥¢ÁªèÊó•ÂéÜÂíåÈáçÂ§ß‰∫ã‰ª∂Êó∂Èó¥ËΩ¥ÂäüËÉΩ&lt;/h3&gt; 
&lt;h3&gt;2025.06.25 Ê∑ªÂä†ÁÉ≠Èó®ËÇ°Á•®„ÄÅ‰∫ã‰ª∂ÂíåËØùÈ¢òÂäüËÉΩ&lt;/h3&gt; 
&lt;h3&gt;2025.06.18 Êõ¥Êñ∞ÂÜÖÁΩÆËÇ°Á•®Âü∫Á°ÄÊï∞ÊçÆ,ËΩØ‰ª∂ÂÜÖÂÆûÊó∂Â∏ÇÂú∫ËµÑËÆØ‰ø°ÊÅØÊèêÈÜíÔºåÊ∑ªÂä†Ë°å‰∏öÁ†îÁ©∂ÂäüËÉΩ&lt;/h3&gt; 
&lt;h3&gt;2025.06.15 Ê∑ªÂä†ÂÖ¨Âè∏ÂÖ¨Âëä‰ø°ÊÅØÊêúÁ¥¢/Êü•ÁúãÂäüËÉΩ&lt;/h3&gt; 
&lt;h3&gt;2025.06.15 Ê∑ªÂä†‰∏™ËÇ°Á†îÊä•Âà∞ÂºπÂá∫ËèúÂçï&lt;/h3&gt; 
&lt;h3&gt;2025.06.13 Ê∑ªÂä†‰∏™ËÇ°Á†îÊä•ÂäüËÉΩ&lt;/h3&gt; 
&lt;h3&gt;2025.06.12 Ê∑ªÂä†ÈæôËôéÊ¶úÂäüËÉΩÔºåÊñ∞Â¢ûË°å‰∏öÊéíÂêçÂàÜÁ±ª&lt;/h3&gt; 
&lt;h3&gt;2025.05.30 ‰ºòÂåñËÇ°Á•®ÂàÜÊó∂ÂõæÊòæÁ§∫&lt;/h3&gt; 
&lt;h3&gt;2025.05.20 ‰øÆÂ§çË¥¢ËÅîÁ§æÁîµÊä•Ëé∑ÂèñÈóÆÈ¢ò&lt;/h3&gt; 
&lt;h3&gt;2025.05.16 ‰ºòÂåñËµÑÈáëË∂ãÂäøÂõæË°®ÁªÑ‰ª∂&lt;/h3&gt; 
&lt;h3&gt;2025.05.15 ÈáçÊûÑÂ∫îÁî®Âä†ËΩΩÂíåÊï∞ÊçÆÂàùÂßãÂåñÈÄªËæëÔºåÊ∑ªÂä†ËÇ°Á•®ËµÑÈáëË∂ãÂäøÂäüËÉΩÔºåËµÑÈáëË∂ãÂäøÂõæË°®Â¢ûÂä†‰∏ªÂäõÂΩìÊó•ÂáÄÊµÅÂÖ•Êï∞ÊçÆÂπ∂‰ºòÂåñÂ±ïÁ§∫ÊïàÊûú&lt;/h3&gt; 
&lt;h3&gt;2025.05.14 Ê∑ªÂä†‰∏™ËÇ°ËµÑÈáëÊµÅÂêëÂäüËÉΩÔºåÊéíË°åÊ¶úÂ¢ûÂä†ËÇ°Á•®Ë°åÊÉÖKÁ∫øÂõæÂºπÁ™ó&lt;/h3&gt; 
&lt;h3&gt;2025.05.13 Ê∑ªÂä†Ë°å‰∏öÊéíÂêçÂäüËÉΩ&lt;/h3&gt; 
&lt;h3&gt;2025.05.09 Ê∑ªÂä†AËÇ°ÁõòÂè£Êï∞ÊçÆËß£ÊûêÂíåÂ±ïÁ§∫ÂäüËÉΩ&lt;/h3&gt; 
&lt;h3&gt;2025.05.07 ‰ºòÂåñÂàÜÊó∂ÂõæÁöÑÂ±ïÁ§∫&lt;/h3&gt; 
&lt;h3&gt;2025.04.29 Ë°•ÂÖ®Ê∏ØËÇ°/ÁæéËÇ°Âü∫Á°ÄÊï∞ÊçÆÔºå‰ºòÂåñÊ∏ØËÇ°ËÇ°‰ª∑Âª∂ËøüÈóÆÈ¢òÔºå‰ºòÂåñÂàùÂßãÂåñÈÄªËæë&lt;/h3&gt; 
&lt;h3&gt;2025.04.25 Â∏ÇÂú∫ËµÑËÆØÊîØÊåÅAIÂàÜÊûêÂíåÊÄªÁªìÔºöËÆ©AIÂ∏Æ‰Ω†ËØªÂ∏ÇÂú∫ÔºÅ&lt;/h3&gt; 
&lt;h3&gt;2025.04.24 Êñ∞Â¢ûÂ∏ÇÂú∫Ë°åÊÉÖÊ®°ÂùóÔºöÂç≥Êó∂ÊéåÊè°ÂÖ®ÁêÉÂ∏ÇÂú∫Ë°åÊÉÖËµÑËÆØ/Âä®ÊÄÅÔºå‰ªéÊ≠§ÂÜç‰πü‰∏çÁî®ÂÅ∑Êë∏ÂéªÂêÑÂ§ßË¥¢ÁªèÁΩëÁ´ôÂï¶„ÄÇgo-stock‰∏ÄÈîÆÂ∏Æ‰Ω†ÊêûÂÆöÔºÅ&lt;/h3&gt; 
&lt;h3&gt;2025.04.22 ‰ºòÂåñKÁ∫øÂõæÂ±ïÁ§∫ÔºåÊîØÊåÅÊãâ‰º∏ÊîæÂ§ßÔºåÁúãÂæóÊõ¥ËàíÊúçÂï¶ÔºÅ&lt;/h3&gt; 
&lt;h3&gt;2025.04.21 Ê∏ØËÇ°ÔºåÁæéËÇ°KÁ∫øÊï∞ÊçÆËé∑Âèñ‰ºòÂåñ&lt;/h3&gt; 
&lt;h3&gt;2025.04.01 ‰ºòÂåñÈÉ®ÂàÜËÆæÁΩÆÈÄâÈ°πÔºåÈÅøÂÖçÈáçÂêØËΩØ‰ª∂&lt;/h3&gt; 
&lt;h3&gt;2025.03.31 ‰ºòÂåñÊï∞ÊçÆÁà¨Âèñ&lt;/h3&gt; 
&lt;h3&gt;2025.03.30 AIËá™Âä®ÂÆöÊó∂ÂàÜÊûêÂäüËÉΩ&lt;/h3&gt; 
&lt;h3&gt;2025.03.29 Â§öÊèêÁ§∫ËØçÊ®°ÊùøÁÆ°ÁêÜÔºåAIÂàÜÊûêÊó∂ÊîØÊåÅÈÄâÊã©‰∏çÂêåÊèêÁ§∫ËØçÊ®°Êùø&lt;/h3&gt; 
&lt;h3&gt;2025.03.28 AIÂàÜÊûêÁªìÊûú‰øùÂ≠ò‰∏∫markdownÊñá‰ª∂Êó∂ÔºåÊîØÊåÅ‰øùÂ≠ò‰ΩçÁΩÆÁõÆÂΩïÈÄâÊã©&lt;/h3&gt; 
&lt;h3&gt;2025.03.15 Ëá™ÂÆö‰πâÁà¨Ëô´‰ΩøÁî®ÁöÑÊµèËßàÂô®Ë∑ØÂæÑÈÖçÁΩÆ&lt;/h3&gt; 
&lt;h3&gt;2025.03.14 ‰ºòÂåñÁºñËØëÊûÑÂª∫ÔºåÂ§ßÂπÖÂáèÂ∞ëÁºñËØëÂêéÁöÑÁ®ãÂ∫èÊñá‰ª∂Â§ßÂ∞è&lt;/h3&gt; 
&lt;h3&gt;2025.03.09 Âü∫Èáë‰º∞ÂÄºÂíåÂáÄÂÄºÁõëÊéßÊü•Áúã&lt;/h3&gt; 
&lt;h3&gt;2025.03.06 È°πÁõÆÁ§æÂå∫ÂàÜ‰∫´ÂäüËÉΩ&lt;/h3&gt; 
&lt;h3&gt;2025.02.28 ÁæéËÇ°Êï∞ÊçÆÊîØÊåÅ&lt;/h3&gt; 
&lt;h3&gt;2025.02.23 ÂºπÂπïÂäüËÉΩÔºåÁõØÁõò‰∏çÂÜçÂ≠§ÂçïÔºåÊó†ËÅäÂàí‰∏™Ê∞¥ÔºÅüòé&lt;/h3&gt; 
&lt;h3&gt;2025.02.22 Ê∏ØËÇ°Êï∞ÊçÆÊîØÊåÅ(ÁõÆÂâçÊúâÂª∂Ëøü)&lt;/h3&gt; 
&lt;h3&gt;2025.02.16 AIÂàÜÊûêÂêéÂèØÁªßÁª≠ÂØπËØùÊèêÈóÆ&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ArvinLovegood/go-stock/releases/tag/v2025.2.16.1-alpha"&gt;v2025.2.16.1-alpha&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;2025.02.12 ÂèØÈÖçÁΩÆÁöÑÊèêÈóÆÊ®°Êùø&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ArvinLovegood/go-stock/releases/tag/v2025.2.12.7-alpha"&gt;v2025.2.12.7-alpha&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ü¶Ñ ÈáçÂ§ßÊõ¥Êñ∞&lt;/h2&gt; 
&lt;h3&gt;BIG NEWS !!! ÈáçÂ§ßÊõ¥Êñ∞ÔºÅÔºÅÔºÅ&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;2025.11.21 Êñ∞Â¢ûÂ∏¶È¢ëÁéáÊùÉÈáçÁöÑÊÉÖÊÑüÂàÜÊûêÂäüËÉΩ &lt;img src="https://raw.githubusercontent.com/ArvinLovegood/go-stock/dev/build/screenshot/img15.png" alt="img_1.png" /&gt;&lt;/li&gt; 
 &lt;li&gt;2025.04.25 Â∏ÇÂú∫ËµÑËÆØÊîØÊåÅAIÂàÜÊûêÂíåÊÄªÁªìÔºöËÆ©AIÂ∏Æ‰Ω†ËØªÂ∏ÇÂú∫ÔºÅ &lt;img src="https://raw.githubusercontent.com/ArvinLovegood/go-stock/dev/img.png" alt="img.png" /&gt;&lt;/li&gt; 
 &lt;li&gt;2025.04.24 Êñ∞Â¢ûÂ∏ÇÂú∫Ë°åÊÉÖÊ®°ÂùóÔºöÂç≥Êó∂ÊéåÊè°ÂÖ®ÁêÉÂ∏ÇÂú∫Ë°åÊÉÖËµÑËÆØ/Âä®ÊÄÅÔºå‰ªéÊ≠§ÂÜç‰πü‰∏çÁî®ÂÅ∑Êë∏ÂéªÂêÑÂ§ßË¥¢ÁªèÁΩëÁ´ôÂï¶„ÄÇgo-stock‰∏ÄÈîÆÂ∏Æ‰Ω†ÊêûÂÆöÔºÅ &lt;img src="https://raw.githubusercontent.com/ArvinLovegood/go-stock/dev/build/screenshot/img13.png" alt="img.png" /&gt; &lt;img src="https://raw.githubusercontent.com/ArvinLovegood/go-stock/dev/build/screenshot/img_13.png" alt="img_13.png" /&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;img src="https://raw.githubusercontent.com/ArvinLovegood/go-stock/dev/build/screenshot/img_14.png" alt="img_14.png" /&gt;&lt;/li&gt; 
 &lt;li&gt;2025.01.17 Êñ∞Â¢ûAIÂ§ßÊ®°ÂûãÂàÜÊûêËÇ°Á•®ÂäüËÉΩ &lt;img src="https://raw.githubusercontent.com/ArvinLovegood/go-stock/dev/build/screenshot/img.png" alt="img_5.png" /&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üì∏ ÂäüËÉΩÊà™Âõæ&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/ArvinLovegood/go-stock/dev/build/screenshot/img_6.png" alt="img_1.png" /&gt;&lt;/p&gt; 
&lt;h3&gt;ËÆæÁΩÆ&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/ArvinLovegood/go-stock/dev/build/screenshot/img_4.png" alt="img_12.png" /&gt;&lt;/p&gt; 
&lt;h3&gt;ÊàêÊú¨ËÆæÁΩÆ&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/ArvinLovegood/go-stock/dev/build/screenshot/img_7.png" alt="img.png" /&gt;&lt;/p&gt; 
&lt;h3&gt;Êó•K&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/ArvinLovegood/go-stock/dev/build/screenshot/img_12.png" alt="img_12.png" /&gt;&lt;/p&gt; 
&lt;h3&gt;ÂàÜÊó∂&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/ArvinLovegood/go-stock/dev/build/screenshot/img_9.png" alt="img_3.png" /&gt;&lt;/p&gt; 
&lt;h3&gt;ÈíâÈíâÊä•Ë≠¶ÈÄöÁü•&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/ArvinLovegood/go-stock/dev/build/screenshot/img_5.png" alt="img_4.png" /&gt;&lt;/p&gt; 
&lt;h3&gt;AIÂàÜÊûêËÇ°Á•®&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/ArvinLovegood/go-stock/dev/build/screenshot/img.png" alt="img_5.png" /&gt;&lt;/p&gt; 
&lt;h3&gt;ÁâàÊú¨‰ø°ÊÅØÊèêÁ§∫&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/ArvinLovegood/go-stock/dev/build/screenshot/img_11.png" alt="img_11.png" /&gt;&lt;/p&gt; 
&lt;h2&gt;üíï ÊÑüË∞¢‰ª•‰∏ãÈ°πÁõÆ&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.naiveui.com/"&gt;NaiveUI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://wails.io/"&gt;Wails&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://vuejs.org/"&gt;Vue&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://vitejs.dev/"&gt;Vite&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://tushare.pro/register?reg=701944"&gt;Tushare&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üòò ËµûÂä©Êàë&lt;/h2&gt; 
&lt;h3&gt;ÈÉΩÂàíÂà∞Ëøô‰∫ÜÔºåÂ¶ÇÊûúÊàëÁöÑÈ°πÁõÆÂØπÊÇ®ÊúâÂ∏ÆÂä©ÔºåËØ∑ËµûÂä©ÊàëÂêßÔºÅüòäüòäüòä&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;ÊîØ‰ªòÂÆù&lt;/th&gt; 
   &lt;th&gt;ÂæÆ‰ø°&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/ArvinLovegood/go-stock/dev/build/screenshot/alipay.jpg" alt="alipay.jpg" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/ArvinLovegood/go-stock/dev/build/screenshot/wxpay.jpg" alt="wxpay.jpg" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;‚≠ê Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#ArvinLovegood/go-stock&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=ArvinLovegood/go-stock&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;ü§ñ Áä∂ÊÄÅ&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://repobeats.axiom.co/api/embed/40b07d415a42c2264a18c4fe1b6f182ff1470687.svg?sanitize=true" alt="Alt" title="Repobeats analytics image" /&gt;&lt;/p&gt; 
&lt;h2&gt;üê≥ ÂÖ≥‰∫éÊäÄÊúØÊîØÊåÅÁî≥Êòé&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Êú¨ËΩØ‰ª∂Âü∫‰∫éÂºÄÊ∫êÊäÄÊúØÊûÑÂª∫Ôºå‰ΩøÁî®Wails„ÄÅNaiveUI„ÄÅVue„ÄÅAIÂ§ßÊ®°ÂûãÁ≠âÂºÄÊ∫êÈ°πÁõÆ„ÄÇ ÊäÄÊúØ‰∏äÂ¶ÇÊúâÈóÆÈ¢òÔºåÂèØ‰ª•ÂÖàÂêëÂØπÂ∫îÁöÑÂºÄÊ∫êÁ§æÂå∫ËØ∑Ê±ÇÂ∏ÆÂä©„ÄÇ&lt;/li&gt; 
 &lt;li&gt;ÂºÄÊ∫ê‰∏çÊòìÔºåÊú¨‰∫∫Á≤æÂäõÂíåÊó∂Èó¥ÊúâÈôêÔºåÂ¶ÇÈúÄ‰∏ÄÂØπ‰∏ÄÊäÄÊúØÊîØÊåÅÔºåËØ∑ÂÖàËµûÂä©„ÄÇËÅîÁ≥ªQQ(Â§áÊ≥® ÊäÄÊúØÊîØÊåÅ)Ôºö506808970&lt;/li&gt; 
&lt;/ul&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;ÊäÄÊúØÊîØÊåÅÊñπÂºè&lt;/th&gt; 
   &lt;th align="center"&gt;ËµûÂä©(ÂÖÉ)&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;Âä† QQÔºö506808970&lt;/td&gt; 
   &lt;td align="center"&gt;100/Ê¨°&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;ÈïøÊúüÊäÄÊúØÊîØÊåÅÔºà‰∏çÈôêÊ¨°Êï∞ÔºåÊñ∞ÂäüËÉΩ‰ºòÂÖà‰ΩìÈ™åÁ≠âÔºâ&lt;/td&gt; 
   &lt;td align="center"&gt;5000&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/ArvinLovegood/go-stock/dev/LICENSE"&gt;Apache License 2.0&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>moby/buildkit</title>
      <link>https://github.com/moby/buildkit</link>
      <description>&lt;p&gt;concurrent, cache-efficient, and Dockerfile-agnostic builder toolkit&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href="https://asciinema.org/a/gPEIEo1NzmDTUu2bEPsUboqmU"&gt;&lt;img src="https://asciinema.org/a/gPEIEo1NzmDTUu2bEPsUboqmU.png" alt="asciicinema example" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;BuildKit 
 &lt;!-- omit in toc --&gt;&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/moby/buildkit/releases/latest"&gt;&lt;img src="https://img.shields.io/github/release/moby/buildkit.svg?style=flat-square" alt="GitHub Release" /&gt;&lt;/a&gt; &lt;a href="https://pkg.go.dev/github.com/moby/buildkit/client/llb"&gt;&lt;img src="https://img.shields.io/badge/go.dev-docs-007d9c?style=flat-square&amp;amp;logo=go&amp;amp;logoColor=white" alt="PkgGoDev" /&gt;&lt;/a&gt; &lt;a href="https://github.com/moby/buildkit/actions?query=workflow%3Abuildkit"&gt;&lt;img src="https://img.shields.io/github/actions/workflow/status/moby/buildkit/buildkit.yml?label=buildkit&amp;amp;logo=github&amp;amp;style=flat-square" alt="CI BuildKit Status" /&gt;&lt;/a&gt; &lt;a href="https://github.com/moby/buildkit/actions?query=workflow%3Afrontend"&gt;&lt;img src="https://img.shields.io/github/actions/workflow/status/moby/buildkit/frontend.yml?label=frontend&amp;amp;logo=github&amp;amp;style=flat-square" alt="CI Frontend Status" /&gt;&lt;/a&gt; &lt;a href="https://goreportcard.com/report/github.com/moby/buildkit"&gt;&lt;img src="https://goreportcard.com/badge/github.com/moby/buildkit?style=flat-square" alt="Go Report Card" /&gt;&lt;/a&gt; &lt;a href="https://codecov.io/gh/moby/buildkit"&gt;&lt;img src="https://img.shields.io/codecov/c/github/moby/buildkit?logo=codecov&amp;amp;style=flat-square" alt="Codecov" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;BuildKit is a toolkit for converting source code to build artifacts in an efficient, expressive and repeatable manner.&lt;/p&gt; 
&lt;p&gt;Key features:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Automatic garbage collection&lt;/li&gt; 
 &lt;li&gt;Extendable frontend formats&lt;/li&gt; 
 &lt;li&gt;Concurrent dependency resolution&lt;/li&gt; 
 &lt;li&gt;Efficient instruction caching&lt;/li&gt; 
 &lt;li&gt;Build cache import/export&lt;/li&gt; 
 &lt;li&gt;Nested build job invocations&lt;/li&gt; 
 &lt;li&gt;Distributable workers&lt;/li&gt; 
 &lt;li&gt;Multiple output formats&lt;/li&gt; 
 &lt;li&gt;Pluggable architecture&lt;/li&gt; 
 &lt;li&gt;Execution without root privileges&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Read the proposal from &lt;a href="https://github.com/moby/moby/issues/32925"&gt;https://github.com/moby/moby/issues/32925&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Introductory blog post &lt;a href="https://blog.mobyproject.org/introducing-buildkit-17e056cc5317"&gt;https://blog.mobyproject.org/introducing-buildkit-17e056cc5317&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Join &lt;code&gt;#buildkit&lt;/code&gt; channel on &lt;a href="https://dockr.ly/comm-slack"&gt;Docker Community Slack&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] If you are visiting this repo for the usage of BuildKit-only Dockerfile features like &lt;code&gt;RUN --mount=type=(bind|cache|tmpfs|secret|ssh)&lt;/code&gt;, please refer to the &lt;a href="https://docs.docker.com/engine/reference/builder/"&gt;Dockerfile reference&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] &lt;code&gt;docker build&lt;/code&gt; &lt;a href="https://docs.docker.com/build/architecture/"&gt;uses Buildx and BuildKit by default&lt;/a&gt; since Docker Engine 23.0. You don't need to read this document unless you want to use the full-featured standalone version of BuildKit.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;!-- START doctoc generated TOC please keep comment here to allow auto update --&gt; 
&lt;!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE --&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/#used-by"&gt;Used by&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/#quick-start"&gt;Quick start&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/#linux-setup"&gt;Linux Setup&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/#windows-setup"&gt;Windows Setup&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/#macos-setup"&gt;macOS Setup&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/#build-from-source"&gt;Build from source&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/#exploring-llb"&gt;Exploring LLB&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/#exploring-dockerfiles"&gt;Exploring Dockerfiles&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/#building-a-dockerfile-with-buildctl"&gt;Building a Dockerfile with &lt;code&gt;buildctl&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/#building-a-dockerfile-using-external-frontend"&gt;Building a Dockerfile using external frontend&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/#output"&gt;Output&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/#imageregistry"&gt;Image/Registry&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/#local-directory"&gt;Local directory&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/#docker-tarball"&gt;Docker tarball&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/#oci-tarball"&gt;OCI tarball&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/#containerd-image-store"&gt;containerd image store&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/#cache"&gt;Cache&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/#garbage-collection"&gt;Garbage collection&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/#export-cache"&gt;Export cache&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/#inline-push-image-and-cache-together"&gt;Inline (push image and cache together)&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/#registry-push-image-and-cache-separately"&gt;Registry (push image and cache separately)&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/#local-directory-1"&gt;Local directory&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/#github-actions-cache-experimental"&gt;GitHub Actions cache (experimental)&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/#s3-cache-experimental"&gt;S3 cache (experimental)&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/#azure-blob-storage-cache-experimental"&gt;Azure Blob Storage cache (experimental)&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/#consistent-hashing"&gt;Consistent hashing&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/#metadata"&gt;Metadata&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/#systemd-socket-activation"&gt;Systemd socket activation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/#expose-buildkit-as-a-tcp-service"&gt;Expose BuildKit as a TCP service&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/#load-balancing"&gt;Load balancing&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/#containerizing-buildkit"&gt;Containerizing BuildKit&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/#podman"&gt;Podman&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/#nerdctl"&gt;Nerdctl&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/#kubernetes"&gt;Kubernetes&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/#daemonless"&gt;Daemonless&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/#opentelemetry-support"&gt;OpenTelemetry support&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/#running-buildkit-without-root-privileges"&gt;Running BuildKit without root privileges&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/#building-multi-platform-images"&gt;Building multi-platform images&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/#configuring-buildctl"&gt;Configuring &lt;code&gt;buildctl&lt;/code&gt;&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/#color-output-controls"&gt;Color Output Controls&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/#number-of-log-lines-for-active-steps-in-tty-mode"&gt;Number of log lines (for active steps in tty mode)&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/#contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- END doctoc generated TOC please keep comment here to allow auto update --&gt; 
&lt;h2&gt;Used by&lt;/h2&gt; 
&lt;p&gt;BuildKit is used by the following projects:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/moby/moby/pull/37151"&gt;Moby &amp;amp; Docker&lt;/a&gt; (&lt;code&gt;DOCKER_BUILDKIT=1 docker build&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/genuinetools/img"&gt;img&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/openfaas/openfaas-cloud"&gt;OpenFaaS Cloud&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/containerbuilding/cbi"&gt;container build interface&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/tektoncd/catalog"&gt;Tekton Pipelines&lt;/a&gt; (formerly &lt;a href="https://github.com/knative/build-templates"&gt;Knative Build Templates&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/distributed-containers-inc/sanic"&gt;the Sanic build tool&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/stellarproject/vab"&gt;vab&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/rancher/rio"&gt;Rio&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/rancher/kim"&gt;kim&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/alibaba/pouch"&gt;PouchContainer&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/docker/buildx"&gt;Docker buildx&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://okteto.com/"&gt;Okteto Cloud&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/vladaionescu/earthly"&gt;Earthly earthfiles&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/gitpod-io/gitpod"&gt;Gitpod&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://dagger.io"&gt;Dagger&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/tensorchord/envd/"&gt;envd&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://depot.dev"&gt;Depot&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://namespace.so"&gt;Namespace&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://unikraft.org"&gt;Unikraft&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://devzero.io"&gt;DevZero&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/r2d4/dacc"&gt;dacc&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Quick start&lt;/h2&gt; 
&lt;p&gt;&lt;span&gt;‚Ñπ&lt;/span&gt; For Kubernetes deployments, see &lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/examples/kubernetes"&gt;&lt;code&gt;examples/kubernetes&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;BuildKit is composed of the &lt;code&gt;buildkitd&lt;/code&gt; daemon and the &lt;code&gt;buildctl&lt;/code&gt; client. While the &lt;code&gt;buildctl&lt;/code&gt; client is available for Linux, macOS, and Windows, the &lt;code&gt;buildkitd&lt;/code&gt; daemon is only available for Linux and *Windows currently.&lt;/p&gt; 
&lt;p&gt;The latest binaries of BuildKit are available &lt;a href="https://github.com/moby/buildkit/releases"&gt;here&lt;/a&gt; for Linux, macOS, and Windows.&lt;/p&gt; 
&lt;h3&gt;Linux Setup&lt;/h3&gt; 
&lt;p&gt;The &lt;code&gt;buildkitd&lt;/code&gt; daemon requires the following components to be installed:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/opencontainers/runc"&gt;runc&lt;/a&gt; or &lt;a href="https://github.com/containers/crun"&gt;crun&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/containerd/containerd"&gt;containerd&lt;/a&gt; (if you want to use containerd worker)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Starting the &lt;code&gt;buildkitd&lt;/code&gt; daemon:&lt;/strong&gt; You need to run &lt;code&gt;buildkitd&lt;/code&gt; as the root user on the host.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;$ sudo buildkitd
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To run &lt;code&gt;buildkitd&lt;/code&gt; as a non-root user, see &lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/docs/rootless.md"&gt;&lt;code&gt;docs/rootless.md&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The buildkitd daemon supports two worker backends: OCI (runc) and containerd.&lt;/p&gt; 
&lt;p&gt;By default, the OCI (runc) worker is used. You can set &lt;code&gt;--oci-worker=false --containerd-worker=true&lt;/code&gt; to use the containerd worker.&lt;/p&gt; 
&lt;p&gt;We are open to adding more backends.&lt;/p&gt; 
&lt;p&gt;To start the buildkitd daemon using systemd socket activation, you can install the buildkit systemd unit files. See &lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/#systemd-socket-activation"&gt;Systemd socket activation&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;The buildkitd daemon listens gRPC API on &lt;code&gt;/run/buildkit/buildkitd.sock&lt;/code&gt; by default, but you can also use TCP sockets. See &lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/#expose-buildkit-as-a-tcp-service"&gt;Expose BuildKit as a TCP service&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Windows Setup&lt;/h3&gt; 
&lt;p&gt;See instructions and notes at &lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/docs/windows.md"&gt;&lt;code&gt;docs/windows.md&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;macOS Setup&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://formulae.brew.sh/formula/buildkit"&gt;Homebrew formula&lt;/a&gt; (unofficial) is available for macOS.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;$ brew install buildkit
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The Homebrew formula does not contain the daemon (&lt;code&gt;buildkitd&lt;/code&gt;).&lt;/p&gt; 
&lt;p&gt;For example, &lt;a href="https://lima-vm.io"&gt;Lima&lt;/a&gt; can be used for launching the daemon inside a Linux VM.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;brew install lima
limactl start template://buildkit
export BUILDKIT_HOST="unix://$HOME/.lima/buildkit/sock/buildkitd.sock"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Build from source&lt;/h3&gt; 
&lt;p&gt;To build BuildKit from source, see &lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/.github/CONTRIBUTING.md"&gt;&lt;code&gt;.github/CONTRIBUTING.md&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For a &lt;code&gt;buildctl&lt;/code&gt; reference, see &lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/docs/reference/buildctl.md"&gt;this document&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Exploring LLB&lt;/h3&gt; 
&lt;p&gt;BuildKit builds are based on a binary intermediate format called LLB that is used for defining the dependency graph for processes running part of your build. tl;dr: LLB is to Dockerfile what LLVM IR is to C.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Marshaled as Protobuf messages&lt;/li&gt; 
 &lt;li&gt;Concurrently executable&lt;/li&gt; 
 &lt;li&gt;Efficiently cacheable&lt;/li&gt; 
 &lt;li&gt;Vendor-neutral (i.e. non-Dockerfile languages can be easily implemented)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/solver/pb/ops.proto"&gt;&lt;code&gt;solver/pb/ops.proto&lt;/code&gt;&lt;/a&gt; for the format definition, and see &lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/examples/README.md"&gt;&lt;code&gt;./examples/README.md&lt;/code&gt;&lt;/a&gt; for example LLB applications.&lt;/p&gt; 
&lt;p&gt;Currently, the following high-level languages have been implemented for LLB:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Dockerfile (See &lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/#exploring-dockerfiles"&gt;Exploring Dockerfiles&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/tonistiigi/buildkit-pack"&gt;Buildpacks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://matt-rickard.com/building-a-new-dockerfile-frontend/"&gt;Mockerfile&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/po3rin/gockerfile"&gt;Gockerfile&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/talos-systems/bldr/"&gt;bldr (Pkgfile)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/openllb/hlb"&gt;HLB&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/earthly/earthly"&gt;Earthfile (Earthly)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/denzp/cargo-wharf"&gt;Cargo Wharf (Rust)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/reproducible-containers/buildkit-nix"&gt;Nix&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/cmdjulian/mopy"&gt;mopy (Python)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/tensorchord/envd/"&gt;envd (starlark)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://gitlab.wikimedia.org/repos/releng/blubber"&gt;Blubber&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/vito/bass"&gt;Bass&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/unikraft/kraftkit/tree/staging/tools/dockerfile-llb-frontend"&gt;kraft.yaml (Unikraft)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/r2d4/llb"&gt;r2d4/llb (JSON Gateway)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/marxarelli/masse"&gt;Mass√©&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/project-dalec/dalec"&gt;DALEC&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;(open a PR to add your own language)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Exploring Dockerfiles&lt;/h3&gt; 
&lt;p&gt;Frontends are components that run inside BuildKit and convert any build definition to LLB. There is a special frontend called gateway (&lt;code&gt;gateway.v0&lt;/code&gt;) that allows using any image as a frontend.&lt;/p&gt; 
&lt;p&gt;During development, Dockerfile frontend (&lt;code&gt;dockerfile.v0&lt;/code&gt;) is also part of the BuildKit repo. In the future, this will be moved out, and Dockerfiles can be built using an external image.&lt;/p&gt; 
&lt;h4&gt;Building a Dockerfile with &lt;code&gt;buildctl&lt;/code&gt;&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;buildctl build \
    --frontend=dockerfile.v0 \
    --local context=. \
    --local dockerfile=.
# or
buildctl build \
    --frontend=dockerfile.v0 \
    --local context=. \
    --local dockerfile=. \
    --opt target=foo \
    --opt build-arg:foo=bar
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;code&gt;--local&lt;/code&gt; exposes local source files from client to the builder. &lt;code&gt;context&lt;/code&gt; and &lt;code&gt;dockerfile&lt;/code&gt; are the names Dockerfile frontend looks for build context and Dockerfile location.&lt;/p&gt; 
&lt;p&gt;If the Dockerfile has a different filename it can be specified with &lt;code&gt;--opt filename=./Dockerfile-alternative&lt;/code&gt;.&lt;/p&gt; 
&lt;h4&gt;Building a Dockerfile using external frontend&lt;/h4&gt; 
&lt;p&gt;External versions of the Dockerfile frontend are pushed to &lt;a href="https://hub.docker.com/r/docker/dockerfile-upstream"&gt;https://hub.docker.com/r/docker/dockerfile-upstream&lt;/a&gt; and &lt;a href="https://hub.docker.com/r/docker/dockerfile"&gt;https://hub.docker.com/r/docker/dockerfile&lt;/a&gt; and can be used with the gateway frontend. The source for the external frontend is currently located in &lt;code&gt;./frontend/dockerfile/cmd/dockerfile-frontend&lt;/code&gt; but will move out of this repository in the future (&lt;a href="https://github.com/moby/buildkit/issues/163"&gt;#163&lt;/a&gt;). For automatic build from master branch of this repository &lt;code&gt;docker/dockerfile-upstream:master&lt;/code&gt; or &lt;code&gt;docker/dockerfile-upstream:master-labs&lt;/code&gt; image can be used.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;buildctl build \
    --frontend gateway.v0 \
    --opt source=docker/dockerfile \
    --local context=. \
    --local dockerfile=.
buildctl build \
    --frontend gateway.v0 \
    --opt source=docker/dockerfile \
    --opt context=https://github.com/moby/moby.git \
    --opt build-arg:APT_MIRROR=cdn-fastly.deb.debian.org
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Output&lt;/h3&gt; 
&lt;p&gt;By default, the build result and intermediate cache will only remain internally in BuildKit. An output needs to be specified to retrieve the result.&lt;/p&gt; 
&lt;h4&gt;Image/Registry&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;buildctl build ... --output type=image,name=docker.io/username/image,push=true
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To export the image to multiple registries:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;buildctl build ... --output type=image,\"name=docker.io/username/image,docker.io/username2/image2\",push=true
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To export the cache embed with the image and pushing them to registry together, type &lt;code&gt;registry&lt;/code&gt; is required to import the cache, you should specify &lt;code&gt;--export-cache type=inline&lt;/code&gt; and &lt;code&gt;--import-cache type=registry,ref=...&lt;/code&gt;. To export the cache to a local directly, you should specify &lt;code&gt;--export-cache type=local&lt;/code&gt;. Details in &lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/#export-cache"&gt;Export cache&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;buildctl build ...\
  --output type=image,name=docker.io/username/image,push=true \
  --export-cache type=inline \
  --import-cache type=registry,ref=docker.io/username/image
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Keys supported by image output:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;name=&amp;lt;value&amp;gt;&lt;/code&gt;: specify image name(s)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;push=true&lt;/code&gt;: push after creating the image&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;push-by-digest=true&lt;/code&gt;: push unnamed image&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;registry.insecure=true&lt;/code&gt;: push to insecure HTTP registry&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;oci-mediatypes=true&lt;/code&gt;: use OCI mediatypes in configuration JSON instead of Docker's&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;oci-artifact=false&lt;/code&gt;: use OCI artifact format for attestations&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;unpack=true&lt;/code&gt;: unpack image after creation (for use with containerd)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;dangling-name-prefix=&amp;lt;value&amp;gt;&lt;/code&gt;: name image with &lt;code&gt;prefix@&amp;lt;digest&amp;gt;&lt;/code&gt;, used for anonymous images&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;name-canonical=true&lt;/code&gt;: add additional canonical name &lt;code&gt;name@&amp;lt;digest&amp;gt;&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;compression=&amp;lt;uncompressed|gzip|estargz|zstd&amp;gt;&lt;/code&gt;: choose compression type for layers newly created and cached, gzip is default value. estargz should be used with &lt;code&gt;oci-mediatypes=true&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;compression-level=&amp;lt;value&amp;gt;&lt;/code&gt;: compression level for gzip, estargz (0-9) and zstd (0-22)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;rewrite-timestamp=true&lt;/code&gt;: rewrite the file timestamps to the &lt;code&gt;SOURCE_DATE_EPOCH&lt;/code&gt; value. See &lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/docs/build-repro.md"&gt;&lt;code&gt;docs/build-repro.md&lt;/code&gt;&lt;/a&gt; for how to specify the &lt;code&gt;SOURCE_DATE_EPOCH&lt;/code&gt; value.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;force-compression=true&lt;/code&gt;: forcefully apply &lt;code&gt;compression&lt;/code&gt; option to all layers (including already existing layers)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;store=true&lt;/code&gt;: store the result images to the worker's (e.g. containerd) image store as well as ensures that the image has all blobs in the content store (default &lt;code&gt;true&lt;/code&gt;). Ignored if the worker doesn't have image store (e.g. OCI worker).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;annotation.&amp;lt;key&amp;gt;=&amp;lt;value&amp;gt;&lt;/code&gt;: attach an annotation with the respective &lt;code&gt;key&lt;/code&gt; and &lt;code&gt;value&lt;/code&gt; to the built image 
  &lt;ul&gt; 
   &lt;li&gt;Using the extended syntaxes, &lt;code&gt;annotation-&amp;lt;type&amp;gt;.&amp;lt;key&amp;gt;=&amp;lt;value&amp;gt;&lt;/code&gt;, &lt;code&gt;annotation[&amp;lt;platform&amp;gt;].&amp;lt;key&amp;gt;=&amp;lt;value&amp;gt;&lt;/code&gt; and both combined with &lt;code&gt;annotation-&amp;lt;type&amp;gt;[&amp;lt;platform&amp;gt;].&amp;lt;key&amp;gt;=&amp;lt;value&amp;gt;&lt;/code&gt;, allows configuring exactly where to attach the annotation.&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;&amp;lt;type&amp;gt;&lt;/code&gt; specifies what object to attach to, and can be any of &lt;code&gt;manifest&lt;/code&gt; (the default), &lt;code&gt;manifest-descriptor&lt;/code&gt;, &lt;code&gt;index&lt;/code&gt; and &lt;code&gt;index-descriptor&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;&amp;lt;platform&amp;gt;&lt;/code&gt; specifies which objects to attach to (by default, all), and is the same key passed into the &lt;code&gt;platform&lt;/code&gt; opt, see &lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/docs/multi-platform.md"&gt;&lt;code&gt;docs/multi-platform.md&lt;/code&gt;&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;See &lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/docs/annotations.md"&gt;&lt;code&gt;docs/annotations.md&lt;/code&gt;&lt;/a&gt; for more details.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If credentials are required, &lt;code&gt;buildctl&lt;/code&gt; will attempt to read Docker configuration file &lt;code&gt;$DOCKER_CONFIG/config.json&lt;/code&gt;. &lt;code&gt;$DOCKER_CONFIG&lt;/code&gt; defaults to &lt;code&gt;~/.docker&lt;/code&gt;.&lt;/p&gt; 
&lt;h4&gt;Local directory&lt;/h4&gt; 
&lt;p&gt;The local client will copy the files directly to the client. This is useful if BuildKit is being used for building something else than container images.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;buildctl build ... --output type=local,dest=path/to/output-dir
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To export specific files use multi-stage builds with a scratch stage and copy the needed files into that stage with &lt;code&gt;COPY --from&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-dockerfile"&gt;...
FROM scratch as testresult

COPY --from=builder /usr/src/app/testresult.xml .
...
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;buildctl build ... --opt target=testresult --output type=local,dest=path/to/output-dir
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;With a &lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/docs/multi-platform.md"&gt;multi-platform build&lt;/a&gt;, a subfolder matching each target platform will be created in the destination directory:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-dockerfile"&gt;FROM busybox AS build
ARG TARGETOS
ARG TARGETARCH
RUN mkdir /out &amp;amp;&amp;amp; echo foo &amp;gt; /out/hello-$TARGETOS-$TARGETARCH

FROM scratch
COPY --from=build /out /
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;$ buildctl build \
  --frontend dockerfile.v0 \
  --opt platform=linux/amd64,linux/arm64 \
  --output type=local,dest=./bin/release

$ tree ./bin
./bin/
‚îî‚îÄ‚îÄ release
    ‚îú‚îÄ‚îÄ linux_amd64
    ‚îÇ   ‚îî‚îÄ‚îÄ hello-linux-amd64
    ‚îî‚îÄ‚îÄ linux_arm64
        ‚îî‚îÄ‚îÄ hello-linux-arm64
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can set &lt;code&gt;platform-split=false&lt;/code&gt; to merge files from all platforms together into same directory:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;$ buildctl build \
  --frontend dockerfile.v0 \
  --opt platform=linux/amd64,linux/arm64 \
  --output type=local,dest=./bin/release,platform-split=false

$ tree ./bin
./bin/
‚îî‚îÄ‚îÄ release
    ‚îú‚îÄ‚îÄ hello-linux-amd64
    ‚îî‚îÄ‚îÄ hello-linux-arm64
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Tar exporter is similar to local exporter but transfers the files through a tarball.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;buildctl build ... --output type=tar,dest=out.tar
buildctl build ... --output type=tar &amp;gt; out.tar
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Docker tarball&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# exported tarball is also compatible with OCI spec
buildctl build ... --output type=docker,name=myimage | docker load
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;OCI tarball&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;buildctl build ... --output type=oci,dest=path/to/output.tar
buildctl build ... --output type=oci &amp;gt; output.tar
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;containerd image store&lt;/h4&gt; 
&lt;p&gt;The containerd worker needs to be used&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;buildctl build ... --output type=image,name=docker.io/username/image
ctr --namespace=buildkit images ls
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To change the containerd namespace, you need to change &lt;code&gt;worker.containerd.namespace&lt;/code&gt; in &lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/docs/buildkitd.toml.md"&gt;&lt;code&gt;/etc/buildkit/buildkitd.toml&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Cache&lt;/h2&gt; 
&lt;p&gt;To show local build cache (&lt;code&gt;/var/lib/buildkit&lt;/code&gt;):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;buildctl du -v
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To prune local build cache:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;buildctl prune
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Garbage collection&lt;/h3&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/docs/buildkitd.toml.md"&gt;&lt;code&gt;./docs/buildkitd.toml.md&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Export cache&lt;/h3&gt; 
&lt;p&gt;BuildKit supports the following cache exporters:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;inline&lt;/code&gt;: embed the cache into the image, and push them to the registry together&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;registry&lt;/code&gt;: push the image and the cache separately&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;local&lt;/code&gt;: export to a local directory&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;gha&lt;/code&gt;: export to GitHub Actions cache&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;In most case you want to use the &lt;code&gt;inline&lt;/code&gt; cache exporter. However, note that the &lt;code&gt;inline&lt;/code&gt; cache exporter only supports &lt;code&gt;min&lt;/code&gt; cache mode. To enable &lt;code&gt;max&lt;/code&gt; cache mode, push the image and the cache separately by using &lt;code&gt;registry&lt;/code&gt; cache exporter.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;inline&lt;/code&gt; and &lt;code&gt;registry&lt;/code&gt; exporters both store the cache in the registry. For importing the cache, &lt;code&gt;type=registry&lt;/code&gt; is sufficient for both, as specifying the cache format is not necessary.&lt;/p&gt; 
&lt;h4&gt;Inline (push image and cache together)&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;buildctl build ... \
  --output type=image,name=docker.io/username/image,push=true \
  --export-cache type=inline \
  --import-cache type=registry,ref=docker.io/username/image
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note that the inline cache is not imported unless &lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/#registry-push-image-and-cache-separately"&gt;&lt;code&gt;--import-cache type=registry,ref=...&lt;/code&gt;&lt;/a&gt; is provided.&lt;/p&gt; 
&lt;p&gt;Inline cache embeds cache metadata into the image config. The layers in the image will be left untouched compared to the image with no cache information.&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;‚Ñπ&lt;/span&gt; Docker-integrated BuildKit (&lt;code&gt;DOCKER_BUILDKIT=1 docker build&lt;/code&gt;) and &lt;code&gt;docker buildx&lt;/code&gt;requires &lt;code&gt;--build-arg BUILDKIT_INLINE_CACHE=1&lt;/code&gt; to be specified to enable the &lt;code&gt;inline&lt;/code&gt; cache exporter. However, the standalone &lt;code&gt;buildctl&lt;/code&gt; does NOT require &lt;code&gt;--opt build-arg:BUILDKIT_INLINE_CACHE=1&lt;/code&gt; and the build-arg is simply ignored.&lt;/p&gt; 
&lt;h4&gt;Registry (push image and cache separately)&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;buildctl build ... \
  --output type=image,name=localhost:5000/myrepo:image,push=true \
  --export-cache type=registry,ref=localhost:5000/myrepo:buildcache \
  --import-cache type=registry,ref=localhost:5000/myrepo:buildcache
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;code&gt;--export-cache&lt;/code&gt; options:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;type=registry&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;mode=&amp;lt;min|max&amp;gt;&lt;/code&gt;: specify cache layers to export (default: &lt;code&gt;min&lt;/code&gt;) 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;min&lt;/code&gt;: only export layers for the resulting image&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;max&lt;/code&gt;: export all the layers of all intermediate steps&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ref=&amp;lt;ref&amp;gt;&lt;/code&gt;: specify repository reference to store cache, e.g. &lt;code&gt;docker.io/user/image:tag&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;image-manifest=&amp;lt;true|false&amp;gt;&lt;/code&gt;: whether to export cache manifest as an OCI-compatible image manifest rather than a manifest list/index (default: &lt;code&gt;true&lt;/code&gt; since BuildKit &lt;code&gt;v0.21&lt;/code&gt;, must be used with &lt;code&gt;oci-mediatypes=true&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;oci-mediatypes=&amp;lt;true|false&amp;gt;&lt;/code&gt;: whether to use OCI mediatypes in exported manifests (default: &lt;code&gt;true&lt;/code&gt;, since BuildKit &lt;code&gt;v0.8&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;compression=&amp;lt;uncompressed|gzip|estargz|zstd&amp;gt;&lt;/code&gt;: choose compression type for layers newly created and cached, gzip is default value. estargz and zstd should be used with &lt;code&gt;oci-mediatypes=true&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;compression-level=&amp;lt;value&amp;gt;&lt;/code&gt;: choose compression level for gzip, estargz (0-9) and zstd (0-22)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;force-compression=true&lt;/code&gt;: forcibly apply &lt;code&gt;compression&lt;/code&gt; option to all layers&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ignore-error=&amp;lt;false|true&amp;gt;&lt;/code&gt;: specify if error is ignored in case cache export fails (default: &lt;code&gt;false&lt;/code&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;code&gt;--import-cache&lt;/code&gt; options:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;type=registry&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ref=&amp;lt;ref&amp;gt;&lt;/code&gt;: specify repository reference to retrieve cache from, e.g. &lt;code&gt;docker.io/user/image:tag&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Local directory&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;buildctl build ... --export-cache type=local,dest=path/to/output-dir
buildctl build ... --import-cache type=local,src=path/to/input-dir
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The directory layout conforms to OCI Image Spec v1.0.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;--export-cache&lt;/code&gt; options:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;type=local&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;mode=&amp;lt;min|max&amp;gt;&lt;/code&gt;: specify cache layers to export (default: &lt;code&gt;min&lt;/code&gt;) 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;min&lt;/code&gt;: only export layers for the resulting image&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;max&lt;/code&gt;: export all the layers of all intermediate steps&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;dest=&amp;lt;path&amp;gt;&lt;/code&gt;: destination directory for cache exporter&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;tag=&amp;lt;tag&amp;gt;&lt;/code&gt;: specify custom tag of image to write to local index (default: &lt;code&gt;latest&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;image-manifest=&amp;lt;true|false&amp;gt;&lt;/code&gt;: whether to export cache manifest as an OCI-compatible image manifest rather than a manifest list/index (default: &lt;code&gt;true&lt;/code&gt; since BuildKit &lt;code&gt;v0.21&lt;/code&gt;, must be used with &lt;code&gt;oci-mediatypes=true&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;oci-mediatypes=&amp;lt;true|false&amp;gt;&lt;/code&gt;: whether to use OCI mediatypes in exported manifests (default &lt;code&gt;true&lt;/code&gt;, since BuildKit &lt;code&gt;v0.8&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;compression=&amp;lt;uncompressed|gzip|estargz|zstd&amp;gt;&lt;/code&gt;: choose compression type for layers newly created and cached, gzip is default value. estargz and zstd should be used with &lt;code&gt;oci-mediatypes=true&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;compression-level=&amp;lt;value&amp;gt;&lt;/code&gt;: compression level for gzip, estargz (0-9) and zstd (0-22)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;force-compression=true&lt;/code&gt;: forcibly apply &lt;code&gt;compression&lt;/code&gt; option to all layers&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ignore-error=&amp;lt;false|true&amp;gt;&lt;/code&gt;: specify if error is ignored in case cache export fails (default: &lt;code&gt;false&lt;/code&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;code&gt;--import-cache&lt;/code&gt; options:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;type=local&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;src=&amp;lt;path&amp;gt;&lt;/code&gt;: source directory for cache importer&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;tag=&amp;lt;tag&amp;gt;&lt;/code&gt;: specify custom tag of image to read from local index (default: &lt;code&gt;latest&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;digest=sha256:&amp;lt;sha256digest&amp;gt;&lt;/code&gt;: specify explicit digest of the manifest list to import&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;GitHub Actions cache (experimental)&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;buildctl build ... \
  --output type=image,name=docker.io/username/image,push=true \
  --export-cache type=gha \
  --import-cache type=gha
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;GitHub Actions cache saves both cache metadata and layers to GitHub's Cache service. This cache currently has a &lt;a href="https://docs.github.com/en/actions/advanced-guides/caching-dependencies-to-speed-up-workflows#usage-limits-and-eviction-policy"&gt;size limit of 10GB&lt;/a&gt; that is shared across different caches in the repo. If you exceed this limit, GitHub will save your cache but will begin evicting caches until the total size is less than 10 GB. Recycling caches too often can result in slower runtimes overall.&lt;/p&gt; 
&lt;p&gt;Similarly to using &lt;a href="https://github.com/actions/cache"&gt;actions/cache&lt;/a&gt;, caches are &lt;a href="https://docs.github.com/en/actions/advanced-guides/caching-dependencies-to-speed-up-workflows#restrictions-for-accessing-a-cache"&gt;scoped by branch&lt;/a&gt;, with the default and target branches being available to every branch.&lt;/p&gt; 
&lt;p&gt;Following attributes are required to authenticate against the &lt;a href="https://github.com/tonistiigi/go-actions-cache/raw/master/api.md#authentication"&gt;GitHub Actions Cache service API&lt;/a&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;url&lt;/code&gt;: Cache server URL (default &lt;code&gt;$ACTIONS_CACHE_URL&lt;/code&gt; or fallback to &lt;code&gt;$ACTIONS_RESULTS_URL&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;url_v2&lt;/code&gt;: Cache v2 server URL if &lt;code&gt;$ACTIONS_CACHE_SERVICE_V2&lt;/code&gt; set on the runner (default &lt;code&gt;$ACTIONS_RESULTS_URL&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;token&lt;/code&gt;: Access token (default &lt;code&gt;$ACTIONS_RUNTIME_TOKEN&lt;/code&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;span&gt;‚Ñπ&lt;/span&gt; This type of cache can be used with &lt;a href="https://github.com/docker/build-push-action"&gt;Docker Build Push Action&lt;/a&gt; where &lt;code&gt;url&lt;/code&gt; and &lt;code&gt;token&lt;/code&gt; will be automatically set. To use this backend in an inline &lt;code&gt;run&lt;/code&gt; step, you have to include &lt;a href="https://github.com/crazy-max/ghaction-github-runtime"&gt;crazy-max/ghaction-github-runtime&lt;/a&gt; in your workflow to expose the runtime.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;--export-cache&lt;/code&gt; options:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;type=gha&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;mode=&amp;lt;min|max&amp;gt;&lt;/code&gt;: specify cache layers to export (default: &lt;code&gt;min&lt;/code&gt;) 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;min&lt;/code&gt;: only export layers for the resulting image&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;max&lt;/code&gt;: export all the layers of all intermediate steps&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;scope=&amp;lt;scope&amp;gt;&lt;/code&gt;: which scope cache object belongs to (default &lt;code&gt;buildkit&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ignore-error=&amp;lt;false|true&amp;gt;&lt;/code&gt;: specify if error is ignored in case cache export fails (default: &lt;code&gt;false&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;timeout=&amp;lt;duration&amp;gt;&lt;/code&gt;: sets the timeout duration for cache export (default: &lt;code&gt;10m&lt;/code&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;code&gt;--import-cache&lt;/code&gt; options:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;type=gha&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;scope=&amp;lt;scope&amp;gt;&lt;/code&gt;: which scope cache object belongs to (default &lt;code&gt;buildkit&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;timeout=&amp;lt;duration&amp;gt;&lt;/code&gt;: sets the timeout duration for cache import (default: &lt;code&gt;10m&lt;/code&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;S3 cache (experimental)&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;buildctl build ... \
  --output type=image,name=docker.io/username/image,push=true \
  --export-cache type=s3,region=eu-west-1,bucket=my_bucket,name=my_image \
  --import-cache type=s3,region=eu-west-1,bucket=my_bucket,name=my_image
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The following attributes are required:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;bucket&lt;/code&gt;: AWS S3 bucket (default: &lt;code&gt;$AWS_BUCKET&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;region&lt;/code&gt;: AWS region (default: &lt;code&gt;$AWS_REGION&lt;/code&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Storage locations:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;blobs: &lt;code&gt;s3://&amp;lt;bucket&amp;gt;/&amp;lt;prefix&amp;gt;&amp;lt;blobs_prefix&amp;gt;/&amp;lt;sha256&amp;gt;&lt;/code&gt;, default: &lt;code&gt;s3://&amp;lt;bucket&amp;gt;/blobs/&amp;lt;sha256&amp;gt;&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;manifests: &lt;code&gt;s3://&amp;lt;bucket&amp;gt;/&amp;lt;prefix&amp;gt;&amp;lt;manifests_prefix&amp;gt;/&amp;lt;name&amp;gt;&lt;/code&gt;, default: &lt;code&gt;s3://&amp;lt;bucket&amp;gt;/manifests/&amp;lt;name&amp;gt;&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;S3 configuration:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;blobs_prefix&lt;/code&gt;: global prefix to store / read blobs on s3 (default: &lt;code&gt;blobs/&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;manifests_prefix&lt;/code&gt;: global prefix to store / read manifests on s3 (default: &lt;code&gt;manifests/&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;endpoint_url&lt;/code&gt;: specify a specific S3 endpoint (default: empty)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;use_path_style&lt;/code&gt;: if set to &lt;code&gt;true&lt;/code&gt;, put the bucket name in the URL instead of in the hostname (default: &lt;code&gt;false&lt;/code&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;AWS Authentication:&lt;/p&gt; 
&lt;p&gt;BuildKit relies on the &lt;a href="https://docs.aws.amazon.com/sdk-for-go/v2/developer-guide/configure-gosdk.html"&gt;AWS Go SDK&lt;/a&gt;. This means that all standard authentication methods through &lt;a href="https://pkg.go.dev/github.com/aws/aws-sdk-go-v2/config#EnvConfig"&gt;environment variables&lt;/a&gt; or config files are supported. This is especially true for AWS EC2 IAM Profile and AWS Web Identity Token (IAM roles in Kubernetes).&lt;/p&gt; 
&lt;p&gt;Beware, these configurations must be available at buildkit daemon level, not at client level.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;The following attributes can be used to forward static credentials from a buildkit client to the daemon (buildx for example). 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;access_key_id&lt;/code&gt;: Access Key ID&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;secret_access_key&lt;/code&gt;: Secret Access Key&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;session_token&lt;/code&gt;: Session Token&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;code&gt;--export-cache&lt;/code&gt; options:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;type=s3&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;mode=&amp;lt;min|max&amp;gt;&lt;/code&gt;: specify cache layers to export (default: &lt;code&gt;min&lt;/code&gt;) 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;min&lt;/code&gt;: only export layers for the resulting image&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;max&lt;/code&gt;: export all the layers of all intermediate steps&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;prefix=&amp;lt;prefix&amp;gt;&lt;/code&gt;: set global prefix to store / read files on s3 (default: empty)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;name=&amp;lt;manifest&amp;gt;&lt;/code&gt;: specify name of the manifest to use (default &lt;code&gt;buildkit&lt;/code&gt;) 
  &lt;ul&gt; 
   &lt;li&gt;Multiple manifest names can be specified at the same time, separated by &lt;code&gt;;&lt;/code&gt;. The standard use case is to use the git sha1 as name, and the branch name as duplicate, and load both with 2 &lt;code&gt;import-cache&lt;/code&gt; commands.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ignore-error=&amp;lt;false|true&amp;gt;&lt;/code&gt;: specify if error is ignored in case cache export fails (default: &lt;code&gt;false&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;touch_refresh=24h&lt;/code&gt;: Instead of being uploaded again when not changed, blobs files will be "touched" on s3 every &lt;code&gt;touch_refresh&lt;/code&gt;, default is 24h. Due to this, an expiration policy can be set on the S3 bucket to cleanup useless files automatically. Manifests files are systematically rewritten, there is no need to touch them.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;upload_parallelism=4&lt;/code&gt;: This parameter changes the number of layers uploaded to s3 in parallel. Each individual layer is uploaded with 5 threads, using the Upload manager provided by the AWS SDK.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;code&gt;--import-cache&lt;/code&gt; options:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;type=s3&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;prefix=&amp;lt;prefix&amp;gt;&lt;/code&gt;: set global prefix to store / read files on s3 (default: empty)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;blobs_prefix=&amp;lt;prefix&amp;gt;&lt;/code&gt;: set global prefix to store / read blobs on s3 (default: &lt;code&gt;blobs/&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;manifests_prefix=&amp;lt;prefix&amp;gt;&lt;/code&gt;: set global prefix to store / read manifests on s3 (default: &lt;code&gt;manifests/&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;name=&amp;lt;manifest&amp;gt;&lt;/code&gt;: name of the manifest to use (default &lt;code&gt;buildkit&lt;/code&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Azure Blob Storage cache (experimental)&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;buildctl build ... \
  --output type=image,name=docker.io/username/image,push=true \
  --export-cache type=azblob,account_url=https://myaccount.blob.core.windows.net,name=my_image \
  --import-cache type=azblob,account_url=https://myaccount.blob.core.windows.net,name=my_image
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The following attributes are required:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;account_url&lt;/code&gt;: The Azure Blob Storage account URL (default: &lt;code&gt;$BUILDKIT_AZURE_STORAGE_ACCOUNT_URL&lt;/code&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Storage locations:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;blobs: &lt;code&gt;&amp;lt;account_url&amp;gt;/&amp;lt;container&amp;gt;/&amp;lt;prefix&amp;gt;&amp;lt;blobs_prefix&amp;gt;/&amp;lt;sha256&amp;gt;&lt;/code&gt;, default: &lt;code&gt;&amp;lt;account_url&amp;gt;/&amp;lt;container&amp;gt;/blobs/&amp;lt;sha256&amp;gt;&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;manifests: &lt;code&gt;&amp;lt;account_url&amp;gt;/&amp;lt;container&amp;gt;/&amp;lt;prefix&amp;gt;&amp;lt;manifests_prefix&amp;gt;/&amp;lt;name&amp;gt;&lt;/code&gt;, default: &lt;code&gt;&amp;lt;account_url&amp;gt;/&amp;lt;container&amp;gt;/manifests/&amp;lt;name&amp;gt;&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Azure Blob Storage configuration:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;container&lt;/code&gt;: The Azure Blob Storage container name (default: &lt;code&gt;buildkit-cache&lt;/code&gt; or &lt;code&gt;$BUILDKIT_AZURE_STORAGE_CONTAINER&lt;/code&gt; if set)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;blobs_prefix&lt;/code&gt;: Global prefix to store / read blobs on the Azure Blob Storage container (&lt;code&gt;&amp;lt;container&amp;gt;&lt;/code&gt;) (default: &lt;code&gt;blobs/&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;manifests_prefix&lt;/code&gt;: Global prefix to store / read blobs on the Azure Blob Storage container (&lt;code&gt;&amp;lt;container&amp;gt;&lt;/code&gt;) (default: &lt;code&gt;manifests/&lt;/code&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Azure Blob Storage authentication:&lt;/p&gt; 
&lt;p&gt;There are 2 options supported for Azure Blob Storage authentication:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Any system using environment variables supported by the &lt;a href="https://docs.microsoft.com/en-us/azure/developer/go/azure-sdk-authentication"&gt;Azure SDK for Go&lt;/a&gt;. The configuration must be available for the buildkit daemon, not for the client.&lt;/li&gt; 
 &lt;li&gt;Secret Access Key, using the &lt;code&gt;secret_access_key&lt;/code&gt; attribute to specify the primary or secondary account key for your Azure Blob Storage account. &lt;a href="https://docs.microsoft.com/en-us/azure/storage/common/storage-account-keys-manage"&gt;Azure Blob Storage account keys&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Account name can also be specified with &lt;code&gt;account_name&lt;/code&gt; attribute (or &lt;code&gt;$BUILDKIT_AZURE_STORAGE_ACCOUNT_NAME&lt;/code&gt;) if it is not part of the account URL host.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;code&gt;--export-cache&lt;/code&gt; options:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;type=azblob&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;mode=&amp;lt;min|max&amp;gt;&lt;/code&gt;: specify cache layers to export (default: &lt;code&gt;min&lt;/code&gt;) 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;min&lt;/code&gt;: only export layers for the resulting image&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;max&lt;/code&gt;: export all the layers of all intermediate steps&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;prefix=&amp;lt;prefix&amp;gt;&lt;/code&gt;: set global prefix to store / read files on the Azure Blob Storage container (&lt;code&gt;&amp;lt;container&amp;gt;&lt;/code&gt;) (default: empty)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;name=&amp;lt;manifest&amp;gt;&lt;/code&gt;: specify name of the manifest to use (default: &lt;code&gt;buildkit&lt;/code&gt;) 
  &lt;ul&gt; 
   &lt;li&gt;Multiple manifest names can be specified at the same time, separated by &lt;code&gt;;&lt;/code&gt;. The standard use case is to use the git sha1 as name, and the branch name as duplicate, and load both with 2 &lt;code&gt;import-cache&lt;/code&gt; commands.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ignore-error=&amp;lt;false|true&amp;gt;&lt;/code&gt;: specify if error is ignored in case cache export fails (default: &lt;code&gt;false&lt;/code&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;code&gt;--import-cache&lt;/code&gt; options:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;type=azblob&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;prefix=&amp;lt;prefix&amp;gt;&lt;/code&gt;: set global prefix to store / read files on the Azure Blob Storage container (&lt;code&gt;&amp;lt;container&amp;gt;&lt;/code&gt;) (default: empty)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;blobs_prefix=&amp;lt;prefix&amp;gt;&lt;/code&gt;: set global prefix to store / read blobs on the Azure Blob Storage container (&lt;code&gt;&amp;lt;container&amp;gt;&lt;/code&gt;) (default: &lt;code&gt;blobs/&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;manifests_prefix=&amp;lt;prefix&amp;gt;&lt;/code&gt;: set global prefix to store / read manifests on the Azure Blob Storage container (&lt;code&gt;&amp;lt;container&amp;gt;&lt;/code&gt;) (default: &lt;code&gt;manifests/&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;name=&amp;lt;manifest&amp;gt;&lt;/code&gt;: name of the manifest to use (default: &lt;code&gt;buildkit&lt;/code&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Consistent hashing&lt;/h3&gt; 
&lt;p&gt;If you have multiple BuildKit daemon instances, but you don't want to use registry for sharing cache across the cluster, consider client-side load balancing using consistent hashing.&lt;/p&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/examples/kubernetes/consistenthash"&gt;&lt;code&gt;./examples/kubernetes/consistenthash&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Metadata&lt;/h2&gt; 
&lt;p&gt;To output build metadata such as the image digest, pass the &lt;code&gt;--metadata-file&lt;/code&gt; flag. The metadata will be written as a JSON object to the specified file. The directory of the specified file must already exist and be writable.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;buildctl build ... --metadata-file metadata.json
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;jq '.' metadata.json
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "containerimage.config.digest": "sha256:2937f66a9722f7f4a2df583de2f8cb97fc9196059a410e7f00072fc918930e66",
  "containerimage.descriptor": {
    "annotations": {
      "config.digest": "sha256:2937f66a9722f7f4a2df583de2f8cb97fc9196059a410e7f00072fc918930e66",
      "org.opencontainers.image.created": "2022-02-08T21:28:03Z"
    },
    "digest": "sha256:19ffeab6f8bc9293ac2c3fdf94ebe28396254c993aea0b5a542cfb02e0883fa3",
    "mediaType": "application/vnd.oci.image.manifest.v1+json",
    "size": 506
  },
  "containerimage.digest": "sha256:19ffeab6f8bc9293ac2c3fdf94ebe28396254c993aea0b5a542cfb02e0883fa3"
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Systemd socket activation&lt;/h2&gt; 
&lt;p&gt;On Systemd based systems, you can communicate with the daemon via &lt;a href="http://0pointer.de/blog/projects/socket-activation.html"&gt;Systemd socket activation&lt;/a&gt;, use &lt;code&gt;buildkitd --addr fd://&lt;/code&gt;. You can find examples of using Systemd socket activation with BuildKit and Systemd in &lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/examples/systemd"&gt;&lt;code&gt;./examples/systemd&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Expose BuildKit as a TCP service&lt;/h2&gt; 
&lt;p&gt;The &lt;code&gt;buildkitd&lt;/code&gt; daemon can listen the gRPC API on a TCP socket.&lt;/p&gt; 
&lt;p&gt;It is highly recommended to create TLS certificates for both the daemon and the client (mTLS). Enabling TCP without mTLS is dangerous because the executor containers (aka Dockerfile &lt;code&gt;RUN&lt;/code&gt; containers) can call BuildKit API as well.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;buildkitd \
  --addr tcp://0.0.0.0:1234 \
  --tlscacert /path/to/ca.pem \
  --tlscert /path/to/cert.pem \
  --tlskey /path/to/key.pem
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;buildctl \
  --addr tcp://example.com:1234 \
  --tlscacert /path/to/ca.pem \
  --tlscert /path/to/clientcert.pem \
  --tlskey /path/to/clientkey.pem \
  build ...
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Load balancing&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;buildctl build&lt;/code&gt; can be called against randomly load balanced &lt;code&gt;buildkitd&lt;/code&gt; daemons.&lt;/p&gt; 
&lt;p&gt;See also &lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/#consistent-hashing"&gt;Consistent hashing&lt;/a&gt; for client-side load balancing.&lt;/p&gt; 
&lt;h2&gt;Containerizing BuildKit&lt;/h2&gt; 
&lt;p&gt;BuildKit can also be used by running the &lt;code&gt;buildkitd&lt;/code&gt; daemon inside a Docker container and accessing it remotely.&lt;/p&gt; 
&lt;p&gt;We provide the container images as &lt;a href="https://hub.docker.com/r/moby/buildkit/tags/"&gt;&lt;code&gt;moby/buildkit&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;moby/buildkit:latest&lt;/code&gt;: built from the latest regular &lt;a href="https://github.com/moby/buildkit/releases"&gt;release&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;moby/buildkit:rootless&lt;/code&gt;: same as &lt;code&gt;latest&lt;/code&gt; but runs as an unprivileged user, see &lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/docs/rootless.md"&gt;&lt;code&gt;docs/rootless.md&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;moby/buildkit:master&lt;/code&gt;: built from the master branch&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;moby/buildkit:master-rootless&lt;/code&gt;: same as master but runs as an unprivileged user, see &lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/docs/rootless.md"&gt;&lt;code&gt;docs/rootless.md&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To run daemon in a container:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run -d --name buildkitd --privileged moby/buildkit:latest
export BUILDKIT_HOST=docker-container://buildkitd
buildctl build --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Podman&lt;/h3&gt; 
&lt;p&gt;To connect to a BuildKit daemon running in a Podman container, use &lt;code&gt;podman-container://&lt;/code&gt; instead of &lt;code&gt;docker-container://&lt;/code&gt; .&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;podman run -d --name buildkitd --privileged moby/buildkit:latest
buildctl --addr=podman-container://buildkitd build --frontend dockerfile.v0 --local context=. --local dockerfile=. --output type=oci | podman load foo
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;code&gt;sudo&lt;/code&gt; is not required.&lt;/p&gt; 
&lt;h3&gt;Nerdctl&lt;/h3&gt; 
&lt;p&gt;To connect to a BuildKit daemon running in a Nerdctl container, use &lt;code&gt;nerdctl-container://&lt;/code&gt; instead of &lt;code&gt;docker-container://&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;nerdctl run -d --name buildkitd --privileged moby/buildkit:latest
buildctl --addr=nerdctl-container://buildkitd build --frontend dockerfile.v0 --local context=. --local dockerfile=. --output type=oci | nerdctl load
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;code&gt;sudo&lt;/code&gt; is not required.&lt;/p&gt; 
&lt;h3&gt;Kubernetes&lt;/h3&gt; 
&lt;p&gt;For Kubernetes deployments, see &lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/examples/kubernetes"&gt;&lt;code&gt;examples/kubernetes&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Daemonless&lt;/h3&gt; 
&lt;p&gt;To run the client and an ephemeral daemon in a single container ("daemonless mode"):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run \
    -it \
    --rm \
    --privileged \
    -v /path/to/dir:/tmp/work \
    --entrypoint buildctl-daemonless.sh \
    moby/buildkit:master \
        build \
        --frontend dockerfile.v0 \
        --local context=/tmp/work \
        --local dockerfile=/tmp/work
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run \
    -it \
    --rm \
    --security-opt seccomp=unconfined \
    --security-opt apparmor=unconfined \
    --security-opt systempaths=unconfined \
    -v /path/to/dir:/tmp/work \
    --entrypoint buildctl-daemonless.sh \
    moby/buildkit:master-rootless \
        build \
        --frontend \
        dockerfile.v0 \
        --local context=/tmp/work \
        --local dockerfile=/tmp/work
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;OpenTelemetry support&lt;/h2&gt; 
&lt;p&gt;BuildKit supports &lt;a href="https://opentelemetry.io/"&gt;OpenTelemetry&lt;/a&gt; for buildkitd gRPC API and buildctl commands. To capture the trace to &lt;a href="https://github.com/jaegertracing/jaeger"&gt;Jaeger&lt;/a&gt;, set &lt;code&gt;JAEGER_TRACE&lt;/code&gt; environment variable to the collection address.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run -d -p6831:6831/udp -p16686:16686 jaegertracing/all-in-one:latest
export JAEGER_TRACE=0.0.0.0:6831
# restart buildkitd and buildctl so they know JAEGER_TRACE
# any buildctl command should be traced to http://127.0.0.1:16686/
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;On Windows, if you are running Jaeger outside of a container, &lt;a href="https://www.jaegertracing.io/docs/1.57/getting-started/#all-in-one"&gt;&lt;code&gt;jaeger-all-in-one.exe&lt;/code&gt;&lt;/a&gt;, set the environment variable &lt;code&gt;setx -m JAEGER_TRACE "0.0.0.0:6831"&lt;/code&gt;, restart &lt;code&gt;buildkitd&lt;/code&gt; in a new terminal and the traces will be collected automatically.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Running BuildKit without root privileges&lt;/h2&gt; 
&lt;p&gt;Please refer to &lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/docs/rootless.md"&gt;&lt;code&gt;docs/rootless.md&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Building multi-platform images&lt;/h2&gt; 
&lt;p&gt;Please refer to &lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/docs/multi-platform.md"&gt;&lt;code&gt;docs/multi-platform.md&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Configuring &lt;code&gt;buildctl&lt;/code&gt;&lt;/h3&gt; 
&lt;h4&gt;Color Output Controls&lt;/h4&gt; 
&lt;p&gt;&lt;code&gt;buildctl&lt;/code&gt; has support for modifying the colors that are used to output information to the terminal. You can set the environment variable &lt;code&gt;BUILDKIT_COLORS&lt;/code&gt; to something like &lt;code&gt;run=green:warning=yellow:error=red:cancel=255,165,0&lt;/code&gt; to set the colors that you would like to use. Setting &lt;code&gt;NO_COLOR&lt;/code&gt; to anything will disable any colorized output as recommended by &lt;a href="https://no-color.org/"&gt;no-color.org&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Parsing errors will be reported but ignored. This will result in default color values being used where needed.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/moby/buildkit/raw/master/util/progress/progressui/colors.go"&gt;The list of pre-defined colors&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Number of log lines (for active steps in tty mode)&lt;/h4&gt; 
&lt;p&gt;You can change how many log lines are visible for active steps in tty mode by setting &lt;code&gt;BUILDKIT_TTY_LOG_LINES&lt;/code&gt; to a number (default: 6).&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Want to contribute to BuildKit? Awesome! You can find information about contributing to this project in the &lt;a href="https://raw.githubusercontent.com/moby/buildkit/master/.github/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>rancher/rancher</title>
      <link>https://github.com/rancher/rancher</link>
      <description>&lt;p&gt;Complete container management platform&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Rancher&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://store.docker.com/community/images/rancher/rancher"&gt;&lt;img src="https://img.shields.io/docker/pulls/rancher/rancher.svg?sanitize=true" alt="Docker Pulls" /&gt;&lt;/a&gt; &lt;a href="https://goreportcard.com/report/github.com/rancher/rancher"&gt;&lt;img src="https://goreportcard.com/badge/github.com/rancher/rancher" alt="Go Report Card" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Rancher is an open source container management platform built for organizations that deploy containers in production. Rancher makes it easy to run Kubernetes everywhere, meet IT requirements, and empower DevOps teams.&lt;/p&gt; 
&lt;h2&gt;Stable Release&lt;/h2&gt; 
&lt;!-- stable v2.13.1 DO NOT REMOVE THIS LINE --&gt; 
&lt;ul&gt; 
 &lt;li&gt;v2.13.1 - &lt;code&gt;rancher/rancher:v2.13.1&lt;/code&gt; / &lt;code&gt;rancher/rancher:stable&lt;/code&gt; - Read the full release &lt;a href="https://github.com/rancher/rancher/releases/tag/v2.13.1"&gt;notes&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To get automated notifications of our latest release, you can watch the announcements category in our &lt;a href="http://forums.rancher.com/c/announcements"&gt;forums&lt;/a&gt;, or subscribe to the RSS feed &lt;code&gt;https://forums.rancher.com/c/announcements.rss&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;sudo docker run -d --restart=unless-stopped -p 80:80 -p 443:443 --privileged rancher/rancher
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Open your browser to &lt;a href="https://localhost"&gt;https://localhost&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://ranchermanager.docs.rancher.com/v2.8/pages-for-subheaders/installation-and-upgrade"&gt;Installing/Upgrading Rancher&lt;/a&gt; for all installation options.&lt;/p&gt; 
&lt;h3&gt;Minimum Requirements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Operating Systems 
  &lt;ul&gt; 
   &lt;li&gt;Please see &lt;a href="https://rancher.com/support-matrix/"&gt;Support Matrix&lt;/a&gt; for specific OS versions for each Rancher version. Note that the link will default to the support matrix for the latest version of Rancher. Use the left navigation menu to select a different Rancher version.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Hardware &amp;amp; Software 
  &lt;ul&gt; 
   &lt;li&gt;Please see &lt;a href="https://ranchermanager.docs.rancher.com/v2.8/pages-for-subheaders/installation-requirements"&gt;Installation Requirements&lt;/a&gt; for hardware and software requirements.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Using Rancher&lt;/h3&gt; 
&lt;p&gt;To learn more about using Rancher, please refer to our &lt;a href="https://ranchermanager.docs.rancher.com/v2.8"&gt;Rancher Documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Source Code&lt;/h2&gt; 
&lt;p&gt;This repo is a meta-repo used for packaging and contains the majority of Rancher codebase. For other Rancher projects and modules, &lt;a href="https://github.com/rancher/rancher/raw/release/v2.8/go.mod"&gt;see go.mod&lt;/a&gt; for the full list.&lt;/p&gt; 
&lt;p&gt;Rancher also includes other open source libraries and projects, &lt;a href="https://github.com/rancher/rancher/raw/release/v2.8/go.mod"&gt;see go.mod&lt;/a&gt; for the full list.&lt;/p&gt; 
&lt;h2&gt;Build configuration&lt;/h2&gt; 
&lt;p&gt;Refer to the &lt;a href="https://raw.githubusercontent.com/rancher/rancher/main/docs/build.md"&gt;build docs&lt;/a&gt; on how to customize the building and packaging of Rancher.&lt;/p&gt; 
&lt;h2&gt;Support, Discussion, and Community&lt;/h2&gt; 
&lt;p&gt;If you need any help with Rancher, please join us at either our &lt;a href="http://forums.rancher.com/"&gt;Rancher forums&lt;/a&gt; or &lt;a href="https://slack.rancher.io/"&gt;Slack&lt;/a&gt; where most of our team hangs out at.&lt;/p&gt; 
&lt;p&gt;Please submit any Rancher bugs, issues, and feature requests to &lt;a href="https://github.com/rancher/rancher/issues"&gt;rancher/rancher&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For security issues, please first check our &lt;a href="https://github.com/rancher/rancher/security"&gt;security policy&lt;/a&gt; and email &lt;a href="mailto:security-rancher@suse.com"&gt;security-rancher@suse.com&lt;/a&gt; instead of posting a public issue in GitHub. You may (but are not required to) use the GPG key located on &lt;a href="https://keybase.io/rancher"&gt;Keybase&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;License&lt;/h1&gt; 
&lt;p&gt;Copyright (c) 2014-2025 &lt;a href="http://rancher.com"&gt;SUSE&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at&lt;/p&gt; 
&lt;p&gt;&lt;a href="http://www.apache.org/licenses/LICENSE-2.0"&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>microsoft/typescript-go</title>
      <link>https://github.com/microsoft/typescript-go</link>
      <description>&lt;p&gt;Staging repo for development of native port of TypeScript&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;TypeScript 7&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://devblogs.microsoft.com/typescript/typescript-native-port/"&gt;Not sure what this is? Read the announcement post!&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Preview&lt;/h2&gt; 
&lt;p&gt;A preview build is available on npm as &lt;a href="https://www.npmjs.com/package/@typescript/native-preview"&gt;&lt;code&gt;@typescript/native-preview&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;npm install @typescript/native-preview
npx tsgo # Use this as you would tsc.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;A preview VS Code extension is &lt;a href="https://marketplace.visualstudio.com/items?itemName=TypeScriptTeam.native-preview"&gt;available on the VS Code marketplace&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;To use this, set this in your VS Code settings:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
    "typescript.experimental.useTsgo": true
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;What Works So Far?&lt;/h2&gt; 
&lt;p&gt;This is still a work in progress and is not yet at full feature parity with TypeScript. Bugs may exist. Please check this list carefully before logging a new issue or assuming an intentional change.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Feature&lt;/th&gt; 
   &lt;th&gt;Status&lt;/th&gt; 
   &lt;th&gt;Notes&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Program creation&lt;/td&gt; 
   &lt;td&gt;done&lt;/td&gt; 
   &lt;td&gt;Same files and module resolution as TS 5.9. Not all resolution modes supported yet.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Parsing/scanning&lt;/td&gt; 
   &lt;td&gt;done&lt;/td&gt; 
   &lt;td&gt;Exact same syntax errors as TS 5.9&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Commandline and &lt;code&gt;tsconfig.json&lt;/code&gt; parsing&lt;/td&gt; 
   &lt;td&gt;done&lt;/td&gt; 
   &lt;td&gt;Done, though &lt;code&gt;tsconfig&lt;/code&gt; errors may not be as helpful.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Type resolution&lt;/td&gt; 
   &lt;td&gt;done&lt;/td&gt; 
   &lt;td&gt;Same types as TS 5.9.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Type checking&lt;/td&gt; 
   &lt;td&gt;done&lt;/td&gt; 
   &lt;td&gt;Same errors, locations, and messages as TS 5.9. Types printback in errors may display differently.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;JavaScript-specific inference and JSDoc&lt;/td&gt; 
   &lt;td&gt;in progress&lt;/td&gt; 
   &lt;td&gt;Mostly complete, but intentionally lacking some features. Declaration emit not complete.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;JSX&lt;/td&gt; 
   &lt;td&gt;done&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Declaration emit&lt;/td&gt; 
   &lt;td&gt;in progress&lt;/td&gt; 
   &lt;td&gt;Most common features are in place, but some edge cases and feature flags are still unhandled.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Emit (JS output)&lt;/td&gt; 
   &lt;td&gt;in progress&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;target: esnext&lt;/code&gt; well-supported, other targets may have gaps.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Watch mode&lt;/td&gt; 
   &lt;td&gt;prototype&lt;/td&gt; 
   &lt;td&gt;Watches files and rebuilds, but no incremental rechecking. Not optimized.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Build mode / project references&lt;/td&gt; 
   &lt;td&gt;done&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Incremental build&lt;/td&gt; 
   &lt;td&gt;done&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Language service (LSP)&lt;/td&gt; 
   &lt;td&gt;in progress&lt;/td&gt; 
   &lt;td&gt;Most functionality. More features coming soon.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;API&lt;/td&gt; 
   &lt;td&gt;not ready&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Definitions:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;done&lt;/strong&gt; aka "believed done": We're not currently aware of any deficits or major left work to do. OK to log bugs&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;in progress&lt;/strong&gt;: currently being worked on; some features may work and some might not. OK to log panics, but nothing else please&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;prototype&lt;/strong&gt;: proof-of-concept only; do not log bugs&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;not ready&lt;/strong&gt;: either haven't even started yet, or far enough from ready that you shouldn't bother messing with it yet&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Other Notes&lt;/h2&gt; 
&lt;p&gt;Long-term, we expect that this repo and its contents will be merged into &lt;code&gt;microsoft/TypeScript&lt;/code&gt;. As a result, the repo and issue tracker for typescript-go will eventually be closed, so treat discussions/issues accordingly.&lt;/p&gt; 
&lt;p&gt;For a list of intentional changes with respect to TypeScript 5.9, see CHANGES.md.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href="https://cla.opensource.microsoft.com"&gt;Contributor License Agreements&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; 
&lt;p&gt;This project has adopted the &lt;a href="https://opensource.microsoft.com/codeofconduct/"&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href="https://opensource.microsoft.com/codeofconduct/faq/"&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href="mailto:opencode@microsoft.com"&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; 
&lt;h2&gt;Trademarks&lt;/h2&gt; 
&lt;p&gt;This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow &lt;a href="https://www.microsoft.com/legal/intellectualproperty/trademarks/usage/general"&gt;Microsoft's Trademark &amp;amp; Brand Guidelines&lt;/a&gt;. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>vllm-project/semantic-router</title>
      <link>https://github.com/vllm-project/semantic-router</link>
      <description>&lt;p&gt;System Level Intelligent Router for Mixture-of-Models at Cloud, Data Center and Edge&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/vllm-project/semantic-router/main/website/static/img/code.png" alt="vLLM Semantic Router" width="100%" /&gt; 
 &lt;p&gt;&lt;a href="https://vllm-semantic-router.com"&gt;&lt;img src="https://img.shields.io/badge/docs-read%20the%20docs-blue" alt="Documentation" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/LLM-Semantic-Router"&gt;&lt;img src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Community-yellow" alt="Hugging Face" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/vllm-project/semantic-router/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/license-Apache%202.0-blue.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://crates.io/crates/candle-semantic-router"&gt;&lt;img src="https://img.shields.io/crates/v/candle-semantic-router.svg?sanitize=true" alt="Crates.io" /&gt;&lt;/a&gt; &lt;img src="https://github.com/vllm-project/semantic-router/workflows/Test%20And%20Build/badge.svg?sanitize=true" alt="Test And Build" /&gt; &lt;a href="https://deepwiki.com/vllm-project/semantic-router"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;üìö &lt;a href="https://vllm-semantic-router.com"&gt;Complete Documentation&lt;/a&gt; | üöÄ &lt;a href="https://vllm-semantic-router.com/docs/installation"&gt;Quick Start&lt;/a&gt; | üì£ &lt;a href="https://vllm-semantic-router.com/blog/"&gt;Blog&lt;/a&gt; | üìñ &lt;a href="https://vllm-semantic-router.com/publications/"&gt;Publications&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;em&gt;Latest News&lt;/em&gt; üî•&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;[2026/01/05] Iris v0.1 is Released: &lt;a href="https://blog.vllm.ai/2026/01/05/vllm-sr-iris.html"&gt;vLLM Semantic Router v0.1 Iris: The First Major Release&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[2025/12/16] Collaboration: &lt;a href="https://blog.vllm.ai/2025/12/16/vllm-sr-amd.html"&gt;AMD √ó vLLM Semantic Router: Building the System Intelligence Together&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[2025/12/15] New Blog: &lt;a href="https://blog.vllm.ai/2025/12/14/halugate.html"&gt;Token-Level Truth: Real-Time Hallucination Detection for Production LLMs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[2025/11/19] New Blog: &lt;a href="https://blog.vllm.ai/2025/11/19/signal-decision.html"&gt;Signal-Decision Driven Architecture: Reshaping Semantic Routing at Scale&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[2025/11/03] Our paper &lt;a href="https://arxiv.org/abs/2510.26835"&gt;Category-Aware Semantic Caching for Heterogeneous LLM Workloads&lt;/a&gt; published&lt;/li&gt; 
 &lt;li&gt;[2025/10/27] New Blog: &lt;a href="https://blog.vllm.ai/2025/10/27/semantic-router-modular.html"&gt;Scaling Semantic Routing with Extensible LoRA&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[2025/10/12] Our paper &lt;a href="https://arxiv.org/abs/2510.08731"&gt;When to Reason: Semantic Router for vLLM&lt;/a&gt; accepted by NeurIPS 2025 MLForSys.&lt;/li&gt; 
 &lt;li&gt;[2025/10/08] Collaboration: vLLM Semantic Router with &lt;a href="https://github.com/vllm-project/production-stack"&gt;vLLM Production Stack&lt;/a&gt; Team.&lt;/li&gt; 
 &lt;li&gt;[2025/09/01] Released the project: &lt;a href="https://blog.vllm.ai/2025/09/11/semantic-router.html"&gt;vLLM Semantic Router: Next Phase in LLM inference&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Goals&lt;/h2&gt; 
&lt;p&gt;We are building the &lt;strong&gt;System Level Intelligence&lt;/strong&gt; for Mixture-of-Models (MoM), bringing the &lt;strong&gt;Collective Intelligence&lt;/strong&gt; into &lt;strong&gt;LLM systems&lt;/strong&gt;, answering the following questions:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;How to capture the missing signals in request, response and context?&lt;/li&gt; 
 &lt;li&gt;How to combine the signals to make better decisions?&lt;/li&gt; 
 &lt;li&gt;How to collaborate more efficiently between different models?&lt;/li&gt; 
 &lt;li&gt;How to secure the real world and LLM system from jailbreaks, pii leaks, hallucinations?&lt;/li&gt; 
 &lt;li&gt;How to collect the valuable signals and build a self-learning system?&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/vllm-project/semantic-router/main/website/static/img/banner.png" alt="vLLM Semantic Router Banner" /&gt;&lt;/p&gt; 
&lt;h3&gt;Where it lives&lt;/h3&gt; 
&lt;p&gt;It lives between the real world and models:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/vllm-project/semantic-router/main/website/static/img/level.png" alt="level" /&gt;&lt;/p&gt; 
&lt;h3&gt;Architecture&lt;/h3&gt; 
&lt;p&gt;A quick overview of the current architecture:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/vllm-project/semantic-router/main/website/static/img/architecture.png" alt="architecture" /&gt;&lt;/p&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] We recommend that you setup a Python virtual environment to manage dependencies.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;$ python -m venv vsr
$ source vsr/bin/activate
$ pip install vllm-sr
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Installed successfully if you see the following help message:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;$ vllm-sr

       _ _     __  __       ____  ____
__   _| | |_ _|  \/  |     / ___||  _ \
\ \ / / | | | | |\/| |_____\___ \| |_) |
 \ V /| | | |_| | |  |_____|___) |  _ &amp;lt;
  \_/ |_|_|\__,_|_|  |     |____/|_| \_\

vLLM Semantic Router - Intelligent routing for vLLM

Usage: vllm-sr [OPTIONS] COMMAND [ARGS]...

  vLLM Semantic Router CLI - Intelligent routing and caching for vLLM
  endpoints.

Options:
  --version  Show version and exit.
  --help     Show this message and exit.

Commands:
  config  Print generated configuration.
  init    Initialize vLLM Semantic Router configuration.
  dashboard  Launch the vLLM Semantic Router dashboard.
  logs    Show logs from vLLM Semantic Router service.
  serve   Start vLLM Semantic Router.
  status  Show status of vLLM Semantic Router services.
  stop    Stop vLLM Semantic Router.
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] You can specify the HF_ENDPOINT, HF_TOKEN, and HF_HOME environment variables to configure the Hugging Face credentials.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Set environment variables (optional)
export HF_ENDPOINT=https://huggingface.co  # Or use mirror: https://hf-mirror.com
export HF_TOKEN=your_token_here  # Only for gated models
export HF_HOME=/path/to/cache  # Optional: custom cache directory

# Start the service - models download automatically
# Environment variables are automatically passed to the container
vllm-sr serve
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Configuration&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;File Descriptor Limits&lt;/strong&gt;: The CLI automatically sets file descriptor limits to 65,536 for Envoy proxy. For custom limits:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export VLLM_SR_NOFILE_LIMIT=100000  # Optional: custom limit (min: 8192)
vllm-sr serve
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/vllm-project/semantic-router/main/src/vllm-sr/README.md#configuration"&gt;vllm-sr README&lt;/a&gt; for detailed configuration options and troubleshooting.&lt;/p&gt; 
&lt;h2&gt;Documentation üìñ&lt;/h2&gt; 
&lt;p&gt;For comprehensive documentation including detailed setup instructions, architecture guides, and API references, visit:&lt;/p&gt; 
&lt;p&gt;Complete Documentation at Read the &lt;strong&gt;&lt;a href="https://vllm-semantic-router.com/"&gt;Docs&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;The documentation includes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://vllm-semantic-router.com/docs/installation/"&gt;Installation Guide&lt;/a&gt;&lt;/strong&gt; - Complete setup instructions&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://vllm-semantic-router.com/docs/overview/architecture/system-architecture/"&gt;System Architecture&lt;/a&gt;&lt;/strong&gt; - Technical deep dive&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://vllm-semantic-router.com/docs/training/training-overview/"&gt;Model Training&lt;/a&gt;&lt;/strong&gt; - How classification models work&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://vllm-semantic-router.com/docs/api/router/"&gt;API Reference&lt;/a&gt;&lt;/strong&gt; - Complete API documentation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Community üëã&lt;/h2&gt; 
&lt;p&gt;For questions, feedback, or to contribute, please join &lt;code&gt;#semantic-router&lt;/code&gt; channel in vLLM Slack.&lt;/p&gt; 
&lt;h3&gt;Community Meetings üìÖ&lt;/h3&gt; 
&lt;p&gt;We host bi-weekly community meetings to sync up with contributors across different time zones:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;First Tuesday of the month&lt;/strong&gt;: 9:00-10:00 AM EST (accommodates US EST, EU, and Asia Pacific contributors) 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://us05web.zoom.us/j/84122485631?pwd=BB88v03mMNLVHn60YzVk4PihuqBV9d.1"&gt;Zoom Link&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://us05web.zoom.us/meeting/tZAsdeuspj4sGdVraOOR4UaXSstrH2jjPYFq/calendar/google/add?meetingMasterEventId=4jjzUKSLSLiBHtIKZpGc3g"&gt;Google Calendar Invite&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://drive.google.com/file/d/15wO8cg0ZjNxdr8OtGiZyAgkSS8_Wry0J/view?usp=sharing"&gt;ics file&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Third Tuesday of the month&lt;/strong&gt;: 1:00-2:00 PM EST (accommodates US EST and California contributors) 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://us06web.zoom.us/j/86871492845?pwd=LcTtXm9gtGu23JeWqXxbnLLCCvbumB.1"&gt;Zoom Link&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://us05web.zoom.us/meeting/tZIlcOispzkiHtH2dlkWlLym68bEqvuf3MU5/calendar/google/add?meetingMasterEventId=PqWz2vk7TOCszPXqconGAA"&gt;Google Calendar Invite&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://drive.google.com/file/d/1T54mwYpXXoV9QfR76I56BFBPNbykSsTw/view?usp=sharing"&gt;ics file&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Meeting Recordings: &lt;a href="https://www.youtube.com/@vLLMSemanticRouter/videos"&gt;YouTube&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Join us to discuss the latest developments, share ideas, and collaborate on the project!&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you find Semantic Router helpful in your research or projects, please consider citing it:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@misc{semanticrouter2025,
  title={vLLM Semantic Router},
  author={vLLM Semantic Router Team},
  year={2025},
  howpublished={\url{https://github.com/vllm-project/semantic-router}},
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Star History üî•&lt;/h2&gt; 
&lt;p&gt;We opened the project at Aug 31, 2025. We love open source and collaboration ‚ù§Ô∏è&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.star-history.com/#vllm-project/semantic-router&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=vllm-project/semantic-router&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Sponsors üëã&lt;/h2&gt; 
&lt;p&gt;We are grateful to our sponsors who support us:&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;a href="https://www.amd.com"&gt;&lt;strong&gt;AMD&lt;/strong&gt;&lt;/a&gt; provides us with GPU resources and &lt;a href="https://www.amd.com/en/products/software/rocm.html"&gt;ROCm‚Ñ¢&lt;/a&gt; Software for training and researching the frontier router models, enhancing e2e testing, and building online models playground.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://www.amd.com"&gt; &lt;img src="https://raw.githubusercontent.com/vllm-project/semantic-router/main/website/static/img/amd-logo.svg?sanitize=true" alt="AMD" width="40%" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt;</description>
    </item>
    
    <item>
      <title>charmbracelet/crush</title>
      <link>https://github.com/charmbracelet/crush</link>
      <description>&lt;p&gt;Glamourous agentic coding for all üíò&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Crush&lt;/h1&gt; 
&lt;p align="center"&gt; &lt;a href="https://stuff.charm.sh/crush/charm-crush.png"&gt;&lt;img width="450" alt="Charm Crush Logo" src="https://github.com/user-attachments/assets/cf8ca3ce-8b02-43f0-9d0f-5a331488da4b" /&gt;&lt;/a&gt;&lt;br /&gt; &lt;a href="https://github.com/charmbracelet/crush/releases"&gt;&lt;img src="https://img.shields.io/github/release/charmbracelet/crush" alt="Latest Release" /&gt;&lt;/a&gt; &lt;a href="https://github.com/charmbracelet/crush/actions"&gt;&lt;img src="https://github.com/charmbracelet/crush/actions/workflows/build.yml/badge.svg?sanitize=true" alt="Build Status" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt;Your new coding bestie, now available in your favourite terminal.&lt;br /&gt;Your tools, your code, and your workflows, wired into your LLM of choice.&lt;/p&gt; 
&lt;p align="center"&gt;‰Ω†ÁöÑÊñ∞ÁºñÁ®ã‰ºô‰º¥ÔºåÁé∞Âú®Â∞±Âú®‰Ω†ÊúÄÁà±ÁöÑÁªàÁ´Ø‰∏≠„ÄÇ&lt;br /&gt;‰Ω†ÁöÑÂ∑•ÂÖ∑„ÄÅ‰ª£Á†ÅÂíåÂ∑•‰ΩúÊµÅÔºåÈÉΩ‰∏éÊÇ®ÈÄâÊã©ÁöÑ LLM Ê®°ÂûãÁ¥ßÂØÜÁõ∏Ëøû„ÄÇ&lt;/p&gt; 
&lt;p align="center"&gt;&lt;img width="800" alt="Crush Demo" src="https://github.com/user-attachments/assets/58280caf-851b-470a-b6f7-d5c4ea8a1968" /&gt;&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-Model:&lt;/strong&gt; choose from a wide range of LLMs or add your own via OpenAI- or Anthropic-compatible APIs&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Flexible:&lt;/strong&gt; switch LLMs mid-session while preserving context&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Session-Based:&lt;/strong&gt; maintain multiple work sessions and contexts per project&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;LSP-Enhanced:&lt;/strong&gt; Crush uses LSPs for additional context, just like you do&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Extensible:&lt;/strong&gt; add capabilities via MCPs (&lt;code&gt;http&lt;/code&gt;, &lt;code&gt;stdio&lt;/code&gt;, and &lt;code&gt;sse&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Works Everywhere:&lt;/strong&gt; first-class support in every terminal on macOS, Linux, Windows (PowerShell and WSL), Android, FreeBSD, OpenBSD, and NetBSD&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Industrial Grade:&lt;/strong&gt; built on the Charm ecosystem, powering 25k+ applications, from leading open source projects to business-critical infrastructure&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;Use a package manager:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Homebrew
brew install charmbracelet/tap/crush

# NPM
npm install -g @charmland/crush

# Arch Linux (btw)
yay -S crush-bin

# Nix
nix run github:numtide/nix-ai-tools#crush

# FreeBSD
pkg install crush
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Windows users:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Winget
winget install charmbracelet.crush

# Scoop
scoop bucket add charm https://github.com/charmbracelet/scoop-bucket.git
scoop install crush
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Nix (NUR)&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;Crush is available via the official Charm &lt;a href="https://github.com/nix-community/NUR"&gt;NUR&lt;/a&gt; in &lt;code&gt;nur.repos.charmbracelet.crush&lt;/code&gt;, which is the most up-to-date way to get Crush in Nix.&lt;/p&gt; 
 &lt;p&gt;You can also try out Crush via the NUR with &lt;code&gt;nix-shell&lt;/code&gt;:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Add the NUR channel.
nix-channel --add https://github.com/nix-community/NUR/archive/main.tar.gz nur
nix-channel --update

# Get Crush in a Nix shell.
nix-shell -p '(import &amp;lt;nur&amp;gt; { pkgs = import &amp;lt;nixpkgs&amp;gt; {}; }).repos.charmbracelet.crush'
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;NixOS &amp;amp; Home Manager Module Usage via NUR&lt;/h3&gt; 
 &lt;p&gt;Crush provides NixOS and Home Manager modules via NUR. You can use these modules directly in your flake by importing them from NUR. Since it auto detects whether its a home manager or nixos context you can use the import the exact same way :)&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-nix"&gt;{
  inputs = {
    nixpkgs.url = "github:NixOS/nixpkgs/nixos-unstable";
    nur.url = "github:nix-community/NUR";
  };

  outputs = { self, nixpkgs, nur, ... }: {
    nixosConfigurations.your-hostname = nixpkgs.lib.nixosSystem {
      system = "x86_64-linux";
      modules = [
        nur.modules.nixos.default
        nur.repos.charmbracelet.modules.crush
        {
          programs.crush = {
            enable = true;
            settings = {
              providers = {
                openai = {
                  id = "openai";
                  name = "OpenAI";
                  base_url = "https://api.openai.com/v1";
                  type = "openai";
                  api_key = "sk-fake123456789abcdef...";
                  models = [
                    {
                      id = "gpt-4";
                      name = "GPT-4";
                    }
                  ];
                };
              };
              lsp = {
                go = { command = "gopls"; enabled = true; };
                nix = { command = "nil"; enabled = true; };
              };
              options = {
                context_paths = [ "/etc/nixos/configuration.nix" ];
                tui = { compact_mode = true; };
                debug = false;
              };
            };
          };
        }
      ];
    };
  };
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Debian/Ubuntu&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;sudo mkdir -p /etc/apt/keyrings
curl -fsSL https://repo.charm.sh/apt/gpg.key | sudo gpg --dearmor -o /etc/apt/keyrings/charm.gpg
echo "deb [signed-by=/etc/apt/keyrings/charm.gpg] https://repo.charm.sh/apt/ * *" | sudo tee /etc/apt/sources.list.d/charm.list
sudo apt update &amp;amp;&amp;amp; sudo apt install crush
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Fedora/RHEL&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;echo '[charm]
name=Charm
baseurl=https://repo.charm.sh/yum/
enabled=1
gpgcheck=1
gpgkey=https://repo.charm.sh/yum/gpg.key' | sudo tee /etc/yum.repos.d/charm.repo
sudo yum install crush
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p&gt;Or, download it:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/charmbracelet/crush/releases"&gt;Packages&lt;/a&gt; are available in Debian and RPM formats&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/charmbracelet/crush/releases"&gt;Binaries&lt;/a&gt; are available for Linux, macOS, Windows, FreeBSD, OpenBSD, and NetBSD&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Or just install it with Go:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;go install github.com/charmbracelet/crush@latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING] Productivity may increase when using Crush and you may find yourself nerd sniped when first using the application. If the symptoms persist, join the &lt;a href="https://charm.land/discord"&gt;Discord&lt;/a&gt; and nerd snipe the rest of us.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;The quickest way to get started is to grab an API key for your preferred provider such as Anthropic, OpenAI, Groq, or OpenRouter and just start Crush. You'll be prompted to enter your API key.&lt;/p&gt; 
&lt;p&gt;That said, you can also set environment variables for preferred providers.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Environment Variable&lt;/th&gt; 
   &lt;th&gt;Provider&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ANTHROPIC_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Anthropic&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENAI_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;OpenAI&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENROUTER_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;OpenRouter&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;GEMINI_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Google Gemini&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;CEREBRAS_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Cerebras&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;HF_TOKEN&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Huggingface Inference&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;VERTEXAI_PROJECT&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Google Cloud VertexAI (Gemini)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;VERTEXAI_LOCATION&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Google Cloud VertexAI (Gemini)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;GROQ_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Groq&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;AWS_ACCESS_KEY_ID&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Amazon Bedrock (Claude)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;AWS_SECRET_ACCESS_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Amazon Bedrock (Claude)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;AWS_REGION&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Amazon Bedrock (Claude)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;AWS_PROFILE&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Amazon Bedrock (Custom Profile)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;AWS_BEARER_TOKEN_BEDROCK&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Amazon Bedrock&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;AZURE_OPENAI_API_ENDPOINT&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Azure OpenAI models&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;AZURE_OPENAI_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Azure OpenAI models (optional when using Entra ID)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;AZURE_OPENAI_API_VERSION&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Azure OpenAI models&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;By the Way&lt;/h3&gt; 
&lt;p&gt;Is there a provider you‚Äôd like to see in Crush? Is there an existing model that needs an update?&lt;/p&gt; 
&lt;p&gt;Crush‚Äôs default model listing is managed in &lt;a href="https://github.com/charmbracelet/catwalk"&gt;Catwalk&lt;/a&gt;, a community-supported, open source repository of Crush-compatible models, and you‚Äôre welcome to contribute.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/charmbracelet/catwalk"&gt;&lt;img width="174" height="174" alt="Catwalk Badge" src="https://github.com/user-attachments/assets/95b49515-fe82-4409-b10d-5beb0873787d" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Configuration&lt;/h2&gt; 
&lt;p&gt;Crush runs great with no configuration. That said, if you do need or want to customize Crush, configuration can be added either local to the project itself, or globally, with the following priority:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;code&gt;.crush.json&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;crush.json&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;$HOME/.config/crush/crush.json&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Configuration itself is stored as a JSON object:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "this-setting": { "this": "that" },
  "that-setting": ["ceci", "cela"]
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;As an additional note, Crush also stores ephemeral data, such as application state, in one additional location:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Unix
$HOME/.local/share/crush/crush.json

# Windows
%LOCALAPPDATA%\crush\crush.json
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] You can override the user and data config locations by setting:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;CRUSH_GLOBAL_CONFIG&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;CRUSH_GLOBAL_DATA&lt;/code&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;LSPs&lt;/h3&gt; 
&lt;p&gt;Crush can use LSPs for additional context to help inform its decisions, just like you would. LSPs can be added manually like so:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "$schema": "https://charm.land/crush.json",
  "lsp": {
    "go": {
      "command": "gopls",
      "env": {
        "GOTOOLCHAIN": "go1.24.5"
      }
    },
    "typescript": {
      "command": "typescript-language-server",
      "args": ["--stdio"]
    },
    "nix": {
      "command": "nil"
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;MCPs&lt;/h3&gt; 
&lt;p&gt;Crush also supports Model Context Protocol (MCP) servers through three transport types: &lt;code&gt;stdio&lt;/code&gt; for command-line servers, &lt;code&gt;http&lt;/code&gt; for HTTP endpoints, and &lt;code&gt;sse&lt;/code&gt; for Server-Sent Events. Environment variable expansion is supported using &lt;code&gt;$(echo $VAR)&lt;/code&gt; syntax.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "$schema": "https://charm.land/crush.json",
  "mcp": {
    "filesystem": {
      "type": "stdio",
      "command": "node",
      "args": ["/path/to/mcp-server.js"],
      "timeout": 120,
      "disabled": false,
      "disabled_tools": ["some-tool-name"],
      "env": {
        "NODE_ENV": "production"
      }
    },
    "github": {
      "type": "http",
      "url": "https://api.githubcopilot.com/mcp/",
      "timeout": 120,
      "disabled": false,
      "disabled_tools": ["create_issue", "create_pull_request"],
      "headers": {
        "Authorization": "Bearer $GH_PAT"
      }
    },
    "streaming-service": {
      "type": "sse",
      "url": "https://example.com/mcp/sse",
      "timeout": 120,
      "disabled": false,
      "headers": {
        "API-Key": "$(echo $API_KEY)"
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Ignoring Files&lt;/h3&gt; 
&lt;p&gt;Crush respects &lt;code&gt;.gitignore&lt;/code&gt; files by default, but you can also create a &lt;code&gt;.crushignore&lt;/code&gt; file to specify additional files and directories that Crush should ignore. This is useful for excluding files that you want in version control but don't want Crush to consider when providing context.&lt;/p&gt; 
&lt;p&gt;The &lt;code&gt;.crushignore&lt;/code&gt; file uses the same syntax as &lt;code&gt;.gitignore&lt;/code&gt; and can be placed in the root of your project or in subdirectories.&lt;/p&gt; 
&lt;h3&gt;Allowing Tools&lt;/h3&gt; 
&lt;p&gt;By default, Crush will ask you for permission before running tool calls. If you'd like, you can allow tools to be executed without prompting you for permissions. Use this with care.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "$schema": "https://charm.land/crush.json",
  "permissions": {
    "allowed_tools": [
      "view",
      "ls",
      "grep",
      "edit",
      "mcp_context7_get-library-doc"
    ]
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also skip all permission prompts entirely by running Crush with the &lt;code&gt;--yolo&lt;/code&gt; flag. Be very, very careful with this feature.&lt;/p&gt; 
&lt;h3&gt;Disabling Built-In Tools&lt;/h3&gt; 
&lt;p&gt;If you'd like to prevent Crush from using certain built-in tools entirely, you can disable them via the &lt;code&gt;options.disabled_tools&lt;/code&gt; list. Disabled tools are completely hidden from the agent.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "$schema": "https://charm.land/crush.json",
  "options": {
    "disabled_tools": [
      "bash",
      "sourcegraph"
    ]
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To disable tools from MCP servers, see the &lt;a href="https://raw.githubusercontent.com/charmbracelet/crush/main/#mcps"&gt;MCP config section&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Agent Skills&lt;/h3&gt; 
&lt;p&gt;Crush supports the &lt;a href="https://agentskills.io"&gt;Agent Skills&lt;/a&gt; open standard for extending agent capabilities with reusable skill packages. Skills are folders containing a &lt;code&gt;SKILL.md&lt;/code&gt; file with instructions that Crush can discover and activate on demand.&lt;/p&gt; 
&lt;p&gt;Skills are discovered from:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;~/.config/crush/skills/&lt;/code&gt; on Unix (default, can be overridden with &lt;code&gt;CRUSH_SKILLS_DIR&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;%LOCALAPPDATA%\crush\skills\&lt;/code&gt; on Windows (default, can be overridden with &lt;code&gt;CRUSH_SKILLS_DIR&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Additional paths configured via &lt;code&gt;options.skills_paths&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-jsonc"&gt;{
  "$schema": "https://charm.land/crush.json",
  "options": {
    "skills_paths": [
      "~/.config/crush/skills", // Windows: "%LOCALAPPDATA%\\crush\\skills",
      "./project-skills"
    ]
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can get started with example skills from &lt;a href="https://github.com/anthropics/skills"&gt;anthropics/skills&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Unix
mkdir -p ~/.config/crush/skills
cd ~/.config/crush/skills
git clone https://github.com/anthropics/skills.git _temp
mv _temp/skills/* . &amp;amp;&amp;amp; rm -rf _temp
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-powershell"&gt;# Windows (PowerShell)
mkdir -Force "$env:LOCALAPPDATA\crush\skills"
cd "$env:LOCALAPPDATA\crush\skills"
git clone https://github.com/anthropics/skills.git _temp
mv _temp/skills/* . ; rm -r -force _temp
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Initialization&lt;/h3&gt; 
&lt;p&gt;When you initialize a project, Crush analyzes your codebase and creates a context file that helps it work more effectively in future sessions. By default, this file is named &lt;code&gt;AGENTS.md&lt;/code&gt;, but you can customize the name and location with the &lt;code&gt;initialize_as&lt;/code&gt; option:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "$schema": "https://charm.land/crush.json",
  "options": {
    "initialize_as": "AGENTS.md"
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This is useful if you prefer a different naming convention or want to place the file in a specific directory (e.g., &lt;code&gt;CRUSH.md&lt;/code&gt; or &lt;code&gt;docs/LLMs.md&lt;/code&gt;). Crush will fill the file with project-specific context like build commands, code patterns, and conventions it discovered during initialization.&lt;/p&gt; 
&lt;h3&gt;Attribution Settings&lt;/h3&gt; 
&lt;p&gt;By default, Crush adds attribution information to Git commits and pull requests it creates. You can customize this behavior with the &lt;code&gt;attribution&lt;/code&gt; option:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "$schema": "https://charm.land/crush.json",
  "options": {
    "attribution": {
      "trailer_style": "co-authored-by",
      "generated_with": true
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;trailer_style&lt;/code&gt;: Controls the attribution trailer added to commit messages (default: &lt;code&gt;assisted-by&lt;/code&gt;) 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;assisted-by&lt;/code&gt;: Adds &lt;code&gt;Assisted-by: [Model Name] via Crush &amp;lt;crush@charm.land&amp;gt;&lt;/code&gt; (includes the model name)&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;co-authored-by&lt;/code&gt;: Adds &lt;code&gt;Co-Authored-By: Crush &amp;lt;crush@charm.land&amp;gt;&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;none&lt;/code&gt;: No attribution trailer&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;generated_with&lt;/code&gt;: When true (default), adds &lt;code&gt;üíò Generated with Crush&lt;/code&gt; line to commit messages and PR descriptions&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Custom Providers&lt;/h3&gt; 
&lt;p&gt;Crush supports custom provider configurations for both OpenAI-compatible and Anthropic-compatible APIs.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Note that we support two "types" for OpenAI. Make sure to choose the right one to ensure the best experience!&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;code&gt;openai&lt;/code&gt; should be used when proxying or routing requests through OpenAI.&lt;/li&gt; 
  &lt;li&gt;&lt;code&gt;openai-compat&lt;/code&gt; should be used when using non-OpenAI providers that have OpenAI-compatible APIs.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;OpenAI-Compatible APIs&lt;/h4&gt; 
&lt;p&gt;Here‚Äôs an example configuration for Deepseek, which uses an OpenAI-compatible API. Don't forget to set &lt;code&gt;DEEPSEEK_API_KEY&lt;/code&gt; in your environment.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "$schema": "https://charm.land/crush.json",
  "providers": {
    "deepseek": {
      "type": "openai-compat",
      "base_url": "https://api.deepseek.com/v1",
      "api_key": "$DEEPSEEK_API_KEY",
      "models": [
        {
          "id": "deepseek-chat",
          "name": "Deepseek V3",
          "cost_per_1m_in": 0.27,
          "cost_per_1m_out": 1.1,
          "cost_per_1m_in_cached": 0.07,
          "cost_per_1m_out_cached": 1.1,
          "context_window": 64000,
          "default_max_tokens": 5000
        }
      ]
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Anthropic-Compatible APIs&lt;/h4&gt; 
&lt;p&gt;Custom Anthropic-compatible providers follow this format:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "$schema": "https://charm.land/crush.json",
  "providers": {
    "custom-anthropic": {
      "type": "anthropic",
      "base_url": "https://api.anthropic.com/v1",
      "api_key": "$ANTHROPIC_API_KEY",
      "extra_headers": {
        "anthropic-version": "2023-06-01"
      },
      "models": [
        {
          "id": "claude-sonnet-4-20250514",
          "name": "Claude Sonnet 4",
          "cost_per_1m_in": 3,
          "cost_per_1m_out": 15,
          "cost_per_1m_in_cached": 3.75,
          "cost_per_1m_out_cached": 0.3,
          "context_window": 200000,
          "default_max_tokens": 50000,
          "can_reason": true,
          "supports_attachments": true
        }
      ]
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Amazon Bedrock&lt;/h3&gt; 
&lt;p&gt;Crush currently supports running Anthropic models through Bedrock, with caching disabled.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;A Bedrock provider will appear once you have AWS configured, i.e. &lt;code&gt;aws configure&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Crush also expects the &lt;code&gt;AWS_REGION&lt;/code&gt; or &lt;code&gt;AWS_DEFAULT_REGION&lt;/code&gt; to be set&lt;/li&gt; 
 &lt;li&gt;To use a specific AWS profile set &lt;code&gt;AWS_PROFILE&lt;/code&gt; in your environment, i.e. &lt;code&gt;AWS_PROFILE=myprofile crush&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Alternatively to &lt;code&gt;aws configure&lt;/code&gt;, you can also just set &lt;code&gt;AWS_BEARER_TOKEN_BEDROCK&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Vertex AI Platform&lt;/h3&gt; 
&lt;p&gt;Vertex AI will appear in the list of available providers when &lt;code&gt;VERTEXAI_PROJECT&lt;/code&gt; and &lt;code&gt;VERTEXAI_LOCATION&lt;/code&gt; are set. You will also need to be authenticated:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;gcloud auth application-default login
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To add specific models to the configuration, configure as such:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "$schema": "https://charm.land/crush.json",
  "providers": {
    "vertexai": {
      "models": [
        {
          "id": "claude-sonnet-4@20250514",
          "name": "VertexAI Sonnet 4",
          "cost_per_1m_in": 3,
          "cost_per_1m_out": 15,
          "cost_per_1m_in_cached": 3.75,
          "cost_per_1m_out_cached": 0.3,
          "context_window": 200000,
          "default_max_tokens": 50000,
          "can_reason": true,
          "supports_attachments": true
        }
      ]
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Local Models&lt;/h3&gt; 
&lt;p&gt;Local models can also be configured via OpenAI-compatible API. Here are two common examples:&lt;/p&gt; 
&lt;h4&gt;Ollama&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "providers": {
    "ollama": {
      "name": "Ollama",
      "base_url": "http://localhost:11434/v1/",
      "type": "openai-compat",
      "models": [
        {
          "name": "Qwen 3 30B",
          "id": "qwen3:30b",
          "context_window": 256000,
          "default_max_tokens": 20000
        }
      ]
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;LM Studio&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "providers": {
    "lmstudio": {
      "name": "LM Studio",
      "base_url": "http://localhost:1234/v1/",
      "type": "openai-compat",
      "models": [
        {
          "name": "Qwen 3 30B",
          "id": "qwen/qwen3-30b-a3b-2507",
          "context_window": 256000,
          "default_max_tokens": 20000
        }
      ]
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Logging&lt;/h2&gt; 
&lt;p&gt;Sometimes you need to look at logs. Luckily, Crush logs all sorts of stuff. Logs are stored in &lt;code&gt;./.crush/logs/crush.log&lt;/code&gt; relative to the project.&lt;/p&gt; 
&lt;p&gt;The CLI also contains some helper commands to make perusing recent logs easier:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Print the last 1000 lines
crush logs

# Print the last 500 lines
crush logs --tail 500

# Follow logs in real time
crush logs --follow
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Want more logging? Run &lt;code&gt;crush&lt;/code&gt; with the &lt;code&gt;--debug&lt;/code&gt; flag, or enable it in the config:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "$schema": "https://charm.land/crush.json",
  "options": {
    "debug": true,
    "debug_lsp": true
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Provider Auto-Updates&lt;/h2&gt; 
&lt;p&gt;By default, Crush automatically checks for the latest and greatest list of providers and models from &lt;a href="https://github.com/charmbracelet/catwalk"&gt;Catwalk&lt;/a&gt;, the open source Crush provider database. This means that when new providers and models are available, or when model metadata changes, Crush automatically updates your local configuration.&lt;/p&gt; 
&lt;h3&gt;Disabling automatic provider updates&lt;/h3&gt; 
&lt;p&gt;For those with restricted internet access, or those who prefer to work in air-gapped environments, this might not be want you want, and this feature can be disabled.&lt;/p&gt; 
&lt;p&gt;To disable automatic provider updates, set &lt;code&gt;disable_provider_auto_update&lt;/code&gt; into your &lt;code&gt;crush.json&lt;/code&gt; config:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "$schema": "https://charm.land/crush.json",
  "options": {
    "disable_provider_auto_update": true
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or set the &lt;code&gt;CRUSH_DISABLE_PROVIDER_AUTO_UPDATE&lt;/code&gt; environment variable:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export CRUSH_DISABLE_PROVIDER_AUTO_UPDATE=1
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Manually updating providers&lt;/h3&gt; 
&lt;p&gt;Manually updating providers is possible with the &lt;code&gt;crush update-providers&lt;/code&gt; command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Update providers remotely from Catwalk.
crush update-providers

# Update providers from a custom Catwalk base URL.
crush update-providers https://example.com/

# Update providers from a local file.
crush update-providers /path/to/local-providers.json

# Reset providers to the embedded version, embedded at crush at build time.
crush update-providers embedded

# For more info:
crush update-providers --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Metrics&lt;/h2&gt; 
&lt;p&gt;Crush records pseudonymous usage metrics (tied to a device-specific hash), which maintainers rely on to inform development and support priorities. The metrics include solely usage metadata; prompts and responses are NEVER collected.&lt;/p&gt; 
&lt;p&gt;Details on exactly what‚Äôs collected are in the source code (&lt;a href="https://github.com/charmbracelet/crush/tree/main/internal/event"&gt;here&lt;/a&gt; and &lt;a href="https://github.com/charmbracelet/crush/raw/main/internal/llm/agent/event.go"&gt;here&lt;/a&gt;).&lt;/p&gt; 
&lt;p&gt;You can opt out of metrics collection at any time by setting the environment variable by setting the following in your environment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export CRUSH_DISABLE_METRICS=1
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or by setting the following in your config:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "options": {
    "disable_metrics": true
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Crush also respects the &lt;a href="https://consoledonottrack.com"&gt;&lt;code&gt;DO_NOT_TRACK&lt;/code&gt;&lt;/a&gt; convention which can be enabled via &lt;code&gt;export DO_NOT_TRACK=1&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;See the &lt;a href="https://github.com/charmbracelet/crush?tab=contributing-ov-file#contributing"&gt;contributing guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Whatcha think?&lt;/h2&gt; 
&lt;p&gt;We‚Äôd love to hear your thoughts on this project. Need help? We gotchu. You can find us on:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://twitter.com/charmcli"&gt;Twitter&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://charm.land/slack"&gt;Slack&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://charm.land/discord"&gt;Discord&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://mastodon.social/@charmcli"&gt;The Fediverse&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://bsky.app/profile/charm.land"&gt;Bluesky&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/charmbracelet/crush/raw/main/LICENSE.md"&gt;FSL-1.1-MIT&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;Part of &lt;a href="https://charm.land"&gt;Charm&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://charm.land/"&gt;&lt;img alt="The Charm logo" width="400" src="https://stuff.charm.sh/charm-banner-next.jpg" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;!--prettier-ignore--&gt; 
&lt;p&gt;CharmÁÉ≠Áà±ÂºÄÊ∫ê ‚Ä¢ Charm loves open source&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>coder/coder</title>
      <link>https://github.com/coder/coder</link>
      <description>&lt;p&gt;Secure environments for developers and their agents&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;a href="https://coder.com#gh-light-mode-only"&gt; &lt;img src="https://raw.githubusercontent.com/coder/coder/main/docs/images/logo-black.png" alt="Coder Logo Light" style="width: 128px" /&gt; &lt;/a&gt; 
 &lt;a href="https://coder.com#gh-dark-mode-only"&gt; &lt;img src="https://raw.githubusercontent.com/coder/coder/main/docs/images/logo-white.png" alt="Coder Logo Dark" style="width: 128px" /&gt; &lt;/a&gt; 
 &lt;h1&gt; Self-Hosted Cloud Development Environments &lt;/h1&gt; 
 &lt;a href="https://coder.com#gh-light-mode-only"&gt; &lt;img src="https://raw.githubusercontent.com/coder/coder/main/docs/images/banner-black.png" alt="Coder Banner Light" style="width: 650px" /&gt; &lt;/a&gt; 
 &lt;a href="https://coder.com#gh-dark-mode-only"&gt; &lt;img src="https://raw.githubusercontent.com/coder/coder/main/docs/images/banner-white.png" alt="Coder Banner Dark" style="width: 650px" /&gt; &lt;/a&gt; 
 &lt;br /&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/coder/coder/main/#quickstart"&gt;Quickstart&lt;/a&gt; | &lt;a href="https://coder.com/docs"&gt;Docs&lt;/a&gt; | &lt;a href="https://coder.com/why"&gt;Why Coder&lt;/a&gt; | &lt;a href="https://coder.com/pricing#compare-plans"&gt;Premium&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://discord.gg/coder"&gt;&lt;img src="https://img.shields.io/discord/747933592273027093?label=discord" alt="discord" /&gt;&lt;/a&gt; &lt;a href="https://github.com/coder/coder/releases/latest"&gt;&lt;img src="https://img.shields.io/github/v/release/coder/coder" alt="release" /&gt;&lt;/a&gt; &lt;a href="https://pkg.go.dev/github.com/coder/coder"&gt;&lt;img src="https://pkg.go.dev/badge/github.com/coder/coder.svg?sanitize=true" alt="godoc" /&gt;&lt;/a&gt; &lt;a href="https://goreportcard.com/report/github.com/coder/coder/v2"&gt;&lt;img src="https://goreportcard.com/badge/github.com/coder/coder/v2" alt="Go Report Card" /&gt;&lt;/a&gt; &lt;a href="https://www.bestpractices.dev/projects/9511"&gt;&lt;img src="https://www.bestpractices.dev/projects/9511/badge" alt="OpenSSF Best Practices" /&gt;&lt;/a&gt; &lt;a href="https://scorecard.dev/viewer/?uri=github.com%2Fcoder%2Fcoder"&gt;&lt;img src="https://api.securityscorecards.dev/projects/github.com/coder/coder/badge" alt="OpenSSF Scorecard" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/coder/coder/main/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/coder/coder" alt="license" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;a href="https://coder.com"&gt;Coder&lt;/a&gt; enables organizations to set up development environments in their public or private cloud infrastructure. Cloud development environments are defined with Terraform, connected through a secure high-speed Wireguard¬Æ tunnel, and automatically shut down when not used to save on costs. Coder gives engineering teams the flexibility to use the cloud for workloads most beneficial to them.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Define cloud development environments in Terraform 
  &lt;ul&gt; 
   &lt;li&gt;EC2 VMs, Kubernetes Pods, Docker Containers, etc.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Automatically shutdown idle resources to save on costs&lt;/li&gt; 
 &lt;li&gt;Onboard developers in seconds instead of days&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/coder/coder/main/docs/images/hero-image.png" alt="Coder Hero Image" /&gt; &lt;/p&gt; 
&lt;h2&gt;Quickstart&lt;/h2&gt; 
&lt;p&gt;The most convenient way to try Coder is to install it on your local machine and experiment with provisioning cloud development environments using Docker (works on Linux, macOS, and Windows).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# First, install Coder
curl -L https://coder.com/install.sh | sh

# Start the Coder server (caches data in ~/.cache/coder)
coder server

# Navigate to http://localhost:3000 to create your initial user,
# create a Docker template and provision a workspace
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Install&lt;/h2&gt; 
&lt;p&gt;The easiest way to install Coder is to use our &lt;a href="https://github.com/coder/coder/raw/main/install.sh"&gt;install script&lt;/a&gt; for Linux and macOS. For Windows, use the latest &lt;code&gt;..._installer.exe&lt;/code&gt; file from GitHub Releases.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;curl -L https://coder.com/install.sh | sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can run the install script with &lt;code&gt;--dry-run&lt;/code&gt; to see the commands that will be used to install without executing them. Run the install script with &lt;code&gt;--help&lt;/code&gt; for additional flags.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;See &lt;a href="https://coder.com/docs/install"&gt;install&lt;/a&gt; for additional methods.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Once installed, you can start a production deployment with a single command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# Automatically sets up an external access URL on *.try.coder.app
coder server

# Requires a PostgreSQL instance (version 13 or higher) and external access URL
coder server --postgres-url &amp;lt;url&amp;gt; --access-url &amp;lt;url&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Use &lt;code&gt;coder --help&lt;/code&gt; to get a list of flags and environment variables. Use our &lt;a href="https://coder.com/docs/install"&gt;install guides&lt;/a&gt; for a complete walkthrough.&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;Browse our docs &lt;a href="https://coder.com/docs"&gt;here&lt;/a&gt; or visit a specific section below:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://coder.com/docs/templates"&gt;&lt;strong&gt;Templates&lt;/strong&gt;&lt;/a&gt;: Templates are written in Terraform and describe the infrastructure for workspaces&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://coder.com/docs/workspaces"&gt;&lt;strong&gt;Workspaces&lt;/strong&gt;&lt;/a&gt;: Workspaces contain the IDEs, dependencies, and configuration information needed for software development&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://coder.com/docs/ides"&gt;&lt;strong&gt;IDEs&lt;/strong&gt;&lt;/a&gt;: Connect your existing editor to a workspace&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://coder.com/docs/admin"&gt;&lt;strong&gt;Administration&lt;/strong&gt;&lt;/a&gt;: Learn how to operate Coder&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://coder.com/pricing#compare-plans"&gt;&lt;strong&gt;Premium&lt;/strong&gt;&lt;/a&gt;: Learn about our paid features built for large teams&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Support&lt;/h2&gt; 
&lt;p&gt;Feel free to &lt;a href="https://github.com/coder/coder/issues/new"&gt;open an issue&lt;/a&gt; if you have questions, run into bugs, or have a feature request.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://discord.gg/coder"&gt;Join our Discord&lt;/a&gt; to provide feedback on in-progress features and chat with the community using Coder!&lt;/p&gt; 
&lt;h2&gt;Integrations&lt;/h2&gt; 
&lt;p&gt;We are always working on new integrations. Please feel free to open an issue and ask for an integration. Contributions are welcome in any official or community repositories.&lt;/p&gt; 
&lt;h3&gt;Official&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://marketplace.visualstudio.com/items?itemName=coder.coder-remote"&gt;&lt;strong&gt;VS Code Extension&lt;/strong&gt;&lt;/a&gt;: Open any Coder workspace in VS Code with a single click&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://plugins.jetbrains.com/plugin/26968-coder"&gt;&lt;strong&gt;JetBrains Toolbox Plugin&lt;/strong&gt;&lt;/a&gt;: Open any Coder workspace from JetBrains Toolbox with a single click&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://plugins.jetbrains.com/plugin/19620-coder"&gt;&lt;strong&gt;JetBrains Gateway Plugin&lt;/strong&gt;&lt;/a&gt;: Open any Coder workspace in JetBrains Gateway with a single click&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/coder/envbuilder"&gt;&lt;strong&gt;Dev Container Builder&lt;/strong&gt;&lt;/a&gt;: Build development environments using &lt;code&gt;devcontainer.json&lt;/code&gt; on Docker, Kubernetes, and OpenShift&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://registry.coder.com"&gt;&lt;strong&gt;Coder Registry&lt;/strong&gt;&lt;/a&gt;: Build and extend development environments with common use-cases&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/coder/coder-logstream-kube"&gt;&lt;strong&gt;Kubernetes Log Stream&lt;/strong&gt;&lt;/a&gt;: Stream Kubernetes Pod events to the Coder startup logs&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/coder/code-marketplace"&gt;&lt;strong&gt;Self-Hosted VS Code Extension Marketplace&lt;/strong&gt;&lt;/a&gt;: A private extension marketplace that works in restricted or airgapped networks integrating with &lt;a href="https://github.com/coder/code-server"&gt;code-server&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/marketplace/actions/setup-coder"&gt;&lt;strong&gt;Setup Coder&lt;/strong&gt;&lt;/a&gt;: An action to setup coder CLI in GitHub workflows.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Community&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ElliotG/coder-oss-tf"&gt;&lt;strong&gt;Provision Coder with Terraform&lt;/strong&gt;&lt;/a&gt;: Provision Coder on Google GKE, Azure AKS, AWS EKS, DigitalOcean DOKS, IBMCloud K8s, OVHCloud K8s, and Scaleway K8s Kapsule with Terraform&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/marketplace/actions/update-coder-template"&gt;&lt;strong&gt;Coder Template GitHub Action&lt;/strong&gt;&lt;/a&gt;: A GitHub Action that updates Coder templates&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We are always happy to see new contributors to Coder. If you are new to the Coder codebase, we have &lt;a href="https://coder.com/docs/CONTRIBUTING"&gt;a guide on how to get started&lt;/a&gt;. We'd love to see your contributions!&lt;/p&gt; 
&lt;h2&gt;Hiring&lt;/h2&gt; 
&lt;p&gt;Apply &lt;a href="https://jobs.ashbyhq.com/coder?utm_source=github&amp;amp;utm_medium=readme&amp;amp;utm_campaign=unknown"&gt;here&lt;/a&gt; if you're interested in joining our team.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>cilium/cilium</title>
      <link>https://github.com/cilium/cilium</link>
      <description>&lt;p&gt;eBPF-based Networking, Security, and Observability&lt;/p&gt;&lt;hr&gt;&lt;p&gt;.. raw:: html&lt;/p&gt; 
&lt;picture&gt; 
 &lt;source media="(prefers-color-scheme: light)" srcset="https://cdn.jsdelivr.net/gh/cilium/cilium@main/Documentation/images/logo.png" width="350" alt="Cilium Logo" /&gt; 
 &lt;img src="https://cdn.jsdelivr.net/gh/cilium/cilium@main/Documentation/images/logo-dark.png" width="350" alt="Cilium Logo" /&gt; 
&lt;/picture&gt; 
&lt;p&gt;|cii| |go-report| |clomonitor| |artifacthub| |slack| |go-doc| |rtd| |apache| |bsd| |gpl| |fossa| |gateway-api| |codespaces|&lt;/p&gt; 
&lt;p&gt;Cilium is a networking, observability, and security solution with an eBPF-based dataplane. It provides a simple flat Layer 3 network with the ability to span multiple clusters in either a native routing or overlay mode. It is L7-protocol aware and can enforce network policies on L3-L7 using an identity based security model that is decoupled from network addressing.&lt;/p&gt; 
&lt;p&gt;Cilium implements distributed load balancing for traffic between pods and to external services, and is able to fully replace kube-proxy, using efficient hash tables in eBPF allowing for almost unlimited scale. It also supports advanced functionality like integrated ingress and egress gateway, bandwidth management and service mesh, and provides deep network and security visibility and monitoring.&lt;/p&gt; 
&lt;p&gt;A new Linux kernel technology called eBPF_ is at the foundation of Cilium. It supports dynamic insertion of eBPF bytecode into the Linux kernel at various integration points such as: network IO, application sockets, and tracepoints to implement security, networking and visibility logic. eBPF is highly efficient and flexible. To learn more about eBPF, visit &lt;code&gt;eBPF.io&lt;/code&gt;_.&lt;/p&gt; 
&lt;p&gt;.. image:: Documentation/images/cilium-overview.png :alt: Overview of Cilium features for networking, observability, service mesh, and runtime security&lt;/p&gt; 
&lt;p&gt;.. raw:: html&lt;/p&gt; 
&lt;a href="https://cncf.io/"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://github.com/cncf/artwork/blob/main/other/cncf-member/graduated/color/cncf-graduated-color.svg" /&gt; 
  &lt;img src="https://github.com/cncf/artwork/raw/main/other/cncf-member/graduated/white/cncf-graduated-white.svg?sanitize=true" alt="CNCF Graduated Project" height="80" /&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;a href="https://ebpf.io/"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset=".github/assets/ebpf-horizontal.svg" /&gt; 
  &lt;img src="https://raw.githubusercontent.com/cilium/cilium/main/.github/assets/ebpf-horizontal-dark-back.svg?sanitize=true" alt="eBPF Logo" height="80" align="right" /&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;h1&gt;Stable Releases&lt;/h1&gt; 
&lt;p&gt;The Cilium community maintains minor stable releases for the last three minor Cilium versions. Older Cilium stable versions from minor releases prior to that are considered EOL.&lt;/p&gt; 
&lt;p&gt;For upgrades to new minor releases please consult the &lt;code&gt;Cilium Upgrade Guide&lt;/code&gt;_.&lt;/p&gt; 
&lt;p&gt;Listed below are the actively maintained release branches along with their latest patch release, corresponding image pull tags and their release notes:&lt;/p&gt; 
&lt;p&gt;+---------------------------------------------------------+------------+------------------------------------+----------------------------------------------------------------------------+ | &lt;code&gt;v1.18 &amp;lt;https://github.com/cilium/cilium/tree/v1.18&amp;gt;&lt;/code&gt;__ | 2026-01-13 | &lt;code&gt;quay.io/cilium/cilium:v1.18.6&lt;/code&gt; | &lt;code&gt;Release Notes &amp;lt;https://github.com/cilium/cilium/releases/tag/v1.18.6&amp;gt;&lt;/code&gt;__ | +---------------------------------------------------------+------------+------------------------------------+----------------------------------------------------------------------------+ | &lt;code&gt;v1.17 &amp;lt;https://github.com/cilium/cilium/tree/v1.17&amp;gt;&lt;/code&gt;__ | 2026-01-13 | &lt;code&gt;quay.io/cilium/cilium:v1.17.12&lt;/code&gt; | &lt;code&gt;Release Notes &amp;lt;https://github.com/cilium/cilium/releases/tag/v1.17.12&amp;gt;&lt;/code&gt;__ | +---------------------------------------------------------+------------+------------------------------------+----------------------------------------------------------------------------+ | &lt;code&gt;v1.16 &amp;lt;https://github.com/cilium/cilium/tree/v1.16&amp;gt;&lt;/code&gt;__ | 2026-01-13 | &lt;code&gt;quay.io/cilium/cilium:v1.16.19&lt;/code&gt; | &lt;code&gt;Release Notes &amp;lt;https://github.com/cilium/cilium/releases/tag/v1.16.19&amp;gt;&lt;/code&gt;__ | +---------------------------------------------------------+------------+------------------------------------+----------------------------------------------------------------------------+&lt;/p&gt; 
&lt;h2&gt;Architectures&lt;/h2&gt; 
&lt;p&gt;Cilium images are distributed for AMD64 and AArch64 architectures.&lt;/p&gt; 
&lt;h2&gt;Software Bill of Materials&lt;/h2&gt; 
&lt;p&gt;Starting with Cilium version 1.13.0, all images include a Software Bill of Materials (SBOM). The SBOM is generated in &lt;code&gt;SPDX&lt;/code&gt;_ format. More information on this is available on &lt;code&gt;Cilium SBOM&lt;/code&gt;_.&lt;/p&gt; 
&lt;p&gt;.. _&lt;code&gt;SPDX&lt;/code&gt;: &lt;a href="https://spdx.dev/"&gt;https://spdx.dev/&lt;/a&gt; .. _&lt;code&gt;Cilium SBOM&lt;/code&gt;: &lt;a href="https://docs.cilium.io/en/latest/configuration/sbom/"&gt;https://docs.cilium.io/en/latest/configuration/sbom/&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Development&lt;/h1&gt; 
&lt;p&gt;For development and testing purpose, the Cilium community publishes snapshots, early release candidates (RC) and CI container images build from the &lt;code&gt;main branch &amp;lt;https://github.com/cilium/cilium/commits/main&amp;gt;&lt;/code&gt;_. These images are not for use in production.&lt;/p&gt; 
&lt;p&gt;For testing upgrades to new development releases please consult the latest development build of the &lt;code&gt;Cilium Upgrade Guide&lt;/code&gt;_.&lt;/p&gt; 
&lt;p&gt;Listed below are branches for testing along with their snapshots or RC releases, corresponding image pull tags and their release notes where applicable:&lt;/p&gt; 
&lt;p&gt;+----------------------------------------------------------------------------+------------+-----------------------------------------+---------------------------------------------------------------------------------+ | &lt;code&gt;main &amp;lt;https://github.com/cilium/cilium/commits/main&amp;gt;&lt;/code&gt;__ | daily | &lt;code&gt;quay.io/cilium/cilium-ci:latest&lt;/code&gt; | N/A | +----------------------------------------------------------------------------+------------+-----------------------------------------+---------------------------------------------------------------------------------+ | &lt;code&gt;v1.19.0-rc.0 &amp;lt;https://github.com/cilium/cilium/commits/v1.19.0-rc.0&amp;gt;&lt;/code&gt;__ | 2026-01-15 | &lt;code&gt;quay.io/cilium/cilium:v1.19.0-rc.0&lt;/code&gt; | &lt;code&gt;Release Notes &amp;lt;https://github.com/cilium/cilium/releases/tag/v1.19.0-rc.0&amp;gt;&lt;/code&gt;__ | +----------------------------------------------------------------------------+------------+-----------------------------------------+---------------------------------------------------------------------------------+&lt;/p&gt; 
&lt;h1&gt;Functionality Overview&lt;/h1&gt; 
&lt;p&gt;.. begin-functionality-overview&lt;/p&gt; 
&lt;h2&gt;CNI (Container Network Interface)&lt;/h2&gt; 
&lt;p&gt;&lt;code&gt;Cilium as a CNI plugin &amp;lt;https://cilium.io/use-cases/cni/&amp;gt;&lt;/code&gt;_ provides a fast, scalable, and secure networking layer for Kubernetes clusters. Built on eBPF, it offers several deployment options:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Overlay networking:&lt;/strong&gt; encapsulation-based virtual network spanning all hosts with support for VXLAN and Geneve. It works on almost any network infrastructure as the only requirement is IP connectivity between hosts which is typically already given.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Native routing mode:&lt;/strong&gt; Use of the regular routing table of the Linux host. The network is required to be capable of routing the IP addresses of the application containers. It integrates with cloud routers, routing daemons, and IPv6-native infrastructure.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Flexible routing options:&lt;/strong&gt; Cilium can automate route learning and advertisement in common topologies such as using L2 neighbor discovery when nodes share a layer 2 domain, or BGP when routing across layer 3 boundaries.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Each mode is designed for maximum interoperability with existing infrastructure while minimizing operational burden.&lt;/p&gt; 
&lt;h2&gt;Load Balancing&lt;/h2&gt; 
&lt;p&gt;Cilium implements distributed load balancing for traffic between application containers and to/from external services. The load balancing is implemented in eBPF using efficient hashtables enabling high service density and low latency at scale.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;East-west load balancing&lt;/strong&gt; rewrites service connections at the socket level (&lt;code&gt;connect()&lt;/code&gt;), avoiding the overhead of per-packet NAT and fully &lt;code&gt;replacing kube-proxy &amp;lt;https://cilium.io/use-cases/kube-proxy/&amp;gt;&lt;/code&gt;_.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;North-south load balancing&lt;/strong&gt; supports XDP for high-throughput scenarios and &lt;code&gt;layer 4 load balancing &amp;lt;https://cilium.io/use-cases/load-balancer/&amp;gt;&lt;/code&gt;_ including Direct Server Return (DSR), and Maglev consistent hashing.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Cluster Mesh&lt;/h2&gt; 
&lt;p&gt;Cilium &lt;code&gt;Cluster Mesh &amp;lt;https://cilium.io/use-cases/cluster-mesh/&amp;gt;&lt;/code&gt;_ enables secure, seamless connectivity across multiple Kubernetes clusters. For operators running hybrid or multi-cloud environments, Cluster Mesh ensures a consistent security and connectivity experience.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Global service discovery&lt;/strong&gt;: Workloads across clusters can discover and connect to services as if they were local. This enables fault tolerance, like automatically failing over to backends in another cluster, and exposes shared services like logging, auth, or databases across environments.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Unified identity model:&lt;/strong&gt; Security policies are enforced based on identity, not IP address, across all clusters.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Network Policy&lt;/h2&gt; 
&lt;p&gt;Cilium &lt;code&gt;Network Policy &amp;lt;https://cilium.io/use-cases/network-policy/&amp;gt;&lt;/code&gt;_ provides identity-aware enforcement across L3-L7. Typical container firewalls secure workloads by filtering on source IP addresses and destination ports. This concept requires the firewalls on all servers to be manipulated whenever a container is started anywhere in the cluster.&lt;/p&gt; 
&lt;p&gt;In order to avoid this situation which limits scale, Cilium assigns a security identity to groups of application containers which share identical security policies. The identity is then associated with all network packets emitted by the application containers, allowing to validate the identity at the receiving node.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Identity-based security&lt;/strong&gt; removes reliance on brittle IP addresses.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;L3/L4 policies&lt;/strong&gt; restrict traffic based on labels, protocols, and ports.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;DNS-based policies:&lt;/strong&gt; Allow or deny traffic to FQDNs or wildcard domains (e.g., &lt;code&gt;api.example.com&lt;/code&gt;, &lt;code&gt;*.trusted.com&lt;/code&gt;). This is especially useful for securing egress traffic to third-party services.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;L7-aware policies&lt;/strong&gt; allow filtering by HTTP method, URL path, gRPC call, and more:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;Example: Allow only GET requests to &lt;code&gt;/public/.*&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Enforce the presence of headers like &lt;code&gt;X-Token: [0-9]+&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;CIDR-based egress and ingress policies are also supported for controlling access to external IPs, ideal for integrating with legacy systems or regulatory boundaries.&lt;/p&gt; 
&lt;h2&gt;Service Mesh&lt;/h2&gt; 
&lt;p&gt;With Cilium &lt;code&gt;Service Mesh &amp;lt;https://cilium.io/use-cases/service-mesh/&amp;gt;&lt;/code&gt;_, operators gain the benefits of fine-grained traffic control, encryption, observability, access control, without the cost and complexity of traditional proxy-based designs. Key features include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Mutual authentication&lt;/strong&gt; with automatic identity-based encryption between workloads using IPSec or WireGuard.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;L7-aware policy enforcement&lt;/strong&gt; for security and compliance.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Deep integration with the Kubernetes Gateway API :&lt;/strong&gt; Acts as a &lt;code&gt;Gateway API &amp;lt;https://cilium.io/use-cases/gateway-api/&amp;gt;&lt;/code&gt;_ compliant data plane, allowing you to declaratively manage ingress, traffic splitting, and routing behavior using Kubernetes-native CRDs.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Observability and Troubleshooting&lt;/h2&gt; 
&lt;p&gt;Observability is built into Cilium from the ground up, providing rich visibility that helps operators diagnose and understand system behavior including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Hubble&lt;/strong&gt;: A fully integrated observability platform that offers real-time service maps, flow visibility with identity and label metadata, and DNS-aware filtering and protocol-specific insights&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Metrics and alerting&lt;/strong&gt;: Integration with Prometheus, Grafana, and other monitoring systems.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Drop reasons and audit trails&lt;/strong&gt;: Get actionable insights into why traffic was dropped, including policy or port violations and issues like failed DNS lookups.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;.. end-functionality-overview&lt;/p&gt; 
&lt;h1&gt;Getting Started&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;Why Cilium?&lt;/code&gt;_&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Getting Started&lt;/code&gt;_&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Architecture and Concepts&lt;/code&gt;_&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Installing Cilium&lt;/code&gt;_&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Frequently Asked Questions&lt;/code&gt;_&lt;/li&gt; 
 &lt;li&gt;Contributing_&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Community&lt;/h1&gt; 
&lt;h2&gt;Slack&lt;/h2&gt; 
&lt;p&gt;Join the Cilium &lt;code&gt;Slack channel &amp;lt;https://slack.cilium.io&amp;gt;&lt;/code&gt;_ to chat with Cilium developers and other Cilium users. This is a good place to learn about Cilium, ask questions, and share your experiences.&lt;/p&gt; 
&lt;h2&gt;Special Interest Groups (SIG)&lt;/h2&gt; 
&lt;p&gt;See &lt;code&gt;Special Interest groups &amp;lt;https://github.com/cilium/community/blob/main/sigs.yaml&amp;gt;&lt;/code&gt;_ for a list of all SIGs and their meeting times.&lt;/p&gt; 
&lt;h2&gt;Developer meetings&lt;/h2&gt; 
&lt;p&gt;The Cilium developer community hangs out on Zoom to chat. Everybody is welcome.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Weekly, Wednesday, 5:00 pm &lt;code&gt;Europe/Zurich time &amp;lt;https://time.is/Canton_of_Zurich&amp;gt;&lt;/code&gt;__ (CET/CEST), usually equivalent to 8:00 am PT, or 11:00 am ET. &lt;code&gt;Meeting Notes and Zoom Info&lt;/code&gt;_&lt;/li&gt; 
 &lt;li&gt;Third Wednesday of each month, 9:00 am &lt;code&gt;Japan time &amp;lt;https://time.is/Tokyo&amp;gt;&lt;/code&gt;__ (JST). &lt;code&gt;APAC Meeting Notes and Zoom Info&lt;/code&gt;_&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;eBPF &amp;amp; Cilium Office Hours livestream&lt;/h2&gt; 
&lt;p&gt;We host a weekly community &lt;code&gt;YouTube livestream called eCHO &amp;lt;https://www.youtube.com/channel/UCJFUxkVQTBJh3LD1wYBWvuQ&amp;gt;&lt;/code&gt;_ which (very loosely!) stands for eBPF &amp;amp; Cilium Office Hours. Join us live, catch up with past episodes, or head over to the &lt;code&gt;eCHO repo &amp;lt;https://github.com/isovalent/eCHO&amp;gt;&lt;/code&gt;_ and let us know your ideas for topics we should cover.&lt;/p&gt; 
&lt;h2&gt;Governance&lt;/h2&gt; 
&lt;p&gt;The Cilium project is governed by a group of &lt;code&gt;Maintainers and Committers &amp;lt;https://raw.githubusercontent.com/cilium/cilium/main/MAINTAINERS.md&amp;gt;&lt;/code&gt;&lt;strong&gt;. How they are selected and govern is outlined in our &lt;code&gt;governance document &amp;lt;https://github.com/cilium/community/blob/main/GOVERNANCE.md&amp;gt;&lt;/code&gt;&lt;/strong&gt;.&lt;/p&gt; 
&lt;h2&gt;Adopters&lt;/h2&gt; 
&lt;p&gt;A list of adopters of the Cilium project who are deploying it in production, and of their use cases, can be found in file &lt;code&gt;USERS.md &amp;lt;https://github.com/cilium/cilium/blob/main/USERS.md&amp;gt;&lt;/code&gt;__.&lt;/p&gt; 
&lt;h1&gt;License&lt;/h1&gt; 
&lt;p&gt;.. _apache-license: LICENSE .. _bsd-license: bpf/LICENSE.BSD-2-Clause .. _gpl-license: bpf/LICENSE.GPL-2.0&lt;/p&gt; 
&lt;p&gt;The Cilium user space components are licensed under the &lt;code&gt;Apache License, Version 2.0 &amp;lt;apache-license_&amp;gt;&lt;/code&gt;&lt;strong&gt;. The BPF code templates are dual-licensed under the &lt;code&gt;General Public License, Version 2.0 (only) &amp;lt;gpl-license_&amp;gt;&lt;/code&gt;&lt;/strong&gt; and the &lt;code&gt;2-Clause BSD License &amp;lt;bsd-license_&amp;gt;&lt;/code&gt;__ (you can use the terms of either license, at your option).&lt;/p&gt; 
&lt;p&gt;.. _&lt;code&gt;Cilium Upgrade Guide&lt;/code&gt;: &lt;a href="https://docs.cilium.io/en/stable/operations/upgrade/"&gt;https://docs.cilium.io/en/stable/operations/upgrade/&lt;/a&gt; .. _&lt;code&gt;Why Cilium?&lt;/code&gt;: &lt;a href="https://docs.cilium.io/en/stable/overview/intro"&gt;https://docs.cilium.io/en/stable/overview/intro&lt;/a&gt; .. _&lt;code&gt;Getting Started&lt;/code&gt;: &lt;a href="https://docs.cilium.io/en/stable/#getting-started"&gt;https://docs.cilium.io/en/stable/#getting-started&lt;/a&gt; .. _&lt;code&gt;Architecture and Concepts&lt;/code&gt;: &lt;a href="https://docs.cilium.io/en/stable/overview/component-overview/"&gt;https://docs.cilium.io/en/stable/overview/component-overview/&lt;/a&gt; .. _&lt;code&gt;Installing Cilium&lt;/code&gt;: &lt;a href="https://docs.cilium.io/en/stable/gettingstarted/k8s-install-default/"&gt;https://docs.cilium.io/en/stable/gettingstarted/k8s-install-default/&lt;/a&gt; .. _&lt;code&gt;Frequently Asked Questions&lt;/code&gt;: &lt;a href="https://github.com/cilium/cilium/issues?utf8=%E2%9C%93&amp;amp;q=is%3Aissue+label%3Akind%2Fquestion+"&gt;https://github.com/cilium/cilium/issues?utf8=%E2%9C%93&amp;amp;q=is%3Aissue+label%3Akind%2Fquestion+&lt;/a&gt; .. _Contributing: &lt;a href="https://docs.cilium.io/en/stable/contributing/development/"&gt;https://docs.cilium.io/en/stable/contributing/development/&lt;/a&gt; .. _Prerequisites: &lt;a href="https://docs.cilium.io/en/stable/operations/system_requirements/"&gt;https://docs.cilium.io/en/stable/operations/system_requirements/&lt;/a&gt; .. _&lt;code&gt;eBPF&lt;/code&gt;: &lt;a href="https://ebpf.io"&gt;https://ebpf.io&lt;/a&gt; .. _&lt;code&gt;eBPF.io&lt;/code&gt;: &lt;a href="https://ebpf.io"&gt;https://ebpf.io&lt;/a&gt; .. _&lt;code&gt;Meeting Notes and Zoom Info&lt;/code&gt;: &lt;a href="https://docs.google.com/document/d/1Y_4chDk4rznD6UgXPlPvn3Dc7l-ZutGajUv1eF0VDwQ/edit#"&gt;https://docs.google.com/document/d/1Y_4chDk4rznD6UgXPlPvn3Dc7l-ZutGajUv1eF0VDwQ/edit#&lt;/a&gt; .. _&lt;code&gt;APAC Meeting Notes and Zoom Info&lt;/code&gt;: &lt;a href="https://docs.google.com/document/d/1egv4qLydr0geP-GjQexYKm4tz3_tHy-LCBjVQcXcT5M/edit#"&gt;https://docs.google.com/document/d/1egv4qLydr0geP-GjQexYKm4tz3_tHy-LCBjVQcXcT5M/edit#&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;.. |go-report| image:: &lt;a href="https://goreportcard.com/badge/github.com/cilium/cilium"&gt;https://goreportcard.com/badge/github.com/cilium/cilium&lt;/a&gt; :alt: Go Report Card :target: &lt;a href="https://goreportcard.com/report/github.com/cilium/cilium"&gt;https://goreportcard.com/report/github.com/cilium/cilium&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;.. |go-doc| image:: &lt;a href="https://godoc.org/github.com/cilium/cilium?status.svg"&gt;https://godoc.org/github.com/cilium/cilium?status.svg&lt;/a&gt; :alt: GoDoc :target: &lt;a href="https://godoc.org/github.com/cilium/cilium"&gt;https://godoc.org/github.com/cilium/cilium&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;.. |rtd| image:: &lt;a href="https://readthedocs.org/projects/docs/badge/?version=latest"&gt;https://readthedocs.org/projects/docs/badge/?version=latest&lt;/a&gt; :alt: Read the Docs :target: &lt;a href="https://docs.cilium.io/"&gt;https://docs.cilium.io/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;.. |apache| image:: &lt;a href="https://img.shields.io/badge/license-Apache-blue.svg"&gt;https://img.shields.io/badge/license-Apache-blue.svg&lt;/a&gt; :alt: Apache licensed :target: apache-license_&lt;/p&gt; 
&lt;p&gt;.. |bsd| image:: &lt;a href="https://img.shields.io/badge/license-BSD-blue.svg"&gt;https://img.shields.io/badge/license-BSD-blue.svg&lt;/a&gt; :alt: BSD licensed :target: bsd-license_&lt;/p&gt; 
&lt;p&gt;.. |gpl| image:: &lt;a href="https://img.shields.io/badge/license-GPL-blue.svg"&gt;https://img.shields.io/badge/license-GPL-blue.svg&lt;/a&gt; :alt: GPL licensed :target: gpl-license_&lt;/p&gt; 
&lt;p&gt;.. |slack| image:: &lt;a href="https://img.shields.io/badge/slack-cilium-brightgreen.svg?logo=slack"&gt;https://img.shields.io/badge/slack-cilium-brightgreen.svg?logo=slack&lt;/a&gt; :alt: Join the Cilium slack channel :target: &lt;a href="https://slack.cilium.io"&gt;https://slack.cilium.io&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;.. |cii| image:: &lt;a href="https://bestpractices.coreinfrastructure.org/projects/1269/badge"&gt;https://bestpractices.coreinfrastructure.org/projects/1269/badge&lt;/a&gt; :alt: CII Best Practices :target: &lt;a href="https://bestpractices.coreinfrastructure.org/projects/1269"&gt;https://bestpractices.coreinfrastructure.org/projects/1269&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;.. |clomonitor| image:: &lt;a href="https://img.shields.io/endpoint?url=https://clomonitor.io/api/projects/cncf/cilium/badge"&gt;https://img.shields.io/endpoint?url=https://clomonitor.io/api/projects/cncf/cilium/badge&lt;/a&gt; :alt: CLOMonitor :target: &lt;a href="https://clomonitor.io/projects/cncf/cilium"&gt;https://clomonitor.io/projects/cncf/cilium&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;.. |artifacthub| image:: &lt;a href="https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/cilium"&gt;https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/cilium&lt;/a&gt; :alt: Artifact Hub :target: &lt;a href="https://artifacthub.io/packages/helm/cilium/cilium"&gt;https://artifacthub.io/packages/helm/cilium/cilium&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;.. |fossa| image:: &lt;a href="https://app.fossa.com/api/projects/custom%2B162%2Fgit%40github.com%3Acilium%2Fcilium.git.svg?type=shield"&gt;https://app.fossa.com/api/projects/custom%2B162%2Fgit%40github.com%3Acilium%2Fcilium.git.svg?type=shield&lt;/a&gt; :alt: FOSSA Status :target: &lt;a href="https://app.fossa.com/projects/custom%2B162%2Fgit%40github.com%3Acilium%2Fcilium.git?ref=badge_shield"&gt;https://app.fossa.com/projects/custom%2B162%2Fgit%40github.com%3Acilium%2Fcilium.git?ref=badge_shield&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;.. |gateway-api| image:: &lt;a href="https://img.shields.io/badge/Gateway%20API%20Conformance%20v1.2.0-Cilium-green"&gt;https://img.shields.io/badge/Gateway%20API%20Conformance%20v1.2.0-Cilium-green&lt;/a&gt; :alt: Gateway API Status :target: &lt;a href="https://github.com/kubernetes-sigs/gateway-api/tree/main/conformance/reports/v1.2.0/cilium-cilium"&gt;https://github.com/kubernetes-sigs/gateway-api/tree/main/conformance/reports/v1.2.0/cilium-cilium&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;.. |codespaces| image:: &lt;a href="https://img.shields.io/badge/Open_in_GitHub_Codespaces-gray?logo=github"&gt;https://img.shields.io/badge/Open_in_GitHub_Codespaces-gray?logo=github&lt;/a&gt; :alt: Github Codespaces :target: &lt;a href="https://github.com/codespaces/new?hide_repo_select=true&amp;amp;ref=master&amp;amp;repo=48109239&amp;amp;machine=standardLinux32gb&amp;amp;location=WestEurope"&gt;https://github.com/codespaces/new?hide_repo_select=true&amp;amp;ref=master&amp;amp;repo=48109239&amp;amp;machine=standardLinux32gb&amp;amp;location=WestEurope&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>kgateway-dev/kgateway</title>
      <link>https://github.com/kgateway-dev/kgateway</link>
      <description>&lt;p&gt;The Cloud-Native API Gateway and AI Gateway&lt;/p&gt;&lt;hr&gt;&lt;h1 align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://raw.githubusercontent.com/kgateway-dev/kgateway.dev/main/static/logo-dark.svg" alt="kgateway" width="400" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://raw.githubusercontent.com/kgateway-dev/kgateway.dev/main/static/logo.svg" alt="kgateway" width="400" /&gt; 
  &lt;img alt="kgateway" src="https://raw.githubusercontent.com/kgateway-dev/kgateway.dev/main/static/logo.svg?sanitize=true" /&gt; 
 &lt;/picture&gt; &lt;br /&gt; The most widely deployed gateway in Kubernetes for microservices and AI agents &lt;/h1&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/kgateway-dev/kgateway/releases"&gt; &lt;img src="https://img.shields.io/github/v/release/kgateway-dev/kgateway?style=flat&amp;amp;label=Latest%20version" alt="Release" /&gt; &lt;/a&gt; 
 &lt;a href="https://opensource.org/licenses/Apache-2.0"&gt; &lt;img src="https://img.shields.io/badge/License-Apache2.0-brightgreen.svg?style=flat" alt="License: Apache 2.0" /&gt; &lt;/a&gt; 
 &lt;a href="https://github.com/kgateway-dev/kgateway"&gt; &lt;img src="https://img.shields.io/github/stars/kgateway-dev/kgateway.svg?style=flat&amp;amp;logo=github&amp;amp;label=Stars" alt="Stars" /&gt; &lt;/a&gt; 
 &lt;a href="https://www.bestpractices.dev/projects/10534"&gt;&lt;img src="https://www.bestpractices.dev/projects/10534/badge" alt="OpenSSF Best Practices" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;About kgateway&lt;/h2&gt; 
&lt;p&gt;Kgateway is the most mature and widely deployed gateway in the market today. Built on open source and open standards, &lt;strong&gt;kgateway is a dual control plane that implements the &lt;a href="https://gateway-api.sigs.k8s.io/"&gt;Kubernetes Gateway API&lt;/a&gt; for both &lt;a href="https://github.com/envoyproxy/envoy"&gt;Envoy&lt;/a&gt; and &lt;a href="https://github.com/agentgateway/agentgateway"&gt;agentgateway&lt;/a&gt;&lt;/strong&gt;. This unique architecture enables kgateway to provide unified API connectivity spanning from traditional HTTP/gRPC workloads to advanced AI agent orchestration.&lt;/p&gt; 
&lt;p&gt;With a control plane that scales from lightweight microgateway deployments between services, to massively parallel centralized gateways handling billions of API calls, to advanced AI gateway use cases for safety, security, and governance, kgateway brings omni-directional API connectivity to any cloud and any environment.&lt;/p&gt; 
&lt;h3&gt;Use Cases&lt;/h3&gt; 
&lt;p&gt;Kgateway is designed for:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Advanced Ingress Controller and Next-Gen API Gateway&lt;/strong&gt;: Aggregate web APIs and apply functions like authentication, authorization and rate limiting in one place. Powered by &lt;a href="https://www.envoyproxy.io"&gt;Envoy&lt;/a&gt; or &lt;a href="https://github.com/agentgateway/agentgateway"&gt;agentgateway&lt;/a&gt; and programmed with the &lt;a href="https://gateway-api.sigs.k8s.io/"&gt;Gateway API&lt;/a&gt;, kgateway is a world-leading Cloud Native ingress.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;AI Gateway for LLM Consumption&lt;/strong&gt;: Protect models, tools, agents, and data from inappropriate access. Manage traffic to LLM providers, enrich prompts at a system level, and apply prompt guards for safety and compliance.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Inference Gateway for Generative Models&lt;/strong&gt;: Intelligently route to AI inference workloads in Kubernetes environments utilizing the &lt;a href="https://gateway-api-inference-extension.sigs.k8s.io/"&gt;Inference Extension&lt;/a&gt; project.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Native MCP and Agent-to-Agent Gateway&lt;/strong&gt;: Federate Model Context Protocol tool services and secure agent-to-agent communications with a single scalable endpoint powered by agentgateway.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Hybrid Application Migration&lt;/strong&gt;: Route to backends implemented as microservices, serverless functions or legacy apps. Gradually migrate from legacy code while maintaining existing systems.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Kgateway is feature-rich, fast, and flexible. It excels in function-level routing, supports legacy apps, microservices and serverless, offers robust discovery capabilities, integrates seamlessly with open-source projects, and is designed to support hybrid applications with various technologies, architectures, protocols, and clouds.&lt;/p&gt; 
&lt;h3&gt;History&lt;/h3&gt; 
&lt;p&gt;The project was launched in 2018 as &lt;strong&gt;Gloo&lt;/strong&gt; by Solo.io and has been &lt;a href="https://www.solo.io/blog/announcing-gloo-1-0-a-production-ready-envoy-based-api-gateway"&gt;production-ready since 2019&lt;/a&gt;. Since then, it has steadily evolved to become the most trusted and feature-rich API gateway for Kubernetes, processing billions of API requests for many of the world's biggest companies. Please see &lt;a href="https://github.com/kgateway-dev/kgateway/issues/10363"&gt;the migration plan&lt;/a&gt; for more information about the transition from Gloo to kgateway.&lt;/p&gt; 
&lt;h2&gt;Get involved&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://kgateway.dev/slack/"&gt;Join us on our Slack channel&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://kgateway.dev/docs"&gt;Check out the docs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://kgateway.dev/blog/"&gt;Read the kgateway blog&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/kgateway-dev/community"&gt;Learn more about the community&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/@kgateway-dev"&gt;Watch a video on our YouTube channel&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Follow us on &lt;a href="https://x.com/kgatewaydev"&gt;X&lt;/a&gt;, &lt;a href="https://bsky.app/profile/kgateway.dev"&gt;Bluesky&lt;/a&gt;, &lt;a href="https://mastodon.social/@kgateway"&gt;Mastodon&lt;/a&gt; or &lt;a href="https://www.linkedin.com/company/kgateway/"&gt;LinkedIn&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing to kgateway&lt;/h2&gt; 
&lt;p&gt;Please refer to &lt;a href="https://raw.githubusercontent.com/kgateway-dev/kgateway/main/devel/contributing/README.md"&gt;devel/contributing/README.md&lt;/a&gt; as a starting point for contributing to the project.&lt;/p&gt; 
&lt;h2&gt;Releasing kgateway&lt;/h2&gt; 
&lt;p&gt;Please refer to &lt;a href="https://raw.githubusercontent.com/kgateway-dev/kgateway/main/devel/contributing/releasing.md"&gt;devel/contributing/releasing.md&lt;/a&gt; as a starting point for understanding releases of the project.&lt;/p&gt; 
&lt;h2&gt;Security&lt;/h2&gt; 
&lt;p&gt;See our &lt;a href="https://raw.githubusercontent.com/kgateway-dev/kgateway/main/SECURITY.md"&gt;SECURITY.md&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;h2&gt;Thanks&lt;/h2&gt; 
&lt;p&gt;Kgateway would not be possible without the valuable open source work of projects in the community. We would like to extend a special thank-you to &lt;a href="https://www.envoyproxy.io"&gt;Envoy&lt;/a&gt; and &lt;a href="https://github.com/agentgateway/agentgateway"&gt;agentgateway&lt;/a&gt;, the two data planes upon which we build our dual control plane architecture.&lt;/p&gt; 
&lt;h2&gt;Contributors&lt;/h2&gt; 
&lt;p&gt;Thanks to all contributors who are helping to make kgateway better!&lt;/p&gt; 
&lt;a href="https://github.com/kgateway-dev/kgateway/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=kgateway-dev/kgateway" /&gt; &lt;/a&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;a href="https://www.star-history.com/#kgateway-dev/kgateway&amp;amp;Date"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=kgateway-dev/kgateway&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=kgateway-dev/kgateway&amp;amp;type=Date" /&gt; 
  &lt;img alt="Star history of kgateway-dev/kgateway over time" src="https://api.star-history.com/svg?repos=kgateway-dev/kgateway&amp;amp;type=Date" /&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/cncf/artwork/main/other/cncf-sandbox/horizontal/color/cncf-sandbox-horizontal-color.svg?sanitize=true" width="300" alt="Cloud Native Computing Foundation logo" /&gt; 
 &lt;p&gt;kgateway is a &lt;a href="https://cncf.io"&gt;Cloud Native Computing Foundation&lt;/a&gt; sandbox project.&lt;/p&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>docker/mcp-gateway</title>
      <link>https://github.com/docker/mcp-gateway</link>
      <description>&lt;p&gt;docker mcp CLI plugin / MCP Gateway&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Docker MCP Plugin and Docker MCP Gateway&lt;/h1&gt; 
&lt;p&gt;&lt;img src="https://github.com/docker/mcp-gateway/actions/workflows/ci.yml/badge.svg?sanitize=true" alt="build" /&gt;&lt;/p&gt; 
&lt;p&gt;The &lt;a href="https://docs.docker.com/ai/mcp-catalog-and-toolkit/toolkit/"&gt;MCP Toolkit&lt;/a&gt;, in Docker Desktop, allows developers to configure and consume MCP servers from the &lt;a href="https://hub.docker.com/mcp"&gt;Docker MCP Catalog&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Underneath, the Toolkit is powered by a docker CLI plugin: &lt;code&gt;docker-mcp&lt;/code&gt;. This repository is the code of this CLI plugin. It can work in Docker Desktop or independently.&lt;/p&gt; 
&lt;p&gt;The main feature of this CLI is the &lt;strong&gt;Docker MCP Gateway&lt;/strong&gt; which allows easy and secure running and deployment of MCP servers. See &lt;a href="https://raw.githubusercontent.com/docker/mcp-gateway/main/#Features"&gt;Features&lt;/a&gt; for a list of all the features.&lt;/p&gt; 
&lt;h2&gt;What is MCP?&lt;/h2&gt; 
&lt;p&gt;The &lt;a href="https://spec.modelcontextprotocol.io/"&gt;Model Context Protocol (MCP)&lt;/a&gt; is an open protocol that standardizes how AI applications connect to external data sources and tools. It provides a secure, controlled way for language models to access and interact with various services, databases, and APIs.&lt;/p&gt; 
&lt;h2&gt;Overview&lt;/h2&gt; 
&lt;p&gt;Developers face criticial barriers when integrating Model Context Protocol (MCP) tools into production workflows:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Managing MCP server lifecycle&lt;/strong&gt; Each local MCP sever in the catalog runs in an isolated Docker container. npx and uvx servers are granted minimal host privileges.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Providing a unified interface&lt;/strong&gt; AI models access MCP servers through a single Gateway.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Handling authentication and security&lt;/strong&gt; Keep secrets out of environment variables using Docker Desktop's secrets management.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Supports dynamic tool discovery&lt;/strong&gt; and configuration. Each MCP client (eg VS Code, Cursor, Claude Desktop, etc.) connects to the same Gateway configuration, ensuring consistency across different clients.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Enables OAuth flows&lt;/strong&gt; for MCPs that require OAuth access token service connections.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;üê≥ &lt;strong&gt;Container-based Servers&lt;/strong&gt;: Run MCP servers as Docker containers with proper isolation.&lt;/li&gt; 
 &lt;li&gt;üîß &lt;strong&gt;Server Management&lt;/strong&gt;: List, inspect, and call MCP tools, resources and prompts from multiple servers.&lt;/li&gt; 
 &lt;li&gt;üîê &lt;strong&gt;Secrets Management&lt;/strong&gt;: Secure handling of API keys and credentials via Docker Desktop.&lt;/li&gt; 
 &lt;li&gt;üåê &lt;strong&gt;OAuth Integration&lt;/strong&gt;: Built-in OAuth flows for service authentication.&lt;/li&gt; 
 &lt;li&gt;üìã &lt;strong&gt;Server Catalog&lt;/strong&gt;: Manage and configure multiple MCP catalogs.&lt;/li&gt; 
 &lt;li&gt;üîç &lt;strong&gt;Dynamic Discovery&lt;/strong&gt;: Automatic tool, prompt, and resource discovery from running servers.&lt;/li&gt; 
 &lt;li&gt;üìä &lt;strong&gt;Monitoring&lt;/strong&gt;: Built-in logging and call tracing capabilities.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Docker Desktop (with MCP Toolkit feature enabled)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align="left"&gt; 
 &lt;img src="https://raw.githubusercontent.com/docker/mcp-gateway/main/img/enable_toolkit.png" width="400" /&gt; 
&lt;/div&gt; - Go 1.24+ (for development) 
&lt;h3&gt;Install as Docker CLI Plugin&lt;/h3&gt; 
&lt;p&gt;The MCP cli will already be installed on recent versions of Docker Desktop but you can build and install the latest version by following these steps:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Clone the repository
git clone https://github.com/docker/mcp-gateway.git
cd mcp-gateway
mkdir -p "$HOME/.docker/cli-plugins/"

# Build and install the plugin
make docker-mcp
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After installation, the plugin will be available as:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker mcp --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;h3&gt;Catalog Management&lt;/h3&gt; 
&lt;p&gt;Manage the catalogs available to the MCP gateway. The &lt;a href="https://hub.docker.com/mcp"&gt;default catalog&lt;/a&gt; is available with the name 'docker-mcp'.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Manage server catalogs
docker mcp catalog --help

# Initialize the default Docker MCP Catalog
docker mcp catalog init

# List available catalogs
docker mcp catalog ls

# Show all servers in a catalog
docker mcp catalog show docker-mcp
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;more about &lt;a href="https://raw.githubusercontent.com/docker/mcp-gateway/main/docs/catalog.md"&gt;the MCP Catalog&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;more about &lt;a href="https://raw.githubusercontent.com/docker/mcp-gateway/main/docs/catalog.md#importing-from-the-oss-mcp-community-registry"&gt;importing from the OSS MCP Community Registry&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;MCP Gateway Operations&lt;/h3&gt; 
&lt;p&gt;Start up an MCP Gateway. This can be used for one client, or to service multiple clients if using either &lt;code&gt;sse&lt;/code&gt; or &lt;code&gt;streaming&lt;/code&gt; transports.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Run the MCP gateway (stdio)
docker mcp gateway run

# Run the MCP gateway (streaming)
docker mcp gateway run --port 8080 --transport streaming
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;more about &lt;a href="https://raw.githubusercontent.com/docker/mcp-gateway/main/docs/mcp-gateway.md"&gt;the MCP Gateway&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/docker/mcp-gateway/main/docs/self-configured.md"&gt;running an unpublished local image&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Server Management&lt;/h3&gt; 
&lt;p&gt;Enable and disable the set of MCP servers that will be available for default clients. The MCP gateway can be configured to expose different sets of servers and tools but enabling and disabling servers here impacts the default gateway configuration.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# List enabled servers
docker mcp server ls

# Enable one or more servers
docker mcp server enable &amp;lt;server-name&amp;gt; [server-name...]

# Disable servers
docker mcp server disable &amp;lt;server-name&amp;gt; [server-name...]

# Get detailed information about a server
docker mcp server inspect &amp;lt;server-name&amp;gt;

# Reset (disable all servers)
docker mcp server reset
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Configuration Management&lt;/h3&gt; 
&lt;p&gt;Configure any MCP servers that require custom runtime configuration.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Read current configuration
docker mcp config read

# Write new configuration
docker mcp config write '&amp;lt;yaml-config&amp;gt;'

# Reset configuration to defaults
docker mcp config reset
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Secrets and OAuth&lt;/h3&gt; 
&lt;p&gt;Configure MCP servers that require either secrets or OAuth.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Manage secrets
docker mcp secret --help

# Handle OAuth flows
docker mcp oauth --help

# Manage access policies
docker mcp policy --help

# export any desktop secrets needed by either server1 or server2
#   (temporary requirement to export secrets for docker cloud runs - this command
#    will no longer be required once Docker Cloud can access secret stores) 
docker mcp secret export server1 server2
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Tool Management&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Show available commands
docker mcp --help

# Count available tools
docker mcp tools count

# List all available MCP tools
docker mcp tools ls

# List all available MCP tools in JSON format
docker mcp tools ls --format=json

# Inspect a specific tool
docker mcp tools inspect &amp;lt;tool-name&amp;gt;

# Call a tool with arguments
docker mcp tools call &amp;lt;tool-name&amp;gt; [arguments...]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Configuration&lt;/h2&gt; 
&lt;p&gt;The MCP CLI uses several configuration files:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;docker-mcp.yaml&lt;/code&gt;&lt;/strong&gt;: Server catalog defining available MCP servers&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;registry.yaml&lt;/code&gt;&lt;/strong&gt;: Registry of enabled servers&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;config.yaml&lt;/code&gt;&lt;/strong&gt;: Configuration per server&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;tools.yaml&lt;/code&gt;&lt;/strong&gt;: Enabled tools per server&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Configuration files are typically stored in &lt;code&gt;~/.docker/mcp/&lt;/code&gt;. This is in this directory that Docker Desktop's MCP Toolkit with store its configuration.&lt;/p&gt; 
&lt;h3&gt;Environment Variables&lt;/h3&gt; 
&lt;p&gt;The MCP CLI respects the following environment variables for client configuration:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;CLAUDE_CONFIG_DIR&lt;/code&gt;&lt;/strong&gt;: Override the default Claude Code configuration directory (&lt;code&gt;~/.claude&lt;/code&gt;). When set, Claude Code will use &lt;code&gt;$CLAUDE_CONFIG_DIR/.claude.json&lt;/code&gt; instead of &lt;code&gt;~/.claude.json&lt;/code&gt; for its MCP server configuration. This is useful for: 
  &lt;ul&gt; 
   &lt;li&gt;Maintaining separate Claude Code installations for work and personal use&lt;/li&gt; 
   &lt;li&gt;Testing configuration changes in isolation&lt;/li&gt; 
   &lt;li&gt;Managing multiple Claude Code profiles&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Example usage:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Set custom Claude Code configuration directory
export CLAUDE_CONFIG_DIR=/path/to/custom/config

# Connect MCP Gateway to Claude Code
docker mcp client connect claude-code --global

# Claude Code will now use /path/to/custom/config/.claude.json
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Architecture&lt;/h2&gt; 
&lt;p&gt;The Docker MCP CLI implements a gateway pattern:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;AI Client ‚Üí MCP Gateway ‚Üí MCP Servers (Docker Containers)
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;AI Client&lt;/strong&gt;: Language model or AI application&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MCP Gateway&lt;/strong&gt;: This CLI tool managing protocol translation and routing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MCP Servers&lt;/strong&gt;: Individual MCP servers running in Docker containers&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/docker/mcp-gateway/main/docs/message-flow.md"&gt;docs/message-flow.md&lt;/a&gt; for detailed message flow diagrams.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;The build instructions are available in the &lt;a href="https://raw.githubusercontent.com/docker/mcp-gateway/main/CONTRIBUTING.md"&gt;contribution guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the MIT License - see the &lt;a href="https://raw.githubusercontent.com/docker/mcp-gateway/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;h2&gt;Support&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;üí¨ &lt;a href="https://raw.githubusercontent.com/docker/mcp-gateway/main/docs/troubleshooting.md"&gt;Troubleshooting&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üìñ &lt;a href="https://modelcontextprotocol.io/specification/2025-11-25"&gt;MCP Specification&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üê≥ &lt;a href="https://docs.docker.com/desktop/"&gt;Docker Desktop Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üêõ &lt;a href="https://github.com/docker/mcp-gateway/issues"&gt;Report Issues&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üí¨ &lt;a href="https://github.com/docker/mcp-gateway/discussions"&gt;Discussions&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>maximhq/bifrost</title>
      <link>https://github.com/maximhq/bifrost</link>
      <description>&lt;p&gt;Fastest LLM gateway (50x faster than LiteLLM) with adaptive load balancer, cluster mode, guardrails, 1000+ models support &amp; &lt;100 ¬µs overhead at 5k RPS.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Bifrost&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://goreportcard.com/report/github.com/maximhq/bifrost/core"&gt;&lt;img src="https://goreportcard.com/badge/github.com/maximhq/bifrost/core" alt="Go Report Card" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/exN5KAydbU"&gt;&lt;img src="https://dcbadge.limes.pink/api/server/https://discord.gg/exN5KAydbU?style=flat" alt="Discord badge" /&gt;&lt;/a&gt; &lt;a href="https://snyk.io/test/github/maximhq/bifrost"&gt;&lt;img src="https://snyk.io/test/github/maximhq/bifrost/badge.svg?sanitize=true" alt="Known Vulnerabilities" /&gt;&lt;/a&gt; &lt;a href="https://codecov.io/gh/maximhq/bifrost"&gt;&lt;img src="https://codecov.io/gh/maximhq/bifrost/branch/main/graph/badge.svg?sanitize=true" alt="codecov" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/docker/pulls/maximhq/bifrost" alt="Docker Pulls" /&gt; &lt;a href="https://app.getpostman.com/run-collection/31642484-2ba0e658-4dcd-49f4-845a-0c7ed745b916?action=collection%2Ffork&amp;amp;source=rip_markdown&amp;amp;collection-url=entityId%3D31642484-2ba0e658-4dcd-49f4-845a-0c7ed745b916%26entityType%3Dcollection%26workspaceId%3D63e853c8-9aec-477f-909c-7f02f543150e"&gt;&lt;img src="https://run.pstmn.io/button.svg?sanitize=true" alt="Run In Postman" style="width: 95px; height: 21px;" /&gt;&lt;/a&gt; &lt;a href="https://artifacthub.io/packages/search?repo=bifrost"&gt;&lt;img src="https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/bifrost" alt="Artifact Hub" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/maximhq/bifrost/main/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/maximhq/bifrost" alt="License" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;The fastest way to build AI applications that never go down&lt;/h2&gt; 
&lt;p&gt;Bifrost is a high-performance AI gateway that unifies access to 15+ providers (OpenAI, Anthropic, AWS Bedrock, Google Vertex, and more) through a single OpenAI-compatible API. Deploy in seconds with zero configuration and get automatic failover, load balancing, semantic caching, and enterprise-grade features.&lt;/p&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/maximhq/bifrost/main/docs/media/getting-started.png" alt="Get started" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Go from zero to production-ready AI gateway in under a minute.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Step 1:&lt;/strong&gt; Start Bifrost Gateway&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install and run locally
npx -y @maximhq/bifrost

# Or use Docker
docker run -p 8080:8080 maximhq/bifrost
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Step 2:&lt;/strong&gt; Configure via Web UI&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Open the built-in web interface
open http://localhost:8080
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Step 3:&lt;/strong&gt; Make your first API call&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "openai/gpt-4o-mini",
    "messages": [{"role": "user", "content": "Hello, Bifrost!"}]
  }'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;That's it!&lt;/strong&gt; Your AI gateway is running with a web interface for visual configuration, real-time monitoring, and analytics.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Complete Setup Guides:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.getbifrost.ai/quickstart/gateway/setting-up"&gt;Gateway Setup&lt;/a&gt; - HTTP API deployment&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.getbifrost.ai/quickstart/go-sdk/setting-up"&gt;Go SDK Setup&lt;/a&gt; - Direct integration&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Enterprise Deployments&lt;/h2&gt; 
&lt;p&gt;Bifrost supports enterprise-grade, private deployments for teams running production AI systems at scale. In addition to private networking, custom security controls, and governance, enterprise deployments unlock advanced capabilities including adaptive load balancing, clustering, guardrails, MCP gateway and and other features designed for enterprise-grade scale and reliability.&lt;/p&gt; 
&lt;p&gt;üëâ &lt;a href="https://www.getmaxim.ai/bifrost/enterprise" target="_blank"&gt;Explore enterprise capabilities&lt;/a&gt;&lt;/p&gt; 
&lt;div align="left"&gt; 
 &lt;a href="https://calendly.com/maximai/bifrost-demo"&gt; &lt;img src="https://raw.githubusercontent.com/maximhq/bifrost/main/.github/assets/book-demo-button.png" alt="Book a Demo" width="170" style="margin-top:5px;" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;h3&gt;Core Infrastructure&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://docs.getbifrost.ai/features/unified-interface"&gt;Unified Interface&lt;/a&gt;&lt;/strong&gt; - Single OpenAI-compatible API for all providers&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://docs.getbifrost.ai/quickstart/gateway/provider-configuration"&gt;Multi-Provider Support&lt;/a&gt;&lt;/strong&gt; - OpenAI, Anthropic, AWS Bedrock, Google Vertex, Azure, Cerebras, Cohere, Mistral, Ollama, Groq, and more&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://docs.getbifrost.ai/features/fallbacks"&gt;Automatic Fallbacks&lt;/a&gt;&lt;/strong&gt; - Seamless failover between providers and models with zero downtime&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://docs.getbifrost.ai/features/fallbacks"&gt;Load Balancing&lt;/a&gt;&lt;/strong&gt; - Intelligent request distribution across multiple API keys and providers&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Advanced Features&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://docs.getbifrost.ai/features/mcp"&gt;Model Context Protocol (MCP)&lt;/a&gt;&lt;/strong&gt; - Enable AI models to use external tools (filesystem, web search, databases)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://docs.getbifrost.ai/features/semantic-caching"&gt;Semantic Caching&lt;/a&gt;&lt;/strong&gt; - Intelligent response caching based on semantic similarity to reduce costs and latency&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://docs.getbifrost.ai/quickstart/gateway/streaming"&gt;Multimodal Support&lt;/a&gt;&lt;/strong&gt; - Support for text,images, audio, and streaming, all behind a common interface.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://docs.getbifrost.ai/enterprise/custom-plugins"&gt;Custom Plugins&lt;/a&gt;&lt;/strong&gt; - Extensible middleware architecture for analytics, monitoring, and custom logic&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://docs.getbifrost.ai/features/governance"&gt;Governance&lt;/a&gt;&lt;/strong&gt; - Usage tracking, rate limiting, and fine-grained access control&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Enterprise &amp;amp; Security&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://docs.getbifrost.ai/features/governance"&gt;Budget Management&lt;/a&gt;&lt;/strong&gt; - Hierarchical cost control with virtual keys, teams, and customer budgets&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://docs.getbifrost.ai/features/sso-with-google-github"&gt;SSO Integration&lt;/a&gt;&lt;/strong&gt; - Google and GitHub authentication support&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://docs.getbifrost.ai/features/observability"&gt;Observability&lt;/a&gt;&lt;/strong&gt; - Native Prometheus metrics, distributed tracing, and comprehensive logging&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://docs.getbifrost.ai/enterprise/vault-support"&gt;Vault Support&lt;/a&gt;&lt;/strong&gt; - Secure API key management with HashiCorp Vault integration&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Developer Experience&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://docs.getbifrost.ai/quickstart/gateway/setting-up"&gt;Zero-Config Startup&lt;/a&gt;&lt;/strong&gt; - Start immediately with dynamic provider configuration&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://docs.getbifrost.ai/features/drop-in-replacement"&gt;Drop-in Replacement&lt;/a&gt;&lt;/strong&gt; - Replace OpenAI/Anthropic/GenAI APIs with one line of code&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://docs.getbifrost.ai/integrations/what-is-an-integration"&gt;SDK Integrations&lt;/a&gt;&lt;/strong&gt; - Native support for popular AI SDKs with zero code changes&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://docs.getbifrost.ai/quickstart/gateway/provider-configuration"&gt;Configuration Flexibility&lt;/a&gt;&lt;/strong&gt; - Web UI, API-driven, or file-based configuration options&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Repository Structure&lt;/h2&gt; 
&lt;p&gt;Bifrost uses a modular architecture for maximum flexibility:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-text"&gt;bifrost/
‚îú‚îÄ‚îÄ npx/                 # NPX script for easy installation
‚îú‚îÄ‚îÄ core/                # Core functionality and shared components
‚îÇ   ‚îú‚îÄ‚îÄ providers/       # Provider-specific implementations (OpenAI, Anthropic, etc.)
‚îÇ   ‚îú‚îÄ‚îÄ schemas/         # Interfaces and structs used throughout Bifrost
‚îÇ   ‚îî‚îÄ‚îÄ bifrost.go       # Main Bifrost implementation
‚îú‚îÄ‚îÄ framework/           # Framework components for data persistence
‚îÇ   ‚îú‚îÄ‚îÄ configstore/     # Configuration storages
‚îÇ   ‚îú‚îÄ‚îÄ logstore/        # Request logging storages
‚îÇ   ‚îî‚îÄ‚îÄ vectorstore/     # Vector storages
‚îú‚îÄ‚îÄ transports/          # HTTP gateway and other interface layers
‚îÇ   ‚îî‚îÄ‚îÄ bifrost-http/    # HTTP transport implementation
‚îú‚îÄ‚îÄ ui/                  # Web interface for HTTP gateway
‚îú‚îÄ‚îÄ plugins/             # Extensible plugin system
‚îÇ   ‚îú‚îÄ‚îÄ governance/      # Budget management and access control
‚îÇ   ‚îú‚îÄ‚îÄ jsonparser/      # JSON parsing and manipulation utilities
‚îÇ   ‚îú‚îÄ‚îÄ logging/         # Request logging and analytics
‚îÇ   ‚îú‚îÄ‚îÄ maxim/           # Maxim's observability integration
‚îÇ   ‚îú‚îÄ‚îÄ mocker/          # Mock responses for testing and development
‚îÇ   ‚îú‚îÄ‚îÄ semanticcache/   # Intelligent response caching
‚îÇ   ‚îî‚îÄ‚îÄ telemetry/       # Monitoring and observability
‚îú‚îÄ‚îÄ docs/                # Documentation and guides
‚îî‚îÄ‚îÄ tests/               # Comprehensive test suites
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Getting Started Options&lt;/h2&gt; 
&lt;p&gt;Choose the deployment method that fits your needs:&lt;/p&gt; 
&lt;h3&gt;1. Gateway (HTTP API)&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Best for:&lt;/strong&gt; Language-agnostic integration, microservices, and production deployments&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# NPX - Get started in 30 seconds
npx -y @maximhq/bifrost

# Docker - Production ready
docker run -p 8080:8080 -v $(pwd)/data:/app/data maximhq/bifrost
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Features:&lt;/strong&gt; Web UI, real-time monitoring, multi-provider management, zero-config startup&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Learn More:&lt;/strong&gt; &lt;a href="https://docs.getbifrost.ai/quickstart/gateway/setting-up"&gt;Gateway Setup Guide&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;2. Go SDK&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Best for:&lt;/strong&gt; Direct Go integration with maximum performance and control&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;go get github.com/maximhq/bifrost/core
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Features:&lt;/strong&gt; Native Go APIs, embedded deployment, custom middleware integration&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Learn More:&lt;/strong&gt; &lt;a href="https://docs.getbifrost.ai/quickstart/go-sdk/setting-up"&gt;Go SDK Guide&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;3. Drop-in Replacement&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Best for:&lt;/strong&gt; Migrating existing applications with zero code changes&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-diff"&gt;# OpenAI SDK
- base_url = "https://api.openai.com"
+ base_url = "http://localhost:8080/openai"

# Anthropic SDK  
- base_url = "https://api.anthropic.com"
+ base_url = "http://localhost:8080/anthropic"

# Google GenAI SDK
- api_endpoint = "https://generativelanguage.googleapis.com"
+ api_endpoint = "http://localhost:8080/genai"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Learn More:&lt;/strong&gt; &lt;a href="https://docs.getbifrost.ai/integrations/what-is-an-integration"&gt;Integration Guides&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Performance&lt;/h2&gt; 
&lt;p&gt;Bifrost adds virtually zero overhead to your AI requests. In sustained 5,000 RPS benchmarks, the gateway added only &lt;strong&gt;11 ¬µs&lt;/strong&gt; of overhead per request.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Metric&lt;/th&gt; 
   &lt;th&gt;t3.medium&lt;/th&gt; 
   &lt;th&gt;t3.xlarge&lt;/th&gt; 
   &lt;th&gt;Improvement&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Added latency (Bifrost overhead)&lt;/td&gt; 
   &lt;td&gt;59 ¬µs&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;11 ¬µs&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;-81%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Success rate @ 5k RPS&lt;/td&gt; 
   &lt;td&gt;100%&lt;/td&gt; 
   &lt;td&gt;100%&lt;/td&gt; 
   &lt;td&gt;No failed requests&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Avg. queue wait time&lt;/td&gt; 
   &lt;td&gt;47 ¬µs&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;1.67 ¬µs&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;-96%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Avg. request latency (incl. provider)&lt;/td&gt; 
   &lt;td&gt;2.12 s&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;1.61 s&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;-24%&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Key Performance Highlights:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Perfect Success Rate&lt;/strong&gt; - 100% request success rate even at 5k RPS&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Minimal Overhead&lt;/strong&gt; - Less than 15 ¬µs additional latency per request&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Efficient Queuing&lt;/strong&gt; - Sub-microsecond average wait times&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Fast Key Selection&lt;/strong&gt; - ~10 ns to pick weighted API keys&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Complete Benchmarks:&lt;/strong&gt; &lt;a href="https://docs.getbifrost.ai/benchmarking/getting-started"&gt;Performance Analysis&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Complete Documentation:&lt;/strong&gt; &lt;a href="https://docs.getbifrost.ai"&gt;https://docs.getbifrost.ai&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Quick Start&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.getbifrost.ai/quickstart/gateway/setting-up"&gt;Gateway Setup&lt;/a&gt; - HTTP API deployment in 30 seconds&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.getbifrost.ai/quickstart/go-sdk/setting-up"&gt;Go SDK Setup&lt;/a&gt; - Direct Go integration&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.getbifrost.ai/quickstart/gateway/provider-configuration"&gt;Provider Configuration&lt;/a&gt; - Multi-provider setup&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Features&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.getbifrost.ai/features/unified-interface"&gt;Multi-Provider Support&lt;/a&gt; - Single API for all providers&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.getbifrost.ai/features/mcp"&gt;MCP Integration&lt;/a&gt; - External tool calling&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.getbifrost.ai/features/semantic-caching"&gt;Semantic Caching&lt;/a&gt; - Intelligent response caching&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.getbifrost.ai/features/fallbacks"&gt;Fallbacks &amp;amp; Load Balancing&lt;/a&gt; - Reliability features&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.getbifrost.ai/features/governance"&gt;Budget Management&lt;/a&gt; - Cost control and governance&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Integrations&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.getbifrost.ai/integrations/openai-sdk"&gt;OpenAI SDK&lt;/a&gt; - Drop-in OpenAI replacement&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.getbifrost.ai/integrations/anthropic-sdk"&gt;Anthropic SDK&lt;/a&gt; - Drop-in Anthropic replacement&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.getbifrost.ai/integrations/bedrock-sdk"&gt;AWS Bedrock SDK&lt;/a&gt; - AWS Bedrock integration&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.getbifrost.ai/integrations/genai-sdk"&gt;Google GenAI SDK&lt;/a&gt; - Drop-in GenAI replacement&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.getbifrost.ai/integrations/litellm-sdk"&gt;LiteLLM SDK&lt;/a&gt; - LiteLLM integration&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.getbifrost.ai/integrations/langchain-sdk"&gt;Langchain SDK&lt;/a&gt; - Langchain integration&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Enterprise&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.getbifrost.ai/enterprise/custom-plugins"&gt;Custom Plugins&lt;/a&gt; - Extend functionality&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.getbifrost.ai/enterprise/clustering"&gt;Clustering&lt;/a&gt; - Multi-node deployment&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.getbifrost.ai/enterprise/vault-support"&gt;Vault Support&lt;/a&gt; - Secure key management&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.getbifrost.ai/deployment/docker-setup"&gt;Production Deployment&lt;/a&gt; - Scaling and monitoring&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Need Help?&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://discord.gg/exN5KAydbU"&gt;Join our Discord&lt;/a&gt;&lt;/strong&gt; for community support and discussions.&lt;/p&gt; 
&lt;p&gt;Get help with:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Quick setup assistance and troubleshooting&lt;/li&gt; 
 &lt;li&gt;Best practices and configuration tips&lt;/li&gt; 
 &lt;li&gt;Community discussions and support&lt;/li&gt; 
 &lt;li&gt;Real-time help with integrations&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions of all kinds! See our &lt;a href="https://docs.getbifrost.ai/contributing/setting-up-repo"&gt;Contributing Guide&lt;/a&gt; for:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Setting up the development environment&lt;/li&gt; 
 &lt;li&gt;Code conventions and best practices&lt;/li&gt; 
 &lt;li&gt;How to submit pull requests&lt;/li&gt; 
 &lt;li&gt;Building and testing locally&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For development requirements and build instructions, see our &lt;a href="https://docs.getbifrost.ai/contributing/setting-up-repo#development-environment-setup"&gt;Development Setup Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the Apache 2.0 License - see the &lt;a href="https://raw.githubusercontent.com/maximhq/bifrost/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;p&gt;Built with ‚ù§Ô∏è by &lt;a href="https://github.com/maximhq"&gt;Maxim&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>modelcontextprotocol/registry</title>
      <link>https://github.com/modelcontextprotocol/registry</link>
      <description>&lt;p&gt;A community driven registry service for Model Context Protocol (MCP) servers.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MCP Registry&lt;/h1&gt; 
&lt;p&gt;The MCP registry provides MCP clients with a list of MCP servers, like an app store for MCP servers.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/registry/main/docs/modelcontextprotocol-io/quickstart.mdx"&gt;&lt;strong&gt;üì§ Publish my MCP server&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://registry.modelcontextprotocol.io/docs"&gt;&lt;strong&gt;‚ö°Ô∏è Live API docs&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/registry/main/docs/design/ecosystem-vision.md"&gt;&lt;strong&gt;üëÄ Ecosystem vision&lt;/strong&gt;&lt;/a&gt; | üìñ &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/registry/main/docs"&gt;Full documentation&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;Development Status&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;2025-10-24 update&lt;/strong&gt;: The Registry API has entered an &lt;strong&gt;API freeze (v0.1)&lt;/strong&gt; üéâ. For the next month or more, the API will remain stable with no breaking changes, allowing integrators to confidently implement support. This freeze applies to v0.1 while development continues on v0. We'll use this period to validate the API in real-world integrations and gather feedback to shape v1 for general availability. Thank you to everyone for your contributions and patience‚Äîyour involvement has been key to getting us here!&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;2025-09-08 update&lt;/strong&gt;: The registry has launched in preview üéâ (&lt;a href="https://blog.modelcontextprotocol.io/posts/2025-09-08-mcp-registry-preview/"&gt;announcement blog post&lt;/a&gt;). While the system is now more stable, this is still a preview release and breaking changes or data resets may occur. A general availability (GA) release will follow later. We'd love your feedback in &lt;a href="https://github.com/modelcontextprotocol/registry/discussions/new?category=ideas"&gt;GitHub discussions&lt;/a&gt; or in the &lt;a href="https://discord.com/channels/1358869848138059966/1369487942862504016"&gt;#registry-dev Discord&lt;/a&gt; (&lt;a href="https://modelcontextprotocol.io/community/communication"&gt;joining details here&lt;/a&gt;).&lt;/p&gt; 
&lt;p&gt;Current key maintainers:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Adam Jones&lt;/strong&gt; (Anthropic) &lt;a href="https://github.com/domdomegg"&gt;@domdomegg&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Tadas Antanavicius&lt;/strong&gt; (PulseMCP) &lt;a href="https://github.com/tadasant"&gt;@tadasant&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Toby Padilla&lt;/strong&gt; (GitHub) &lt;a href="https://github.com/toby"&gt;@toby&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Radoslav (Rado) Dimitrov&lt;/strong&gt; (Stacklok) &lt;a href="https://github.com/rdimitrov"&gt;@rdimitrov&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We use multiple channels for collaboration - see &lt;a href="https://modelcontextprotocol.io/community/communication"&gt;modelcontextprotocol.io/community/communication&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Often (but not always) ideas flow through this pipeline:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://modelcontextprotocol.io/community/communication"&gt;Discord&lt;/a&gt;&lt;/strong&gt; - Real-time community discussions&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/modelcontextprotocol/registry/discussions"&gt;Discussions&lt;/a&gt;&lt;/strong&gt; - Propose and discuss product/technical requirements&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/modelcontextprotocol/registry/issues"&gt;Issues&lt;/a&gt;&lt;/strong&gt; - Track well-scoped technical work&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/modelcontextprotocol/registry/pulls"&gt;Pull Requests&lt;/a&gt;&lt;/strong&gt; - Contribute work towards issues&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Quick start:&lt;/h3&gt; 
&lt;h4&gt;Pre-requisites&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Docker&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Go 1.24.x&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ko&lt;/strong&gt; - Container image builder for Go (&lt;a href="https://ko.build/install/"&gt;installation instructions&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;golangci-lint v2.4.0&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Running the server&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Start full development environment
make dev-compose
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This starts the registry at &lt;a href="http://localhost:8080"&gt;&lt;code&gt;localhost:8080&lt;/code&gt;&lt;/a&gt; with PostgreSQL. The database uses ephemeral storage and is reset each time you restart the containers, ensuring a clean state for development and testing.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; The registry uses &lt;a href="https://ko.build"&gt;ko&lt;/a&gt; to build container images. The &lt;code&gt;make dev-compose&lt;/code&gt; command automatically builds the registry image with ko and loads it into your local Docker daemon before starting the services.&lt;/p&gt; 
&lt;p&gt;By default, the registry seeds from the production API with a filtered subset of servers (to keep startup fast). This ensures your local environment mirrors production behavior and all seed data passes validation. For offline development you can seed from a file without validation with &lt;code&gt;MCP_REGISTRY_SEED_FROM=data/seed.json MCP_REGISTRY_ENABLE_REGISTRY_VALIDATION=false make dev-compose&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;The setup can be configured with environment variables in &lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/registry/main/docker-compose.yml"&gt;docker-compose.yml&lt;/a&gt; - see &lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/registry/main/.env.example"&gt;.env.example&lt;/a&gt; for a reference.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Alternative: Running a pre-built Docker image&lt;/summary&gt; 
 &lt;p&gt;Pre-built Docker images are automatically published to GitHub Container Registry:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# Run latest stable release
docker run -p 8080:8080 ghcr.io/modelcontextprotocol/registry:latest

# Run latest from main branch (continuous deployment)
docker run -p 8080:8080 ghcr.io/modelcontextprotocol/registry:main

# Run specific release version
docker run -p 8080:8080 ghcr.io/modelcontextprotocol/registry:v1.0.0

# Run development build from main branch
docker run -p 8080:8080 ghcr.io/modelcontextprotocol/registry:main-20250906-abc123d
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Available tags:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Releases&lt;/strong&gt;: &lt;code&gt;latest&lt;/code&gt;, &lt;code&gt;v1.0.0&lt;/code&gt;, &lt;code&gt;v1.1.0&lt;/code&gt;, etc.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Continuous&lt;/strong&gt;: &lt;code&gt;main&lt;/code&gt; (latest main branch build)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Development&lt;/strong&gt;: &lt;code&gt;main-&amp;lt;date&amp;gt;-&amp;lt;sha&amp;gt;&lt;/code&gt; (specific commit builds)&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h4&gt;Publishing a server&lt;/h4&gt; 
&lt;p&gt;To publish a server, we've built a simple CLI. You can use it with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Build the latest CLI
make publisher

# Use it!
./bin/mcp-publisher --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/registry/main/docs/modelcontextprotocol-io/quickstart.mdx"&gt;the publisher guide&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h4&gt;Other commands&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Run lint, unit tests and integration tests
make check
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;There are also a few more helpful commands for development. Run &lt;code&gt;make help&lt;/code&gt; to learn more, or look in &lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/registry/main/Makefile"&gt;Makefile&lt;/a&gt;.&lt;/p&gt; 
&lt;!--
For Claude and other AI tools: Always prefer make targets over custom commands where possible.
--&gt; 
&lt;h2&gt;Architecture&lt;/h2&gt; 
&lt;h3&gt;Project Structure&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;‚îú‚îÄ‚îÄ cmd/                     # Application entry points
‚îÇ   ‚îî‚îÄ‚îÄ publisher/           # Server publishing tool
‚îú‚îÄ‚îÄ data/                    # Seed data
‚îú‚îÄ‚îÄ deploy/                  # Deployment configuration (Pulumi)
‚îú‚îÄ‚îÄ docs/                    # Documentation
‚îú‚îÄ‚îÄ internal/                # Private application code
‚îÇ   ‚îú‚îÄ‚îÄ api/                 # HTTP handlers and routing
‚îÇ   ‚îú‚îÄ‚îÄ auth/                # Authentication (GitHub OAuth, JWT, namespace blocking)
‚îÇ   ‚îú‚îÄ‚îÄ config/              # Configuration management
‚îÇ   ‚îú‚îÄ‚îÄ database/            # Data persistence (PostgreSQL)
‚îÇ   ‚îú‚îÄ‚îÄ service/             # Business logic
‚îÇ   ‚îú‚îÄ‚îÄ telemetry/           # Metrics and monitoring
‚îÇ   ‚îî‚îÄ‚îÄ validators/          # Input validation
‚îú‚îÄ‚îÄ pkg/                     # Public packages
‚îÇ   ‚îú‚îÄ‚îÄ api/                 # API types and structures
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ v0/              # Version 0 API types
‚îÇ   ‚îî‚îÄ‚îÄ model/               # Data models for server.json
‚îú‚îÄ‚îÄ scripts/                 # Development and testing scripts
‚îú‚îÄ‚îÄ tests/                   # Integration tests
‚îî‚îÄ‚îÄ tools/                   # CLI tools and utilities
    ‚îî‚îÄ‚îÄ validate-*.sh        # Schema validation tools
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Authentication&lt;/h3&gt; 
&lt;p&gt;Publishing supports multiple authentication methods:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;GitHub OAuth&lt;/strong&gt; - For publishing by logging into GitHub&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;GitHub OIDC&lt;/strong&gt; - For publishing from GitHub Actions&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;DNS verification&lt;/strong&gt; - For proving ownership of a domain and its subdomains&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;HTTP verification&lt;/strong&gt; - For proving ownership of a domain&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The registry validates namespace ownership when publishing. E.g. to publish...:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;io.github.domdomegg/my-cool-mcp&lt;/code&gt; you must login to GitHub as &lt;code&gt;domdomegg&lt;/code&gt;, or be in a GitHub Action on domdomegg's repos&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;me.adamjones/my-cool-mcp&lt;/code&gt; you must prove ownership of &lt;code&gt;adamjones.me&lt;/code&gt; via DNS or HTTP challenge&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Community Projects&lt;/h2&gt; 
&lt;p&gt;Check out &lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/registry/main/docs/community-projects.md"&gt;community projects&lt;/a&gt; to explore notable registry-related work created by the community.&lt;/p&gt; 
&lt;h2&gt;More documentation&lt;/h2&gt; 
&lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/modelcontextprotocol/registry/main/docs"&gt;documentation&lt;/a&gt; for more details if your question has not been answered here!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Tencent/WeKnora</title>
      <link>https://github.com/Tencent/WeKnora</link>
      <description>&lt;p&gt;LLM-powered framework for deep document understanding, semantic retrieval, and context-aware answers using RAG paradigm.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; 
 &lt;picture&gt; 
  &lt;img src="https://raw.githubusercontent.com/Tencent/WeKnora/main/docs/images/logo.png" alt="WeKnora Logo" height="120" /&gt; 
 &lt;/picture&gt; &lt;/p&gt; 
&lt;p align="center"&gt; 
 &lt;picture&gt; 
  &lt;a href="https://trendshift.io/repositories/15289" target="_blank"&gt; &lt;img src="https://trendshift.io/api/badge/repositories/15289" alt="Tencent%2FWeKnora | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt; &lt;/a&gt; 
 &lt;/picture&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://weknora.weixin.qq.com" target="_blank"&gt; &lt;img alt="ÂÆòÊñπÁΩëÁ´ô" src="https://img.shields.io/badge/ÂÆòÊñπÁΩëÁ´ô-WeKnora-4e6b99" /&gt; &lt;/a&gt; &lt;a href="https://chatbot.weixin.qq.com" target="_blank"&gt; &lt;img alt="ÂæÆ‰ø°ÂØπËØùÂºÄÊîæÂπ≥Âè∞" src="https://img.shields.io/badge/ÂæÆ‰ø°ÂØπËØùÂºÄÊîæÂπ≥Âè∞-5ac725" /&gt; &lt;/a&gt; &lt;a href="https://github.com/Tencent/WeKnora/raw/main/LICENSE"&gt; &lt;img src="https://img.shields.io/badge/License-MIT-ffffff?labelColor=d4eaf7&amp;amp;color=2e6cc4" alt="License" /&gt; &lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Tencent/WeKnora/main/CHANGELOG.md"&gt; &lt;img alt="Version" src="https://img.shields.io/badge/version-0.2.10-2e6cc4?labelColor=d4eaf7" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; | &lt;b&gt;English&lt;/b&gt; | &lt;a href="https://raw.githubusercontent.com/Tencent/WeKnora/main/README_CN.md"&gt;&lt;b&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/Tencent/WeKnora/main/README_JA.md"&gt;&lt;b&gt;Êó•Êú¨Ë™û&lt;/b&gt;&lt;/a&gt; | &lt;/p&gt; 
&lt;p align="center"&gt; &lt;/p&gt;
&lt;h4 align="center"&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/Tencent/WeKnora/main/#-overview"&gt;Overview&lt;/a&gt; ‚Ä¢ &lt;a href="https://raw.githubusercontent.com/Tencent/WeKnora/main/#-architecture"&gt;Architecture&lt;/a&gt; ‚Ä¢ &lt;a href="https://raw.githubusercontent.com/Tencent/WeKnora/main/#-key-features"&gt;Key Features&lt;/a&gt; ‚Ä¢ &lt;a href="https://raw.githubusercontent.com/Tencent/WeKnora/main/#-getting-started"&gt;Getting Started&lt;/a&gt; ‚Ä¢ &lt;a href="https://raw.githubusercontent.com/Tencent/WeKnora/main/#-api-reference"&gt;API Reference&lt;/a&gt; ‚Ä¢ &lt;a href="https://raw.githubusercontent.com/Tencent/WeKnora/main/#-developer-guide"&gt;Developer Guide&lt;/a&gt;&lt;/p&gt; &lt;/h4&gt; 
&lt;p&gt;&lt;/p&gt; 
&lt;h1&gt;üí° WeKnora - LLM-Powered Document Understanding &amp;amp; Retrieval Framework&lt;/h1&gt; 
&lt;h2&gt;üìå Overview&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://weknora.weixin.qq.com"&gt;&lt;strong&gt;WeKnora&lt;/strong&gt;&lt;/a&gt; is an LLM-powered framework designed for deep document understanding and semantic retrieval, especially for handling complex, heterogeneous documents.&lt;/p&gt; 
&lt;p&gt;It adopts a modular architecture that combines multimodal preprocessing, semantic vector indexing, intelligent retrieval, and large language model inference. At its core, WeKnora follows the &lt;strong&gt;RAG (Retrieval-Augmented Generation)&lt;/strong&gt; paradigm, enabling high-quality, context-aware answers by combining relevant document chunks with model reasoning.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Website:&lt;/strong&gt; &lt;a href="https://weknora.weixin.qq.com"&gt;https://weknora.weixin.qq.com&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;‚ú® Latest Updates&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;v0.2.0 Highlights:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ü§ñ &lt;strong&gt;Agent Mode&lt;/strong&gt;: New ReACT Agent mode that can call built-in tools, MCP tools, and web search, providing comprehensive summary reports through multiple iterations and reflection&lt;/li&gt; 
 &lt;li&gt;üìö &lt;strong&gt;Multi-Type Knowledge Bases&lt;/strong&gt;: Support for FAQ and document knowledge base types, with new features including folder import, URL import, tag management, and online entry&lt;/li&gt; 
 &lt;li&gt;‚öôÔ∏è &lt;strong&gt;Conversation Strategy&lt;/strong&gt;: Support for configuring Agent models, normal mode models, retrieval thresholds, and Prompts, with precise control over multi-turn conversation behavior&lt;/li&gt; 
 &lt;li&gt;üåê &lt;strong&gt;Web Search&lt;/strong&gt;: Support for extensible web search engines with built-in DuckDuckGo search engine&lt;/li&gt; 
 &lt;li&gt;üîå &lt;strong&gt;MCP Tool Integration&lt;/strong&gt;: Support for extending Agent capabilities through MCP, with built-in uvx and npx launchers, supporting multiple transport methods&lt;/li&gt; 
 &lt;li&gt;üé® &lt;strong&gt;New UI&lt;/strong&gt;: Optimized conversation interface with Agent mode/normal mode switching, tool call process display, and comprehensive knowledge base management interface upgrade&lt;/li&gt; 
 &lt;li&gt;‚ö° &lt;strong&gt;Infrastructure Upgrade&lt;/strong&gt;: Introduced MQ async task management, support for automatic database migration, and fast development mode&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üîí Security Notice&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Important:&lt;/strong&gt; Starting from v0.1.3, WeKnora includes login authentication functionality to enhance system security. For production deployments, we strongly recommend:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Deploy WeKnora services in internal/private network environments rather than public internet&lt;/li&gt; 
 &lt;li&gt;Avoid exposing the service directly to public networks to prevent potential information leakage&lt;/li&gt; 
 &lt;li&gt;Configure proper firewall rules and access controls for your deployment environment&lt;/li&gt; 
 &lt;li&gt;Regularly update to the latest version for security patches and improvements&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üèóÔ∏è Architecture&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Tencent/WeKnora/main/docs/images/architecture.png" alt="weknora-architecture.png" /&gt;&lt;/p&gt; 
&lt;p&gt;WeKnora employs a modern modular design to build a complete document understanding and retrieval pipeline. The system primarily includes document parsing, vector processing, retrieval engine, and large model inference as core modules, with each component being flexibly configurable and extendable.&lt;/p&gt; 
&lt;h2&gt;üéØ Key Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ü§ñ Agent Mode&lt;/strong&gt;: Support for ReACT Agent mode that can use built-in tools to retrieve knowledge bases, MCP tools, and web search tools to access external services, providing comprehensive summary reports through multiple iterations and reflection&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üîç Precise Understanding&lt;/strong&gt;: Structured content extraction from PDFs, Word documents, images and more into unified semantic views&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üß† Intelligent Reasoning&lt;/strong&gt;: Leverages LLMs to understand document context and user intent for accurate Q&amp;amp;A and multi-turn conversations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üìö Multi-Type Knowledge Bases&lt;/strong&gt;: Support for FAQ and document knowledge base types, with folder import, URL import, tag management, and online entry capabilities&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üîß Flexible Extension&lt;/strong&gt;: All components from parsing and embedding to retrieval and generation are decoupled for easy customization&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;‚ö° Efficient Retrieval&lt;/strong&gt;: Hybrid retrieval strategies combining keywords, vectors, and knowledge graphs, with cross-knowledge base retrieval support&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üåê Web Search&lt;/strong&gt;: Support for extensible web search engines with built-in DuckDuckGo search engine&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üîå MCP Tool Integration&lt;/strong&gt;: Support for extending Agent capabilities through MCP, with built-in uvx and npx launchers, supporting multiple transport methods&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;‚öôÔ∏è Conversation Strategy&lt;/strong&gt;: Support for configuring Agent models, normal mode models, retrieval thresholds, and Prompts, with precise control over multi-turn conversation behavior&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üéØ User-Friendly&lt;/strong&gt;: Intuitive web interface and standardized APIs for zero technical barriers&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üîí Secure &amp;amp; Controlled&lt;/strong&gt;: Support for local deployment and private cloud, ensuring complete data sovereignty&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üìä Application Scenarios&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Scenario&lt;/th&gt; 
   &lt;th&gt;Applications&lt;/th&gt; 
   &lt;th&gt;Core Value&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Enterprise Knowledge Management&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Internal document retrieval, policy Q&amp;amp;A, operation manual search&lt;/td&gt; 
   &lt;td&gt;Improve knowledge discovery efficiency, reduce training costs&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Academic Research Analysis&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Paper retrieval, research report analysis, scholarly material organization&lt;/td&gt; 
   &lt;td&gt;Accelerate literature review, assist research decisions&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Product Technical Support&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Product manual Q&amp;amp;A, technical documentation search, troubleshooting&lt;/td&gt; 
   &lt;td&gt;Enhance customer service quality, reduce support burden&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Legal &amp;amp; Compliance Review&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Contract clause retrieval, regulatory policy search, case analysis&lt;/td&gt; 
   &lt;td&gt;Improve compliance efficiency, reduce legal risks&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Medical Knowledge Assistance&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Medical literature retrieval, treatment guideline search, case analysis&lt;/td&gt; 
   &lt;td&gt;Support clinical decisions, improve diagnosis quality&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;üß© Feature Matrix&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Module&lt;/th&gt; 
   &lt;th&gt;Support&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Agent Mode&lt;/td&gt; 
   &lt;td&gt;‚úÖ ReACT Agent Mode&lt;/td&gt; 
   &lt;td&gt;Support for using built-in tools to retrieve knowledge bases, MCP tools, and web search, with cross-knowledge base retrieval and multiple iterations&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Knowledge Base Types&lt;/td&gt; 
   &lt;td&gt;‚úÖ FAQ / Document&lt;/td&gt; 
   &lt;td&gt;Support for creating FAQ and document knowledge base types, with folder import, URL import, tag management, and online entry&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Document Formats&lt;/td&gt; 
   &lt;td&gt;‚úÖ PDF / Word / Txt / Markdown / Images (with OCR / Caption)&lt;/td&gt; 
   &lt;td&gt;Support for structured and unstructured documents with text extraction from images&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Model Management&lt;/td&gt; 
   &lt;td&gt;‚úÖ Centralized configuration, built-in model sharing&lt;/td&gt; 
   &lt;td&gt;Centralized model configuration with model selection in knowledge base settings, support for multi-tenant shared built-in models&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Embedding Models&lt;/td&gt; 
   &lt;td&gt;‚úÖ Local models, BGE / GTE APIs, etc.&lt;/td&gt; 
   &lt;td&gt;Customizable embedding models, compatible with local deployment and cloud vector generation APIs&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Vector DB Integration&lt;/td&gt; 
   &lt;td&gt;‚úÖ PostgreSQL (pgvector), Elasticsearch&lt;/td&gt; 
   &lt;td&gt;Support for mainstream vector index backends, flexible switching for different retrieval scenarios&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Retrieval Strategies&lt;/td&gt; 
   &lt;td&gt;‚úÖ BM25 / Dense Retrieval / GraphRAG&lt;/td&gt; 
   &lt;td&gt;Support for sparse/dense recall and knowledge graph-enhanced retrieval with customizable retrieve-rerank-generate pipelines&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;LLM Integration&lt;/td&gt; 
   &lt;td&gt;‚úÖ Support for Qwen, DeepSeek, etc., with thinking/non-thinking mode switching&lt;/td&gt; 
   &lt;td&gt;Compatible with local models (e.g., via Ollama) or external API services with flexible inference configuration&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Conversation Strategy&lt;/td&gt; 
   &lt;td&gt;‚úÖ Agent models, normal mode models, retrieval thresholds, Prompt configuration&lt;/td&gt; 
   &lt;td&gt;Support for configuring Agent models, normal mode models, retrieval thresholds, online Prompt configuration, precise control over multi-turn conversation behavior&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Web Search&lt;/td&gt; 
   &lt;td&gt;‚úÖ Extensible search engines, DuckDuckGo / Google&lt;/td&gt; 
   &lt;td&gt;Support for extensible web search engines with built-in DuckDuckGo search engine&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MCP Tools&lt;/td&gt; 
   &lt;td&gt;‚úÖ uvx, npx launchers, Stdio/HTTP Streamable/SSE&lt;/td&gt; 
   &lt;td&gt;Support for extending Agent capabilities through MCP, with built-in uvx and npx launchers, supporting three transport methods&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;QA Capabilities&lt;/td&gt; 
   &lt;td&gt;‚úÖ Context-aware, multi-turn dialogue, prompt templates&lt;/td&gt; 
   &lt;td&gt;Support for complex semantic modeling, instruction control and chain-of-thought Q&amp;amp;A with configurable prompts and context windows&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;E2E Testing&lt;/td&gt; 
   &lt;td&gt;‚úÖ Retrieval+generation process visualization and metric evaluation&lt;/td&gt; 
   &lt;td&gt;End-to-end testing tools for evaluating recall hit rates, answer coverage, BLEU/ROUGE and other metrics&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Deployment Modes&lt;/td&gt; 
   &lt;td&gt;‚úÖ Support for local deployment / Docker images&lt;/td&gt; 
   &lt;td&gt;Meets private, offline deployment and flexible operation requirements, with fast development mode support&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;User Interfaces&lt;/td&gt; 
   &lt;td&gt;‚úÖ Web UI + RESTful API&lt;/td&gt; 
   &lt;td&gt;Interactive interface and standard API endpoints, with Agent mode/normal mode switching and tool call process display&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Task Management&lt;/td&gt; 
   &lt;td&gt;‚úÖ MQ async tasks, automatic database migration&lt;/td&gt; 
   &lt;td&gt;MQ-based async task state maintenance, support for automatic database schema and data migration during version upgrades&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;üöÄ Getting Started&lt;/h2&gt; 
&lt;h3&gt;üõ† Prerequisites&lt;/h3&gt; 
&lt;p&gt;Make sure the following tools are installed on your system:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.docker.com/"&gt;Docker&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.docker.com/compose/"&gt;Docker Compose&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://git-scm.com/"&gt;Git&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üì¶ Installation&lt;/h3&gt; 
&lt;h4&gt;‚ë† Clone the repository&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Clone the main repository
git clone https://github.com/Tencent/WeKnora.git
cd WeKnora
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;‚ë° Configure environment variables&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Copy example env file
cp .env.example .env

# Edit .env and set required values
# All variables are documented in the .env.example comments
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;‚ë¢ Start the services (include Ollama)&lt;/h4&gt; 
&lt;p&gt;Check the images that need to be started in the .env file.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;./scripts/start_all.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;make start-all
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;‚ë¢.0 Start ollama services (Optional)&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;ollama serve &amp;gt; /dev/null 2&amp;gt;&amp;amp;1 &amp;amp;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;‚ë¢.1 Activate different combinations of features&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Minimum core services&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker compose up -d
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;All features enabled&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker-compose --profile full up -d
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Tracing logs required&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker-compose --profile jaeger up -d
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Neo4j knowledge graph required&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker-compose --profile neo4j up -d
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Minio file storage service required&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker-compose --profile minio up -d
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Multiple options combination&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker-compose --profile neo4j --profile minio up -d
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;‚ë£ Stop the services&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;./scripts/start_all.sh --stop
# Or
make stop-all
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;üåê Access Services&lt;/h3&gt; 
&lt;p&gt;Once started, services will be available at:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Web UI: &lt;code&gt;http://localhost&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Backend API: &lt;code&gt;http://localhost:8080&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Jaeger Tracing: &lt;code&gt;http://localhost:16686&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üîå Using WeChat Dialog Open Platform&lt;/h3&gt; 
&lt;p&gt;WeKnora serves as the core technology framework for the &lt;a href="https://chatbot.weixin.qq.com"&gt;WeChat Dialog Open Platform&lt;/a&gt;, providing a more convenient usage approach:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Zero-code Deployment&lt;/strong&gt;: Simply upload knowledge to quickly deploy intelligent Q&amp;amp;A services within the WeChat ecosystem, achieving an "ask and answer" experience&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Efficient Question Management&lt;/strong&gt;: Support for categorized management of high-frequency questions, with rich data tools to ensure accurate, reliable, and easily maintainable answers&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;WeChat Ecosystem Integration&lt;/strong&gt;: Through the WeChat Dialog Open Platform, WeKnora's intelligent Q&amp;amp;A capabilities can be seamlessly integrated into WeChat Official Accounts, Mini Programs, and other WeChat scenarios, enhancing user interaction experiences&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üîó Access WeKnora via MCP Server&lt;/h3&gt; 
&lt;h4&gt;1Ô∏è‚É£ Clone the repository&lt;/h4&gt; 
&lt;pre&gt;&lt;code&gt;git clone https://github.com/Tencent/WeKnora
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;2Ô∏è‚É£ Configure MCP Server&lt;/h4&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;It is recommended to directly refer to the &lt;a href="https://raw.githubusercontent.com/Tencent/WeKnora/main/mcp-server/MCP_CONFIG.md"&gt;MCP Configuration Guide&lt;/a&gt; for configuration.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Configure the MCP client to connect to the server:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "mcpServers": {
    "weknora": {
      "args": [
        "path/to/WeKnora/mcp-server/run_server.py"
      ],
      "command": "python",
      "env":{
        "WEKNORA_API_KEY":"Enter your WeKnora instance, open developer tools, check the request header x-api-key starting with sk",
        "WEKNORA_BASE_URL":"http(s)://your-weknora-address/api/v1"
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Run directly using stdio command:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install weknora-mcp-server
python -m weknora-mcp-server
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;üîß Initialization Configuration Guide&lt;/h2&gt; 
&lt;p&gt;To help users quickly configure various models and reduce trial-and-error costs, we've improved the original configuration file initialization method by adding a Web UI interface for model configuration. Before using, please ensure the code is updated to the latest version. The specific steps are as follows: If this is your first time using this project, you can skip steps ‚ë†‚ë° and go directly to steps ‚ë¢‚ë£.&lt;/p&gt; 
&lt;h3&gt;‚ë† Stop the services&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;./scripts/start_all.sh --stop
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;‚ë° Clear existing data tables (recommended when no important data exists)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;make clean-db
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;‚ë¢ Compile and start services&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;./scripts/start_all.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;‚ë£ Access Web UI&lt;/h3&gt; 
&lt;p&gt;&lt;a href="http://localhost"&gt;http://localhost&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;On your first visit, you will be automatically redirected to the registration/login page. After completing registration, please create a new knowledge base and finish the relevant settings on its configuration page.&lt;/p&gt; 
&lt;h2&gt;üì± Interface Showcase&lt;/h2&gt; 
&lt;h3&gt;Web UI Interface&lt;/h3&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;Knowledge Base Management&lt;/b&gt;&lt;br /&gt;&lt;img src="https://raw.githubusercontent.com/Tencent/WeKnora/main/docs/images/knowledgebases.png" alt="Knowledge Base Management" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;b&gt;Conversation Settings&lt;/b&gt;&lt;br /&gt;&lt;img src="https://raw.githubusercontent.com/Tencent/WeKnora/main/docs/images/settings.png" alt="Conversation Settings" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td colspan="2"&gt;&lt;b&gt;Agent Mode Tool Call Process&lt;/b&gt;&lt;br /&gt;&lt;img src="https://raw.githubusercontent.com/Tencent/WeKnora/main/docs/images/agent-qa.png" alt="Agent Mode Tool Call Process" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Knowledge Base Management:&lt;/strong&gt; Support for creating FAQ and document knowledge base types, with multiple import methods including drag-and-drop, folder import, and URL import. Automatically identifies document structures and extracts core knowledge to establish indexes. Supports tag management and online entry. The system clearly displays processing progress and document status, achieving efficient knowledge base management.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Agent Mode:&lt;/strong&gt; Support for ReACT Agent mode that can use built-in tools to retrieve knowledge bases, call user-configured MCP tools and web search tools to access external services, providing comprehensive summary reports through multiple iterations and reflection. Supports cross-knowledge base retrieval, allowing selection of multiple knowledge bases for simultaneous retrieval.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Conversation Strategy:&lt;/strong&gt; Support for configuring Agent models, normal mode models, retrieval thresholds, and online Prompt configuration, with precise control over multi-turn conversation behavior and retrieval execution methods. The conversation input box supports Agent mode/normal mode switching, enabling/disabling web search, and selecting conversation models.&lt;/p&gt; 
&lt;h3&gt;Document Knowledge Graph&lt;/h3&gt; 
&lt;p&gt;WeKnora supports transforming documents into knowledge graphs, displaying the relationships between different sections of the documents. Once the knowledge graph feature is enabled, the system analyzes and constructs an internal semantic association network that not only helps users understand document content but also provides structured support for indexing and retrieval, enhancing the relevance and breadth of search results.&lt;/p&gt; 
&lt;p&gt;For detailed configuration, please refer to the &lt;a href="https://raw.githubusercontent.com/Tencent/WeKnora/main/docs/KnowledgeGraph.md"&gt;Knowledge Graph Configuration Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;MCP Server&lt;/h3&gt; 
&lt;p&gt;Please refer to the &lt;a href="https://raw.githubusercontent.com/Tencent/WeKnora/main/mcp-server/MCP_CONFIG.md"&gt;MCP Configuration Guide&lt;/a&gt; for the necessary setup.&lt;/p&gt; 
&lt;h2&gt;üìò API Reference&lt;/h2&gt; 
&lt;p&gt;Troubleshooting FAQ: &lt;a href="https://raw.githubusercontent.com/Tencent/WeKnora/main/docs/QA.md"&gt;Troubleshooting FAQ&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Detailed API documentation is available at: &lt;a href="https://raw.githubusercontent.com/Tencent/WeKnora/main/docs/api/README.md"&gt;API Docs&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;üß≠ Developer Guide&lt;/h2&gt; 
&lt;h3&gt;‚ö° Fast Development Mode (Recommended)&lt;/h3&gt; 
&lt;p&gt;If you need to frequently modify code, &lt;strong&gt;you don't need to rebuild Docker images every time&lt;/strong&gt;! Use fast development mode:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Method 1: Using Make commands (Recommended)
make dev-start      # Start infrastructure
make dev-app        # Start backend (new terminal)
make dev-frontend   # Start frontend (new terminal)

# Method 2: One-click start
./scripts/quick-dev.sh

# Method 3: Using scripts
./scripts/dev.sh start     # Start infrastructure
./scripts/dev.sh app       # Start backend (new terminal)
./scripts/dev.sh frontend  # Start frontend (new terminal)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Development Advantages:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;‚úÖ Frontend modifications auto hot-reload (no restart needed)&lt;/li&gt; 
 &lt;li&gt;‚úÖ Backend modifications quick restart (5-10 seconds, supports Air hot-reload)&lt;/li&gt; 
 &lt;li&gt;‚úÖ No need to rebuild Docker images&lt;/li&gt; 
 &lt;li&gt;‚úÖ Support IDE breakpoint debugging&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Detailed Documentation:&lt;/strong&gt; &lt;a href="https://raw.githubusercontent.com/Tencent/WeKnora/main/docs/%E5%BC%80%E5%8F%91%E6%8C%87%E5%8D%97.md"&gt;Development Environment Quick Start&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;üìÅ Directory Structure&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;WeKnora/
‚îú‚îÄ‚îÄ client/      # go client
‚îú‚îÄ‚îÄ cmd/         # Main entry point
‚îú‚îÄ‚îÄ config/      # Configuration files
‚îú‚îÄ‚îÄ docker/      # docker images files
‚îú‚îÄ‚îÄ docreader/   # Document parsing app
‚îú‚îÄ‚îÄ docs/        # Project documentation
‚îú‚îÄ‚îÄ frontend/    # Frontend app
‚îú‚îÄ‚îÄ internal/    # Core business logic
‚îú‚îÄ‚îÄ mcp-server/  # MCP server
‚îú‚îÄ‚îÄ migrations/  # DB migration scripts
‚îî‚îÄ‚îÄ scripts/     # Shell scripts
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ü§ù Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome community contributions! For suggestions, bugs, or feature requests, please submit an &lt;a href="https://github.com/Tencent/WeKnora/issues"&gt;Issue&lt;/a&gt; or directly create a Pull Request.&lt;/p&gt; 
&lt;h3&gt;üéØ How to Contribute&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;üêõ &lt;strong&gt;Bug Fixes&lt;/strong&gt;: Discover and fix system defects&lt;/li&gt; 
 &lt;li&gt;‚ú® &lt;strong&gt;New Features&lt;/strong&gt;: Propose and implement new capabilities&lt;/li&gt; 
 &lt;li&gt;üìö &lt;strong&gt;Documentation&lt;/strong&gt;: Improve project documentation&lt;/li&gt; 
 &lt;li&gt;üß™ &lt;strong&gt;Test Cases&lt;/strong&gt;: Write unit and integration tests&lt;/li&gt; 
 &lt;li&gt;üé® &lt;strong&gt;UI/UX Enhancements&lt;/strong&gt;: Improve user interface and experience&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üìã Contribution Process&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Fork the project&lt;/strong&gt; to your GitHub account&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Create a feature branch&lt;/strong&gt; &lt;code&gt;git checkout -b feature/amazing-feature&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Commit changes&lt;/strong&gt; &lt;code&gt;git commit -m 'Add amazing feature'&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Push branch&lt;/strong&gt; &lt;code&gt;git push origin feature/amazing-feature&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Create a Pull Request&lt;/strong&gt; with detailed description of changes&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;üé® Code Standards&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Follow &lt;a href="https://github.com/golang/go/wiki/CodeReviewComments"&gt;Go Code Review Comments&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Format code using &lt;code&gt;gofmt&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Add necessary unit tests&lt;/li&gt; 
 &lt;li&gt;Update relevant documentation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üìù Commit Guidelines&lt;/h3&gt; 
&lt;p&gt;Use &lt;a href="https://www.conventionalcommits.org/"&gt;Conventional Commits&lt;/a&gt; standard:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;feat: Add document batch upload functionality
fix: Resolve vector retrieval precision issue
docs: Update API documentation
test: Add retrieval engine test cases
refactor: Restructure document parsing module
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;üë• Contributors&lt;/h2&gt; 
&lt;p&gt;Thanks to these excellent contributors:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/Tencent/WeKnora/graphs/contributors"&gt;&lt;img src="https://contrib.rocks/image?repo=Tencent/WeKnora" alt="Contributors" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;üìÑ License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the &lt;a href="https://raw.githubusercontent.com/Tencent/WeKnora/main/LICENSE"&gt;MIT License&lt;/a&gt;. You are free to use, modify, and distribute the code with proper attribution.&lt;/p&gt; 
&lt;h2&gt;üìà Project Statistics&lt;/h2&gt; 
&lt;a href="https://www.star-history.com/#Tencent/WeKnora&amp;amp;type=date&amp;amp;legend=top-left"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=Tencent/WeKnora&amp;amp;type=date&amp;amp;theme=dark&amp;amp;legend=top-left" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=Tencent/WeKnora&amp;amp;type=date&amp;amp;legend=top-left" /&gt; 
  &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=Tencent/WeKnora&amp;amp;type=date&amp;amp;legend=top-left" /&gt; 
 &lt;/picture&gt; &lt;/a&gt;</description>
    </item>
    
    <item>
      <title>ollama/ollama</title>
      <link>https://github.com/ollama/ollama</link>
      <description>&lt;p&gt;Get up and running with OpenAI gpt-oss, DeepSeek-R1, Gemma 3 and other models.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt;
  &amp;nbsp; 
 &lt;a href="https://ollama.com"&gt; &lt;img alt="ollama" width="240" src="https://github.com/ollama/ollama/assets/3325447/0d0b44e2-8f4a-4e99-9b52-a5c1c741c8f7" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h1&gt;Ollama&lt;/h1&gt; 
&lt;p&gt;Get up and running with large language models.&lt;/p&gt; 
&lt;h3&gt;macOS&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://ollama.com/download/Ollama.dmg"&gt;Download&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Windows&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://ollama.com/download/OllamaSetup.exe"&gt;Download&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Linux&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;curl -fsSL https://ollama.com/install.sh | sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href="https://docs.ollama.com/linux#manual-install"&gt;Manual install instructions&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Docker&lt;/h3&gt; 
&lt;p&gt;The official &lt;a href="https://hub.docker.com/r/ollama/ollama"&gt;Ollama Docker image&lt;/a&gt; &lt;code&gt;ollama/ollama&lt;/code&gt; is available on Docker Hub.&lt;/p&gt; 
&lt;h3&gt;Libraries&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ollama/ollama-python"&gt;ollama-python&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ollama/ollama-js"&gt;ollama-js&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Community&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://discord.gg/ollama"&gt;Discord&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://reddit.com/r/ollama"&gt;Reddit&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Quickstart&lt;/h2&gt; 
&lt;p&gt;To run and chat with &lt;a href="https://ollama.com/library/gemma3"&gt;Gemma 3&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;ollama run gemma3
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Model library&lt;/h2&gt; 
&lt;p&gt;Ollama supports a list of models available on &lt;a href="https://ollama.com/library" title="ollama model library"&gt;ollama.com/library&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Here are some example models that can be downloaded:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Parameters&lt;/th&gt; 
   &lt;th&gt;Size&lt;/th&gt; 
   &lt;th&gt;Download&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Gemma 3&lt;/td&gt; 
   &lt;td&gt;1B&lt;/td&gt; 
   &lt;td&gt;815MB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ollama run gemma3:1b&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Gemma 3&lt;/td&gt; 
   &lt;td&gt;4B&lt;/td&gt; 
   &lt;td&gt;3.3GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ollama run gemma3&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Gemma 3&lt;/td&gt; 
   &lt;td&gt;12B&lt;/td&gt; 
   &lt;td&gt;8.1GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ollama run gemma3:12b&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Gemma 3&lt;/td&gt; 
   &lt;td&gt;27B&lt;/td&gt; 
   &lt;td&gt;17GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ollama run gemma3:27b&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;QwQ&lt;/td&gt; 
   &lt;td&gt;32B&lt;/td&gt; 
   &lt;td&gt;20GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ollama run qwq&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;DeepSeek-R1&lt;/td&gt; 
   &lt;td&gt;7B&lt;/td&gt; 
   &lt;td&gt;4.7GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ollama run deepseek-r1&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;DeepSeek-R1&lt;/td&gt; 
   &lt;td&gt;671B&lt;/td&gt; 
   &lt;td&gt;404GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ollama run deepseek-r1:671b&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Llama 4&lt;/td&gt; 
   &lt;td&gt;109B&lt;/td&gt; 
   &lt;td&gt;67GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ollama run llama4:scout&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Llama 4&lt;/td&gt; 
   &lt;td&gt;400B&lt;/td&gt; 
   &lt;td&gt;245GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ollama run llama4:maverick&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Llama 3.3&lt;/td&gt; 
   &lt;td&gt;70B&lt;/td&gt; 
   &lt;td&gt;43GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ollama run llama3.3&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Llama 3.2&lt;/td&gt; 
   &lt;td&gt;3B&lt;/td&gt; 
   &lt;td&gt;2.0GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ollama run llama3.2&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Llama 3.2&lt;/td&gt; 
   &lt;td&gt;1B&lt;/td&gt; 
   &lt;td&gt;1.3GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ollama run llama3.2:1b&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Llama 3.2 Vision&lt;/td&gt; 
   &lt;td&gt;11B&lt;/td&gt; 
   &lt;td&gt;7.9GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ollama run llama3.2-vision&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Llama 3.2 Vision&lt;/td&gt; 
   &lt;td&gt;90B&lt;/td&gt; 
   &lt;td&gt;55GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ollama run llama3.2-vision:90b&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Llama 3.1&lt;/td&gt; 
   &lt;td&gt;8B&lt;/td&gt; 
   &lt;td&gt;4.7GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ollama run llama3.1&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Llama 3.1&lt;/td&gt; 
   &lt;td&gt;405B&lt;/td&gt; 
   &lt;td&gt;231GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ollama run llama3.1:405b&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Phi 4&lt;/td&gt; 
   &lt;td&gt;14B&lt;/td&gt; 
   &lt;td&gt;9.1GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ollama run phi4&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Phi 4 Mini&lt;/td&gt; 
   &lt;td&gt;3.8B&lt;/td&gt; 
   &lt;td&gt;2.5GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ollama run phi4-mini&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Mistral&lt;/td&gt; 
   &lt;td&gt;7B&lt;/td&gt; 
   &lt;td&gt;4.1GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ollama run mistral&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Moondream 2&lt;/td&gt; 
   &lt;td&gt;1.4B&lt;/td&gt; 
   &lt;td&gt;829MB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ollama run moondream&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Neural Chat&lt;/td&gt; 
   &lt;td&gt;7B&lt;/td&gt; 
   &lt;td&gt;4.1GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ollama run neural-chat&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Starling&lt;/td&gt; 
   &lt;td&gt;7B&lt;/td&gt; 
   &lt;td&gt;4.1GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ollama run starling-lm&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Code Llama&lt;/td&gt; 
   &lt;td&gt;7B&lt;/td&gt; 
   &lt;td&gt;3.8GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ollama run codellama&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Llama 2 Uncensored&lt;/td&gt; 
   &lt;td&gt;7B&lt;/td&gt; 
   &lt;td&gt;3.8GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ollama run llama2-uncensored&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;LLaVA&lt;/td&gt; 
   &lt;td&gt;7B&lt;/td&gt; 
   &lt;td&gt;4.5GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ollama run llava&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Granite-3.3&lt;/td&gt; 
   &lt;td&gt;8B&lt;/td&gt; 
   &lt;td&gt;4.9GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ollama run granite3.3&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] You should have at least 8 GB of RAM available to run the 7B models, 16 GB to run the 13B models, and 32 GB to run the 33B models.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Customize a model&lt;/h2&gt; 
&lt;h3&gt;Import from GGUF&lt;/h3&gt; 
&lt;p&gt;Ollama supports importing GGUF models in the Modelfile:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Create a file named &lt;code&gt;Modelfile&lt;/code&gt;, with a &lt;code&gt;FROM&lt;/code&gt; instruction with the local filepath to the model you want to import.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;FROM ./vicuna-33b.Q4_0.gguf
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Create the model in Ollama&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;ollama create example -f Modelfile
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run the model&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;ollama run example
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Import from Safetensors&lt;/h3&gt; 
&lt;p&gt;See the &lt;a href="https://docs.ollama.com/import"&gt;guide&lt;/a&gt; on importing models for more information.&lt;/p&gt; 
&lt;h3&gt;Customize a prompt&lt;/h3&gt; 
&lt;p&gt;Models from the Ollama library can be customized with a prompt. For example, to customize the &lt;code&gt;llama3.2&lt;/code&gt; model:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;ollama pull llama3.2
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Create a &lt;code&gt;Modelfile&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;FROM llama3.2

# set the temperature to 1 [higher is more creative, lower is more coherent]
PARAMETER temperature 1

# set the system message
SYSTEM """
You are Mario from Super Mario Bros. Answer as Mario, the assistant, only.
"""
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Next, create and run the model:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;ollama create mario -f ./Modelfile
ollama run mario
&amp;gt;&amp;gt;&amp;gt; hi
Hello! It's your friend Mario.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more information on working with a Modelfile, see the &lt;a href="https://docs.ollama.com/modelfile"&gt;Modelfile&lt;/a&gt; documentation.&lt;/p&gt; 
&lt;h2&gt;CLI Reference&lt;/h2&gt; 
&lt;h3&gt;Create a model&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;ollama create&lt;/code&gt; is used to create a model from a Modelfile.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;ollama create mymodel -f ./Modelfile
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Pull a model&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;ollama pull llama3.2
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;This command can also be used to update a local model. Only the diff will be pulled.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Remove a model&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;ollama rm llama3.2
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Copy a model&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;ollama cp llama3.2 my-model
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Multiline input&lt;/h3&gt; 
&lt;p&gt;For multiline input, you can wrap text with &lt;code&gt;"""&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; """Hello,
... world!
... """
I'm a basic program that prints the famous "Hello, world!" message to the console.
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Multimodal models&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;ollama run llava "What's in this image? /Users/jmorgan/Desktop/smile.png"
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Output&lt;/strong&gt;: The image features a yellow smiley face, which is likely the central focus of the picture.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Pass the prompt as an argument&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;ollama run llama3.2 "Summarize this file: $(cat README.md)"
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Output&lt;/strong&gt;: Ollama is a lightweight, extensible framework for building and running language models on the local machine. It provides a simple API for creating, running, and managing models, as well as a library of pre-built models that can be easily used in a variety of applications.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Show model information&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;ollama show llama3.2
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;List models on your computer&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;ollama list
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;List which models are currently loaded&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;ollama ps
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Stop a model which is currently running&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;ollama stop llama3.2
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Generate embeddings from the CLI&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;ollama run embeddinggemma "Your text to embed"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also pipe text for scripted workflows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;echo "Your text to embed" | ollama run embeddinggemma
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Start Ollama&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;ollama serve&lt;/code&gt; is used when you want to start ollama without running the desktop application.&lt;/p&gt; 
&lt;h2&gt;Building&lt;/h2&gt; 
&lt;p&gt;See the &lt;a href="https://github.com/ollama/ollama/raw/main/docs/development.md"&gt;developer guide&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Running local builds&lt;/h3&gt; 
&lt;p&gt;Next, start the server:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;./ollama serve
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Finally, in a separate shell, run a model:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;./ollama run llama3.2
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Building with MLX (experimental)&lt;/h2&gt; 
&lt;p&gt;First build the MLX libraries:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;cmake --preset MLX
cmake --build --preset MLX --parallel
cmake --install build --component MLX
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;When building with the &lt;code&gt;-tags mlx&lt;/code&gt; flag, the main &lt;code&gt;ollama&lt;/code&gt; binary includes MLX support for experimental features like image generation:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;go build -tags mlx .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Finally, start the server:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;./ollama serve
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Building MLX with CUDA&lt;/h3&gt; 
&lt;p&gt;When building with CUDA, use the preset "MLX CUDA 13" or "MLX CUDA 12" to enable CUDA with default architectures:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;cmake --preset 'MLX CUDA 13'
cmake --build --preset 'MLX CUDA 13' --parallel
cmake --install build --component MLX
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;REST API&lt;/h2&gt; 
&lt;p&gt;Ollama has a REST API for running and managing models.&lt;/p&gt; 
&lt;h3&gt;Generate a response&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2",
  "prompt":"Why is the sky blue?"
}'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Chat with a model&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;curl http://localhost:11434/api/chat -d '{
  "model": "llama3.2",
  "messages": [
    { "role": "user", "content": "why is the sky blue?" }
  ]
}'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/ollama/ollama/main/docs/api.md"&gt;API documentation&lt;/a&gt; for all endpoints.&lt;/p&gt; 
&lt;h2&gt;Community Integrations&lt;/h2&gt; 
&lt;h3&gt;Web &amp;amp; Desktop&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/onyx-dot-app/onyx"&gt;Onyx&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/open-webui/open-webui"&gt;Open WebUI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aws-samples/swift-chat"&gt;SwiftChat (macOS with ReactNative)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/AugustDev/enchanted"&gt;Enchanted (macOS native)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/fmaclen/hollama"&gt;Hollama&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ParisNeo/lollms-webui"&gt;Lollms WebUI (Single user)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ParisNeo/lollms"&gt;Lollms (Multi users)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/danny-avila/LibreChat"&gt;LibreChat&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/bionic-gpt/bionic-gpt"&gt;Bionic GPT&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/rtcfirefly/ollama-ui"&gt;HTML UI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/bajahaw/ai-ui"&gt;AI-UI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/jikkuatwork/saddle"&gt;Saddle&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.tagspaces.org"&gt;TagSpaces&lt;/a&gt; (A platform for file-based apps, &lt;a href="https://docs.tagspaces.org/ai/"&gt;utilizing Ollama&lt;/a&gt; for the generation of tags and descriptions)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ivanfioravanti/chatbot-ollama"&gt;Chatbot UI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/mckaywrigley/chatbot-ui"&gt;Chatbot UI v2&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ollama-interface/Ollama-Gui?tab=readme-ov-file"&gt;Typescript UI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/richawo/minimal-llm-ui"&gt;Minimalistic React UI for Ollama Models&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/kevinhermawan/Ollamac"&gt;Ollamac&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/enricoros/big-AGI"&gt;big-AGI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/cheshire-cat-ai/core"&gt;Cheshire Cat assistant framework&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/semperai/amica"&gt;Amica&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/BruceMacD/chatd"&gt;chatd&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/kghandour/Ollama-SwiftUI"&gt;Ollama-SwiftUI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/langgenius/dify"&gt;Dify.AI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://mindmac.app"&gt;MindMac&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/jakobhoeg/nextjs-ollama-llm-ui"&gt;NextJS Web Interface for Ollama&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://msty.app"&gt;Msty&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Bin-Huang/Chatbox"&gt;Chatbox&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/tgraupmann/WinForm_Ollama_Copilot"&gt;WinForm Ollama Copilot&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ChatGPTNextWeb/ChatGPT-Next-Web"&gt;NextChat&lt;/a&gt; with &lt;a href="https://docs.nextchat.dev/models/ollama"&gt;Get Started Doc&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/mmo80/alpaca-webui"&gt;Alpaca WebUI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/enoch1118/ollamaGUI"&gt;OllamaGUI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/InternLM/OpenAOE"&gt;OpenAOE&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/leonid20000/OdinRunes"&gt;Odin Runes&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/mrdjohnson/llm-x"&gt;LLM-X&lt;/a&gt; (Progressive Web App)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Mintplex-Labs/anything-llm"&gt;AnythingLLM (Docker + MacOs/Windows/Linux native app)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/rapidarchitect/ollama_basic_chat"&gt;Ollama Basic Chat: Uses HyperDiv Reactive UI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/drazdra/ollama-chats"&gt;Ollama-chats RPG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://intellibar.app/"&gt;IntelliBar&lt;/a&gt; (AI-powered assistant for macOS)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/AliAhmedNada/jirapt"&gt;Jirapt&lt;/a&gt; (Jira Integration to generate issues, tasks, epics)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/AliAhmedNada/ojira"&gt;ojira&lt;/a&gt; (Jira chrome plugin to easily generate descriptions for tasks)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/reid41/QA-Pilot"&gt;QA-Pilot&lt;/a&gt; (Interactive chat tool that can leverage Ollama models for rapid understanding and navigation of GitHub code repositories)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/sugarforever/chat-ollama"&gt;ChatOllama&lt;/a&gt; (Open Source Chatbot based on Ollama with Knowledge Bases)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Nagi-ovo/CRAG-Ollama-Chat"&gt;CRAG Ollama Chat&lt;/a&gt; (Simple Web Search with Corrective RAG)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/infiniflow/ragflow"&gt;RAGFlow&lt;/a&gt; (Open-source Retrieval-Augmented Generation engine based on deep document understanding)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/StreamDeploy-DevRel/streamdeploy-llm-app-scaffold"&gt;StreamDeploy&lt;/a&gt; (LLM Application Scaffold)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/swuecho/chat"&gt;chat&lt;/a&gt; (chat web app for teams)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/lobehub/lobe-chat"&gt;Lobe Chat&lt;/a&gt; with &lt;a href="https://lobehub.com/docs/self-hosting/examples/ollama"&gt;Integrating Doc&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/datvodinh/rag-chatbot.git"&gt;Ollama RAG Chatbot&lt;/a&gt; (Local Chat with multiple PDFs using Ollama and RAG)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.nurgo-software.com/products/brainsoup"&gt;BrainSoup&lt;/a&gt; (Flexible native client with RAG &amp;amp; multi-agent automation)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Renset/macai"&gt;macai&lt;/a&gt; (macOS client for Ollama, ChatGPT, and other compatible API back-ends)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/josStorer/RWKV-Runner"&gt;RWKV-Runner&lt;/a&gt; (RWKV offline LLM deployment tool, also usable as a client for ChatGPT and Ollama)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/dezoito/ollama-grid-search"&gt;Ollama Grid Search&lt;/a&gt; (app to evaluate and compare models)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Otacon/olpaka"&gt;Olpaka&lt;/a&gt; (User-friendly Flutter Web App for Ollama)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://casibase.org"&gt;Casibase&lt;/a&gt; (An open source AI knowledge base and dialogue system combining the latest RAG, SSO, ollama support, and multiple large language models.)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/CrazyNeil/OllamaSpring"&gt;OllamaSpring&lt;/a&gt; (Ollama Client for macOS)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/kartikm7/llocal"&gt;LLocal.in&lt;/a&gt; (Easy to use Electron Desktop Client for Ollama)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/dcSpark/shinkai-apps"&gt;Shinkai Desktop&lt;/a&gt; (Two click install Local AI using Ollama + Files + RAG)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/zeyoyt/ailama"&gt;AiLama&lt;/a&gt; (A Discord User App that allows you to interact with Ollama anywhere in Discord)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/rapidarchitect/ollama_mesop/"&gt;Ollama with Google Mesop&lt;/a&gt; (Mesop Chat Client implementation with Ollama)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/SciPhi-AI/R2R"&gt;R2R&lt;/a&gt; (Open-source RAG engine)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/elearningshow/ollama-kis"&gt;Ollama-Kis&lt;/a&gt; (A simple easy-to-use GUI with sample custom LLM for Drivers Education)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://opengpa.org"&gt;OpenGPA&lt;/a&gt; (Open-source offline-first Enterprise Agentic Application)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/mateuszmigas/painting-droid"&gt;Painting Droid&lt;/a&gt; (Painting app with AI integrations)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.kerlig.com/"&gt;Kerlig AI&lt;/a&gt; (AI writing assistant for macOS)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/MindWorkAI/AI-Studio"&gt;AI Studio&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/gyopak/sidellama"&gt;Sidellama&lt;/a&gt; (browser-based LLM client)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/trypromptly/LLMStack"&gt;LLMStack&lt;/a&gt; (No-code multi-agent framework to build LLM agents and workflows)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://boltai.com"&gt;BoltAI for Mac&lt;/a&gt; (AI Chat Client for Mac)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/av/harbor"&gt;Harbor&lt;/a&gt; (Containerized LLM Toolkit with Ollama as default backend)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/szczyglis-dev/py-gpt"&gt;PyGPT&lt;/a&gt; (AI desktop assistant for Linux, Windows, and Mac)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Jeffser/Alpaca"&gt;Alpaca&lt;/a&gt; (An Ollama client application for Linux and macOS made with GTK4 and Adwaita)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Significant-Gravitas/AutoGPT/raw/master/docs/content/platform/ollama.md"&gt;AutoGPT&lt;/a&gt; (AutoGPT Ollama integration)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.jonathanhecl.com/go-crew/"&gt;Go-CREW&lt;/a&gt; (Powerful Offline RAG in Golang)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/openvmp/partcad/"&gt;PartCAD&lt;/a&gt; (CAD model generation with OpenSCAD and CadQuery)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ollama4j/ollama4j-web-ui"&gt;Ollama4j Web UI&lt;/a&gt; - Java-based Web UI for Ollama built with Vaadin, Spring Boot, and Ollama4j&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/kspviswa/pyOllaMx"&gt;PyOllaMx&lt;/a&gt; - macOS application capable of chatting with both Ollama and Apple MLX models.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/cline/cline"&gt;Cline&lt;/a&gt; - Formerly known as Claude Dev is a VS Code extension for multi-file/whole-repo coding&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/voideditor/void"&gt;Void&lt;/a&gt; (Open source AI code editor and Cursor alternative)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/kangfenmao/cherry-studio"&gt;Cherry Studio&lt;/a&gt; (Desktop client with Ollama support)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/1runeberg/confichat"&gt;ConfiChat&lt;/a&gt; (Lightweight, standalone, multi-platform, and privacy-focused LLM chat interface with optional encryption)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/nickthecook/archyve"&gt;Archyve&lt;/a&gt; (RAG-enabling document library)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/rapidarchitect/ollama-crew-mesop"&gt;crewAI with Mesop&lt;/a&gt; (Mesop Web Interface to run crewAI with Ollama)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/chyok/ollama-gui"&gt;Tkinter-based client&lt;/a&gt; (Python tkinter-based Client for Ollama)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/trendy-design/llmchat"&gt;LLMChat&lt;/a&gt; (Privacy focused, 100% local, intuitive all-in-one chat interface)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Leon-Sander/Local-Multimodal-AI-Chat"&gt;Local Multimodal AI Chat&lt;/a&gt; (Ollama-based LLM Chat with support for multiple features, including PDF RAG, voice chat, image-based interactions, and integration with OpenAI.)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/xark-argo/argo"&gt;ARGO&lt;/a&gt; (Locally download and run Ollama and Huggingface models with RAG and deep research on Mac/Windows/Linux)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/EliasPereirah/OrionChat"&gt;OrionChat&lt;/a&gt; - OrionChat is a web interface for chatting with different AI providers&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/bklieger-groq/g1"&gt;G1&lt;/a&gt; (Prototype of using prompting strategies to improve the LLM's reasoning through o1-like reasoning chains.)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/lemonit-eric-mao/ollama-web-management"&gt;Web management&lt;/a&gt; (Web management page)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/promptery/promptery"&gt;Promptery&lt;/a&gt; (desktop client for Ollama.)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/JHubi1/ollama-app"&gt;Ollama App&lt;/a&gt; (Modern and easy-to-use multi-platform client for Ollama)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/annilq/chat-ollama"&gt;chat-ollama&lt;/a&gt; (a React Native client for Ollama)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/tcsenpai/spacellama"&gt;SpaceLlama&lt;/a&gt; (Firefox and Chrome extension to quickly summarize web pages with ollama in a sidebar)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/tcsenpai/youlama"&gt;YouLama&lt;/a&gt; (Webapp to quickly summarize any YouTube video, supporting Invidious as well)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/tcsenpai/dualmind"&gt;DualMind&lt;/a&gt; (Experimental app allowing two models to talk to each other in the terminal or in a web interface)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/h1ddenpr0cess20/ollamarama-matrix"&gt;ollamarama-matrix&lt;/a&gt; (Ollama chatbot for the Matrix chat protocol)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/anan1213095357/ollama-chat-app"&gt;ollama-chat-app&lt;/a&gt; (Flutter-based chat app)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.perfectmemory.ai/"&gt;Perfect Memory AI&lt;/a&gt; (Productivity AI assists personalized by what you have seen on your screen, heard, and said in the meetings)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/hexastack/hexabot"&gt;Hexabot&lt;/a&gt; (A conversational AI builder)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/rapidarchitect/reddit_analyzer"&gt;Reddit Rate&lt;/a&gt; (Search and Rate Reddit topics with a weighted summation)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/adarshM84/OpenTalkGpt"&gt;OpenTalkGpt&lt;/a&gt; (Chrome Extension to manage open-source models supported by Ollama, create custom models, and chat with models from a user-friendly UI)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/vinhnx/vt.ai"&gt;VT&lt;/a&gt; (A minimal multimodal AI chat app, with dynamic conversation routing. Supports local models via Ollama)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/nosia-ai/nosia"&gt;Nosia&lt;/a&gt; (Easy to install and use RAG platform based on Ollama)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/nbonamy/witsy"&gt;Witsy&lt;/a&gt; (An AI Desktop application available for Mac/Windows/Linux)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/US-Artificial-Intelligence/abbey"&gt;Abbey&lt;/a&gt; (A configurable AI interface server with notebooks, document storage, and YouTube support)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/dmayboroda/minima"&gt;Minima&lt;/a&gt; (RAG with on-premises or fully local workflow)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/AidfulAI/aidful-ollama-model-delete"&gt;aidful-ollama-model-delete&lt;/a&gt; (User interface for simplified model cleanup)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ItzCrazyKns/Perplexica"&gt;Perplexica&lt;/a&gt; (An AI-powered search engine &amp;amp; an open-source alternative to Perplexity AI)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/oslook/ollama-webui"&gt;Ollama Chat WebUI for Docker &lt;/a&gt; (Support for local docker deployment, lightweight ollama webui)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aka.ms/ai-tooklit/ollama-docs"&gt;AI Toolkit for Visual Studio Code&lt;/a&gt; (Microsoft-official VS Code extension to chat, test, evaluate models with Ollama support, and use them in your AI applications.)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/anilkay/MinimalNextOllamaChat"&gt;MinimalNextOllamaChat&lt;/a&gt; (Minimal Web UI for Chat and Model Control)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/TilmanGriesel/chipper"&gt;Chipper&lt;/a&gt; AI interface for tinkerers (Ollama, Haystack RAG, Python)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/CosmicEventHorizon/ChibiChat"&gt;ChibiChat&lt;/a&gt; (Kotlin-based Android app to chat with Ollama and Koboldcpp API endpoints)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/qusaismael/localllm"&gt;LocalLLM&lt;/a&gt; (Minimal Web-App to run ollama models on it with a GUI)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/buiducnhat/ollamazing"&gt;Ollamazing&lt;/a&gt; (Web extension to run Ollama models)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/benhaotang/OpenDeepResearcher-via-searxng"&gt;OpenDeepResearcher-via-searxng&lt;/a&gt; (A Deep Research equivalent endpoint with Ollama support for running locally)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/AIDotNet/AntSK"&gt;AntSK&lt;/a&gt; (Out-of-the-box &amp;amp; Adaptable RAG Chatbot)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/1Panel-dev/MaxKB/"&gt;MaxKB&lt;/a&gt; (Ready-to-use &amp;amp; flexible RAG Chatbot)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/danielekp/yla"&gt;yla&lt;/a&gt; (Web interface to freely interact with your customized models)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/RockChinQ/LangBot"&gt;LangBot&lt;/a&gt; (LLM-based instant messaging bots platform, with Agents, RAG features, supports multiple platforms)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/1Panel-dev/1Panel/"&gt;1Panel&lt;/a&gt; (Web-based Linux Server Management Tool)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Soulter/AstrBot/"&gt;AstrBot&lt;/a&gt; (User-friendly LLM-based multi-platform chatbot with a WebUI, supporting RAG, LLM agents, and plugins integration)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ibrahimcetin/reins"&gt;Reins&lt;/a&gt; (Easily tweak parameters, customize system prompts per chat, and enhance your AI experiments with reasoning model support.)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Aharon-Bensadoun/Flufy"&gt;Flufy&lt;/a&gt; (A beautiful chat interface for interacting with Ollama's API. Built with React, TypeScript, and Material-UI.)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/zeozeozeo/ellama"&gt;Ellama&lt;/a&gt; (Friendly native app to chat with an Ollama instance)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/mediar-ai/screenpipe"&gt;screenpipe&lt;/a&gt; Build agents powered by your screen history&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/hengkysteen/ollamb"&gt;Ollamb&lt;/a&gt; (Simple yet rich in features, cross-platform built with Flutter and designed for Ollama. Try the &lt;a href="https://hengkysteen.github.io/demo/ollamb/"&gt;web demo&lt;/a&gt;.)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Writeopia/Writeopia"&gt;Writeopia&lt;/a&gt; (Text editor with integration with Ollama)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/AppFlowy-IO/AppFlowy"&gt;AppFlowy&lt;/a&gt; (AI collaborative workspace with Ollama, cross-platform and self-hostable)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/cushydigit/lumina.git"&gt;Lumina&lt;/a&gt; (A lightweight, minimal React.js frontend for interacting with Ollama servers)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://pypi.org/project/tiny-notepad"&gt;Tiny Notepad&lt;/a&gt; (A lightweight, notepad-like interface to chat with ollama available on PyPI)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/hellotunamayo/macLlama"&gt;macLlama (macOS native)&lt;/a&gt; (A native macOS GUI application for interacting with Ollama models, featuring a chat interface.)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/philberndt/GPTranslate"&gt;GPTranslate&lt;/a&gt; (A fast and lightweight, AI powered desktop translation application written with Rust and Tauri. Features real-time translation with OpenAI/Azure/Ollama.)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/NGC13009/ollama-launcher"&gt;ollama launcher&lt;/a&gt; (A launcher for Ollama, aiming to provide users with convenient functions such as ollama server launching, management, or configuration.)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Aj-Seven/ai-hub"&gt;ai-hub&lt;/a&gt; (AI Hub supports multiple models via API keys and Chat support via Ollama API.)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://gitlab.com/mayan-edms/mayan-edms"&gt;Mayan EDMS&lt;/a&gt; (Open source document management system to organize, tag, search, and automate your files with powerful Ollama driven workflows.)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/doolijb/serene-pub"&gt;Serene Pub&lt;/a&gt; (Beginner friendly, open source AI Roleplaying App for Windows, Mac OS and Linux. Search, download and use models with Ollama all inside the app.)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aqerd/andes"&gt;Andes&lt;/a&gt; (A Visual Studio Code extension that provides a local UI interface for Ollama models)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/kdeps/kdeps"&gt;KDeps&lt;/a&gt; (Kdeps is an offline-first AI framework for building Dockerized full-stack AI applications declaratively using Apple PKL and integrates APIs with Ollama on the backend.)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/KashyapTan/clueless"&gt;Clueless&lt;/a&gt; (Open Source &amp;amp; Local Cluely: A desktop application LLM assistant to help you talk to anything on your screen using locally served Ollama models. Also undetectable to screenshare)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/carbonatedWaterOrg/ollama-co2"&gt;ollama-co2&lt;/a&gt; (FastAPI web interface for monitoring and managing local and remote Ollama servers with real-time model monitoring and concurrent downloads)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://hillnote.com"&gt;Hillnote&lt;/a&gt; (A Markdown-first workspace designed to supercharge your AI workflow. Create documents ready to integrate with Claude, ChatGPT, Gemini, Cursor, and more - all while keeping your work on your device.)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Cloud&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://cloud.google.com/run/docs/tutorials/gpu-gemma2-with-ollama"&gt;Google Cloud&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://fly.io/docs/python/do-more/add-ollama/"&gt;Fly.io&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.koyeb.com/deploy/ollama"&gt;Koyeb&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Tutorial&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/datawhalechina/handy-ollama"&gt;handy-ollama&lt;/a&gt; (Chinese Tutorial for Ollama by &lt;a href="https://github.com/datawhalechina"&gt;Datawhale &lt;/a&gt; - China's Largest Open Source AI Learning Community)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Terminal&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ggozad/oterm"&gt;oterm&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/s-kostyaev/ellama"&gt;Ellama Emacs client&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/zweifisch/ollama"&gt;Emacs client&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/paradoxical-dev/neollama"&gt;neollama&lt;/a&gt; UI client for interacting with models from within Neovim&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/David-Kunz/gen.nvim"&gt;gen.nvim&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/nomnivore/ollama.nvim"&gt;ollama.nvim&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/marco-souza/ollero.nvim"&gt;ollero.nvim&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/gerazov/ollama-chat.nvim"&gt;ollama-chat.nvim&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/huynle/ogpt.nvim"&gt;ogpt.nvim&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/karthink/gptel"&gt;gptel Emacs client&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/dustinblackman/oatmeal"&gt;Oatmeal&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pgibler/cmdh"&gt;cmdh&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/npahlfer/ooo"&gt;ooo&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/reid41/shell-pilot"&gt;shell-pilot&lt;/a&gt;(Interact with models via pure shell scripts on Linux or macOS)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pythops/tenere"&gt;tenere&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/taketwo/llm-ollama"&gt;llm-ollama&lt;/a&gt; for &lt;a href="https://llm.datasette.io/en/stable/"&gt;Datasette's LLM CLI&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/anaisbetts/typechat-cli"&gt;typechat-cli&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/djcopley/ShellOracle"&gt;ShellOracle&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/yusufcanb/tlm"&gt;tlm&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ericcurtin/podman-ollama"&gt;podman-ollama&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/sammcj/gollama"&gt;gollama&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/paulrobello/parllama"&gt;ParLlama&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/cognitivetech/ollama-ebook-summary/"&gt;Ollama eBook Summary&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/rapidarchitect/ollama_moe"&gt;Ollama Mixture of Experts (MOE) in 50 lines of code&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pepo-ec/vim-intelligence-bridge"&gt;vim-intelligence-bridge&lt;/a&gt; Simple interaction of "Ollama" with the Vim editor&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://x-cmd.com/mod/ollama"&gt;x-cmd ollama&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/drunkwcodes/bb7"&gt;bb7&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/marcusziade/Swollama"&gt;SwollamaCLI&lt;/a&gt; bundled with the Swollama Swift package. &lt;a href="https://github.com/marcusziade/Swollama?tab=readme-ov-file#cli-usage"&gt;Demo&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/sigoden/aichat"&gt;aichat&lt;/a&gt; All-in-one LLM CLI tool featuring Shell Assistant, Chat-REPL, RAG, AI tools &amp;amp; agents, with access to OpenAI, Claude, Gemini, Ollama, Groq, and more.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/rrg92/powershai"&gt;PowershAI&lt;/a&gt; PowerShell module that brings AI to terminal on Windows, including support for Ollama&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Abyss-c0re/deepshell"&gt;DeepShell&lt;/a&gt; Your self-hosted AI assistant. Interactive Shell, Files and Folders analysis.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/xyproto/orbiton"&gt;orbiton&lt;/a&gt; Configuration-free text editor and IDE with support for tab completion with Ollama.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/molbal/orca-cli"&gt;orca-cli&lt;/a&gt; Ollama Registry CLI Application - Browse, pull, and download models from Ollama Registry in your terminal.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/jonathanhecl/gguf-to-ollama"&gt;GGUF-to-Ollama&lt;/a&gt; - Importing GGUF to Ollama made easy (multiplatform)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/rapidarchitect/ollama_strands"&gt;AWS-Strands-With-Ollama&lt;/a&gt; - AWS Strands Agents with Ollama Examples&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/attogram/ollama-multirun"&gt;ollama-multirun&lt;/a&gt; - A bash shell script to run a single prompt against any or all of your locally installed ollama models, saving the output and performance statistics as easily navigable web pages. (&lt;a href="https://attogram.github.io/ai_test_zone/"&gt;Demo&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/attogram/ollama-bash-toolshed"&gt;ollama-bash-toolshed&lt;/a&gt; - Bash scripts to chat with tool using models. Add new tools to your shed with ease. Runs on Ollama.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/mags0ft/hle-eval-ollama"&gt;hle-eval-ollama&lt;/a&gt; - Runs benchmarks like "Humanity's Last Exam" (HLE) on your favorite local Ollama models and evaluates the quality of their responses&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/vinhnx/vtcode"&gt;VT Code&lt;/a&gt; - VT Code is a Rust-based terminal coding agent with semantic code intelligence via Tree-sitter. Ollama integration for running local/cloud models with configurable endpoints.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Apple Vision Pro&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aws-samples/swift-chat"&gt;SwiftChat&lt;/a&gt; (Cross-platform AI chat app supporting Apple Vision Pro via "Designed for iPad")&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/AugustDev/enchanted"&gt;Enchanted&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Database&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/timescale/pgai"&gt;pgai&lt;/a&gt; - PostgreSQL as a vector database (Create and search embeddings from Ollama models using pgvector) 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://github.com/timescale/pgai/raw/main/docs/vectorizer-quick-start.md"&gt;Get started guide&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/mindsdb/mindsdb/raw/staging/mindsdb/integrations/handlers/ollama_handler/README.md"&gt;MindsDB&lt;/a&gt; (Connects Ollama models with nearly 200 data platforms and apps)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/philippgille/chromem-go/raw/v0.5.0/embed_ollama.go"&gt;chromem-go&lt;/a&gt; with &lt;a href="https://github.com/philippgille/chromem-go/tree/v0.5.0/examples/rag-wikipedia-ollama"&gt;example&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/dbkangaroo/kangaroo"&gt;Kangaroo&lt;/a&gt; (AI-powered SQL client and admin tool for popular databases)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Package managers&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://archlinux.org/packages/extra/x86_64/ollama/"&gt;Pacman&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/gentoo/guru/tree/master/app-misc/ollama"&gt;Gentoo&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://formulae.brew.sh/formula/ollama"&gt;Homebrew&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://artifacthub.io/packages/helm/ollama-helm/ollama"&gt;Helm Chart&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://codeberg.org/tusharhero/ollama-guix"&gt;Guix channel&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://search.nixos.org/packages?show=ollama&amp;amp;from=0&amp;amp;size=50&amp;amp;sort=relevance&amp;amp;type=packages&amp;amp;query=ollama"&gt;Nix package&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://flox.dev/blog/ollama-part-one"&gt;Flox&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Libraries&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://python.langchain.com/docs/integrations/chat/ollama/"&gt;LangChain&lt;/a&gt; and &lt;a href="https://js.langchain.com/docs/integrations/chat/ollama/"&gt;LangChain.js&lt;/a&gt; with &lt;a href="https://js.langchain.com/docs/tutorials/local_rag/"&gt;example&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://firebase.google.com/docs/genkit/plugins/ollama"&gt;Firebase Genkit&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/crewAIInc/crewAI"&gt;crewAI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://remembersoftwares.github.io/yacana/"&gt;Yacana&lt;/a&gt; (User-friendly multi-agent framework for brainstorming and executing predetermined flows with built-in tool integration)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/strands-agents/sdk-python"&gt;Strands Agents&lt;/a&gt; (A model-driven approach to building AI agents in just a few lines of code)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/spring-projects/spring-ai"&gt;Spring AI&lt;/a&gt; with &lt;a href="https://docs.spring.io/spring-ai/reference/api/chat/ollama-chat.html"&gt;reference&lt;/a&gt; and &lt;a href="https://github.com/tzolov/ollama-tools"&gt;example&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/tmc/langchaingo/"&gt;LangChainGo&lt;/a&gt; with &lt;a href="https://github.com/tmc/langchaingo/tree/main/examples/ollama-completion-example"&gt;example&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/langchain4j/langchain4j"&gt;LangChain4j&lt;/a&gt; with &lt;a href="https://github.com/langchain4j/langchain4j-examples/tree/main/ollama-examples/src/main/java"&gt;example&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Abraxas-365/langchain-rust"&gt;LangChainRust&lt;/a&gt; with &lt;a href="https://github.com/Abraxas-365/langchain-rust/raw/main/examples/llm_ollama.rs"&gt;example&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/tryAGI/LangChain"&gt;LangChain for .NET&lt;/a&gt; with &lt;a href="https://github.com/tryAGI/LangChain/raw/main/examples/LangChain.Samples.OpenAI/Program.cs"&gt;example&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/theodo-group/LLPhant?tab=readme-ov-file#ollama"&gt;LLPhant&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.llamaindex.ai/en/stable/examples/llm/ollama/"&gt;LlamaIndex&lt;/a&gt; and &lt;a href="https://ts.llamaindex.ai/modules/llms/available_llms/ollama"&gt;LlamaIndexTS&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/BerriAI/litellm"&gt;LiteLLM&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/presbrey/ollamafarm"&gt;OllamaFarm for Go&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/awaescher/OllamaSharp"&gt;OllamaSharp for .NET&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/gbaptista/ollama-ai"&gt;Ollama for Ruby&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pepperoni21/ollama-rs"&gt;Ollama-rs for Rust&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/jmont-dev/ollama-hpp"&gt;Ollama-hpp for C++&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ollama4j/ollama4j"&gt;Ollama4j for Java&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://modelfusion.dev/integration/model-provider/ollama"&gt;ModelFusion Typescript Library&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/kevinhermawan/OllamaKit"&gt;OllamaKit for Swift&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/breitburg/dart-ollama"&gt;Ollama for Dart&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/cloudstudio/ollama-laravel"&gt;Ollama for Laravel&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/davidmigloz/langchain_dart"&gt;LangChainDart&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/microsoft/semantic-kernel/tree/main/python/semantic_kernel/connectors/ai/ollama"&gt;Semantic Kernel - Python&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/deepset-ai/haystack-integrations/raw/main/integrations/ollama.md"&gt;Haystack&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/brainlid/langchain"&gt;Elixir LangChain&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/JBGruber/rollama"&gt;Ollama for R - rollama&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/hauselin/ollama-r"&gt;Ollama for R - ollama-r&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/lebrunel/ollama-ex"&gt;Ollama-ex for Elixir&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/b-tocs/abap_btocs_ollama"&gt;Ollama Connector for SAP ABAP&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://testcontainers.com/modules/ollama/"&gt;Testcontainers&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://portkey.ai/docs/welcome/integration-guides/ollama"&gt;Portkey&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/svilupp/PromptingTools.jl"&gt;PromptingTools.jl&lt;/a&gt; with an &lt;a href="https://svilupp.github.io/PromptingTools.jl/dev/examples/working_with_ollama"&gt;example&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Project-Llama/llamascript"&gt;LlamaScript&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/emirsahin1/llm-axe"&gt;llm-axe&lt;/a&gt; (Python Toolkit for Building LLM Powered Apps)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.gollm.co/examples/ollama-example"&gt;Gollm&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/jonathanhecl/gollama"&gt;Gollama for Golang&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/xyproto/ollamaclient"&gt;Ollamaclient for Golang&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://gitlab.com/tozd/go/fun"&gt;High-level function abstraction in Go&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ArdaGnsrn/ollama-php"&gt;Ollama PHP&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/agents-flex/agents-flex"&gt;Agents-Flex for Java&lt;/a&gt; with &lt;a href="https://github.com/agents-flex/agents-flex/tree/main/agents-flex-llm/agents-flex-llm-ollama/src/test/java/com/agentsflex/llm/ollama"&gt;example&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/parakeet-nest/parakeet"&gt;Parakeet&lt;/a&gt; is a GoLang library, made to simplify the development of small generative AI applications with Ollama.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/andygill/haverscript"&gt;Haverscript&lt;/a&gt; with &lt;a href="https://github.com/andygill/haverscript/tree/main/examples"&gt;examples&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/mattt/ollama-swift"&gt;Ollama for Swift&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/guitaripod/Swollama"&gt;Swollama for Swift&lt;/a&gt; with &lt;a href="https://guitaripod.github.io/Swollama/documentation/swollama"&gt;DocC&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/prasad89/golamify"&gt;GoLamify&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/tusharad/ollama-haskell"&gt;Ollama for Haskell&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/nbonamy/multi-llm-ts"&gt;multi-llm-ts&lt;/a&gt; (A Typescript/JavaScript library allowing access to different LLM in a unified API)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/lofcz/llmtornado"&gt;LlmTornado&lt;/a&gt; (C# library providing a unified interface for major FOSS &amp;amp; Commercial inference APIs)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/dravenk/ollama-zig"&gt;Ollama for Zig&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/lunary-ai/abso"&gt;Abso&lt;/a&gt; (OpenAI-compatible TypeScript SDK for any LLM provider)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/goodreasonai/nichey"&gt;Nichey&lt;/a&gt; is a Python package for generating custom wikis for your research topic&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/kassane/ollama-d"&gt;Ollama for D&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/HardCodeDev777/OllamaPlusPlus"&gt;OllamaPlusPlus&lt;/a&gt; (Very simple C++ library for Ollama)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/mozilla-ai/any-llm"&gt;any-llm&lt;/a&gt; (A single interface to use different llm providers by &lt;a href="https://www.mozilla.ai/"&gt;mozilla.ai&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/mozilla-ai/any-agent"&gt;any-agent&lt;/a&gt; (A single interface to use and evaluate different agent frameworks by &lt;a href="https://www.mozilla.ai/"&gt;mozilla.ai&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/cognizant-ai-lab/neuro-san-studio"&gt;Neuro SAN&lt;/a&gt; (Data-driven multi-agent orchestration framework) with &lt;a href="https://github.com/cognizant-ai-lab/neuro-san-studio/raw/main/docs/user_guide.md#ollama"&gt;example&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ai-bot-pro/achatbot-go"&gt;achatbot-go&lt;/a&gt; a multimodal(text/audio/image) chatbot.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/attogram/ollama-bash-lib"&gt;Ollama Bash Lib&lt;/a&gt; - A Bash Library for Ollama. Run LLM prompts straight from your shell, and more&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Mobile&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aws-samples/swift-chat"&gt;SwiftChat&lt;/a&gt; (Lightning-fast Cross-platform AI chat app with native UI for Android, iOS, and iPad)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/AugustDev/enchanted"&gt;Enchanted&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Mobile-Artificial-Intelligence/maid"&gt;Maid&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/JHubi1/ollama-app"&gt;Ollama App&lt;/a&gt; (Modern and easy-to-use multi-platform client for Ollama)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/1runeberg/confichat"&gt;ConfiChat&lt;/a&gt; (Lightweight, standalone, multi-platform, and privacy-focused LLM chat interface with optional encryption)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/sunshine0523/OllamaServer"&gt;Ollama Android Chat&lt;/a&gt; (No need for Termux, start the Ollama service with one click on an Android device)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ibrahimcetin/reins"&gt;Reins&lt;/a&gt; (Easily tweak parameters, customize system prompts per chat, and enhance your AI experiments with reasoning model support.)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Extensions &amp;amp; Plugins&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/MassimilianoPasquini97/raycast_ollama"&gt;Raycast extension&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/mxyng/discollama"&gt;Discollama&lt;/a&gt; (Discord bot inside the Ollama discord channel)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/continuedev/continue"&gt;Continue&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/thewh1teagle/vibe"&gt;Vibe&lt;/a&gt; (Transcribe and analyze meetings with Ollama)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/hinterdupfinger/obsidian-ollama"&gt;Obsidian Ollama plugin&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/omagdy7/ollama-logseq"&gt;Logseq Ollama plugin&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/andersrex/notesollama"&gt;NotesOllama&lt;/a&gt; (Apple Notes Ollama plugin)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/samalba/dagger-chatbot"&gt;Dagger Chatbot&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/mekb-turtle/discord-ai-bot"&gt;Discord AI Bot&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ruecat/ollama-telegram"&gt;Ollama Telegram Bot&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ej52/hass-ollama-conversation"&gt;Hass Ollama Conversation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/abrenneke/rivet-plugin-ollama"&gt;Rivet plugin&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/longy2k/obsidian-bmo-chatbot"&gt;Obsidian BMO Chatbot plugin&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/herval/cliobot"&gt;Cliobot&lt;/a&gt; (Telegram bot with Ollama support)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/logancyang/obsidian-copilot"&gt;Copilot for Obsidian plugin&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pfrankov/obsidian-local-gpt"&gt;Obsidian Local GPT plugin&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.openinterpreter.com/language-model-setup/local-models/ollama"&gt;Open Interpreter&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ex3ndr/llama-coder"&gt;Llama Coder&lt;/a&gt; (Copilot alternative using Ollama)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/bernardo-bruning/ollama-copilot"&gt;Ollama Copilot&lt;/a&gt; (Proxy that allows you to use Ollama as a copilot like GitHub Copilot)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/rjmacarthy/twinny"&gt;twinny&lt;/a&gt; (Copilot and Copilot chat alternative using Ollama)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/RussellCanfield/wingman-ai"&gt;Wingman-AI&lt;/a&gt; (Copilot code and chat alternative using Ollama and Hugging Face)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/n4ze3m/page-assist"&gt;Page Assist&lt;/a&gt; (Chrome Extension)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/imoize/plasmoid-ollamacontrol"&gt;Plasmoid Ollama Control&lt;/a&gt; (KDE Plasma extension that allows you to quickly manage/control Ollama model)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/tusharhero/aitelegrambot"&gt;AI Telegram Bot&lt;/a&gt; (Telegram bot using Ollama in backend)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/yaroslavyaroslav/OpenAI-sublime-text"&gt;AI ST Completion&lt;/a&gt; (Sublime Text 4 AI assistant plugin with Ollama support)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/kevinthedang/discord-ollama"&gt;Discord-Ollama Chat Bot&lt;/a&gt; (Generalized TypeScript Discord Bot w/ Tuning Documentation)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/josStorer/chatGPTBox"&gt;ChatGPTBox: All in one browser extension&lt;/a&gt; with &lt;a href="https://github.com/josStorer/chatGPTBox/issues/616#issuecomment-1975186467"&gt;Integrating Tutorial&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/rapmd73/Companion"&gt;Discord AI chat/moderation bot&lt;/a&gt; Chat/moderation bot written in python. Uses Ollama to create personalities.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/nischalj10/headless-ollama"&gt;Headless Ollama&lt;/a&gt; (Scripts to automatically install ollama client &amp;amp; models on any OS for apps that depend on ollama server)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/xuyangbocn/terraform-aws-self-host-llm"&gt;Terraform AWS Ollama &amp;amp; Open WebUI&lt;/a&gt; (A Terraform module to deploy on AWS a ready-to-use Ollama service, together with its front-end Open WebUI service.)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/jakubburkiewicz/node-red-contrib-ollama"&gt;node-red-contrib-ollama&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ivostoykov/localAI"&gt;Local AI Helper&lt;/a&gt; (Chrome and Firefox extensions that enable interactions with the active tab and customisable API endpoints. Includes secure storage for user prompts.)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/SilasMarvin/lsp-ai"&gt;LSP-AI&lt;/a&gt; (Open-source language server for AI-powered functionality)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Palm1r/QodeAssist"&gt;QodeAssist&lt;/a&gt; (AI-powered coding assistant plugin for Qt Creator)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ECuiDev/obsidian-quiz-generator"&gt;Obsidian Quiz Generator plugin&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/philffm/ai-summary-helper"&gt;AI Summary Helper plugin&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/suncloudsmoon/TextCraft"&gt;TextCraft&lt;/a&gt; (Copilot in Word alternative using Ollama)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/zeitlings/alfred-ollama"&gt;Alfred Ollama&lt;/a&gt; (Alfred Workflow)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/adarshM84/TextLLaMA"&gt;TextLLaMA&lt;/a&gt; A Chrome Extension that helps you write emails, correct grammar, and translate into any language&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/zyphixor/simple-discord-ai"&gt;Simple-Discord-AI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/innightwolfsleep/llm_telegram_bot"&gt;LLM Telegram Bot&lt;/a&gt; (telegram bot, primary for RP. Oobabooga-like buttons, &lt;a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui"&gt;A1111&lt;/a&gt; API integration e.t.c)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/sammcj/mcp-llm"&gt;mcp-llm&lt;/a&gt; (MCP Server to allow LLMs to call other LLMs)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/HardCodeDev777/SimpleOllamaUnity"&gt;SimpleOllamaUnity&lt;/a&gt; (Unity Engine extension for communicating with Ollama in a few lines of code. Also works at runtime)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/HardCodeDev777/UnityCodeLama"&gt;UnityCodeLama&lt;/a&gt; (Unity Editor tool to analyze scripts via Ollama)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/NativeMindBrowser/NativeMindExtension"&gt;NativeMind&lt;/a&gt; (Private, on-device AI Assistant, no cloud dependencies)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://gmai.premex.se/"&gt;GMAI - Gradle Managed AI&lt;/a&gt; (Gradle plugin for automated Ollama lifecycle management during build phases)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/nomyo-ai/nomyo-router"&gt;NOMYO Router&lt;/a&gt; (A transparent Ollama proxy with model deployment aware routing which auto-manages multiple Ollama instances in a given network)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Supported backends&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ggml-org/llama.cpp"&gt;llama.cpp&lt;/a&gt; project founded by Georgi Gerganov.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Observability&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.comet.com/docs/opik/cookbook/ollama"&gt;Opik&lt;/a&gt; is an open-source platform to debug, evaluate, and monitor your LLM applications, RAG systems, and agentic workflows with comprehensive tracing, automated evaluations, and production-ready dashboards. Opik supports native integration to Ollama.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://lunary.ai/docs/integrations/ollama"&gt;Lunary&lt;/a&gt; is the leading open-source LLM observability platform. It provides a variety of enterprise-grade features such as real-time analytics, prompt templates management, PII masking, and comprehensive agent tracing.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/openlit/openlit"&gt;OpenLIT&lt;/a&gt; is an OpenTelemetry-native tool for monitoring Ollama Applications &amp;amp; GPUs using traces and metrics.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.honeyhive.ai/integrations/ollama"&gt;HoneyHive&lt;/a&gt; is an AI observability and evaluation platform for AI agents. Use HoneyHive to evaluate agent performance, interrogate failures, and monitor quality in production.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://langfuse.com/docs/integrations/ollama"&gt;Langfuse&lt;/a&gt; is an open source LLM observability platform that enables teams to collaboratively monitor, evaluate and debug AI applications.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://mlflow.org/docs/latest/llms/tracing/index.html#automatic-tracing"&gt;MLflow Tracing&lt;/a&gt; is an open source LLM observability tool with a convenient API to log and visualize traces, making it easy to debug and evaluate GenAI applications.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Security&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ParisNeo/ollama_proxy_server"&gt;Ollama Fortress&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
  </channel>
</rss>