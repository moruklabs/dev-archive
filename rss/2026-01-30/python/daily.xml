<rss version="2.0">
  <channel>
    <title>GitHub Python Daily Trending</title>
    <description>Daily Trending of Python in GitHub</description>
    <pubDate>Thu, 29 Jan 2026 01:41:46 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>MoonshotAI/kimi-cli</title>
      <link>https://github.com/MoonshotAI/kimi-cli</link>
      <description>&lt;p&gt;Kimi Code CLI is your next CLI agent.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Kimi Code CLI&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/MoonshotAI/kimi-cli/graphs/commit-activity"&gt;&lt;img src="https://img.shields.io/github/commit-activity/w/MoonshotAI/kimi-cli" alt="Commit Activity" /&gt;&lt;/a&gt; &lt;a href="https://github.com/MoonshotAI/kimi-cli/actions"&gt;&lt;img src="https://img.shields.io/github/check-runs/MoonshotAI/kimi-cli/main" alt="Checks" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/kimi-cli/"&gt;&lt;img src="https://img.shields.io/pypi/v/kimi-cli" alt="Version" /&gt;&lt;/a&gt; &lt;a href="https://pypistats.org/packages/kimi-cli"&gt;&lt;img src="https://img.shields.io/pypi/dw/kimi-cli" alt="Downloads" /&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/MoonshotAI/kimi-cli"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.kimi.com/code/"&gt;Kimi Code&lt;/a&gt; | &lt;a href="https://moonshotai.github.io/kimi-cli/en/"&gt;Documentation&lt;/a&gt; | &lt;a href="https://moonshotai.github.io/kimi-cli/zh/"&gt;ÊñáÊ°£&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Kimi Code CLI is an AI agent that runs in the terminal, helping you complete software development tasks and terminal operations. It can read and edit code, execute shell commands, search and fetch web pages, and autonomously plan and adjust actions during execution.&lt;/p&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://moonshotai.github.io/kimi-cli/en/guides/getting-started.html"&gt;Getting Started&lt;/a&gt; for how to install and start using Kimi Code CLI.&lt;/p&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;h3&gt;Shell command mode&lt;/h3&gt; 
&lt;p&gt;Kimi Code CLI is not only a coding agent, but also a shell. You can switch the shell command mode by pressing &lt;code&gt;Ctrl-X&lt;/code&gt;. In this mode, you can directly run shell commands without leaving Kimi Code CLI.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/MoonshotAI/kimi-cli/main/docs/media/shell-mode.gif" alt="" /&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Built-in shell commands like &lt;code&gt;cd&lt;/code&gt; are not supported yet.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;IDE integration via ACP&lt;/h3&gt; 
&lt;p&gt;Kimi Code CLI supports &lt;a href="https://github.com/agentclientprotocol/agent-client-protocol"&gt;Agent Client Protocol&lt;/a&gt; out of the box. You can use it together with any ACP-compatible editor or IDE.&lt;/p&gt; 
&lt;p&gt;To use Kimi Code CLI with ACP clients, make sure to run Kimi Code CLI in the terminal and send &lt;code&gt;/login&lt;/code&gt; to complete the login first. Then, you can configure your ACP client to start Kimi Code CLI as an ACP agent server with command &lt;code&gt;kimi acp&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;For example, to use Kimi Code CLI with &lt;a href="https://zed.dev/"&gt;Zed&lt;/a&gt; or &lt;a href="https://blog.jetbrains.com/ai/2025/12/bring-your-own-ai-agent-to-jetbrains-ides/"&gt;JetBrains&lt;/a&gt;, add the following configuration to your &lt;code&gt;~/.config/zed/settings.json&lt;/code&gt; or &lt;code&gt;~/.jetbrains/acp.json&lt;/code&gt; file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "agent_servers": {
    "Kimi Code CLI": {
      "command": "kimi",
      "args": ["acp"],
      "env": {}
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then you can create Kimi Code CLI threads in IDE's agent panel.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/MoonshotAI/kimi-cli/main/docs/media/acp-integration.gif" alt="" /&gt;&lt;/p&gt; 
&lt;h3&gt;Zsh integration&lt;/h3&gt; 
&lt;p&gt;You can use Kimi Code CLI together with Zsh, to empower your shell experience with AI agent capabilities.&lt;/p&gt; 
&lt;p&gt;Install the &lt;a href="https://github.com/MoonshotAI/zsh-kimi-cli"&gt;zsh-kimi-cli&lt;/a&gt; plugin via:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;git clone https://github.com/MoonshotAI/zsh-kimi-cli.git \
  ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/kimi-cli
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] If you are using a plugin manager other than Oh My Zsh, you may need to refer to the plugin's README for installation instructions.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Then add &lt;code&gt;kimi-cli&lt;/code&gt; to your Zsh plugin list in &lt;code&gt;~/.zshrc&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;plugins=(... kimi-cli)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After restarting Zsh, you can switch to agent mode by pressing &lt;code&gt;Ctrl-X&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;MCP support&lt;/h3&gt; 
&lt;p&gt;Kimi Code CLI supports MCP (Model Context Protocol) tools.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;code&gt;kimi mcp&lt;/code&gt; sub-command group&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;You can manage MCP servers with &lt;code&gt;kimi mcp&lt;/code&gt; sub-command group. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Add streamable HTTP server:
kimi mcp add --transport http context7 https://mcp.context7.com/mcp --header "CONTEXT7_API_KEY: ctx7sk-your-key"

# Add streamable HTTP server with OAuth authorization:
kimi mcp add --transport http --auth oauth linear https://mcp.linear.app/mcp

# Add stdio server:
kimi mcp add --transport stdio chrome-devtools -- npx chrome-devtools-mcp@latest

# List added MCP servers:
kimi mcp list

# Remove an MCP server:
kimi mcp remove chrome-devtools

# Authorize an MCP server:
kimi mcp auth linear
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Ad-hoc MCP configuration&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Kimi Code CLI also supports ad-hoc MCP server configuration via CLI option.&lt;/p&gt; 
&lt;p&gt;Given an MCP config file in the well-known MCP config format like the following:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "mcpServers": {
    "context7": {
      "url": "https://mcp.context7.com/mcp",
      "headers": {
        "CONTEXT7_API_KEY": "YOUR_API_KEY"
      }
    },
    "chrome-devtools": {
      "command": "npx",
      "args": ["-y", "chrome-devtools-mcp@latest"]
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Run &lt;code&gt;kimi&lt;/code&gt; with &lt;code&gt;--mcp-config-file&lt;/code&gt; option to connect to the specified MCP servers:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;kimi --mcp-config-file /path/to/mcp.json
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;More&lt;/h3&gt; 
&lt;p&gt;See more features in the &lt;a href="https://moonshotai.github.io/kimi-cli/en/"&gt;Documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Development&lt;/h2&gt; 
&lt;p&gt;To develop Kimi Code CLI, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;git clone https://github.com/MoonshotAI/kimi-cli.git
cd kimi-cli

make prepare  # prepare the development environment
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then you can start working on Kimi Code CLI.&lt;/p&gt; 
&lt;p&gt;Refer to the following commands after you make changes:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;uv run kimi  # run Kimi Code CLI

make format  # format code
make check  # run linting and type checking
make test  # run tests
make test-kimi-cli  # run Kimi Code CLI tests only
make test-kosong  # run kosong tests only
make test-pykaos  # run pykaos tests only
make build  # build python packages
make build-bin  # build standalone binary
make help  # show all make targets
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions to Kimi Code CLI! Please refer to &lt;a href="https://raw.githubusercontent.com/MoonshotAI/kimi-cli/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for more information.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>karpathy/minGPT</title>
      <link>https://github.com/karpathy/minGPT</link>
      <description>&lt;p&gt;A minimal PyTorch re-implementation of the OpenAI GPT (Generative Pretrained Transformer) training&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;minGPT&lt;/h1&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/karpathy/minGPT/master/mingpt.jpg" alt="mingpt" /&gt;&lt;/p&gt; 
&lt;p&gt;A PyTorch re-implementation of &lt;a href="https://github.com/openai/gpt-2"&gt;GPT&lt;/a&gt;, both training and inference. minGPT tries to be small, clean, interpretable and educational, as most of the currently available GPT model implementations can a bit sprawling. GPT is not a complicated model and this implementation is appropriately about 300 lines of code (see &lt;a href="https://raw.githubusercontent.com/karpathy/minGPT/master/mingpt/model.py"&gt;mingpt/model.py&lt;/a&gt;). All that's going on is that a sequence of indices feeds into a &lt;a href="https://arxiv.org/abs/1706.03762"&gt;Transformer&lt;/a&gt;, and a probability distribution over the next index in the sequence comes out. The majority of the complexity is just being clever with batching (both across examples and over sequence length) for efficiency.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;note (Jan 2023)&lt;/strong&gt;: though I may continue to accept and change some details, minGPT is in a semi-archived state. For more recent developments see my rewrite &lt;a href="https://github.com/karpathy/nanoGPT"&gt;nanoGPT&lt;/a&gt;. Basically, minGPT became referenced across a wide variety of places (notebooks, blogs, courses, books, etc.) which made me less willing to make the bigger changes I wanted to make to move the code forward. I also wanted to change the direction a bit, from a sole focus on education to something that is still simple and hackable but has teeth (reproduces medium-sized industry benchmarks, accepts some tradeoffs to gain runtime efficiency, etc).&lt;/p&gt; 
&lt;p&gt;The minGPT library is three files: &lt;a href="https://raw.githubusercontent.com/karpathy/minGPT/master/mingpt/model.py"&gt;mingpt/model.py&lt;/a&gt; contains the actual Transformer model definition, &lt;a href="https://raw.githubusercontent.com/karpathy/minGPT/master/mingpt/bpe.py"&gt;mingpt/bpe.py&lt;/a&gt; contains a mildly refactored Byte Pair Encoder that translates between text and sequences of integers exactly like OpenAI did in GPT, &lt;a href="https://raw.githubusercontent.com/karpathy/minGPT/master/mingpt/trainer.py"&gt;mingpt/trainer.py&lt;/a&gt; is (GPT-independent) PyTorch boilerplate code that trains the model. Then there are a number of demos and projects that use the library in the &lt;code&gt;projects&lt;/code&gt; folder:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;projects/adder&lt;/code&gt; trains a GPT from scratch to add numbers (inspired by the addition section in the GPT-3 paper)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;projects/chargpt&lt;/code&gt; trains a GPT to be a character-level language model on some input text file&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;demo.ipynb&lt;/code&gt; shows a minimal usage of the &lt;code&gt;GPT&lt;/code&gt; and &lt;code&gt;Trainer&lt;/code&gt; in a notebook format on a simple sorting example&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;generate.ipynb&lt;/code&gt; shows how one can load a pretrained GPT2 and generate text given some prompt&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Library Installation&lt;/h3&gt; 
&lt;p&gt;If you want to &lt;code&gt;import mingpt&lt;/code&gt; into your project:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;git clone https://github.com/karpathy/minGPT.git
cd minGPT
pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Usage&lt;/h3&gt; 
&lt;p&gt;Here's how you'd instantiate a GPT-2 (124M param version):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mingpt.model import GPT
model_config = GPT.get_default_config()
model_config.model_type = 'gpt2'
model_config.vocab_size = 50257 # openai's model vocabulary
model_config.block_size = 1024  # openai's model block_size (i.e. input context length)
model = GPT(model_config)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;And here's how you'd train it:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# your subclass of torch.utils.data.Dataset that emits example
# torch LongTensor of lengths up to 1024, with integers from [0,50257)
train_dataset = YourDataset()

from mingpt.trainer import Trainer
train_config = Trainer.get_default_config()
train_config.learning_rate = 5e-4 # many possible options, see the file
train_config.max_iters = 1000
train_config.batch_size = 32
trainer = Trainer(train_config, model, train_dataset)
trainer.run()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;code&gt;demo.ipynb&lt;/code&gt; for a more concrete example.&lt;/p&gt; 
&lt;h3&gt;Unit tests&lt;/h3&gt; 
&lt;p&gt;Coverage is not super amazing just yet but:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;python -m unittest discover tests
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;todos&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;add gpt-2 finetuning demo on arbitrary given text file&lt;/li&gt; 
 &lt;li&gt;add dialog agent demo&lt;/li&gt; 
 &lt;li&gt;better docs of outcomes for existing projects (adder, chargpt)&lt;/li&gt; 
 &lt;li&gt;add mixed precision and related training scaling goodies&lt;/li&gt; 
 &lt;li&gt;distributed training support&lt;/li&gt; 
 &lt;li&gt;reproduce some benchmarks in projects/, e.g. text8 or other language modeling&lt;/li&gt; 
 &lt;li&gt;proper logging instead of print statement amateur hour haha&lt;/li&gt; 
 &lt;li&gt;i probably should have a requirements.txt file...&lt;/li&gt; 
 &lt;li&gt;it should be possible to load in many other model weights other than just gpt2-*&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;References&lt;/h3&gt; 
&lt;p&gt;Code:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/openai/gpt-2"&gt;openai/gpt-2&lt;/a&gt; has the model definition in TensorFlow, but not the training code&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/openai/image-gpt"&gt;openai/image-gpt&lt;/a&gt; has some more modern gpt-3 like modification in its code, good reference as well&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/huggingface/transformers"&gt;huggingface/transformers&lt;/a&gt; has a &lt;a href="https://github.com/huggingface/transformers/tree/master/examples/pytorch/language-modeling"&gt;language-modeling example&lt;/a&gt;. It is full-featured but as a result also somewhat challenging to trace. E.g. some large functions have as much as 90% unused code behind various branching statements that is unused in the default setting of simple language modeling&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Papers + some implementation notes:&lt;/p&gt; 
&lt;h4&gt;Improving Language Understanding by Generative Pre-Training (GPT-1)&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Our model largely follows the original transformer work&lt;/li&gt; 
 &lt;li&gt;We trained a 12-layer decoder-only transformer with masked self-attention heads (768 dimensional states and 12 attention heads). For the position-wise feed-forward networks, we used 3072 dimensional inner states.&lt;/li&gt; 
 &lt;li&gt;Adam max learning rate of 2.5e-4. (later GPT-3 for this model size uses 6e-4)&lt;/li&gt; 
 &lt;li&gt;LR decay: increased linearly from zero over the first 2000 updates and annealed to 0 using a cosine schedule&lt;/li&gt; 
 &lt;li&gt;We train for 100 epochs on minibatches of 64 randomly sampled, contiguous sequences of 512 tokens.&lt;/li&gt; 
 &lt;li&gt;Since layernorm is used extensively throughout the model, a simple weight initialization of N(0, 0.02) was sufficient&lt;/li&gt; 
 &lt;li&gt;bytepair encoding (BPE) vocabulary with 40,000 merges&lt;/li&gt; 
 &lt;li&gt;residual, embedding, and attention dropouts with a rate of 0.1 for regularization.&lt;/li&gt; 
 &lt;li&gt;modified version of L2 regularization proposed in (37), with w = 0.01 on all non bias or gain weights&lt;/li&gt; 
 &lt;li&gt;For the activation function, we used the Gaussian Error Linear Unit (GELU).&lt;/li&gt; 
 &lt;li&gt;We used learned position embeddings instead of the sinusoidal version proposed in the original work&lt;/li&gt; 
 &lt;li&gt;For finetuning: We add dropout to the classifier with a rate of 0.1. learning rate of 6.25e-5 and a batchsize of 32. 3 epochs. We use a linear learning rate decay schedule with warmup over 0.2% of training. Œª was set to 0.5.&lt;/li&gt; 
 &lt;li&gt;GPT-1 model is 12 layers and d_model 768, ~117M params&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Language Models are Unsupervised Multitask Learners (GPT-2)&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;LayerNorm was moved to the input of each sub-block, similar to a pre-activation residual network&lt;/li&gt; 
 &lt;li&gt;an additional layer normalization was added after the final self-attention block.&lt;/li&gt; 
 &lt;li&gt;modified initialization which accounts for the accumulation on the residual path with model depth is used. We scale the weights of residual layers at initialization by a factor of 1/‚àöN where N is the number of residual layers. (weird because in their released code i can only find a simple use of the old 0.02... in their release of image-gpt I found it used for c_proj, and even then only for attn, not for mlp. huh. &lt;a href="https://github.com/openai/image-gpt/raw/master/src/model.py"&gt;https://github.com/openai/image-gpt/blob/master/src/model.py&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;the vocabulary is expanded to 50,257&lt;/li&gt; 
 &lt;li&gt;increase the context size from 512 to 1024 tokens&lt;/li&gt; 
 &lt;li&gt;larger batchsize of 512 is used&lt;/li&gt; 
 &lt;li&gt;GPT-2 used 48 layers and d_model 1600 (vs. original 12 layers and d_model 768). ~1.542B params&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Language Models are Few-Shot Learners (GPT-3)&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;GPT-3: 96 layers, 96 heads, with d_model of 12,288 (175B parameters).&lt;/li&gt; 
 &lt;li&gt;GPT-1-like: 12 layers, 12 heads, d_model 768 (125M)&lt;/li&gt; 
 &lt;li&gt;We use the same model and architecture as GPT-2, including the modified initialization, pre-normalization, and reversible tokenization described therein&lt;/li&gt; 
 &lt;li&gt;we use alternating dense and locally banded sparse attention patterns in the layers of the transformer, similar to the Sparse Transformer&lt;/li&gt; 
 &lt;li&gt;we always have the feedforward layer four times the size of the bottleneck layer, dff = 4 ‚àó dmodel&lt;/li&gt; 
 &lt;li&gt;all models use a context window of nctx = 2048 tokens.&lt;/li&gt; 
 &lt;li&gt;Adam with Œ≤1 = 0.9, Œ≤2 = 0.95, and eps = 10‚àí8&lt;/li&gt; 
 &lt;li&gt;All models use weight decay of 0.1 to provide a small amount of regularization. (NOTE: GPT-1 used 0.01 I believe, see above)&lt;/li&gt; 
 &lt;li&gt;clip the global norm of the gradient at 1.0&lt;/li&gt; 
 &lt;li&gt;Linear LR warmup over the first 375 million tokens. Then use cosine decay for learning rate down to 10% of its value, over 260 billion tokens.&lt;/li&gt; 
 &lt;li&gt;gradually increase the batch size linearly from a small value (32k tokens) to the full value over the first 4-12 billion tokens of training, depending on the model size.&lt;/li&gt; 
 &lt;li&gt;full 2048-sized time context window is always used, with a special END OF DOCUMENT token delimiter&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Generative Pretraining from Pixels (Image GPT)&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;When working with images, we pick the identity permutation œÄi = i for 1 ‚â§ i ‚â§ n, also known as raster order.&lt;/li&gt; 
 &lt;li&gt;we create our own 9-bit color palette by clustering (R, G, B) pixel values using k-means with k = 512.&lt;/li&gt; 
 &lt;li&gt;Our largest model, iGPT-XL, contains L = 60 layers and uses an embedding size of d = 3072 for a total of 6.8B parameters.&lt;/li&gt; 
 &lt;li&gt;Our next largest model, iGPT-L, is essentially identical to GPT-2 with L = 48 layers, but contains a slightly smaller embedding size of d = 1536 (vs 1600) for a total of 1.4B parameters.&lt;/li&gt; 
 &lt;li&gt;We use the same model code as GPT-2, except that we initialize weights in the layerdependent fashion as in Sparse Transformer (Child et al., 2019) and zero-initialize all projections producing logits.&lt;/li&gt; 
 &lt;li&gt;We also train iGPT-M, a 455M parameter model with L = 36 and d = 1024&lt;/li&gt; 
 &lt;li&gt;iGPT-S, a 76M parameter model with L = 24 and d = 512 (okay, and how many heads? looks like the Github code claims 8)&lt;/li&gt; 
 &lt;li&gt;When pre-training iGPT-XL, we use a batch size of 64 and train for 2M iterations, and for all other models we use a batch size of 128 and train for 1M iterations.&lt;/li&gt; 
 &lt;li&gt;Adam with Œ≤1 = 0.9 and Œ≤2 = 0.95&lt;/li&gt; 
 &lt;li&gt;The learning rate is warmed up for one epoch, and then decays to 0&lt;/li&gt; 
 &lt;li&gt;We did not use weight decay because applying a small weight decay of 0.01 did not change representation quality.&lt;/li&gt; 
 &lt;li&gt;iGPT-S lr 0.003&lt;/li&gt; 
 &lt;li&gt;No dropout is used.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;License&lt;/h3&gt; 
&lt;p&gt;MIT&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>open-webui/open-webui</title>
      <link>https://github.com/open-webui/open-webui</link>
      <description>&lt;p&gt;User-friendly AI Interface (Supports Ollama, OpenAI API, ...)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Open WebUI üëã&lt;/h1&gt; 
&lt;p&gt;&lt;img src="https://img.shields.io/github/stars/open-webui/open-webui?style=social" alt="GitHub stars" /&gt; &lt;img src="https://img.shields.io/github/forks/open-webui/open-webui?style=social" alt="GitHub forks" /&gt; &lt;img src="https://img.shields.io/github/watchers/open-webui/open-webui?style=social" alt="GitHub watchers" /&gt; &lt;img src="https://img.shields.io/github/repo-size/open-webui/open-webui" alt="GitHub repo size" /&gt; &lt;img src="https://img.shields.io/github/languages/count/open-webui/open-webui" alt="GitHub language count" /&gt; &lt;img src="https://img.shields.io/github/languages/top/open-webui/open-webui" alt="GitHub top language" /&gt; &lt;img src="https://img.shields.io/github/last-commit/open-webui/open-webui?color=red" alt="GitHub last commit" /&gt; &lt;a href="https://discord.gg/5rJgQTnV4s"&gt;&lt;img src="https://img.shields.io/badge/Discord-Open_WebUI-blue?logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://github.com/sponsors/tjbck"&gt;&lt;img src="https://img.shields.io/static/v1?label=Sponsor&amp;amp;message=%E2%9D%A4&amp;amp;logo=GitHub&amp;amp;color=%23fe8e86" alt="" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/open-webui/open-webui/main/banner.png" alt="Open WebUI Banner" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Open WebUI is an &lt;a href="https://docs.openwebui.com/features/plugin/"&gt;extensible&lt;/a&gt;, feature-rich, and user-friendly self-hosted AI platform designed to operate entirely offline.&lt;/strong&gt; It supports various LLM runners like &lt;strong&gt;Ollama&lt;/strong&gt; and &lt;strong&gt;OpenAI-compatible APIs&lt;/strong&gt;, with &lt;strong&gt;built-in inference engine&lt;/strong&gt; for RAG, making it a &lt;strong&gt;powerful AI deployment solution&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;Passionate about open-source AI? &lt;a href="https://careers.openwebui.com/"&gt;Join our team ‚Üí&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/open-webui/open-webui/main/demo.png" alt="Open WebUI Demo" /&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP]&lt;br /&gt; &lt;strong&gt;Looking for an &lt;a href="https://docs.openwebui.com/enterprise"&gt;Enterprise Plan&lt;/a&gt;?&lt;/strong&gt; ‚Äì &lt;strong&gt;&lt;a href="https://docs.openwebui.com/enterprise"&gt;Speak with Our Sales Team Today!&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;Get &lt;strong&gt;enhanced capabilities&lt;/strong&gt;, including &lt;strong&gt;custom theming and branding&lt;/strong&gt;, &lt;strong&gt;Service Level Agreement (SLA) support&lt;/strong&gt;, &lt;strong&gt;Long-Term Support (LTS) versions&lt;/strong&gt;, and &lt;strong&gt;more!&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;For more information, be sure to check out our &lt;a href="https://docs.openwebui.com/"&gt;Open WebUI Documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Key Features of Open WebUI ‚≠ê&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;üöÄ &lt;strong&gt;Effortless Setup&lt;/strong&gt;: Install seamlessly using Docker or Kubernetes (kubectl, kustomize or helm) for a hassle-free experience with support for both &lt;code&gt;:ollama&lt;/code&gt; and &lt;code&gt;:cuda&lt;/code&gt; tagged images.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;ü§ù &lt;strong&gt;Ollama/OpenAI API Integration&lt;/strong&gt;: Effortlessly integrate OpenAI-compatible APIs for versatile conversations alongside Ollama models. Customize the OpenAI API URL to link with &lt;strong&gt;LMStudio, GroqCloud, Mistral, OpenRouter, and more&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üõ°Ô∏è &lt;strong&gt;Granular Permissions and User Groups&lt;/strong&gt;: By allowing administrators to create detailed user roles and permissions, we ensure a secure user environment. This granularity not only enhances security but also allows for customized user experiences, fostering a sense of ownership and responsibility amongst users.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üì± &lt;strong&gt;Responsive Design&lt;/strong&gt;: Enjoy a seamless experience across Desktop PC, Laptop, and Mobile devices.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üì± &lt;strong&gt;Progressive Web App (PWA) for Mobile&lt;/strong&gt;: Enjoy a native app-like experience on your mobile device with our PWA, providing offline access on localhost and a seamless user interface.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;‚úíÔ∏èüî¢ &lt;strong&gt;Full Markdown and LaTeX Support&lt;/strong&gt;: Elevate your LLM experience with comprehensive Markdown and LaTeX capabilities for enriched interaction.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üé§üìπ &lt;strong&gt;Hands-Free Voice/Video Call&lt;/strong&gt;: Experience seamless communication with integrated hands-free voice and video call features using multiple Speech-to-Text providers (Local Whisper, OpenAI, Deepgram, Azure) and Text-to-Speech engines (Azure, ElevenLabs, OpenAI, Transformers, WebAPI), allowing for dynamic and interactive chat environments.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üõ†Ô∏è &lt;strong&gt;Model Builder&lt;/strong&gt;: Easily create Ollama models via the Web UI. Create and add custom characters/agents, customize chat elements, and import models effortlessly through &lt;a href="https://openwebui.com/"&gt;Open WebUI Community&lt;/a&gt; integration.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üêç &lt;strong&gt;Native Python Function Calling Tool&lt;/strong&gt;: Enhance your LLMs with built-in code editor support in the tools workspace. Bring Your Own Function (BYOF) by simply adding your pure Python functions, enabling seamless integration with LLMs.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üíæ &lt;strong&gt;Persistent Artifact Storage&lt;/strong&gt;: Built-in key-value storage API for artifacts, enabling features like journals, trackers, leaderboards, and collaborative tools with both personal and shared data scopes across sessions.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üìö &lt;strong&gt;Local RAG Integration&lt;/strong&gt;: Dive into the future of chat interactions with groundbreaking Retrieval Augmented Generation (RAG) support using your choice of 9 vector databases and multiple content extraction engines (Tika, Docling, Document Intelligence, Mistral OCR, External loaders). Load documents directly into chat or add files to your document library, effortlessly accessing them using the &lt;code&gt;#&lt;/code&gt; command before a query.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üîç &lt;strong&gt;Web Search for RAG&lt;/strong&gt;: Perform web searches using 15+ providers including &lt;code&gt;SearXNG&lt;/code&gt;, &lt;code&gt;Google PSE&lt;/code&gt;, &lt;code&gt;Brave Search&lt;/code&gt;, &lt;code&gt;Kagi&lt;/code&gt;, &lt;code&gt;Mojeek&lt;/code&gt;, &lt;code&gt;Tavily&lt;/code&gt;, &lt;code&gt;Perplexity&lt;/code&gt;, &lt;code&gt;serpstack&lt;/code&gt;, &lt;code&gt;serper&lt;/code&gt;, &lt;code&gt;Serply&lt;/code&gt;, &lt;code&gt;DuckDuckGo&lt;/code&gt;, &lt;code&gt;SearchApi&lt;/code&gt;, &lt;code&gt;SerpApi&lt;/code&gt;, &lt;code&gt;Bing&lt;/code&gt;, &lt;code&gt;Jina&lt;/code&gt;, &lt;code&gt;Exa&lt;/code&gt;, &lt;code&gt;Sougou&lt;/code&gt;, &lt;code&gt;Azure AI Search&lt;/code&gt;, and &lt;code&gt;Ollama Cloud&lt;/code&gt;, injecting results directly into your chat experience.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üåê &lt;strong&gt;Web Browsing Capability&lt;/strong&gt;: Seamlessly integrate websites into your chat experience using the &lt;code&gt;#&lt;/code&gt; command followed by a URL. This feature allows you to incorporate web content directly into your conversations, enhancing the richness and depth of your interactions.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üé® &lt;strong&gt;Image Generation &amp;amp; Editing Integration&lt;/strong&gt;: Create and edit images using multiple engines including OpenAI's DALL-E, Gemini, ComfyUI (local), and AUTOMATIC1111 (local), with support for both generation and prompt-based editing workflows.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;‚öôÔ∏è &lt;strong&gt;Many Models Conversations&lt;/strong&gt;: Effortlessly engage with various models simultaneously, harnessing their unique strengths for optimal responses. Enhance your experience by leveraging a diverse set of models in parallel.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üîê &lt;strong&gt;Role-Based Access Control (RBAC)&lt;/strong&gt;: Ensure secure access with restricted permissions; only authorized individuals can access your Ollama, and exclusive model creation/pulling rights are reserved for administrators.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üóÑÔ∏è &lt;strong&gt;Flexible Database &amp;amp; Storage Options&lt;/strong&gt;: Choose from SQLite (with optional encryption), PostgreSQL, or configure cloud storage backends (S3, Google Cloud Storage, Azure Blob Storage) for scalable deployments.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üîç &lt;strong&gt;Advanced Vector Database Support&lt;/strong&gt;: Select from 9 vector database options including ChromaDB, PGVector, Qdrant, Milvus, Elasticsearch, OpenSearch, Pinecone, S3Vector, and Oracle 23ai for optimal RAG performance.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üîê &lt;strong&gt;Enterprise Authentication&lt;/strong&gt;: Full support for LDAP/Active Directory integration, SCIM 2.0 automated provisioning, and SSO via trusted headers alongside OAuth providers. Enterprise-grade user and group provisioning through SCIM 2.0 protocol, enabling seamless integration with identity providers like Okta, Azure AD, and Google Workspace for automated user lifecycle management.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;‚òÅÔ∏è &lt;strong&gt;Cloud-Native Integration&lt;/strong&gt;: Native support for Google Drive and OneDrive/SharePoint file picking, enabling seamless document import from enterprise cloud storage.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üìä &lt;strong&gt;Production Observability&lt;/strong&gt;: Built-in OpenTelemetry support for traces, metrics, and logs, enabling comprehensive monitoring with your existing observability stack.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;‚öñÔ∏è &lt;strong&gt;Horizontal Scalability&lt;/strong&gt;: Redis-backed session management and WebSocket support for multi-worker and multi-node deployments behind load balancers.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üåêüåç &lt;strong&gt;Multilingual Support&lt;/strong&gt;: Experience Open WebUI in your preferred language with our internationalization (i18n) support. Join us in expanding our supported languages! We're actively seeking contributors!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üß© &lt;strong&gt;Pipelines, Open WebUI Plugin Support&lt;/strong&gt;: Seamlessly integrate custom logic and Python libraries into Open WebUI using &lt;a href="https://github.com/open-webui/pipelines"&gt;Pipelines Plugin Framework&lt;/a&gt;. Launch your Pipelines instance, set the OpenAI URL to the Pipelines URL, and explore endless possibilities. &lt;a href="https://github.com/open-webui/pipelines/tree/main/examples"&gt;Examples&lt;/a&gt; include &lt;strong&gt;Function Calling&lt;/strong&gt;, User &lt;strong&gt;Rate Limiting&lt;/strong&gt; to control access, &lt;strong&gt;Usage Monitoring&lt;/strong&gt; with tools like Langfuse, &lt;strong&gt;Live Translation with LibreTranslate&lt;/strong&gt; for multilingual support, &lt;strong&gt;Toxic Message Filtering&lt;/strong&gt; and much more.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üåü &lt;strong&gt;Continuous Updates&lt;/strong&gt;: We are committed to improving Open WebUI with regular updates, fixes, and new features.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Want to learn more about Open WebUI's features? Check out our &lt;a href="https://docs.openwebui.com/features"&gt;Open WebUI documentation&lt;/a&gt; for a comprehensive overview!&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;We are incredibly grateful for the generous support of our sponsors. Their contributions help us to maintain and improve our project, ensuring we can continue to deliver quality work to our community. Thank you!&lt;/p&gt; 
&lt;h2&gt;How to Install üöÄ&lt;/h2&gt; 
&lt;h3&gt;Installation via Python pip üêç&lt;/h3&gt; 
&lt;p&gt;Open WebUI can be installed using pip, the Python package installer. Before proceeding, ensure you're using &lt;strong&gt;Python 3.11&lt;/strong&gt; to avoid compatibility issues.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install Open WebUI&lt;/strong&gt;: Open your terminal and run the following command to install Open WebUI:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pip install open-webui
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Running Open WebUI&lt;/strong&gt;: After installation, you can start Open WebUI by executing:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;open-webui serve
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;This will start the Open WebUI server, which you can access at &lt;a href="http://localhost:8080"&gt;http://localhost:8080&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Quick Start with Docker üê≥&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;br /&gt; Please note that for certain Docker environments, additional configurations might be needed. If you encounter any connection issues, our detailed guide on &lt;a href="https://docs.openwebui.com/"&gt;Open WebUI Documentation&lt;/a&gt; is ready to assist you.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING] When using Docker to install Open WebUI, make sure to include the &lt;code&gt;-v open-webui:/app/backend/data&lt;/code&gt; in your Docker command. This step is crucial as it ensures your database is properly mounted and prevents any loss of data.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP]&lt;br /&gt; If you wish to utilize Open WebUI with Ollama included or CUDA acceleration, we recommend utilizing our official images tagged with either &lt;code&gt;:cuda&lt;/code&gt; or &lt;code&gt;:ollama&lt;/code&gt;. To enable CUDA, you must install the &lt;a href="https://docs.nvidia.com/dgx/nvidia-container-runtime-upgrade/"&gt;Nvidia CUDA container toolkit&lt;/a&gt; on your Linux/WSL system.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Installation with Default Configuration&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;If Ollama is on your computer&lt;/strong&gt;, use this command:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;If Ollama is on a Different Server&lt;/strong&gt;, use this command:&lt;/p&gt; &lt;p&gt;To connect to Ollama on another server, change the &lt;code&gt;OLLAMA_BASE_URL&lt;/code&gt; to the server's URL:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;docker run -d -p 3000:8080 -e OLLAMA_BASE_URL=https://example.com -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;To run Open WebUI with Nvidia GPU support&lt;/strong&gt;, use this command:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;docker run -d -p 3000:8080 --gpus all --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:cuda
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Installation for OpenAI API Usage Only&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;If you're only using OpenAI API&lt;/strong&gt;, use this command:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;docker run -d -p 3000:8080 -e OPENAI_API_KEY=your_secret_key -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Installing Open WebUI with Bundled Ollama Support&lt;/h3&gt; 
&lt;p&gt;This installation method uses a single container image that bundles Open WebUI with Ollama, allowing for a streamlined setup via a single command. Choose the appropriate command based on your hardware setup:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;With GPU Support&lt;/strong&gt;: Utilize GPU resources by running the following command:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;docker run -d -p 3000:8080 --gpus=all -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;For CPU Only&lt;/strong&gt;: If you're not using a GPU, use this command instead:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;docker run -d -p 3000:8080 -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Both commands facilitate a built-in, hassle-free installation of both Open WebUI and Ollama, ensuring that you can get everything up and running swiftly.&lt;/p&gt; 
&lt;p&gt;After installation, you can access Open WebUI at &lt;a href="http://localhost:3000"&gt;http://localhost:3000&lt;/a&gt;. Enjoy! üòÑ&lt;/p&gt; 
&lt;h3&gt;Other Installation Methods&lt;/h3&gt; 
&lt;p&gt;We offer various installation alternatives, including non-Docker native installation methods, Docker Compose, Kustomize, and Helm. Visit our &lt;a href="https://docs.openwebui.com/getting-started/"&gt;Open WebUI Documentation&lt;/a&gt; or join our &lt;a href="https://discord.gg/5rJgQTnV4s"&gt;Discord community&lt;/a&gt; for comprehensive guidance.&lt;/p&gt; 
&lt;p&gt;Look at the &lt;a href="https://docs.openwebui.com/getting-started/advanced-topics/development"&gt;Local Development Guide&lt;/a&gt; for instructions on setting up a local development environment.&lt;/p&gt; 
&lt;h3&gt;Troubleshooting&lt;/h3&gt; 
&lt;p&gt;Encountering connection issues? Our &lt;a href="https://docs.openwebui.com/troubleshooting/"&gt;Open WebUI Documentation&lt;/a&gt; has got you covered. For further assistance and to join our vibrant community, visit the &lt;a href="https://discord.gg/5rJgQTnV4s"&gt;Open WebUI Discord&lt;/a&gt;.&lt;/p&gt; 
&lt;h4&gt;Open WebUI: Server Connection Error&lt;/h4&gt; 
&lt;p&gt;If you're experiencing connection issues, it‚Äôs often due to the WebUI docker container not being able to reach the Ollama server at 127.0.0.1:11434 (host.docker.internal:11434) inside the container . Use the &lt;code&gt;--network=host&lt;/code&gt; flag in your docker command to resolve this. Note that the port changes from 3000 to 8080, resulting in the link: &lt;code&gt;http://localhost:8080&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Example Docker Command&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run -d --network=host -v open-webui:/app/backend/data -e OLLAMA_BASE_URL=http://127.0.0.1:11434 --name open-webui --restart always ghcr.io/open-webui/open-webui:main
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Keeping Your Docker Installation Up-to-Date&lt;/h3&gt; 
&lt;p&gt;Check our Updating Guide available in our &lt;a href="https://docs.openwebui.com/getting-started/updating"&gt;Open WebUI Documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Using the Dev Branch üåô&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING] The &lt;code&gt;:dev&lt;/code&gt; branch contains the latest unstable features and changes. Use it at your own risk as it may have bugs or incomplete features.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;If you want to try out the latest bleeding-edge features and are okay with occasional instability, you can use the &lt;code&gt;:dev&lt;/code&gt; tag like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run -d -p 3000:8080 -v open-webui:/app/backend/data --name open-webui --add-host=host.docker.internal:host-gateway --restart always ghcr.io/open-webui/open-webui:dev
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Offline Mode&lt;/h3&gt; 
&lt;p&gt;If you are running Open WebUI in an offline environment, you can set the &lt;code&gt;HF_HUB_OFFLINE&lt;/code&gt; environment variable to &lt;code&gt;1&lt;/code&gt; to prevent attempts to download models from the internet.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export HF_HUB_OFFLINE=1
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;What's Next? üåü&lt;/h2&gt; 
&lt;p&gt;Discover upcoming features on our roadmap in the &lt;a href="https://docs.openwebui.com/roadmap/"&gt;Open WebUI Documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;License üìú&lt;/h2&gt; 
&lt;p&gt;This project contains code under multiple licenses. The current codebase includes components licensed under the Open WebUI License with an additional requirement to preserve the "Open WebUI" branding, as well as prior contributions under their respective original licenses. For a detailed record of license changes and the applicable terms for each section of the code, please refer to &lt;a href="https://raw.githubusercontent.com/open-webui/open-webui/main/LICENSE_HISTORY"&gt;LICENSE_HISTORY&lt;/a&gt;. For complete and updated licensing details, please see the &lt;a href="https://raw.githubusercontent.com/open-webui/open-webui/main/LICENSE"&gt;LICENSE&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/open-webui/open-webui/main/LICENSE_HISTORY"&gt;LICENSE_HISTORY&lt;/a&gt; files.&lt;/p&gt; 
&lt;h2&gt;Support üí¨&lt;/h2&gt; 
&lt;p&gt;If you have any questions, suggestions, or need assistance, please open an issue or join our &lt;a href="https://discord.gg/5rJgQTnV4s"&gt;Open WebUI Discord community&lt;/a&gt; to connect with us! ü§ù&lt;/p&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;a href="https://star-history.com/#open-webui/open-webui&amp;amp;Date"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=open-webui/open-webui&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=open-webui/open-webui&amp;amp;type=Date" /&gt; 
  &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=open-webui/open-webui&amp;amp;type=Date" /&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;hr /&gt; 
&lt;p&gt;Created by &lt;a href="https://github.com/tjbck"&gt;Timothy Jaeryang Baek&lt;/a&gt; - Let's make Open WebUI even more amazing together! üí™&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Z4nzu/hackingtool</title>
      <link>https://github.com/Z4nzu/hackingtool</link>
      <description>&lt;p&gt;ALL IN ONE Hacking Tool For Hackers&lt;/p&gt;&lt;hr&gt;&lt;h3&gt;All in One Hacking tool For Hackersü•á&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://img.shields.io/github/license/Z4nzu/hackingtool" alt="" /&gt; &lt;img src="https://img.shields.io/github/issues/Z4nzu/hackingtool" alt="" /&gt; &lt;img src="https://img.shields.io/github/issues-closed/Z4nzu/hackingtool" alt="" /&gt; &lt;img src="https://img.shields.io/badge/Python-3-blue" alt="" /&gt; &lt;img src="https://img.shields.io/github/forks/Z4nzu/hackingtool" alt="" /&gt; &lt;img src="https://img.shields.io/github/stars/Z4nzu/hackingtool" alt="" /&gt; &lt;img src="https://img.shields.io/github/last-commit/Z4nzu/hackingtool" alt="" /&gt; &lt;a href="http://hits.dwyl.com/Z4nzu/hackingtool"&gt;&lt;img src="http://hits.dwyl.com/Z4nzu/hackingtool.svg?sanitize=true" alt="HitCount" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/badge/platform-Linux%20%7C%20KaliLinux%20%7C%20ParrotOs-blue" alt="" /&gt;&lt;/p&gt; 
&lt;h4&gt;Install Kali Linux in WIndows10 Without VirtualBox &lt;a href="https://youtu.be/BsFhpIDcd9I"&gt;YOUTUBE&lt;/a&gt; or use Docker&lt;/h4&gt; 
&lt;h2&gt;Update Available V1.2.0 üöÄ&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;[‚úî] Installation Bug Fixed&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Added New Tools 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Reverse Engineering&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; RAT Tools&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Web Crawling&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Payload Injector&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Multitor Tools update&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Added Tool in wifijamming&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Added Tool in steganography&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Hackingtool Menu üß∞&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Z4nzu/hackingtool/master/#anonymously-hiding-tools"&gt;Anonymously Hiding Tools&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Z4nzu/hackingtool/master/#information-gathering-tools"&gt;Information gathering tools&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Z4nzu/hackingtool/master/#wordlist-generator"&gt;Wordlist Generator&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Z4nzu/hackingtool/master/#wireless-attack-tools"&gt;Wireless attack tools&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Z4nzu/hackingtool/master/#sql-injection-tools"&gt;SQL Injection Tools&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Z4nzu/hackingtool/master/#phishing-attack-tools"&gt;Phishing attack tools&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Z4nzu/hackingtool/master/#web-attack-tools"&gt;Web Attack tools&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Z4nzu/hackingtool/master/#post-exploitation-tools"&gt;Post exploitation tools&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Z4nzu/hackingtool/master/#forensic-tools"&gt;Forensic tools&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Z4nzu/hackingtool/master/#payload-creation-tools"&gt;Payload creation tools&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Z4nzu/hackingtool/master/#exploit-framework"&gt;Exploit framework&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Z4nzu/hackingtool/master/#reverse-engineering-tools"&gt;Reverse engineering tools&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Z4nzu/hackingtool/master/#ddos-attack-tools"&gt;DDOS Attack Tools&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Z4nzu/hackingtool/master/#remote-administrator-tools--rat-"&gt;Remote Administrator Tools (RAT)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Z4nzu/hackingtool/master/#xss-attack-tools"&gt;XSS Attack Tools&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Z4nzu/hackingtool/master/#steganograhy-tools"&gt;Steganograhy tools&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Z4nzu/hackingtool/master/#other-tools"&gt;Other tools&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Z4nzu/hackingtool/master/#socialmedia-bruteforce"&gt;SocialMedia Bruteforce&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Z4nzu/hackingtool/master/#android-hacking-tools"&gt;Android Hacking tools&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Z4nzu/hackingtool/master/#idn-homograph-attack"&gt;IDN Homograph Attack&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Z4nzu/hackingtool/master/#email-verify-tools"&gt;Email Verify tools&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Z4nzu/hackingtool/master/#hash-cracking-tools"&gt;Hash cracking tools&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Z4nzu/hackingtool/master/#wifi-deauthenticate"&gt;Wifi Deauthenticate&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Z4nzu/hackingtool/master/#socialmedia-finder"&gt;SocialMedia Finder&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Z4nzu/hackingtool/master/#payload-injector"&gt;Payload Injector&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Z4nzu/hackingtool/master/#web-crawling"&gt;Web crawling&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Z4nzu/hackingtool/master/#mix-tools"&gt;Mix tools&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Anonymously Hiding Tools&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Und3rf10w/kali-anonsurf"&gt;Anonmously Surf&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/trimstray/multitor"&gt;Multitor&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Information gathering tools&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/nmap/nmap"&gt;Network Map (nmap)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Screetsec/Dracnmap"&gt;Dracnmap&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Port scanning&lt;/li&gt; 
 &lt;li&gt;Host to IP&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/LionSec/xerosploit"&gt;Xerosploit&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Tuhinshubhra/RED_HAWK"&gt;RED HAWK (All In One Scanning)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/bhavsec/reconspider"&gt;ReconSpider(For All Scanning)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;IsItDown (Check Website Down/Up)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/m4ll0k/Infoga"&gt;Infoga - Email OSINT&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/s0md3v/ReconDog"&gt;ReconDog&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/s0md3v/Striker"&gt;Striker&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/m4ll0k/SecretFinder"&gt;SecretFinder (like API &amp;amp; etc)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/m4ll0k/Shodanfy.py"&gt;Find Info Using Shodan&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/floriankunushevci/rang3r"&gt;Port Scanner - rang3r (Python 2.7)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/joeyagreco/ranger-reloaded"&gt;Port Scanner - Ranger Reloaded (Python 3+)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/s0md3v/Breacher"&gt;Breacher&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Wordlist Generator&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Mebus/cupp.git"&gt;Cupp&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Z4nzu/wlcreator"&gt;WordlistCreator&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/UndeadSec/GoblinWordGenerator.git"&gt;Goblin WordGenerator&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Viralmaniar/SMWYG-Show-Me-What-You-Got"&gt;Password list (1.4 Billion Clear Text Password)&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Wireless attack tools&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/P0cL4bs/wifipumpkin3"&gt;WiFi-Pumpkin&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/wiire/pixiewps"&gt;pixiewps&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/andrewmichaelsmith/bluepot"&gt;Bluetooth Honeypot GUI Framework&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/thehackingsage/Fluxion"&gt;Fluxion&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/wifiphisher/wifiphisher"&gt;Wifiphisher&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/derv82/wifite2"&gt;Wifite&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Z4nzu/fakeap"&gt;EvilTwin&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Z4nzu/fastssh"&gt;Fastssh&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Howmanypeople&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;SQL Injection Tools&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/sqlmapproject/sqlmap"&gt;Sqlmap tool&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/codingo/NoSQLMap"&gt;NoSqlMap&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/stamparm/DSSS"&gt;Damn Small SQLi Scanner&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/dtag-dev-sec/explo"&gt;Explo&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/JohnTroony/Blisqy"&gt;Blisqy - Exploit Time-based blind-SQL injection&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/leviathan-framework/leviathan"&gt;Leviathan - Wide Range Mass Audit Toolkit&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Cvar1984/sqlscan"&gt;SQLScan&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Phishing attack tools&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/trustedsec/social-engineer-toolkit"&gt;Setoolkit&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/UndeadSec/SocialFish"&gt;SocialFish&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/DarkSecDevelopers/HiddenEye"&gt;HiddenEye&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/kgretzky/evilginx2"&gt;Evilginx2&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Viralmaniar/I-See-You"&gt;I-See_You(Get Location using phishing attack)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/hangetzzu/saycheese"&gt;SayCheese (Grab target's Webcam Shots)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/cryptedwolf/ohmyqr"&gt;QR Code Jacking&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/An0nUD4Y/shellphish"&gt;ShellPhish&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/iinc0gnit0/BlackPhish"&gt;BlackPhish&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Web Attack tools&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/santatic/web2attack"&gt;Web2Attack&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Skipfish&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aboul3la/Sublist3r"&gt;SubDomain Finder&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/UndeadSec/checkURL"&gt;CheckURL&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/UltimateHackers/Blazy"&gt;Blazy(Also Find ClickJacking)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/edoardottt/takeover"&gt;Sub-Domain TakeOver&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://gitlab.com/kalilinux/packages/dirb"&gt;Dirb&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Post exploitation tools&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Screetsec/Vegile"&gt;Vegile - Ghost In The Shell&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/UndeadSec/HeraKeylogger"&gt;Chrome Keylogger&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Forensic tools&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Autopsy&lt;/li&gt; 
 &lt;li&gt;Wireshark&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/simsong/bulk_extractor"&gt;Bulk extractor&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://guymager.sourceforge.io/"&gt;Disk Clone and ISO Image Acquire&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.toolsley.com/"&gt;Toolsley&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/volatilityfoundation/volatility3/"&gt;Volatility3&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Payload creation tools&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Screetsec/TheFatRat"&gt;The FatRat&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Screetsec/Brutal"&gt;Brutal&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nathanlopez.github.io/Stitch"&gt;Stitch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/g0tmi1k/msfpc"&gt;MSFvenom Payload Creator&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/r00t-3xp10it/venom"&gt;Venom Shellcode Generator&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/indexnotfound404/spycam"&gt;Spycam&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/kinghacker0/Mob-Droid"&gt;Mob-Droid&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/UndeadSec/Enigma"&gt;Enigma&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Exploit framework&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/threat9/routersploit"&gt;RouterSploit&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/The404Hacking/websploit"&gt;WebSploit&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/commixproject/commix"&gt;Commix&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/santatic/web2attack"&gt;Web2Attack&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Reverse engineering tools&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/androguard/androguard"&gt;Androguard&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/lxdvs/apk2gold"&gt;Apk2Gold&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/skylot/jadx"&gt;JadX&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;DDOS Attack Tools&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;SlowLoris&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/fatihsnsy/aSYNcrone"&gt;Asyncrone | Multifunction SYN Flood DDoS Weapon&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/epsylon/ufonet"&gt;UFOnet&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/jseidl/GoldenEye"&gt;GoldenEye&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Remote Administrator Tools (RAT)&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/nathanlopez/Stitch"&gt;Stitch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/knassar702/pyshell"&gt;Pyshell&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;XSS Attack Tools&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/hahwul/dalfox"&gt;DalFox(Finder of XSS)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/capture0x/XSS-LOADER.git"&gt;XSS Payload Generator&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Damian89/extended-xss-search"&gt;Extended XSS Searcher and Finder&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/PR0PH3CY33/XSS-Freak"&gt;XSS-Freak&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/hahwul/XSpear"&gt;XSpear&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/menkrep1337/XSSCon"&gt;XSSCon&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Ekultek/XanXSS"&gt;XanXSS&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/UltimateHackers/XSStrike"&gt;Advanced XSS Detection Suite&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/iinc0gnit0/RVuln"&gt;RVuln&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/v8blink/Chromium-based-XSS-Taint-Tracking"&gt;Cyclops&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Steganograhy tools&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;SteganoHide&lt;/li&gt; 
 &lt;li&gt;StegnoCracker&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/W1LDN16H7/StegoCracker"&gt;StegoCracker&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/beardog108/snow10"&gt;Whitespace&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Other tools&lt;/h3&gt; 
&lt;h4&gt;SocialMedia Bruteforce&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/chinoogawa/instaBrute"&gt;Instagram Attack&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Matrix07ksa/Brute_Force"&gt;AllinOne SocialMedia Attack&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Matrix07ksa/Brute_Force"&gt;Facebook Attack&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/jakuta-tech/underhanded"&gt;Application Checker&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Android Hacking tools&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/F4dl0/keydroid"&gt;Keydroid&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/papusingh2sms/mysms"&gt;MySMS&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/JasonJerry/lockphish"&gt;Lockphish (Grab target LOCK PIN)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/kinghacker0/WishFish"&gt;DroidCam (Capture Image)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/crypticterminal/EvilApp"&gt;EvilApp (Hijack Session)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/HatBashBR/HatCloud"&gt;HatCloud(Bypass CloudFlare for IP)&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;IDN Homograph Attack&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/UndeadSec/EvilURL"&gt;EvilURL&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Email Verify tools&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/4w4k3/KnockMail"&gt;Knockmail&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Hash cracking tools&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/s0md3v/Hash-Buster"&gt;Hash Buster&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Wifi Deauthenticate&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/MisterBianco/wifijammer-ng"&gt;WifiJammer-NG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/aryanrtm/KawaiiDeauther"&gt;KawaiiDeauther&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;SocialMedia Finder&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Greenwolf/social_mapper"&gt;Find SocialMedia By Facial Recognation System&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/xHak9x/finduser"&gt;Find SocialMedia By UserName&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/sherlock-project/sherlock"&gt;Sherlock&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/iojw/socialscan"&gt;SocialScan | Username or Email&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Payload Injector&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/UndeadSec/Debinject"&gt;Debinject&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/chinarulezzz/pixload"&gt;Pixload&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Web crawling&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/jaeles-project/gospider"&gt;Gospider&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Mix tools&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Terminal Multiplexer&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/GMDSantana/crivo"&gt;Crivo&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://github.com/Z4nzu/hackingtool/raw/master/images/A.png" alt="" /&gt; &lt;img src="https://github.com/Z4nzu/hackingtool/raw/master/images/AA.png" alt="" /&gt; &lt;img src="https://github.com/Z4nzu/hackingtool/raw/master/images/AAA.png" alt="" /&gt; &lt;img src="https://github.com/Z4nzu/hackingtool/raw/master/images/AAAA.png" alt="" /&gt; &lt;img src="https://github.com/Z4nzu/hackingtool/raw/master/images/AAAAA.png" alt="" /&gt;&lt;/p&gt; 
&lt;h2&gt;Installation For Linux &lt;img src="https://konpa.github.io/devicon/devicon.git/icons/linux/linux-original.svg?sanitize=true" alt="linux" width="25" height="25" /&gt;&lt;p&gt;&lt;/p&gt;&lt;p align="center"&gt;&lt;/p&gt;&lt;/h2&gt; 
&lt;h3&gt;!! RUN HACKINGTOOL AS ROOT !!&lt;/h3&gt; 
&lt;h2&gt;Steps are given below :&lt;/h2&gt; 
&lt;h2&gt;Step : 1 Download hackingtool&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;git clone https://github.com/Z4nzu/hackingtool.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Step : 2 Give Permission to hackingtool&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;chmod -R 755 hackingtool  
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Step : 3 Move to hackingtool directory&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;cd hackingtool
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Step : 4 Run hackingtool&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;sudo python install.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Step : 5 For installing tools in directory&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;sudo hackingtool
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Use image with Docker&lt;/h2&gt; 
&lt;h3&gt;Create Docker Image&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Create the docker image&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker buitl -t vgpastor/hackingtool .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Run as container&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker-compose up -d
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Interact with terminal&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Get into the container&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker exec -it hackingtool bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;OUTPUT:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;Select Best Option : 

              [1] Kali Linux / Parrot-Os (apt)
              [2] Arch Linux (pacman)
              [0] Exit 
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Enter the options and continue.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;If need open other ports you can edit the docker-compose.yml file&lt;/li&gt; 
 &lt;li&gt;Volumes are mounted in the container to persist data and can share files between the host and the container&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Thanks to original Author of the tools used in hackingtool&lt;/h4&gt; 
&lt;img src="https://img.shields.io/badge/Important-notice-red" /&gt; 
&lt;h4&gt;Please Don't Use for illegal Activity&lt;/h4&gt; 
&lt;h3&gt;To do&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Release Tool&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Add Tools for CTF&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Want to do automatic&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Social Media &lt;span&gt;üì≠&lt;/span&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://twitter.com/_Zinzu07"&gt;&lt;img src="https://img.shields.io/twitter/url?color=%231DA1F2&amp;amp;label=follow&amp;amp;logo=twitter&amp;amp;logoColor=%231DA1F2&amp;amp;style=flat-square&amp;amp;url=https%3A%2F%2Fwww.reddit.com%2Fuser%2FFatChicken277" alt="Twitter" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Z4nzu/"&gt;&lt;img src="https://img.shields.io/badge/-GitHub-181717?style=flat-square&amp;amp;logo=github&amp;amp;link=https://github.com/Z4nzu/" alt="GitHub" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h5&gt;Your Favourite Tool is not in hackingtool or Suggestions Please &lt;a href="https://forms.gle/b235JoCKyUq5iM3t8"&gt;CLICK HERE&lt;/a&gt;&lt;/h5&gt; 
&lt;p&gt;&lt;img src="https://github-readme-stats.vercel.app/api?username=Z4nzu&amp;amp;show_icons=true&amp;amp;title_color=fff&amp;amp;icon_color=79ff97&amp;amp;text_color=9f9f9f&amp;amp;bg_color=151515" alt="Z4nzu's github stats" /&gt;&lt;/p&gt; 
&lt;h4&gt;Don't Forgot to share with Your Friends&lt;/h4&gt; 
&lt;h3&gt;The new Update get will soon stay updated&lt;/h3&gt; 
&lt;h4&gt;Thank you..!!&lt;/h4&gt;</description>
    </item>
    
    <item>
      <title>gyoridavid/ai_agents_az</title>
      <link>https://github.com/gyoridavid/ai_agents_az</link>
      <description>&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AI Agents A-Z&lt;/h1&gt; 
&lt;p&gt;In this repo, you can find the n8n templates we created for the episodes of &lt;a href="https://www.youtube.com/channel/UCloXqLhp_KGhHBe1kwaL2Tg"&gt;AI Agents A-Z&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Season 1&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_1"&gt;Episode 1: Creating a prescription agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_2"&gt;Episode 2: Making a daily digest agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_3"&gt;Episode 3: Making LinkedIn posts using Human in the Loop approval process&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_4"&gt;Episode 4: Deep Research Agent using Google&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_5"&gt;Episode 5: Creating a blog writing system using deep research&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_6"&gt;Episode 6: Lead generation with X-Ray search and LinkedIn&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_7"&gt;Episode 7: Creating Youtube short videos using our custom MCP server&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_8"&gt;Episode 8: Creating an AI influencer on Instagram using n8n&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_9"&gt;Episode 9: Create revenge story videos for YouTube&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_10"&gt;Episode 10: n8n best practices&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_11"&gt;Episode 11: Create short (motivational) stories for YouTube and TikTok&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_12"&gt;Episode 12: Scheduling social media posts with Postiz and n8n&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_13"&gt;Episode 13: Create AI videos with MiniMax Hailuo 2 and n8n&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_14"&gt;Episode 14: Create AI videos with Seedance and n8n&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_15"&gt;Episode 15: Generate AI startup ideas from Reddit&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_16"&gt;Episode 16: Create AI poem videos with n8n for TikTok&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_17"&gt;Episode 17: Create Shopify product videos with Seedance, ElevenLabs, Latentsync, Flux Kontext and n8n&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_18"&gt;Episode 18: Scary story TikTok videos workflow&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_19"&gt;Episode 19: Run FLUX.1 Kontext [dev] with modal.com&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_20"&gt;Episode 20: Use Wan 2.2, ComfyUI and n8n to generate videos for free&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_21"&gt;Episode 21: 10 EASY faceless niches that pay well - monetize in a MONTH (2025)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_22"&gt;Episode 22: Sleep long-form videos with GPT-5, ElevenMusic, Imagen4, Seendance and n8n&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_23"&gt;Episode 23: UGC videos with nanobanana and n8n&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_24"&gt;Episode 24: generate images with Qwen Image, Flux.1 [dev] and Flux.1 Schnell with modal.com and Cloudflare Workers AI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_25"&gt;Episode 25: Fal.ai n8n subworkflows for Qwen Image Edit Plus and Wan 2.2 animate&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_31"&gt;Episode 31: Veo 3.1 is now in n8n - how to use it for FREE&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_35"&gt;Episode 35: Instagram influencer machine&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_36"&gt;Episode 36: Viral bodycam footage creator with Sora 2&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_38"&gt;Episode 38: Create AI reaction videos with Veo 3.1 and n8n&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_39"&gt;Episode 39: Create infographics with Nano Banana Pro in n8n&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_40"&gt;Episode 40: Flux.2[dev] with n8n&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_41"&gt;Episode 41: FREE z-image-turbo with n8n&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/gyoridavid/ai_agents_az/main/episode_42"&gt;Episode 42: 100% FREE explainer videos with n8n and Z-Image&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;servers&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://hub.docker.com/r/gyoridavid/ai-agents-no-code-tools"&gt;AI Agents No-Code Tools&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/gyoridavid/short-video-maker"&gt;Short video maker MCP/REST server&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://hub.docker.com/r/gyoridavid/narrated-story-creator"&gt;Narrated story creator REST/MCP server&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>GetStream/Vision-Agents</title>
      <link>https://github.com/GetStream/Vision-Agents</link>
      <description>&lt;p&gt;Open Vision Agents by Stream. Build Vision Agents quickly with any model or video provider. Uses Stream's edge network for ultra-low latency.&lt;/p&gt;&lt;hr&gt;&lt;img width="1280" height="360" alt="Readme" src="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/assets/repo_image.png" /&gt; 
&lt;h1&gt;Open Vision Agents by Stream&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/GetStream/Vision-Agents/actions"&gt;&lt;img src="https://github.com/GetStream/Vision-Agents/actions/workflows/ci.yml/badge.svg?sanitize=true" alt="build" /&gt;&lt;/a&gt; &lt;a href="http://badge.fury.io/py/vision-agents"&gt;&lt;img src="https://badge.fury.io/py/vision-agents.svg?sanitize=true" alt="PyPI version" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/pypi/pyversions/vision-agents.svg?sanitize=true" alt="PyPI - Python Version" /&gt; &lt;a href="https://github.com/GetStream/Vision-Agents/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/GetStream/Vision-Agents" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/RkhX9PxMS6"&gt;&lt;img src="https://img.shields.io/discord/1108586339550638090" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Build Real-Time Vision AI Agents&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/d9778ab9-938d-4101-8605-ff879c29b0e4"&gt;https://github.com/user-attachments/assets/d9778ab9-938d-4101-8605-ff879c29b0e4&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Multi-modal AI agents that watch, listen, and understand video.&lt;/h3&gt; 
&lt;p&gt;Vision Agents give you the building blocks to create intelligent, low-latency video experiences powered by your models, your infrastructure, and your use cases.&lt;/p&gt; 
&lt;h3&gt;Key Highlights&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Video AI:&lt;/strong&gt; Built for real-time video AI. Combine YOLO, Roboflow, and others with Gemini/OpenAI in real-time.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Low Latency:&lt;/strong&gt; Join quickly (500ms) and maintain audio/video latency under 30ms using &lt;a href="https://getstream.io/video/"&gt;Stream's edge network&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Open:&lt;/strong&gt; Built by Stream, but works with any video edge network.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Native APIs:&lt;/strong&gt; Native SDK methods from OpenAI (&lt;code&gt;create response&lt;/code&gt;), Gemini (&lt;code&gt;generate&lt;/code&gt;), and Claude ( &lt;code&gt;create message&lt;/code&gt;) ‚Äî always access the latest LLM capabilities.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;SDKs:&lt;/strong&gt; SDKs for React, Android, iOS, Flutter, React Native, and Unity, powered by Stream's ultra-low-latency network.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/d66587ea-7af4-40c4-9966-5c04fbcf467c"&gt;https://github.com/user-attachments/assets/d66587ea-7af4-40c4-9966-5c04fbcf467c&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;See It In Action&lt;/h2&gt; 
&lt;h3&gt;Sports Coaching&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/d1258ac2-ca98-4019-80e4-41ec5530117e"&gt;https://github.com/user-attachments/assets/d1258ac2-ca98-4019-80e4-41ec5530117e&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;This example shows you how to build golf coaching AI with YOLO and Gemini Live. Combining a fast object detection model (like YOLO) with a full realtime AI is useful for many different video AI use cases. For example: Drone fire detection, sports/video game coaching, physical therapy, workout coaching, just dance style games etc.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# partial example, full example: examples/02_golf_coach_example/golf_coach_example.py
agent = Agent(
    edge=getstream.Edge(),
    agent_user=agent_user,
    instructions="Read @golf_coach.md",
    llm=gemini.Realtime(fps=10),
    # llm=openai.Realtime(fps=1), # Careful with FPS can get expensive
    processors=[ultralytics.YOLOPoseProcessor(model_path="yolo11n-pose.pt", device="cuda")],
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Security Camera with Package Theft Detection&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/92a2cdd8-909c-46d8-aab7-039a90efc186"&gt;https://github.com/user-attachments/assets/92a2cdd8-909c-46d8-aab7-039a90efc186&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;This example shows a security camera system that detects faces, tracks packages and detects when a package is stolen. It automatically generates "WANTED" posters, posting them to X in real-time.&lt;/p&gt; 
&lt;p&gt;It combines face recognition, YOLOv11 object detection, Nano Banana and Gemini for a complete security workflow with voice interaction.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# partial example, full example: examples/04_security_camera_example/security_camera_example.py
security_processor = SecurityCameraProcessor(
    fps=5,
    model_path="weights_custom.pt",  # YOLOv11 for package detection
    package_conf_threshold=0.7,
)

agent = Agent(
    edge=getstream.Edge(),
    agent_user=User(name="Security AI", id="agent"),
    instructions="Read @instructions.md",
    processors=[security_processor],
    llm=gemini.LLM("gemini-2.5-flash-lite"),
    tts=elevenlabs.TTS(),
    stt=deepgram.STT(),
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Cluely style Invisible Assistant (coming soon)&lt;/h3&gt; 
&lt;p&gt;Apps like Cluely offer realtime coaching via an invisible overlay. This example shows you how you can build your own invisible assistant. It combines Gemini realtime (to watch your screen and audio), and doesn't broadcast audio (only text). This approach is quite versatile and can be used for: Sales coaching, job interview cheating, physical world/ on the job coaching with glasses&lt;/p&gt; 
&lt;p&gt;Demo video&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;agent = Agent(
    edge=StreamEdge(),  # low latency edge. clients for React, iOS, Android, RN, Flutter etc.
    agent_user=agent_user,  # the user object for the agent (name, image etc)
    instructions="You are silently helping the user pass this interview. See @interview_coach.md",
    # gemini realtime, no need to set tts, or sst (though that's also supported)
    llm=gemini.Realtime()
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Step 1: Install via uv&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;uv add vision-agents&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Step 2: (Optional) Install with extra integrations&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;uv add "vision-agents[getstream, openai, elevenlabs, deepgram]"&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Step 3: Obtain your Stream API credentials&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Get a free API key from &lt;a href="https://getstream.io/"&gt;Stream&lt;/a&gt;. Developers receive &lt;strong&gt;333,000 participant minutes&lt;/strong&gt; per month, plus extra credits via the Maker Program.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;strong&gt;Feature&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;True real-time via WebRTC&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Stream directly to model providers that support it for instant visual understanding.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Interval/processor pipeline&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;For providers without WebRTC, process frames with pluggable video processors (e.g., YOLO, Roboflow, or custom PyTorch/ONNX) before/after model calls.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Turn detection &amp;amp; diarization&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Keep conversations natural; know when the agent should speak or stay quiet and who's talking.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Voice activity detection (VAD)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Trigger actions intelligently and use resources efficiently.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Speech‚ÜîText‚ÜîSpeech&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Enable low-latency loops for smooth, conversational voice UX.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Tool/function calling&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Execute arbitrary code and APIs mid-conversation. Create Linear issues, query weather, trigger telephony, or hit internal services.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Built-in memory via Stream Chat&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Agents recall context naturally across turns and sessions.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Text back-channel&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Message the agent silently during a call.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Phone and RAG&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Interact with the Agent via inbound or outbound phone calls using Twilio and Turbopuffer&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Out-of-the-Box Integrations&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;strong&gt;Plugin Name&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Docs Link&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;AWS Bedrock&lt;/td&gt; 
   &lt;td&gt;Realtime speech-to-speech plugin using Amazon Nova models with automatic reconnection&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/aws-bedrock"&gt;AWS&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;AWS Polly&lt;/td&gt; 
   &lt;td&gt;TTS plugin using Amazon's cloud-based service with natural-sounding voices and neural engine support&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/aws-polly"&gt;AWS Polly&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Cartesia&lt;/td&gt; 
   &lt;td&gt;TTS plugin for realistic voice synthesis in real-time voice applications&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/cartesia"&gt;Cartesia&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Decart&lt;/td&gt; 
   &lt;td&gt;Real-time AI video transformation service for applying artistic styles and effects to video streams&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/decart"&gt;Decart&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Deepgram&lt;/td&gt; 
   &lt;td&gt;STT plugin for fast, accurate real-time transcription with speaker diarization&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/deepgram"&gt;Deepgram&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ElevenLabs&lt;/td&gt; 
   &lt;td&gt;TTS plugin with highly realistic and expressive voices for conversational agents&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/elevenlabs"&gt;ElevenLabs&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Fast-Whisper&lt;/td&gt; 
   &lt;td&gt;High-performance STT plugin using OpenAI's Whisper model with CTranslate2 for fast inference&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/fast-whisper"&gt;Fast-Whisper&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Fish Audio&lt;/td&gt; 
   &lt;td&gt;STT and TTS plugin with automatic language detection and voice cloning capabilities&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/fish"&gt;Fish Audio&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Gemini&lt;/td&gt; 
   &lt;td&gt;Realtime API for building conversational agents with support for both voice and video&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/gemini"&gt;Gemini&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;HeyGen&lt;/td&gt; 
   &lt;td&gt;Realtime interactive avatars powered by &lt;a href="https://heygen.com/"&gt;HeyGen&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/heygen"&gt;HeyGen&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Hugging Face&lt;/td&gt; 
   &lt;td&gt;LLM plugin providing access to many open-source language models hosted on the Hugging Face Hub and powered by external providers (Cerebras, Together, Groq, etc.)&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/huggingface"&gt;Hugging Face&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Inworld&lt;/td&gt; 
   &lt;td&gt;TTS plugin with high-quality streaming voices for real-time conversational AI agents&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/inworld"&gt;Inworld&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Kokoro&lt;/td&gt; 
   &lt;td&gt;Local TTS engine for offline voice synthesis with low latency&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/kokoro"&gt;Kokoro&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Moondream&lt;/td&gt; 
   &lt;td&gt;Moondream provides realtime detection and VLM capabilities. Developers can choose from using the hosted API or running locally on their CUDA devices. Vision Agents supports Moondream's Detect, Caption and VQA skills out-of-the-box.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/moondream"&gt;Moondream&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;NVIDIA Cosmos 2&lt;/td&gt; 
   &lt;td&gt;VLM plugin using NVIDIA's Cosmos 2 models for video understanding with automatic frame buffering and streaming responses&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/nvidia"&gt;NVIDIA&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenAI&lt;/td&gt; 
   &lt;td&gt;Realtime API for building conversational agents with out of the box support for real-time video directly over WebRTC, LLMs and Open AI TTS&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/openai"&gt;OpenAI&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenRouter&lt;/td&gt; 
   &lt;td&gt;LLM plugin providing access to multiple providers (Anthropic, Google, OpenAI) through a unified API&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/openrouter"&gt;OpenRouter&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Qwen&lt;/td&gt; 
   &lt;td&gt;Realtime audio plugin using Alibaba's Qwen3 with native audio output and built-in speech recognition&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/qwen"&gt;Qwen&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Roboflow&lt;/td&gt; 
   &lt;td&gt;Object detection processor using Roboflow's hosted API or local RF-DETR models&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/roboflow"&gt;Roboflow&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Smart Turn&lt;/td&gt; 
   &lt;td&gt;Advanced turn detection system combining Silero VAD, Whisper, and neural models for natural conversation flow&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/smart-turn"&gt;Smart Turn&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;TurboPuffer&lt;/td&gt; 
   &lt;td&gt;RAG plugin using TurboPuffer for hybrid search (vector + BM25) with Gemini embeddings for retrieval augmented generation&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/guides/rag"&gt;TurboPuffer&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Twilio&lt;/td&gt; 
   &lt;td&gt;Voice call integration plugin enabling bidirectional audio streaming via Twilio Media Streams with call registry and audio conversion&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/GetStream/Vision-Agents/tree/main/examples/03_phone_and_rag_example"&gt;Twilio&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ultralytics&lt;/td&gt; 
   &lt;td&gt;Real-time pose detection processor using YOLO models with skeleton overlays&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/ultralytics"&gt;Ultralytics&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Vogent&lt;/td&gt; 
   &lt;td&gt;Neural turn detection system for intelligent turn-taking in voice conversations&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/vogent"&gt;Vogent&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Wizper&lt;/td&gt; 
   &lt;td&gt;STT plugin with real-time translation capabilities powered by Whisper v3&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/wizper"&gt;Wizper&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;xAI&lt;/td&gt; 
   &lt;td&gt;LLM plugin using xAI's Grok models with advanced reasoning and real-time knowledge&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://visionagents.ai/integrations/xai"&gt;xAI&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Processors&lt;/h2&gt; 
&lt;p&gt;Processors let your agent &lt;strong&gt;manage state&lt;/strong&gt; and &lt;strong&gt;handle audio/video&lt;/strong&gt; in real-time.&lt;/p&gt; 
&lt;p&gt;They take care of the hard stuff, like:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Running smaller models&lt;/li&gt; 
 &lt;li&gt;Making API calls&lt;/li&gt; 
 &lt;li&gt;Transforming media&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;‚Ä¶ so you can focus on your agent logic.&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;Check out our getting started guide at &lt;a href="https://visionagents.ai/"&gt;VisionAgents.ai&lt;/a&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Quickstart:&lt;/strong&gt; &lt;a href="https://visionagents.ai/introduction/voice-agents"&gt;Building a Voice AI app&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Quickstart:&lt;/strong&gt; &lt;a href="https://visionagents.ai/introduction/video-agents"&gt;Building a Video AI app&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Tutorial:&lt;/strong&gt; &lt;a href="https://github.com/GetStream/Vision-Agents/tree/main/examples/01_simple_agent_example"&gt;Building a real-time meeting assistant&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Tutorial:&lt;/strong&gt; &lt;a href="https://github.com/GetStream/Vision-Agents/tree/main/examples/02_golf_coach_example"&gt;Building real-time sports coaching&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;üîÆ Demo Applications&lt;/th&gt; 
   &lt;th&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;br /&gt;&lt;h3&gt;Cartesia&lt;/h3&gt;Using Cartesia's Sonic 3 model to visually look at what's in the frame and tell a story with emotion.&lt;br /&gt;&lt;br /&gt;‚Ä¢ Real-time visual understanding&lt;br /&gt;‚Ä¢ Emotional storytelling&lt;br /&gt;‚Ä¢ Frame-by-frame analysis&lt;br /&gt;&lt;br /&gt; &lt;a href="https://github.com/GetStream/Vision-Agents/tree/main/plugins/cartesia/example"&gt;&amp;gt;Source Code and tutorial&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/assets/demo_gifs/cartesia.gif" width="320" alt="Cartesia Demo" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;br /&gt;&lt;h3&gt;Realtime Stable Diffusion&lt;/h3&gt;Realtime stable diffusion using Vision Agents and Decart's Mirage 2 model to create interactive scenes and stories.&lt;br /&gt;&lt;br /&gt;‚Ä¢ Real-time video restyling&lt;br /&gt;‚Ä¢ Interactive scene generation&lt;br /&gt;‚Ä¢ Stable diffusion integration&lt;br /&gt;&lt;br /&gt; &lt;a href="https://github.com/GetStream/Vision-Agents/tree/main/plugins/decart/example"&gt;&amp;gt;Source Code and tutorial&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/assets/demo_gifs/mirage.gif" width="320" alt="Mirage Demo" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;br /&gt;&lt;h3&gt;Golf Coach&lt;/h3&gt;Using Gemini Live together with Vision Agents and Ultralytics YOLO, we're able to track the user's pose and provide realtime actionable feedback on their golf game.&lt;br /&gt;&lt;br /&gt;‚Ä¢ Real-time pose tracking&lt;br /&gt;‚Ä¢ Actionable coaching feedback&lt;br /&gt;‚Ä¢ YOLO pose detection&lt;br /&gt;‚Ä¢ Gemini Live integration&lt;br /&gt;&lt;br /&gt; &lt;a href="https://github.com/GetStream/Vision-Agents/tree/main/examples/02_golf_coach_example"&gt;&amp;gt;Source Code and tutorial&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/assets/demo_gifs/golf.gif" width="320" alt="Golf Coach Demo" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;br /&gt;&lt;h3&gt;GeoGuesser&lt;/h3&gt;Together with OpenAI Realtime and Vision Agents, we can take GeoGuesser to the next level by asking it to identify places in our real world surroundings.&lt;br /&gt;&lt;br /&gt;‚Ä¢ Real-world location identification&lt;br /&gt;‚Ä¢ OpenAI Realtime integration&lt;br /&gt;‚Ä¢ Visual scene understanding&lt;br /&gt;&lt;br /&gt; &lt;a href="https://visionagents.ai/integrations/openai#openai-realtime"&gt;&amp;gt;Source Code and tutorial&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/assets/demo_gifs/geoguesser.gif" width="320" alt="GeoGuesser Demo" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;br /&gt;&lt;h3&gt;Phone and RAG&lt;/h3&gt;Interact with your Agent over the phone using Twilio. This example demonstrates how to use TurboPuffer for Retrieval Augmented Generation (RAG) to give your agent specialized knowledge.&lt;br /&gt;&lt;br /&gt;‚Ä¢ Inbound/Outbound telephony&lt;br /&gt;‚Ä¢ Twilio Media Streams integration&lt;br /&gt;‚Ä¢ Vector search with TurboPuffer&lt;br /&gt;‚Ä¢ Retrieval Augmented Generation&lt;br /&gt;&lt;br /&gt; &lt;a href="https://github.com/GetStream/Vision-Agents/tree/main/examples/03_phone_and_rag_example"&gt;&amp;gt;Source Code and tutorial&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/assets/demo_gifs/va_phone.png" width="320" alt="Phone and RAG Demo" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;br /&gt;&lt;h3&gt;Security Camera&lt;/h3&gt;A security camera with face recognition, package detection and automated theft response. Generates WANTED posters with Nano Banana and posts them to X when packages disappear.&lt;br /&gt;&lt;br /&gt;‚Ä¢ Face detection &amp;amp; named recognition&lt;br /&gt;‚Ä¢ YOLOv11 package detection&lt;br /&gt;‚Ä¢ Automated WANTED poster generation&lt;br /&gt;‚Ä¢ Real-time X posting&lt;br /&gt;&lt;br /&gt; &lt;a href="https://github.com/GetStream/Vision-Agents/tree/main/examples/04_security_camera_example"&gt;&amp;gt;Source Code and tutorial&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/assets/demo_gifs/security_camera.gif" width="320" alt="Security Camera Demo" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Development&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/DEVELOPMENT.md"&gt;DEVELOPMENT.md&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Open Platform&lt;/h2&gt; 
&lt;p&gt;Want to add your platform or provider? Reach out to &lt;strong&gt;&lt;a href="mailto:nash@getstream.io"&gt;nash@getstream.io&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; 
&lt;h2&gt;Awesome Video AI&lt;/h2&gt; 
&lt;p&gt;Our favorite people &amp;amp; projects to follow for vision AI&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;&lt;a href="https://x.com/demishassabis"&gt;&lt;img src="https://github.com/user-attachments/assets/9149e871-cfe8-4169-a4ce-4073417e645c" width="80" /&gt;&lt;/a&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;a href="https://x.com/OfficialLoganK"&gt;&lt;img src="https://github.com/user-attachments/assets/2e1335d3-58af-4988-b879-1db8d862cd34" width="80" /&gt;&lt;/a&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;a href="https://x.com/ultralytics"&gt;&lt;img src="https://github.com/user-attachments/assets/c9249ae9-e66a-4a70-9393-f6fe4ab5c0b0" width="80" /&gt;&lt;/a&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://x.com/demishassabis"&gt;@demishassabis&lt;/a&gt;&lt;br /&gt;CEO @ Google DeepMind&lt;br /&gt;&lt;sub&gt;Won a Nobel prize&lt;/sub&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://x.com/OfficialLoganK"&gt;@OfficialLoganK&lt;/a&gt;&lt;br /&gt;Product Lead @ Gemini&lt;br /&gt;&lt;sub&gt;Posts about robotics vision&lt;/sub&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://x.com/ultralytics"&gt;@ultralytics&lt;/a&gt;&lt;br /&gt;Various fast vision AI models&lt;br /&gt;&lt;sub&gt;Pose, detect, segment, classify&lt;/sub&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;&lt;a href="https://x.com/skalskip92"&gt;&lt;img src="https://github.com/user-attachments/assets/c1fe873d-6f41-4155-9be1-afc287ca9ac7" width="80" /&gt;&lt;/a&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;a href="https://x.com/moondreamai"&gt;&lt;img src="https://github.com/user-attachments/assets/43359165-c23d-4d5d-a5a6-1de58d71fabd" width="80" /&gt;&lt;/a&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;a href="https://x.com/kwindla"&gt;&lt;img src="https://github.com/user-attachments/assets/490d349c-7152-4dfb-b705-04e57bb0a4ca" width="80" /&gt;&lt;/a&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://x.com/skalskip92"&gt;@skalskip92&lt;/a&gt;&lt;br /&gt;Open Source Lead @ Roboflow&lt;br /&gt;&lt;sub&gt;Building tools for vision AI&lt;/sub&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://x.com/moondreamai"&gt;@moondreamai&lt;/a&gt;&lt;br /&gt;The tiny vision model that could&lt;br /&gt;&lt;sub&gt;Lightweight, fast, efficient&lt;/sub&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://x.com/kwindla"&gt;@kwindla&lt;/a&gt;&lt;br /&gt;Pipecat / Daily&lt;br /&gt;&lt;sub&gt;Sharing AI and vision insights&lt;/sub&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;&lt;a href="https://x.com/juberti"&gt;&lt;img src="https://github.com/user-attachments/assets/d7ade584-781f-4dac-95b8-1acc6db4a7c4" width="80" /&gt;&lt;/a&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;a href="https://x.com/romainhuet"&gt;&lt;img src="https://github.com/user-attachments/assets/00a1ed37-3620-426d-b47d-07dd59c19b28" width="80" /&gt;&lt;/a&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;a href="https://x.com/thorwebdev"&gt;&lt;img src="https://github.com/user-attachments/assets/eb5928c7-83b9-4aaa-854f-1d4f641426f2" width="80" /&gt;&lt;/a&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://x.com/juberti"&gt;@juberti&lt;/a&gt;&lt;br /&gt;Head of Realtime AI @ OpenAI&lt;br /&gt;&lt;sub&gt;Realtime AI systems&lt;/sub&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://x.com/romainhuet"&gt;@romainhuet&lt;/a&gt;&lt;br /&gt;Head of DX @ OpenAI&lt;br /&gt;&lt;sub&gt;Developer tooling &amp;amp; APIs&lt;/sub&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://x.com/thorwebdev"&gt;@thorwebdev&lt;/a&gt;&lt;br /&gt;Eleven Labs&lt;br /&gt;&lt;sub&gt;Voice and AI experiments&lt;/sub&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;&lt;a href="https://x.com/mervenoyann"&gt;&lt;img src="https://github.com/user-attachments/assets/ab5ef918-7c97-4c6d-be10-2e2aeefec015" width="80" /&gt;&lt;/a&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;a href="https://x.com/stash_pomichter"&gt;&lt;img src="https://github.com/user-attachments/assets/af936e13-22cf-4000-a35b-bfe30d44c320" width="80" /&gt;&lt;/a&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;a href="https://x.com/Mentraglass"&gt;&lt;img src="https://pbs.twimg.com/profile_images/1893061651152121856/Op4W8mza_400x400.jpg" width="80" /&gt;&lt;/a&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://x.com/mervenoyann"&gt;@mervenoyann&lt;/a&gt;&lt;br /&gt;Hugging Face&lt;br /&gt;&lt;sub&gt;Posts extensively about Video AI&lt;/sub&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://x.com/stash_pomichter"&gt;@stash_pomichter&lt;/a&gt;&lt;br /&gt;Spatial memory for robots&lt;br /&gt;&lt;sub&gt;Robotics &amp;amp; AI navigation&lt;/sub&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://x.com/Mentraglass"&gt;@Mentraglass&lt;/a&gt;&lt;br /&gt;Open-source smart glasses&lt;br /&gt;&lt;sub&gt;Open-Source, hackable AR glasses with AI capabilities built in&lt;/sub&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;&lt;a href="https://x.com/vikhyatk"&gt;&lt;img src="https://pbs.twimg.com/profile_images/1664559115581145088/UMD1vtMw_400x400.jpg" width="80" /&gt;&lt;/a&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://x.com/vikhyatk"&gt;@vikhyatk&lt;/a&gt;&lt;br /&gt;AI Engineer&lt;br /&gt;&lt;sub&gt;Open-source AI projects, Creator of Moondream AI&lt;/sub&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Inspiration&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Livekit Agents: Great syntax, Livekit only&lt;/li&gt; 
 &lt;li&gt;Pipecat: Flexible, but more verbose.&lt;/li&gt; 
 &lt;li&gt;OpenAI Agents: Focused on openAI only&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Roadmap&lt;/h2&gt; 
&lt;h3&gt;0.1 ‚Äì First Release - Oct&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Working TTS, Gemini &amp;amp; OpenAI&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;0.2 - Simplification - Nov&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Simplified the library &amp;amp; improved code quality&lt;/li&gt; 
 &lt;li&gt;Deepgram Nova 3, Elevenlabs Scribe 2, Fish, Moondream, QWen3, Smart turn, Vogent, Inworld, Heygen, AWS and more&lt;/li&gt; 
 &lt;li&gt;Improved openAI &amp;amp; Gemini realtime performance&lt;/li&gt; 
 &lt;li&gt;Audio &amp;amp; Video utilities&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;0.3 - Examples and Deploys - Jan&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Production-grade HTTP API for agent deployment (&lt;code&gt;uv run &amp;lt;agent.py&amp;gt; serve&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Metrics &amp;amp; Observability stack&lt;/li&gt; 
 &lt;li&gt;Phone/voice integration with RAG capabilities&lt;/li&gt; 
 &lt;li&gt;10 new LLM plugins (&lt;a href="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/plugins/aws"&gt;AWS Nova 2&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/plugins/qwen"&gt;Qwen 3 Realtime&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/plugins/nvidia"&gt;NVIDIA Cosmos 2&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/plugins/pocket"&gt;Pocket TTS&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/plugins/deepgram"&gt;Deepgram TTS&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/plugins/openrouter"&gt;OpenRouter&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/plugins/huggingface"&gt;HuggingFace Inference&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/plugins/roboflow"&gt;Roboflow&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/plugins/twilio"&gt;Twilio&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/plugins/turbopuffer"&gt;Turbopuffer&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;Real-world examples (&lt;a href="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/examples/05_security_camera_example"&gt;security camera&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/examples/03_phone_and_rag_example"&gt;phone integration&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/examples/04_football_commentator_example"&gt;football commentator&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/examples/07_deploy_example"&gt;Docker deployment with GPU support&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/GetStream/Vision-Agents/main/examples/08_agent_server_example"&gt;agent server&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;Stability: Fixes for participant sync, video frame handling, agent lifecycle, and screen sharing&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;0.4 Documentation/polish&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Excellence on documentation/polish&lt;/li&gt; 
 &lt;li&gt;Better Roboflow annotation docs&lt;/li&gt; 
 &lt;li&gt;Automated workflows for maintenance&lt;/li&gt; 
 &lt;li&gt;Local camera/audio support AND/OR WebRTC connection&lt;/li&gt; 
 &lt;li&gt;Embedded/robotics examples&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Vision AI limitations&lt;/h2&gt; 
&lt;p&gt;Video AI is the frontier of AI. The state of the art is changing daily to help models understand live video. While building the integrations, here are the limitations we've noticed (Dec 2025)&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Video AI struggles with small text. If you want the AI to read the score in a game it will often get it wrong and hallucinate&lt;/li&gt; 
 &lt;li&gt;Longer videos can cause the AI to lose context. For instance if it's watching a soccer match it will get confused after 30 seconds&lt;/li&gt; 
 &lt;li&gt;Most applications require a combination of small specialized models like Yolo/Roboflow/Moondream, API calls to get more context and larger models like gemini/openAI&lt;/li&gt; 
 &lt;li&gt;Image size &amp;amp; FPS need to stay relatively low due to performance constraints&lt;/li&gt; 
 &lt;li&gt;Video doesn‚Äôt trigger responses in realtime models. You always need to send audio/text to trigger a response.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;We are hiring&lt;/h2&gt; 
&lt;p&gt;Join the team behind this project - we‚Äôre hiring a Staff Python Engineer to architect, build, and maintain a powerful toolkit for developers integrating voice and video AI into their products.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://jobs.ashbyhq.com/stream/3bea7dba-54e1-4c71-aa02-712a075842df?utm_source=Jmv9QOkznl"&gt;Apply here&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.star-history.com/#GetStream/vision-agents&amp;amp;type=timeline&amp;amp;legend=top-left"&gt;&lt;img src="https://api.star-history.com/svg?repos=GetStream/vision-agents&amp;amp;type=timeline&amp;amp;legend=top-left" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>frappe/erpnext</title>
      <link>https://github.com/frappe/erpnext</link>
      <description>&lt;p&gt;Free and Open Source Enterprise Resource Planning (ERP)&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;a href="https://frappe.io/erpnext"&gt; &lt;img src="https://raw.githubusercontent.com/frappe/erpnext/develop/erpnext/public/images/v16/erpnext.svg?sanitize=true" alt="ERPNext Logo" height="80px" width="80xp" /&gt; &lt;/a&gt; 
 &lt;h2&gt;ERPNext&lt;/h2&gt; 
 &lt;p align="center"&gt; &lt;/p&gt;
 &lt;p&gt;Powerful, Intuitive and Open-Source ERP&lt;/p&gt; 
 &lt;p&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://frappe.school"&gt;&lt;img src="https://img.shields.io/badge/Frappe%20School-Learn%20ERPNext-blue?style=flat-square" alt="Learn on Frappe School" /&gt;&lt;/a&gt;&lt;br /&gt;&lt;br /&gt; &lt;a href="https://github.com/frappe/erpnext/actions/workflows/server-tests-mariadb.yml"&gt;&lt;img src="https://github.com/frappe/erpnext/actions/workflows/server-tests-mariadb.yml/badge.svg?event=schedule" alt="CI" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/frappe/erpnext-worker"&gt;&lt;img src="https://img.shields.io/docker/pulls/frappe/erpnext-worker.svg?sanitize=true" alt="docker pulls" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/frappe/erpnext/develop/erpnext/public/images/v16/hero_image.png" /&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://erpnext-demo.frappe.cloud/api/method/erpnext_demo.erpnext_demo.auth.login_demo"&gt;Live Demo&lt;/a&gt; - 
 &lt;a href="https://frappe.io/erpnext"&gt;Website&lt;/a&gt; - 
 &lt;a href="https://docs.frappe.io/erpnext/"&gt;Documentation&lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;ERPNext&lt;/h2&gt; 
&lt;p&gt;100% Open-Source ERP system to help you run your business.&lt;/p&gt; 
&lt;h3&gt;Motivation&lt;/h3&gt; 
&lt;p&gt;Running a business is a complex task - handling invoices, tracking stock, managing personnel and even more ad-hoc activities. In a market where software is sold separately to manage each of these tasks, ERPNext does all of the above and more, for free.&lt;/p&gt; 
&lt;h3&gt;Key Features&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Accounting&lt;/strong&gt;: All the tools you need to manage cash flow in one place, right from recording transactions to summarizing and analyzing financial reports.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Order Management&lt;/strong&gt;: Track inventory levels, replenish stock, and manage sales orders, customers, suppliers, shipments, deliverables, and order fulfillment.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Manufacturing&lt;/strong&gt;: Simplifies the production cycle, helps track material consumption, exhibits capacity planning, handles subcontracting, and more!&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Asset Management&lt;/strong&gt;: From purchase to perishment, IT infrastructure to equipment. Cover every branch of your organization, all in one centralized system.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Projects&lt;/strong&gt;: Delivery both internal and external Projects on time, budget and Profitability. Track tasks, timesheets, and issues by project.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details open&gt; 
 &lt;summary&gt;More&lt;/summary&gt; 
 &lt;img src="https://erpnext.com/files/v16_bom.png" /&gt; 
 &lt;img src="https://erpnext.com/files/v16_stock_summary.png" /&gt; 
 &lt;img src="https://erpnext.com/files/v16_job_card.png" /&gt; 
 &lt;img src="https://erpnext.com/files/v16_tasks.png" /&gt; 
&lt;/details&gt; 
&lt;h3&gt;Under the Hood&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/frappe/frappe"&gt;&lt;strong&gt;Frappe Framework&lt;/strong&gt;&lt;/a&gt;: A full-stack web application framework written in Python and Javascript. The framework provides a robust foundation for building web applications, including a database abstraction layer, user authentication, and a REST API.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/frappe/frappe-ui"&gt;&lt;strong&gt;Frappe UI&lt;/strong&gt;&lt;/a&gt;: A Vue-based UI library, to provide a modern user interface. The Frappe UI library provides a variety of components that can be used to build single-page applications on top of the Frappe Framework.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Production Setup&lt;/h2&gt; 
&lt;h3&gt;Managed Hosting&lt;/h3&gt; 
&lt;p&gt;You can try &lt;a href="https://frappecloud.com"&gt;Frappe Cloud&lt;/a&gt;, a simple, user-friendly and sophisticated &lt;a href="https://github.com/frappe/press"&gt;open-source&lt;/a&gt; platform to host Frappe applications with peace of mind.&lt;/p&gt; 
&lt;p&gt;It takes care of installation, setup, upgrades, monitoring, maintenance and support of your Frappe deployments. It is a fully featured developer platform with an ability to manage and control multiple Frappe deployments.&lt;/p&gt; 
&lt;div&gt; 
 &lt;a href="https://erpnext-demo.frappe.cloud/app/home" target="_blank"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="https://frappe.io/files/try-on-fc-white.png" /&gt; 
   &lt;img src="https://frappe.io/files/try-on-fc-black.png" alt="Try on Frappe Cloud" height="28" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h3&gt;Self-Hosted&lt;/h3&gt; 
&lt;h4&gt;Docker&lt;/h4&gt; 
&lt;p&gt;Prerequisites: docker, docker-compose, git. Refer &lt;a href="https://docs.docker.com"&gt;Docker Documentation&lt;/a&gt; for more details on Docker setup.&lt;/p&gt; 
&lt;p&gt;Run following commands:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;git clone https://github.com/frappe/frappe_docker
cd frappe_docker
docker compose -f pwd.yml up -d
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After a couple of minutes, site should be accessible on your localhost port: 8080. Use below default login credentials to access the site.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Username: Administrator&lt;/li&gt; 
 &lt;li&gt;Password: admin&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See &lt;a href="https://github.com/frappe/frappe_docker?tab=readme-ov-file#to-run-on-arm64-architecture-follow-this-instructions"&gt;Frappe Docker&lt;/a&gt; for ARM based docker setup.&lt;/p&gt; 
&lt;h2&gt;Development Setup&lt;/h2&gt; 
&lt;h3&gt;Manual Install&lt;/h3&gt; 
&lt;p&gt;The Easy Way: our install script for bench will install all dependencies (e.g. MariaDB). See &lt;a href="https://github.com/frappe/bench"&gt;https://github.com/frappe/bench&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;p&gt;New passwords will be created for the ERPNext "Administrator" user, the MariaDB root user, and the frappe user (the script displays the passwords and saves them to ~/frappe_passwords.txt).&lt;/p&gt; 
&lt;h3&gt;Local&lt;/h3&gt; 
&lt;p&gt;To setup the repository locally follow the steps mentioned below:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Setup bench by following the &lt;a href="https://frappeframework.com/docs/user/en/installation"&gt;Installation Steps&lt;/a&gt; and start the server&lt;/p&gt; &lt;pre&gt;&lt;code&gt;bench start
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;In a separate terminal window, run the following commands:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Create a new site
bench new-site erpnext.localhost
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Get the ERPNext app and install it&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Get the ERPNext app
bench get-app https://github.com/frappe/erpnext

# Install the app
bench --site erpnext.localhost install-app erpnext
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Open the URL &lt;code&gt;http://erpnext.localhost:8000/app&lt;/code&gt; in your browser, you should see the app running&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Learning and community&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://school.frappe.io"&gt;Frappe School&lt;/a&gt; - Learn Frappe Framework and ERPNext from the various courses by the maintainers or from the community.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.erpnext.com/"&gt;Official documentation&lt;/a&gt; - Extensive documentation for ERPNext.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://discuss.frappe.io/c/erpnext/6"&gt;Discussion Forum&lt;/a&gt; - Engage with community of ERPNext users and service providers.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://erpnext_public.t.me"&gt;Telegram Group&lt;/a&gt; - Get instant help from huge community of users.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://github.com/frappe/erpnext/wiki/Issue-Guidelines"&gt;Issue Guidelines&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://erpnext.com/security"&gt;Report Security Vulnerabilities&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/frappe/erpnext/wiki/Contribution-Guidelines"&gt;Pull Request Requirements&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://crowdin.com/project/frappe"&gt;Translations&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Logo and Trademark Policy&lt;/h2&gt; 
&lt;p&gt;Please read our &lt;a href="https://raw.githubusercontent.com/frappe/erpnext/develop/TRADEMARK_POLICY.md"&gt;Logo and Trademark Policy&lt;/a&gt;.&lt;/p&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;div align="center" style="padding-top: 0.75rem;"&gt; 
 &lt;a href="https://frappe.io" target="_blank"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="https://frappe.io/files/Frappe-white.png" /&gt; 
   &lt;img src="https://frappe.io/files/Frappe-black.png" alt="Frappe Technologies" height="28" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>suitenumerique/docs</title>
      <link>https://github.com/suitenumerique/docs</link>
      <description>&lt;p&gt;A collaborative note taking, wiki and documentation platform that scales. Built with Django and React.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="https://github.com/suitenumerique/docs"&gt; &lt;img alt="Docs" src="https://raw.githubusercontent.com/suitenumerique/docs/main/docs/assets/banner-docs.png" width="100%" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/suitenumerique/docs/stargazers/"&gt; &lt;img src="https://img.shields.io/github/stars/suitenumerique/docs" alt="" /&gt; &lt;/a&gt; &lt;a href="https://github.com/suitenumerique/docs/raw/main/CONTRIBUTING.md"&gt;&lt;img alt="PRs Welcome" src="https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=shields" /&gt;&lt;/a&gt; &lt;img alt="GitHub commit activity" src="https://img.shields.io/github/commit-activity/m/suitenumerique/docs" /&gt; &lt;img alt="GitHub closed issues" src="https://img.shields.io/github/issues-closed/suitenumerique/docs" /&gt; &lt;a href="https://github.com/suitenumerique/docs/raw/main/LICENSE"&gt; &lt;img alt="MIT License" src="https://img.shields.io/github/license/suitenumerique/docs" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://matrix.to/#/#docs-official:matrix.org"&gt; Chat on Matrix &lt;/a&gt; - &lt;a href="https://raw.githubusercontent.com/suitenumerique/docs/main/docs/"&gt; Documentation &lt;/a&gt; - &lt;a href="https://raw.githubusercontent.com/suitenumerique/docs/main/#getting-started-"&gt; Getting started &lt;/a&gt; - &lt;a href="mailto:docs@numerique.gouv.fr"&gt; Reach out &lt;/a&gt; &lt;/p&gt; 
&lt;h1&gt;La Suite Docs : Collaborative Text Editing&lt;/h1&gt; 
&lt;p&gt;Docs, where your notes can become knowledge through live collaboration.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/suitenumerique/docs/main/docs/assets/docs_live_collaboration_light.gif" width="100%" align="center" /&gt; 
&lt;h2&gt;Why use Docs ‚ùì&lt;/h2&gt; 
&lt;p&gt;Docs is a collaborative text editor designed to address common challenges in knowledge building and sharing.&lt;/p&gt; 
&lt;h3&gt;Write&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;üòå Get simple, accessible online editing for your team.&lt;/li&gt; 
 &lt;li&gt;üíÖ Create clean documents with beautiful formatting options.&lt;/li&gt; 
 &lt;li&gt;üñåÔ∏è Focus on your content using either the in-line editor, or &lt;a href="https://www.markdownguide.org/basic-syntax/"&gt;the Markdown syntax&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;üß± Quickly design your page thanks to the many block types, accessible from the &lt;code&gt;/&lt;/code&gt; slash commands, as well as keyboard shortcuts.&lt;/li&gt; 
 &lt;li&gt;üîå Write offline! Your edits will be synced once you're back online.&lt;/li&gt; 
 &lt;li&gt;‚ú® Save time thanks to our AI actions, such as rephrasing, summarizing, fixing typos, translating, etc. You can even turn your selected text into a prompt!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Work together&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ü§ù Enjoy live editing! See your team collaborate in real time.&lt;/li&gt; 
 &lt;li&gt;üîí Keep your information secure thanks to granular access control. Only share with the right people.&lt;/li&gt; 
 &lt;li&gt;üìë Export your content in multiple formats (&lt;code&gt;.odt&lt;/code&gt;, &lt;code&gt;.docx&lt;/code&gt;, &lt;code&gt;.pdf&lt;/code&gt;) with customizable templates.&lt;/li&gt; 
 &lt;li&gt;üìö Turn your team's collaborative work into organized knowledge with Subpages.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Self-host&lt;/h3&gt; 
&lt;h4&gt;üöÄ Docs is easy to install on your own servers&lt;/h4&gt; 
&lt;p&gt;We use Kubernetes for our &lt;a href="https://docs.numerique.gouv.fr/"&gt;production instance&lt;/a&gt; but also support Docker Compose. The community contributed a couple other methods (Nix, YunoHost etc.) check out the &lt;a href="https://raw.githubusercontent.com/suitenumerique/docs/main/docs/installation/README.md"&gt;docs&lt;/a&gt; to get detailed instructions and examples.&lt;/p&gt; 
&lt;h4&gt;üåç Known instances&lt;/h4&gt; 
&lt;p&gt;We hope to see many more, here is an incomplete list of public Docs instances. Feel free to make a PR to add ones that are not listed belowüôè&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Url&lt;/th&gt; 
   &lt;th&gt;Org&lt;/th&gt; 
   &lt;th&gt;Public&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.numerique.gouv.fr/"&gt;docs.numerique.gouv.fr&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;DINUM&lt;/td&gt; 
   &lt;td&gt;French public agents working for the central administration and the extended public sphere. ProConnect is required to login in or sign up&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.suite.anct.gouv.fr/"&gt;docs.suite.anct.gouv.fr&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ANCT&lt;/td&gt; 
   &lt;td&gt;French public agents working for the territorial administration and the extended public sphere. ProConnect is required to login in or sign up&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://notes.demo.opendesk.eu"&gt;notes.demo.opendesk.eu&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ZenDiS&lt;/td&gt; 
   &lt;td&gt;Demo instance of OpenDesk. Request access to get credentials&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://notes.liiib.re/"&gt;notes.liiib.re&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;lasuite.coop&lt;/td&gt; 
   &lt;td&gt;Free and open demo to all. Content and accounts are reset after one month&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.federated.nexus/"&gt;docs.federated.nexus&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;federated.nexus&lt;/td&gt; 
   &lt;td&gt;Public instance, but you have to &lt;a href="https://federated.nexus/register/"&gt;sign up for a Federated Nexus account&lt;/a&gt;.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://docs.demo.mosacloud.eu/"&gt;docs.demo.mosacloud.eu&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;mosa.cloud&lt;/td&gt; 
   &lt;td&gt;Demo instance of mosa.cloud, a dutch company providing services around La Suite apps.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h4&gt;‚ö†Ô∏è Advanced features&lt;/h4&gt; 
&lt;p&gt;For some advanced features (ex: Export as PDF) Docs relies on XL packages from BlockNote. These are licenced under GPL and are not MIT compatible. You can perfectly use Docs without these packages by setting the environment variable &lt;code&gt;PUBLISH_AS_MIT&lt;/code&gt; to true. That way you'll build an image of the application without the features that are not MIT compatible. Read the &lt;a href="https://raw.githubusercontent.com/suitenumerique/docs/main/docs/env.md"&gt;environment variables documentation&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h2&gt;Getting started üîß&lt;/h2&gt; 
&lt;h3&gt;Test it&lt;/h3&gt; 
&lt;p&gt;You can test Docs on your browser by visiting this &lt;a href="https://impress-preprod.beta.numerique.gouv.fr/docs/6ee5aac4-4fb9-457d-95bf-bb56c2467713/"&gt;demo document&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Run Docs locally&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;‚ö†Ô∏è The methods described below for running Docs locally is &lt;strong&gt;for testing purposes only&lt;/strong&gt;. It is based on building Docs using &lt;a href="https://min.io/"&gt;Minio&lt;/a&gt; as an S3-compatible storage solution. Of course you can choose any S3-compatible storage solution.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;Prerequisite&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Make sure you have a recent version of Docker and &lt;a href="https://docs.docker.com/compose/install"&gt;Docker Compose&lt;/a&gt; installed on your laptop, then type:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shellscript"&gt;$ docker -v

Docker version 20.10.2, build 2291f61

$ docker compose version

Docker Compose version v2.32.4
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;‚ö†Ô∏è You may need to run the following commands with &lt;code&gt;sudo&lt;/code&gt;, but this can be avoided by adding your user to the local &lt;code&gt;docker&lt;/code&gt; group.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;Project bootstrap&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;The easiest way to start working on the project is to use &lt;a href="https://www.gnu.org/software/make/"&gt;GNU Make&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shellscript"&gt;$ make bootstrap FLUSH_ARGS='--no-input'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This command builds the &lt;code&gt;app-dev&lt;/code&gt; and &lt;code&gt;frontend-dev&lt;/code&gt; containers, installs dependencies, performs database migrations and compiles translations. It's a good idea to use this command each time you are pulling code from the project repository to avoid dependency-related or migration-related issues.&lt;/p&gt; 
&lt;p&gt;Your Docker services should now be up and running üéâ&lt;/p&gt; 
&lt;p&gt;You can access the project by going to &lt;a href="http://localhost:3000"&gt;http://localhost:3000&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You will be prompted to log in. The default credentials are:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;username: impress
password: impress
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;üìù Note that if you need to run them afterwards, you can use the eponymous Make rule:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shellscript"&gt;$ make run
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;‚ö†Ô∏è For the frontend developer, it is often better to run the frontend in development mode locally.&lt;/p&gt; 
&lt;p&gt;To do so, install the frontend dependencies with the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shellscript"&gt;$ make frontend-development-install
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;And run the frontend locally in development mode with the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shellscript"&gt;$ make run-frontend-development
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To start all the services, except the frontend container, you can use the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shellscript"&gt;$ make run-backend
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To execute frontend tests &amp;amp; linting only&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shellscript"&gt;$ make frontend-test
$ make frontend-lint
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Adding content&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;You can create a basic demo site by running this command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shellscript"&gt;$ make demo
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Finally, you can check all available Make rules using this command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shellscript"&gt;$ make help
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Django admin&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;You can access the Django admin site at:&lt;/p&gt; 
&lt;p&gt;&lt;a href="http://localhost:8071/admin"&gt;http://localhost:8071/admin&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You first need to create a superuser account:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shellscript"&gt;$ make superuser
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Feedback üôã‚Äç‚ôÇÔ∏èüôã‚Äç‚ôÄÔ∏è&lt;/h2&gt; 
&lt;p&gt;We'd love to hear your thoughts, and hear about your experiments, so come and say hi on &lt;a href="https://matrix.to/#/#docs-official:matrix.org"&gt;Matrix&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Roadmap üí°&lt;/h2&gt; 
&lt;p&gt;Want to know where the project is headed? &lt;a href="https://github.com/orgs/numerique-gouv/projects/13/views/11"&gt;üó∫Ô∏è Checkout our roadmap&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;License üìù&lt;/h2&gt; 
&lt;p&gt;This work is released under the MIT License (see &lt;a href="https://github.com/suitenumerique/docs/raw/main/LICENSE"&gt;LICENSE&lt;/a&gt;).&lt;/p&gt; 
&lt;p&gt;While Docs is a public-driven initiative, our license choice is an invitation for private sector actors to use, sell and contribute to the project.&lt;/p&gt; 
&lt;h2&gt;Contributing üôå&lt;/h2&gt; 
&lt;p&gt;This project is intended to be community-driven, so please, do not hesitate to &lt;a href="https://matrix.to/#/#docs-official:matrix.org"&gt;get in touch&lt;/a&gt; if you have any question related to our implementation or design decisions.&lt;/p&gt; 
&lt;p&gt;You can help us with translations on &lt;a href="https://crowdin.com/project/lasuite-docs"&gt;Crowdin&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you intend to make pull requests, see &lt;a href="https://github.com/suitenumerique/docs/raw/main/CONTRIBUTING.md"&gt;CONTRIBUTING&lt;/a&gt; for guidelines.&lt;/p&gt; 
&lt;h2&gt;Directory structure:&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-markdown"&gt;docs
‚îú‚îÄ‚îÄ bin - executable scripts or binaries that are used for various tasks, such as setup scripts, utility scripts, or custom commands.
‚îú‚îÄ‚îÄ crowdin - for crowdin translations, a tool or service that helps manage translations for the project.
‚îú‚îÄ‚îÄ docker - Dockerfiles and related configuration files used to build Docker images for the project. These images can be used for development, testing, or production environments.
‚îú‚îÄ‚îÄ docs - documentation for the project, including user guides, API documentation, and other helpful resources.
‚îú‚îÄ‚îÄ env.d/development - environment-specific configuration files for the development environment. These files might include environment variables, configuration settings, or other setup files needed for development.
‚îú‚îÄ‚îÄ gitlint - configuration files for `gitlint`, a tool that enforces commit message guidelines to ensure consistency and quality in commit messages.
‚îú‚îÄ‚îÄ playground - experimental or temporary code, where developers can test new features or ideas without affecting the main codebase.
‚îî‚îÄ‚îÄ src - main source code directory, containing the core application code, libraries, and modules of the project.
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Credits ‚ù§Ô∏è&lt;/h2&gt; 
&lt;h3&gt;Stack&lt;/h3&gt; 
&lt;p&gt;Docs is built on top of &lt;a href="https://www.django-rest-framework.org/"&gt;Django Rest Framework&lt;/a&gt;, &lt;a href="https://nextjs.org/"&gt;Next.js&lt;/a&gt;, &lt;a href="https://www.blocknotejs.org/"&gt;BlockNote.js&lt;/a&gt;, &lt;a href="https://tiptap.dev/docs/hocuspocus/introduction"&gt;HocusPocus&lt;/a&gt; and &lt;a href="https://yjs.dev/"&gt;Yjs&lt;/a&gt;. We thank the contributors of all these projects for their awesome work!&lt;/p&gt; 
&lt;p&gt;We are proud sponsors of &lt;a href="https://www.blocknotejs.org/"&gt;BlockNotejs&lt;/a&gt; and &lt;a href="https://yjs.dev/"&gt;Yjs&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Gov ‚ù§Ô∏è open source&lt;/h3&gt; 
&lt;p&gt;Docs is the result of a joint effort led by the French üá´üá∑ü•ñ (&lt;a href="https://www.numerique.gouv.fr/dinum/"&gt;DINUM&lt;/a&gt;) and German üá©üá™ü•® governments (&lt;a href="https://zendis.de/"&gt;ZenDiS&lt;/a&gt;).&lt;/p&gt; 
&lt;p&gt;We are always looking for new public partners (we are currently onboarding the Netherlands üá≥üá±üßÄ), feel free to &lt;a href="mailto:docs@numerique.gouv.fr"&gt;reach out&lt;/a&gt; if you are interested in using or contributing to Docs.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/suitenumerique/docs/main/docs/assets/europe_opensource.png" width="50%" /&gt; &lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Shubhamsaboo/awesome-llm-apps</title>
      <link>https://github.com/Shubhamsaboo/awesome-llm-apps</link>
      <description>&lt;p&gt;Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="http://www.theunwindai.com"&gt; &lt;img src="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/docs/banner/unwind_black.png" width="900px" alt="Unwind AI" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://www.linkedin.com/in/shubhamsaboo/"&gt; &lt;img src="https://img.shields.io/badge/-Follow%20Shubham%20Saboo-blue?logo=linkedin&amp;amp;style=flat-square" alt="LinkedIn" /&gt; &lt;/a&gt; &lt;a href="https://twitter.com/Saboo_Shubham_"&gt; &lt;img src="https://img.shields.io/twitter/follow/Shubham_Saboo" alt="Twitter" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; 
 &lt;!-- Keep these links. Translations will automatically update with the README. --&gt; &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=de"&gt;Deutsch&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=es"&gt;Espa√±ol&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=fr"&gt;fran√ßais&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ja"&gt;Êó•Êú¨Ë™û&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ko"&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=pt"&gt;Portugu√™s&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ru"&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=zh"&gt;‰∏≠Êñá&lt;/a&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;h1&gt;üåü Awesome LLM Apps&lt;/h1&gt; 
&lt;p&gt;A curated collection of &lt;strong&gt;Awesome LLM apps built with RAG, AI Agents, Multi-agent Teams, MCP, Voice Agents, and more.&lt;/strong&gt; This repository features LLM apps that use models from &lt;img src="https://cdn.simpleicons.org/openai" alt="openai logo" width="25" height="15" /&gt;&lt;strong&gt;OpenAI&lt;/strong&gt; , &lt;img src="https://cdn.simpleicons.org/anthropic" alt="anthropic logo" width="25" height="15" /&gt;&lt;strong&gt;Anthropic&lt;/strong&gt;, &lt;img src="https://cdn.simpleicons.org/googlegemini" alt="google logo" width="25" height="18" /&gt;&lt;strong&gt;Google&lt;/strong&gt;, &lt;img src="https://cdn.simpleicons.org/x" alt="X logo" width="25" height="15" /&gt;&lt;strong&gt;xAI&lt;/strong&gt; and open-source models like &lt;img src="https://cdn.simpleicons.org/alibabacloud" alt="alibaba logo" width="25" height="15" /&gt;&lt;strong&gt;Qwen&lt;/strong&gt; or &lt;img src="https://cdn.simpleicons.org/meta" alt="meta logo" width="25" height="15" /&gt;&lt;strong&gt;Llama&lt;/strong&gt; that you can run locally on your computer.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/9876" target="_blank"&gt; &lt;img src="https://trendshift.io/api/badge/repositories/9876" alt="Shubhamsaboo%2Fawesome-llm-apps | Trendshift" style="width: 250px; height: 55px;" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;ü§î Why Awesome LLM Apps?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;üí° Discover practical and creative ways LLMs can be applied across different domains, from code repositories to email inboxes and more.&lt;/li&gt; 
 &lt;li&gt;üî• Explore apps that combine LLMs from OpenAI, Anthropic, Gemini, and open-source alternatives with AI Agents, Agent Teams, MCP &amp;amp; RAG.&lt;/li&gt; 
 &lt;li&gt;üéì Learn from well-documented projects and contribute to the growing open-source ecosystem of LLM-powered applications.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üôè Thanks to our sponsors&lt;/h2&gt; 
&lt;table align="center" cellpadding="16" cellspacing="12"&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td align="center"&gt; &lt;a href="https://tsdb.co/shubham-gh" target="_blank" rel="noopener" title="Tiger Data"&gt; &lt;img src="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/docs/banner/sponsors/tigerdata.png" alt="Tiger Data" width="500" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;a href="https://tsdb.co/shubham-gh" target="_blank" rel="noopener" style="text-decoration: none; color: #333; font-weight: bold; font-size: 18px;"&gt; Tiger Data MCP &lt;/a&gt; &lt;/td&gt; 
   &lt;td align="center"&gt; &lt;a href="https://github.com/speechmatics/speechmatics-academy" target="_blank" rel="noopener" title="Speechmatics"&gt; &lt;img src="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/docs/banner/sponsors/speechmatics.png" alt="Speechmatics" width="500" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;a href="https://github.com/speechmatics/speechmatics-academy" target="_blank" rel="noopener" style="text-decoration: none; color: #333; font-weight: bold; font-size: 18px;"&gt; Speechmatics &lt;/a&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt; &lt;a href="https://okara.ai/?utm_source=oss&amp;amp;utm_medium=sponsorship&amp;amp;utm_campaign=awesome-llm-apps" title="Okara"&gt; &lt;img src="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/docs/banner/sponsors/okara.png" alt="Okara" width="500" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;a href="https://okara.ai/?utm_source=oss&amp;amp;utm_medium=sponsorship&amp;amp;utm_campaign=awesome-llm-apps" style="text-decoration: none; color: #333; font-weight: bold; font-size: 18px;"&gt; Okara AI &lt;/a&gt; &lt;/td&gt; 
   &lt;td align="center"&gt; &lt;a href="https://sponsorunwindai.com/" title="Become a Sponsor"&gt; &lt;img src="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/docs/banner/sponsor_awesome_llm_apps.png" alt="Become a Sponsor" width="500" /&gt; &lt;/a&gt; &lt;br /&gt; &lt;a href="https://sponsorunwindai.com/" style="text-decoration: none; color: #333; font-weight: bold; font-size: 18px;"&gt; Become a Sponsor &lt;/a&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;üìÇ Featured AI Projects&lt;/h2&gt; 
&lt;h3&gt;AI Agents&lt;/h3&gt; 
&lt;h3&gt;üå± Starter AI Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_blog_to_podcast_agent/"&gt;üéôÔ∏è AI Blog to Podcast Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_breakup_recovery_agent/"&gt;‚ù§Ô∏è‚Äçü©π AI Breakup Recovery Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_data_analysis_agent/"&gt;üìä AI Data Analysis Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_medical_imaging_agent/"&gt;ü©ª AI Medical Imaging Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_meme_generator_agent_browseruse/"&gt;üòÇ AI Meme Generator Agent (Browser)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_music_generator_agent/"&gt;üéµ AI Music Generator Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/ai_travel_agent/"&gt;üõ´ AI Travel Agent (Local &amp;amp; Cloud)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/gemini_multimodal_agent_demo/"&gt;‚ú® Gemini Multimodal Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/mixture_of_agents/"&gt;üîÑ Mixture of Agents&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/xai_finance_agent/"&gt;üìä xAI Finance Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/opeani_research_agent/"&gt;üîç OpenAI Research Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/starter_ai_agents/web_scrapping_ai_agent/"&gt;üï∏Ô∏è Web Scraping AI Agent (Local &amp;amp; Cloud SDK)&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üöÄ Advanced AI Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/ai_home_renovation_agent"&gt;üèöÔ∏è üçå AI Home Renovation Agent with Nano Banana Pro&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_deep_research_agent/"&gt;üîç AI Deep Research Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_vc_due_diligence_agent_team"&gt;üìä AI VC Due Diligence Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/research_agent_gemini_interaction_api"&gt;üî¨ AI Research Planner &amp;amp; Executor (Google Interactions API)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_consultant_agent"&gt;ü§ù AI Consultant Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_system_architect_r1/"&gt;üèóÔ∏è AI System Architect Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/ai_financial_coach_agent/"&gt;üí∞ AI Financial Coach Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_movie_production_agent/"&gt;üé¨ AI Movie Production Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_investment_agent/"&gt;üìà AI Investment Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_health_fitness_agent/"&gt;üèãÔ∏è‚Äç‚ôÇÔ∏è AI Health &amp;amp; Fitness Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/product_launch_intelligence_agent"&gt;üöÄ AI Product Launch Intelligence Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_journalist_agent/"&gt;üóûÔ∏è AI Journalist Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/ai_mental_wellbeing_agent/"&gt;üß† AI Mental Wellbeing Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/single_agent_apps/ai_meeting_agent/"&gt;üìë AI Meeting Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/ai_Self-Evolving_agent/"&gt;üß¨ AI Self-Evolving Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_sales_intelligence_agent_team"&gt;üë®üèª‚Äçüíº AI Sales Intelligence Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/ai_news_and_podcast_agents/"&gt;üéß AI Social Media News and Podcast Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/accomplish-ai/openwork"&gt;üåê Openwork - Open Browser Automation Agent&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üéÆ Autonomous Game Playing Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/autonomous_game_playing_agent_apps/ai_3dpygame_r1/"&gt;üéÆ AI 3D Pygame Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/autonomous_game_playing_agent_apps/ai_chess_agent/"&gt;‚ôú AI Chess Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/autonomous_game_playing_agent_apps/ai_tic_tac_toe_agent/"&gt;üé≤ AI Tic-Tac-Toe Agent&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ü§ù Multi-agent Teams&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_competitor_intelligence_agent_team/"&gt;üß≤ AI Competitor Intelligence Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_finance_agent_team/"&gt;üí≤ AI Finance Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_game_design_agent_team/"&gt;üé® AI Game Design Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_legal_agent_team/"&gt;üë®‚Äç‚öñÔ∏è AI Legal Agent Team (Cloud &amp;amp; Local)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_recruitment_agent_team/"&gt;üíº AI Recruitment Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_real_estate_agent_team"&gt;üè† AI Real Estate Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_services_agency/"&gt;üë®‚Äçüíº AI Services Agency (CrewAI)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_teaching_agent_team/"&gt;üë®‚Äçüè´ AI Teaching Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_coding_agent_team/"&gt;üíª Multimodal Coding Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_design_agent_team/"&gt;‚ú® Multimodal Design Agent Team&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_uiux_feedback_agent_team/"&gt;üé® üçå Multimodal UI/UX Feedback Agent Team with Nano Banana&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_ai_agents/multi_agent_apps/agent_teams/ai_travel_planner_agent_team/"&gt;üåè AI Travel Planner Agent Team&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üó£Ô∏è Voice AI Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/voice_ai_agents/ai_audio_tour_agent/"&gt;üó£Ô∏è AI Audio Tour Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/voice_ai_agents/customer_support_voice_agent/"&gt;üìû Customer Support Voice Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/voice_ai_agents/voice_rag_openaisdk/"&gt;üîä Voice RAG Agent (OpenAI SDK)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/akshayaggarwal99/jarvis-ai-assistant"&gt;üéôÔ∏è OpenSource Voice Dictation Agent (like Wispr Flow&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;img src="https://cdn.simpleicons.org/modelcontextprotocol" alt="mcp logo" width="25" height="20" /&gt; MCP AI Agents&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/mcp_ai_agents/browser_mcp_agent/"&gt;‚ôæÔ∏è Browser MCP Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/mcp_ai_agents/github_mcp_agent/"&gt;üêô GitHub MCP Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/mcp_ai_agents/notion_mcp_agent"&gt;üìë Notion MCP Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/mcp_ai_agents/ai_travel_planner_mcp_agent_team"&gt;üåç AI Travel Planner MCP Agent&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üìÄ RAG (Retrieval Augmented Generation)&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/agentic_rag_embedding_gemma"&gt;üî• Agentic RAG with Embedding Gemma&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/agentic_rag_with_reasoning/"&gt;üßê Agentic RAG with Reasoning&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/ai_blog_search/"&gt;üì∞ AI Blog Search (RAG)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/autonomous_rag/"&gt;üîç Autonomous RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/contextualai_rag_agent/"&gt;üîÑ Contextual AI RAG Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/corrective_rag/"&gt;üîÑ Corrective RAG (CRAG)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/deepseek_local_rag_agent/"&gt;üêã Deepseek Local RAG Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/gemini_agentic_rag/"&gt;ü§î Gemini Agentic RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/hybrid_search_rag/"&gt;üëÄ Hybrid Search RAG (Cloud)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/llama3.1_local_rag/"&gt;üîÑ Llama 3.1 Local RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/local_hybrid_search_rag/"&gt;üñ•Ô∏è Local Hybrid Search RAG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/local_rag_agent/"&gt;ü¶ô Local RAG Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/rag-as-a-service/"&gt;üß© RAG-as-a-Service&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/rag_agent_cohere/"&gt;‚ú® RAG Agent with Cohere&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/rag_chain/"&gt;‚õìÔ∏è Basic RAG Chain&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/rag_database_routing/"&gt;üì† RAG with Database Routing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/rag_tutorials/vision_rag/"&gt;üñºÔ∏è Vision RAG&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üíæ LLM Apps with Memory Tutorials&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/ai_arxiv_agent_memory/"&gt;üíæ AI ArXiv Agent with Memory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/ai_travel_agent_memory/"&gt;üõ©Ô∏è AI Travel Agent with Memory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/llama3_stateful_chat/"&gt;üí¨ Llama3 Stateful Chat&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/llm_app_personalized_memory/"&gt;üìù LLM App with Personalized Memory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/local_chatgpt_with_memory/"&gt;üóÑÔ∏è Local ChatGPT Clone with Memory&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_apps_with_memory_tutorials/multi_llm_memory/"&gt;üß† Multi-LLM Application with Shared Memory&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üí¨ Chat with X Tutorials&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_github/"&gt;üí¨ Chat with GitHub (GPT &amp;amp; Llama3)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_gmail/"&gt;üì® Chat with Gmail&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_pdf/"&gt;üìÑ Chat with PDF (GPT &amp;amp; Llama3)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_research_papers/"&gt;üìö Chat with Research Papers (ArXiv) (GPT &amp;amp; Llama3)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_substack/"&gt;üìù Chat with Substack&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/chat_with_X_tutorials/chat_with_youtube_videos/"&gt;üìΩÔ∏è Chat with YouTube Videos&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üéØ LLM Optimization Tools&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_optimization_tools/toonify_token_optimization/"&gt;üéØ Toonify Token Optimization&lt;/a&gt; - Reduce LLM API costs by 30-60% using TOON format&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_optimization_tools/headroom_context_optimization/"&gt;üß† Headroom Context Optimization&lt;/a&gt; - Reduce LLM API costs by 50-90% through intelligent context compression for AI agents (includes persistent memory &amp;amp; MCP support)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üîß LLM Fine-tuning Tutorials&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;img src="https://cdn.simpleicons.org/google" alt="google logo" width="20" height="15" /&gt; &lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_finetuning_tutorials/gemma3_finetuning/"&gt;Gemma 3 Fine-tuning&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;img src="https://cdn.simpleicons.org/meta" alt="meta logo" width="25" height="15" /&gt; &lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/advanced_llm_apps/llm_finetuning_tutorials/llama3.2_finetuning/"&gt;Llama 3.2 Fine-tuning&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üßë‚Äçüè´ AI Agent Framework Crash Course&lt;/h3&gt; 
&lt;p&gt;&lt;img src="https://cdn.simpleicons.org/google" alt="google logo" width="25" height="15" /&gt; &lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/ai_agent_framework_crash_course/google_adk_crash_course/"&gt;Google ADK Crash Course&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Starter agent; model‚Äëagnostic (OpenAI, Claude)&lt;/li&gt; 
 &lt;li&gt;Structured outputs (Pydantic)&lt;/li&gt; 
 &lt;li&gt;Tools: built‚Äëin, function, third‚Äëparty, MCP tools&lt;/li&gt; 
 &lt;li&gt;Memory; callbacks; Plugins&lt;/li&gt; 
 &lt;li&gt;Simple multi‚Äëagent; Multi‚Äëagent patterns&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://cdn.simpleicons.org/openai" alt="openai logo" width="25" height="15" /&gt; &lt;a href="https://raw.githubusercontent.com/Shubhamsaboo/awesome-llm-apps/main/ai_agent_framework_crash_course/openai_sdk_crash_course/"&gt;OpenAI Agents SDK Crash Course&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Starter agent; function calling; structured outputs&lt;/li&gt; 
 &lt;li&gt;Tools: built‚Äëin, function, third‚Äëparty integrations&lt;/li&gt; 
 &lt;li&gt;Memory; callbacks; evaluation&lt;/li&gt; 
 &lt;li&gt;Multi‚Äëagent patterns; agent handoffs&lt;/li&gt; 
 &lt;li&gt;Swarm orchestration; routing logic&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üöÄ Getting Started&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Clone the repository&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/Shubhamsaboo/awesome-llm-apps.git 
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Navigate to the desired project directory&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cd awesome-llm-apps/starter_ai_agents/ai_travel_agent
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install the required dependencies&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Follow the project-specific instructions&lt;/strong&gt; in each project's &lt;code&gt;README.md&lt;/code&gt; file to set up and run the app.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;&lt;img src="https://cdn.simpleicons.org/github" alt="github logo" width="25" height="20" /&gt; Thank You, Community, for the Support! üôè&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#Shubhamsaboo/awesome-llm-apps&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=Shubhamsaboo/awesome-llm-apps&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;üåü &lt;strong&gt;Don‚Äôt miss out on future updates! Star the repo now and be the first to know about new and exciting LLM apps with RAG and AI Agents.&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>NVIDIA/earth2studio</title>
      <link>https://github.com/NVIDIA/earth2studio</link>
      <description>&lt;p&gt;Open-source deep-learning framework for exploring, building and deploying AI weather/climate workflows.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h1&gt;NVIDIA Earth2Studio&lt;/h1&gt; 
 &lt;p&gt;&lt;a href="https://www.python.org/downloads/"&gt;&lt;img src="https://img.shields.io/badge/Python-3.11%20%7C%203.12%20%7C%203.13-blue?style=flat-square&amp;amp;logo=python" alt="python version" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/NVIDIA/earth2studio/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/License-Apache%202.0-green?style=flat-square" alt="license" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/NVIDIA/earth2studio/main/test/"&gt;&lt;img src="https://img.shields.io/codecov/c/github/nickgeneva/earth2studio?style=flat-square&amp;amp;logo=codecov" alt="coverage" /&gt;&lt;/a&gt; &lt;a href="https://mypy-lang.org/"&gt;&lt;img src="https://img.shields.io/badge/mypy-Checked-blue?style=flat-square&amp;amp;labelColor=grey" alt="mypy" /&gt;&lt;/a&gt; &lt;a href="https://github.com/psf/black"&gt;&lt;img src="https://img.shields.io/badge/Code%20Style-Black-black?style=flat-square" alt="format" /&gt;&lt;/a&gt; &lt;a href="https://github.com/astral-sh/ruff"&gt;&lt;img src="https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json&amp;amp;style=flat-square" alt="ruff" /&gt;&lt;/a&gt; &lt;a href="https://github.com/astral-sh/uv"&gt;&lt;img src="https://img.shields.io/endpoint?url=https%3A%2F%2Fraw.githubusercontent.com%2Fastral-sh%2Fuv%2Fmain%2Fassets%2Fbadge%2Fv0.json&amp;amp;style=flat-square" alt="uv" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;Earth2Studio is a Python-based package designed to get users up and running with AI Earth system models &lt;em&gt;fast&lt;/em&gt;. Our mission is to enable everyone to build, research and explore AI driven weather and climate science.&lt;/p&gt; 
 &lt;!-- markdownlint-disable MD036 --&gt; 
 &lt;p&gt;&lt;strong&gt;- Earth2Studio Documentation -&lt;/strong&gt;&lt;/p&gt; 
 &lt;!-- markdownlint-enable MD036 --&gt; 
 &lt;p&gt;&lt;a href="https://nvidia.github.io/earth2studio/userguide/about/install.html"&gt;Install&lt;/a&gt; | &lt;a href="https://nvidia.github.io/earth2studio/userguide/"&gt;User-Guide&lt;/a&gt; | &lt;a href="https://nvidia.github.io/earth2studio/examples/"&gt;Examples&lt;/a&gt; | &lt;a href="https://nvidia.github.io/earth2studio/modules/"&gt;API&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://huggingface.co/datasets/NickGeneva/Earth2StudioAssets/raw/main/0.2.0/earth2studio_feature_banner.png?id=1" alt="Earth2Studio Banner" /&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;Quick start&lt;/h2&gt; 
&lt;p&gt;Running AI weather prediction can be done with just a few lines of code.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;For detailed installation steps, including model-specific installations, see the &lt;a href="https://nvidia.github.io/earth2studio/userguide/about/install.html"&gt;install guide&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;See the &lt;a href="https://nvidia.github.io/earth2studio/examples/"&gt;examples&lt;/a&gt; gallery providing different inference workflow samples.&lt;/li&gt; 
 &lt;li&gt;Swap out &lt;a href="https://nvidia.github.io/earth2studio/modules/datasources.html"&gt;data sources&lt;/a&gt; or &lt;a href="https://nvidia.github.io/earth2studio/modules/models.html#earth2studio-models-px-prognostic"&gt;models&lt;/a&gt; depending on your use case!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;NVIDIA FourCastNet3&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from earth2studio.models.px import FCN3
from earth2studio.data import GFS
from earth2studio.io import ZarrBackend
from earth2studio.run import deterministic as run

model = FCN3.load_model(FCN3.load_default_package())
data = GFS()
io = ZarrBackend("outputs/fcn3_forecast.zarr")
run(["2025-01-01T00:00:00"], 10, model, data, io)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;ECMWF AIFS&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from earth2studio.models.px import AIFS
from earth2studio.data import IFS
from earth2studio.io import ZarrBackend
from earth2studio.run import deterministic as run

model = AIFS.load_model(AIFS.load_default_package())
data = IFS()
io = ZarrBackend("outputs/aifs_forecast.zarr")
run(["2025-01-01T00:00:00"], 10, model, data, io)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Google Graphcast&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from earth2studio.models.px import GraphCastOperational
from earth2studio.data import GFS
from earth2studio.io import ZarrBackend
from earth2studio.run import deterministic as run

package = GraphCastOperational.load_default_package()
model = GraphCastOperational.load_model(package)
data = GFS()
io = ZarrBackend("outputs/graphcast_operational_forecast.zarr")
run(["2025-01-01T00:00:00"], 4, model, data, io)
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] Earth2Studio is an interface to third‚Äëparty models, checkpoints, and datasets. Licenses for these assets are owned by their providers. Ensure you have the rights to download, use, and (if applicable) redistribute each model and dataset. Links to the original license and source are often provided in the API docs for each model/data source.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=Sog6aCapZeA"&gt;&lt;img src="https://img.youtube.com/vi/Sog6aCapZeA/hqdefault.jpg" alt="Watch the video" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Latest News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://nvidia.github.io/earth2studio/modules/generated/models/px/earth2studio.models.px.StormScopeGOES.html"&gt;&lt;strong&gt;StormScope&lt;/strong&gt;&lt;/a&gt;, NVIDIA's latest regional nowcasting model, is now available in Earth2Studio with a dedicated &lt;a href="https://nvidia.github.io/earth2studio/examples/20_stormscope_goes_example.html"&gt;inference example&lt;/a&gt; to demonstrate its use for predicting CONUS satellite and radar data.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nvidia.github.io/earth2studio/modules/generated/models/px/earth2studio.models.px.Atlas.html"&gt;&lt;strong&gt;Atlas&lt;/strong&gt;&lt;/a&gt;, NVIDIA's latest medium range model, is now available in Earth2Studio providing &lt;em&gt;state-of-the-art&lt;/em&gt; accuracy for medium range global forecasts.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nvidia.github.io/earth2studio/modules/generated/models/dx/earth2studio.models.dx.CorrDiffCMIP6.html"&gt;&lt;strong&gt;CorrDiff for CMIP6 to ERA5&lt;/strong&gt;&lt;/a&gt; is a novel generative downscaling model to generate ERA5 fields from CMIP data enabling users to run ERA5 based prognostic and diagnostic models on future climate simulations.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ECMWF AIFSENS&lt;/strong&gt; model wrapper added, an ensemble-based probablistic data driven forecast model developed by the European Centre for Medium-Range Weather Forecasts (ECMWF).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For a complete list of latest features and improvements see the &lt;a href="https://raw.githubusercontent.com/NVIDIA/earth2studio/main/CHANGELOG.md"&gt;changelog&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Overview&lt;/h2&gt; 
&lt;p&gt;Earth2Studio is an &lt;em&gt;AI inference pipeline toolkit&lt;/em&gt; focused on weather and climate applications that is designed to ride on top of different AI frameworks, model architectures, data sources and SciML tooling while providing a unified API.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;img src="https://huggingface.co/datasets/NickGeneva/Earth2StudioAssets/resolve/main/0.9.0/earth2studio-readme-overview-1.png?id=1" alt="Earth2Studio Overview 1" /&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;The composability of the different core components in Earth2Studio easily allows the development and deployment of increasingly complex pipelines that may chain multiple data sources, AI models and other modules together.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;img src="https://huggingface.co/datasets/NickGeneva/Earth2StudioAssets/resolve/main/0.9.0/earth2studio-readme-overview-2.png?id=1" alt="Earth2Studio Overview 1" /&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;The unified ecosystem of Earth2Studio provides users the opportunity to rapidly swap out components for alternatives. In addition to the largest model zoo of weather/climate AI models, Earth2Studio is packed with useful functionality such as optimized data access to cloud data stores, statistical operations and more to accelerate your pipelines.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;img src="https://huggingface.co/datasets/NickGeneva/Earth2StudioAssets/resolve/main/0.9.0/earth2studio-readme-overview-3.webp?id=1" alt="Earth2Studio Overview 1" /&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h3&gt;Earth-2 Open Models&lt;/h3&gt; 
&lt;p&gt;Access state of the art Nvidia open models for climate and weather: &lt;a href="https://huggingface.co/collections/nvidia/earth-2"&gt;Earth-2 Open Models&lt;/a&gt;. For training recipes for these models, see the &lt;a href="https://github.com/NVIDIA/physicsnemo"&gt;PhysicsNeMo repository&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;p&gt;Earth2Studio package focuses on supplying users the tools to build their own workflows, pipelines, APIs, packages, etc. via modular components including:&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Prognostic Models&lt;/summary&gt; 
 &lt;p&gt;&lt;a href="https://nvidia.github.io/earth2studio/userguide/components/prognostic.html"&gt;Prognostic models&lt;/a&gt; in Earth2Studio perform time integration, taking atmospheric fields at a specific time and auto-regressively predicting the same fields into the future (typically 6 hours per step), enabling both single time-step predictions and extended time-series forecasting.&lt;/p&gt; 
 &lt;p&gt;Earth2Studio maintains the largest collection of pre-trained state-of-the-art AI weather/climate models ranging from global forecast models to regional specialized models, covering various resolutions, architectures, and forecasting capabilities to suit different computational and accuracy requirements.&lt;/p&gt; 
 &lt;p&gt;Available models include but are not limited to:&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model&lt;/th&gt; 
    &lt;th&gt;Resolution&lt;/th&gt; 
    &lt;th&gt;Architecture&lt;/th&gt; 
    &lt;th&gt;Time Step&lt;/th&gt; 
    &lt;th&gt;Coverage&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;GraphCast Small&lt;/td&gt; 
    &lt;td&gt;1.0¬∞&lt;/td&gt; 
    &lt;td&gt;Graph Neural Network&lt;/td&gt; 
    &lt;td&gt;6h&lt;/td&gt; 
    &lt;td&gt;Global&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;GraphCast Operational&lt;/td&gt; 
    &lt;td&gt;0.25¬∞&lt;/td&gt; 
    &lt;td&gt;Graph Neural Network&lt;/td&gt; 
    &lt;td&gt;6h&lt;/td&gt; 
    &lt;td&gt;Global&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Pangu 3hr&lt;/td&gt; 
    &lt;td&gt;0.25¬∞&lt;/td&gt; 
    &lt;td&gt;Transformer&lt;/td&gt; 
    &lt;td&gt;3h&lt;/td&gt; 
    &lt;td&gt;Global&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Pangu 6hr&lt;/td&gt; 
    &lt;td&gt;0.25¬∞&lt;/td&gt; 
    &lt;td&gt;Transformer&lt;/td&gt; 
    &lt;td&gt;6h&lt;/td&gt; 
    &lt;td&gt;Global&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Pangu 24hr&lt;/td&gt; 
    &lt;td&gt;0.25¬∞&lt;/td&gt; 
    &lt;td&gt;Transformer&lt;/td&gt; 
    &lt;td&gt;24h&lt;/td&gt; 
    &lt;td&gt;Global&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Aurora&lt;/td&gt; 
    &lt;td&gt;0.25¬∞&lt;/td&gt; 
    &lt;td&gt;Transformer&lt;/td&gt; 
    &lt;td&gt;6h&lt;/td&gt; 
    &lt;td&gt;Global&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;FuXi&lt;/td&gt; 
    &lt;td&gt;0.25¬∞&lt;/td&gt; 
    &lt;td&gt;Transformer&lt;/td&gt; 
    &lt;td&gt;6h&lt;/td&gt; 
    &lt;td&gt;Global&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;AIFS&lt;/td&gt; 
    &lt;td&gt;0.25¬∞&lt;/td&gt; 
    &lt;td&gt;Transformer&lt;/td&gt; 
    &lt;td&gt;6h&lt;/td&gt; 
    &lt;td&gt;Global&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;AIFS Ensemble&lt;/td&gt; 
    &lt;td&gt;0.25¬∞&lt;/td&gt; 
    &lt;td&gt;Transformer Ensemble&lt;/td&gt; 
    &lt;td&gt;6h&lt;/td&gt; 
    &lt;td&gt;Global&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;StormCast&lt;/td&gt; 
    &lt;td&gt;3km&lt;/td&gt; 
    &lt;td&gt;Diffusion + Regression&lt;/td&gt; 
    &lt;td&gt;1h&lt;/td&gt; 
    &lt;td&gt;Regional (US)&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;SFNO&lt;/td&gt; 
    &lt;td&gt;0.25¬∞&lt;/td&gt; 
    &lt;td&gt;Neural Operator&lt;/td&gt; 
    &lt;td&gt;6h&lt;/td&gt; 
    &lt;td&gt;Global&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;DLESyM&lt;/td&gt; 
    &lt;td&gt;0.25¬∞&lt;/td&gt; 
    &lt;td&gt;Convolutional&lt;/td&gt; 
    &lt;td&gt;6h&lt;/td&gt; 
    &lt;td&gt;Global&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;p&gt;For a complete list, see the &lt;a href="https://nvidia.github.io/earth2studio/modules/models.html#earth2studio-models-px-prognostic"&gt;prognostic model API docs&lt;/a&gt;.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Diagnostic Models&lt;/summary&gt; 
 &lt;p&gt;&lt;a href="https://nvidia.github.io/earth2studio/userguide/components/diagnostic.html"&gt;Diagnostic models&lt;/a&gt; in Earth2Studio perform time-independent transformations, typically taking geospatial fields at a specific time and predicting new derived quantities without performing time integration enabling users to build pipelines to predict specific quantities of interest that may not be provided by forecasting models.&lt;/p&gt; 
 &lt;p&gt;Earth2Studio contains a growing collection of specialized diagnostic models for various phenomena including precipitation prediction, tropical cyclone tracking, solar radiation estimation, wind gust forecasting, and more.&lt;/p&gt; 
 &lt;p&gt;Available diagnostics include but are not limited to:&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model&lt;/th&gt; 
    &lt;th&gt;Resolution&lt;/th&gt; 
    &lt;th&gt;Architecture&lt;/th&gt; 
    &lt;th&gt;Coverage&lt;/th&gt; 
    &lt;th&gt;Output&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;PrecipitationAFNO&lt;/td&gt; 
    &lt;td&gt;0.25¬∞&lt;/td&gt; 
    &lt;td&gt;Neural Operator&lt;/td&gt; 
    &lt;td&gt;Global&lt;/td&gt; 
    &lt;td&gt;Total precipitation&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;SolarRadiationAFNO1H&lt;/td&gt; 
    &lt;td&gt;0.25¬∞&lt;/td&gt; 
    &lt;td&gt;Neural Operator&lt;/td&gt; 
    &lt;td&gt;Global&lt;/td&gt; 
    &lt;td&gt;Surface solar radiation&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;WindgustAFNO&lt;/td&gt; 
    &lt;td&gt;0.25¬∞&lt;/td&gt; 
    &lt;td&gt;AFNO&lt;/td&gt; 
    &lt;td&gt;Global&lt;/td&gt; 
    &lt;td&gt;Maximum wind gust&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;TCTrackerVitart&lt;/td&gt; 
    &lt;td&gt;0.25¬∞&lt;/td&gt; 
    &lt;td&gt;Algorithmic&lt;/td&gt; 
    &lt;td&gt;Global&lt;/td&gt; 
    &lt;td&gt;TC tracks &amp;amp; properties&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CBottleInfill&lt;/td&gt; 
    &lt;td&gt;100km&lt;/td&gt; 
    &lt;td&gt;Diffusion&lt;/td&gt; 
    &lt;td&gt;Global&lt;/td&gt; 
    &lt;td&gt;Global climate sample&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CBottleSR&lt;/td&gt; 
    &lt;td&gt;5km&lt;/td&gt; 
    &lt;td&gt;Diffusion&lt;/td&gt; 
    &lt;td&gt;Regional / Global&lt;/td&gt; 
    &lt;td&gt;High-res climate&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CorrDiff&lt;/td&gt; 
    &lt;td&gt;Variable&lt;/td&gt; 
    &lt;td&gt;Diffusion&lt;/td&gt; 
    &lt;td&gt;Regional&lt;/td&gt; 
    &lt;td&gt;Fine-scale weather&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CorrDiffTaiwan&lt;/td&gt; 
    &lt;td&gt;2km&lt;/td&gt; 
    &lt;td&gt;Diffusion&lt;/td&gt; 
    &lt;td&gt;Regional (Taiwan)&lt;/td&gt; 
    &lt;td&gt;Taiwan fine-scale weather&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;p&gt;For a complete list, see the &lt;a href="https://nvidia.github.io/earth2studio/modules/models.html#earth2studio-models-dx-diagnostic"&gt;diagnostic model API docs&lt;/a&gt;.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Datasources&lt;/summary&gt; 
 &lt;p&gt;&lt;a href="https://nvidia.github.io/earth2studio/userguide/components/datasources.html"&gt;Data sources&lt;/a&gt; in Earth2Studio provide a standardized API for accessing weather and climate datasets from various providers (numerical models, data assimilation results, and AI-generated data), enabling seamless integration of initial conditions for model inference and validation data for scoring across different data formats and storage systems.&lt;/p&gt; 
 &lt;p&gt;Earth2Studio includes data sources ranging from operational weather models (GFS, HRRR, IFS) and reanalysis datasets (ERA5 via ARCO, CDS) to AI-generated climate data (cBottle) and local file systems. Fetching data is just plain easy, Earth2Studio handles the complicated parts giving the users an easy to use Xarray data array of requested data under a shared package wide &lt;a href="https://nvidia.github.io/earth2studio/userguide/advanced/lexicon.html"&gt;vocabulary&lt;/a&gt; and coordinate system.&lt;/p&gt; 
 &lt;p&gt;Available data sources include but are not limited to:&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Data Source&lt;/th&gt; 
    &lt;th&gt;Type&lt;/th&gt; 
    &lt;th&gt;Resolution&lt;/th&gt; 
    &lt;th&gt;Coverage&lt;/th&gt; 
    &lt;th&gt;Data Format&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;GFS&lt;/td&gt; 
    &lt;td&gt;Operational&lt;/td&gt; 
    &lt;td&gt;0.25¬∞&lt;/td&gt; 
    &lt;td&gt;Global&lt;/td&gt; 
    &lt;td&gt;GRIB2&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;GFS_FX&lt;/td&gt; 
    &lt;td&gt;Forecast&lt;/td&gt; 
    &lt;td&gt;0.25¬∞&lt;/td&gt; 
    &lt;td&gt;Global&lt;/td&gt; 
    &lt;td&gt;GRIB2&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;HRRR&lt;/td&gt; 
    &lt;td&gt;Operational&lt;/td&gt; 
    &lt;td&gt;3km&lt;/td&gt; 
    &lt;td&gt;Regional (US)&lt;/td&gt; 
    &lt;td&gt;GRIB2&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;HRRR_FX&lt;/td&gt; 
    &lt;td&gt;Forecast&lt;/td&gt; 
    &lt;td&gt;3km&lt;/td&gt; 
    &lt;td&gt;Regional (US)&lt;/td&gt; 
    &lt;td&gt;GRIB2&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;ARCO ERA5&lt;/td&gt; 
    &lt;td&gt;Reanalysis&lt;/td&gt; 
    &lt;td&gt;0.25¬∞&lt;/td&gt; 
    &lt;td&gt;Global&lt;/td&gt; 
    &lt;td&gt;Zarr&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CDS&lt;/td&gt; 
    &lt;td&gt;Reanalysis&lt;/td&gt; 
    &lt;td&gt;0.25¬∞&lt;/td&gt; 
    &lt;td&gt;Global&lt;/td&gt; 
    &lt;td&gt;NetCDF&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;IFS&lt;/td&gt; 
    &lt;td&gt;Operational&lt;/td&gt; 
    &lt;td&gt;0.25¬∞&lt;/td&gt; 
    &lt;td&gt;Global&lt;/td&gt; 
    &lt;td&gt;GRIB2&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;NCAR_ERA5&lt;/td&gt; 
    &lt;td&gt;Reanalysis&lt;/td&gt; 
    &lt;td&gt;0.25¬∞&lt;/td&gt; 
    &lt;td&gt;Global&lt;/td&gt; 
    &lt;td&gt;NetCDF&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;WeatherBench2&lt;/td&gt; 
    &lt;td&gt;Reanalysis&lt;/td&gt; 
    &lt;td&gt;0.25¬∞&lt;/td&gt; 
    &lt;td&gt;Global&lt;/td&gt; 
    &lt;td&gt;Zarr&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;GEFS_FX&lt;/td&gt; 
    &lt;td&gt;Ensemble Forecast&lt;/td&gt; 
    &lt;td&gt;0.25¬∞&lt;/td&gt; 
    &lt;td&gt;Global&lt;/td&gt; 
    &lt;td&gt;GRIB2&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;ISD&lt;/td&gt; 
    &lt;td&gt;Observational&lt;/td&gt; 
    &lt;td&gt;Point&lt;/td&gt; 
    &lt;td&gt;Regional (US)&lt;/td&gt; 
    &lt;td&gt;CSV&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;MRMS&lt;/td&gt; 
    &lt;td&gt;Reanalysis&lt;/td&gt; 
    &lt;td&gt;1km&lt;/td&gt; 
    &lt;td&gt;Regional (US)&lt;/td&gt; 
    &lt;td&gt;GRIB2&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;p&gt;For a complete list, see the &lt;a href="https://nvidia.github.io/earth2studio/modules/datasources.html"&gt;data source API docs&lt;/a&gt;.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;IO Backends&lt;/summary&gt; 
 &lt;p&gt;&lt;a href="https://nvidia.github.io/earth2studio/userguide/components/io.html"&gt;IO backends&lt;/a&gt; in Earth2Studio provides a standardized interface for writing and storing pipeline outputs across different file formats and storage systems enabling users to store inference outputs for later processing.&lt;/p&gt; 
 &lt;p&gt;Earth2Studio includes IO backends ranging from traditional scientific formats (NetCDF) and modern cloud-optimized formats (Zarr) to in-memory storage backends.&lt;/p&gt; 
 &lt;p&gt;Available IO backends include:&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;IO Backend&lt;/th&gt; 
    &lt;th&gt;Format&lt;/th&gt; 
    &lt;th&gt;Features&lt;/th&gt; 
    &lt;th&gt;Location&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;ZarrBackend&lt;/td&gt; 
    &lt;td&gt;Zarr&lt;/td&gt; 
    &lt;td&gt;Compression, Chunking&lt;/td&gt; 
    &lt;td&gt;In-Memory/Local&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;AsyncZarrBackend&lt;/td&gt; 
    &lt;td&gt;Zarr&lt;/td&gt; 
    &lt;td&gt;Async writes, Parallel I/O&lt;/td&gt; 
    &lt;td&gt;In-Memory/Local/Remote&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;NetCDF4Backend&lt;/td&gt; 
    &lt;td&gt;NetCDF4&lt;/td&gt; 
    &lt;td&gt;CF-compliant, Metadata&lt;/td&gt; 
    &lt;td&gt;In-Memory/Local&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;XarrayBackend&lt;/td&gt; 
    &lt;td&gt;Xarray Dataset&lt;/td&gt; 
    &lt;td&gt;Rich metadata, Analysis-ready&lt;/td&gt; 
    &lt;td&gt;In-Memory&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;KVBackend&lt;/td&gt; 
    &lt;td&gt;Key-Value&lt;/td&gt; 
    &lt;td&gt;Fast Temporary Access&lt;/td&gt; 
    &lt;td&gt;In-Memory&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;p&gt;For a complete list, see the &lt;a href="https://nvidia.github.io/earth2studio/modules/io.html"&gt;IO API docs&lt;/a&gt;.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Perturbation Methods&lt;/summary&gt; 
 &lt;p&gt;&lt;a href="https://nvidia.github.io/earth2studio/userguide/components/perturbation.html"&gt;Perturbation methods&lt;/a&gt; in Earth2Studio provide a standardized interface for adding noise to data arrays, typically enabling the creation of ensembling forecast pipelines that capture uncertainty in weather and climate predictions.&lt;/p&gt; 
 &lt;p&gt;Available perturbations include but are not limited to:&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Perturbation Method&lt;/th&gt; 
    &lt;th&gt;Type&lt;/th&gt; 
    &lt;th&gt;Spatial Correlation&lt;/th&gt; 
    &lt;th&gt;Temporal Correlation&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Gaussian&lt;/td&gt; 
    &lt;td&gt;Noise&lt;/td&gt; 
    &lt;td&gt;None&lt;/td&gt; 
    &lt;td&gt;None&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Correlated SphericalGaussian&lt;/td&gt; 
    &lt;td&gt;Noise&lt;/td&gt; 
    &lt;td&gt;Spherical&lt;/td&gt; 
    &lt;td&gt;AR(1) process&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Spherical Gaussian&lt;/td&gt; 
    &lt;td&gt;Noise&lt;/td&gt; 
    &lt;td&gt;Spherical (Matern)&lt;/td&gt; 
    &lt;td&gt;None&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Brown&lt;/td&gt; 
    &lt;td&gt;Noise&lt;/td&gt; 
    &lt;td&gt;2D Fourier&lt;/td&gt; 
    &lt;td&gt;None&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Bred Vector&lt;/td&gt; 
    &lt;td&gt;Dynamical&lt;/td&gt; 
    &lt;td&gt;Model-dependent&lt;/td&gt; 
    &lt;td&gt;Model-dependent&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Hemispheric Centred Bred Vector&lt;/td&gt; 
    &lt;td&gt;Dynamical&lt;/td&gt; 
    &lt;td&gt;Hemispheric&lt;/td&gt; 
    &lt;td&gt;Model-dependent&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;p&gt;For a complete list, see the &lt;a href="https://nvidia.github.io/earth2studio/userguide/components/perturbation.html"&gt;perturbations API docs&lt;/a&gt;.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Statistics / Metrics&lt;/summary&gt; 
 &lt;p&gt;&lt;a href="https://nvidia.github.io/earth2studio/userguide/components/statistics.html"&gt;Statistics and metrics&lt;/a&gt; in Earth2Studio provide operations typically useful for in-pipeline evaluation of forecast performance across different dimensions (spatial, temporal, ensemble) through various statistical measures including error metrics, correlation coefficients, and ensemble verification statistics.&lt;/p&gt; 
 &lt;p&gt;Available operations include but are not limited to:&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Statistic&lt;/th&gt; 
    &lt;th&gt;Type&lt;/th&gt; 
    &lt;th&gt;Application&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;RMSE&lt;/td&gt; 
    &lt;td&gt;Error Metric&lt;/td&gt; 
    &lt;td&gt;Forecast accuracy&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;ACC&lt;/td&gt; 
    &lt;td&gt;Correlation&lt;/td&gt; 
    &lt;td&gt;Pattern correlation&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CRPS&lt;/td&gt; 
    &lt;td&gt;Ensemble Metric&lt;/td&gt; 
    &lt;td&gt;Probabilistic skill&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Rank Histogram&lt;/td&gt; 
    &lt;td&gt;Ensemble Metric&lt;/td&gt; 
    &lt;td&gt;Ensemble reliability&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Standard Deviation&lt;/td&gt; 
    &lt;td&gt;Moment&lt;/td&gt; 
    &lt;td&gt;Spread measure&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Spread-Skill Ratio&lt;/td&gt; 
    &lt;td&gt;Ensemble Metric&lt;/td&gt; 
    &lt;td&gt;Ensemble calibration&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;p&gt;For a complete list, see the &lt;a href="https://nvidia.github.io/earth2studio/modules/statistics.html"&gt;statistics API docs&lt;/a&gt;.&lt;/p&gt; 
&lt;/details&gt; 
&lt;p&gt;For a more complete list of features, be sure to view the &lt;a href="https://nvidia.github.io/earth2studio/"&gt;documentation&lt;/a&gt;. Don't see what you need? Great news, extension and customization are at the heart of our &lt;a href="https://nvidia.github.io/earth2studio/examples/extend/index.html"&gt;design&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributors&lt;/h2&gt; 
&lt;p&gt;Check out the &lt;a href="https://raw.githubusercontent.com/NVIDIA/earth2studio/main/CONTRIBUTING.md"&gt;contributing&lt;/a&gt; document for details about the technical requirements and the userguide for higher level philosophy, structure, and design.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Earth2Studio is provided under the Apache License 2.0, please see the &lt;a href="https://raw.githubusercontent.com/NVIDIA/earth2studio/main/LICENSE"&gt;LICENSE file&lt;/a&gt; for full license text.&lt;/p&gt; 
&lt;!-- Badge links --&gt; 
&lt;!-- Doc links --&gt; 
&lt;!-- Misc links --&gt;</description>
    </item>
    
    <item>
      <title>NevaMind-AI/memU</title>
      <link>https://github.com/NevaMind-AI/memU</link>
      <description>&lt;p&gt;Memory for 24/7 proactive agents like moltbot (clawdbot).&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/banner.png" alt="MemU Banner" /&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;h1&gt;memU&lt;/h1&gt; 
 &lt;h3&gt;Always-On Proactive Memory for AI Agents&lt;/h3&gt; 
 &lt;p&gt;&lt;a href="https://badge.fury.io/py/memu-py"&gt;&lt;img src="https://badge.fury.io/py/memu-py.svg?sanitize=true" alt="PyPI version" /&gt;&lt;/a&gt; &lt;a href="https://opensource.org/licenses/Apache-2.0"&gt;&lt;img src="https://img.shields.io/badge/License-Apache%202.0-blue.svg?sanitize=true" alt="License: Apache 2.0" /&gt;&lt;/a&gt; &lt;a href="https://www.python.org/downloads/"&gt;&lt;img src="https://img.shields.io/badge/python-3.13+-blue.svg?sanitize=true" alt="Python 3.13+" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/memu"&gt;&lt;img src="https://img.shields.io/badge/Discord-Join%20Chat-5865F2?logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://x.com/memU_ai"&gt;&lt;img src="https://img.shields.io/badge/Twitter-Follow-1DA1F2?logo=x&amp;amp;logoColor=white" alt="Twitter" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/17374" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/17374" alt="NevaMind-AI%2FmemU | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/NevaMind-AI/memU/main/readme/README_en.md"&gt;English&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/NevaMind-AI/memU/main/readme/README_zh.md"&gt;‰∏≠Êñá&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/NevaMind-AI/memU/main/readme/README_ja.md"&gt;Êó•Êú¨Ë™û&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/NevaMind-AI/memU/main/readme/README_ko.md"&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/NevaMind-AI/memU/main/readme/README_es.md"&gt;Espa√±ol&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/NevaMind-AI/memU/main/readme/README_fr.md"&gt;Fran√ßais&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;p&gt;memU is a memory framework built for &lt;strong&gt;24/7 proactive agents&lt;/strong&gt;. It is designed for long-running use and greatly &lt;strong&gt;reduces the LLM token cost&lt;/strong&gt; of keeping agents always online, making always-on, evolving agents practical in production systems. memU &lt;strong&gt;continuously captures and understands user intent&lt;/strong&gt;. Even without a command, the agent can tell what you are about to do and act on it by itself.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;‚≠êÔ∏è Star the repository&lt;/h2&gt; 
&lt;img width="100%" src="https://github.com/NevaMind-AI/memU/raw/main/assets/star.gif" /&gt; If you find memU useful or interesting, a GitHub Star ‚≠êÔ∏è would be greatly appreciated. 
&lt;hr /&gt; 
&lt;h2&gt;‚ú® Core Features&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Capability&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ü§ñ &lt;strong&gt;24/7 Proactive Agent&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Always-on memory agent that works continuously in the background‚Äînever sleeps, never forgets&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üéØ &lt;strong&gt;User Intention Capture&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Understands and remembers user goals, preferences, and context across sessions automatically&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;üí∞ &lt;strong&gt;Cost Efficient&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Reduces long-running token costs by caching insights and avoiding redundant LLM calls&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üîÑ How Proactive Memory Works&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;
cd examples/proactive
python proactive.py

&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Proactive Memory Lifecycle&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  1. USER INITIAL QUERY                          ‚îÇ
‚îÇ  ‚îî‚îÄ User input, context, or any trigger event   ‚îÇ
‚îÇ     Conversation starts here                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  2. AGENT PLANNING / ACTIONS                    ‚îÇ
    ‚îÇ  ‚îî‚îÄ Analyze request, execute tasks              ‚îÇ
    ‚îÇ     Retrieve relevant memories for context      ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  3. MEMORIZE &amp;amp; UPDATE TODOLIST                  ‚îÇ
    ‚îÇ  ‚îî‚îÄ Store new insights, facts, preferences      ‚îÇ
    ‚îÇ     Modify task list based on progress          ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  4. PREDICT USER INTENT                         ‚îÇ
    ‚îÇ  ‚îî‚îÄ Anticipate next steps and needs             ‚îÇ
    ‚îÇ     Proactively prepare relevant context        ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  5. LOOP (2 ‚Üí 4)                                ‚îÇ
    ‚îÇ  ‚îî‚îÄ Continuous iteration until task complete    ‚îÇ
    ‚îÇ     Agent-driven proactive workflow             ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üéØ Proactive Use Cases&lt;/h2&gt; 
&lt;h3&gt;1. &lt;strong&gt;Information Recommendation&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;em&gt;Agent monitors interests and proactively surfaces relevant content&lt;/em&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# User has been researching AI topics
MemU tracks: reading history, saved articles, search queries

# When new content arrives:
Agent: "I found 3 new papers on RAG optimization that align with
        your recent research on retrieval systems. One author
        (Dr. Chen) you've cited before published yesterday."

# Proactive behaviors:
- Learns topic preferences from browsing patterns
- Tracks author/source credibility preferences
- Filters noise based on engagement history
- Times recommendations for optimal attention
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. &lt;strong&gt;Email Management&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;em&gt;Agent learns communication patterns and handles routine correspondence&lt;/em&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# MemU observes email patterns over time:
- Response templates for common scenarios
- Priority contacts and urgent keywords
- Scheduling preferences and availability
- Writing style and tone variations

# Proactive email assistance:
Agent: "You have 12 new emails. I've drafted responses for 3 routine
        requests and flagged 2 urgent items from your priority contacts.
        Should I also reschedule tomorrow's meeting based on the
        conflict John mentioned?"

# Autonomous actions:
‚úì Draft context-aware replies
‚úì Categorize and prioritize inbox
‚úì Detect scheduling conflicts
‚úì Summarize long threads with key decisions
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;3. &lt;strong&gt;Trading &amp;amp; Financial Monitoring&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;em&gt;Agent tracks market context and user investment behavior&lt;/em&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# MemU learns trading preferences:
- Risk tolerance from historical decisions
- Preferred sectors and asset classes
- Response patterns to market events
- Portfolio rebalancing triggers

# Proactive alerts:
Agent: "NVDA dropped 5% in after-hours trading. Based on your past
        behavior, you typically buy tech dips above 3%. Your current
        allocation allows for $2,000 additional exposure while
        maintaining your 70/30 equity-bond target."

# Continuous monitoring:
- Track price alerts tied to user-defined thresholds
- Correlate news events with portfolio impact
- Learn from executed vs. ignored recommendations
- Anticipate tax-loss harvesting opportunities
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;...&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üóÇÔ∏è Hierarchical Memory Architecture&lt;/h2&gt; 
&lt;p&gt;MemU's three-layer system enables both &lt;strong&gt;reactive queries&lt;/strong&gt; and &lt;strong&gt;proactive context loading&lt;/strong&gt;:&lt;/p&gt; 
&lt;img width="100%" alt="structure" src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/structure.png" /&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Layer&lt;/th&gt; 
   &lt;th&gt;Reactive Use&lt;/th&gt; 
   &lt;th&gt;Proactive Use&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Resource&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Direct access to original data&lt;/td&gt; 
   &lt;td&gt;Background monitoring for new patterns&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Item&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Targeted fact retrieval&lt;/td&gt; 
   &lt;td&gt;Real-time extraction from ongoing interactions&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Category&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Summary-level overview&lt;/td&gt; 
   &lt;td&gt;Automatic context assembly for anticipation&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Proactive Benefits:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Auto-categorization&lt;/strong&gt;: New memories self-organize into topics&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Pattern Detection&lt;/strong&gt;: System identifies recurring themes&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Context Prediction&lt;/strong&gt;: Anticipates what information will be needed next&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üöÄ Quick Start&lt;/h2&gt; 
&lt;h3&gt;Option 1: Cloud Version&lt;/h3&gt; 
&lt;p&gt;Experience proactive memory instantly:&lt;/p&gt; 
&lt;p&gt;üëâ &lt;strong&gt;&lt;a href="https://memu.so"&gt;memu.so&lt;/a&gt;&lt;/strong&gt; - Hosted service with 7√ó24 continuous learning&lt;/p&gt; 
&lt;p&gt;For enterprise deployment with custom proactive workflows, contact &lt;strong&gt;&lt;a href="mailto:info@nevamind.ai"&gt;info@nevamind.ai&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h4&gt;Cloud API (v3)&lt;/h4&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Base URL&lt;/th&gt; 
   &lt;th&gt;&lt;code&gt;https://api.memu.so&lt;/code&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Auth&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;Authorization: Bearer YOUR_API_KEY&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Method&lt;/th&gt; 
   &lt;th&gt;Endpoint&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;POST&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;/api/v3/memory/memorize&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Register continuous learning task&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;GET&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;/api/v3/memory/memorize/status/{task_id}&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Check real-time processing status&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;POST&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;/api/v3/memory/categories&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;List auto-generated categories&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;POST&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;/api/v3/memory/retrieve&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Query memory (supports proactive context loading)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;üìö &lt;strong&gt;&lt;a href="https://memu.pro/docs#cloud-version"&gt;Full API Documentation&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Option 2: Self-Hosted&lt;/h3&gt; 
&lt;h4&gt;Installation&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Basic Example&lt;/h4&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Requirements&lt;/strong&gt;: Python 3.13+ and an OpenAI API key&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;Test Continuous Learning&lt;/strong&gt; (in-memory):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export OPENAI_API_KEY=your_api_key
cd tests
python test_inmemory.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Test with Persistent Storage&lt;/strong&gt; (PostgreSQL):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Start PostgreSQL with pgvector
docker run -d \
  --name memu-postgres \
  -e POSTGRES_USER=postgres \
  -e POSTGRES_PASSWORD=postgres \
  -e POSTGRES_DB=memu \
  -p 5432:5432 \
  pgvector/pgvector:pg16

# Run continuous learning test
export OPENAI_API_KEY=your_api_key
cd tests
python test_postgres.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Both examples demonstrate &lt;strong&gt;proactive memory workflows&lt;/strong&gt;:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Continuous Ingestion&lt;/strong&gt;: Process multiple files sequentially&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Auto-Extraction&lt;/strong&gt;: Immediate memory creation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Proactive Retrieval&lt;/strong&gt;: Context-aware memory surfacing&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/NevaMind-AI/memU/main/tests/test_inmemory.py"&gt;&lt;code&gt;tests/test_inmemory.py&lt;/code&gt;&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/NevaMind-AI/memU/main/tests/test_postgres.py"&gt;&lt;code&gt;tests/test_postgres.py&lt;/code&gt;&lt;/a&gt; for implementation details.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Custom LLM and Embedding Providers&lt;/h3&gt; 
&lt;p&gt;MemU supports custom LLM and embedding providers beyond OpenAI. Configure them via &lt;code&gt;llm_profiles&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from memu import MemUService

service = MemUService(
    llm_profiles={
        # Default profile for LLM operations
        "default": {
            "base_url": "https://dashscope.aliyuncs.com/compatible-mode/v1",
            "api_key": "your_api_key",
            "chat_model": "qwen3-max",
            "client_backend": "sdk"  # "sdk" or "http"
        },
        # Separate profile for embeddings
        "embedding": {
            "base_url": "https://api.voyageai.com/v1",
            "api_key": "your_voyage_api_key",
            "embed_model": "voyage-3.5-lite"
        }
    },
    # ... other configuration
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h3&gt;OpenRouter Integration&lt;/h3&gt; 
&lt;p&gt;MemU supports &lt;a href="https://openrouter.ai"&gt;OpenRouter&lt;/a&gt; as a model provider, giving you access to multiple LLM providers through a single API.&lt;/p&gt; 
&lt;h4&gt;Configuration&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from memu import MemoryService

service = MemoryService(
    llm_profiles={
        "default": {
            "provider": "openrouter",
            "client_backend": "httpx",
            "base_url": "https://openrouter.ai",
            "api_key": "your_openrouter_api_key",
            "chat_model": "anthropic/claude-3.5-sonnet",  # Any OpenRouter model
            "embed_model": "openai/text-embedding-3-small",  # Embedding model
        },
    },
    database_config={
        "metadata_store": {"provider": "inmemory"},
    },
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Environment Variables&lt;/h4&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Variable&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;OPENROUTER_API_KEY&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Your OpenRouter API key from &lt;a href="https://openrouter.ai/keys"&gt;openrouter.ai/keys&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h4&gt;Supported Features&lt;/h4&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Feature&lt;/th&gt; 
   &lt;th&gt;Status&lt;/th&gt; 
   &lt;th&gt;Notes&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Chat Completions&lt;/td&gt; 
   &lt;td&gt;Supported&lt;/td&gt; 
   &lt;td&gt;Works with any OpenRouter chat model&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Embeddings&lt;/td&gt; 
   &lt;td&gt;Supported&lt;/td&gt; 
   &lt;td&gt;Use OpenAI embedding models via OpenRouter&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Vision&lt;/td&gt; 
   &lt;td&gt;Supported&lt;/td&gt; 
   &lt;td&gt;Use vision-capable models (e.g., &lt;code&gt;openai/gpt-4o&lt;/code&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h4&gt;Running OpenRouter Tests&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export OPENROUTER_API_KEY=your_api_key

# Full workflow test (memorize + retrieve)
python tests/test_openrouter.py

# Embedding-specific tests
python tests/test_openrouter_embedding.py

# Vision-specific tests
python tests/test_openrouter_vision.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/NevaMind-AI/memU/main/examples/example_4_openrouter_memory.py"&gt;&lt;code&gt;examples/example_4_openrouter_memory.py&lt;/code&gt;&lt;/a&gt; for a complete working example.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üìñ Core APIs&lt;/h2&gt; 
&lt;h3&gt;&lt;code&gt;memorize()&lt;/code&gt; - Continuous Learning Pipeline&lt;/h3&gt; 
&lt;p&gt;Processes inputs in real-time and immediately updates memory:&lt;/p&gt; 
&lt;img width="100%" alt="memorize" src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/memorize.png" /&gt; ```python result = await service.memorize( resource_url="path/to/file.json", # File path or URL modality="conversation", # conversation | document | image | video | audio user={"user_id": "123"} # Optional: scope to a user ) 
&lt;h1&gt;Returns immediately with extracted memory:&lt;/h1&gt; 
&lt;p&gt;{ "resource": {...}, # Stored resource metadata "items": [...], # Extracted memory items (available instantly) "categories": [...] # Auto-updated category structure }&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;
**Proactive Features:**
- Zero-delay processing‚Äîmemories available immediately
- Automatic categorization without manual tagging
- Cross-reference with existing memories for pattern detection

### `retrieve()` - Dual-Mode Intelligence

MemU supports both **proactive context loading** and **reactive querying**:

&amp;lt;img width="100%" alt="retrieve" src="assets/retrieve.png" /&amp;gt;

#### RAG-based Retrieval (`method="rag"`)

Fast **proactive context assembly** using embeddings:

- ‚úÖ **Instant context**: Sub-second memory surfacing
- ‚úÖ **Background monitoring**: Can run continuously without LLM costs
- ‚úÖ **Similarity scoring**: Identifies most relevant memories automatically

#### LLM-based Retrieval (`method="llm"`)

Deep **anticipatory reasoning** for complex contexts:

- ‚úÖ **Intent prediction**: LLM infers what user needs before they ask
- ‚úÖ **Query evolution**: Automatically refines search as context develops
- ‚úÖ **Early termination**: Stops when sufficient context is gathered

#### Comparison

| Aspect | RAG (Fast Context) | LLM (Deep Reasoning) |
|--------|-------------------|---------------------|
| **Speed** | ‚ö° Milliseconds | üê¢ Seconds |
| **Cost** | üí∞ Embedding only | üí∞üí∞ LLM inference |
| **Proactive use** | Continuous monitoring | Triggered context loading |
| **Best for** | Real-time suggestions | Complex anticipation |

#### Usage
```python
# Proactive retrieval with context history
result = await service.retrieve(
    queries=[
        {"role": "user", "content": {"text": "What are their preferences?"}},
        {"role": "user", "content": {"text": "Tell me about work habits"}}
    ],
    where={"user_id": "123"},  # Optional: scope filter
    method="rag"  # or "llm" for deeper reasoning
)

# Returns context-aware results:
{
    "categories": [...],     # Relevant topic areas (auto-prioritized)
    "items": [...],          # Specific memory facts
    "resources": [...],      # Original sources for traceability
    "next_step_query": "..." # Predicted follow-up context
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Proactive Filtering&lt;/strong&gt;: Use &lt;code&gt;where&lt;/code&gt; to scope continuous monitoring:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;where={"user_id": "123"}&lt;/code&gt; - User-specific context&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;where={"agent_id__in": ["1", "2"]}&lt;/code&gt; - Multi-agent coordination&lt;/li&gt; 
 &lt;li&gt;Omit &lt;code&gt;where&lt;/code&gt; for global context awareness&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;üìö &lt;strong&gt;For complete API documentation&lt;/strong&gt;, see &lt;a href="https://raw.githubusercontent.com/NevaMind-AI/memU/main/docs/SERVICE_API.md"&gt;SERVICE_API.md&lt;/a&gt; - includes proactive workflow patterns, pipeline configuration, and real-time update handling.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üí° Proactive Scenarios&lt;/h2&gt; 
&lt;h3&gt;Example 1: Always-Learning Assistant&lt;/h3&gt; 
&lt;p&gt;Continuously learns from every interaction without explicit memory commands:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export OPENAI_API_KEY=your_api_key
python examples/example_1_conversation_memory.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Proactive Behavior:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Automatically extracts preferences from casual mentions&lt;/li&gt; 
 &lt;li&gt;Builds relationship models from interaction patterns&lt;/li&gt; 
 &lt;li&gt;Surfaces relevant context in future conversations&lt;/li&gt; 
 &lt;li&gt;Adapts communication style based on learned preferences&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Best for:&lt;/strong&gt; Personal AI assistants, customer support that remembers, social chatbots&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Example 2: Self-Improving Agent&lt;/h3&gt; 
&lt;p&gt;Learns from execution logs and proactively suggests optimizations:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export OPENAI_API_KEY=your_api_key
python examples/example_2_skill_extraction.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Proactive Behavior:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Monitors agent actions and outcomes continuously&lt;/li&gt; 
 &lt;li&gt;Identifies patterns in successes and failures&lt;/li&gt; 
 &lt;li&gt;Auto-generates skill guides from experience&lt;/li&gt; 
 &lt;li&gt;Proactively suggests strategies for similar future tasks&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Best for:&lt;/strong&gt; DevOps automation, agent self-improvement, knowledge capture&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Example 3: Multimodal Context Builder&lt;/h3&gt; 
&lt;p&gt;Unifies memory across different input types for comprehensive context:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export OPENAI_API_KEY=your_api_key
python examples/example_3_multimodal_memory.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Proactive Behavior:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Cross-references text, images, and documents automatically&lt;/li&gt; 
 &lt;li&gt;Builds unified understanding across modalities&lt;/li&gt; 
 &lt;li&gt;Surfaces visual context when discussing related topics&lt;/li&gt; 
 &lt;li&gt;Anticipates information needs by combining multiple sources&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Best for:&lt;/strong&gt; Documentation systems, learning platforms, research assistants&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üìä Performance&lt;/h2&gt; 
&lt;p&gt;MemU achieves &lt;strong&gt;92.09% average accuracy&lt;/strong&gt; on the Locomo benchmark across all reasoning tasks, demonstrating reliable proactive memory operations.&lt;/p&gt; 
&lt;img width="100%" alt="benchmark" src="https://github.com/user-attachments/assets/6fec4884-94e5-4058-ad5c-baac3d7e76d9" /&gt; 
&lt;p&gt;View detailed experimental data: &lt;a href="https://github.com/NevaMind-AI/memU-experiment"&gt;memU-experiment&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üß© Ecosystem&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Repository&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Proactive Features&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;&lt;a href="https://github.com/NevaMind-AI/memU"&gt;memU&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Core proactive memory engine&lt;/td&gt; 
   &lt;td&gt;7√ó24 learning pipeline, auto-categorization&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;&lt;a href="https://github.com/NevaMind-AI/memU-server"&gt;memU-server&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Backend with continuous sync&lt;/td&gt; 
   &lt;td&gt;Real-time memory updates, webhook triggers&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;&lt;a href="https://github.com/NevaMind-AI/memU-ui"&gt;memU-ui&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Visual memory dashboard&lt;/td&gt; 
   &lt;td&gt;Live memory evolution monitoring&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Quick Links:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üöÄ &lt;a href="https://app.memu.so/quick-start"&gt;Try MemU Cloud&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üìö &lt;a href="https://memu.pro/docs"&gt;API Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üí¨ &lt;a href="https://discord.gg/memu"&gt;Discord Community&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ü§ù Partners&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/TEN-framework/ten-framework"&gt;&lt;img src="https://avatars.githubusercontent.com/u/113095513?s=200&amp;amp;v=4" alt="Ten" height="40" style="margin: 10px;" /&gt;&lt;/a&gt; &lt;a href="https://openagents.org"&gt;&lt;img src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/partners/openagents.png" alt="OpenAgents" height="40" style="margin: 10px;" /&gt;&lt;/a&gt; &lt;a href="https://github.com/milvus-io/milvus"&gt;&lt;img src="https://miro.medium.com/v2/resize:fit:2400/1*-VEGyAgcIBD62XtZWavy8w.png" alt="Milvus" height="40" style="margin: 10px;" /&gt;&lt;/a&gt; &lt;a href="https://xroute.ai/"&gt;&lt;img src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/partners/xroute.png" alt="xRoute" height="40" style="margin: 10px;" /&gt;&lt;/a&gt; &lt;a href="https://jaaz.app/"&gt;&lt;img src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/partners/jazz.png" alt="Jazz" height="40" style="margin: 10px;" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Buddie-AI/Buddie"&gt;&lt;img src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/partners/buddie.png" alt="Buddie" height="40" style="margin: 10px;" /&gt;&lt;/a&gt; &lt;a href="https://github.com/bytebase/bytebase"&gt;&lt;img src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/partners/bytebase.png" alt="Bytebase" height="40" style="margin: 10px;" /&gt;&lt;/a&gt; &lt;a href="https://github.com/LazyAGI/LazyLLM"&gt;&lt;img src="https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/partners/LazyLLM.png" alt="LazyLLM" height="40" style="margin: 10px;" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ü§ù How to Contribute&lt;/h2&gt; 
&lt;p&gt;We welcome contributions from the community! Whether you're fixing bugs, adding features, or improving documentation, your help is appreciated.&lt;/p&gt; 
&lt;h3&gt;Getting Started&lt;/h3&gt; 
&lt;p&gt;To start contributing to MemU, you'll need to set up your development environment:&lt;/p&gt; 
&lt;h4&gt;Prerequisites&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.13+&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/astral-sh/uv"&gt;uv&lt;/a&gt; (Python package manager)&lt;/li&gt; 
 &lt;li&gt;Git&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Setup Development Environment&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 1. Fork and clone the repository
git clone https://github.com/YOUR_USERNAME/memU.git
cd memU

# 2. Install development dependencies
make install
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The &lt;code&gt;make install&lt;/code&gt; command will:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Create a virtual environment using &lt;code&gt;uv&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Install all project dependencies&lt;/li&gt; 
 &lt;li&gt;Set up pre-commit hooks for code quality checks&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Running Quality Checks&lt;/h4&gt; 
&lt;p&gt;Before submitting your contribution, ensure your code passes all quality checks:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;make check
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The &lt;code&gt;make check&lt;/code&gt; command runs:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Lock file verification&lt;/strong&gt;: Ensures &lt;code&gt;pyproject.toml&lt;/code&gt; consistency&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Pre-commit hooks&lt;/strong&gt;: Lints code with Ruff, formats with Black&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Type checking&lt;/strong&gt;: Runs &lt;code&gt;mypy&lt;/code&gt; for static type analysis&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Dependency analysis&lt;/strong&gt;: Uses &lt;code&gt;deptry&lt;/code&gt; to find obsolete dependencies&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Contributing Guidelines&lt;/h3&gt; 
&lt;p&gt;For detailed contribution guidelines, code standards, and development practices, please see &lt;a href="https://raw.githubusercontent.com/NevaMind-AI/memU/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Quick tips:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Create a new branch for each feature or bug fix&lt;/li&gt; 
 &lt;li&gt;Write clear commit messages&lt;/li&gt; 
 &lt;li&gt;Add tests for new functionality&lt;/li&gt; 
 &lt;li&gt;Update documentation as needed&lt;/li&gt; 
 &lt;li&gt;Run &lt;code&gt;make check&lt;/code&gt; before pushing&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üìÑ License&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/NevaMind-AI/memU/main/LICENSE.txt"&gt;Apache License 2.0&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üåç Community&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;GitHub Issues&lt;/strong&gt;: &lt;a href="https://github.com/NevaMind-AI/memU/issues"&gt;Report bugs &amp;amp; request features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Discord&lt;/strong&gt;: &lt;a href="https://discord.com/invite/hQZntfGsbJ"&gt;Join the community&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;X (Twitter)&lt;/strong&gt;: &lt;a href="https://x.com/memU_ai"&gt;Follow @memU_ai&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Contact&lt;/strong&gt;: &lt;a href="mailto:info@nevamind.ai"&gt;info@nevamind.ai&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;‚≠ê &lt;strong&gt;Star us on GitHub&lt;/strong&gt; to get notified about new releases!&lt;/p&gt; 
&lt;/div&gt;</description>
    </item>
    
  </channel>
</rss>