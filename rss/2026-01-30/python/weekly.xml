<rss version="2.0">
  <channel>
    <title>GitHub Python Weekly Trending</title>
    <description>Weekly Trending of Python in GitHub</description>
    <pubDate>Thu, 29 Jan 2026 01:55:18 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>MoonshotAI/kimi-cli</title>
      <link>https://github.com/MoonshotAI/kimi-cli</link>
      <description>&lt;p&gt;Kimi Code CLI is your next CLI agent.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Kimi Code CLI&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/MoonshotAI/kimi-cli/graphs/commit-activity"&gt;&lt;img src="https://img.shields.io/github/commit-activity/w/MoonshotAI/kimi-cli" alt="Commit Activity" /&gt;&lt;/a&gt; &lt;a href="https://github.com/MoonshotAI/kimi-cli/actions"&gt;&lt;img src="https://img.shields.io/github/check-runs/MoonshotAI/kimi-cli/main" alt="Checks" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/kimi-cli/"&gt;&lt;img src="https://img.shields.io/pypi/v/kimi-cli" alt="Version" /&gt;&lt;/a&gt; &lt;a href="https://pypistats.org/packages/kimi-cli"&gt;&lt;img src="https://img.shields.io/pypi/dw/kimi-cli" alt="Downloads" /&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/MoonshotAI/kimi-cli"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.kimi.com/code/"&gt;Kimi Code&lt;/a&gt; | &lt;a href="https://moonshotai.github.io/kimi-cli/en/"&gt;Documentation&lt;/a&gt; | &lt;a href="https://moonshotai.github.io/kimi-cli/zh/"&gt;ÊñáÊ°£&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Kimi Code CLI is an AI agent that runs in the terminal, helping you complete software development tasks and terminal operations. It can read and edit code, execute shell commands, search and fetch web pages, and autonomously plan and adjust actions during execution.&lt;/p&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://moonshotai.github.io/kimi-cli/en/guides/getting-started.html"&gt;Getting Started&lt;/a&gt; for how to install and start using Kimi Code CLI.&lt;/p&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;h3&gt;Shell command mode&lt;/h3&gt; 
&lt;p&gt;Kimi Code CLI is not only a coding agent, but also a shell. You can switch the shell command mode by pressing &lt;code&gt;Ctrl-X&lt;/code&gt;. In this mode, you can directly run shell commands without leaving Kimi Code CLI.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/MoonshotAI/kimi-cli/main/docs/media/shell-mode.gif" alt="" /&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Built-in shell commands like &lt;code&gt;cd&lt;/code&gt; are not supported yet.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;IDE integration via ACP&lt;/h3&gt; 
&lt;p&gt;Kimi Code CLI supports &lt;a href="https://github.com/agentclientprotocol/agent-client-protocol"&gt;Agent Client Protocol&lt;/a&gt; out of the box. You can use it together with any ACP-compatible editor or IDE.&lt;/p&gt; 
&lt;p&gt;To use Kimi Code CLI with ACP clients, make sure to run Kimi Code CLI in the terminal and send &lt;code&gt;/login&lt;/code&gt; to complete the login first. Then, you can configure your ACP client to start Kimi Code CLI as an ACP agent server with command &lt;code&gt;kimi acp&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;For example, to use Kimi Code CLI with &lt;a href="https://zed.dev/"&gt;Zed&lt;/a&gt; or &lt;a href="https://blog.jetbrains.com/ai/2025/12/bring-your-own-ai-agent-to-jetbrains-ides/"&gt;JetBrains&lt;/a&gt;, add the following configuration to your &lt;code&gt;~/.config/zed/settings.json&lt;/code&gt; or &lt;code&gt;~/.jetbrains/acp.json&lt;/code&gt; file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "agent_servers": {
    "Kimi Code CLI": {
      "command": "kimi",
      "args": ["acp"],
      "env": {}
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then you can create Kimi Code CLI threads in IDE's agent panel.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/MoonshotAI/kimi-cli/main/docs/media/acp-integration.gif" alt="" /&gt;&lt;/p&gt; 
&lt;h3&gt;Zsh integration&lt;/h3&gt; 
&lt;p&gt;You can use Kimi Code CLI together with Zsh, to empower your shell experience with AI agent capabilities.&lt;/p&gt; 
&lt;p&gt;Install the &lt;a href="https://github.com/MoonshotAI/zsh-kimi-cli"&gt;zsh-kimi-cli&lt;/a&gt; plugin via:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;git clone https://github.com/MoonshotAI/zsh-kimi-cli.git \
  ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/kimi-cli
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] If you are using a plugin manager other than Oh My Zsh, you may need to refer to the plugin's README for installation instructions.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Then add &lt;code&gt;kimi-cli&lt;/code&gt; to your Zsh plugin list in &lt;code&gt;~/.zshrc&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;plugins=(... kimi-cli)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After restarting Zsh, you can switch to agent mode by pressing &lt;code&gt;Ctrl-X&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;MCP support&lt;/h3&gt; 
&lt;p&gt;Kimi Code CLI supports MCP (Model Context Protocol) tools.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;code&gt;kimi mcp&lt;/code&gt; sub-command group&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;You can manage MCP servers with &lt;code&gt;kimi mcp&lt;/code&gt; sub-command group. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Add streamable HTTP server:
kimi mcp add --transport http context7 https://mcp.context7.com/mcp --header "CONTEXT7_API_KEY: ctx7sk-your-key"

# Add streamable HTTP server with OAuth authorization:
kimi mcp add --transport http --auth oauth linear https://mcp.linear.app/mcp

# Add stdio server:
kimi mcp add --transport stdio chrome-devtools -- npx chrome-devtools-mcp@latest

# List added MCP servers:
kimi mcp list

# Remove an MCP server:
kimi mcp remove chrome-devtools

# Authorize an MCP server:
kimi mcp auth linear
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Ad-hoc MCP configuration&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Kimi Code CLI also supports ad-hoc MCP server configuration via CLI option.&lt;/p&gt; 
&lt;p&gt;Given an MCP config file in the well-known MCP config format like the following:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "mcpServers": {
    "context7": {
      "url": "https://mcp.context7.com/mcp",
      "headers": {
        "CONTEXT7_API_KEY": "YOUR_API_KEY"
      }
    },
    "chrome-devtools": {
      "command": "npx",
      "args": ["-y", "chrome-devtools-mcp@latest"]
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Run &lt;code&gt;kimi&lt;/code&gt; with &lt;code&gt;--mcp-config-file&lt;/code&gt; option to connect to the specified MCP servers:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;kimi --mcp-config-file /path/to/mcp.json
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;More&lt;/h3&gt; 
&lt;p&gt;See more features in the &lt;a href="https://moonshotai.github.io/kimi-cli/en/"&gt;Documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Development&lt;/h2&gt; 
&lt;p&gt;To develop Kimi Code CLI, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;git clone https://github.com/MoonshotAI/kimi-cli.git
cd kimi-cli

make prepare  # prepare the development environment
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then you can start working on Kimi Code CLI.&lt;/p&gt; 
&lt;p&gt;Refer to the following commands after you make changes:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;uv run kimi  # run Kimi Code CLI

make format  # format code
make check  # run linting and type checking
make test  # run tests
make test-kimi-cli  # run Kimi Code CLI tests only
make test-kosong  # run kosong tests only
make test-pykaos  # run pykaos tests only
make build  # build python packages
make build-bin  # build standalone binary
make help  # show all make targets
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions to Kimi Code CLI! Please refer to &lt;a href="https://raw.githubusercontent.com/MoonshotAI/kimi-cli/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for more information.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>VectifyAI/PageIndex</title>
      <link>https://github.com/VectifyAI/PageIndex</link>
      <description>&lt;p&gt;üìë PageIndex: Document Index for Vectorless, Reasoning-based RAG&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;a href="https://vectify.ai/pageindex" target="_blank"&gt; &lt;img src="https://github.com/user-attachments/assets/46201e72-675b-43bc-bfbd-081cc6b65a1d" alt="PageIndex Banner" /&gt; &lt;/a&gt; 
 &lt;br /&gt; 
 &lt;br /&gt; 
 &lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/14736" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/14736" alt="VectifyAI%2FPageIndex | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; &lt;/p&gt; 
 &lt;h1&gt;PageIndex: Vectorless, Reasoning-based RAG&lt;/h1&gt; 
 &lt;p align="center"&gt;&lt;b&gt;Reasoning-based RAG&amp;nbsp; ‚ó¶ &amp;nbsp;No Vector DB&amp;nbsp; ‚ó¶ &amp;nbsp;No Chunking&amp;nbsp; ‚ó¶ &amp;nbsp;Human-like Retrieval&lt;/b&gt;&lt;/p&gt; 
 &lt;h4 align="center"&gt; &lt;a href="https://vectify.ai"&gt;üè† Homepage&lt;/a&gt;&amp;nbsp; ‚Ä¢ &amp;nbsp; &lt;a href="https://chat.pageindex.ai"&gt;üñ•Ô∏è Chat Platform&lt;/a&gt;&amp;nbsp; ‚Ä¢ &amp;nbsp; &lt;a href="https://pageindex.ai/mcp"&gt;üîå MCP&lt;/a&gt;&amp;nbsp; ‚Ä¢ &amp;nbsp; &lt;a href="https://docs.pageindex.ai"&gt;üìö Docs&lt;/a&gt;&amp;nbsp; ‚Ä¢ &amp;nbsp; &lt;a href="https://discord.com/invite/VuXuf29EUj"&gt;üí¨ Discord&lt;/a&gt;&amp;nbsp; ‚Ä¢ &amp;nbsp; &lt;a href="https://ii2abc2jejf.typeform.com/to/tK3AXl8T"&gt;‚úâÔ∏è Contact&lt;/a&gt;&amp;nbsp; &lt;/h4&gt; 
&lt;/div&gt; 
&lt;details open&gt; 
 &lt;summary&gt;&lt;h3&gt;üì¢ Latest Updates&lt;/h3&gt;&lt;/summary&gt; 
 &lt;p&gt;&lt;strong&gt;üî• Releases:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://chat.pageindex.ai"&gt;&lt;strong&gt;PageIndex Chat&lt;/strong&gt;&lt;/a&gt;: The first human-like document-analysis agent &lt;a href="https://chat.pageindex.ai"&gt;platform&lt;/a&gt; built for professional long documents. Can also be integrated via &lt;a href="https://pageindex.ai/mcp"&gt;MCP&lt;/a&gt; or &lt;a href="https://docs.pageindex.ai/quickstart"&gt;API&lt;/a&gt; (beta).&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;!-- - [**PageIndex Chat API**](https://docs.pageindex.ai/quickstart): An API that brings PageIndex's advanced long-document intelligence directly into your applications and workflows. --&gt; 
 &lt;!-- - [PageIndex MCP](https://pageindex.ai/mcp): Bring PageIndex into Claude, Cursor, or any MCP-enabled agent. Chat with long PDFs in a reasoning-based, human-like way. --&gt; 
 &lt;p&gt;&lt;strong&gt;üìù Articles:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://pageindex.ai/blog/pageindex-intro"&gt;&lt;strong&gt;PageIndex Framework&lt;/strong&gt;&lt;/a&gt;: Introduces the PageIndex framework ‚Äî an &lt;em&gt;agentic, in-context&lt;/em&gt; &lt;em&gt;tree index&lt;/em&gt; that enables LLMs to perform &lt;em&gt;reasoning-based&lt;/em&gt;, &lt;em&gt;human-like retrieval&lt;/em&gt; over long documents, without vector DB or chunking.&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;!-- - [Do We Still Need OCR?](https://pageindex.ai/blog/do-we-need-ocr): Explores how vision-based, reasoning-native RAG challenges the traditional OCR pipeline, and why the future of document AI might be *vectorless* and *vision-based*. --&gt; 
 &lt;p&gt;&lt;strong&gt;üß™ Cookbooks:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://docs.pageindex.ai/cookbook/vectorless-rag-pageindex"&gt;Vectorless RAG&lt;/a&gt;: A minimal, hands-on example of reasoning-based RAG using PageIndex. No vectors, no chunking, and human-like retrieval.&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://docs.pageindex.ai/cookbook/vision-rag-pageindex"&gt;Vision-based Vectorless RAG&lt;/a&gt;: OCR-free, vision-only RAG with PageIndex's reasoning-native retrieval workflow that works directly over PDF page images.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h1&gt;üìë Introduction to PageIndex&lt;/h1&gt; 
&lt;p&gt;Are you frustrated with vector database retrieval accuracy for long professional documents? Traditional vector-based RAG relies on semantic &lt;em&gt;similarity&lt;/em&gt; rather than true &lt;em&gt;relevance&lt;/em&gt;. But &lt;strong&gt;similarity ‚â† relevance&lt;/strong&gt; ‚Äî what we truly need in retrieval is &lt;strong&gt;relevance&lt;/strong&gt;, and that requires &lt;strong&gt;reasoning&lt;/strong&gt;. When working with professional documents that demand domain expertise and multi-step reasoning, similarity search often falls short.&lt;/p&gt; 
&lt;p&gt;Inspired by AlphaGo, we propose &lt;strong&gt;&lt;a href="https://vectify.ai/pageindex"&gt;PageIndex&lt;/a&gt;&lt;/strong&gt; ‚Äî a &lt;strong&gt;vectorless&lt;/strong&gt;, &lt;strong&gt;reasoning-based RAG&lt;/strong&gt; system that builds a &lt;strong&gt;hierarchical tree index&lt;/strong&gt; from long documents and uses LLMs to &lt;strong&gt;reason&lt;/strong&gt; &lt;em&gt;over that index&lt;/em&gt; for &lt;strong&gt;agentic, context-aware retrieval&lt;/strong&gt;. It simulates how &lt;em&gt;human experts&lt;/em&gt; navigate and extract knowledge from complex documents through &lt;em&gt;tree search&lt;/em&gt;, enabling LLMs to &lt;em&gt;think&lt;/em&gt; and &lt;em&gt;reason&lt;/em&gt; their way to the most relevant document sections. PageIndex performs retrieval in two steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Generate a ‚ÄúTable-of-Contents‚Äù &lt;strong&gt;tree structure index&lt;/strong&gt; of documents&lt;/li&gt; 
 &lt;li&gt;Perform reasoning-based retrieval through &lt;strong&gt;tree search&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://pageindex.ai/blog/pageindex-intro" target="_blank" title="The PageIndex Framework"&gt; &lt;img src="https://docs.pageindex.ai/images/cookbook/vectorless-rag.png" width="70%" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h3&gt;üéØ Core Features&lt;/h3&gt; 
&lt;p&gt;Compared to traditional vector-based RAG, &lt;strong&gt;PageIndex&lt;/strong&gt; features:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;No Vector DB&lt;/strong&gt;: Uses document structure and LLM reasoning for retrieval, instead of vector similarity search.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;No Chunking&lt;/strong&gt;: Documents are organized into natural sections, not artificial chunks.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Human-like Retrieval&lt;/strong&gt;: Simulates how human experts navigate and extract knowledge from complex documents.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Better Explainability and Traceability&lt;/strong&gt;: Retrieval is based on reasoning ‚Äî traceable and interpretable, with page and section references. No more opaque, approximate vector search (‚Äúvibe retrieval‚Äù).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;PageIndex powers a reasoning-based RAG system that achieved &lt;strong&gt;state-of-the-art&lt;/strong&gt; &lt;a href="https://github.com/VectifyAI/Mafin2.5-FinanceBench"&gt;98.7% accuracy&lt;/a&gt; on FinanceBench, demonstrating superior performance over vector-based RAG solutions in professional document analysis (see our &lt;a href="https://vectify.ai/blog/Mafin2.5"&gt;blog post&lt;/a&gt; for details).&lt;/p&gt; 
&lt;h3&gt;üìç Explore PageIndex&lt;/h3&gt; 
&lt;p&gt;To learn more, please see a detailed introduction of the &lt;a href="https://pageindex.ai/blog/pageindex-intro"&gt;PageIndex framework&lt;/a&gt;. Check out this GitHub repo for open-source code, and the &lt;a href="https://docs.pageindex.ai/cookbook"&gt;cookbooks&lt;/a&gt;, &lt;a href="https://docs.pageindex.ai/tutorials"&gt;tutorials&lt;/a&gt;, and &lt;a href="https://pageindex.ai/blog"&gt;blog&lt;/a&gt; for additional usage guides and examples.&lt;/p&gt; 
&lt;p&gt;The PageIndex service is available as a ChatGPT-style &lt;a href="https://chat.pageindex.ai"&gt;chat platform&lt;/a&gt;, or can be integrated via &lt;a href="https://pageindex.ai/mcp"&gt;MCP&lt;/a&gt; or &lt;a href="https://docs.pageindex.ai/quickstart"&gt;API&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;üõ†Ô∏è Deployment Options&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Self-host ‚Äî run locally with this open-source repo.&lt;/li&gt; 
 &lt;li&gt;Cloud Service ‚Äî try instantly with our &lt;a href="https://chat.pageindex.ai/"&gt;Chat Platform&lt;/a&gt;, or integrate with &lt;a href="https://pageindex.ai/mcp"&gt;MCP&lt;/a&gt; or &lt;a href="https://docs.pageindex.ai/quickstart"&gt;API&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;Enterprise&lt;/em&gt; ‚Äî private or on-prem deployment. &lt;a href="https://ii2abc2jejf.typeform.com/to/tK3AXl8T"&gt;Contact us&lt;/a&gt; or &lt;a href="https://calendly.com/pageindex/meet"&gt;book a demo&lt;/a&gt; for more details.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üß™ Quick Hands-on&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Try the &lt;a href="https://github.com/VectifyAI/PageIndex/raw/main/cookbook/pageindex_RAG_simple.ipynb"&gt;&lt;strong&gt;Vectorless RAG&lt;/strong&gt;&lt;/a&gt; notebook ‚Äî a &lt;em&gt;minimal&lt;/em&gt;, hands-on example of reasoning-based RAG using PageIndex.&lt;/li&gt; 
 &lt;li&gt;Experiment with &lt;a href="https://github.com/VectifyAI/PageIndex/raw/main/cookbook/vision_RAG_pageindex.ipynb"&gt;&lt;em&gt;Vision-based Vectorless RAG&lt;/em&gt;&lt;/a&gt; ‚Äî no OCR; a minimal, reasoning-native RAG pipeline that works directly over page images.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://colab.research.google.com/github/VectifyAI/PageIndex/blob/main/cookbook/pageindex_RAG_simple.ipynb" target="_blank" rel="noopener"&gt; &lt;img src="https://img.shields.io/badge/Open_In_Colab-Vectorless_RAG-orange?style=for-the-badge&amp;amp;logo=googlecolab" alt="Open in Colab: Vectorless RAG" /&gt; &lt;/a&gt; &amp;nbsp;&amp;nbsp; 
 &lt;a href="https://colab.research.google.com/github/VectifyAI/PageIndex/blob/main/cookbook/vision_RAG_pageindex.ipynb" target="_blank" rel="noopener"&gt; &lt;img src="https://img.shields.io/badge/Open_In_Colab-Vision_RAG-orange?style=for-the-badge&amp;amp;logo=googlecolab" alt="Open in Colab: Vision RAG" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h1&gt;üå≤ PageIndex Tree Structure&lt;/h1&gt; 
&lt;p&gt;PageIndex can transform lengthy PDF documents into a semantic &lt;strong&gt;tree structure&lt;/strong&gt;, similar to a &lt;em&gt;"table of contents"&lt;/em&gt; but optimized for use with Large Language Models (LLMs). It's ideal for: financial reports, regulatory filings, academic textbooks, legal or technical manuals, and any document that exceeds LLM context limits.&lt;/p&gt; 
&lt;p&gt;Below is an example PageIndex tree structure. Also see more example &lt;a href="https://github.com/VectifyAI/PageIndex/tree/main/tests/pdfs"&gt;documents&lt;/a&gt; and generated &lt;a href="https://github.com/VectifyAI/PageIndex/tree/main/tests/results"&gt;tree structures&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-jsonc"&gt;...
{
  "title": "Financial Stability",
  "node_id": "0006",
  "start_index": 21,
  "end_index": 22,
  "summary": "The Federal Reserve ...",
  "nodes": [
    {
      "title": "Monitoring Financial Vulnerabilities",
      "node_id": "0007",
      "start_index": 22,
      "end_index": 28,
      "summary": "The Federal Reserve's monitoring ..."
    },
    {
      "title": "Domestic and International Cooperation and Coordination",
      "node_id": "0008",
      "start_index": 28,
      "end_index": 31,
      "summary": "In 2023, the Federal Reserve collaborated ..."
    }
  ]
}
...
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can generate the PageIndex tree structure with this open-source repo, or use our &lt;a href="https://docs.pageindex.ai/quickstart"&gt;API&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h1&gt;‚öôÔ∏è Package Usage&lt;/h1&gt; 
&lt;p&gt;You can follow these steps to generate a PageIndex tree from a PDF document.&lt;/p&gt; 
&lt;h3&gt;1. Install dependencies&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip3 install --upgrade -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. Set your OpenAI API key&lt;/h3&gt; 
&lt;p&gt;Create a &lt;code&gt;.env&lt;/code&gt; file in the root directory and add your API key:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;CHATGPT_API_KEY=your_openai_key_here
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;3. Run PageIndex on your PDF&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python3 run_pageindex.py --pdf_path /path/to/your/document.pdf
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Optional parameters&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; You can customize the processing with additional optional arguments: 
 &lt;pre&gt;&lt;code&gt;--model                 OpenAI model to use (default: gpt-4o-2024-11-20)
--toc-check-pages       Pages to check for table of contents (default: 20)
--max-pages-per-node    Max pages per node (default: 10)
--max-tokens-per-node   Max tokens per node (default: 20000)
--if-add-node-id        Add node ID (yes/no, default: yes)
--if-add-node-summary   Add node summary (yes/no, default: yes)
--if-add-doc-description Add doc description (yes/no, default: yes)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Markdown support&lt;/strong&gt;&lt;/summary&gt; 
 &lt;br /&gt; We also provide markdown support for PageIndex. You can use the `-md_path` flag to generate a tree structure for a markdown file. 
 &lt;pre&gt;&lt;code class="language-bash"&gt;python3 run_pageindex.py --md_path /path/to/your/document.md
&lt;/code&gt;&lt;/pre&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;Note: in this function, we use "#" to determine node heading and their levels. For example, "##" is level 2, "###" is level 3, etc. Make sure your markdown file is formatted correctly. If your Markdown file was converted from a PDF or HTML, we don't recommend using this function, since most existing conversion tools cannot preserve the original hierarchy. Instead, use our &lt;a href="https://pageindex.ai/blog/ocr"&gt;PageIndex OCR&lt;/a&gt;, which is designed to preserve the original hierarchy, to convert the PDF to a markdown file and then use this function.&lt;/p&gt; 
 &lt;/blockquote&gt; 
&lt;/details&gt; 
&lt;!-- 
# ‚òÅÔ∏è Improved Tree Generation with PageIndex OCR

This repo is designed for generating PageIndex tree structure for simple PDFs, but many real-world use cases involve complex PDFs that are hard to parse by classic Python tools. However, extracting high-quality text from PDF documents remains a non-trivial challenge. Most OCR tools only extract page-level content, losing the broader document context and hierarchy.

To address this, we introduced PageIndex OCR ‚Äî the first long-context OCR model designed to preserve the global structure of documents. PageIndex OCR significantly outperforms other leading OCR tools, such as those from Mistral and Contextual AI, in recognizing true hierarchy and semantic relationships across document pages.

- Experience next-level OCR quality with PageIndex OCR at our [Dashboard](https://dash.pageindex.ai/).
- Integrate PageIndex OCR seamlessly into your stack via our [API](https://docs.pageindex.ai/quickstart).

&lt;p align="center"&gt;
  &lt;img src="https://github.com/user-attachments/assets/eb35d8ae-865c-4e60-a33b-ebbd00c41732" width="80%"&gt;
&lt;/p&gt;
--&gt; 
&lt;hr /&gt; 
&lt;h1&gt;üìà Case Study: PageIndex Leads Finance QA Benchmark&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://vectify.ai/mafin"&gt;Mafin 2.5&lt;/a&gt; is a reasoning-based RAG system for financial document analysis, powered by &lt;strong&gt;PageIndex&lt;/strong&gt;. It achieved a state-of-the-art &lt;a href="https://vectify.ai/blog/Mafin2.5"&gt;&lt;strong&gt;98.7% accuracy&lt;/strong&gt;&lt;/a&gt; on the &lt;a href="https://arxiv.org/abs/2311.11944"&gt;FinanceBench&lt;/a&gt; benchmark, significantly outperforming traditional vector-based RAG systems.&lt;/p&gt; 
&lt;p&gt;PageIndex's hierarchical indexing and reasoning-driven retrieval enable precise navigation and extraction of relevant context from complex financial reports, such as SEC filings and earnings disclosures.&lt;/p&gt; 
&lt;p&gt;Explore the full &lt;a href="https://github.com/VectifyAI/Mafin2.5-FinanceBench"&gt;benchmark results&lt;/a&gt; and our &lt;a href="https://vectify.ai/blog/Mafin2.5"&gt;blog post&lt;/a&gt; for detailed comparisons and performance metrics.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://github.com/VectifyAI/Mafin2.5-FinanceBench"&gt; &lt;img src="https://github.com/user-attachments/assets/571aa074-d803-43c7-80c4-a04254b782a3" width="70%" /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h1&gt;üß≠ Resources&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;üß™ &lt;a href="https://docs.pageindex.ai/cookbook/vectorless-rag-pageindex"&gt;Cookbooks&lt;/a&gt;: hands-on, runnable examples and advanced use cases.&lt;/li&gt; 
 &lt;li&gt;üìñ &lt;a href="https://docs.pageindex.ai/doc-search"&gt;Tutorials&lt;/a&gt;: practical guides and strategies, including &lt;em&gt;Document Search&lt;/em&gt; and &lt;em&gt;Tree Search&lt;/em&gt;.&lt;/li&gt; 
 &lt;li&gt;üìù &lt;a href="https://pageindex.ai/blog"&gt;Blog&lt;/a&gt;: technical articles, research insights, and product updates.&lt;/li&gt; 
 &lt;li&gt;üîå &lt;a href="https://pageindex.ai/mcp#quick-setup"&gt;MCP setup&lt;/a&gt; &amp;amp; &lt;a href="https://docs.pageindex.ai/quickstart"&gt;API docs&lt;/a&gt;: integration details and configuration options.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h1&gt;‚≠ê Support Us&lt;/h1&gt; 
&lt;p&gt;Leave us a star üåü if you like our project. Thank you!&lt;/p&gt; 
&lt;p&gt; &lt;img src="https://github.com/user-attachments/assets/eae4ff38-48ae-4a7c-b19f-eab81201d794" width="80%" /&gt; &lt;/p&gt; 
&lt;h3&gt;Connect with Us&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://x.com/PageIndexAI"&gt;&lt;img src="https://img.shields.io/badge/Twitter-000000?style=for-the-badge&amp;amp;logo=x&amp;amp;logoColor=white" alt="Twitter" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://www.linkedin.com/company/vectify-ai/"&gt;&lt;img src="https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&amp;amp;logo=linkedin&amp;amp;logoColor=white" alt="LinkedIn" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://discord.com/invite/VuXuf29EUj"&gt;&lt;img src="https://img.shields.io/badge/Discord-5865F2?style=for-the-badge&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://ii2abc2jejf.typeform.com/to/tK3AXl8T"&gt;&lt;img src="https://img.shields.io/badge/Contact_Us-3B82F6?style=for-the-badge&amp;amp;logo=envelope&amp;amp;logoColor=white" alt="Contact Us" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;¬© 2025 &lt;a href="https://vectify.ai"&gt;Vectify AI&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ManimCommunity/manim</title>
      <link>https://github.com/ManimCommunity/manim</link>
      <description>&lt;p&gt;A community-maintained Python framework for creating mathematical animations.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="https://www.manim.community/"&gt;&lt;img src="https://raw.githubusercontent.com/ManimCommunity/manim/main/logo/cropped.png" /&gt;&lt;/a&gt; &lt;br /&gt; &lt;br /&gt; &lt;a href="https://pypi.org/project/manim/"&gt;&lt;img src="https://img.shields.io/pypi/v/manim.svg?style=flat&amp;amp;logo=pypi" alt="PyPI Latest Release" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/manimcommunity/manim"&gt;&lt;img src="https://img.shields.io/docker/v/manimcommunity/manim?color=%23099cec&amp;amp;label=docker%20image&amp;amp;logo=docker" alt="Docker image" /&gt; &lt;/a&gt; &lt;a href="https://mybinder.org/v2/gh/ManimCommunity/jupyter_examples/HEAD?filepath=basic_example_scenes.ipynb"&gt;&lt;img src="https://mybinder.org/badge_logo.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="http://choosealicense.com/licenses/mit/"&gt;&lt;img src="https://img.shields.io/badge/license-MIT-red.svg?style=flat" alt="MIT License" /&gt;&lt;/a&gt; &lt;a href="https://www.reddit.com/r/manim/"&gt;&lt;img src="https://img.shields.io/reddit/subreddit-subscribers/manim.svg?color=orange&amp;amp;label=reddit&amp;amp;logo=reddit" alt="Reddit" href /&gt;&lt;/a&gt; &lt;a href="https://twitter.com/manimcommunity/"&gt;&lt;img src="https://img.shields.io/twitter/url/https/twitter.com/cloudposse.svg?style=social&amp;amp;label=Follow%20%40manimcommunity" alt="Twitter" /&gt; &lt;/a&gt;&lt;a href="https://manim.community/discord/"&gt;&lt;img src="https://img.shields.io/discord/581738731934056449.svg?label=discord&amp;amp;color=yellow&amp;amp;logo=discord" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://docs.manim.community/"&gt;&lt;img src="https://readthedocs.org/projects/manimce/badge/?version=latest" alt="Documentation Status" /&gt;&lt;/a&gt; &lt;img src="https://github.com/ManimCommunity/manim/workflows/CI/badge.svg?sanitize=true" alt="CI" /&gt; &lt;br /&gt; &lt;br /&gt; &lt;i&gt;An animation engine for explanatory math videos&lt;/i&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;Manim is an animation engine for explanatory math videos. It's used to create precise animations programmatically, as demonstrated in the videos of &lt;a href="https://www.3blue1brown.com/"&gt;3Blue1Brown&lt;/a&gt;.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] The community edition of Manim (ManimCE) is a version maintained and developed by the community. It was forked from 3b1b/manim, a tool originally created and open-sourced by Grant Sanderson, also creator of the 3Blue1Brown educational math videos. While Grant Sanderson continues to maintain his own repository, we recommend this version for its continued development, improved features, enhanced documentation, and more active community-driven maintenance. If you would like to study how Grant makes his videos, head over to his repository (&lt;a href="https://github.com/3b1b/manim"&gt;3b1b/manim&lt;/a&gt;).&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Table of Contents:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ManimCommunity/manim/main/#installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ManimCommunity/manim/main/#usage"&gt;Usage&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ManimCommunity/manim/main/#documentation"&gt;Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ManimCommunity/manim/main/#docker"&gt;Docker&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ManimCommunity/manim/main/#help-with-manim"&gt;Help with Manim&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ManimCommunity/manim/main/#contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ManimCommunity/manim/main/#license"&gt;License&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!CAUTION] These instructions are for the community version &lt;em&gt;only&lt;/em&gt;. Trying to use these instructions to install &lt;a href="https://github.com/3b1b/manim"&gt;3b1b/manim&lt;/a&gt; or instructions there to install this version will cause problems. Read &lt;a href="https://docs.manim.community/en/stable/faq/installation.html#why-are-there-different-versions-of-manim"&gt;this&lt;/a&gt; and decide which version you wish to install, then only follow the instructions for your desired version.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Manim requires a few dependencies that must be installed prior to using it. If you want to try it out first before installing it locally, you can do so &lt;a href="https://try.manim.community/"&gt;in our online Jupyter environment&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For local installation, please visit the &lt;a href="https://docs.manim.community/en/stable/installation.html"&gt;Documentation&lt;/a&gt; and follow the appropriate instructions for your operating system.&lt;/p&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;p&gt;Manim is an extremely versatile package. The following is an example &lt;code&gt;Scene&lt;/code&gt; you can construct:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from manim import *


class SquareToCircle(Scene):
    def construct(self):
        circle = Circle()
        square = Square()
        square.flip(RIGHT)
        square.rotate(-3 * TAU / 8)
        circle.set_fill(PINK, opacity=0.5)

        self.play(Create(square))
        self.play(Transform(square, circle))
        self.play(FadeOut(square))
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;In order to view the output of this scene, save the code in a file called &lt;code&gt;example.py&lt;/code&gt;. Then, run the following in a terminal window:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;manim -p -ql example.py SquareToCircle
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You should see your native video player program pop up and play a simple scene in which a square is transformed into a circle. You may find some more simple examples within this &lt;a href="https://raw.githubusercontent.com/ManimCommunity/manim/main/example_scenes"&gt;GitHub repository&lt;/a&gt;. You can also visit the &lt;a href="https://docs.manim.community/en/stable/examples.html"&gt;official gallery&lt;/a&gt; for more advanced examples.&lt;/p&gt; 
&lt;p&gt;Manim also ships with a &lt;code&gt;%%manim&lt;/code&gt; IPython magic which allows to use it conveniently in JupyterLab (as well as classic Jupyter) notebooks. See the &lt;a href="https://docs.manim.community/en/stable/reference/manim.utils.ipython_magic.ManimMagic.html"&gt;corresponding documentation&lt;/a&gt; for some guidance and &lt;a href="https://mybinder.org/v2/gh/ManimCommunity/jupyter_examples/HEAD?filepath=basic_example_scenes.ipynb"&gt;try it out online&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Command line arguments&lt;/h2&gt; 
&lt;p&gt;The general usage of Manim is as follows:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/ManimCommunity/manim/main/docs/source/_static/command.png" alt="manim-illustration" /&gt;&lt;/p&gt; 
&lt;p&gt;The &lt;code&gt;-p&lt;/code&gt; flag in the command above is for previewing, meaning the video file will automatically open when it is done rendering. The &lt;code&gt;-ql&lt;/code&gt; flag is for a faster rendering at a lower quality.&lt;/p&gt; 
&lt;p&gt;Some other useful flags include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;-s&lt;/code&gt; to skip to the end and just show the final frame.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-n &amp;lt;number&amp;gt;&lt;/code&gt; to skip ahead to the &lt;code&gt;n&lt;/code&gt;'th animation of a scene.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-f&lt;/code&gt; show the file in the file browser.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For a thorough list of command line arguments, visit the &lt;a href="https://docs.manim.community/en/stable/guides/configuration.html"&gt;documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;Documentation is in progress at &lt;a href="https://docs.manim.community/"&gt;ReadTheDocs&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Docker&lt;/h2&gt; 
&lt;p&gt;The community also maintains a docker image (&lt;code&gt;manimcommunity/manim&lt;/code&gt;), which can be found &lt;a href="https://hub.docker.com/r/manimcommunity/manim"&gt;on DockerHub&lt;/a&gt;. Instructions on how to install and use it can be found in our &lt;a href="https://docs.manim.community/en/stable/installation/docker.html"&gt;documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Help with Manim&lt;/h2&gt; 
&lt;p&gt;If you need help installing or using Manim, feel free to reach out to our &lt;a href="https://www.manim.community/discord/"&gt;Discord Server&lt;/a&gt; or &lt;a href="https://www.reddit.com/r/manim"&gt;Reddit Community&lt;/a&gt;. If you would like to submit a bug report or feature request, please open an issue.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Contributions to Manim are always welcome. In particular, there is a dire need for tests and documentation. For contribution guidelines, please see the &lt;a href="https://docs.manim.community/en/stable/contributing.html"&gt;documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;However, please note that Manim is currently undergoing a major refactor. In general, contributions implementing new features will not be accepted in this period. The contribution guide may become outdated quickly; we highly recommend joining our &lt;a href="https://www.manim.community/discord/"&gt;Discord server&lt;/a&gt; to discuss any potential contributions and keep up to date with the latest developments.&lt;/p&gt; 
&lt;p&gt;Most developers on the project use &lt;code&gt;uv&lt;/code&gt; for management. You'll want to have uv installed and available in your environment. Learn more about &lt;code&gt;uv&lt;/code&gt; at its &lt;a href="https://docs.astral.sh/uv/"&gt;documentation&lt;/a&gt; and find out how to install manim with uv at the &lt;a href="https://docs.manim.community/en/latest/contributing/development.html"&gt;manim dev-installation guide&lt;/a&gt; in the manim documentation.&lt;/p&gt; 
&lt;h2&gt;How to Cite Manim&lt;/h2&gt; 
&lt;p&gt;We acknowledge the importance of good software to support research, and we note that research becomes more valuable when it is communicated effectively. To demonstrate the value of Manim, we ask that you cite Manim in your work. Currently, the best way to cite Manim is to go to our &lt;a href="https://github.com/ManimCommunity/manim"&gt;repository page&lt;/a&gt; (if you aren't already) and click the "cite this repository" button on the right sidebar. This will generate a citation in your preferred format, and will also integrate well with citation managers.&lt;/p&gt; 
&lt;h2&gt;Code of Conduct&lt;/h2&gt; 
&lt;p&gt;Our full code of conduct, and how we enforce it, can be read on &lt;a href="https://docs.manim.community/en/stable/conduct.html"&gt;our website&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;The software is double-licensed under the MIT license, with copyright by 3blue1brown LLC (see LICENSE), and copyright by Manim Community Developers (see LICENSE.community).&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>OpenBMB/UltraRAG</title>
      <link>https://github.com/OpenBMB/UltraRAG</link>
      <description>&lt;p&gt;UltraRAG v3: A Low-Code MCP Framework for Building Complex and Innovative RAG Pipelines&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="./docs/ultrarag_dark.svg" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="./docs/ultrarag.svg" /&gt; 
  &lt;img alt="UltraRAG" src="https://raw.githubusercontent.com/OpenBMB/UltraRAG/main/docs/ultrarag.svg?sanitize=true" width="55%" /&gt; 
 &lt;/picture&gt; &lt;/p&gt; 
&lt;h3 align="center"&gt; Less Code, Lower Barrier, Faster Deployment &lt;/h3&gt; 
&lt;p align="center"&gt; | &lt;a href="https://ultrarag.openbmb.cn/pages/en/getting_started/introduction"&gt;&lt;b&gt;Documentation&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://modelscope.cn/datasets/UltraRAG/UltraRAG_Benchmark"&gt;&lt;b&gt;Dataset&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://github.com/OpenBMB/UltraRAG/tree/rag-paper-daily/rag-paper-daily"&gt;&lt;b&gt;Paper Daily&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/OpenBMB/UltraRAG/main/docs/README_zh.md"&gt;&lt;b&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/b&gt;&lt;/a&gt; | &lt;b&gt;English&lt;/b&gt; | &lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;em&gt;Latest News&lt;/em&gt; üî•&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;[2026.01.23] üéâ UltraRAG 3.0 Released: Say no to "black box" development‚Äîmake every line of reasoning logic clearly visible üëâ|&lt;a href="https://github.com/OpenBMB/UltraRAG/raw/page/project/blog/en/ultrarag3_0.md"&gt;üìñ Blog&lt;/a&gt;|&lt;/li&gt; 
 &lt;li&gt;[2026.01.20] üéâ AgentCPM-Report Model Released! DeepResearch is finally localized: 8B on-device writing agent AgentCPM-Report is open-sourced üëâ |&lt;a href="https://huggingface.co/openbmb/AgentCPM-Report"&gt;ü§ó Model&lt;/a&gt;|&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;Previous News&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;[2025.11.11] üéâ UltraRAG 2.1 Released: Enhanced knowledge ingestion &amp;amp; multimodal support, with a more complete unified evaluation system!&lt;/li&gt; 
  &lt;li&gt;[2025.09.23] New daily RAG paper digest, updated every day üëâ |&lt;a href="https://github.com/OpenBMB/UltraRAG/tree/rag-paper-daily/rag-paper-daily"&gt;üìñ Papers&lt;/a&gt;|&lt;/li&gt; 
  &lt;li&gt;[2025.09.09] Released a Lightweight DeepResearch Pipeline local setup tutorial üëâ |&lt;a href="https://www.bilibili.com/video/BV1p8JfziEwM"&gt;üì∫ bilibili&lt;/a&gt;|&lt;a href="https://github.com/OpenBMB/UltraRAG/raw/page/project/blog/en/01_build_light_deepresearch.md"&gt;üìñ Blog&lt;/a&gt;|&lt;/li&gt; 
  &lt;li&gt;[2025.09.01] Released a step-by-step UltraRAG installation and full RAG walkthrough video üëâ |&lt;a href="https://www.bilibili.com/video/BV1B9apz4E7K/?share_source=copy_web&amp;amp;vd_source=7035ae721e76c8149fb74ea7a2432710"&gt;üì∫ bilibili&lt;/a&gt;|&lt;a href="https://github.com/OpenBMB/UltraRAG/raw/page/project/blog/en/00_Installing_and_Running_RAG.md"&gt;üìñ Blog&lt;/a&gt;|&lt;/li&gt; 
  &lt;li&gt;[2025.08.28] üéâ UltraRAG 2.0 Released! UltraRAG 2.0 is fully upgraded: build a high-performance RAG with just a few dozen lines of code, empowering researchers to focus on ideas and innovation! We have preserved the UltraRAG v2 code, which can be viewed at &lt;a href="https://github.com/OpenBMB/UltraRAG/tree/v2"&gt;v2&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2025.01.23] UltraRAG Released! Enabling large models to better comprehend and utilize knowledge bases. The UltraRAG 1.0 code is still available at &lt;a href="https://github.com/OpenBMB/UltraRAG/tree/v1"&gt;v1&lt;/a&gt;.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h2&gt;About UltraRAG&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://trendshift.io/repositories/18747" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/18747" alt="OpenBMB%2FUltraRAG | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;UltraRAG is the first lightweight RAG development framework based on the &lt;a href="https://modelcontextprotocol.io/docs/getting-started/intro"&gt;Model Context Protocol (MCP)&lt;/a&gt; architecture design, jointly launched by &lt;a href="https://nlp.csai.tsinghua.edu.cn/"&gt;THUNLP&lt;/a&gt; at Tsinghua University, &lt;a href="https://neuir.github.io"&gt;NEUIR&lt;/a&gt; at Northeastern University, &lt;a href="https://www.openbmb.cn/home"&gt;OpenBMB&lt;/a&gt;, and &lt;a href="https://github.com/AI9Stars"&gt;AI9stars&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Designed for research exploration and industrial prototyping, UltraRAG standardizes core RAG components (Retriever, Generation, etc.) as independent &lt;strong&gt;MCP Servers&lt;/strong&gt;, combined with the powerful workflow orchestration capabilities of the &lt;strong&gt;MCP Client&lt;/strong&gt;. Developers can achieve precise orchestration of complex control structures such as conditional branches and loops simply through YAML configuration.&lt;/p&gt; 
&lt;p align="center"&gt; 
 &lt;picture&gt; 
  &lt;img alt="UltraRAG" src="https://raw.githubusercontent.com/OpenBMB/UltraRAG/main/docs/architecture.png" width="90%" /&gt; 
 &lt;/picture&gt; &lt;/p&gt; 
&lt;h3&gt;UltraRAG UI&lt;/h3&gt; 
&lt;p&gt;UltraRAG UI transcends the boundaries of traditional chat interfaces, evolving into a visual RAG Integrated Development Environment (IDE) that combines orchestration, debugging, and demonstration.&lt;/p&gt; 
&lt;p&gt;The system features a powerful built-in Pipeline Builder that supports bidirectional real-time synchronization between "Canvas Construction" and "Code Editing," allowing for granular online adjustments of pipeline parameters and prompts. Furthermore, it introduces an Intelligent AI Assistant to empower the entire development lifecycle, from pipeline structural design to parameter tuning and prompt generation. Once constructed, logic flows can be converted into interactive dialogue systems with a single click. The system seamlessly integrates Knowledge Base Management components, enabling users to build custom knowledge bases for document Q&amp;amp;A. This truly realizes a one-stop closed loop, spanning from underlying logic construction and data governance to final application deployment.&lt;/p&gt; 
&lt;!-- &lt;p align="center"&gt;
  &lt;picture&gt;
    &lt;img alt="UltraRAG_UI" src="./docs/chat_menu.png" width=80%&gt;
  &lt;/picture&gt;
&lt;/p&gt; --&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/fcf437b7-8b79-42f2-bf4e-e3b7c2a896b9"&gt;https://github.com/user-attachments/assets/fcf437b7-8b79-42f2-bf4e-e3b7c2a896b9&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Key Highlights&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;üöÄ &lt;strong&gt;Low-Code Orchestration of Complex Workflows&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Inference Orchestration&lt;/strong&gt;: Natively supports control structures such as sequential, loop, and conditional branches. Developers only need to write YAML configuration files to implement complex iterative RAG logic in dozens of lines of code.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;‚ö° &lt;strong&gt;Modular Extension and Reproduction&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Atomic Servers&lt;/strong&gt;: Based on the MCP architecture, functions are decoupled into independent Servers. New features only need to be registered as function-level Tools to seamlessly integrate into workflows, achieving extremely high reusability.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üìä &lt;strong&gt;Unified Evaluation and Benchmark Comparison&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Research Efficiency&lt;/strong&gt;: Built-in standardized evaluation workflows, ready-to-use mainstream research benchmarks. Through unified metric management and baseline integration, significantly improves experiment reproducibility and comparison efficiency.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;‚ú® &lt;strong&gt;Rapid Interactive Prototype Generation&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;One-Click Delivery&lt;/strong&gt;: Say goodbye to tedious UI development. With just one command, Pipeline logic can be instantly converted into an interactive conversational Web UI, shortening the distance from algorithm to demonstration.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;We provide two installation methods: local source code installation (recommended using &lt;code&gt;uv&lt;/code&gt; for package management) and Docker container deployment&lt;/p&gt; 
&lt;h3&gt;Method 1: Source Code Installation&lt;/h3&gt; 
&lt;p&gt;We strongly recommend using &lt;a href="https://github.com/astral-sh/uv"&gt;uv&lt;/a&gt; to manage Python environments and dependencies, as it can greatly improve installation speed.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Prepare Environment&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;If you haven't installed uv yet, please execute:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;## Direct installation
pip install uv
## Download
curl -LsSf https://astral.sh/uv/install.sh | sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Download Source Code&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;git clone https://github.com/OpenBMB/UltraRAG.git --depth 1
cd UltraRAG
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Install Dependencies&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Choose one of the following modes to install dependencies based on your use case:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;A: Create a New Environment&lt;/strong&gt; Use &lt;code&gt;uv sync&lt;/code&gt; to automatically create a virtual environment and synchronize dependencies:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Core dependencies: If you only need to run basic core functions, such as only using UltraRAG UI:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;uv sync
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Full installation: If you want to fully experience UltraRAG's retrieval, generation, corpus processing, and evaluation functions, please run:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;uv sync --all-extras
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;On-demand installation: If you only need to run specific modules, keep the corresponding &lt;code&gt;--extra&lt;/code&gt; as needed, for example:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-shell"&gt;uv sync --extra retriever   # Retrieval module only
uv sync --extra generation  # Generation module only
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Once installed, activate the virtual environment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# Windows CMD
.venv\Scripts\activate.bat

# Windows Powershell
.venv\Scripts\Activate.ps1

# macOS / Linux
source .venv/bin/activate
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;B: Install into an Existing Environment&lt;/strong&gt; To install UltraRAG into your currently active Python environment, use &lt;code&gt;uv pip&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# Core dependencies
uv pip install -e .

# Full installation
uv pip install -e ".[all]"

# On-demand installation
uv pip install -e ".[retriever]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Method 2: Docker Container Deployment&lt;/h3&gt; 
&lt;p&gt;If you prefer not to configure a local Python environment, you can deploy using Docker.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Get Code and Images&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# 1. Clone the repository
git clone https://github.com/OpenBMB/UltraRAG.git --depth 1
cd UltraRAG

# 2. Prepare the image (choose one)
# Option A: Pull from Docker Hub
docker pull hdxin2002/ultrarag:v0.3.0-base-cpu # Base version (CPU)
docker pull hdxin2002/ultrarag:v0.3.0-base-gpu # Base version (GPU)
docker pull hdxin2002/ultrarag:v0.3.0          # Full version (GPU)

# Option B: Build locally
docker build -t ultrarag:v0.3.0 .

# 3. Start container (port 5050 is automatically mapped)
docker run -it --gpus all -p 5050:5050 &amp;lt;docker_image_name&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Start the Container&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# Start the container (Port 5050 is mapped by default)
docker run -it --gpus all -p 5050:5050 &amp;lt;docker_image_name&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note: After the container starts, UltraRAG UI will run automatically. You can directly access &lt;code&gt;http://localhost:5050&lt;/code&gt; in your browser to use it.&lt;/p&gt; 
&lt;h3&gt;Verify Installation&lt;/h3&gt; 
&lt;p&gt;After installation, run the following example command to check if the environment is normal:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;ultrarag run examples/sayhello.yaml
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you see the following output, the installation is successful:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Hello, UltraRAG v3!
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;p&gt;We provide complete tutorial examples from beginner to advanced. Whether you are conducting academic research or building industrial applications, you can find guidance here. Welcome to visit the &lt;a href="https://ultrarag.openbmb.cn/pages/en/getting_started/introduction"&gt;Documentation&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h3&gt;Research Experiments&lt;/h3&gt; 
&lt;p&gt;Designed for researchers, providing data, experimental workflows, and visualization analysis tools.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://ultrarag.openbmb.cn/pages/en/getting_started/quick_start"&gt;Getting Started&lt;/a&gt;: Learn how to quickly run standard RAG experimental workflows based on UltraRAG.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ultrarag.openbmb.cn/pages/en/develop_guide/dataset"&gt;Evaluation Data&lt;/a&gt;: Download the most commonly used public evaluation datasets in the RAG field and large-scale retrieval corpora, directly for research benchmark testing.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ultrarag.openbmb.cn/pages/en/develop_guide/case_study"&gt;Case Analysis&lt;/a&gt;: Provides a visual Case Study interface to deeply track each intermediate output of the workflow, assisting in analysis and error attribution.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ultrarag.openbmb.cn/pages/en/develop_guide/code_integration"&gt;Code Integration&lt;/a&gt;: Learn how to directly call UltraRAG components in Python code to achieve more flexible customized development.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Demo Systems&lt;/h3&gt; 
&lt;p&gt;Designed for developers and end users, providing complete UI interaction and complex application cases.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://ultrarag.openbmb.cn/pages/en/ui/start"&gt;Quick Start&lt;/a&gt;: Learn how to start UltraRAG UI and familiarize yourself with various advanced configurations in administrator mode.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ultrarag.openbmb.cn/pages/en/ui/prepare"&gt;Deployment Guide&lt;/a&gt;: Detailed production environment deployment tutorials, covering the setup of Retriever, Generation models (LLM), and Milvus vector database.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ultrarag.openbmb.cn/pages/en/demo/deepresearch"&gt;Deep Research&lt;/a&gt;: Flagship case, deploy a Deep Research Pipeline. Combined with the AgentCPM-Report model, it can automatically perform multi-step retrieval and integration to generate tens of thousands of words of survey reports.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Thanks to the following contributors for their code submissions and testing. We also welcome new members to join us in collectively building a comprehensive RAG ecosystem!&lt;/p&gt; 
&lt;p&gt;You can contribute by following the standard process: &lt;strong&gt;Fork this repository ‚Üí Submit Issues ‚Üí Create Pull Requests (PRs)&lt;/strong&gt;.&lt;/p&gt; 
&lt;a href="https://github.com/OpenBMB/UltraRAG/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=OpenBMB/UltraRAG&amp;amp;nocache=true" /&gt; &lt;/a&gt; 
&lt;h2&gt;Support Us&lt;/h2&gt; 
&lt;p&gt;If you find this repository helpful for your research, please consider giving us a ‚≠ê to show your support.&lt;/p&gt; 
&lt;a href="https://star-history.com/#OpenBMB/UltraRAG&amp;amp;Date"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=OpenBMB/UltraRAG&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=OpenBMB/UltraRAG&amp;amp;type=Date" /&gt; 
  &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=OpenBMB/UltraRAG&amp;amp;type=Date" /&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;h2&gt;Contact Us&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;For technical issues and feature requests, please use &lt;a href="https://github.com/OpenBMB/UltraRAG/issues"&gt;GitHub Issues&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;For questions about usage, feedback, or any discussions related to RAG technologies, you are welcome to join our &lt;a href="https://github.com/OpenBMB/UltraRAG/raw/main/docs/wechat_qr.png"&gt;WeChat group&lt;/a&gt;, &lt;a href="https://github.com/OpenBMB/UltraRAG/raw/main/docs/feishu_qr.png"&gt;Feishu group&lt;/a&gt;, and &lt;a href="https://discord.gg/yRFFjjJnnS"&gt;Discord&lt;/a&gt; to exchange ideas with us.&lt;/li&gt; 
 &lt;li&gt;If you have any questions, feedback, or would like to get in touch, please feel free to reach out to us via email at &lt;a href="mailto:yanyk.thu@gmail.com"&gt;yanyk.thu@gmail.com&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/UltraRAG/main/docs/wechat_qr.png" alt="WeChat Group QR Code" width="220" /&gt;&lt;br /&gt; &lt;b&gt;WeChat Group&lt;/b&gt; &lt;/td&gt; 
   &lt;td align="center"&gt; &lt;img src="https://raw.githubusercontent.com/OpenBMB/UltraRAG/main/docs/feishu_qr.png" alt="Feishu Group QR Code" width="220" /&gt;&lt;br /&gt; &lt;b&gt;Feishu Group&lt;/b&gt; &lt;/td&gt; 
   &lt;td align="center"&gt; &lt;a href="https://discord.gg/yRFFjjJnnS"&gt; &lt;img src="https://img.shields.io/badge/Discord-5865F2?logo=discord&amp;amp;logoColor=white" alt="Join Discord" /&gt; &lt;/a&gt;&lt;br /&gt; &lt;b&gt;Discord&lt;/b&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt;</description>
    </item>
    
    <item>
      <title>microsoft/VibeVoice</title>
      <link>https://github.com/microsoft/VibeVoice</link>
      <description>&lt;p&gt;Open-Source Frontier Voice AI&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h2&gt;üéôÔ∏è VibeVoice: Open-Source Frontier Voice AI&lt;/h2&gt; 
 &lt;p&gt;&lt;a href="https://microsoft.github.io/VibeVoice"&gt;&lt;img src="https://img.shields.io/badge/Project-Page-blue?logo=githubpages" alt="Project Page" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/collections/microsoft/vibevoice-68a2ef24a875c44be47b034f"&gt;&lt;img src="https://img.shields.io/badge/HuggingFace-Collection-orange?logo=huggingface" alt="Hugging Face" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/pdf/2508.19205"&gt;&lt;img src="https://img.shields.io/badge/TTS-Report-red?logo=arxiv" alt="TTS Report" /&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/pdf/2601.18184"&gt;&lt;img src="https://img.shields.io/badge/ASR-Report-yellow?logo=arxiv" alt="ASR Report" /&gt;&lt;/a&gt; &lt;a href="https://colab.research.google.com/github/microsoft/VibeVoice/blob/main/demo/VibeVoice_colab.ipynb"&gt;&lt;img src="https://img.shields.io/badge/StreamingTTS-Colab-green?logo=googlecolab" alt="Colab" /&gt;&lt;/a&gt; &lt;a href="https://aka.ms/vibevoice-asr"&gt;&lt;img src="https://img.shields.io/badge/ASR-Playground-6F42C1?logo=gradio" alt="ASR Playground" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="Figures/VibeVoice_logo_white.png" /&gt; 
  &lt;img src="https://raw.githubusercontent.com/microsoft/VibeVoice/main/Figures/VibeVoice_logo.png" alt="VibeVoice Logo" width="300" /&gt; 
 &lt;/picture&gt; 
&lt;/div&gt; 
&lt;div align="left"&gt; 
 &lt;h3&gt;üì∞ News&lt;/h3&gt; 
 &lt;p&gt;&lt;strong&gt;2026-01-21: üì£ We open-sourced &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-asr.md"&gt;&lt;strong&gt;VibeVoice-ASR&lt;/strong&gt;&lt;/a&gt;, a unified speech-to-text model designed to handle 60-minute long-form audio in a single pass, generating structured transcriptions containing Who (Speaker), When (Timestamps), and What (Content), with support for User-Customized Context. Try it in &lt;a href="https://aka.ms/vibevoice-asr"&gt;Playground&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;‚≠êÔ∏è VibeVoice-ASR is natively multilingual, supporting over 50 languages ‚Äî check the &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-asr.md#language-distribution"&gt;supported languages&lt;/a&gt; for details.&lt;/li&gt; 
  &lt;li&gt;üî• The VibeVoice-ASR &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/finetuning-asr/README.md"&gt;finetuning code&lt;/a&gt; is now available!&lt;/li&gt; 
  &lt;li&gt;‚ö°Ô∏è &lt;strong&gt;vLLM inference&lt;/strong&gt; is now supported for faster inference; see &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-vllm-asr.md"&gt;vllm-asr&lt;/a&gt; for more details.&lt;/li&gt; 
  &lt;li&gt;üìë &lt;a href="https://arxiv.org/pdf/2601.18184"&gt;VibeVoice-ASR Technique Report&lt;/a&gt; is available.&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;2025-12-16: üì£ We added experimental speakers to &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-realtime-0.5b.md"&gt;&lt;strong&gt;VibeVoice‚ÄëRealtime‚Äë0.5B&lt;/strong&gt;&lt;/a&gt; for exploration, including multilingual voices in nine languages (DE, FR, IT, JP, KR, NL, PL, PT, ES) and 11 distinct English style voices. &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-realtime-0.5b.md#optional-more-experimental-voices"&gt;Try it&lt;/a&gt;. More speaker types will be added over time.&lt;/p&gt; 
 &lt;p&gt;2025-12-03: üì£ We open-sourced &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-realtime-0.5b.md"&gt;&lt;strong&gt;VibeVoice‚ÄëRealtime‚Äë0.5B&lt;/strong&gt;&lt;/a&gt;, a real‚Äëtime text‚Äëto‚Äëspeech model that supports streaming text input and robust long-form speech generation. Try it on &lt;a href="https://colab.research.google.com/github/microsoft/VibeVoice/blob/main/demo/vibevoice_realtime_colab.ipynb"&gt;Colab&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;2025-09-05: VibeVoice is an open-source research framework intended to advance collaboration in the speech synthesis community. After release, we discovered instances where the tool was used in ways inconsistent with the stated intent. Since responsible use of AI is one of Microsoft‚Äôs guiding principles, we have removed the VibeVoice-TTS code from this repository.&lt;/p&gt; 
 &lt;p&gt;2025-08-25: üì£ We open-sourced &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-tts.md"&gt;&lt;strong&gt;VibeVoice-TTS&lt;/strong&gt;&lt;/a&gt;, a long-form multi-speaker text-to-speech model that can synthesize speech up to 90 minutes long with up to 4 distinct speakers.&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;Overview&lt;/h2&gt; 
&lt;p&gt;VibeVoice is a &lt;strong&gt;family of open-source frontier voice AI models&lt;/strong&gt; that includes both Text-to-Speech (TTS) and Automatic Speech Recognition (ASR) models.&lt;/p&gt; 
&lt;p&gt;A core innovation of VibeVoice is its use of continuous speech tokenizers (Acoustic and Semantic) operating at an ultra-low frame rate of &lt;strong&gt;7.5 Hz&lt;/strong&gt;. These tokenizers efficiently preserve audio fidelity while significantly boosting computational efficiency for processing long sequences. VibeVoice employs a &lt;a href="https://arxiv.org/abs/2412.08635"&gt;next-token diffusion&lt;/a&gt; framework, leveraging a Large Language Model (LLM) to understand textual context and dialogue flow, and a diffusion head to generate high-fidelity acoustic details.&lt;/p&gt; 
&lt;p&gt;For more information, demos, and examples, please visit our &lt;a href="https://microsoft.github.io/VibeVoice"&gt;Project Page&lt;/a&gt;.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model&lt;/th&gt; 
    &lt;th&gt;Weight&lt;/th&gt; 
    &lt;th&gt;Quick Try&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;VibeVoice-ASR-7B&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://huggingface.co/microsoft/VibeVoice-ASR"&gt;HF Link&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://aka.ms/vibevoice-asr"&gt;Playground&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;VibeVoice-TTS-1.5B&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://huggingface.co/microsoft/VibeVoice-1.5B"&gt;HF Link&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Disabled&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;VibeVoice-Realtime-0.5B&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://huggingface.co/microsoft/VibeVoice-Realtime-0.5B"&gt;HF Link&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://colab.research.google.com/github/microsoft/VibeVoice/blob/main/demo/vibevoice_realtime_colab.ipynb"&gt;Colab&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h2&gt;Models&lt;/h2&gt; 
&lt;h3&gt;1. üìñ &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-asr.md"&gt;VibeVoice-ASR&lt;/a&gt; - Long-form Speech Recognition&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;VibeVoice-ASR&lt;/strong&gt; is a unified speech-to-text model designed to handle &lt;strong&gt;60-minute long-form audio&lt;/strong&gt; in a single pass, generating structured transcriptions containing &lt;strong&gt;Who (Speaker), When (Timestamps), and What (Content)&lt;/strong&gt;, with support for &lt;strong&gt;Customized Hotwords&lt;/strong&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üïí 60-minute Single-Pass Processing&lt;/strong&gt;: Unlike conventional ASR models that slice audio into short chunks (often losing global context), VibeVoice ASR accepts up to &lt;strong&gt;60 minutes&lt;/strong&gt; of continuous audio input within 64K token length. This ensures consistent speaker tracking and semantic coherence across the entire hour.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üë§ Customized Hotwords&lt;/strong&gt;: Users can provide customized hotwords (e.g., specific names, technical terms, or background info) to guide the recognition process, significantly improving accuracy on domain-specific content.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üìù Rich Transcription (Who, When, What)&lt;/strong&gt;: The model jointly performs ASR, diarization, and timestamping, producing a structured output that indicates &lt;em&gt;who&lt;/em&gt; said &lt;em&gt;what&lt;/em&gt; and &lt;em&gt;when&lt;/em&gt;.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-asr.md"&gt;üìñ Documentation&lt;/a&gt; | &lt;a href="https://huggingface.co/microsoft/VibeVoice-ASR"&gt;ü§ó Hugging Face&lt;/a&gt; | &lt;a href="https://aka.ms/vibevoice-asr"&gt;üéÆ Playground&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/finetuning-asr/README.md"&gt;üõ†Ô∏è Finetuning&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/VibeVoice-ASR-Report.pdf"&gt;üìä Paper&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/VibeVoice/main/Figures/DER.jpg" alt="DER" width="50%" /&gt;&lt;br /&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/VibeVoice/main/Figures/cpWER.jpg" alt="cpWER" width="50%" /&gt;&lt;br /&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/VibeVoice/main/Figures/tcpWER.jpg" alt="tcpWER" width="50%" /&gt; &lt;/p&gt; 
&lt;div align="center" id="vibevoice-asr"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/acde5602-dc17-4314-9e3b-c630bc84aefa"&gt;https://github.com/user-attachments/assets/acde5602-dc17-4314-9e3b-c630bc84aefa&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;h3&gt;2. üéôÔ∏è &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-tts.md"&gt;VibeVoice-TTS&lt;/a&gt; - Long-form Multi-speaker TTS&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Best for&lt;/strong&gt;: Long-form conversational audio, podcasts, multi-speaker dialogues&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;‚è±Ô∏è 90-minute Long-form Generation&lt;/strong&gt;: Synthesizes conversational/single-speaker speech up to &lt;strong&gt;90 minutes&lt;/strong&gt; in a single pass, maintaining speaker consistency and semantic coherence throughout.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üë• Multi-speaker Support&lt;/strong&gt;: Supports up to &lt;strong&gt;4 distinct speakers&lt;/strong&gt; in a single conversation, with natural turn-taking and speaker consistency across long dialogues.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üé≠ Expressive Speech&lt;/strong&gt;: Generates expressive, natural-sounding speech that captures conversational dynamics and emotional nuances.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üåê Multi-lingual Support&lt;/strong&gt;: Supports English, Chinese and other languages.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-tts.md"&gt;üìñ Documentation&lt;/a&gt; | &lt;a href="https://huggingface.co/microsoft/VibeVoice-1.5B"&gt;ü§ó Hugging Face&lt;/a&gt; | &lt;a href="https://arxiv.org/pdf/2508.19205"&gt;üìä Paper&lt;/a&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/microsoft/VibeVoice/main/Figures/VibeVoice-TTS-results.jpg" alt="VibeVoice Results" width="80%" /&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;English&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/0967027c-141e-4909-bec8-091558b1b784"&gt;https://github.com/user-attachments/assets/0967027c-141e-4909-bec8-091558b1b784&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Chinese&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/322280b7-3093-4c67-86e3-10be4746c88f"&gt;https://github.com/user-attachments/assets/322280b7-3093-4c67-86e3-10be4746c88f&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Cross-Lingual&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/838d8ad9-a201-4dde-bb45-8cd3f59ce722"&gt;https://github.com/user-attachments/assets/838d8ad9-a201-4dde-bb45-8cd3f59ce722&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Spontaneous Singing&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/6f27a8a5-0c60-4f57-87f3-7dea2e11c730"&gt;https://github.com/user-attachments/assets/6f27a8a5-0c60-4f57-87f3-7dea2e11c730&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Long Conversation with 4 people&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/a357c4b6-9768-495c-a576-1618f6275727"&gt;https://github.com/user-attachments/assets/a357c4b6-9768-495c-a576-1618f6275727&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;h3&gt;3. ‚ö° &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-realtime-0.5b.md"&gt;VibeVoice-Streaming&lt;/a&gt; - Real-time Streaming TTS&lt;/h3&gt; 
&lt;p&gt;VibeVoice-Realtime is a &lt;strong&gt;lightweight real‚Äëtime&lt;/strong&gt; text-to-speech model supporting &lt;strong&gt;streaming text input&lt;/strong&gt; and &lt;strong&gt;robust long-form speech generation&lt;/strong&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Parameter size: 0.5B (deployment-friendly)&lt;/li&gt; 
 &lt;li&gt;Real-time TTS (~300 milliseconds first audible latency)&lt;/li&gt; 
 &lt;li&gt;Streaming text input&lt;/li&gt; 
 &lt;li&gt;Robust long-form speech generation (~10 minutes)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-realtime-0.5b.md"&gt;üìñ Documentation&lt;/a&gt; | &lt;a href="https://huggingface.co/microsoft/VibeVoice-Realtime-0.5B"&gt;ü§ó Hugging Face&lt;/a&gt; | &lt;a href="https://colab.research.google.com/github/microsoft/VibeVoice/blob/main/demo/vibevoice_realtime_colab.ipynb"&gt;üöÄ Colab&lt;/a&gt;&lt;/p&gt; 
&lt;div align="center" id="generated-example-audio-vibevoice-realtime"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/0901d274-f6ae-46ef-a0fd-3c4fba4f76dc"&gt;https://github.com/user-attachments/assets/0901d274-f6ae-46ef-a0fd-3c4fba4f76dc&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Please see &lt;a href="https://raw.githubusercontent.com/microsoft/VibeVoice/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for detailed contribution guidelines.&lt;/p&gt; 
&lt;h2&gt;‚ö†Ô∏è Risks and Limitations&lt;/h2&gt; 
&lt;p&gt;While efforts have been made to optimize it through various techniques, it may still produce outputs that are unexpected, biased, or inaccurate. VibeVoice inherits any biases, errors, or omissions produced by its base model (specifically, Qwen2.5 1.5b in this release). Potential for Deepfakes and Disinformation: High-quality synthetic speech can be misused to create convincing fake audio content for impersonation, fraud, or spreading disinformation. Users must ensure transcripts are reliable, check content accuracy, and avoid using generated content in misleading ways. Users are expected to use the generated content and to deploy the models in a lawful manner, in full compliance with all applicable laws and regulations in the relevant jurisdictions. It is best practice to disclose the use of AI when sharing AI-generated content.&lt;/p&gt; 
&lt;p&gt;We do not recommend using VibeVoice in commercial or real-world applications without further testing and development. This model is intended for research and development purposes only. Please use responsibly.&lt;/p&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://api.star-history.com/svg?repos=Microsoft/vibevoice&amp;amp;type=date&amp;amp;legend=top-left" alt="Star History Chart" /&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>microsoft/agent-lightning</title>
      <link>https://github.com/microsoft/agent-lightning</link>
      <description>&lt;p&gt;The absolute trainer to light up AI agents.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/agent-lightning/main/docs/assets/readme-banner.svg?sanitize=true" alt="Agent-lightning-banner" style="width:600px" /&gt; &lt;/p&gt; 
&lt;h1&gt;Agent Lightning‚ö°&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/microsoft/agent-lightning/actions/workflows/badge-unit.yml"&gt;&lt;img src="https://github.com/microsoft/agent-lightning/actions/workflows/badge-unit.yml/badge.svg?sanitize=true" alt="Unit Tests" /&gt;&lt;/a&gt; &lt;a href="https://microsoft.github.io/agent-lightning/"&gt;&lt;img src="https://img.shields.io/badge/GitHub%20Pages-Documentation-blue" alt="Documentation" /&gt;&lt;/a&gt; &lt;a href="https://badge.fury.io/py/agentlightning"&gt;&lt;img src="https://badge.fury.io/py/agentlightning.svg?sanitize=true" alt="PyPI version" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/microsoft/agent-lightning/main/LICENSE"&gt;&lt;img src="https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/microsoft/agent-lightning"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/RYk7CdvDR7"&gt;&lt;img src="https://img.shields.io/badge/Discord-Join-5865F2?logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;The absolute trainer to light up AI agents.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Join our &lt;a href="https://discord.gg/RYk7CdvDR7"&gt;Discord community&lt;/a&gt; to connect with other users and contributors.&lt;/p&gt; 
&lt;h2&gt;‚ö° Core Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Turn your agent into an optimizable beast with &lt;strong&gt;ZERO CODE CHANGE&lt;/strong&gt; (almost)! üí§&lt;/li&gt; 
 &lt;li&gt;Build with &lt;strong&gt;ANY&lt;/strong&gt; agent framework (LangChain, OpenAI Agent SDK, AutoGen, CrewAI, Microsoft Agent Framework...); or even WITHOUT agent framework (Python OpenAI). You name it! ü§ñ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Selectively&lt;/strong&gt; optimize one or more agents in a multi-agent system. üéØ&lt;/li&gt; 
 &lt;li&gt;Embraces &lt;strong&gt;Algorithms&lt;/strong&gt; like Reinforcement Learning, Automatic Prompt Optimization, Supervised Fine-tuning and more. ü§ó&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Read more on our &lt;a href="https://microsoft.github.io/agent-lightning/"&gt;documentation website&lt;/a&gt;.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/agent-lightning/main/docs/assets/readme-diff.svg?sanitize=true" alt="Agent-Lightning Core Quickstart" style="width:100%" /&gt; &lt;/p&gt; 
&lt;h2&gt;‚ö° Installation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install agentlightning
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For the latest nightly build (cutting-edge features), you can install from Test PyPI:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install --upgrade --index-url https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple/ --pre agentlightning
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Please refer to our &lt;a href="https://microsoft.github.io/agent-lightning/stable/tutorials/installation/"&gt;installation guide&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;p&gt;To start using Agent-lightning, check out our &lt;a href="https://microsoft.github.io/agent-lightning/"&gt;documentation&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/microsoft/agent-lightning/main/examples"&gt;examples&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;‚ö° Articles&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;12/17/2025 &lt;a href="https://agent-lightning.github.io/posts/trajectory_level_aggregation/"&gt;Adopting the Trajectory Level Aggregation for Faster Training&lt;/a&gt; Agent-lightning blog.&lt;/li&gt; 
 &lt;li&gt;11/4/2025 &lt;a href="https://medium.com/@yugez/tuning-any-ai-agent-with-tinker-agent-lightning-part-1-1d8c9a397f0e"&gt;Tuning ANY AI agent with Tinker ‚úï Agent-lightning&lt;/a&gt; Medium. See also &lt;a href="https://medium.com/@yugez/tuning-any-ai-agent-with-tinker-agent-lightning-part-2-332c5437f0dc"&gt;Part 2&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;10/22/2025 &lt;a href="https://blog.vllm.ai/2025/10/22/agent-lightning.html"&gt;No More Retokenization Drift: Returning Token IDs via the OpenAI Compatible API Matters in Agent RL&lt;/a&gt; vLLM blog. See also &lt;a href="https://zhuanlan.zhihu.com/p/1965067274642785725"&gt;Zhihu writeup&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;8/11/2025 &lt;a href="https://medium.com/@yugez/training-ai-agents-to-write-and-self-correct-sql-with-reinforcement-learning-571ed31281ad"&gt;Training AI Agents to Write and Self-correct SQL with Reinforcement Learning&lt;/a&gt; Medium.&lt;/li&gt; 
 &lt;li&gt;8/5/2025 &lt;a href="https://arxiv.org/abs/2508.03680"&gt;Agent Lightning: Train ANY AI Agents with Reinforcement Learning&lt;/a&gt; arXiv paper.&lt;/li&gt; 
 &lt;li&gt;7/26/2025 &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1m9m670/we_discovered_an_approach_to_train_any_ai_agent/"&gt;We discovered an approach to train any AI agent with RL, with (almost) zero code changes.&lt;/a&gt; Reddit.&lt;/li&gt; 
 &lt;li&gt;6/6/2025 &lt;a href="https://www.microsoft.com/en-us/research/project/agent-lightning/"&gt;Agent Lightning - Microsoft Research&lt;/a&gt; Project page.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;‚ö° Community Projects&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/af-74413592/DeepWerewolf"&gt;DeepWerewolf&lt;/a&gt; ‚Äî A case study of agent RL training for the Chinese Werewolf game built with AgentScope and Agent Lightning.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://agentflow.stanford.edu/"&gt;AgentFlow&lt;/a&gt; ‚Äî A modular multi-agent framework that combines planner, executor, verifier, and generator agents with the Flow-GRPO algorithm to tackle long-horizon, sparse-reward tasks.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/TencentCloudADP/Youtu-agent"&gt;Youtu-Agent&lt;/a&gt; ‚Äî Youtu-Agent lets you build and train your agent with ease. Built with &lt;a href="https://github.com/microsoft/agent-lightning/tree/contrib/youtu-agent-lightning"&gt;a modified branch&lt;/a&gt; of Agent Lightning, Youtu-Agent has verified up to 128 GPUs RL training on maths/code and search capabilities with steady convergence. Also check &lt;a href="https://github.com/TencentCloudADP/youtu-agent/tree/rl/agl"&gt;the recipe&lt;/a&gt; and their blog &lt;a href="https://spotted-coconut-df8.notion.site/Stop-Wrestling-with-Your-Agent-RL-How-Youtu-Agent-Achieved-Stable-128-GPU-Scaling-Without-Breaking-2ca5e8f089ba80539a98c582b65e0233"&gt;&lt;em&gt;Stop Wrestling with Your Agent RL: How Youtu-Agent Achieved Stable, 128-GPU Scaling Without Breaking a Sweat&lt;/em&gt;&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;‚ö° Architecture&lt;/h2&gt; 
&lt;p&gt;Agent Lightning keeps the moving parts to a minimum so you can focus on your idea, not the plumbing. Your agent continues to run as usual; you can still use any agent framework you like; you drop in the lightweight &lt;code&gt;agl.emit_xxx()&lt;/code&gt; helper, or let the tracer collect every prompt, tool call, and reward. Those events become structured spans that flow into the LightningStore, a central hub that keeps tasks, resources, and traces in sync.&lt;/p&gt; 
&lt;p&gt;On the other side of the store sits the algorithm you choose, or write yourself. The algorithm reads spans, learns from them, and posts updated resources such as refined prompt templates or new policy weights. The Trainer ties it all together: it streams datasets to runners, ferries resources between the store and the algorithm, and updates the inference engine when improvements land. You can either stop there, or simply let the same loop keep turning.&lt;/p&gt; 
&lt;p&gt;No rewrites, no lock-in, just a clear path from first rollout to steady improvement.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/microsoft/agent-lightning/main/docs/assets/readme-architecture.svg?sanitize=true" alt="Agent-lightning Architecture" style="width:100%" /&gt; &lt;/p&gt; 
&lt;h2&gt;‚ö° CI Status&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Workflow&lt;/th&gt; 
   &lt;th&gt;Status&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;CPU Tests&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/agent-lightning/actions/workflows/tests.yml"&gt;&lt;img src="https://github.com/microsoft/agent-lightning/actions/workflows/tests.yml/badge.svg?sanitize=true" alt="tests workflow status" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Full Tests&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/agent-lightning/actions/workflows/badge-unit.yml"&gt;&lt;img src="https://github.com/microsoft/agent-lightning/actions/workflows/badge-unit.yml/badge.svg?sanitize=true" alt="tests summary workflow status" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;UI Tests&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/agent-lightning/actions/workflows/dashboard.yml"&gt;&lt;img src="https://github.com/microsoft/agent-lightning/actions/workflows/dashboard.yml/badge.svg?sanitize=true" alt="UI Tests" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Examples Integration&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/agent-lightning/actions/workflows/badge-examples.yml"&gt;&lt;img src="https://github.com/microsoft/agent-lightning/actions/workflows/badge-examples.yml/badge.svg?sanitize=true" alt="examples summary workflow status" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Latest Dependency Compatibility&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/agent-lightning/actions/workflows/badge-latest.yml"&gt;&lt;img src="https://github.com/microsoft/agent-lightning/actions/workflows/badge-latest.yml/badge.svg?sanitize=true" alt="latest summary workflow status" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Legacy Examples Compatibility&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/agent-lightning/actions/workflows/badge-compat.yml"&gt;&lt;img src="https://github.com/microsoft/agent-lightning/actions/workflows/badge-compat.yml/badge.svg?sanitize=true" alt="compat summary workflow status" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;‚ö° Citation&lt;/h2&gt; 
&lt;p&gt;If you find Agent Lightning useful in your research or projects, please cite our paper:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{luo2025agentlightningtrainai,
      title={Agent Lightning: Train ANY AI Agents with Reinforcement Learning},
      author={Xufang Luo and Yuge Zhang and Zhiyuan He and Zilong Wang and Siyun Zhao and Dongsheng Li and Luna K. Qiu and Yuqing Yang},
      year={2025},
      eprint={2508.03680},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2508.03680},
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;‚ö° Contributing&lt;/h2&gt; 
&lt;p&gt;This project welcomes contributions and suggestions. Start by reading the &lt;a href="https://raw.githubusercontent.com/microsoft/agent-lightning/main/docs/community/contributing.md"&gt;Contributing Guide&lt;/a&gt; for recommended contribution points, environment setup, branching conventions, and pull request expectations. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href="https://cla.opensource.microsoft.com"&gt;https://cla.opensource.microsoft.com&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; 
&lt;p&gt;This project has adopted the &lt;a href="https://opensource.microsoft.com/codeofconduct/"&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href="https://opensource.microsoft.com/codeofconduct/faq/"&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href="mailto:opencode@microsoft.com"&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; 
&lt;h2&gt;‚ö° Trademarks&lt;/h2&gt; 
&lt;p&gt;This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow &lt;a href="https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general"&gt;Microsoft's Trademark &amp;amp; Brand Guidelines&lt;/a&gt;. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.&lt;/p&gt; 
&lt;h2&gt;‚ö° Responsible AI&lt;/h2&gt; 
&lt;p&gt;This project has been evaluated and certified to comply with the Microsoft Responsible AI Standard. The team will continue to monitor and maintain the repository, addressing any severe issues, including potential harms, if they arise.&lt;/p&gt; 
&lt;h2&gt;‚ö° License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the MIT License. See the &lt;a href="https://raw.githubusercontent.com/microsoft/agent-lightning/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>browser-use/browser-use</title>
      <link>https://github.com/browser-use/browser-use</link>
      <description>&lt;p&gt;üåê Make websites accessible for AI agents. Automate tasks online with ease.&lt;/p&gt;&lt;hr&gt;&lt;picture&gt; 
 &lt;source media="(prefers-color-scheme: light)" srcset="https://github.com/user-attachments/assets/2ccdb752-22fb-41c7-8948-857fc1ad7e24" " /&gt; 
 &lt;source media="(prefers-color-scheme: dark)" srcset="https://github.com/user-attachments/assets/774a46d5-27a0-490c-b7d0-e65fcbbfa358" /&gt; 
 &lt;img alt="Shows a black Browser Use Logo in light color mode and a white one in dark color mode." src="https://github.com/user-attachments/assets/2ccdb752-22fb-41c7-8948-857fc1ad7e24" width="full" /&gt; 
&lt;/picture&gt; 
&lt;div align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://github.com/user-attachments/assets/9955dda9-ede3-4971-8ee0-91cbc3850125" " /&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://github.com/user-attachments/assets/6797d09b-8ac3-4cb9-ba07-b289e080765a" /&gt; 
  &lt;img alt="The AI browser agent." src="https://github.com/user-attachments/assets/9955dda9-ede3-4971-8ee0-91cbc3850125" width="400" /&gt; 
 &lt;/picture&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://cloud.browser-use.com"&gt;&lt;img src="https://media.browser-use.tools/badges/package" height="48" alt="Browser-Use Package Download Statistics" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://raw.githubusercontent.com/browser-use/browser-use/main/#demos"&gt;&lt;img src="https://media.browser-use.tools/badges/demos" alt="Demos" /&gt;&lt;/a&gt; 
 &lt;img width="16" height="1" alt="" /&gt; 
 &lt;a href="https://docs.browser-use.com"&gt;&lt;img src="https://media.browser-use.tools/badges/docs" alt="Docs" /&gt;&lt;/a&gt; 
 &lt;img width="16" height="1" alt="" /&gt; 
 &lt;a href="https://browser-use.com/posts"&gt;&lt;img src="https://media.browser-use.tools/badges/blog" alt="Blog" /&gt;&lt;/a&gt; 
 &lt;img width="16" height="1" alt="" /&gt; 
 &lt;a href="https://browsermerch.com"&gt;&lt;img src="https://media.browser-use.tools/badges/merch" alt="Merch" /&gt;&lt;/a&gt; 
 &lt;img width="100" height="1" alt="" /&gt; 
 &lt;a href="https://github.com/browser-use/browser-use"&gt;&lt;img src="https://media.browser-use.tools/badges/github" alt="Github Stars" /&gt;&lt;/a&gt; 
 &lt;img width="4" height="1" alt="" /&gt; 
 &lt;a href="https://x.com/intent/user?screen_name=browser_use"&gt;&lt;img src="https://media.browser-use.tools/badges/twitter" alt="Twitter" /&gt;&lt;/a&gt; 
 &lt;img width="4 height=" 1" alt="" /&gt; 
 &lt;a href="https://link.browser-use.com/discord"&gt;&lt;img src="https://media.browser-use.tools/badges/discord" alt="Discord" /&gt;&lt;/a&gt; 
 &lt;img width="4" height="1" alt="" /&gt; 
 &lt;a href="https://cloud.browser-use.com"&gt;&lt;img src="https://media.browser-use.tools/badges/cloud" height="48" alt="Browser-Use Cloud" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;p&gt;üå§Ô∏è Want to skip the setup? Use our &lt;b&gt;&lt;a href="https://cloud.browser-use.com"&gt;cloud&lt;/a&gt;&lt;/b&gt; for faster, scalable, stealth-enabled browser automation!&lt;/p&gt; 
&lt;h1&gt;ü§ñ LLM Quickstart&lt;/h1&gt; 
&lt;ol&gt; 
 &lt;li&gt;Direct your favorite coding agent (Cursor, Claude Code, etc) to &lt;a href="https://docs.browser-use.com/llms-full.txt"&gt;Agents.md&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Prompt away!&lt;/li&gt; 
&lt;/ol&gt; 
&lt;br /&gt; 
&lt;h1&gt;üëã Human Quickstart&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;1. Create environment with &lt;a href="https://docs.astral.sh/uv/"&gt;uv&lt;/a&gt; (Python&amp;gt;=3.11):&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv init
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;2. Install Browser-Use package:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;#  We ship every day - use the latest version!
uv add browser-use
uv sync
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;3. Get your API key from &lt;a href="https://cloud.browser-use.com/new-api-key"&gt;Browser Use Cloud&lt;/a&gt; and add it to your &lt;code&gt;.env&lt;/code&gt; file (new signups get $10 free credits):&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;# .env
BROWSER_USE_API_KEY=your-key
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;4. Install Chromium browser:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uvx browser-use install
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;5. Run your first agent:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from browser_use import Agent, Browser, ChatBrowserUse
import asyncio

async def example():
    browser = Browser(
        # use_cloud=True,  # Uncomment to use a stealth browser on Browser Use Cloud
    )

    llm = ChatBrowserUse()

    agent = Agent(
        task="Find the number of stars of the browser-use repo",
        llm=llm,
        browser=browser,
    )

    history = await agent.run()
    return history

if __name__ == "__main__":
    history = asyncio.run(example())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Check out the &lt;a href="https://docs.browser-use.com"&gt;library docs&lt;/a&gt; and the &lt;a href="https://docs.cloud.browser-use.com"&gt;cloud docs&lt;/a&gt; for more!&lt;/p&gt; 
&lt;br /&gt; 
&lt;h1&gt;üî• Deploy on Sandboxes&lt;/h1&gt; 
&lt;p&gt;We handle agents, browsers, persistence, auth, cookies, and LLMs. The agent runs right next to the browser for minimal latency.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from browser_use import Browser, sandbox, ChatBrowserUse
from browser_use.agent.service import Agent
import asyncio

@sandbox()
async def my_task(browser: Browser):
    agent = Agent(task="Find the top HN post", browser=browser, llm=ChatBrowserUse())
    await agent.run()

# Just call it like any async function
asyncio.run(my_task())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;a href="https://docs.browser-use.com/production"&gt;Going to Production&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;br /&gt; 
&lt;h1&gt;üöÄ Template Quickstart&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;Want to get started even faster?&lt;/strong&gt; Generate a ready-to-run template:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uvx browser-use init --template default
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This creates a &lt;code&gt;browser_use_default.py&lt;/code&gt; file with a working example. Available templates:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;default&lt;/code&gt; - Minimal setup to get started quickly&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;advanced&lt;/code&gt; - All configuration options with detailed comments&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;tools&lt;/code&gt; - Examples of custom tools and extending the agent&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can also specify a custom output path:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uvx browser-use init --template default --output my_agent.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;br /&gt; 
&lt;h1&gt;üíª CLI&lt;/h1&gt; 
&lt;p&gt;Fast, persistent browser automation from the command line:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;browser-use open https://example.com    # Navigate to URL
browser-use state                       # See clickable elements
browser-use click 5                     # Click element by index
browser-use type "Hello"                # Type text
browser-use screenshot page.png         # Take screenshot
browser-use close                       # Close browser
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The CLI keeps the browser running between commands for fast iteration. See &lt;a href="https://raw.githubusercontent.com/browser-use/browser-use/main/browser_use/skill_cli/README.md"&gt;CLI docs&lt;/a&gt; for all commands.&lt;/p&gt; 
&lt;h3&gt;Claude Code Skill&lt;/h3&gt; 
&lt;p&gt;For &lt;a href="https://claude.ai/code"&gt;Claude Code&lt;/a&gt;, install the skill to enable AI-assisted browser automation:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;mkdir -p ~/.claude/skills/browser-use
curl -o ~/.claude/skills/browser-use/SKILL.md \
  https://raw.githubusercontent.com/browser-use/browser-use/main/skills/browser-use/SKILL.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;br /&gt; 
&lt;h1&gt;Demos&lt;/h1&gt; 
&lt;h3&gt;üìã Form-Filling&lt;/h3&gt; 
&lt;h4&gt;Task = "Fill in this job application with my resume and information."&lt;/h4&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/57865ee6-6004-49d5-b2c2-6dff39ec2ba9" alt="Job Application Demo" /&gt; &lt;a href="https://github.com/browser-use/browser-use/raw/main/examples/use-cases/apply_to_job.py"&gt;Example code ‚Üó&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;üçé Grocery-Shopping&lt;/h3&gt; 
&lt;h4&gt;Task = "Put this list of items into my instacart."&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/a6813fa7-4a7c-40a6-b4aa-382bf88b1850"&gt;https://github.com/user-attachments/assets/a6813fa7-4a7c-40a6-b4aa-382bf88b1850&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/browser-use/browser-use/raw/main/examples/use-cases/buy_groceries.py"&gt;Example code ‚Üó&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;üíª Personal-Assistant.&lt;/h3&gt; 
&lt;h4&gt;Task = "Help me find parts for a custom PC."&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/ac34f75c-057a-43ef-ad06-5b2c9d42bf06"&gt;https://github.com/user-attachments/assets/ac34f75c-057a-43ef-ad06-5b2c9d42bf06&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/browser-use/browser-use/raw/main/examples/use-cases/pcpartpicker.py"&gt;Example code ‚Üó&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;üí°See &lt;a href="https://docs.browser-use.com/examples"&gt;more examples here ‚Üó&lt;/a&gt; and give us a star!&lt;/h3&gt; 
&lt;br /&gt; 
&lt;h2&gt;Integrations, hosting, custom tools, MCP, and more on our &lt;a href="https://docs.browser-use.com"&gt;Docs ‚Üó&lt;/a&gt;&lt;/h2&gt; 
&lt;br /&gt; 
&lt;h1&gt;FAQ&lt;/h1&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;What's the best model to use?&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;We optimized &lt;strong&gt;ChatBrowserUse()&lt;/strong&gt; specifically for browser automation tasks. On avg it completes tasks 3-5x faster than other models with SOTA accuracy.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Pricing (per 1M tokens):&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Input tokens: $0.20&lt;/li&gt; 
  &lt;li&gt;Cached input tokens: $0.02&lt;/li&gt; 
  &lt;li&gt;Output tokens: $2.00&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;For other LLM providers, see our &lt;a href="https://docs.browser-use.com/supported-models"&gt;supported models documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Can I use custom tools with the agent?&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;Yes! You can add custom tools to extend the agent's capabilities:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-python"&gt;from browser_use import Tools

tools = Tools()

@tools.action(description='Description of what this tool does.')
def custom_tool(param: str) -&amp;gt; str:
    return f"Result: {param}"

agent = Agent(
    task="Your task",
    llm=llm,
    browser=browser,
    tools=tools,
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;Can I use this for free?&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;Yes! Browser-Use is open source and free to use. You only need to choose an LLM provider (like OpenAI, Google, ChatBrowserUse, or run local models with Ollama).&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;How do I handle authentication?&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;Check out our authentication examples:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://github.com/browser-use/browser-use/raw/main/examples/browser/real_browser.py"&gt;Using real browser profiles&lt;/a&gt; - Reuse your existing Chrome profile with saved logins&lt;/li&gt; 
  &lt;li&gt;If you want to use temporary accounts with inbox, choose AgentMail&lt;/li&gt; 
  &lt;li&gt;To sync your auth profile with the remote browser, run &lt;code&gt;curl -fsSL https://browser-use.com/profile.sh | BROWSER_USE_API_KEY=XXXX sh&lt;/code&gt; (replace XXXX with your API key)&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;These examples show how to maintain sessions and handle authentication seamlessly.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;How do I solve CAPTCHAs?&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;For CAPTCHA handling, you need better browser fingerprinting and proxies. Use &lt;a href="https://cloud.browser-use.com"&gt;Browser Use Cloud&lt;/a&gt; which provides stealth browsers designed to avoid detection and CAPTCHA challenges.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;b&gt;How do I go into production?&lt;/b&gt;&lt;/summary&gt; 
 &lt;p&gt;Chrome can consume a lot of memory, and running many agents in parallel can be tricky to manage.&lt;/p&gt; 
 &lt;p&gt;For production use cases, use our &lt;a href="https://cloud.browser-use.com"&gt;Browser Use Cloud API&lt;/a&gt; which handles:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Scalable browser infrastructure&lt;/li&gt; 
  &lt;li&gt;Memory management&lt;/li&gt; 
  &lt;li&gt;Proxy rotation&lt;/li&gt; 
  &lt;li&gt;Stealth browser fingerprinting&lt;/li&gt; 
  &lt;li&gt;High-performance parallel execution&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;br /&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;strong&gt;Tell your computer what to do, and it gets it done.&lt;/strong&gt;&lt;/p&gt; 
 &lt;img src="https://github.com/user-attachments/assets/06fa3078-8461-4560-b434-445510c1766f" width="400" /&gt; 
 &lt;p&gt;&lt;a href="https://x.com/intent/user?screen_name=mamagnus00"&gt;&lt;img src="https://img.shields.io/twitter/follow/Magnus?style=social" alt="Twitter Follow" /&gt;&lt;/a&gt; ‚ÄÉ‚ÄÉ‚ÄÉ &lt;a href="https://x.com/intent/user?screen_name=gregpr07"&gt;&lt;img src="https://img.shields.io/twitter/follow/Gregor?style=social" alt="Twitter Follow" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt;
  Made with ‚ù§Ô∏è in Zurich and San Francisco 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>IAmTomShaw/f1-race-replay</title>
      <link>https://github.com/IAmTomShaw/f1-race-replay</link>
      <description>&lt;p&gt;An interactive Formula 1 race visualisation and data analysis tool built with Python! üèéÔ∏è&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;F1 Race Replay üèéÔ∏è üèÅ&lt;/h1&gt; 
&lt;p&gt;A Python application for visualizing Formula 1 race telemetry and replaying race events with interactive controls and a graphical interface.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/IAmTomShaw/f1-race-replay/main/resources/preview.png" alt="Race Replay Preview" /&gt;&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Race Replay Visualization:&lt;/strong&gt; Watch the race unfold with real-time driver positions on a rendered track.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Leaderboard:&lt;/strong&gt; See live driver positions and current tyre compounds.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Lap &amp;amp; Time Display:&lt;/strong&gt; Track the current lap and total race time.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Driver Status:&lt;/strong&gt; Drivers who retire or go out are marked as "OUT" on the leaderboard.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Interactive Controls:&lt;/strong&gt; Pause, rewind, fast forward, and adjust playback speed using on-screen buttons or keyboard shortcuts.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Legend:&lt;/strong&gt; On-screen legend explains all controls.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Driver Telemetry Insights:&lt;/strong&gt; View speed, gear, DRS status, and current lap for selected drivers when selected on the leaderboard.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Controls&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Pause/Resume:&lt;/strong&gt; SPACE or Pause button&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Rewind/Fast Forward:&lt;/strong&gt; ‚Üê / ‚Üí or Rewind/Fast Forward buttons&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Playback Speed:&lt;/strong&gt; ‚Üë / ‚Üì or Speed button (cycles through 0.5x, 1x, 2x, 4x)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Set Speed Directly:&lt;/strong&gt; Keys 1‚Äì4&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Restart&lt;/strong&gt;: &lt;strong&gt;R&lt;/strong&gt; to restart replay&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Toggle DRS Zone&lt;/strong&gt;: &lt;strong&gt;D&lt;/strong&gt; to hide/showi DRS Zone&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Toggle Progress Bar&lt;/strong&gt;: &lt;strong&gt;B&lt;/strong&gt; to hide/show progress bar&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Toggle Driver Names&lt;/strong&gt;: &lt;strong&gt;L&lt;/strong&gt; to hide/show driver names on track&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Select driver/drivers&lt;/strong&gt;: Click to select driver or shift click to select multiple drivers&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Qualifying Session Support (in development)&lt;/h2&gt; 
&lt;p&gt;Recently added support for Qualifying session replays with telemetry visualization including speed, gear, throttle, and brake over the lap distance. This feature is still being refined.&lt;/p&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.11+&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/theOehrly/Fast-F1"&gt;FastF1&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://api.arcade.academy/en/latest/"&gt;Arcade&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;numpy&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Install dependencies:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;FastF1 cache folder will be created automatically on first run. If it is not created, you can manually create a folder named &lt;code&gt;.fastf1-cache&lt;/code&gt; in the project root.&lt;/p&gt; 
&lt;h2&gt;Environment Setup&lt;/h2&gt; 
&lt;p&gt;To get started with this project locally, you can follow these steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Clone the Repository:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/IAmTomShaw/f1-race-replay
 cd f1-race-replay
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Create a Virtual Environment:&lt;/strong&gt; This process differs based on your operating system.&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;On macOS/Linux: &lt;pre&gt;&lt;code class="language-bash"&gt;python3 -m venv venv
source venv/bin/activate
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt;On Windows: &lt;pre&gt;&lt;code class="language-bash"&gt;python -m venv venv
.\venv\Scripts\activate
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install Dependencies:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Run the Application:&lt;/strong&gt; You can now run the application using the instructions in the Usage section below.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;DEFAULT GUI MENU:&lt;/strong&gt; To use the new GUI menu system, you can simply run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python main.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/IAmTomShaw/f1-race-replay/main/resources/gui-menu.png" alt="GUI Menu Preview" /&gt;&lt;/p&gt; 
&lt;p&gt;This will open a graphical interface where you can select the year and round of the race weekend you want to replay. This is still a new feature, so please report any issues you encounter.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;OPTIONAL CLI MENU:&lt;/strong&gt; To use the CLI menu system, you can simply run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python main.py --cli
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/IAmTomShaw/f1-race-replay/main/resources/cli-menu.gif" alt="CLI Menu Preview" /&gt;&lt;/p&gt; 
&lt;p&gt;This will prompt you with series of questions and a list of options to make your choice from using the arrow keys and enter key.&lt;/p&gt; 
&lt;p&gt;If you would already know the year and round number of the session you would like to watch, you run the commands directly as follows:&lt;/p&gt; 
&lt;p&gt;Run the main script and specify the year and round:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python main.py --viewer --year 2025 --round 12
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To run without HUD:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python main.py --viewer --year 2025 --round 12 --no-hud
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To run a Sprint session (if the event has one), add &lt;code&gt;--sprint&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python main.py --viewer --year 2025 --round 12 --sprint
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The application will load a pre-computed telemetry dataset if you have run it before for the same event. To force re-computation of telemetry data, use the &lt;code&gt;--refresh-data&lt;/code&gt; flag:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python main.py --viewer --year 2025 --round 12 --refresh-data
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Qualifying Session Replay&lt;/h3&gt; 
&lt;p&gt;To run a Qualifying session replay, use the &lt;code&gt;--qualifying&lt;/code&gt; flag:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python main.py --viewer --year 2025 --round 12 --qualifying
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To run a Sprint Qualifying session (if the event has one), add &lt;code&gt;--sprint&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python main.py --viewer --year 2025 --round 12 --qualifying --sprint
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;File Structure&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;f1-race-replay/
‚îú‚îÄ‚îÄ main.py                    # Entry point, handles session loading and starts the replay
‚îú‚îÄ‚îÄ requirements.txt           # Python dependencies
‚îú‚îÄ‚îÄ README.md                  # Project documentation
‚îú‚îÄ‚îÄ roadmap.md                 # Planned features and project vision
‚îú‚îÄ‚îÄ resources/
‚îÇ   ‚îî‚îÄ‚îÄ preview.png           # Race replay preview image
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ f1_data.py            # Telemetry loading, processing, and frame generation
‚îÇ   ‚îú‚îÄ‚îÄ arcade_replay.py      # Visualization and UI logic
‚îÇ   ‚îî‚îÄ‚îÄ ui_components.py      # UI components like buttons and leaderboard
‚îÇ   ‚îú‚îÄ‚îÄ interfaces/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ qualifying.py     # Qualifying session interface and telemetry visualization
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ race_replay.py    # Race replay interface and telemetry visualization
‚îÇ   ‚îî‚îÄ‚îÄ lib/
‚îÇ       ‚îî‚îÄ‚îÄ tyres.py          # Type definitions for telemetry data structures
‚îÇ       ‚îî‚îÄ‚îÄ time.py           # Time formatting utilities
‚îî‚îÄ‚îÄ .fastf1-cache/            # FastF1 cache folder (created automatically upon first run)
‚îî‚îÄ‚îÄ computed_data/            # Computed telemetry data (created automatically upon first run)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Customization&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Change track width, colors, and UI layout in &lt;code&gt;src/arcade_replay.py&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Adjust telemetry processing in &lt;code&gt;src/f1_data.py&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;There have been several contributions from the community that have helped enhance this project. I have added a &lt;a href="https://raw.githubusercontent.com/IAmTomShaw/f1-race-replay/main/contributors.md"&gt;contributors.md&lt;/a&gt; file to acknowledge those who have contributed features and improvements.&lt;/p&gt; 
&lt;p&gt;If you would like to contribute, feel free to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Open pull requests for UI improvements or new features.&lt;/li&gt; 
 &lt;li&gt;Report issues on GitHub.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Please see &lt;a href="https://raw.githubusercontent.com/IAmTomShaw/f1-race-replay/main/roadmap.md"&gt;roadmap.md&lt;/a&gt; for planned features and project vision.&lt;/p&gt; 
&lt;h1&gt;Known Issues&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;The leaderboard appears to be inaccurate for the first few corners of the race. The leaderboard is also temporarily affected by a driver going in the pits. At the end of the race, the leaderboard is sometimes affected by the drivers' final x,y positions being further ahead than other drivers. These are known issues caused by inaccuracies in the telemetry and are being worked on for future releases. It's likely that these issues will be fixed in stages as improving the leaderboard accuracy is a complex task.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üìù License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the MIT License.&lt;/p&gt; 
&lt;h2&gt;‚ö†Ô∏è Disclaimer&lt;/h2&gt; 
&lt;p&gt;No copyright infringement intended. Formula 1 and related trademarks are the property of their respective owners. All data used is sourced from publicly available APIs and is used for educational and non-commercial purposes only.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;Built with ‚ù§Ô∏è by &lt;a href="https://tomshaw.dev"&gt;Tom Shaw&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>anthropics/skills</title>
      <link>https://github.com/anthropics/skills</link>
      <description>&lt;p&gt;Public repository for Agent Skills&lt;/p&gt;&lt;hr&gt;&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; This repository contains Anthropic's implementation of skills for Claude. For information about the Agent Skills standard, see &lt;a href="http://agentskills.io"&gt;agentskills.io&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h1&gt;Skills&lt;/h1&gt; 
&lt;p&gt;Skills are folders of instructions, scripts, and resources that Claude loads dynamically to improve performance on specialized tasks. Skills teach Claude how to complete specific tasks in a repeatable way, whether that's creating documents with your company's brand guidelines, analyzing data using your organization's specific workflows, or automating personal tasks.&lt;/p&gt; 
&lt;p&gt;For more information, check out:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://support.claude.com/en/articles/12512176-what-are-skills"&gt;What are skills?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://support.claude.com/en/articles/12512180-using-skills-in-claude"&gt;Using skills in Claude&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://support.claude.com/en/articles/12512198-creating-custom-skills"&gt;How to create custom skills&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://anthropic.com/engineering/equipping-agents-for-the-real-world-with-agent-skills"&gt;Equipping agents for the real world with Agent Skills&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;About This Repository&lt;/h1&gt; 
&lt;p&gt;This repository contains skills that demonstrate what's possible with Claude's skills system. These skills range from creative applications (art, music, design) to technical tasks (testing web apps, MCP server generation) to enterprise workflows (communications, branding, etc.).&lt;/p&gt; 
&lt;p&gt;Each skill is self-contained in its own folder with a &lt;code&gt;SKILL.md&lt;/code&gt; file containing the instructions and metadata that Claude uses. Browse through these skills to get inspiration for your own skills or to understand different patterns and approaches.&lt;/p&gt; 
&lt;p&gt;Many skills in this repo are open source (Apache 2.0). We've also included the document creation &amp;amp; editing skills that power &lt;a href="https://www.anthropic.com/news/create-files"&gt;Claude's document capabilities&lt;/a&gt; under the hood in the &lt;a href="https://raw.githubusercontent.com/anthropics/skills/main/skills/docx"&gt;&lt;code&gt;skills/docx&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/anthropics/skills/main/skills/pdf"&gt;&lt;code&gt;skills/pdf&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://raw.githubusercontent.com/anthropics/skills/main/skills/pptx"&gt;&lt;code&gt;skills/pptx&lt;/code&gt;&lt;/a&gt;, and &lt;a href="https://raw.githubusercontent.com/anthropics/skills/main/skills/xlsx"&gt;&lt;code&gt;skills/xlsx&lt;/code&gt;&lt;/a&gt; subfolders. These are source-available, not open source, but we wanted to share these with developers as a reference for more complex skills that are actively used in a production AI application.&lt;/p&gt; 
&lt;h2&gt;Disclaimer&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;These skills are provided for demonstration and educational purposes only.&lt;/strong&gt; While some of these capabilities may be available in Claude, the implementations and behaviors you receive from Claude may differ from what is shown in these skills. These skills are meant to illustrate patterns and possibilities. Always test skills thoroughly in your own environment before relying on them for critical tasks.&lt;/p&gt; 
&lt;h1&gt;Skill Sets&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/anthropics/skills/main/skills"&gt;./skills&lt;/a&gt;: Skill examples for Creative &amp;amp; Design, Development &amp;amp; Technical, Enterprise &amp;amp; Communication, and Document Skills&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/anthropics/skills/main/spec"&gt;./spec&lt;/a&gt;: The Agent Skills specification&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/anthropics/skills/main/template"&gt;./template&lt;/a&gt;: Skill template&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Try in Claude Code, Claude.ai, and the API&lt;/h1&gt; 
&lt;h2&gt;Claude Code&lt;/h2&gt; 
&lt;p&gt;You can register this repository as a Claude Code Plugin marketplace by running the following command in Claude Code:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;/plugin marketplace add anthropics/skills
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then, to install a specific set of skills:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Select &lt;code&gt;Browse and install plugins&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Select &lt;code&gt;anthropic-agent-skills&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Select &lt;code&gt;document-skills&lt;/code&gt; or &lt;code&gt;example-skills&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Select &lt;code&gt;Install now&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Alternatively, directly install either Plugin via:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;/plugin install document-skills@anthropic-agent-skills
/plugin install example-skills@anthropic-agent-skills
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After installing the plugin, you can use the skill by just mentioning it. For instance, if you install the &lt;code&gt;document-skills&lt;/code&gt; plugin from the marketplace, you can ask Claude Code to do something like: "Use the PDF skill to extract the form fields from &lt;code&gt;path/to/some-file.pdf&lt;/code&gt;"&lt;/p&gt; 
&lt;h2&gt;Claude.ai&lt;/h2&gt; 
&lt;p&gt;These example skills are all already available to paid plans in Claude.ai.&lt;/p&gt; 
&lt;p&gt;To use any skill from this repository or upload custom skills, follow the instructions in &lt;a href="https://support.claude.com/en/articles/12512180-using-skills-in-claude#h_a4222fa77b"&gt;Using skills in Claude&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Claude API&lt;/h2&gt; 
&lt;p&gt;You can use Anthropic's pre-built skills, and upload custom skills, via the Claude API. See the &lt;a href="https://docs.claude.com/en/api/skills-guide#creating-a-skill"&gt;Skills API Quickstart&lt;/a&gt; for more.&lt;/p&gt; 
&lt;h1&gt;Creating a Basic Skill&lt;/h1&gt; 
&lt;p&gt;Skills are simple to create - just a folder with a &lt;code&gt;SKILL.md&lt;/code&gt; file containing YAML frontmatter and instructions. You can use the &lt;strong&gt;template-skill&lt;/strong&gt; in this repository as a starting point:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-markdown"&gt;---
name: my-skill-name
description: A clear description of what this skill does and when to use it
---

# My Skill Name

[Add your instructions here that Claude will follow when this skill is active]

## Examples
- Example usage 1
- Example usage 2

## Guidelines
- Guideline 1
- Guideline 2
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The frontmatter requires only two fields:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;name&lt;/code&gt; - A unique identifier for your skill (lowercase, hyphens for spaces)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;description&lt;/code&gt; - A complete description of what the skill does and when to use it&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The markdown content below contains the instructions, examples, and guidelines that Claude will follow. For more details, see &lt;a href="https://support.claude.com/en/articles/12512198-creating-custom-skills"&gt;How to create custom skills&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Partner Skills&lt;/h1&gt; 
&lt;p&gt;Skills are a great way to teach Claude how to get better at using specific pieces of software. As we see awesome example skills from partners, we may highlight some of them here:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Notion&lt;/strong&gt; - &lt;a href="https://www.notion.so/notiondevs/Notion-Skills-for-Claude-28da4445d27180c7af1df7d8615723d0"&gt;Notion Skills for Claude&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>KellerJordan/modded-nanogpt</title>
      <link>https://github.com/KellerJordan/modded-nanogpt</link>
      <description>&lt;p&gt;NanoGPT (124M) in 2 minutes&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Modded-NanoGPT&lt;/h1&gt; 
&lt;p&gt;This repository hosts the &lt;em&gt;NanoGPT speedrun&lt;/em&gt;, in which we (collaboratively|competitively) search for the fastest algorithm to use 8 NVIDIA H100 GPUs to train a language model that attains 3.28 cross-entropy loss on the &lt;a href="https://huggingface.co/datasets/HuggingFaceFW/fineweb"&gt;FineWeb&lt;/a&gt; validation set.&lt;/p&gt; 
&lt;p&gt;The target (3.28 validation loss on FineWeb) follows Andrej Karpathy's &lt;a href="https://github.com/karpathy/llm.c/discussions/481#:~:text=By%20the%20end%20of%20the%20optimization%20we%27ll%20get%20to%20about%203.29"&gt;GPT-2 replication in llm.c, which attains that loss after running for 45 minutes&lt;/a&gt;. The speedrun code also descends from llm.c's &lt;a href="https://github.com/karpathy/llm.c/raw/master/train_gpt2.py"&gt;PyTorch trainer&lt;/a&gt;, which itself descends from NanoGPT, hence the name of the repo. Thanks to the efforts of many contributors, this repo now contains a training algorithm which attains the target performance in:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Under 100 seconds on 8xH100 (the llm.c GPT-2 replication needed 45 minutes)&lt;/li&gt; 
 &lt;li&gt;under 500M tokens (the llm.c GPT-2 replication needed 10B)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This improvement in training speed has been brought about by the following techniques:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Modernized architecture: Rotary embeddings, QK-Norm, and ReLU¬≤&lt;/li&gt; 
 &lt;li&gt;The Muon optimizer [&lt;a href="https://kellerjordan.github.io/posts/muon/"&gt;writeup&lt;/a&gt;] [&lt;a href="https://github.com/KellerJordan/Muon"&gt;repo&lt;/a&gt;]&lt;/li&gt; 
 &lt;li&gt;Use FP8 matmul for head, and asymmetric rescale and softcap logits&lt;/li&gt; 
 &lt;li&gt;Initialization of projections to zero (muP-like)&lt;/li&gt; 
 &lt;li&gt;Skip connections from embedding to every block as well as from block 3 to 6&lt;/li&gt; 
 &lt;li&gt;Extra embeddings which are mixed into the values in attention layers (inspired by Zhou et al. 2024)&lt;/li&gt; 
 &lt;li&gt;Flash Attention 3 with long-short sliding window attention pattern (inspired by Gemma 2) and window size warmup with YaRN&lt;/li&gt; 
 &lt;li&gt;Align training batch starts with EoS and set a max document length&lt;/li&gt; 
 &lt;li&gt;Accumulate gradients for 2 steps for embedding and lm_head before updating parameters&lt;/li&gt; 
 &lt;li&gt;Enable model to back out contributions from first 2/3 layers before prediction&lt;/li&gt; 
 &lt;li&gt;Polar Express implementation in Muon&lt;/li&gt; 
 &lt;li&gt;Smear module to enable 1 token look back&lt;/li&gt; 
 &lt;li&gt;Sparse attention gate&lt;/li&gt; 
 &lt;li&gt;NorMuon&lt;/li&gt; 
 &lt;li&gt;Cautious Weight Decay w/ schedule tied to LR&lt;/li&gt; 
 &lt;li&gt;Exponential decay of residual stream&lt;/li&gt; 
 &lt;li&gt;Batch size schedule&lt;/li&gt; 
 &lt;li&gt;Partial Key Offset&lt;/li&gt; 
 &lt;li&gt;Multi token prediction&lt;/li&gt; 
 &lt;li&gt;Untie embed and lm_head at 2/3 of training&lt;/li&gt; 
 &lt;li&gt;Additional gating on value embeddings and skip connection&lt;/li&gt; 
 &lt;li&gt;Paired head attention&lt;/li&gt; 
 &lt;li&gt;Bigram hash embedding&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;As well as many systems optimizations.&lt;/p&gt; 
&lt;p&gt;Contributors list (growing with each new record): &lt;a href="https://x.com/bozavlado"&gt;@bozavlado&lt;/a&gt;; &lt;a href="https://x.com/brendanh0gan"&gt;@brendanh0gan&lt;/a&gt;; &lt;a href="https://bsky.app/profile/fernbear.bsky.social"&gt;@fernbear.bsky.social&lt;/a&gt;; &lt;a href="https://x.com/Grad62304977"&gt;@Grad62304977&lt;/a&gt;; &lt;a href="https://x.com/jxbz"&gt;@jxbz&lt;/a&gt;; &lt;a href="https://x.com/kellerjordan0"&gt;@kellerjordan0&lt;/a&gt;; &lt;a href="https://x.com/KoszarskyB"&gt;@KoszarskyB&lt;/a&gt;; &lt;a href="https://x.com/@leloykun"&gt;@leloykun&lt;/a&gt;; &lt;a href="https://x.com/YouJiacheng"&gt;@YouJiacheng&lt;/a&gt;; &lt;a href="https://x.com/jadenj3o"&gt;@jadenj3o&lt;/a&gt;; &lt;a href="https://github.com/KonstantinWilleke"&gt;@KonstantinWilleke&lt;/a&gt;, &lt;a href="https://github.com/alexrgilbert"&gt;@alexrgilbert&lt;/a&gt;, &lt;a href="https://github.com/adricarda"&gt;@adricarda&lt;/a&gt;, &lt;a href="https://github.com/tuttyfrutyee"&gt;@tuttyfrutyee&lt;/a&gt;, &lt;a href="https://github.com/vdlad"&gt;@vdlad&lt;/a&gt;; &lt;a href="https://x.com/ryanyang0"&gt;@ryanyang0&lt;/a&gt;, &lt;a href="https://github.com/vagrawal"&gt;@vagrawal&lt;/a&gt;, &lt;a href="https://x.com/classiclarryd"&gt;@classiclarryd&lt;/a&gt;, &lt;a href="https://github.com/byronxu99"&gt;@byronxu99&lt;/a&gt;, &lt;a href="https://x.com/varunneal"&gt;@varunneal&lt;/a&gt;, &lt;a href="https://github.com/EmelyanenkoK"&gt;@EmelyanenkoK&lt;/a&gt;, &lt;a href="https://github.com/bernard24"&gt;@bernard24&lt;/a&gt;/&lt;a href="https://www.hiverge.ai/"&gt;https://www.hiverge.ai/&lt;/a&gt;, &lt;a href="https://x.com/Gusarich"&gt;@Gusarich&lt;/a&gt;, &lt;a href="https://x.com/li_zichong"&gt;@li_zichong&lt;/a&gt;, &lt;a href="https://github.com/akash5474"&gt;@akash5474&lt;/a&gt;, &lt;a href="https://x.com/omouamoua"&gt;@snimu&lt;/a&gt;, &lt;a href="https://x.com/roeeshenberg"&gt;@roeeshenberg&lt;/a&gt;, &lt;a href="https://x.com/ChrisJMcCormick"&gt;@ChrisJMcCormick&lt;/a&gt;, &lt;a href="https://github.com/dominikkallusky"&gt;@dominikkallusky&lt;/a&gt;, &lt;a href="https://github.com/acutkosky"&gt;@acutkosky&lt;/a&gt;, &lt;a href="https://github.com/manikbhandari"&gt;@manikbhandari&lt;/a&gt;, &lt;a href="https://github.com/andrewbriand"&gt;@andrewbriand&lt;/a&gt;, &lt;a href="https://github.com/jrauvola"&gt;@jrauvola&lt;/a&gt;, &lt;a href="https://x.com/soren_dunn_"&gt;@soren_dunn_&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Running the current record&lt;/h2&gt; 
&lt;p&gt;To run the current record, run the following commands.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/KellerJordan/modded-nanogpt.git &amp;amp;&amp;amp; cd modded-nanogpt
pip install -r requirements.txt
pip install torch==2.10.0.dev20251210+cu126 --index-url https://download.pytorch.org/whl/nightly/cu126
# downloads only the first 900M training tokens to save time
python data/cached_fineweb10B.py 9
./run.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Add torchrun to path if ./run.sh gives error &lt;code&gt;torchrun: command not found&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note: torch.compile will add around 7 minutes of latency the first time you run the code.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Official records are timed on 8 NVIDIA H100 GPUs from &lt;a href="https://app.primeintellect.ai/"&gt;https://app.primeintellect.ai/&lt;/a&gt;. PrimeIntellect has generously sponsored recent validation runs.&lt;/p&gt; 
&lt;h2&gt;Alternative: Running with Docker (recommended for precise timing)&lt;/h2&gt; 
&lt;p&gt;For cases where CUDA or NCCL versions aren't compatible with your current system setup, Docker can be a helpful alternative. This approach standardizes versions for CUDA, NCCL, CUDNN, and Python, reducing dependency issues and simplifying setup. Note: an NVIDIA driver must already be installed on the system (useful if only the NVIDIA driver and Docker are available).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/KellerJordan/modded-nanogpt.git &amp;amp;&amp;amp; cd modded-nanogpt
sudo docker build -t modded-nanogpt .
sudo docker run -it --rm --gpus all -v $(pwd):/modded-nanogpt modded-nanogpt python data/cached_fineweb10B.py 8
sudo docker run -it --rm --gpus all -v $(pwd):/modded-nanogpt modded-nanogpt sh run.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To get an interactive docker, you can use&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo docker run -it --rm --gpus all -v $(pwd):/modded-nanogpt modded-nanogpt bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;World record history&lt;/h2&gt; 
&lt;p&gt;The following is the historical progression of world speed records for the following competitive task:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;em&gt;Train a neural network to ‚â§3.28 validation loss on FineWeb using 8x NVIDIA H100s.&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Note: The 3.28 target was selected to match &lt;a href="https://github.com/karpathy/llm.c/discussions/481"&gt;Andrej Karpathy's GPT-2 (small) reproduction&lt;/a&gt;.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;#&lt;/th&gt; 
   &lt;th&gt;Record time&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Date&lt;/th&gt; 
   &lt;th&gt;Log&lt;/th&gt; 
   &lt;th&gt;Contributors&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;1&lt;/td&gt; 
   &lt;td&gt;45 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/karpathy/llm.c/discussions/481"&gt;llm.c baseline&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;05/28/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2024-10-13_llmc/main.log"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@karpathy, llm.c contributors&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;31.4 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/kellerjordan0/status/1798863559243513937"&gt;Tuned learning rate &amp;amp; rotary embeddings&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;06/06/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2024-06-06_AdamW/f66d43d7-e449-4029-8adf-e8537bab49ea.log"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@kellerjordan0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;3&lt;/td&gt; 
   &lt;td&gt;24.9 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/kellerjordan0/status/1842300916864844014"&gt;Introduced the Muon optimizer&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;10/04/24&lt;/td&gt; 
   &lt;td&gt;none&lt;/td&gt; 
   &lt;td&gt;@kellerjordan0, @jxbz&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;4&lt;/td&gt; 
   &lt;td&gt;22.3 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/kellerjordan0/status/1844820919061287009"&gt;Muon improvements&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;10/11/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2024-10-10_Muon/eb5659d0-fb6a-49e5-a311-f1f89412f726.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@kellerjordan0, @bozavlado&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;15.2 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/kellerjordan0/status/1845865698532450646"&gt;Pad embeddings, ReLU¬≤, zero-init projections, QK-norm&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;10/14/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2024-10-14_ModernArch/dabaaddd-237c-4ec9-939d-6608a9ed5e27.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@Grad62304977, @kellerjordan0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;6&lt;/td&gt; 
   &lt;td&gt;13.1 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/kellerjordan0/status/1847291684016783746"&gt;Distributed the overhead of Muon&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;10/18/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2024-10-17_DistributedMuon/22d24867-eb5a-4fcc-ae2c-263d0277dfd1.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@kellerjordan0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;7&lt;/td&gt; 
   &lt;td&gt;12.0 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/kellerjordan0/status/1847358578686152764"&gt;Upgraded PyTorch 2.5.0&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;10/18/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2024-10-18_PyTorch25/d4bfb25f-688d-4da5-8743-33926fad4842.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@kellerjordan0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;10.8 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/kellerjordan0/status/1853188916704387239"&gt;Untied embedding and head&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;11/03/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2024-11-03_UntieEmbed/d6b50d71-f419-4d26-bb39-a60d55ae7a04.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@Grad62304977, @kellerjordan0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;9&lt;/td&gt; 
   &lt;td&gt;8.2 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/kellerjordan0/status/1854296101303800108"&gt;Value and embedding skip connections, momentum warmup, logit softcap&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;11/06/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2024-11-06_ShortcutsTweaks/dd7304a6-cc43-4d5e-adb8-c070111464a1.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@Grad62304977, @kellerjordan0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;10&lt;/td&gt; 
   &lt;td&gt;7.8 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/kellerjordan0/status/1855267054774865980"&gt;Bfloat16 activations&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;11/08/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2024-11-08_CastBf16/a833bed8-2fa8-4cfe-af05-58c1cc48bc30.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@kellerjordan0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;11&lt;/td&gt; 
   &lt;td&gt;7.2 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/kellerjordan0/status/1856053121103093922"&gt;U-net pattern skip connections &amp;amp; double lr&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;11/10/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2024-11-10_UNetDoubleLr/c87bb826-797b-4f37-98c7-d3a5dad2de74.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@brendanh0gan&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;12&lt;/td&gt; 
   &lt;td&gt;5.03 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/kellerjordan0/status/1859331370268623321"&gt;1024-ctx dense causal attention ‚Üí 64K-ctx FlexAttention&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;11/19/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2024-11-19_FlexAttention/8384493d-dba9-4991-b16b-8696953f5e6d.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@KoszarskyB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;13&lt;/td&gt; 
   &lt;td&gt;4.66 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/hi_tysam/status/1860851011797053450"&gt;Attention window warmup&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;11/24/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2024-11-24_WindowWarmup/cf9e4571-c5fc-4323-abf3-a98d862ec6c8.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@fernbear.bsky.social&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;14&lt;/td&gt; 
   &lt;td&gt;4.41 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/KoszarskyB/status/1864746625572257852"&gt;Value Embeddings&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;12/04/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2024-12-04_ValueEmbed"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@KoszarskyB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;15&lt;/td&gt; 
   &lt;td&gt;3.95 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/YouJiacheng/status/1865761473886347747"&gt;U-net pattern value embeddings, assorted code optimizations&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;12/08/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2024-12-08_UNetValueEmbedsTweaks"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@leloykun, @YouJiacheng&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;3.80 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/YouJiacheng/status/1866734331559071981"&gt;Split value embeddings, block sliding window, separate block mask&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;12/10/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2024-12-10_MFUTweaks"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@YouJiacheng&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;17&lt;/td&gt; 
   &lt;td&gt;3.57 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/YouJiacheng/status/1868938024731787640"&gt;Sparsify value embeddings, improve rotary embeddings, drop an attn layer&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;12/17/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2024-12-17_SparsifyEmbeds"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@YouJiacheng&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;18&lt;/td&gt; 
   &lt;td&gt;3.4 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/kellerjordan0/status/1876048851158880624"&gt;Lower logit softcap from 30 to 15&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;01/04/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-01-04_SoftCap/31d6c427-f1f7-4d8a-91be-a67b5dcd13fd.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@KoszarskyB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;19&lt;/td&gt; 
   &lt;td&gt;3.142 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/YouJiacheng/status/1878827972519772241"&gt;FP8 head, offset logits, lr decay to 0.1 instead of 0.0&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;01/13/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-01-13_Fp8LmHead/c51969c2-d04c-40a7-bcea-c092c3c2d11a.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@YouJiacheng&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;20&lt;/td&gt; 
   &lt;td&gt;2.992 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/leloykun/status/1880301753213809016"&gt;Merged QKV weights, long-short attention, attention scale, lower Adam epsilon, batched Muon&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;01/16/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-01-16_Sub3Min/1d3bd93b-a69e-4118-aeb8-8184239d7566.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@leloykun, @fernbear.bsky.social, @YouJiacheng, @brendanh0gan, @scottjmaddox, @Grad62304977&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;21&lt;/td&gt; 
   &lt;td&gt;2.933 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/leloykun/status/1885640350368420160"&gt;Reduced batch size&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;01/26/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-01-26_BatchSize/c44090cc-1b99-4c95-8624-38fb4b5834f9.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@leloykun&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;21&lt;/td&gt; 
   &lt;td&gt;2.997 minutes&lt;/td&gt; 
   &lt;td&gt;21st record with new timing&lt;/td&gt; 
   &lt;td&gt;02/01/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-02-01_RuleTweak/eff63a8c-2f7e-4fc5-97ce-7f600dae0bc7.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;not a new record, just re-timing #21 with the &lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/#timing-change-after-record-21"&gt;updated rules&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;21&lt;/td&gt; 
   &lt;td&gt;3.014 minutes&lt;/td&gt; 
   &lt;td&gt;21st record with latest torch&lt;/td&gt; 
   &lt;td&gt;05/24/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-05-24_StableTorch/89d9f224-3b01-4581-966e-358d692335e0.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;not a new record, just re-timing #21 with latest torch&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;22&lt;/td&gt; 
   &lt;td&gt;2.990 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/KonstantinWille/status/1927137223238909969"&gt;Faster gradient all-reduce&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;05/24/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-05-24_FasterReduce/23f40b75-06fb-4c3f-87a8-743524769a35.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad; The Enigma project&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;23&lt;/td&gt; 
   &lt;td&gt;2.979 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/kellerjordan0/status/1927460573098262616"&gt;Overlap computation and gradient communication&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;05/25/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-05-25_EvenFasterReduce/6ae86d05-5cb2-4e40-a512-63246fd08e45.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@ryanyang0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;24&lt;/td&gt; 
   &lt;td&gt;2.966 minutes&lt;/td&gt; 
   &lt;td&gt;Replace gradient all_reduce with reduce_scatter&lt;/td&gt; 
   &lt;td&gt;05/30/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-05-30_noallreduce/8054c239-3a18-499e-b0c8-dbd27cb4b3ab.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@vagrawal&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;25&lt;/td&gt; 
   &lt;td&gt;2.896 minutes&lt;/td&gt; 
   &lt;td&gt;Upgrade PyTorch to 2.9.0.dev20250713+cu126&lt;/td&gt; 
   &lt;td&gt;07/13/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-07-13_UpgradeTorch190/692f80e0-5e64-4819-97d4-0dc83b7106b9.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@kellerjordan0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;26&lt;/td&gt; 
   &lt;td&gt;2.863 minutes&lt;/td&gt; 
   &lt;td&gt;Align training batch starts with EoS, increase cooldown frac to .45&lt;/td&gt; 
   &lt;td&gt;07/13/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-07-12_BosAlign/c1fd8a38-bb9f-45c4-8af0-d37f70c993f3.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@classiclarryd&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;27&lt;/td&gt; 
   &lt;td&gt;2.817 minutes&lt;/td&gt; 
   &lt;td&gt;Transpose one of the MLP matrices + add Triton kernel for symmetric matmul&lt;/td&gt; 
   &lt;td&gt;07/18/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-07-18_TritonMuon/record.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/109"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@byronxu99&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;28&lt;/td&gt; 
   &lt;td&gt;2.812 minutes&lt;/td&gt; 
   &lt;td&gt;Sparse attention gate&lt;/td&gt; 
   &lt;td&gt;08/23/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-08-23_SparseAttnGate/020630eb-2191-4ba2-9ee4-4cdc94316943.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/117"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@classiclarryd&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;29&lt;/td&gt; 
   &lt;td&gt;2.731 minutes&lt;/td&gt; 
   &lt;td&gt;Flash Attention 3, 2048 max_doc_len, update ws schedule&lt;/td&gt; 
   &lt;td&gt;09/03/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-09-03_FA3/44fc1276-0510-4961-92c0-730c65e5feba.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/118"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@varunneal&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;30&lt;/td&gt; 
   &lt;td&gt;2.717 minutes&lt;/td&gt; 
   &lt;td&gt;Drop first MLP layer&lt;/td&gt; 
   &lt;td&gt;09/05/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-09-05_SkipMLPBlocks/07e7ae76-b7d0-4481-b149-01e7d81b5ad4.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/120"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@EmelyanenkoK&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;31&lt;/td&gt; 
   &lt;td&gt;2.656 minutes&lt;/td&gt; 
   &lt;td&gt;Dynamically incorporate YaRN during training and validation&lt;/td&gt; 
   &lt;td&gt;09/10/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-09-10_Yarn/0ecdb695-510b-4c3b-b030-09861a162ce8.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/122"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@classiclarryd&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;32&lt;/td&gt; 
   &lt;td&gt;2.625 minutes&lt;/td&gt; 
   &lt;td&gt;Optimize distributed training, improve skip connection gating, and enhance bfloat16 usage&lt;/td&gt; 
   &lt;td&gt;09/11/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-09-11_VectSigmoidBFloat16/0d0d9882-c34f-4d82-b961-a17d5659c988.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/125"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@bernard24 &amp;amp; AI system &lt;a href="https://www.hiverge.ai/"&gt;hiverge.ai&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;33&lt;/td&gt; 
   &lt;td&gt;2.565 minutes&lt;/td&gt; 
   &lt;td&gt;Asynchronously fetch and index data batches, extend final layer attention window for validation&lt;/td&gt; 
   &lt;td&gt;09/15/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-09-15_AsyncDataLoadAttnFinalWindow/25db37c7-2bab-4ef4-ae63-d593590ef823.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/127"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@classiclarryd&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;34&lt;/td&gt; 
   &lt;td&gt;2.547 minutes&lt;/td&gt; 
   &lt;td&gt;Smear token embeddings 1 position forward&lt;/td&gt; 
   &lt;td&gt;09/18/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-09-18_Smear/18a1e5c7-947e-479d-bc3a-a57a61a98fc9.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/130"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@classiclarryd&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;35&lt;/td&gt; 
   &lt;td&gt;2.527 minutes&lt;/td&gt; 
   &lt;td&gt;Drop first attn layer, extend all long windows for validation, update schedule&lt;/td&gt; 
   &lt;td&gt;09/21/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-09-21_DropAttn/01fc4a96-f2a0-47a1-8a6a-c7d10bac99fe.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/131"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@classiclarryd&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;36&lt;/td&gt; 
   &lt;td&gt;2.495 minutes&lt;/td&gt; 
   &lt;td&gt;MuonCustomSizing, perform mlp and attn reduce scatter in shared call&lt;/td&gt; 
   &lt;td&gt;09/23/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-09-23_MuonCustomSizing/b067b4ac-72a6-4436-a6f8-ea51c1efeef3.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/132"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@classiclarryd&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;37&lt;/td&gt; 
   &lt;td&gt;2.483 minutes&lt;/td&gt; 
   &lt;td&gt;Compute cross entropy in BF16 during training&lt;/td&gt; 
   &lt;td&gt;09/27/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-09-27_BF16CE/08c0770f-17fc-44cd-971d-734a7a28a3e3.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/133"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@Gusarich&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;38&lt;/td&gt; 
   &lt;td&gt;2.476 minutes&lt;/td&gt; 
   &lt;td&gt;Polar Express, replacement for Newton-Schulz&lt;/td&gt; 
   &lt;td&gt;09/29/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-09-29_PolarExpress/0e3f0af5-ad08-47a6-813d-0c709b50d422.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/134"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@varunneal&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;39&lt;/td&gt; 
   &lt;td&gt;2.447 minutes&lt;/td&gt; 
   &lt;td&gt;Only update Adam params every other step, reduce batch size&lt;/td&gt; 
   &lt;td&gt;09/30/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-09-30_CustomBatching/40b101b1-77ea-45ea-a089-1d3a647daa22.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/136"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@classiclarryd&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;40&lt;/td&gt; 
   &lt;td&gt;2.358 minutes&lt;/td&gt; 
   &lt;td&gt;Backout, misc hyperparameter tuning, optimize lambda padding&lt;/td&gt; 
   &lt;td&gt;10/04/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-10-04_Backout/514e7581-fbd4-4338-a3e4-e556f9c958ce.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/140"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@classiclarryd&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;41&lt;/td&gt; 
   &lt;td&gt;2.345 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://arxiv.org/pdf/2510.05491"&gt;NorMuon&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;10/24/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-10-24_NorMuon/088a77ee-9b67-475a-bbb9-3e92e4698799.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/144"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@li_zichong&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;42&lt;/td&gt; 
   &lt;td&gt;2.313 minutes&lt;/td&gt; 
   &lt;td&gt;Update NorMuon LR, Step Logic&lt;/td&gt; 
   &lt;td&gt;10/27/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-10-27_FixMuonLR/14afd380-d3d9-48d7-ad23-4c13cb96754b.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/146"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@varunneal&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;43&lt;/td&gt; 
   &lt;td&gt;2.284 minutes&lt;/td&gt; 
   &lt;td&gt;Cautious Weight Decay w/ schedule&lt;/td&gt; 
   &lt;td&gt;11/10/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-11-10_CautiousWD/1aac0132-a891-4ed9-b358-0fd2abd1b019.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/154"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@varunneal&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;44&lt;/td&gt; 
   &lt;td&gt;2.269 minutes&lt;/td&gt; 
   &lt;td&gt;Backward hooks on Adam, &lt;a href="https://blog.underfit.ai/profiling-101-nanogpt"&gt;Profiling 101&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;11/16/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-10-31_AdamSyncGradientHook/0c17cdfd-772c-4906-8d11-141b370599a0.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/149"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@akash5474&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;45&lt;/td&gt; 
   &lt;td&gt;2.248 minutes&lt;/td&gt; 
   &lt;td&gt;Refine skip arch, update exponential decay init&lt;/td&gt; 
   &lt;td&gt;11/18/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-11-18_RefineSkip/00f4e1e6-0044-4a08-b88a-3b7ec0624081.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/159"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@classiclarryd&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;46&lt;/td&gt; 
   &lt;td&gt;2.203 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/classiclarryd/status/1998212158770065844"&gt;Batch size schedule&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;11/29/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-11-29_BatchSizeSchedule/10e8f7c6-7175-4467-bdb0-a5de25d771a6.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/163"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@varunneal&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;47&lt;/td&gt; 
   &lt;td&gt;2.193 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/classiclarryd/status/1999630732814348451"&gt;Multiply attn lambda with weight instead of data, fix warmup&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;12/10/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-12-10_SALambdaOnWeights/15ef5eaf-56e1-40e1-9ddf-af010027c9dd.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/166"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@roeeshenberg&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;48&lt;/td&gt; 
   &lt;td&gt;2.170 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/classiclarryd/status/2000272495644152317"&gt;Speed up Muon, additional pre-multiply lambda, reshape matrices, update lr, update NorMuon axis&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;12/11/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-12-11_NorMuonOptimsAndFixes/82edf6be-f343-475d-b93a-47c32acf4de2.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/168"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@ChrisJMcCormick&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;49&lt;/td&gt; 
   &lt;td&gt;2.146 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/classiclarryd/status/2000841339299402142"&gt;Partial Key Offset&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;12/14/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-12-14_PartialKeyOffset/150d40bf-c20b-4568-aac9-26eb919e25fd.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/169"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@classiclarryd&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;50&lt;/td&gt; 
   &lt;td&gt;2.128 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/classiclarryd/status/2002482925741486381"&gt;Extend Cautious Weight Decay to Adam parameters&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;12/18/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-12-18_CautiousWDAdam/1981d492-bc65-4ba9-a0fa-2b30fc5c3eba.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/172"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@roeeshenberg&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;51&lt;/td&gt; 
   &lt;td&gt;2.075 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/classiclarryd/status/2003167208483209668"&gt;Retie Embed to lm_head, retune fp8 scales&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;12/19/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-12-19_RetieLMHead/0828d309-ecfe-4442-9ee9-68fed3a4b599.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/175"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@varunneal&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;52&lt;/td&gt; 
   &lt;td&gt;2.037 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/classiclarryd/status/2003863282613190656"&gt;Smooth scalars via beta increase, decrease smear gate lr, freeze scalars during transitions, adam all reduce&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;12/21/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-12-21_SmoothedScalars/12-21-Smoothed-Scalars/0bc6e909-8ee8-4ae3-ac62-0070e151a808.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/177"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@ChrisJMcCormick&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;53&lt;/td&gt; 
   &lt;td&gt;1.988 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/classiclarryd/status/2004248941878296580"&gt;Multi-token prediction, untie embed/lm_head at 2/3 training, lr update, tweak CWD&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;12/22/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-12-22_MultiTokenPrediction/17aaf854-f338-4d0d-9767-a5db30fd7980.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/178"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@varunneal, feat. @classiclarryd&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;54&lt;/td&gt; 
   &lt;td&gt;1.940 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/classiclarryd/status/2004791008098480232"&gt;Asymmetric Logit Rescale&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;12/26/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-12-26_LogitRescale/03e41c2d-2951-4546-a599-24cd723247fc.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/181"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@classiclarryd&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;55&lt;/td&gt; 
   &lt;td&gt;1.918 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/classiclarryd/status/2005659526960492638"&gt;Gates on value embeds and skip connection&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;12/29/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-12-29_VeSkipGates/2851d7dc-d6a5-4e74-8623-57031425db16.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/186"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@classiclarryd&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;56&lt;/td&gt; 
   &lt;td&gt;1.894 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/classiclarryd/status/2007882371576873445"&gt;Optimize and compile Adam, increase Adam buffer precision, move gates from Muon to Adam parameter banks&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;12/31/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-12-31_GatesToCompiledAdam/12-31-gates-to-adam-20stps/219a5f2f-151e-4c56-ab91-3735ae4610b8.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/187"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@ChrisJMcCormick&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;57&lt;/td&gt; 
   &lt;td&gt;1.878 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/classiclarryd/status/2008261904566022590"&gt;Bfloat16 attn/mlp weights, mixed precision Muon, interweave Adam/Muon, finer-grain Adam beta&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;01/04/26&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2026-01-04_MixedPrecisionInterweavedOptimizer/41f606b6-1b9c-46a3-b46e-2beff1521d18.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/190"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@classiclarryd, feat. @YouJiacheng, @ChrisJMcCormick&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;58&lt;/td&gt; 
   &lt;td&gt;1.820 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/classiclarryd/status/2008963501688324228"&gt;Paired Head Attention&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;01/07/26&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2026-01-07_PairedHeadAttention/2a5d5cde-db5f-4aab-a4a8-cc8e183ea671.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/191"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@classiclarryd&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;59&lt;/td&gt; 
   &lt;td&gt;1.781 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/classiclarryd/status/2010545452832407943"&gt;Fused triton kernel for linear relu square MLP step&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;01/10/26&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2026-01-10_FusedLinearReLUSquare/3c47e63b-075e-4b5b-9c76-9dbe7bad9ad4.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/197"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@andrewbriand, @jrauvola&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;60&lt;/td&gt; 
   &lt;td&gt;1.765 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/classiclarryd/status/2012927211448516796"&gt;Fused triton kernel for softcapped multi-token prediction cross entropy step&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;01/16/26&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2026-01-16_FusedSoftcappedEntropy/45beba56-93e2-4995-bc5b-caff3cb2c1b5.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/199"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@soren_dunn_ &amp;amp; AI System &lt;a href="https://www.intology.ai/blog/previewing-locus"&gt;Locus&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;61&lt;/td&gt; 
   &lt;td&gt;1.748 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/classiclarryd/status/2013399457841160702"&gt;Unified Optimizers and Transposed LM Head&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;01/18/26&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2026-01-18_UnifiedOptimizers/unified-optimizer/2fc79469-a527-4bde-8540-8426ed3352d1.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/200"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@ChrisJMcCormick&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;62&lt;/td&gt; 
   &lt;td&gt;1.655 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/classiclarryd/status/2013520088297558274"&gt;Bigram Hash Embedding&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;01/19/26&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2026-01-19_BigramHashEmbedding/40ec7bb6-14b3-46f8-90b7-bb5ed188faba.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/201"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@classiclarryd&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Rules&lt;/h2&gt; 
&lt;p&gt;New records must:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Not modify the train or validation data pipelines. (You can change the batch size, sequence length, attention structure etc.; just don't change the underlying streams of tokens.)&lt;/li&gt; 
 &lt;li&gt;Attain ‚â§3.28 mean val loss. (Due to inter-run variance, submissions must provide enough run logs to attain a statistical significance level of p&amp;lt;0.01 that their mean val loss is ‚â§3.28. Example code to compute p-value can be found &lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-01-04_SoftCap#softer-softcap"&gt;here&lt;/a&gt;. For submissions which improve speed by optimizing the systems performance, without touching the ML, this requirement is waived.)&lt;/li&gt; 
 &lt;li&gt;Not use any extra &lt;code&gt;torch._inductor.config&lt;/code&gt; or &lt;code&gt;torch.compile&lt;/code&gt; flags. (These can save a few seconds, but they can also make compilation take &amp;gt;30min. This rule was introduced after the 21st record.)&lt;/li&gt; 
 &lt;li&gt;Run faster than the prior record when baselined on the same hardware.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Discretionary reasons why a PR may not be accepted:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Disproportionately degrades the readability of the codebase. A 200 line kernel to drop 300ms is considered worthwhile. 500 lines that convolute the optimizer layout for a 50ms gain will likely be rejected.&lt;/li&gt; 
 &lt;li&gt;The current record is intentionally kept roughly 0.001-0.002 loss below 3.28 to make validation simpler. If a PR substantially consumes this buffer, it should do so in a way that outperforms a simple step count decrease, when measured at equivalent loss.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Note: &lt;code&gt;torch._inductor.config.coordinate_descent_tuning&lt;/code&gt; is allowed for GPT-2 Medium track (a.k.a. 2.92 track).&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Other than that, anything and everything is fair game!&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/KellerJordan/modded-nanogpt/discussions/23?sort=new#discussioncomment-12109560"&gt;further clarifications&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Comment on the target metric&lt;/h3&gt; 
&lt;p&gt;The target metric is &lt;em&gt;cross-entropy loss on the FineWeb val set&lt;/em&gt;. To speak mathematically, the goal of the speedrun is *to obtain a probability model of language which assigns a probability of at least &lt;code&gt;math.exp(-3.28 * 10485760)&lt;/code&gt; to the first 10,485,760 tokens of the FineWeb valset. Hence, e.g., we allow evaluation at any sequence length, so long as we still have a valid probability model of language.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Timing change after record 21&lt;/h3&gt; 
&lt;p&gt;After the 21st record, we made two changes to the timing. First, there used to be an initial "grace period" of 10 untimed steps to allow kernel warmup. We replaced this with an explicit kernel-warmup section which is untimed and uses dummy data. This results in an extra runtime of 850ms from the 10 extra timed steps. Second, we banned the use of &lt;code&gt;torch._inductor.config.coordinate_descent_tuning&lt;/code&gt;. This saves ~25min of untimed pre-run compilation, but results in an extra runtime of ~3s.&lt;/p&gt; 
&lt;!--Note: The original llm.c baseline is intended to be closer to a replication of GPT-2 than to an optimized LLM training.
So it's no surprise that there is room to improve; as @karpathy has said, 'llm.c still has a lot of pending optimizations.'
In addition, many of the techniques used in these records are completely standard, such as rotary embeddings.
The goal of this benchmark/speedrun is simply to find out which techniques actually work, and maybe come up with some new ones.--&gt; 
&lt;!--The goal of this benchmark is simply to find out all the techniques which actually work, because I'm going crazy reading all these
LLM training papers
which claim a huge benefit but then use their own idiosyncratic non-competitive benchmark and therefore no one in the community has any idea if it's legit for months.--&gt; 
&lt;!--[LLM](https://arxiv.org/abs/2305.14342) [training](https://arxiv.org/abs/2402.17764) [papers](https://arxiv.org/abs/2410.01131)--&gt; 
&lt;!--I mean hello??? We're in a completely empirical field; it is insane to not have a benchmark. Ideally everyone uses the same LLM training benchmark,
and then reviewing LLM training papers becomes as simple as checking if they beat the benchmark. It's not like this would be unprecedented, that's how things
were in the ImageNet days.
The only possible 'benefit' I can think of for any empirical field to abandon benchmarks is that it would make it easier to publish false results. Oh, I guess that's why it happened.
Hilarious to think about how, in the often-commented-upon and ongoing collapse of the peer review system, people blame the *reviewers* --
yeah, those guys doing free labor who everyone constantly musters all of their intelligence to lie to, it's *their* fault! My bad, you caught me monologuing.--&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Notable attempts &amp;amp; forks&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Notable runs:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://x.com/alexjc/status/1881410039639863622"&gt;@alexjc's 01/20/2025 2.77-minute TokenMonster-based record&lt;/a&gt;. This record is technically outside the rules of the speedrun, since we specified that the train/val tokens must be kept fixed. However, it's very interesting, and worth including. The run is not more data-efficient; rather, the speedup comes from the improved tokenizer allowing the vocabulary size to be reduced (nearly halved!) while preserving the same bytes-per-token, which saves lots of parameters and FLOPs in the head and embeddings.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Notable forks:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/BlinkDL/modded-nanogpt-rwkv"&gt;https://github.com/BlinkDL/modded-nanogpt-rwkv&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/nikhilvyas/modded-nanogpt-SOAP"&gt;https://github.com/nikhilvyas/modded-nanogpt-SOAP&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Speedrun track 2: GPT-2 Medium&lt;/h2&gt; 
&lt;p&gt;The target loss for this track is lowered from 3.28 to 2.92, as per Andrej Karpathy's 350M-parameter llm.c baseline. This baseline generates a model with performance similar to the original GPT-2 Medium, whereas the first track's baseline generates a model on par with GPT-2 Small. All other rules remain the same.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Note: &lt;code&gt;torch._inductor.config.coordinate_descent_tuning&lt;/code&gt; is turned on after the record 6 (*).&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;#&lt;/th&gt; 
   &lt;th&gt;Record time&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Date&lt;/th&gt; 
   &lt;th&gt;Log&lt;/th&gt; 
   &lt;th&gt;Contributors&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;1&lt;/td&gt; 
   &lt;td&gt;5.8 hours&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/karpathy/llm.c/discussions/481"&gt;llm.c baseline (350M parameters)&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;05/28/24&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_2_medium/2025-01-18/main.log"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@karpathy, llm.c contributors&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;29.3 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/kellerjordan0/status/1881959719012847703"&gt;Initial record based on scaling up the GPT-2 small track speedrun&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;01/18/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_2_medium/2025-01-18/241dd7a7-3d76-4dce-85a4-7df60387f32a.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@kellerjordan0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;3&lt;/td&gt; 
   &lt;td&gt;28.1 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/kellerjordan0/status/1888320690543284449"&gt;Added standard weight decay&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;02/08/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_2_medium/2025-02-08_WeightDecay/b01743db-605c-4326-b5b1-d388ee5bebc5.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@kellerjordan0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;4&lt;/td&gt; 
   &lt;td&gt;27.7 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/leloykun/status/1892793848163946799"&gt;Tuned Muon Newton-Schulz coefficients&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;02/14/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_2_medium/2025-02-14_OptCoeffs/1baa66b2-bff7-4850-aced-d63885ffb4b6.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@leloykun&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;27.2 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_2_medium/2025-03-06_LongerCooldown/779c041a-2a37-45d2-a18b-ec0f223c2bb7.txt"&gt;Increased learning rate cooldown phase duration&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;03/06/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_2_medium/2025-03-06_LongerCooldown/779c041a-2a37-45d2-a18b-ec0f223c2bb7.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@YouJiacheng&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;6&lt;/td&gt; 
   &lt;td&gt;25.95 minutes*&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/YouJiacheng/status/1905861218138804534"&gt;2x MLP wd, qkv norm, all_reduce/opt.step() overlap, optimized skip pattern&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;03/25/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_2_medium/2025-03-25_ArchOptTweaks/train_gpt-20250329.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@YouJiacheng&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;7&lt;/td&gt; 
   &lt;td&gt;25.29 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/YouJiacheng/status/1912570883878842527"&gt;Remove FP8 head; ISRU logits softcap; New sharded mixed precision Muon; merge weights&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;04/16/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_2_medium/2025-04-16_Record7/223_3310d0b1-b24d-48ee-899f-d5c2a254a195.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@YouJiacheng&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;24.50 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://x.com/jadenj3o/status/1914893086276169754"&gt;Cubic sliding window size schedule, 2√ó max window size (24.84 minutes)&lt;/a&gt; &lt;a href="https://x.com/YouJiacheng/status/1915667616913645985"&gt;24.5min repro&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;04/22/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_2_medium/2025-04-22_Record8/075_640429f2-e726-4e83-aa27-684626239ffc.txt"&gt;log&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@jadenj3o&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;9&lt;/td&gt; 
   &lt;td&gt;24.12 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://snimu.github.io/2025/10/07/modded-nanogpt-value-embeddings.html"&gt;Add two value embeddings&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;08/28/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_2_medium/2025-08-28_NewValemb/036_61ef4351-7b68-4897-b440-a99221a1a629.txt"&gt;log&lt;/a&gt;, &lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/119"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@snimu&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;10&lt;/td&gt; 
   &lt;td&gt;24.07 minutes&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://snimu.github.io/2025/10/10/modded-nanogpt-x0.html"&gt;Second input embedding&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;09/11/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_2_medium/2025-09-11_SecondInputEmbed/000_592014ec-6781-4f59-b274-c4af68ccfe75.txt"&gt;log&lt;/a&gt;, &lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/124"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@snimu&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;11&lt;/td&gt; 
   &lt;td&gt;23.45 minutes&lt;/td&gt; 
   &lt;td&gt;Upgrade from torch 2.7 to torch==2.10.0.dev20251210+cu126&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;12&lt;/td&gt; 
   &lt;td&gt;23.28 minutes&lt;/td&gt; 
   &lt;td&gt;Snoo Optimizer (Outer optimizer around Adam and Muon)&lt;/td&gt; 
   &lt;td&gt;09/16/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_2_medium/2025-09-16_Snoo/000_01db7a67-f715-4114-a7b5-6bfe23bac1b1.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/128"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@dominikkallusky&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;13&lt;/td&gt; 
   &lt;td&gt;23.14 minutes&lt;/td&gt; 
   &lt;td&gt;EMA Wrapper on Muon&lt;/td&gt; 
   &lt;td&gt;09/17/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_2_medium/2025-09-17_UpdateSmoothing/001_8379f695-6bc3-4f76-b58b-8fadd3b6ebb0.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/129"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@acutkosky&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;14&lt;/td&gt; 
   &lt;td&gt;23.08 minutes&lt;/td&gt; 
   &lt;td&gt;Combine both records 12 &amp;amp; 13&lt;/td&gt; 
   &lt;td&gt;09/30/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_2_medium/2025-09-30_SmoothedSnooMedium/101_5bc91cd0-cb46-428c-a5da-9d8d228f1f97.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/137"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@acutkosky&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;15&lt;/td&gt; 
   &lt;td&gt;23.03 minutes&lt;/td&gt; 
   &lt;td&gt;Backout (Skip from 2/3 point to pre-lm_head)&lt;/td&gt; 
   &lt;td&gt;10/04/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_2_medium/2025-10-04_GPT2MediumLayerReuse/000_cc3943e4-02b5-4ae3-9441-839d32dfd9b2.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/139"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@snimu&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;22.99 minutes&lt;/td&gt; 
   &lt;td&gt;Smear-MTP&lt;/td&gt; 
   &lt;td&gt;11/02/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_2_medium/2025-11-02-Smear-MTP/000_3b50518d-d542-44bc-8566-3abf633f83ad.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/151"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@snimu&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;17&lt;/td&gt; 
   &lt;td&gt;22.98 minutes&lt;/td&gt; 
   &lt;td&gt;Remove Redundant Mask Op&lt;/td&gt; 
   &lt;td&gt;11/12/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_2_medium/2025-11-12_BlockMaskRedundantOp/000_3b22a9d4-b52e-4916-99bf-3d48b38747a7.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/157/"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@manikbhandari&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;18&lt;/td&gt; 
   &lt;td&gt;17.35 minutes&lt;/td&gt; 
   &lt;td&gt;Bulk transfer short track features&lt;/td&gt; 
   &lt;td&gt;12/31/25&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_2_medium/2025-12-31_BulkSmallTrackTransfer/354be270-7d41-44b7-8064-f040923f024f.txt"&gt;log&lt;/a&gt;,&lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/188"&gt;PR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Q: What is the point of NanoGPT speedrunning?&lt;/h3&gt; 
&lt;p&gt;A: The officially stated goal of NanoGPT speedrunning is as follows: &lt;code&gt;gotta go fast&lt;/code&gt;. But for something a little more verbose involving an argument for good benchmarking, here's some kind of manifesto, adorned with a blessing from the master. &lt;a href="https://x.com/karpathy/status/1846790537262571739"&gt;https://x.com/karpathy/status/1846790537262571739&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Q: What makes "NanoGPT speedrunning" not just another idiosyncratic benchmark?&lt;/h3&gt; 
&lt;p&gt;A: Because it is a &lt;em&gt;competitive&lt;/em&gt; benchmark. In particular, if you attain a new speed record (using whatever method you want), there is an open invitation for you to post that record (on arXiv or X) and thereby vacuum up all the clout for yourself. I will even help you do it by reposting you as much as I can.&lt;/p&gt; 
&lt;!--On the contrary, for example, the benchmark used in the [Sophia](https://arxiv.org/abs/2305.14342) paper does *not* have this property.
There is no such open invitation for anyone to compete on the benchmark they used. In particular, if, for a random and definitely not weirdly specific example, you happen to find better AdamW hyperparameters for their training setup than
the ones they used which significantly close the gap between AdamW and their proposed optimizer,
then there is no clear path for you to publish that result in *any* form.
You could try posting it on X.com, but then you would be risking being perceived as aggressive/confrontational, which is *not a good look* in this racket.
So if you're rational, the result probably just dies with you and no one else learns anything
(unless you're in a frontier lab, in which case you can do a nice internal writeup. Boy I'd love to get my hands on those writeups).--&gt; 
&lt;p&gt;&lt;a href="https://www.argmin.net/p/too-much-information"&gt;"Artificial intelligence advances by inventing games and gloating to goad others to play" - Professor Ben Recht&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Q: NanoGPT speedrunning is cool and all, but meh it probably won't scale and is just overfitting to val loss&lt;/h3&gt; 
&lt;p&gt;A: This is hard to refute, since "at scale" is an infinite category (what if the methods stop working only for &amp;gt;100T models?), making it impossible to fully prove. Also, I would agree that some of the methods used in the speedrun are unlikely to scale, particularly those which &lt;em&gt;impose additional structure&lt;/em&gt; on the network, such as logit softcapping. But if the reader cares about 1.5B models, they might be convinced by this result:&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Straightforwardly scaling up the speedrun (10/18/24 version) to 1.5B parameters yields a model with GPT-2 (1.5B)-level HellaSwag performance 2.5x more cheaply than &lt;a href="https://github.com/karpathy/llm.c/discussions/677"&gt;@karpathy's baseline&lt;/a&gt; ($233 instead of $576):&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/img/nanogpt_speedrun51.png" alt="" /&gt; [&lt;a href="https://github.com/KellerJordan/modded-nanogpt/raw/master/records/track_1_short/2024-10-20_ScaleUp1B/ad8d7ae5-7b2d-4ee9-bc52-f912e9174d7a.txt"&gt;reproducible log&lt;/a&gt;] &lt;img src="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/img/nanogpt_speedrun52.png" alt="" /&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;&lt;a href="https://github.com/KellerJordan/Muon"&gt;Muon optimizer&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;Muon is defined as follows:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/img/algo_optimizer.png" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;Where NewtonSchulz5 is the following Newton-Schulz iteration [2, 3], which approximately replaces &lt;code&gt;G&lt;/code&gt; with &lt;code&gt;U @ V.T&lt;/code&gt; where &lt;code&gt;U, S, V = G.svd()&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;@torch.compile
def zeroth_power_via_newtonschulz5(G, steps=5, eps=1e-7):
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16() / (G.norm() + eps)
    if G.size(0) &amp;gt; G.size(1):
        X = X.T 
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A
        X = a * X + B @ X
    if G.size(0) &amp;gt; G.size(1):
        X = X.T 
    return X.to(G.dtype)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For this training scenario, Muon has the following favorable properties:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Lower memory usage than Adam&lt;/li&gt; 
 &lt;li&gt;~1.5x better sample-efficiency&lt;/li&gt; 
 &lt;li&gt;&amp;lt;2% wallclock overhead&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Provenance&lt;/h3&gt; 
&lt;p&gt;Many of the choices made to generate this optimizer were obtained experimentally by our pursuit of &lt;a href="https://github.com/KellerJordan/cifar10-airbench"&gt;CIFAR-10 speedrunning&lt;/a&gt;. In particular, we experimentally obtained the following practices:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Using Nesterov momentum inside the update, with orthogonalization applied after momentum.&lt;/li&gt; 
 &lt;li&gt;Using a specifically quintic Newton-Schulz iteration as the method of orthogonalization.&lt;/li&gt; 
 &lt;li&gt;Using non-convergent coefficients for the quintic polynomial in order to maximize slope at zero, and thereby minimize the number of necessary Newton-Schulz iterations. It turns out that the variance doesn't actually matter that much, so we end up with a quintic that rapidly converges to the range 0.68, 1.13 upon repeated application, rather than converging more slowly to 1.&lt;/li&gt; 
 &lt;li&gt;Running the Newton-Schulz iteration in bfloat16 (whereas Shampoo implementations often depend on inverse-pth-roots run in fp32 or fp64).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Our use of a Newton-Schulz iteration for orthogonalization traces to &lt;a href="https://arxiv.org/abs/2409.20325"&gt;Bernstein &amp;amp; Newhouse (2024)&lt;/a&gt;, who suggested it as a way to compute Shampoo [5, 6] preconditioners, and theoretically explored Shampoo without preconditioner accumulation. In particular, Jeremy Bernstein @jxbz sent us the draft, which caused us to experiment with various Newton-Schulz iterations as the orthogonalization method for this optimizer. If we had used SVD instead of a Newton-Schulz iteration, this optimizer would have been too slow to be useful. Bernstein &amp;amp; Newhouse also pointed out that Shampoo without preconditioner accumulation is equivalent to steepest descent in the spectral norm, and therefore Shampoo can be thought of as a way to smooth out spectral steepest descent. The proposed optimizer can be thought of as a second way of smoothing spectral steepest descent, with a different set of memory and runtime tradeoffs compared to Shampoo.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Running on fewer GPUs&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;To run experiments on fewer GPUs, simply modify &lt;code&gt;run.sh&lt;/code&gt; to have a different &lt;code&gt;--nproc_per_node&lt;/code&gt;. This should not change the behavior of the training.&lt;/li&gt; 
 &lt;li&gt;If you're running out of memory, you may need to reduce the sequence length for FlexAttention (which does change the training. see &lt;a href="https://github.com/KellerJordan/modded-nanogpt/pull/38"&gt;here&lt;/a&gt; for a guide)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;References&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/2406.17557"&gt;Guilherme Penedo et al. "The fineweb datasets: Decanting the web for the finest text data at scale." arXiv preprint arXiv:2406.17557 (2024).&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Nicholas J. Higham. Functions of Matrices. Society for Industrial and Applied Mathematics (2008). Equation 5.22.&lt;/li&gt; 
 &lt;li&gt;G√É¬ºnther Schulz. Iterative Berechnung der reziproken Matrix. Z. Angew. Math. Mech., 13:57√¢¬Ä¬ì59 (1933).&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/2409.20325"&gt;Jeremy Bernstein and Laker Newhouse. "Old Optimizer, New Norm: An Anthology." arxiv preprint arXiv:2409.20325 (2024).&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/1802.09568"&gt;Vineet Gupta, Tomer Koren, and Yoram Singer. "Shampoo: Preconditioned stochastic tensor optimization." International Conference on Machine Learning. PMLR, 2018.&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/2002.09018"&gt;Rohan Anil et al. "Scalable second order optimization for deep learning." arXiv preprint arXiv:2002.09018 (2020).&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/2405.18392"&gt;Alexander H√É¬§gele et al. "Scaling Laws and Compute-Optimal Training Beyond Fixed Training Durations." arXiv preprint arXiv:2405.18392 (2024).&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/2410.17897"&gt;Zhanchao Zhou et al. "Value Residual Learning For Alleviating Attention Concentration In Transformers." arXiv preprint arXiv:2410.17897 (2024).&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://arxiv.org/abs/2408.00118"&gt;Team, Gemma, et al. "Gemma 2: Improving open language models at a practical size." arXiv preprint arXiv:2408.00118 (2024).&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf"&gt;Alec Radford et al. "Language models are unsupervised multitask learners." OpenAI blog 1.8 (2019).&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;@misc{modded_nanogpt_2024,
  author       = {Keller Jordan and Jeremy Bernstein and Brendan Rappazzo and
                  @fernbear.bsky.social and Boza Vlado and You Jiacheng and
                  Franz Cesista and Braden Koszarsky and @Grad62304977},
  title        = {modded-nanogpt: Speedrunning the NanoGPT baseline},
  year         = {2024},
  url          = {https://github.com/KellerJordan/modded-nanogpt}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;img src="https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/img/dofa.jpg" alt="itsover_wereback" style="width:100%;" /&gt;</description>
    </item>
    
    <item>
      <title>Blaizzy/mlx-audio</title>
      <link>https://github.com/Blaizzy/mlx-audio</link>
      <description>&lt;p&gt;A text-to-speech (TTS), speech-to-text (STT) and speech-to-speech (STS) library built on Apple's MLX framework, providing efficient speech analysis on Apple Silicon.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MLX-Audio&lt;/h1&gt; 
&lt;p&gt;The best audio processing library built on Apple's MLX framework, providing fast and efficient text-to-speech (TTS), speech-to-text (STT), and speech-to-speech (STS) on Apple Silicon.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Fast inference optimized for Apple Silicon (M series chips)&lt;/li&gt; 
 &lt;li&gt;Multiple model architectures for TTS, STT, and STS&lt;/li&gt; 
 &lt;li&gt;Multilingual support across models&lt;/li&gt; 
 &lt;li&gt;Voice customization and cloning capabilities&lt;/li&gt; 
 &lt;li&gt;Adjustable speech speed control&lt;/li&gt; 
 &lt;li&gt;Interactive web interface with 3D audio visualization&lt;/li&gt; 
 &lt;li&gt;OpenAI-compatible REST API&lt;/li&gt; 
 &lt;li&gt;Quantization support (3-bit, 4-bit, 6-bit, 8-bit, and more) for optimized performance&lt;/li&gt; 
 &lt;li&gt;Swift package for iOS/macOS integration&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;Using pip&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install mlx-audio
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Using uv to install only the command line tools&lt;/h3&gt; 
&lt;p&gt;Latest release from pypi:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv tool install --force mlx-audio --prerelease=allow
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Latest code from github:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv tool install --force git+https://github.com/Blaizzy/mlx-audio.git --prerelease=allow
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;For development or web interface:&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/Blaizzy/mlx-audio.git
cd mlx-audio
pip install -e ".[dev]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;h3&gt;Command Line&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Basic TTS generation
mlx_audio.tts.generate --model mlx-community/Kokoro-82M-bf16 --text 'Hello, world!' --lang_code a

# With voice selection and speed adjustment
mlx_audio.tts.generate --model mlx-community/Kokoro-82M-bf16 --text 'Hello!' --voice af_heart --speed 1.2 --lang_code a

# Play audio immediately
mlx_audio.tts.generate --model mlx-community/Kokoro-82M-bf16 --text 'Hello!' --play  --lang_code a

# Save to a specific directory
mlx_audio.tts.generate --model mlx-community/Kokoro-82M-bf16 --text 'Hello!' --output_path ./my_audio  --lang_code a
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Python API&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mlx_audio.tts.utils import load_model

# Load model
model = load_model("mlx-community/Kokoro-82M-bf16")

# Generate speech
for result in model.generate("Hello from MLX-Audio!", voice="af_heart"):
    print(f"Generated {result.audio.shape[0]} samples")
    # result.audio contains the waveform as mx.array
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Supported Models&lt;/h2&gt; 
&lt;h3&gt;Text-to-Speech (TTS)&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Languages&lt;/th&gt; 
   &lt;th&gt;Repo&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Kokoro&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Fast, high-quality multilingual TTS&lt;/td&gt; 
   &lt;td&gt;EN, JA, ZH, FR, ES, IT, PT, HI&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/Kokoro-82M-bf16"&gt;mlx-community/Kokoro-82M-bf16&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Qwen3-TTS&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Alibaba's multilingual TTS with voice design&lt;/td&gt; 
   &lt;td&gt;ZH, EN, JA, KO, + more&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/Qwen3-TTS-12Hz-1.7B-VoiceDesign-bf16"&gt;mlx-community/Qwen3-TTS-12Hz-1.7B-VoiceDesign-bf16&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;CSM&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Conversational Speech Model with voice cloning&lt;/td&gt; 
   &lt;td&gt;EN&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/csm-1b"&gt;mlx-community/csm-1b&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Dia&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Dialogue-focused TTS&lt;/td&gt; 
   &lt;td&gt;EN&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/Dia-1.6B-bf16"&gt;mlx-community/Dia-1.6B-bf16&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;OuteTTS&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Efficient TTS model&lt;/td&gt; 
   &lt;td&gt;EN&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/OuteTTS-0.2-500M"&gt;mlx-community/OuteTTS-0.2-500M&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Spark&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;SparkTTS model&lt;/td&gt; 
   &lt;td&gt;EN, ZH&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/SparkTTS-0.5B-bf16"&gt;mlx-community/SparkTTS-0.5B-bf16&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Chatterbox&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Expressive multilingual TTS&lt;/td&gt; 
   &lt;td&gt;EN, ES, FR, DE, IT, PT, PL, TR, RU, NL, CS, AR, ZH, JA, HU, KO&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/Chatterbox-bf16"&gt;mlx-community/Chatterbox-bf16&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Soprano&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;High-quality TTS&lt;/td&gt; 
   &lt;td&gt;EN&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/Soprano-bf16"&gt;mlx-community/Soprano-bf16&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Speech-to-Text (STT)&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Languages&lt;/th&gt; 
   &lt;th&gt;Repo&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Whisper&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;OpenAI's robust STT model&lt;/td&gt; 
   &lt;td&gt;99+ languages&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/whisper-large-v3-turbo-asr-fp16"&gt;mlx-community/whisper-large-v3-turbo-asr-fp16&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Parakeet&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;NVIDIA's accurate STT&lt;/td&gt; 
   &lt;td&gt;EN&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/parakeet-tdt-0.6b-v2"&gt;mlx-community/parakeet-tdt-0.6b-v2&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Voxtral&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Mistral's speech model&lt;/td&gt; 
   &lt;td&gt;Multiple&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/Voxtral-Mini-3B-2507-bf16"&gt;mlx-community/Voxtral-Mini-3B-2507-bf16&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;VibeVoice-ASR&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Microsoft's 9B ASR with diarization &amp;amp; timestamps&lt;/td&gt; 
   &lt;td&gt;Multiple&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/VibeVoice-ASR-bf16"&gt;mlx-community/VibeVoice-ASR-bf16&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Speech-to-Speech (STS)&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Use Case&lt;/th&gt; 
   &lt;th&gt;Repo&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;SAM-Audio&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Text-guided source separation&lt;/td&gt; 
   &lt;td&gt;Extract specific sounds&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/sam-audio-large"&gt;mlx-community/sam-audio-large&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Liquid2.5-Audio&lt;/strong&gt;*&lt;/td&gt; 
   &lt;td&gt;Speech-to-Speech, Text-to-Speech and Speech-to-Text&lt;/td&gt; 
   &lt;td&gt;Speech interactions&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/mlx-community/LFM2.5-Audio-1.5B-8bit"&gt;mlx-community/LFM2.5-Audio-1.5B-8bit&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;MossFormer2 SE&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Speech enhancement&lt;/td&gt; 
   &lt;td&gt;Noise removal&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/starkdmi/MossFormer2_SE_48K_MLX"&gt;starkdmi/MossFormer2_SE_48K_MLX&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Model Examples&lt;/h2&gt; 
&lt;h3&gt;Kokoro TTS&lt;/h3&gt; 
&lt;p&gt;Kokoro is a fast, multilingual TTS model with 54 voice presets.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mlx_audio.tts.utils import load_model

model = load_model("mlx-community/Kokoro-82M-bf16")

# Generate with different voices
for result in model.generate(
    text="Welcome to MLX-Audio!",
    voice="af_heart",  # American female
    speed=1.0,
    lang_code="a"  # American English
):
    audio = result.audio
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Available Voices:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;American English: &lt;code&gt;af_heart&lt;/code&gt;, &lt;code&gt;af_bella&lt;/code&gt;, &lt;code&gt;af_nova&lt;/code&gt;, &lt;code&gt;af_sky&lt;/code&gt;, &lt;code&gt;am_adam&lt;/code&gt;, &lt;code&gt;am_echo&lt;/code&gt;, etc.&lt;/li&gt; 
 &lt;li&gt;British English: &lt;code&gt;bf_alice&lt;/code&gt;, &lt;code&gt;bf_emma&lt;/code&gt;, &lt;code&gt;bm_daniel&lt;/code&gt;, &lt;code&gt;bm_george&lt;/code&gt;, etc.&lt;/li&gt; 
 &lt;li&gt;Japanese: &lt;code&gt;jf_alpha&lt;/code&gt;, &lt;code&gt;jm_kumo&lt;/code&gt;, etc.&lt;/li&gt; 
 &lt;li&gt;Chinese: &lt;code&gt;zf_xiaobei&lt;/code&gt;, &lt;code&gt;zm_yunxi&lt;/code&gt;, etc.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Language Codes:&lt;/strong&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Code&lt;/th&gt; 
   &lt;th&gt;Language&lt;/th&gt; 
   &lt;th&gt;Note&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;a&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;American English&lt;/td&gt; 
   &lt;td&gt;Default&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;b&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;British English&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;j&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Japanese&lt;/td&gt; 
   &lt;td&gt;Requires &lt;code&gt;pip install misaki[ja]&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;z&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Mandarin Chinese&lt;/td&gt; 
   &lt;td&gt;Requires &lt;code&gt;pip install misaki[zh]&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;e&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Spanish&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;f&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;French&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Qwen3-TTS&lt;/h3&gt; 
&lt;p&gt;Alibaba's state-of-the-art multilingual TTS with voice cloning, emotion control, and voice design capabilities.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mlx_audio.tts.utils import load_model

model = load_model("mlx-community/Qwen3-TTS-12Hz-0.6B-Base-bf16")
results = list(model.generate(
    text="Hello, welcome to MLX-Audio!",
    voice="Chelsie",
    language="English",
))

audio = results[0].audio  # mx.array
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See the &lt;a href="https://raw.githubusercontent.com/Blaizzy/mlx-audio/main/mlx_audio/tts/models/qwen3_tts/README.md"&gt;Qwen3-TTS README&lt;/a&gt; for voice cloning, CustomVoice, VoiceDesign, and all available models.&lt;/p&gt; 
&lt;h3&gt;CSM (Voice Cloning)&lt;/h3&gt; 
&lt;p&gt;Clone any voice using a reference audio sample:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;mlx_audio.tts.generate \
    --model mlx-community/csm-1b \
    --text "Hello from Sesame." \
    --ref_audio ./reference_voice.wav \
    --play
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Whisper STT&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mlx_audio.stt.generate import generate_transcription

result = generate_transcription(
    model="mlx-community/whisper-large-v3-turbo-asr-fp16",
    audio="audio.wav",
)
print(result.text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;VibeVoice-ASR&lt;/h3&gt; 
&lt;p&gt;Microsoft's 9B parameter speech-to-text model with speaker diarization and timestamps. Supports long-form audio (up to 60 minutes) and outputs structured JSON.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mlx_audio.stt.utils import load

model = load("mlx-community/VibeVoice-ASR-bf16")

# Basic transcription
result = model.generate(audio="meeting.wav", max_tokens=8192, temperature=0.0)
print(result.text)
# [{"Start":0,"End":5.2,"Speaker":0,"Content":"Hello everyone, let's begin."},
#  {"Start":5.5,"End":9.8,"Speaker":1,"Content":"Thanks for joining today."}]

# Access parsed segments
for seg in result.segments:
    print(f"[{seg['start_time']:.1f}-{seg['end_time']:.1f}] Speaker {seg['speaker_id']}: {seg['text']}")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Streaming transcription:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Stream tokens as they are generated
for text in model.stream_transcribe(audio="speech.wav", max_tokens=4096):
    print(text, end="", flush=True)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;With context (hotwords/metadata):&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;result = model.generate(
    audio="technical_talk.wav",
    context="MLX, Apple Silicon, PyTorch, Transformer",
    max_tokens=8192,
    temperature=0.0,
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;CLI usage:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Basic transcription
python -m mlx_audio.stt.generate \
    --model mlx-community/VibeVoice-ASR-bf16 \
    --audio meeting.wav \
    --output-path output \
    --format json \
    --max-tokens 8192 \
    --verbose

# With context/hotwords
python -m mlx_audio.stt.generate \
    --model mlx-community/VibeVoice-ASR-bf16 \
    --audio technical_talk.wav \
    --output-path output \
    --format json \
    --max-tokens 8192 \
    --context "MLX, Apple Silicon, PyTorch, Transformer" \
    --verbose
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;SAM-Audio (Source Separation)&lt;/h3&gt; 
&lt;p&gt;Separate specific sounds from audio using text prompts:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mlx_audio.sts import SAMAudio, SAMAudioProcessor, save_audio

model = SAMAudio.from_pretrained("mlx-community/sam-audio-large")
processor = SAMAudioProcessor.from_pretrained("mlx-community/sam-audio-large")

batch = processor(
    descriptions=["A person speaking"],
    audios=["mixed_audio.wav"],
)

result = model.separate_long(
    batch.audios,
    descriptions=batch.descriptions,
    anchors=batch.anchor_ids,
    chunk_seconds=10.0,
    overlap_seconds=3.0,
    ode_opt={"method": "midpoint", "step_size": 2/32},
)

save_audio(result.target[0], "voice.wav")
save_audio(result.residual[0], "background.wav")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;MossFormer2 (Speech Enhancement)&lt;/h3&gt; 
&lt;p&gt;Remove noise from speech recordings:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from mlx_audio.sts import MossFormer2SEModel, save_audio

model = MossFormer2SEModel.from_pretrained("starkdmi/MossFormer2_SE_48K_MLX")
enhanced = model.enhance("noisy_speech.wav")
save_audio(enhanced, "clean.wav", 48000)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Web Interface &amp;amp; API Server&lt;/h2&gt; 
&lt;p&gt;MLX-Audio includes a modern web interface and OpenAI-compatible API.&lt;/p&gt; 
&lt;h3&gt;Starting the Server&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Start API server
mlx_audio.server --host 0.0.0.0 --port 8000

# Start web UI (in another terminal)
cd mlx_audio/ui
npm install &amp;amp;&amp;amp; npm run dev
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;API Endpoints&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Text-to-Speech&lt;/strong&gt; (OpenAI-compatible):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -X POST http://localhost:8000/v1/audio/speech \
  -H "Content-Type: application/json" \
  -d '{"model": "mlx-community/Kokoro-82M-bf16", "input": "Hello!", "voice": "af_heart"}' \
  --output speech.wav
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Speech-to-Text&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -X POST http://localhost:8000/v1/audio/transcriptions \
  -F "file=@audio.wav" \
  -F "model=mlx-community/whisper-large-v3-turbo-asr-fp16"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Quantization&lt;/h2&gt; 
&lt;p&gt;Reduce model size and improve performance with quantization using the convert script:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Convert and quantize to 4-bit
python -m mlx_audio.convert \
    --hf-path prince-canuma/Kokoro-82M \
    --mlx-path ./Kokoro-82M-4bit \
    --quantize \
    --q-bits 4 \
    --upload-repo username/Kokoro-82M-4bit (optional: if you want to upload the model to Hugging Face)

# Convert with specific dtype (bfloat16)
python -m mlx_audio.convert \
    --hf-path prince-canuma/Kokoro-82M \
    --mlx-path ./Kokoro-82M-bf16 \
    --dtype bfloat16 \
    --upload-repo username/Kokoro-82M-bf16 (optional: if you want to upload the model to Hugging Face)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Options:&lt;/strong&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Flag&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--hf-path&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Source Hugging Face model or local path&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--mlx-path&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Output directory for converted model&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-q, --quantize&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Enable quantization&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--q-bits&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Bits per weight (4, 6, or 8)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--q-group-size&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Group size for quantization (default: 64)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--dtype&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Weight dtype: &lt;code&gt;float16&lt;/code&gt;, &lt;code&gt;bfloat16&lt;/code&gt;, &lt;code&gt;float32&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--upload-repo&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Upload converted model to HF Hub&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Swift&lt;/h2&gt; 
&lt;p&gt;Looking for Swift/iOS support? Check out &lt;a href="https://github.com/Blaizzy/mlx-audio-swift"&gt;mlx-audio-swift&lt;/a&gt; for on-device TTS using MLX on macOS and iOS.&lt;/p&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.10+&lt;/li&gt; 
 &lt;li&gt;Apple Silicon Mac (M1/M2/M3/M4)&lt;/li&gt; 
 &lt;li&gt;MLX framework&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ffmpeg&lt;/strong&gt; (required for MP3/FLAC audio encoding)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Installing ffmpeg&lt;/h3&gt; 
&lt;p&gt;ffmpeg is required for saving audio in MP3 or FLAC format. Install it using:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# macOS (using Homebrew)
brew install ffmpeg

# Ubuntu/Debian
sudo apt install ffmpeg
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;WAV format works without ffmpeg.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/Blaizzy/mlx-audio/main/LICENSE"&gt;MIT License&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{mlx-audio,
  author = {Canuma, Prince},
  title = {MLX Audio},
  year = {2025},
  howpublished = {\url{https://github.com/Blaizzy/mlx-audio}},
  note = {Audio processing library for Apple Silicon with TTS, STT, and STS capabilities.}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ml-explore/mlx"&gt;Apple MLX Team&lt;/a&gt; for the MLX framework&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>Free-TV/IPTV</title>
      <link>https://github.com/Free-TV/IPTV</link>
      <description>&lt;p&gt;M3U Playlist for free TV channels&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Free TV&lt;/h1&gt; 
&lt;p&gt;This is an M3U playlist for free TV channels around the World.&lt;/p&gt; 
&lt;p&gt;Either free locally (over the air):&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/usa.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/us.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/canada.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/ca.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/uk.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/gb.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/ireland.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/ie.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/australia.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/au.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/india.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/in.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/japan.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/jp.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/china.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/cn.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/hong_kong.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/hk.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/macau.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/mo.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/taiwan.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/tw.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/north_korea.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/kp.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/korea.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/kr.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/denmark.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/dk.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/faroe_islands.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/fo.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/greenland.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/gl.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/finland.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/fi.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/iceland.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/is.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/norway.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/no.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/sweden.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/se.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/estonia.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/ee.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/latvia.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/lv.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/lithuania.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/lt.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/belgium.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/be.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/netherlands.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/nl.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/luxembourg.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/lu.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/germany.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/de.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/austria.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/at.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/switzerland.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/ch.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/poland.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/pl.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/czech_republic.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/cz.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/slovakia.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/sk.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/hungary.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/hu.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/romania.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/ro.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/moldova.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/md.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/bulgaria.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/bg.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/france.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/fr.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/italy.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/it.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/portugal.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/pt.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/spain.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/es.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/russia.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/ru.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/belarus.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/by.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/ukraine.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/ua.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/armenia.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/am.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/azerbaijan.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/az.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/georgia.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/ge.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/bosnia_and_herzegovina.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/ba.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/croatia.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/hr.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/montenegro.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/me.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/north_macedonia.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/mk.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/serbia.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/rs.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/slovenia.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/si.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/albania.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/al.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/kosovo.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/xk.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/greece.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/gr.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/cyprus.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/cy.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/andorra.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/ad.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/malta.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/mt.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/monaco.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/mc.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/san_marino.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/sm.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/iran.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/ir.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/iraq.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/iq.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/israel.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/il.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/qatar.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/qa.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/turkey.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/tr.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/united_arab_emirates.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/ae.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/argentina.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/ar.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/costa_rica.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/cr.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/dominican_republic.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/do.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/mexico.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/mx.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/paraguay.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/py.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/peru.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/pe.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/venezuela.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/ve.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/brazil.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/br.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/trinidad.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/tt.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/chad.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/td.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/somalia.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/so.svg?sanitize=true" width="24" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/lists/indonesia.md"&gt;&lt;img src="https://hatscripts.github.io/circle-flags/flags/id.svg?sanitize=true" width="24" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Or free on the Internet:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Plex TV&lt;/li&gt; 
 &lt;li&gt;Pluto TV (English, Spanish, French, Italian)&lt;/li&gt; 
 &lt;li&gt;Redbox Live TV&lt;/li&gt; 
 &lt;li&gt;Roku TV&lt;/li&gt; 
 &lt;li&gt;Samsung TV Plus&lt;/li&gt; 
 &lt;li&gt;Youtube live channels&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To use it point your IPTV player to &lt;a href="https://raw.githubusercontent.com/Free-TV/IPTV/master/playlist.m3u8"&gt;https://raw.githubusercontent.com/Free-TV/IPTV/master/playlist.m3u8&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Philosophy&lt;/h1&gt; 
&lt;p&gt;The main goals for this playlist are listed below.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Quality over quantity&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;The less channels we support the better.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;All channels should work well.&lt;/li&gt; 
 &lt;li&gt;As much as possible channels should be in HD, not SD.&lt;/li&gt; 
 &lt;li&gt;Only one URL per channel (no +1, no alternate feeds, no regional declinations)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Only free channels&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;If a channel is normally only available via commercial subscriptions it has nothing to do in this playlist. If on the other hand it is provided for free to everybody in a particular country, then it should be in this playlist.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;No paid channels&lt;/li&gt; 
 &lt;li&gt;Only channels which are officially provided for free (via DVB-S, DVB-T, analog, etc..)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Only mainstream channels&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;This is a playlist for everybody.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;No adult channels&lt;/li&gt; 
 &lt;li&gt;No channels dedicated to any particular religion&lt;/li&gt; 
 &lt;li&gt;No channels dedicated to any particular political party&lt;/li&gt; 
 &lt;li&gt;No channels made for a country and funded by a different country&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Feed sources&lt;/h1&gt; 
&lt;p&gt;It can be quite hard to find up to date URLs, here's a list of sources:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/iptv-org/iptv/tree/master/streams"&gt;https://github.com/iptv-org/iptv/tree/master/streams&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Youtube: As long as the channel is live and its URL doesn't change (check the age of the stream, the number of viewers..)&lt;/li&gt; 
 &lt;li&gt;Dailymotion: Same criteria as for youtube&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Format&lt;/h1&gt; 
&lt;p&gt;The m3u8 playlist is generated by &lt;code&gt;make_playlist.py&lt;/code&gt;, using the &lt;code&gt;.md&lt;/code&gt; files located in &lt;code&gt;lists&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Each .md file represesnts a group. The &lt;code&gt;&amp;lt;h1&amp;gt;&lt;/code&gt; line is used as the group title.&lt;/p&gt; 
&lt;p&gt;Only channels which URL column starts with &lt;code&gt;[&amp;gt;]&lt;/code&gt; are included in the playlist.&lt;/p&gt; 
&lt;p&gt;Channels which are not in HD are marked with an &lt;code&gt;‚ìà&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Channels which use GeoIP blocking are marked with a &lt;code&gt;‚íº&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Channels which are live Youtube channels are marked with a &lt;code&gt;‚ìé&lt;/code&gt;.&lt;/p&gt; 
&lt;h1&gt;Issues&lt;/h1&gt; 
&lt;p&gt;Only create issues for bugs and feature requests.&lt;/p&gt; 
&lt;p&gt;Do not create issues to add/edit or to remove channels. If you want to add/edit/remove channels, create a pull request directly.&lt;/p&gt; 
&lt;h1&gt;Pull Requests&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;Only modify .md files&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;If your Pull Request modifies channels, only modify .md files. Do not modify m3u8 files in your pull request.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Adding a new Channel&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;To add a new channel, make a Pull Request.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;In your Pull Request you need to provide information to show that the channel is free.&lt;/li&gt; 
 &lt;li&gt;Use imgur.com to host the channel logo and point to it.&lt;/li&gt; 
 &lt;li&gt;If you have a valid stream, add it and put &lt;code&gt;[&amp;gt;]&lt;/code&gt; in front of it.&lt;/li&gt; 
 &lt;li&gt;If you don't have an stream for the channel, add &lt;code&gt;[x]()&lt;/code&gt; in the url column and place your channel in the Invalid category.&lt;/li&gt; 
 &lt;li&gt;If you have a stream but it doesn't work well, put the channel in the Invalid category and put &lt;code&gt;[x]&lt;/code&gt; in front of the url.&lt;/li&gt; 
 &lt;li&gt;If you're adding geoblocked URLs specify it in your PR and specify which country they're working in. The PR will only be merged if these URLs can be tested.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Removing a Channel&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;To remove a channel, make a Pull Request.&lt;/p&gt; 
&lt;p&gt;In your Pull Request you need to provide information to show that the channel is only available via a private paid subscription.&lt;/p&gt; 
&lt;p&gt;Note: Public taxes (whether national or regional, whether called TV License or not) do not constitute a private paid subscription.&lt;/p&gt; 
&lt;p&gt;If a stream is broken, simply move the channel to the invalid category and replace &lt;code&gt;[&amp;gt;]&lt;/code&gt; with &lt;code&gt;[x]&lt;/code&gt; in the url column.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>business-science/ai-data-science-team</title>
      <link>https://github.com/business-science/ai-data-science-team</link>
      <description>&lt;p&gt;An AI-powered data science team of agents to help you perform common data science tasks 10X faster.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;a href="https://github.com/business-science/ai-data-science-team"&gt; 
  &lt;picture&gt; 
   &lt;img src="https://raw.githubusercontent.com/business-science/ai-data-science-team/master/img/ai_data_science_logo.png" alt="AI Data Science Team" width="360" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;em&gt;AI Data Science Team + AI Pipeline Studio&lt;/em&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;a href="https://pypi.python.org/pypi/ai-data-science-team"&gt;&lt;img src="https://img.shields.io/pypi/v/ai-data-science-team.svg?style=for-the-badge" alt="PyPI" /&gt;&lt;/a&gt; 
 &lt;a href="https://github.com/business-science/ai-data-science-team"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/ai-data-science-team.svg?style=for-the-badge" alt="versions" /&gt;&lt;/a&gt; 
 &lt;a href="https://github.com/business-science/ai-data-science-team/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/business-science/ai-data-science-team.svg?style=for-the-badge" alt="license" /&gt;&lt;/a&gt; 
 &lt;img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/business-science/ai-data-science-team?style=for-the-badge" /&gt; 
&lt;/div&gt; 
&lt;h1&gt;AI Data Science Team&lt;/h1&gt; 
&lt;p&gt;AI Data Science Team is a Python library of specialized agents for common data science workflows, plus a flagship app: &lt;strong&gt;AI Pipeline Studio&lt;/strong&gt;. The Studio turns your work into a visual, reproducible pipeline, while the AI team handles data loading, cleaning, visualization, and modeling.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Status:&lt;/strong&gt; Beta. Breaking changes may occur until 0.1.0.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/business-science/ai-data-science-team"&gt;&lt;strong&gt;Please ‚≠ê us on GitHub (it takes 2 seconds and means a lot).&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;AI Pipeline Studio (Flagship App)&lt;/h2&gt; 
&lt;p&gt;AI Pipeline Studio is the main example of the AI Data Science Team in action.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/business-science/ai-data-science-team/master/img/apps/ai_pipeline_studio_app.jpg" alt="AI Pipeline Studio" /&gt;&lt;/p&gt; 
&lt;p&gt;Highlights:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Pipeline-first workspace: Visual Editor, Table, Chart, EDA, Code, Model, Predictions, MLflow&lt;/li&gt; 
 &lt;li&gt;Manual + AI steps with lineage and reproducible scripts&lt;/li&gt; 
 &lt;li&gt;Multi-dataset handling and merge workflows&lt;/li&gt; 
 &lt;li&gt;Project saves: metadata-only or full-data&lt;/li&gt; 
 &lt;li&gt;Storage footprint controls and rehydrate workflows&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Run it:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;streamlit run apps/ai-pipeline-studio-app/app.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Full app docs: &lt;code&gt;apps/ai-pipeline-studio-app/README.md&lt;/code&gt;&lt;/p&gt; 
&lt;h2&gt;Quickstart&lt;/h2&gt; 
&lt;h3&gt;Requirements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.10+&lt;/li&gt; 
 &lt;li&gt;OpenAI API key (or Ollama for local models)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Install the app and library&lt;/h3&gt; 
&lt;p&gt;Clone the repo and install in editable mode:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Run the AI Pipeline Studio app&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;streamlit run apps/ai-pipeline-studio-app/app.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Library Overview&lt;/h2&gt; 
&lt;p&gt;The repository includes both the &lt;strong&gt;AI Pipeline Studio&lt;/strong&gt; app and the underlying &lt;strong&gt;AI Data Science Team&lt;/strong&gt; library. The library provides agent building blocks and multi-agent workflows for:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Data loading and inspection&lt;/li&gt; 
 &lt;li&gt;Cleaning, wrangling, and feature engineering&lt;/li&gt; 
 &lt;li&gt;Visualization and EDA&lt;/li&gt; 
 &lt;li&gt;Modeling and evaluation (H2O + MLflow tools)&lt;/li&gt; 
 &lt;li&gt;SQL database interaction&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Agents (Snapshot)&lt;/h3&gt; 
&lt;p&gt;Agent examples live in &lt;code&gt;examples/&lt;/code&gt;. Notable agents:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Data Loader Tools Agent&lt;/li&gt; 
 &lt;li&gt;Data Wrangling Agent&lt;/li&gt; 
 &lt;li&gt;Data Cleaning Agent&lt;/li&gt; 
 &lt;li&gt;Data Visualization Agent&lt;/li&gt; 
 &lt;li&gt;EDA Tools Agent&lt;/li&gt; 
 &lt;li&gt;Feature Engineering Agent&lt;/li&gt; 
 &lt;li&gt;SQL Database Agent&lt;/li&gt; 
 &lt;li&gt;H2O ML Agent&lt;/li&gt; 
 &lt;li&gt;MLflow Tools Agent&lt;/li&gt; 
 &lt;li&gt;Multi-agent workflows (e.g., Pandas Data Analyst, SQL Data Analyst)&lt;/li&gt; 
 &lt;li&gt;Supervisor Agent (oversees other agents)&lt;/li&gt; 
 &lt;li&gt;Custom tools for data science tasks&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Apps&lt;/h2&gt; 
&lt;p&gt;See all apps in &lt;code&gt;apps/&lt;/code&gt;. Notable apps:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;AI Pipeline Studio: &lt;code&gt;apps/ai-pipeline-studio-app/&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;EDA Explorer App: &lt;code&gt;apps/exploratory-copilot-app/&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Pandas Data Analyst App: &lt;code&gt;apps/pandas-data-analyst-app/&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Use OpenAI&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from langchain_openai import ChatOpenAI
llm = ChatOpenAI(
    model_name="gpt-4.1-mini",
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Use Ollama (Local LLM)&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;ollama serve
ollama pull llama3.1:8b
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from langchain_ollama import ChatOllama

llm = ChatOllama(
    model="llama3.1:8b",
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Next-Gen AI Agentic Workshop&lt;/h2&gt; 
&lt;p&gt;Want to learn how to build AI agents and AI apps for real data science workflows? Join my next‚Äëgen AI workshop: &lt;a href="https://learn.business-science.io/ai-register"&gt;https://learn.business-science.io/ai-register&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>xai-org/grok-1</title>
      <link>https://github.com/xai-org/grok-1</link>
      <description>&lt;p&gt;Grok open release&lt;/p&gt;&lt;hr&gt;</description>
    </item>
    
  </channel>
</rss>