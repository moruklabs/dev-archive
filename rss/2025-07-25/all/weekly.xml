<rss version="2.0">
  <channel>
    <title>GitHub All Languages Weekly Trending</title>
    <description>Weekly Trending of All Languages in GitHub</description>
    <pubDate>Thu, 24 Jul 2025 01:37:22 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>ChatGPTNextWeb/NextChat</title>
      <link>https://github.com/ChatGPTNextWeb/NextChat</link>
      <description>&lt;p&gt;âœ¨ Light and Fast AI Assistant. Support: Web | iOS | MacOS | Android | Linux | Windows&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;a href="https://nextchat.club"&gt; &lt;img src="https://github.com/user-attachments/assets/83bdcc07-ae5e-4954-a53a-ac151ba6ccf3" width="1000" alt="icon"&gt; &lt;/a&gt; 
 &lt;h1 align="center"&gt;NextChat&lt;/h1&gt; 
 &lt;p&gt;English / &lt;a href="https://raw.githubusercontent.com/ChatGPTNextWeb/NextChat/main/README_CN.md"&gt;ç®€ä½“ä¸­æ–‡&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/5973" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/5973" alt="ChatGPTNextWeb%2FChatGPT-Next-Web | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;âœ¨ Light and Fast AI Assistant,with Claude, DeepSeek, GPT4 &amp;amp; Gemini Pro support.&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://nextchat.club?utm_source=readme"&gt;&lt;img src="https://img.shields.io/badge/NextChat-Saas-green?logo=microsoftedge" alt="Saas"&gt;&lt;/a&gt; &lt;a href="https://app.nextchat.club/"&gt;&lt;img src="https://img.shields.io/badge/Web-PWA-orange?logo=microsoftedge" alt="Web"&gt;&lt;/a&gt; &lt;a href="https://github.com/Yidadaa/ChatGPT-Next-Web/releases"&gt;&lt;img src="https://img.shields.io/badge/-Windows-blue?logo=windows" alt="Windows"&gt;&lt;/a&gt; &lt;a href="https://github.com/Yidadaa/ChatGPT-Next-Web/releases"&gt;&lt;img src="https://img.shields.io/badge/-MacOS-black?logo=apple" alt="MacOS"&gt;&lt;/a&gt; &lt;a href="https://github.com/Yidadaa/ChatGPT-Next-Web/releases"&gt;&lt;img src="https://img.shields.io/badge/-Linux-333?logo=ubuntu" alt="Linux"&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://nextchat.club?utm_source=readme"&gt;NextChatAI&lt;/a&gt; / &lt;a href="https://apps.apple.com/us/app/nextchat-ai/id6743085599"&gt;iOS APP&lt;/a&gt; / &lt;a href="https://app.nextchat.club"&gt;Web App Demo&lt;/a&gt; / &lt;a href="https://github.com/Yidadaa/ChatGPT-Next-Web/releases"&gt;Desktop App&lt;/a&gt; / &lt;a href="https://raw.githubusercontent.com/ChatGPTNextWeb/NextChat/main/#enterprise-edition"&gt;Enterprise Edition&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://zeabur.com/templates/ZBUEFA"&gt;&lt;img src="https://zeabur.com/button.svg?sanitize=true" alt="Deploy on Zeabur" height="30"&gt;&lt;/a&gt; &lt;a href="https://vercel.com/new/clone?repository-url=https%3A%2F%2Fgithub.com%2FChatGPTNextWeb%2FChatGPT-Next-Web&amp;amp;env=OPENAI_API_KEY&amp;amp;env=CODE&amp;amp;project-name=nextchat&amp;amp;repository-name=NextChat"&gt;&lt;img src="https://vercel.com/button" alt="Deploy on Vercel" height="30"&gt;&lt;/a&gt; &lt;a href="https://gitpod.io/#https://github.com/ChatGPTNextWeb/NextChat"&gt;&lt;img src="https://gitpod.io/button/open-in-gitpod.svg?sanitize=true" alt="Open in Gitpod" height="30"&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://monica.im/?utm=nxcrp"&gt;&lt;img src="https://github.com/user-attachments/assets/903482d4-3e87-4134-9af1-f2588fa90659" height="50" width=""&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;â¤ï¸ Sponsor AI API&lt;/h2&gt; 
&lt;a href="https://302.ai/"&gt; &lt;img src="https://github.com/user-attachments/assets/a03edf82-2031-4f23-bdb8-bfc0bfd168a4" width="100%" alt="icon"&gt; &lt;/a&gt; 
&lt;p&gt;&lt;a href="https://302.ai/"&gt;302.AI&lt;/a&gt; is a pay-as-you-go AI application platform that offers the most comprehensive AI APIs and online applications available.&lt;/p&gt; 
&lt;h2&gt;ğŸ¥³ Cheer for NextChat iOS Version Online!&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;a href="https://apps.apple.com/us/app/nextchat-ai/id6743085599"&gt;ğŸ‘‰ Click Here to Install Now&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;a href="https://github.com/ChatGPTNextWeb/NextChat-iOS"&gt;â¤ï¸ Source Code Coming Soon&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/e0aa334f-4c13-4dc9-8310-e3b09fa4b9f3" alt="Github iOS Image"&gt;&lt;/p&gt; 
&lt;h2&gt;ğŸ«£ NextChat Support MCP !&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Before build, please set env ENABLE_MCP=true&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;img src="https://github.com/user-attachments/assets/d8851f40-4e36-4335-b1a4-ec1e11488c7e"&gt; 
&lt;h2&gt;Enterprise Edition&lt;/h2&gt; 
&lt;p&gt;Meeting Your Company's Privatization and Customization Deployment Requirements:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Brand Customization&lt;/strong&gt;: Tailored VI/UI to seamlessly align with your corporate brand image.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Resource Integration&lt;/strong&gt;: Unified configuration and management of dozens of AI resources by company administrators, ready for use by team members.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Permission Control&lt;/strong&gt;: Clearly defined member permissions, resource permissions, and knowledge base permissions, all controlled via a corporate-grade Admin Panel.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Knowledge Integration&lt;/strong&gt;: Combining your internal knowledge base with AI capabilities, making it more relevant to your company's specific business needs compared to general AI.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Security Auditing&lt;/strong&gt;: Automatically intercept sensitive inquiries and trace all historical conversation records, ensuring AI adherence to corporate information security standards.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Private Deployment&lt;/strong&gt;: Enterprise-level private deployment supporting various mainstream private cloud solutions, ensuring data security and privacy protection.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Continuous Updates&lt;/strong&gt;: Ongoing updates and upgrades in cutting-edge capabilities like multimodal AI, ensuring consistent innovation and advancement.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For enterprise inquiries, please contact: &lt;strong&gt;&lt;a href="mailto:business@nextchat.dev"&gt;business@nextchat.dev&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;Screenshots&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/ChatGPTNextWeb/NextChat/main/docs/images/settings.png" alt="Settings"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/ChatGPTNextWeb/NextChat/main/docs/images/more.png" alt="More"&gt;&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Deploy for free with one-click&lt;/strong&gt; on Vercel in under 1 minute&lt;/li&gt; 
 &lt;li&gt;Compact client (~5MB) on Linux/Windows/MacOS, &lt;a href="https://github.com/Yidadaa/ChatGPT-Next-Web/releases"&gt;download it now&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Fully compatible with self-deployed LLMs, recommended for use with &lt;a href="https://github.com/josStorer/RWKV-Runner"&gt;RWKV-Runner&lt;/a&gt; or &lt;a href="https://github.com/go-skynet/LocalAI"&gt;LocalAI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Privacy first, all data is stored locally in the browser&lt;/li&gt; 
 &lt;li&gt;Markdown support: LaTex, mermaid, code highlight, etc.&lt;/li&gt; 
 &lt;li&gt;Responsive design, dark mode and PWA&lt;/li&gt; 
 &lt;li&gt;Fast first screen loading speed (~100kb), support streaming response&lt;/li&gt; 
 &lt;li&gt;New in v2: create, share and debug your chat tools with prompt templates (mask)&lt;/li&gt; 
 &lt;li&gt;Awesome prompts powered by &lt;a href="https://github.com/PlexPt/awesome-chatgpt-prompts-zh"&gt;awesome-chatgpt-prompts-zh&lt;/a&gt; and &lt;a href="https://github.com/f/awesome-chatgpt-prompts"&gt;awesome-chatgpt-prompts&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Automatically compresses chat history to support long conversations while also saving your tokens&lt;/li&gt; 
 &lt;li&gt;I18n: English, ç®€ä½“ä¸­æ–‡, ç¹ä½“ä¸­æ–‡, æ—¥æœ¬èª, FranÃ§ais, EspaÃ±ol, Italiano, TÃ¼rkÃ§e, Deutsch, Tiáº¿ng Viá»‡t, Ğ ÑƒÑÑĞºĞ¸Ğ¹, ÄŒeÅ¡tina, í•œêµ­ì–´, Indonesia&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/ChatGPTNextWeb/NextChat/main/docs/images/cover.png" alt="ä¸»ç•Œé¢"&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;Roadmap&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; System Prompt: pin a user defined prompt as system prompt &lt;a href="https://github.com/Yidadaa/ChatGPT-Next-Web/issues/138"&gt;#138&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; User Prompt: user can edit and save custom prompts to prompt list&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; Prompt Template: create a new chat with pre-defined in-context prompts &lt;a href="https://github.com/Yidadaa/ChatGPT-Next-Web/issues/993"&gt;#993&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; Share as image, share to ShareGPT &lt;a href="https://github.com/Yidadaa/ChatGPT-Next-Web/pull/1741"&gt;#1741&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; Desktop App with tauri&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; Self-host Model: Fully compatible with &lt;a href="https://github.com/josStorer/RWKV-Runner"&gt;RWKV-Runner&lt;/a&gt;, as well as server deployment of &lt;a href="https://github.com/go-skynet/LocalAI"&gt;LocalAI&lt;/a&gt;: llama/gpt4all/rwkv/vicuna/koala/gpt4all-j/cerebras/falcon/dolly etc.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; Artifacts: Easily preview, copy and share generated content/webpages through a separate window &lt;a href="https://github.com/ChatGPTNextWeb/ChatGPT-Next-Web/pull/5092"&gt;#5092&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; Plugins: support network search, calculator, any other apis etc. &lt;a href="https://github.com/Yidadaa/ChatGPT-Next-Web/issues/165"&gt;#165&lt;/a&gt; &lt;a href="https://github.com/ChatGPTNextWeb/ChatGPT-Next-Web/issues/5353"&gt;#5353&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; network search, calculator, any other apis etc. &lt;a href="https://github.com/Yidadaa/ChatGPT-Next-Web/issues/165"&gt;#165&lt;/a&gt; &lt;a href="https://github.com/ChatGPTNextWeb/ChatGPT-Next-Web/issues/5353"&gt;#5353&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; Supports Realtime Chat &lt;a href="https://github.com/ChatGPTNextWeb/ChatGPT-Next-Web/issues/5672"&gt;#5672&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled&gt; local knowledge base&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;What's New&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ğŸš€ v2.15.8 Now supports Realtime Chat &lt;a href="https://github.com/ChatGPTNextWeb/ChatGPT-Next-Web/issues/5672"&gt;#5672&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸš€ v2.15.4 The Application supports using Tauri fetch LLM API, MORE SECURITY! &lt;a href="https://github.com/ChatGPTNextWeb/ChatGPT-Next-Web/issues/5379"&gt;#5379&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸš€ v2.15.0 Now supports Plugins! Read this: &lt;a href="https://github.com/ChatGPTNextWeb/NextChat-Awesome-Plugins"&gt;NextChat-Awesome-Plugins&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ğŸš€ v2.14.0 Now supports Artifacts &amp;amp; SD&lt;/li&gt; 
 &lt;li&gt;ğŸš€ v2.10.1 support Google Gemini Pro model.&lt;/li&gt; 
 &lt;li&gt;ğŸš€ v2.9.11 you can use azure endpoint now.&lt;/li&gt; 
 &lt;li&gt;ğŸš€ v2.8 now we have a client that runs across all platforms!&lt;/li&gt; 
 &lt;li&gt;ğŸš€ v2.7 let's share conversations as image, or share to ShareGPT!&lt;/li&gt; 
 &lt;li&gt;ğŸš€ v2.0 is released, now you can create prompt templates, turn your ideas into reality! Read this: &lt;a href="https://www.allabtai.com/prompt-engineering-tips-zero-one-and-few-shot-prompting/"&gt;ChatGPT Prompt Engineering Tips: Zero, One and Few Shot Prompting&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Get Started&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Get &lt;a href="https://platform.openai.com/account/api-keys"&gt;OpenAI API Key&lt;/a&gt;;&lt;/li&gt; 
 &lt;li&gt;Click &lt;a href="https://vercel.com/new/clone?repository-url=https%3A%2F%2Fgithub.com%2FYidadaa%2FChatGPT-Next-Web&amp;amp;env=OPENAI_API_KEY&amp;amp;env=CODE&amp;amp;project-name=chatgpt-next-web&amp;amp;repository-name=ChatGPT-Next-Web"&gt;&lt;img src="https://vercel.com/button" alt="Deploy with Vercel"&gt;&lt;/a&gt;, remember that &lt;code&gt;CODE&lt;/code&gt; is your page password;&lt;/li&gt; 
 &lt;li&gt;Enjoy :)&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;FAQ&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/ChatGPTNextWeb/NextChat/main/docs/faq-en.md"&gt;English &amp;gt; FAQ&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Keep Updated&lt;/h2&gt; 
&lt;p&gt;If you have deployed your own project with just one click following the steps above, you may encounter the issue of "Updates Available" constantly showing up. This is because Vercel will create a new project for you by default instead of forking this project, resulting in the inability to detect updates correctly.&lt;/p&gt; 
&lt;p&gt;We recommend that you follow the steps below to re-deploy:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Delete the original repository;&lt;/li&gt; 
 &lt;li&gt;Use the fork button in the upper right corner of the page to fork this project;&lt;/li&gt; 
 &lt;li&gt;Choose and deploy in Vercel again, &lt;a href="https://raw.githubusercontent.com/ChatGPTNextWeb/NextChat/main/docs/vercel-cn.md"&gt;please see the detailed tutorial&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Enable Automatic Updates&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;If you encounter a failure of Upstream Sync execution, please &lt;a href="https://raw.githubusercontent.com/ChatGPTNextWeb/NextChat/main/README.md#manually-updating-code"&gt;manually update code&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;After forking the project, due to the limitations imposed by GitHub, you need to manually enable Workflows and Upstream Sync Action on the Actions page of the forked project. Once enabled, automatic updates will be scheduled every hour:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/ChatGPTNextWeb/NextChat/main/docs/images/enable-actions.jpg" alt="Automatic Updates"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/ChatGPTNextWeb/NextChat/main/docs/images/enable-actions-sync.jpg" alt="Enable Automatic Updates"&gt;&lt;/p&gt; 
&lt;h3&gt;Manually Updating Code&lt;/h3&gt; 
&lt;p&gt;If you want to update instantly, you can check out the &lt;a href="https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/syncing-a-fork"&gt;GitHub documentation&lt;/a&gt; to learn how to synchronize a forked project with upstream code.&lt;/p&gt; 
&lt;p&gt;You can star or watch this project or follow author to get release notifications in time.&lt;/p&gt; 
&lt;h2&gt;Access Password&lt;/h2&gt; 
&lt;p&gt;This project provides limited access control. Please add an environment variable named &lt;code&gt;CODE&lt;/code&gt; on the vercel environment variables page. The value should be passwords separated by comma like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;code1,code2,code3
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After adding or modifying this environment variable, please redeploy the project for the changes to take effect.&lt;/p&gt; 
&lt;h2&gt;Environment Variables&lt;/h2&gt; 
&lt;h3&gt;&lt;code&gt;CODE&lt;/code&gt; (optional)&lt;/h3&gt; 
&lt;p&gt;Access password, separated by comma.&lt;/p&gt; 
&lt;h3&gt;&lt;code&gt;OPENAI_API_KEY&lt;/code&gt; (required)&lt;/h3&gt; 
&lt;p&gt;Your openai api key, join multiple api keys with comma.&lt;/p&gt; 
&lt;h3&gt;&lt;code&gt;BASE_URL&lt;/code&gt; (optional)&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Default: &lt;code&gt;https://api.openai.com&lt;/code&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Examples: &lt;code&gt;http://your-openai-proxy.com&lt;/code&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Override openai api request base url.&lt;/p&gt; 
&lt;h3&gt;&lt;code&gt;OPENAI_ORG_ID&lt;/code&gt; (optional)&lt;/h3&gt; 
&lt;p&gt;Specify OpenAI organization ID.&lt;/p&gt; 
&lt;h3&gt;&lt;code&gt;AZURE_URL&lt;/code&gt; (optional)&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Example: https://{azure-resource-url}/openai&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Azure deploy url.&lt;/p&gt; 
&lt;h3&gt;&lt;code&gt;AZURE_API_KEY&lt;/code&gt; (optional)&lt;/h3&gt; 
&lt;p&gt;Azure Api Key.&lt;/p&gt; 
&lt;h3&gt;&lt;code&gt;AZURE_API_VERSION&lt;/code&gt; (optional)&lt;/h3&gt; 
&lt;p&gt;Azure Api Version, find it at &lt;a href="https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#chat-completions"&gt;Azure Documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;&lt;code&gt;GOOGLE_API_KEY&lt;/code&gt; (optional)&lt;/h3&gt; 
&lt;p&gt;Google Gemini Pro Api Key.&lt;/p&gt; 
&lt;h3&gt;&lt;code&gt;GOOGLE_URL&lt;/code&gt; (optional)&lt;/h3&gt; 
&lt;p&gt;Google Gemini Pro Api Url.&lt;/p&gt; 
&lt;h3&gt;&lt;code&gt;ANTHROPIC_API_KEY&lt;/code&gt; (optional)&lt;/h3&gt; 
&lt;p&gt;anthropic claude Api Key.&lt;/p&gt; 
&lt;h3&gt;&lt;code&gt;ANTHROPIC_API_VERSION&lt;/code&gt; (optional)&lt;/h3&gt; 
&lt;p&gt;anthropic claude Api version.&lt;/p&gt; 
&lt;h3&gt;&lt;code&gt;ANTHROPIC_URL&lt;/code&gt; (optional)&lt;/h3&gt; 
&lt;p&gt;anthropic claude Api Url.&lt;/p&gt; 
&lt;h3&gt;&lt;code&gt;BAIDU_API_KEY&lt;/code&gt; (optional)&lt;/h3&gt; 
&lt;p&gt;Baidu Api Key.&lt;/p&gt; 
&lt;h3&gt;&lt;code&gt;BAIDU_SECRET_KEY&lt;/code&gt; (optional)&lt;/h3&gt; 
&lt;p&gt;Baidu Secret Key.&lt;/p&gt; 
&lt;h3&gt;&lt;code&gt;BAIDU_URL&lt;/code&gt; (optional)&lt;/h3&gt; 
&lt;p&gt;Baidu Api Url.&lt;/p&gt; 
&lt;h3&gt;&lt;code&gt;BYTEDANCE_API_KEY&lt;/code&gt; (optional)&lt;/h3&gt; 
&lt;p&gt;ByteDance Api Key.&lt;/p&gt; 
&lt;h3&gt;&lt;code&gt;BYTEDANCE_URL&lt;/code&gt; (optional)&lt;/h3&gt; 
&lt;p&gt;ByteDance Api Url.&lt;/p&gt; 
&lt;h3&gt;&lt;code&gt;ALIBABA_API_KEY&lt;/code&gt; (optional)&lt;/h3&gt; 
&lt;p&gt;Alibaba Cloud Api Key.&lt;/p&gt; 
&lt;h3&gt;&lt;code&gt;ALIBABA_URL&lt;/code&gt; (optional)&lt;/h3&gt; 
&lt;p&gt;Alibaba Cloud Api Url.&lt;/p&gt; 
&lt;h3&gt;&lt;code&gt;IFLYTEK_URL&lt;/code&gt; (Optional)&lt;/h3&gt; 
&lt;p&gt;iflytek Api Url.&lt;/p&gt; 
&lt;h3&gt;&lt;code&gt;IFLYTEK_API_KEY&lt;/code&gt; (Optional)&lt;/h3&gt; 
&lt;p&gt;iflytek Api Key.&lt;/p&gt; 
&lt;h3&gt;&lt;code&gt;IFLYTEK_API_SECRET&lt;/code&gt; (Optional)&lt;/h3&gt; 
&lt;p&gt;iflytek Api Secret.&lt;/p&gt; 
&lt;h3&gt;&lt;code&gt;CHATGLM_API_KEY&lt;/code&gt; (optional)&lt;/h3&gt; 
&lt;p&gt;ChatGLM Api Key.&lt;/p&gt; 
&lt;h3&gt;&lt;code&gt;CHATGLM_URL&lt;/code&gt; (optional)&lt;/h3&gt; 
&lt;p&gt;ChatGLM Api Url.&lt;/p&gt; 
&lt;h3&gt;&lt;code&gt;DEEPSEEK_API_KEY&lt;/code&gt; (optional)&lt;/h3&gt; 
&lt;p&gt;DeepSeek Api Key.&lt;/p&gt; 
&lt;h3&gt;&lt;code&gt;DEEPSEEK_URL&lt;/code&gt; (optional)&lt;/h3&gt; 
&lt;p&gt;DeepSeek Api Url.&lt;/p&gt; 
&lt;h3&gt;&lt;code&gt;HIDE_USER_API_KEY&lt;/code&gt; (optional)&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Default: Empty&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;If you do not want users to input their own API key, set this value to 1.&lt;/p&gt; 
&lt;h3&gt;&lt;code&gt;DISABLE_GPT4&lt;/code&gt; (optional)&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Default: Empty&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;If you do not want users to use GPT-4, set this value to 1.&lt;/p&gt; 
&lt;h3&gt;&lt;code&gt;ENABLE_BALANCE_QUERY&lt;/code&gt; (optional)&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Default: Empty&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;If you do want users to query balance, set this value to 1.&lt;/p&gt; 
&lt;h3&gt;&lt;code&gt;DISABLE_FAST_LINK&lt;/code&gt; (optional)&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Default: Empty&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;If you want to disable parse settings from url, set this to 1.&lt;/p&gt; 
&lt;h3&gt;&lt;code&gt;CUSTOM_MODELS&lt;/code&gt; (optional)&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Default: Empty Example: &lt;code&gt;+llama,+claude-2,-gpt-3.5-turbo,gpt-4-1106-preview=gpt-4-turbo&lt;/code&gt; means add &lt;code&gt;llama, claude-2&lt;/code&gt; to model list, and remove &lt;code&gt;gpt-3.5-turbo&lt;/code&gt; from list, and display &lt;code&gt;gpt-4-1106-preview&lt;/code&gt; as &lt;code&gt;gpt-4-turbo&lt;/code&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;To control custom models, use &lt;code&gt;+&lt;/code&gt; to add a custom model, use &lt;code&gt;-&lt;/code&gt; to hide a model, use &lt;code&gt;name=displayName&lt;/code&gt; to customize model name, separated by comma.&lt;/p&gt; 
&lt;p&gt;User &lt;code&gt;-all&lt;/code&gt; to disable all default models, &lt;code&gt;+all&lt;/code&gt; to enable all default models.&lt;/p&gt; 
&lt;p&gt;For Azure: use &lt;code&gt;modelName@Azure=deploymentName&lt;/code&gt; to customize model name and deployment name.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Example: &lt;code&gt;+gpt-3.5-turbo@Azure=gpt35&lt;/code&gt; will show option &lt;code&gt;gpt35(Azure)&lt;/code&gt; in model list. If you only can use Azure model, &lt;code&gt;-all,+gpt-3.5-turbo@Azure=gpt35&lt;/code&gt; will &lt;code&gt;gpt35(Azure)&lt;/code&gt; the only option in model list.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;For ByteDance: use &lt;code&gt;modelName@bytedance=deploymentName&lt;/code&gt; to customize model name and deployment name.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Example: &lt;code&gt;+Doubao-lite-4k@bytedance=ep-xxxxx-xxx&lt;/code&gt; will show option &lt;code&gt;Doubao-lite-4k(ByteDance)&lt;/code&gt; in model list.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;&lt;code&gt;DEFAULT_MODEL&lt;/code&gt; ï¼ˆoptionalï¼‰&lt;/h3&gt; 
&lt;p&gt;Change default model&lt;/p&gt; 
&lt;h3&gt;&lt;code&gt;VISION_MODELS&lt;/code&gt; (optional)&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Default: Empty Example: &lt;code&gt;gpt-4-vision,claude-3-opus,my-custom-model&lt;/code&gt; means add vision capabilities to these models in addition to the default pattern matches (which detect models containing keywords like "vision", "claude-3", "gemini-1.5", etc).&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Add additional models to have vision capabilities, beyond the default pattern matching. Multiple models should be separated by commas.&lt;/p&gt; 
&lt;h3&gt;&lt;code&gt;WHITE_WEBDAV_ENDPOINTS&lt;/code&gt; (optional)&lt;/h3&gt; 
&lt;p&gt;You can use this option if you want to increase the number of webdav service addresses you are allowed to access, as required by the formatï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Each address must be a complete endpoint 
  &lt;blockquote&gt; 
   &lt;p&gt;&lt;code&gt;https://xxxx/yyy&lt;/code&gt;&lt;/p&gt; 
  &lt;/blockquote&gt; &lt;/li&gt; 
 &lt;li&gt;Multiple addresses are connected by ', '&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;code&gt;DEFAULT_INPUT_TEMPLATE&lt;/code&gt; (optional)&lt;/h3&gt; 
&lt;p&gt;Customize the default template used to initialize the User Input Preprocessing configuration item in Settings.&lt;/p&gt; 
&lt;h3&gt;&lt;code&gt;STABILITY_API_KEY&lt;/code&gt; (optional)&lt;/h3&gt; 
&lt;p&gt;Stability API key.&lt;/p&gt; 
&lt;h3&gt;&lt;code&gt;STABILITY_URL&lt;/code&gt; (optional)&lt;/h3&gt; 
&lt;p&gt;Customize Stability API url.&lt;/p&gt; 
&lt;h3&gt;&lt;code&gt;ENABLE_MCP&lt;/code&gt; (optional)&lt;/h3&gt; 
&lt;p&gt;Enable MCPï¼ˆModel Context Protocolï¼‰Feature&lt;/p&gt; 
&lt;h3&gt;&lt;code&gt;SILICONFLOW_API_KEY&lt;/code&gt; (optional)&lt;/h3&gt; 
&lt;p&gt;SiliconFlow API Key.&lt;/p&gt; 
&lt;h3&gt;&lt;code&gt;SILICONFLOW_URL&lt;/code&gt; (optional)&lt;/h3&gt; 
&lt;p&gt;SiliconFlow API URL.&lt;/p&gt; 
&lt;h3&gt;&lt;code&gt;AI302_API_KEY&lt;/code&gt; (optional)&lt;/h3&gt; 
&lt;p&gt;302.AI API Key.&lt;/p&gt; 
&lt;h3&gt;&lt;code&gt;AI302_URL&lt;/code&gt; (optional)&lt;/h3&gt; 
&lt;p&gt;302.AI API URL.&lt;/p&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;p&gt;NodeJS &amp;gt;= 18, Docker &amp;gt;= 20&lt;/p&gt; 
&lt;h2&gt;Development&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://gitpod.io/#https://github.com/Yidadaa/ChatGPT-Next-Web"&gt;&lt;img src="https://gitpod.io/button/open-in-gitpod.svg?sanitize=true" alt="Open in Gitpod"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Before starting development, you must create a new &lt;code&gt;.env.local&lt;/code&gt; file at project root, and place your api key into it:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;OPENAI_API_KEY=&amp;lt;your api key here&amp;gt;

# if you are not able to access openai service, use this BASE_URL
BASE_URL=https://chatgpt1.nextweb.fun/api/proxy
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Local Development&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;# 1. install nodejs and yarn first
# 2. config local env vars in `.env.local`
# 3. run
yarn install
yarn dev
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Deployment&lt;/h2&gt; 
&lt;h3&gt;Docker (Recommended)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;docker pull yidadaa/chatgpt-next-web

docker run -d -p 3000:3000 \
   -e OPENAI_API_KEY=sk-xxxx \
   -e CODE=your-password \
   yidadaa/chatgpt-next-web
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can start service behind a proxy:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;docker run -d -p 3000:3000 \
   -e OPENAI_API_KEY=sk-xxxx \
   -e CODE=your-password \
   -e PROXY_URL=http://localhost:7890 \
   yidadaa/chatgpt-next-web
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If your proxy needs password, use:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;-e PROXY_URL="http://127.0.0.1:7890 user pass"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If enable MCP, useï¼š&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;docker run -d -p 3000:3000 \
   -e OPENAI_API_KEY=sk-xxxx \
   -e CODE=your-password \
   -e ENABLE_MCP=true \
   yidadaa/chatgpt-next-web
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Shell&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;bash &amp;lt;(curl -s https://raw.githubusercontent.com/Yidadaa/ChatGPT-Next-Web/main/scripts/setup.sh)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Synchronizing Chat Records (UpStash)&lt;/h2&gt; 
&lt;p&gt;| &lt;a href="https://raw.githubusercontent.com/ChatGPTNextWeb/NextChat/main/docs/synchronise-chat-logs-cn.md"&gt;ç®€ä½“ä¸­æ–‡&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/ChatGPTNextWeb/NextChat/main/docs/synchronise-chat-logs-en.md"&gt;English&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/ChatGPTNextWeb/NextChat/main/docs/synchronise-chat-logs-es.md"&gt;Italiano&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/ChatGPTNextWeb/NextChat/main/docs/synchronise-chat-logs-ja.md"&gt;æ—¥æœ¬èª&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/ChatGPTNextWeb/NextChat/main/docs/synchronise-chat-logs-ko.md"&gt;í•œêµ­ì–´&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Please go to the [docs][./docs] directory for more documentation instructions.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ChatGPTNextWeb/NextChat/main/docs/cloudflare-pages-en.md"&gt;Deploy with cloudflare (Deprecated)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ChatGPTNextWeb/NextChat/main/docs/faq-en.md"&gt;Frequent Ask Questions&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ChatGPTNextWeb/NextChat/main/docs/translation.md"&gt;How to add a new translation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ChatGPTNextWeb/NextChat/main/docs/vercel-cn.md"&gt;How to use Vercel (No English)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ChatGPTNextWeb/NextChat/main/docs/user-manual-cn.md"&gt;User Manual (Only Chinese, WIP)&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Translation&lt;/h2&gt; 
&lt;p&gt;If you want to add a new translation, read this &lt;a href="https://raw.githubusercontent.com/ChatGPTNextWeb/NextChat/main/docs/translation.md"&gt;document&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Donation&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.buymeacoffee.com/yidadaa"&gt;Buy Me a Coffee&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Special Thanks&lt;/h2&gt; 
&lt;h3&gt;Contributors&lt;/h3&gt; 
&lt;a href="https://github.com/ChatGPTNextWeb/ChatGPT-Next-Web/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?repo=ChatGPTNextWeb/ChatGPT-Next-Web"&gt; &lt;/a&gt; 
&lt;h2&gt;LICENSE&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://opensource.org/license/mit/"&gt;MIT&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>hesreallyhim/awesome-claude-code</title>
      <link>https://github.com/hesreallyhim/awesome-claude-code</link>
      <description>&lt;p&gt;A curated list of awesome commands, files, and workflows for Claude Code&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;/h1&gt; 
&lt;!-- [![Awesome](https://awesome.re/badge-flat2.svg)](https://awesome.re) --&gt; 
&lt;pre style="display: inline-block; text-align: left;"&gt;
 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ” â–ˆâ–ˆâ”    â–ˆâ–ˆâ”â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ” â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ” â–ˆâ–ˆâ–ˆâ”   â–ˆâ–ˆâ–ˆâ”â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”
â–ˆâ–ˆâ”Œâ”€â”€â–ˆâ–ˆâ”â–ˆâ–ˆâ”‚    â–ˆâ–ˆâ”‚â–ˆâ–ˆâ”Œâ”€â”€â”€â”€â”˜â–ˆâ–ˆâ”Œâ”€â”€â”€â”€â”˜â–ˆâ–ˆâ”Œâ”€â”€â”€â–ˆâ–ˆâ”â–ˆâ–ˆâ–ˆâ–ˆâ” â–ˆâ–ˆâ–ˆâ–ˆâ”‚â–ˆâ–ˆâ”Œâ”€â”€â”€â”€â”˜
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚â–ˆâ–ˆâ”‚ â–ˆâ” â–ˆâ–ˆâ”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”â–ˆâ–ˆâ”‚   â–ˆâ–ˆâ”‚â–ˆâ–ˆâ”Œâ–ˆâ–ˆâ–ˆâ–ˆâ”Œâ–ˆâ–ˆâ”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”
â–ˆâ–ˆâ”Œâ”€â”€â–ˆâ–ˆâ”‚â–ˆâ–ˆâ”‚â–ˆâ–ˆâ–ˆâ”â–ˆâ–ˆâ”‚â–ˆâ–ˆâ”Œâ”€â”€â”˜  â””â”€â”€â”€â”€â–ˆâ–ˆâ”‚â–ˆâ–ˆâ”‚   â–ˆâ–ˆâ”‚â–ˆâ–ˆâ”‚â””â–ˆâ–ˆâ”Œâ”˜â–ˆâ–ˆâ”‚â–ˆâ–ˆâ”Œâ”€â”€â”˜
â–ˆâ–ˆâ”‚  â–ˆâ–ˆâ”‚â””â–ˆâ–ˆâ–ˆâ”Œâ–ˆâ–ˆâ–ˆâ”Œâ”˜â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚â””â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”Œâ”˜â–ˆâ–ˆâ”‚ â””â”€â”˜ â–ˆâ–ˆâ”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”
â””â”€â”˜  â””â”€â”˜ â””â”€â”€â”˜â””â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”˜     â””â”€â”˜â””â”€â”€â”€â”€â”€â”€â”˜

 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”â–ˆâ–ˆâ”      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ” â–ˆâ–ˆâ”   â–ˆâ–ˆâ”â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ” â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ” â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ” â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ” â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”
â–ˆâ–ˆâ”Œâ”€â”€â”€â”€â”˜â–ˆâ–ˆâ”‚     â–ˆâ–ˆâ”Œâ”€â”€â–ˆâ–ˆâ”â–ˆâ–ˆâ”‚   â–ˆâ–ˆâ”‚â–ˆâ–ˆâ”Œâ”€â”€â–ˆâ–ˆâ”â–ˆâ–ˆâ”Œâ”€â”€â”€â”€â”˜    â–ˆâ–ˆâ”Œâ”€â”€â”€â”€â”˜â–ˆâ–ˆâ”Œâ”€â”€â”€â–ˆâ–ˆâ”â–ˆâ–ˆâ”Œâ”€â”€â–ˆâ–ˆâ”â–ˆâ–ˆâ”Œâ”€â”€â”€â”€â”˜
â–ˆâ–ˆâ”‚     â–ˆâ–ˆâ”‚     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚â–ˆâ–ˆâ”‚   â–ˆâ–ˆâ”‚â–ˆâ–ˆâ”‚  â–ˆâ–ˆâ”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”      â–ˆâ–ˆâ”‚     â–ˆâ–ˆâ”‚   â–ˆâ–ˆâ”‚â–ˆâ–ˆâ”‚  â–ˆâ–ˆâ”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”
â–ˆâ–ˆâ”‚     â–ˆâ–ˆâ”‚     â–ˆâ–ˆâ”Œâ”€â”€â–ˆâ–ˆâ”‚â–ˆâ–ˆâ”‚   â–ˆâ–ˆâ”‚â–ˆâ–ˆâ”‚  â–ˆâ–ˆâ”‚â–ˆâ–ˆâ”Œâ”€â”€â”˜      â–ˆâ–ˆâ”‚     â–ˆâ–ˆâ”‚   â–ˆâ–ˆâ”‚â–ˆâ–ˆâ”‚  â–ˆâ–ˆâ”‚â–ˆâ–ˆâ”Œâ”€â”€â”˜
â””â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”â–ˆâ–ˆâ”‚  â–ˆâ–ˆâ”‚â””â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”Œâ”˜â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”Œâ”˜â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”    â””â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”â””â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”Œâ”˜â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”Œâ”˜â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”
 â””â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”˜â””â”€â”˜  â””â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜
&lt;/pre&gt; 
&lt;!--lint enable remark-lint:awesome-badge--&gt; 
&lt;p&gt;&lt;a href="https://awesome.re"&gt;&lt;img src="https://awesome.re/badge-flat2.svg?sanitize=true" alt="Awesome"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;!--lint enable remark-lint:awesome-badge--&gt; 
&lt;!--lint disable double-link--&gt; 
&lt;p&gt;This is a curated list of slash-commands, &lt;code&gt;CLAUDE.md&lt;/code&gt; files, CLI tools, and other resources and guides for enhancing your &lt;a href="https://docs.anthropic.com/en/docs/claude-code"&gt;Claude Code&lt;/a&gt; workflow, productivity, and vibes.&lt;/p&gt; 
&lt;!--lint enable double-link--&gt; 
&lt;p&gt;Claude Code is a cutting-edge CLI-based coding assistant and agent that you can access in your terminal or IDE. It is a rapidly evolving tool that offers a number of powerful capabilities, and allows for a lot of configuration, in a lot of different ways. Users are actively working out best practices and workflows. It is the hope that this repo will help the community share knowledge and understand how to get the most out of Claude Code.&lt;/p&gt; 
&lt;h3&gt;Announcements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;2025-07-18 - I ended up over-engineering the submission workflow, but I think it's done, I just have to smoke test it and update the docs. For anyone with existing PR's, don't worry about updating them (for formatting purposes, that is), I can take care of it myself. For anoyne with new PR's, you &lt;em&gt;should&lt;/em&gt; be able to run &lt;code&gt;make submit&lt;/code&gt; from the root directory of your fork for an interactive experience (as I said, needs smoke testing) - alternatively, add your entry to the bottom of &lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/THE_RESOURCES_TABLE.csv"&gt;&lt;code&gt;THE_RESOURCES_TABLE&lt;/code&gt;&lt;/a&gt; and run &lt;code&gt;make generate&lt;/code&gt; to automatically update the &lt;code&gt;README.md&lt;/code&gt; based on the information you filled in. If it's not working, just open a PR with the relevant information and I'll deal with it, I created this mess anyway ğŸ˜ƒ.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;br&gt; 
&lt;h2&gt;Contents&lt;/h2&gt; 
&lt;p&gt;â–ª&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#workflows--knowledge-guides-"&gt;Workflows &amp;amp; Knowledge Guides&lt;/a&gt;&lt;br&gt; â–ª&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#tooling-"&gt;Tooling&lt;/a&gt;&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;â–«&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#ide-integrations"&gt;IDE Integrations&lt;/a&gt;&lt;br&gt; â–ª&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#hooks-"&gt;Hooks&lt;/a&gt;&lt;br&gt; â–ª&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#slash-commands-"&gt;Slash-Commands&lt;/a&gt;&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;â–«&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#version-control--git"&gt;Version Control &amp;amp; Git&lt;/a&gt;&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;â–«&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#code-analysis--testing"&gt;Code Analysis &amp;amp; Testing&lt;/a&gt;&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;â–«&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#context-loading--priming"&gt;Context Loading &amp;amp; Priming&lt;/a&gt;&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;â–«&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#documentation--changelogs"&gt;Documentation &amp;amp; Changelogs&lt;/a&gt;&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;â–«&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#ci--deployment"&gt;CI / Deployment&lt;/a&gt;&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;â–«&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#project--task-management"&gt;Project &amp;amp; Task Management&lt;/a&gt;&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;â–«&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#miscellaneous"&gt;Miscellaneous&lt;/a&gt;&lt;br&gt; â–ª&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#claudemd-files-"&gt;CLAUDE.md Files&lt;/a&gt;&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;â–«&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#language-specific"&gt;Language-Specific&lt;/a&gt;&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;â–«&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#domain-specific"&gt;Domain-Specific&lt;/a&gt;&lt;br&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;â–«&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#project-scaffolding--mcp"&gt;Project Scaffolding &amp;amp; MCP&lt;/a&gt;&lt;br&gt; â–ª&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/#official-documentation-"&gt;Official Documentation&lt;/a&gt;&lt;/p&gt; 
&lt;br&gt; 
&lt;h2&gt;Workflows &amp;amp; Knowledge Guides ğŸ§ &lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;A &lt;strong&gt;workflow&lt;/strong&gt; is a tightly coupled set of Claude Code-native resources that facilitate specific projects&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;a href="https://github.com/cloudartisan/cloudartisan.github.io/tree/main/.claude/commands"&gt;&lt;code&gt;Blogging Platform Instructions&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/cloudartisan"&gt;cloudartisan&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;CC-BY-SA-4.0&lt;br&gt; Provides a well-structured set of commands for publishing and maintaining a blogging platform, including commands for creating posts, managing categories, and handling media files.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://claudelog.com"&gt;&lt;code&gt;ClaudeLog&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://www.reddit.com/user/inventor_black/"&gt;InventorBlack&lt;/a&gt;&lt;br&gt; A comprehensive knowledge repository that features detailed breakdowns of advanced Claude Code mechanics including &lt;a href="https://claudelog.com/mechanics/claude-md-supremacy"&gt;CLAUDE.md best practices&lt;/a&gt;, practical technique guides like &lt;a href="https://claudelog.com/mechanics/plan-mode"&gt;plan mode&lt;/a&gt;, and a &lt;a href="https://claudelog.com/configuration"&gt;configuration guide&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/disler/just-prompt/tree/main/.claude/commands"&gt;&lt;code&gt;Context Priming&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/disler"&gt;disler&lt;/a&gt;&lt;br&gt; Provides a systematic approach to priming Claude Code with comprehensive project context through specialized commands for different project scenarios and development contexts.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/kingler/n8n_agent/tree/main/.claude/commands"&gt;&lt;code&gt;n8n_agent&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/kingler"&gt;kingler&lt;/a&gt;&lt;br&gt; Amazing comprehensive set of comments for code analysis, QA, design, documentation, project structure, project management, optimization, and many more.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/steadycursor/steadystart/tree/main/.claude/commands"&gt;&lt;code&gt;Project Bootstrapping and Task Management&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/steadycursor"&gt;steadycursor&lt;/a&gt;&lt;br&gt; Provides a structured set of commands for bootstrapping and managing a new project, including meta-commands for creating and editing custom slash-commands.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/scopecraft/command/tree/main/.claude/commands"&gt;&lt;code&gt;Project Management, Implementation, Planning, and Release&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/scopecraft"&gt;scopecraft&lt;/a&gt;&lt;br&gt; Really comprehensive set of commands for all aspects of SDLC.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/harperreed/dotfiles/tree/master/.claude/commands"&gt;&lt;code&gt;Project Workflow System&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/harperreed"&gt;harperreed&lt;/a&gt;&lt;br&gt; A set of commands that provide a comprehensive workflow system for managing projects, including task management, code review, and deployment processes.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://diwank.space/field-notes-from-shipping-real-code-with-claude"&gt;&lt;code&gt;Shipping Real Code w/ Claude&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/creatorrr"&gt;Diwank&lt;/a&gt;&lt;br&gt; A detailed blog post explaining the author's process for shipping a product with Claude Code, including CLAUDE.md files and other interesting resources.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/Helmi/claude-simone"&gt;&lt;code&gt;Simone&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Helmi"&gt;Helmi&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; A broader project management workflow for Claude Code that encompasses not just a set of commands, but a system of documents, guidelines, and processes to facilitate project planning and execution.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/wcygan/dotfiles/tree/d8ab6b9f5a7a81007b7f5fa3025d4f83ce12cc02/claude/commands"&gt;&lt;code&gt;Slash-commands megalist&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/wcygan"&gt;wcygan&lt;/a&gt;&lt;br&gt; A pretty stunning list (88 at the time of this post!) of slash-commands ranging from agent orchestration, code review, project management, security, documentation, self-assessment, almost anything you can dream of.&lt;/p&gt; 
&lt;br&gt; 
&lt;h2&gt;Tooling ğŸ§°&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Tooling&lt;/strong&gt; denotes applications that are built on top of Claude Code and consist of more components than slash-commands and &lt;code&gt;CLAUDE.md&lt;/code&gt; files&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;a href="https://github.com/ryoppippi/ccusage"&gt;&lt;code&gt;CC Usage&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/ryoppippi"&gt;ryoppippi&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Handy CLI tool for managing and analyzing Claude Code usage, based on analyzing local Claude Code logs. Presents a nice dashboard regarding cost information, token consumption, etc.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/nyatinte/ccexp"&gt;&lt;code&gt;ccexp&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/nyatinte"&gt;nyatinte&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Interactive CLI tool for discovering and managing Claude Code configuration files and slash commands with a beautiful terminal UI.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/ruvnet/claude-code-flow"&gt;&lt;code&gt;Claude Code Flow&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/ruvnet"&gt;ruvnet&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; This mode serves as a code-first orchestration layer, enabling Claude to write, edit, test, and optimize code autonomously across recursive agent cycles.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/possibilities/claude-composer"&gt;&lt;code&gt;Claude Composer&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/possibilities"&gt;Mike Bannister&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;Unlicense&lt;br&gt; A tool that adds small enhancements to Claude Code.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/claude-did-this/claude-hub"&gt;&lt;code&gt;Claude Hub&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/claude-did-this"&gt;Claude Did This&lt;/a&gt;&lt;br&gt; A webhook service that connects Claude Code to GitHub repositories, enabling AI-powered code assistance directly through pull requests and issues. This integration allows Claude to analyze repositories, answer technical questions, and help developers understand and improve their codebase through simple @mentions.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/smtg-ai/claude-squad"&gt;&lt;code&gt;Claude Squad&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/smtg-ai"&gt;smtg-ai&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;AGPL-3.0&lt;br&gt; Claude Squad is a terminal app that manages multiple Claude Code, Codex (and other local agents including Aider) in separate workspaces, allowing you to work on multiple tasks simultaneously.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/parruda/claude-swarm"&gt;&lt;code&gt;Claude Swarm&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/parruda"&gt;parruda&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Launch Claude Code session that is connected to a swarm of Claude Code Agents.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/eyaltoledano/claude-task-master"&gt;&lt;code&gt;Claude Task Master&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/eyaltoledano"&gt;eyaltoledano&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;NOASSERTION&lt;br&gt; A task management system for AI-driven development with Claude, designed to work seamlessly with Cursor AI.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/grahama1970/claude-task-runner"&gt;&lt;code&gt;Claude Task Runner&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/grahama1970"&gt;grahama1970&lt;/a&gt;&lt;br&gt; A specialized tool to manage context isolation and focused task execution with Claude Code, solving the critical challenge of context length limitations and task focus when working with Claude on complex, multi-step projects.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/dagger/container-use"&gt;&lt;code&gt;Container Use&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/dagger"&gt;dagger&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;Apache-2.0&lt;br&gt; Development environments for coding agents. Enable multiple agents to work safely and independently with your preferred stack.&lt;/p&gt; 
&lt;h3&gt;IDE Integrations&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/stevemolitor/claude-code.el"&gt;&lt;code&gt;claude-code.el&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/stevemolitor"&gt;stevemolitor&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;Apache-2.0&lt;br&gt; An Emacs interface for Claude Code CLI.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/greggh/claude-code.nvim"&gt;&lt;code&gt;claude-code.nvim&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/greggh"&gt;greggh&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; A seamless integration between Claude Code AI assistant and Neovim.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/stravu/crystal"&gt;&lt;code&gt;crystal&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/stravu"&gt;stravu&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; A full-fledged desktop application for orchestrating, monitoring, and interacting with Claude Code agents.&lt;/p&gt; 
&lt;br&gt; 
&lt;h2&gt;Hooks ğŸª&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Hooks&lt;/strong&gt; are a brand new API for Claude Code that allows users to activate commands and run scripts at different points in Claude's agentic lifecycle.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;strong&gt;[Experimental]&lt;/strong&gt; - The resources listed in this section have not been fully vetted and may not work as expected, given the bleeding-edge nature of Claude Code hooks. Nevertheless, I wished to include them at least as a source of inspiration and to explore this unknown terrain. YMMV!&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/beyondcode/claude-hooks-sdk"&gt;&lt;code&gt;claude-code-hooks-sdk&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/beyondcode"&gt;beyondcode&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; A Laravel-inspired PHP SDK for building Claude Code hook responses with a clean, fluent API. This SDK makes it easy to create structured JSON responses for Claude Code hooks using an expressive, chainable interface.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/johnlindquist/claude-hooks"&gt;&lt;code&gt;claude-hooks&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/johnlindquist"&gt;John Lindquist&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; A TypeScript-based system for configuring and customizing Claude Code hooks with a powerful and flexible interface.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/Veraticus/nix-config/tree/main/home-manager/claude-code/hooks"&gt;&lt;code&gt;Linting, testing, and notifications (in go)&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Veraticus"&gt;Josh Symonds&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Nice set of hooks for enforcing code quality (linting, testing, notifications), with a nice configuration setup as well.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/nizos/tdd-guard"&gt;&lt;code&gt;TDD Guard&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/nizos"&gt;Nizar Selander&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; A hooks-driven system that monitors file operations in real-time and blocks changes that violate TDD principles.&lt;/p&gt; 
&lt;br&gt; 
&lt;h2&gt;Slash-Commands ğŸ”ª&lt;/h2&gt; 
&lt;h3&gt;Version Control &amp;amp; Git&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/danielscholl/mvn-mcp-server/raw/main/.claude/commands/bug-fix.md"&gt;&lt;code&gt;/bug-fix&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/danielscholl"&gt;danielscholl&lt;/a&gt;&lt;br&gt; Streamlines bug fixing by creating a GitHub issue first, then a feature branch for implementing and thoroughly testing the solution before merging.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/evmts/tevm-monorepo/raw/main/.claude/commands/commit.md"&gt;&lt;code&gt;/commit&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/evmts"&gt;evmts&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Creates git commits using conventional commit format with appropriate emojis, following project standards and creating descriptive messages that explain the purpose of changes.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/steadycursor/steadystart/raw/main/.claude/commands/2-commit-fast.md"&gt;&lt;code&gt;/commit-fast&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/steadycursor"&gt;steadycursor&lt;/a&gt;&lt;br&gt; Automates git commit process by selecting the first suggested message, generating structured commits with consistent formatting while skipping manual confirmation and removing Claude co-Contributorship footer&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/toyamarinyon/giselle/raw/main/.claude/commands/create-pr.md"&gt;&lt;code&gt;/create-pr&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/toyamarinyon"&gt;toyamarinyon&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;Apache-2.0&lt;br&gt; Streamlines pull request creation by handling the entire workflow: creating a new branch, committing changes, formatting modified files with Biome, and submitting the PR.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/liam-hq/liam/raw/main/.claude/commands/create-pull-request.md"&gt;&lt;code&gt;/create-pull-request&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/liam-hq"&gt;liam-hq&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;Apache-2.0&lt;br&gt; Provides comprehensive PR creation guidance with GitHub CLI, enforcing title conventions, following template structure, and offering concrete command examples with best practices.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/evmts/tevm-monorepo/raw/main/.claude/commands/create-worktrees.md"&gt;&lt;code&gt;/create-worktrees&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/evmts"&gt;evmts&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Creates git worktrees for all open PRs or specific branches, handling branches with slashes, cleaning up stale worktrees, and supporting custom branch creation for development.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/jeremymailen/kotlinter-gradle/raw/master/.claude/commands/fix-github-issue.md"&gt;&lt;code&gt;/fix-github-issue&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/jeremymailen"&gt;jeremymailen&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;Apache-2.0&lt;br&gt; Analyzes and fixes GitHub issues using a structured approach with GitHub CLI for issue details, implementing necessary code changes, running tests, and creating proper commit messages.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/metabase/metabase/raw/master/.claude/commands/fix-issue.md"&gt;&lt;code&gt;/fix-issue&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/metabase"&gt;metabase&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;NOASSERTION&lt;br&gt; Addresses GitHub issues by taking issue number as parameter, analyzing context, implementing solution, and testing/validating the fix for proper integration.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/metabase/metabase/raw/master/.claude/commands/fix-pr.md"&gt;&lt;code&gt;/fix-pr&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/metabase"&gt;metabase&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;NOASSERTION&lt;br&gt; Fetches and fixes unresolved PR comments by automatically retrieving feedback, addressing reviewer concerns, making targeted code improvements, and streamlining the review process.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/evmts/tevm-monorepo/raw/main/.claude/commands/husky.md"&gt;&lt;code&gt;/husky&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/evmts"&gt;evmts&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Sets up and manages Husky Git hooks by configuring pre-commit hooks, establishing commit message standards, integrating with linting tools, and ensuring code quality on commits.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/arkavo-org/opentdf-rs/raw/main/.claude/commands/pr-review.md"&gt;&lt;code&gt;/pr-review&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/arkavo-org"&gt;arkavo-org&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Reviews pull request changes to provide feedback, check for issues, and suggest improvements before merging into the main codebase.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/giselles-ai/giselle/raw/main/.claude/commands/update-branch-name.md"&gt;&lt;code&gt;/update-branch-name&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/giselles-ai"&gt;giselles-ai&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;Apache-2.0&lt;br&gt; Updates branch names with proper prefixes and formats, enforcing naming conventions, supporting semantic prefixes, and managing remote branch updates.&lt;/p&gt; 
&lt;h3&gt;Code Analysis &amp;amp; Testing&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/rygwdn/slack-tools/raw/main/.claude/commands/check.md"&gt;&lt;code&gt;/check&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/rygwdn"&gt;rygwdn&lt;/a&gt;&lt;br&gt; Performs comprehensive code quality and security checks, featuring static analysis integration, security vulnerability scanning, code style enforcement, and detailed reporting.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/Graphlet-AI/eridu/raw/main/.claude/commands/clean.md"&gt;&lt;code&gt;/clean&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Graphlet-AI"&gt;Graphlet-AI&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;Apache-2.0&lt;br&gt; Addresses code formatting and quality issues by fixing black formatting problems, organizing imports with isort, resolving flake8 linting issues, and correcting mypy type errors.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/kingler/n8n_agent/raw/main/.claude/commands/code_analysis.md"&gt;&lt;code&gt;/code_analysis&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/kingler"&gt;kingler&lt;/a&gt;&lt;br&gt; Provides a menu of advanced code analysis commands for deep inspection, including knowledge graph generation, optimization suggestions, and quality evaluation.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/to4iki/ai-project-rules/raw/main/.claude/commands/optimize.md"&gt;&lt;code&gt;/optimize&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/to4iki"&gt;to4iki&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Analyzes code performance to identify bottlenecks, proposing concrete optimizations with implementation guidance for improved application performance.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/rzykov/metabase/raw/master/.claude/commands/repro-issue.md"&gt;&lt;code&gt;/repro-issue&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/rzykov"&gt;rzykov&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;NOASSERTION&lt;br&gt; Creates reproducible test cases for GitHub issues, ensuring tests fail reliably and documenting clear reproduction steps for developers.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/zscott/pane/raw/main/.claude/commands/tdd.md"&gt;&lt;code&gt;/tdd&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/zscott"&gt;zscott&lt;/a&gt;&lt;br&gt; Guides development using Test-Driven Development principles, enforcing Red-Green-Refactor discipline, integrating with git workflow, and managing PR creation.&lt;/p&gt; 
&lt;h3&gt;Context Loading &amp;amp; Priming&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/elizaOS/elizaos.github.io/raw/main/.claude/commands/context-prime.md"&gt;&lt;code&gt;/context-prime&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/elizaOS"&gt;elizaOS&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Primes Claude with comprehensive project understanding by loading repository structure, setting development context, establishing project goals, and defining collaboration parameters.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/okuvshynov/cubestat/raw/main/.claude/commands/initref.md"&gt;&lt;code&gt;/initref&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/okuvshynov"&gt;okuvshynov&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Initializes reference documentation structure with standard doc templates, API reference setup, documentation conventions, and placeholder content generation.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/ethpandaops/xatu-data/raw/master/.claude/commands/load-llms-txt.md"&gt;&lt;code&gt;/load-llms-txt&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/ethpandaops"&gt;ethpandaops&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Loads LLM configuration files to context, importing specific terminology, model configurations, and establishing baseline terminology for AI discussions.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/Mjvolk3/torchcell/raw/main/.claude/commands/load_coo_context.md"&gt;&lt;code&gt;/load_coo_context&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Mjvolk3"&gt;Mjvolk3&lt;/a&gt;&lt;br&gt; References specific files for sparse matrix operations, explains transform usage, compares with previous approaches, and sets data formatting context for development.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/Mjvolk3/torchcell/raw/main/.claude/commands/load_dango_pipeline.md"&gt;&lt;code&gt;/load_dango_pipeline&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Mjvolk3"&gt;Mjvolk3&lt;/a&gt;&lt;br&gt; Sets context for model training by referencing pipeline files, establishing working context, and preparing for pipeline work with relevant documentation.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/yzyydev/AI-Engineering-Structure/raw/main/.claude/commands/prime.md"&gt;&lt;code&gt;/prime&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/yzyydev"&gt;yzyydev&lt;/a&gt;&lt;br&gt; Sets up initial project context by viewing directory structure and reading key files, creating standardized context with directory visualization and key documentation focus.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/ddisisto/si/raw/main/.claude/commands/rsi.md"&gt;&lt;code&gt;/rsi&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/ddisisto"&gt;ddisisto&lt;/a&gt;&lt;br&gt; Reads all commands and key project files to optimize AI-assisted development by streamlining the process, loading command context, and setting up for better development workflow.&lt;/p&gt; 
&lt;h3&gt;Documentation &amp;amp; Changelogs&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/berrydev-ai/blockdoc-python/raw/main/.claude/commands/add-to-changelog.md"&gt;&lt;code&gt;/add-to-changelog&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/berrydev-ai"&gt;berrydev-ai&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Adds new entries to changelog files while maintaining format consistency, properly documenting changes, and following established project standards for version tracking.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/jerseycheese/Narraitor/tree/feature/issue-227-ai-suggestions/.claude/commands/analyze-issue.md"&gt;&lt;code&gt;/create-docs&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/jerseycheese"&gt;jerseycheese&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Analyzes code structure and purpose to create comprehensive documentation detailing inputs/outputs, behavior, user interaction flows, and edge cases with error handling.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/slunsford/coffee-analytics/raw/main/.claude/commands/docs.md"&gt;&lt;code&gt;/docs&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/slunsford"&gt;slunsford&lt;/a&gt;&lt;br&gt; Generates comprehensive documentation that follows project structure, documenting APIs and usage patterns with consistent formatting for better user understanding.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/hackdays-io/toban-contribution-viewer/raw/main/.claude/commands/explain-issue-fix.md"&gt;&lt;code&gt;/explain-issue-fix&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/hackdays-io"&gt;hackdays-io&lt;/a&gt;&lt;br&gt; Documents solution approaches for GitHub issues, explaining technical decisions, detailing challenges overcome, and providing implementation context for better understanding.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/Consiliency/Flutter-Structurizr/raw/main/.claude/commands/update-docs.md"&gt;&lt;code&gt;/update-docs&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Consiliency"&gt;Consiliency&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Reviews current documentation status, updates implementation progress, reviews phase documents, and maintains documentation consistency across the project.&lt;/p&gt; 
&lt;h3&gt;CI / Deployment&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/kelp/webdown/raw/main/.claude/commands/release.md"&gt;&lt;code&gt;/release&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/kelp"&gt;kelp&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Manages software releases by updating changelogs, reviewing README changes, evaluating version increments, and documenting release changes for better version tracking.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/hackdays-io/toban-contribution-viewer/raw/main/.claude/commands/run-ci.md"&gt;&lt;code&gt;/run-ci&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/hackdays-io"&gt;hackdays-io&lt;/a&gt;&lt;br&gt; Activates virtual environments, runs CI-compatible check scripts, iteratively fixes errors, and ensures all tests pass before completion.&lt;/p&gt; 
&lt;h3&gt;Project &amp;amp; Task Management&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/scopecraft/command/raw/main/.claude/commands/create-command.md"&gt;&lt;code&gt;/create-command&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/scopecraft"&gt;scopecraft&lt;/a&gt;&lt;br&gt; Guides Claude through creating new custom commands with proper structure by analyzing requirements, templating commands by category, enforcing command standards, and creating supporting documentation.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/taddyorg/inkverse/raw/main/.claude/commands/create-jtbd.md"&gt;&lt;code&gt;/create-jtbd&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/taddyorg"&gt;taddyorg&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;AGPL-3.0&lt;br&gt; Creates Jobs-to-be-Done frameworks that outline user needs with structured format, focusing on specific user problems and organizing by job categories for product development.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/taddyorg/inkverse/raw/main/.claude/commands/create-prd.md"&gt;&lt;code&gt;/create-prd&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/taddyorg"&gt;taddyorg&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;AGPL-3.0&lt;br&gt; Generates comprehensive product requirement documents outlining detailed specifications, requirements, and features following standardized document structure and format.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/Wirasm/claudecode-utils/raw/main/.claude/commands/create-prp.md"&gt;&lt;code&gt;/create-prp&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Wirasm"&gt;Wirasm&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Creates product requirement plans by reading PRP methodology, following template structure, creating comprehensive requirements, and structuring product definitions for development.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/disler/just-prompt/raw/main/.claude/commands/project_hello_w_name.md"&gt;&lt;code&gt;/project_hello_w_name&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/disler"&gt;disler&lt;/a&gt;&lt;br&gt; Creates customizable greeting components with name input, demonstrating argument passing, component reusability, state management, and user input handling.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/chrisleyva/todo-slash-command/raw/main/todo.md"&gt;&lt;code&gt;/todo&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/chrisleyva"&gt;chrisleyva&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; A convenient command to quickly manage project todo items without leaving the Claude Code interface, featuring due dates, sorting, task prioritization, and comprehensive todo list management.&lt;/p&gt; 
&lt;h3&gt;Miscellaneous&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/TuckerTucker/tkr-portfolio/raw/main/.claude/commands/five.md"&gt;&lt;code&gt;/five&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/TuckerTucker"&gt;TuckerTucker&lt;/a&gt;&lt;br&gt; Applies the "five whys" methodology to perform root cause analysis, identify underlying issues, and create solution approaches for complex problems.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/Mjvolk3/torchcell/raw/main/.claude/commands/fixing_go_in_graph.md"&gt;&lt;code&gt;/fixing_go_in_graph&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Mjvolk3"&gt;Mjvolk3&lt;/a&gt;&lt;br&gt; Focuses on Gene Ontology annotation integration in graph databases, handling multiple data sources, addressing graph representation issues, and ensuring correct data incorporation.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/GaloyMoney/lana-bank/raw/main/.claude/commands/mermaid.md"&gt;&lt;code&gt;/mermaid&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/GaloyMoney"&gt;GaloyMoney&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;NOASSERTION&lt;br&gt; Generates Mermaid diagrams from SQL schema files, creating entity relationship diagrams with table properties, validating diagram compilation, and ensuring complete entity coverage.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/Mjvolk3/torchcell/raw/main/.claude/commands/review_dcell_model.md"&gt;&lt;code&gt;/review_dcell_model&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Mjvolk3"&gt;Mjvolk3&lt;/a&gt;&lt;br&gt; Reviews old Dcell implementation files, comparing with newer Dango model, noting changes over time, and analyzing refactoring approaches for better code organization.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/zuplo/docs/raw/main/.claude/commands/use-stepper.md"&gt;&lt;code&gt;/use-stepper&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/zuplo"&gt;zuplo&lt;/a&gt;&lt;br&gt; Reformats documentation to use React Stepper component, transforming heading formats, applying proper indentation, and maintaining markdown compatibility with admonition formatting.&lt;/p&gt; 
&lt;br&gt; 
&lt;h2&gt;CLAUDE.md Files ğŸ“‚&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;&lt;code&gt;CLAUDE.md&lt;/code&gt; files&lt;/strong&gt; are files that contain important guidelines and context-specfic information or instructions that help Claude Code to better understand your project and your coding standards&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Language-Specific&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/didalgolab/ai-intellij-plugin/raw/main/CLAUDE.md"&gt;&lt;code&gt;AI IntelliJ Plugin&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/didalgolab"&gt;didalgolab&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;Apache-2.0&lt;br&gt; Provides comprehensive Gradle commands for IntelliJ plugin development with platform-specific coding patterns, detailed package structure guidelines, and clear internationalization standards.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/alexei-led/aws-mcp-server/raw/main/CLAUDE.md"&gt;&lt;code&gt;AWS MCP Server&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/alexei-led"&gt;alexei-led&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Features multiple Python environment setup options with detailed code style guidelines, comprehensive error handling recommendations, and security considerations for AWS CLI interactions.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/touchlab/DroidconKotlin/raw/main/CLAUDE.md"&gt;&lt;code&gt;DroidconKotlin&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/touchlab"&gt;touchlab&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;Apache-2.0&lt;br&gt; Delivers comprehensive Gradle commands for cross-platform Kotlin Multiplatform development with clear module structure and practical guidance for dependency injection.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/expectedparrot/edsl/raw/main/CLAUDE.md"&gt;&lt;code&gt;EDSL&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/expectedparrot"&gt;expectedparrot&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Offers detailed build and test commands with strict code style enforcement, comprehensive testing requirements, and standardized development workflow using Black and mypy.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/giselles-ai/giselle/raw/main/CLAUDE.md"&gt;&lt;code&gt;Giselle&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/giselles-ai"&gt;giselles-ai&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;Apache-2.0&lt;br&gt; Provides detailed build and test commands using pnpm and Vitest with strict code formatting requirements and comprehensive naming conventions for code consistency.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/hashintel/hash/raw/main/CLAUDE.md"&gt;&lt;code&gt;HASH&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/hashintel"&gt;hashintel&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;NOASSERTION&lt;br&gt; Features comprehensive repository structure breakdown with strong emphasis on coding standards, detailed Rust documentation guidelines, and systematic PR review process.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/inkline/inkline/raw/main/CLAUDE.md"&gt;&lt;code&gt;Inkline&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/inkline"&gt;inkline&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;NOASSERTION&lt;br&gt; Structures development workflow using pnpm with emphasis on TypeScript and Vue 3 Composition API, detailed component creation process, and comprehensive testing recommendations.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/mattgodbolt/jsbeeb/raw/main/CLAUDE.md"&gt;&lt;code&gt;JSBeeb&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/mattgodbolt"&gt;mattgodbolt&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;GPL-3.0&lt;br&gt; Provides development guide for JavaScript BBC Micro emulator with build and testing instructions, architecture documentation, and debugging workflows.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/LamoomAI/lamoom-python/raw/main/CLAUDE.md"&gt;&lt;code&gt;Lamoom Python&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/LamoomAI"&gt;LamoomAI&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;Apache-2.0&lt;br&gt; Serves as reference for production prompt engineering library with load balancing of AI Models, API documentation, and usage patterns with examples.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/langchain-ai/langgraphjs/raw/main/CLAUDE.md"&gt;&lt;code&gt;LangGraphJS&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/langchain-ai"&gt;langchain-ai&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Offers comprehensive build and test commands with detailed TypeScript style guidelines, layered library architecture, and monorepo structure using yarn workspaces.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/metabase/metabase/raw/master/CLAUDE.md"&gt;&lt;code&gt;Metabase&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/metabase"&gt;metabase&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;NOASSERTION&lt;br&gt; Details workflow for REPL-driven development in Clojure/ClojureScript with emphasis on incremental development, testing, and step-by-step approach for feature implementation.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/sgcarstrends/backend/raw/main/CLAUDE.md"&gt;&lt;code&gt;SG Cars Trends Backend&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/sgcarstrends"&gt;sgcarstrends&lt;/a&gt;&lt;br&gt; Provides comprehensive structure for TypeScript monorepo projects with detailed commands for development, testing, deployment, and AWS/Cloudflare integration.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/spylang/spy/raw/main/CLAUDE.md"&gt;&lt;code&gt;SPy&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/spylang"&gt;spylang&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Enforces strict coding conventions with comprehensive testing guidelines, multiple code compilation options, and backend-specific test decorators for targeted filtering.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/KarpelesLab/tpl/raw/master/CLAUDE.md"&gt;&lt;code&gt;TPL&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/KarpelesLab"&gt;KarpelesLab&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Details Go project conventions with comprehensive error handling recommendations, table-driven testing approach guidelines, and modernization suggestions for latest Go features.&lt;/p&gt; 
&lt;h3&gt;Domain-Specific&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/Layr-Labs/avs-vibe-developer-guide/raw/master/CLAUDE.md"&gt;&lt;code&gt;AVS Vibe Developer Guide&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Layr-Labs"&gt;Layr-Labs&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Structures AI-assisted EigenLayer AVS development workflow with consistent naming conventions for prompt files and established terminology standards for blockchain concepts.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/CommE2E/comm/raw/master/CLAUDE.md"&gt;&lt;code&gt;Comm&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/CommE2E"&gt;CommE2E&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;BSD-3-Clause&lt;br&gt; Serves as a development reference for E2E-encrypted messaging applications with code organization architecture, security implementation details, and testing procedures.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/badass-courses/course-builder/raw/main/CLAUDE.md"&gt;&lt;code&gt;Course Builder&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/badass-courses"&gt;badass-courses&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Enables real-time multiplayer capabilities for collaborative course creation with diverse tech stack integration and monorepo architecture using Turborepo.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/eastlondoner/cursor-tools/raw/main/CLAUDE.md"&gt;&lt;code&gt;Cursor Tools&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/eastlondoner"&gt;eastlondoner&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Creates a versatile AI command interface supporting multiple providers and models with flexible command options and browser automation through "Stagehand" feature.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/soramimi/Guitar/raw/master/CLAUDE.md"&gt;&lt;code&gt;Guitar&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/soramimi"&gt;soramimi&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;GPL-2.0&lt;br&gt; Serves as development guide for Guitar Git GUI Client with build commands for various platforms, code style guidelines for contributing, and project structure explanation.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/Fimeg/NetworkChronicles/raw/legacy-v1/CLAUDE.md"&gt;&lt;code&gt;Network Chronicles&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Fimeg"&gt;Fimeg&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Presents detailed implementation plan for AI-driven game characters with technical specifications for LLM integration, character guidelines, and service discovery mechanics.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/different-ai/note-companion/raw/master/CLAUDE.md"&gt;&lt;code&gt;Note Companion&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/different-ai"&gt;different-ai&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Provides detailed styling isolation techniques for Obsidian plugins using Tailwind with custom prefix to prevent style conflicts and practical troubleshooting steps.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/ParetoSecurity/pareto-mac/raw/main/CLAUDE.md"&gt;&lt;code&gt;Pareto Mac&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/ParetoSecurity"&gt;ParetoSecurity&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;GPL-3.0&lt;br&gt; Serves as development guide for Mac security audit tool with build instructions, contribution guidelines, testing procedures, and workflow documentation.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/steadycursor/steadystart/raw/main/CLAUDE.md"&gt;&lt;code&gt;SteadyStart&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/steadycursor"&gt;steadycursor&lt;/a&gt;&lt;br&gt; Clear and direct instructives about style, permissions, Claude's "role", communications, and documentation of Claude Code sessions for other team members to stay abreast.&lt;/p&gt; 
&lt;h3&gt;Project Scaffolding &amp;amp; MCP&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/basicmachines-co/basic-memory/raw/main/CLAUDE.md"&gt;&lt;code&gt;Basic Memory&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/basicmachines-co"&gt;basicmachines-co&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;AGPL-3.0&lt;br&gt; Presents an innovative AI-human collaboration framework with Model Context Protocol for bidirectional LLM-markdown communication and flexible knowledge structure for complex projects.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/grahama1970/claude-code-mcp-enhanced/raw/main/CLAUDE.md"&gt;&lt;code&gt;claude-code-mcp-enhanced&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/grahama1970"&gt;grahama1970&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Provides detailed and emphatic instructions for Claude to follow as a coding agent, with testing guidance, code examples, and compliance checks.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/Family-IT-Guy/perplexity-mcp/raw/main/CLAUDE.md"&gt;&lt;code&gt;Perplexity MCP&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/Family-IT-Guy"&gt;Family-IT-Guy&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;ISC&lt;br&gt; Offers clear step-by-step installation instructions with multiple configuration options, detailed troubleshooting guidance, and concise architecture overview of the MCP protocol.&lt;/p&gt; 
&lt;br&gt; 
&lt;h2&gt;Official Documentation ğŸ›ï¸&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Links to some of Anthropic's terrific documentation and resources regarding Claude Code&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;!--lint disable double-link--&gt; 
&lt;p&gt;&lt;a href="https://docs.anthropic.com/en/docs/claude-code"&gt;&lt;code&gt;Anthropic Documentation&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/anthropics"&gt;Anthropic&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;Â©&lt;br&gt; The official documentation for Claude Code, including installation instructions, usage guidelines, API references, tutorials, examples, loads of information that I won't list individually. Like Claude Code, the documentation is frequently updated.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/anthropics/anthropic-quickstarts/raw/main/CLAUDE.md"&gt;&lt;code&gt;Anthropic Quickstarts&lt;/code&gt;&lt;/a&gt; &amp;nbsp; by &amp;nbsp; &lt;a href="https://github.com/anthropics"&gt;Anthropic&lt;/a&gt; &amp;nbsp;&amp;nbsp;âš–ï¸&amp;nbsp;&amp;nbsp;MIT&lt;br&gt; Offers comprehensive development guides for three distinct AI-powered demo projects with standardized workflows, strict code style guidelines, and containerization instructions.&lt;/p&gt; 
&lt;h2&gt;Contributing ğŸŒ»&lt;/h2&gt; 
&lt;p&gt;Please note that this project is released with a &lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/code-of-conduct.md"&gt;Contributor Code of Conduct&lt;/a&gt;. By participating in this project you agree to abide by its terms.&lt;/p&gt; 
&lt;p&gt;Regarding content, we especially welcome:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Proven, effective resources that follow best practices and may even be in use in production.&lt;/li&gt; 
 &lt;li&gt;Innovative, creative, or experimental workflows that perhaps are still being iterated upon, but have high potential value, and push the boundaries of Claude Code's documented capabilities and use cases.&lt;/li&gt; 
 &lt;li&gt;Additional libraries and tooling that are built on top of Claude Code and offer enhanced functionality.&lt;/li&gt; 
 &lt;li&gt;Applications of Claude Code outside of the traditional "coding assistant" context, e.g., CI/CD integration, testing, documentation, dev-ops, etc.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/hesreallyhim/awesome-claude-code/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for more information on how to contribute to this project. Or, fire up Claude Code and invoke the &lt;code&gt;/project:add-new-resource&lt;/code&gt; command and let Claude walk you through it!&lt;/p&gt; 
&lt;p&gt;If you have any suggestions or thoughts on how to improve the repo, or how to best organize the list, feel free to start a Discussion topic. This is meant to be for the Claude Code community, and in general I prefer not to act on sole authority.&lt;/p&gt; 
&lt;h3&gt;A note about licenses&lt;/h3&gt; 
&lt;p&gt;Because simply listing a hyperlink does not qualify as redistribution, the license of the original source is not relevant to its inclusion. However, for posterity and convenience, we do host copies of all resources whose license permits it. Therefore, please include information about the resource's license. Additionally, take note: &lt;em&gt;if you do not include a LICENSE in your GitHub repo, then by default it is fully copyrighted and redistribution is not allowed&lt;/em&gt;. So, if you are intending to make an open source project, it's critical to pick from one of the many available open source licenses. This is just a reminder that without a LICENSE, your project is not open source (it's merely source-code-available) - it may of course still be included on this list, but this notice is to inform readers about the default rules regarding GitHub and LICENSE files. See &lt;a href="https://docs.github.com/en/repositories/managing-your-repositorys-settings-and-features/customizing-your-repository/licensing-a-repository"&gt;here&lt;/a&gt; for more details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>microsoft/markitdown</title>
      <link>https://github.com/microsoft/markitdown</link>
      <description>&lt;p&gt;Python tool for converting files and office documents to Markdown.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MarkItDown&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://pypi.org/project/markitdown/"&gt;&lt;img src="https://img.shields.io/pypi/v/markitdown.svg?sanitize=true" alt="PyPI"&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/pypi/dd/markitdown" alt="PyPI - Downloads"&gt; &lt;a href="https://github.com/microsoft/autogen"&gt;&lt;img src="https://img.shields.io/badge/Built%20by-AutoGen%20Team-blue" alt="Built by AutoGen Team"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] MarkItDown now offers an MCP (Model Context Protocol) server for integration with LLM applications like Claude Desktop. See &lt;a href="https://github.com/microsoft/markitdown/tree/main/packages/markitdown-mcp"&gt;markitdown-mcp&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] Breaking changes between 0.0.1 to 0.1.0:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Dependencies are now organized into optional feature-groups (further details below). Use &lt;code&gt;pip install 'markitdown[all]'&lt;/code&gt; to have backward-compatible behavior.&lt;/li&gt; 
  &lt;li&gt;convert_stream() now requires a binary file-like object (e.g., a file opened in binary mode, or an io.BytesIO object). This is a breaking change from the previous version, where it previously also accepted text file-like objects, like io.StringIO.&lt;/li&gt; 
  &lt;li&gt;The DocumentConverter class interface has changed to read from file-like streams rather than file paths. &lt;em&gt;No temporary files are created anymore&lt;/em&gt;. If you are the maintainer of a plugin, or custom DocumentConverter, you likely need to update your code. Otherwise, if only using the MarkItDown class or CLI (as in these examples), you should not need to change anything.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;MarkItDown is a lightweight Python utility for converting various files to Markdown for use with LLMs and related text analysis pipelines. To this end, it is most comparable to &lt;a href="https://github.com/deanmalmgren/textract"&gt;textract&lt;/a&gt;, but with a focus on preserving important document structure and content as Markdown (including: headings, lists, tables, links, etc.) While the output is often reasonably presentable and human-friendly, it is meant to be consumed by text analysis tools -- and may not be the best option for high-fidelity document conversions for human consumption.&lt;/p&gt; 
&lt;p&gt;MarkItDown currently supports the conversion from:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;PDF&lt;/li&gt; 
 &lt;li&gt;PowerPoint&lt;/li&gt; 
 &lt;li&gt;Word&lt;/li&gt; 
 &lt;li&gt;Excel&lt;/li&gt; 
 &lt;li&gt;Images (EXIF metadata and OCR)&lt;/li&gt; 
 &lt;li&gt;Audio (EXIF metadata and speech transcription)&lt;/li&gt; 
 &lt;li&gt;HTML&lt;/li&gt; 
 &lt;li&gt;Text-based formats (CSV, JSON, XML)&lt;/li&gt; 
 &lt;li&gt;ZIP files (iterates over contents)&lt;/li&gt; 
 &lt;li&gt;Youtube URLs&lt;/li&gt; 
 &lt;li&gt;EPubs&lt;/li&gt; 
 &lt;li&gt;... and more!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Why Markdown?&lt;/h2&gt; 
&lt;p&gt;Markdown is extremely close to plain text, with minimal markup or formatting, but still provides a way to represent important document structure. Mainstream LLMs, such as OpenAI's GPT-4o, natively "&lt;em&gt;speak&lt;/em&gt;" Markdown, and often incorporate Markdown into their responses unprompted. This suggests that they have been trained on vast amounts of Markdown-formatted text, and understand it well. As a side benefit, Markdown conventions are also highly token-efficient.&lt;/p&gt; 
&lt;h2&gt;Prerequisites&lt;/h2&gt; 
&lt;p&gt;MarkItDown requires Python 3.10 or higher. It is recommended to use a virtual environment to avoid dependency conflicts.&lt;/p&gt; 
&lt;p&gt;With the standard Python installation, you can create and activate a virtual environment using the following commands:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m venv .venv
source .venv/bin/activate
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If using &lt;code&gt;uv&lt;/code&gt;, you can create a virtual environment with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv venv --python=3.12 .venv
source .venv/bin/activate
# NOTE: Be sure to use 'uv pip install' rather than just 'pip install' to install packages in this virtual environment
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you are using Anaconda, you can create a virtual environment with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;conda create -n markitdown python=3.12
conda activate markitdown
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;To install MarkItDown, use pip: &lt;code&gt;pip install 'markitdown[all]'&lt;/code&gt;. Alternatively, you can install it from the source:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone git@github.com:microsoft/markitdown.git
cd markitdown
pip install -e 'packages/markitdown[all]'
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;h3&gt;Command-Line&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown path-to-file.pdf &amp;gt; document.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or use &lt;code&gt;-o&lt;/code&gt; to specify the output file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown path-to-file.pdf -o document.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also pipe content:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cat path-to-file.pdf | markitdown
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Optional Dependencies&lt;/h3&gt; 
&lt;p&gt;MarkItDown has optional dependencies for activating various file formats. Earlier in this document, we installed all optional dependencies with the &lt;code&gt;[all]&lt;/code&gt; option. However, you can also install them individually for more control. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install 'markitdown[pdf, docx, pptx]'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;will install only the dependencies for PDF, DOCX, and PPTX files.&lt;/p&gt; 
&lt;p&gt;At the moment, the following optional dependencies are available:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;[all]&lt;/code&gt; Installs all optional dependencies&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[pptx]&lt;/code&gt; Installs dependencies for PowerPoint files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[docx]&lt;/code&gt; Installs dependencies for Word files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[xlsx]&lt;/code&gt; Installs dependencies for Excel files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[xls]&lt;/code&gt; Installs dependencies for older Excel files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[pdf]&lt;/code&gt; Installs dependencies for PDF files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[outlook]&lt;/code&gt; Installs dependencies for Outlook messages&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[az-doc-intel]&lt;/code&gt; Installs dependencies for Azure Document Intelligence&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[audio-transcription]&lt;/code&gt; Installs dependencies for audio transcription of wav and mp3 files&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[youtube-transcription]&lt;/code&gt; Installs dependencies for fetching YouTube video transcription&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Plugins&lt;/h3&gt; 
&lt;p&gt;MarkItDown also supports 3rd-party plugins. Plugins are disabled by default. To list installed plugins:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown --list-plugins
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To enable plugins use:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown --use-plugins path-to-file.pdf
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To find available plugins, search GitHub for the hashtag &lt;code&gt;#markitdown-plugin&lt;/code&gt;. To develop a plugin, see &lt;code&gt;packages/markitdown-sample-plugin&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Azure Document Intelligence&lt;/h3&gt; 
&lt;p&gt;To use Microsoft Document Intelligence for conversion:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;markitdown path-to-file.pdf -o document.md -d -e "&amp;lt;document_intelligence_endpoint&amp;gt;"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;More information about how to set up an Azure Document Intelligence Resource can be found &lt;a href="https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/how-to-guides/create-document-intelligence-resource?view=doc-intel-4.0.0"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Python API&lt;/h3&gt; 
&lt;p&gt;Basic usage in Python:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from markitdown import MarkItDown

md = MarkItDown(enable_plugins=False) # Set to True to enable plugins
result = md.convert("test.xlsx")
print(result.text_content)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Document Intelligence conversion in Python:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from markitdown import MarkItDown

md = MarkItDown(docintel_endpoint="&amp;lt;document_intelligence_endpoint&amp;gt;")
result = md.convert("test.pdf")
print(result.text_content)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To use Large Language Models for image descriptions, provide &lt;code&gt;llm_client&lt;/code&gt; and &lt;code&gt;llm_model&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from markitdown import MarkItDown
from openai import OpenAI

client = OpenAI()
md = MarkItDown(llm_client=client, llm_model="gpt-4o")
result = md.convert("example.jpg")
print(result.text_content)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Docker&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;docker build -t markitdown:latest .
docker run --rm -i markitdown:latest &amp;lt; ~/your-file.pdf &amp;gt; output.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href="https://cla.opensource.microsoft.com"&gt;https://cla.opensource.microsoft.com&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; 
&lt;p&gt;This project has adopted the &lt;a href="https://opensource.microsoft.com/codeofconduct/"&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href="https://opensource.microsoft.com/codeofconduct/faq/"&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href="mailto:opencode@microsoft.com"&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; 
&lt;h3&gt;How to Contribute&lt;/h3&gt; 
&lt;p&gt;You can help by looking at issues or helping review PRs. Any issue or PR is welcome, but we have also marked some as 'open for contribution' and 'open for reviewing' to help facilitate community contributions. These are ofcourse just suggestions and you are welcome to contribute in any way you like.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;&lt;/th&gt; 
    &lt;th&gt;All&lt;/th&gt; 
    &lt;th&gt;Especially Needs Help from Community&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Issues&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/microsoft/markitdown/issues"&gt;All Issues&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/microsoft/markitdown/issues?q=is%3Aissue+is%3Aopen+label%3A%22open+for+contribution%22"&gt;Issues open for contribution&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;PRs&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/microsoft/markitdown/pulls"&gt;All PRs&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href="https://github.com/microsoft/markitdown/pulls?q=is%3Apr+is%3Aopen+label%3A%22open+for+reviewing%22"&gt;PRs open for reviewing&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;h3&gt;Running Tests and Checks&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Navigate to the MarkItDown package:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;cd packages/markitdown
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install &lt;code&gt;hatch&lt;/code&gt; in your environment and run tests:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;pip install hatch  # Other ways of installing hatch: https://hatch.pypa.io/dev/install/
hatch shell
hatch test
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;(Alternative) Use the Devcontainer which has all the dependencies installed:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;# Reopen the project in Devcontainer and run:
hatch test
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run pre-commit checks before submitting a PR: &lt;code&gt;pre-commit run --all-files&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Contributing 3rd-party Plugins&lt;/h3&gt; 
&lt;p&gt;You can also contribute by creating and sharing 3rd party plugins. See &lt;code&gt;packages/markitdown-sample-plugin&lt;/code&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;Trademarks&lt;/h2&gt; 
&lt;p&gt;This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow &lt;a href="https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general"&gt;Microsoft's Trademark &amp;amp; Brand Guidelines&lt;/a&gt;. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party's policies.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>langchain-ai/open_deep_research</title>
      <link>https://github.com/langchain-ai/open_deep_research</link>
      <description>&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Open Deep Research&lt;/h1&gt; 
&lt;img width="1388" height="298" alt="full_diagram" src="https://github.com/user-attachments/assets/12a2371b-8be2-4219-9b48-90503eb43c69"&gt; 
&lt;p&gt;Deep research has broken out as one of the most popular agent applications. This is a simple, configurable, fully open source deep research agent that works across many model providers, search tools, and MCP servers.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Read more in our &lt;a href="https://blog.langchain.com/open-deep-research/"&gt;blog&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;See our &lt;a href="https://www.youtube.com/watch?v=agGiWUpxkhg"&gt;video&lt;/a&gt; for a quick overview&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸš€ Quickstart&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Clone the repository and activate a virtual environment:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/langchain-ai/open_deep_research.git
cd open_deep_research
uv venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Install dependencies:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv pip install -r pyproject.toml
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Set up your &lt;code&gt;.env&lt;/code&gt; file to customize the environment variables (for model selection, search tools, and other configuration settings):&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cp .env.example .env
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="4"&gt; 
 &lt;li&gt;Launch the assistant with the LangGraph server locally to open LangGraph Studio in your browser:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install dependencies and start the LangGraph server
uvx --refresh --from "langgraph-cli[inmem]" --with-editable . --python 3.11 langgraph dev --allow-blocking
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Use this to open the Studio UI:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;- ğŸš€ API: http://127.0.0.1:2024
- ğŸ¨ Studio UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024
- ğŸ“š API Docs: http://127.0.0.1:2024/docs
&lt;/code&gt;&lt;/pre&gt; 
&lt;img width="817" height="666" alt="Screenshot 2025-07-13 at 11 21 12â€¯PM" src="https://github.com/user-attachments/assets/052f2ed3-c664-4a4f-8ec2-074349dcaa3f"&gt; 
&lt;p&gt;Ask a question in the &lt;code&gt;messages&lt;/code&gt; input field and click &lt;code&gt;Submit&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Configurations&lt;/h3&gt; 
&lt;p&gt;Open Deep Research offers extensive configuration options to customize the research process and model behavior. All configurations can be set via the web UI, environment variables, or by modifying the configuration directly.&lt;/p&gt; 
&lt;h4&gt;General Settings&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Max Structured Output Retries&lt;/strong&gt; (default: 3): Maximum number of retries for structured output calls from models when parsing fails&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Allow Clarification&lt;/strong&gt; (default: true): Whether to allow the researcher to ask clarifying questions before starting research&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Max Concurrent Research Units&lt;/strong&gt; (default: 5): Maximum number of research units to run concurrently using sub-agents. Higher values enable faster research but may hit rate limits&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Research Configuration&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Search API&lt;/strong&gt; (default: Tavily): Choose from Tavily (works with all models), OpenAI Native Web Search, Anthropic Native Web Search, or None&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Max Researcher Iterations&lt;/strong&gt; (default: 3): Number of times the Research Supervisor will reflect on research and ask follow-up questions&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Max React Tool Calls&lt;/strong&gt; (default: 5): Maximum number of tool calling iterations in a single researcher step&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Models&lt;/h4&gt; 
&lt;p&gt;Open Deep Research uses multiple specialized models for different research tasks:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Summarization Model&lt;/strong&gt; (default: &lt;code&gt;openai:gpt-4.1-nano&lt;/code&gt;): Summarizes research results from search APIs&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Research Model&lt;/strong&gt; (default: &lt;code&gt;openai:gpt-4.1&lt;/code&gt;): Conducts research and analysis&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Compression Model&lt;/strong&gt; (default: &lt;code&gt;openai:gpt-4.1-mini&lt;/code&gt;): Compresses research findings from sub-agents&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Final Report Model&lt;/strong&gt; (default: &lt;code&gt;openai:gpt-4.1&lt;/code&gt;): Writes the final comprehensive report&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;All models are configured using &lt;a href="https://python.langchain.com/docs/how_to/chat_models_universal_init/"&gt;init_chat_model() API&lt;/a&gt; which supports providers like OpenAI, Anthropic, Google Vertex AI, and others.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Important Model Requirements:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Structured Outputs&lt;/strong&gt;: All models must support structured outputs. Check support &lt;a href="https://python.langchain.com/docs/integrations/chat/"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Search API Compatibility&lt;/strong&gt;: Research and Compression models must support your selected search API:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Anthropic search requires Anthropic models with web search capability&lt;/li&gt; 
   &lt;li&gt;OpenAI search requires OpenAI models with web search capability&lt;/li&gt; 
   &lt;li&gt;Tavily works with all models&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Tool Calling&lt;/strong&gt;: All models must support tool calling functionality&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Special Configurations&lt;/strong&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;For OpenRouter: Follow &lt;a href="https://github.com/langchain-ai/open_deep_research/issues/75#issuecomment-2811472408"&gt;this guide&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;For local models via Ollama: See &lt;a href="https://github.com/langchain-ai/open_deep_research/issues/65#issuecomment-2743586318"&gt;setup instructions&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Example MCP (Model Context Protocol) Servers&lt;/h4&gt; 
&lt;p&gt;Open Deep Research supports MCP servers to extend research capabilities.&lt;/p&gt; 
&lt;h4&gt;Local MCP Servers&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;Filesystem MCP Server&lt;/strong&gt; provides secure file system operations with robust access control:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Read, write, and manage files and directories&lt;/li&gt; 
 &lt;li&gt;Perform operations like reading file contents, creating directories, moving files, and searching&lt;/li&gt; 
 &lt;li&gt;Restrict operations to predefined directories for security&lt;/li&gt; 
 &lt;li&gt;Support for both command-line configuration and dynamic MCP roots&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Example usage:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;mcp-server-filesystem /path/to/allowed/dir1 /path/to/allowed/dir2
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Remote MCP Servers&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;Remote MCP servers&lt;/strong&gt; enable distributed agent coordination and support streamable HTTP requests. Unlike local servers, they can be multi-tenant and require more complex authentication.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Arcade MCP Server Example&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "url": "https://api.arcade.dev/v1/mcps/ms_0ujssxh0cECutqzMgbtXSGnjorm",
  "tools": ["Search_SearchHotels", "Search_SearchOneWayFlights", "Search_SearchRoundtripFlights"]
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Remote servers can be configured as authenticated or unauthenticated and support JWT-based authentication through OAuth endpoints.&lt;/p&gt; 
&lt;h3&gt;Evaluation&lt;/h3&gt; 
&lt;p&gt;A comprehensive batch evaluation system designed for detailed analysis and comparative studies.&lt;/p&gt; 
&lt;h4&gt;&lt;strong&gt;Features:&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-dimensional Scoring&lt;/strong&gt;: Specialized evaluators with 0-1 scale ratings&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Dataset-driven Evaluation&lt;/strong&gt;: Batch processing across multiple test cases&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;&lt;strong&gt;Usage:&lt;/strong&gt;&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Run comprehensive evaluation on LangSmith datasets
python tests/run_evaluate.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;&lt;strong&gt;Key Files:&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;tests/run_evaluate.py&lt;/code&gt;: Main evaluation script&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;tests/evaluators.py&lt;/code&gt;: Specialized evaluator functions&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;tests/prompts.py&lt;/code&gt;: Evaluation prompts for each dimension&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Deployments and Usages&lt;/h3&gt; 
&lt;h4&gt;LangGraph Studio&lt;/h4&gt; 
&lt;p&gt;Follow the &lt;a href="https://raw.githubusercontent.com/langchain-ai/open_deep_research/main/#-quickstart"&gt;quickstart&lt;/a&gt; to start LangGraph server locally and test the agent out on LangGraph Studio.&lt;/p&gt; 
&lt;h4&gt;Hosted deployment&lt;/h4&gt; 
&lt;p&gt;You can easily deploy to &lt;a href="https://langchain-ai.github.io/langgraph/concepts/#deployment-options"&gt;LangGraph Platform&lt;/a&gt;.&lt;/p&gt; 
&lt;h4&gt;Open Agent Platform&lt;/h4&gt; 
&lt;p&gt;Open Agent Platform (OAP) is a UI from which non-technical users can build and configure their own agents. OAP is great for allowing users to configure the Deep Researcher with different MCP tools and search APIs that are best suited to their needs and the problems that they want to solve.&lt;/p&gt; 
&lt;p&gt;We've deployed Open Deep Research to our public demo instance of OAP. All you need to do is add your API Keys, and you can test out the Deep Researcher for yourself! Try it out &lt;a href="https://oap.langchain.com"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;You can also deploy your own instance of OAP, and make your own custom agents (like Deep Researcher) available on it to your users.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://docs.oap.langchain.com/quickstart"&gt;Deploy Open Agent Platform&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.oap.langchain.com/setup/agents"&gt;Add Deep Researcher to OAP&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Updates ğŸ”¥&lt;/h3&gt; 
&lt;h3&gt;Legacy Implementations ğŸ›ï¸&lt;/h3&gt; 
&lt;p&gt;The &lt;code&gt;src/legacy/&lt;/code&gt; folder contains two earlier implementations that provide alternative approaches to automated research:&lt;/p&gt; 
&lt;h4&gt;1. Workflow Implementation (&lt;code&gt;legacy/graph.py&lt;/code&gt;)&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Plan-and-Execute&lt;/strong&gt;: Structured workflow with human-in-the-loop planning&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Sequential Processing&lt;/strong&gt;: Creates sections one by one with reflection&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Interactive Control&lt;/strong&gt;: Allows feedback and approval of report plans&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Quality Focused&lt;/strong&gt;: Emphasizes accuracy through iterative refinement&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;2. Multi-Agent Implementation (&lt;code&gt;legacy/multi_agent.py&lt;/code&gt;)&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Supervisor-Researcher Architecture&lt;/strong&gt;: Coordinated multi-agent system&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Parallel Processing&lt;/strong&gt;: Multiple researchers work simultaneously&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Speed Optimized&lt;/strong&gt;: Faster report generation through concurrency&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MCP Support&lt;/strong&gt;: Extensive Model Context Protocol integration&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See &lt;code&gt;src/legacy/legacy.md&lt;/code&gt; for detailed documentation, configuration options, and usage examples for both legacy implementations.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>musistudio/claude-code-router</title>
      <link>https://github.com/musistudio/claude-code-router</link>
      <description>&lt;p&gt;Use Claude Code as the foundation for coding infrastructure, allowing you to decide how to interact with the model while enjoying updates from Anthropic.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Claude Code Router&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/musistudio/claude-code-router/main/README_zh.md"&gt;ä¸­æ–‡ç‰ˆ&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;A powerful tool to route Claude Code requests to different models and customize any request.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/musistudio/claude-code-router/main/blog/images/claude-code.png" alt=""&gt;&lt;/p&gt; 
&lt;h2&gt;âœ¨ Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Model Routing&lt;/strong&gt;: Route requests to different models based on your needs (e.g., background tasks, thinking, long context).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-Provider Support&lt;/strong&gt;: Supports various model providers like OpenRouter, DeepSeek, Ollama, Gemini, Volcengine, and SiliconFlow.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Request/Response Transformation&lt;/strong&gt;: Customize requests and responses for different providers using transformers.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Dynamic Model Switching&lt;/strong&gt;: Switch models on-the-fly within Claude Code using the &lt;code&gt;/model&lt;/code&gt; command.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;GitHub Actions Integration&lt;/strong&gt;: Trigger Claude Code tasks in your GitHub workflows.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Plugin System&lt;/strong&gt;: Extend functionality with custom transformers.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸš€ Getting Started&lt;/h2&gt; 
&lt;h3&gt;1. Installation&lt;/h3&gt; 
&lt;p&gt;First, ensure you have &lt;a href="https://docs.anthropic.com/en/docs/claude-code/quickstart"&gt;Claude Code&lt;/a&gt; installed:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;npm install -g @anthropic-ai/claude-code
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then, install Claude Code Router:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;npm install -g @musistudio/claude-code-router
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. Configuration&lt;/h3&gt; 
&lt;p&gt;Create and configure your &lt;code&gt;~/.claude-code-router/config.json&lt;/code&gt; file. For more details, you can refer to &lt;code&gt;config.example.json&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;The &lt;code&gt;config.json&lt;/code&gt; file has several key sections:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;PROXY_URL&lt;/code&gt;&lt;/strong&gt; (optional): You can set a proxy for API requests, for example: &lt;code&gt;"PROXY_URL": "http://127.0.0.1:7890"&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;LOG&lt;/code&gt;&lt;/strong&gt; (optional): You can enable logging by setting it to &lt;code&gt;true&lt;/code&gt;. The log file will be located at &lt;code&gt;$HOME/.claude-code-router.log&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;APIKEY&lt;/code&gt;&lt;/strong&gt; (optional): You can set a secret key to authenticate requests. When set, clients must provide this key in the &lt;code&gt;Authorization&lt;/code&gt; header (e.g., &lt;code&gt;Bearer your-secret-key&lt;/code&gt;) or the &lt;code&gt;x-api-key&lt;/code&gt; header. Example: &lt;code&gt;"APIKEY": "your-secret-key"&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;HOST&lt;/code&gt;&lt;/strong&gt; (optional): You can set the host address for the server. If &lt;code&gt;APIKEY&lt;/code&gt; is not set, the host will be forced to &lt;code&gt;127.0.0.1&lt;/code&gt; for security reasons to prevent unauthorized access. Example: &lt;code&gt;"HOST": "0.0.0.0"&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;Providers&lt;/code&gt;&lt;/strong&gt;: Used to configure different model providers.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;Router&lt;/code&gt;&lt;/strong&gt;: Used to set up routing rules. &lt;code&gt;default&lt;/code&gt; specifies the default model, which will be used for all requests if no other route is configured.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Here is a comprehensive example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "APIKEY": "your-secret-key",
  "PROXY_URL": "http://127.0.0.1:7890",
  "LOG": true,
  "Providers": [
    {
      "name": "openrouter",
      "api_base_url": "https://openrouter.ai/api/v1/chat/completions",
      "api_key": "sk-xxx",
      "models": [
        "google/gemini-2.5-pro-preview",
        "anthropic/claude-sonnet-4",
        "anthropic/claude-3.5-sonnet",
        "anthropic/claude-3.7-sonnet:thinking"
      ],
      "transformer": {
        "use": ["openrouter"]
      }
    },
    {
      "name": "deepseek",
      "api_base_url": "https://api.deepseek.com/chat/completions",
      "api_key": "sk-xxx",
      "models": ["deepseek-chat", "deepseek-reasoner"],
      "transformer": {
        "use": ["deepseek"],
        "deepseek-chat": {
          "use": ["tooluse"]
        }
      }
    },
    {
      "name": "ollama",
      "api_base_url": "http://localhost:11434/v1/chat/completions",
      "api_key": "ollama",
      "models": ["qwen2.5-coder:latest"]
    },
    {
      "name": "gemini",
      "api_base_url": "https://generativelanguage.googleapis.com/v1beta/models/",
      "api_key": "sk-xxx",
      "models": ["gemini-2.5-flash", "gemini-2.5-pro"],
      "transformer": {
        "use": ["gemini"]
      }
    },
    {
      "name": "volcengine",
      "api_base_url": "https://ark.cn-beijing.volces.com/api/v3/chat/completions",
      "api_key": "sk-xxx",
      "models": ["deepseek-v3-250324", "deepseek-r1-250528"],
      "transformer": {
        "use": ["deepseek"]
      }
    },
    {
      "name": "modelscope",
      "api_base_url": "https://api-inference.modelscope.cn/v1/chat/completions",
      "api_key": "",
      "models": ["Qwen/Qwen3-Coder-480B-A35B-Instruct"],
      "transformer": {
        "use": [
          [
            "maxtoken",
            {
              "max_tokens": 8192
            }
          ]
        ]
      }
    },
    {
      "name": "dashscope",
      "api_base_url": "https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions",
      "api_key": "",
      "models": ["qwen3-coder-plus"],
      "transformer": {
        "use": [
          [
            "maxtoken",
            {
              "max_tokens": 8192
            }
          ]
        ]
      }
    }
  ],
  "Router": {
    "default": "deepseek,deepseek-chat",
    "background": "ollama,qwen2.5-coder:latest",
    "think": "deepseek,deepseek-reasoner",
    "longContext": "openrouter,google/gemini-2.5-pro-preview",
    "webSearch": "gemini,gemini-2.5-flash"
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;3. Running Claude Code with the Router&lt;/h3&gt; 
&lt;p&gt;Start Claude Code using the router:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;ccr code
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Providers&lt;/h4&gt; 
&lt;p&gt;The &lt;code&gt;Providers&lt;/code&gt; array is where you define the different model providers you want to use. Each provider object requires:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;name&lt;/code&gt;: A unique name for the provider.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;api_base_url&lt;/code&gt;: The full API endpoint for chat completions.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;api_key&lt;/code&gt;: Your API key for the provider.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;models&lt;/code&gt;: A list of model names available from this provider.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;transformer&lt;/code&gt; (optional): Specifies transformers to process requests and responses.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Transformers&lt;/h4&gt; 
&lt;p&gt;Transformers allow you to modify the request and response payloads to ensure compatibility with different provider APIs.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Global Transformer&lt;/strong&gt;: Apply a transformer to all models from a provider. In this example, the &lt;code&gt;openrouter&lt;/code&gt; transformer is applied to all models under the &lt;code&gt;openrouter&lt;/code&gt; provider.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-json"&gt; {
   "name": "openrouter",
   "api_base_url": "https://openrouter.ai/api/v1/chat/completions",
   "api_key": "sk-xxx",
   "models": [
     "google/gemini-2.5-pro-preview",
     "anthropic/claude-sonnet-4",
     "anthropic/claude-3.5-sonnet"
   ],
   "transformer": { "use": ["openrouter"] }
 }
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Model-Specific Transformer&lt;/strong&gt;: Apply a transformer to a specific model. In this example, the &lt;code&gt;deepseek&lt;/code&gt; transformer is applied to all models, and an additional &lt;code&gt;tooluse&lt;/code&gt; transformer is applied only to the &lt;code&gt;deepseek-chat&lt;/code&gt; model.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-json"&gt; {
   "name": "deepseek",
   "api_base_url": "https://api.deepseek.com/chat/completions",
   "api_key": "sk-xxx",
   "models": ["deepseek-chat", "deepseek-reasoner"],
   "transformer": {
     "use": ["deepseek"],
     "deepseek-chat": { "use": ["tooluse"] }
   }
 }
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Passing Options to a Transformer&lt;/strong&gt;: Some transformers, like &lt;code&gt;maxtoken&lt;/code&gt;, accept options. To pass options, use a nested array where the first element is the transformer name and the second is an options object.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-json"&gt;{
  "name": "siliconflow",
  "api_base_url": "https://api.siliconflow.cn/v1/chat/completions",
  "api_key": "sk-xxx",
  "models": ["moonshotai/Kimi-K2-Instruct"],
  "transformer": {
    "use": [
      [
        "maxtoken",
        {
          "max_tokens": 16384
        }
      ]
    ]
  }
}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Available Built-in Transformers:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;deepseek&lt;/code&gt;: Adapts requests/responses for DeepSeek API.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;gemini&lt;/code&gt;: Adapts requests/responses for Gemini API.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;openrouter&lt;/code&gt;: Adapts requests/responses for OpenRouter API.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;groq&lt;/code&gt;: Adapts requests/responses for groq API.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;maxtoken&lt;/code&gt;: Sets a specific &lt;code&gt;max_tokens&lt;/code&gt; value.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;tooluse&lt;/code&gt;: Optimizes tool usage for certain models via &lt;code&gt;tool_choice&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;gemini-cli&lt;/code&gt; (experimental): Unofficial support for Gemini via Gemini CLI &lt;a href="https://gist.github.com/musistudio/1c13a65f35916a7ab690649d3df8d1cd"&gt;gemini-cli.js&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Custom Transformers:&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;You can also create your own transformers and load them via the &lt;code&gt;transformers&lt;/code&gt; field in &lt;code&gt;config.json&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "transformers": [
      {
        "path": "$HOME/.claude-code-router/plugins/gemini-cli.js",
        "options": {
          "project": "xxx"
        }
      }
  ]
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Router&lt;/h4&gt; 
&lt;p&gt;The &lt;code&gt;Router&lt;/code&gt; object defines which model to use for different scenarios:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;default&lt;/code&gt;: The default model for general tasks.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;background&lt;/code&gt;: A model for background tasks. This can be a smaller, local model to save costs.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;think&lt;/code&gt;: A model for reasoning-heavy tasks, like Plan Mode.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;longContext&lt;/code&gt;: A model for handling long contexts (e.g., &amp;gt; 60K tokens).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;webSearch&lt;/code&gt;: Used for handling web search tasks and this requires the model itself to support the feature. If you're using openrouter, you need to add the &lt;code&gt;:online&lt;/code&gt; suffix after the model name.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can also switch models dynamically in Claude Code with the &lt;code&gt;/model&lt;/code&gt; command: &lt;code&gt;/model provider_name,model_name&lt;/code&gt; Example: &lt;code&gt;/model openrouter,anthropic/claude-3.5-sonnet&lt;/code&gt;&lt;/p&gt; 
&lt;h4&gt;Custom Router&lt;/h4&gt; 
&lt;p&gt;For more advanced routing logic, you can specify a custom router script via the &lt;code&gt;CUSTOM_ROUTER_PATH&lt;/code&gt; in your &lt;code&gt;config.json&lt;/code&gt;. This allows you to implement complex routing rules beyond the default scenarios.&lt;/p&gt; 
&lt;p&gt;In your &lt;code&gt;config.json&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "CUSTOM_ROUTER_PATH": "$HOME/.claude-code-router/custom-router.js"
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The custom router file must be a JavaScript module that exports an &lt;code&gt;async&lt;/code&gt; function. This function receives the request object and the config object as arguments and should return the provider and model name as a string (e.g., &lt;code&gt;"provider_name,model_name"&lt;/code&gt;), or &lt;code&gt;null&lt;/code&gt; to fall back to the default router.&lt;/p&gt; 
&lt;p&gt;Here is an example of a &lt;code&gt;custom-router.js&lt;/code&gt; based on &lt;code&gt;custom-router.example.js&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-javascript"&gt;// $HOME/.claude-code-router/custom-router.js

/**
 * A custom router function to determine which model to use based on the request.
 *
 * @param {object} req - The request object from Claude Code, containing the request body.
 * @param {object} config - The application's config object.
 * @returns {Promise&amp;lt;string|null&amp;gt;} - A promise that resolves to the "provider,model_name" string, or null to use the default router.
 */
module.exports = async function router(req, config) {
  const userMessage = req.body.messages.find(m =&amp;gt; m.role === 'user')?.content;

  if (userMessage &amp;amp;&amp;amp; userMessage.includes('explain this code')) {
    // Use a powerful model for code explanation
    return 'openrouter,anthropic/claude-3.5-sonnet';
  }

  // Fallback to the default router configuration
  return null;
};
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ğŸ¤– GitHub Actions&lt;/h2&gt; 
&lt;p&gt;Integrate Claude Code Router into your CI/CD pipeline. After setting up &lt;a href="https://docs.anthropic.com/en/docs/claude-code/github-actions"&gt;Claude Code Actions&lt;/a&gt;, modify your &lt;code&gt;.github/workflows/claude.yaml&lt;/code&gt; to use the router:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;name: Claude Code

on:
  issue_comment:
    types: [created]
  # ... other triggers

jobs:
  claude:
    if: |
      (github.event_name == 'issue_comment' &amp;amp;&amp;amp; contains(github.event.comment.body, '@claude')) ||
      # ... other conditions
    runs-on: ubuntu-latest
    permissions:
      contents: read
      pull-requests: read
      issues: read
      id-token: write
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Prepare Environment
        run: |
          curl -fsSL https://bun.sh/install | bash
          mkdir -p $HOME/.claude-code-router
          cat &amp;lt;&amp;lt; 'EOF' &amp;gt; $HOME/.claude-code-router/config.json
          {
            "log": true,
            "OPENAI_API_KEY": "${{ secrets.OPENAI_API_KEY }}",
            "OPENAI_BASE_URL": "https://api.deepseek.com",
            "OPENAI_MODEL": "deepseek-chat"
          }
          EOF
        shell: bash

      - name: Start Claude Code Router
        run: |
          nohup ~/.bun/bin/bunx @musistudio/claude-code-router@1.0.8 start &amp;amp;
        shell: bash

      - name: Run Claude Code
        id: claude
        uses: anthropics/claude-code-action@beta
        env:
          ANTHROPIC_BASE_URL: http://localhost:3456
        with:
          anthropic_api_key: "any-string-is-ok"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This setup allows for interesting automations, like running tasks during off-peak hours to reduce API costs.&lt;/p&gt; 
&lt;h2&gt;ğŸ“ Further Reading&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/musistudio/claude-code-router/main/blog/en/project-motivation-and-how-it-works.md"&gt;Project Motivation and How It Works&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/musistudio/claude-code-router/main/blog/en/maybe-we-can-do-more-with-the-route.md"&gt;Maybe We Can Do More with the Router&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;â¤ï¸ Support &amp;amp; Sponsoring&lt;/h2&gt; 
&lt;p&gt;If you find this project helpful, please consider sponsoring its development. Your support is greatly appreciated!&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://ko-fi.com/F1F31GN2GM"&gt;&lt;img src="https://ko-fi.com/img/githubbutton_sm.svg?sanitize=true" alt="ko-fi"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/musistudio/claude-code-router/main/blog/images/alipay.jpg" width="200" alt="Alipay"&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/musistudio/claude-code-router/main/blog/images/wechat.jpg" width="200" alt="WeChat Pay"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h3&gt;Our Sponsors&lt;/h3&gt; 
&lt;p&gt;A huge thank you to all our sponsors for their generous support!&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;@Simon Leischnig&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/duanshuaimin"&gt;@duanshuaimin&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/vrgitadmin"&gt;@vrgitadmin&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;@*o&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ceilwoo"&gt;@ceilwoo&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;@*è¯´&lt;/li&gt; 
 &lt;li&gt;@*æ›´&lt;/li&gt; 
 &lt;li&gt;@K*g&lt;/li&gt; 
 &lt;li&gt;@R*R&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/bobleer"&gt;@bobleer&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;@*è‹—&lt;/li&gt; 
 &lt;li&gt;@*åˆ’&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Clarence-pan"&gt;@Clarence-pan&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/carter003"&gt;@carter003&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;@S*r&lt;/li&gt; 
 &lt;li&gt;@*æ™–&lt;/li&gt; 
 &lt;li&gt;@*æ•&lt;/li&gt; 
 &lt;li&gt;@Z*z&lt;/li&gt; 
 &lt;li&gt;@*ç„¶&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/cluic"&gt;@cluic&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;@*è‹—&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/PromptExpert"&gt;@PromptExpert&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;@*åº”&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/yusnake"&gt;@yusnake&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;@*é£&lt;/li&gt; 
 &lt;li&gt;@è‘£*&lt;/li&gt; 
 &lt;li&gt;@*æ±€&lt;/li&gt; 
 &lt;li&gt;@*æ¶¯&lt;/li&gt; 
 &lt;li&gt;@*:-ï¼‰&lt;/li&gt; 
 &lt;li&gt;@**ç£Š&lt;/li&gt; 
 &lt;li&gt;@*ç¢&lt;/li&gt; 
 &lt;li&gt;@*æˆ&lt;/li&gt; 
 &lt;li&gt;@Z*o&lt;/li&gt; 
 &lt;li&gt;@*ç¨&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;(If your name is masked, please contact me via my homepage email to update it with your GitHub username.)&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>OpenPipe/ART</title>
      <link>https://github.com/OpenPipe/ART</link>
      <description>&lt;p&gt;Agent Reinforcement Trainer: train multi-step agents for real-world tasks using GRPO. Give your agents on-the-job training. Reinforcement learning for Qwen2.5, Qwen3, Llama, Kimi, and more!&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://art.openpipe.ai"&gt;
   &lt;picture&gt; 
    &lt;img alt="ART logo" src="https://github.com/openpipe/art/raw/main/assets/ART_logo.png" width="160px"&gt; 
   &lt;/picture&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;/p&gt;
 &lt;h1&gt;Agent Reinforcement Trainer&lt;/h1&gt; 
 &lt;p&gt;&lt;/p&gt; 
 &lt;p&gt; Train multi-step agents for real-world tasks using GRPO. &lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://github.com/openpipe/art/raw/main/CONTRIBUTING.md"&gt;&lt;img src="https://img.shields.io/badge/PRs-welcome-blue.svg?sanitize=true" alt="PRs-Welcome"&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/openpipe-art/"&gt;&lt;img src="https://img.shields.io/pypi/dm/openpipe-art?color=364fc7&amp;amp;logoColor=364fc7" alt="Downloads"&gt;&lt;/a&gt; &lt;a href="https://colab.research.google.com/github/openpipe/art/blob/main/examples/2048/2048.ipynb"&gt;&lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Train Agent"&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href="https://discord.gg/zbBHRUpwf4"&gt;&lt;img src="https://img.shields.io/badge/Join%20Discord-5865F2?style=plastic&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Join Discord"&gt;&lt;/a&gt; &lt;a href="https://art.openpipe.ai"&gt;&lt;img src="https://img.shields.io/badge/Documentation-orange?style=plastic&amp;amp;logo=gitbook&amp;amp;logoColor=white" alt="Documentation"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;ğŸ“ RULER: Zero-Shot Agent Rewards&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;RULER&lt;/strong&gt; (Relative Universal LLM-Elicited Rewards) eliminates the need for hand-crafted reward functions by using an LLM-as-judge to automatically score agent trajectories. Simply define your task in the system prompt, and RULER handles the restâ€”&lt;strong&gt;no labeled data, expert feedback, or reward engineering required&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;âœ¨ &lt;strong&gt;Key Benefits:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;2-3x faster development&lt;/strong&gt; - Skip reward function engineering entirely&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;General-purpose&lt;/strong&gt; - Works across any task without modification&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Strong performance&lt;/strong&gt; - Matches or exceeds hand-crafted rewards in 3/4 benchmarks&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Easy integration&lt;/strong&gt; - Drop-in replacement for manual reward functions&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;# Before: Hours of reward engineering
def complex_reward_function(trajectory):
    # 50+ lines of careful scoring logic...
    pass

# After: One line with RULER
judged_group = await ruler_score_group(group, "openai/o3")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href="https://art.openpipe.ai/fundamentals/ruler"&gt;ğŸ“– Learn more about RULER â†’&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;ART Overview&lt;/h2&gt; 
&lt;p&gt;ART is an open-source RL framework that improves agent reliability by allowing LLMs to &lt;strong&gt;learn from experience&lt;/strong&gt;. ART provides an ergonomic harness for integrating GRPO into any python application. For a quick hands-on introduction, run one of the notebooks below. When you're ready to learn more, check out the &lt;a href="https://art.openpipe.ai"&gt;docs&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;ğŸ“’ Notebooks&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Agent Task&lt;/th&gt; 
   &lt;th&gt;Example Notebook&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Comparative Performance&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ARTâ€¢E [RULER]&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/openpipe/art/blob/main/examples/art-e/art-e.ipynb"&gt;ğŸ‹ï¸ Train agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Qwen 2.5 7B learns to search emails using RULER&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/openpipe/art/raw/main/assets/benchmarks/email_agent/accuracy-training-progress.svg?sanitize=true" height="72"&gt; &lt;a href="https://raw.githubusercontent.com/OpenPipe/ART/main/examples/art-e/art_e/evaluate/display_benchmarks.ipynb"&gt;benchmarks&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;2048&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/openpipe/art/blob/main/examples/2048/2048.ipynb"&gt;ğŸ‹ï¸ Train agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Qwen 2.5 3B learns to play 2048&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/openpipe/art/raw/main/assets/benchmarks/2048/accuracy-training-progress.svg?sanitize=true" height="72"&gt; &lt;a href="https://raw.githubusercontent.com/OpenPipe/ART/main/examples/2048/benchmark_2048.ipynb"&gt;benchmarks&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Temporal Clue&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/openpipe/art/blob/main/examples/temporal_clue/temporal-clue.ipynb"&gt;ğŸ‹ï¸ Train agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Qwen 2.5 7B learns to solve Temporal Clue&lt;/td&gt; 
   &lt;td&gt;[Link coming soon]&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Tic Tac Toe&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/openpipe/art/blob/main/examples/tic_tac_toe/tic-tac-toe.ipynb"&gt;ğŸ‹ï¸ Train agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Qwen 2.5 3B learns to play Tic Tac Toe&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/openpipe/art/raw/main/assets/benchmarks/tic-tac-toe-local/accuracy-training-progress.svg?sanitize=true" height="72"&gt; &lt;a href="https://raw.githubusercontent.com/OpenPipe/ART/main/examples/tic_tac_toe/benchmark_tic_tac_toe.ipynb"&gt;benchmarks&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Codenames&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://colab.research.google.com/github/openpipe/art/blob/main/examples/codenames/Codenames_RL.ipynb"&gt;ğŸ‹ï¸ Train agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Qwen 2.5 3B learns to play Codenames&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://github.com/openpipe/art/raw/main/assets/benchmarks/codenames/win_rate_over_time.png" height="72"&gt; &lt;a href="https://raw.githubusercontent.com/OpenPipe/ART/main/examples/codenames/Codenames_RL.ipynb"&gt;benchmarks&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Why ART?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ART provides convenient wrappers for introducing RL training into &lt;strong&gt;existing applications&lt;/strong&gt;. We abstract the training server into a modular service that your code doesn't need to interface with.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Train from anywhere.&lt;/strong&gt; Run the ART client on your laptop and let the ART server kick off an ephemeral GPU-enabled environment, or run on a local GPU.&lt;/li&gt; 
 &lt;li&gt;Integrations with hosted platforms like W&amp;amp;B, Langfuse, and OpenPipe provide flexible observability and &lt;strong&gt;simplify debugging&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;ART is customizable with &lt;strong&gt;intelligent defaults&lt;/strong&gt;. You can configure training parameters and inference engine configurations to meet specific needs, or take advantage of the defaults, which have been optimized for training efficiency and stability.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;ART agents can be trained from any client machine that runs python. To add to an existing project, run this command:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install openpipe-art
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ğŸ¤– ARTâ€¢E Agent&lt;/h2&gt; 
&lt;p&gt;Curious about how to use ART for a real-world task? Check out the &lt;a href="https://openpipe.ai/blog/art-e-mail-agent"&gt;ARTâ€¢E Agent&lt;/a&gt; blog post, where we detail how we trained Qwen 2.5 14B to beat o3 at email retrieval!&lt;/p&gt; 
&lt;img src="https://github.com/openpipe/art/raw/main/assets/ART_E_graphs.png" width="700"&gt; 
&lt;h2&gt;ğŸ” Training Loop Overview&lt;/h2&gt; 
&lt;p&gt;ART's functionality is divided into a &lt;strong&gt;client&lt;/strong&gt; and a &lt;strong&gt;server&lt;/strong&gt;. The OpenAI-compatible client is responsible for interfacing between ART and your codebase. Using the client, you can pass messages and get completions from your LLM as it improves. The server runs independently on any machine with a GPU. It abstracts away the complexity of the inference and training portions of the RL loop while allowing for some custom configuration. An outline of the training loop is shown below:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Inference&lt;/strong&gt;&lt;/p&gt; 
  &lt;ol&gt; 
   &lt;li&gt;Your code uses the ART client to perform an agentic workflow (usually executing several rollouts in parallel to gather data faster).&lt;/li&gt; 
   &lt;li&gt;Completion requests are routed to the ART server, which runs the model's latest LoRA in vLLM.&lt;/li&gt; 
   &lt;li&gt;As the agent executes, each &lt;code&gt;system&lt;/code&gt;, &lt;code&gt;user&lt;/code&gt;, and &lt;code&gt;assistant&lt;/code&gt; message is stored in a Trajectory.&lt;/li&gt; 
   &lt;li&gt;When a rollout finishes, your code assigns a &lt;code&gt;reward&lt;/code&gt; to its Trajectory, indicating the performance of the LLM.&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Training&lt;/strong&gt;&lt;/p&gt; 
  &lt;ol&gt; 
   &lt;li&gt;When each rollout has finished, Trajectories are grouped and sent to the server. Inference is blocked while training executes.&lt;/li&gt; 
   &lt;li&gt;The server trains your model using GRPO, initializing from the latest checkpoint (or an empty LoRA on the first iteration).&lt;/li&gt; 
   &lt;li&gt;The server saves the newly trained LoRA to a local directory and loads it into vLLM.&lt;/li&gt; 
   &lt;li&gt;Inference is unblocked and the loop resumes at step 1.&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;This training loop runs until a specified number of inference and training iterations have completed.&lt;/p&gt; 
&lt;h2&gt;ğŸ§© Supported Models&lt;/h2&gt; 
&lt;p&gt;ART should work with most vLLM/HuggingFace-transformers compatible causal language models, or at least the ones supported by &lt;a href="https://docs.unsloth.ai/get-started/all-our-models"&gt;Unsloth&lt;/a&gt;. Gemma 3 does not appear to be supported for the time being. If any other model isn't working for you, please let us know on &lt;a href="https://discord.gg/zbBHRUpwf4"&gt;Discord&lt;/a&gt; or open an issue on &lt;a href="https://github.com/openpipe/art/issues"&gt;GitHub&lt;/a&gt;!&lt;/p&gt; 
&lt;h2&gt;ğŸ¤ Contributing&lt;/h2&gt; 
&lt;p&gt;ART is in active development, and contributions are most welcome! Please see the &lt;a href="https://raw.githubusercontent.com/OpenPipe/ART/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; file for more information.&lt;/p&gt; 
&lt;h2&gt;ğŸ“– Citation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{hilton2025art,
  author = {Brad Hilton and Kyle Corbitt and David Corbitt and Saumya Gandhi and Angky William and Bohdan Kovalenskyi and Andie Jones},
  title = {ART: Agent Reinforcement Trainer},
  year = {2025},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/openpipe/art}}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;âš–ï¸ License&lt;/h2&gt; 
&lt;p&gt;This repository's source code is available under the &lt;a href="https://raw.githubusercontent.com/OpenPipe/ART/main/LICENSE"&gt;Apache-2.0 License&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;ğŸ™ Credits&lt;/h2&gt; 
&lt;p&gt;ART stands on the shoulders of giants. While we owe many of the ideas and early experiments that led to ART's development to the open source RL community at large, we're especially grateful to the authors of the following projects:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/unslothai/unsloth"&gt;Unsloth&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/vllm-project/vllm"&gt;vLLM&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/huggingface/trl"&gt;trl&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pytorch/torchtune"&gt;torchtune&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/skypilot-org/skypilot"&gt;SkyPilot&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Finally, thank you to our partners who've helped us test ART in the wild! We're excited to see what you all build with it.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Lightricks/LTX-Video</title>
      <link>https://github.com/Lightricks/LTX-Video</link>
      <description>&lt;p&gt;Official repository for LTX-Video&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h1&gt;LTX-Video&lt;/h1&gt; 
 &lt;p&gt;&lt;a href="https://www.lightricks.com/ltxv"&gt;&lt;img src="https://img.shields.io/badge/Website-LTXV-181717?logo=google-chrome" alt="Website"&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/Lightricks/LTX-Video"&gt;&lt;img src="https://img.shields.io/badge/HuggingFace-Model-orange?logo=huggingface" alt="Model"&gt;&lt;/a&gt; &lt;a href="https://app.ltx.studio/motion-workspace?videoModel=ltxv-13b"&gt;&lt;img src="https://img.shields.io/badge/Demo-Try%20Now-brightgreen?logo=vercel" alt="Demo"&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2501.00103"&gt;&lt;img src="https://img.shields.io/badge/Paper-arXiv-B31B1B?logo=arxiv" alt="Paper"&gt;&lt;/a&gt; &lt;a href="https://github.com/Lightricks/LTX-Video-Trainer"&gt;&lt;img src="https://img.shields.io/badge/LTXV-Trainer-9146FF?logo=github" alt="Trainer"&gt;&lt;/a&gt; &lt;a href="https://discord.gg/Mn8BRgUKKy"&gt;&lt;img src="https://img.shields.io/badge/Join-Discord-5865F2?logo=discord" alt="Discord"&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;This is the official repository for LTX-Video.&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#introduction"&gt;Introduction&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#news"&gt;What's new&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#models"&gt;Models&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#quick-start-guide"&gt;Quick Start Guide&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#online-inference"&gt;Online demo&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#run-locally"&gt;Run locally&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#inference"&gt;Inference&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#comfyui-integration"&gt;ComfyUI Integration&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#diffusers-integration"&gt;Diffusers Integration&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#model-user-guide"&gt;Model User Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#community-contribution"&gt;Community Contribution&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#training"&gt;Training&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#control-models"&gt;Control Models&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#join-us-"&gt;Join Us!&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#acknowledgement"&gt;Acknowledgement&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Introduction&lt;/h1&gt; 
&lt;p&gt;LTX-Video is the first DiT-based video generation model that can generate high-quality videos in &lt;em&gt;real-time&lt;/em&gt;. It can generate 30 FPS videos at 1216Ã—704 resolution, faster than it takes to watch them. The model is trained on a large-scale dataset of diverse videos and can generate high-resolution videos with realistic and diverse content.&lt;/p&gt; 
&lt;p&gt;The model supports image-to-video, keyframe-based animation, video extension (both forward and backward), video-to-video transformations, and any combination of these features.&lt;/p&gt; 
&lt;h3&gt;Image-to-video examples&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00001.gif" alt="example1"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00002.gif" alt="example2"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00003.gif" alt="example3"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00004.gif" alt="example4"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00005.gif" alt="example5"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00006.gif" alt="example6"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00007.gif" alt="example7"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00008.gif" alt="example8"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_i2v_example_00009.gif" alt="example9"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Controlled video examples&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_ic_2v_example_00000.gif" alt="control0"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_ic_2v_example_00001.gif" alt="control1"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_ic_2v_example_00002.gif" alt="control2"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;&lt;/th&gt; 
   &lt;th align="center"&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_ic_2v_example_00003.gif" alt="control3"&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;img src="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_ic_2v_example_00004.gif" alt="control4"&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;News&lt;/h1&gt; 
&lt;h2&gt;July, 16th, 2025: New Distilled models v0.9.8 with up to 60 seconds of video:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Long shot generation in LTXV-13B! 
  &lt;ul&gt; 
   &lt;li&gt;LTX-Video now supports up to 60 seconds of video.&lt;/li&gt; 
   &lt;li&gt;Compatible also with the official IC-LoRAs.&lt;/li&gt; 
   &lt;li&gt;Try now in &lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo/tree/master/example_workflows/ltxv-13b-i2v-long-multi-prompt.json"&gt;ComfyUI&lt;/a&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Release a new distilled models: 
  &lt;ul&gt; 
   &lt;li&gt;13B distilled model &lt;a href="https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-13b-0.9.8-distilled.yaml"&gt;ltxv-13b-0.9.8-distilled&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;2B distilled model &lt;a href="https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-2b-0.9.8-distilled.yaml"&gt;ltxv-2b-0.9.8-distilled&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Both models are distilled from the same base model &lt;a href="https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-13b-0.9.8-dev.yaml"&gt;ltxv-13b-0.9.8-dev&lt;/a&gt; and are compatible for use together in the same multiscale pipeline.&lt;/li&gt; 
   &lt;li&gt;Improved prompt understanding and detail generation&lt;/li&gt; 
   &lt;li&gt;Includes corresponding FP8 weights and workflows.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Release a new detailer model &lt;a href="https://huggingface.co/Lightricks/LTX-Video-ICLoRA-detailer-13b-0.9.8"&gt;LTX-Video-ICLoRA-detailer-13B-0.9.8&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Available in &lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo/tree/master/example_workflows/ltxv-13b-upscale.json"&gt;ComfyUI&lt;/a&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;July, 8th, 2025: New Control Models Released!&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Released three new control models for LTX-Video on HuggingFace: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Depth Control&lt;/strong&gt;: &lt;a href="https://huggingface.co/Lightricks/LTX-Video-ICLoRA-depth-13b-0.9.7"&gt;LTX-Video-ICLoRA-depth-13b-0.9.7&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Pose Control&lt;/strong&gt;: &lt;a href="https://huggingface.co/Lightricks/LTX-Video-ICLoRA-pose-13b-0.9.7"&gt;LTX-Video-ICLoRA-pose-13b-0.9.7&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Canny Control&lt;/strong&gt;: &lt;a href="https://huggingface.co/Lightricks/LTX-Video-ICLoRA-canny-13b-0.9.7"&gt;LTX-Video-ICLoRA-canny-13b-0.9.7&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;May, 14th, 2025: New distilled model 13B v0.9.7:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Release a new 13B distilled model &lt;a href="https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-distilled.safetensors"&gt;ltxv-13b-0.9.7-distilled&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Amazing for iterative work - generates HD videos in 10 seconds, with low-res preview after just 3 seconds (on H100)!&lt;/li&gt; 
   &lt;li&gt;Does not require classifier-free guidance and spatio-temporal guidance.&lt;/li&gt; 
   &lt;li&gt;Supports sampling with 8 (recommended), or less diffusion steps.&lt;/li&gt; 
   &lt;li&gt;Also released a LoRA version of the distilled model, &lt;a href="https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-distilled-lora128.safetensors"&gt;ltxv-13b-0.9.7-distilled-lora128&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Requires only 1GB of VRAM&lt;/li&gt; 
     &lt;li&gt;Can be used with the full 13B model for fast inference&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Release a new quantized distilled model &lt;a href="https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-distilled-fp8.safetensors"&gt;ltxv-13b-0.9.7-distilled-fp8&lt;/a&gt; for &lt;em&gt;real-time&lt;/em&gt; generation (on H100) with even less VRAM&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;May, 5th, 2025: New model 13B v0.9.7:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Release a new 13B model &lt;a href="https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-dev.safetensors"&gt;ltxv-13b-0.9.7-dev&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Release a new quantized model &lt;a href="https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-dev-fp8.safetensors"&gt;ltxv-13b-0.9.7-dev-fp8&lt;/a&gt; for faster inference with less VRam&lt;/li&gt; 
 &lt;li&gt;Release a new upscalers 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-temporal-upscaler-0.9.7.safetensors"&gt;ltxv-temporal-upscaler-0.9.7&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-spatial-upscaler-0.9.7.safetensors"&gt;ltxv-spatial-upscaler-0.9.7&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Breakthrough prompt adherence and physical understanding.&lt;/li&gt; 
 &lt;li&gt;New Pipeline for multi-scale video rendering for fast and high quality results&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;April, 15th, 2025: New checkpoints v0.9.6:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Release a new checkpoint &lt;a href="https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-2b-0.9.6-dev-04-25.safetensors"&gt;ltxv-2b-0.9.6-dev-04-25&lt;/a&gt; with improved quality&lt;/li&gt; 
 &lt;li&gt;Release a new distilled model &lt;a href="https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-2b-0.9.6-distilled-04-25.safetensors"&gt;ltxv-2b-0.9.6-distilled-04-25&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;15x faster inference than non-distilled model.&lt;/li&gt; 
   &lt;li&gt;Does not require classifier-free guidance and spatio-temporal guidance.&lt;/li&gt; 
   &lt;li&gt;Supports sampling with 8 (recommended), or less diffusion steps.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Improved prompt adherence, motion quality and fine details.&lt;/li&gt; 
 &lt;li&gt;New default resolution and FPS: 1216 Ã— 704 pixels at 30 FPS 
  &lt;ul&gt; 
   &lt;li&gt;Still real time on H100 with the distilled model.&lt;/li&gt; 
   &lt;li&gt;Other resolutions and FPS are still supported.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Support stochastic inference (can improve visual quality when using the distilled model)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;March, 5th, 2025: New checkpoint v0.9.5&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;New license for commercial use (&lt;a href="https://huggingface.co/Lightricks/LTX-Video/ltx-video-2b-v0.9.5.license.txt"&gt;OpenRail-M&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;Release a new checkpoint v0.9.5 with improved quality&lt;/li&gt; 
 &lt;li&gt;Support keyframes and video extension&lt;/li&gt; 
 &lt;li&gt;Support higher resolutions&lt;/li&gt; 
 &lt;li&gt;Improved prompt understanding&lt;/li&gt; 
 &lt;li&gt;Improved VAE&lt;/li&gt; 
 &lt;li&gt;New online web app in &lt;a href="https://app.ltx.studio/ltx-video"&gt;LTX-Studio&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Automatic prompt enhancement&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;February, 20th, 2025: More inference options&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Improve STG (Spatiotemporal Guidance) for LTX-Video&lt;/li&gt; 
 &lt;li&gt;Support MPS on macOS with PyTorch 2.3.0&lt;/li&gt; 
 &lt;li&gt;Add support for 8-bit model, LTX-VideoQ8&lt;/li&gt; 
 &lt;li&gt;Add TeaCache for LTX-Video&lt;/li&gt; 
 &lt;li&gt;Add &lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#comfyui-integration"&gt;ComfyUI-LTXTricks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Add Diffusion-Pipe&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;December 31st, 2024: Research paper&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Release the &lt;a href="https://arxiv.org/abs/2501.00103"&gt;research paper&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;December 20th, 2024: New checkpoint v0.9.1&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Release a new checkpoint v0.9.1 with improved quality&lt;/li&gt; 
 &lt;li&gt;Support for STG / PAG&lt;/li&gt; 
 &lt;li&gt;Support loading checkpoints of LTX-Video in Diffusers format (conversion is done on-the-fly)&lt;/li&gt; 
 &lt;li&gt;Support offloading unused parts to CPU&lt;/li&gt; 
 &lt;li&gt;Support the new timestep-conditioned VAE decoder&lt;/li&gt; 
 &lt;li&gt;Reference contributions from the community in the readme file&lt;/li&gt; 
 &lt;li&gt;Relax transformers dependency&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;November 21th, 2024: Initial release v0.9.0&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Initial release of LTX-Video&lt;/li&gt; 
 &lt;li&gt;Support text-to-video and image-to-video generation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Models &amp;amp; Workflows&lt;/h1&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Name&lt;/th&gt; 
   &lt;th&gt;Notes&lt;/th&gt; 
   &lt;th&gt;inference.py config&lt;/th&gt; 
   &lt;th&gt;ComfyUI workflow (Recommended)&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ltxv-13b-0.9.8-dev&lt;/td&gt; 
   &lt;td&gt;Highest quality, requires more VRAM&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-13b-0.9.8-dev.yaml"&gt;ltxv-13b-0.9.8-dev.yaml&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/ltxv-13b-i2v-base.json"&gt;ltxv-13b-i2v-base.json&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://app.ltx.studio/motion-workspace?videoModel=ltxv-13b"&gt;ltxv-13b-0.9.8-mix&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Mix ltxv-13b-dev and ltxv-13b-distilled in the same multi-scale rendering workflow for balanced speed-quality&lt;/td&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/ltxv-13b-i2v-mixed-multiscale.json"&gt;ltxv-13b-i2v-mixed-multiscale.json&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://app.ltx.studio/motion-workspace?videoModel=ltxv"&gt;ltxv-13b-0.9.8-distilled&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Faster, less VRAM usage, slight quality reduction compared to 13b. Ideal for rapid iterations&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-13b-0.9.8-distilled.yaml"&gt;ltxv-13b-0.9.8-distilled.yaml&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/13b-distilled/ltxv-13b-dist-i2v-base.json"&gt;ltxv-13b-dist-i2v-base.json&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ltxv-2b-0.9.8-distilled&lt;/td&gt; 
   &lt;td&gt;Smaller model, slight quality reduction compared to 13b distilled. Ideal for fast generation with light VRAM usage&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-2b-0.9.8-distilled.yaml"&gt;ltxv-2b-0.9.8-distilled.yaml&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ltxv-13b-0.9.8-dev-fp8&lt;/td&gt; 
   &lt;td&gt;Quantized version of ltxv-13b&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-13b-0.9.8-dev-fp8.yaml"&gt;ltxv-13b-0.9.8-dev-fp8.yaml&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/ltxv-13b-i2v-base-fp8.json"&gt;ltxv-13b-i2v-base-fp8.json&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ltxv-13b-0.9.8-distilled-fp8&lt;/td&gt; 
   &lt;td&gt;Quantized version of ltxv-13b-distilled&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-13b-0.9.8-distilled-fp8.yaml"&gt;ltxv-13b-0.9.8-distilled-fp8.yaml&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/13b-distilled/ltxv-13b-dist-i2v-base-fp8.json"&gt;ltxv-13b-dist-i2v-base-fp8.json&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ltxv-2b-0.9.8-distilled-fp8&lt;/td&gt; 
   &lt;td&gt;Quantized version of ltxv-2b-distilled&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-2b-0.9.8-distilled-fp8.yaml"&gt;ltxv-2b-0.9.8-distilled-fp8.yaml&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ltxv-2b-0.9.6&lt;/td&gt; 
   &lt;td&gt;Good quality, lower VRAM requirement than ltxv-13b&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-2b-0.9.6-dev.yaml"&gt;ltxv-2b-0.9.6-dev.yaml&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/low_level/ltxvideo-i2v.json"&gt;ltxvideo-i2v.json&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ltxv-2b-0.9.6-distilled&lt;/td&gt; 
   &lt;td&gt;15Ã— faster, real-time capable, fewer steps needed, no STG/CFG required&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-2b-0.9.6-distilled.yaml"&gt;ltxv-2b-0.9.6-distilled.yaml&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/low_level/ltxvideo-i2v-distilled.json"&gt;ltxvideo-i2v-distilled.json&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;Quick Start Guide&lt;/h1&gt; 
&lt;h2&gt;Online inference&lt;/h2&gt; 
&lt;p&gt;The model is accessible right away via the following links:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://app.ltx.studio/motion-workspace?videoModel=ltxv-13b"&gt;LTX-Studio image-to-video (13B-mix)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://app.ltx.studio/motion-workspace?videoModel=ltxv"&gt;LTX-Studio image-to-video (13B distilled)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://fal.ai/models/fal-ai/ltx-video-13b-dev/image-to-video"&gt;Fal.ai image-to-video (13B full)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://fal.ai/models/fal-ai/ltx-video-13b-distilled/image-to-video"&gt;Fal.ai image-to-video (13B distilled)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://replicate.com/lightricks/ltx-video"&gt;Replicate image-to-video&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Run locally&lt;/h2&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;p&gt;The codebase was tested with Python 3.10.5, CUDA version 12.2, and supports PyTorch &amp;gt;= 2.1.2. On macOS, MPS was tested with PyTorch 2.3.0, and should support PyTorch == 2.3 or &amp;gt;= 2.6.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/Lightricks/LTX-Video.git
cd LTX-Video

# create env
python -m venv env
source env/bin/activate
python -m pip install -e .\[inference\]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;FP8 Kernels (optional)&lt;/h4&gt; 
&lt;p&gt;&lt;a href="https://github.com/Lightricks/LTXVideo-Q8-Kernels"&gt;FP8 kernels&lt;/a&gt; developed for LTX-Video provide performance boost on supported graphics cards (Ada architecture and later). To install FP8 kernels, follow the instructions in that repository.&lt;/p&gt; 
&lt;h3&gt;Inference&lt;/h3&gt; 
&lt;p&gt;ğŸ“ &lt;strong&gt;Note:&lt;/strong&gt; For best results, we recommend using our &lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#comfyui-integration"&gt;ComfyUI&lt;/a&gt; workflow. We're working on updating the inference.py script to match the high quality and output fidelity of ComfyUI.&lt;/p&gt; 
&lt;p&gt;To use our model, please follow the inference code in &lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/inference.py"&gt;inference.py&lt;/a&gt;:&lt;/p&gt; 
&lt;h4&gt;For image-to-video generation:&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python inference.py --prompt "PROMPT" --conditioning_media_paths IMAGE_PATH --conditioning_start_frames 0 --height HEIGHT --width WIDTH --num_frames NUM_FRAMES --seed SEED --pipeline_config configs/ltxv-13b-0.9.8-distilled.yaml
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Extending a video:&lt;/h4&gt; 
&lt;p&gt;ğŸ“ &lt;strong&gt;Note:&lt;/strong&gt; Input video segments must contain a multiple of 8 frames plus 1 (e.g., 9, 17, 25, etc.), and the target frame number should be a multiple of 8.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python inference.py --prompt "PROMPT" --conditioning_media_paths VIDEO_PATH --conditioning_start_frames START_FRAME --height HEIGHT --width WIDTH --num_frames NUM_FRAMES --seed SEED --pipeline_config configs/ltxv-13b-0.9.8-distilled.yaml
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;For video generation with multiple conditions:&lt;/h4&gt; 
&lt;p&gt;You can now generate a video conditioned on a set of images and/or short video segments. Simply provide a list of paths to the images or video segments you want to condition on, along with their target frame numbers in the generated video. You can also specify the conditioning strength for each item (default: 1.0).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python inference.py --prompt "PROMPT" --conditioning_media_paths IMAGE_OR_VIDEO_PATH_1 IMAGE_OR_VIDEO_PATH_2 --conditioning_start_frames TARGET_FRAME_1 TARGET_FRAME_2 --height HEIGHT --width WIDTH --num_frames NUM_FRAMES --seed SEED --pipeline_config configs/ltxv-13b-0.9.8-distilled.yaml
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Using as a library&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from ltx_video.inference import infer, InferenceConfig

infer(
    InferenceConfig(
        pipeline_config="configs/ltxv-13b-0.9.8-distilled.yaml",
        prompt=PROMPT,
        height=HEIGHT,
        width=WIDTH,
        num_frames=NUM_FRAMES,
        output_path="output.mp4",
    )
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;ComfyUI Integration&lt;/h2&gt; 
&lt;p&gt;To use our model with ComfyUI, please follow the instructions at &lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo/"&gt;https://github.com/Lightricks/ComfyUI-LTXVideo/&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Diffusers Integration&lt;/h2&gt; 
&lt;p&gt;To use our model with the Diffusers Python library, check out the &lt;a href="https://huggingface.co/docs/diffusers/main/en/api/pipelines/ltx_video"&gt;official documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Diffusers also support an 8-bit version of LTX-Video, &lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#ltx-videoq8"&gt;see details below&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Model User Guide&lt;/h1&gt; 
&lt;h2&gt;ğŸ“ Prompt Engineering&lt;/h2&gt; 
&lt;p&gt;When writing prompts, focus on detailed, chronological descriptions of actions and scenes. Include specific movements, appearances, camera angles, and environmental details - all in a single flowing paragraph. Start directly with the action, and keep descriptions literal and precise. Think like a cinematographer describing a shot list. Keep within 200 words. For best results, build your prompts using this structure:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Start with main action in a single sentence&lt;/li&gt; 
 &lt;li&gt;Add specific details about movements and gestures&lt;/li&gt; 
 &lt;li&gt;Describe character/object appearances precisely&lt;/li&gt; 
 &lt;li&gt;Include background and environment details&lt;/li&gt; 
 &lt;li&gt;Specify camera angles and movements&lt;/li&gt; 
 &lt;li&gt;Describe lighting and colors&lt;/li&gt; 
 &lt;li&gt;Note any changes or sudden events&lt;/li&gt; 
 &lt;li&gt;See &lt;a href="https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#introduction"&gt;examples&lt;/a&gt; for more inspiration.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Automatic Prompt Enhancement&lt;/h3&gt; 
&lt;p&gt;When using &lt;code&gt;LTXVideoPipeline&lt;/code&gt; directly, you can enable prompt enhancement by setting &lt;code&gt;enhance_prompt=True&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;ğŸ® Parameter Guide&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Resolution Preset: Higher resolutions for detailed scenes, lower for faster generation and simpler scenes. The model works on resolutions that are divisible by 32 and number of frames that are divisible by 8 + 1 (e.g. 257). In case the resolution or number of frames are not divisible by 32 or 8 + 1, the input will be padded with -1 and then cropped to the desired resolution and number of frames. The model works best on resolutions under 720 x 1280 and number of frames below 257&lt;/li&gt; 
 &lt;li&gt;Seed: Save seed values to recreate specific styles or compositions you like&lt;/li&gt; 
 &lt;li&gt;Guidance Scale: 3-3.5 are the recommended values&lt;/li&gt; 
 &lt;li&gt;Inference Steps: More steps (40+) for quality, fewer steps (20-30) for speed&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;ğŸ“ For advanced parameters usage, please see &lt;code&gt;python inference.py --help&lt;/code&gt;&lt;/p&gt; 
&lt;h2&gt;Community Contribution&lt;/h2&gt; 
&lt;h3&gt;ComfyUI-LTXTricks ğŸ› ï¸&lt;/h3&gt; 
&lt;p&gt;A community project providing additional nodes for enhanced control over the LTX Video model. It includes implementations of advanced techniques like RF-Inversion, RF-Edit, FlowEdit, and more. These nodes enable workflows such as Image and Video to Video (I+V2V), enhanced sampling via Spatiotemporal Skip Guidance (STG), and interpolation with precise frame settings.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Repository:&lt;/strong&gt; &lt;a href="https://github.com/logtd/ComfyUI-LTXTricks"&gt;ComfyUI-LTXTricks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Features:&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;ğŸ”„ &lt;strong&gt;RF-Inversion:&lt;/strong&gt; Implements &lt;a href="https://rf-inversion.github.io/"&gt;RF-Inversion&lt;/a&gt; with an &lt;a href="https://github.com/logtd/ComfyUI-LTXTricks/raw/main/example_workflows/example_ltx_inversion.json"&gt;example workflow here&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;âœ‚ï¸ &lt;strong&gt;RF-Edit:&lt;/strong&gt; Implements &lt;a href="https://github.com/wangjiangshan0725/RF-Solver-Edit"&gt;RF-Solver-Edit&lt;/a&gt; with an &lt;a href="https://github.com/logtd/ComfyUI-LTXTricks/raw/main/example_workflows/example_ltx_rf_edit.json"&gt;example workflow here&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;ğŸŒŠ &lt;strong&gt;FlowEdit:&lt;/strong&gt; Implements &lt;a href="https://github.com/fallenshock/FlowEdit"&gt;FlowEdit&lt;/a&gt; with an &lt;a href="https://github.com/logtd/ComfyUI-LTXTricks/raw/main/example_workflows/example_ltx_flow_edit.json"&gt;example workflow here&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;ğŸ¥ &lt;strong&gt;I+V2V:&lt;/strong&gt; Enables Video to Video with a reference image. &lt;a href="https://github.com/logtd/ComfyUI-LTXTricks/raw/main/example_workflows/example_ltx_iv2v.json"&gt;Example workflow&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;âœ¨ &lt;strong&gt;Enhance:&lt;/strong&gt; Partial implementation of &lt;a href="https://junhahyung.github.io/STGuidance/"&gt;STGuidance&lt;/a&gt;. &lt;a href="https://github.com/logtd/ComfyUI-LTXTricks/raw/main/example_workflows/example_ltxv_stg.json"&gt;Example workflow&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;ğŸ–¼ï¸ &lt;strong&gt;Interpolation and Frame Setting:&lt;/strong&gt; Nodes for precise control of latents per frame. &lt;a href="https://github.com/logtd/ComfyUI-LTXTricks/raw/main/example_workflows/example_ltx_interpolation.json"&gt;Example workflow&lt;/a&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;LTX-VideoQ8 ğŸ± &lt;a id="ltx-videoq8"&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;LTX-VideoQ8&lt;/strong&gt; is an 8-bit optimized version of &lt;a href="https://github.com/Lightricks/LTX-Video"&gt;LTX-Video&lt;/a&gt;, designed for faster performance on NVIDIA ADA GPUs.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Repository:&lt;/strong&gt; &lt;a href="https://github.com/KONAKONA666/LTX-Video"&gt;LTX-VideoQ8&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Features:&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;ğŸš€ Up to 3X speed-up with no accuracy loss&lt;/li&gt; 
   &lt;li&gt;ğŸ¥ Generate 720x480x121 videos in under a minute on RTX 4060 (8GB VRAM)&lt;/li&gt; 
   &lt;li&gt;ğŸ› ï¸ Fine-tune 2B transformer models with precalculated latents&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Community Discussion:&lt;/strong&gt; &lt;a href="https://www.reddit.com/r/StableDiffusion/comments/1h79ks2/fast_ltx_video_on_rtx_4060_and_other_ada_gpus/"&gt;Reddit Thread&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Diffusers integration:&lt;/strong&gt; A diffusers integration for the 8-bit model is already out! &lt;a href="https://github.com/sayakpaul/q8-ltx-video"&gt;Details here&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;TeaCache for LTX-Video ğŸµ &lt;a id="TeaCache"&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;TeaCache&lt;/strong&gt; is a training-free caching approach that leverages timestep differences across model outputs to accelerate LTX-Video inference by up to 2x without significant visual quality degradation.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Repository:&lt;/strong&gt; &lt;a href="https://github.com/ali-vilab/TeaCache/tree/main/TeaCache4LTX-Video"&gt;TeaCache4LTX-Video&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Features:&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;ğŸš€ Speeds up LTX-Video inference.&lt;/li&gt; 
   &lt;li&gt;ğŸ“Š Adjustable trade-offs between speed (up to 2x) and visual quality using configurable parameters.&lt;/li&gt; 
   &lt;li&gt;ğŸ› ï¸ No retraining required: Works directly with existing models.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Your Contribution&lt;/h3&gt; 
&lt;p&gt;...is welcome! If you have a project or tool that integrates with LTX-Video, please let us know by opening an issue or pull request.&lt;/p&gt; 
&lt;h1&gt;âš¡ï¸ Training&lt;/h1&gt; 
&lt;p&gt;We provide an open-source repository for fine-tuning the LTX-Video model: &lt;a href="https://github.com/Lightricks/LTX-Video-Trainer"&gt;LTX-Video-Trainer&lt;/a&gt;. This repository supports both the 2B and 13B model variants, enabling full fine-tuning as well as LoRA (Low-Rank Adaptation) fine-tuning for more efficient training. This includes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Control LoRAs&lt;/strong&gt;: Train custom control models like depth, pose, and canny control&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Effect LoRAs&lt;/strong&gt;: Create specialized effects and transformations for video generation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Explore the repository to customize the model for your specific use cases! More information and training instructions can be found in the &lt;a href="https://github.com/Lightricks/LTX-Video-Trainer/raw/main/README.md"&gt;README&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;ğŸ¬ Control Models&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo"&gt;ComfyUI-LTXVideo&lt;/a&gt; repository now contains workflows and models for 3 specialized models that enable precise control over LTX-Video generation:&lt;/p&gt; 
&lt;p&gt;Pose Control, Depth Control and Canny Control&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Example ComfyUI Workflow (for all control types):&lt;/strong&gt; &lt;a href="https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/ic_lora/ic-lora.json"&gt;ic-lora.json&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;ğŸš€ Join Us&lt;/h1&gt; 
&lt;p&gt;Want to work on cutting-edge AI research and make a real impact on millions of users worldwide?&lt;/p&gt; 
&lt;p&gt;At &lt;strong&gt;Lightricks&lt;/strong&gt;, an AI-first company, we're revolutionizing how visual content is created.&lt;/p&gt; 
&lt;p&gt;If you are passionate about AI, computer vision, and video generation, we would love to hear from you!&lt;/p&gt; 
&lt;p&gt;Please visit our &lt;a href="https://careers.lightricks.com/careers?query=&amp;amp;office=all&amp;amp;department=R%26D"&gt;careers page&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h1&gt;Acknowledgement&lt;/h1&gt; 
&lt;p&gt;We are grateful for the following awesome projects when implementing LTX-Video:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/facebookresearch/DiT"&gt;DiT&lt;/a&gt; and &lt;a href="https://github.com/PixArt-alpha/PixArt-alpha"&gt;PixArt-alpha&lt;/a&gt;: vision transformers for image generation.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;ğŸ“„ Our tech report is out! If you find our work helpful, please â­ï¸ star the repository and cite our paper.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@article{HaCohen2024LTXVideo,
  title={LTX-Video: Realtime Video Latent Diffusion},
  author={HaCohen, Yoav and Chiprut, Nisan and Brazowski, Benny and Shalem, Daniel and Moshe, Dudu and Richardson, Eitan and Levin, Eran and Shiran, Guy and Zabari, Nir and Gordon, Ori and Panet, Poriya and Weissbuch, Sapir and Kulikov, Victor and Bitterman, Yaki and Melumian, Zeev and Bibi, Ofir},
  journal={arXiv preprint arXiv:2501.00103},
  year={2024}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>maybe-finance/maybe</title>
      <link>https://github.com/maybe-finance/maybe</link>
      <description>&lt;p&gt;The personal finance app for everyone&lt;/p&gt;&lt;hr&gt;&lt;img width="1190" alt="maybe_hero" src="https://github.com/user-attachments/assets/13fc5ef4-ce0f-4073-a163-9dbc3eb4c8e5"&gt; 
&lt;h1&gt;Maybe: The personal finance app for everyone&lt;/h1&gt; 
&lt;p&gt;&lt;b&gt;Get involved: &lt;a href="https://link.maybe.co/discord"&gt;Discord&lt;/a&gt; â€¢ &lt;a href="https://maybefinance.com"&gt;Website&lt;/a&gt; â€¢ &lt;a href="https://github.com/maybe-finance/maybe/issues"&gt;Issues&lt;/a&gt;&lt;/b&gt;&lt;/p&gt; 
&lt;h2&gt;Backstory&lt;/h2&gt; 
&lt;p&gt;We spent the better part of 2021/2022 building a personal finance + wealth management app called, Maybe. Very full-featured, including an "Ask an Advisor" feature which connected users with an actual CFP/CFA to help them with their finances (all included in your subscription).&lt;/p&gt; 
&lt;p&gt;The business end of things didn't work out, and so we shut things down mid-2023.&lt;/p&gt; 
&lt;p&gt;We spent the better part of $1,000,000 building the app (employees + contractors, data providers/services, infrastructure, etc.).&lt;/p&gt; 
&lt;p&gt;We're now reviving the product as a fully open-source project. The goal is to let you run the app yourself, for free, and use it to manage your own finances and eventually offer a hosted version of the app for a small monthly fee.&lt;/p&gt; 
&lt;h2&gt;Maybe Hosting&lt;/h2&gt; 
&lt;p&gt;There are 2 primary ways to use the Maybe app:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Managed (easiest) - we're in alpha and release invites in our Discord&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/maybe-finance/maybe/main/docs/hosting/docker.md"&gt;Self-host with Docker&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Before contributing, you'll likely find it helpful to &lt;a href="https://github.com/maybe-finance/maybe/wiki"&gt;understand context and general vision/direction&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Once you've done that, please visit our &lt;a href="https://github.com/maybe-finance/maybe/raw/main/CONTRIBUTING.md"&gt;contributing guide&lt;/a&gt; to get started!&lt;/p&gt; 
&lt;h3&gt;Performance Issues&lt;/h3&gt; 
&lt;p&gt;With data-heavy apps, inevitably, there are performance issues. We've set up a public dashboard showing the problematic requests, along with the stacktraces to help debug them.&lt;/p&gt; 
&lt;p&gt;Any contributions that help improve performance are very much welcome.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://oss.skylight.io/app/applications/XDpPIXEX52oi/recent/6h/endpoints"&gt;https://oss.skylight.io/app/applications/XDpPIXEX52oi/recent/6h/endpoints&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Local Development Setup&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;If you are trying to &lt;em&gt;self-host&lt;/em&gt; the Maybe app, stop here. You should &lt;a href="https://raw.githubusercontent.com/maybe-finance/maybe/main/docs/hosting/docker.md"&gt;read this guide to get started&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;The instructions below are for developers to get started with contributing to the app.&lt;/p&gt; 
&lt;h3&gt;Requirements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;See &lt;code&gt;.ruby-version&lt;/code&gt; file for required Ruby version&lt;/li&gt; 
 &lt;li&gt;PostgreSQL &amp;gt;9.3 (ideally, latest stable version)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;After cloning the repo, the basic setup commands are:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;cd maybe
cp .env.local.example .env.local
bin/setup
bin/dev

# Optionally, load demo data
rake demo_data:default
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;And visit &lt;a href="http://localhost:3000"&gt;http://localhost:3000&lt;/a&gt; to see the app. You can use the following credentials to log in (generated by DB seed):&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Email: &lt;code&gt;user@maybe.local&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Password: &lt;code&gt;password&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For further instructions, see guides below.&lt;/p&gt; 
&lt;h3&gt;Multi-currency support&lt;/h3&gt; 
&lt;p&gt;If you'd like multi-currency support, there are a few extra steps to follow.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Sign up for an API key at &lt;a href="https://synthfinance.com"&gt;Synth&lt;/a&gt;. It's a Maybe product and the free plan is sufficient for basic multi-currency support.&lt;/li&gt; 
 &lt;li&gt;Add your API key to your &lt;code&gt;.env&lt;/code&gt; file.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Setup Guides&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/maybe-finance/maybe/wiki/Mac-Dev-Setup-Guide"&gt;Mac dev setup guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/maybe-finance/maybe/wiki/Linux-Dev-Setup-Guide"&gt;Linux dev setup guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/maybe-finance/maybe/wiki/Windows-Dev-Setup-Guide"&gt;Windows dev setup guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Dev containers - visit &lt;a href="https://code.visualstudio.com/docs/devcontainers/containers"&gt;this guide&lt;/a&gt; to learn more&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Repo Activity&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://repobeats.axiom.co/api/embed/7866c9790deba0baf63ca1688b209130b306ea4e.svg?sanitize=true" alt="Repo Activity" title="Repobeats analytics image"&gt;&lt;/p&gt; 
&lt;h2&gt;Copyright &amp;amp; license&lt;/h2&gt; 
&lt;p&gt;Maybe is distributed under an &lt;a href="https://github.com/maybe-finance/maybe/raw/main/LICENSE"&gt;AGPLv3 license&lt;/a&gt;. " Maybe" is a trademark of Maybe Finance, Inc.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>facebookresearch/segment-anything</title>
      <link>https://github.com/facebookresearch/segment-anything</link>
      <description>&lt;p&gt;The repository provides code for running inference with the SegmentAnything Model (SAM), links for downloading the trained model checkpoints, and example notebooks that show how to use the model.&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;Latest updates -- SAM 2: Segment Anything in Images and Videos&lt;/h2&gt; 
&lt;p&gt;Please check out our new release on &lt;a href="https://github.com/facebookresearch/segment-anything-2"&gt;&lt;strong&gt;Segment Anything Model 2 (SAM 2)&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;SAM 2 code: &lt;a href="https://github.com/facebookresearch/segment-anything-2"&gt;https://github.com/facebookresearch/segment-anything-2&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;SAM 2 demo: &lt;a href="https://sam2.metademolab.com/"&gt;https://sam2.metademolab.com/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;SAM 2 paper: &lt;a href="https://arxiv.org/abs/2408.00714"&gt;https://arxiv.org/abs/2408.00714&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://github.com/facebookresearch/segment-anything-2/raw/main/assets/model_diagram.png?raw=true" alt="SAM 2 architecture"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Segment Anything Model 2 (SAM 2)&lt;/strong&gt; is a foundation model towards solving promptable visual segmentation in images and videos. We extend SAM to video by considering images as a video with a single frame. The model design is a simple transformer architecture with streaming memory for real-time video processing. We build a model-in-the-loop data engine, which improves model and data via user interaction, to collect &lt;a href="https://ai.meta.com/datasets/segment-anything-video"&gt;&lt;strong&gt;our SA-V dataset&lt;/strong&gt;&lt;/a&gt;, the largest video segmentation dataset to date. SAM 2 trained on our data provides strong performance across a wide range of tasks and visual domains.&lt;/p&gt; 
&lt;h1&gt;Segment Anything&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://ai.facebook.com/research/"&gt;Meta AI Research, FAIR&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://alexander-kirillov.github.io/"&gt;Alexander Kirillov&lt;/a&gt;, &lt;a href="https://ericmintun.github.io/"&gt;Eric Mintun&lt;/a&gt;, &lt;a href="https://nikhilaravi.com/"&gt;Nikhila Ravi&lt;/a&gt;, &lt;a href="https://hanzimao.me/"&gt;Hanzi Mao&lt;/a&gt;, Chloe Rolland, Laura Gustafson, &lt;a href="https://tetexiao.com"&gt;Tete Xiao&lt;/a&gt;, &lt;a href="https://www.spencerwhitehead.com/"&gt;Spencer Whitehead&lt;/a&gt;, Alex Berg, Wan-Yen Lo, &lt;a href="https://pdollar.github.io/"&gt;Piotr Dollar&lt;/a&gt;, &lt;a href="https://www.rossgirshick.info/"&gt;Ross Girshick&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[&lt;a href="https://ai.facebook.com/research/publications/segment-anything/"&gt;&lt;code&gt;Paper&lt;/code&gt;&lt;/a&gt;] [&lt;a href="https://segment-anything.com/"&gt;&lt;code&gt;Project&lt;/code&gt;&lt;/a&gt;] [&lt;a href="https://segment-anything.com/demo"&gt;&lt;code&gt;Demo&lt;/code&gt;&lt;/a&gt;] [&lt;a href="https://segment-anything.com/dataset/index.html"&gt;&lt;code&gt;Dataset&lt;/code&gt;&lt;/a&gt;] [&lt;a href="https://ai.facebook.com/blog/segment-anything-foundation-model-image-segmentation/"&gt;&lt;code&gt;Blog&lt;/code&gt;&lt;/a&gt;] [&lt;a href="https://raw.githubusercontent.com/facebookresearch/segment-anything/main/#citing-segment-anything"&gt;&lt;code&gt;BibTeX&lt;/code&gt;&lt;/a&gt;]&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/facebookresearch/segment-anything/main/assets/model_diagram.png?raw=true" alt="SAM design"&gt;&lt;/p&gt; 
&lt;p&gt;The &lt;strong&gt;Segment Anything Model (SAM)&lt;/strong&gt; produces high quality object masks from input prompts such as points or boxes, and it can be used to generate masks for all objects in an image. It has been trained on a &lt;a href="https://segment-anything.com/dataset/index.html"&gt;dataset&lt;/a&gt; of 11 million images and 1.1 billion masks, and has strong zero-shot performance on a variety of segmentation tasks.&lt;/p&gt; 
&lt;p float="left"&gt; &lt;img src="https://raw.githubusercontent.com/facebookresearch/segment-anything/main/assets/masks1.png?raw=true" width="37.25%"&gt; &lt;img src="https://raw.githubusercontent.com/facebookresearch/segment-anything/main/assets/masks2.jpg?raw=true" width="61.5%"&gt; &lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;The code requires &lt;code&gt;python&amp;gt;=3.8&lt;/code&gt;, as well as &lt;code&gt;pytorch&amp;gt;=1.7&lt;/code&gt; and &lt;code&gt;torchvision&amp;gt;=0.8&lt;/code&gt;. Please follow the instructions &lt;a href="https://pytorch.org/get-started/locally/"&gt;here&lt;/a&gt; to install both PyTorch and TorchVision dependencies. Installing both PyTorch and TorchVision with CUDA support is strongly recommended.&lt;/p&gt; 
&lt;p&gt;Install Segment Anything:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install git+https://github.com/facebookresearch/segment-anything.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or clone the repository locally and install with&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;git clone git@github.com:facebookresearch/segment-anything.git
cd segment-anything; pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The following optional dependencies are necessary for mask post-processing, saving masks in COCO format, the example notebooks, and exporting the model in ONNX format. &lt;code&gt;jupyter&lt;/code&gt; is also required to run the example notebooks.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install opencv-python pycocotools matplotlib onnxruntime onnx
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;&lt;a name="GettingStarted"&gt;&lt;/a&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;First download a &lt;a href="https://raw.githubusercontent.com/facebookresearch/segment-anything/main/#model-checkpoints"&gt;model checkpoint&lt;/a&gt;. Then the model can be used in just a few lines to get masks from a given prompt:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;from segment_anything import SamPredictor, sam_model_registry
sam = sam_model_registry["&amp;lt;model_type&amp;gt;"](checkpoint="&amp;lt;path/to/checkpoint&amp;gt;")
predictor = SamPredictor(sam)
predictor.set_image(&amp;lt;your_image&amp;gt;)
masks, _, _ = predictor.predict(&amp;lt;input_prompts&amp;gt;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or generate masks for an entire image:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;from segment_anything import SamAutomaticMaskGenerator, sam_model_registry
sam = sam_model_registry["&amp;lt;model_type&amp;gt;"](checkpoint="&amp;lt;path/to/checkpoint&amp;gt;")
mask_generator = SamAutomaticMaskGenerator(sam)
masks = mask_generator.generate(&amp;lt;your_image&amp;gt;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Additionally, masks can be generated for images from the command line:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;python scripts/amg.py --checkpoint &amp;lt;path/to/checkpoint&amp;gt; --model-type &amp;lt;model_type&amp;gt; --input &amp;lt;image_or_folder&amp;gt; --output &amp;lt;path/to/output&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See the examples notebooks on &lt;a href="https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/predictor_example.ipynb"&gt;using SAM with prompts&lt;/a&gt; and &lt;a href="https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/automatic_mask_generator_example.ipynb"&gt;automatically generating masks&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;p float="left"&gt; &lt;img src="https://raw.githubusercontent.com/facebookresearch/segment-anything/main/assets/notebook1.png?raw=true" width="49.1%"&gt; &lt;img src="https://raw.githubusercontent.com/facebookresearch/segment-anything/main/assets/notebook2.png?raw=true" width="48.9%"&gt; &lt;/p&gt; 
&lt;h2&gt;ONNX Export&lt;/h2&gt; 
&lt;p&gt;SAM's lightweight mask decoder can be exported to ONNX format so that it can be run in any environment that supports ONNX runtime, such as in-browser as showcased in the &lt;a href="https://segment-anything.com/demo"&gt;demo&lt;/a&gt;. Export the model with&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;python scripts/export_onnx_model.py --checkpoint &amp;lt;path/to/checkpoint&amp;gt; --model-type &amp;lt;model_type&amp;gt; --output &amp;lt;path/to/output&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See the &lt;a href="https://github.com/facebookresearch/segment-anything/raw/main/notebooks/onnx_model_example.ipynb"&gt;example notebook&lt;/a&gt; for details on how to combine image preprocessing via SAM's backbone with mask prediction using the ONNX model. It is recommended to use the latest stable version of PyTorch for ONNX export.&lt;/p&gt; 
&lt;h3&gt;Web demo&lt;/h3&gt; 
&lt;p&gt;The &lt;code&gt;demo/&lt;/code&gt; folder has a simple one page React app which shows how to run mask prediction with the exported ONNX model in a web browser with multithreading. Please see &lt;a href="https://github.com/facebookresearch/segment-anything/raw/main/demo/README.md"&gt;&lt;code&gt;demo/README.md&lt;/code&gt;&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;&lt;a name="Models"&gt;&lt;/a&gt;Model Checkpoints&lt;/h2&gt; 
&lt;p&gt;Three model versions of the model are available with different backbone sizes. These models can be instantiated by running&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;from segment_anything import sam_model_registry
sam = sam_model_registry["&amp;lt;model_type&amp;gt;"](checkpoint="&amp;lt;path/to/checkpoint&amp;gt;")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Click the links below to download the checkpoint for the corresponding model type.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;default&lt;/code&gt; or &lt;code&gt;vit_h&lt;/code&gt;: &lt;a href="https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"&gt;ViT-H SAM model.&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;vit_l&lt;/code&gt;: &lt;a href="https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth"&gt;ViT-L SAM model.&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;vit_b&lt;/code&gt;: &lt;a href="https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth"&gt;ViT-B SAM model.&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Dataset&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://ai.facebook.com/datasets/segment-anything/"&gt;here&lt;/a&gt; for an overview of the datastet. The dataset can be downloaded &lt;a href="https://ai.facebook.com/datasets/segment-anything-downloads/"&gt;here&lt;/a&gt;. By downloading the datasets you agree that you have read and accepted the terms of the SA-1B Dataset Research License.&lt;/p&gt; 
&lt;p&gt;We save masks per image as a json file. It can be loaded as a dictionary in python in the below format.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;{
    "image"                 : image_info,
    "annotations"           : [annotation],
}

image_info {
    "image_id"              : int,              # Image id
    "width"                 : int,              # Image width
    "height"                : int,              # Image height
    "file_name"             : str,              # Image filename
}

annotation {
    "id"                    : int,              # Annotation id
    "segmentation"          : dict,             # Mask saved in COCO RLE format.
    "bbox"                  : [x, y, w, h],     # The box around the mask, in XYWH format
    "area"                  : int,              # The area in pixels of the mask
    "predicted_iou"         : float,            # The model's own prediction of the mask's quality
    "stability_score"       : float,            # A measure of the mask's quality
    "crop_box"              : [x, y, w, h],     # The crop of the image used to generate the mask, in XYWH format
    "point_coords"          : [[x, y]],         # The point coordinates input to the model to generate the mask
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Image ids can be found in sa_images_ids.txt which can be downloaded using the above &lt;a href="https://ai.facebook.com/datasets/segment-anything-downloads/"&gt;link&lt;/a&gt; as well.&lt;/p&gt; 
&lt;p&gt;To decode a mask in COCO RLE format into binary:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;from pycocotools import mask as mask_utils
mask = mask_utils.decode(annotation["segmentation"])
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;a href="https://github.com/cocodataset/cocoapi/raw/master/PythonAPI/pycocotools/mask.py"&gt;here&lt;/a&gt; for more instructions to manipulate masks stored in RLE format.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;The model is licensed under the &lt;a href="https://raw.githubusercontent.com/facebookresearch/segment-anything/main/LICENSE"&gt;Apache 2.0 license&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/facebookresearch/segment-anything/main/CONTRIBUTING.md"&gt;contributing&lt;/a&gt; and the &lt;a href="https://raw.githubusercontent.com/facebookresearch/segment-anything/main/CODE_OF_CONDUCT.md"&gt;code of conduct&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributors&lt;/h2&gt; 
&lt;p&gt;The Segment Anything project was made possible with the help of many contributors (alphabetical):&lt;/p&gt; 
&lt;p&gt;Aaron Adcock, Vaibhav Aggarwal, Morteza Behrooz, Cheng-Yang Fu, Ashley Gabriel, Ahuva Goldstand, Allen Goodman, Sumanth Gurram, Jiabo Hu, Somya Jain, Devansh Kukreja, Robert Kuo, Joshua Lane, Yanghao Li, Lilian Luong, Jitendra Malik, Mallika Malhotra, William Ngan, Omkar Parkhi, Nikhil Raina, Dirk Rowe, Neil Sejoor, Vanessa Stark, Bala Varadarajan, Bram Wasti, Zachary Winstrom&lt;/p&gt; 
&lt;h2&gt;Citing Segment Anything&lt;/h2&gt; 
&lt;p&gt;If you use SAM or SA-1B in your research, please use the following BibTeX entry.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@article{kirillov2023segany,
  title={Segment Anything},
  author={Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Doll{\'a}r, Piotr and Girshick, Ross},
  journal={arXiv:2304.02643},
  year={2023}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>getzep/graphiti</title>
      <link>https://github.com/getzep/graphiti</link>
      <description>&lt;p&gt;Build Real-Time Knowledge Graphs for AI Agents&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="https://www.getzep.com/"&gt; &lt;img src="https://github.com/user-attachments/assets/119c5682-9654-4257-8922-56b7cb8ffd73" width="150" alt="Zep Logo"&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h1 align="center"&gt; Graphiti &lt;/h1&gt; 
&lt;h2 align="center"&gt; Build Real-Time Knowledge Graphs for AI Agents&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/getzep/Graphiti/actions/workflows/lint.yml"&gt;&lt;img src="https://github.com/getzep/Graphiti/actions/workflows/lint.yml/badge.svg?style=flat" alt="Lint"&gt;&lt;/a&gt; &lt;a href="https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml"&gt;&lt;img src="https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml/badge.svg?sanitize=true" alt="Unit Tests"&gt;&lt;/a&gt; &lt;a href="https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml"&gt;&lt;img src="https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml/badge.svg?sanitize=true" alt="MyPy Check"&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://img.shields.io/github/stars/getzep/graphiti" alt="GitHub Repo stars"&gt; &lt;a href="https://discord.com/invite/W8Kw6bsgXQ"&gt;&lt;img src="https://img.shields.io/badge/Discord-%235865F2.svg?&amp;amp;logo=discord&amp;amp;logoColor=white" alt="Discord"&gt;&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2501.13956"&gt;&lt;img src="https://img.shields.io/badge/arXiv-2501.13956-b31b1b.svg?style=flat" alt="arXiv"&gt;&lt;/a&gt; &lt;a href="https://github.com/getzep/graphiti/releases"&gt;&lt;img src="https://img.shields.io/github/v/release/getzep/graphiti?style=flat&amp;amp;label=Release&amp;amp;color=limegreen" alt="Release"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;a href="https://trendshift.io/repositories/12986" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/12986" alt="getzep%2Fgraphiti | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;span&gt;â­&lt;/span&gt; &lt;em&gt;Help us reach more developers and grow the Graphiti community. Star this repo!&lt;/em&gt;&lt;/p&gt; 
&lt;br&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] Check out the new &lt;a href="https://raw.githubusercontent.com/getzep/graphiti/main/mcp_server/README.md"&gt;MCP server for Graphiti&lt;/a&gt;! Give Claude, Cursor, and other MCP clients powerful Knowledge Graph-based memory.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Graphiti is a framework for building and querying temporally-aware knowledge graphs, specifically tailored for AI agents operating in dynamic environments. Unlike traditional retrieval-augmented generation (RAG) methods, Graphiti continuously integrates user interactions, structured and unstructured enterprise data, and external information into a coherent, queryable graph. The framework supports incremental data updates, efficient retrieval, and precise historical queries without requiring complete graph recomputation, making it suitable for developing interactive, context-aware AI applications.&lt;/p&gt; 
&lt;p&gt;Use Graphiti to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Integrate and maintain dynamic user interactions and business data.&lt;/li&gt; 
 &lt;li&gt;Facilitate state-based reasoning and task automation for agents.&lt;/li&gt; 
 &lt;li&gt;Query complex, evolving data with semantic, keyword, and graph-based search methods.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;br&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/getzep/graphiti/main/images/graphiti-graph-intro.gif" alt="Graphiti temporal walkthrough" width="700px"&gt; &lt;/p&gt; 
&lt;br&gt; 
&lt;p&gt;A knowledge graph is a network of interconnected facts, such as &lt;em&gt;"Kendra loves Adidas shoes."&lt;/em&gt; Each fact is a "triplet" represented by two entities, or nodes ("Kendra", "Adidas shoes"), and their relationship, or edge ("loves"). Knowledge Graphs have been explored extensively for information retrieval. What makes Graphiti unique is its ability to autonomously build a knowledge graph while handling changing relationships and maintaining historical context.&lt;/p&gt; 
&lt;h2&gt;Graphiti and Zep Memory&lt;/h2&gt; 
&lt;p&gt;Graphiti powers the core of &lt;a href="https://www.getzep.com"&gt;Zep's memory layer&lt;/a&gt; for AI Agents.&lt;/p&gt; 
&lt;p&gt;Using Graphiti, we've demonstrated Zep is the &lt;a href="https://blog.getzep.com/state-of-the-art-agent-memory/"&gt;State of the Art in Agent Memory&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Read our paper: &lt;a href="https://arxiv.org/abs/2501.13956"&gt;Zep: A Temporal Knowledge Graph Architecture for Agent Memory&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;We're excited to open-source Graphiti, believing its potential reaches far beyond AI memory applications.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://arxiv.org/abs/2501.13956"&gt;&lt;img src="https://raw.githubusercontent.com/getzep/graphiti/main/images/arxiv-screenshot.png" alt="Zep: A Temporal Knowledge Graph Architecture for Agent Memory" width="700px"&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;Why Graphiti?&lt;/h2&gt; 
&lt;p&gt;Traditional RAG approaches often rely on batch processing and static data summarization, making them inefficient for frequently changing data. Graphiti addresses these challenges by providing:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Real-Time Incremental Updates:&lt;/strong&gt; Immediate integration of new data episodes without batch recomputation.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Bi-Temporal Data Model:&lt;/strong&gt; Explicit tracking of event occurrence and ingestion times, allowing accurate point-in-time queries.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Efficient Hybrid Retrieval:&lt;/strong&gt; Combines semantic embeddings, keyword (BM25), and graph traversal to achieve low-latency queries without reliance on LLM summarization.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Custom Entity Definitions:&lt;/strong&gt; Flexible ontology creation and support for developer-defined entities through straightforward Pydantic models.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Scalability:&lt;/strong&gt; Efficiently manages large datasets with parallel processing, suitable for enterprise environments.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/getzep/graphiti/main/images/graphiti-intro-slides-stock-2.gif" alt="Graphiti structured + unstructured demo" width="700px"&gt; &lt;/p&gt; 
&lt;h2&gt;Graphiti vs. GraphRAG&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Aspect&lt;/th&gt; 
   &lt;th&gt;GraphRAG&lt;/th&gt; 
   &lt;th&gt;Graphiti&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Primary Use&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Static document summarization&lt;/td&gt; 
   &lt;td&gt;Dynamic data management&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Data Handling&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Batch-oriented processing&lt;/td&gt; 
   &lt;td&gt;Continuous, incremental updates&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Knowledge Structure&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Entity clusters &amp;amp; community summaries&lt;/td&gt; 
   &lt;td&gt;Episodic data, semantic entities, communities&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Retrieval Method&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Sequential LLM summarization&lt;/td&gt; 
   &lt;td&gt;Hybrid semantic, keyword, and graph-based search&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Adaptability&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Low&lt;/td&gt; 
   &lt;td&gt;High&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Temporal Handling&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Basic timestamp tracking&lt;/td&gt; 
   &lt;td&gt;Explicit bi-temporal tracking&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Contradiction Handling&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;LLM-driven summarization judgments&lt;/td&gt; 
   &lt;td&gt;Temporal edge invalidation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Query Latency&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Seconds to tens of seconds&lt;/td&gt; 
   &lt;td&gt;Typically sub-second latency&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Custom Entity Types&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;No&lt;/td&gt; 
   &lt;td&gt;Yes, customizable&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Scalability&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Moderate&lt;/td&gt; 
   &lt;td&gt;High, optimized for large datasets&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Graphiti is specifically designed to address the challenges of dynamic and frequently updated datasets, making it particularly suitable for applications requiring real-time interaction and precise historical queries.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;Requirements:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.10 or higher&lt;/li&gt; 
 &lt;li&gt;Neo4j 5.26 / FalkorDB 1.1.2 or higher (serves as the embeddings storage backend)&lt;/li&gt; 
 &lt;li&gt;OpenAI API key (Graphiti defaults to OpenAI for LLM inference and embedding)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] Graphiti works best with LLM services that support Structured Output (such as OpenAI and Gemini). Using other services may result in incorrect output schemas and ingestion failures. This is particularly problematic when using smaller models.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Optional:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Google Gemini, Anthropic, or Groq API key (for alternative LLM providers)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] The simplest way to install Neo4j is via &lt;a href="https://neo4j.com/download/"&gt;Neo4j Desktop&lt;/a&gt;. It provides a user-friendly interface to manage Neo4j instances and databases. Alternatively, you can use FalkorDB on-premises via Docker and instantly start with the quickstart example:&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run -p 6379:6379 -p 3000:3000 -it --rm falkordb/falkordb:latest

&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install graphiti-core
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv add graphiti-core
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Installing with FalkorDB Support&lt;/h3&gt; 
&lt;p&gt;If you plan to use FalkorDB as your graph database backend, install with the FalkorDB extra:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install graphiti-core[falkordb]

# or with uv
uv add graphiti-core[falkordb]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;You can also install optional LLM providers as extras:&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install with Anthropic support
pip install graphiti-core[anthropic]

# Install with Groq support
pip install graphiti-core[groq]

# Install with Google Gemini support
pip install graphiti-core[google-genai]

# Install with multiple providers
pip install graphiti-core[anthropic,groq,google-genai]

# Install with FalkorDB and LLM providers
pip install graphiti-core[falkordb,anthropic,google-genai]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] Graphiti defaults to using OpenAI for LLM inference and embedding. Ensure that an &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; is set in your environment. Support for Anthropic and Groq LLM inferences is available, too. Other LLM providers may be supported via OpenAI compatible APIs.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;For a complete working example, see the &lt;a href="https://raw.githubusercontent.com/getzep/graphiti/main/examples/quickstart/README.md"&gt;Quickstart Example&lt;/a&gt; in the examples directory. The quickstart demonstrates:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Connecting to a Neo4j or FalkorDB database&lt;/li&gt; 
 &lt;li&gt;Initializing Graphiti indices and constraints&lt;/li&gt; 
 &lt;li&gt;Adding episodes to the graph (both text and structured JSON)&lt;/li&gt; 
 &lt;li&gt;Searching for relationships (edges) using hybrid search&lt;/li&gt; 
 &lt;li&gt;Reranking search results using graph distance&lt;/li&gt; 
 &lt;li&gt;Searching for nodes using predefined search recipes&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;The example is fully documented with clear explanations of each functionality and includes a comprehensive README with setup instructions and next steps.&lt;/p&gt; 
&lt;h2&gt;MCP Server&lt;/h2&gt; 
&lt;p&gt;The &lt;code&gt;mcp_server&lt;/code&gt; directory contains a Model Context Protocol (MCP) server implementation for Graphiti. This server allows AI assistants to interact with Graphiti's knowledge graph capabilities through the MCP protocol.&lt;/p&gt; 
&lt;p&gt;Key features of the MCP server include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Episode management (add, retrieve, delete)&lt;/li&gt; 
 &lt;li&gt;Entity management and relationship handling&lt;/li&gt; 
 &lt;li&gt;Semantic and hybrid search capabilities&lt;/li&gt; 
 &lt;li&gt;Group management for organizing related data&lt;/li&gt; 
 &lt;li&gt;Graph maintenance operations&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The MCP server can be deployed using Docker with Neo4j, making it easy to integrate Graphiti into your AI assistant workflows.&lt;/p&gt; 
&lt;p&gt;For detailed setup instructions and usage examples, see the &lt;a href="https://raw.githubusercontent.com/getzep/graphiti/main/mcp_server/README.md"&gt;MCP server README&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;REST Service&lt;/h2&gt; 
&lt;p&gt;The &lt;code&gt;server&lt;/code&gt; directory contains an API service for interacting with the Graphiti API. It is built using FastAPI.&lt;/p&gt; 
&lt;p&gt;Please see the &lt;a href="https://raw.githubusercontent.com/getzep/graphiti/main/server/README.md"&gt;server README&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h2&gt;Optional Environment Variables&lt;/h2&gt; 
&lt;p&gt;In addition to the Neo4j and OpenAi-compatible credentials, Graphiti also has a few optional environment variables. If you are using one of our supported models, such as Anthropic or Voyage models, the necessary environment variables must be set.&lt;/p&gt; 
&lt;h3&gt;Database Configuration&lt;/h3&gt; 
&lt;p&gt;Database names are configured directly in the driver constructors:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Neo4j&lt;/strong&gt;: Database name defaults to &lt;code&gt;neo4j&lt;/code&gt; (hardcoded in Neo4jDriver)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;FalkorDB&lt;/strong&gt;: Database name defaults to &lt;code&gt;default_db&lt;/code&gt; (hardcoded in FalkorDriver)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;As of v0.17.0, if you need to customize your database configuration, you can instantiate a database driver and pass it to the Graphiti constructor using the &lt;code&gt;graph_driver&lt;/code&gt; parameter.&lt;/p&gt; 
&lt;h4&gt;Neo4j with Custom Database Name&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from graphiti_core import Graphiti
from graphiti_core.driver.neo4j_driver import Neo4jDriver

# Create a Neo4j driver with custom database name
driver = Neo4jDriver(
    uri="bolt://localhost:7687",
    user="neo4j",
    password="password",
    database="my_custom_database"  # Custom database name
)

# Pass the driver to Graphiti
graphiti = Graphiti(graph_driver=driver)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;FalkorDB with Custom Database Name&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from graphiti_core import Graphiti
from graphiti_core.driver.falkordb_driver import FalkorDriver

# Create a FalkorDB driver with custom database name
driver = FalkorDriver(
    host="localhost",
    port=6379,
    username="falkor_user",  # Optional
    password="falkor_password",  # Optional
    database="my_custom_graph"  # Custom database name
)

# Pass the driver to Graphiti
graphiti = Graphiti(graph_driver=driver)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Performance Configuration&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;USE_PARALLEL_RUNTIME&lt;/code&gt; is an optional boolean variable that can be set to true if you wish to enable Neo4j's parallel runtime feature for several of our search queries. Note that this feature is not supported for Neo4j Community edition or for smaller AuraDB instances, as such this feature is off by default.&lt;/p&gt; 
&lt;h2&gt;Using Graphiti with Azure OpenAI&lt;/h2&gt; 
&lt;p&gt;Graphiti supports Azure OpenAI for both LLM inference and embeddings. Azure deployments often require different endpoints for LLM and embedding services, and separate deployments for default and small models.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from openai import AsyncAzureOpenAI
from graphiti_core import Graphiti
from graphiti_core.llm_client import LLMConfig, OpenAIClient
from graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig
from graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient

# Azure OpenAI configuration - use separate endpoints for different services
api_key = "&amp;lt;your-api-key&amp;gt;"
api_version = "&amp;lt;your-api-version&amp;gt;"
llm_endpoint = "&amp;lt;your-llm-endpoint&amp;gt;"  # e.g., "https://your-llm-resource.openai.azure.com/"
embedding_endpoint = "&amp;lt;your-embedding-endpoint&amp;gt;"  # e.g., "https://your-embedding-resource.openai.azure.com/"

# Create separate Azure OpenAI clients for different services
llm_client_azure = AsyncAzureOpenAI(
    api_key=api_key,
    api_version=api_version,
    azure_endpoint=llm_endpoint
)

embedding_client_azure = AsyncAzureOpenAI(
    api_key=api_key,
    api_version=api_version,
    azure_endpoint=embedding_endpoint
)

# Create LLM Config with your Azure deployment names
azure_llm_config = LLMConfig(
    small_model="gpt-4.1-nano",
    model="gpt-4.1-mini",
)

# Initialize Graphiti with Azure OpenAI clients
graphiti = Graphiti(
    "bolt://localhost:7687",
    "neo4j",
    "password",
    llm_client=OpenAIClient(
        llm_config=azure_llm_config,
        client=llm_client_azure
    ),
    embedder=OpenAIEmbedder(
        config=OpenAIEmbedderConfig(
            embedding_model="text-embedding-3-small-deployment"  # Your Azure embedding deployment name
        ),
        client=embedding_client_azure
    ),
    cross_encoder=OpenAIRerankerClient(
        llm_config=LLMConfig(
            model=azure_llm_config.small_model  # Use small model for reranking
        ),
        client=llm_client_azure
    )
)

# Now you can use Graphiti with Azure OpenAI
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Make sure to replace the placeholder values with your actual Azure OpenAI credentials and deployment names that match your Azure OpenAI service configuration.&lt;/p&gt; 
&lt;h2&gt;Using Graphiti with Google Gemini&lt;/h2&gt; 
&lt;p&gt;Graphiti supports Google's Gemini models for LLM inference, embeddings, and cross-encoding/reranking. To use Gemini, you'll need to configure the LLM client, embedder, and the cross-encoder with your Google API key.&lt;/p&gt; 
&lt;p&gt;Install Graphiti:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;uv add "graphiti-core[google-genai]"

# or

pip install "graphiti-core[google-genai]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from graphiti_core import Graphiti
from graphiti_core.llm_client.gemini_client import GeminiClient, LLMConfig
from graphiti_core.embedder.gemini import GeminiEmbedder, GeminiEmbedderConfig
from graphiti_core.cross_encoder.gemini_reranker_client import GeminiRerankerClient

# Google API key configuration
api_key = "&amp;lt;your-google-api-key&amp;gt;"

# Initialize Graphiti with Gemini clients
graphiti = Graphiti(
    "bolt://localhost:7687",
    "neo4j",
    "password",
    llm_client=GeminiClient(
        config=LLMConfig(
            api_key=api_key,
            model="gemini-2.0-flash"
        )
    ),
    embedder=GeminiEmbedder(
        config=GeminiEmbedderConfig(
            api_key=api_key,
            embedding_model="embedding-001"
        )
    ),
    cross_encoder=GeminiRerankerClient(
        config=LLMConfig(
            api_key=api_key,
            model="gemini-2.5-flash-lite-preview-06-17"
        )
    )
)

# Now you can use Graphiti with Google Gemini for all components
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The Gemini reranker uses the &lt;code&gt;gemini-2.5-flash-lite-preview-06-17&lt;/code&gt; model by default, which is optimized for cost-effective and low-latency classification tasks. It uses the same boolean classification approach as the OpenAI reranker, leveraging Gemini's log probabilities feature to rank passage relevance.&lt;/p&gt; 
&lt;h2&gt;Using Graphiti with Ollama (Local LLM)&lt;/h2&gt; 
&lt;p&gt;Graphiti supports Ollama for running local LLMs and embedding models via Ollama's OpenAI-compatible API. This is ideal for privacy-focused applications or when you want to avoid API costs.&lt;/p&gt; 
&lt;p&gt;Install the models: ollama pull deepseek-r1:7b # LLM ollama pull nomic-embed-text # embeddings&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from graphiti_core import Graphiti
from graphiti_core.llm_client.config import LLMConfig
from graphiti_core.llm_client.openai_client import OpenAIClient
from graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig
from graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient

# Configure Ollama LLM client
llm_config = LLMConfig(
    api_key="abc",  # Ollama doesn't require a real API key
    model="deepseek-r1:7b",
    small_model="deepseek-r1:7b",
    base_url="http://localhost:11434/v1", # Ollama provides this port
)

llm_client = OpenAIClient(config=llm_config)

# Initialize Graphiti with Ollama clients
graphiti = Graphiti(
    "bolt://localhost:7687",
    "neo4j",
    "password",
    llm_client=llm_client,
    embedder=OpenAIEmbedder(
        config=OpenAIEmbedderConfig(
            api_key="abc",
            embedding_model="nomic-embed-text",
            embedding_dim=768,
            base_url="http://localhost:11434/v1",
        )
    ),
    cross_encoder=OpenAIRerankerClient(client=llm_client, config=llm_config),
)

# Now you can use Graphiti with local Ollama models
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Ensure Ollama is running (&lt;code&gt;ollama serve&lt;/code&gt;) and that you have pulled the models you want to use.&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://help.getzep.com/graphiti"&gt;Guides and API documentation&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://help.getzep.com/graphiti/graphiti/quick-start"&gt;Quick Start&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://help.getzep.com/graphiti/graphiti/lang-graph-agent"&gt;Building an agent with LangChain's LangGraph and Graphiti&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Telemetry&lt;/h2&gt; 
&lt;p&gt;Graphiti collects anonymous usage statistics to help us understand how the framework is being used and improve it for everyone. We believe transparency is important, so here's exactly what we collect and why.&lt;/p&gt; 
&lt;h3&gt;What We Collect&lt;/h3&gt; 
&lt;p&gt;When you initialize a Graphiti instance, we collect:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Anonymous identifier&lt;/strong&gt;: A randomly generated UUID stored locally in &lt;code&gt;~/.cache/graphiti/telemetry_anon_id&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;System information&lt;/strong&gt;: Operating system, Python version, and system architecture&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Graphiti version&lt;/strong&gt;: The version you're using&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Configuration choices&lt;/strong&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;LLM provider type (OpenAI, Azure, Anthropic, etc.)&lt;/li&gt; 
   &lt;li&gt;Database backend (Neo4j, FalkorDB)&lt;/li&gt; 
   &lt;li&gt;Embedder provider (OpenAI, Azure, Voyage, etc.)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;What We Don't Collect&lt;/h3&gt; 
&lt;p&gt;We are committed to protecting your privacy. We &lt;strong&gt;never&lt;/strong&gt; collect:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Personal information or identifiers&lt;/li&gt; 
 &lt;li&gt;API keys or credentials&lt;/li&gt; 
 &lt;li&gt;Your actual data, queries, or graph content&lt;/li&gt; 
 &lt;li&gt;IP addresses or hostnames&lt;/li&gt; 
 &lt;li&gt;File paths or system-specific information&lt;/li&gt; 
 &lt;li&gt;Any content from your episodes, nodes, or edges&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Why We Collect This Data&lt;/h3&gt; 
&lt;p&gt;This information helps us:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Understand which configurations are most popular to prioritize support and testing&lt;/li&gt; 
 &lt;li&gt;Identify which LLM and database providers to focus development efforts on&lt;/li&gt; 
 &lt;li&gt;Track adoption patterns to guide our roadmap&lt;/li&gt; 
 &lt;li&gt;Ensure compatibility across different Python versions and operating systems&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;By sharing this anonymous information, you help us make Graphiti better for everyone in the community.&lt;/p&gt; 
&lt;h3&gt;View the Telemetry Code&lt;/h3&gt; 
&lt;p&gt;The Telemetry code &lt;a href="https://raw.githubusercontent.com/getzep/graphiti/main/graphiti_core/telemetry/telemetry.py"&gt;may be found here&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;How to Disable Telemetry&lt;/h3&gt; 
&lt;p&gt;Telemetry is &lt;strong&gt;opt-out&lt;/strong&gt; and can be disabled at any time. To disable telemetry collection:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Option 1: Environment Variable&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;export GRAPHITI_TELEMETRY_ENABLED=false
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Option 2: Set in your shell profile&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# For bash users (~/.bashrc or ~/.bash_profile)
echo 'export GRAPHITI_TELEMETRY_ENABLED=false' &amp;gt;&amp;gt; ~/.bashrc

# For zsh users (~/.zshrc)
echo 'export GRAPHITI_TELEMETRY_ENABLED=false' &amp;gt;&amp;gt; ~/.zshrc
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Option 3: Set for a specific Python session&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import os
os.environ['GRAPHITI_TELEMETRY_ENABLED'] = 'false'

# Then initialize Graphiti as usual
from graphiti_core import Graphiti
graphiti = Graphiti(...)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Telemetry is automatically disabled during test runs (when &lt;code&gt;pytest&lt;/code&gt; is detected).&lt;/p&gt; 
&lt;h3&gt;Technical Details&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Telemetry uses PostHog for anonymous analytics collection&lt;/li&gt; 
 &lt;li&gt;All telemetry operations are designed to fail silently - they will never interrupt your application or affect Graphiti functionality&lt;/li&gt; 
 &lt;li&gt;The anonymous ID is stored locally and is not tied to any personal information&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Status and Roadmap&lt;/h2&gt; 
&lt;p&gt;Graphiti is under active development. We aim to maintain API stability while working on:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; Supporting custom graph schemas: 
  &lt;ul&gt; 
   &lt;li&gt;Allow developers to provide their own defined node and edge classes when ingesting episodes&lt;/li&gt; 
   &lt;li&gt;Enable more flexible knowledge representation tailored to specific use cases&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; Enhancing retrieval capabilities with more robust and configurable options&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled&gt; Graphiti MCP Server&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled&gt; Expanding test coverage to ensure reliability and catch edge cases&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or answering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer to &lt;a href="https://raw.githubusercontent.com/getzep/graphiti/main/CONTRIBUTING.md"&gt;CONTRIBUTING&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Support&lt;/h2&gt; 
&lt;p&gt;Join the &lt;a href="https://discord.com/invite/W8Kw6bsgXQ"&gt;Zep Discord server&lt;/a&gt; and make your way to the &lt;strong&gt;#Graphiti&lt;/strong&gt; channel!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>sherlock-project/sherlock</title>
      <link>https://github.com/sherlock-project/sherlock</link>
      <description>&lt;p&gt;Hunt down social media accounts by username across social networks&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;br&gt; &lt;a href="https://sherlock-project.github.io/" target="_blank"&gt;&lt;img src="https://raw.githubusercontent.com/sherlock-project/sherlock/master/images/sherlock-logo.png"&gt;&lt;/a&gt; &lt;br&gt; &lt;span&gt;Hunt down social media accounts by username across &lt;a href="https://sherlockproject.xyz/sites"&gt;400+ social networks&lt;/a&gt;&lt;/span&gt; &lt;br&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://sherlockproject.xyz/installation"&gt;Installation&lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;â€¢&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;a href="https://sherlockproject.xyz/usage"&gt;Usage&lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;â€¢&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;a href="https://sherlockproject.xyz/contribute"&gt;Contributing&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;img width="70%" height="70%" src="https://raw.githubusercontent.com/sherlock-project/sherlock/master/images/demo.png"&gt;  &lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING]&lt;br&gt; Packages for ParrotOS and Ubuntu 24.04, maintained by a third party, appear to be &lt;strong&gt;broken&lt;/strong&gt;.&lt;br&gt; Users of these systems should defer to pipx/pip or Docker.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Method&lt;/th&gt; 
   &lt;th&gt;Notes&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;pipx install sherlock-project&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;pip&lt;/code&gt; may be used in place of &lt;code&gt;pipx&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;docker run -it --rm sherlock/sherlock&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;dnf install sherlock-project&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Community-maintained packages are available for Debian (&amp;gt;= 13), Ubuntu (&amp;gt;= 22.10), Homebrew, Kali, and BlackArch. These packages are not directly supported or maintained by the Sherlock Project.&lt;/p&gt; 
&lt;p&gt;See all alternative installation methods &lt;a href="https://sherlockproject.xyz/installation"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;General usage&lt;/h2&gt; 
&lt;p&gt;To search for only one user:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sherlock user123
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To search for more than one user:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sherlock user1 user2 user3
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Accounts found will be stored in an individual text file with the corresponding username (e.g &lt;code&gt;user123.txt&lt;/code&gt;).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;$ sherlock --help
usage: sherlock [-h] [--version] [--verbose] [--folderoutput FOLDEROUTPUT]
                [--output OUTPUT] [--tor] [--unique-tor] [--csv] [--xlsx]
                [--site SITE_NAME] [--proxy PROXY_URL] [--json JSON_FILE]
                [--timeout TIMEOUT] [--print-all] [--print-found] [--no-color]
                [--browse] [--local] [--nsfw]
                USERNAMES [USERNAMES ...]

Sherlock: Find Usernames Across Social Networks (Version 0.14.3)

positional arguments:
  USERNAMES             One or more usernames to check with social networks.
                        Check similar usernames using {?} (replace to '_', '-', '.').

optional arguments:
  -h, --help            show this help message and exit
  --version             Display version information and dependencies.
  --verbose, -v, -d, --debug
                        Display extra debugging information and metrics.
  --folderoutput FOLDEROUTPUT, -fo FOLDEROUTPUT
                        If using multiple usernames, the output of the results will be
                        saved to this folder.
  --output OUTPUT, -o OUTPUT
                        If using single username, the output of the result will be saved
                        to this file.
  --tor, -t             Make requests over Tor; increases runtime; requires Tor to be
                        installed and in system path.
  --unique-tor, -u      Make requests over Tor with new Tor circuit after each request;
                        increases runtime; requires Tor to be installed and in system
                        path.
  --csv                 Create Comma-Separated Values (CSV) File.
  --xlsx                Create the standard file for the modern Microsoft Excel
                        spreadsheet (xlsx).
  --site SITE_NAME      Limit analysis to just the listed sites. Add multiple options to
                        specify more than one site.
  --proxy PROXY_URL, -p PROXY_URL
                        Make requests over a proxy. e.g. socks5://127.0.0.1:1080
  --json JSON_FILE, -j JSON_FILE
                        Load data from a JSON file or an online, valid, JSON file.
  --timeout TIMEOUT     Time (in seconds) to wait for response to requests (Default: 60)
  --print-all           Output sites where the username was not found.
  --print-found         Output sites where the username was found.
  --no-color            Don't color terminal output
  --browse, -b          Browse to all results on default browser.
  --local, -l           Force the use of the local data.json file.
  --nsfw                Include checking of NSFW sites from default list.
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Apify Actor Usage &lt;a href="https://apify.com/netmilk/sherlock?fpr=sherlock"&gt;&lt;img src="https://apify.com/actor-badge?actor=netmilk/sherlock" alt="Sherlock Actor"&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://apify.com/netmilk/sherlock?fpr=sherlock"&gt;&lt;img src="https://apify.com/ext/run-on-apify.png" alt="Run Sherlock Actor on Apify" width="176" height="39"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;You can run Sherlock in the cloud without installation using the &lt;a href="https://apify.com/netmilk/sherlock?fpr=sherlock"&gt;Sherlock Actor&lt;/a&gt; on &lt;a href="https://apify.com?fpr=sherlock"&gt;Apify&lt;/a&gt; free of charge.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;$ echo '{"usernames":["user123"]}' | apify call -so netmilk/sherlock
[{
  "username": "user123",
  "links": [
    "https://www.1337x.to/user/user123/",
    ...
  ]
}]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Read more about the &lt;a href="https://raw.githubusercontent.com/sherlock-project/sherlock/.actor/README.md"&gt;Sherlock Actor&lt;/a&gt;, including how to use it programmaticaly via the Apify &lt;a href="https://apify.com/netmilk/sherlock/api?fpr=sherlock"&gt;API&lt;/a&gt;, &lt;a href="https://docs.apify.com/cli/?fpr=sherlock"&gt;CLI&lt;/a&gt; and &lt;a href="https://docs.apify.com/sdk?fpr=sherlock"&gt;JS/TS and Python SDKs&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Credits&lt;/h2&gt; 
&lt;p&gt;Thank you to everyone who has contributed to Sherlock! â¤ï¸&lt;/p&gt; 
&lt;a href="https://github.com/sherlock-project/sherlock/graphs/contributors"&gt; &lt;img src="https://contrib.rocks/image?&amp;amp;columns=25&amp;amp;max=10000&amp;amp;&amp;amp;repo=sherlock-project/sherlock" noZoom&gt; &lt;/a&gt; 
&lt;h2&gt;Star history&lt;/h2&gt; 
&lt;picture&gt; 
 &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=sherlock-project/sherlock&amp;amp;type=Date&amp;amp;theme=dark"&gt; 
 &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=sherlock-project/sherlock&amp;amp;type=Date"&gt; 
 &lt;img alt="Sherlock Project Star History Chart" src="https://api.star-history.com/svg?repos=sherlock-project/sherlock&amp;amp;type=Date"&gt; 
&lt;/picture&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;MIT Â© Sherlock Project&lt;br&gt; Original Creator - &lt;a href="https://github.com/sdushantha"&gt;Siddharth Dushantha&lt;/a&gt;&lt;/p&gt; 
&lt;!-- Reference Links --&gt;</description>
    </item>
    
    <item>
      <title>hyprwm/Hyprland</title>
      <link>https://github.com/hyprwm/Hyprland</link>
      <description>&lt;p&gt;Hyprland is an independent, highly customizable, dynamic tiling Wayland compositor that doesn't sacrifice on its looks.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/hyprwm/Hyprland/main/assets/header.svg?sanitize=true" width="750" height="300" alt="banner"&gt; 
 &lt;br&gt; 
 &lt;p&gt;&lt;a href="https://github.com/hyprwm/Hyprland/actions/workflows/ci.yaml"&gt;&lt;img src="https://github.com/hyprwm/Hyprland/actions/workflows/ci.yaml/badge.svg?sanitize=true" alt="Badge Workflow"&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/hyprwm/Hyprland/main/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/hyprwm/Hyprland" alt="Badge License"&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/github/languages/top/hyprwm/Hyprland" alt="Badge Language"&gt; &lt;a href="https://github.com/hyprwm/Hyprland/pulls"&gt;&lt;img src="https://img.shields.io/github/issues-pr/hyprwm/Hyprland" alt="Badge Pull Requests"&gt;&lt;/a&gt; &lt;a href="https://github.com/hyprwm/Hyprland/issues"&gt;&lt;img src="https://img.shields.io/github/issues/hyprwm/Hyprland" alt="Badge Issues"&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/badge/Hi-mom!-ff69b4" alt="Badge Hi Mom"&gt;&lt;br&gt;&lt;/p&gt; 
 &lt;br&gt; 
 &lt;p&gt;Hyprland is a 100% independent, dynamic tiling Wayland compositor that doesn't sacrifice on its looks.&lt;/p&gt; 
 &lt;p&gt;It provides the latest Wayland features, is highly customizable, has all the eyecandy, the most powerful plugins, easy IPC, much more QoL stuff than other compositors and more... &lt;br&gt; &lt;br&gt;&lt;/p&gt; 
 &lt;hr&gt; 
 &lt;p&gt;&lt;strong&gt;&lt;a href="https://wiki.hypr.land/Getting-Started/Installation/"&gt;&lt;kbd&gt;â€ƒ&lt;br&gt;â€ƒInstallâ€ƒ&lt;br&gt;â€ƒ&lt;/kbd&gt;&lt;/a&gt;&lt;/strong&gt;â€ƒ &lt;strong&gt;&lt;a href="https://wiki.hypr.land/Getting-Started/Master-Tutorial/"&gt;&lt;kbd&gt;â€ƒ&lt;br&gt;â€ƒQuick Startâ€ƒ&lt;br&gt;â€ƒ&lt;/kbd&gt;&lt;/a&gt;&lt;/strong&gt;â€ƒ &lt;strong&gt;&lt;a href="https://wiki.hypr.land/Configuring/"&gt;&lt;kbd&gt;â€ƒ&lt;br&gt;â€ƒConfigureâ€ƒ&lt;br&gt;â€ƒ&lt;/kbd&gt;&lt;/a&gt;&lt;/strong&gt;â€ƒ &lt;strong&gt;&lt;a href="https://wiki.hypr.land/Contributing-and-Debugging/"&gt;&lt;kbd&gt;â€ƒ&lt;br&gt;â€ƒContributeâ€ƒ&lt;br&gt;â€ƒ&lt;/kbd&gt;&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
 &lt;hr&gt; 
 &lt;br&gt; 
&lt;/div&gt; 
&lt;h1&gt;Features&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;All of the eyecandy: gradient borders, blur, animations, shadows and much more&lt;/li&gt; 
 &lt;li&gt;A lot of customization&lt;/li&gt; 
 &lt;li&gt;100% independent, no wlroots, no libweston, no kwin, no mutter.&lt;/li&gt; 
 &lt;li&gt;Custom bezier curves for the best animations&lt;/li&gt; 
 &lt;li&gt;Powerful plugin support&lt;/li&gt; 
 &lt;li&gt;Built-in plugin manager&lt;/li&gt; 
 &lt;li&gt;Tearing support for better gaming performance&lt;/li&gt; 
 &lt;li&gt;Easily expandable and readable codebase&lt;/li&gt; 
 &lt;li&gt;Fast and active development&lt;/li&gt; 
 &lt;li&gt;Not afraid to provide bleeding-edge features&lt;/li&gt; 
 &lt;li&gt;Config reloaded instantly upon saving&lt;/li&gt; 
 &lt;li&gt;Fully dynamic workspaces&lt;/li&gt; 
 &lt;li&gt;Two built-in layouts and more available as plugins&lt;/li&gt; 
 &lt;li&gt;Global keybinds passed to your apps of choice&lt;/li&gt; 
 &lt;li&gt;Tiling/pseudotiling/floating/fullscreen windows&lt;/li&gt; 
 &lt;li&gt;Special workspaces (scratchpads)&lt;/li&gt; 
 &lt;li&gt;Window groups (tabbed mode)&lt;/li&gt; 
 &lt;li&gt;Powerful window/monitor/layer rules&lt;/li&gt; 
 &lt;li&gt;Socket-based IPC&lt;/li&gt; 
 &lt;li&gt;Native IME and Input Panels Support&lt;/li&gt; 
 &lt;li&gt;and much more...&lt;/li&gt; 
&lt;/ul&gt; 
&lt;br&gt; 
&lt;br&gt; 
&lt;div align="center"&gt; 
 &lt;h1&gt;Gallery&lt;/h1&gt; 
 &lt;br&gt; 
 &lt;p&gt;&lt;img src="https://i.ibb.co/XxFY75Mk/greerggergerhtrytghjnyhjn.png" alt="Preview A"&gt;&lt;/p&gt; 
 &lt;br&gt; 
 &lt;p&gt;&lt;img src="https://i.ibb.co/C1yTb0r/falf.png" alt="Preview B"&gt;&lt;/p&gt; 
 &lt;br&gt; 
 &lt;p&gt;&lt;img src="https://i.ibb.co/2Yc4q835/hyprland-preview-b.png" alt="Preview C"&gt;&lt;/p&gt; 
 &lt;br&gt; 
 &lt;br&gt; 
&lt;/div&gt; 
&lt;h1&gt;Special Thanks&lt;/h1&gt; 
&lt;br&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://gitlab.freedesktop.org/wlroots/wlroots"&gt;wlroots&lt;/a&gt;&lt;/strong&gt; - &lt;em&gt;For powering Hyprland in the past&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://gitlab.freedesktop.org/wlroots/wlroots/-/blob/master/tinywl/tinywl.c"&gt;tinywl&lt;/a&gt;&lt;/strong&gt; - &lt;em&gt;For showing how 2 do stuff&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/swaywm/sway"&gt;Sway&lt;/a&gt;&lt;/strong&gt; - &lt;em&gt;For showing how 2 do stuff the overkill way&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/inclement/vivarium"&gt;Vivarium&lt;/a&gt;&lt;/strong&gt; - &lt;em&gt;For showing how 2 do stuff the simple way&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://codeberg.org/dwl/dwl"&gt;dwl&lt;/a&gt;&lt;/strong&gt; - &lt;em&gt;For showing how 2 do stuff the hacky way&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/WayfireWM/wayfire"&gt;Wayfire&lt;/a&gt;&lt;/strong&gt; - &lt;em&gt;For showing how 2 do some graphics stuff&lt;/em&gt;&lt;/p&gt; 
&lt;!----&gt; 
&lt;!--{ Thanks }---------------------------------&gt; 
&lt;!--{ Images }---------------------------------&gt; 
&lt;!--{ Badges }---------------------------------&gt;</description>
    </item>
    
    <item>
      <title>gitleaks/gitleaks</title>
      <link>https://github.com/gitleaks/gitleaks</link>
      <description>&lt;p&gt;Find secrets with Gitleaks ğŸ”‘&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Gitleaks&lt;/h1&gt; 
&lt;pre&gt;&lt;code&gt;â”Œâ”€â—‹â”€â”€â”€â”
â”‚ â”‚â•²  â”‚
â”‚ â”‚ â—‹ â”‚
â”‚ â—‹ â–‘ â”‚
â””â”€â–‘â”€â”€â”€â”˜
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href="https://github.com/gitleaks/gitleaks/actions/workflows/test.yml"&gt;&lt;img src="https://github.com/gitleaks/gitleaks/actions/workflows/test.yml/badge.svg?sanitize=true" alt="GitHub Action Test"&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/zricethezav/gitleaks"&gt;&lt;img src="https://img.shields.io/docker/pulls/zricethezav/gitleaks.svg?sanitize=true" alt="Docker Hub"&gt;&lt;/a&gt; &lt;a href="https://gitleaks.io/playground"&gt;&lt;img src="https://img.shields.io/badge/gitleaks%20-playground-blue" alt="Gitleaks Playground"&gt;&lt;/a&gt; &lt;a href="https://github.com/gitleaks/gitleaks-action"&gt;&lt;img src="https://img.shields.io/badge/protected%20by-gitleaks-blue" alt="Gitleaks Action"&gt;&lt;/a&gt; &lt;a href="https://pkg.go.dev/github.com/zricethezav/gitleaks/v8"&gt;&lt;img src="https://pkg.go.dev/badge/github.com/gitleaks/gitleaks/v8?status" alt="GoDoc"&gt;&lt;/a&gt; &lt;a href="https://goreportcard.com/report/github.com/gitleaks/gitleaks/v8"&gt;&lt;img src="https://goreportcard.com/badge/github.com/gitleaks/gitleaks/v8" alt="GoReportCard"&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/gitleaks/gitleaks/master/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/gitleaks/gitleaks.svg?sanitize=true" alt="License"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Join our Discord! &lt;a href="https://discord.gg/8Hzbrnkr7E"&gt;&lt;img src="https://img.shields.io/discord/1102689410522284044.svg?label=&amp;amp;logo=discord&amp;amp;logoColor=ffffff&amp;amp;color=7389D8&amp;amp;labelColor=6A7EC2" alt="Discord"&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Gitleaks is a tool for &lt;strong&gt;detecting&lt;/strong&gt; secrets like passwords, API keys, and tokens in git repos, files, and whatever else you wanna throw at it via &lt;code&gt;stdin&lt;/code&gt;. If you wanna learn more about how the detection engine works check out this blog: &lt;a href="https://lookingatcomputer.substack.com/p/regex-is-almost-all-you-need"&gt;Regex is (almost) all you need&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;âœ  ~/code(master) gitleaks git -v

    â—‹
    â”‚â•²
    â”‚ â—‹
    â—‹ â–‘
    â–‘    gitleaks


Finding:     "export BUNDLE_ENTERPRISE__CONTRIBSYS__COM=cafebabe:deadbeef",
Secret:      cafebabe:deadbeef
RuleID:      sidekiq-secret
Entropy:     2.609850
File:        cmd/generate/config/rules/sidekiq.go
Line:        23
Commit:      cd5226711335c68be1e720b318b7bc3135a30eb2
Author:      John
Email:       john@users.noreply.github.com
Date:        2022-08-03T12:31:40Z
Fingerprint: cd5226711335c68be1e720b318b7bc3135a30eb2:cmd/generate/config/rules/sidekiq.go:sidekiq-secret:23
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;Gitleaks can be installed using Homebrew, Docker, or Go. Gitleaks is also available in binary form for many popular platforms and OS types on the &lt;a href="https://github.com/gitleaks/gitleaks/releases"&gt;releases page&lt;/a&gt;. In addition, Gitleaks can be implemented as a pre-commit hook directly in your repo or as a GitHub action using &lt;a href="https://github.com/gitleaks/gitleaks-action"&gt;Gitleaks-Action&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Installing&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# MacOS
brew install gitleaks

# Docker (DockerHub)
docker pull zricethezav/gitleaks:latest
docker run -v ${path_to_host_folder_to_scan}:/path zricethezav/gitleaks:latest [COMMAND] [OPTIONS] [SOURCE_PATH]

# Docker (ghcr.io)
docker pull ghcr.io/gitleaks/gitleaks:latest
docker run -v ${path_to_host_folder_to_scan}:/path ghcr.io/gitleaks/gitleaks:latest [COMMAND] [OPTIONS] [SOURCE_PATH]

# From Source (make sure `go` is installed)
git clone https://github.com/gitleaks/gitleaks.git
cd gitleaks
make build
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;GitHub Action&lt;/h3&gt; 
&lt;p&gt;Check out the official &lt;a href="https://github.com/gitleaks/gitleaks-action"&gt;Gitleaks GitHub Action&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;name: gitleaks
on: [pull_request, push, workflow_dispatch]
jobs:
  scan:
    name: gitleaks
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
        with:
          fetch-depth: 0
      - uses: gitleaks/gitleaks-action@v2
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          GITLEAKS_LICENSE: ${{ secrets.GITLEAKS_LICENSE}} # Only required for Organizations, not personal accounts.
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Pre-Commit&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Install pre-commit from &lt;a href="https://pre-commit.com/#install"&gt;https://pre-commit.com/#install&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Create a &lt;code&gt;.pre-commit-config.yaml&lt;/code&gt; file at the root of your repository with the following content:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;repos:
  - repo: https://github.com/gitleaks/gitleaks
    rev: v8.24.2
    hooks:
      - id: gitleaks
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;for a &lt;a href="https://github.com/gitleaks/gitleaks/releases"&gt;native execution of gitleaks&lt;/a&gt; or use the &lt;a href="https://github.com/gitleaks/gitleaks/raw/master/.pre-commit-hooks.yaml"&gt;&lt;code&gt;gitleaks-docker&lt;/code&gt; pre-commit ID&lt;/a&gt; for executing gitleaks using the &lt;a href="https://raw.githubusercontent.com/gitleaks/gitleaks/master/#docker"&gt;official Docker images&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Auto-update the config to the latest repos' versions by executing &lt;code&gt;pre-commit autoupdate&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install with &lt;code&gt;pre-commit install&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Now you're all set!&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code&gt;âœ git commit -m "this commit contains a secret"
Detect hardcoded secrets.................................................Failed
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note: to disable the gitleaks pre-commit hook you can prepend &lt;code&gt;SKIP=gitleaks&lt;/code&gt; to the commit command and it will skip running gitleaks&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;âœ SKIP=gitleaks git commit -m "skip gitleaks check"
Detect hardcoded secrets................................................Skipped
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;Usage:
  gitleaks [command]

Available Commands:
  dir         scan directories or files for secrets
  git         scan git repositories for secrets
  help        Help about any command
  stdin       detect secrets from stdin
  version     display gitleaks version

Flags:
  -b, --baseline-path string          path to baseline with issues that can be ignored
  -c, --config string                 config file path
                                      order of precedence:
                                      1. --config/-c
                                      2. env var GITLEAKS_CONFIG
                                      3. env var GITLEAKS_CONFIG_TOML with the file content
                                      4. (target path)/.gitleaks.toml
                                      If none of the four options are used, then gitleaks will use the default config
      --diagnostics string            enable diagnostics (comma-separated list: cpu,mem,trace). cpu=CPU profiling, mem=memory profiling, trace=execution tracing
      --diagnostics-dir string        directory to store diagnostics output files (defaults to current directory)
      --enable-rule strings           only enable specific rules by id
      --exit-code int                 exit code when leaks have been encountered (default 1)
  -i, --gitleaks-ignore-path string   path to .gitleaksignore file or folder containing one (default ".")
  -h, --help                          help for gitleaks
      --ignore-gitleaks-allow         ignore gitleaks:allow comments
  -l, --log-level string              log level (trace, debug, info, warn, error, fatal) (default "info")
      --max-decode-depth int          allow recursive decoding up to this depth (default "0", no decoding is done)
      --max-archive-depth int         allow scanning into nested archives up to this depth (default "0", no archive traversal is done)
      --max-target-megabytes int      files larger than this will be skipped
      --no-banner                     suppress banner
      --no-color                      turn off color for verbose output
      --redact uint[=100]             redact secrets from logs and stdout. To redact only parts of the secret just apply a percent value from 0..100. For example --redact=20 (default 100%)
  -f, --report-format string          output format (json, csv, junit, sarif, template)
  -r, --report-path string            report file
      --report-template string        template file used to generate the report (implies --report-format=template)
  -v, --verbose                       show verbose output from scan
      --version                       version for gitleaks

Use "gitleaks [command] --help" for more information about a command.
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Commands&lt;/h3&gt; 
&lt;p&gt;âš ï¸ v8.19.0 introduced a change that deprecated &lt;code&gt;detect&lt;/code&gt; and &lt;code&gt;protect&lt;/code&gt;. Those commands are still available but are hidden in the &lt;code&gt;--help&lt;/code&gt; menu. Take a look at this &lt;a href="https://gist.github.com/zricethezav/b325bb93ebf41b9c0b0507acf12810d2"&gt;gist&lt;/a&gt; for easy command translations. If you find v8.19.0 broke an existing command (&lt;code&gt;detect&lt;/code&gt;/&lt;code&gt;protect&lt;/code&gt;), please open an issue.&lt;/p&gt; 
&lt;p&gt;There are three scanning modes: &lt;code&gt;git&lt;/code&gt;, &lt;code&gt;dir&lt;/code&gt;, and &lt;code&gt;stdin&lt;/code&gt;.&lt;/p&gt; 
&lt;h4&gt;Git&lt;/h4&gt; 
&lt;p&gt;The &lt;code&gt;git&lt;/code&gt; command lets you scan local git repos. Under the hood, gitleaks uses the &lt;code&gt;git log -p&lt;/code&gt; command to scan patches. You can configure the behavior of &lt;code&gt;git log -p&lt;/code&gt; with the &lt;code&gt;log-opts&lt;/code&gt; option. For example, if you wanted to run gitleaks on a range of commits you could use the following command: &lt;code&gt;gitleaks git -v --log-opts="--all commitA..commitB" path_to_repo&lt;/code&gt;. See the &lt;a href="https://git-scm.com/docs/git-log"&gt;git log&lt;/a&gt; documentation for more information. If there is no target specified as a positional argument, then gitleaks will attempt to scan the current working directory as a git repo.&lt;/p&gt; 
&lt;h4&gt;Dir&lt;/h4&gt; 
&lt;p&gt;The &lt;code&gt;dir&lt;/code&gt; (aliases include &lt;code&gt;files&lt;/code&gt;, &lt;code&gt;directory&lt;/code&gt;) command lets you scan directories and files. Example: &lt;code&gt;gitleaks dir -v path_to_directory_or_file&lt;/code&gt;. If there is no target specified as a positional argument, then gitleaks will scan the current working directory.&lt;/p&gt; 
&lt;h4&gt;Stdin&lt;/h4&gt; 
&lt;p&gt;You can also stream data to gitleaks with the &lt;code&gt;stdin&lt;/code&gt; command. Example: &lt;code&gt;cat some_file | gitleaks -v stdin&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Creating a baseline&lt;/h3&gt; 
&lt;p&gt;When scanning large repositories or repositories with a long history, it can be convenient to use a baseline. When using a baseline, gitleaks will ignore any old findings that are present in the baseline. A baseline can be any gitleaks report. To create a gitleaks report, run gitleaks with the &lt;code&gt;--report-path&lt;/code&gt; parameter.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;gitleaks git --report-path gitleaks-report.json # This will save the report in a file called gitleaks-report.json
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Once as baseline is created it can be applied when running the detect command again:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;gitleaks git --baseline-path gitleaks-report.json --report-path findings.json
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After running the detect command with the --baseline-path parameter, report output (findings.json) will only contain new issues.&lt;/p&gt; 
&lt;h2&gt;Pre-Commit hook&lt;/h2&gt; 
&lt;p&gt;You can run Gitleaks as a pre-commit hook by copying the example &lt;code&gt;pre-commit.py&lt;/code&gt; script into your &lt;code&gt;.git/hooks/&lt;/code&gt; directory.&lt;/p&gt; 
&lt;h2&gt;Load Configuration&lt;/h2&gt; 
&lt;p&gt;The order of precedence is:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;code&gt;--config/-c&lt;/code&gt; option: &lt;pre&gt;&lt;code class="language-bash"&gt;gitleaks git --config /home/dev/customgitleaks.toml .
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Environment variable &lt;code&gt;GITLEAKS_CONFIG&lt;/code&gt; with the file path: &lt;pre&gt;&lt;code class="language-bash"&gt;export GITLEAKS_CONFIG="/home/dev/customgitleaks.toml"
gitleaks git .
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Environment variable &lt;code&gt;GITLEAKS_CONFIG_TOML&lt;/code&gt; with the file content: &lt;pre&gt;&lt;code class="language-bash"&gt;export GITLEAKS_CONFIG_TOML=`cat customgitleaks.toml`
gitleaks git .
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;A &lt;code&gt;.gitleaks.toml&lt;/code&gt; file within the target path: &lt;pre&gt;&lt;code class="language-bash"&gt;gitleaks git .
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;If none of the four options are used, then gitleaks will use the default config.&lt;/p&gt; 
&lt;h2&gt;Configuration&lt;/h2&gt; 
&lt;p&gt;Gitleaks offers a configuration format you can follow to write your own secret detection rules:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-toml"&gt;# Title for the gitleaks configuration file.
title = "Custom Gitleaks configuration"

# You have basically two options for your custom configuration:
#
# 1. define your own configuration, default rules do not apply
#
#    use e.g., the default configuration as starting point:
#    https://github.com/gitleaks/gitleaks/blob/master/config/gitleaks.toml
#
# 2. extend a configuration, the rules are overwritten or extended
#
#    When you extend a configuration the extended rules take precedence over the
#    default rules. I.e., if there are duplicate rules in both the extended
#    configuration and the default configuration the extended rules or
#    attributes of them will override the default rules.
#    Another thing to know with extending configurations is you can chain
#    together multiple configuration files to a depth of 2. Allowlist arrays are
#    appended and can contain duplicates.

# useDefault and path can NOT be used at the same time. Choose one.
[extend]
# useDefault will extend the default gitleaks config built in to the binary
# the latest version is located at:
# https://github.com/gitleaks/gitleaks/blob/master/config/gitleaks.toml
useDefault = true
# or you can provide a path to a configuration to extend from.
# The path is relative to where gitleaks was invoked,
# not the location of the base config.
# path = "common_config.toml"
# If there are any rules you don't want to inherit, they can be specified here.
disabledRules = [ "generic-api-key"]

# An array of tables that contain information that define instructions
# on how to detect secrets
[[rules]]
# Unique identifier for this rule
id = "awesome-rule-1"

# Short human-readable description of the rule.
description = "awesome rule 1"

# Golang regular expression used to detect secrets. Note Golang's regex engine
# does not support lookaheads.
regex = '''one-go-style-regex-for-this-rule'''

# Int used to extract secret from regex match and used as the group that will have
# its entropy checked if `entropy` is set.
secretGroup = 3

# Float representing the minimum shannon entropy a regex group must have to be considered a secret.
entropy = 3.5

# Golang regular expression used to match paths. This can be used as a standalone rule or it can be used
# in conjunction with a valid `regex` entry.
path = '''a-file-path-regex'''

# Keywords are used for pre-regex check filtering. Rules that contain
# keywords will perform a quick string compare check to make sure the
# keyword(s) are in the content being scanned. Ideally these values should
# either be part of the identiifer or unique strings specific to the rule's regex
# (introduced in v8.6.0)
keywords = [
  "auth",
  "password",
  "token",
]

# Array of strings used for metadata and reporting purposes.
tags = ["tag","another tag"]

    # âš ï¸ In v8.21.0 `[rules.allowlist]` was replaced with `[[rules.allowlists]]`.
    # This change was backwards-compatible: instances of `[rules.allowlist]` still  work.
    #
    # You can define multiple allowlists for a rule to reduce false positives.
    # A finding will be ignored if _ANY_ `[[rules.allowlists]]` matches.
    [[rules.allowlists]]
    description = "ignore commit A"
    # When multiple criteria are defined the default condition is "OR".
    # e.g., this can match on |commits| OR |paths| OR |stopwords|.
    condition = "OR"
    commits = [ "commit-A", "commit-B"]
    paths = [
      '''go\.mod''',
      '''go\.sum'''
    ]
    # note: stopwords targets the extracted secret, not the entire regex match
    # like 'regexes' does. (stopwords introduced in 8.8.0)
    stopwords = [
      '''client''',
      '''endpoint''',
    ]

    [[rules.allowlists]]
    # The "AND" condition can be used to make sure all criteria match.
    # e.g., this matches if |regexes| AND |paths| are satisfied.
    condition = "AND"
    # note: |regexes| defaults to check the _Secret_ in the finding.
    # Acceptable values for |regexTarget| are "secret" (default), "match", and "line".
    regexTarget = "match"
    regexes = [ '''(?i)parseur[il]''' ]
    paths = [ '''package-lock\.json''' ]

# You can extend a particular rule from the default config. e.g., gitlab-pat
# if you have defined a custom token prefix on your GitLab instance
[[rules]]
id = "gitlab-pat"
# all the other attributes from the default rule are inherited

    [[rules.allowlists]]
    regexTarget = "line"
    regexes = [ '''MY-glpat-''' ]


# âš ï¸ In v8.25.0 `[allowlist]` was replaced with `[[allowlists]]`.
#
# Global allowlists have a higher order of precedence than rule-specific allowlists.
# If a commit listed in the `commits` field below is encountered then that commit will be skipped and no
# secrets will be detected for said commit. The same logic applies for regexes and paths.
[[allowlists]]
description = "global allow list"
commits = [ "commit-A", "commit-B", "commit-C"]
paths = [
  '''gitleaks\.toml''',
  '''(.*?)(jpg|gif|doc)'''
]
# note: (global) regexTarget defaults to check the _Secret_ in the finding.
# Acceptable values for regexTarget are "match" and "line"
regexTarget = "match"
regexes = [
  '''219-09-9999''',
  '''078-05-1120''',
  '''(9[0-9]{2}|666)-\d{2}-\d{4}''',
]
# note: stopwords targets the extracted secret, not the entire regex match
# like 'regexes' does. (stopwords introduced in 8.8.0)
stopwords = [
  '''client''',
  '''endpoint''',
]

# âš ï¸ In v8.25.0, `[[allowlists]]` have a new field called |targetRules|.
#
# Common allowlists can be defined once and assigned to multiple rules using |targetRules|.
# This will only run on the specified rules, not globally.
[[allowlists]]
targetRules = ["awesome-rule-1", "awesome-rule-2"]
description = "Our test assets trigger false-positives in a couple rules."
paths = ['''tests/expected/._\.json$''']
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Refer to the default &lt;a href="https://github.com/gitleaks/gitleaks/raw/master/config/gitleaks.toml"&gt;gitleaks config&lt;/a&gt; for examples or follow the &lt;a href="https://github.com/gitleaks/gitleaks/raw/master/CONTRIBUTING.md"&gt;contributing guidelines&lt;/a&gt; if you would like to contribute to the default configuration. Additionally, you can check out &lt;a href="https://blog.gitleaks.io/stop-leaking-secrets-configuration-2-3-aeed293b1fbf"&gt;this gitleaks blog post&lt;/a&gt; which covers advanced configuration setups.&lt;/p&gt; 
&lt;h3&gt;Additional Configuration&lt;/h3&gt; 
&lt;h4&gt;Composite Rules (Multi-part or &lt;code&gt;required&lt;/code&gt; Rules)&lt;/h4&gt; 
&lt;p&gt;In v8.28.0 Gitleaks introduced composite rules, which are made up of a single "primary" rule and one or more auxiliary or &lt;code&gt;required&lt;/code&gt; rules. To create a composite rule, add a &lt;code&gt;[[rules.required]]&lt;/code&gt; table to the primary rule specifying an &lt;code&gt;id&lt;/code&gt; and optionally &lt;code&gt;withinLines&lt;/code&gt; and/or &lt;code&gt;withinColumns&lt;/code&gt; proximity constraints. A fragment is a chunk of content that Gitleaks processes at once (typically a file, part of a file, or git diff), and proximity matching instructs the primary rule to only report a finding if the auxiliary &lt;code&gt;required&lt;/code&gt; rules also find matches within the specified area of the fragment.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Proximity matching:&lt;/strong&gt; Using the &lt;code&gt;withinLines&lt;/code&gt; and &lt;code&gt;withinColumns&lt;/code&gt; fields instructs the primary rule to only report a finding if the auxiliary &lt;code&gt;required&lt;/code&gt; rules also find matches within the specified proximity. You can set:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;withinLines: N&lt;/code&gt;&lt;/strong&gt; - required findings must be within N lines (vertically)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;withinColumns: N&lt;/code&gt;&lt;/strong&gt; - required findings must be within N characters (horizontally)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Both&lt;/strong&gt; - creates a rectangular search area (both constraints must be satisfied)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Neither&lt;/strong&gt; - fragment-level matching (required findings can be anywhere in the same fragment)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Here are diagrams illustrating each proximity behavior:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;p = primary captured secret
a = auxiliary (required) captured secret
fragment = section of data gitleaks is looking at


    *Fragment-level proximity*               
    Any required finding in the fragment
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”                       
   â”Œâ”€â”€â”€â”€â”€â”€â”¤fragmentâ”œâ”€â”€â”€â”€â”€â”                 
   â”‚      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”¤     â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”       
   â”‚             â”‚aâ”‚â—€â”€â”€â”€â”€â”¼â”€â”‚âœ“ MATCHâ”‚       
   â”‚          â”Œâ”€â”â””â”€â”˜     â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”˜       
   â”‚â”Œâ”€â”       â”‚pâ”‚        â”‚                 
   â”‚â”‚aâ”‚    â”Œâ”€â”â””â”€â”˜        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”       
   â”‚â””â”€â”˜    â”‚aâ”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”‚âœ“ MATCHâ”‚       
   â””â”€â–²â”€â”€â”€â”€â”€â”´â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜       
     â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”                        
     â””â”€â”€â”€â”€â”‚âœ“ MATCHâ”‚                        
          â””â”€â”€â”€â”€â”€â”€â”€â”˜                        
                                           
                                           
   *Column bounded proximity*
   `withinColumns = 3`                    
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”                       
   â”Œâ”€â”€â”€â”€â”¬â”€â”¤fragmentâ”œâ”€â”¬â”€â”€â”€â”                 
   â”‚      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”¤     â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   
   â”‚    â”‚        â”‚aâ”‚â—€â”¼â”€â”€â”€â”¼â”€â”‚+1C âœ“ MATCHâ”‚   
   â”‚          â”Œâ”€â”â””â”€â”˜     â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   
   â”‚â”Œâ”€â” â”‚     â”‚pâ”‚    â”‚   â”‚                 
â”Œâ”€â”€â–¶â”‚aâ”‚  â”Œâ”€â”  â””â”€â”˜        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   
â”‚  â”‚â””â”€â”˜ â”‚â”‚aâ”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”‚-2C âœ“ MATCHâ”‚   
â”‚  â”‚       â”˜             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   
â”‚  â””â”€â”€ -3C â”€â”€â”€0Câ”€â”€â”€ +3C â”€â”˜                 
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                             
â”‚  â”‚ -4C âœ— NOâ”‚                             
â””â”€â”€â”‚  MATCH  â”‚                             
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             
                                           
                                           
   *Line bounded proximity*
   `withinLines = 4`                      
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”                        
   â”Œâ”€â”€â”€â”€â”€â”¤fragmentâ”œâ”€â”€â”€â”€â”€â”                  
  +4Lâ”€ â”€ â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”€ â”€ â”€â”‚                  
   â”‚                    â”‚                  
   â”‚              â”Œâ”€â”   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   
   â”‚         â”Œâ”€â”  â”‚aâ”‚â—€â”€â”€â”¼â”€â”‚+1L âœ“ MATCH â”‚   
   0L  â”Œâ”€â”   â”‚pâ”‚  â””â”€â”˜   â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   
   â”‚   â”‚aâ”‚â—€â”€â”€â”´â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”‚-1L âœ“ MATCH â”‚   
   â”‚   â””â”€â”˜              â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   
   â”‚                    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      
  -4Lâ”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€â”Œâ”€â”â”€â”‚ â”‚-5L âœ— NO â”‚      
   â”‚                â”‚aâ”‚â—€â”¼â”€â”‚  MATCH  â”‚      
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”´â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      
                                           
                                           
   *Line and column bounded proximity*
   `withinLines = 4`                      
   `withinColumns = 3`                    
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”                        
   â”Œâ”€â”€â”€â”€â”€â”¤fragmentâ”œâ”€â”€â”€â”€â”€â”                  
  +4L   â”Œâ””â”€â”€â”€â”€â”€â”€â”€â”€â”´ â”   â”‚                  
   â”‚            â”Œâ”€â”     â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚    â”‚       â”‚aâ”‚â—€â”¼â”€â”€â”€â”¼â”€â”‚+2L/+1C âœ“ MATCHâ”‚
   â”‚         â”Œâ”€â”â””â”€â”˜     â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   0L   â”‚    â”‚pâ”‚    â”‚   â”‚                  
   â”‚         â””â”€â”˜        â”‚                  
   â”‚    â”‚           â”‚   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   
  -4L    â”€ â”€ â”€ â”€ â”€ â”€â”Œâ”€â” â”‚ â”‚-5L/+3C âœ— NOâ”‚   
   â”‚                â”‚aâ”‚â—€â”¼â”€â”‚   MATCH    â”‚   
   â””â”€â”€â”€-3Câ”€â”€â”€â”€0Lâ”€â”€â”€+3Câ”´â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt;
 &lt;summary&gt;Some final quick thoughts on composite rules.&lt;/summary&gt;This is an experimental feature! It's subject to change so don't go sellin' a new B2B SaaS feature built ontop of this feature. Scan type (git vs dir) based context is interesting. I'm monitoring the situation. Composite rules might not be super useful for git scans because gitleaks only looks at additions in the git history. It could be useful to scan non-additions in git history for `required` rules. Oh, right this is a readme, I'll shut up now.
&lt;/details&gt; 
&lt;h4&gt;gitleaks:allow&lt;/h4&gt; 
&lt;p&gt;If you are knowingly committing a test secret that gitleaks will catch you can add a &lt;code&gt;gitleaks:allow&lt;/code&gt; comment to that line which will instruct gitleaks to ignore that secret. Ex:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;class CustomClass:
    discord_client_secret = '8dyfuiRyq=vVc3RRr_edRk-fK__JItpZ'  #gitleaks:allow

&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;.gitleaksignore&lt;/h4&gt; 
&lt;p&gt;You can ignore specific findings by creating a &lt;code&gt;.gitleaksignore&lt;/code&gt; file at the root of your repo. In release v8.10.0 Gitleaks added a &lt;code&gt;Fingerprint&lt;/code&gt; value to the Gitleaks report. Each leak, or finding, has a Fingerprint that uniquely identifies a secret. Add this fingerprint to the &lt;code&gt;.gitleaksignore&lt;/code&gt; file to ignore that specific secret. See Gitleaks' &lt;a href="https://github.com/gitleaks/gitleaks/raw/master/.gitleaksignore"&gt;.gitleaksignore&lt;/a&gt; for an example. Note: this feature is experimental and is subject to change in the future.&lt;/p&gt; 
&lt;h4&gt;Decoding&lt;/h4&gt; 
&lt;p&gt;Sometimes secrets are encoded in a way that can make them difficult to find with just regex. Now you can tell gitleaks to automatically find and decode encoded text. The flag &lt;code&gt;--max-decode-depth&lt;/code&gt; enables this feature (the default value "0" means the feature is disabled by default).&lt;/p&gt; 
&lt;p&gt;Recursive decoding is supported since decoded text can also contain encoded text. The flag &lt;code&gt;--max-decode-depth&lt;/code&gt; sets the recursion limit. Recursion stops when there are no new segments of encoded text to decode, so setting a really high max depth doesn't mean it will make that many passes. It will only make as many as it needs to decode the text. Overall, decoding only minimally increases scan times.&lt;/p&gt; 
&lt;p&gt;The findings for encoded text differ from normal findings in the following ways:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;The location points the bounds of the encoded text 
  &lt;ul&gt; 
   &lt;li&gt;If the rule matches outside the encoded text, the bounds are adjusted to include that as well&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;The match and secret contain the decoded value&lt;/li&gt; 
 &lt;li&gt;Two tags are added &lt;code&gt;decoded:&amp;lt;encoding&amp;gt;&lt;/code&gt; and &lt;code&gt;decode-depth:&amp;lt;depth&amp;gt;&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Currently supported encodings:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;percent&lt;/strong&gt; - Any printable ASCII percent encoded values&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;hex&lt;/strong&gt; - Any printable ASCII hex encoded values &amp;gt;= 32 characters&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;base64&lt;/strong&gt; - Any printable ASCII base64 encoded values &amp;gt;= 16 characters&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Archive Scanning&lt;/h4&gt; 
&lt;p&gt;Sometimes secrets are packaged within archive files like zip files or tarballs, making them difficult to discover. Now you can tell gitleaks to automatically extract and scan the contents of archives. The flag &lt;code&gt;--max-archive-depth&lt;/code&gt; enables this feature for both &lt;code&gt;dir&lt;/code&gt; and &lt;code&gt;git&lt;/code&gt; scan types. The default value of "0" means this feature is disabled by default.&lt;/p&gt; 
&lt;p&gt;Recursive scanning is supported since archives can also contain other archives. The &lt;code&gt;--max-archive-depth&lt;/code&gt; flag sets the recursion limit. Recursion stops when there are no new archives to extract, so setting a very high max depth just sets the potential to go that deep. It will only go as deep as it needs to.&lt;/p&gt; 
&lt;p&gt;The findings for secrets located within an archive will include the path to the file inside the archive. Inner paths are separated with &lt;code&gt;!&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Example finding (shortened for brevity):&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Finding:     DB_PASSWORD=8ae31cacf141669ddfb5da
...
File:        testdata/archives/nested.tar.gz!archives/files.tar!files/.env.prod
Line:        4
Commit:      6e6ee6596d337bb656496425fb98644eb62b4a82
...
Fingerprint: 6e6ee6596d337bb656496425fb98644eb62b4a82:testdata/archives/nested.tar.gz!archives/files.tar!files/.env.prod:generic-api-key:4
Link:        https://github.com/leaktk/gitleaks/blob/6e6ee6596d337bb656496425fb98644eb62b4a82/testdata/archives/nested.tar.gz
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This means a secret was detected on line 4 of &lt;code&gt;files/.env.prod.&lt;/code&gt; which is in &lt;code&gt;archives/files.tar&lt;/code&gt; which is in &lt;code&gt;testdata/archives/nested.tar.gz&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Currently supported formats:&lt;/p&gt; 
&lt;p&gt;The &lt;a href="https://github.com/mholt/archives?tab=readme-ov-file#supported-compression-formats"&gt;compression&lt;/a&gt; and &lt;a href="https://github.com/mholt/archives?tab=readme-ov-file#supported-archive-formats"&gt;archive&lt;/a&gt; formats supported by mholt's &lt;a href="https://github.com/mholt/archives"&gt;archives package&lt;/a&gt; are supported.&lt;/p&gt; 
&lt;h4&gt;Reporting&lt;/h4&gt; 
&lt;p&gt;Gitleaks has built-in support for several report formats: &lt;a href="https://github.com/gitleaks/gitleaks/raw/master/testdata/expected/report/json_simple.json"&gt;&lt;code&gt;json&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://github.com/gitleaks/gitleaks/raw/master/testdata/expected/report/csv_simple.csv?plain=1"&gt;&lt;code&gt;csv&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://github.com/gitleaks/gitleaks/raw/master/testdata/expected/report/junit_simple.xml"&gt;&lt;code&gt;junit&lt;/code&gt;&lt;/a&gt;, and &lt;a href="https://github.com/gitleaks/gitleaks/raw/master/testdata/expected/report/sarif_simple.sarif"&gt;&lt;code&gt;sarif&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If none of these formats fit your need, you can create your own report format with a &lt;a href="https://www.digitalocean.com/community/tutorials/how-to-use-templates-in-go#step-4-writing-a-template"&gt;Go &lt;code&gt;text/template&lt;/code&gt; .tmpl file&lt;/a&gt; and the &lt;code&gt;--report-template&lt;/code&gt; flag. The template can use &lt;a href="https://masterminds.github.io/sprig/"&gt;extended functionality from the &lt;code&gt;Masterminds/sprig&lt;/code&gt; template library&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For example, the following template provides a custom JSON output:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-gotemplate"&gt;# jsonextra.tmpl
[{{ $lastFinding := (sub (len . ) 1) }}
{{- range $i, $finding := . }}{{with $finding}}
    {
        "Description": {{ quote .Description }},
        "StartLine": {{ .StartLine }},
        "EndLine": {{ .EndLine }},
        "StartColumn": {{ .StartColumn }},
        "EndColumn": {{ .EndColumn }},
        "Line": {{ quote .Line }},
        "Match": {{ quote .Match }},
        "Secret": {{ quote .Secret }},
        "File": "{{ .File }}",
        "SymlinkFile": {{ quote .SymlinkFile }},
        "Commit": {{ quote .Commit }},
        "Entropy": {{ .Entropy }},
        "Author": {{ quote .Author }},
        "Email": {{ quote .Email }},
        "Date": {{ quote .Date }},
        "Message": {{ quote .Message }},
        "Tags": [{{ $lastTag := (sub (len .Tags ) 1) }}{{ range $j, $tag := .Tags }}{{ quote . }}{{ if ne $j $lastTag }},{{ end }}{{ end }}],
        "RuleID": {{ quote .RuleID }},
        "Fingerprint": {{ quote .Fingerprint }}
    }{{ if ne $i $lastFinding }},{{ end }}
{{- end}}{{ end }}
]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Usage:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;$ gitleaks dir ~/leaky-repo/ --report-path "report.json" --report-format template --report-template testdata/report/jsonextra.tmpl
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Sponsorships&lt;/h2&gt; 
&lt;p align="left"&gt; &lt;/p&gt;
&lt;h3&gt;&lt;a href="https://coderabbit.ai/?utm_source=oss&amp;amp;utm_medium=sponsorship&amp;amp;utm_campaign=gitleaks"&gt;coderabbit.ai&lt;/a&gt;&lt;/h3&gt;
&lt;a href="https://coderabbit.ai/?utm_source=oss&amp;amp;utm_medium=sponsorship&amp;amp;utm_campaign=gitleaks"&gt; &lt;/a&gt;
&lt;a href="https://coderabbit.ai/?utm_source=oss&amp;amp;utm_medium=sponsorship&amp;amp;utm_campaign=gitleaks"&gt; &lt;img alt="CodeRabbit.ai Sponsorship" src="https://github.com/gitleaks/gitleaks/assets/15034943/76c30a85-887b-47ca-9956-17a8e55c6c41" width="200"&gt; &lt;/a&gt; 
&lt;p&gt;&lt;/p&gt; 
&lt;h2&gt;Exit Codes&lt;/h2&gt; 
&lt;p&gt;You can always set the exit code when leaks are encountered with the --exit-code flag. Default exit codes below:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;0 - no leaks present
1 - leaks or error encountered
126 - unknown flag
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
  </channel>
</rss>