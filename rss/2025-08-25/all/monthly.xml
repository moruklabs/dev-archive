<rss version="2.0">
  <channel>
    <title>GitHub All Languages Monthly Trending</title>
    <description>Monthly Trending of All Languages in GitHub</description>
    <pubDate>Sun, 24 Aug 2025 01:54:07 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>simstudioai/sim</title>
      <link>https://github.com/simstudioai/sim</link>
      <description>&lt;p&gt;Sim is an open-source AI agent workflow builder. Sim's interface is a lightweight, intuitive way to rapidly build and deploy LLMs that connect with your favorite tools.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;a href="https://sim.ai" target="_blank" rel="noopener noreferrer"&gt; &lt;img src="https://raw.githubusercontent.com/simstudioai/sim/main/apps/sim/public/logo/reverse/text/large.png" alt="Sim Logo" width="500" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt;Build and deploy AI agent workflows in minutes.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://sim.ai" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://img.shields.io/badge/sim.ai-6F3DFA" alt="Sim.ai" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/Hr4UWYEcTT" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://img.shields.io/badge/Discord-Join%20Server-5865F2?logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://x.com/simdotai" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://img.shields.io/twitter/follow/simstudioai?style=social" alt="Twitter" /&gt;&lt;/a&gt; &lt;a href="https://docs.sim.ai" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://img.shields.io/badge/Docs-6F3DFA.svg?sanitize=true" alt="Documentation" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/simstudioai/sim/main/apps/sim/public/static/demo.gif" alt="Sim Demo" width="800" /&gt; &lt;/p&gt; 
&lt;h2&gt;Quickstart&lt;/h2&gt; 
&lt;h3&gt;Cloud-hosted: &lt;a href="https://sim.ai"&gt;sim.ai&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://sim.ai" target="_blank" rel="noopener noreferrer"&gt;&lt;img src="https://img.shields.io/badge/sim.ai-6F3DFA?logo=data:image/svg%2bxml;base64,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&amp;amp;logoColor=white" alt="Sim.ai" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Self-hosted: NPM Package&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;npx simstudio
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;â†’ &lt;a href="http://localhost:3000"&gt;http://localhost:3000&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;Note&lt;/h4&gt; 
&lt;p&gt;Docker must be installed and running on your machine.&lt;/p&gt; 
&lt;h4&gt;Options&lt;/h4&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Flag&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;-p, --port &amp;lt;port&amp;gt;&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Port to run Sim on (default &lt;code&gt;3000&lt;/code&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--no-pull&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Skip pulling latest Docker images&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Self-hosted: Docker Compose&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Clone the repository
git clone https://github.com/simstudioai/sim.git

# Navigate to the project directory
cd sim

# Start Sim
docker compose -f docker-compose.prod.yml up -d
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Access the application at &lt;a href="http://localhost:3000/"&gt;http://localhost:3000/&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;Using Local Models with Ollama&lt;/h4&gt; 
&lt;p&gt;Run Sim with local AI models using &lt;a href="https://ollama.ai"&gt;Ollama&lt;/a&gt; - no external APIs required:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Start with GPU support (automatically downloads gemma3:4b model)
docker compose -f docker-compose.ollama.yml --profile setup up -d

# For CPU-only systems:
docker compose -f docker-compose.ollama.yml --profile cpu --profile setup up -d
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Wait for the model to download, then visit &lt;a href="http://localhost:3000"&gt;http://localhost:3000&lt;/a&gt;. Add more models with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker compose -f docker-compose.ollama.yml exec ollama ollama pull llama3.1:8b
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Self-hosted: Dev Containers&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Open VS Code with the &lt;a href="https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers"&gt;Remote - Containers extension&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Open the project and click "Reopen in Container" when prompted&lt;/li&gt; 
 &lt;li&gt;Run &lt;code&gt;bun run dev:full&lt;/code&gt; in the terminal or use the &lt;code&gt;sim-start&lt;/code&gt; alias 
  &lt;ul&gt; 
   &lt;li&gt;This starts both the main application and the realtime socket server&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Self-hosted: Manual Setup&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Requirements:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://bun.sh/"&gt;Bun&lt;/a&gt; runtime&lt;/li&gt; 
 &lt;li&gt;PostgreSQL 12+ with &lt;a href="https://github.com/pgvector/pgvector"&gt;pgvector extension&lt;/a&gt; (required for AI embeddings)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Sim uses vector embeddings for AI features like knowledge bases and semantic search, which requires the &lt;code&gt;pgvector&lt;/code&gt; PostgreSQL extension.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Clone and install dependencies:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/simstudioai/sim.git
cd sim
bun install
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Set up PostgreSQL with pgvector:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;You need PostgreSQL with the &lt;code&gt;vector&lt;/code&gt; extension for embedding support. Choose one option:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Option A: Using Docker (Recommended)&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Start PostgreSQL with pgvector extension
docker run --name simstudio-db \
  -e POSTGRES_PASSWORD=your_password \
  -e POSTGRES_DB=simstudio \
  -p 5432:5432 -d \
  pgvector/pgvector:pg17
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Option B: Manual Installation&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Install PostgreSQL 12+ and the pgvector extension&lt;/li&gt; 
 &lt;li&gt;See &lt;a href="https://github.com/pgvector/pgvector#installation"&gt;pgvector installation guide&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Set up environment:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd apps/sim
cp .env.example .env  # Configure with required variables (DATABASE_URL, BETTER_AUTH_SECRET, BETTER_AUTH_URL)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Update your &lt;code&gt;.env&lt;/code&gt; file with the database URL:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;DATABASE_URL="postgresql://postgres:your_password@localhost:5432/simstudio"
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="4"&gt; 
 &lt;li&gt;Set up the database:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;bunx drizzle-kit migrate 
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="5"&gt; 
 &lt;li&gt;Start the development servers:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Recommended approach - run both servers together (from project root):&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;bun run dev:full
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This starts both the main Next.js application and the realtime socket server required for full functionality.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Alternative - run servers separately:&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Next.js app (from project root):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;bun run dev
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Realtime socket server (from &lt;code&gt;apps/sim&lt;/code&gt; directory in a separate terminal):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cd apps/sim
bun run dev:sockets
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Copilot API Keys&lt;/h2&gt; 
&lt;p&gt;Copilot is a Sim-managed service. To use Copilot on a self-hosted instance:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Go to &lt;a href="https://sim.ai"&gt;https://sim.ai&lt;/a&gt; â†’ Settings â†’ Copilot and generate a Copilot API key&lt;/li&gt; 
 &lt;li&gt;Set &lt;code&gt;COPILOT_API_KEY&lt;/code&gt; in your self-hosted environment to that value&lt;/li&gt; 
 &lt;li&gt;Host Sim on a publicly available DNS and set NEXT_PUBLIC_APP_URL and BETTER_AUTH_URL to that value (&lt;a href="https://ngrok.com/"&gt;ngrok&lt;/a&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Tech Stack&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Framework&lt;/strong&gt;: &lt;a href="https://nextjs.org/"&gt;Next.js&lt;/a&gt; (App Router)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Runtime&lt;/strong&gt;: &lt;a href="https://bun.sh/"&gt;Bun&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Database&lt;/strong&gt;: PostgreSQL with &lt;a href="https://orm.drizzle.team"&gt;Drizzle ORM&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Authentication&lt;/strong&gt;: &lt;a href="https://better-auth.com"&gt;Better Auth&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;UI&lt;/strong&gt;: &lt;a href="https://ui.shadcn.com/"&gt;Shadcn&lt;/a&gt;, &lt;a href="https://tailwindcss.com"&gt;Tailwind CSS&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;State Management&lt;/strong&gt;: &lt;a href="https://zustand-demo.pmnd.rs/"&gt;Zustand&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Flow Editor&lt;/strong&gt;: &lt;a href="https://reactflow.dev/"&gt;ReactFlow&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Docs&lt;/strong&gt;: &lt;a href="https://fumadocs.vercel.app/"&gt;Fumadocs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Monorepo&lt;/strong&gt;: &lt;a href="https://turborepo.org/"&gt;Turborepo&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Realtime&lt;/strong&gt;: &lt;a href="https://socket.io/"&gt;Socket.io&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Background Jobs&lt;/strong&gt;: &lt;a href="https://trigger.dev/"&gt;Trigger.dev&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions! Please see our &lt;a href="https://raw.githubusercontent.com/simstudioai/sim/main/.github/CONTRIBUTING.md"&gt;Contributing Guide&lt;/a&gt; for details.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the Apache License 2.0 - see the &lt;a href="https://raw.githubusercontent.com/simstudioai/sim/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;p align="center"&gt;Made with â¤ï¸ by the Sim Team&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>OpenBB-finance/OpenBB</title>
      <link>https://github.com/OpenBB-finance/OpenBB</link>
      <description>&lt;p&gt;Financial data platform for analysts, quants and AI agents.&lt;/p&gt;&lt;hr&gt;&lt;br /&gt; 
&lt;img src="https://github.com/OpenBB-finance/OpenBB/raw/develop/images/platform-light.svg?raw=true#gh-light-mode-only" alt="OpenBB Platform logo" width="600" /&gt; 
&lt;img src="https://github.com/OpenBB-finance/OpenBB/raw/develop/images/platform-dark.svg?raw=true#gh-dark-mode-only" alt="OpenBB Platform logo" width="600" /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;a href="https://x.com/openbb_finance"&gt;&lt;img src="https://img.shields.io/twitter/url/https/twitter.com/openbb_finance.svg?style=social&amp;amp;label=Follow%20%40openbb_finance" alt="Twitter" /&gt;&lt;/a&gt; &lt;a href="https://discord.com/invite/xPHTuHCmuV"&gt;&lt;img src="https://img.shields.io/discord/831165782750789672" alt="Discord Shield" /&gt;&lt;/a&gt; &lt;a href="https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/OpenBB-finance/OpenBB"&gt;&lt;img src="https://img.shields.io/static/v1?label=Dev%20Containers&amp;amp;message=Open&amp;amp;color=blue&amp;amp;logo=visualstudiocode" alt="Open in Dev Containers" /&gt;&lt;/a&gt; &lt;a href="https://codespaces.new/OpenBB-finance/OpenBB"&gt; &lt;img src="https://github.com/codespaces/badge.svg?sanitize=true" height="20" /&gt; &lt;/a&gt; &lt;a target="_blank" href="https://colab.research.google.com/github/OpenBB-finance/OpenBB/blob/develop/examples/googleColab.ipynb"&gt; &lt;img src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" alt="Open In Colab" /&gt; &lt;/a&gt; &lt;a href="https://pypi.org/project/openbb/"&gt;&lt;img src="https://img.shields.io/pypi/v/openbb?color=blue&amp;amp;label=PyPI%20Package" alt="PyPI" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;The first financial Platform that is open source.&lt;/p&gt; 
&lt;p&gt;The OpenBB Platform offers access to equity, options, crypto, forex, macro economy, fixed income, and more while also offering a broad range of extensions to enhance the user experience according to their needs.&lt;/p&gt; 
&lt;p&gt;Get started with: &lt;code&gt;pip install openbb&lt;/code&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from openbb import obb
output = obb.equity.price.historical("AAPL")
df = output.to_dataframe()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can sign up to the &lt;a href="https://my.openbb.co/login"&gt;OpenBB Hub&lt;/a&gt; to get the most out of the OpenBB ecosystem.&lt;/p&gt; 
&lt;p&gt;Data integrations available can be found here: &lt;a href="https://docs.openbb.co/platform/reference"&gt;https://docs.openbb.co/platform/reference&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;OpenBB Workspace&lt;/h2&gt; 
&lt;p&gt;While the OpenBB Platform is all about an integration to dozens of different data vendors, the interface is either Python or a CLI.&lt;/p&gt; 
&lt;p&gt;If you want an enterprise UI to visualize this datasets and use AI agents on top, you can find OpenBB Workspace at &lt;a href="https://pro.openbb.co"&gt;https://pro.openbb.co&lt;/a&gt;.&lt;/p&gt; 
&lt;a href="https://pro.openbb.co"&gt; 
 &lt;div align="center"&gt; 
  &lt;img src="https://openbb-cms.directus.app/assets/f69b6aaf-0821-4bc8-a43c-715e03a924ef.png" alt="Logo" width="1000" /&gt; 
 &lt;/div&gt; &lt;/a&gt; 
&lt;p&gt;Data integration:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;You can learn more about adding data to the OpenBB workspace from the &lt;a href="https://docs.openbb.co/workspace"&gt;docs&lt;/a&gt; or &lt;a href="https://github.com/OpenBB-finance/backends-for-openbb"&gt;this open source repository&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;AI Agents integration:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;You can learn more about adding AI agents to the OpenBB workspace from &lt;a href="https://github.com/OpenBB-finance/agents-for-openbb"&gt;this open source repository&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Integrating OpenBB Platform to the OpenBB Workspace&lt;/h3&gt; 
&lt;p&gt;Connect this library to the OpenBB Workspace with a few simple commands, in a Python (3.9.21 - 3.12) environment.&lt;/p&gt; 
&lt;h4&gt;Run OpenBB Platform backend&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Install the packages.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;pip install "openbb[all]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Start the API server over localhost.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;openbb-api
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will launch a FastAPI server, via Uvicorn, at &lt;code&gt;127.0.0.1:6900&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;You can check that it works by going to &lt;a href="http://127.0.0.1:6900"&gt;http://127.0.0.1:6900&lt;/a&gt;.&lt;/p&gt; 
&lt;h4&gt;Integrate OpenBB Platform backend to OpenBB Workspace&lt;/h4&gt; 
&lt;p&gt;Sign-in to the &lt;a href="https://pro.openbb.co/"&gt;OpenBB Workspace&lt;/a&gt;, and follow the following steps:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/user-attachments/assets/75cffb4a-5e95-470a-b9d0-6ffd4067e069" alt="CleanShot 2025-05-17 at 09 51 56@2x" /&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Go to the "Apps" tab&lt;/li&gt; 
 &lt;li&gt;Click on "Connect backend"&lt;/li&gt; 
 &lt;li&gt;Fill in the form with: Name: OpenBB Platform URL: &lt;a href="http://127.0.0.1:6900"&gt;http://127.0.0.1:6900&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Click on "Test". You should get a "Test successful" with the number of apps found.&lt;/li&gt; 
 &lt;li&gt;Click on "Add".&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;That's it.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;!-- TABLE OF CONTENTS --&gt; 
&lt;details closed="closed"&gt; 
 &lt;summary&gt;&lt;h2 style="display: inline-block"&gt;Table of Contents&lt;/h2&gt;&lt;/summary&gt; 
 &lt;ol&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#1-installation"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#2-contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#3-license"&gt;License&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#4-disclaimer"&gt;Disclaimer&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#5-contacts"&gt;Contacts&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#6-star-history"&gt;Star History&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/OpenBB-finance/OpenBB/develop/#7-contributors"&gt;Contributors&lt;/a&gt;&lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/details&gt; 
&lt;h2&gt;1. Installation&lt;/h2&gt; 
&lt;p&gt;The OpenBB Platform can be installed as a &lt;a href="https://pypi.org/project/openbb/"&gt;PyPI package&lt;/a&gt; by running &lt;code&gt;pip install openbb&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;or by cloning the repository directly with &lt;code&gt;git clone https://github.com/OpenBB-finance/OpenBB.git&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Please find more about the installation process, in the &lt;a href="https://docs.openbb.co/platform/installation"&gt;OpenBB Documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;OpenBB Platform CLI installation&lt;/h3&gt; 
&lt;p&gt;The OpenBB Platform CLI is a command-line interface that allows you to access the OpenBB Platform directly from your command line.&lt;/p&gt; 
&lt;p&gt;It can be installed by running &lt;code&gt;pip install openbb-cli&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;or by cloning the repository directly with &lt;code&gt;git clone https://github.com/OpenBB-finance/OpenBB.git&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Please find more about the installation process in the &lt;a href="https://docs.openbb.co/cli/installation"&gt;OpenBB Documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;2. Contributing&lt;/h2&gt; 
&lt;p&gt;There are three main ways of contributing to this project. (Hopefully you have starred the project by now â­ï¸)&lt;/p&gt; 
&lt;h3&gt;Become a Contributor&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;More information on our &lt;a href="https://docs.openbb.co/platform/developer_guide/misc/contributing"&gt;Contributing Documentation&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Create a GitHub ticket&lt;/h3&gt; 
&lt;p&gt;Before creating a ticket make sure the one you are creating doesn't exist already &lt;a href="https://github.com/OpenBB-finance/OpenBB/issues"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&amp;amp;labels=bug&amp;amp;template=bug_report.md&amp;amp;title=%5BBug%5D"&gt;Report bug&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&amp;amp;labels=enhancement&amp;amp;template=enhancement.md&amp;amp;title=%5BIMPROVE%5D"&gt;Suggest improvement&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&amp;amp;labels=new+feature&amp;amp;template=feature_request.md&amp;amp;title=%5BFR%5D"&gt;Request a feature&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Provide feedback&lt;/h3&gt; 
&lt;p&gt;We are most active on &lt;a href="https://openbb.co/discord"&gt;our Discord&lt;/a&gt;, but feel free to reach out to us in any of &lt;a href="https://openbb.co/links"&gt;our social media&lt;/a&gt; for feedback.&lt;/p&gt; 
&lt;h2&gt;3. License&lt;/h2&gt; 
&lt;p&gt;Distributed under the AGPLv3 License. See &lt;a href="https://github.com/OpenBB-finance/OpenBB/raw/main/LICENSE"&gt;LICENSE&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h2&gt;4. Disclaimer&lt;/h2&gt; 
&lt;p&gt;Trading in financial instruments involves high risks including the risk of losing some, or all, of your investment amount, and may not be suitable for all investors.&lt;/p&gt; 
&lt;p&gt;Before deciding to trade in a financial instrument you should be fully informed of the risks and costs associated with trading the financial markets, carefully consider your investment objectives, level of experience, and risk appetite, and seek professional advice where needed.&lt;/p&gt; 
&lt;p&gt;The data contained in the OpenBB Platform is not necessarily accurate.&lt;/p&gt; 
&lt;p&gt;OpenBB and any provider of the data contained in this website will not accept liability for any loss or damage as a result of your trading, or your reliance on the information displayed.&lt;/p&gt; 
&lt;p&gt;All names, logos, and brands of third parties that may be referenced in our sites, products or documentation are trademarks of their respective owners. Unless otherwise specified, OpenBB and its products and services are not endorsed by, sponsored by, or affiliated with these third parties.&lt;/p&gt; 
&lt;p&gt;Our use of these names, logos, and brands is for identification purposes only, and does not imply any such endorsement, sponsorship, or affiliation.&lt;/p&gt; 
&lt;h2&gt;5. Contacts&lt;/h2&gt; 
&lt;p&gt;If you have any questions about the platform or anything OpenBB, feel free to email us at &lt;code&gt;support@openbb.co&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;If you want to say hi, or are interested in partnering with us, feel free to reach us at &lt;code&gt;hello@openbb.co&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Any of our social media platforms: &lt;a href="https://openbb.co/links"&gt;openbb.co/links&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;6. Star History&lt;/h2&gt; 
&lt;p&gt;This is a proxy of our growth and that we are just getting started.&lt;/p&gt; 
&lt;p&gt;But for more metrics important to us check &lt;a href="https://openbb.co/open"&gt;openbb.co/open&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://api.star-history.com/svg?repos=openbb-finance/OpenBB&amp;amp;type=Date&amp;amp;theme=dark"&gt;&lt;img src="https://api.star-history.com/svg?repos=openbb-finance/OpenBB&amp;amp;type=Date&amp;amp;theme=dark" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;7. Contributors&lt;/h2&gt; 
&lt;p&gt;OpenBB wouldn't be OpenBB without you. If we are going to disrupt financial industry, every contribution counts. Thank you for being part of this journey.&lt;/p&gt; 
&lt;a href="https://github.com/OpenBB-finance/OpenBB/graphs/contributors"&gt; &lt;img src="https://contributors-img.web.app/image?repo=OpenBB-finance/OpenBB" width="800" /&gt; &lt;/a&gt; 
&lt;!-- MARKDOWN LINKS &amp; IMAGES --&gt; 
&lt;!-- https://www.markdownguide.org/basic-syntax/#reference-style-links --&gt;</description>
    </item>
    
    <item>
      <title>QwenLM/Qwen3-Coder</title>
      <link>https://github.com/QwenLM/Qwen3-Coder</link>
      <description>&lt;p&gt;Qwen3-Coder is the code version of Qwen3, the large language model series developed by Qwen team, Alibaba Cloud.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a name="readme-top"&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Coder/qwen3_coder.png" width="400" /&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p align="center"&gt; &lt;img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Coder/qwen3-coder-main.jpg" width="800" /&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p align="center"&gt; ğŸ’œ &lt;a href="https://chat.qwenlm.ai/"&gt;&lt;b&gt;Qwen Chat&lt;/b&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ğŸ¤— &lt;a href="https://huggingface.co/collections/Qwen/qwen3-coder-687fc861e53c939e52d52d10"&gt;Hugging Face&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ğŸ¤– &lt;a href="https://modelscope.cn/organization/qwen"&gt;ModelScope&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp; ğŸ“‘ &lt;a href="https://qwenlm.github.io/blog/qwen3-coder"&gt;Blog&lt;/a&gt; &amp;nbsp;&amp;nbsp; ï½œ &amp;nbsp;&amp;nbsp;ğŸ“– &lt;a href="https://qwen.readthedocs.io/"&gt;Documentation&lt;/a&gt; &lt;br /&gt; &amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp; ğŸŒ &lt;a href="https://huggingface.co/spaces/Qwen/Qwen3-Coder-WebDev"&gt;WebDev&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ğŸ’¬ &lt;a href="https://github.com/QwenLM/Qwen/raw/main/assets/wechat.png"&gt;WeChat (å¾®ä¿¡)&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ğŸ«¨ &lt;a href="https://discord.gg/CV4E9rpNSD"&gt; Discord&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp; ğŸ“„ &lt;a href="https://arxiv.org/abs/2505.09388"&gt;Arxiv&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp; ğŸ‘½ &lt;a href="https://github.com/QwenLM/qwen-code"&gt;Qwen Code&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;Visit our Hugging Face or ModelScope organization (click links above), search checkpoints with names starting with &lt;code&gt;Qwen3-Coder-&lt;/code&gt;, and you will find all you need! Enjoy!&lt;/p&gt; 
&lt;h1&gt;Latest News&lt;/h1&gt; 
&lt;p&gt;ğŸ”¥ğŸ”¥ğŸ”¥ Qwen3-Coder-30B-A3B-Instruct has been released, for more information &lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct/tree/main"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Qwen3-Coder: Agentic Coding in the World.&lt;/h1&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;Today, we're announcing Qwen3-Coder, our most agentic code model to date. &lt;strong&gt;Qwen3-Coder&lt;/strong&gt; is available in multiple sizes, but we're excited to introduce its most powerful variant first: &lt;strong&gt;Qwen3-Coder-480B-A35B-Instruct&lt;/strong&gt; â€” a 480B-parameter Mixture-of-Experts model with 35B active parameters, offering exceptional performance in both coding and agentic tasks. &lt;strong&gt;Qwen3-Coder-480B-A35B-Instruct&lt;/strong&gt; sets new state-of-the-art results among open models on Agentic Coding, Agentic Browser-Use, and Agentic Tool-Use, comparable to Claude Sonnet.&lt;/p&gt; 
&lt;p&gt;ğŸ’» &lt;strong&gt;Significant Performance&lt;/strong&gt;: among open models on &lt;strong&gt;Agentic Coding&lt;/strong&gt;, &lt;strong&gt;Agentic Browser-Use&lt;/strong&gt;, and other foundational coding tasks, achieving results comparable to Claude Sonnet;&lt;/p&gt; 
&lt;p&gt;ğŸ“š &lt;strong&gt;Long-context Capabilities&lt;/strong&gt;: with native support for &lt;strong&gt;256K&lt;/strong&gt; tokens, extendable up to &lt;strong&gt;1M&lt;/strong&gt; tokens using Yarn, optimized for repository-scale understanding;&lt;/p&gt; 
&lt;p&gt;ğŸ›  &lt;strong&gt;Agentic Coding&lt;/strong&gt;: supporting for most platform such as &lt;strong&gt;Qwen Code&lt;/strong&gt;, &lt;strong&gt;CLINE&lt;/strong&gt;, featuring a specially designed function call format;&lt;/p&gt; 
&lt;h2&gt;Basic information&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;âœ¨ Supporting long context understanding and generation with the context length of 256K tokens;&lt;/li&gt; 
 &lt;li&gt;âœ¨ Supporting 358 coding languages;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code&gt;['ABAP', 'ActionScript', 'Ada', 'Agda', 'Alloy', 'ApacheConf', 'AppleScript', 'Arc', 'Arduino', 'AsciiDoc', 'AspectJ', 'Assembly', 'Augeas', 'AutoHotkey', 'AutoIt', 'Awk', 'Batchfile', 'Befunge', 'Bison', 'BitBake', 'BlitzBasic', 'BlitzMax', 'Bluespec', 'Boo', 'Brainfuck', 'Brightscript', 'Bro', 'C', 'C#', 'C++', 'C2hs Haskell', 'CLIPS', 'CMake', 'COBOL', 'CSS', 'CSV', "Cap'n Proto", 'CartoCSS', 'Ceylon', 'Chapel', 'ChucK', 'Cirru', 'Clarion', 'Clean', 'Click', 'Clojure', 'CoffeeScript', 'ColdFusion', 'ColdFusion CFC', 'Common Lisp', 'Component Pascal', 'Coq', 'Creole', 'Crystal', 'Csound', 'Cucumber', 'Cuda', 'Cycript', 'Cython', 'D', 'DIGITAL Command Language', 'DM', 'DNS Zone', 'Darcs Patch', 'Dart', 'Diff', 'Dockerfile', 'Dogescript', 'Dylan', 'E', 'ECL', 'Eagle', 'Ecere Projects', 'Eiffel', 'Elixir', 'Elm', 'Emacs Lisp', 'EmberScript', 'Erlang', 'F#', 'FLUX', 'FORTRAN', 'Factor', 'Fancy', 'Fantom', 'Forth', 'FreeMarker', 'G-code', 'GAMS', 'GAP', 'GAS', 'GDScript', 'GLSL', 'Genshi', 'Gentoo Ebuild', 'Gentoo Eclass', 'Gettext Catalog', 'Glyph', 'Gnuplot', 'Go', 'Golo', 'Gosu', 'Grace', 'Gradle', 'Grammatical Framework', 'GraphQL', 'Graphviz (DOT)', 'Groff', 'Groovy', 'Groovy Server Pages', 'HCL', 'HLSL', 'HTML', 'HTML+Django', 'HTML+EEX', 'HTML+ERB', 'HTML+PHP', 'HTTP', 'Haml', 'Handlebars', 'Harbour', 'Haskell', 'Haxe', 'Hy', 'IDL', 'IGOR Pro', 'INI', 'IRC log', 'Idris', 'Inform 7', 'Inno Setup', 'Io', 'Ioke', 'Isabelle', 'J', 'JFlex', 'JSON', 'JSON5', 'JSONLD', 'JSONiq', 'JSX', 'Jade', 'Jasmin', 'Java', 'Java Server Pages', 'JavaScript', 'Julia', 'Jupyter Notebook', 'KRL', 'KiCad', 'Kit', 'Kotlin', 'LFE', 'LLVM', 'LOLCODE', 'LSL', 'LabVIEW', 'Lasso', 'Latte', 'Lean', 'Less', 'Lex', 'LilyPond', 'Linker Script', 'Liquid', 'Literate Agda', 'Literate CoffeeScript', 'Literate Haskell', 'LiveScript', 'Logos', 'Logtalk', 'LookML', 'Lua', 'M', 'M4', 'MAXScript', 'MTML', 'MUF', 'Makefile', 'Mako', 'Maple', 'Markdown', 'Mask', 'Mathematica', 'Matlab', 'Max', 'MediaWiki', 'Metal', 'MiniD', 'Mirah', 'Modelica', 'Module Management System', 'Monkey', 'MoonScript', 'Myghty', 'NSIS', 'NetLinx', 'NetLogo', 'Nginx', 'Nimrod', 'Ninja', 'Nit', 'Nix', 'Nu', 'NumPy', 'OCaml', 'ObjDump', 'Objective-C++', 'Objective-J', 'Octave', 'Omgrofl', 'Opa', 'Opal', 'OpenCL', 'OpenEdge ABL', 'OpenSCAD', 'Org', 'Ox', 'Oxygene', 'Oz', 'PAWN', 'PHP', 'POV-Ray SDL', 'Pan', 'Papyrus', 'Parrot', 'Parrot Assembly', 'Parrot Internal Representation', 'Pascal', 'Perl', 'Perl6', 'Pickle', 'PigLatin', 'Pike', 'Pod', 'PogoScript', 'Pony', 'PostScript', 'PowerShell', 'Processing', 'Prolog', 'Propeller Spin', 'Protocol Buffer', 'Public Key', 'Pure Data', 'PureBasic', 'PureScript', 'Python', 'Python traceback', 'QML', 'QMake', 'R', 'RAML', 'RDoc', 'REALbasic', 'RHTML', 'RMarkdown', 'Racket', 'Ragel in Ruby Host', 'Raw token data', 'Rebol', 'Red', 'Redcode', "Ren'Py", 'RenderScript', 'RobotFramework', 'Rouge', 'Ruby', 'Rust', 'SAS', 'SCSS', 'SMT', 'SPARQL', 'SQF', 'SQL', 'STON', 'SVG', 'Sage', 'SaltStack', 'Sass', 'Scala', 'Scaml', 'Scheme', 'Scilab', 'Self', 'Shell', 'ShellSession', 'Shen', 'Slash', 'Slim', 'Smali', 'Smalltalk', 'Smarty', 'Solidity', 'SourcePawn', 'Squirrel', 'Stan', 'Standard ML', 'Stata', 'Stylus', 'SuperCollider', 'Swift', 'SystemVerilog', 'TOML', 'TXL', 'Tcl', 'Tcsh', 'TeX', 'Tea', 'Text', 'Textile', 'Thrift', 'Turing', 'Turtle', 'Twig', 'TypeScript', 'Unified Parallel C', 'Unity3D Asset', 'Uno', 'UnrealScript', 'UrWeb', 'VCL', 'VHDL', 'Vala', 'Verilog', 'VimL', 'Visual Basic', 'Volt', 'Vue', 'Web Ontology Language', 'WebAssembly', 'WebIDL', 'X10', 'XC', 'XML', 'XPages', 'XProc', 'XQuery', 'XS', 'XSLT', 'Xojo', 'Xtend', 'YAML', 'YANG', 'Yacc', 'Zephir', 'Zig', 'Zimpl', 'desktop', 'eC', 'edn', 'fish', 'mupad', 'nesC', 'ooc', 'reStructuredText', 'wisp', 'xBase']
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;âœ¨ Retain strengths in math and general capabilities from base model.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Important]&lt;/p&gt; 
 &lt;p&gt;Qwen3-coder function calling relies on our new tool parser &lt;code&gt;qwen3coder_tool_parser.py&lt;/code&gt; &lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct/blob/main/qwen3coder_tool_parser.py"&gt;here&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;We updated both the special tokens and their corresponding token ids, in order to maintain consistency with Qwen3. Please make sure to use the new tokenizer.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;model name&lt;/th&gt; 
   &lt;th&gt;type&lt;/th&gt; 
   &lt;th&gt;length&lt;/th&gt; 
   &lt;th&gt;Download&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Qwen3-Coder-480B-A35B-Instruct&lt;/td&gt; 
   &lt;td&gt;instruct&lt;/td&gt; 
   &lt;td&gt;256k&lt;/td&gt; 
   &lt;td&gt;ğŸ¤— &lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct"&gt;Hugging Face&lt;/a&gt; â€¢ ğŸ¤– &lt;a href="https://modelscope.cn/models/Qwen/Qwen3-Coder-480B-A35B-Instruct"&gt;ModelScope&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Qwen3-Coder-480B-A35B-Instruct-FP8&lt;/td&gt; 
   &lt;td&gt;instruct&lt;/td&gt; 
   &lt;td&gt;256k&lt;/td&gt; 
   &lt;td&gt;ğŸ¤— &lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8"&gt;Hugging Face&lt;/a&gt; â€¢ ğŸ¤– &lt;a href="https://modelscope.cn/models/Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8"&gt;ModelScope&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Qwen3-Coder-30B-A3B-Instruct&lt;/td&gt; 
   &lt;td&gt;instruct&lt;/td&gt; 
   &lt;td&gt;256k&lt;/td&gt; 
   &lt;td&gt;ğŸ¤— &lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct"&gt;Hugging Face&lt;/a&gt; â€¢ ğŸ¤– &lt;a href="https://modelscope.cn/models/Qwen/Qwen3-Coder-30B-A3B-Instruct"&gt;ModelScope&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Qwen3-Coder-30B-A3B-Instruct-FP8&lt;/td&gt; 
   &lt;td&gt;instruct&lt;/td&gt; 
   &lt;td&gt;256k&lt;/td&gt; 
   &lt;td&gt;ğŸ¤— &lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct-FP8"&gt;Hugging Face&lt;/a&gt; â€¢ ğŸ¤– &lt;a href="https://modelscope.cn/models/Qwen/Qwen3-Coder-30B-A3B-Instruct-FP8"&gt;ModelScope&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Detailed performance and introduction are shown in this &lt;a href="https://qwenlm.github.io/blog/qwen3-coder"&gt; ğŸ“‘ blog&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Important] &lt;strong&gt;Qwen3-Coder-480B-A35B-Instruct&lt;/strong&gt; are instruction models for chatting;&lt;/p&gt; 
 &lt;p&gt;This model supports only non-thinking mode and does not generate &lt;code&gt;&amp;lt;think&amp;gt;&amp;lt;/think&amp;gt;&lt;/code&gt; blocks in its output. Meanwhile, specifying &lt;code&gt;enable_thinking=False&lt;/code&gt; is no longer required.**&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;ğŸ‘‰ğŸ» Chat with Qwen3-Coder&lt;/h3&gt; 
&lt;p&gt;You can just write several lines of code with &lt;code&gt;transformers&lt;/code&gt; to chat with Qwen3-Coder-480B-A35B-Instruct. Essentially, we build the tokenizer and the model with &lt;code&gt;from_pretrained&lt;/code&gt; method, and we use generate method to perform chatting with the help of chat template provided by the tokenizer. Below is an example of how to chat with &lt;strong&gt;Qwen3-Coder-480B-A35B-Instruct&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "Qwen/Qwen3-Coder-480B-A35B-Instruct"

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

prompt = "write a quick sort algorithm."
messages = [
    {"role": "user", "content": prompt}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=65536
)
generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]

response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The &lt;code&gt;apply_chat_template()&lt;/code&gt; function is used to convert the messages into a format that the model can understand. The &lt;code&gt;add_generation_prompt&lt;/code&gt; argument is used to add a generation prompt, which refers to &lt;code&gt;&amp;lt;|im_start|&amp;gt;assistant\n&lt;/code&gt; to the input. Notably, we apply ChatML template for chat models following our previous practice. The &lt;code&gt;max_new_tokens&lt;/code&gt; argument is used to set the maximum length of the response. The &lt;code&gt;tokenizer.batch_decode()&lt;/code&gt; function is used to decode the response. In terms of the input, the above messages is an example to show how to format your dialog history and system prompt. You can use the other size of instruct model in the same way.&lt;/p&gt; 
&lt;h4&gt;Fill in the middle with Qwen3-Coder&lt;/h4&gt; 
&lt;p&gt;The code insertion task, also referred to as the "fill-in-the-middle" challenge, requires the insertion of code segments in a manner that bridges the gaps within a given code context. For an approach aligned with best practices, we recommend adhering to the formatting guidelines outlined in the paper "Efficient Training of Language Models to Fill in the Middle"[&lt;a href="https://arxiv.org/abs/2207.14255"&gt;arxiv&lt;/a&gt;].&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!Important] It should be noted that FIM is supported in every version of Qwen3-Coder. Qwen3-Coder-480B-A35B-Instruct is shown here as an example.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;The prompt should be structured as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;prompt = '&amp;lt;|fim_prefix|&amp;gt;' + prefix_code + '&amp;lt;|fim_suffix|&amp;gt;' + suffix_code + '&amp;lt;|fim_middle|&amp;gt;'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Following the approach mentioned, an example would be structured in this manner:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from transformers import AutoTokenizer, AutoModelForCausalLM
# load model
device = "cuda" # the device to load the model onto

TOKENIZER = AutoTokenizer.from_pretrained("Qwen/Qwen3-Coder-480B-A35B-Instruct")
MODEL = AutoModelForCausalLM.from_pretrained("Qwen/Qwen3-Coder-480B-A35B-Instruct", device_map="auto").eval()


input_text = """&amp;lt;|fim_prefix|&amp;gt;def quicksort(arr):
    if len(arr) &amp;lt;= 1:
        return arr
    pivot = arr[len(arr) // 2]
    &amp;lt;|fim_suffix|&amp;gt;
    middle = [x for x in arr if x == pivot]
    right = [x for x in arr if x &amp;gt; pivot]
    return quicksort(left) + middle + quicksort(right)&amp;lt;|fim_middle|&amp;gt;"""
            
messages = [
    {"role": "system", "content": "You are a code completion assistant."},
    {"role": "user", "content": input_text}
]


text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)
model_inputs = TOKENIZER([text], return_tensors="pt").to(model.device)

# Use `max_new_tokens` to control the maximum output length.
eos_token_ids = [151659, 151661, 151662, 151663, 151664, 151643, 151645]
generated_ids = MODEL.generate(model_inputs.input_ids, max_new_tokens=512, do_sample=False, eos_token_id=eos_token_ids)[0]
# The generated_ids include prompt_ids, we only need to decode the tokens after prompt_ids.
output_text = TOKENIZER.decode(generated_ids[len(model_inputs.input_ids[0]):], skip_special_tokens=True)

print(f"Prompt: {input_text}\n\nGenerated text: {output_text}")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Use Cases&lt;/h2&gt; 
&lt;h3&gt;Example: Physics-Based Chimney Demolition Simulation with Controlled Explosion&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;Prompt with Qwen Chat Web Dev &lt;/summary&gt; 
 &lt;pre&gt;&lt;code&gt;ä½¿ç”¨ three.js, cannon-es.js ç”Ÿæˆä¸€ä¸ªéœ‡æ’¼çš„3Då»ºç­‘æ‹†é™¤æ¼”ç¤ºã€‚

## åœºæ™¯è®¾ç½®ï¼š
- åœ°é¢æ˜¯ä¸€ä¸ªæ·±ç°è‰²æ··å‡åœŸå¹³é¢ï¼Œå°ºå¯¸80*80ï¼Œ
- æ‰€æœ‰ç‰©ä½“ä¸¥æ ¼éµå¾ªç°å®ç‰©ç†è§„åˆ™ï¼ŒåŒ…æ‹¬é‡åŠ›ã€æ‘©æ“¦åŠ›ã€ç¢°æ’æ£€æµ‹å’ŒåŠ¨é‡å®ˆæ’

## å»ºç­‘ç»“æ„ï¼š
- ä¸€åº§åœ†å½¢é«˜å±‚å»ºç­‘ï¼Œå‘¨é•¿å¯¹åº”20ä¸ªæ–¹å—
- å»ºç­‘æ€»é«˜åº¦60ä¸ªæ–¹å—
- æ¯å±‚é‡‡ç”¨ç –ç Œç»“æ„ï¼Œæ–¹å—ä¸ç –ç»“æ„å»ºç­‘ä¸€è‡´, é”™å¼€50%æ’åˆ—ï¼Œå¢å¼ºç»“æ„ç¨³å®šæ€§
- å»ºç­‘å¤–å¢™ä½¿ç”¨ç±³è‰²æ–¹å—
- **é‡è¦ï¼šæ–¹å—åˆå§‹æ’åˆ—æ—¶å¿…é¡»ç¡®ä¿ç´§å¯†è´´åˆï¼Œæ— é—´éš™ï¼Œå¯ä»¥é€šè¿‡è½»å¾®é‡å æˆ–è°ƒæ•´åŠå¾„æ¥å®ç°**
- **é‡è¦ï¼šå»ºç­‘åˆå§‹åŒ–å®Œæˆåï¼Œæ‰€æœ‰æ–¹å—åº”è¯¥å¤„äºç‰©ç†"ç¡çœ "çŠ¶æ€ï¼Œç¡®ä¿å»ºç­‘åœ¨çˆ†ç‚¸å‰ä¿æŒå®Œç¾çš„é™æ­¢çŠ¶æ€ï¼Œä¸ä¼šå› é‡åŠ›è€Œä¸‹æ²‰æˆ–æ¾æ•£**
- å»ºç­‘ç –å—ä¹‹é—´ä½¿ç”¨ç²˜æ€§ææ–™å¡«å……ï¼ˆä¸å¯è§ï¼‰ï¼Œé€šè¿‡é«˜æ‘©æ“¦åŠ›ï¼ˆ0.8+ï¼‰å’Œä½å¼¹æ€§ï¼ˆ0.05ä»¥ä¸‹ï¼‰æ¥æ¨¡æ‹Ÿç²˜åˆæ•ˆæœ
- ç –å—åœ¨å»ºç­‘å€’å¡Œç¬é—´ä¸ä¼šæ•£æ‰ï¼Œè€Œæ˜¯å»ºç­‘ä½œä¸ºä¸€ä¸ªæ•´ä½“å€’åœ¨åœ°é¢çš„æ—¶å€™æ‰å› å—åŠ›è¿‡å¤§è€Œæ•£æ‰

## å®šå‘çˆ†ç ´ç³»ç»Ÿï¼š
- åœ¨å»ºç­‘çš„ç¬¬1å±‚çš„æœ€å³ä¾§æ–¹å—é™„è¿‘å®‰è£…çˆ†ç‚¸è£…ç½®ï¼ˆä¸å¯è§ï¼‰
- æä¾›æ“ä½œæŒ‰é’®ç‚¹å‡»çˆ†ç‚¸
- **çˆ†ç‚¸æ—¶å”¤é†’æ‰€æœ‰ç›¸å…³æ–¹å—çš„ç‰©ç†çŠ¶æ€**
- çˆ†ç‚¸ç‚¹äº§ç”ŸåŠå¾„2çš„å¼ºåŠ›å†²å‡»æ³¢ï¼Œå†²å‡»æ³¢å½±å“åˆ°çš„æ–¹å—, å—åˆ°2-5å•ä½çš„å†²å‡»åŠ›

## å»ºç­‘ç¨³å®šæ€§è¦æ±‚ï¼š
- **ç¡®ä¿å»ºç­‘åœ¨æœªçˆ†ç‚¸æ—¶å®Œå…¨é™æ­¢ï¼Œæ— ä»»ä½•æ™ƒåŠ¨æˆ–ä¸‹æ²‰**
- **ç‰©ç†ä¸–ç•Œåˆå§‹åŒ–åç»™å»ºç­‘å‡ ä¸ªç‰©ç†æ­¥éª¤æ¥è‡ªç„¶ç¨³å®šï¼Œæˆ–ä½¿ç”¨ç¡çœ æœºåˆ¶**
- **æ–¹å—é—´çš„æ¥è§¦ææ–™åº”å…·æœ‰é«˜æ‘©æ“¦åŠ›å’Œæä½å¼¹æ€§ï¼Œæ¨¡æ‹Ÿç –å—é—´çš„ç ‚æµ†ç²˜åˆ**

## éœ‡æ’¼çš„å€’å¡Œæ•ˆæœï¼š
- æ–¹å—åœ¨çˆ†ç‚¸å†²å‡»ä¸‹ä¸ä»…é£æ•£ï¼Œè¿˜ä¼šåœ¨ç©ºä¸­ç¿»æ»šå’Œç¢°æ’
- çƒŸå°˜ä¼šéšç€å»ºç­‘å€’å¡Œé€æ¸æ‰©æ•£ï¼Œè¥é€ çœŸå®çš„æ‹†é™¤ç°åœºæ°›å›´

## å¢å¼ºçš„è§†è§‰æ•ˆæœï¼š
- æ·»åŠ ç¯å¢ƒå…‰ç…§å˜åŒ–ï¼šçˆ†ç‚¸ç¬é—´äº®åº¦æ¿€å¢ï¼Œç„¶åè¢«çƒŸå°˜é®æŒ¡å˜æš—
- ç²’å­ç³»ç»ŸåŒ…æ‹¬ï¼šçƒŸé›¾ã€ç°å°˜

## æŠ€æœ¯è¦æ±‚ï¼š
- ç²’å­ç³»ç»Ÿç”¨äºçƒŸé›¾å’Œç°å°˜æ•ˆæœ
- æ‰€æœ‰ä»£ç é›†æˆåœ¨å•ä¸ªHTMLæ–‡ä»¶ä¸­ï¼ŒåŒ…å«å¿…è¦çš„CSSæ ·å¼
- æ·»åŠ ç®€å•çš„UIæ§åˆ¶ï¼šé‡ç½®æŒ‰é’®ã€ç›¸æœºè§’åº¦åˆ‡æ¢, çˆ†ç‚¸æŒ‰é’®, é¼ æ ‡å·¦é”®æ§åˆ¶æ‘„åƒæœºè§’åº¦ï¼Œå³é”®æ§åˆ¶æ‘„åƒæœºä½ç½®ï¼Œæ»šè½®æ§åˆ¶æ‘„åƒæœºç„¦è·
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p align="center"&gt; &lt;a href="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Coder/demo1.mp4"&gt; &lt;img src="https://raw.githubusercontent.com/QwenLM/Qwen3-Coder/main/assets/usage_demo_example1.png" width="400" /&gt; &lt;/a&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3&gt;Example: Multicolor and Interactive Animation&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;Prompt with Cline [act mode] &lt;/summary&gt; 
 &lt;pre&gt;&lt;code&gt;Create an amazing animation multicolor and interactive using p5js

use this cdn:
https://cdn.jsdelivr.net/npm/p5@1.7.0/lib/p5.min.js
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p align="center"&gt; &lt;a href="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Coder/demo2.mp4"&gt; &lt;img src="https://raw.githubusercontent.com/QwenLM/Qwen3-Coder/main/assets/usage_demo_example2.png" width="400" /&gt; &lt;/a&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3&gt;Example: 3D Google Earth&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;Prompt with Qwen Chat Web Dev &lt;/summary&gt; 
 &lt;pre&gt;&lt;code&gt;To create a 3D Google Earth, you need to load the terrain map correctly. You can use any online resource. The code is written into an HTML file.
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p align="center"&gt; &lt;a href="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Coder/demo3.mp4"&gt; &lt;img src="https://raw.githubusercontent.com/QwenLM/Qwen3-Coder/main/assets/usage_demo_example3.png" width="400" /&gt; &lt;/a&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3&gt;Example: Testing Your WPM with a Famous Quote&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt; Prompt with Qwen-Code CLI &lt;/summary&gt; 
 &lt;pre&gt;&lt;code&gt;Create an interesting typing game with a keyboard in the lower middle of the screen and some famous articles in the upper middle. When the user types a word correctly, a cool reaction should be given to encourage him. Design a modern soft color scheme inspired by macarons. Come up with a very creative solution first, and then start writing code.
The game should be able to support typing, and you need to neglect upcase and lowercase.
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p align="center"&gt; &lt;a href="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Coder/demo4.mp4"&gt; &lt;img src="https://raw.githubusercontent.com/QwenLM/Qwen3-Coder/main/assets/usage_demo_example4.png" width="400" /&gt; &lt;/a&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3&gt;Example: Bouncing Ball in Rotation Hypercube&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt; Prompt with Qwen Chat Web Dev &lt;/summary&gt; 
 &lt;pre&gt;&lt;code&gt;Make a page in HTML that shows an animation of a ball bouncing in a rotating hypercube
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p align="center"&gt; &lt;a href="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Coder/demo5.mp4"&gt; &lt;img src="https://raw.githubusercontent.com/QwenLM/Qwen3-Coder/main/assets/usage_demo_example5.png" width="400" /&gt; &lt;/a&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3&gt;Example: Solar System Simulation&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt; Prompt with Cline [act mode] &lt;/summary&gt; 
 &lt;pre&gt;&lt;code&gt;write a web page to show the solar system simulation
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p align="center"&gt; &lt;a href="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Coder/demo6.mp4"&gt; &lt;img src="https://raw.githubusercontent.com/QwenLM/Qwen3-Coder/main/assets/usage_demo_example6.png" width="400" /&gt; &lt;/a&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3&gt;Example: DUET Game&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt; Prompt with Cline [act mode] &lt;/summary&gt; 
 &lt;pre&gt;&lt;code&gt;Create a complete, single-file HTML game with CSS and JavaScript. The game is inspired by "Duet".

Gameplay:

There are two balls, one red and one blue, rotating around a central point.
The player uses the 'A' and 'D' keys to rotate them counter-clockwise and clockwise.
White rectangular obstacles move down from the top of the screen.
The player must rotate the balls to avoid hitting the obstacles.
If a ball hits an obstacle, the game is over.
Visuals:

Make the visual effects amazing.
Use a dark background with neon glowing effects for the balls and obstacles.
Animations should be very smooth.
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;p align="center"&gt; &lt;a href="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Coder/demo7.mp4"&gt; &lt;img src="https://raw.githubusercontent.com/QwenLM/Qwen3-Coder/main/assets/usage_demo_example7.png" width="400" /&gt; &lt;/a&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#QwenLM/Qwen3-Coder&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=QwenLM/Qwen3-Coder&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you find our work helpful, feel free to give us a cite.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{qwen3technicalreport,
      title={Qwen3 Technical Report}, 
      author={Qwen Team},
      year={2025},
      eprint={2505.09388},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2505.09388},
}
@article{hui2024qwen2,
  title={Qwen2. 5-Coder Technical Report},
  author={Hui, Binyuan and Yang, Jian and Cui, Zeyu and Yang, Jiaxi and Liu, Dayiheng and Zhang, Lei and Liu, Tianyu and Zhang, Jiajun and Yu, Bowen and Dang, Kai and others},
  journal={arXiv preprint arXiv:2409.12186},
  year={2024}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Contact Us&lt;/h2&gt; 
&lt;p&gt;If you are interested to leave a message to either our research team or product team, join our &lt;a href="https://discord.gg/z3GAxXZ9Ce"&gt;Discord&lt;/a&gt; or &lt;a href="https://github.com/QwenLM/Qwen/raw/main/assets/wechat.png"&gt;WeChat groups&lt;/a&gt;!&lt;/p&gt; 
&lt;p align="right" style="font-size: 14px; color: #555; margin-top: 20px;"&gt; &lt;a href="https://raw.githubusercontent.com/QwenLM/Qwen3-Coder/main/#readme-top" style="text-decoration: none; color: #007bff; font-weight: bold;"&gt; â†‘ Back to Top â†‘ &lt;/a&gt; &lt;/p&gt;</description>
    </item>
    
    <item>
      <title>coleam00/Archon</title>
      <link>https://github.com/coleam00/Archon</link>
      <description>&lt;p&gt;Beta release of Archon OS - the knowledge and task management backbone for AI coding assistants.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/coleam00/Archon/main/archon-ui-main/public/archon-main-graphic.png" alt="Archon Main Graphic" width="853" height="422" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;em&gt;Power up your AI coding assistants with your own custom knowledge base and task management as an MCP server&lt;/em&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://raw.githubusercontent.com/coleam00/Archon/main/#quick-start"&gt;Quick Start&lt;/a&gt; â€¢ &lt;a href="https://raw.githubusercontent.com/coleam00/Archon/main/#whats-included"&gt;What's Included&lt;/a&gt; â€¢ &lt;a href="https://raw.githubusercontent.com/coleam00/Archon/main/#architecture"&gt;Architecture&lt;/a&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ğŸ¯ What is Archon?&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Archon is currently in beta! Expect things to not work 100%, and please feel free to share any feedback and contribute with fixes/new features! Thank you to everyone for all the excitement we have for Archon already, as well as the bug reports, PRs, and discussions. It's a lot for our small team to get through but we're committed to addressing everything and making Archon into the best tool it possibly can be!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Archon is the &lt;strong&gt;command center&lt;/strong&gt; for AI coding assistants. For you, it's a sleek interface to manage knowledge, context, and tasks for your projects. For the AI coding assistant(s), it's a &lt;strong&gt;Model Context Protocol (MCP) server&lt;/strong&gt; to collaborate on and leverage the same knowledge, context, and tasks. Connect Claude Code, Kiro, Cursor, Windsurf, etc. to give your AI agents access to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Your documentation&lt;/strong&gt; (crawled websites, uploaded PDFs/docs)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Smart search capabilities&lt;/strong&gt; with advanced RAG strategies&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Task management&lt;/strong&gt; integrated with your knowledge base&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Real-time updates&lt;/strong&gt; as you add new content and collaborate with your coding assistant on tasks&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Much more&lt;/strong&gt; coming soon to build Archon into an integrated environment for all context engineering&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This new vision for Archon replaces the old one (the agenteer). Archon used to be the AI agent that builds other agents, and now you can use Archon to do that and more.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;It doesn't matter what you're building or if it's a new/existing codebase - Archon's knowledge and task management capabilities will improve the output of &lt;strong&gt;any&lt;/strong&gt; AI driven coding.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;ğŸ”— Important Links&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/coleam00/Archon/discussions"&gt;GitHub Discussions&lt;/a&gt;&lt;/strong&gt; - Join the conversation and share ideas about Archon&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/coleam00/Archon/main/CONTRIBUTING.md"&gt;Contributing Guide&lt;/a&gt;&lt;/strong&gt; - How to get involved and contribute to Archon&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://youtu.be/8pRc_s2VQIo"&gt;Introduction Video&lt;/a&gt;&lt;/strong&gt; - Getting started guide and vision for Archon&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/users/coleam00/projects/1"&gt;Archon Kanban Board&lt;/a&gt;&lt;/strong&gt; - Where maintainers are managing issues/features&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://dynamous.ai"&gt;Dynamous AI Mastery&lt;/a&gt;&lt;/strong&gt; - The birthplace of Archon - come join a vibrant community of other early AI adopters all helping each other transform their careers and businesses!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.docker.com/products/docker-desktop/"&gt;Docker Desktop&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://nodejs.org/"&gt;Node.js 18+&lt;/a&gt; (for hybrid development mode)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://supabase.com/"&gt;Supabase&lt;/a&gt; account (free tier or local Supabase both work)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://platform.openai.com/api-keys"&gt;OpenAI API key&lt;/a&gt; (Gemini and Ollama are supported too!)&lt;/li&gt; 
 &lt;li&gt;(OPTIONAL) &lt;a href="https://www.gnu.org/software/make/"&gt;Make&lt;/a&gt; (see &lt;a href="https://raw.githubusercontent.com/coleam00/Archon/main/#installing-make"&gt;Installing Make&lt;/a&gt; below)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Setup Instructions&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Clone Repository&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/coleam00/archon.git
&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cd archon
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Environment Configuration&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cp .env.example .env
# Edit .env and add your Supabase credentials:
# SUPABASE_URL=https://your-project.supabase.co
# SUPABASE_SERVICE_KEY=your-service-key-here
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;NOTE: Supabase introduced a new type of service key but use the legacy one (the longer one).&lt;/p&gt; &lt;p&gt;OPTIONAL: If you want to enable the reranking RAG strategy, uncomment lines 20-22 in &lt;code&gt;python\requirements.server.txt&lt;/code&gt;. This will significantly increase the size of the Archon Server container which is why it's off by default.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Database Setup&lt;/strong&gt;: In your &lt;a href="https://supabase.com/dashboard"&gt;Supabase project&lt;/a&gt; SQL Editor, copy, paste, and execute the contents of &lt;code&gt;migration/complete_setup.sql&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Start Services&lt;/strong&gt; (choose one):&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Full Docker Mode (Recommended for Normal Archon Usage)&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;docker compose --profile full up --build -d
# or
make dev-docker # (Alternative: Requires make installed )
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This starts all core microservices in Docker:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Server&lt;/strong&gt;: Core API and business logic (Port: 8181)&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;MCP Server&lt;/strong&gt;: Protocol interface for AI clients (Port: 8051)&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Agents (coming soon!)&lt;/strong&gt;: AI operations and streaming (Port: 8052)&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;UI&lt;/strong&gt;: Web interface (Port: 3737)&lt;/li&gt; 
  &lt;/ul&gt; &lt;p&gt;Ports are configurable in your .env as well!&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Configure API Keys&lt;/strong&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Open &lt;a href="http://localhost:3737"&gt;http://localhost:3737&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Go to &lt;strong&gt;Settings&lt;/strong&gt; â†’ Select your LLM/embedding provider and set the API key (OpenAI is default)&lt;/li&gt; 
   &lt;li&gt;Test by uploading a document or crawling a website&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;ğŸš€ Quick Command Reference&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Command&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;make dev&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Start hybrid dev (backend in Docker, frontend local) â­&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;make dev-docker&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Everything in Docker&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;make stop&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Stop all services&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;make test&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Run all tests&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;make lint&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Run linters&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;make install&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Install dependencies&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;make check&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Check environment setup&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;make clean&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Remove containers and volumes (with confirmation)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;ğŸ”„ Database Reset (Start Fresh if Needed)&lt;/h2&gt; 
&lt;p&gt;If you need to completely reset your database and start fresh:&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;âš ï¸ &lt;strong&gt;Reset Database - This will delete ALL data for Archon!&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ol&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Run Reset Script&lt;/strong&gt;: In your Supabase SQL Editor, run the contents of &lt;code&gt;migration/RESET_DB.sql&lt;/code&gt;&lt;/p&gt; &lt;p&gt;âš ï¸ WARNING: This will delete all Archon specific tables and data! Nothing else will be touched in your DB though.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Rebuild Database&lt;/strong&gt;: After reset, run &lt;code&gt;migration/complete_setup.sql&lt;/code&gt; to create all the tables again.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Restart Services&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;docker compose --profile full up -d
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Reconfigure&lt;/strong&gt;:&lt;/p&gt; 
   &lt;ul&gt; 
    &lt;li&gt;Select your LLM/embedding provider and set the API key again&lt;/li&gt; 
    &lt;li&gt;Re-upload any documents or re-crawl websites&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;The reset script safely removes all tables, functions, triggers, and policies with proper dependency handling.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;ğŸ› ï¸ Installing Make (OPTIONAL)&lt;/h2&gt; 
&lt;p&gt;Make is required for the local development workflow. Installation varies by platform:&lt;/p&gt; 
&lt;h3&gt;Windows&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Option 1: Using Chocolatey
choco install make

# Option 2: Using Scoop
scoop install make

# Option 3: Using WSL2
wsl --install
# Then in WSL: sudo apt-get install make
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;macOS&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Make comes pre-installed on macOS
# If needed: brew install make
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Linux&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Debian/Ubuntu
sudo apt-get install make

# RHEL/CentOS/Fedora
sudo yum install make
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;âš¡ Quick Test&lt;/h2&gt; 
&lt;p&gt;Once everything is running:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Test Web Crawling&lt;/strong&gt;: Go to &lt;a href="http://localhost:3737"&gt;http://localhost:3737&lt;/a&gt; â†’ Knowledge Base â†’ "Crawl Website" â†’ Enter a doc URL (such as &lt;a href="https://ai.pydantic.dev/llms-full.txt"&gt;https://ai.pydantic.dev/llms-full.txt&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Test Document Upload&lt;/strong&gt;: Knowledge Base â†’ Upload a PDF&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Test Projects&lt;/strong&gt;: Projects â†’ Create a new project and add tasks&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Integrate with your AI coding assistant&lt;/strong&gt;: MCP Dashboard â†’ Copy connection config for your AI coding assistant&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;ğŸ“š Documentation&lt;/h2&gt; 
&lt;h3&gt;Core Services&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Service&lt;/th&gt; 
   &lt;th&gt;Container Name&lt;/th&gt; 
   &lt;th&gt;Default URL&lt;/th&gt; 
   &lt;th&gt;Purpose&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Web Interface&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;archon-ui&lt;/td&gt; 
   &lt;td&gt;&lt;a href="http://localhost:3737"&gt;http://localhost:3737&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Main dashboard and controls&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;API Service&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;archon-server&lt;/td&gt; 
   &lt;td&gt;&lt;a href="http://localhost:8181"&gt;http://localhost:8181&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Web crawling, document processing&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;MCP Server&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;archon-mcp&lt;/td&gt; 
   &lt;td&gt;&lt;a href="http://localhost:8051"&gt;http://localhost:8051&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Model Context Protocol interface&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Agents Service&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;archon-agents&lt;/td&gt; 
   &lt;td&gt;&lt;a href="http://localhost:8052"&gt;http://localhost:8052&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;AI/ML operations, reranking&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;What's Included&lt;/h2&gt; 
&lt;h3&gt;ğŸ§  Knowledge Management&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Smart Web Crawling&lt;/strong&gt;: Automatically detects and crawls entire documentation sites, sitemaps, and individual pages&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Document Processing&lt;/strong&gt;: Upload and process PDFs, Word docs, markdown files, and text documents with intelligent chunking&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Code Example Extraction&lt;/strong&gt;: Automatically identifies and indexes code examples from documentation for enhanced search&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Vector Search&lt;/strong&gt;: Advanced semantic search with contextual embeddings for precise knowledge retrieval&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Source Management&lt;/strong&gt;: Organize knowledge by source, type, and tags for easy filtering&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ¤– AI Integration&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Model Context Protocol (MCP)&lt;/strong&gt;: Connect any MCP-compatible client (Claude Code, Cursor, even non-AI coding assistants like Claude Desktop)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;10 MCP Tools&lt;/strong&gt;: Comprehensive yet simple set of tools for RAG queries, task management, and project operations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-LLM Support&lt;/strong&gt;: Works with OpenAI, Ollama, and Google Gemini models&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;RAG Strategies&lt;/strong&gt;: Hybrid search, contextual embeddings, and result reranking for optimal AI responses&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Real-time Streaming&lt;/strong&gt;: Live responses from AI agents with progress tracking&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ“‹ Project &amp;amp; Task Management&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Hierarchical Projects&lt;/strong&gt;: Organize work with projects, features, and tasks in a structured workflow&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;AI-Assisted Creation&lt;/strong&gt;: Generate project requirements and tasks using integrated AI agents&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Document Management&lt;/strong&gt;: Version-controlled documents with collaborative editing capabilities&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Progress Tracking&lt;/strong&gt;: Real-time updates and status management across all project activities&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ğŸ”„ Real-time Collaboration&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;WebSocket Updates&lt;/strong&gt;: Live progress tracking for crawling, processing, and AI operations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-user Support&lt;/strong&gt;: Collaborative knowledge building and project management&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Background Processing&lt;/strong&gt;: Asynchronous operations that don't block the user interface&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Health Monitoring&lt;/strong&gt;: Built-in service health checks and automatic reconnection&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Architecture&lt;/h2&gt; 
&lt;h3&gt;Microservices Structure&lt;/h3&gt; 
&lt;p&gt;Archon uses true microservices architecture with clear separation of concerns:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend UI   â”‚    â”‚  Server (API)   â”‚    â”‚   MCP Server    â”‚    â”‚ Agents Service  â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚  React + Vite   â”‚â—„â”€â”€â–ºâ”‚    FastAPI +    â”‚â—„â”€â”€â–ºâ”‚    Lightweight  â”‚â—„â”€â”€â–ºâ”‚   PydanticAI    â”‚
â”‚  Port 3737      â”‚    â”‚    SocketIO     â”‚    â”‚    HTTP Wrapper â”‚    â”‚   Port 8052     â”‚
â”‚                 â”‚    â”‚    Port 8181    â”‚    â”‚    Port 8051    â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                        â”‚                        â”‚                        â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚                        â”‚
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
                         â”‚    Database     â”‚               â”‚
                         â”‚                 â”‚               â”‚
                         â”‚    Supabase     â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚    PostgreSQL   â”‚
                         â”‚    PGVector     â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Service Responsibilities&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Service&lt;/th&gt; 
   &lt;th&gt;Location&lt;/th&gt; 
   &lt;th&gt;Purpose&lt;/th&gt; 
   &lt;th&gt;Key Features&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Frontend&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;archon-ui-main/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Web interface and dashboard&lt;/td&gt; 
   &lt;td&gt;React, TypeScript, TailwindCSS, Socket.IO client&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Server&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;python/src/server/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Core business logic and APIs&lt;/td&gt; 
   &lt;td&gt;FastAPI, service layer, Socket.IO broadcasts, all ML/AI operations&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;MCP Server&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;python/src/mcp/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;MCP protocol interface&lt;/td&gt; 
   &lt;td&gt;Lightweight HTTP wrapper, 10 MCP tools, session management&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Agents&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;python/src/agents/&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;PydanticAI agent hosting&lt;/td&gt; 
   &lt;td&gt;Document and RAG agents, streaming responses&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Communication Patterns&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;HTTP-based&lt;/strong&gt;: All inter-service communication uses HTTP APIs&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Socket.IO&lt;/strong&gt;: Real-time updates from Server to Frontend&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MCP Protocol&lt;/strong&gt;: AI clients connect to MCP Server via SSE or stdio&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;No Direct Imports&lt;/strong&gt;: Services are truly independent with no shared code dependencies&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Key Architectural Benefits&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Lightweight Containers&lt;/strong&gt;: Each service contains only required dependencies&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Independent Scaling&lt;/strong&gt;: Services can be scaled independently based on load&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Development Flexibility&lt;/strong&gt;: Teams can work on different services without conflicts&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Technology Diversity&lt;/strong&gt;: Each service uses the best tools for its specific purpose&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ”§ Configuring Custom Ports &amp;amp; Hostname&lt;/h2&gt; 
&lt;p&gt;By default, Archon services run on the following ports:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;archon-ui&lt;/strong&gt;: 3737&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;archon-server&lt;/strong&gt;: 8181&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;archon-mcp&lt;/strong&gt;: 8051&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;archon-agents&lt;/strong&gt;: 8052&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;archon-docs&lt;/strong&gt;: 3838 (optional)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Changing Ports&lt;/h3&gt; 
&lt;p&gt;To use custom ports, add these variables to your &lt;code&gt;.env&lt;/code&gt; file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Service Ports Configuration
ARCHON_UI_PORT=3737
ARCHON_SERVER_PORT=8181
ARCHON_MCP_PORT=8051
ARCHON_AGENTS_PORT=8052
ARCHON_DOCS_PORT=3838
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Example: Running on different ports:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;ARCHON_SERVER_PORT=8282
ARCHON_MCP_PORT=8151
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Configuring Hostname&lt;/h3&gt; 
&lt;p&gt;By default, Archon uses &lt;code&gt;localhost&lt;/code&gt; as the hostname. You can configure a custom hostname or IP address by setting the &lt;code&gt;HOST&lt;/code&gt; variable in your &lt;code&gt;.env&lt;/code&gt; file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Hostname Configuration
HOST=localhost  # Default

# Examples of custom hostnames:
HOST=192.168.1.100     # Use specific IP address
HOST=archon.local      # Use custom domain
HOST=myserver.com      # Use public domain
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This is useful when:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Running Archon on a different machine and accessing it remotely&lt;/li&gt; 
 &lt;li&gt;Using a custom domain name for your installation&lt;/li&gt; 
 &lt;li&gt;Deploying in a network environment where &lt;code&gt;localhost&lt;/code&gt; isn't accessible&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;After changing hostname or ports:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Restart Docker containers: &lt;code&gt;docker compose down &amp;amp;&amp;amp; docker compose --profile full up -d&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Access the UI at: &lt;code&gt;http://${HOST}:${ARCHON_UI_PORT}&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Update your AI client configuration with the new hostname and MCP port&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;ğŸ”§ Development&lt;/h2&gt; 
&lt;h3&gt;Quick Start&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install dependencies
make install

# Start development (recommended)
make dev        # Backend in Docker, frontend local with hot reload

# Alternative: Everything in Docker
make dev-docker # All services in Docker

# Stop everything (local FE needs to be stopped manually)
make stop
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Development Modes&lt;/h3&gt; 
&lt;h4&gt;Hybrid Mode (Recommended) - &lt;code&gt;make dev&lt;/code&gt;&lt;/h4&gt; 
&lt;p&gt;Best for active development with instant frontend updates:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Backend services run in Docker (isolated, consistent)&lt;/li&gt; 
 &lt;li&gt;Frontend runs locally with hot module replacement&lt;/li&gt; 
 &lt;li&gt;Instant UI updates without Docker rebuilds&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Full Docker Mode - &lt;code&gt;make dev-docker&lt;/code&gt;&lt;/h4&gt; 
&lt;p&gt;For all services in Docker environment:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;All services run in Docker containers&lt;/li&gt; 
 &lt;li&gt;Better for integration testing&lt;/li&gt; 
 &lt;li&gt;Slower frontend updates&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Testing &amp;amp; Code Quality&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Run tests
make test       # Run all tests
make test-fe    # Run frontend tests
make test-be    # Run backend tests

# Run linters
make lint       # Lint all code
make lint-fe    # Lint frontend code
make lint-be    # Lint backend code

# Check environment
make check      # Verify environment setup

# Clean up
make clean      # Remove containers and volumes (asks for confirmation)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Viewing Logs&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# View logs using Docker Compose directly
docker compose logs -f              # All services
docker compose logs -f archon-server # API server
docker compose logs -f archon-mcp    # MCP server
docker compose logs -f archon-ui     # Frontend
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: The backend services are configured with &lt;code&gt;--reload&lt;/code&gt; flag in their uvicorn commands and have source code mounted as volumes for automatic hot reloading when you make changes.&lt;/p&gt; 
&lt;h2&gt;ğŸ” Troubleshooting&lt;/h2&gt; 
&lt;h3&gt;Common Issues and Solutions&lt;/h3&gt; 
&lt;h4&gt;Port Conflicts&lt;/h4&gt; 
&lt;p&gt;If you see "Port already in use" errors:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Check what's using a port (e.g., 3737)
lsof -i :3737

# Stop all containers and local services
make stop

# Change the port in .env
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Docker Permission Issues (Linux)&lt;/h4&gt; 
&lt;p&gt;If you encounter permission errors with Docker:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Add your user to the docker group
sudo usermod -aG docker $USER

# Log out and back in, or run
newgrp docker
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Windows-Specific Issues&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Make not found&lt;/strong&gt;: Install Make via Chocolatey, Scoop, or WSL2 (see &lt;a href="https://raw.githubusercontent.com/coleam00/Archon/main/#installing-make"&gt;Installing Make&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Line ending issues&lt;/strong&gt;: Configure Git to use LF endings: &lt;pre&gt;&lt;code class="language-bash"&gt;git config --global core.autocrlf false
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Frontend Can't Connect to Backend&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Check backend is running: &lt;code&gt;curl http://localhost:8181/health&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Verify port configuration in &lt;code&gt;.env&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;For custom ports, ensure both &lt;code&gt;ARCHON_SERVER_PORT&lt;/code&gt; and &lt;code&gt;VITE_ARCHON_SERVER_PORT&lt;/code&gt; are set&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Docker Compose Hangs&lt;/h4&gt; 
&lt;p&gt;If &lt;code&gt;docker compose&lt;/code&gt; commands hang:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Reset Docker Compose
docker compose down --remove-orphans
docker system prune -f

# Restart Docker Desktop (if applicable)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Hot Reload Not Working&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Frontend&lt;/strong&gt;: Ensure you're running in hybrid mode (&lt;code&gt;make dev&lt;/code&gt;) for best HMR experience&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Backend&lt;/strong&gt;: Check that volumes are mounted correctly in &lt;code&gt;docker-compose.yml&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;File permissions&lt;/strong&gt;: On some systems, mounted volumes may have permission issues&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ“ˆ Progress&lt;/h2&gt; 
&lt;p align="center"&gt; &lt;a href="https://star-history.com/#coleam00/Archon&amp;amp;Date"&gt; &lt;img src="https://api.star-history.com/svg?repos=coleam00/Archon&amp;amp;type=Date" width="500" alt="Star History Chart" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;ğŸ“„ License&lt;/h2&gt; 
&lt;p&gt;Archon Community License (ACL) v1.2 - see &lt;a href="https://raw.githubusercontent.com/coleam00/Archon/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: Archon is free, open, and hackable. Run it, fork it, share it - just don't sell it as-a-service without permission.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>moeru-ai/airi</title>
      <link>https://github.com/moeru-ai/airi</link>
      <description>&lt;p&gt;ğŸ’–ğŸ§¸ Self hosted, you owned Grok Companion, a container of souls of waifu, cyber livings to bring them into our worlds, wishing to achieve Neuro-sama's altitude. Capable of realtime voice chat, Minecraft, Factorio playing. Web / macOS / Windows supported.&lt;/p&gt;&lt;hr&gt;&lt;picture&gt; 
 &lt;source width="100%" srcset="./docs/content/public/banner-dark-1280x640.avif" media="(prefers-color-scheme: dark)" /&gt; 
 &lt;source width="100%" srcset="./docs/content/public/banner-light-1280x640.avif" media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)" /&gt; 
 &lt;img width="250" src="https://raw.githubusercontent.com/moeru-ai/airi/main/docs/content/public/banner-light-1280x640.avif" /&gt; 
&lt;/picture&gt; 
&lt;h1 align="center"&gt;Project AIRI&lt;/h1&gt; 
&lt;p align="center"&gt;Re-creating Neuro-sama, a container of souls of AI waifu / virtual characters to bring them into our worlds.&lt;/p&gt; 
&lt;p align="center"&gt; [&lt;a href="https://discord.gg/TgQ3Cu2F7A"&gt;Join Discord Server&lt;/a&gt;] [&lt;a href="https:///airi.moeru.ai"&gt;Try it&lt;/a&gt;] [&lt;a href="https://github.com/moeru-ai/airi/raw/main/docs/README.zh-CN.md"&gt;ç®€ä½“ä¸­æ–‡&lt;/a&gt;] [&lt;a href="https://github.com/moeru-ai/airi/raw/main/docs/README.ja-JP.md"&gt;æ—¥æœ¬èª&lt;/a&gt;] &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://deepwiki.com/moeru-ai/airi"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://github.com/moeru-ai/airi/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/moeru-ai/airi.svg?style=flat&amp;amp;colorA=080f12&amp;amp;colorB=1fa669" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/TgQ3Cu2F7A"&gt;&lt;img src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fdiscord.com%2Fapi%2Finvites%2FTgQ3Cu2F7A%3Fwith_counts%3Dtrue&amp;amp;query=%24.approximate_member_count&amp;amp;suffix=%20members&amp;amp;logo=discord&amp;amp;logoColor=white&amp;amp;label=%20&amp;amp;color=7389D8&amp;amp;labelColor=6A7EC2" /&gt;&lt;/a&gt; &lt;a href="https://x.com/proj_airi"&gt;&lt;img src="https://img.shields.io/badge/%40proj__airi-black?style=flat&amp;amp;logo=x&amp;amp;labelColor=%23101419&amp;amp;color=%232d2e30" /&gt;&lt;/a&gt; &lt;a href="https://t.me/+7M_ZKO3zUHFlOThh"&gt;&lt;img src="https://img.shields.io/badge/Telegram-%235AA9E6?logo=telegram&amp;amp;labelColor=FFFFFF" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://www.producthunt.com/products/airi?embed=true&amp;amp;utm_source=badge-featured&amp;amp;utm_medium=badge&amp;amp;utm_source=badge-airi" target="_blank"&gt;&lt;img src="https://api.producthunt.com/widgets/embed-image/v1/featured.svg?post_id=993524&amp;amp;theme=neutral&amp;amp;t=1752696535380" alt="AIRI - A container of cyber living souls, re-creation of Neuro-sama | Product Hunt" style="width: 250px; height: 54px;" width="250" height="54" /&gt;&lt;/a&gt; &lt;a href="https://trendshift.io/repositories/14636" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/14636" alt="moeru-ai%2Fairi | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Heavily inspired by &lt;a href="https://www.youtube.com/@Neurosama"&gt;Neuro-sama&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING] &lt;strong&gt;Attention:&lt;/strong&gt; We &lt;strong&gt;do not&lt;/strong&gt; have any officially minted cryptocurrency or token associated with this project. Please check the information and proceed with caution.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;/p&gt; 
 &lt;p&gt;We got a whole dedicated organization &lt;a href="https://github.com/proj-airi"&gt;@proj-airi&lt;/a&gt; for all the sub-project that born from Project AIRI, check it out!&lt;/p&gt; 
 &lt;p&gt;RAG, memory system, embedded database, icons, Live2D utilities, and more!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Have you dreamed about having a cyber living being (cyber waifu / husbando, digital pet), or digital companion that could play with and talk to you?&lt;/p&gt; 
&lt;p&gt;With the power of modern large language models like &lt;a href="https://chatgpt.com"&gt;ChatGPT&lt;/a&gt;, and famous &lt;a href="https://claude.ai"&gt;Claude&lt;/a&gt;, asking a virtual being able to have role playing and chat with us is already easy enough for everyone. Platforms like &lt;a href="https://character.ai"&gt;Character.ai (a.k.a. c.ai)&lt;/a&gt; and &lt;a href="https://janitorai.com/"&gt;JanitorAI&lt;/a&gt;, and local playgrounds like &lt;a href="https://github.com/SillyTavern/SillyTavern"&gt;SillyTavern&lt;/a&gt; is already a well-enough solution for chat based, or visual adventure game like experience.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;But, what about the abilities to play games? And see what you are coding at? Chatting while playing games, watching videos, and capable of doing many other things.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Perhaps you know &lt;a href="https://www.youtube.com/@Neurosama"&gt;Neuro-sama&lt;/a&gt; already, she is currently the best companion capable of playing games, chatting, and interacting with you and the participants (in VTuber community), some call this kind of being, "digital human" too. &lt;strong&gt;Sadly, it's not open sourced, you cannot interact with her after she went offline from live stream&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;Therefore, this project, AIRI, offers another possibility here: &lt;strong&gt;let you own your digital life, cyber living, easily, anywhere, anytime&lt;/strong&gt;.&lt;/p&gt; 
&lt;h2&gt;DevLogs we posted &amp;amp; Recent updates&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://airi.moeru.ai/docs/blog/DevLog-2025.07.18/"&gt;DevLog @ 2025.07.18&lt;/a&gt; on July 18, 2025&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://airi.moeru.ai/docs/blog/dreamlog-0x1/"&gt;DreamLog 0x1&lt;/a&gt; on June 16, 2025&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://airi.moeru.ai/docs/blog/DevLog-2025.06.08/"&gt;DevLog @ 2025.06.08&lt;/a&gt; on June 8, 2025&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://airi.moeru.ai/docs/blog/DevLog-2025.05.16/"&gt;DevLog @ 2025.05.16&lt;/a&gt; on May 16, 2025&lt;/li&gt; 
 &lt;li&gt;...more on &lt;a href="https://airi.moeru.ai/docs"&gt;documentation site&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;What's so special for this project?&lt;/h2&gt; 
&lt;p&gt;Unlike the other AI driven VTuber open source projects, ã‚¢ã‚¤ãƒª VTuber was built with many support of Web technologies such as &lt;a href="https://www.w3.org/TR/webgpu/"&gt;WebGPU&lt;/a&gt;, &lt;a href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API"&gt;WebAudio&lt;/a&gt;, &lt;a href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Workers_API/Using_web_workers"&gt;Web Workers&lt;/a&gt;, &lt;a href="https://webassembly.org/"&gt;WebAssembly&lt;/a&gt;, &lt;a href="https://developer.mozilla.org/en-US/docs/Web/API/WebSocket"&gt;WebSocket&lt;/a&gt;, etc. from the first day.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] Worry about the performance drop since we are using Web related technologies?&lt;/p&gt; 
 &lt;p&gt;Don't worry, while Web browser version meant to give a insight about how much we can push and do inside browsers, and webviews, we will never fully rely on this, the desktop version of AIRI is capable of using native &lt;a href="https://developer.nvidia.com/cuda-toolkit"&gt;NVIDIA CUDA&lt;/a&gt; and &lt;a href="https://developer.apple.com/metal/"&gt;Apple Metal&lt;/a&gt; by default (thanks to HuggingFace &amp;amp; beloved &lt;a href="https://github.com/huggingface/candle"&gt;candle&lt;/a&gt; project), without any complex dependency managements, considering the tradeoff, it was partially powered by Web technologies for graphics, layouts, animations, and the WIP plugin systems for everyone to integrate things.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;This means that &lt;strong&gt;ã‚¢ã‚¤ãƒª VTuber is capable to run on modern browsers and devices&lt;/strong&gt;, and even on mobile devices (already done with PWA support), this brought a lot of possibilities for us (the developers) to build and extend the power of ã‚¢ã‚¤ãƒª VTuber to the next level, while still left the flexibilities for users to enable features that requires TCP connections or other non-Web technologies such as connect to voice channel to Discord, or playing Minecraft, Factorio with you and your friends.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;/p&gt; 
 &lt;p&gt;We are still in the early stage of development where we are seeking out talented developers to join us and help us to make ã‚¢ã‚¤ãƒª VTuber a reality.&lt;/p&gt; 
 &lt;p&gt;It's ok if you are not familiar with Vue.js, TypeScript, and devtools that required for this project, you can join us as an artist, designer, or even help us to launch our first live stream.&lt;/p&gt; 
 &lt;p&gt;Even you are a big fan of React or Svelte, even Solid, we welcome you, you can open a sub-directory to add features that you want to see in ã‚¢ã‚¤ãƒª VTuber, or would like to experiment with.&lt;/p&gt; 
 &lt;p&gt;Fields (and related projects) that we are looking for:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Live2D modeller&lt;/li&gt; 
  &lt;li&gt;VRM modeller&lt;/li&gt; 
  &lt;li&gt;VRChat avatar designer&lt;/li&gt; 
  &lt;li&gt;Computer Vision&lt;/li&gt; 
  &lt;li&gt;Reinforcement Learning&lt;/li&gt; 
  &lt;li&gt;Speech Recognition&lt;/li&gt; 
  &lt;li&gt;Speech Synthesis&lt;/li&gt; 
  &lt;li&gt;ONNX Runtime&lt;/li&gt; 
  &lt;li&gt;Transformers.js&lt;/li&gt; 
  &lt;li&gt;vLLM&lt;/li&gt; 
  &lt;li&gt;WebGPU&lt;/li&gt; 
  &lt;li&gt;Three.js&lt;/li&gt; 
  &lt;li&gt;WebXR (&lt;a href="https://github.com/moeru-ai/chat"&gt;checkout the another project&lt;/a&gt; we have under @moeru-ai organization)&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;&lt;strong&gt;If you are interested in, why not introduce yourself here? &lt;a href="https://github.com/moeru-ai/airi/discussions/33"&gt;Would like to join part of us to build AIRI?&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Current progress&lt;/h2&gt; 
&lt;img src="https://raw.githubusercontent.com/moeru-ai/airi/main/docs/content/public/readme-image-pc-preview.avif" /&gt; 
&lt;p&gt;Capable of&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Brain 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Play &lt;a href="https://www.minecraft.net"&gt;Minecraft&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Play &lt;a href="https://www.factorio.com"&gt;Factorio&lt;/a&gt; (WIP, but &lt;a href="https://github.com/moeru-ai/airi-factorio"&gt;PoC and demo available&lt;/a&gt;)&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Chat in &lt;a href="https://telegram.org"&gt;Telegram&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Chat in &lt;a href="https://discord.com"&gt;Discord&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Memory 
    &lt;ul&gt; 
     &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Pure in-browser database support (DuckDB WASM | &lt;code&gt;pglite&lt;/code&gt;)&lt;/li&gt; 
     &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Memory Alaya (WIP)&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Pure in-browser local (WebGPU) inference&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Ears 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Audio input from browser&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Audio input from &lt;a href="https://discord.com"&gt;Discord&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Client side speech recognition&lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Client side talking detection&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Mouth 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://elevenlabs.io/"&gt;ElevenLabs&lt;/a&gt; voice synthesis&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Body 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; VRM support 
    &lt;ul&gt; 
     &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Control VRM model&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; VRM model animations 
    &lt;ul&gt; 
     &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Auto blink&lt;/li&gt; 
     &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Auto look at&lt;/li&gt; 
     &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Idle eye movement&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Live2D support 
    &lt;ul&gt; 
     &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Control Live2D model&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Live2D model animations 
    &lt;ul&gt; 
     &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Auto blink&lt;/li&gt; 
     &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Auto look at&lt;/li&gt; 
     &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Idle eye movement&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Development&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;For detailed instructions to develop this project, follow the &lt;a href="https://raw.githubusercontent.com/moeru-ai/airi/main/.github/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] By default, &lt;code&gt;pnpm dev&lt;/code&gt; will start the development server for the Stage Web (browser version), if you would like to try developing the desktop version, please make sure you read &lt;a href="https://raw.githubusercontent.com/moeru-ai/airi/main/.github/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; to setup the environment correctly.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pnpm i
pnpm dev
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Stage Web (Browser version for &lt;a href="https://airi.moeru.ai"&gt;airi.moeru.ai&lt;/a&gt;)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pnpm dev
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Stage Tamagotchi (Desktop version)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pnpm dev:tamagotchi
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Documentation site&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;pnpm dev:docs
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Publish&lt;/h3&gt; 
&lt;p&gt;Please update the version in &lt;code&gt;Cargo.toml&lt;/code&gt; after running the &lt;code&gt;bumpp&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;npx bumpp --no-commit --no-tag
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Supported the following LLM API Providers (powered by &lt;a href="https://github.com/moeru-ai/xsai"&gt;xsai&lt;/a&gt;)&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://openrouter.ai/"&gt;OpenRouter&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://github.com/vllm-project/vllm"&gt;vLLM&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://github.com/sgl-project/sglang"&gt;SGLang&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://github.com/ollama/ollama"&gt;Ollama&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://developers.generativeai.google"&gt;Google Gemini&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://platform.openai.com/docs/guides/gpt/chat-completions-api"&gt;OpenAI&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;a href="https://learn.microsoft.com/en-us/azure/ai-services/openai/reference"&gt;Azure OpenAI API&lt;/a&gt; (PR welcome)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://anthropic.com"&gt;Anthropic Claude&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;a href="https://docs.anthropic.com/en/api/claude-on-amazon-bedrock"&gt;AWS Claude&lt;/a&gt; (PR welcome)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://www.deepseek.com/"&gt;DeepSeek&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://help.aliyun.com/document_detail/2400395.html"&gt;Qwen&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://x.ai/"&gt;xAI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://wow.groq.com/"&gt;Groq&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://mistral.ai/"&gt;Mistral&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://developers.cloudflare.com/workers-ai/"&gt;Cloudflare Workers AI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://www.together.ai/"&gt;Together.ai&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://www.together.ai/"&gt;Fireworks.ai&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://www.novita.ai/"&gt;Novita&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://bigmodel.cn"&gt;Zhipu&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://cloud.siliconflow.cn/i/rKXmRobW"&gt;SiliconFlow&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://platform.stepfun.com/"&gt;Stepfun&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://platform.baichuan-ai.com"&gt;Baichuan&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://api.minimax.chat/"&gt;Minimax&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://platform.moonshot.cn/"&gt;Moonshot AI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://player2.game/"&gt;Player2&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://cloud.tencent.com/document/product/1729"&gt;Tencent Cloud&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;a href="https://www.xfyun.cn/doc/spark/Web.html"&gt;Sparks&lt;/a&gt; (PR welcome)&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; &lt;a href="https://www.volcengine.com/experience/ark?utm_term=202502dsinvite&amp;amp;ac=DSASUQY5&amp;amp;rc=2QXCA1VI"&gt;Volcano Engine&lt;/a&gt; (PR welcome)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Sub-projects born from this project&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/proj-airi/awesome-ai-vtuber"&gt;Awesome AI VTuber&lt;/a&gt;: A curated list of AI VTubers and related projects&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/moeru-ai/unspeech"&gt;&lt;code&gt;unspeech&lt;/code&gt;&lt;/a&gt;: Universal endpoint proxy server for &lt;code&gt;/audio/transcriptions&lt;/code&gt; and &lt;code&gt;/audio/speech&lt;/code&gt;, like LiteLLM but for any ASR and TTS&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/moeru-ai/hfup"&gt;&lt;code&gt;hfup&lt;/code&gt;&lt;/a&gt;: tools to help on deploying, bundling to HuggingFace Spaces&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/moeru-ai/xsai-transformers"&gt;&lt;code&gt;xsai-transformers&lt;/code&gt;&lt;/a&gt;: Experimental &lt;a href="https://github.com/huggingface/transformers.js"&gt;ğŸ¤— Transformers.js&lt;/a&gt; provider for &lt;a href="https://github.com/moeru-ai/xsai"&gt;xsAI&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/proj-airi/webai-realtime-voice-chat"&gt;WebAI: Realtime Voice Chat&lt;/a&gt;: Full example of implementing ChatGPT's realtime voice from scratch with VAD + STT + LLM + TTS.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/moeru-ai/airi/tree/main/packages/drizzle-duckdb-wasm/README.md"&gt;&lt;code&gt;@proj-airi/drizzle-duckdb-wasm&lt;/code&gt;&lt;/a&gt;: Drizzle ORM driver for DuckDB WASM&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/moeru-ai/airi/tree/main/packages/duckdb-wasm/README.md"&gt;&lt;code&gt;@proj-airi/duckdb-wasm&lt;/code&gt;&lt;/a&gt;: Easy to use wrapper for &lt;code&gt;@duckdb/duckdb-wasm&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/moeru-ai/airi/raw/main/crates/tauri-plugin-mcp/README.md"&gt;&lt;code&gt;tauri-plugin-mcp&lt;/code&gt;&lt;/a&gt;: A Tauri plugin for interacting with MCP servers.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/moeru-ai/airi-factorio"&gt;AIRI Factorio&lt;/a&gt;: Allow AIRI to play Factorio&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/nekomeowww/factorio-rcon-api"&gt;Factorio RCON API&lt;/a&gt;: RESTful API wrapper for Factorio headless server console&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/moeru-ai/airi-factorio/tree/main/packages/autorio"&gt;&lt;code&gt;autorio&lt;/code&gt;&lt;/a&gt;: Factorio automation library&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/moeru-ai/airi-factorio/tree/main/packages/tstl-plugin-reload-factorio-mod"&gt;&lt;code&gt;tstl-plugin-reload-factorio-mod&lt;/code&gt;&lt;/a&gt;: Reload Factorio mod when developing&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/luoling8192/velin"&gt;Velin&lt;/a&gt;: Use Vue SFC and Markdown to write easy to manage stateful prompts for LLM&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/moeru-ai/demodel"&gt;&lt;code&gt;demodel&lt;/code&gt;&lt;/a&gt;: Easily boost the speed of pulling your models and datasets from various of inference runtimes.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/moeru-ai/inventory"&gt;&lt;code&gt;inventory&lt;/code&gt;&lt;/a&gt;: Centralized model catalog and default provider configurations backend service&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/moeru-ai/mcp-launcher"&gt;MCP Launcher&lt;/a&gt;: Easy to use MCP builder &amp;amp; launcher for all possible MCP servers, just like Ollama for models!&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/moeru-ai/sad"&gt;ğŸ¥º SAD&lt;/a&gt;: Documentation and notes for self-host and browser running LLMs.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-mermaid"&gt;%%{ init: { 'flowchart': { 'curve': 'catmullRom' } } }%%

flowchart TD
  Core("Core")
  Unspeech("unspeech")
  DBDriver("@proj-airi/drizzle-duckdb-wasm")
  MemoryDriver("[WIP] Memory Alaya")
  DB1("@proj-airi/duckdb-wasm")
  SVRT("@proj-airi/server-runtime")
  Memory("Memory")
  STT("STT")
  Stage("Stage")
  StageUI("@proj-airi/stage-ui")
  UI("@proj-airi/ui")

  subgraph AIRI
    DB1 --&amp;gt; DBDriver --&amp;gt; MemoryDriver --&amp;gt; Memory --&amp;gt; Core
    UI --&amp;gt; StageUI --&amp;gt; Stage --&amp;gt; Core
    Core --&amp;gt; STT
    Core --&amp;gt; SVRT
  end

  subgraph UI_Components
    UI --&amp;gt; StageUI
    UITransitions("@proj-airi/ui-transitions") --&amp;gt; StageUI
    UILoadingScreens("@proj-airi/ui-loading-screens") --&amp;gt; StageUI
    FontCJK("@proj-airi/font-cjkfonts-allseto") --&amp;gt; StageUI
    FontXiaolai("@proj-airi/font-xiaolai") --&amp;gt; StageUI
  end

  subgraph Apps
    Stage --&amp;gt; StageWeb("@proj-airi/stage-web")
    Stage --&amp;gt; StageTamagotchi("@proj-airi/stage-tamagotchi")
    Core --&amp;gt; RealtimeAudio("@proj-airi/realtime-audio")
    Core --&amp;gt; PromptEngineering("@proj-airi/playground-prompt-engineering")
  end

  subgraph Server_Components
    Core --&amp;gt; ServerSDK("@proj-airi/server-sdk")
    ServerShared("@proj-airi/server-shared") --&amp;gt; SVRT
    ServerShared --&amp;gt; ServerSDK
  end

  STT --&amp;gt;|Speaking| Unspeech
  SVRT --&amp;gt;|Playing Factorio| F_AGENT
  SVRT --&amp;gt;|Playing Minecraft| MC_AGENT

  subgraph Factorio_Agent
    F_AGENT("Factorio Agent")
    F_API("Factorio RCON API")
    factorio-server("factorio-server")
    F_MOD1("autorio")

    F_AGENT --&amp;gt; F_API -.-&amp;gt; factorio-server
    F_MOD1 -.-&amp;gt; factorio-server
  end

  subgraph Minecraft_Agent
    MC_AGENT("Minecraft Agent")
    Mineflayer("Mineflayer")
    minecraft-server("minecraft-server")

    MC_AGENT --&amp;gt; Mineflayer -.-&amp;gt; minecraft-server
  end

  XSAI("xsAI") --&amp;gt; Core
  XSAI --&amp;gt; F_AGENT
  XSAI --&amp;gt; MC_AGENT

  Core --&amp;gt; TauriMCP("@proj-airi/tauri-plugin-mcp")
  Memory_PGVector("@proj-airi/memory-pgvector") --&amp;gt; Memory

  style Core fill:#f9d4d4,stroke:#333,stroke-width:1px
  style AIRI fill:#fcf7f7,stroke:#333,stroke-width:1px
  style UI fill:#d4f9d4,stroke:#333,stroke-width:1px
  style Stage fill:#d4f9d4,stroke:#333,stroke-width:1px
  style UI_Components fill:#d4f9d4,stroke:#333,stroke-width:1px
  style Server_Components fill:#d4e6f9,stroke:#333,stroke-width:1px
  style Apps fill:#d4d4f9,stroke:#333,stroke-width:1px
  style Factorio_Agent fill:#f9d4f2,stroke:#333,stroke-width:1px
  style Minecraft_Agent fill:#f9d4f2,stroke:#333,stroke-width:1px

  style DBDriver fill:#f9f9d4,stroke:#333,stroke-width:1px
  style MemoryDriver fill:#f9f9d4,stroke:#333,stroke-width:1px
  style DB1 fill:#f9f9d4,stroke:#333,stroke-width:1px
  style Memory fill:#f9f9d4,stroke:#333,stroke-width:1px
  style Memory_PGVector fill:#f9f9d4,stroke:#333,stroke-width:1px
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Similar Projects&lt;/h2&gt; 
&lt;h3&gt;Open sourced ones&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/kimjammer/Neuro"&gt;kimjammer/Neuro: A recreation of Neuro-Sama originally created in 7 days.&lt;/a&gt;: very well completed implementation.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/SugarcaneDefender/z-waif"&gt;SugarcaneDefender/z-waif&lt;/a&gt;: Great at gaming, autonomous, and prompt engineering&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/semperai/amica/"&gt;semperai/amica&lt;/a&gt;: Great at VRM, WebXR&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/elizaOS/eliza"&gt;elizaOS/eliza&lt;/a&gt;: Great examples and software engineering on how to integrate agent into various of systems and APIs&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ardha27/AI-Waifu-Vtuber"&gt;ardha27/AI-Waifu-Vtuber&lt;/a&gt;: Great about Twitch API integrations&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/InsanityLabs/AIVTuber"&gt;InsanityLabs/AIVTuber&lt;/a&gt;: Nice UI and UX&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/IRedDragonICY/vixevia"&gt;IRedDragonICY/vixevia&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/t41372/Open-LLM-VTuber"&gt;t41372/Open-LLM-VTuber&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/PeterH0323/Streamer-Sales"&gt;PeterH0323/Streamer-Sales&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Non-open-sourced ones&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://clips.twitch.tv/WanderingCaringDeerDxCat-Qt55xtiGDSoNmDDr"&gt;https://clips.twitch.tv/WanderingCaringDeerDxCat-Qt55xtiGDSoNmDDr&lt;/a&gt; &lt;a href="https://www.youtube.com/watch?v=8Giv5mupJNE"&gt;https://www.youtube.com/watch?v=8Giv5mupJNE&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://clips.twitch.tv/TriangularAthleticBunnySoonerLater-SXpBk1dFso21VcWD"&gt;https://clips.twitch.tv/TriangularAthleticBunnySoonerLater-SXpBk1dFso21VcWD&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/@NOWA_Mirai"&gt;https://www.youtube.com/@NOWA_Mirai&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Project Status&lt;/h2&gt; 
&lt;p&gt;&lt;img src="https://repobeats.axiom.co/api/embed/a1d6fe2c13ea2bb53a5154435a71e2431f70c2ee.svg?sanitize=true" alt="Repobeats analytics image" title="Repobeats analytics image" /&gt;&lt;/p&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/unovue/reka-ui"&gt;Reka UI&lt;/a&gt;: for designing the documentation site, new landing page is based on this, as well as implementing massive amount of UI components. (shadcn-vue is using Reka UI as the headless, do checkout!)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/pixiv/ChatVRM"&gt;pixiv/ChatVRM&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/josephrocca/ChatVRM-js"&gt;josephrocca/ChatVRM-js: A JS conversion/adaptation of parts of the ChatVRM (TypeScript) code for standalone use in OpenCharacters and elsewhere&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Design of UI and style was inspired by &lt;a href="https://store.steampowered.com/app/2919650/Cookard/"&gt;Cookard&lt;/a&gt;, &lt;a href="https://store.steampowered.com/app/2240620/UNBEATABLE/"&gt;UNBEATABLE&lt;/a&gt;, and &lt;a href="https://store.steampowered.com/app/2957700/_/"&gt;Sensei! I like you so much!&lt;/a&gt;, and artworks of &lt;a href="https://dribbble.com/shots/22157656-Ayame"&gt;Ayame by Mercedes Bazan&lt;/a&gt; with &lt;a href="https://dribbble.com/shots/24501019-Wish"&gt;Wish by Mercedes Bazan&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/mallorbc/whisper_mic"&gt;mallorbc/whisper_mic&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/moeru-ai/xsai"&gt;&lt;code&gt;xsai&lt;/code&gt;&lt;/a&gt;: Implemented a decent amount of packages to interact with LLMs and models, like &lt;a href="https://sdk.vercel.ai/"&gt;Vercel AI SDK&lt;/a&gt; but way small.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.star-history.com/#moeru-ai/airi&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=moeru-ai/airi&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>cloudwego/eino</title>
      <link>https://github.com/cloudwego/eino</link>
      <description>&lt;p&gt;The ultimate LLM/AI application development framework in Golang.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Eino&lt;/h1&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/cloudwego/eino/badges/.badges/main/coverage.svg?sanitize=true" alt="coverage" /&gt; &lt;a href="https://github.com/cloudwego/eino/releases"&gt;&lt;img src="https://img.shields.io/github/v/release/cloudwego/eino" alt="Release" /&gt;&lt;/a&gt; &lt;a href="https://www.cloudwego.io/"&gt;&lt;img src="https://img.shields.io/website?up_message=cloudwego&amp;amp;url=https%3A%2F%2Fwww.cloudwego.io%2F" alt="WebSite" /&gt;&lt;/a&gt; &lt;a href="https://github.com/cloudwego/eino/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/cloudwego/eino" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://goreportcard.com/report/github.com/cloudwego/eino"&gt;&lt;img src="https://goreportcard.com/badge/github.com/cloudwego/eino" alt="Go Report Card" /&gt;&lt;/a&gt; &lt;a href="https://github.com/cloudwego/kitex/eino"&gt;&lt;img src="https://img.shields.io/github/issues/cloudwego/eino" alt="OpenIssue" /&gt;&lt;/a&gt; &lt;a href="https://github.com/cloudwego/eino/issues?q=is%3Aissue+is%3Aclosed"&gt;&lt;img src="https://img.shields.io/github/issues-closed/cloudwego/eino" alt="ClosedIssue" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/github/stars/cloudwego/eino" alt="Stars" /&gt; &lt;img src="https://img.shields.io/github/forks/cloudwego/eino" alt="Forks" /&gt;&lt;/p&gt; 
&lt;p&gt;English | &lt;a href="https://raw.githubusercontent.com/cloudwego/eino/main/README.zh_CN.md"&gt;ä¸­æ–‡&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Overview&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;Eino['aino]&lt;/strong&gt; (pronounced similarly to "I know") aims to be the ultimate LLM application development framework in Golang. Drawing inspirations from many excellent LLM application development frameworks in the open-source community such as LangChain &amp;amp; LlamaIndex, etc., as well as learning from cutting-edge research and real world applications, Eino offers an LLM application development framework that emphasizes on simplicity, scalability, reliability and effectiveness that better aligns with Golang programming conventions.&lt;/p&gt; 
&lt;p&gt;What Eino provides are:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;a carefully curated list of &lt;strong&gt;component&lt;/strong&gt; abstractions and implementations that can be easily reused and combined to build LLM applications&lt;/li&gt; 
 &lt;li&gt;a powerful &lt;strong&gt;composition&lt;/strong&gt; framework that does the heavy lifting of strong type checking, stream processing, concurrency management, aspect injection, option assignment, etc. for the user.&lt;/li&gt; 
 &lt;li&gt;a set of meticulously designed &lt;strong&gt;API&lt;/strong&gt; that obsesses on simplicity and clarity.&lt;/li&gt; 
 &lt;li&gt;an ever-growing collection of best practices in the form of bundled &lt;strong&gt;flows&lt;/strong&gt; and &lt;strong&gt;examples&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;a useful set of tools that covers the entire development cycle, from visualized development and debugging to online tracing and evaluation.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;With the above arsenal, Eino can standardize, simplify, and improve efficiency at different stages of the AI application development cycle: &lt;img src="https://raw.githubusercontent.com/cloudwego/eino/main/.github/static/img/eino/eino_concept.jpeg" alt="" /&gt;&lt;/p&gt; 
&lt;h1&gt;A quick walkthrough&lt;/h1&gt; 
&lt;p&gt;Use a component directly:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-Go"&gt;model, _ := openai.NewChatModel(ctx, config) // create an invokable LLM instance
message, _ := model.Generate(ctx, []*Message{
    SystemMessage("you are a helpful assistant."),
    UserMessage("what does the future AI App look like?")})
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Of course, you can do that, Eino provides lots of useful components to use out of the box. But you can do more by using orchestration, for three reasons:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;orchestration encapsulates common patterns of LLM application.&lt;/li&gt; 
 &lt;li&gt;orchestration solves the difficult problem of processing stream response by the LLM.&lt;/li&gt; 
 &lt;li&gt;orchestration handles type safety, concurrency management, aspect injection and option assignment for you.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Eino provides three set of APIs for orchestration&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;API&lt;/th&gt; 
   &lt;th&gt;Characteristics and usage&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Chain&lt;/td&gt; 
   &lt;td&gt;Simple chained directed graph that can only go forward.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Graph&lt;/td&gt; 
   &lt;td&gt;Cyclic or Acyclic directed graph. Powerful and flexible.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Workflow&lt;/td&gt; 
   &lt;td&gt;Acyclic graph that supports data mapping at struct field level.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Let's create a simple chain: a ChatTemplate followed by a ChatModel.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/cloudwego/eino/main/.github/static/img/eino/simple_chain.png" alt="" /&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-Go"&gt;chain, _ := NewChain[map[string]any, *Message]().
           AppendChatTemplate(prompt).
           AppendChatModel(model).
           Compile(ctx)

chain.Invoke(ctx, map[string]any{"query": "what's your name?"})
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Now let's create a graph that uses a ChatModel to generate answer or tool calls, then uses a ToolsNode to execute those tools if needed.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/cloudwego/eino/main/.github/static/img/eino/tool_call_graph.png" alt="" /&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-Go"&gt;graph := NewGraph[map[string]any, *schema.Message]()

_ = graph.AddChatTemplateNode("node_template", chatTpl)
_ = graph.AddChatModelNode("node_model", chatModel)
_ = graph.AddToolsNode("node_tools", toolsNode)
_ = graph.AddLambdaNode("node_converter", takeOne)

_ = graph.AddEdge(START, "node_template")
_ = graph.AddEdge("node_template", "node_model")
_ = graph.AddBranch("node_model", branch)
_ = graph.AddEdge("node_tools", "node_converter")
_ = graph.AddEdge("node_converter", END)

compiledGraph, err := graph.Compile(ctx)
if err != nil {
return err
}
out, err := r.Invoke(ctx, map[string]any{"query":"Beijing's weather this weekend"})
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Now let's create a workflow that flexibly maps input &amp;amp; output at the field level:&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/cloudwego/eino/main/.github/static/img/eino/simple_workflow.png" alt="" /&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-Go"&gt;type Input1 struct {
    Input string
}

type Output1 struct {
    Output string
}

type Input2 struct {
    Role schema.RoleType
}

type Output2 struct {
    Output string
}

type Input3 struct {
    Query string
    MetaData string
}

var (
    ctx context.Context
    m model.BaseChatModel
    lambda1 func(context.Context, Input1) (Output1, error)
    lambda2 func(context.Context, Input2) (Output2, error)
    lambda3 func(context.Context, Input3) (*schema.Message, error)
)

wf := NewWorkflow[[]*schema.Message, *schema.Message]()
wf.AddChatModelNode("model", m).AddInput(START)
wf.AddLambdaNode("lambda1", InvokableLambda(lambda1)).
    AddInput("model", MapFields("Content", "Input"))
wf.AddLambdaNode("lambda2", InvokableLambda(lambda2)).
    AddInput("model", MapFields("Role", "Role"))
wf.AddLambdaNode("lambda3", InvokableLambda(lambda3)).
    AddInput("lambda1", MapFields("Output", "Query")).
    AddInput("lambda2", MapFields("Output", "MetaData"))
wf.End().AddInput("lambda3")
runnable, err := wf.Compile(ctx)
if err != nil {
    return err
}
our, err := runnable.Invoke(ctx, []*schema.Message{
    schema.UserMessage("kick start this workflow!"),
})
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Now let's create a 'ReAct' agent: A ChatModel binds to Tools. It receives input Messages and decides independently whether to call the Tool or output the final result. The execution result of the Tool will again become the input Message for the ChatModel and serve as the context for the next round of independent judgment.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/cloudwego/eino/main/.github/static/img/eino/react.png" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;We provide a complete implementation for ReAct Agent out of the box in the &lt;code&gt;flow&lt;/code&gt; package. Check out the code here: &lt;a href="https://github.com/cloudwego/eino/raw/main/flow/agent/react/react.go"&gt;flow/agent/react&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Our implementation of ReAct Agent uses Eino's &lt;strong&gt;graph orchestration&lt;/strong&gt; exclusively, which provides the following benefits out of the box:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Type checking: it makes sure the two nodes' input and output types match at compile time.&lt;/li&gt; 
 &lt;li&gt;Stream processing: concatenates message stream before passing to chatModel and toolsNode if needed, and copies the stream into callback handlers.&lt;/li&gt; 
 &lt;li&gt;Concurrency management: the shared state can be safely read and written because the StatePreHandler is concurrency safe.&lt;/li&gt; 
 &lt;li&gt;Aspect injection: injects callback aspects before and after the execution of ChatModel if the specified ChatModel implementation hasn't injected itself.&lt;/li&gt; 
 &lt;li&gt;Option assignment: call options are assigned either globally, to specific component type or to specific node.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For example, you could easily extend the compiled graph with callbacks:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-Go"&gt;handler := NewHandlerBuilder().
  OnStartFn(
    func(ctx context.Context, info *RunInfo, input CallbackInput) context.Context) {
        log.Infof("onStart, runInfo: %v, input: %v", info, input)
    }).
  OnEndFn(
    func(ctx context.Context, info *RunInfo, output CallbackOutput) context.Context) {
        log.Infof("onEnd, runInfo: %v, out: %v", info, output)
    }).
  Build()
  
compiledGraph.Invoke(ctx, input, WithCallbacks(handler))
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or you could easily assign options to different nodes:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-Go"&gt;// assign to All nodes
compiledGraph.Invoke(ctx, input, WithCallbacks(handler))

// assign only to ChatModel nodes
compiledGraph.Invoke(ctx, input, WithChatModelOption(WithTemperature(0.5))

// assign only to node_1
compiledGraph.Invoke(ctx, input, WithCallbacks(handler).DesignateNode("node_1"))
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;Key Features&lt;/h1&gt; 
&lt;h2&gt;Rich Components&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Encapsulates common building blocks into &lt;strong&gt;component abstractions&lt;/strong&gt;, each have multiple &lt;strong&gt;component implementations&lt;/strong&gt; that are ready to be used out of the box.&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;component abstractions such as ChatModel, Tool, ChatTemplate, Retriever, Document Loader, Lambda, etc.&lt;/li&gt; 
   &lt;li&gt;each component type has an interface of its own: defined Input &amp;amp; Output Type, defined Option type, and streaming paradigms that make sense.&lt;/li&gt; 
   &lt;li&gt;implementations are transparent. Abstractions are all you care about when orchestrating components together.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Implementations can be nested and captures complex business logic.&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;ReAct Agent, MultiQueryRetriever, Host MultiAgent, etc. They consist of multiple components and non-trivial business logic.&lt;/li&gt; 
   &lt;li&gt;They are still transparent from the outside. A MultiQueryRetriever can be used anywhere that accepts a Retriever.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Powerful Orchestration&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Data flows from Retriever / Document Loaders / ChatTemplate to ChatModel, then flows to Tools and parsed as Final Answer. This directed, controlled flow of data through multiple components can be implemented through &lt;strong&gt;graph orchestration&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;Component instances are graph nodes, and edges are data flow channels.&lt;/li&gt; 
 &lt;li&gt;Graph orchestration is powerful and flexible enough to implement complex business logic: 
  &lt;ul&gt; 
   &lt;li&gt;type checking, stream processing, concurrency management, aspect injection and option assignment are handled by the framework.&lt;/li&gt; 
   &lt;li&gt;branch out execution at runtime, read and write global state, or do field level data mapping using workflow(currently in alpha stage).&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Complete Stream Processing&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Stream processing is important because ChatModel outputs chunks of messages in real time as it generates them. It's especially important with orchestration because more components need to handle streaming data.&lt;/li&gt; 
 &lt;li&gt;Eino automatically &lt;strong&gt;concatenates&lt;/strong&gt; stream chunks for downstream nodes that only accepts non-stream input, such as ToolsNode.&lt;/li&gt; 
 &lt;li&gt;Eino automatically &lt;strong&gt;boxes&lt;/strong&gt; non stream into stream when stream is needed during graph execution.&lt;/li&gt; 
 &lt;li&gt;Eino automatically &lt;strong&gt;merges&lt;/strong&gt; multiple streams as they converge into a single downward node.&lt;/li&gt; 
 &lt;li&gt;Eino automatically &lt;strong&gt;copies&lt;/strong&gt; stream as they fan out to different downward node, or is passed to callback handlers.&lt;/li&gt; 
 &lt;li&gt;Orchestration elements such as &lt;strong&gt;branch&lt;/strong&gt; and &lt;strong&gt;state handlers&lt;/strong&gt; are also stream aware.&lt;/li&gt; 
 &lt;li&gt;With these streaming processing abilities, the streaming paradigms of components themselves become transparent to the user.&lt;/li&gt; 
 &lt;li&gt;A compiled Graph can run with 4 different streaming paradigms:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Streaming Paradigm&lt;/th&gt; 
   &lt;th&gt;Explanation&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Invoke&lt;/td&gt; 
   &lt;td&gt;Accepts non-stream type I and returns non-stream type O&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Stream&lt;/td&gt; 
   &lt;td&gt;Accepts non-stream type I and returns stream type StreamReader[O]&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Collect&lt;/td&gt; 
   &lt;td&gt;Accepts stream type StreamReader[I] and returns non-stream type O&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Transform&lt;/td&gt; 
   &lt;td&gt;Accepts stream type StreamReader[I] and returns stream type StreamReader[O]&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Highly Extensible Aspects (Callbacks)&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Aspects handle cross-cutting concerns such as logging, tracing, metrics, etc., as well as exposing internal details of component implementations.&lt;/li&gt; 
 &lt;li&gt;Five aspects are supported: &lt;strong&gt;OnStart, OnEnd, OnError, OnStartWithStreamInput, OnEndWithStreamOutput&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;Developers can easily create custom callback handlers, add them during graph run via options, and they will be invoked during graph run.&lt;/li&gt; 
 &lt;li&gt;Graph can also inject aspects to those component implementations that do not support callbacks on their own.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Eino Framework Structure&lt;/h1&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/cloudwego/eino/main/.github/static/img/eino/eino_framework.jpeg" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;The Eino framework consists of several parts:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Eino(this repo): Contains Eino's type definitions, streaming mechanism, component abstractions, orchestration capabilities, aspect mechanisms, etc.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/cloudwego/eino-ext"&gt;EinoExt&lt;/a&gt;: Component implementations, callback handlers implementations, component usage examples, and various tools such as evaluators, prompt optimizers.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/cloudwego/eino-ext/tree/main/devops"&gt;Eino Devops&lt;/a&gt;: visualized developing, visualized debugging etc.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/cloudwego/eino-examples"&gt;EinoExamples&lt;/a&gt; is the repo containing example applications and best practices for Eino.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Detailed Documentation&lt;/h2&gt; 
&lt;p&gt;For learning and using Eino, we provide a comprehensive Eino User Manual to help you quickly understand the concepts in Eino and master the skills of developing AI applications based on Eino. Start exploring through the &lt;a href="https://www.cloudwego.io/zh/docs/eino/"&gt;Eino User Manual&lt;/a&gt; now!&lt;/p&gt; 
&lt;p&gt;For a quick introduction to building AI applications with Eino, we recommend starting with &lt;a href="https://www.cloudwego.io/zh/docs/eino/quick_start/"&gt;Eino: Quick Start&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Dependencies&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Go 1.18 and above.&lt;/li&gt; 
 &lt;li&gt;Eino relies on &lt;a href="https://github.com/getkin/kin-openapi"&gt;kin-openapi&lt;/a&gt; 's OpenAPI JSONSchema implementation. In order to remain compatible with Go 1.18, we have fixed kin-openapi's version to be v0.118.0.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Security&lt;/h2&gt; 
&lt;p&gt;If you discover a potential security issue in this project, or think you may have discovered a security issue, we ask that you notify Bytedance Security via our &lt;a href="https://security.bytedance.com/src"&gt;security center&lt;/a&gt; or &lt;a href="https://raw.githubusercontent.com/cloudwego/eino/main/sec@bytedance.com"&gt;vulnerability reporting email&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Please do &lt;strong&gt;not&lt;/strong&gt; create a public GitHub issue.&lt;/p&gt; 
&lt;h2&gt;Contact US&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;How to become a member: &lt;a href="https://github.com/cloudwego/community/raw/main/COMMUNITY_MEMBERSHIP.md"&gt;COMMUNITY MEMBERSHIP&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Issues: &lt;a href="https://github.com/cloudwego/eino/issues"&gt;Issues&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Lark: Scan the QR code below with &lt;a href="https://www.feishu.cn/en/"&gt;Register Feishu&lt;/a&gt; to join our CloudWeGo/eino user group.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;â€‚â€‚â€‚ &lt;img src="https://raw.githubusercontent.com/cloudwego/eino/main/.github/static/img/eino/lark_group_zh.png" alt="LarkGroup" width="200" /&gt;&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the &lt;a href="https://raw.githubusercontent.com/cloudwego/eino/main/LICENSE-APACHE"&gt;Apache-2.0 License&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ggml-org/llama.cpp</title>
      <link>https://github.com/ggml-org/llama.cpp</link>
      <description>&lt;p&gt;LLM inference in C/C++&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;llama.cpp&lt;/h1&gt; 
&lt;p&gt;&lt;img src="https://user-images.githubusercontent.com/1991296/230134379-7181e485-c521-4d23-a0d6-f7b3b61ba524.png" alt="llama" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://opensource.org/licenses/MIT"&gt;&lt;img src="https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true" alt="License: MIT" /&gt;&lt;/a&gt; &lt;a href="https://github.com/ggml-org/llama.cpp/releases"&gt;&lt;img src="https://img.shields.io/github/v/release/ggml-org/llama.cpp" alt="Release" /&gt;&lt;/a&gt; &lt;a href="https://github.com/ggml-org/llama.cpp/actions/workflows/server.yml"&gt;&lt;img src="https://github.com/ggml-org/llama.cpp/actions/workflows/server.yml/badge.svg?sanitize=true" alt="Server" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/discussions/205"&gt;Manifesto&lt;/a&gt; / &lt;a href="https://github.com/ggml-org/ggml"&gt;ggml&lt;/a&gt; / &lt;a href="https://github.com/ggml-org/llama.cpp/raw/master/docs/ops.md"&gt;ops&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;LLM inference in C/C++&lt;/p&gt; 
&lt;h2&gt;Recent API changes&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/issues/9289"&gt;Changelog for &lt;code&gt;libllama&lt;/code&gt; API&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/issues/9291"&gt;Changelog for &lt;code&gt;llama-server&lt;/code&gt; REST API&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Hot topics&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/discussions/15396"&gt;guide : running gpt-oss with llama.cpp&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/discussions/15313"&gt;[FEEDBACK] Better packaging for llama.cpp to support downstream consumers ğŸ¤—&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Support for the &lt;code&gt;gpt-oss&lt;/code&gt; model with native MXFP4 format has been added | &lt;a href="https://github.com/ggml-org/llama.cpp/pull/15091"&gt;PR&lt;/a&gt; | &lt;a href="https://blogs.nvidia.com/blog/rtx-ai-garage-openai-oss"&gt;Collaboration with NVIDIA&lt;/a&gt; | &lt;a href="https://github.com/ggml-org/llama.cpp/discussions/15095"&gt;Comment&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Hot PRs: &lt;a href="https://github.com/ggml-org/llama.cpp/pulls?q=is%3Apr+label%3Ahot+"&gt;All&lt;/a&gt; | &lt;a href="https://github.com/ggml-org/llama.cpp/pulls?q=is%3Apr+label%3Ahot+is%3Aopen"&gt;Open&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Multimodal support arrived in &lt;code&gt;llama-server&lt;/code&gt;: &lt;a href="https://github.com/ggml-org/llama.cpp/pull/12898"&gt;#12898&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/multimodal.md"&gt;documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;VS Code extension for FIM completions: &lt;a href="https://github.com/ggml-org/llama.vscode"&gt;https://github.com/ggml-org/llama.vscode&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Vim/Neovim plugin for FIM completions: &lt;a href="https://github.com/ggml-org/llama.vim"&gt;https://github.com/ggml-org/llama.vim&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Introducing GGUF-my-LoRA &lt;a href="https://github.com/ggml-org/llama.cpp/discussions/10123"&gt;https://github.com/ggml-org/llama.cpp/discussions/10123&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Hugging Face Inference Endpoints now support GGUF out of the box! &lt;a href="https://github.com/ggml-org/llama.cpp/discussions/9669"&gt;https://github.com/ggml-org/llama.cpp/discussions/9669&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Hugging Face GGUF editor: &lt;a href="https://github.com/ggml-org/llama.cpp/discussions/9268"&gt;discussion&lt;/a&gt; | &lt;a href="https://huggingface.co/spaces/CISCai/gguf-editor"&gt;tool&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Quick start&lt;/h2&gt; 
&lt;p&gt;Getting started with llama.cpp is straightforward. Here are several ways to install it on your machine:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Install &lt;code&gt;llama.cpp&lt;/code&gt; using &lt;a href="https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/install.md"&gt;brew, nix or winget&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Run with Docker - see our &lt;a href="https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/docker.md"&gt;Docker documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Download pre-built binaries from the &lt;a href="https://github.com/ggml-org/llama.cpp/releases"&gt;releases page&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Build from source by cloning this repository - check out &lt;a href="https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/build.md"&gt;our build guide&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Once installed, you'll need a model to work with. Head to the &lt;a href="https://raw.githubusercontent.com/ggml-org/llama.cpp/master/#obtaining-and-quantizing-models"&gt;Obtaining and quantizing models&lt;/a&gt; section to learn more.&lt;/p&gt; 
&lt;p&gt;Example command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Use a local model file
llama-cli -m my_model.gguf

# Or download and run a model directly from Hugging Face
llama-cli -hf ggml-org/gemma-3-1b-it-GGUF

# Launch OpenAI-compatible API server
llama-server -hf ggml-org/gemma-3-1b-it-GGUF
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Description&lt;/h2&gt; 
&lt;p&gt;The main goal of &lt;code&gt;llama.cpp&lt;/code&gt; is to enable LLM inference with minimal setup and state-of-the-art performance on a wide range of hardware - locally and in the cloud.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Plain C/C++ implementation without any dependencies&lt;/li&gt; 
 &lt;li&gt;Apple silicon is a first-class citizen - optimized via ARM NEON, Accelerate and Metal frameworks&lt;/li&gt; 
 &lt;li&gt;AVX, AVX2, AVX512 and AMX support for x86 architectures&lt;/li&gt; 
 &lt;li&gt;1.5-bit, 2-bit, 3-bit, 4-bit, 5-bit, 6-bit, and 8-bit integer quantization for faster inference and reduced memory use&lt;/li&gt; 
 &lt;li&gt;Custom CUDA kernels for running LLMs on NVIDIA GPUs (support for AMD GPUs via HIP and Moore Threads GPUs via MUSA)&lt;/li&gt; 
 &lt;li&gt;Vulkan and SYCL backend support&lt;/li&gt; 
 &lt;li&gt;CPU+GPU hybrid inference to partially accelerate models larger than the total VRAM capacity&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The &lt;code&gt;llama.cpp&lt;/code&gt; project is the main playground for developing new features for the &lt;a href="https://github.com/ggml-org/ggml"&gt;ggml&lt;/a&gt; library.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Models&lt;/summary&gt; 
 &lt;p&gt;Typically finetunes of the base models below are supported as well.&lt;/p&gt; 
 &lt;p&gt;Instructions for adding support for new models: &lt;a href="https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/development/HOWTO-add-model.md"&gt;HOWTO-add-model.md&lt;/a&gt;&lt;/p&gt; 
 &lt;h4&gt;Text-only&lt;/h4&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; LLaMA ğŸ¦™&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; LLaMA 2 ğŸ¦™ğŸ¦™&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; LLaMA 3 ğŸ¦™ğŸ¦™ğŸ¦™&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://huggingface.co/mistralai/Mistral-7B-v0.1"&gt;Mistral 7B&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://huggingface.co/models?search=mistral-ai/Mixtral"&gt;Mixtral MoE&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://huggingface.co/databricks/dbrx-instruct"&gt;DBRX&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://huggingface.co/models?search=tiiuae/falcon"&gt;Falcon&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://github.com/ymcui/Chinese-LLaMA-Alpaca"&gt;Chinese LLaMA / Alpaca&lt;/a&gt; and &lt;a href="https://github.com/ymcui/Chinese-LLaMA-Alpaca-2"&gt;Chinese LLaMA-2 / Alpaca-2&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://github.com/bofenghuang/vigogne"&gt;Vigogne (French)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://github.com/ggml-org/llama.cpp/pull/5423"&gt;BERT&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://bair.berkeley.edu/blog/2023/04/03/koala/"&gt;Koala&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://huggingface.co/models?search=baichuan-inc/Baichuan"&gt;Baichuan 1 &amp;amp; 2&lt;/a&gt; + &lt;a href="https://huggingface.co/hiyouga/baichuan-7b-sft"&gt;derivations&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://huggingface.co/models?search=BAAI/Aquila"&gt;Aquila 1 &amp;amp; 2&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://github.com/ggml-org/llama.cpp/pull/3187"&gt;Starcoder models&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://huggingface.co/smallcloudai/Refact-1_6B-fim"&gt;Refact&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://github.com/ggml-org/llama.cpp/pull/3417"&gt;MPT&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://github.com/ggml-org/llama.cpp/pull/3553"&gt;Bloom&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://huggingface.co/models?search=01-ai/Yi"&gt;Yi models&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://huggingface.co/stabilityai"&gt;StableLM models&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://huggingface.co/models?search=deepseek-ai/deepseek"&gt;Deepseek models&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://huggingface.co/models?search=Qwen/Qwen"&gt;Qwen models&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://github.com/ggml-org/llama.cpp/pull/3557"&gt;PLaMo-13B&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://huggingface.co/models?search=microsoft/phi"&gt;Phi models&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://github.com/ggml-org/llama.cpp/pull/11003"&gt;PhiMoE&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://huggingface.co/gpt2"&gt;GPT-2&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://github.com/ggml-org/llama.cpp/pull/5118"&gt;Orion 14B&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://huggingface.co/models?search=internlm2"&gt;InternLM2&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://github.com/WisdomShell/codeshell"&gt;CodeShell&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://ai.google.dev/gemma"&gt;Gemma&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://github.com/state-spaces/mamba"&gt;Mamba&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://huggingface.co/keyfan/grok-1-hf"&gt;Grok-1&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://huggingface.co/models?search=xverse"&gt;Xverse&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://huggingface.co/models?search=CohereForAI/c4ai-command-r"&gt;Command-R models&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://huggingface.co/models?search=sea-lion"&gt;SEA-LION&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://huggingface.co/GritLM/GritLM-7B"&gt;GritLM-7B&lt;/a&gt; + &lt;a href="https://huggingface.co/GritLM/GritLM-8x7B"&gt;GritLM-8x7B&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://allenai.org/olmo"&gt;OLMo&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://allenai.org/olmo"&gt;OLMo 2&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://huggingface.co/allenai/OLMoE-1B-7B-0924"&gt;OLMoE&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://huggingface.co/collections/ibm-granite/granite-code-models-6624c5cec322e4c148c8b330"&gt;Granite models&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://github.com/EleutherAI/gpt-neox"&gt;GPT-NeoX&lt;/a&gt; + &lt;a href="https://github.com/EleutherAI/pythia"&gt;Pythia&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://huggingface.co/collections/Snowflake/arctic-66290090abe542894a5ac520"&gt;Snowflake-Arctic MoE&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://huggingface.co/models?search=Smaug"&gt;Smaug&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://huggingface.co/LumiOpen/Poro-34B"&gt;Poro 34B&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://huggingface.co/1bitLLM"&gt;Bitnet b1.58 models&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://huggingface.co/models?search=flan-t5"&gt;Flan T5&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://huggingface.co/collections/apple/openelm-instruct-models-6619ad295d7ae9f868b759ca"&gt;Open Elm models&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://huggingface.co/THUDM/chatglm3-6b"&gt;ChatGLM3-6b&lt;/a&gt; + &lt;a href="https://huggingface.co/THUDM/glm-4-9b"&gt;ChatGLM4-9b&lt;/a&gt; + &lt;a href="https://huggingface.co/THUDM/glm-edge-1.5b-chat"&gt;GLMEdge-1.5b&lt;/a&gt; + &lt;a href="https://huggingface.co/THUDM/glm-edge-4b-chat"&gt;GLMEdge-4b&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://huggingface.co/collections/THUDM/glm-4-0414-67f3cbcb34dd9d252707cb2e"&gt;GLM-4-0414&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://huggingface.co/collections/HuggingFaceTB/smollm-6695016cad7167254ce15966"&gt;SmolLM&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://huggingface.co/LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct"&gt;EXAONE-3.0-7.8B-Instruct&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://huggingface.co/collections/tiiuae/falconmamba-7b-66b9a580324dd1598b0f6d4a"&gt;FalconMamba Models&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://huggingface.co/inceptionai/jais-13b-chat"&gt;Jais&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://huggingface.co/collections/speakleash/bielik-11b-v23-66ee813238d9b526a072408a"&gt;Bielik-11B-v2.3&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://github.com/BlinkDL/RWKV-LM"&gt;RWKV-6&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://huggingface.co/recursal/QRWKV6-32B-Instruct-Preview-v0.1"&gt;QRWKV-6&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://huggingface.co/ai-sage/GigaChat-20B-A3B-instruct"&gt;GigaChat-20B-A3B&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://huggingface.co/trillionlabs/Trillion-7B-preview"&gt;Trillion-7B-preview&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://huggingface.co/collections/inclusionAI/ling-67c51c85b34a7ea0aba94c32"&gt;Ling models&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://huggingface.co/collections/LiquidAI/lfm2-686d721927015b2ad73eaa38"&gt;LFM2 models&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h4&gt;Multimodal&lt;/h4&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://huggingface.co/collections/liuhaotian/llava-15-653aac15d994e992e2677a7e"&gt;LLaVA 1.5 models&lt;/a&gt;, &lt;a href="https://huggingface.co/collections/liuhaotian/llava-16-65b9e40155f60fd046a5ccf2"&gt;LLaVA 1.6 models&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://huggingface.co/models?search=SkunkworksAI/Bakllava"&gt;BakLLaVA&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://huggingface.co/NousResearch/Obsidian-3B-V0.5"&gt;Obsidian&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://huggingface.co/models?search=Lin-Chen/ShareGPT4V"&gt;ShareGPT4V&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://huggingface.co/models?search=mobileVLM"&gt;MobileVLM 1.7B/3B models&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://huggingface.co/models?search=Yi-VL"&gt;Yi-VL&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://huggingface.co/models?search=MiniCPM"&gt;Mini CPM&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://huggingface.co/vikhyatk/moondream2"&gt;Moondream&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://github.com/BAAI-DCAI/Bunny"&gt;Bunny&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://huggingface.co/models?search=glm-edge"&gt;GLM-EDGE&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://huggingface.co/collections/Qwen/qwen2-vl-66cee7455501d7126940800d"&gt;Qwen2-VL&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; &lt;a href="https://huggingface.co/collections/LiquidAI/lfm2-vl-68963bbc84a610f7638d5ffa"&gt;LFM2-VL&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Bindings&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Python: &lt;a href="https://github.com/ddh0/easy-llama"&gt;ddh0/easy-llama&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Python: &lt;a href="https://github.com/abetlen/llama-cpp-python"&gt;abetlen/llama-cpp-python&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Go: &lt;a href="https://github.com/go-skynet/go-llama.cpp"&gt;go-skynet/go-llama.cpp&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Node.js: &lt;a href="https://github.com/withcatai/node-llama-cpp"&gt;withcatai/node-llama-cpp&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;JS/TS (llama.cpp server client): &lt;a href="https://modelfusion.dev/integration/model-provider/llamacpp"&gt;lgrammel/modelfusion&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;JS/TS (Programmable Prompt Engine CLI): &lt;a href="https://github.com/offline-ai/cli"&gt;offline-ai/cli&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;JavaScript/Wasm (works in browser): &lt;a href="https://github.com/tangledgroup/llama-cpp-wasm"&gt;tangledgroup/llama-cpp-wasm&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Typescript/Wasm (nicer API, available on npm): &lt;a href="https://github.com/ngxson/wllama"&gt;ngxson/wllama&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Ruby: &lt;a href="https://github.com/yoshoku/llama_cpp.rb"&gt;yoshoku/llama_cpp.rb&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Rust (more features): &lt;a href="https://github.com/edgenai/llama_cpp-rs"&gt;edgenai/llama_cpp-rs&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Rust (nicer API): &lt;a href="https://github.com/mdrokz/rust-llama.cpp"&gt;mdrokz/rust-llama.cpp&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Rust (more direct bindings): &lt;a href="https://github.com/utilityai/llama-cpp-rs"&gt;utilityai/llama-cpp-rs&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Rust (automated build from crates.io): &lt;a href="https://github.com/ShelbyJenkins/llm_client"&gt;ShelbyJenkins/llm_client&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;C#/.NET: &lt;a href="https://github.com/SciSharp/LLamaSharp"&gt;SciSharp/LLamaSharp&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;C#/VB.NET (more features - community license): &lt;a href="https://docs.lm-kit.com/lm-kit-net/index.html"&gt;LM-Kit.NET&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Scala 3: &lt;a href="https://github.com/donderom/llm4s"&gt;donderom/llm4s&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Clojure: &lt;a href="https://github.com/phronmophobic/llama.clj"&gt;phronmophobic/llama.clj&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;React Native: &lt;a href="https://github.com/mybigday/llama.rn"&gt;mybigday/llama.rn&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Java: &lt;a href="https://github.com/kherud/java-llama.cpp"&gt;kherud/java-llama.cpp&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zig: &lt;a href="https://github.com/Deins/llama.cpp.zig"&gt;deins/llama.cpp.zig&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Flutter/Dart: &lt;a href="https://github.com/netdur/llama_cpp_dart"&gt;netdur/llama_cpp_dart&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Flutter: &lt;a href="https://github.com/xuegao-tzx/Fllama"&gt;xuegao-tzx/Fllama&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;PHP (API bindings and features built on top of llama.cpp): &lt;a href="https://github.com/distantmagic/resonance"&gt;distantmagic/resonance&lt;/a&gt; &lt;a href="https://github.com/ggml-org/llama.cpp/pull/6326"&gt;(more info)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Guile Scheme: &lt;a href="https://savannah.nongnu.org/projects/guile-llama-cpp"&gt;guile_llama_cpp&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Swift &lt;a href="https://github.com/srgtuszy/llama-cpp-swift"&gt;srgtuszy/llama-cpp-swift&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Swift &lt;a href="https://github.com/ShenghaiWang/SwiftLlama"&gt;ShenghaiWang/SwiftLlama&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Delphi &lt;a href="https://github.com/Embarcadero/llama-cpp-delphi"&gt;Embarcadero/llama-cpp-delphi&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;UIs&lt;/summary&gt; 
 &lt;p&gt;&lt;em&gt;(to have a project listed here, it should clearly state that it depends on &lt;code&gt;llama.cpp&lt;/code&gt;)&lt;/em&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://github.com/yaroslavyaroslav/OpenAI-sublime-text"&gt;AI Sublime Text plugin&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/cztomsik/ava"&gt;cztomsik/ava&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/alexpinel/Dot"&gt;Dot&lt;/a&gt; (GPL)&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/ylsdamxssjxxdd/eva"&gt;eva&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/iohub/coLLaMA"&gt;iohub/collama&lt;/a&gt; (Apache-2.0)&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/janhq/jan"&gt;janhq/jan&lt;/a&gt; (AGPL)&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/johnbean393/Sidekick"&gt;johnbean393/Sidekick&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/zhouwg/kantv?tab=readme-ov-file"&gt;KanTV&lt;/a&gt; (Apache-2.0)&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/firatkiral/kodibot"&gt;KodiBot&lt;/a&gt; (GPL)&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/ggml-org/llama.vim"&gt;llama.vim&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/abgulati/LARS"&gt;LARS&lt;/a&gt; (AGPL)&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/vietanhdev/llama-assistant"&gt;Llama Assistant&lt;/a&gt; (GPL)&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/guinmoon/LLMFarm?tab=readme-ov-file"&gt;LLMFarm&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/undreamai/LLMUnity"&gt;LLMUnity&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://lmstudio.ai/"&gt;LMStudio&lt;/a&gt; (proprietary)&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/mudler/LocalAI"&gt;LocalAI&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/LostRuins/koboldcpp"&gt;LostRuins/koboldcpp&lt;/a&gt; (AGPL)&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://mindmac.app"&gt;MindMac&lt;/a&gt; (proprietary)&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/MindWorkAI/AI-Studio"&gt;MindWorkAI/AI-Studio&lt;/a&gt; (FSL-1.1-MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/Mobile-Artificial-Intelligence/maid"&gt;Mobile-Artificial-Intelligence/maid&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/Mozilla-Ocho/llamafile"&gt;Mozilla-Ocho/llamafile&lt;/a&gt; (Apache-2.0)&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/nat/openplayground"&gt;nat/openplayground&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/nomic-ai/gpt4all"&gt;nomic-ai/gpt4all&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/ollama/ollama"&gt;ollama/ollama&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/oobabooga/text-generation-webui"&gt;oobabooga/text-generation-webui&lt;/a&gt; (AGPL)&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/a-ghorbani/pocketpal-ai"&gt;PocketPal AI&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/psugihara/FreeChat"&gt;psugihara/FreeChat&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/ptsochantaris/emeltal"&gt;ptsochantaris/emeltal&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/pythops/tenere"&gt;pythops/tenere&lt;/a&gt; (AGPL)&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/containers/ramalama"&gt;ramalama&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/semperai/amica"&gt;semperai/amica&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/withcatai/catai"&gt;withcatai/catai&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/blackhole89/autopen"&gt;Autopen&lt;/a&gt; (GPL)&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Tools&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://github.com/akx/ggify"&gt;akx/ggify&lt;/a&gt; â€“ download PyTorch models from HuggingFace Hub and convert them to GGML&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/akx/ollama-dl"&gt;akx/ollama-dl&lt;/a&gt; â€“ download models from the Ollama library to be used directly with llama.cpp&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/crashr/gppm"&gt;crashr/gppm&lt;/a&gt; â€“ launch llama.cpp instances utilizing NVIDIA Tesla P40 or P100 GPUs with reduced idle power consumption&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/gpustack/gguf-parser-go/tree/main/cmd/gguf-parser"&gt;gpustack/gguf-parser&lt;/a&gt; - review/check the GGUF file and estimate the memory usage&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://marketplace.unity.com/packages/tools/generative-ai/styled-lines-llama-cpp-model-292902"&gt;Styled Lines&lt;/a&gt; (proprietary licensed, async wrapper of inference part for game development in Unity3d with pre-built Mobile and Web platform wrappers and a model example)&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Infrastructure&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://github.com/intentee/paddler"&gt;Paddler&lt;/a&gt; - Open-source LLMOps platform for hosting and scaling AI in your own infrastructure&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/gpustack/gpustack"&gt;GPUStack&lt;/a&gt; - Manage GPU clusters for running LLMs&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/onicai/llama_cpp_canister"&gt;llama_cpp_canister&lt;/a&gt; - llama.cpp as a smart contract on the Internet Computer, using WebAssembly&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/mostlygeek/llama-swap"&gt;llama-swap&lt;/a&gt; - transparent proxy that adds automatic model switching with llama-server&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/kalavai-net/kalavai-client"&gt;Kalavai&lt;/a&gt; - Crowdsource end to end LLM deployment at any scale&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/InftyAI/llmaz"&gt;llmaz&lt;/a&gt; - â˜¸ï¸ Easy, advanced inference platform for large language models on Kubernetes.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Games&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://github.com/MorganRO8/Lucys_Labyrinth"&gt;Lucy's Labyrinth&lt;/a&gt; - A simple maze game where agents controlled by an AI model will try to trick you.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;Supported backends&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Backend&lt;/th&gt; 
   &lt;th&gt;Target devices&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/build.md#metal-build"&gt;Metal&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Apple Silicon&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/build.md#blas-build"&gt;BLAS&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;All&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/backend/BLIS.md"&gt;BLIS&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;All&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/backend/SYCL.md"&gt;SYCL&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Intel and Nvidia GPU&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/build.md#musa"&gt;MUSA&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Moore Threads GPU&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/build.md#cuda"&gt;CUDA&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Nvidia GPU&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/build.md#hip"&gt;HIP&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;AMD GPU&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/build.md#vulkan"&gt;Vulkan&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;GPU&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/build.md#cann"&gt;CANN&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Ascend NPU&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/backend/OPENCL.md"&gt;OpenCL&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Adreno GPU&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/build.md#webgpu"&gt;WebGPU [In Progress]&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;All&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/tree/master/tools/rpc"&gt;RPC&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;All&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Obtaining and quantizing models&lt;/h2&gt; 
&lt;p&gt;The &lt;a href="https://huggingface.co"&gt;Hugging Face&lt;/a&gt; platform hosts a &lt;a href="https://huggingface.co/models?library=gguf&amp;amp;sort=trending"&gt;number of LLMs&lt;/a&gt; compatible with &lt;code&gt;llama.cpp&lt;/code&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/models?library=gguf&amp;amp;sort=trending"&gt;Trending&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://huggingface.co/models?sort=trending&amp;amp;search=llama+gguf"&gt;LLaMA&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can either manually download the GGUF file or directly use any &lt;code&gt;llama.cpp&lt;/code&gt;-compatible models from &lt;a href="https://huggingface.co/"&gt;Hugging Face&lt;/a&gt; or other model hosting sites, such as &lt;a href="https://modelscope.cn/"&gt;ModelScope&lt;/a&gt;, by using this CLI argument: &lt;code&gt;-hf &amp;lt;user&amp;gt;/&amp;lt;model&amp;gt;[:quant]&lt;/code&gt;. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;llama-cli -hf ggml-org/gemma-3-1b-it-GGUF
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;By default, the CLI would download from Hugging Face, you can switch to other options with the environment variable &lt;code&gt;MODEL_ENDPOINT&lt;/code&gt;. For example, you may opt to downloading model checkpoints from ModelScope or other model sharing communities by setting the environment variable, e.g. &lt;code&gt;MODEL_ENDPOINT=https://www.modelscope.cn/&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;After downloading a model, use the CLI tools to run it locally - see below.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;llama.cpp&lt;/code&gt; requires the model to be stored in the &lt;a href="https://github.com/ggml-org/ggml/raw/master/docs/gguf.md"&gt;GGUF&lt;/a&gt; file format. Models in other data formats can be converted to GGUF using the &lt;code&gt;convert_*.py&lt;/code&gt; Python scripts in this repo.&lt;/p&gt; 
&lt;p&gt;The Hugging Face platform provides a variety of online tools for converting, quantizing and hosting models with &lt;code&gt;llama.cpp&lt;/code&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Use the &lt;a href="https://huggingface.co/spaces/ggml-org/gguf-my-repo"&gt;GGUF-my-repo space&lt;/a&gt; to convert to GGUF format and quantize model weights to smaller sizes&lt;/li&gt; 
 &lt;li&gt;Use the &lt;a href="https://huggingface.co/spaces/ggml-org/gguf-my-lora"&gt;GGUF-my-LoRA space&lt;/a&gt; to convert LoRA adapters to GGUF format (more info: &lt;a href="https://github.com/ggml-org/llama.cpp/discussions/10123"&gt;https://github.com/ggml-org/llama.cpp/discussions/10123&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;Use the &lt;a href="https://huggingface.co/spaces/CISCai/gguf-editor"&gt;GGUF-editor space&lt;/a&gt; to edit GGUF meta data in the browser (more info: &lt;a href="https://github.com/ggml-org/llama.cpp/discussions/9268"&gt;https://github.com/ggml-org/llama.cpp/discussions/9268&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;Use the &lt;a href="https://ui.endpoints.huggingface.co/"&gt;Inference Endpoints&lt;/a&gt; to directly host &lt;code&gt;llama.cpp&lt;/code&gt; in the cloud (more info: &lt;a href="https://github.com/ggml-org/llama.cpp/discussions/9669"&gt;https://github.com/ggml-org/llama.cpp/discussions/9669&lt;/a&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To learn more about model quantization, &lt;a href="https://raw.githubusercontent.com/ggml-org/llama.cpp/master/tools/quantize/README.md"&gt;read this documentation&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;&lt;a href="https://raw.githubusercontent.com/ggml-org/llama.cpp/master/tools/main"&gt;&lt;code&gt;llama-cli&lt;/code&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;h4&gt;A CLI tool for accessing and experimenting with most of &lt;code&gt;llama.cpp&lt;/code&gt;'s functionality.&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt; 
  &lt;details open&gt; 
   &lt;summary&gt;Run in conversation mode&lt;/summary&gt; 
   &lt;p&gt;Models with a built-in chat template will automatically activate conversation mode. If this doesn't occur, you can manually enable it by adding &lt;code&gt;-cnv&lt;/code&gt; and specifying a suitable chat template with &lt;code&gt;--chat-template NAME&lt;/code&gt;&lt;/p&gt; 
   &lt;pre&gt;&lt;code class="language-bash"&gt;llama-cli -m model.gguf

# &amp;gt; hi, who are you?
# Hi there! I'm your helpful assistant! I'm an AI-powered chatbot designed to assist and provide information to users like you. I'm here to help answer your questions, provide guidance, and offer support on a wide range of topics. I'm a friendly and knowledgeable AI, and I'm always happy to help with anything you need. What's on your mind, and how can I assist you today?
#
# &amp;gt; what is 1+1?
# Easy peasy! The answer to 1+1 is... 2!
&lt;/code&gt;&lt;/pre&gt; 
  &lt;/details&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;details&gt; 
   &lt;summary&gt;Run in conversation mode with custom chat template&lt;/summary&gt; 
   &lt;pre&gt;&lt;code class="language-bash"&gt;# use the "chatml" template (use -h to see the list of supported templates)
llama-cli -m model.gguf -cnv --chat-template chatml

# use a custom template
llama-cli -m model.gguf -cnv --in-prefix 'User: ' --reverse-prompt 'User:'
&lt;/code&gt;&lt;/pre&gt; 
  &lt;/details&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;details&gt; 
   &lt;summary&gt;Run simple text completion&lt;/summary&gt; 
   &lt;p&gt;To disable conversation mode explicitly, use &lt;code&gt;-no-cnv&lt;/code&gt;&lt;/p&gt; 
   &lt;pre&gt;&lt;code class="language-bash"&gt;llama-cli -m model.gguf -p "I believe the meaning of life is" -n 128 -no-cnv

# I believe the meaning of life is to find your own truth and to live in accordance with it. For me, this means being true to myself and following my passions, even if they don't align with societal expectations. I think that's what I love about yoga â€“ it's not just a physical practice, but a spiritual one too. It's about connecting with yourself, listening to your inner voice, and honoring your own unique journey.
&lt;/code&gt;&lt;/pre&gt; 
  &lt;/details&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;details&gt; 
   &lt;summary&gt;Constrain the output with a custom grammar&lt;/summary&gt; 
   &lt;pre&gt;&lt;code class="language-bash"&gt;llama-cli -m model.gguf -n 256 --grammar-file grammars/json.gbnf -p 'Request: schedule a call at 8pm; Command:'

# {"appointmentTime": "8pm", "appointmentDetails": "schedule a a call"}
&lt;/code&gt;&lt;/pre&gt; 
   &lt;p&gt;The &lt;a href="https://raw.githubusercontent.com/ggml-org/llama.cpp/master/grammars/"&gt;grammars/&lt;/a&gt; folder contains a handful of sample grammars. To write your own, check out the &lt;a href="https://raw.githubusercontent.com/ggml-org/llama.cpp/master/grammars/README.md"&gt;GBNF Guide&lt;/a&gt;.&lt;/p&gt; 
   &lt;p&gt;For authoring more complex JSON grammars, check out &lt;a href="https://grammar.intrinsiclabs.ai/"&gt;https://grammar.intrinsiclabs.ai/&lt;/a&gt;&lt;/p&gt; 
  &lt;/details&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;a href="https://raw.githubusercontent.com/ggml-org/llama.cpp/master/tools/server"&gt;&lt;code&gt;llama-server&lt;/code&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;h4&gt;A lightweight, &lt;a href="https://github.com/openai/openai-openapi"&gt;OpenAI API&lt;/a&gt; compatible, HTTP server for serving LLMs.&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt; 
  &lt;details open&gt; 
   &lt;summary&gt;Start a local HTTP server with default configuration on port 8080&lt;/summary&gt; 
   &lt;pre&gt;&lt;code class="language-bash"&gt;llama-server -m model.gguf --port 8080

# Basic web UI can be accessed via browser: http://localhost:8080
# Chat completion endpoint: http://localhost:8080/v1/chat/completions
&lt;/code&gt;&lt;/pre&gt; 
  &lt;/details&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;details&gt; 
   &lt;summary&gt;Support multiple-users and parallel decoding&lt;/summary&gt; 
   &lt;pre&gt;&lt;code class="language-bash"&gt;# up to 4 concurrent requests, each with 4096 max context
llama-server -m model.gguf -c 16384 -np 4
&lt;/code&gt;&lt;/pre&gt; 
  &lt;/details&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;details&gt; 
   &lt;summary&gt;Enable speculative decoding&lt;/summary&gt; 
   &lt;pre&gt;&lt;code class="language-bash"&gt;# the draft.gguf model should be a small variant of the target model.gguf
llama-server -m model.gguf -md draft.gguf
&lt;/code&gt;&lt;/pre&gt; 
  &lt;/details&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;details&gt; 
   &lt;summary&gt;Serve an embedding model&lt;/summary&gt; 
   &lt;pre&gt;&lt;code class="language-bash"&gt;# use the /embedding endpoint
llama-server -m model.gguf --embedding --pooling cls -ub 8192
&lt;/code&gt;&lt;/pre&gt; 
  &lt;/details&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;details&gt; 
   &lt;summary&gt;Serve a reranking model&lt;/summary&gt; 
   &lt;pre&gt;&lt;code class="language-bash"&gt;# use the /reranking endpoint
llama-server -m model.gguf --reranking
&lt;/code&gt;&lt;/pre&gt; 
  &lt;/details&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;details&gt; 
   &lt;summary&gt;Constrain all outputs with a grammar&lt;/summary&gt; 
   &lt;pre&gt;&lt;code class="language-bash"&gt;# custom grammar
llama-server -m model.gguf --grammar-file grammar.gbnf

# JSON
llama-server -m model.gguf --grammar-file grammars/json.gbnf
&lt;/code&gt;&lt;/pre&gt; 
  &lt;/details&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;a href="https://raw.githubusercontent.com/ggml-org/llama.cpp/master/tools/perplexity"&gt;&lt;code&gt;llama-perplexity&lt;/code&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;h4&gt;A tool for measuring the &lt;a href="https://raw.githubusercontent.com/ggml-org/llama.cpp/master/tools/perplexity/README.md"&gt;perplexity&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/ggml-org/llama.cpp/master/%5Bhttps://huggingface.co/docs/transformers/perplexity%5D(https://huggingface.co/docs/transformers/perplexity)"&gt;^1&lt;/a&gt; (and other quality metrics) of a model over a given text.&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt; 
  &lt;details open&gt; 
   &lt;summary&gt;Measure the perplexity over a text file&lt;/summary&gt; 
   &lt;pre&gt;&lt;code class="language-bash"&gt;llama-perplexity -m model.gguf -f file.txt

# [1]15.2701,[2]5.4007,[3]5.3073,[4]6.2965,[5]5.8940,[6]5.6096,[7]5.7942,[8]4.9297, ...
# Final estimate: PPL = 5.4007 +/- 0.67339
&lt;/code&gt;&lt;/pre&gt; 
  &lt;/details&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;details&gt; 
   &lt;summary&gt;Measure KL divergence&lt;/summary&gt; 
   &lt;pre&gt;&lt;code class="language-bash"&gt;# TODO
&lt;/code&gt;&lt;/pre&gt; 
  &lt;/details&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;a href="https://raw.githubusercontent.com/ggml-org/llama.cpp/master/tools/llama-bench"&gt;&lt;code&gt;llama-bench&lt;/code&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;h4&gt;Benchmark the performance of the inference for various parameters.&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt; 
  &lt;details open&gt; 
   &lt;summary&gt;Run default benchmark&lt;/summary&gt; 
   &lt;pre&gt;&lt;code class="language-bash"&gt;llama-bench -m model.gguf

# Output:
# | model               |       size |     params | backend    | threads |          test |                  t/s |
# | ------------------- | ---------: | ---------: | ---------- | ------: | ------------: | -------------------: |
# | qwen2 1.5B Q4_0     | 885.97 MiB |     1.54 B | Metal,BLAS |      16 |         pp512 |      5765.41 Â± 20.55 |
# | qwen2 1.5B Q4_0     | 885.97 MiB |     1.54 B | Metal,BLAS |      16 |         tg128 |        197.71 Â± 0.81 |
#
# build: 3e0ba0e60 (4229)
&lt;/code&gt;&lt;/pre&gt; 
  &lt;/details&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;a href="https://raw.githubusercontent.com/ggml-org/llama.cpp/master/tools/run"&gt;&lt;code&gt;llama-run&lt;/code&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;h4&gt;A comprehensive example for running &lt;code&gt;llama.cpp&lt;/code&gt; models. Useful for inferencing. Used with RamaLama &lt;a href="https://raw.githubusercontent.com/ggml-org/llama.cpp/master/%5BRamaLama%5D(https://github.com/containers/ramalama)"&gt;^3&lt;/a&gt;.&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt; 
  &lt;details&gt; 
   &lt;summary&gt;Run a model with a specific prompt (by default it's pulled from Ollama registry)&lt;/summary&gt; 
   &lt;pre&gt;&lt;code class="language-bash"&gt;llama-run granite-code
&lt;/code&gt;&lt;/pre&gt; 
  &lt;/details&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;a href="https://raw.githubusercontent.com/ggml-org/llama.cpp/master/examples/simple"&gt;&lt;code&gt;llama-simple&lt;/code&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;h4&gt;A minimal example for implementing apps with &lt;code&gt;llama.cpp&lt;/code&gt;. Useful for developers.&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt; 
  &lt;details&gt; 
   &lt;summary&gt;Basic text completion&lt;/summary&gt; 
   &lt;pre&gt;&lt;code class="language-bash"&gt;llama-simple -m model.gguf

# Hello my name is Kaitlyn and I am a 16 year old girl. I am a junior in high school and I am currently taking a class called "The Art of
&lt;/code&gt;&lt;/pre&gt; 
  &lt;/details&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Contributors can open PRs&lt;/li&gt; 
 &lt;li&gt;Collaborators can push to branches in the &lt;code&gt;llama.cpp&lt;/code&gt; repo and merge PRs into the &lt;code&gt;master&lt;/code&gt; branch&lt;/li&gt; 
 &lt;li&gt;Collaborators will be invited based on contributions&lt;/li&gt; 
 &lt;li&gt;Any help with managing issues, PRs and projects is very appreciated!&lt;/li&gt; 
 &lt;li&gt;See &lt;a href="https://github.com/ggml-org/llama.cpp/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22"&gt;good first issues&lt;/a&gt; for tasks suitable for first contributions&lt;/li&gt; 
 &lt;li&gt;Read the &lt;a href="https://raw.githubusercontent.com/ggml-org/llama.cpp/master/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for more information&lt;/li&gt; 
 &lt;li&gt;Make sure to read this: &lt;a href="https://github.com/ggml-org/llama.cpp/discussions/205"&gt;Inference at the edge&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;A bit of backstory for those who are interested: &lt;a href="https://changelog.com/podcast/532"&gt;Changelog podcast&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Other documentation&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ggml-org/llama.cpp/master/tools/main/README.md"&gt;main (cli)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ggml-org/llama.cpp/master/tools/server/README.md"&gt;server&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ggml-org/llama.cpp/master/grammars/README.md"&gt;GBNF grammars&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Development documentation&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/build.md"&gt;How to build&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/docker.md"&gt;Running on Docker&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/android.md"&gt;Build on Android&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/development/token_generation_performance_tips.md"&gt;Performance troubleshooting&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/wiki/GGML-Tips-&amp;amp;-Tricks"&gt;GGML tips &amp;amp; tricks&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Seminal papers and background on the models&lt;/h4&gt; 
&lt;p&gt;If your issue is with model generation quality, then please at least scan the following links and papers to understand the limitations of LLaMA models. This is especially important when choosing an appropriate model size and appreciating both the significant and subtle differences between LLaMA models and ChatGPT:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;LLaMA: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://ai.facebook.com/blog/large-language-model-llama-meta-ai/"&gt;Introducing LLaMA: A foundational, 65-billion-parameter large language model&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://arxiv.org/abs/2302.13971"&gt;LLaMA: Open and Efficient Foundation Language Models&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;GPT-3 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://arxiv.org/abs/2005.14165"&gt;Language Models are Few-Shot Learners&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;GPT-3.5 / InstructGPT / ChatGPT: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://openai.com/research/instruction-following"&gt;Aligning language models to follow instructions&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://arxiv.org/abs/2203.02155"&gt;Training language models to follow instructions with human feedback&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;XCFramework&lt;/h2&gt; 
&lt;p&gt;The XCFramework is a precompiled version of the library for iOS, visionOS, tvOS, and macOS. It can be used in Swift projects without the need to compile the library from source. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-swift"&gt;// swift-tools-version: 5.10
// The swift-tools-version declares the minimum version of Swift required to build this package.

import PackageDescription

let package = Package(
    name: "MyLlamaPackage",
    targets: [
        .executableTarget(
            name: "MyLlamaPackage",
            dependencies: [
                "LlamaFramework"
            ]),
        .binaryTarget(
            name: "LlamaFramework",
            url: "https://github.com/ggml-org/llama.cpp/releases/download/b5046/llama-b5046-xcframework.zip",
            checksum: "c19be78b5f00d8d29a25da41042cb7afa094cbf6280a225abe614b03b20029ab"
        )
    ]
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The above example is using an intermediate build &lt;code&gt;b5046&lt;/code&gt; of the library. This can be modified to use a different version by changing the URL and checksum.&lt;/p&gt; 
&lt;h2&gt;Completions&lt;/h2&gt; 
&lt;p&gt;Command-line completion is available for some environments.&lt;/p&gt; 
&lt;h4&gt;Bash Completion&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;$ build/bin/llama-cli --completion-bash &amp;gt; ~/.llama-completion.bash
$ source ~/.llama-completion.bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Optionally this can be added to your &lt;code&gt;.bashrc&lt;/code&gt; or &lt;code&gt;.bash_profile&lt;/code&gt; to load it automatically. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-console"&gt;$ echo "source ~/.llama-completion.bash" &amp;gt;&amp;gt; ~/.bashrc
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Dependencies&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/yhirose/cpp-httplib"&gt;yhirose/cpp-httplib&lt;/a&gt; - Single-header HTTP server, used by &lt;code&gt;llama-server&lt;/code&gt; - MIT license&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/nothings/stb"&gt;stb-image&lt;/a&gt; - Single-header image format decoder, used by multimodal subsystem - Public domain&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/nlohmann/json"&gt;nlohmann/json&lt;/a&gt; - Single-header JSON library, used by various tools/examples - MIT License&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/google/minja"&gt;minja&lt;/a&gt; - Minimal Jinja parser in C++, used by various tools/examples - MIT License&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/ggml-org/llama.cpp/master/tools/run/linenoise.cpp/linenoise.cpp"&gt;linenoise.cpp&lt;/a&gt; - C++ library that provides readline-like line editing capabilities, used by &lt;code&gt;llama-run&lt;/code&gt; - BSD 2-Clause License&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://curl.se/"&gt;curl&lt;/a&gt; - Client-side URL transfer library, used by various tools/examples - &lt;a href="https://curl.se/docs/copyright.html"&gt;CURL License&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/mackron/miniaudio"&gt;miniaudio.h&lt;/a&gt; - Single-header audio format decoder, used by multimodal subsystem - Public domain&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>dyad-sh/dyad</title>
      <link>https://github.com/dyad-sh/dyad</link>
      <description>&lt;p&gt;Free, local, open-source AI app builder âœ¨ v0 / lovable / Bolt alternative ğŸŒŸ Star if you like it!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Dyad&lt;/h1&gt; 
&lt;p&gt;Dyad is a local, open-source AI app builder. It's fast, private, and fully under your control â€” like Lovable, v0, or Bolt, but running right on your machine.&lt;/p&gt; 
&lt;p&gt;&lt;a href="http://dyad.sh/"&gt;&lt;img src="https://github.com/user-attachments/assets/f6c83dfc-6ffd-4d32-93dd-4b9c46d17790" alt="Image" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;More info at: &lt;a href="http://dyad.sh/"&gt;http://dyad.sh/&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;ğŸš€ Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;âš¡ï¸ &lt;strong&gt;Local&lt;/strong&gt;: Fast, private and no lock-in.&lt;/li&gt; 
 &lt;li&gt;ğŸ›  &lt;strong&gt;Bring your own keys&lt;/strong&gt;: Use your own AI API keys â€” no vendor lock-in.&lt;/li&gt; 
 &lt;li&gt;ğŸ–¥ï¸ &lt;strong&gt;Cross-platform&lt;/strong&gt;: Easy to run on Mac or Windows.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸ“¦ Download&lt;/h2&gt; 
&lt;p&gt;No sign-up required. Just download and go.&lt;/p&gt; 
&lt;h3&gt;&lt;a href="https://www.dyad.sh/#download"&gt;ğŸ‘‰ Download for your platform&lt;/a&gt;&lt;/h3&gt; 
&lt;h2&gt;ğŸ¤ Community&lt;/h2&gt; 
&lt;p&gt;Join our growing community of AI app builders on &lt;strong&gt;Reddit&lt;/strong&gt;: &lt;a href="https://www.reddit.com/r/dyadbuilders/"&gt;r/dyadbuilders&lt;/a&gt; - share your projects and get help from the community!&lt;/p&gt; 
&lt;h2&gt;ğŸ› ï¸ Contributing&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Dyad&lt;/strong&gt; is open-source (Apache 2.0 licensed).&lt;/p&gt; 
&lt;p&gt;If you're interested in contributing to dyad, please read our &lt;a href="https://raw.githubusercontent.com/dyad-sh/dyad/main/CONTRIBUTING.md"&gt;contributing&lt;/a&gt; doc.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>dtyq/magic</title>
      <link>https://github.com/dtyq/magic</link>
      <description>&lt;p&gt;Super Magic. The first open-source all-in-one AI productivity platform (Generalist AI Agent + Workflow Engine + IM + Online collaborative office system)&lt;/p&gt;&lt;hr&gt;&lt;div align="left"&gt; 
 &lt;a href="https://raw.githubusercontent.com/dtyq/magic/master/README.md"&gt;&lt;img alt="README in English" src="https://img.shields.io/badge/English-d9d9d9" /&gt;&lt;/a&gt; 
 &lt;a href="https://raw.githubusercontent.com/dtyq/magic/master/README_CN.md"&gt;&lt;img alt="ç®€ä½“ä¸­æ–‡ç‰ˆè‡ªè¿°æ–‡ä»¶" src="https://img.shields.io/badge/ç®€ä½“ä¸­æ–‡-d9d9d9" /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;img src="https://public-cdn.letsmagic.ai/static/img/super-magic-publish-header-en.png?v=20250819" alt="Magic Open Source Product Matrix" /&gt;&lt;/p&gt; 
&lt;h1&gt;ğŸ”¥ Magic - First Open-Source All-in-One AI Productivity Platform&lt;/h1&gt; 
&lt;div align="center"&gt; 
 &lt;p align="center"&gt; &lt;a href="https://www.letsmagic.ai" target="_blank"&gt; &lt;img alt="Static Badge" src="https://img.shields.io/badge/Official Website-301AD2" /&gt; &lt;/a&gt; &lt;a href="https://github.com/dtyq/magic/releases"&gt; &lt;img src="https://poser.pugx.org/dtyq/magic/v/stable" alt="Stable Version" /&gt; &lt;/a&gt; &lt;a href="https://github.com/dtyq/magic/graphs/commit-activity" target="_blank"&gt; &lt;img alt="Commits last month" src="https://img.shields.io/github/commit-activity/m/dtyq/magic?labelColor=%20%2332b583&amp;amp;color=%20%2312b76a" /&gt; &lt;/a&gt; &lt;a href="https://github.com/dtyq/magic/" target="_blank"&gt; &lt;img alt="Issues closed" src="https://img.shields.io/github/issues-search?query=repo%3Adtyq%2Fmagic%20is%3Aclosed&amp;amp;label=issues%20closed&amp;amp;labelColor=%20%237d89b0&amp;amp;color=%20%235d6b98" /&gt; &lt;/a&gt; &lt;a href="https://github.com/dtyq/magic/discussions/" target="_blank"&gt; &lt;img alt="Discussion posts" src="https://img.shields.io/github/discussions/dtyq/magic?labelColor=%20%239b8afb&amp;amp;color=%20%237a5af8" /&gt; &lt;/a&gt; &lt;a href="https://www.letsmagic.ai" target="_blank"&gt; &lt;img alt="Static Badge" src="https://img.shields.io/badge/Build with Magic ğŸ”®-301AD2" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;Magic aims to help enterprises of all sizes quickly build and deploy AI applications to achieve a 100x increase in productivity.&lt;/p&gt; 
&lt;h2&gt;Magic Product Matrix&lt;/h2&gt; 
&lt;p&gt;Magic is the first &lt;strong&gt;"open-source all-in-one AI productivity platform"&lt;/strong&gt;, not a single AI product, but a comprehensive product matrix with rich capabilities.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://public-cdn.letsmagic.ai/static/img/super-magic-open-source-projects-en.png?v=20250819" alt="Product Matrix" /&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/dtyq/super-magic"&gt;Super Magic&lt;/a&gt;&lt;/strong&gt; - A &lt;strong&gt;general-purpose AI Agent&lt;/strong&gt; designed for complex task scenarios&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/dtyq/magic"&gt;Magic IM&lt;/a&gt;&lt;/strong&gt; - An enterprise-grade instant messaging system that integrates AI Agent conversations with internal enterprise communication&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/dtyq/magic"&gt;Magic Flow&lt;/a&gt;&lt;/strong&gt; - A powerful visual AI workflow orchestration system&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Teamshare OS&lt;/strong&gt; (Coming soon) - An enterprise-grade online collaborative office system&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;In addition to the above AI products, we have also open-sourced some of the infrastructure we used to build these products:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/dtyq/agentlang"&gt;Agentlang&lt;/a&gt;&lt;/strong&gt; - A language-first AI Agent Framework for building AI agents with natural language (currently available in Python version, TypeScript version coming soon)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href="https://github.com/dtyq/magiclens"&gt;Magic Lens&lt;/a&gt;&lt;/strong&gt; - A powerful and flexible HTML to Markdown conversion tool that uses an extensible rule system to accurately convert complex HTML documents to concise Markdown format&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Magic Use&lt;/strong&gt; (Coming soon) - A revolutionary browser operation tool specifically designed for AI Agents&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Magic Space&lt;/strong&gt; (Coming soon) - A new static content hosting management system specifically designed for AI Agents&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Sandbox OS&lt;/strong&gt; (Coming soon) - A powerful sandbox system for AI Agent runtime&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Super Magic&lt;/h3&gt; 
&lt;p&gt;A powerful &lt;strong&gt;general-purpose AI Agent&lt;/strong&gt; specially designed for complex task scenarios. Through a multi-agent design system and rich tool capabilities, Super Magic supports intelligent abilities such as &lt;strong&gt;autonomous task understanding&lt;/strong&gt;, &lt;strong&gt;autonomous task planning&lt;/strong&gt;, &lt;strong&gt;autonomous action&lt;/strong&gt;, and &lt;strong&gt;autonomous error correction&lt;/strong&gt;. It can understand natural language instructions, execute various business processes, and deliver final target results. As the flagship product of the Magic product matrix, Super Magic provides powerful secondary development capabilities through open source, allowing enterprises to quickly build and deploy intelligent assistants that meet specific business needs, greatly improving decision-making efficiency and quality.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://public-cdn.letsmagic.ai/static/img/super-magic-buffett.gif" alt="Super Magic" /&gt;&lt;/p&gt; 
&lt;h4&gt;Super Magic Case Studies&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.letsmagic.cn/share/777665156986277889"&gt;Analysis of Investment Insights from Buffett's 2025 Shareholders Meeting&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.letsmagic.cn/share/774280936479625217"&gt;Analysis of Stocks Related to Beijing Humanoid Robot Half Marathon&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.letsmagic.cn/share/777461325648195584"&gt;Summary of Key Points from 'Thinking, Fast and Slow&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.letsmagic.cn/share/777604044873928705"&gt;Auntie Jenny IPO Analysis and Investment Recommendations&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.letsmagic.cn/share/771022574397648897"&gt;SKU Sales Forecast Requirements&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;For more case studies, please visit the &lt;a href="https://www.letsmagic.ai"&gt;Official Website&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Magic Flow&lt;/h3&gt; 
&lt;p&gt;Magic Flow is a powerful visual AI workflow orchestration system that allows users to build complex AI Agent workflows on a free canvas. It has the following core features:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Visual Orchestration&lt;/strong&gt;: Intuitive drag-and-drop interface allows designing complex AI workflows without coding, easily implementing various functional combinations through node connections.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Rich Component Library&lt;/strong&gt;: Built-in variety of preset components, including text processing, image generation, code execution modules, meeting diverse business needs.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Comprehensive Model Support&lt;/strong&gt;: Compatible with any large model following the OpenAI API protocol, flexibly choosing AI capabilities suitable for business scenarios.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;System Integration Capability&lt;/strong&gt;: Seamless integration with Magic IM and other third-party IM systems (WeCom, DingTalk, Feishu), enabling cross-platform collaboration.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Custom Extensions&lt;/strong&gt;: Support for custom tool node development to meet specific business scenario requirements.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Real-time Debugging and Monitoring&lt;/strong&gt;: Providing comprehensive debugging and monitoring functions to help quickly identify and solve problems in workflows, ensuring stable operation of AI applications.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://public-cdn.letsmagic.ai/static/img/magic-flow.png" alt="Magic Flow" /&gt;&lt;/p&gt; 
&lt;p&gt;As an important component of the Magic product matrix, Magic Flow can be seamlessly integrated with other Magic products to create a complete enterprise-level AI application ecosystem.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://public-cdn.letsmagic.ai/static/img/super-magic-multi-agents-and-events-en.png?v=20250819" alt="Magic Multi-Agents and Events" /&gt;&lt;/p&gt; 
&lt;h3&gt;Magic IM&lt;/h3&gt; 
&lt;p&gt;Magic IM is an enterprise-grade AI Agent conversation system designed specifically for internal knowledge management and intelligent customer service scenarios. It provides rich conversational capabilities, supporting multi-turn dialogues, context understanding, knowledge base retrieval, and other functions, allowing enterprises to quickly build intelligent customer service, knowledge assistants, and other applications.&lt;/p&gt; 
&lt;p&gt;Magic IM has the following core features:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Knowledge Base Management&lt;/strong&gt;: Powerful knowledge base management functions, supporting import of various document formats, automatic indexing, and semantic retrieval, ensuring AI answers based on authentic enterprise knowledge.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Conversation Management&lt;/strong&gt;: Comprehensive conversation management, supporting topic distinction for different conversation content, enabling both AI Agent conversations and communication with people within the organization.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Group Chat Capability&lt;/strong&gt;: Powerful group chat functionality, supporting real-time collaborative discussions among multiple people, with AI intelligently participating in group chats and providing instant answers, promoting efficient team communication and knowledge sharing.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-organizational Architecture&lt;/strong&gt;: Support for multi-organization deployment and strict organizational data isolation, with each organization having independent data space and access permissions.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Data Security&lt;/strong&gt;: Strict data isolation and access control mechanisms, multi-level permission management, safeguarding sensitive enterprise information and ensuring no data leakage between organizations.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://public-cdn.letsmagic.ai/static/img/magic-im-group-chat-en.png?v=20250819" alt="Magic IM" /&gt;&lt;/p&gt; 
&lt;h2&gt;Teamshare OS&lt;/h2&gt; 
&lt;p&gt;Teamshare OS is a modern enterprise-grade collaborative office platform designed to enhance team collaboration efficiency and knowledge management. As an important component of the Magic product matrix, Teamshare deeply integrates AI capabilities into daily office scenarios, achieving intelligent workflows and knowledge management.&lt;/p&gt; 
&lt;p&gt;Teamshare OS has the following core features:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Intelligent Document Management&lt;/strong&gt;: Support for online editing, collaboration, and version control of various document formats, AI-assisted content generation and optimization, making team document management more efficient.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Magic Table&lt;/strong&gt;: Powerful multi-dimensional data management tool, supporting custom field types, diverse views, and automated workflows, combined with AI capabilities to achieve intelligent data processing, meeting diverse data management needs.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Project Collaboration Management&lt;/strong&gt;: Intuitive project boards and task management, supporting custom workflows, combined with AI intelligent analysis to provide project progress forecasting and resource optimization suggestions.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Knowledge Base&lt;/strong&gt;: Powerful knowledge consolidation and retrieval system, automatically structuring internal enterprise documents to form sustainable accumulated enterprise knowledge assets.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Comprehensive Integration Capability&lt;/strong&gt;: Seamless integration with Magic product matrix, while supporting connection with mainstream office software and enterprise applications, creating a unified work platform.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Magic Table&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://gist.github.com/user-attachments/assets/6ef46e66-292c-4a8a-8a00-a3b9fb7beec7"&gt;https://gist.github.com/user-attachments/assets/6ef46e66-292c-4a8a-8a00-a3b9fb7beec7&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Magic Doc&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://gist.github.com/user-attachments/assets/7327f331-be7d-4aeb-8e19-0949adde66b2"&gt;https://gist.github.com/user-attachments/assets/7327f331-be7d-4aeb-8e19-0949adde66b2&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;ğŸš€ Using Super Magic&lt;/h2&gt; 
&lt;h3&gt;Cloud Service&lt;/h3&gt; 
&lt;p&gt;We provide &lt;a href="https://www.letsmagic.ai"&gt;cloud services&lt;/a&gt; for &lt;a href="https://www.letsmagic.ai"&gt;Super Magic&lt;/a&gt;, &lt;a href="https://www.letsmagic.ai"&gt;Magic IM&lt;/a&gt;, and &lt;a href="https://www.letsmagic.ai"&gt;Magic Flow&lt;/a&gt;, allowing anyone to start trying and using them with zero setup, providing all features of the open-source version. &lt;em&gt;Currently, an invitation code is required for access, which can be applied for online and granted for trial use after approval.&lt;/em&gt;&lt;/p&gt; 
&lt;h3&gt;Magic for Enterprises/Organizations&lt;/h3&gt; 
&lt;p&gt;We provide more powerful management capabilities and features for teams and enterprises. &lt;a href="mailto:bd@dtyq.com?subject=%5BGitHub%5DBusiness%20License%20Inquiry"&gt;Send us an email&lt;/a&gt; to discuss enterprise needs.&lt;/p&gt; 
&lt;h3&gt;Self-hosted Community Edition&lt;/h3&gt; 
&lt;h4&gt;System Requirements&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Docker 24.0+&lt;/li&gt; 
 &lt;li&gt;Docker Compose 2.0+&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Start the System Using Docker&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Clone repository
git clone https://github.com/dtyq/magic.git
cd magic

# Start service in foreground
./bin/magic.sh start
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;Other Commands&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Start service in background
./bin/magic.sh daemon

# Check service status
./bin/magic.sh status

# View logs
./bin/magic.sh logs
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;Configure Environment Variables&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Configure Magic environment variables, must configure at least one large language model's environment variables to use Magic normally
cp .env.example .env

# Configure Super Magic environment variables, must configure any large language model that supports OpenAI format to use it normally
./bin/magic.sh status
cp config/.env_super_magic.example .env_super_magic
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;Access Services&lt;/h5&gt; 
&lt;ul&gt; 
 &lt;li&gt;API Service: &lt;a href="http://localhost:9501"&gt;http://localhost:9501&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Web Application: &lt;a href="http://localhost:8080"&gt;http://localhost:8080&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Account &lt;code&gt;13812345678&lt;/code&gt;ï¼šPassword &lt;code&gt;letsmagic.ai&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Account &lt;code&gt;13912345678&lt;/code&gt;ï¼šPassword &lt;code&gt;letsmagic.ai&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;RabbitMQ Management Interface: &lt;a href="http://localhost:15672"&gt;http://localhost:15672&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Username: admin&lt;/li&gt; 
   &lt;li&gt;Password: magic123456&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Official Website: &lt;a href="https://www.letsmagic.ai"&gt;https://www.letsmagic.ai&lt;/a&gt; Documentation: &lt;a href="https://docs.letsmagic.cn/en"&gt;https://docs.letsmagic.cn/en&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Contribution&lt;/h2&gt; 
&lt;p&gt;For those who want to contribute code, please refer to our &lt;a href="https://github.com/dtyq/magic/raw/master/CONTRIBUTING.md"&gt;Contribution Guide&lt;/a&gt;. Also, please consider supporting Magic through social media, events, and conferences. The development of Magic relies on your support.&lt;/p&gt; 
&lt;h2&gt;Security Vulnerabilities&lt;/h2&gt; 
&lt;p&gt;If you discover a security vulnerability in Magic, please send an email to the Magic official team at &lt;a href="mailto:team@dtyq.com"&gt;team@dtyq.com&lt;/a&gt;. All security vulnerabilities will be promptly addressed.&lt;/p&gt; 
&lt;h2&gt;ğŸ“„ License&lt;/h2&gt; 
&lt;p&gt;This repository follows the &lt;a href="https://raw.githubusercontent.com/dtyq/magic/master/LICENSE"&gt;Magic Open Source License&lt;/a&gt;, which is essentially Apache 2.0 but with some additional restrictions.&lt;/p&gt; 
&lt;h2&gt;ğŸ™ Acknowledgements&lt;/h2&gt; 
&lt;p&gt;Thanks to all developers who have contributed to Magic!&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://star-history.com/#dtyq/magic&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=dtyq/magic&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>vllm-project/vllm</title>
      <link>https://github.com/vllm-project/vllm</link>
      <description>&lt;p&gt;A high-throughput and memory-efficient inference and serving engine for LLMs&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://raw.githubusercontent.com/vllm-project/vllm/main/docs/assets/logos/vllm-logo-text-dark.png" /&gt; 
  &lt;img alt="vLLM" src="https://raw.githubusercontent.com/vllm-project/vllm/main/docs/assets/logos/vllm-logo-text-light.png" width="55%" /&gt; 
 &lt;/picture&gt; &lt;/p&gt; 
&lt;h3 align="center"&gt; Easy, fast, and cheap LLM serving for everyone &lt;/h3&gt; 
&lt;p align="center"&gt; | &lt;a href="https://docs.vllm.ai"&gt;&lt;b&gt;Documentation&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://blog.vllm.ai/"&gt;&lt;b&gt;Blog&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://arxiv.org/abs/2309.06180"&gt;&lt;b&gt;Paper&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://x.com/vllm_project"&gt;&lt;b&gt;Twitter/X&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://discuss.vllm.ai"&gt;&lt;b&gt;User Forum&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://slack.vllm.ai"&gt;&lt;b&gt;Developer Slack&lt;/b&gt;&lt;/a&gt; | &lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;em&gt;Latest News&lt;/em&gt; ğŸ”¥&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;[2025/08] We hosted &lt;a href="https://mp.weixin.qq.com/s/dgkWg1WFpWGO2jCdTqQHxA"&gt;vLLM Beijing Meetup&lt;/a&gt; focusing on large-scale LLM deployment! Please find the meetup slides &lt;a href="https://drive.google.com/drive/folders/1Pid6NSFLU43DZRi0EaTcPgXsAzDvbBqF"&gt;here&lt;/a&gt; and the recording &lt;a href="https://www.chaspark.com/#/live/1166916873711665152"&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;[2025/05] We hosted &lt;a href="https://lu.ma/c1rqyf1f"&gt;NYC vLLM Meetup&lt;/a&gt;! Please find the meetup slides &lt;a href="https://docs.google.com/presentation/d/1_q_aW_ioMJWUImf1s1YM-ZhjXz8cUeL0IJvaquOYBeA/edit?usp=sharing"&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;[2025/05] vLLM is now a hosted project under PyTorch Foundation! Please find the announcement &lt;a href="https://pytorch.org/blog/pytorch-foundation-welcomes-vllm/"&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;[2025/01] We are excited to announce the alpha release of vLLM V1: A major architectural upgrade with 1.7x speedup! Clean code, optimized execution loop, zero-overhead prefix caching, enhanced multimodal support, and more. Please check out our blog post &lt;a href="https://blog.vllm.ai/2025/01/27/v1-alpha-release.html"&gt;here&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;Previous News&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;[2025/04] We hosted &lt;a href="https://www.sginnovate.com/event/limited-availability-morning-evening-slots-remaining-inaugural-vllm-asia-developer-day"&gt;Asia Developer Day&lt;/a&gt;! Please find the meetup slides from the vLLM team &lt;a href="https://docs.google.com/presentation/d/19cp6Qu8u48ihB91A064XfaXruNYiBOUKrBxAmDOllOo/edit?usp=sharing"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2025/03] We hosted &lt;a href="https://lu.ma/vllm-ollama"&gt;vLLM x Ollama Inference Night&lt;/a&gt;! Please find the meetup slides from the vLLM team &lt;a href="https://docs.google.com/presentation/d/16T2PDD1YwRnZ4Tu8Q5r6n53c5Lr5c73UV9Vd2_eBo4U/edit?usp=sharing"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2025/03] We hosted &lt;a href="https://mp.weixin.qq.com/s/n77GibL2corAtQHtVEAzfg"&gt;the first vLLM China Meetup&lt;/a&gt;! Please find the meetup slides from vLLM team &lt;a href="https://docs.google.com/presentation/d/1REHvfQMKGnvz6p3Fd23HhSO4c8j5WPGZV0bKYLwnHyQ/edit?usp=sharing"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2025/03] We hosted &lt;a href="https://lu.ma/7mu4k4xx"&gt;the East Coast vLLM Meetup&lt;/a&gt;! Please find the meetup slides &lt;a href="https://docs.google.com/presentation/d/1NHiv8EUFF1NLd3fEYODm56nDmL26lEeXCaDgyDlTsRs/edit#slide=id.g31441846c39_0_0"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2025/02] We hosted &lt;a href="https://lu.ma/h7g3kuj9"&gt;the ninth vLLM meetup&lt;/a&gt; with Meta! Please find the meetup slides from vLLM team &lt;a href="https://docs.google.com/presentation/d/1jzC_PZVXrVNSFVCW-V4cFXb6pn7zZ2CyP_Flwo05aqg/edit?usp=sharing"&gt;here&lt;/a&gt; and AMD &lt;a href="https://drive.google.com/file/d/1Zk5qEJIkTmlQ2eQcXQZlljAx3m9s7nwn/view?usp=sharing"&gt;here&lt;/a&gt;. The slides from Meta will not be posted.&lt;/li&gt; 
  &lt;li&gt;[2025/01] We hosted &lt;a href="https://lu.ma/zep56hui"&gt;the eighth vLLM meetup&lt;/a&gt; with Google Cloud! Please find the meetup slides from vLLM team &lt;a href="https://docs.google.com/presentation/d/1epVkt4Zu8Jz_S5OhEHPc798emsYh2BwYfRuDDVEF7u4/edit?usp=sharing"&gt;here&lt;/a&gt;, and Google Cloud team &lt;a href="https://drive.google.com/file/d/1h24pHewANyRL11xy5dXUbvRC9F9Kkjix/view?usp=sharing"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2024/12] vLLM joins &lt;a href="https://pytorch.org/blog/vllm-joins-pytorch"&gt;pytorch ecosystem&lt;/a&gt;! Easy, Fast, and Cheap LLM Serving for Everyone!&lt;/li&gt; 
  &lt;li&gt;[2024/11] We hosted &lt;a href="https://lu.ma/h0qvrajz"&gt;the seventh vLLM meetup&lt;/a&gt; with Snowflake! Please find the meetup slides from vLLM team &lt;a href="https://docs.google.com/presentation/d/1e3CxQBV3JsfGp30SwyvS3eM_tW-ghOhJ9PAJGK6KR54/edit?usp=sharing"&gt;here&lt;/a&gt;, and Snowflake team &lt;a href="https://docs.google.com/presentation/d/1qF3RkDAbOULwz9WK5TOltt2fE9t6uIc_hVNLFAaQX6A/edit?usp=sharing"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2024/10] We have just created a developer slack (&lt;a href="https://slack.vllm.ai"&gt;slack.vllm.ai&lt;/a&gt;) focusing on coordinating contributions and discussing features. Please feel free to join us there!&lt;/li&gt; 
  &lt;li&gt;[2024/10] Ray Summit 2024 held a special track for vLLM! Please find the opening talk slides from the vLLM team &lt;a href="https://docs.google.com/presentation/d/1B_KQxpHBTRa_mDF-tR6i8rWdOU5QoTZNcEg2MKZxEHM/edit?usp=sharing"&gt;here&lt;/a&gt;. Learn more from the &lt;a href="https://www.youtube.com/playlist?list=PLzTswPQNepXl6AQwifuwUImLPFRVpksjR"&gt;talks&lt;/a&gt; from other vLLM contributors and users!&lt;/li&gt; 
  &lt;li&gt;[2024/09] We hosted &lt;a href="https://lu.ma/87q3nvnh"&gt;the sixth vLLM meetup&lt;/a&gt; with NVIDIA! Please find the meetup slides &lt;a href="https://docs.google.com/presentation/d/1wrLGwytQfaOTd5wCGSPNhoaW3nq0E-9wqyP7ny93xRs/edit?usp=sharing"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2024/07] We hosted &lt;a href="https://lu.ma/lp0gyjqr"&gt;the fifth vLLM meetup&lt;/a&gt; with AWS! Please find the meetup slides &lt;a href="https://docs.google.com/presentation/d/1RgUD8aCfcHocghoP3zmXzck9vX3RCI9yfUAB2Bbcl4Y/edit?usp=sharing"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2024/07] In partnership with Meta, vLLM officially supports Llama 3.1 with FP8 quantization and pipeline parallelism! Please check out our blog post &lt;a href="https://blog.vllm.ai/2024/07/23/llama31.html"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2024/06] We hosted &lt;a href="https://lu.ma/agivllm"&gt;the fourth vLLM meetup&lt;/a&gt; with Cloudflare and BentoML! Please find the meetup slides &lt;a href="https://docs.google.com/presentation/d/1iJ8o7V2bQEi0BFEljLTwc5G1S10_Rhv3beed5oB0NJ4/edit?usp=sharing"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2024/04] We hosted &lt;a href="https://robloxandvllmmeetup2024.splashthat.com/"&gt;the third vLLM meetup&lt;/a&gt; with Roblox! Please find the meetup slides &lt;a href="https://docs.google.com/presentation/d/1A--47JAK4BJ39t954HyTkvtfwn0fkqtsL8NGFuslReM/edit?usp=sharing"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2024/01] We hosted &lt;a href="https://lu.ma/ygxbpzhl"&gt;the second vLLM meetup&lt;/a&gt; with IBM! Please find the meetup slides &lt;a href="https://docs.google.com/presentation/d/12mI2sKABnUw5RBWXDYY-HtHth4iMSNcEoQ10jDQbxgA/edit?usp=sharing"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2023/10] We hosted &lt;a href="https://lu.ma/first-vllm-meetup"&gt;the first vLLM meetup&lt;/a&gt; with a16z! Please find the meetup slides &lt;a href="https://docs.google.com/presentation/d/1QL-XPFXiFpDBh86DbEegFXBXFXjix4v032GhShbKf3s/edit?usp=sharing"&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2023/08] We would like to express our sincere gratitude to &lt;a href="https://a16z.com/2023/08/30/supporting-the-open-source-ai-community/"&gt;Andreessen Horowitz&lt;/a&gt; (a16z) for providing a generous grant to support the open-source development and research of vLLM.&lt;/li&gt; 
  &lt;li&gt;[2023/06] We officially released vLLM! FastChat-vLLM integration has powered &lt;a href="https://chat.lmsys.org"&gt;LMSYS Vicuna and Chatbot Arena&lt;/a&gt; since mid-April. Check out our &lt;a href="https://vllm.ai"&gt;blog post&lt;/a&gt;.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h2&gt;About&lt;/h2&gt; 
&lt;p&gt;vLLM is a fast and easy-to-use library for LLM inference and serving.&lt;/p&gt; 
&lt;p&gt;Originally developed in the &lt;a href="https://sky.cs.berkeley.edu"&gt;Sky Computing Lab&lt;/a&gt; at UC Berkeley, vLLM has evolved into a community-driven project with contributions from both academia and industry.&lt;/p&gt; 
&lt;p&gt;vLLM is fast with:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;State-of-the-art serving throughput&lt;/li&gt; 
 &lt;li&gt;Efficient management of attention key and value memory with &lt;a href="https://blog.vllm.ai/2023/06/20/vllm.html"&gt;&lt;strong&gt;PagedAttention&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Continuous batching of incoming requests&lt;/li&gt; 
 &lt;li&gt;Fast model execution with CUDA/HIP graph&lt;/li&gt; 
 &lt;li&gt;Quantizations: &lt;a href="https://arxiv.org/abs/2210.17323"&gt;GPTQ&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/2306.00978"&gt;AWQ&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/2309.05516"&gt;AutoRound&lt;/a&gt;, INT4, INT8, and FP8&lt;/li&gt; 
 &lt;li&gt;Optimized CUDA kernels, including integration with FlashAttention and FlashInfer&lt;/li&gt; 
 &lt;li&gt;Speculative decoding&lt;/li&gt; 
 &lt;li&gt;Chunked prefill&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;vLLM is flexible and easy to use with:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Seamless integration with popular Hugging Face models&lt;/li&gt; 
 &lt;li&gt;High-throughput serving with various decoding algorithms, including &lt;em&gt;parallel sampling&lt;/em&gt;, &lt;em&gt;beam search&lt;/em&gt;, and more&lt;/li&gt; 
 &lt;li&gt;Tensor, pipeline, data and expert parallelism support for distributed inference&lt;/li&gt; 
 &lt;li&gt;Streaming outputs&lt;/li&gt; 
 &lt;li&gt;OpenAI-compatible API server&lt;/li&gt; 
 &lt;li&gt;Support NVIDIA GPUs, AMD CPUs and GPUs, Intel CPUs and GPUs, PowerPC CPUs, TPU, and AWS Neuron&lt;/li&gt; 
 &lt;li&gt;Prefix caching support&lt;/li&gt; 
 &lt;li&gt;Multi-LoRA support&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;vLLM seamlessly supports most popular open-source models on HuggingFace, including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Transformer-like LLMs (e.g., Llama)&lt;/li&gt; 
 &lt;li&gt;Mixture-of-Expert LLMs (e.g., Mixtral, Deepseek-V2 and V3)&lt;/li&gt; 
 &lt;li&gt;Embedding Models (e.g., E5-Mistral)&lt;/li&gt; 
 &lt;li&gt;Multi-modal LLMs (e.g., LLaVA)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Find the full list of supported models &lt;a href="https://docs.vllm.ai/en/latest/models/supported_models.html"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;Install vLLM with &lt;code&gt;pip&lt;/code&gt; or &lt;a href="https://docs.vllm.ai/en/latest/getting_started/installation/gpu/index.html#build-wheel-from-source"&gt;from source&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install vllm
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Visit our &lt;a href="https://docs.vllm.ai/en/latest/"&gt;documentation&lt;/a&gt; to learn more.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.vllm.ai/en/latest/getting_started/installation.html"&gt;Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.vllm.ai/en/latest/getting_started/quickstart.html"&gt;Quickstart&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.vllm.ai/en/latest/models/supported_models.html"&gt;List of Supported Models&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome and value any contributions and collaborations. Please check out &lt;a href="https://docs.vllm.ai/en/latest/contributing/index.html"&gt;Contributing to vLLM&lt;/a&gt; for how to get involved.&lt;/p&gt; 
&lt;h2&gt;Sponsors&lt;/h2&gt; 
&lt;p&gt;vLLM is a community project. Our compute resources for development and testing are supported by the following organizations. Thank you for your support!&lt;/p&gt; 
&lt;!-- Note: Please sort them in alphabetical order. --&gt; 
&lt;!-- Note: Please keep these consistent with docs/community/sponsors.md --&gt; 
&lt;p&gt;Cash Donations:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;a16z&lt;/li&gt; 
 &lt;li&gt;Dropbox&lt;/li&gt; 
 &lt;li&gt;Sequoia Capital&lt;/li&gt; 
 &lt;li&gt;Skywork AI&lt;/li&gt; 
 &lt;li&gt;ZhenFund&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Compute Resources:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Alibaba Cloud&lt;/li&gt; 
 &lt;li&gt;AMD&lt;/li&gt; 
 &lt;li&gt;Anyscale&lt;/li&gt; 
 &lt;li&gt;AWS&lt;/li&gt; 
 &lt;li&gt;Crusoe Cloud&lt;/li&gt; 
 &lt;li&gt;Databricks&lt;/li&gt; 
 &lt;li&gt;DeepInfra&lt;/li&gt; 
 &lt;li&gt;Google Cloud&lt;/li&gt; 
 &lt;li&gt;Intel&lt;/li&gt; 
 &lt;li&gt;Lambda Lab&lt;/li&gt; 
 &lt;li&gt;Nebius&lt;/li&gt; 
 &lt;li&gt;Novita AI&lt;/li&gt; 
 &lt;li&gt;NVIDIA&lt;/li&gt; 
 &lt;li&gt;Replicate&lt;/li&gt; 
 &lt;li&gt;Roblox&lt;/li&gt; 
 &lt;li&gt;RunPod&lt;/li&gt; 
 &lt;li&gt;Trainy&lt;/li&gt; 
 &lt;li&gt;UC Berkeley&lt;/li&gt; 
 &lt;li&gt;UC San Diego&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Slack Sponsor: Anyscale&lt;/p&gt; 
&lt;p&gt;We also have an official fundraising venue through &lt;a href="https://opencollective.com/vllm"&gt;OpenCollective&lt;/a&gt;. We plan to use the fund to support the development, maintenance, and adoption of vLLM.&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you use vLLM for your research, please cite our &lt;a href="https://arxiv.org/abs/2309.06180"&gt;paper&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@inproceedings{kwon2023efficient,
  title={Efficient Memory Management for Large Language Model Serving with PagedAttention},
  author={Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica},
  booktitle={Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles},
  year={2023}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Contact Us&lt;/h2&gt; 
&lt;!-- --8&lt;-- [start:contact-us] --&gt; 
&lt;ul&gt; 
 &lt;li&gt;For technical questions and feature requests, please use GitHub &lt;a href="https://github.com/vllm-project/vllm/issues"&gt;Issues&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;For discussing with fellow users, please use the &lt;a href="https://discuss.vllm.ai"&gt;vLLM Forum&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;For coordinating contributions and development, please use &lt;a href="https://slack.vllm.ai"&gt;Slack&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;For security disclosures, please use GitHub's &lt;a href="https://github.com/vllm-project/vllm/security/advisories"&gt;Security Advisories&lt;/a&gt; feature&lt;/li&gt; 
 &lt;li&gt;For collaborations and partnerships, please contact us at &lt;a href="mailto:vllm-questions@lists.berkeley.edu"&gt;vllm-questions@lists.berkeley.edu&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- --8&lt;-- [end:contact-us] --&gt; 
&lt;h2&gt;Media Kit&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;If you wish to use vLLM's logo, please refer to &lt;a href="https://github.com/vllm-project/media-kit"&gt;our media kit repo&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>hao-ai-lab/FastVideo</title>
      <link>https://github.com/hao-ai-lab/FastVideo</link>
      <description>&lt;p&gt;A unified inference and post-training framework for accelerated video generation.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/hao-ai-lab/FastVideo/main/assets/logo.png" width="30%" /&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;FastVideo is a unified post-training and inference framework for accelerated video generation.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;FastVideo features an end-to-end unified pipeline for accelerating diffusion models, starting from data preprocessing to model training, finetuning, distillation, and inference. FastVideo is designed to be modular and extensible, allowing users to easily add new optimizations and techniques. Whether it is training-free optimizations or post-training optimizations, FastVideo has you covered.&lt;/p&gt; 
&lt;p align="center"&gt; | ğŸ•¹ï¸ &lt;a href="https://fastwan.fastvideo.org/" &lt;b&gt;Online Demo&lt;/a&gt; | &lt;a href="https://hao-ai-lab.github.io/FastVideo"&gt;&lt;b&gt;Documentation&lt;/b&gt;&lt;/a&gt; | &lt;a href="https://hao-ai-lab.github.io/FastVideo/inference/inference_quick_start.html"&gt;&lt;b&gt; Quick Start&lt;/b&gt;&lt;/a&gt; | ğŸ¤— &lt;a href="https://huggingface.co/collections/FastVideo/fastwan-6886a305d9799c8cd1496408" target="_blank"&gt;&lt;b&gt;FastWan&lt;/b&gt;&lt;/a&gt; | ğŸŸ£ğŸ’¬ &lt;a href="https://join.slack.com/t/fastvideo/shared_invite/zt-38u6p1jqe-yDI1QJOCEnbtkLoaI5bjZQ" target="_blank"&gt; &lt;b&gt;Slack&lt;/b&gt; &lt;/a&gt; | ğŸŸ£ğŸ’¬ &lt;a href="https://ibb.co/wZPZTLKg" target="_blank"&gt; &lt;b&gt; WeChat &lt;/b&gt; &lt;/a&gt; | &lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://raw.githubusercontent.com/hao-ai-lab/FastVideo/main/assets/fastwan.png" width="90%" /&gt; 
&lt;/div&gt; 
&lt;h2&gt;NEWS&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;2025/08/04&lt;/code&gt;: Release &lt;a href="https://hao-ai-lab.github.io/FastVideo/distillation/dmd.html"&gt;FastWan&lt;/a&gt; models and &lt;a href="https://hao-ai-lab.github.io/blogs/fastvideo_post_training/"&gt;Sparse-Distillation&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;2025/06/14&lt;/code&gt;: Release finetuning and inference code for &lt;a href="https://arxiv.org/pdf/2505.13389"&gt;VSA&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;2025/04/24&lt;/code&gt;: &lt;a href="https://hao-ai-lab.github.io/blogs/fastvideo/"&gt;FastVideo V1&lt;/a&gt; is released!&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;2025/02/18&lt;/code&gt;: Release the inference code for &lt;a href="https://hao-ai-lab.github.io/blogs/sta/"&gt;Sliding Tile Attention&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;p&gt;FastVideo has the following features:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;End-to-end post-training support: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://hao-ai-lab.github.io/blogs/fastvideo_post_training/"&gt;Sparse distillation&lt;/a&gt; for Wan2.1 and Wan2.2 to achineve &amp;gt;50x denoising speedup&lt;/li&gt; 
   &lt;li&gt;Data preprocessing pipeline for video data&lt;/li&gt; 
   &lt;li&gt;Support full finetuning and LoRA finetuning for state-of-the-art open video DiTs&lt;/li&gt; 
   &lt;li&gt;Scalable training with FSDP2, sequence parallelism, and selective activation checkpointing, with near linear scaling to 64 GPUs&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;State-of-the-art performance optimizations for inference 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://arxiv.org/pdf/2505.13389"&gt;Video Sparse Attention&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://arxiv.org/pdf/2502.04507"&gt;Sliding Tile Attention&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://arxiv.org/pdf/2411.19108"&gt;TeaCache&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://arxiv.org/abs/2410.02367"&gt;Sage Attention&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Diverse hardware and OS support 
  &lt;ul&gt; 
   &lt;li&gt;Support H100, A100, 4090&lt;/li&gt; 
   &lt;li&gt;Support Linux, Windows, MacOS&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;We recommend using an environment manager such as &lt;code&gt;Conda&lt;/code&gt; to create a clean environment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Create and activate a new conda environment
conda create -n fastvideo python=3.12
conda activate fastvideo

# Install FastVideo
pip install fastvideo
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Please see our &lt;a href="https://hao-ai-lab.github.io/FastVideo/getting_started/installation.html"&gt;docs&lt;/a&gt; for more detailed installation instructions.&lt;/p&gt; 
&lt;h2&gt;Sparse Distillation&lt;/h2&gt; 
&lt;p&gt;For our sparse distillation techniques, please see our &lt;a href="https://hao-ai-lab.github.io/FastVideo/distillation/dmd.html"&gt;distillation docs&lt;/a&gt; and check out our &lt;a href="https://hao-ai-lab.github.io/blogs/fastvideo_post_training/"&gt;blog&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;See below for recipes and datasets:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="center"&gt;Model&lt;/th&gt; 
   &lt;th align="center"&gt;Sparse Distillation&lt;/th&gt; 
   &lt;th align="center"&gt;Dataset&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/FastVideo/FastWan2.1-T2V-1.3B-Diffusers"&gt;FastWan2.1-T2V-1.3B&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/hao-ai-lab/FastVideo/tree/main/examples/distill/Wan2.1-T2V/Wan-Syn-Data-480P"&gt;Recipe&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/datasets/FastVideo/Wan-Syn_77x448x832_600k"&gt;FastVideo Synthetic Wan2.1 480P&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/FastVideo/FastWan2.1-T2V-14B-Diffusers"&gt;FastWan2.1-T2V-14B-Preview&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;Coming soon!&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/datasets/FastVideo/Wan-Syn_77x768x1280_250k"&gt;FastVideo Synthetic Wan2.1 720P&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/FastVideo/FastWan2.2-TI2V-5B-Diffusers"&gt;FastWan2.2-TI2V-5B&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://github.com/hao-ai-lab/FastVideo/tree/main/examples/distill/Wan2.2-TI2V-5B-Diffusers/Data-free"&gt;Recipe&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="center"&gt;&lt;a href="https://huggingface.co/datasets/FastVideo/Wan2.2-Syn-121x704x1280_32k"&gt;FastVideo Synthetic Wan2.2 720P&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Inference&lt;/h2&gt; 
&lt;h3&gt;Generating Your First Video&lt;/h3&gt; 
&lt;p&gt;Here's a minimal example to generate a video using the default settings. Make sure VSA kernels are &lt;a href="https://hao-ai-lab.github.io/FastVideo/video_sparse_attention/installation.html"&gt;installed&lt;/a&gt;. Create a file called &lt;code&gt;example.py&lt;/code&gt; with the following code:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import os
from fastvideo import VideoGenerator

def main():
    os.environ["FASTVIDEO_ATTENTION_BACKEND"] = "VIDEO_SPARSE_ATTN"

    # Create a video generator with a pre-trained model
    generator = VideoGenerator.from_pretrained(
        "FastVideo/FastWan2.1-T2V-1.3B-Diffusers",
        num_gpus=1,  # Adjust based on your hardware
    )

    # Define a prompt for your video
    prompt = "A curious raccoon peers through a vibrant field of yellow sunflowers, its eyes wide with interest."

    # Generate the video
    video = generator.generate_video(
        prompt,
        return_frames=True,  # Also return frames from this call (defaults to False)
        output_path="my_videos/",  # Controls where videos are saved
        save_video=True
    )

if __name__ == '__main__':
    main()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Run the script with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python example.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For a more detailed guide, please see our &lt;a href="https://hao-ai-lab.github.io/FastVideo/inference/inference_quick_start.html"&gt;inference quick start&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Other docs:&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://hao-ai-lab.github.io/FastVideo/design/overview.html"&gt;Design Overview&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://hao-ai-lab.github.io/FastVideo/getting_started/installation.html"&gt;Contribution Guide&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Distillation and Finetuning&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://hao-ai-lab.github.io/FastVideo/distillation/dmd.html"&gt;Distillation Guide&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- - [Finetuning Guide](https://hao-ai-lab.github.io/FastVideo/training/finetune.html) --&gt; 
&lt;h2&gt;ğŸ“‘ Development Plan&lt;/h2&gt; 
&lt;!-- - More distillation methods --&gt; 
&lt;!-- - [ ] Add Distribution Matching Distillation --&gt; 
&lt;p&gt;More FastWan Models Coming Soon!&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Add FastWan2.1-T2V-14B&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Add FastWan2.2-T2V-14B&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Add FastWan2.2-I2V-14B&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- - Optimization features
- Code updates --&gt; 
&lt;!-- - [ ] fp8 support --&gt; 
&lt;!-- - [ ] faster load model and save model support --&gt; 
&lt;p&gt;See details in &lt;a href="https://github.com/hao-ai-lab/FastVideo/issues/468"&gt;development roadmap&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;ğŸ¤ Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome all contributions. Please check out our guide &lt;a href="https://hao-ai-lab.github.io/FastVideo/contributing/overview.html"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Acknowledgement&lt;/h2&gt; 
&lt;p&gt;We learned and reused code from the following projects:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Wan-Video"&gt;Wan-Video&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/HazyResearch/ThunderKittens"&gt;ThunderKittens&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/triton-lang/triton"&gt;Triton&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/tianweiy/DMD2"&gt;DMD2&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/huggingface/diffusers"&gt;diffusers&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/xdit-project/xDiT"&gt;xDiT&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/vllm-project/vllm"&gt;vLLM&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/sgl-project/sglang"&gt;SGLang&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We thank &lt;a href="https://ifm.mbzuai.ac.ae/"&gt;MBZUAI&lt;/a&gt;, &lt;a href="https://www.anyscale.com/"&gt;Anyscale&lt;/a&gt;, and &lt;a href="https://www.gmicloud.ai/"&gt;GMI Cloud&lt;/a&gt; for their support throughout this project.&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you find FastVideo useful, please considering citing our work:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@software{fastvideo2024,
  title        = {FastVideo: A Unified Framework for Accelerated Video Generation},
  author       = {The FastVideo Team},
  url          = {https://github.com/hao-ai-lab/FastVideo},
  month        = apr,
  year         = {2024},
}

@article{zhang2025vsa,
  title={VSA: Faster Video Diffusion with Trainable Sparse Attention},
  author={Zhang, Peiyuan and Huang, Haofeng and Chen, Yongqi and Lin, Will and Liu, Zhengzhong and Stoica, Ion and Xing, Eric and Zhang, Hao},
  journal={arXiv preprint arXiv:2505.13389},
  year={2025}
}

@article{zhang2025fast,
  title={Fast video generation with sliding tile attention},
  author={Zhang, Peiyuan and Chen, Yongqi and Su, Runlong and Ding, Hangliang and Stoica, Ion and Liu, Zhengzhong and Zhang, Hao},
  journal={arXiv preprint arXiv:2502.04507},
  year={2025}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>nautechsystems/nautilus_trader</title>
      <link>https://github.com/nautechsystems/nautilus_trader</link>
      <description>&lt;p&gt;A high-performance algorithmic trading platform and event-driven backtester&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;img src="https://github.com/nautechsystems/nautilus_trader/raw/develop/assets/nautilus-trader-logo.png" width="500" /&gt;&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://codecov.io/gh/nautechsystems/nautilus_trader"&gt;&lt;img src="https://codecov.io/gh/nautechsystems/nautilus_trader/branch/master/graph/badge.svg?token=DXO9QQI40H" alt="codecov" /&gt;&lt;/a&gt; &lt;a href="https://codspeed.io/nautechsystems/nautilus_trader"&gt;&lt;img src="https://img.shields.io/endpoint?url=https://codspeed.io/badge.json" alt="codspeed" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/pypi/pyversions/nautilus_trader" alt="pythons" /&gt; &lt;img src="https://img.shields.io/pypi/v/nautilus_trader" alt="pypi-version" /&gt; &lt;img src="https://img.shields.io/pypi/format/nautilus_trader?color=blue" alt="pypi-format" /&gt; &lt;a href="https://pepy.tech/project/nautilus-trader"&gt;&lt;img src="https://pepy.tech/badge/nautilus-trader" alt="Downloads" /&gt;&lt;/a&gt; &lt;a href="https://discord.gg/NautilusTrader"&gt;&lt;img src="https://img.shields.io/badge/Discord-%235865F2.svg?logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Branch&lt;/th&gt; 
   &lt;th align="left"&gt;Version&lt;/th&gt; 
   &lt;th align="left"&gt;Status&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;master&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://packages.nautechsystems.io/simple/nautilus-trader/index.html"&gt;&lt;img src="https://img.shields.io/endpoint?url=https%3A%2F%2Fraw.githubusercontent.com%2Fnautechsystems%2Fnautilus_trader%2Fmaster%2Fversion.json" alt="version" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/nautechsystems/nautilus_trader/actions/workflows/build.yml"&gt;&lt;img src="https://github.com/nautechsystems/nautilus_trader/actions/workflows/build.yml/badge.svg?branch=nightly" alt="build" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;nightly&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://packages.nautechsystems.io/simple/nautilus-trader/index.html"&gt;&lt;img src="https://img.shields.io/endpoint?url=https%3A%2F%2Fraw.githubusercontent.com%2Fnautechsystems%2Fnautilus_trader%2Fnightly%2Fversion.json" alt="version" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/nautechsystems/nautilus_trader/actions/workflows/build.yml"&gt;&lt;img src="https://github.com/nautechsystems/nautilus_trader/actions/workflows/build.yml/badge.svg?branch=nightly" alt="build" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;develop&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://packages.nautechsystems.io/simple/nautilus-trader/index.html"&gt;&lt;img src="https://img.shields.io/endpoint?url=https%3A%2F%2Fraw.githubusercontent.com%2Fnautechsystems%2Fnautilus_trader%2Fdevelop%2Fversion.json" alt="version" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://github.com/nautechsystems/nautilus_trader/actions/workflows/build.yml"&gt;&lt;img src="https://github.com/nautechsystems/nautilus_trader/actions/workflows/build.yml/badge.svg?branch=develop" alt="build" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Platform&lt;/th&gt; 
   &lt;th align="left"&gt;Rust&lt;/th&gt; 
   &lt;th align="left"&gt;Python&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;Linux (x86_64)&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;1.89.0&lt;/td&gt; 
   &lt;td align="left"&gt;3.11-3.13&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;Linux (ARM64)&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;1.89.0&lt;/td&gt; 
   &lt;td align="left"&gt;3.11-3.13&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;macOS (ARM64)&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;1.89.0&lt;/td&gt; 
   &lt;td align="left"&gt;3.11-3.13&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;Windows (x86_64)&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;1.89.0&lt;/td&gt; 
   &lt;td align="left"&gt;3.11-3.13&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Docs&lt;/strong&gt;: &lt;a href="https://nautilustrader.io/docs/"&gt;https://nautilustrader.io/docs/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Website&lt;/strong&gt;: &lt;a href="https://nautilustrader.io"&gt;https://nautilustrader.io&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Support&lt;/strong&gt;: &lt;a href="mailto:support@nautilustrader.io"&gt;support@nautilustrader.io&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;NautilusTrader is an open-source, high-performance, production-grade algorithmic trading platform, providing quantitative traders with the ability to backtest portfolios of automated trading strategies on historical data with an event-driven engine, and also deploy those same strategies live, with no code changes.&lt;/p&gt; 
&lt;p&gt;The platform is &lt;em&gt;AI-first&lt;/em&gt;, designed to develop and deploy algorithmic trading strategies within a highly performant and robust Python-native environment. This helps to address the parity challenge of keeping the Python research/backtest environment consistent with the production live trading environment.&lt;/p&gt; 
&lt;p&gt;NautilusTrader's design, architecture, and implementation philosophy prioritizes software correctness and safety at the highest level, with the aim of supporting Python-native, mission-critical, trading system backtesting and live deployment workloads.&lt;/p&gt; 
&lt;p&gt;The platform is also universal, and asset-class-agnostic â€” with any REST API or WebSocket feed able to be integrated via modular adapters. It supports high-frequency trading across a wide range of asset classes and instrument types including FX, Equities, Futures, Options, Crypto, DeFi, and Betting, enabling seamless operations across multiple venues simultaneously.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/nautechsystems/nautilus_trader/raw/develop/assets/nautilus-trader.png" alt="nautilus-trader" title="nautilus-trader" /&gt;&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Fast&lt;/strong&gt;: Core is written in Rust with asynchronous networking using &lt;a href="https://crates.io/crates/tokio"&gt;tokio&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Reliable&lt;/strong&gt;: Rust-powered type- and thread-safety, with optional Redis-backed state persistence.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Portable&lt;/strong&gt;: OS independent, runs on Linux, macOS, and Windows. Deploy using Docker.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Flexible&lt;/strong&gt;: Modular adapters mean any REST API or WebSocket feed can be integrated.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Advanced&lt;/strong&gt;: Time in force &lt;code&gt;IOC&lt;/code&gt;, &lt;code&gt;FOK&lt;/code&gt;, &lt;code&gt;GTC&lt;/code&gt;, &lt;code&gt;GTD&lt;/code&gt;, &lt;code&gt;DAY&lt;/code&gt;, &lt;code&gt;AT_THE_OPEN&lt;/code&gt;, &lt;code&gt;AT_THE_CLOSE&lt;/code&gt;, advanced order types and conditional triggers. Execution instructions &lt;code&gt;post-only&lt;/code&gt;, &lt;code&gt;reduce-only&lt;/code&gt;, and icebergs. Contingency orders including &lt;code&gt;OCO&lt;/code&gt;, &lt;code&gt;OUO&lt;/code&gt;, &lt;code&gt;OTO&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Customizable&lt;/strong&gt;: Add user-defined custom components, or assemble entire systems from scratch leveraging the &lt;a href="https://nautilustrader.io/docs/latest/concepts/cache"&gt;cache&lt;/a&gt; and &lt;a href="https://nautilustrader.io/docs/latest/concepts/message_bus"&gt;message bus&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Backtesting&lt;/strong&gt;: Run with multiple venues, instruments and strategies simultaneously using historical quote tick, trade tick, bar, order book and custom data with nanosecond resolution.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Live&lt;/strong&gt;: Use identical strategy implementations between backtesting and live deployments.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-venue&lt;/strong&gt;: Multiple venue capabilities facilitate market-making and statistical arbitrage strategies.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;AI Training&lt;/strong&gt;: Backtest engine fast enough to be used to train AI trading agents (RL/ES).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src="https://github.com/nautechsystems/nautilus_trader/raw/develop/assets/nautilus-art.png" alt="Alt text" title="nautilus" /&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;em&gt;nautilus - from ancient Greek 'sailor' and naus 'ship'.&lt;/em&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;The nautilus shell consists of modular chambers with a growth factor which approximates a logarithmic spiral. The idea is that this can be translated to the aesthetics of design and architecture.&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Why NautilusTrader?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Highly performant event-driven Python&lt;/strong&gt;: Native binary core components.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Parity between backtesting and live trading&lt;/strong&gt;: Identical strategy code.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Reduced operational risk&lt;/strong&gt;: Enhanced risk management functionality, logical accuracy, and type safety.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Highly extendable&lt;/strong&gt;: Message bus, custom components and actors, custom data, custom adapters.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Traditionally, trading strategy research and backtesting might be conducted in Python using vectorized methods, with the strategy then needing to be reimplemented in a more event-driven way using C++, C#, Java or other statically typed language(s). The reasoning here is that vectorized backtesting code cannot express the granular time and event dependent complexity of real-time trading, where compiled languages have proven to be more suitable due to their inherently higher performance, and type safety.&lt;/p&gt; 
&lt;p&gt;One of the key advantages of NautilusTrader here, is that this reimplementation step is now circumvented - as the critical core components of the platform have all been written entirely in &lt;a href="https://www.rust-lang.org/"&gt;Rust&lt;/a&gt; or &lt;a href="https://cython.org/"&gt;Cython&lt;/a&gt;. This means we're using the right tools for the job, where systems programming languages compile performant binaries, with CPython C extension modules then able to offer a Python-native environment, suitable for professional quantitative traders and trading firms.&lt;/p&gt; 
&lt;h2&gt;Why Python?&lt;/h2&gt; 
&lt;p&gt;Python was originally created decades ago as a simple scripting language with a clean straightforward syntax. It has since evolved into a fully fledged general purpose object-oriented programming language. Based on the TIOBE index, Python is currently the most popular programming language in the world. Not only that, Python has become the &lt;em&gt;de facto lingua franca&lt;/em&gt; of data science, machine learning, and artificial intelligence.&lt;/p&gt; 
&lt;p&gt;developer/user communities. However, Python has performance and typing limitations for large-scale, latency-sensitive systems. Cython addresses many of these issues by introducing static typing into Python's rich ecosystem of libraries and communities.&lt;/p&gt; 
&lt;h2&gt;Why Rust?&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.rust-lang.org/"&gt;Rust&lt;/a&gt; is a multi-paradigm programming language designed for performance and safety, especially safe concurrency. Rust is "blazingly fast" and memory-efficient (comparable to C and C++) with no garbage collector. It can power mission-critical systems, run on embedded devices, and easily integrates with other languages.&lt;/p&gt; 
&lt;p&gt;Rustâ€™s rich type system and ownership model guarantees memory-safety and thread-safety deterministically â€” eliminating many classes of bugs at compile-time.&lt;/p&gt; 
&lt;p&gt;The project increasingly utilizes Rust for core performance-critical components. Python bindings are implemented via Cython and &lt;a href="https://pyo3.rs"&gt;PyO3&lt;/a&gt;â€”no Rust toolchain is required at install time.&lt;/p&gt; 
&lt;p&gt;This project makes the &lt;a href="https://raphlinus.github.io/rust/2020/01/18/soundness-pledge.html"&gt;Soundness Pledge&lt;/a&gt;:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;â€œThe intent of this project is to be free of soundness bugs. The developers will do their best to avoid them, and welcome help in analyzing and fixing them.â€&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;MSRV:&lt;/strong&gt; NautilusTrader relies heavily on improvements in the Rust language and compiler. As a result, the Minimum Supported Rust Version (MSRV) is generally equal to the latest stable release of Rust.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Integrations&lt;/h2&gt; 
&lt;p&gt;NautilusTrader is modularly designed to work with &lt;em&gt;adapters&lt;/em&gt;, enabling connectivity to trading venues and data providers by translating their raw APIs into a unified interface and normalized domain model.&lt;/p&gt; 
&lt;p&gt;The following integrations are currently supported; see &lt;a href="https://nautilustrader.io/docs/latest/integrations/"&gt;docs/integrations/&lt;/a&gt; for details:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Name&lt;/th&gt; 
   &lt;th align="left"&gt;ID&lt;/th&gt; 
   &lt;th align="left"&gt;Type&lt;/th&gt; 
   &lt;th align="left"&gt;Status&lt;/th&gt; 
   &lt;th align="left"&gt;Docs&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://betfair.com"&gt;Betfair&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;BETFAIR&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Sports Betting Exchange&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/stable-green" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/betfair.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://binance.com"&gt;Binance&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;BINANCE&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Crypto Exchange (CEX)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/stable-green" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/binance.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://binance.us"&gt;Binance US&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;BINANCE&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Crypto Exchange (CEX)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/stable-green" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/binance.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://www.binance.com/en/futures"&gt;Binance Futures&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;BINANCE&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Crypto Exchange (CEX)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/stable-green" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/binance.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://www.bitmex.com"&gt;BitMEX&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;BITMEX&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Crypto Exchange (CEX)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/building-orange" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/bitmex.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://www.bybit.com"&gt;Bybit&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;BYBIT&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Crypto Exchange (CEX)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/stable-green" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/bybit.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://www.coinbase.com/en/international-exchange"&gt;Coinbase International&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;COINBASE_INTX&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Crypto Exchange (CEX)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/stable-green" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/coinbase_intx.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://databento.com"&gt;Databento&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;DATABENTO&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Data Provider&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/stable-green" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/databento.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://dydx.exchange/"&gt;dYdX&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;DYDX&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Crypto Exchange (DEX)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/stable-green" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/dydx.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://hyperliquid.xyz"&gt;Hyperliquid&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;HYPERLIQUID&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Crypto Exchange (DEX)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/building-orange" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/hyperliquid.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://www.interactivebrokers.com"&gt;Interactive Brokers&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;INTERACTIVE_BROKERS&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Brokerage (multi-venue)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/stable-green" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/ib.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://okx.com"&gt;OKX&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;OKX&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Crypto Exchange (CEX)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/beta-yellow" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/okx.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://polymarket.com"&gt;Polymarket&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;POLYMARKET&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Prediction Market (DEX)&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/stable-green" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/polymarket.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;a href="https://tardis.dev"&gt;Tardis&lt;/a&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;code&gt;TARDIS&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;Crypto Data Provider&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;img src="https://img.shields.io/badge/stable-green" alt="status" /&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/tardis.md"&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ID&lt;/strong&gt;: The default client ID for the integrations adapter clients.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Type&lt;/strong&gt;: The type of integration (often the venue type).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Status&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;building&lt;/code&gt;: Under construction and likely not in a usable state.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;beta&lt;/code&gt;: Completed to a minimally working state and in a beta testing phase.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;stable&lt;/code&gt;: Stabilized feature set and API, the integration has been tested by both developers and users to a reasonable level (some bugs may still remain).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See the &lt;a href="https://nautilustrader.io/docs/latest/integrations/index.html"&gt;Integrations&lt;/a&gt; documentation for further details.&lt;/p&gt; 
&lt;h2&gt;Versioning and releases&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;NautilusTrader is still under active development&lt;/strong&gt;. Some features may be incomplete, and while the API is becoming more stable, breaking changes can occur between releases. We strive to document these changes in the release notes on a &lt;strong&gt;best-effort basis&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;We aim to follow a &lt;strong&gt;bi-weekly release schedule&lt;/strong&gt;, though experimental or larger features may cause delays.&lt;/p&gt; 
&lt;h3&gt;Branches&lt;/h3&gt; 
&lt;p&gt;We aim to maintain a stable, passing build across all branches.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;master&lt;/code&gt;: Reflects the source code for the latest released version; recommended for production use.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;nightly&lt;/code&gt;: Daily snapshots of the &lt;code&gt;develop&lt;/code&gt; branch for early testing; merged at &lt;strong&gt;14:00 UTC&lt;/strong&gt; or on demand.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;develop&lt;/code&gt;: Active development branch for contributors and feature work.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;/p&gt; 
 &lt;p&gt;Our &lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/ROADMAP.md"&gt;roadmap&lt;/a&gt; aims to achieve a &lt;strong&gt;stable API for version 2.x&lt;/strong&gt; (likely after the Rust port). Once this milestone is reached, we plan to implement a formal deprecation process for any API changes. This approach allows us to maintain a rapid development pace for now.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Precision mode&lt;/h2&gt; 
&lt;p&gt;NautilusTrader supports two precision modes for its core value types (&lt;code&gt;Price&lt;/code&gt;, &lt;code&gt;Quantity&lt;/code&gt;, &lt;code&gt;Money&lt;/code&gt;), which differ in their internal bit-width and maximum decimal precision.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;High-precision&lt;/strong&gt;: 128-bit integers with up to 16 decimals of precision, and a larger value range.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Standard-precision&lt;/strong&gt;: 64-bit integers with up to 9 decimals of precision, and a smaller value range.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;/p&gt; 
 &lt;p&gt;By default, the official Python wheels &lt;strong&gt;ship&lt;/strong&gt; in high-precision (128-bit) mode on Linux and macOS. On Windows, only standard-precision (64-bit) is available due to the lack of native 128-bit integer support. For the Rust crates, the default is standard-precision unless you explicitly enable the &lt;code&gt;high-precision&lt;/code&gt; feature flag.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;See the &lt;a href="https://nautilustrader.io/docs/latest/getting_started/installation"&gt;Installation Guide&lt;/a&gt; for further details.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Rust feature flag&lt;/strong&gt;: To enable high-precision mode in Rust, add the &lt;code&gt;high-precision&lt;/code&gt; feature to your Cargo.toml:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-toml"&gt;[dependencies]
nautilus_model = { version = "*", features = ["high-precision"] }
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;We recommend using the latest supported version of Python and installing &lt;a href="https://pypi.org/project/nautilus_trader/"&gt;nautilus_trader&lt;/a&gt; inside a virtual environment to isolate dependencies.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;There are two supported ways to install&lt;/strong&gt;:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Pre-built binary wheel from PyPI &lt;em&gt;or&lt;/em&gt; the Nautech Systems package index.&lt;/li&gt; 
 &lt;li&gt;Build from source.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP]&lt;/p&gt; 
 &lt;p&gt;We highly recommend installing using the &lt;a href="https://docs.astral.sh/uv"&gt;uv&lt;/a&gt; package manager with a "vanilla" CPython.&lt;/p&gt; 
 &lt;p&gt;Conda and other Python distributions &lt;em&gt;may&lt;/em&gt; work but arenâ€™t officially supported.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;From PyPI&lt;/h3&gt; 
&lt;p&gt;To install the latest binary wheel (or sdist package) from PyPI using Python's pip package manager:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -U nautilus_trader
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;From the Nautech Systems package index&lt;/h3&gt; 
&lt;p&gt;The Nautech Systems package index (&lt;code&gt;packages.nautechsystems.io&lt;/code&gt;) complies with &lt;a href="https://peps.python.org/pep-0503/"&gt;PEP-503&lt;/a&gt; and hosts both stable and development binary wheels for &lt;code&gt;nautilus_trader&lt;/code&gt;. This enables users to install either the latest stable release or pre-release versions for testing.&lt;/p&gt; 
&lt;h4&gt;Stable wheels&lt;/h4&gt; 
&lt;p&gt;Stable wheels correspond to official releases of &lt;code&gt;nautilus_trader&lt;/code&gt; on PyPI, and use standard versioning.&lt;/p&gt; 
&lt;p&gt;To install the latest stable release:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -U nautilus_trader --index-url=https://packages.nautechsystems.io/simple
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Development wheels&lt;/h4&gt; 
&lt;p&gt;Development wheels are published from both the &lt;code&gt;nightly&lt;/code&gt; and &lt;code&gt;develop&lt;/code&gt; branches, allowing users to test features and fixes ahead of stable releases.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Wheels from the &lt;code&gt;develop&lt;/code&gt; branch are only built for the Linux x86_64 platform to save time and compute resources, while &lt;code&gt;nightly&lt;/code&gt; wheels support additional platforms as shown below.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align="left"&gt;Platform&lt;/th&gt; 
   &lt;th align="left"&gt;Nightly&lt;/th&gt; 
   &lt;th align="left"&gt;Develop&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;Linux (x86_64)&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;âœ“&lt;/td&gt; 
   &lt;td align="left"&gt;âœ“&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;Linux (ARM64)&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;âœ“&lt;/td&gt; 
   &lt;td align="left"&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;macOS (ARM64)&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;âœ“&lt;/td&gt; 
   &lt;td align="left"&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align="left"&gt;&lt;code&gt;Windows (x86_64)&lt;/code&gt;&lt;/td&gt; 
   &lt;td align="left"&gt;âœ“&lt;/td&gt; 
   &lt;td align="left"&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;This process also helps preserve compute resources and ensures easy access to the exact binaries tested in CI pipelines, while adhering to &lt;a href="https://peps.python.org/pep-0440/"&gt;PEP-440&lt;/a&gt; versioning standards:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;develop&lt;/code&gt; wheels use the version format &lt;code&gt;dev{date}+{build_number}&lt;/code&gt; (e.g., &lt;code&gt;1.208.0.dev20241212+7001&lt;/code&gt;).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;nightly&lt;/code&gt; wheels use the version format &lt;code&gt;a{date}&lt;/code&gt; (alpha) (e.g., &lt;code&gt;1.208.0a20241212&lt;/code&gt;).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING]&lt;/p&gt; 
 &lt;p&gt;We do not recommend using development wheels in production environments, such as live trading controlling real capital.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Installation commands&lt;/h4&gt; 
&lt;p&gt;By default, pip will install the latest stable release. Adding the &lt;code&gt;--pre&lt;/code&gt; flag ensures that pre-release versions, including development wheels, are considered.&lt;/p&gt; 
&lt;p&gt;To install the latest available pre-release (including development wheels):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -U nautilus_trader --pre --index-url=https://packages.nautechsystems.io/simple
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To install a specific development wheel (e.g., &lt;code&gt;1.208.0a20241212&lt;/code&gt; for December 12, 2024):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install nautilus_trader==1.208.0a20241212 --index-url=https://packages.nautechsystems.io/simple
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Available versions&lt;/h4&gt; 
&lt;p&gt;You can view all available versions of &lt;code&gt;nautilus_trader&lt;/code&gt; on the &lt;a href="https://packages.nautechsystems.io/simple/nautilus-trader/index.html"&gt;package index&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;To programmatically fetch and list available versions:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -s https://packages.nautechsystems.io/simple/nautilus-trader/index.html | grep -oP '(?&amp;lt;=&amp;lt;a href=")[^"]+(?=")' | awk -F'#' '{print $1}' | sort
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Branch updates&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;develop&lt;/code&gt; branch wheels (&lt;code&gt;.dev&lt;/code&gt;): Build and publish continuously with every merged commit.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;nightly&lt;/code&gt; branch wheels (&lt;code&gt;a&lt;/code&gt;): Build and publish daily when we automatically merge the &lt;code&gt;develop&lt;/code&gt; branch at &lt;strong&gt;14:00 UTC&lt;/strong&gt; (if there are changes).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Retention policies&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;develop&lt;/code&gt; branch wheels (&lt;code&gt;.dev&lt;/code&gt;): We retain only the most recent wheel build.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;nightly&lt;/code&gt; branch wheels (&lt;code&gt;a&lt;/code&gt;): We retain only the 30 most recent wheel builds.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;From Source&lt;/h3&gt; 
&lt;p&gt;It's possible to install from source using pip if you first install the build dependencies as specified in the &lt;code&gt;pyproject.toml&lt;/code&gt;.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Install &lt;a href="https://rustup.rs/"&gt;rustup&lt;/a&gt; (the Rust toolchain installer):&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;Linux and macOS:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;curl https://sh.rustup.rs -sSf | sh
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Windows:&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Download and install &lt;a href="https://win.rustup.rs/x86_64"&gt;&lt;code&gt;rustup-init.exe&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;Install "Desktop development with C++" with &lt;a href="https://visualstudio.microsoft.com/thank-you-downloading-visual-studio/?sku=BuildTools&amp;amp;rel=16"&gt;Build Tools for Visual Studio 2019&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Verify (any system): from a terminal session run: &lt;code&gt;rustc --version&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Enable &lt;code&gt;cargo&lt;/code&gt; in the current shell:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;Linux and macOS:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;source $HOME/.cargo/env
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Windows:&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Start a new PowerShell&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install &lt;a href="https://clang.llvm.org/"&gt;clang&lt;/a&gt; (a C language frontend for LLVM):&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;Linux:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;sudo apt-get install clang
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Windows:&lt;/p&gt; 
    &lt;ol&gt; 
     &lt;li&gt; &lt;p&gt;Add Clang to your &lt;a href="https://visualstudio.microsoft.com/thank-you-downloading-visual-studio/?sku=BuildTools&amp;amp;rel=16"&gt;Build Tools for Visual Studio 2019&lt;/a&gt;:&lt;/p&gt; 
      &lt;ul&gt; 
       &lt;li&gt;Start | Visual Studio Installer | Modify | C++ Clang tools for Windows (12.0.0 - x64â€¦) = checked | Modify&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;Enable &lt;code&gt;clang&lt;/code&gt; in the current shell:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-powershell"&gt;[System.Environment]::SetEnvironmentVariable('path', "C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\VC\Tools\Llvm\x64\bin\;" + $env:Path,"User")
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
    &lt;/ol&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Verify (any system): from a terminal session run: &lt;code&gt;clang --version&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install uv (see the &lt;a href="https://docs.astral.sh/uv/getting-started/installation"&gt;uv installation guide&lt;/a&gt; for more details):&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;curl -LsSf https://astral.sh/uv/install.sh | sh
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Clone the source with &lt;code&gt;git&lt;/code&gt;, and install from the project's root directory:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git clone --branch develop --depth 1 https://github.com/nautechsystems/nautilus_trader
cd nautilus_trader
uv sync --all-extras
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;/p&gt; 
 &lt;p&gt;The &lt;code&gt;--depth 1&lt;/code&gt; flag fetches just the latest commit for a faster, lightweight clone.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ol start="6"&gt; 
 &lt;li&gt; &lt;p&gt;Set environment variables for PyO3 compilation (Linux and macOS only):&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;# Set the library path for the Python interpreter (in this case Python 3.13.4)
export LD_LIBRARY_PATH="$HOME/.local/share/uv/python/cpython-3.13.4-linux-x86_64-gnu/lib:$LD_LIBRARY_PATH"

# Set the Python executable path for PyO3
export PYO3_PYTHON=$(pwd)/.venv/bin/python
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;/p&gt; 
 &lt;p&gt;Adjust the Python version and architecture in the &lt;code&gt;LD_LIBRARY_PATH&lt;/code&gt; to match your system. Use &lt;code&gt;uv python list&lt;/code&gt; to find the exact path for your Python installation.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;See the &lt;a href="https://nautilustrader.io/docs/latest/getting_started/installation"&gt;Installation Guide&lt;/a&gt; for other options and further details.&lt;/p&gt; 
&lt;h2&gt;Redis&lt;/h2&gt; 
&lt;p&gt;Using &lt;a href="https://redis.io"&gt;Redis&lt;/a&gt; with NautilusTrader is &lt;strong&gt;optional&lt;/strong&gt; and only required if configured as the backend for a &lt;a href="https://nautilustrader.io/docs/latest/concepts/cache"&gt;cache&lt;/a&gt; database or &lt;a href="https://nautilustrader.io/docs/latest/concepts/message_bus"&gt;message bus&lt;/a&gt;. See the &lt;strong&gt;Redis&lt;/strong&gt; section of the &lt;a href="https://nautilustrader.io/docs/latest/getting_started/installation#redis"&gt;Installation Guide&lt;/a&gt; for further details.&lt;/p&gt; 
&lt;h2&gt;Makefile&lt;/h2&gt; 
&lt;p&gt;A &lt;code&gt;Makefile&lt;/code&gt; is provided to automate most installation and build tasks for development. Some of the targets include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;make install&lt;/code&gt;: Installs in &lt;code&gt;release&lt;/code&gt; build mode with all dependency groups and extras.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make install-debug&lt;/code&gt;: Same as &lt;code&gt;make install&lt;/code&gt; but with &lt;code&gt;debug&lt;/code&gt; build mode.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make install-just-deps&lt;/code&gt;: Installs just the &lt;code&gt;main&lt;/code&gt;, &lt;code&gt;dev&lt;/code&gt; and &lt;code&gt;test&lt;/code&gt; dependencies (does not install package).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make build&lt;/code&gt;: Runs the build script in &lt;code&gt;release&lt;/code&gt; build mode (default).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make build-debug&lt;/code&gt;: Runs the build script in &lt;code&gt;debug&lt;/code&gt; build mode.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make build-wheel&lt;/code&gt;: Runs uv build with a wheel format in &lt;code&gt;release&lt;/code&gt; mode.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make build-wheel-debug&lt;/code&gt;: Runs uv build with a wheel format in &lt;code&gt;debug&lt;/code&gt; mode.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make cargo-test&lt;/code&gt;: Runs all Rust crate tests using &lt;code&gt;cargo-nextest&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make clean&lt;/code&gt;: Deletes all build results, such as &lt;code&gt;.so&lt;/code&gt; or &lt;code&gt;.dll&lt;/code&gt; files.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make distclean&lt;/code&gt;: &lt;strong&gt;CAUTION&lt;/strong&gt; Removes all artifacts not in the git index from the repository. This includes source files which have not been &lt;code&gt;git add&lt;/code&gt;ed.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make docs&lt;/code&gt;: Builds the documentation HTML using Sphinx.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make pre-commit&lt;/code&gt;: Runs the pre-commit checks over all files.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make ruff&lt;/code&gt;: Runs ruff over all files using the &lt;code&gt;pyproject.toml&lt;/code&gt; config (with autofix).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make pytest&lt;/code&gt;: Runs all tests with &lt;code&gt;pytest&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make test-performance&lt;/code&gt;: Runs performance tests with &lt;a href="https://codspeed.io"&gt;codspeed&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP]&lt;/p&gt; 
 &lt;p&gt;Run &lt;code&gt;make help&lt;/code&gt; for documentation on all available make targets.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP]&lt;/p&gt; 
 &lt;p&gt;See the &lt;a href="https://github.com/nautechsystems/nautilus_trader/raw/develop/crates/infrastructure/TESTS.md"&gt;crates/infrastructure/TESTS.md&lt;/a&gt; file for running the infrastructure integration tests.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;p&gt;Indicators and strategies can be developed in both Python and Cython. For performance and latency-sensitive applications, we recommend using Cython. Below are some examples:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/nautilus_trader/examples/indicators/ema_python.py"&gt;indicator&lt;/a&gt; example written in Python.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/nautilus_trader/indicators/"&gt;indicator&lt;/a&gt; examples written in Cython.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/nautilus_trader/examples/strategies/"&gt;strategy&lt;/a&gt; examples written in Python.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/examples/backtest/"&gt;backtest&lt;/a&gt; examples using a &lt;code&gt;BacktestEngine&lt;/code&gt; directly.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Docker&lt;/h2&gt; 
&lt;p&gt;Docker containers are built using the base image &lt;code&gt;python:3.12-slim&lt;/code&gt; with the following variant tags:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;nautilus_trader:latest&lt;/code&gt; has the latest release version installed.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;nautilus_trader:nightly&lt;/code&gt; has the head of the &lt;code&gt;nightly&lt;/code&gt; branch installed.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;jupyterlab:latest&lt;/code&gt; has the latest release version installed along with &lt;code&gt;jupyterlab&lt;/code&gt; and an example backtest notebook with accompanying data.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;jupyterlab:nightly&lt;/code&gt; has the head of the &lt;code&gt;nightly&lt;/code&gt; branch installed along with &lt;code&gt;jupyterlab&lt;/code&gt; and an example backtest notebook with accompanying data.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can pull the container images as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker pull ghcr.io/nautechsystems/&amp;lt;image_variant_tag&amp;gt; --platform linux/amd64
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can launch the backtest example container by running:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker pull ghcr.io/nautechsystems/jupyterlab:nightly --platform linux/amd64
docker run -p 8888:8888 ghcr.io/nautechsystems/jupyterlab:nightly
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then open your browser at the following address:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;http://127.0.0.1:8888/lab
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING]&lt;/p&gt; 
 &lt;p&gt;NautilusTrader currently exceeds the rate limit for Jupyter notebook logging (stdout output). Therefore, we set the &lt;code&gt;log_level&lt;/code&gt; to &lt;code&gt;ERROR&lt;/code&gt; in the examples. Lowering this level to see more logging will cause the notebook to hang during cell execution. We are investigating a fix that may involve either raising the configured rate limits for Jupyter or throttling the log flushing from Nautilus.&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href="https://github.com/jupyterlab/jupyterlab/issues/12845"&gt;https://github.com/jupyterlab/jupyterlab/issues/12845&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href="https://github.com/deshaw/jupyterlab-limit-output"&gt;https://github.com/deshaw/jupyterlab-limit-output&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Development&lt;/h2&gt; 
&lt;p&gt;We aim to provide the most pleasant developer experience possible for this hybrid codebase of Python, Cython and Rust. See the &lt;a href="https://nautilustrader.io/docs/latest/developer_guide/index.html"&gt;Developer Guide&lt;/a&gt; for helpful information.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP]&lt;/p&gt; 
 &lt;p&gt;Run &lt;code&gt;make build-debug&lt;/code&gt; to compile after changes to Rust or Cython code for the most efficient development workflow.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Testing with Rust&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://nexte.st"&gt;cargo-nextest&lt;/a&gt; is the standard Rust test runner for NautilusTrader. Its key benefit is isolating each test in its own process, ensuring test reliability by avoiding interference.&lt;/p&gt; 
&lt;p&gt;You can install cargo-nextest by running:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;cargo install cargo-nextest
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP]&lt;/p&gt; 
 &lt;p&gt;Run Rust tests with &lt;code&gt;make cargo-test&lt;/code&gt;, which uses &lt;strong&gt;cargo-nextest&lt;/strong&gt; with an efficient profile.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Thank you for considering contributing to NautilusTrader! We welcome any and all help to improve the project. If you have an idea for an enhancement or a bug fix, the first step is to open an &lt;a href="https://github.com/nautechsystems/nautilus_trader/issues"&gt;issue&lt;/a&gt; on GitHub to discuss it with the team. This helps to ensure that your contribution will be well-aligned with the goals of the project and avoids duplication of effort.&lt;/p&gt; 
&lt;p&gt;Before getting started, be sure to review the &lt;a href="https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/ROADMAP.md#open-source-scope"&gt;open-source scope&lt;/a&gt; outlined in the projectâ€™s roadmap to understand whatâ€™s in and out of scope.&lt;/p&gt; 
&lt;p&gt;Once you're ready to start working on your contribution, make sure to follow the guidelines outlined in the &lt;a href="https://github.com/nautechsystems/nautilus_trader/raw/develop/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; file. This includes signing a Contributor License Agreement (CLA) to ensure that your contributions can be included in the project.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;/p&gt; 
 &lt;p&gt;Pull requests should target the &lt;code&gt;develop&lt;/code&gt; branch (the default branch). This is where new features and improvements are integrated before release.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Thank you again for your interest in NautilusTrader! We look forward to reviewing your contributions and working with you to improve the project.&lt;/p&gt; 
&lt;h2&gt;Community&lt;/h2&gt; 
&lt;p&gt;Join our community of users and contributors on &lt;a href="https://discord.gg/NautilusTrader"&gt;Discord&lt;/a&gt; to chat and stay up-to-date with the latest announcements and features of NautilusTrader. Whether you're a developer looking to contribute or just want to learn more about the platform, all are welcome on our Discord server.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING]&lt;/p&gt; 
 &lt;p&gt;NautilusTrader does not issue, promote, or endorse any cryptocurrency tokens. Any claims or communications suggesting otherwise are unauthorized and false.&lt;/p&gt; 
 &lt;p&gt;All official updates and communications from NautilusTrader will be shared exclusively through &lt;a href="https://nautilustrader.io"&gt;https://nautilustrader.io&lt;/a&gt;, our &lt;a href="https://discord.gg/NautilusTrader"&gt;Discord server&lt;/a&gt;, or our X (Twitter) account: &lt;a href="https://x.com/NautilusTrader"&gt;@NautilusTrader&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;If you encounter any suspicious activity, please report it to the appropriate platform and contact us at &lt;a href="mailto:info@nautechsystems.io"&gt;info@nautechsystems.io&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;The source code for NautilusTrader is available on GitHub under the &lt;a href="https://www.gnu.org/licenses/lgpl-3.0.en.html"&gt;GNU Lesser General Public License v3.0&lt;/a&gt;. Contributions to the project are welcome and require the completion of a standard &lt;a href="https://github.com/nautechsystems/nautilus_trader/raw/develop/CLA.md"&gt;Contributor License Agreement (CLA)&lt;/a&gt;.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;NautilusTraderâ„¢ is developed and maintained by Nautech Systems, a technology company specializing in the development of high-performance trading systems. For more information, visit &lt;a href="https://nautilustrader.io"&gt;https://nautilustrader.io&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Â© 2015-2025 Nautech Systems Pty Ltd. All rights reserved.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://github.com/nautechsystems/nautilus_trader/raw/develop/assets/ns-logo.png" alt="nautechsystems" title="nautechsystems" /&gt; &lt;img src="https://github.com/nautechsystems/nautilus_trader/raw/develop/assets/ferris.png" width="128" /&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>musistudio/claude-code-router</title>
      <link>https://github.com/musistudio/claude-code-router</link>
      <description>&lt;p&gt;Use Claude Code as the foundation for coding infrastructure, allowing you to decide how to interact with the model while enjoying updates from Anthropic.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Claude Code Router&lt;/h1&gt; 
&lt;p&gt;I am seeking funding support for this project to better sustain its development. If you have any ideas, feel free to reach out to me: &lt;a href="mailto:m@musiiot.top"&gt;m@musiiot.top&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/musistudio/claude-code-router/main/README_zh.md"&gt;ä¸­æ–‡ç‰ˆ&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;A powerful tool to route Claude Code requests to different models and customize any request.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/musistudio/claude-code-router/main/blog/images/claude-code.png" alt="" /&gt;&lt;/p&gt; 
&lt;h2&gt;âœ¨ Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Model Routing&lt;/strong&gt;: Route requests to different models based on your needs (e.g., background tasks, thinking, long context).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-Provider Support&lt;/strong&gt;: Supports various model providers like OpenRouter, DeepSeek, Ollama, Gemini, Volcengine, and SiliconFlow.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Request/Response Transformation&lt;/strong&gt;: Customize requests and responses for different providers using transformers.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Dynamic Model Switching&lt;/strong&gt;: Switch models on-the-fly within Claude Code using the &lt;code&gt;/model&lt;/code&gt; command.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;GitHub Actions Integration&lt;/strong&gt;: Trigger Claude Code tasks in your GitHub workflows.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Plugin System&lt;/strong&gt;: Extend functionality with custom transformers.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ğŸš€ Getting Started&lt;/h2&gt; 
&lt;h3&gt;1. Installation&lt;/h3&gt; 
&lt;p&gt;First, ensure you have &lt;a href="https://docs.anthropic.com/en/docs/claude-code/quickstart"&gt;Claude Code&lt;/a&gt; installed:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;npm install -g @anthropic-ai/claude-code
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then, install Claude Code Router:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;npm install -g @musistudio/claude-code-router
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. Configuration&lt;/h3&gt; 
&lt;p&gt;Create and configure your &lt;code&gt;~/.claude-code-router/config.json&lt;/code&gt; file. For more details, you can refer to &lt;code&gt;config.example.json&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;The &lt;code&gt;config.json&lt;/code&gt; file has several key sections:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;PROXY_URL&lt;/code&gt;&lt;/strong&gt; (optional): You can set a proxy for API requests, for example: &lt;code&gt;"PROXY_URL": "http://127.0.0.1:7890"&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;LOG&lt;/code&gt;&lt;/strong&gt; (optional): You can enable logging by setting it to &lt;code&gt;true&lt;/code&gt;. When set to &lt;code&gt;false&lt;/code&gt;, no log files will be created. Default is &lt;code&gt;true&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;LOG_LEVEL&lt;/code&gt;&lt;/strong&gt; (optional): Set the logging level. Available options are: &lt;code&gt;"fatal"&lt;/code&gt;, &lt;code&gt;"error"&lt;/code&gt;, &lt;code&gt;"warn"&lt;/code&gt;, &lt;code&gt;"info"&lt;/code&gt;, &lt;code&gt;"debug"&lt;/code&gt;, &lt;code&gt;"trace"&lt;/code&gt;. Default is &lt;code&gt;"debug"&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Logging Systems&lt;/strong&gt;: The Claude Code Router uses two separate logging systems:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Server-level logs&lt;/strong&gt;: HTTP requests, API calls, and server events are logged using pino in the &lt;code&gt;~/.claude-code-router/logs/&lt;/code&gt; directory with filenames like &lt;code&gt;ccr-*.log&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Application-level logs&lt;/strong&gt;: Routing decisions and business logic events are logged in &lt;code&gt;~/.claude-code-router/claude-code-router.log&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;APIKEY&lt;/code&gt;&lt;/strong&gt; (optional): You can set a secret key to authenticate requests. When set, clients must provide this key in the &lt;code&gt;Authorization&lt;/code&gt; header (e.g., &lt;code&gt;Bearer your-secret-key&lt;/code&gt;) or the &lt;code&gt;x-api-key&lt;/code&gt; header. Example: &lt;code&gt;"APIKEY": "your-secret-key"&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;HOST&lt;/code&gt;&lt;/strong&gt; (optional): You can set the host address for the server. If &lt;code&gt;APIKEY&lt;/code&gt; is not set, the host will be forced to &lt;code&gt;127.0.0.1&lt;/code&gt; for security reasons to prevent unauthorized access. Example: &lt;code&gt;"HOST": "0.0.0.0"&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;NON_INTERACTIVE_MODE&lt;/code&gt;&lt;/strong&gt; (optional): When set to &lt;code&gt;true&lt;/code&gt;, enables compatibility with non-interactive environments like GitHub Actions, Docker containers, or other CI/CD systems. This sets appropriate environment variables (&lt;code&gt;CI=true&lt;/code&gt;, &lt;code&gt;FORCE_COLOR=0&lt;/code&gt;, etc.) and configures stdin handling to prevent the process from hanging in automated environments. Example: &lt;code&gt;"NON_INTERACTIVE_MODE": true&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;Providers&lt;/code&gt;&lt;/strong&gt;: Used to configure different model providers.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;Router&lt;/code&gt;&lt;/strong&gt;: Used to set up routing rules. &lt;code&gt;default&lt;/code&gt; specifies the default model, which will be used for all requests if no other route is configured.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;API_TIMEOUT_MS&lt;/code&gt;&lt;/strong&gt;: Specifies the timeout for API calls in milliseconds.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Environment Variable Interpolation&lt;/h4&gt; 
&lt;p&gt;Claude Code Router supports environment variable interpolation for secure API key management. You can reference environment variables in your &lt;code&gt;config.json&lt;/code&gt; using either &lt;code&gt;$VAR_NAME&lt;/code&gt; or &lt;code&gt;${VAR_NAME}&lt;/code&gt; syntax:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "OPENAI_API_KEY": "$OPENAI_API_KEY",
  "GEMINI_API_KEY": "${GEMINI_API_KEY}",
  "Providers": [
    {
      "name": "openai",
      "api_base_url": "https://api.openai.com/v1/chat/completions",
      "api_key": "$OPENAI_API_KEY",
      "models": ["gpt-5", "gpt-5-mini"]
    }
  ]
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This allows you to keep sensitive API keys in environment variables instead of hardcoding them in configuration files. The interpolation works recursively through nested objects and arrays.&lt;/p&gt; 
&lt;p&gt;Here is a comprehensive example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "APIKEY": "your-secret-key",
  "PROXY_URL": "http://127.0.0.1:7890",
  "LOG": true,
  "API_TIMEOUT_MS": 600000,
  "NON_INTERACTIVE_MODE": false,
  "Providers": [
    {
      "name": "openrouter",
      "api_base_url": "https://openrouter.ai/api/v1/chat/completions",
      "api_key": "sk-xxx",
      "models": [
        "google/gemini-2.5-pro-preview",
        "anthropic/claude-sonnet-4",
        "anthropic/claude-3.5-sonnet",
        "anthropic/claude-3.7-sonnet:thinking"
      ],
      "transformer": {
        "use": ["openrouter"]
      }
    },
    {
      "name": "deepseek",
      "api_base_url": "https://api.deepseek.com/chat/completions",
      "api_key": "sk-xxx",
      "models": ["deepseek-chat", "deepseek-reasoner"],
      "transformer": {
        "use": ["deepseek"],
        "deepseek-chat": {
          "use": ["tooluse"]
        }
      }
    },
    {
      "name": "ollama",
      "api_base_url": "http://localhost:11434/v1/chat/completions",
      "api_key": "ollama",
      "models": ["qwen2.5-coder:latest"]
    },
    {
      "name": "gemini",
      "api_base_url": "https://generativelanguage.googleapis.com/v1beta/models/",
      "api_key": "sk-xxx",
      "models": ["gemini-2.5-flash", "gemini-2.5-pro"],
      "transformer": {
        "use": ["gemini"]
      }
    },
    {
      "name": "volcengine",
      "api_base_url": "https://ark.cn-beijing.volces.com/api/v3/chat/completions",
      "api_key": "sk-xxx",
      "models": ["deepseek-v3-250324", "deepseek-r1-250528"],
      "transformer": {
        "use": ["deepseek"]
      }
    },
    {
      "name": "modelscope",
      "api_base_url": "https://api-inference.modelscope.cn/v1/chat/completions",
      "api_key": "",
      "models": ["Qwen/Qwen3-Coder-480B-A35B-Instruct", "Qwen/Qwen3-235B-A22B-Thinking-2507"],
      "transformer": {
        "use": [
          [
            "maxtoken",
            {
              "max_tokens": 65536
            }
          ],
          "enhancetool"
        ],
        "Qwen/Qwen3-235B-A22B-Thinking-2507": {
          "use": ["reasoning"]
        }
      }
    },
    {
      "name": "dashscope",
      "api_base_url": "https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions",
      "api_key": "",
      "models": ["qwen3-coder-plus"],
      "transformer": {
        "use": [
          [
            "maxtoken",
            {
              "max_tokens": 65536
            }
          ],
          "enhancetool"
        ]
      }
    },
    {
      "name": "aihubmix",
      "api_base_url": "https://aihubmix.com/v1/chat/completions",
      "api_key": "sk-",
      "models": [
        "Z/glm-4.5",
        "claude-opus-4-20250514",
        "gemini-2.5-pro"
      ]
    }
  ],
  "Router": {
    "default": "deepseek,deepseek-chat",
    "background": "ollama,qwen2.5-coder:latest",
    "think": "deepseek,deepseek-reasoner",
    "longContext": "openrouter,google/gemini-2.5-pro-preview",
    "longContextThreshold": 60000,
    "webSearch": "gemini,gemini-2.5-flash"
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;3. Running Claude Code with the Router&lt;/h3&gt; 
&lt;p&gt;Start Claude Code using the router:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;ccr code
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: After modifying the configuration file, you need to restart the service for the changes to take effect:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;ccr restart
&lt;/code&gt;&lt;/pre&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;4. UI Mode&lt;/h3&gt; 
&lt;p&gt;For a more intuitive experience, you can use the UI mode to manage your configuration:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;ccr ui
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will open a web-based interface where you can easily view and edit your &lt;code&gt;config.json&lt;/code&gt; file.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/musistudio/claude-code-router/main/blog/images/ui.png" alt="UI" /&gt;&lt;/p&gt; 
&lt;h4&gt;Providers&lt;/h4&gt; 
&lt;p&gt;The &lt;code&gt;Providers&lt;/code&gt; array is where you define the different model providers you want to use. Each provider object requires:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;name&lt;/code&gt;: A unique name for the provider.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;api_base_url&lt;/code&gt;: The full API endpoint for chat completions.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;api_key&lt;/code&gt;: Your API key for the provider.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;models&lt;/code&gt;: A list of model names available from this provider.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;transformer&lt;/code&gt; (optional): Specifies transformers to process requests and responses.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Transformers&lt;/h4&gt; 
&lt;p&gt;Transformers allow you to modify the request and response payloads to ensure compatibility with different provider APIs.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Global Transformer&lt;/strong&gt;: Apply a transformer to all models from a provider. In this example, the &lt;code&gt;openrouter&lt;/code&gt; transformer is applied to all models under the &lt;code&gt;openrouter&lt;/code&gt; provider.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-json"&gt;{
  "name": "openrouter",
  "api_base_url": "https://openrouter.ai/api/v1/chat/completions",
  "api_key": "sk-xxx",
  "models": [
    "google/gemini-2.5-pro-preview",
    "anthropic/claude-sonnet-4",
    "anthropic/claude-3.5-sonnet"
  ],
  "transformer": { "use": ["openrouter"] }
}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Model-Specific Transformer&lt;/strong&gt;: Apply a transformer to a specific model. In this example, the &lt;code&gt;deepseek&lt;/code&gt; transformer is applied to all models, and an additional &lt;code&gt;tooluse&lt;/code&gt; transformer is applied only to the &lt;code&gt;deepseek-chat&lt;/code&gt; model.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-json"&gt;{
  "name": "deepseek",
  "api_base_url": "https://api.deepseek.com/chat/completions",
  "api_key": "sk-xxx",
  "models": ["deepseek-chat", "deepseek-reasoner"],
  "transformer": {
    "use": ["deepseek"],
    "deepseek-chat": { "use": ["tooluse"] }
  }
}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Passing Options to a Transformer&lt;/strong&gt;: Some transformers, like &lt;code&gt;maxtoken&lt;/code&gt;, accept options. To pass options, use a nested array where the first element is the transformer name and the second is an options object.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-json"&gt;{
  "name": "siliconflow",
  "api_base_url": "https://api.siliconflow.cn/v1/chat/completions",
  "api_key": "sk-xxx",
  "models": ["moonshotai/Kimi-K2-Instruct"],
  "transformer": {
    "use": [
      [
        "maxtoken",
        {
          "max_tokens": 16384
        }
      ]
    ]
  }
}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Available Built-in Transformers:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;Anthropic&lt;/code&gt;:If you use only the &lt;code&gt;Anthropic&lt;/code&gt; transformer, it will preserve the original request and response parameters(you can use it to connect directly to an Anthropic endpoint).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;deepseek&lt;/code&gt;: Adapts requests/responses for DeepSeek API.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;gemini&lt;/code&gt;: Adapts requests/responses for Gemini API.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;openrouter&lt;/code&gt;: Adapts requests/responses for OpenRouter API. It can also accept a &lt;code&gt;provider&lt;/code&gt; routing parameter to specify which underlying providers OpenRouter should use. For more details, refer to the &lt;a href="https://openrouter.ai/docs/features/provider-routing"&gt;OpenRouter documentation&lt;/a&gt;. See an example below: &lt;pre&gt;&lt;code class="language-json"&gt;  "transformer": {
    "use": ["openrouter"],
    "moonshotai/kimi-k2": {
      "use": [
        [
          "openrouter",
          {
            "provider": {
              "only": ["moonshotai/fp8"]
            }
          }
        ]
      ]
    }
  }
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;groq&lt;/code&gt;: Adapts requests/responses for groq API.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;maxtoken&lt;/code&gt;: Sets a specific &lt;code&gt;max_tokens&lt;/code&gt; value.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;tooluse&lt;/code&gt;: Optimizes tool usage for certain models via &lt;code&gt;tool_choice&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;gemini-cli&lt;/code&gt; (experimental): Unofficial support for Gemini via Gemini CLI &lt;a href="https://gist.github.com/musistudio/1c13a65f35916a7ab690649d3df8d1cd"&gt;gemini-cli.js&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;reasoning&lt;/code&gt;: Used to process the &lt;code&gt;reasoning_content&lt;/code&gt; field.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;sampling&lt;/code&gt;: Used to process sampling information fields such as &lt;code&gt;temperature&lt;/code&gt;, &lt;code&gt;top_p&lt;/code&gt;, &lt;code&gt;top_k&lt;/code&gt;, and &lt;code&gt;repetition_penalty&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;enhancetool&lt;/code&gt;: Adds a layer of error tolerance to the tool call parameters returned by the LLM (this will cause the tool call information to no longer be streamed).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;cleancache&lt;/code&gt;: Clears the &lt;code&gt;cache_control&lt;/code&gt; field from requests.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;vertex-gemini&lt;/code&gt;: Handles the Gemini API using Vertex authentication.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;qwen-cli&lt;/code&gt; (experimental): Unofficial support for qwen3-coder-plus model via Qwen CLI &lt;a href="https://gist.github.com/musistudio/f5a67841ced39912fd99e42200d5ca8b"&gt;qwen-cli.js&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;rovo-cli&lt;/code&gt; (experimental): Unofficial support for gpt-5 via Atlassian Rovo Dev CLI &lt;a href="https://gist.github.com/SaseQ/c2a20a38b11276537ec5332d1f7a5e53"&gt;rovo-cli.js&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Custom Transformers:&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;You can also create your own transformers and load them via the &lt;code&gt;transformers&lt;/code&gt; field in &lt;code&gt;config.json&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "transformers": [
    {
      "path": "/User/xxx/.claude-code-router/plugins/gemini-cli.js",
      "options": {
        "project": "xxx"
      }
    }
  ]
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Router&lt;/h4&gt; 
&lt;p&gt;The &lt;code&gt;Router&lt;/code&gt; object defines which model to use for different scenarios:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;default&lt;/code&gt;: The default model for general tasks.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;background&lt;/code&gt;: A model for background tasks. This can be a smaller, local model to save costs.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;think&lt;/code&gt;: A model for reasoning-heavy tasks, like Plan Mode.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;longContext&lt;/code&gt;: A model for handling long contexts (e.g., &amp;gt; 60K tokens).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;longContextThreshold&lt;/code&gt; (optional): The token count threshold for triggering the long context model. Defaults to 60000 if not specified.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;webSearch&lt;/code&gt;: Used for handling web search tasks and this requires the model itself to support the feature. If you're using openrouter, you need to add the &lt;code&gt;:online&lt;/code&gt; suffix after the model name.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can also switch models dynamically in Claude Code with the &lt;code&gt;/model&lt;/code&gt; command: &lt;code&gt;/model provider_name,model_name&lt;/code&gt; Example: &lt;code&gt;/model openrouter,anthropic/claude-3.5-sonnet&lt;/code&gt;&lt;/p&gt; 
&lt;h4&gt;Custom Router&lt;/h4&gt; 
&lt;p&gt;For more advanced routing logic, you can specify a custom router script via the &lt;code&gt;CUSTOM_ROUTER_PATH&lt;/code&gt; in your &lt;code&gt;config.json&lt;/code&gt;. This allows you to implement complex routing rules beyond the default scenarios.&lt;/p&gt; 
&lt;p&gt;In your &lt;code&gt;config.json&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-json"&gt;{
  "CUSTOM_ROUTER_PATH": "/User/xxx/.claude-code-router/custom-router.js"
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The custom router file must be a JavaScript module that exports an &lt;code&gt;async&lt;/code&gt; function. This function receives the request object and the config object as arguments and should return the provider and model name as a string (e.g., &lt;code&gt;"provider_name,model_name"&lt;/code&gt;), or &lt;code&gt;null&lt;/code&gt; to fall back to the default router.&lt;/p&gt; 
&lt;p&gt;Here is an example of a &lt;code&gt;custom-router.js&lt;/code&gt; based on &lt;code&gt;custom-router.example.js&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-javascript"&gt;// /User/xxx/.claude-code-router/custom-router.js

/**
 * A custom router function to determine which model to use based on the request.
 *
 * @param {object} req - The request object from Claude Code, containing the request body.
 * @param {object} config - The application's config object.
 * @returns {Promise&amp;lt;string|null&amp;gt;} - A promise that resolves to the "provider,model_name" string, or null to use the default router.
 */
module.exports = async function router(req, config) {
  const userMessage = req.body.messages.find((m) =&amp;gt; m.role === "user")?.content;

  if (userMessage &amp;amp;&amp;amp; userMessage.includes("explain this code")) {
    // Use a powerful model for code explanation
    return "openrouter,anthropic/claude-3.5-sonnet";
  }

  // Fallback to the default router configuration
  return null;
};
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;Subagent Routing&lt;/h5&gt; 
&lt;p&gt;For routing within subagents, you must specify a particular provider and model by including &lt;code&gt;&amp;lt;CCR-SUBAGENT-MODEL&amp;gt;provider,model&amp;lt;/CCR-SUBAGENT-MODEL&amp;gt;&lt;/code&gt; at the &lt;strong&gt;beginning&lt;/strong&gt; of the subagent's prompt. This allows you to direct specific subagent tasks to designated models.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;lt;CCR-SUBAGENT-MODEL&amp;gt;openrouter,anthropic/claude-3.5-sonnet&amp;lt;/CCR-SUBAGENT-MODEL&amp;gt;
Please help me analyze this code snippet for potential optimizations...
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Status Line (Beta)&lt;/h2&gt; 
&lt;p&gt;To better monitor the status of claude-code-router at runtime, version v1.0.40 includes a built-in statusline tool, which you can enable in the UI. &lt;img src="https://raw.githubusercontent.com/musistudio/claude-code-router/main/blog/images/statusline-config.png" alt="statusline-config.png" /&gt;&lt;/p&gt; 
&lt;p&gt;The effect is as follows: &lt;img src="https://raw.githubusercontent.com/musistudio/claude-code-router/main/blog/images/statusline.png" alt="statusline" /&gt;&lt;/p&gt; 
&lt;h2&gt;ğŸ¤– GitHub Actions&lt;/h2&gt; 
&lt;p&gt;Integrate Claude Code Router into your CI/CD pipeline. After setting up &lt;a href="https://docs.anthropic.com/en/docs/claude-code/github-actions"&gt;Claude Code Actions&lt;/a&gt;, modify your &lt;code&gt;.github/workflows/claude.yaml&lt;/code&gt; to use the router:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;name: Claude Code

on:
  issue_comment:
    types: [created]
  # ... other triggers

jobs:
  claude:
    if: |
      (github.event_name == 'issue_comment' &amp;amp;&amp;amp; contains(github.event.comment.body, '@claude')) ||
      # ... other conditions
    runs-on: ubuntu-latest
    permissions:
      contents: read
      pull-requests: read
      issues: read
      id-token: write
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Prepare Environment
        run: |
          curl -fsSL https://bun.sh/install | bash
          mkdir -p $HOME/.claude-code-router
          cat &amp;lt;&amp;lt; 'EOF' &amp;gt; $HOME/.claude-code-router/config.json
          {
            "log": true,
            "NON_INTERACTIVE_MODE": true,
            "OPENAI_API_KEY": "${{ secrets.OPENAI_API_KEY }}",
            "OPENAI_BASE_URL": "https://api.deepseek.com",
            "OPENAI_MODEL": "deepseek-chat"
          }
          EOF
        shell: bash

      - name: Start Claude Code Router
        run: |
          nohup ~/.bun/bin/bunx @musistudio/claude-code-router@1.0.8 start &amp;amp;
        shell: bash

      - name: Run Claude Code
        id: claude
        uses: anthropics/claude-code-action@beta
        env:
          ANTHROPIC_BASE_URL: http://localhost:3456
        with:
          anthropic_api_key: "any-string-is-ok"
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: When running in GitHub Actions or other automation environments, make sure to set &lt;code&gt;"NON_INTERACTIVE_MODE": true&lt;/code&gt; in your configuration to prevent the process from hanging due to stdin handling issues.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;This setup allows for interesting automations, like running tasks during off-peak hours to reduce API costs.&lt;/p&gt; 
&lt;h2&gt;ğŸ“ Further Reading&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/musistudio/claude-code-router/main/blog/en/project-motivation-and-how-it-works.md"&gt;Project Motivation and How It Works&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/musistudio/claude-code-router/main/blog/en/maybe-we-can-do-more-with-the-route.md"&gt;Maybe We Can Do More with the Router&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;â¤ï¸ Support &amp;amp; Sponsoring&lt;/h2&gt; 
&lt;p&gt;If you find this project helpful, please consider sponsoring its development. Your support is greatly appreciated!&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://ko-fi.com/F1F31GN2GM"&gt;&lt;img src="https://ko-fi.com/img/githubbutton_sm.svg?sanitize=true" alt="ko-fi" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://paypal.me/musistudio1999"&gt;Paypal&lt;/a&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/musistudio/claude-code-router/main/blog/images/alipay.jpg" width="200" alt="Alipay" /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src="https://raw.githubusercontent.com/musistudio/claude-code-router/main/blog/images/wechat.jpg" width="200" alt="WeChat Pay" /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h3&gt;Our Sponsors&lt;/h3&gt; 
&lt;p&gt;A huge thank you to all our sponsors for their generous support!&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://aihubmix.com/"&gt;AIHubmix&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ai.burncloud.com"&gt;BurnCloud&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;@Simon Leischnig&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/duanshuaimin"&gt;@duanshuaimin&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/vrgitadmin"&gt;@vrgitadmin&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;@*o&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/ceilwoo"&gt;@ceilwoo&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;@*è¯´&lt;/li&gt; 
 &lt;li&gt;@*æ›´&lt;/li&gt; 
 &lt;li&gt;@K*g&lt;/li&gt; 
 &lt;li&gt;@R*R&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/bobleer"&gt;@bobleer&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;@*è‹—&lt;/li&gt; 
 &lt;li&gt;@*åˆ’&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/Clarence-pan"&gt;@Clarence-pan&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/carter003"&gt;@carter003&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;@S*r&lt;/li&gt; 
 &lt;li&gt;@*æ™–&lt;/li&gt; 
 &lt;li&gt;@*æ•&lt;/li&gt; 
 &lt;li&gt;@Z*z&lt;/li&gt; 
 &lt;li&gt;@*ç„¶&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/cluic"&gt;@cluic&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;@*è‹—&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/PromptExpert"&gt;@PromptExpert&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;@*åº”&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/yusnake"&gt;@yusnake&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;@*é£&lt;/li&gt; 
 &lt;li&gt;@è‘£*&lt;/li&gt; 
 &lt;li&gt;@*æ±€&lt;/li&gt; 
 &lt;li&gt;@*æ¶¯&lt;/li&gt; 
 &lt;li&gt;@*:-ï¼‰&lt;/li&gt; 
 &lt;li&gt;@**ç£Š&lt;/li&gt; 
 &lt;li&gt;@*ç¢&lt;/li&gt; 
 &lt;li&gt;@*æˆ&lt;/li&gt; 
 &lt;li&gt;@Z*o&lt;/li&gt; 
 &lt;li&gt;@*ç¨&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/congzhangzh"&gt;@congzhangzh&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;@*_&lt;/li&gt; 
 &lt;li&gt;@Z*m&lt;/li&gt; 
 &lt;li&gt;@*é‘«&lt;/li&gt; 
 &lt;li&gt;@c*y&lt;/li&gt; 
 &lt;li&gt;@*æ˜•&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/witsice"&gt;@witsice&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;@b*g&lt;/li&gt; 
 &lt;li&gt;@*äº¿&lt;/li&gt; 
 &lt;li&gt;@*è¾‰&lt;/li&gt; 
 &lt;li&gt;@JACK&lt;/li&gt; 
 &lt;li&gt;@*å…‰&lt;/li&gt; 
 &lt;li&gt;@W*l&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/kesku"&gt;@kesku&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;@æ°´*ä¸«&lt;/li&gt; 
 &lt;li&gt;@äºŒå‰å‰&lt;/li&gt; 
 &lt;li&gt;@a*g&lt;/li&gt; 
 &lt;li&gt;@*æ—&lt;/li&gt; 
 &lt;li&gt;@*å’¸&lt;/li&gt; 
 &lt;li&gt;@*æ˜&lt;/li&gt; 
 &lt;li&gt;@S*y&lt;/li&gt; 
 &lt;li&gt;@f*o&lt;/li&gt; 
 &lt;li&gt;@*æ™º&lt;/li&gt; 
 &lt;li&gt;@F*t&lt;/li&gt; 
 &lt;li&gt;@r*c&lt;/li&gt; 
 &lt;li&gt;&lt;a href="http://github.com/qierkang"&gt;@qierkang&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;@*å†›&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;(If your name is masked, please contact me via my homepage email to update it with your GitHub username.)&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>