<rss version="2.0">
  <channel>
    <title>GitHub Python Daily Trending</title>
    <description>Daily Trending of Python in GitHub</description>
    <pubDate>Sun, 24 Aug 2025 01:38:51 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>HunxByts/GhostTrack</title>
      <link>https://github.com/HunxByts/GhostTrack</link>
      <description>&lt;p&gt;Useful tool to track location or mobile number&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;GhostTrack&lt;/h1&gt; 
&lt;p&gt;Useful tool to track location or mobile number, so this tool can be called osint or also information gathering&lt;/p&gt; 
&lt;img src="https://github.com/HunxByts/GhostTrack/raw/main/asset/bn.png" /&gt; 
&lt;p&gt;New update : &lt;code&gt;Version 2.2&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Instalation on Linux (deb)&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;sudo apt-get install git
sudo apt-get install python3
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Instalation on Termux&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;pkg install git
pkg install python3
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Usage Tool&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;git clone https://github.com/HunxByts/GhostTrack.git
cd GhostTrack
pip3 install -r requirements.txt
python3 GhostTR.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Display on the menu &lt;code&gt;IP Tracker&lt;/code&gt;&lt;/p&gt; 
&lt;img src="https://github.com/HunxByts/GhostTrack/blob/main/asset/ip.png " /&gt; 
&lt;p&gt;on the IP Track menu, you can combo with the seeker tool to get the target IP&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;span&gt;‚ö°&lt;/span&gt; Install Seeker :&lt;/summary&gt; - 
 &lt;strong&gt;&lt;a href="https://github.com/thewhiteh4t/seeker"&gt;Get Seeker&lt;/a&gt;&lt;/strong&gt; 
&lt;/details&gt; 
&lt;p&gt;Display on the menu &lt;code&gt;Phone Tracker&lt;/code&gt;&lt;/p&gt; 
&lt;img src="https://github.com/HunxByts/GhostTrack/raw/main/asset/phone.png" /&gt; 
&lt;p&gt;on this menu you can search for information from the target phone number&lt;/p&gt; 
&lt;p&gt;Display on the menu &lt;code&gt;Username Tracker&lt;/code&gt;&lt;/p&gt; 
&lt;img src="https://github.com/HunxByts/GhostTrack/raw/main/asset/User.png" /&gt; on this menu you can search for information from the target username on social media 
&lt;details&gt; 
 &lt;summary&gt;&lt;span&gt;‚ö°&lt;/span&gt; Author :&lt;/summary&gt; - 
 &lt;strong&gt;&lt;a href="https://github.com/HunxByts"&gt;HunxByts&lt;/a&gt;&lt;/strong&gt; 
&lt;/details&gt;</description>
    </item>
    
    <item>
      <title>deepseek-ai/DeepSeek-V3</title>
      <link>https://github.com/deepseek-ai/DeepSeek-V3</link>
      <description>&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;img src="https://github.com/deepseek-ai/DeepSeek-V2/raw/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" /&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;div align="center" style="line-height: 1;"&gt; 
 &lt;a href="https://www.deepseek.com/"&gt;&lt;img alt="Homepage" src="https://github.com/deepseek-ai/DeepSeek-V2/raw/main/figures/badge.svg?raw=true" /&gt;&lt;/a&gt; 
 &lt;a href="https://chat.deepseek.com/"&gt;&lt;img alt="Chat" src="https://img.shields.io/badge/ü§ñ%20Chat-DeepSeek%20V3-536af5?color=536af5&amp;amp;logoColor=white" /&gt;&lt;/a&gt; 
 &lt;a href="https://huggingface.co/deepseek-ai"&gt;&lt;img alt="Hugging Face" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&amp;amp;logoColor=white" /&gt;&lt;/a&gt; 
 &lt;br /&gt; 
 &lt;a href="https://discord.gg/Tc7c45Zzu5"&gt;&lt;img alt="Discord" src="https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&amp;amp;logoColor=white&amp;amp;color=7289da" /&gt;&lt;/a&gt; 
 &lt;a href="https://github.com/deepseek-ai/DeepSeek-V2/raw/main/figures/qr.jpeg?raw=true"&gt;&lt;img alt="Wechat" src="https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&amp;amp;logoColor=white" /&gt;&lt;/a&gt; 
 &lt;a href="https://twitter.com/deepseek_ai"&gt;&lt;img alt="Twitter Follow" src="https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&amp;amp;logoColor=white" /&gt;&lt;/a&gt; 
 &lt;br /&gt; 
 &lt;a href="https://github.com/deepseek-ai/DeepSeek-V3/raw/main/LICENSE-CODE"&gt;&lt;img alt="Code License" src="https://img.shields.io/badge/Code_License-MIT-f5de53?&amp;amp;color=f5de53" /&gt;&lt;/a&gt; 
 &lt;a href="https://github.com/deepseek-ai/DeepSeek-V3/raw/main/LICENSE-MODEL"&gt;&lt;img alt="Model License" src="https://img.shields.io/badge/Model_License-Model_Agreement-f5de53?&amp;amp;color=f5de53" /&gt;&lt;/a&gt; 
 &lt;br /&gt; 
 &lt;a href="https://arxiv.org/pdf/2412.19437"&gt;&lt;b&gt;Paper Link&lt;/b&gt;üëÅÔ∏è&lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/#1-introduction"&gt;Introduction&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/#2-model-summary"&gt;Model Summary&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/#3-model-downloads"&gt;Model Downloads&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/#4-evaluation-results"&gt;Evaluation Results&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/#5-chat-website--api-platform"&gt;Chat Website &amp;amp; API Platform&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/#6-how-to-run-locally"&gt;How to Run Locally&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/#7-license"&gt;License&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/#8-citation"&gt;Citation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/#9-contact"&gt;Contact&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;1. Introduction&lt;/h2&gt; 
&lt;p&gt;We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. In addition, its training process is remarkably stable. Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks.&lt;/p&gt; 
&lt;p align="center"&gt; &lt;img width="80%" src="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/figures/benchmark.png" /&gt; &lt;/p&gt; 
&lt;h2&gt;2. Model Summary&lt;/h2&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;strong&gt;Architecture: Innovative Load Balancing Strategy and Training Objective&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;On top of the efficient architecture of DeepSeek-V2, we pioneer an auxiliary-loss-free strategy for load balancing, which minimizes the performance degradation that arises from encouraging load balancing.&lt;/li&gt; 
 &lt;li&gt;We investigate a Multi-Token Prediction (MTP) objective and prove it beneficial to model performance. It can also be used for speculative decoding for inference acceleration.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;strong&gt;Pre-Training: Towards Ultimate Training Efficiency&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;We design an FP8 mixed precision training framework and, for the first time, validate the feasibility and effectiveness of FP8 training on an extremely large-scale model.&lt;/li&gt; 
 &lt;li&gt;Through co-design of algorithms, frameworks, and hardware, we overcome the communication bottleneck in cross-node MoE training, nearly achieving full computation-communication overlap.&lt;br /&gt; This significantly enhances our training efficiency and reduces the training costs, enabling us to further scale up the model size without additional overhead.&lt;/li&gt; 
 &lt;li&gt;At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. The subsequent training stages after pre-training require only 0.1M GPU hours.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;strong&gt;Post-Training: Knowledge Distillation from DeepSeek-R1&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;We introduce an innovative methodology to distill reasoning capabilities from the long-Chain-of-Thought (CoT) model, specifically from one of the DeepSeek R1 series models, into standard LLMs, particularly DeepSeek-V3. Our pipeline elegantly incorporates the verification and reflection patterns of R1 into DeepSeek-V3 and notably improves its reasoning performance. Meanwhile, we also maintain a control over the output style and length of DeepSeek-V3.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;3. Model Downloads&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;#Total Params&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;#Activated Params&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;Context Length&lt;/strong&gt;&lt;/th&gt; 
    &lt;th align="center"&gt;&lt;strong&gt;Download&lt;/strong&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;DeepSeek-V3-Base&lt;/td&gt; 
    &lt;td align="center"&gt;671B&lt;/td&gt; 
    &lt;td align="center"&gt;37B&lt;/td&gt; 
    &lt;td align="center"&gt;128K&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3-Base"&gt;ü§ó Hugging Face&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center"&gt;DeepSeek-V3&lt;/td&gt; 
    &lt;td align="center"&gt;671B&lt;/td&gt; 
    &lt;td align="center"&gt;37B&lt;/td&gt; 
    &lt;td align="center"&gt;128K&lt;/td&gt; 
    &lt;td align="center"&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3"&gt;ü§ó Hugging Face&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] The total size of DeepSeek-V3 models on Hugging Face is 685B, which includes 671B of the Main Model weights and 14B of the Multi-Token Prediction (MTP) Module weights.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;To ensure optimal performance and flexibility, we have partnered with open-source communities and hardware vendors to provide multiple ways to run the model locally. For step-by-step guidance, check out Section 6: &lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/#6-how-to-run-locally"&gt;How_to Run_Locally&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For developers looking to dive deeper, we recommend exploring &lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/README_WEIGHTS.md"&gt;README_WEIGHTS.md&lt;/a&gt; for details on the Main Model weights and the Multi-Token Prediction (MTP) Modules. Please note that MTP support is currently under active development within the community, and we welcome your contributions and feedback.&lt;/p&gt; 
&lt;h2&gt;4. Evaluation Results&lt;/h2&gt; 
&lt;h3&gt;Base Model&lt;/h3&gt; 
&lt;h4&gt;Standard Benchmarks&lt;/h4&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;&lt;/th&gt; 
    &lt;th&gt;Benchmark (Metric)&lt;/th&gt; 
    &lt;th&gt;# Shots&lt;/th&gt; 
    &lt;th&gt;DeepSeek-V2&lt;/th&gt; 
    &lt;th&gt;Qwen2.5 72B&lt;/th&gt; 
    &lt;th&gt;LLaMA3.1 405B&lt;/th&gt; 
    &lt;th&gt;DeepSeek-V3&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;Architecture&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;MoE&lt;/td&gt; 
    &lt;td&gt;Dense&lt;/td&gt; 
    &lt;td&gt;Dense&lt;/td&gt; 
    &lt;td&gt;MoE&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;# Activated Params&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;21B&lt;/td&gt; 
    &lt;td&gt;72B&lt;/td&gt; 
    &lt;td&gt;405B&lt;/td&gt; 
    &lt;td&gt;37B&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;# Total Params&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;236B&lt;/td&gt; 
    &lt;td&gt;72B&lt;/td&gt; 
    &lt;td&gt;405B&lt;/td&gt; 
    &lt;td&gt;671B&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;English&lt;/td&gt; 
    &lt;td&gt;Pile-test (BPB)&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;0.606&lt;/td&gt; 
    &lt;td&gt;0.638&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;0.542&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;0.548&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;BBH (EM)&lt;/td&gt; 
    &lt;td&gt;3-shot&lt;/td&gt; 
    &lt;td&gt;78.8&lt;/td&gt; 
    &lt;td&gt;79.8&lt;/td&gt; 
    &lt;td&gt;82.9&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;87.5&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;MMLU (Acc.)&lt;/td&gt; 
    &lt;td&gt;5-shot&lt;/td&gt; 
    &lt;td&gt;78.4&lt;/td&gt; 
    &lt;td&gt;85.0&lt;/td&gt; 
    &lt;td&gt;84.4&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;87.1&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;MMLU-Redux (Acc.)&lt;/td&gt; 
    &lt;td&gt;5-shot&lt;/td&gt; 
    &lt;td&gt;75.6&lt;/td&gt; 
    &lt;td&gt;83.2&lt;/td&gt; 
    &lt;td&gt;81.3&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;86.2&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;MMLU-Pro (Acc.)&lt;/td&gt; 
    &lt;td&gt;5-shot&lt;/td&gt; 
    &lt;td&gt;51.4&lt;/td&gt; 
    &lt;td&gt;58.3&lt;/td&gt; 
    &lt;td&gt;52.8&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;64.4&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;DROP (F1)&lt;/td&gt; 
    &lt;td&gt;3-shot&lt;/td&gt; 
    &lt;td&gt;80.4&lt;/td&gt; 
    &lt;td&gt;80.6&lt;/td&gt; 
    &lt;td&gt;86.0&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;89.0&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;ARC-Easy (Acc.)&lt;/td&gt; 
    &lt;td&gt;25-shot&lt;/td&gt; 
    &lt;td&gt;97.6&lt;/td&gt; 
    &lt;td&gt;98.4&lt;/td&gt; 
    &lt;td&gt;98.4&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;98.9&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;ARC-Challenge (Acc.)&lt;/td&gt; 
    &lt;td&gt;25-shot&lt;/td&gt; 
    &lt;td&gt;92.2&lt;/td&gt; 
    &lt;td&gt;94.5&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;95.3&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;95.3&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;HellaSwag (Acc.)&lt;/td&gt; 
    &lt;td&gt;10-shot&lt;/td&gt; 
    &lt;td&gt;87.1&lt;/td&gt; 
    &lt;td&gt;84.8&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;89.2&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;88.9&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;PIQA (Acc.)&lt;/td&gt; 
    &lt;td&gt;0-shot&lt;/td&gt; 
    &lt;td&gt;83.9&lt;/td&gt; 
    &lt;td&gt;82.6&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;85.9&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;84.7&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;WinoGrande (Acc.)&lt;/td&gt; 
    &lt;td&gt;5-shot&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;86.3&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;82.3&lt;/td&gt; 
    &lt;td&gt;85.2&lt;/td&gt; 
    &lt;td&gt;84.9&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;RACE-Middle (Acc.)&lt;/td&gt; 
    &lt;td&gt;5-shot&lt;/td&gt; 
    &lt;td&gt;73.1&lt;/td&gt; 
    &lt;td&gt;68.1&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;74.2&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;67.1&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;RACE-High (Acc.)&lt;/td&gt; 
    &lt;td&gt;5-shot&lt;/td&gt; 
    &lt;td&gt;52.6&lt;/td&gt; 
    &lt;td&gt;50.3&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;56.8&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;51.3&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;TriviaQA (EM)&lt;/td&gt; 
    &lt;td&gt;5-shot&lt;/td&gt; 
    &lt;td&gt;80.0&lt;/td&gt; 
    &lt;td&gt;71.9&lt;/td&gt; 
    &lt;td&gt;82.7&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;82.9&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;NaturalQuestions (EM)&lt;/td&gt; 
    &lt;td&gt;5-shot&lt;/td&gt; 
    &lt;td&gt;38.6&lt;/td&gt; 
    &lt;td&gt;33.2&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;41.5&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;40.0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;AGIEval (Acc.)&lt;/td&gt; 
    &lt;td&gt;0-shot&lt;/td&gt; 
    &lt;td&gt;57.5&lt;/td&gt; 
    &lt;td&gt;75.8&lt;/td&gt; 
    &lt;td&gt;60.6&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;79.6&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Code&lt;/td&gt; 
    &lt;td&gt;HumanEval (Pass@1)&lt;/td&gt; 
    &lt;td&gt;0-shot&lt;/td&gt; 
    &lt;td&gt;43.3&lt;/td&gt; 
    &lt;td&gt;53.0&lt;/td&gt; 
    &lt;td&gt;54.9&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;65.2&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;MBPP (Pass@1)&lt;/td&gt; 
    &lt;td&gt;3-shot&lt;/td&gt; 
    &lt;td&gt;65.0&lt;/td&gt; 
    &lt;td&gt;72.6&lt;/td&gt; 
    &lt;td&gt;68.4&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;75.4&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;LiveCodeBench-Base (Pass@1)&lt;/td&gt; 
    &lt;td&gt;3-shot&lt;/td&gt; 
    &lt;td&gt;11.6&lt;/td&gt; 
    &lt;td&gt;12.9&lt;/td&gt; 
    &lt;td&gt;15.5&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;19.4&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;CRUXEval-I (Acc.)&lt;/td&gt; 
    &lt;td&gt;2-shot&lt;/td&gt; 
    &lt;td&gt;52.5&lt;/td&gt; 
    &lt;td&gt;59.1&lt;/td&gt; 
    &lt;td&gt;58.5&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;67.3&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;CRUXEval-O (Acc.)&lt;/td&gt; 
    &lt;td&gt;2-shot&lt;/td&gt; 
    &lt;td&gt;49.8&lt;/td&gt; 
    &lt;td&gt;59.9&lt;/td&gt; 
    &lt;td&gt;59.9&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;69.8&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Math&lt;/td&gt; 
    &lt;td&gt;GSM8K (EM)&lt;/td&gt; 
    &lt;td&gt;8-shot&lt;/td&gt; 
    &lt;td&gt;81.6&lt;/td&gt; 
    &lt;td&gt;88.3&lt;/td&gt; 
    &lt;td&gt;83.5&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;89.3&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;MATH (EM)&lt;/td&gt; 
    &lt;td&gt;4-shot&lt;/td&gt; 
    &lt;td&gt;43.4&lt;/td&gt; 
    &lt;td&gt;54.4&lt;/td&gt; 
    &lt;td&gt;49.0&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;61.6&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;MGSM (EM)&lt;/td&gt; 
    &lt;td&gt;8-shot&lt;/td&gt; 
    &lt;td&gt;63.6&lt;/td&gt; 
    &lt;td&gt;76.2&lt;/td&gt; 
    &lt;td&gt;69.9&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;79.8&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;CMath (EM)&lt;/td&gt; 
    &lt;td&gt;3-shot&lt;/td&gt; 
    &lt;td&gt;78.7&lt;/td&gt; 
    &lt;td&gt;84.5&lt;/td&gt; 
    &lt;td&gt;77.3&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;90.7&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Chinese&lt;/td&gt; 
    &lt;td&gt;CLUEWSC (EM)&lt;/td&gt; 
    &lt;td&gt;5-shot&lt;/td&gt; 
    &lt;td&gt;82.0&lt;/td&gt; 
    &lt;td&gt;82.5&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;83.0&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;82.7&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;C-Eval (Acc.)&lt;/td&gt; 
    &lt;td&gt;5-shot&lt;/td&gt; 
    &lt;td&gt;81.4&lt;/td&gt; 
    &lt;td&gt;89.2&lt;/td&gt; 
    &lt;td&gt;72.5&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;90.1&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;CMMLU (Acc.)&lt;/td&gt; 
    &lt;td&gt;5-shot&lt;/td&gt; 
    &lt;td&gt;84.0&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;89.5&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;73.7&lt;/td&gt; 
    &lt;td&gt;88.8&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;CMRC (EM)&lt;/td&gt; 
    &lt;td&gt;1-shot&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;77.4&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;75.8&lt;/td&gt; 
    &lt;td&gt;76.0&lt;/td&gt; 
    &lt;td&gt;76.3&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;C3 (Acc.)&lt;/td&gt; 
    &lt;td&gt;0-shot&lt;/td&gt; 
    &lt;td&gt;77.4&lt;/td&gt; 
    &lt;td&gt;76.7&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;79.7&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;78.6&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;CCPM (Acc.)&lt;/td&gt; 
    &lt;td&gt;0-shot&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;93.0&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;88.5&lt;/td&gt; 
    &lt;td&gt;78.6&lt;/td&gt; 
    &lt;td&gt;92.0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Multilingual&lt;/td&gt; 
    &lt;td&gt;MMMLU-non-English (Acc.)&lt;/td&gt; 
    &lt;td&gt;5-shot&lt;/td&gt; 
    &lt;td&gt;64.0&lt;/td&gt; 
    &lt;td&gt;74.8&lt;/td&gt; 
    &lt;td&gt;73.8&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;79.4&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Best results are shown in bold. Scores with a gap not exceeding 0.3 are considered to be at the same level. DeepSeek-V3 achieves the best performance on most benchmarks, especially on math and code tasks. For more evaluation details, please check our paper.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Context Window&lt;/h4&gt; 
&lt;p align="center"&gt; &lt;img width="80%" src="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/figures/niah.png" /&gt; &lt;/p&gt; 
&lt;p&gt;Evaluation results on the &lt;code&gt;Needle In A Haystack&lt;/code&gt; (NIAH) tests. DeepSeek-V3 performs well across all context window lengths up to &lt;strong&gt;128K&lt;/strong&gt;.&lt;/p&gt; 
&lt;h3&gt;Chat Model&lt;/h3&gt; 
&lt;h4&gt;Standard Benchmarks (Models larger than 67B)&lt;/h4&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;Benchmark (Metric)&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;DeepSeek V2-0506&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;DeepSeek V2.5-0905&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;Qwen2.5 72B-Inst.&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;Llama3.1 405B-Inst.&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;Claude-3.5-Sonnet-1022&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;GPT-4o 0513&lt;/strong&gt;&lt;/th&gt; 
    &lt;th&gt;&lt;strong&gt;DeepSeek V3&lt;/strong&gt;&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;Architecture&lt;/td&gt; 
    &lt;td&gt;MoE&lt;/td&gt; 
    &lt;td&gt;MoE&lt;/td&gt; 
    &lt;td&gt;Dense&lt;/td&gt; 
    &lt;td&gt;Dense&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;MoE&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;# Activated Params&lt;/td&gt; 
    &lt;td&gt;21B&lt;/td&gt; 
    &lt;td&gt;21B&lt;/td&gt; 
    &lt;td&gt;72B&lt;/td&gt; 
    &lt;td&gt;405B&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;37B&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;# Total Params&lt;/td&gt; 
    &lt;td&gt;236B&lt;/td&gt; 
    &lt;td&gt;236B&lt;/td&gt; 
    &lt;td&gt;72B&lt;/td&gt; 
    &lt;td&gt;405B&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;671B&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;English&lt;/td&gt; 
    &lt;td&gt;MMLU (EM)&lt;/td&gt; 
    &lt;td&gt;78.2&lt;/td&gt; 
    &lt;td&gt;80.6&lt;/td&gt; 
    &lt;td&gt;85.3&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;88.6&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;88.3&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;87.2&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;88.5&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;MMLU-Redux (EM)&lt;/td&gt; 
    &lt;td&gt;77.9&lt;/td&gt; 
    &lt;td&gt;80.3&lt;/td&gt; 
    &lt;td&gt;85.6&lt;/td&gt; 
    &lt;td&gt;86.2&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;88.9&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;88.0&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;89.1&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;MMLU-Pro (EM)&lt;/td&gt; 
    &lt;td&gt;58.5&lt;/td&gt; 
    &lt;td&gt;66.2&lt;/td&gt; 
    &lt;td&gt;71.6&lt;/td&gt; 
    &lt;td&gt;73.3&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;78.0&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;72.6&lt;/td&gt; 
    &lt;td&gt;75.9&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;DROP (3-shot F1)&lt;/td&gt; 
    &lt;td&gt;83.0&lt;/td&gt; 
    &lt;td&gt;87.8&lt;/td&gt; 
    &lt;td&gt;76.7&lt;/td&gt; 
    &lt;td&gt;88.7&lt;/td&gt; 
    &lt;td&gt;88.3&lt;/td&gt; 
    &lt;td&gt;83.7&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;91.6&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;IF-Eval (Prompt Strict)&lt;/td&gt; 
    &lt;td&gt;57.7&lt;/td&gt; 
    &lt;td&gt;80.6&lt;/td&gt; 
    &lt;td&gt;84.1&lt;/td&gt; 
    &lt;td&gt;86.0&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;86.5&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;84.3&lt;/td&gt; 
    &lt;td&gt;86.1&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;GPQA-Diamond (Pass@1)&lt;/td&gt; 
    &lt;td&gt;35.3&lt;/td&gt; 
    &lt;td&gt;41.3&lt;/td&gt; 
    &lt;td&gt;49.0&lt;/td&gt; 
    &lt;td&gt;51.1&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;65.0&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;49.9&lt;/td&gt; 
    &lt;td&gt;59.1&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;SimpleQA (Correct)&lt;/td&gt; 
    &lt;td&gt;9.0&lt;/td&gt; 
    &lt;td&gt;10.2&lt;/td&gt; 
    &lt;td&gt;9.1&lt;/td&gt; 
    &lt;td&gt;17.1&lt;/td&gt; 
    &lt;td&gt;28.4&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;38.2&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;24.9&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;FRAMES (Acc.)&lt;/td&gt; 
    &lt;td&gt;66.9&lt;/td&gt; 
    &lt;td&gt;65.4&lt;/td&gt; 
    &lt;td&gt;69.8&lt;/td&gt; 
    &lt;td&gt;70.0&lt;/td&gt; 
    &lt;td&gt;72.5&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;80.5&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;73.3&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;LongBench v2 (Acc.)&lt;/td&gt; 
    &lt;td&gt;31.6&lt;/td&gt; 
    &lt;td&gt;35.4&lt;/td&gt; 
    &lt;td&gt;39.4&lt;/td&gt; 
    &lt;td&gt;36.1&lt;/td&gt; 
    &lt;td&gt;41.0&lt;/td&gt; 
    &lt;td&gt;48.1&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;48.7&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Code&lt;/td&gt; 
    &lt;td&gt;HumanEval-Mul (Pass@1)&lt;/td&gt; 
    &lt;td&gt;69.3&lt;/td&gt; 
    &lt;td&gt;77.4&lt;/td&gt; 
    &lt;td&gt;77.3&lt;/td&gt; 
    &lt;td&gt;77.2&lt;/td&gt; 
    &lt;td&gt;81.7&lt;/td&gt; 
    &lt;td&gt;80.5&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;82.6&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;LiveCodeBench (Pass@1-COT)&lt;/td&gt; 
    &lt;td&gt;18.8&lt;/td&gt; 
    &lt;td&gt;29.2&lt;/td&gt; 
    &lt;td&gt;31.1&lt;/td&gt; 
    &lt;td&gt;28.4&lt;/td&gt; 
    &lt;td&gt;36.3&lt;/td&gt; 
    &lt;td&gt;33.4&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;40.5&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;LiveCodeBench (Pass@1)&lt;/td&gt; 
    &lt;td&gt;20.3&lt;/td&gt; 
    &lt;td&gt;28.4&lt;/td&gt; 
    &lt;td&gt;28.7&lt;/td&gt; 
    &lt;td&gt;30.1&lt;/td&gt; 
    &lt;td&gt;32.8&lt;/td&gt; 
    &lt;td&gt;34.2&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;37.6&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;Codeforces (Percentile)&lt;/td&gt; 
    &lt;td&gt;17.5&lt;/td&gt; 
    &lt;td&gt;35.6&lt;/td&gt; 
    &lt;td&gt;24.8&lt;/td&gt; 
    &lt;td&gt;25.3&lt;/td&gt; 
    &lt;td&gt;20.3&lt;/td&gt; 
    &lt;td&gt;23.6&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;51.6&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;SWE Verified (Resolved)&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;22.6&lt;/td&gt; 
    &lt;td&gt;23.8&lt;/td&gt; 
    &lt;td&gt;24.5&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;50.8&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;38.8&lt;/td&gt; 
    &lt;td&gt;42.0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;Aider-Edit (Acc.)&lt;/td&gt; 
    &lt;td&gt;60.3&lt;/td&gt; 
    &lt;td&gt;71.6&lt;/td&gt; 
    &lt;td&gt;65.4&lt;/td&gt; 
    &lt;td&gt;63.9&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;84.2&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;72.9&lt;/td&gt; 
    &lt;td&gt;79.7&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;Aider-Polyglot (Acc.)&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;18.2&lt;/td&gt; 
    &lt;td&gt;7.6&lt;/td&gt; 
    &lt;td&gt;5.8&lt;/td&gt; 
    &lt;td&gt;45.3&lt;/td&gt; 
    &lt;td&gt;16.0&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;49.6&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Math&lt;/td&gt; 
    &lt;td&gt;AIME 2024 (Pass@1)&lt;/td&gt; 
    &lt;td&gt;4.6&lt;/td&gt; 
    &lt;td&gt;16.7&lt;/td&gt; 
    &lt;td&gt;23.3&lt;/td&gt; 
    &lt;td&gt;23.3&lt;/td&gt; 
    &lt;td&gt;16.0&lt;/td&gt; 
    &lt;td&gt;9.3&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;39.2&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;MATH-500 (EM)&lt;/td&gt; 
    &lt;td&gt;56.3&lt;/td&gt; 
    &lt;td&gt;74.7&lt;/td&gt; 
    &lt;td&gt;80.0&lt;/td&gt; 
    &lt;td&gt;73.8&lt;/td&gt; 
    &lt;td&gt;78.3&lt;/td&gt; 
    &lt;td&gt;74.6&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;90.2&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;CNMO 2024 (Pass@1)&lt;/td&gt; 
    &lt;td&gt;2.8&lt;/td&gt; 
    &lt;td&gt;10.8&lt;/td&gt; 
    &lt;td&gt;15.9&lt;/td&gt; 
    &lt;td&gt;6.8&lt;/td&gt; 
    &lt;td&gt;13.1&lt;/td&gt; 
    &lt;td&gt;10.8&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;43.2&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Chinese&lt;/td&gt; 
    &lt;td&gt;CLUEWSC (EM)&lt;/td&gt; 
    &lt;td&gt;89.9&lt;/td&gt; 
    &lt;td&gt;90.4&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;91.4&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;84.7&lt;/td&gt; 
    &lt;td&gt;85.4&lt;/td&gt; 
    &lt;td&gt;87.9&lt;/td&gt; 
    &lt;td&gt;90.9&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;C-Eval (EM)&lt;/td&gt; 
    &lt;td&gt;78.6&lt;/td&gt; 
    &lt;td&gt;79.5&lt;/td&gt; 
    &lt;td&gt;86.1&lt;/td&gt; 
    &lt;td&gt;61.5&lt;/td&gt; 
    &lt;td&gt;76.7&lt;/td&gt; 
    &lt;td&gt;76.0&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;86.5&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;C-SimpleQA (Correct)&lt;/td&gt; 
    &lt;td&gt;48.5&lt;/td&gt; 
    &lt;td&gt;54.1&lt;/td&gt; 
    &lt;td&gt;48.4&lt;/td&gt; 
    &lt;td&gt;50.4&lt;/td&gt; 
    &lt;td&gt;51.3&lt;/td&gt; 
    &lt;td&gt;59.3&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;64.8&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] All models are evaluated in a configuration that limits the output length to 8K. Benchmarks containing fewer than 1000 samples are tested multiple times using varying temperature settings to derive robust final results. DeepSeek-V3 stands as the best-performing open-source model, and also exhibits competitive performance against frontier closed-source models.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Open Ended Generation Evaluation&lt;/h4&gt; 
&lt;div align="center"&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model&lt;/th&gt; 
    &lt;th&gt;Arena-Hard&lt;/th&gt; 
    &lt;th&gt;AlpacaEval 2.0&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;DeepSeek-V2.5-0905&lt;/td&gt; 
    &lt;td&gt;76.2&lt;/td&gt; 
    &lt;td&gt;50.5&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Qwen2.5-72B-Instruct&lt;/td&gt; 
    &lt;td&gt;81.2&lt;/td&gt; 
    &lt;td&gt;49.1&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;LLaMA-3.1 405B&lt;/td&gt; 
    &lt;td&gt;69.3&lt;/td&gt; 
    &lt;td&gt;40.5&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;GPT-4o-0513&lt;/td&gt; 
    &lt;td&gt;80.4&lt;/td&gt; 
    &lt;td&gt;51.1&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Claude-Sonnet-3.5-1022&lt;/td&gt; 
    &lt;td&gt;85.2&lt;/td&gt; 
    &lt;td&gt;52.0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;DeepSeek-V3&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;85.5&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;70.0&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] English open-ended conversation evaluations. For AlpacaEval 2.0, we use the length-controlled win rate as the metric.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;5. Chat Website &amp;amp; API Platform&lt;/h2&gt; 
&lt;p&gt;You can chat with DeepSeek-V3 on DeepSeek's official website: &lt;a href="https://chat.deepseek.com/sign_in"&gt;chat.deepseek.com&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;We also provide OpenAI-Compatible API at DeepSeek Platform: &lt;a href="https://platform.deepseek.com/"&gt;platform.deepseek.com&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;6. How to Run Locally&lt;/h2&gt; 
&lt;p&gt;DeepSeek-V3 can be deployed locally using the following hardware and open-source community software:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;DeepSeek-Infer Demo&lt;/strong&gt;: We provide a simple and lightweight demo for FP8 and BF16 inference.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;SGLang&lt;/strong&gt;: Fully support the DeepSeek-V3 model in both BF16 and FP8 inference modes, with Multi-Token Prediction &lt;a href="https://github.com/sgl-project/sglang/issues/2591"&gt;coming soon&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;LMDeploy&lt;/strong&gt;: Enables efficient FP8 and BF16 inference for local and cloud deployment.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;TensorRT-LLM&lt;/strong&gt;: Currently supports BF16 inference and INT4/8 quantization, with FP8 support coming soon.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;vLLM&lt;/strong&gt;: Support DeepSeek-V3 model with FP8 and BF16 modes for tensor parallelism and pipeline parallelism.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;LightLLM&lt;/strong&gt;: Supports efficient single-node or multi-node deployment for FP8 and BF16.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;AMD GPU&lt;/strong&gt;: Enables running the DeepSeek-V3 model on AMD GPUs via SGLang in both BF16 and FP8 modes.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Huawei Ascend NPU&lt;/strong&gt;: Supports running DeepSeek-V3 on Huawei Ascend devices in both INT8 and BF16.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Since FP8 training is natively adopted in our framework, we only provide FP8 weights. If you require BF16 weights for experimentation, you can use the provided conversion script to perform the transformation.&lt;/p&gt; 
&lt;p&gt;Here is an example of converting FP8 weights to BF16:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;cd inference
python fp8_cast_bf16.py --input-fp8-hf-path /path/to/fp8_weights --output-bf16-hf-path /path/to/bf16_weights
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Hugging Face's Transformers has not been directly supported yet.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;6.1 Inference with DeepSeek-Infer Demo (example only)&lt;/h3&gt; 
&lt;h4&gt;System Requirements&lt;/h4&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Linux with Python 3.10 only. Mac and Windows are not supported.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Dependencies:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-pip-requirements"&gt;torch==2.4.1
triton==3.0.0
transformers==4.46.3
safetensors==0.4.5
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Model Weights &amp;amp; Demo Code Preparation&lt;/h4&gt; 
&lt;p&gt;First, clone our DeepSeek-V3 GitHub repository:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;git clone https://github.com/deepseek-ai/DeepSeek-V3.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Navigate to the &lt;code&gt;inference&lt;/code&gt; folder and install dependencies listed in &lt;code&gt;requirements.txt&lt;/code&gt;. Easiest way is to use a package manager like &lt;code&gt;conda&lt;/code&gt; or &lt;code&gt;uv&lt;/code&gt; to create a new virtual environment and install the dependencies.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;cd DeepSeek-V3/inference
pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Download the model weights from Hugging Face, and put them into &lt;code&gt;/path/to/DeepSeek-V3&lt;/code&gt; folder.&lt;/p&gt; 
&lt;h4&gt;Model Weights Conversion&lt;/h4&gt; 
&lt;p&gt;Convert Hugging Face model weights to a specific format:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;python convert.py --hf-ckpt-path /path/to/DeepSeek-V3 --save-path /path/to/DeepSeek-V3-Demo --n-experts 256 --model-parallel 16
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Run&lt;/h4&gt; 
&lt;p&gt;Then you can chat with DeepSeek-V3:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;torchrun --nnodes 2 --nproc-per-node 8 --node-rank $RANK --master-addr $ADDR generate.py --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --interactive --temperature 0.7 --max-new-tokens 200
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or batch inference on a given file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;torchrun --nnodes 2 --nproc-per-node 8 --node-rank $RANK --master-addr $ADDR generate.py --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --input-file $FILE
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;6.2 Inference with SGLang (recommended)&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/sgl-project/sglang"&gt;SGLang&lt;/a&gt; currently supports &lt;a href="https://lmsys.org/blog/2024-09-04-sglang-v0-3/#deepseek-multi-head-latent-attention-mla-throughput-optimizations"&gt;MLA optimizations&lt;/a&gt;, &lt;a href="https://lmsys.org/blog/2024-12-04-sglang-v0-4/#data-parallelism-attention-for-deepseek-models"&gt;DP Attention&lt;/a&gt;, FP8 (W8A8), FP8 KV Cache, and Torch Compile, delivering state-of-the-art latency and throughput performance among open-source frameworks.&lt;/p&gt; 
&lt;p&gt;Notably, &lt;a href="https://github.com/sgl-project/sglang/releases/tag/v0.4.1"&gt;SGLang v0.4.1&lt;/a&gt; fully supports running DeepSeek-V3 on both &lt;strong&gt;NVIDIA and AMD GPUs&lt;/strong&gt;, making it a highly versatile and robust solution.&lt;/p&gt; 
&lt;p&gt;SGLang also supports &lt;a href="https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3#example-serving-with-2-h208"&gt;multi-node tensor parallelism&lt;/a&gt;, enabling you to run this model on multiple network-connected machines.&lt;/p&gt; 
&lt;p&gt;Multi-Token Prediction (MTP) is in development, and progress can be tracked in the &lt;a href="https://github.com/sgl-project/sglang/issues/2591"&gt;optimization plan&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Here are the launch instructions from the SGLang team: &lt;a href="https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3"&gt;https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;6.3 Inference with LMDeploy (recommended)&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/InternLM/lmdeploy"&gt;LMDeploy&lt;/a&gt;, a flexible and high-performance inference and serving framework tailored for large language models, now supports DeepSeek-V3. It offers both offline pipeline processing and online deployment capabilities, seamlessly integrating with PyTorch-based workflows.&lt;/p&gt; 
&lt;p&gt;For comprehensive step-by-step instructions on running DeepSeek-V3 with LMDeploy, please refer to here: &lt;a href="https://github.com/InternLM/lmdeploy/issues/2960"&gt;https://github.com/InternLM/lmdeploy/issues/2960&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;6.4 Inference with TRT-LLM (recommended)&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/NVIDIA/TensorRT-LLM"&gt;TensorRT-LLM&lt;/a&gt; now supports the DeepSeek-V3 model, offering precision options such as BF16 and INT4/INT8 weight-only. Support for FP8 is currently in progress and will be released soon. You can access the custom branch of TRTLLM specifically for DeepSeek-V3 support through the following link to experience the new features directly: &lt;a href="https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/deepseek_v3"&gt;https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/deepseek_v3&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;6.5 Inference with vLLM (recommended)&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/vllm-project/vllm"&gt;vLLM&lt;/a&gt; v0.6.6 supports DeepSeek-V3 inference for FP8 and BF16 modes on both NVIDIA and AMD GPUs. Aside from standard techniques, vLLM offers &lt;em&gt;pipeline parallelism&lt;/em&gt; allowing you to run this model on multiple machines connected by networks. For detailed guidance, please refer to the &lt;a href="https://docs.vllm.ai/en/latest/serving/distributed_serving.html"&gt;vLLM instructions&lt;/a&gt;. Please feel free to follow &lt;a href="https://github.com/vllm-project/vllm/issues/11539"&gt;the enhancement plan&lt;/a&gt; as well.&lt;/p&gt; 
&lt;h3&gt;6.6 Inference with LightLLM (recommended)&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://github.com/ModelTC/lightllm/tree/main"&gt;LightLLM&lt;/a&gt; v1.0.1 supports single-machine and multi-machine tensor parallel deployment for DeepSeek-R1 (FP8/BF16) and provides mixed-precision deployment, with more quantization modes continuously integrated. For more details, please refer to &lt;a href="https://lightllm-en.readthedocs.io/en/latest/getting_started/quickstart.html"&gt;LightLLM instructions&lt;/a&gt;. Additionally, LightLLM offers PD-disaggregation deployment for DeepSeek-V2, and the implementation of PD-disaggregation for DeepSeek-V3 is in development.&lt;/p&gt; 
&lt;h3&gt;6.7 Recommended Inference Functionality with AMD GPUs&lt;/h3&gt; 
&lt;p&gt;In collaboration with the AMD team, we have achieved Day-One support for AMD GPUs using SGLang, with full compatibility for both FP8 and BF16 precision. For detailed guidance, please refer to the &lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/#63-inference-with-lmdeploy-recommended"&gt;SGLang instructions&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;6.8 Recommended Inference Functionality with Huawei Ascend NPUs&lt;/h3&gt; 
&lt;p&gt;The &lt;a href="https://www.hiascend.com/en/software/mindie"&gt;MindIE&lt;/a&gt; framework from the Huawei Ascend community has successfully adapted the BF16 version of DeepSeek-V3. For step-by-step guidance on Ascend NPUs, please follow the &lt;a href="https://modelers.cn/models/MindIE/deepseekv3"&gt;instructions here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;7. License&lt;/h2&gt; 
&lt;p&gt;This code repository is licensed under &lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/LICENSE-CODE"&gt;the MIT License&lt;/a&gt;. The use of DeepSeek-V3 Base/Chat models is subject to &lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/LICENSE-MODEL"&gt;the Model License&lt;/a&gt;. DeepSeek-V3 series (including Base and Chat) supports commercial use.&lt;/p&gt; 
&lt;h2&gt;8. Citation&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;@misc{deepseekai2024deepseekv3technicalreport,
      title={DeepSeek-V3 Technical Report}, 
      author={DeepSeek-AI},
      year={2024},
      eprint={2412.19437},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.19437}, 
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;9. Contact&lt;/h2&gt; 
&lt;p&gt;If you have any questions, please raise an issue or contact us at &lt;a href="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-V3/main/service@deepseek.com"&gt;service@deepseek.com&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>strands-agents/sdk-python</title>
      <link>https://github.com/strands-agents/sdk-python</link>
      <description>&lt;p&gt;A model-driven approach to building AI agents in just a few lines of code.&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;div&gt; 
  &lt;a href="https://strandsagents.com"&gt; &lt;img src="https://strandsagents.com/latest/assets/logo-github.svg?sanitize=true" alt="Strands Agents" width="55px" height="105px" /&gt; &lt;/a&gt; 
 &lt;/div&gt; 
 &lt;h1&gt; Strands Agents &lt;/h1&gt; 
 &lt;h2&gt; A model-driven approach to building AI agents in just a few lines of code. &lt;/h2&gt; 
 &lt;div align="center"&gt; 
  &lt;a href="https://github.com/strands-agents/sdk-python/graphs/commit-activity"&gt;&lt;img alt="GitHub commit activity" src="https://img.shields.io/github/commit-activity/m/strands-agents/sdk-python" /&gt;&lt;/a&gt; 
  &lt;a href="https://github.com/strands-agents/sdk-python/issues"&gt;&lt;img alt="GitHub open issues" src="https://img.shields.io/github/issues/strands-agents/sdk-python" /&gt;&lt;/a&gt; 
  &lt;a href="https://github.com/strands-agents/sdk-python/pulls"&gt;&lt;img alt="GitHub open pull requests" src="https://img.shields.io/github/issues-pr/strands-agents/sdk-python" /&gt;&lt;/a&gt; 
  &lt;a href="https://github.com/strands-agents/sdk-python/raw/main/LICENSE"&gt;&lt;img alt="License" src="https://img.shields.io/github/license/strands-agents/sdk-python" /&gt;&lt;/a&gt; 
  &lt;a href="https://pypi.org/project/strands-agents/"&gt;&lt;img alt="PyPI version" src="https://img.shields.io/pypi/v/strands-agents" /&gt;&lt;/a&gt; 
  &lt;a href="https://python.org"&gt;&lt;img alt="Python versions" src="https://img.shields.io/pypi/pyversions/strands-agents" /&gt;&lt;/a&gt; 
 &lt;/div&gt; 
 &lt;p&gt; &lt;a href="https://strandsagents.com/"&gt;Documentation&lt;/a&gt; ‚óÜ &lt;a href="https://github.com/strands-agents/samples"&gt;Samples&lt;/a&gt; ‚óÜ &lt;a href="https://github.com/strands-agents/sdk-python"&gt;Python SDK&lt;/a&gt; ‚óÜ &lt;a href="https://github.com/strands-agents/tools"&gt;Tools&lt;/a&gt; ‚óÜ &lt;a href="https://github.com/strands-agents/agent-builder"&gt;Agent Builder&lt;/a&gt; ‚óÜ &lt;a href="https://github.com/strands-agents/mcp-server"&gt;MCP Server&lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;Strands Agents is a simple yet powerful SDK that takes a model-driven approach to building and running AI agents. From simple conversational assistants to complex autonomous workflows, from local development to production deployment, Strands Agents scales with your needs.&lt;/p&gt; 
&lt;h2&gt;Feature Overview&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Lightweight &amp;amp; Flexible&lt;/strong&gt;: Simple agent loop that just works and is fully customizable&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Model Agnostic&lt;/strong&gt;: Support for Amazon Bedrock, Anthropic, LiteLLM, Llama, Ollama, OpenAI, Writer, and custom providers&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Advanced Capabilities&lt;/strong&gt;: Multi-agent systems, autonomous agents, and streaming support&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Built-in MCP&lt;/strong&gt;: Native support for Model Context Protocol (MCP) servers, enabling access to thousands of pre-built tools&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Install Strands Agents
pip install strands-agents strands-agents-tools
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from strands import Agent
from strands_tools import calculator
agent = Agent(tools=[calculator])
agent("What is the square root of 1764")
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: For the default Amazon Bedrock model provider, you'll need AWS credentials configured and model access enabled for Claude 4 Sonnet in the us-west-2 region. See the &lt;a href="https://strandsagents.com/"&gt;Quickstart Guide&lt;/a&gt; for details on configuring other model providers.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;Ensure you have Python 3.10+ installed, then:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Create and activate virtual environment
python -m venv .venv
source .venv/bin/activate  # On Windows use: .venv\Scripts\activate

# Install Strands and tools
pip install strands-agents strands-agents-tools
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Features at a Glance&lt;/h2&gt; 
&lt;h3&gt;Python-Based Tools&lt;/h3&gt; 
&lt;p&gt;Easily build tools using Python decorators:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from strands import Agent, tool

@tool
def word_count(text: str) -&amp;gt; int:
    """Count words in text.

    This docstring is used by the LLM to understand the tool's purpose.
    """
    return len(text.split())

agent = Agent(tools=[word_count])
response = agent("How many words are in this sentence?")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Hot Reloading from Directory:&lt;/strong&gt; Enable automatic tool loading and reloading from the &lt;code&gt;./tools/&lt;/code&gt; directory:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from strands import Agent

# Agent will watch ./tools/ directory for changes
agent = Agent(load_tools_from_directory=True)
response = agent("Use any tools you find in the tools directory")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;MCP Support&lt;/h3&gt; 
&lt;p&gt;Seamlessly integrate Model Context Protocol (MCP) servers:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from strands import Agent
from strands.tools.mcp import MCPClient
from mcp import stdio_client, StdioServerParameters

aws_docs_client = MCPClient(
    lambda: stdio_client(StdioServerParameters(command="uvx", args=["awslabs.aws-documentation-mcp-server@latest"]))
)

with aws_docs_client:
   agent = Agent(tools=aws_docs_client.list_tools_sync())
   response = agent("Tell me about Amazon Bedrock and how to use it with Python")
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Multiple Model Providers&lt;/h3&gt; 
&lt;p&gt;Support for various model providers:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from strands import Agent
from strands.models import BedrockModel
from strands.models.ollama import OllamaModel
from strands.models.llamaapi import LlamaAPIModel

# Bedrock
bedrock_model = BedrockModel(
  model_id="us.amazon.nova-pro-v1:0",
  temperature=0.3,
  streaming=True, # Enable/disable streaming
)
agent = Agent(model=bedrock_model)
agent("Tell me about Agentic AI")

# Ollama
ollama_model = OllamaModel(
  host="http://localhost:11434",
  model_id="llama3"
)
agent = Agent(model=ollama_model)
agent("Tell me about Agentic AI")

# Llama API
llama_model = LlamaAPIModel(
    model_id="Llama-4-Maverick-17B-128E-Instruct-FP8",
)
agent = Agent(model=llama_model)
response = agent("Tell me about Agentic AI")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Built-in providers:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/latest/user-guide/concepts/model-providers/amazon-bedrock/"&gt;Amazon Bedrock&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/latest/user-guide/concepts/model-providers/anthropic/"&gt;Anthropic&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/latest/user-guide/concepts/model-providers/litellm/"&gt;LiteLLM&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/latest/user-guide/concepts/model-providers/llamaapi/"&gt;LlamaAPI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/latest/user-guide/concepts/model-providers/ollama/"&gt;Ollama&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/latest/user-guide/concepts/model-providers/openai/"&gt;OpenAI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/latest/documentation/docs/user-guide/concepts/model-providers/writer/"&gt;Writer&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Custom providers can be implemented using &lt;a href="https://strandsagents.com/latest/user-guide/concepts/model-providers/custom_model_provider/"&gt;Custom Providers&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Example tools&lt;/h3&gt; 
&lt;p&gt;Strands offers an optional strands-agents-tools package with pre-built tools for quick experimentation:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from strands import Agent
from strands_tools import calculator
agent = Agent(tools=[calculator])
agent("What is the square root of 1764")
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;It's also available on GitHub via &lt;a href="https://github.com/strands-agents/tools"&gt;strands-agents/tools&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;For detailed guidance &amp;amp; examples, explore our documentation:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/"&gt;User Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/latest/user-guide/quickstart/"&gt;Quick Start Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/latest/user-guide/concepts/agents/agent-loop/"&gt;Agent Loop&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/latest/examples/"&gt;Examples&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/latest/api-reference/agent/"&gt;API Reference&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/latest/user-guide/deploy/operating-agents-in-production/"&gt;Production &amp;amp; Deployment Guide&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing ‚ù§Ô∏è&lt;/h2&gt; 
&lt;p&gt;We welcome contributions! See our &lt;a href="https://raw.githubusercontent.com/strands-agents/sdk-python/main/CONTRIBUTING.md"&gt;Contributing Guide&lt;/a&gt; for details on:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Reporting bugs &amp;amp; features&lt;/li&gt; 
 &lt;li&gt;Development setup&lt;/li&gt; 
 &lt;li&gt;Contributing via Pull Requests&lt;/li&gt; 
 &lt;li&gt;Code of Conduct&lt;/li&gt; 
 &lt;li&gt;Reporting of security issues&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the Apache License 2.0 - see the &lt;a href="https://raw.githubusercontent.com/strands-agents/sdk-python/main/LICENSE"&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;h2&gt;Security&lt;/h2&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/strands-agents/sdk-python/main/CONTRIBUTING.md#security-issue-notifications"&gt;CONTRIBUTING&lt;/a&gt; for more information.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ansible/ansible</title>
      <link>https://github.com/ansible/ansible</link>
      <description>&lt;p&gt;Ansible is a radically simple IT automation platform that makes your applications and systems easier to deploy and maintain. Automate everything from code deployment to network configuration to cloud management, in a language that approaches plain English, using SSH, with no agents to install on remote systems. https://docs.ansible.com.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href="https://pypi.org/project/ansible-core"&gt;&lt;img src="https://img.shields.io/pypi/v/ansible-core.svg?sanitize=true" alt="PyPI version" /&gt;&lt;/a&gt; &lt;a href="https://docs.ansible.com/ansible/latest/"&gt;&lt;img src="https://img.shields.io/badge/docs-latest-brightgreen.svg?sanitize=true" alt="Docs badge" /&gt;&lt;/a&gt; &lt;a href="https://docs.ansible.com/ansible/devel/community/communication.html"&gt;&lt;img src="https://img.shields.io/badge/chat-IRC-brightgreen.svg?sanitize=true" alt="Chat badge" /&gt;&lt;/a&gt; &lt;a href="https://dev.azure.com/ansible/ansible/_build/latest?definitionId=20&amp;amp;branchName=devel"&gt;&lt;img src="https://dev.azure.com/ansible/ansible/_apis/build/status/CI?branchName=devel" alt="Build Status" /&gt;&lt;/a&gt; &lt;a href="https://docs.ansible.com/ansible/devel/community/code_of_conduct.html"&gt;&lt;img src="https://img.shields.io/badge/code%20of%20conduct-Ansible-silver.svg?sanitize=true" alt="Ansible Code of Conduct" /&gt;&lt;/a&gt; &lt;a href="https://docs.ansible.com/ansible/devel/community/communication.html#mailing-list-information"&gt;&lt;img src="https://img.shields.io/badge/mailing%20lists-Ansible-orange.svg?sanitize=true" alt="Ansible mailing lists" /&gt;&lt;/a&gt; &lt;a href="https://raw.githubusercontent.com/ansible/ansible/devel/COPYING"&gt;&lt;img src="https://img.shields.io/badge/license-GPL%20v3.0-brightgreen.svg?sanitize=true" alt="Repository License" /&gt;&lt;/a&gt; &lt;a href="https://bestpractices.coreinfrastructure.org/projects/2372"&gt;&lt;img src="https://bestpractices.coreinfrastructure.org/projects/2372/badge" alt="Ansible CII Best Practices certification" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Ansible&lt;/h1&gt; 
&lt;p&gt;Ansible is a radically simple IT automation system. It handles configuration management, application deployment, cloud provisioning, ad-hoc task execution, network automation, and multi-node orchestration. Ansible makes complex changes like zero-downtime rolling updates with load balancers easy. More information on the Ansible &lt;a href="https://ansible.com/"&gt;website&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Design Principles&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Have an extremely simple setup process with a minimal learning curve.&lt;/li&gt; 
 &lt;li&gt;Manage machines quickly and in parallel.&lt;/li&gt; 
 &lt;li&gt;Avoid custom-agents and additional open ports, be agentless by leveraging the existing SSH daemon.&lt;/li&gt; 
 &lt;li&gt;Describe infrastructure in a language that is both machine and human friendly.&lt;/li&gt; 
 &lt;li&gt;Focus on security and easy auditability/review/rewriting of content.&lt;/li&gt; 
 &lt;li&gt;Manage new remote machines instantly, without bootstrapping any software.&lt;/li&gt; 
 &lt;li&gt;Allow module development in any dynamic language, not just Python.&lt;/li&gt; 
 &lt;li&gt;Be usable as non-root.&lt;/li&gt; 
 &lt;li&gt;Be the easiest IT automation system to use, ever.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Use Ansible&lt;/h2&gt; 
&lt;p&gt;You can install a released version of Ansible with &lt;code&gt;pip&lt;/code&gt; or a package manager. See our &lt;a href="https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html"&gt;installation guide&lt;/a&gt; for details on installing Ansible on a variety of platforms.&lt;/p&gt; 
&lt;p&gt;Power users and developers can run the &lt;code&gt;devel&lt;/code&gt; branch, which has the latest features and fixes, directly. Although it is reasonably stable, you are more likely to encounter breaking changes when running the &lt;code&gt;devel&lt;/code&gt; branch. We recommend getting involved in the Ansible community if you want to run the &lt;code&gt;devel&lt;/code&gt; branch.&lt;/p&gt; 
&lt;h2&gt;Communication&lt;/h2&gt; 
&lt;p&gt;Join the Ansible forum to ask questions, get help, and interact with the community.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://forum.ansible.com/c/help/6"&gt;Get Help&lt;/a&gt;: Find help or share your Ansible knowledge to help others. Use tags to filter and subscribe to posts, such as the following: 
  &lt;ul&gt; 
   &lt;li&gt;Posts tagged with &lt;a href="https://forum.ansible.com/tag/ansible"&gt;ansible&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Posts tagged with &lt;a href="https://forum.ansible.com/tag/ansible-core"&gt;ansible-core&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Posts tagged with &lt;a href="https://forum.ansible.com/tag/playbook"&gt;playbook&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://forum.ansible.com/c/chat/4"&gt;Social Spaces&lt;/a&gt;: Meet and interact with fellow enthusiasts.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://forum.ansible.com/c/news/5"&gt;News &amp;amp; Announcements&lt;/a&gt;: Track project-wide announcements including social events.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.ansible.com/ansible/devel/community/communication.html#the-bullhorn"&gt;Bullhorn newsletter&lt;/a&gt;: Get release announcements and important changes.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For more ways to get in touch, see &lt;a href="https://docs.ansible.com/ansible/devel/community/communication.html"&gt;Communicating with the Ansible community&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contribute to Ansible&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Check out the &lt;a href="https://raw.githubusercontent.com/ansible/ansible/devel/.github/CONTRIBUTING.md"&gt;Contributor's Guide&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Read &lt;a href="https://docs.ansible.com/ansible/devel/community"&gt;Community Information&lt;/a&gt; for all kinds of ways to contribute to and interact with the project, including how to submit bug reports and code to Ansible.&lt;/li&gt; 
 &lt;li&gt;Submit a proposed code update through a pull request to the &lt;code&gt;devel&lt;/code&gt; branch.&lt;/li&gt; 
 &lt;li&gt;Talk to us before making larger changes to avoid duplicate efforts. This not only helps everyone know what is going on, but it also helps save time and effort if we decide some changes are needed.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Coding Guidelines&lt;/h2&gt; 
&lt;p&gt;We document our Coding Guidelines in the &lt;a href="https://docs.ansible.com/ansible/devel/dev_guide/"&gt;Developer Guide&lt;/a&gt;. We particularly suggest you review:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.ansible.com/ansible/devel/dev_guide/developing_modules_checklist.html"&gt;Contributing your module to Ansible&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.ansible.com/ansible/devel/dev_guide/developing_modules_best_practices.html"&gt;Conventions, tips, and pitfalls&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Branch Info&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;The &lt;code&gt;devel&lt;/code&gt; branch corresponds to the release actively under development.&lt;/li&gt; 
 &lt;li&gt;The &lt;code&gt;stable-2.X&lt;/code&gt; branches correspond to stable releases.&lt;/li&gt; 
 &lt;li&gt;Create a branch based on &lt;code&gt;devel&lt;/code&gt; and set up a &lt;a href="https://docs.ansible.com/ansible/devel/dev_guide/developing_modules_general.html#common-environment-setup"&gt;dev environment&lt;/a&gt; if you want to open a PR.&lt;/li&gt; 
 &lt;li&gt;See the &lt;a href="https://docs.ansible.com/ansible/devel/reference_appendices/release_and_maintenance.html"&gt;Ansible release and maintenance&lt;/a&gt; page for information about active branches.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Roadmap&lt;/h2&gt; 
&lt;p&gt;Based on team and community feedback, an initial roadmap will be published for a major or minor version (ex: 2.7, 2.8). The &lt;a href="https://docs.ansible.com/ansible/devel/roadmap/"&gt;Ansible Roadmap page&lt;/a&gt; details what is planned and how to influence the roadmap.&lt;/p&gt; 
&lt;h2&gt;Authors&lt;/h2&gt; 
&lt;p&gt;Ansible was created by &lt;a href="https://github.com/mpdehaan"&gt;Michael DeHaan&lt;/a&gt; and has contributions from over 5000 users (and growing). Thanks everyone!&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.ansible.com"&gt;Ansible&lt;/a&gt; is sponsored by &lt;a href="https://www.redhat.com"&gt;Red Hat, Inc.&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;GNU General Public License v3.0 or later&lt;/p&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/ansible/ansible/devel/COPYING"&gt;COPYING&lt;/a&gt; to see the full text.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>livekit/agents</title>
      <link>https://github.com/livekit/agents</link>
      <description>&lt;p&gt;A powerful framework for building realtime voice AI agents ü§ñüéôÔ∏èüìπ&lt;/p&gt;&lt;hr&gt;&lt;picture&gt; 
 &lt;source media="(prefers-color-scheme: dark)" srcset="/.github/banner_dark.png" /&gt; 
 &lt;source media="(prefers-color-scheme: light)" srcset="/.github/banner_light.png" /&gt; 
 &lt;img style="width:100%;" alt="The LiveKit icon, the name of the repository and some sample code in the background." src="https://raw.githubusercontent.com/livekit/agents/main/.github/banner_light.png" /&gt; 
&lt;/picture&gt; 
&lt;!--END_BANNER_IMAGE--&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;img src="https://img.shields.io/pypi/v/livekit-agents" alt="PyPI - Version" /&gt; &lt;a href="https://pepy.tech/projects/livekit-agents"&gt;&lt;img src="https://static.pepy.tech/badge/livekit-agents/month" alt="PyPI Downloads" /&gt;&lt;/a&gt; &lt;a href="https://livekit.io/join-slack"&gt;&lt;img src="https://img.shields.io/endpoint?url=https%3A%2F%2Flivekit.io%2Fbadges%2Fslack" alt="Slack community" /&gt;&lt;/a&gt; &lt;a href="https://twitter.com/livekit"&gt;&lt;img src="https://img.shields.io/twitter/follow/livekit" alt="Twitter Follow" /&gt;&lt;/a&gt; &lt;a href="https://deepwiki.com/livekit/agents"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki for understanding the codebase" /&gt;&lt;/a&gt; &lt;a href="https://github.com/livekit/livekit/raw/master/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/livekit/livekit" alt="License" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;p&gt;Looking for the JS/TS library? Check out &lt;a href="https://github.com/livekit/agents-js"&gt;AgentsJS&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;What is Agents?&lt;/h2&gt; 
&lt;!--BEGIN_DESCRIPTION--&gt; 
&lt;p&gt;The Agent Framework is designed for building realtime, programmable participants that run on servers. Use it to create conversational, multi-modal voice agents that can see, hear, and understand.&lt;/p&gt; 
&lt;!--END_DESCRIPTION--&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Flexible integrations&lt;/strong&gt;: A comprehensive ecosystem to mix and match the right STT, LLM, TTS, and Realtime API to suit your use case.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Integrated job scheduling&lt;/strong&gt;: Built-in task scheduling and distribution with &lt;a href="https://docs.livekit.io/agents/build/dispatch/"&gt;dispatch APIs&lt;/a&gt; to connect end users to agents.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Extensive WebRTC clients&lt;/strong&gt;: Build client applications using LiveKit's open-source SDK ecosystem, supporting all major platforms.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Telephony integration&lt;/strong&gt;: Works seamlessly with LiveKit's &lt;a href="https://docs.livekit.io/sip/"&gt;telephony stack&lt;/a&gt;, allowing your agent to make calls to or receive calls from phones.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Exchange data with clients&lt;/strong&gt;: Use &lt;a href="https://docs.livekit.io/home/client/data/rpc/"&gt;RPCs&lt;/a&gt; and other &lt;a href="https://docs.livekit.io/home/client/data/"&gt;Data APIs&lt;/a&gt; to seamlessly exchange data with clients.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Semantic turn detection&lt;/strong&gt;: Uses a transformer model to detect when a user is done with their turn, helps to reduce interruptions.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MCP support&lt;/strong&gt;: Native support for MCP. Integrate tools provided by MCP servers with one loc.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Builtin test framework&lt;/strong&gt;: Write tests and use judges to ensure your agent is performing as expected.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Open-source&lt;/strong&gt;: Fully open-source, allowing you to run the entire stack on your own servers, including &lt;a href="https://github.com/livekit/livekit"&gt;LiveKit server&lt;/a&gt;, one of the most widely used WebRTC media servers.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;To install the core Agents library, along with plugins for popular model providers:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install "livekit-agents[openai,silero,deepgram,cartesia,turn-detector]~=1.0"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Docs and guides&lt;/h2&gt; 
&lt;p&gt;Documentation on the framework and how to use it can be found &lt;a href="https://docs.livekit.io/agents/"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Core concepts&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Agent: An LLM-based application with defined instructions.&lt;/li&gt; 
 &lt;li&gt;AgentSession: A container for agents that manages interactions with end users.&lt;/li&gt; 
 &lt;li&gt;entrypoint: The starting point for an interactive session, similar to a request handler in a web server.&lt;/li&gt; 
 &lt;li&gt;Worker: The main process that coordinates job scheduling and launches agents for user sessions.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;h3&gt;Simple voice agent&lt;/h3&gt; 
&lt;hr /&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from livekit.agents import (
    Agent,
    AgentSession,
    JobContext,
    RunContext,
    WorkerOptions,
    cli,
    function_tool,
)
from livekit.plugins import deepgram, elevenlabs, openai, silero

@function_tool
async def lookup_weather(
    context: RunContext,
    location: str,
):
    """Used to look up weather information."""

    return {"weather": "sunny", "temperature": 70}


async def entrypoint(ctx: JobContext):
    await ctx.connect()

    agent = Agent(
        instructions="You are a friendly voice assistant built by LiveKit.",
        tools=[lookup_weather],
    )
    session = AgentSession(
        vad=silero.VAD.load(),
        # any combination of STT, LLM, TTS, or realtime API can be used
        stt=deepgram.STT(model="nova-3"),
        llm=openai.LLM(model="gpt-4o-mini"),
        tts=elevenlabs.TTS(),
    )

    await session.start(agent=agent, room=ctx.room)
    await session.generate_reply(instructions="greet the user and ask about their day")


if __name__ == "__main__":
    cli.run_app(WorkerOptions(entrypoint_fnc=entrypoint))
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You'll need the following environment variables for this example:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;DEEPGRAM_API_KEY&lt;/li&gt; 
 &lt;li&gt;OPENAI_API_KEY&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Multi-agent handoff&lt;/h3&gt; 
&lt;hr /&gt; 
&lt;p&gt;This code snippet is abbreviated. For the full example, see &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/voice_agents/multi_agent.py"&gt;multi_agent.py&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;...
class IntroAgent(Agent):
    def __init__(self) -&amp;gt; None:
        super().__init__(
            instructions=f"You are a story teller. Your goal is to gather a few pieces of information from the user to make the story personalized and engaging."
            "Ask the user for their name and where they are from"
        )

    async def on_enter(self):
        self.session.generate_reply(instructions="greet the user and gather information")

    @function_tool
    async def information_gathered(
        self,
        context: RunContext,
        name: str,
        location: str,
    ):
        """Called when the user has provided the information needed to make the story personalized and engaging.

        Args:
            name: The name of the user
            location: The location of the user
        """

        context.userdata.name = name
        context.userdata.location = location

        story_agent = StoryAgent(name, location)
        return story_agent, "Let's start the story!"


class StoryAgent(Agent):
    def __init__(self, name: str, location: str) -&amp;gt; None:
        super().__init__(
            instructions=f"You are a storyteller. Use the user's information in order to make the story personalized."
            f"The user's name is {name}, from {location}"
            # override the default model, switching to Realtime API from standard LLMs
            llm=openai.realtime.RealtimeModel(voice="echo"),
            chat_ctx=chat_ctx,
        )

    async def on_enter(self):
        self.session.generate_reply()


async def entrypoint(ctx: JobContext):
    await ctx.connect()

    userdata = StoryData()
    session = AgentSession[StoryData](
        vad=silero.VAD.load(),
        stt=deepgram.STT(model="nova-3"),
        llm=openai.LLM(model="gpt-4o-mini"),
        tts=openai.TTS(voice="echo"),
        userdata=userdata,
    )

    await session.start(
        agent=IntroAgent(),
        room=ctx.room,
    )
...
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Testing&lt;/h3&gt; 
&lt;p&gt;Automated tests are essential for building reliable agents, especially with the non-deterministic behavior of LLMs. LiveKit Agents include native test integration to help you create dependable agents.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;@pytest.mark.asyncio
async def test_no_availability() -&amp;gt; None:
    llm = google.LLM()
    async AgentSession(llm=llm) as sess:
        await sess.start(MyAgent())
        result = await sess.run(
            user_input="Hello, I need to place an order."
        )
        result.expect.skip_next_event_if(type="message", role="assistant")
        result.expect.next_event().is_function_call(name="start_order")
        result.expect.next_event().is_function_call_output()
        await (
            result.expect.next_event()
            .is_message(role="assistant")
            .judge(llm, intent="assistant should be asking the user what they would like")
        )

&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;üéôÔ∏è Starter Agent&lt;/h3&gt; &lt;p&gt;A starter agent optimized for voice conversations.&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/voice_agents/basic_agent.py"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;üîÑ Multi-user push to talk&lt;/h3&gt; &lt;p&gt;Responds to multiple users in the room via push-to-talk.&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/voice_agents/push_to_talk.py"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;üéµ Background audio&lt;/h3&gt; &lt;p&gt;Background ambient and thinking audio to improve realism.&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/voice_agents/background_audio.py"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;üõ†Ô∏è Dynamic tool creation&lt;/h3&gt; &lt;p&gt;Creating function tools dynamically.&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/voice_agents/dynamic_tool_creation.py"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;‚òéÔ∏è Outbound caller&lt;/h3&gt; &lt;p&gt;Agent that makes outbound phone calls&lt;/p&gt; &lt;p&gt; &lt;a href="https://github.com/livekit-examples/outbound-caller-python"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;üìã Structured output&lt;/h3&gt; &lt;p&gt;Using structured output from LLM to guide TTS tone.&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/voice_agents/structured_output.py"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;üîå MCP support&lt;/h3&gt; &lt;p&gt;Use tools from MCP servers&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/voice_agents/mcp"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;üí¨ Text-only agent&lt;/h3&gt; &lt;p&gt;Skip voice altogether and use the same code for text-only integrations&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/other/text_only.py"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;üìù Multi-user transcriber&lt;/h3&gt; &lt;p&gt;Produce transcriptions from all users in the room&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/other/transcription/multi-user-transcriber.py"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;üé• Video avatars&lt;/h3&gt; &lt;p&gt;Add an AI avatar with Tavus, Beyond Presence, and Bithuman&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/avatar_agents/"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;üçΩÔ∏è Restaurant ordering and reservations&lt;/h3&gt; &lt;p&gt;Full example of an agent that handles calls for a restaurant.&lt;/p&gt; &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/livekit/agents/main/examples/voice_agents/restaurant_agent.py"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
   &lt;td width="50%"&gt; &lt;h3&gt;üëÅÔ∏è Gemini Live vision&lt;/h3&gt; &lt;p&gt;Full example (including iOS app) of Gemini Live agent that can see.&lt;/p&gt; &lt;p&gt; &lt;a href="https://github.com/livekit-examples/vision-demo"&gt;Code&lt;/a&gt; &lt;/p&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;Running your agent&lt;/h2&gt; 
&lt;h3&gt;Testing in terminal&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;python myagent.py console
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Runs your agent in terminal mode, enabling local audio input and output for testing. This mode doesn't require external servers or dependencies and is useful for quickly validating behavior.&lt;/p&gt; 
&lt;h3&gt;Developing with LiveKit clients&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;python myagent.py dev
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Starts the agent server and enables hot reloading when files change. This mode allows each process to host multiple concurrent agents efficiently.&lt;/p&gt; 
&lt;p&gt;The agent connects to LiveKit Cloud or your self-hosted server. Set the following environment variables:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;LIVEKIT_URL&lt;/li&gt; 
 &lt;li&gt;LIVEKIT_API_KEY&lt;/li&gt; 
 &lt;li&gt;LIVEKIT_API_SECRET&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can connect using any LiveKit client SDK or telephony integration. To get started quickly, try the &lt;a href="https://agents-playground.livekit.io/"&gt;Agents Playground&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Running for production&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;python myagent.py start
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Runs the agent with production-ready optimizations.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;The Agents framework is under active development in a rapidly evolving field. We welcome and appreciate contributions of any kind, be it feedback, bugfixes, features, new plugins and tools, or better documentation. You can file issues under this repo, open a PR, or chat with us in LiveKit's &lt;a href="https://livekit.io/join-slack"&gt;Slack community&lt;/a&gt;.&lt;/p&gt; 
&lt;!--BEGIN_REPO_NAV--&gt; 
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;table&gt; 
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th colspan="2"&gt;LiveKit Ecosystem&lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt;
   &lt;td&gt;LiveKit SDKs&lt;/td&gt;
   &lt;td&gt;&lt;a href="https://github.com/livekit/client-sdk-js"&gt;Browser&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/client-sdk-swift"&gt;iOS/macOS/visionOS&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/client-sdk-android"&gt;Android&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/client-sdk-flutter"&gt;Flutter&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/client-sdk-react-native"&gt;React Native&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/rust-sdks"&gt;Rust&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/node-sdks"&gt;Node.js&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/python-sdks"&gt;Python&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/client-sdk-unity"&gt;Unity&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/client-sdk-unity-web"&gt;Unity (WebGL)&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/client-sdk-esp32"&gt;ESP32&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;&lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;Server APIs&lt;/td&gt;
   &lt;td&gt;&lt;a href="https://github.com/livekit/node-sdks"&gt;Node.js&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/server-sdk-go"&gt;Golang&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/server-sdk-ruby"&gt;Ruby&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/server-sdk-kotlin"&gt;Java/Kotlin&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/python-sdks"&gt;Python&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/rust-sdks"&gt;Rust&lt;/a&gt; ¬∑ &lt;a href="https://github.com/agence104/livekit-server-sdk-php"&gt;PHP (community)&lt;/a&gt; ¬∑ &lt;a href="https://github.com/pabloFuente/livekit-server-sdk-dotnet"&gt;.NET (community)&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;&lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;UI Components&lt;/td&gt;
   &lt;td&gt;&lt;a href="https://github.com/livekit/components-js"&gt;React&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/components-android"&gt;Android Compose&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/components-swift"&gt;SwiftUI&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/components-flutter"&gt;Flutter&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;&lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;Agents Frameworks&lt;/td&gt;
   &lt;td&gt;&lt;b&gt;Python&lt;/b&gt; ¬∑ &lt;a href="https://github.com/livekit/agents-js"&gt;Node.js&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/agent-playground"&gt;Playground&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;&lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;Services&lt;/td&gt;
   &lt;td&gt;&lt;a href="https://github.com/livekit/livekit"&gt;LiveKit server&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/egress"&gt;Egress&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/ingress"&gt;Ingress&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/sip"&gt;SIP&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;&lt;/tr&gt; 
  &lt;tr&gt;
   &lt;td&gt;Resources&lt;/td&gt;
   &lt;td&gt;&lt;a href="https://docs.livekit.io"&gt;Docs&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit-examples"&gt;Example apps&lt;/a&gt; ¬∑ &lt;a href="https://livekit.io/cloud"&gt;Cloud&lt;/a&gt; ¬∑ &lt;a href="https://docs.livekit.io/home/self-hosting/deployment"&gt;Self-hosting&lt;/a&gt; ¬∑ &lt;a href="https://github.com/livekit/livekit-cli"&gt;CLI&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;!--END_REPO_NAV--&gt;</description>
    </item>
    
    <item>
      <title>huggingface/transformers</title>
      <link>https://github.com/huggingface/transformers</link>
      <description>&lt;p&gt;ü§ó Transformers: the model-definition framework for state-of-the-art machine learning models in text, vision, audio, and multimodal models, for both inference and training.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; 
 &lt;picture&gt; 
  &lt;source media="(prefers-color-scheme: dark)" srcset="https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-dark.svg" /&gt; 
  &lt;source media="(prefers-color-scheme: light)" srcset="https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg" /&gt; 
  &lt;img alt="Hugging Face Transformers Library" src="https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg?sanitize=true" width="352" height="59" style="max-width: 100%;" /&gt; 
 &lt;/picture&gt; &lt;br /&gt; &lt;br /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://huggingface.com/models"&gt;&lt;img alt="Checkpoints on Hub" src="https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/models&amp;amp;color=brightgreen" /&gt;&lt;/a&gt; &lt;a href="https://circleci.com/gh/huggingface/transformers"&gt;&lt;img alt="Build" src="https://img.shields.io/circleci/build/github/huggingface/transformers/main" /&gt;&lt;/a&gt; &lt;a href="https://github.com/huggingface/transformers/raw/main/LICENSE"&gt;&lt;img alt="GitHub" src="https://img.shields.io/github/license/huggingface/transformers.svg?color=blue" /&gt;&lt;/a&gt; &lt;a href="https://huggingface.co/docs/transformers/index"&gt;&lt;img alt="Documentation" src="https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&amp;amp;down_message=offline&amp;amp;up_message=online" /&gt;&lt;/a&gt; &lt;a href="https://github.com/huggingface/transformers/releases"&gt;&lt;img alt="GitHub release" src="https://img.shields.io/github/release/huggingface/transformers.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://github.com/huggingface/transformers/raw/main/CODE_OF_CONDUCT.md"&gt;&lt;img alt="Contributor Covenant" src="https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg?sanitize=true" /&gt;&lt;/a&gt; &lt;a href="https://zenodo.org/badge/latestdoi/155220641"&gt;&lt;img src="https://zenodo.org/badge/155220641.svg?sanitize=true" alt="DOI" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;h4 align="center"&gt; &lt;p&gt; &lt;b&gt;English&lt;/b&gt; | &lt;a href="https://github.com/huggingface/transformers/raw/main/i18n/README_zh-hans.md"&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt; | &lt;a href="https://github.com/huggingface/transformers/raw/main/i18n/README_zh-hant.md"&gt;ÁπÅÈ´î‰∏≠Êñá&lt;/a&gt; | &lt;a href="https://github.com/huggingface/transformers/raw/main/i18n/README_ko.md"&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt; | &lt;a href="https://github.com/huggingface/transformers/raw/main/i18n/README_es.md"&gt;Espa√±ol&lt;/a&gt; | &lt;a href="https://github.com/huggingface/transformers/raw/main/i18n/README_ja.md"&gt;Êó•Êú¨Ë™û&lt;/a&gt; | &lt;a href="https://github.com/huggingface/transformers/raw/main/i18n/README_hd.md"&gt;‡§π‡§ø‡§®‡•ç‡§¶‡•Ä&lt;/a&gt; | &lt;a href="https://github.com/huggingface/transformers/raw/main/i18n/README_ru.md"&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt; | &lt;a href="https://github.com/huggingface/transformers/raw/main/i18n/README_pt-br.md"&gt;Portugu√™s&lt;/a&gt; | &lt;a href="https://github.com/huggingface/transformers/raw/main/i18n/README_te.md"&gt;‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å&lt;/a&gt; | &lt;a href="https://github.com/huggingface/transformers/raw/main/i18n/README_fr.md"&gt;Fran√ßais&lt;/a&gt; | &lt;a href="https://github.com/huggingface/transformers/raw/main/i18n/README_de.md"&gt;Deutsch&lt;/a&gt; | &lt;a href="https://github.com/huggingface/transformers/raw/main/i18n/README_vi.md"&gt;Ti·∫øng Vi·ªát&lt;/a&gt; | &lt;a href="https://github.com/huggingface/transformers/raw/main/i18n/README_ar.md"&gt;ÿßŸÑÿπÿ±ÿ®Ÿäÿ©&lt;/a&gt; | &lt;a href="https://github.com/huggingface/transformers/raw/main/i18n/README_ur.md"&gt;ÿßÿ±ÿØŸà&lt;/a&gt; | &lt;/p&gt; &lt;/h4&gt; 
&lt;h3 align="center"&gt; &lt;p&gt;State-of-the-art pretrained models for inference and training&lt;/p&gt; &lt;/h3&gt; 
&lt;h3 align="center"&gt; &lt;img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/transformers_as_a_model_definition.png" /&gt; &lt;/h3&gt; 
&lt;p&gt;Transformers acts as the model-definition framework for state-of-the-art machine learning models in text, computer vision, audio, video, and multimodal model, for both inference and training.&lt;/p&gt; 
&lt;p&gt;It centralizes the model definition so that this definition is agreed upon across the ecosystem. &lt;code&gt;transformers&lt;/code&gt; is the pivot across frameworks: if a model definition is supported, it will be compatible with the majority of training frameworks (Axolotl, Unsloth, DeepSpeed, FSDP, PyTorch-Lightning, ...), inference engines (vLLM, SGLang, TGI, ...), and adjacent modeling libraries (llama.cpp, mlx, ...) which leverage the model definition from &lt;code&gt;transformers&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;We pledge to help support new state-of-the-art models and democratize their usage by having their model definition be simple, customizable, and efficient.&lt;/p&gt; 
&lt;p&gt;There are over 1M+ Transformers &lt;a href="https://huggingface.co/models?library=transformers&amp;amp;sort=trending"&gt;model checkpoints&lt;/a&gt; on the &lt;a href="https://huggingface.com/models"&gt;Hugging Face Hub&lt;/a&gt; you can use.&lt;/p&gt; 
&lt;p&gt;Explore the &lt;a href="https://huggingface.com/"&gt;Hub&lt;/a&gt; today to find a model and use Transformers to help you get started right away.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;Transformers works with Python 3.9+ &lt;a href="https://pytorch.org/get-started/locally/"&gt;PyTorch&lt;/a&gt; 2.1+, &lt;a href="https://www.tensorflow.org/install/pip"&gt;TensorFlow&lt;/a&gt; 2.6+, and &lt;a href="https://flax.readthedocs.io/en/latest/"&gt;Flax&lt;/a&gt; 0.4.1+.&lt;/p&gt; 
&lt;p&gt;Create and activate a virtual environment with &lt;a href="https://docs.python.org/3/library/venv.html"&gt;venv&lt;/a&gt; or &lt;a href="https://docs.astral.sh/uv/"&gt;uv&lt;/a&gt;, a fast Rust-based Python package and project manager.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;# venv
python -m venv .my-env
source .my-env/bin/activate
# uv
uv venv .my-env
source .my-env/bin/activate
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Install Transformers in your virtual environment.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;# pip
pip install "transformers[torch]"

# uv
uv pip install "transformers[torch]"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Install Transformers from source if you want the latest changes in the library or are interested in contributing. However, the &lt;em&gt;latest&lt;/em&gt; version may not be stable. Feel free to open an &lt;a href="https://github.com/huggingface/transformers/issues"&gt;issue&lt;/a&gt; if you encounter an error.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;git clone https://github.com/huggingface/transformers.git
cd transformers

# pip
pip install .[torch]

# uv
uv pip install .[torch]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Quickstart&lt;/h2&gt; 
&lt;p&gt;Get started with Transformers right away with the &lt;a href="https://huggingface.co/docs/transformers/pipeline_tutorial"&gt;Pipeline&lt;/a&gt; API. The &lt;code&gt;Pipeline&lt;/code&gt; is a high-level inference class that supports text, audio, vision, and multimodal tasks. It handles preprocessing the input and returns the appropriate output.&lt;/p&gt; 
&lt;p&gt;Instantiate a pipeline and specify model to use for text generation. The model is downloaded and cached so you can easily reuse it again. Finally, pass some text to prompt the model.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;from transformers import pipeline

pipeline = pipeline(task="text-generation", model="Qwen/Qwen2.5-1.5B")
pipeline("the secret to baking a really good cake is ")
[{'generated_text': 'the secret to baking a really good cake is 1) to use the right ingredients and 2) to follow the recipe exactly. the recipe for the cake is as follows: 1 cup of sugar, 1 cup of flour, 1 cup of milk, 1 cup of butter, 1 cup of eggs, 1 cup of chocolate chips. if you want to make 2 cakes, how much sugar do you need? To make 2 cakes, you will need 2 cups of sugar.'}]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To chat with a model, the usage pattern is the same. The only difference is you need to construct a chat history (the input to &lt;code&gt;Pipeline&lt;/code&gt;) between you and the system.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] You can also chat with a model directly from the command line.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-shell"&gt;transformers chat Qwen/Qwen2.5-0.5B-Instruct
&lt;/code&gt;&lt;/pre&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class="language-py"&gt;import torch
from transformers import pipeline

chat = [
    {"role": "system", "content": "You are a sassy, wise-cracking robot as imagined by Hollywood circa 1986."},
    {"role": "user", "content": "Hey, can you tell me any fun things to do in New York?"}
]

pipeline = pipeline(task="text-generation", model="meta-llama/Meta-Llama-3-8B-Instruct", dtype=torch.bfloat16, device_map="auto")
response = pipeline(chat, max_new_tokens=512)
print(response[0]["generated_text"][-1]["content"])
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Expand the examples below to see how &lt;code&gt;Pipeline&lt;/code&gt; works for different modalities and tasks.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Automatic speech recognition&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-py"&gt;from transformers import pipeline

pipeline = pipeline(task="automatic-speech-recognition", model="openai/whisper-large-v3")
pipeline("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Image classification&lt;/summary&gt; 
 &lt;h3 align="center"&gt; &lt;a&gt;&lt;img src="https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png" /&gt;&lt;/a&gt; &lt;/h3&gt; 
 &lt;pre&gt;&lt;code class="language-py"&gt;from transformers import pipeline

pipeline = pipeline(task="image-classification", model="facebook/dinov2-small-imagenet1k-1-layer")
pipeline("https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png")
[{'label': 'macaw', 'score': 0.997848391532898},
 {'label': 'sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita',
  'score': 0.0016551691805943847},
 {'label': 'lorikeet', 'score': 0.00018523589824326336},
 {'label': 'African grey, African gray, Psittacus erithacus',
  'score': 7.85409429227002e-05},
 {'label': 'quail', 'score': 5.502637941390276e-05}]
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Visual question answering&lt;/summary&gt; 
 &lt;h3 align="center"&gt; &lt;a&gt;&lt;img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-few-shot.jpg" /&gt;&lt;/a&gt; &lt;/h3&gt; 
 &lt;pre&gt;&lt;code class="language-py"&gt;from transformers import pipeline

pipeline = pipeline(task="visual-question-answering", model="Salesforce/blip-vqa-base")
pipeline(
    image="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-few-shot.jpg",
    question="What is in the image?",
)
[{'answer': 'statue of liberty'}]
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;Why should I use Transformers?&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Easy-to-use state-of-the-art models:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;High performance on natural language understanding &amp;amp; generation, computer vision, audio, video, and multimodal tasks.&lt;/li&gt; 
   &lt;li&gt;Low barrier to entry for researchers, engineers, and developers.&lt;/li&gt; 
   &lt;li&gt;Few user-facing abstractions with just three classes to learn.&lt;/li&gt; 
   &lt;li&gt;A unified API for using all our pretrained models.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Lower compute costs, smaller carbon footprint:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Share trained models instead of training from scratch.&lt;/li&gt; 
   &lt;li&gt;Reduce compute time and production costs.&lt;/li&gt; 
   &lt;li&gt;Dozens of model architectures with 1M+ pretrained checkpoints across all modalities.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Choose the right framework for every part of a models lifetime:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Train state-of-the-art models in 3 lines of code.&lt;/li&gt; 
   &lt;li&gt;Move a single model between PyTorch/JAX/TF2.0 frameworks at will.&lt;/li&gt; 
   &lt;li&gt;Pick the right framework for training, evaluation, and production.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Easily customize a model or an example to your needs:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;We provide examples for each architecture to reproduce the results published by its original authors.&lt;/li&gt; 
   &lt;li&gt;Model internals are exposed as consistently as possible.&lt;/li&gt; 
   &lt;li&gt;Model files can be used independently of the library for quick experiments.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;a target="_blank" href="https://huggingface.co/enterprise"&gt; &lt;img alt="Hugging Face Enterprise Hub" src="https://github.com/user-attachments/assets/247fb16d-d251-4583-96c4-d3d76dda4925" /&gt; &lt;/a&gt;
&lt;br /&gt; 
&lt;h2&gt;Why shouldn't I use Transformers?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;This library is not a modular toolbox of building blocks for neural nets. The code in the model files is not refactored with additional abstractions on purpose, so that researchers can quickly iterate on each of the models without diving into additional abstractions/files.&lt;/li&gt; 
 &lt;li&gt;The training API is optimized to work with PyTorch models provided by Transformers. For generic machine learning loops, you should use another library like &lt;a href="https://huggingface.co/docs/accelerate"&gt;Accelerate&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;The &lt;a href="https://github.com/huggingface/transformers/tree/main/examples"&gt;example scripts&lt;/a&gt; are only &lt;em&gt;examples&lt;/em&gt;. They may not necessarily work out-of-the-box on your specific use case and you'll need to adapt the code for it to work.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;100 projects using Transformers&lt;/h2&gt; 
&lt;p&gt;Transformers is more than a toolkit to use pretrained models, it's a community of projects built around it and the Hugging Face Hub. We want Transformers to enable developers, researchers, students, professors, engineers, and anyone else to build their dream projects.&lt;/p&gt; 
&lt;p&gt;In order to celebrate Transformers 100,000 stars, we wanted to put the spotlight on the community with the &lt;a href="https://raw.githubusercontent.com/huggingface/transformers/main/awesome-transformers.md"&gt;awesome-transformers&lt;/a&gt; page which lists 100 incredible projects built with Transformers.&lt;/p&gt; 
&lt;p&gt;If you own or use a project that you believe should be part of the list, please open a PR to add it!&lt;/p&gt; 
&lt;h2&gt;Example models&lt;/h2&gt; 
&lt;p&gt;You can test most of our models directly on their &lt;a href="https://huggingface.co/models"&gt;Hub model pages&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Expand each modality below to see a few example models for various use cases.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Audio&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Audio classification with &lt;a href="https://huggingface.co/openai/whisper-large-v3-turbo"&gt;Whisper&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Automatic speech recognition with &lt;a href="https://huggingface.co/UsefulSensors/moonshine"&gt;Moonshine&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Keyword spotting with &lt;a href="https://huggingface.co/superb/wav2vec2-base-superb-ks"&gt;Wav2Vec2&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Speech to speech generation with &lt;a href="https://huggingface.co/kyutai/moshiko-pytorch-bf16"&gt;Moshi&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Text to audio with &lt;a href="https://huggingface.co/facebook/musicgen-large"&gt;MusicGen&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Text to speech with &lt;a href="https://huggingface.co/suno/bark"&gt;Bark&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Computer vision&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Automatic mask generation with &lt;a href="https://huggingface.co/facebook/sam-vit-base"&gt;SAM&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Depth estimation with &lt;a href="https://huggingface.co/apple/DepthPro-hf"&gt;DepthPro&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Image classification with &lt;a href="https://huggingface.co/facebook/dinov2-base"&gt;DINO v2&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Keypoint detection with &lt;a href="https://huggingface.co/magic-leap-community/superpoint"&gt;SuperPoint&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Keypoint matching with &lt;a href="https://huggingface.co/magic-leap-community/superglue_outdoor"&gt;SuperGlue&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Object detection with &lt;a href="https://huggingface.co/PekingU/rtdetr_v2_r50vd"&gt;RT-DETRv2&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Pose Estimation with &lt;a href="https://huggingface.co/usyd-community/vitpose-base-simple"&gt;VitPose&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Universal segmentation with &lt;a href="https://huggingface.co/shi-labs/oneformer_ade20k_swin_large"&gt;OneFormer&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Video classification with &lt;a href="https://huggingface.co/MCG-NJU/videomae-large"&gt;VideoMAE&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Multimodal&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Audio or text to text with &lt;a href="https://huggingface.co/Qwen/Qwen2-Audio-7B"&gt;Qwen2-Audio&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Document question answering with &lt;a href="https://huggingface.co/microsoft/layoutlmv3-base"&gt;LayoutLMv3&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Image or text to text with &lt;a href="https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct"&gt;Qwen-VL&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Image captioning &lt;a href="https://huggingface.co/Salesforce/blip2-opt-2.7b"&gt;BLIP-2&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;OCR-based document understanding with &lt;a href="https://huggingface.co/stepfun-ai/GOT-OCR-2.0-hf"&gt;GOT-OCR2&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Table question answering with &lt;a href="https://huggingface.co/google/tapas-base"&gt;TAPAS&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Unified multimodal understanding and generation with &lt;a href="https://huggingface.co/BAAI/Emu3-Gen"&gt;Emu3&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Vision to text with &lt;a href="https://huggingface.co/llava-hf/llava-onevision-qwen2-0.5b-ov-hf"&gt;Llava-OneVision&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Visual question answering with &lt;a href="https://huggingface.co/llava-hf/llava-1.5-7b-hf"&gt;Llava&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Visual referring expression segmentation with &lt;a href="https://huggingface.co/microsoft/kosmos-2-patch14-224"&gt;Kosmos-2&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;NLP&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Masked word completion with &lt;a href="https://huggingface.co/answerdotai/ModernBERT-base"&gt;ModernBERT&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Named entity recognition with &lt;a href="https://huggingface.co/google/gemma-2-2b"&gt;Gemma&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Question answering with &lt;a href="https://huggingface.co/mistralai/Mixtral-8x7B-v0.1"&gt;Mixtral&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Summarization with &lt;a href="https://huggingface.co/facebook/bart-large-cnn"&gt;BART&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Translation with &lt;a href="https://huggingface.co/google-t5/t5-base"&gt;T5&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Text generation with &lt;a href="https://huggingface.co/meta-llama/Llama-3.2-1B"&gt;Llama&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Text classification with &lt;a href="https://huggingface.co/Qwen/Qwen2.5-0.5B"&gt;Qwen&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;We now have a &lt;a href="https://www.aclweb.org/anthology/2020.emnlp-demos.6/"&gt;paper&lt;/a&gt; you can cite for the ü§ó Transformers library:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@inproceedings{wolf-etal-2020-transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R√©mi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-demos.6",
    pages = "38--45"
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>confident-ai/deepeval</title>
      <link>https://github.com/confident-ai/deepeval</link>
      <description>&lt;p&gt;The LLM Evaluation Framework&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt; &lt;img src="https://github.com/confident-ai/deepeval/raw/main/docs/static/img/deepeval.png" alt="DeepEval Logo" width="100%" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;/p&gt;
&lt;h1 align="center"&gt;The LLM Evaluation Framework&lt;/h1&gt; 
&lt;p&gt;&lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://trendshift.io/repositories/5917" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/5917" alt="confident-ai%2Fdeepeval | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; &lt;a href="https://discord.gg/3SEyvpgu2f"&gt; &lt;img alt="discord-invite" src="https://dcbadge.vercel.app/api/server/3SEyvpgu2f?style=flat" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;h4 align="center"&gt; &lt;p&gt; &lt;a href="https://deepeval.com/docs/getting-started?utm_source=GitHub"&gt;Documentation&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/confident-ai/deepeval/main/#-metrics-and-features"&gt;Metrics and Features&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/confident-ai/deepeval/main/#-quickstart"&gt;Getting Started&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/confident-ai/deepeval/main/#-integrations"&gt;Integrations&lt;/a&gt; | &lt;a href="https://confident-ai.com?utm_source=GitHub"&gt;DeepEval Platform&lt;/a&gt; &lt;/p&gt;&lt;p&gt; &lt;/p&gt;&lt;/h4&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/confident-ai/deepeval/releases"&gt; &lt;img alt="GitHub release" src="https://img.shields.io/github/release/confident-ai/deepeval.svg?color=violet" /&gt; &lt;/a&gt; &lt;a href="https://colab.research.google.com/drive/1PPxYEBa6eu__LquGoFFJZkhYgWVYE6kh?usp=sharing"&gt; &lt;img alt="Try Quickstart in Colab" src="https://colab.research.google.com/assets/colab-badge.svg?sanitize=true" /&gt; &lt;/a&gt; &lt;a href="https://github.com/confident-ai/deepeval/raw/master/LICENSE.md"&gt; &lt;img alt="License" src="https://img.shields.io/github/license/confident-ai/deepeval.svg?color=yellow" /&gt; &lt;/a&gt; &lt;a href="https://x.com/deepeval"&gt; &lt;img alt="Twitter Follow" src="https://img.shields.io/twitter/follow/deepeval?style=social&amp;amp;logo=x" /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align="center"&gt; 
 &lt;!-- Keep these links. Translations will automatically update with the README. --&gt; &lt;a href="https://www.readme-i18n.com/confident-ai/deepeval?lang=de"&gt;Deutsch&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/confident-ai/deepeval?lang=es"&gt;Espa√±ol&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/confident-ai/deepeval?lang=fr"&gt;fran√ßais&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/confident-ai/deepeval?lang=ja"&gt;Êó•Êú¨Ë™û&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/confident-ai/deepeval?lang=ko"&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/confident-ai/deepeval?lang=pt"&gt;Portugu√™s&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/confident-ai/deepeval?lang=ru"&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt; | &lt;a href="https://www.readme-i18n.com/confident-ai/deepeval?lang=zh"&gt;‰∏≠Êñá&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;DeepEval&lt;/strong&gt; is a simple-to-use, open-source LLM evaluation framework, for evaluating and testing large-language model systems. It is similar to Pytest but specialized for unit testing LLM outputs. DeepEval incorporates the latest research to evaluate LLM outputs based on metrics such as G-Eval, hallucination, answer relevancy, RAGAS, etc., which uses LLMs and various other NLP models that runs &lt;strong&gt;locally on your machine&lt;/strong&gt; for evaluation.&lt;/p&gt; 
&lt;p&gt;Whether your LLM applications are RAG pipelines, chatbots, AI agents, implemented via LangChain or LlamaIndex, DeepEval has you covered. With it, you can easily determine the optimal models, prompts, and architecture to improve your RAG pipeline, agentic workflows, prevent prompt drifting, or even transition from OpenAI to hosting your own Deepseek R1 with confidence.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] Need a place for your DeepEval testing data to live üè°‚ù§Ô∏è? &lt;a href="https://confident-ai.com?utm_source=GitHub"&gt;Sign up to the DeepEval platform&lt;/a&gt; to compare iterations of your LLM app, generate &amp;amp; share testing reports, and more.&lt;/p&gt; 
 &lt;p&gt;&lt;img src="https://raw.githubusercontent.com/confident-ai/deepeval/main/assets/demo.gif" alt="Demo GIF" /&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Want to talk LLM evaluation, need help picking metrics, or just to say hi? &lt;a href="https://discord.com/invite/3SEyvpgu2f"&gt;Come join our discord.&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;br /&gt; 
&lt;h1&gt;üî• Metrics and Features&lt;/h1&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ü•≥ You can now share DeepEval's test results on the cloud directly on &lt;a href="https://confident-ai.com?utm_source=GitHub"&gt;Confident AI&lt;/a&gt;'s infrastructure&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;Supports both end-to-end and component-level LLM evaluation.&lt;/li&gt; 
 &lt;li&gt;Large variety of ready-to-use LLM evaluation metrics (all with explanations) powered by &lt;strong&gt;ANY&lt;/strong&gt; LLM of your choice, statistical methods, or NLP models that runs &lt;strong&gt;locally on your machine&lt;/strong&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;G-Eval&lt;/li&gt; 
   &lt;li&gt;DAG (&lt;a href="https://deepeval.com/docs/metrics-dag"&gt;deep acyclic graph&lt;/a&gt;)&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;RAG metrics:&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Answer Relevancy&lt;/li&gt; 
     &lt;li&gt;Faithfulness&lt;/li&gt; 
     &lt;li&gt;Contextual Recall&lt;/li&gt; 
     &lt;li&gt;Contextual Precision&lt;/li&gt; 
     &lt;li&gt;Contextual Relevancy&lt;/li&gt; 
     &lt;li&gt;RAGAS&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Agentic metrics:&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Task Completion&lt;/li&gt; 
     &lt;li&gt;Tool Correctness&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Others:&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Hallucination&lt;/li&gt; 
     &lt;li&gt;Summarization&lt;/li&gt; 
     &lt;li&gt;Bias&lt;/li&gt; 
     &lt;li&gt;Toxicity&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Conversational metrics:&lt;/strong&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Knowledge Retention&lt;/li&gt; 
     &lt;li&gt;Conversation Completeness&lt;/li&gt; 
     &lt;li&gt;Conversation Relevancy&lt;/li&gt; 
     &lt;li&gt;Role Adherence&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;etc.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Build your own custom metrics that are automatically integrated with DeepEval's ecosystem.&lt;/li&gt; 
 &lt;li&gt;Generate synthetic datasets for evaluation.&lt;/li&gt; 
 &lt;li&gt;Integrates seamlessly with &lt;strong&gt;ANY&lt;/strong&gt; CI/CD environment.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://deepeval.com/docs/red-teaming-introduction"&gt;Red team your LLM application&lt;/a&gt; for 40+ safety vulnerabilities in a few lines of code, including: 
  &lt;ul&gt; 
   &lt;li&gt;Toxicity&lt;/li&gt; 
   &lt;li&gt;Bias&lt;/li&gt; 
   &lt;li&gt;SQL Injection&lt;/li&gt; 
   &lt;li&gt;etc., using advanced 10+ attack enhancement strategies such as prompt injections.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Easily benchmark &lt;strong&gt;ANY&lt;/strong&gt; LLM on popular LLM benchmarks in &lt;a href="https://deepeval.com/docs/benchmarks-introduction?utm_source=GitHub"&gt;under 10 lines of code.&lt;/a&gt;, which includes: 
  &lt;ul&gt; 
   &lt;li&gt;MMLU&lt;/li&gt; 
   &lt;li&gt;HellaSwag&lt;/li&gt; 
   &lt;li&gt;DROP&lt;/li&gt; 
   &lt;li&gt;BIG-Bench Hard&lt;/li&gt; 
   &lt;li&gt;TruthfulQA&lt;/li&gt; 
   &lt;li&gt;HumanEval&lt;/li&gt; 
   &lt;li&gt;GSM8K&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://confident-ai.com?utm_source=GitHub"&gt;100% integrated with Confident AI&lt;/a&gt; for the full evaluation lifecycle: 
  &lt;ul&gt; 
   &lt;li&gt;Curate/annotate evaluation datasets on the cloud&lt;/li&gt; 
   &lt;li&gt;Benchmark LLM app using dataset, and compare with previous iterations to experiment which models/prompts works best&lt;/li&gt; 
   &lt;li&gt;Fine-tune metrics for custom results&lt;/li&gt; 
   &lt;li&gt;Debug evaluation results via LLM traces&lt;/li&gt; 
   &lt;li&gt;Monitor &amp;amp; evaluate LLM responses in product to improve datasets with real-world data&lt;/li&gt; 
   &lt;li&gt;Repeat until perfection&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Confident AI is the DeepEval platform. Create an account &lt;a href="https://app.confident-ai.com?utm_source=GitHub"&gt;here.&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;br /&gt; 
&lt;h1&gt;üîå Integrations&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;ü¶Ñ LlamaIndex, to &lt;a href="https://www.deepeval.com/integrations/frameworks/llamaindex?utm_source=GitHub"&gt;&lt;strong&gt;unit test RAG applications in CI/CD&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ü§ó Hugging Face, to &lt;a href="https://www.deepeval.com/integrations/frameworks/huggingface?utm_source=GitHub"&gt;&lt;strong&gt;enable real-time evaluations during LLM fine-tuning&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;h1&gt;üöÄ QuickStart&lt;/h1&gt; 
&lt;p&gt;Let's pretend your LLM application is a RAG based customer support chatbot; here's how DeepEval can help test what you've built.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;pip install -U deepeval
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Create an account (highly recommended)&lt;/h2&gt; 
&lt;p&gt;Using the &lt;code&gt;deepeval&lt;/code&gt; platform will allow you to generate sharable testing reports on the cloud. It is free, takes no additional code to setup, and we highly recommend giving it a try.&lt;/p&gt; 
&lt;p&gt;To login, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;deepeval login
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Follow the instructions in the CLI to create an account, copy your API key, and paste it into the CLI. All test cases will automatically be logged (find more information on data privacy &lt;a href="https://deepeval.com/docs/data-privacy?utm_source=GitHub"&gt;here&lt;/a&gt;).&lt;/p&gt; 
&lt;h2&gt;Writing your first test case&lt;/h2&gt; 
&lt;p&gt;Create a test file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;touch test_chatbot.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Open &lt;code&gt;test_chatbot.py&lt;/code&gt; and write your first test case to run an &lt;strong&gt;end-to-end&lt;/strong&gt; evaluation using DeepEval, which treats your LLM app as a black-box:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import pytest
from deepeval import assert_test
from deepeval.metrics import GEval
from deepeval.test_case import LLMTestCase, LLMTestCaseParams

def test_case():
    correctness_metric = GEval(
        name="Correctness",
        criteria="Determine if the 'actual output' is correct based on the 'expected output'.",
        evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT],
        threshold=0.5
    )
    test_case = LLMTestCase(
        input="What if these shoes don't fit?",
        # Replace this with the actual output from your LLM application
        actual_output="You have 30 days to get a full refund at no extra cost.",
        expected_output="We offer a 30-day full refund at no extra costs.",
        retrieval_context=["All customers are eligible for a 30 day full refund at no extra costs."]
    )
    assert_test(test_case, [correctness_metric])
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Set your &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; as an environment variable (you can also evaluate using your own custom model, for more details visit &lt;a href="https://deepeval.com/docs/metrics-introduction#using-a-custom-llm?utm_source=GitHub"&gt;this part of our docs&lt;/a&gt;):&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;export OPENAI_API_KEY="..."
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;And finally, run &lt;code&gt;test_chatbot.py&lt;/code&gt; in the CLI:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;deepeval test run test_chatbot.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Congratulations! Your test case should have passed ‚úÖ&lt;/strong&gt; Let's breakdown what happened.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;The variable &lt;code&gt;input&lt;/code&gt; mimics a user input, and &lt;code&gt;actual_output&lt;/code&gt; is a placeholder for what your application's supposed to output based on this input.&lt;/li&gt; 
 &lt;li&gt;The variable &lt;code&gt;expected_output&lt;/code&gt; represents the ideal answer for a given &lt;code&gt;input&lt;/code&gt;, and &lt;a href="https://deepeval.com/docs/metrics-llm-evals"&gt;&lt;code&gt;GEval&lt;/code&gt;&lt;/a&gt; is a research-backed metric provided by &lt;code&gt;deepeval&lt;/code&gt; for you to evaluate your LLM output's on any custom with human-like accuracy.&lt;/li&gt; 
 &lt;li&gt;In this example, the metric &lt;code&gt;criteria&lt;/code&gt; is correctness of the &lt;code&gt;actual_output&lt;/code&gt; based on the provided &lt;code&gt;expected_output&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;All metric scores range from 0 - 1, which the &lt;code&gt;threshold=0.5&lt;/code&gt; threshold ultimately determines if your test have passed or not.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://deepeval.com/docs/getting-started?utm_source=GitHub"&gt;Read our documentation&lt;/a&gt; for more information on more options to run end-to-end evaluation, how to use additional metrics, create your own custom metrics, and tutorials on how to integrate with other tools like LangChain and LlamaIndex.&lt;/p&gt; 
&lt;br /&gt; 
&lt;h2&gt;Evaluating Nested Components&lt;/h2&gt; 
&lt;p&gt;If you wish to evaluate individual components within your LLM app, you need to run &lt;strong&gt;component-level&lt;/strong&gt; evals - a powerful way to evaluate any component within an LLM system.&lt;/p&gt; 
&lt;p&gt;Simply trace "components" such as LLM calls, retrievers, tool calls, and agents within your LLM application using the &lt;code&gt;@observe&lt;/code&gt; decorator to apply metrics on a component-level. Tracing with &lt;code&gt;deepeval&lt;/code&gt; is non-instrusive (learn more &lt;a href="https://deepeval.com/docs/evaluation-llm-tracing#dont-be-worried-about-tracing"&gt;here&lt;/a&gt;) and helps you avoid rewriting your codebase just for evals:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from deepeval.tracing import observe, update_current_span
from deepeval.test_case import LLMTestCase
from deepeval.dataset import Golden
from deepeval.metrics import GEval
from deepeval import evaluate

correctness = GEval(name="Correctness", criteria="Determine if the 'actual output' is correct based on the 'expected output'.", evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT])

@observe(metrics=[correctness])
def inner_component():
    # Component can be anything from an LLM call, retrieval, agent, tool use, etc.
    update_current_span(test_case=LLMTestCase(input="...", actual_output="..."))
    return

@observe
def llm_app(input: str):
    inner_component()
    return

evaluate(observed_callback=llm_app, goldens=[Golden(input="Hi!")])
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can learn everything about component-level evaluations &lt;a href="https://www.deepeval.com/docs/evaluation-component-level-llm-evals"&gt;here.&lt;/a&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;h2&gt;Evaluating Without Pytest Integration&lt;/h2&gt; 
&lt;p&gt;Alternatively, you can evaluate without Pytest, which is more suited for a notebook environment.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from deepeval import evaluate
from deepeval.metrics import AnswerRelevancyMetric
from deepeval.test_case import LLMTestCase

answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.7)
test_case = LLMTestCase(
    input="What if these shoes don't fit?",
    # Replace this with the actual output from your LLM application
    actual_output="We offer a 30-day full refund at no extra costs.",
    retrieval_context=["All customers are eligible for a 30 day full refund at no extra costs."]
)
evaluate([test_case], [answer_relevancy_metric])
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Using Standalone Metrics&lt;/h2&gt; 
&lt;p&gt;DeepEval is extremely modular, making it easy for anyone to use any of our metrics. Continuing from the previous example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from deepeval.metrics import AnswerRelevancyMetric
from deepeval.test_case import LLMTestCase

answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.7)
test_case = LLMTestCase(
    input="What if these shoes don't fit?",
    # Replace this with the actual output from your LLM application
    actual_output="We offer a 30-day full refund at no extra costs.",
    retrieval_context=["All customers are eligible for a 30 day full refund at no extra costs."]
)

answer_relevancy_metric.measure(test_case)
print(answer_relevancy_metric.score)
# All metrics also offer an explanation
print(answer_relevancy_metric.reason)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note that some metrics are for RAG pipelines, while others are for fine-tuning. Make sure to use our docs to pick the right one for your use case.&lt;/p&gt; 
&lt;h2&gt;Evaluating a Dataset / Test Cases in Bulk&lt;/h2&gt; 
&lt;p&gt;In DeepEval, a dataset is simply a collection of test cases. Here is how you can evaluate these in bulk:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;import pytest
from deepeval import assert_test
from deepeval.dataset import EvaluationDataset, Golden
from deepeval.metrics import AnswerRelevancyMetric
from deepeval.test_case import LLMTestCase

dataset = EvaluationDataset(goldens=[Golden(input="What's the weather like today?")])

for golden in dataset.goldens:
    test_case = LLMTestCase(
        input=golden.input,
        actual_output=your_llm_app(golden.input)
    )
    dataset.add_test_case(test_case)

@pytest.mark.parametrize(
    "test_case",
    dataset,
)
def test_customer_chatbot(test_case: LLMTestCase):
    answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.5)
    assert_test(test_case, [answer_relevancy_metric])
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Run this in the CLI, you can also add an optional -n flag to run tests in parallel
deepeval test run test_&amp;lt;filename&amp;gt;.py -n 4
&lt;/code&gt;&lt;/pre&gt; 
&lt;br /&gt; 
&lt;p&gt;Alternatively, although we recommend using &lt;code&gt;deepeval test run&lt;/code&gt;, you can evaluate a dataset/test cases without using our Pytest integration:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-python"&gt;from deepeval import evaluate
...

evaluate(dataset, [answer_relevancy_metric])
# or
dataset.evaluate([answer_relevancy_metric])
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;LLM Evaluation With Confident AI&lt;/h1&gt; 
&lt;p&gt;The correct LLM evaluation lifecycle is only achievable with &lt;a href="https://confident-ai.com?utm_source=Github"&gt;the DeepEval platform&lt;/a&gt;. It allows you to:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Curate/annotate evaluation datasets on the cloud&lt;/li&gt; 
 &lt;li&gt;Benchmark LLM app using dataset, and compare with previous iterations to experiment which models/prompts works best&lt;/li&gt; 
 &lt;li&gt;Fine-tune metrics for custom results&lt;/li&gt; 
 &lt;li&gt;Debug evaluation results via LLM traces&lt;/li&gt; 
 &lt;li&gt;Monitor &amp;amp; evaluate LLM responses in product to improve datasets with real-world data&lt;/li&gt; 
 &lt;li&gt;Repeat until perfection&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Everything on Confident AI, including how to use Confident is available &lt;a href="https://www.confident-ai.com/docs?utm_source=GitHub"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;To begin, login from the CLI:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;deepeval login
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Follow the instructions to log in, create your account, and paste your API key into the CLI.&lt;/p&gt; 
&lt;p&gt;Now, run your test file again:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;deepeval test run test_chatbot.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You should see a link displayed in the CLI once the test has finished running. Paste it into your browser to view the results!&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/confident-ai/deepeval/main/assets/demo.gif" alt="Demo GIF" /&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;h1&gt;Contributing&lt;/h1&gt; 
&lt;p&gt;Please read &lt;a href="https://github.com/confident-ai/deepeval/raw/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for details on our code of conduct, and the process for submitting pull requests to us.&lt;/p&gt; 
&lt;br /&gt; 
&lt;h1&gt;Roadmap&lt;/h1&gt; 
&lt;p&gt;Features:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Integration with Confident AI&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Implement G-Eval&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Implement RAG metrics&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Implement Conversational metrics&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Evaluation Dataset Creation&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; Red-Teaming&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; DAG custom metrics&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Guardrails&lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;h1&gt;Authors&lt;/h1&gt; 
&lt;p&gt;Built by the founders of Confident AI. Contact &lt;a href="mailto:jeffreyip@confident-ai.com"&gt;jeffreyip@confident-ai.com&lt;/a&gt; for all enquiries.&lt;/p&gt; 
&lt;br /&gt; 
&lt;h1&gt;License&lt;/h1&gt; 
&lt;p&gt;DeepEval is licensed under Apache 2.0 - see the &lt;a href="https://github.com/confident-ai/deepeval/raw/main/LICENSE.md"&gt;LICENSE.md&lt;/a&gt; file for details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>NVIDIA/NeMo-Agent-Toolkit</title>
      <link>https://github.com/NVIDIA/NeMo-Agent-Toolkit</link>
      <description>&lt;p&gt;The NVIDIA NeMo Agent toolkit is an open-source library for efficiently connecting and optimizing teams of AI agents.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/NVIDIA/NeMo-Agent-Toolkit/develop/docs/source/_static/banner.png" alt="NVIDIA NeMo Agent Toolkit" title="NeMo Agent toolkit banner image" /&gt;&lt;/p&gt; 
&lt;h1&gt;NVIDIA NeMo Agent Toolkit&lt;/h1&gt; 
&lt;!-- vale off (due to hyperlinks) --&gt; 
&lt;p&gt;&lt;a href="https://opensource.org/licenses/Apache-2.0"&gt;&lt;img src="https://img.shields.io/badge/License-Apache%202.0-green.svg?sanitize=true" alt="License: Apache 2.0" /&gt;&lt;/a&gt; &lt;a href="https://github.com/NVIDIA/NeMo-Agent-Toolkit/releases"&gt;&lt;img src="https://img.shields.io/github/v/release/NVIDIA/NeMo-Agent-Toolkit" alt="GitHub Release" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/nvidia-nat/"&gt;&lt;img src="https://img.shields.io/pypi/v/nvidia-nat" alt="PyPI version" /&gt;&lt;/a&gt; &lt;a href="https://pepy.tech/projects/nvidia-nat"&gt;&lt;img src="https://static.pepy.tech/badge/nvidia-nat" alt="PyPI Downloads" /&gt;&lt;/a&gt; &lt;a href="https://github.com/NVIDIA/NeMo-Agent-Toolkit/issues"&gt;&lt;img src="https://img.shields.io/github/issues/NVIDIA/NeMo-Agent-Toolkit" alt="GitHub issues" /&gt;&lt;/a&gt; &lt;a href="https://github.com/NVIDIA/NeMo-Agent-Toolkit/pulls"&gt;&lt;img src="https://img.shields.io/github/issues-pr/NVIDIA/NeMo-Agent-Toolkit" alt="GitHub pull requests" /&gt;&lt;/a&gt; &lt;a href="https://github.com/NVIDIA/NeMo-Agent-Toolkit"&gt;&lt;img src="https://img.shields.io/github/stars/NVIDIA/NeMo-Agent-Toolkit" alt="GitHub Repo stars" /&gt;&lt;/a&gt; &lt;a href="https://github.com/NVIDIA/NeMo-Agent-Toolkit/network/members"&gt;&lt;img src="https://img.shields.io/github/forks/NVIDIA/NeMo-Agent-Toolkit" alt="GitHub forks" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;!-- vale on --&gt; 
&lt;p&gt;NVIDIA NeMo Agent toolkit is a flexible, lightweight, and unifying library that allows you to easily connect existing enterprise agents to data sources and tools across any framework.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] NeMo Agent toolkit was previously known as the Agent Intelligence (AIQ) toolkit, and 
  &lt;!-- vale off --&gt;AgentIQ
  &lt;!-- vale on --&gt;. The library was renamed to better reflect the purpose of the toolkit and to align with the NVIDIA NeMo family of products. The core technologies, performance and roadmap remain unchanged and the API is fully compatible with previous release. Please refer to the &lt;a href="https://raw.githubusercontent.com/NVIDIA/NeMo-Agent-Toolkit/develop/docs/source/resources/migration-guide.md"&gt;Migration Guide&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;‚ú® Key Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;üß© &lt;a href="https://raw.githubusercontent.com/NVIDIA/NeMo-Agent-Toolkit/develop/docs/source/quick-start/installing.md#framework-integrations"&gt;&lt;strong&gt;Framework Agnostic:&lt;/strong&gt;&lt;/a&gt; NeMo Agent toolkit works side-by-side and around existing agentic frameworks, such as &lt;a href="https://www.langchain.com/"&gt;LangChain&lt;/a&gt;, &lt;a href="https://www.llamaindex.ai/"&gt;LlamaIndex&lt;/a&gt;, &lt;a href="https://www.crewai.com/"&gt;CrewAI&lt;/a&gt;, and &lt;a href="https://learn.microsoft.com/en-us/semantic-kernel/"&gt;Microsoft Semantic Kernel&lt;/a&gt;, as well as customer enterprise frameworks and simple Python agents. This allows you to use your current technology stack without replatforming. NeMo Agent toolkit complements any existing agentic framework or memory tool you're using and isn't tied to any specific agentic framework, long-term memory, or data source.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üîÅ &lt;a href="https://raw.githubusercontent.com/NVIDIA/NeMo-Agent-Toolkit/develop/docs/source/extend/sharing-components.md"&gt;&lt;strong&gt;Reusability:&lt;/strong&gt;&lt;/a&gt; Every agent, tool, and agentic workflow in this library exists as a function call that works together in complex software applications. The composability between these agents, tools, and workflows allows you to build once and reuse in different scenarios.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;‚ö° &lt;a href="https://raw.githubusercontent.com/NVIDIA/NeMo-Agent-Toolkit/develop/docs/source/tutorials/customize-a-workflow.md"&gt;&lt;strong&gt;Rapid Development:&lt;/strong&gt;&lt;/a&gt; Start with a pre-built agent, tool, or workflow, and customize it to your needs. This allows you and your development teams to move quickly if you're already developing with agents.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üìà &lt;a href="https://raw.githubusercontent.com/NVIDIA/NeMo-Agent-Toolkit/develop/docs/source/workflows/profiler.md"&gt;&lt;strong&gt;Profiling:&lt;/strong&gt;&lt;/a&gt; Use the profiler to profile entire workflows down to the tool and agent level, track input/output tokens and timings, and identify bottlenecks. While we encourage you to wrap (decorate) every tool and agent to get the most out of the profiler, you have the freedom to integrate your tools, agents, and workflows to whatever level you want. You start small and go to where you believe you'll see the most value and expand from there.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üîé &lt;a href="https://raw.githubusercontent.com/NVIDIA/NeMo-Agent-Toolkit/develop/docs/source/workflows/observe/index.md"&gt;&lt;strong&gt;Observability:&lt;/strong&gt;&lt;/a&gt; Monitor and debug your workflows with dedicated integrations for popular observability platforms such as Phoenix, Weave, and Langfuse, plus compatibility with OpenTelemetry-based observability platforms. Track performance, trace execution flows, and gain insights into your agent behaviors.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üß™ &lt;a href="https://raw.githubusercontent.com/NVIDIA/NeMo-Agent-Toolkit/develop/docs/source/workflows/evaluate.md"&gt;&lt;strong&gt;Evaluation System:&lt;/strong&gt;&lt;/a&gt; Validate and maintain accuracy of agentic workflows with built-in evaluation tools.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üí¨ &lt;a href="https://raw.githubusercontent.com/NVIDIA/NeMo-Agent-Toolkit/develop/docs/source/quick-start/launching-ui.md"&gt;&lt;strong&gt;User Interface:&lt;/strong&gt;&lt;/a&gt; Use the NeMo Agent toolkit UI chat interface to interact with your agents, visualize output, and debug workflows.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üîó &lt;a href="https://raw.githubusercontent.com/NVIDIA/NeMo-Agent-Toolkit/develop/docs/source/workflows/mcp/index.md"&gt;&lt;strong&gt;Full MCP Support:&lt;/strong&gt;&lt;/a&gt; Compatible with &lt;a href="https://modelcontextprotocol.io/"&gt;Model Context Protocol (MCP)&lt;/a&gt;. You can use NeMo Agent toolkit as an &lt;a href="https://raw.githubusercontent.com/NVIDIA/NeMo-Agent-Toolkit/develop/docs/source/workflows/mcp/mcp-client.md"&gt;MCP client&lt;/a&gt; to connect to and use tools served by remote MCP servers. You can also use NeMo Agent toolkit as an &lt;a href="https://raw.githubusercontent.com/NVIDIA/NeMo-Agent-Toolkit/develop/docs/source/workflows/mcp/mcp-server.md"&gt;MCP server&lt;/a&gt; to publish tools via MCP.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;With NeMo Agent toolkit, you can move quickly, experiment freely, and ensure reliability across all your agent-driven projects.&lt;/p&gt; 
&lt;h2&gt;üöÄ Installation&lt;/h2&gt; 
&lt;p&gt;Before you begin using NeMo Agent Toolkit, ensure that you have Python 3.11 or 3.12 installed on your system.&lt;/p&gt; 
&lt;h3&gt;Stable Version&lt;/h3&gt; 
&lt;p&gt;To install the latest stable version of NeMo Agent Toolkit, run the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install nvidia-nat
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;NeMo Agent Toolkit has many optional dependencies which can be installed with the core package. Optional dependencies are grouped by framework and can be installed with the core package. For example, to install the LangChain plugin, run the following:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install nvidia-nat[langchain] # For LangChain
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or for all optional dependencies:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install nvidia-nat[all]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The full list of optional dependencies can be found &lt;a href="https://raw.githubusercontent.com/NVIDIA/NeMo-Agent-Toolkit/develop/docs/source/quick-start/installing.md#framework-integrations"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;From Source (For Running Examples)&lt;/h3&gt; 
&lt;p&gt;To run the examples, it's recommended to clone the repository and install from source. For instructions on how to do this, see the &lt;a href="https://raw.githubusercontent.com/NVIDIA/NeMo-Agent-Toolkit/develop/docs/source/quick-start/installing.md#installation-from-source"&gt;Installation from Source&lt;/a&gt; guide.&lt;/p&gt; 
&lt;h3&gt;Development Version&lt;/h3&gt; 
&lt;p&gt;More information on how to install the latest development version and contribute to the project can be found in the &lt;a href="https://raw.githubusercontent.com/NVIDIA/NeMo-Agent-Toolkit/develop/docs/source/resources/contributing.md"&gt;Contributing&lt;/a&gt; guide.&lt;/p&gt; 
&lt;h2&gt;üåü Hello World Example&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Ensure you have set the &lt;code&gt;NVIDIA_API_KEY&lt;/code&gt; environment variable to allow the example to use NVIDIA NIMs. An API key can be obtained by visiting &lt;a href="https://build.nvidia.com/"&gt;&lt;code&gt;build.nvidia.com&lt;/code&gt;&lt;/a&gt; and creating an account.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;export NVIDIA_API_KEY=&amp;lt;your_api_key&amp;gt;
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Create the NeMo Agent toolkit workflow configuration file. This file will define the agents, tools, and workflows that will be used in the example. Save the following as &lt;code&gt;workflow.yaml&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-yaml"&gt;functions:
   # Add a tool to search wikipedia
   wikipedia_search:
      _type: wiki_search
      max_results: 2

llms:
   # Tell NeMo Agent toolkit which LLM to use for the agent
   nim_llm:
      _type: nim
      model_name: meta/llama-3.1-70b-instruct
      temperature: 0.0

workflow:
   # Use an agent that 'reasons' and 'acts'
   _type: react_agent
   # Give it access to our wikipedia search tool
   tool_names: [wikipedia_search]
   # Tell it which LLM to use
   llm_name: nim_llm
   # Make it verbose
   verbose: true
   # Retry up to 3 times
   parse_agent_response_max_retries: 3
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run the Hello World example using the &lt;code&gt;nat&lt;/code&gt; CLI and the &lt;code&gt;workflow.yaml&lt;/code&gt; file.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;nat run --config_file workflow.yaml --input "List five subspecies of Aardvarks"
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This will run the workflow and output the results to the console.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-console"&gt;Workflow Result:
['Here are five subspecies of Aardvarks:\n\n1. Orycteropus afer afer (Southern aardvark)\n2. O. a. adametzi  Grote, 1921 (Western aardvark)\n3. O. a. aethiopicus  Sundevall, 1843\n4. O. a. angolensis  Zukowsky &amp;amp; Haltenorth, 1957\n5. O. a. erikssoni  L√∂nnberg, 1906']
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;üìö Additional Resources&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìñ &lt;a href="https://docs.nvidia.com/nemo/agent-toolkit/latest"&gt;Documentation&lt;/a&gt;: Explore the full documentation for NeMo Agent toolkit.&lt;/li&gt; 
 &lt;li&gt;üß≠ &lt;a href="https://raw.githubusercontent.com/NVIDIA/NeMo-Agent-Toolkit/develop/docs/source/quick-start/installing.md"&gt;Get Started Guide&lt;/a&gt;: Set up your environment and start building with NeMo Agent toolkit.&lt;/li&gt; 
 &lt;li&gt;üß™ &lt;a href="https://raw.githubusercontent.com/NVIDIA/NeMo-Agent-Toolkit/develop/examples/README.md"&gt;Examples&lt;/a&gt;: Explore examples of NeMo Agent toolkit workflows located in the &lt;a href="https://raw.githubusercontent.com/NVIDIA/NeMo-Agent-Toolkit/develop/examples"&gt;&lt;code&gt;examples&lt;/code&gt;&lt;/a&gt; directory of the source repository.&lt;/li&gt; 
 &lt;li&gt;üõ†Ô∏è &lt;a href="https://raw.githubusercontent.com/NVIDIA/NeMo-Agent-Toolkit/develop/docs/source/tutorials/customize-a-workflow.md"&gt;Create and Customize NeMo Agent toolkit Workflows&lt;/a&gt;: Learn how to create and customize NeMo Agent toolkit workflows.&lt;/li&gt; 
 &lt;li&gt;üéØ &lt;a href="https://raw.githubusercontent.com/NVIDIA/NeMo-Agent-Toolkit/develop/docs/source/workflows/evaluate.md"&gt;Evaluate with NeMo Agent toolkit&lt;/a&gt;: Learn how to evaluate your NeMo Agent toolkit workflows.&lt;/li&gt; 
 &lt;li&gt;üÜò &lt;a href="https://raw.githubusercontent.com/NVIDIA/NeMo-Agent-Toolkit/develop/docs/source/troubleshooting.md"&gt;Troubleshooting&lt;/a&gt;: Get help with common issues.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üìä Component Overview&lt;/h2&gt; 
&lt;p&gt;The following diagram illustrates the key components of NeMo Agent toolkit and how they interact. It provides a high-level view of the architecture, including agents, plugins, workflows, and user interfaces. Use this as a reference to understand how to integrate and extend NeMo Agent toolkit in your projects.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/NVIDIA/NeMo-Agent-Toolkit/develop/docs/source/_static/gitdiagram.png" alt="NeMo Agent toolkit Components Diagram" /&gt;&lt;/p&gt; 
&lt;h2&gt;üõ£Ô∏è Roadmap&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Integrate with &lt;a href="https://github.com/NVIDIA-AI-Blueprints/data-flywheel"&gt;NeMo DataFlywheel&lt;/a&gt; for continuous model improvement from production data.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Add support for &lt;a href="https://google.github.io/adk-docs/"&gt;Google ADK&lt;/a&gt; framework.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Add an agent optimizer to auto-tune hyperparameters and prompts to maximize performance.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; MCP authorization and streamable HTTP support.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; Integration with &lt;a href="https://github.com/NVIDIA/NeMo-Guardrails"&gt;NeMo Guardrails&lt;/a&gt; to secure any function in an agent workflow.&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" disabled /&gt; End-to-end acceleration using intelligent integrations with &lt;a href="https://github.com/ai-dynamo/dynamo"&gt;NVIDIA Dynamo&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üí¨ Feedback&lt;/h2&gt; 
&lt;p&gt;We would love to hear from you! Please file an issue on &lt;a href="https://github.com/NVIDIA/NeMo-Agent-Toolkit/issues"&gt;GitHub&lt;/a&gt; if you have any feedback or feature requests.&lt;/p&gt; 
&lt;h2&gt;ü§ù Acknowledgements&lt;/h2&gt; 
&lt;p&gt;We would like to thank the following open source projects that made NeMo Agent toolkit possible:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/crewAIInc/crewAI"&gt;CrewAI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/tiangolo/fastapi"&gt;FastAPI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/langchain-ai/langchain"&gt;LangChain&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/run-llama/llama_index"&gt;Llama-Index&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/mem0ai/mem0"&gt;Mem0ai&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/explodinggradients/ragas"&gt;Ragas&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/microsoft/semantic-kernel"&gt;Semantic Kernel&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/astral-sh/uv"&gt;uv&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>dataease/SQLBot</title>
      <link>https://github.com/dataease/SQLBot</link>
      <description>&lt;p&gt;Âü∫‰∫éÂ§ßÊ®°ÂûãÂíå RAG ÁöÑÊô∫ËÉΩÈóÆÊï∞Á≥ªÁªü„ÄÇText-to-SQL Generation via LLMs using RAG.&lt;/p&gt;&lt;hr&gt;&lt;p align="center"&gt;&lt;img src="https://resource-fit2cloud-com.oss-cn-hangzhou.aliyuncs.com/sqlbot/sqlbot.png" alt="SQLBot" width="300" /&gt;&lt;/p&gt; 
&lt;h3 align="center"&gt;Âü∫‰∫éÂ§ßÊ®°ÂûãÂíå RAG ÁöÑÊô∫ËÉΩÈóÆÊï∞Á≥ªÁªü&lt;/h3&gt; 
&lt;p align="center"&gt; &lt;a href="https://github.com/dataease/SQLBot/releases/latest"&gt;&lt;img src="https://img.shields.io/github/v/release/dataease/SQLBot" alt="Latest release" /&gt;&lt;/a&gt; &lt;a href="https://github.com/dataease/SQLBot"&gt;&lt;img src="https://img.shields.io/github/stars/dataease/SQLBot?color=%231890FF&amp;amp;style=flat-square" alt="Stars" /&gt;&lt;/a&gt; &lt;a href="https://hub.docker.com/r/dataease/SQLbot"&gt;&lt;img src="https://img.shields.io/docker/pulls/dataease/sqlbot?label=downloads" alt="Download" /&gt;&lt;/a&gt;&lt;br /&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;SQLBot ÊòØ‰∏ÄÊ¨æÂü∫‰∫éÂ§ßÊ®°ÂûãÂíå RAG ÁöÑÊô∫ËÉΩÈóÆÊï∞Á≥ªÁªü„ÄÇSQLBot ÁöÑ‰ºòÂäøÂåÖÊã¨Ôºö&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ÂºÄÁÆ±Âç≥Áî®&lt;/strong&gt;: Âè™ÈúÄÈÖçÁΩÆÂ§ßÊ®°ÂûãÂíåÊï∞ÊçÆÊ∫êÂç≥ÂèØÂºÄÂêØÈóÆÊï∞‰πãÊóÖÔºåÈÄöËøáÂ§ßÊ®°ÂûãÂíå RAG ÁöÑÁªìÂêàÊù•ÂÆûÁé∞È´òË¥®ÈáèÁöÑ text2sqlÔºõ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Êòì‰∫éÈõÜÊàê&lt;/strong&gt;: ÊîØÊåÅÂø´ÈÄüÂµåÂÖ•Âà∞Á¨¨‰∏âÊñπ‰∏öÂä°Á≥ªÁªüÔºå‰πüÊîØÊåÅË¢´ n8n„ÄÅMaxKB„ÄÅDify„ÄÅCoze Á≠â AI Â∫îÁî®ÂºÄÂèëÂπ≥Âè∞ÈõÜÊàêË∞ÉÁî®ÔºåËÆ©ÂêÑÁ±ªÂ∫îÁî®Âø´ÈÄüÊã•ÊúâÊô∫ËÉΩÈóÆÊï∞ËÉΩÂäõÔºõ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ÂÆâÂÖ®ÂèØÊéß&lt;/strong&gt;: Êèê‰æõÂü∫‰∫éÂ∑•‰ΩúÁ©∫Èó¥ÁöÑËµÑÊ∫êÈöîÁ¶ªÊú∫Âà∂ÔºåËÉΩÂ§üÂÆûÁé∞ÁªÜÁ≤íÂ∫¶ÁöÑÊï∞ÊçÆÊùÉÈôêÊéßÂà∂„ÄÇ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Âø´ÈÄüÂºÄÂßã&lt;/h2&gt; 
&lt;h3&gt;ÂÆâË£ÖÈÉ®ÁΩ≤&lt;/h3&gt; 
&lt;p&gt;ÂáÜÂ§á‰∏ÄÂè∞ Linux ÊúçÂä°Âô®ÔºåÊâßË°å‰ª•‰∏ã‰∏ÄÈîÆÂÆâË£ÖËÑöÊú¨„ÄÇ&lt;br /&gt; Âú®ËøêË°å SQLBot ÂâçÔºåËØ∑Á°Æ‰øùÂ∑≤ÂÆâË£ÖÂ•Ω &lt;a href="https://docs.docker.com/get-docker/"&gt;Docker&lt;/a&gt; Âíå &lt;a href="https://docs.docker.com/compose/install/"&gt;Docker Compose&lt;/a&gt;„ÄÇ&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# ÂàõÂª∫ÁõÆÂΩï
mkdir -p /opt/sqlbot
cd /opt/sqlbot

# ‰∏ãËΩΩ docker-compose.yaml
curl -o docker-compose.yaml https://raw.githubusercontent.com/dataease/SQLBot/main/docker-compose.yaml

# ÂêØÂä®ÊúçÂä°
docker compose up -d
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;‰Ω†‰πüÂèØ‰ª•ÈÄöËøá &lt;a href="https://apps.fit2cloud.com/1panel"&gt;1Panel Â∫îÁî®ÂïÜÂ∫ó&lt;/a&gt; Âø´ÈÄüÈÉ®ÁΩ≤ SQLBotÔºõ&lt;/p&gt; 
&lt;h3&gt;ËÆøÈóÆÊñπÂºè&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Âú®ÊµèËßàÂô®‰∏≠ÊâìÂºÄ: http://&amp;lt;‰Ω†ÁöÑÊúçÂä°Âô®IP&amp;gt;:8000/&lt;/li&gt; 
 &lt;li&gt;Áî®Êà∑Âêç: admin&lt;/li&gt; 
 &lt;li&gt;ÂØÜÁ†Å: SQLBot@123456&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ËÅîÁ≥ªÊàë‰ª¨&lt;/h3&gt; 
&lt;p&gt;Â¶Ç‰Ω†ÊúâÊõ¥Â§öÈóÆÈ¢òÔºåÂèØ‰ª•Âä†ÂÖ•Êàë‰ª¨ÁöÑÊäÄÊúØ‰∫§ÊµÅÁæ§‰∏éÊàë‰ª¨‰∫§ÊµÅ„ÄÇ&lt;/p&gt; 
&lt;img width="396" height="396" alt="contact_me_qr" src="https://github.com/user-attachments/assets/2594ff29-5426-4457-b051-279855610030" /&gt; 
&lt;h2&gt;UI Â±ïÁ§∫&lt;/h2&gt;  
&lt;img alt="q&amp;amp;a" src="https://github.com/user-attachments/assets/55526514-52f3-4cfe-98ec-08a986259280" /&gt;  
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.star-history.com/#dataease/sqlbot&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=dataease/sqlbot&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;È£ûËá¥‰∫ëÊóó‰∏ãÁöÑÂÖ∂‰ªñÊòéÊòüÈ°πÁõÆ&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/dataease/dataease/"&gt;DataEase&lt;/a&gt; - ‰∫∫‰∫∫ÂèØÁî®ÁöÑÂºÄÊ∫ê BI Â∑•ÂÖ∑&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/1panel-dev/1panel/"&gt;1Panel&lt;/a&gt; - Áé∞‰ª£Âåñ„ÄÅÂºÄÊ∫êÁöÑ Linux ÊúçÂä°Âô®ËøêÁª¥ÁÆ°ÁêÜÈù¢Êùø&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/1panel-dev/MaxKB/"&gt;MaxKB&lt;/a&gt; - Âº∫Â§ßÊòìÁî®ÁöÑ‰ºÅ‰∏öÁ∫ßÊô∫ËÉΩ‰ΩìÂπ≥Âè∞&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/jumpserver/jumpserver/"&gt;JumpServer&lt;/a&gt; - ÂπøÂèóÊ¨¢ËøéÁöÑÂºÄÊ∫êÂ†°ÂûíÊú∫&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/halo-dev/halo/"&gt;Halo&lt;/a&gt; - Âº∫Â§ßÊòìÁî®ÁöÑÂºÄÊ∫êÂª∫Á´ôÂ∑•ÂÖ∑&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/metersphere/metersphere/"&gt;MeterSphere&lt;/a&gt; - Êñ∞‰∏Ä‰ª£ÁöÑÂºÄÊ∫êÊåÅÁª≠ÊµãËØïÂ∑•ÂÖ∑&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Êú¨‰ªìÂ∫ìÈÅµÂæ™ &lt;a href="https://raw.githubusercontent.com/dataease/SQLBot/main/LICENSE"&gt;FIT2CLOUD Open Source License&lt;/a&gt; ÂºÄÊ∫êÂçèËÆÆÔºåËØ•ËÆ∏ÂèØËØÅÊú¨Ë¥®‰∏äÊòØ GPLv3Ôºå‰ΩÜÊúâ‰∏Ä‰∫õÈ¢ùÂ§ñÁöÑÈôêÂà∂„ÄÇ&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>python/typeshed</title>
      <link>https://github.com/python/typeshed</link>
      <description>&lt;p&gt;Collection of library stubs for Python, with static types&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;typeshed&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://github.com/python/typeshed/actions/workflows/tests.yml"&gt;&lt;img src="https://github.com/python/typeshed/actions/workflows/tests.yml/badge.svg?sanitize=true" alt="Tests" /&gt;&lt;/a&gt; &lt;a href="https://gitter.im/python/typing?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&amp;amp;utm_content=badge"&gt;&lt;img src="https://badges.gitter.im/python/typing.svg?sanitize=true" alt="Chat at https://gitter.im/python/typing" /&gt;&lt;/a&gt; &lt;a href="https://github.com/python/typeshed/raw/main/CONTRIBUTING.md"&gt;&lt;img src="https://img.shields.io/badge/pull%20requests-welcome-brightgreen.svg?sanitize=true" alt="Pull Requests Welcome" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;About&lt;/h2&gt; 
&lt;p&gt;Typeshed contains external type annotations for the Python standard library and Python builtins, as well as third party packages as contributed by people external to those projects.&lt;/p&gt; 
&lt;p&gt;This data can e.g. be used for static analysis, type checking, type inference, and autocompletion.&lt;/p&gt; 
&lt;p&gt;For information on how to use typeshed, read below. Information for contributors can be found in &lt;a href="https://raw.githubusercontent.com/python/typeshed/main/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt;. &lt;strong&gt;Please read it before submitting pull requests; do not report issues with annotations to the project the stubs are for, but instead report them here to typeshed.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Further documentation on stub files, typeshed, and Python's typing system in general, can also be found at &lt;a href="https://typing.readthedocs.io/en/latest/"&gt;https://typing.readthedocs.io/en/latest/&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Typeshed supports Python versions 3.9 to 3.14.&lt;/p&gt; 
&lt;h2&gt;Using&lt;/h2&gt; 
&lt;p&gt;If you're just using a type checker (e.g. &lt;a href="https://github.com/python/mypy/"&gt;mypy&lt;/a&gt;, &lt;a href="https://github.com/microsoft/pyright"&gt;pyright&lt;/a&gt;, or PyCharm's built-in type checker), as opposed to developing it, you don't need to interact with the typeshed repo at all: a copy of standard library part of typeshed is bundled with type checkers. And type stubs for third party packages and modules you are using can be installed from PyPI. For example, if you are using &lt;code&gt;html5lib&lt;/code&gt; and &lt;code&gt;requests&lt;/code&gt;, you can install the type stubs using&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;$ pip install types-html5lib types-requests
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;These PyPI packages follow &lt;a href="http://www.python.org/dev/peps/pep-0561/"&gt;PEP 561&lt;/a&gt; and are automatically released (up to once a day) by &lt;a href="https://github.com/typeshed-internal/stub_uploader"&gt;typeshed internal machinery&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Type checkers should be able to use these stub packages when installed. For more details, see the documentation for your type checker.&lt;/p&gt; 
&lt;h3&gt;Package versioning for third-party stubs&lt;/h3&gt; 
&lt;p&gt;Version numbers of third-party stub packages consist of at least four parts. All parts of the stub version, except for the last part, correspond to the version of the runtime package being stubbed. For example, if the &lt;code&gt;types-foo&lt;/code&gt; package has version &lt;code&gt;1.2.0.20240309&lt;/code&gt;, this guarantees that the &lt;code&gt;types-foo&lt;/code&gt; package contains stubs targeted against &lt;code&gt;foo==1.2.*&lt;/code&gt; and tested against the latest version of &lt;code&gt;foo&lt;/code&gt; matching that specifier. In this example, the final element of the version number (20240309) indicates that the stub package was pushed on March 9, 2024.&lt;/p&gt; 
&lt;p&gt;At typeshed, we try to keep breaking changes to a minimum. However, due to the nature of stubs, any version bump can introduce changes that might make your code fail to type check.&lt;/p&gt; 
&lt;p&gt;There are several strategies available for specifying the version of a stubs package you're using, each with its own tradeoffs:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Use the same bounds that you use for the package being stubbed. For example, if you use &lt;code&gt;requests&amp;gt;=2.30.0,&amp;lt;2.32&lt;/code&gt;, you can use &lt;code&gt;types-requests&amp;gt;=2.30.0,&amp;lt;2.32&lt;/code&gt;. This ensures that the stubs are compatible with the package you are using, but it carries a small risk of breaking type checking due to changes in the stubs.&lt;/p&gt; &lt;p&gt;Another risk of this strategy is that stubs often lag behind the package being stubbed. You might want to force the package being stubbed to a certain minimum version because it fixes a critical bug, but if correspondingly updated stubs have not been released, your type checking results may not be fully accurate.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Pin the stubs to a known good version and update the pin from time to time (either manually, or using a tool such as dependabot or renovate).&lt;/p&gt; &lt;p&gt;For example, if you use &lt;code&gt;types-requests==2.31.0.1&lt;/code&gt;, you can have confidence that upgrading dependencies will not break type checking. However, you will miss out on improvements in the stubs that could potentially improve type checking until you update the pin. This strategy also has the risk that the stubs you are using might become incompatible with the package being stubbed.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Don't pin the stubs. This is the option that demands the least work from you when it comes to updating version pins, and has the advantage that you will automatically benefit from improved stubs whenever a new version of the stubs package is released. However, it carries the risk that the stubs become incompatible with the package being stubbed.&lt;/p&gt; &lt;p&gt;For example, if a new major version of the package is released, there's a chance the stubs might be updated to reflect the new version of the runtime package before you update the package being stubbed.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;You can also switch between the different strategies as needed. For example, you could default to strategy (1), but fall back to strategy (2) when a problem arises that can't easily be fixed.&lt;/p&gt; 
&lt;h3&gt;The &lt;code&gt;_typeshed&lt;/code&gt; package&lt;/h3&gt; 
&lt;p&gt;typeshed includes a package &lt;code&gt;_typeshed&lt;/code&gt; as part of the standard library. This package and its submodules contain utility types, but are not available at runtime. For more information about how to use this package, &lt;a href="https://github.com/python/typeshed/tree/main/stdlib/_typeshed"&gt;see the &lt;code&gt;stdlib/_typeshed&lt;/code&gt; directory&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Discussion&lt;/h2&gt; 
&lt;p&gt;If you've run into behavior in the type checker that suggests the type stubs for a given library are incorrect or incomplete, we want to hear from you!&lt;/p&gt; 
&lt;p&gt;Our main forum for discussion is the project's &lt;a href="https://github.com/python/typeshed/issues"&gt;GitHub issue tracker&lt;/a&gt;. This is the right place to start a discussion of any of the above or most any other topic concerning the project.&lt;/p&gt; 
&lt;p&gt;If you have general questions about typing with Python, or you need a review of your type annotations or stubs outside of typeshed, head over to &lt;a href="https://github.com/python/typing/discussions"&gt;our discussion forum&lt;/a&gt;. For less formal discussion, try the typing chat room on &lt;a href="https://gitter.im/python/typing"&gt;gitter.im&lt;/a&gt;. Some typeshed maintainers are almost always present; feel free to find us there and we're happy to chat. Substantive technical discussion will be directed to the issue tracker.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Arindam200/awesome-ai-apps</title>
      <link>https://github.com/Arindam200/awesome-ai-apps</link>
      <description>&lt;p&gt;A collection of projects showcasing RAG, agents, workflows, and other AI use cases&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Awesome AI Apps &lt;a href="https://awesome.re"&gt;&lt;img src="https://awesome.re/badge.svg?sanitize=true" alt="Awesome" /&gt;&lt;/a&gt;&lt;/h1&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/assets/banner.png" alt="Banner" /&gt;&lt;/p&gt; 
&lt;p&gt;This repository is a comprehensive collection of practical examples, tutorials, and recipes for building powerful LLM-powered applications. From simple chatbots to advanced AI agents, these projects serve as a guide for developers working with various AI frameworks and tools.&lt;/p&gt; 
&lt;p&gt;Powered by &lt;a href="https://dub.sh/nebius"&gt;Nebius AI Studio&lt;/a&gt; - your one-stop platform for building and deploying AI applications.&lt;/p&gt; 
&lt;h2&gt;üöÄ Featured AI Agent Frameworks&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://google.github.io/adk-docs/"&gt;&lt;img src="https://raw.githubusercontent.com/google/adk-python/main/assets/agent-development-kit.png" alt="Google ADK logo" width="20" height="20" /&gt; Google Agent Development Kit (ADK)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://openai.github.io/openai-agents-python/"&gt;&lt;img src="https://avatars.githubusercontent.com/u/14957082?s=200&amp;amp;v=4" alt="OpenAI Agents SDK logo" width="20" height="20" /&gt; OpenAI Agents SDK&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://python.langchain.com/"&gt;&lt;img src="https://cdn.simpleicons.org/langchain" alt="LangChain logo" width="25" height="25" /&gt; LangChain &lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.llamaindex.ai/"&gt;&lt;img src="https://avatars.githubusercontent.com/u/130722866?s=200&amp;amp;v=4" alt="Llamaindex logo" width="20" height="20" /&gt; LlamaIndex&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.agno.com/"&gt;&lt;img src="https://avatars.githubusercontent.com/u/104874993?s=48&amp;amp;v=4" alt="Agno logo" width="20" height="20" /&gt; Agno&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.crewai.com/"&gt;&lt;img src="https://cdn.prod.website-files.com/66cf2bfc3ed15b02da0ca770/66d07240057721394308addd_Logo%20(1).svg?sanitize=true" alt="CrewAI logo" width="35" height="25" /&gt; CrewAI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://strandsagents.com/"&gt;&lt;img src="https://avatars.githubusercontent.com/u/209155962?s=200&amp;amp;v=4" alt="AWS Strands Agents logo" width="20" height="20" /&gt; AWS Strands Agent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://ai.pydantic.dev/"&gt;&lt;img src="https://avatars.githubusercontent.com/u/110818415?s=200&amp;amp;v=4" alt="Pydantic AI logo" width="20" height="20" /&gt; Pydantic AI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.camel-ai.org/"&gt;&lt;img src="https://avatars.githubusercontent.com/u/134388954?s=200&amp;amp;v=4" alt="Camel AI logo" width="20" height="20" /&gt; CAMEL‚ÄëAI&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üß© Starter Agents&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Quick-start agents for learning and extending:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/starter_ai_agents/agno_starter"&gt;Agno HackerNews Analysis&lt;/a&gt; - Agno-based agent for trend analysis on HackerNews.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/starter_ai_agents/openai_agents_sdk"&gt;OpenAI SDK Starter&lt;/a&gt; - OpenAI Agents SDK based email helper &amp;amp; haiku writer.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/starter_ai_agents/llamaindex_starter"&gt;LlamaIndex Task Manager&lt;/a&gt; - LlamaIndex-powered task assistant.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/starter_ai_agents/crewai_starter"&gt;CrewAI Research Crew&lt;/a&gt; - Multi-agent research team.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/starter_ai_agents/pydantic_starter"&gt;PydanticAI Weather Bot&lt;/a&gt; - Real-time weather info.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/starter_ai_agents/langchain_langgraph_starter"&gt;LangChain-LangGraph Starter&lt;/a&gt; - LangChain + LangGraph starter.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/starter_ai_agents/aws_strands_starter"&gt;AWS Strands Agent Starter&lt;/a&gt; - Weather report Agent.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/starter_ai_agents/camel_ai_starter"&gt;Camel AI Starter&lt;/a&gt; - Performance benchmarking tool that compares the performance of various AI models.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ü™∂ Simple Agents&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Straightforward, practical use-cases:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/simple_ai_agents/finance_agent"&gt;Finance Agent&lt;/a&gt; - Tracks live stock &amp;amp; market data.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/simple_ai_agents/human_in_the_loop_agent"&gt;Human-in-the-Loop Agent&lt;/a&gt; - HITL actions for safe AI tasks.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/simple_ai_agents/newsletter_agent"&gt;Newsletter Generator&lt;/a&gt; - AI newsletter builder with Firecrawl.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/simple_ai_agents/reasoning_agent"&gt;Reasoning Agent&lt;/a&gt; - Financial reasoning step-by-step.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/simple_ai_agents/agno_ui_agent"&gt;Agno UI Example&lt;/a&gt; - UI for web &amp;amp; finance agents.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/simple_ai_agents/mastra_ai_weather_agent"&gt;Mastra Weather Bot&lt;/a&gt; - Weather updates with Mastra AI.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/simple_ai_agents/cal_scheduling_agent"&gt;Calendar Assistant&lt;/a&gt; - Calendar scheduling with Cal.com.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/simple_ai_agents/memory_agent"&gt;Memory Agent&lt;/a&gt; - Simple Memory Agent implementation with Agno.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/simple_ai_agents/browser_agent"&gt;Web Automation Agent&lt;/a&gt; - Simple Browser Agent implementation with Nebius &amp;amp; browser use.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/simple_ai_agents/nebius_chat"&gt;Nebius Chat&lt;/a&gt; - Nebius AI Studio Chat interface.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/simple_ai_agents/talk_to_db"&gt;Talk to Your DB&lt;/a&gt; - Talk to your Database with GibsonAI &amp;amp; Langchain&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üóÇÔ∏è MCP Agents&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Examples using Model Context Protocol:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/mcp_ai_agents/doc_mcp"&gt;Doc-MCP&lt;/a&gt; - Semantic RAG docs &amp;amp; Q&amp;amp;A.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/mcp_ai_agents/langchain_langgraph_mcp_agent"&gt;LangGraph MCP Agent&lt;/a&gt; - LangChain ReAct agent with Couchbase.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/mcp_ai_agents/github_mcp_agent"&gt;GitHub MCP Agent&lt;/a&gt; - Repo insights via MCP.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/mcp_ai_agents/mcp_starter"&gt;MCP Starter&lt;/a&gt; - GitHub repo analyzer starter.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/mcp_ai_agents/docs_qna_agent"&gt;Talk to your Docs&lt;/a&gt; - Documentation QnA Agent&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üìö RAG Applications&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Retrieve-augmented generation examples:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/rag_apps/agentic_rag"&gt;Agentic RAG&lt;/a&gt; - Agentic RAG with Agno &amp;amp; GPT 5.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/rag_apps/resume_optimizer"&gt;Resume Optimizer&lt;/a&gt; - Boost resumes with AI.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/rag_apps/llamaIndex_starter"&gt;LlamaIndex RAG Starter&lt;/a&gt; - LlamaIndex + Nebius RAG starter.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/rag_apps/pdf_rag_analyser"&gt;PDF RAG Analyzer&lt;/a&gt; - Chat with multiple PDFs.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/rag_apps/qwen3_rag"&gt;Qwen3 RAG Chat&lt;/a&gt; - PDF chatbot with Streamlit.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/rag_apps/chat_with_code"&gt;Chat with Code&lt;/a&gt; - Conversational code explorer.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/rag_apps/gemma_ocr/"&gt;Gemma3 OCR&lt;/a&gt; - OCR-based document and image processor using Gemma3&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üî¨ Advanced Agents&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Complex pipelines for end-to-end workflows:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/advance_ai_agents/deep_researcher_agent"&gt;Deep Researcher&lt;/a&gt; - Multi-stage research with Agno &amp;amp; Scrapegraph AI.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/advance_ai_agents/candidate_analyser"&gt;Candilyzer&lt;/a&gt; - Analyze GitHub/LinkedIn profiles.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/advance_ai_agents/job_finder_agent"&gt;Job Finder&lt;/a&gt; - LinkedIn job search with Bright Data.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/advance_ai_agents/trend_analyzer_agent"&gt;AI Trend Analyzer&lt;/a&gt; - AI trend mining with Google ADK.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/advance_ai_agents/conference_talk_abstract_generator"&gt;Conference Talk Generator&lt;/a&gt; - Draft talk abstracts with Google ADK &amp;amp; Couchbase.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/advance_ai_agents/finance_service_agent"&gt;Finance Service Agent&lt;/a&gt; - FastAPI server for stock data and predictions with Agno.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/advance_ai_agents/price_monitoring_agent"&gt;Price Monitoring Agent&lt;/a&gt; - Price monitoring and alerting Agent powered by CrewAi, Twilio &amp;amp; Nebius.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/advance_ai_agents/startup_idea_validator_agent"&gt;Startup Idea Validator Agent&lt;/a&gt; - Agentic Workflow to validate and analyze startup ideas.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- * [YouTube to Blog](advance_ai_agents/youtube_to_blog_agent) - Auto-blog from YouTube videos. --&gt; 
&lt;h2&gt;üì∫ Playlist of Demo Videos &amp;amp; Tutorials&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/playlist?list=PLMZM1DAlf0Lolxax4L2HS54Me8gn1gkz4"&gt;Build with MCP&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/playlist?list=PLMZM1DAlf0LqixhAG9BDk4O_FjqnaogK8"&gt;Build AI Agents&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.youtube.com/playlist?list=PL2ambAOfYA6-LDz0KpVKu9vJKAqhv0KKI"&gt;AI Agents, MCP and more...&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python 3.10 or higher&lt;/li&gt; 
 &lt;li&gt;Git&lt;/li&gt; 
 &lt;li&gt;pip (Python package manager) or uv&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Installation Steps&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Clone the repository&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/Arindam200/awesome-ai-apps.git
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Navigate to the desired project directory&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cd awesome-ai-apps/starter_ai_agents/agno_starter
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install the required dependencies&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Follow project-specific instructions&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Each project has its own README.md with detailed setup and usage instructions&lt;/li&gt; 
   &lt;li&gt;Make sure to read the project-specific documentation before running the application&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;ü§ù Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions from the community! Whether you're a beginner or an expert, your examples and tutorials can help others learn and grow. Here's how you can contribute:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Submit a Pull Request with your LLM application example&lt;/li&gt; 
 &lt;li&gt;Add detailed documentation and setup instructions&lt;/li&gt; 
 &lt;li&gt;Include requirements.txt or environment.yml&lt;/li&gt; 
 &lt;li&gt;Share your experience and best practices&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;üìú License&lt;/h2&gt; 
&lt;p&gt;This repository is licensed under the &lt;a href="https://raw.githubusercontent.com/Arindam200/awesome-ai-apps/main/LICENSE"&gt;MIT License&lt;/a&gt;. Feel free to use and modify the examples for your projects.&lt;/p&gt; 
&lt;h2&gt;Thank You for the Support! üôè&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.star-history.com/#Arindam200/awesome-ai-apps&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=Arindam200/awesome-ai-apps&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>SamuelSchmidgall/AgentLaboratory</title>
      <link>https://github.com/SamuelSchmidgall/AgentLaboratory</link>
      <description>&lt;p&gt;Agent Laboratory is an end-to-end autonomous research workflow meant to assist you as the human researcher toward implementing your research ideas&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Agent Laboratory: Using LLM Agents as Research Assistants&lt;/h1&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/SamuelSchmidgall/AgentLaboratory/main/media/AgentLabLogo.png" alt="Demonstration of the flow of AgentClinic" style="width: 99%;" /&gt; &lt;/p&gt; 
&lt;p align="center"&gt; „ÄêEnglish | &lt;a href="https://raw.githubusercontent.com/SamuelSchmidgall/AgentLaboratory/main/readme/README-chinese.md"&gt;‰∏≠Êñá&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/SamuelSchmidgall/AgentLaboratory/main/readme/README-japanese.md"&gt;Êó•Êú¨Ë™û&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/SamuelSchmidgall/AgentLaboratory/main/readme/README-korean.md"&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/SamuelSchmidgall/AgentLaboratory/main/readme/README-filipino.md"&gt;Filipino&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/SamuelSchmidgall/AgentLaboratory/main/readme/README-french.md"&gt;Fran√ßais&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/SamuelSchmidgall/AgentLaboratory/main/readme/README-slovak.md"&gt;Slovenƒçina&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/SamuelSchmidgall/AgentLaboratory/main/readme/README-portugese.md"&gt;Portugu√™s&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/SamuelSchmidgall/AgentLaboratory/main/readme/README-spanish.md"&gt;Espa√±ol&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/SamuelSchmidgall/AgentLaboratory/main/readme/README-turkish.md"&gt;T√ºrk√ße&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/SamuelSchmidgall/AgentLaboratory/main/readme/README-hindi.md"&gt;‡§π‡§ø‡§Ç‡§¶‡•Ä&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/SamuelSchmidgall/AgentLaboratory/main/readme/README-bengali.md"&gt;‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/SamuelSchmidgall/AgentLaboratory/main/readme/README-vietnamese.md"&gt;Ti·∫øng Vi·ªát&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/SamuelSchmidgall/AgentLaboratory/main/readme/README-russian.md"&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/SamuelSchmidgall/AgentLaboratory/main/readme/README-arabic.md"&gt;ÿßŸÑÿπÿ±ÿ®Ÿäÿ©&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/SamuelSchmidgall/AgentLaboratory/main/readme/README-farsi.md"&gt;ŸÅÿßÿ±ÿ≥€å&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/SamuelSchmidgall/AgentLaboratory/main/readme/README-italian.md"&gt;Italiano&lt;/a&gt;„Äë &lt;/p&gt; 
&lt;p align="center"&gt; „Äêüìù &lt;a href="https://arxiv.org/pdf/2501.04227"&gt;Paper&lt;/a&gt; | üåê &lt;a href="https://agentlaboratory.github.io/"&gt;Website&lt;/a&gt; | üåê &lt;a href="https://agentrxiv.github.io/"&gt;AgentRxiv Website&lt;/a&gt; | üíª &lt;a href="https://github.com/SamuelSchmidgall/AgentLaboratory"&gt;Software&lt;/a&gt; | üì∞ &lt;a href="https://agentlaboratory.github.io/#citation-ref"&gt;Citation&lt;/a&gt;„Äë &lt;/p&gt; 
&lt;h3&gt;News&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;[March/24/2025] üéâ üéä üéâ Now introducing &lt;strong&gt;AgentRxiv&lt;/strong&gt;, a framework where autonomous research agents can upload, retrieve, and build on each other‚Äôs research. This allows agents to make cumulative progress on their research.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üìñ Overview&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Agent Laboratory&lt;/strong&gt; is an end-to-end autonomous research workflow meant to assist &lt;strong&gt;you&lt;/strong&gt; as the human researcher toward &lt;strong&gt;implementing your research ideas&lt;/strong&gt;. Agent Laboratory consists of specialized agents driven by large language models to support you through the entire research workflow‚Äîfrom conducting literature reviews and formulating plans to executing experiments and writing comprehensive reports.&lt;/li&gt; 
 &lt;li&gt;This system is not designed to replace your creativity but to complement it, enabling you to focus on ideation and critical thinking while automating repetitive and time-intensive tasks like coding and documentation. By accommodating varying levels of computational resources and human involvement, Agent Laboratory aims to accelerate scientific discovery and optimize your research productivity.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/SamuelSchmidgall/AgentLaboratory/main/media/AgentLab.png" alt="Demonstration of the flow of AgentClinic" style="width: 99%;" /&gt; &lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Agent Laboratory also supports &lt;strong&gt;AgentRxiv&lt;/strong&gt;, a framework where autonomous research agents can upload, retrieve, and build on each other‚Äôs research. This allows agents to make cumulative progress on their research.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/SamuelSchmidgall/AgentLaboratory/main/media/agentrxiv.png" alt="Demonstration of the flow of AgentClinic" style="width: 99%;" /&gt; &lt;/p&gt; 
&lt;h3&gt;üî¨ How does Agent Laboratory work?&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Agent Laboratory consists of three primary phases that systematically guide the research process: (1) Literature Review, (2) Experimentation, and (3) Report Writing. During each phase, specialized agents driven by LLMs collaborate to accomplish distinct objectives, integrating external tools like arXiv, Hugging Face, Python, and LaTeX to optimize outcomes. This structured workflow begins with the independent collection and analysis of relevant research papers, progresses through collaborative planning and data preparation, and results in automated experimentation and comprehensive report generation. Details on specific agent roles and their contributions across these phases are discussed in the paper.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p align="center"&gt; &lt;img src="https://raw.githubusercontent.com/SamuelSchmidgall/AgentLaboratory/main/media/AgentLabWF.png" alt="Demonstration of the flow of AgentClinic" style="width: 99%;" /&gt; &lt;/p&gt; 
&lt;h3&gt;üëæ Currently supported models&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;OpenAI&lt;/strong&gt;: o1, o1-preview, o1-mini, gpt-4o, o3-mini&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;DeepSeek&lt;/strong&gt;: deepseek-chat (deepseek-v3)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To select a specific llm set the flag &lt;code&gt;--llm-backend="llm_model"&lt;/code&gt; for example &lt;code&gt;--llm-backend="gpt-4o"&lt;/code&gt; or &lt;code&gt;--llm-backend="deepseek-chat"&lt;/code&gt;. Please feel free to add a PR supporting new models according to your need!&lt;/p&gt; 
&lt;h2&gt;üñ•Ô∏è Installation&lt;/h2&gt; 
&lt;h3&gt;Python venv option&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;We recommend using python 3.12&lt;/li&gt; 
&lt;/ul&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Clone the GitHub Repository&lt;/strong&gt;: Begin by cloning the repository using the command:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone git@github.com:SamuelSchmidgall/AgentLaboratory.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;&lt;strong&gt;Set up and Activate Python Environment&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m venv venv_agent_lab
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Now activate this environment:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;source venv_agent_lab/bin/activate
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;&lt;strong&gt;Install required libraries&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="4"&gt; 
 &lt;li&gt;&lt;strong&gt;Install pdflatex [OPTIONAL]&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;sudo apt install pdflatex
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;This enables latex source to be compiled by the agents.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;[IMPORTANT]&lt;/strong&gt; If this step cannot be run due to not having sudo access, pdf compiling can be turned off via running Agent Laboratory via setting the &lt;code&gt;--compile-latex&lt;/code&gt; flag to false: &lt;code&gt;--compile-latex "false"&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;ol start="5"&gt; 
 &lt;li&gt;&lt;strong&gt;Now run Agent Laboratory!&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;code&gt;python ai_lab_repo.py --yaml-location "experiment_configs/MATH_agentlab.yaml"&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Co-Pilot mode&lt;/h3&gt; 
&lt;p&gt;To run Agent Laboratory in copilot mode, simply set the copilot-mode flag in your yaml config to &lt;code&gt;"true"&lt;/code&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Tips for better research outcomes&lt;/h2&gt; 
&lt;h4&gt;[Tip #1] üìù Make sure to write extensive notes! üìù&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;Writing extensive notes is important&lt;/strong&gt; for helping your agent understand what you're looking to accomplish in your project, as well as any style preferences. Notes can include any experiments you want the agents to perform, providing API keys, certain plots or figures you want included, or anything you want the agent to know when performing research.&lt;/p&gt; 
&lt;p&gt;This is also your opportunity to let the agent know &lt;strong&gt;what compute resources it has access to&lt;/strong&gt;, e.g. GPUs (how many, what type of GPU, how many GBs), CPUs (how many cores, what type of CPUs), storage limitations, and hardware specs.&lt;/p&gt; 
&lt;p&gt;In order to add notes, you must modify the task_notes_LLM structure inside of &lt;code&gt;ai_lab_repo.py&lt;/code&gt;. Provided below is an example set of notes used for some of our experiments.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;task-notes:
  plan-formulation:
    - 'You should come up with a plan for only ONE experiment aimed at maximizing performance on the test set of MATH using prompting techniques.'
    - 'Please use gpt-4o-mini for your experiments'
    - 'You must evaluate on the entire 500 test questions of MATH'
  data-preparation:
    - 'Please use gpt-4o-mini for your experiments'
    - 'You must evaluate on the entire 500 test questions of MATH'
    - 'Here is a sample code you can use to load MATH\nfrom datasets import load_dataset\nMATH_test_set = load_dataset("HuggingFaceH4/MATH-500")["test"]'
...
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h4&gt;[Tip #2] üöÄ Using more powerful models generally leads to better research üöÄ&lt;/h4&gt; 
&lt;p&gt;When conducting research, &lt;strong&gt;the choice of model can significantly impact the quality of results&lt;/strong&gt;. More powerful models tend to have higher accuracy, better reasoning capabilities, and better report generation. If computational resources allow, prioritize the use of advanced models such as o1-(mini/preview) or similar state-of-the-art large language models.&lt;/p&gt; 
&lt;p&gt;However, &lt;strong&gt;it‚Äôs important to balance performance and cost-effectiveness&lt;/strong&gt;. While powerful models may yield better results, they are often more expensive and time-consuming to run. Consider using them selectively‚Äîfor instance, for key experiments or final analyses‚Äîwhile relying on smaller, more efficient models for iterative tasks or initial prototyping.&lt;/p&gt; 
&lt;p&gt;When resources are limited, &lt;strong&gt;optimize by fine-tuning smaller models&lt;/strong&gt; on your specific dataset or combining pre-trained models with task-specific prompts to achieve the desired balance between performance and computational efficiency.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h4&gt;[Tip #3] ‚úÖ You can load previous saves from checkpoints ‚úÖ&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;If you lose progress, internet connection, or if a subtask fails, you can always load from a previous state.&lt;/strong&gt; All of your progress is saved by default in the &lt;code&gt;state_saves&lt;/code&gt; variable, which stores each individual checkpoint.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h4&gt;[Tip #4] üàØ If you are running in a language other than English üà≤&lt;/h4&gt; 
&lt;p&gt;If you are running Agent Laboratory in a language other than English, no problem, just make sure to provide a language flag to the agents to perform research in your preferred language. Note that we have not extensively studied running Agent Laboratory in other languages, so be sure to report any problems you encounter.&lt;/p&gt; 
&lt;p&gt;For example, if you are running in Chinese set the language in the yaml:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;language: "‰∏≠Êñá"&lt;/code&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h4&gt;[Tip #5] üåü There is a lot of room for improvement üåü&lt;/h4&gt; 
&lt;p&gt;There is a lot of room to improve this codebase, so if you end up making changes and want to help the community, please feel free to share the changes you've made! We hope this tool helps you!&lt;/p&gt; 
&lt;h2&gt;üìú License&lt;/h2&gt; 
&lt;p&gt;Source Code Licensing: Our project's source code is licensed under the MIT License. This license permits the use, modification, and distribution of the code, subject to certain conditions outlined in the MIT License.&lt;/p&gt; 
&lt;h2&gt;üì¨ Contact&lt;/h2&gt; 
&lt;p&gt;If you would like to get in touch, feel free to reach out to &lt;a href="mailto:sschmi46@jhu.edu"&gt;sschmi46@jhu.edu&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Reference / Bibtex&lt;/h2&gt; 
&lt;h3&gt;Agent Laboratory&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{schmidgall2025agentlaboratoryusingllm,
      title={Agent Laboratory: Using LLM Agents as Research Assistants}, 
      author={Samuel Schmidgall and Yusheng Su and Ze Wang and Ximeng Sun and Jialian Wu and Xiaodong Yu and Jiang Liu and Michael Moor and Zicheng Liu and Emad Barsoum},
      year={2025},
      eprint={2501.04227},
      archivePrefix={arXiv},
      primaryClass={cs.HC},
      url={https://arxiv.org/abs/2501.04227}, 
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;AgentRxiv&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{schmidgall2025agentrxiv,
      title={AgentRxiv: Towards Collaborative Autonomous Research}, 
      author={Samuel Schmidgall and Michael Moor},
      year={2025},
      eprint={2503.18102},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2503.18102}, 
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>microsoft/BitNet</title>
      <link>https://github.com/microsoft/BitNet</link>
      <description>&lt;p&gt;Official inference framework for 1-bit LLMs&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;bitnet.cpp&lt;/h1&gt; 
&lt;p&gt;&lt;a href="https://opensource.org/licenses/MIT"&gt;&lt;img src="https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true" alt="License: MIT" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/badge/version-1.0-blue" alt="version" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://huggingface.co/microsoft/BitNet-b1.58-2B-4T"&gt;&lt;img src="https://raw.githubusercontent.com/microsoft/BitNet/main/assets/header_model_release.png" alt="BitNet Model on Hugging Face" width="800" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Try it out via this &lt;a href="https://bitnet-demo.azurewebsites.net/"&gt;demo&lt;/a&gt;, or build and run it on your own &lt;a href="https://github.com/microsoft/BitNet?tab=readme-ov-file#build-from-source"&gt;CPU&lt;/a&gt; or &lt;a href="https://github.com/microsoft/BitNet/raw/main/gpu/README.md"&gt;GPU&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;bitnet.cpp is the official inference framework for 1-bit LLMs (e.g., BitNet b1.58). It offers a suite of optimized kernels, that support &lt;strong&gt;fast&lt;/strong&gt; and &lt;strong&gt;lossless&lt;/strong&gt; inference of 1.58-bit models on CPU and GPU (NPU support will coming next).&lt;/p&gt; 
&lt;p&gt;The first release of bitnet.cpp is to support inference on CPUs. bitnet.cpp achieves speedups of &lt;strong&gt;1.37x&lt;/strong&gt; to &lt;strong&gt;5.07x&lt;/strong&gt; on ARM CPUs, with larger models experiencing greater performance gains. Additionally, it reduces energy consumption by &lt;strong&gt;55.4%&lt;/strong&gt; to &lt;strong&gt;70.0%&lt;/strong&gt;, further boosting overall efficiency. On x86 CPUs, speedups range from &lt;strong&gt;2.37x&lt;/strong&gt; to &lt;strong&gt;6.17x&lt;/strong&gt; with energy reductions between &lt;strong&gt;71.9%&lt;/strong&gt; to &lt;strong&gt;82.2%&lt;/strong&gt;. Furthermore, bitnet.cpp can run a 100B BitNet b1.58 model on a single CPU, achieving speeds comparable to human reading (5-7 tokens per second), significantly enhancing the potential for running LLMs on local devices. Please refer to the &lt;a href="https://arxiv.org/abs/2410.16144"&gt;technical report&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;img src="https://raw.githubusercontent.com/microsoft/BitNet/main/assets/m2_performance.jpg" alt="m2_performance" width="800" /&gt; 
&lt;img src="https://raw.githubusercontent.com/microsoft/BitNet/main/assets/intel_performance.jpg" alt="m2_performance" width="800" /&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;The tested models are dummy setups used in a research context to demonstrate the inference performance of bitnet.cpp.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Demo&lt;/h2&gt; 
&lt;p&gt;A demo of bitnet.cpp running a BitNet b1.58 3B model on Apple M2:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/7f46b736-edec-4828-b809-4be780a3e5b1"&gt;https://github.com/user-attachments/assets/7f46b736-edec-4828-b809-4be780a3e5b1&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;What's New:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;05/20/2025 &lt;a href="https://github.com/microsoft/BitNet/raw/main/gpu/README.md"&gt;BitNet Official GPU inference kernel&lt;/a&gt; &lt;img src="https://img.shields.io/badge/NEW-red" alt="NEW" /&gt;&lt;/li&gt; 
 &lt;li&gt;04/14/2025 &lt;a href="https://huggingface.co/microsoft/BitNet-b1.58-2B-4T"&gt;BitNet Official 2B Parameter Model on Hugging Face&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;02/18/2025 &lt;a href="https://arxiv.org/abs/2502.11880"&gt;Bitnet.cpp: Efficient Edge Inference for Ternary LLMs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;11/08/2024 &lt;a href="https://arxiv.org/abs/2411.04965"&gt;BitNet a4.8: 4-bit Activations for 1-bit LLMs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;10/21/2024 &lt;a href="https://arxiv.org/abs/2410.16144"&gt;1-bit AI Infra: Part 1.1, Fast and Lossless BitNet b1.58 Inference on CPUs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;10/17/2024 bitnet.cpp 1.0 released.&lt;/li&gt; 
 &lt;li&gt;03/21/2024 &lt;a href="https://github.com/microsoft/unilm/raw/master/bitnet/The-Era-of-1-bit-LLMs__Training_Tips_Code_FAQ.pdf"&gt;The-Era-of-1-bit-LLMs__Training_Tips_Code_FAQ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;02/27/2024 &lt;a href="https://arxiv.org/abs/2402.17764"&gt;The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;10/17/2023 &lt;a href="https://arxiv.org/abs/2310.11453"&gt;BitNet: Scaling 1-bit Transformers for Large Language Models&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;p&gt;This project is based on the &lt;a href="https://github.com/ggerganov/llama.cpp"&gt;llama.cpp&lt;/a&gt; framework. We would like to thank all the authors for their contributions to the open-source community. Also, bitnet.cpp's kernels are built on top of the Lookup Table methodologies pioneered in &lt;a href="https://github.com/microsoft/T-MAC/"&gt;T-MAC&lt;/a&gt;. For inference of general low-bit LLMs beyond ternary models, we recommend using T-MAC.&lt;/p&gt; 
&lt;h2&gt;Official Models&lt;/h2&gt; 
&lt;table&gt;  
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;th rowspan="2"&gt;Model&lt;/th&gt; 
   &lt;th rowspan="2"&gt;Parameters&lt;/th&gt; 
   &lt;th rowspan="2"&gt;CPU&lt;/th&gt; 
   &lt;th colspan="3"&gt;Kernel&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;th&gt;I2_S&lt;/th&gt; 
   &lt;th&gt;TL1&lt;/th&gt; 
   &lt;th&gt;TL2&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/microsoft/BitNet-b1.58-2B-4T"&gt;BitNet-b1.58-2B-4T&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;2.4B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;Supported Models&lt;/h2&gt; 
&lt;p&gt;‚ùóÔ∏è&lt;strong&gt;We use existing 1-bit LLMs available on &lt;a href="https://huggingface.co/"&gt;Hugging Face&lt;/a&gt; to demonstrate the inference capabilities of bitnet.cpp. We hope the release of bitnet.cpp will inspire the development of 1-bit LLMs in large-scale settings in terms of model size and training tokens.&lt;/strong&gt;&lt;/p&gt; 
&lt;table&gt;  
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;th rowspan="2"&gt;Model&lt;/th&gt; 
   &lt;th rowspan="2"&gt;Parameters&lt;/th&gt; 
   &lt;th rowspan="2"&gt;CPU&lt;/th&gt; 
   &lt;th colspan="3"&gt;Kernel&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;th&gt;I2_S&lt;/th&gt; 
   &lt;th&gt;TL1&lt;/th&gt; 
   &lt;th&gt;TL2&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/1bitLLM/bitnet_b1_58-large"&gt;bitnet_b1_58-large&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;0.7B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/1bitLLM/bitnet_b1_58-3B"&gt;bitnet_b1_58-3B&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;3.3B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/HF1BitLLM/Llama3-8B-1.58-100B-tokens"&gt;Llama3-8B-1.58-100B-tokens&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;8.0B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/collections/tiiuae/falcon3-67605ae03578be86e4e87026"&gt;Falcon3 Family&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;1B-10B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td rowspan="2"&gt;&lt;a href="https://huggingface.co/collections/tiiuae/falcon-edge-series-6804fd13344d6d8a8fa71130"&gt;Falcon-E Family&lt;/a&gt;&lt;/td&gt; 
   &lt;td rowspan="2"&gt;1B-3B&lt;/td&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ARM&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;‚ùå&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;Requirements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;python&amp;gt;=3.9&lt;/li&gt; 
 &lt;li&gt;cmake&amp;gt;=3.22&lt;/li&gt; 
 &lt;li&gt;clang&amp;gt;=18 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;For Windows users, install &lt;a href="https://visualstudio.microsoft.com/downloads/"&gt;Visual Studio 2022&lt;/a&gt;. In the installer, toggle on at least the following options(this also automatically installs the required additional tools like CMake):&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Desktop-development with C++&lt;/li&gt; 
     &lt;li&gt;C++-CMake Tools for Windows&lt;/li&gt; 
     &lt;li&gt;Git for Windows&lt;/li&gt; 
     &lt;li&gt;C++-Clang Compiler for Windows&lt;/li&gt; 
     &lt;li&gt;MS-Build Support for LLVM-Toolset (clang)&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;For Debian/Ubuntu users, you can download with &lt;a href="https://apt.llvm.org/"&gt;Automatic installation script&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;bash -c "$(wget -O - https://apt.llvm.org/llvm.sh)"&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;conda (highly recommend)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Build from source&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] If you are using Windows, please remember to always use a Developer Command Prompt / PowerShell for VS2022 for the following commands. Please refer to the FAQs below if you see any issues.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ol&gt; 
 &lt;li&gt;Clone the repo&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;git clone --recursive https://github.com/microsoft/BitNet.git
cd BitNet
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Install the dependencies&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# (Recommended) Create a new conda environment
conda create -n bitnet-cpp python=3.9
conda activate bitnet-cpp

pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Build the project&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Manually download the model and run with local path
huggingface-cli download microsoft/BitNet-b1.58-2B-4T-gguf --local-dir models/BitNet-b1.58-2B-4T
python setup_env.py -md models/BitNet-b1.58-2B-4T -q i2_s

&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;
usage: setup_env.py [-h] [--hf-repo {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens,tiiuae/Falcon3-1B-Instruct-1.58bit,tiiuae/Falcon3-3B-Instruct-1.58bit,tiiuae/Falcon3-7B-Instruct-1.58bit,tiiuae/Falcon3-10B-Instruct-1.58bit}] [--model-dir MODEL_DIR] [--log-dir LOG_DIR] [--quant-type {i2_s,tl1}] [--quant-embd]
                    [--use-pretuned]

Setup the environment for running inference

optional arguments:
  -h, --help            show this help message and exit
  --hf-repo {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens,tiiuae/Falcon3-1B-Instruct-1.58bit,tiiuae/Falcon3-3B-Instruct-1.58bit,tiiuae/Falcon3-7B-Instruct-1.58bit,tiiuae/Falcon3-10B-Instruct-1.58bit}, -hr {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens,tiiuae/Falcon3-1B-Instruct-1.58bit,tiiuae/Falcon3-3B-Instruct-1.58bit,tiiuae/Falcon3-7B-Instruct-1.58bit,tiiuae/Falcon3-10B-Instruct-1.58bit}
                        Model used for inference
  --model-dir MODEL_DIR, -md MODEL_DIR
                        Directory to save/load the model
  --log-dir LOG_DIR, -ld LOG_DIR
                        Directory to save the logging info
  --quant-type {i2_s,tl1}, -q {i2_s,tl1}
                        Quantization type
  --quant-embd          Quantize the embeddings to f16
  --use-pretuned, -p    Use the pretuned kernel parameters
&lt;/pre&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;h3&gt;Basic usage&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Run inference with the quantized model
python run_inference.py -m models/BitNet-b1.58-2B-4T/ggml-model-i2_s.gguf -p "You are a helpful assistant" -cnv
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;
usage: run_inference.py [-h] [-m MODEL] [-n N_PREDICT] -p PROMPT [-t THREADS] [-c CTX_SIZE] [-temp TEMPERATURE] [-cnv]

Run inference

optional arguments:
  -h, --help            show this help message and exit
  -m MODEL, --model MODEL
                        Path to model file
  -n N_PREDICT, --n-predict N_PREDICT
                        Number of tokens to predict when generating text
  -p PROMPT, --prompt PROMPT
                        Prompt to generate text from
  -t THREADS, --threads THREADS
                        Number of threads to use
  -c CTX_SIZE, --ctx-size CTX_SIZE
                        Size of the prompt context
  -temp TEMPERATURE, --temperature TEMPERATURE
                        Temperature, a hyperparameter that controls the randomness of the generated text
  -cnv, --conversation  Whether to enable chat mode or not (for instruct models.)
                        (When this option is turned on, the prompt specified by -p will be used as the system prompt.)
&lt;/pre&gt; 
&lt;h3&gt;Benchmark&lt;/h3&gt; 
&lt;p&gt;We provide scripts to run the inference benchmark providing a model.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;usage: e2e_benchmark.py -m MODEL [-n N_TOKEN] [-p N_PROMPT] [-t THREADS]  
   
Setup the environment for running the inference  
   
required arguments:  
  -m MODEL, --model MODEL  
                        Path to the model file. 
   
optional arguments:  
  -h, --help  
                        Show this help message and exit. 
  -n N_TOKEN, --n-token N_TOKEN  
                        Number of generated tokens. 
  -p N_PROMPT, --n-prompt N_PROMPT  
                        Prompt to generate text from. 
  -t THREADS, --threads THREADS  
                        Number of threads to use. 
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Here's a brief explanation of each argument:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;-m&lt;/code&gt;, &lt;code&gt;--model&lt;/code&gt;: The path to the model file. This is a required argument that must be provided when running the script.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-n&lt;/code&gt;, &lt;code&gt;--n-token&lt;/code&gt;: The number of tokens to generate during the inference. It is an optional argument with a default value of 128.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-p&lt;/code&gt;, &lt;code&gt;--n-prompt&lt;/code&gt;: The number of prompt tokens to use for generating text. This is an optional argument with a default value of 512.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-t&lt;/code&gt;, &lt;code&gt;--threads&lt;/code&gt;: The number of threads to use for running the inference. It is an optional argument with a default value of 2.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;-h&lt;/code&gt;, &lt;code&gt;--help&lt;/code&gt;: Show the help message and exit. Use this argument to display usage information.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python utils/e2e_benchmark.py -m /path/to/model -n 200 -p 256 -t 4  
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This command would run the inference benchmark using the model located at &lt;code&gt;/path/to/model&lt;/code&gt;, generating 200 tokens from a 256 token prompt, utilizing 4 threads.&lt;/p&gt; 
&lt;p&gt;For the model layout that do not supported by any public model, we provide scripts to generate a dummy model with the given model layout, and run the benchmark on your machine:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;python utils/generate-dummy-bitnet-model.py models/bitnet_b1_58-large --outfile models/dummy-bitnet-125m.tl1.gguf --outtype tl1 --model-size 125M

# Run benchmark with the generated model, use -m to specify the model path, -p to specify the prompt processed, -n to specify the number of token to generate
python utils/e2e_benchmark.py -m models/dummy-bitnet-125m.tl1.gguf -p 512 -n 128
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Convert from &lt;code&gt;.safetensors&lt;/code&gt; Checkpoints&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Prepare the .safetensors model file
huggingface-cli download microsoft/bitnet-b1.58-2B-4T-bf16 --local-dir ./models/bitnet-b1.58-2B-4T-bf16

# Convert to gguf model
python ./utils/convert-helper-bitnet.py ./models/bitnet-b1.58-2B-4T-bf16
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;FAQ (Frequently Asked Questions)üìå&lt;/h3&gt; 
&lt;h4&gt;Q1: The build dies with errors building llama.cpp due to issues with std::chrono in log.cpp?&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; This is an issue introduced in recent version of llama.cpp. Please refer to this &lt;a href="https://github.com/tinglou/llama.cpp/commit/4e3db1e3d78cc1bcd22bcb3af54bd2a4628dd323"&gt;commit&lt;/a&gt; in the &lt;a href="https://github.com/abetlen/llama-cpp-python/issues/1942"&gt;discussion&lt;/a&gt; to fix this issue.&lt;/p&gt; 
&lt;h4&gt;Q2: How to build with clang in conda environment on windows?&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Before building the project, verify your clang installation and access to Visual Studio tools by running:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;clang -v
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This command checks that you are using the correct version of clang and that the Visual Studio tools are available. If you see an error message such as:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;'clang' is not recognized as an internal or external command, operable program or batch file.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;It indicates that your command line window is not properly initialized for Visual Studio tools.&lt;/p&gt; 
&lt;p&gt;‚Ä¢ If you are using Command Prompt, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;"C:\Program Files\Microsoft Visual Studio\2022\Professional\Common7\Tools\VsDevCmd.bat" -startdir=none -arch=x64 -host_arch=x64
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;‚Ä¢ If you are using Windows PowerShell, run the following commands:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Import-Module "C:\Program Files\Microsoft Visual Studio\2022\Professional\Common7\Tools\Microsoft.VisualStudio.DevShell.dll" Enter-VsDevShell 3f0e31ad -SkipAutomaticLocation -DevCmdArguments "-arch=x64 -host_arch=x64"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;These steps will initialize your environment and allow you to use the correct Visual Studio tools.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>recommenders-team/recommenders</title>
      <link>https://github.com/recommenders-team/recommenders</link>
      <description>&lt;p&gt;Best Practices on Recommendation Systems&lt;/p&gt;&lt;hr&gt;&lt;img src="https://raw.githubusercontent.com/recommenders-team/artwork/main/color/recommenders_color.svg?sanitize=true" width="800" /&gt; 
&lt;p&gt;&lt;a href="https://github.com/recommenders-team/recommenders/actions/workflows/pages/pages-build-deployment"&gt;&lt;img src="https://github.com/recommenders-team/recommenders/actions/workflows/pages/pages-build-deployment/badge.svg?sanitize=true" alt="Documentation status" /&gt;&lt;/a&gt; &lt;a href="https://github.com/recommenders-team/recommenders/raw/main/LICENSE"&gt;&lt;img src="https://img.shields.io/github/license/recommenders-team/recommenders.svg?sanitize=true" alt="License" /&gt;&lt;/a&gt; &lt;a href="https://github.com/psf/black"&gt;&lt;img src="https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true" alt="Black" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/recommenders"&gt;&lt;img src="https://img.shields.io/pypi/v/recommenders.svg?logo=pypi&amp;amp;logoColor=white" alt="PyPI Version" /&gt;&lt;/a&gt; &lt;a href="https://pypi.org/project/recommenders"&gt;&lt;img src="https://img.shields.io/pypi/pyversions/recommenders.svg?logo=python&amp;amp;logoColor=white" alt="Python Versions" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://join.slack.com/t/lfaifoundation/shared_invite/zt-2iyl7zyya-g5rOO5K518CBoevyi28W6w"&gt;&lt;img align="left" width="300" src="https://raw.githubusercontent.com/recommenders-team/artwork/main/mix/join_recommenders_slack.svg?sanitize=true" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;h2&gt;What's New (April, 2025)&lt;/h2&gt; 
&lt;p&gt;We reached 20,000 stars!!&lt;/p&gt; 
&lt;p&gt;We are happy to announce that we have reached 20,000 stars on GitHub! Thank you for your support and contributions to the Recommenders project. We are excited to continue building and improving this project with your help.&lt;/p&gt; 
&lt;p&gt;Check out the release &lt;a href="https://github.com/recommenders-team/recommenders/releases/tag/1.2.1"&gt;Recommenders 1.2.1&lt;/a&gt;!&lt;/p&gt; 
&lt;p&gt;We fixed a lot of bugs due to dependencies, improved security, reviewed the notebooks and the libraries.&lt;/p&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;Recommenders objective is to assist researchers, developers and enthusiasts in prototyping, experimenting with and bringing to production a range of classic and state-of-the-art recommendation systems.&lt;/p&gt; 
&lt;p&gt;Recommenders is a project under the &lt;a href="https://lfaidata.foundation/projects/"&gt;Linux Foundation of AI and Data&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;This repository contains examples and best practices for building recommendation systems, provided as Jupyter notebooks. The examples detail our learnings on five key tasks:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/01_prepare_data"&gt;Prepare Data&lt;/a&gt;: Preparing and loading data for each recommendation algorithm.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/00_quick_start"&gt;Model&lt;/a&gt;: Building models using various classical and deep learning recommendation algorithms such as Alternating Least Squares (&lt;a href="https://spark.apache.org/docs/latest/api/python/_modules/pyspark/ml/recommendation.html#ALS"&gt;ALS&lt;/a&gt;) or eXtreme Deep Factorization Machines (&lt;a href="https://arxiv.org/abs/1803.05170"&gt;xDeepFM&lt;/a&gt;).&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/03_evaluate"&gt;Evaluate&lt;/a&gt;: Evaluating algorithms with offline metrics.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/04_model_select_and_optimize"&gt;Model Select and Optimize&lt;/a&gt;: Tuning and optimizing hyperparameters for recommendation models.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/05_operationalize"&gt;Operationalize&lt;/a&gt;: Operationalizing models in a production environment on Azure.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Several utilities are provided in &lt;a href="https://raw.githubusercontent.com/recommenders-team/recommenders/main/recommenders"&gt;recommenders&lt;/a&gt; to support common tasks such as loading datasets in the format expected by different algorithms, evaluating model outputs, and splitting training/test data. Implementations of several state-of-the-art algorithms are included for self-study and customization in your own applications. See the &lt;a href="https://readthedocs.org/projects/microsoft-recommenders/"&gt;Recommenders documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For a more detailed overview of the repository, please see the documents on the &lt;a href="https://github.com/microsoft/recommenders/wiki/Documents-and-Presentations"&gt;wiki page&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For some of the practical scenarios where recommendation systems have been applied, see &lt;a href="https://raw.githubusercontent.com/recommenders-team/recommenders/main/scenarios"&gt;scenarios&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;We recommend &lt;a href="https://docs.conda.io/projects/conda/en/latest/glossary.html?highlight=environment#conda-environment"&gt;conda&lt;/a&gt; for environment management, and &lt;a href="https://code.visualstudio.com/"&gt;VS Code&lt;/a&gt; for development. To install the recommenders package and run an example notebook on Linux/WSL:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 1. Install gcc if it is not installed already. On Ubuntu, this could done by using the command
# sudo apt install gcc

# 2. Create and activate a new conda environment
conda create -n &amp;lt;environment_name&amp;gt; python=3.9
conda activate &amp;lt;environment_name&amp;gt;

# 3. Install the core recommenders package. It can run all the CPU notebooks.
pip install recommenders

# 4. create a Jupyter kernel
python -m ipykernel install --user --name &amp;lt;environment_name&amp;gt; --display-name &amp;lt;kernel_name&amp;gt;

# 5. Clone this repo within VSCode or using command line:
git clone https://github.com/recommenders-team/recommenders.git

# 6. Within VSCode:
#   a. Open a notebook, e.g., examples/00_quick_start/sar_movielens.ipynb;  
#   b. Select Jupyter kernel &amp;lt;kernel_name&amp;gt;;
#   c. Run the notebook.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more information about setup on other platforms (e.g., Windows and macOS) and different configurations (e.g., GPU, Spark and experimental features), see the &lt;a href="https://raw.githubusercontent.com/recommenders-team/recommenders/main/SETUP.md"&gt;Setup Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;In addition to the core package, several extras are also provided, including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;[gpu]&lt;/code&gt;: Needed for running GPU models.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[spark]&lt;/code&gt;: Needed for running Spark models.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[dev]&lt;/code&gt;: Needed for development for the repo.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[all]&lt;/code&gt;: &lt;code&gt;[gpu]&lt;/code&gt;|&lt;code&gt;[spark]&lt;/code&gt;|&lt;code&gt;[dev]&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;[experimental]&lt;/code&gt;: Models that are not thoroughly tested and/or may require additional steps in installation.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Algorithms&lt;/h2&gt; 
&lt;p&gt;The table below lists the recommendation algorithms currently available in the repository. Notebooks are linked under the Example column as Quick start, showcasing an easy to run example of the algorithm, or as Deep dive, explaining in detail the math and implementation of the algorithm.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Algorithm&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Example&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Alternating Least Squares (ALS)&lt;/td&gt; 
   &lt;td&gt;Collaborative Filtering&lt;/td&gt; 
   &lt;td&gt;Matrix factorization algorithm for explicit or implicit feedback in large datasets, optimized for scalability and distributed computing capability. It works in the PySpark environment.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/00_quick_start/als_movielens.ipynb"&gt;Quick start&lt;/a&gt; / &lt;a href="https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/02_model_collaborative_filtering/als_deep_dive.ipynb"&gt;Deep dive&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Attentive Asynchronous Singular Value Decomposition (A2SVD)&lt;sup&gt;*&lt;/sup&gt;&lt;/td&gt; 
   &lt;td&gt;Collaborative Filtering&lt;/td&gt; 
   &lt;td&gt;Sequential-based algorithm that aims to capture both long and short-term user preferences using attention mechanism. It works in the CPU/GPU environment.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/00_quick_start/sequential_recsys_amazondataset.ipynb"&gt;Quick start&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Cornac/Bayesian Personalized Ranking (BPR)&lt;/td&gt; 
   &lt;td&gt;Collaborative Filtering&lt;/td&gt; 
   &lt;td&gt;Matrix factorization algorithm for predicting item ranking with implicit feedback. It works in the CPU environment.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/02_model_collaborative_filtering/cornac_bpr_deep_dive.ipynb"&gt;Deep dive&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Cornac/Bilateral Variational Autoencoder (BiVAE)&lt;/td&gt; 
   &lt;td&gt;Collaborative Filtering&lt;/td&gt; 
   &lt;td&gt;Generative model for dyadic data (e.g., user-item interactions). It works in the CPU/GPU environment.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/02_model_collaborative_filtering/cornac_bivae_deep_dive.ipynb"&gt;Deep dive&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Convolutional Sequence Embedding Recommendation (Caser)&lt;/td&gt; 
   &lt;td&gt;Collaborative Filtering&lt;/td&gt; 
   &lt;td&gt;Algorithm based on convolutions that aim to capture both user‚Äôs general preferences and sequential patterns. It works in the CPU/GPU environment.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/00_quick_start/sequential_recsys_amazondataset.ipynb"&gt;Quick start&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Deep Knowledge-Aware Network (DKN)&lt;sup&gt;*&lt;/sup&gt;&lt;/td&gt; 
   &lt;td&gt;Content-Based Filtering&lt;/td&gt; 
   &lt;td&gt;Deep learning algorithm incorporating a knowledge graph and article embeddings for providing news or article recommendations. It works in the CPU/GPU environment.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/00_quick_start/dkn_MIND.ipynb"&gt;Quick start&lt;/a&gt; / &lt;a href="https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/02_model_content_based_filtering/dkn_deep_dive.ipynb"&gt;Deep dive&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Extreme Deep Factorization Machine (xDeepFM)&lt;sup&gt;*&lt;/sup&gt;&lt;/td&gt; 
   &lt;td&gt;Collaborative Filtering&lt;/td&gt; 
   &lt;td&gt;Deep learning based algorithm for implicit and explicit feedback with user/item features. It works in the CPU/GPU environment.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/00_quick_start/xdeepfm_criteo.ipynb"&gt;Quick start&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Embedding Dot Bias&lt;/td&gt; 
   &lt;td&gt;Collaborative Filtering&lt;/td&gt; 
   &lt;td&gt;General purpose algorithm with embeddings and biases for users and items. It works in the CPU/GPU environment.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/00_quick_start/embdotbias_movielens.ipynb"&gt;Quick start&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;LightFM/Factorization Machine&lt;/td&gt; 
   &lt;td&gt;Collaborative Filtering&lt;/td&gt; 
   &lt;td&gt;Factorization Machine algorithm for both implicit and explicit feedbacks. It works in the CPU environment.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/02_model_collaborative_filtering/lightfm_deep_dive.ipynb"&gt;Quick start&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;LightGBM/Gradient Boosting Tree&lt;sup&gt;*&lt;/sup&gt;&lt;/td&gt; 
   &lt;td&gt;Content-Based Filtering&lt;/td&gt; 
   &lt;td&gt;Gradient Boosting Tree algorithm for fast training and low memory usage in content-based problems. It works in the CPU/GPU/PySpark environments.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/00_quick_start/lightgbm_tinycriteo.ipynb"&gt;Quick start in CPU&lt;/a&gt; / &lt;a href="https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/02_model_content_based_filtering/mmlspark_lightgbm_criteo.ipynb"&gt;Deep dive in PySpark&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;LightGCN&lt;/td&gt; 
   &lt;td&gt;Collaborative Filtering&lt;/td&gt; 
   &lt;td&gt;Deep learning algorithm which simplifies the design of GCN for predicting implicit feedback. It works in the CPU/GPU environment.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/02_model_collaborative_filtering/lightgcn_deep_dive.ipynb"&gt;Deep dive&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;GeoIMC&lt;sup&gt;*&lt;/sup&gt;&lt;/td&gt; 
   &lt;td&gt;Collaborative Filtering&lt;/td&gt; 
   &lt;td&gt;Matrix completion algorithm that takes into account user and item features using Riemannian conjugate gradient optimization and follows a geometric approach. It works in the CPU environment.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/00_quick_start/geoimc_movielens.ipynb"&gt;Quick start&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;GRU&lt;/td&gt; 
   &lt;td&gt;Collaborative Filtering&lt;/td&gt; 
   &lt;td&gt;Sequential-based algorithm that aims to capture both long and short-term user preferences using recurrent neural networks. It works in the CPU/GPU environment.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/00_quick_start/sequential_recsys_amazondataset.ipynb"&gt;Quick start&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Multinomial VAE&lt;/td&gt; 
   &lt;td&gt;Collaborative Filtering&lt;/td&gt; 
   &lt;td&gt;Generative model for predicting user/item interactions. It works in the CPU/GPU environment.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/02_model_collaborative_filtering/multi_vae_deep_dive.ipynb"&gt;Deep dive&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Neural Recommendation with Long- and Short-term User Representations (LSTUR)&lt;sup&gt;*&lt;/sup&gt;&lt;/td&gt; 
   &lt;td&gt;Content-Based Filtering&lt;/td&gt; 
   &lt;td&gt;Neural recommendation algorithm for recommending news articles with long- and short-term user interest modeling. It works in the CPU/GPU environment.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/00_quick_start/lstur_MIND.ipynb"&gt;Quick start&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Neural Recommendation with Attentive Multi-View Learning (NAML)&lt;sup&gt;*&lt;/sup&gt;&lt;/td&gt; 
   &lt;td&gt;Content-Based Filtering&lt;/td&gt; 
   &lt;td&gt;Neural recommendation algorithm for recommending news articles with attentive multi-view learning. It works in the CPU/GPU environment.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/00_quick_start/naml_MIND.ipynb"&gt;Quick start&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Neural Collaborative Filtering (NCF)&lt;/td&gt; 
   &lt;td&gt;Collaborative Filtering&lt;/td&gt; 
   &lt;td&gt;Deep learning algorithm with enhanced performance for user/item implicit feedback. It works in the CPU/GPU environment.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/00_quick_start/ncf_movielens.ipynb"&gt;Quick start&lt;/a&gt; / &lt;a href="https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/02_model_collaborative_filtering/ncf_deep_dive.ipynb"&gt;Deep dive&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Neural Recommendation with Personalized Attention (NPA)&lt;sup&gt;*&lt;/sup&gt;&lt;/td&gt; 
   &lt;td&gt;Content-Based Filtering&lt;/td&gt; 
   &lt;td&gt;Neural recommendation algorithm for recommending news articles with personalized attention network. It works in the CPU/GPU environment.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/00_quick_start/npa_MIND.ipynb"&gt;Quick start&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Neural Recommendation with Multi-Head Self-Attention (NRMS)&lt;sup&gt;*&lt;/sup&gt;&lt;/td&gt; 
   &lt;td&gt;Content-Based Filtering&lt;/td&gt; 
   &lt;td&gt;Neural recommendation algorithm for recommending news articles with multi-head self-attention. It works in the CPU/GPU environment.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/00_quick_start/nrms_MIND.ipynb"&gt;Quick start&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Next Item Recommendation (NextItNet)&lt;/td&gt; 
   &lt;td&gt;Collaborative Filtering&lt;/td&gt; 
   &lt;td&gt;Algorithm based on dilated convolutions and residual network that aims to capture sequential patterns. It considers both user/item interactions and features. It works in the CPU/GPU environment.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/00_quick_start/sequential_recsys_amazondataset.ipynb"&gt;Quick start&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Restricted Boltzmann Machines (RBM)&lt;/td&gt; 
   &lt;td&gt;Collaborative Filtering&lt;/td&gt; 
   &lt;td&gt;Neural network based algorithm for learning the underlying probability distribution for explicit or implicit user/item feedback. It works in the CPU/GPU environment.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/00_quick_start/rbm_movielens.ipynb"&gt;Quick start&lt;/a&gt; / &lt;a href="https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/02_model_collaborative_filtering/rbm_deep_dive.ipynb"&gt;Deep dive&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Riemannian Low-rank Matrix Completion (RLRMC)&lt;sup&gt;*&lt;/sup&gt;&lt;/td&gt; 
   &lt;td&gt;Collaborative Filtering&lt;/td&gt; 
   &lt;td&gt;Matrix factorization algorithm using Riemannian conjugate gradients optimization with small memory consumption to predict user/item interactions. It works in the CPU environment.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/00_quick_start/rlrmc_movielens.ipynb"&gt;Quick start&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Simple Algorithm for Recommendation (SAR)&lt;sup&gt;*&lt;/sup&gt;&lt;/td&gt; 
   &lt;td&gt;Collaborative Filtering&lt;/td&gt; 
   &lt;td&gt;Similarity-based algorithm for implicit user/item feedback. It works in the CPU environment.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/00_quick_start/sar_movielens.ipynb"&gt;Quick start&lt;/a&gt; / &lt;a href="https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/02_model_collaborative_filtering/sar_deep_dive.ipynb"&gt;Deep dive&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Self-Attentive Sequential Recommendation (SASRec)&lt;/td&gt; 
   &lt;td&gt;Collaborative Filtering&lt;/td&gt; 
   &lt;td&gt;Transformer based algorithm for sequential recommendation. It works in the CPU/GPU environment.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/00_quick_start/sasrec_amazon.ipynb"&gt;Quick start&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Short-term and Long-term Preference Integrated Recommender (SLi-Rec)&lt;sup&gt;*&lt;/sup&gt;&lt;/td&gt; 
   &lt;td&gt;Collaborative Filtering&lt;/td&gt; 
   &lt;td&gt;Sequential-based algorithm that aims to capture both long and short-term user preferences using attention mechanism, a time-aware controller and a content-aware controller. It works in the CPU/GPU environment.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/00_quick_start/sequential_recsys_amazondataset.ipynb"&gt;Quick start&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Multi-Interest-Aware Sequential User Modeling (SUM)&lt;sup&gt;*&lt;/sup&gt;&lt;/td&gt; 
   &lt;td&gt;Collaborative Filtering&lt;/td&gt; 
   &lt;td&gt;An enhanced memory network-based sequential user model which aims to capture users' multiple interests. It works in the CPU/GPU environment.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/00_quick_start/sequential_recsys_amazondataset.ipynb"&gt;Quick start&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Sequential Recommendation Via Personalized Transformer (SSEPT)&lt;/td&gt; 
   &lt;td&gt;Collaborative Filtering&lt;/td&gt; 
   &lt;td&gt;Transformer based algorithm for sequential recommendation with User embedding. It works in the CPU/GPU environment.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/00_quick_start/sasrec_amazon.ipynb"&gt;Quick start&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Standard VAE&lt;/td&gt; 
   &lt;td&gt;Collaborative Filtering&lt;/td&gt; 
   &lt;td&gt;Generative Model for predicting user/item interactions. It works in the CPU/GPU environment.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/02_model_collaborative_filtering/standard_vae_deep_dive.ipynb"&gt;Deep dive&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Surprise/Singular Value Decomposition (SVD)&lt;/td&gt; 
   &lt;td&gt;Collaborative Filtering&lt;/td&gt; 
   &lt;td&gt;Matrix factorization algorithm for predicting explicit rating feedback in small datasets. It works in the CPU/GPU environment.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/02_model_collaborative_filtering/surprise_svd_deep_dive.ipynb"&gt;Deep dive&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Term Frequency - Inverse Document Frequency (TF-IDF)&lt;/td&gt; 
   &lt;td&gt;Content-Based Filtering&lt;/td&gt; 
   &lt;td&gt;Simple similarity-based algorithm for content-based recommendations with text datasets. It works in the CPU environment.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/00_quick_start/tfidf_covid.ipynb"&gt;Quick start&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Vowpal Wabbit (VW)&lt;sup&gt;*&lt;/sup&gt;&lt;/td&gt; 
   &lt;td&gt;Content-Based Filtering&lt;/td&gt; 
   &lt;td&gt;Fast online learning algorithms, great for scenarios where user features / context are constantly changing. It uses the CPU for online learning.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/02_model_content_based_filtering/vowpal_wabbit_deep_dive.ipynb"&gt;Deep dive&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Wide and Deep&lt;/td&gt; 
   &lt;td&gt;Collaborative Filtering&lt;/td&gt; 
   &lt;td&gt;Deep learning algorithm that can memorize feature interactions and generalize user features. It works in the CPU/GPU environment.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/00_quick_start/wide_deep_movielens.ipynb"&gt;Quick start&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;xLearn/Factorization Machine (FM) &amp;amp; Field-Aware FM (FFM)&lt;/td&gt; 
   &lt;td&gt;Collaborative Filtering&lt;/td&gt; 
   &lt;td&gt;Quick and memory efficient algorithm to predict labels with user/item features. It works in the CPU/GPU environment.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/02_model_collaborative_filtering/fm_deep_dive.ipynb"&gt;Deep dive&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: &lt;sup&gt;*&lt;/sup&gt; indicates algorithms invented/contributed by Microsoft.&lt;/p&gt; 
&lt;p&gt;Independent or incubating algorithms and utilities are candidates for the &lt;a href="https://raw.githubusercontent.com/recommenders-team/recommenders/main/contrib"&gt;contrib&lt;/a&gt; folder. This will house contributions which may not easily fit into the core repository or need time to refactor or mature the code and add necessary tests.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Algorithm&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Example&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;SARplus &lt;sup&gt;*&lt;/sup&gt;&lt;/td&gt; 
   &lt;td&gt;Collaborative Filtering&lt;/td&gt; 
   &lt;td&gt;Optimized implementation of SAR for Spark&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/recommenders-team/recommenders/main/contrib/sarplus/README.md"&gt;Quick start&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Algorithm Comparison&lt;/h3&gt; 
&lt;p&gt;We provide a &lt;a href="https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/06_benchmarks/movielens.ipynb"&gt;benchmark notebook&lt;/a&gt; to illustrate how different algorithms could be evaluated and compared. In this notebook, the MovieLens dataset is split into training/test sets at a 75/25 ratio using a stratified split. A recommendation model is trained using each of the collaborative filtering algorithms below. We utilize empirical parameter values reported in literature &lt;a href="http://mymedialite.net/examples/datasets.html"&gt;here&lt;/a&gt;. For ranking metrics we use &lt;code&gt;k=10&lt;/code&gt; (top 10 recommended items). We run the comparison on a machine with 4 CPUs, 30Gb of RAM, and 1 GPU GeForce GTX 1660 Ti with 6Gb of memory. Spark ALS is run in local standalone mode. In this table we show the results on Movielens 100k, running the algorithms for 15 epochs.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Algo&lt;/th&gt; 
   &lt;th&gt;MAP&lt;/th&gt; 
   &lt;th&gt;nDCG@k&lt;/th&gt; 
   &lt;th&gt;Precision@k&lt;/th&gt; 
   &lt;th&gt;Recall@k&lt;/th&gt; 
   &lt;th&gt;RMSE&lt;/th&gt; 
   &lt;th&gt;MAE&lt;/th&gt; 
   &lt;th&gt;R&lt;sup&gt;2&lt;/sup&gt;&lt;/th&gt; 
   &lt;th&gt;Explained Variance&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/00_quick_start/als_movielens.ipynb"&gt;ALS&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;0.004732&lt;/td&gt; 
   &lt;td&gt;0.044239&lt;/td&gt; 
   &lt;td&gt;0.048462&lt;/td&gt; 
   &lt;td&gt;0.017796&lt;/td&gt; 
   &lt;td&gt;0.965038&lt;/td&gt; 
   &lt;td&gt;0.753001&lt;/td&gt; 
   &lt;td&gt;0.255647&lt;/td&gt; 
   &lt;td&gt;0.251648&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/02_model_collaborative_filtering/cornac_bivae_deep_dive.ipynb"&gt;BiVAE&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;0.146126&lt;/td&gt; 
   &lt;td&gt;0.475077&lt;/td&gt; 
   &lt;td&gt;0.411771&lt;/td&gt; 
   &lt;td&gt;0.219145&lt;/td&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/02_model_collaborative_filtering/cornac_bpr_deep_dive.ipynb"&gt;BPR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;0.132478&lt;/td&gt; 
   &lt;td&gt;0.441997&lt;/td&gt; 
   &lt;td&gt;0.388229&lt;/td&gt; 
   &lt;td&gt;0.212522&lt;/td&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/00_quick_start/embdotbias_movielens.ipynb"&gt;embdotbias&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;0.018954&lt;/td&gt; 
   &lt;td&gt;0.117810&lt;/td&gt; 
   &lt;td&gt;0.104242&lt;/td&gt; 
   &lt;td&gt;0.042450&lt;/td&gt; 
   &lt;td&gt;0.992760&lt;/td&gt; 
   &lt;td&gt;0.776040&lt;/td&gt; 
   &lt;td&gt;0.223344&lt;/td&gt; 
   &lt;td&gt;0.223393&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/02_model_collaborative_filtering/lightgcn_deep_dive.ipynb"&gt;LightGCN&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;0.088526&lt;/td&gt; 
   &lt;td&gt;0.419846&lt;/td&gt; 
   &lt;td&gt;0.379626&lt;/td&gt; 
   &lt;td&gt;0.144336&lt;/td&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/02_model_collaborative_filtering/ncf_deep_dive.ipynb"&gt;NCF&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;0.107720&lt;/td&gt; 
   &lt;td&gt;0.396118&lt;/td&gt; 
   &lt;td&gt;0.347296&lt;/td&gt; 
   &lt;td&gt;0.180775&lt;/td&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
   &lt;td&gt;N/A&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/00_quick_start/sar_movielens.ipynb"&gt;SAR&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;0.110591&lt;/td&gt; 
   &lt;td&gt;0.382461&lt;/td&gt; 
   &lt;td&gt;0.330753&lt;/td&gt; 
   &lt;td&gt;0.176385&lt;/td&gt; 
   &lt;td&gt;1.253805&lt;/td&gt; 
   &lt;td&gt;1.048484&lt;/td&gt; 
   &lt;td&gt;-0.569363&lt;/td&gt; 
   &lt;td&gt;0.030474&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples/02_model_collaborative_filtering/surprise_svd_deep_dive.ipynb"&gt;SVD&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;0.012873&lt;/td&gt; 
   &lt;td&gt;0.095930&lt;/td&gt; 
   &lt;td&gt;0.091198&lt;/td&gt; 
   &lt;td&gt;0.032783&lt;/td&gt; 
   &lt;td&gt;0.938681&lt;/td&gt; 
   &lt;td&gt;0.742690&lt;/td&gt; 
   &lt;td&gt;0.291967&lt;/td&gt; 
   &lt;td&gt;0.291971&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;This project welcomes contributions and suggestions. Before contributing, please see our &lt;a href="https://raw.githubusercontent.com/recommenders-team/recommenders/main/CONTRIBUTING.md"&gt;contribution guidelines&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;This project adheres to this &lt;a href="https://raw.githubusercontent.com/recommenders-team/recommenders/main/CODE_OF_CONDUCT.md"&gt;Code of Conduct&lt;/a&gt; in order to foster a welcoming and inspiring community for all.&lt;/p&gt; 
&lt;h2&gt;Build Status&lt;/h2&gt; 
&lt;p&gt;These tests are the nightly builds, which compute the asynchronous tests. &lt;code&gt;main&lt;/code&gt; is our principal branch and &lt;code&gt;staging&lt;/code&gt; is our development branch. We use &lt;a href="https://docs.pytest.org/"&gt;pytest&lt;/a&gt; for testing python utilities in &lt;a href="https://raw.githubusercontent.com/recommenders-team/recommenders/main/recommenders"&gt;recommenders&lt;/a&gt; and the Recommenders &lt;a href="https://raw.githubusercontent.com/recommenders-team/recommenders/main/recommenders/utils/notebook_utils.py"&gt;notebook executor&lt;/a&gt; for the &lt;a href="https://raw.githubusercontent.com/recommenders-team/recommenders/main/examples"&gt;notebooks&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For more information about the testing pipelines, please see the &lt;a href="https://raw.githubusercontent.com/recommenders-team/recommenders/main/tests/README.md"&gt;test documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;AzureML Nightly Build Status&lt;/h3&gt; 
&lt;p&gt;The nightly build tests are run daily on AzureML.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Build Type&lt;/th&gt; 
   &lt;th&gt;Branch&lt;/th&gt; 
   &lt;th&gt;Status&lt;/th&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th&gt;Branch&lt;/th&gt; 
   &lt;th&gt;Status&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Linux CPU&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;main&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/recommenders/actions/workflows/azureml-cpu-nightly.yml?query=branch%3Amain"&gt;&lt;img src="https://github.com/microsoft/recommenders/actions/workflows/azureml-cpu-nightly.yml/badge.svg?branch=main" alt="azureml-cpu-nightly" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;staging&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/recommenders/actions/workflows/azureml-cpu-nightly.yml?query=branch%3Astaging"&gt;&lt;img src="https://github.com/microsoft/recommenders/actions/workflows/azureml-cpu-nightly.yml/badge.svg?branch=staging" alt="azureml-cpu-nightly" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Linux GPU&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;main&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/recommenders/actions/workflows/azureml-gpu-nightly.yml?query=branch%3Amain"&gt;&lt;img src="https://github.com/microsoft/recommenders/actions/workflows/azureml-gpu-nightly.yml/badge.svg?branch=main" alt="azureml-gpu-nightly" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;staging&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/recommenders/actions/workflows/azureml-gpu-nightly.yml?query=branch%3Astaging"&gt;&lt;img src="https://github.com/microsoft/recommenders/actions/workflows/azureml-gpu-nightly.yml/badge.svg?branch=staging" alt="azureml-gpu-nightly" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Linux Spark&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;main&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/recommenders/actions/workflows/azureml-spark-nightly.yml?query=branch%3Amain"&gt;&lt;img src="https://github.com/microsoft/recommenders/actions/workflows/azureml-spark-nightly.yml/badge.svg?branch=main" alt="azureml-spark-nightly" /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;staging&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://github.com/microsoft/recommenders/actions/workflows/azureml-spark-nightly.yml?query=branch%3Astaging"&gt;&lt;img src="https://github.com/microsoft/recommenders/actions/workflows/azureml-spark-nightly.yml/badge.svg?branch=staging" alt="azureml-spark-nightly" /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;References&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;FREE COURSE&lt;/strong&gt;: M. Gonz√°lez-Fierro, "Recommendation Systems: A Practical Introduction", LinkedIn Learning, 2024. &lt;a href="https://www.linkedin.com/learning/recommendation-systems-a-practical-introduction"&gt;Available on this link&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;D. Li, J. Lian, L. Zhang, K. Ren, D. Lu, T. Wu, X. Xie, "Recommender Systems: Frontiers and Practices", Springer, Beijing, 2024. &lt;a href="https://www.amazon.com/Recommender-Systems-Frontiers-Practices-Dongsheng/dp/9819989639/"&gt;Available on this link&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;A. Argyriou, M. Gonz√°lez-Fierro, and L. Zhang, "Microsoft Recommenders: Best Practices for Production-Ready Recommendation Systems", &lt;em&gt;WWW 2020: International World Wide Web Conference Taipei&lt;/em&gt;, 2020. Available online: &lt;a href="https://dl.acm.org/doi/abs/10.1145/3366424.3382692"&gt;https://dl.acm.org/doi/abs/10.1145/3366424.3382692&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;S. Graham, J.K. Min, T. Wu, "Microsoft recommenders: tools to accelerate developing recommender systems", &lt;em&gt;RecSys '19: Proceedings of the 13th ACM Conference on Recommender Systems&lt;/em&gt;, 2019. Available online: &lt;a href="https://dl.acm.org/doi/10.1145/3298689.3346967"&gt;https://dl.acm.org/doi/10.1145/3298689.3346967&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;L. Zhang, T. Wu, X. Xie, A. Argyriou, M. Gonz√°lez-Fierro and J. Lian, "Building Production-Ready Recommendation System at Scale", &lt;em&gt;ACM SIGKDD Conference on Knowledge Discovery and Data Mining 2019 (KDD 2019)&lt;/em&gt;, 2019.&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>robusta-dev/holmesgpt</title>
      <link>https://github.com/robusta-dev/holmesgpt</link>
      <description>&lt;p&gt;Your 24/7 On-Call AI Agent - Solve Alerts Faster with Automatic Correlations, Investigations, and More&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;h1 align="center"&gt;AI Agent for Cloud Troubleshooting and Alert Investigation&lt;/h1&gt; 
 &lt;p&gt;HolmesGPT is an AI agent for investigating problems in your cloud, finding the root cause, and suggesting remediations. It has dozens of built-in integrations for cloud providers, observability tools, and on-call systems.&lt;/p&gt; 
 &lt;p&gt;HolmesGPT has been submitted to the CNCF as a sandbox project (&lt;a href="https://github.com/cncf/sandbox/issues/392"&gt;view status&lt;/a&gt;). You can learn more about HolmesGPT's maintainers and adopters &lt;a href="https://raw.githubusercontent.com/robusta-dev/holmesgpt/master/ADOPTERS.md"&gt;here&lt;/a&gt;.&lt;/p&gt; 
 &lt;p align="center"&gt; &lt;a href="https://raw.githubusercontent.com/robusta-dev/holmesgpt/master/#how-it-works"&gt;&lt;strong&gt;How it Works&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/robusta-dev/holmesgpt/master/#installation"&gt;&lt;strong&gt;Installation&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/robusta-dev/holmesgpt/master/#supported-llm-providers"&gt;&lt;strong&gt;LLM Providers&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://www.youtube.com/watch?v=TfQfx65LsDQ"&gt;&lt;strong&gt;YouTube Demo&lt;/strong&gt;&lt;/a&gt; | &lt;a href="https://deepwiki.com/robusta-dev/holmesgpt"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki" /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;img src="https://holmesgpt.dev/assets/HolmesInvestigation.gif" alt="HolmesGPT Investigation Demo" /&gt;&lt;/p&gt; 
&lt;h2&gt;How it Works&lt;/h2&gt; 
&lt;p&gt;HolmesGPT connects AI models with live observability data and organizational knowledge. It uses an &lt;strong&gt;agentic loop&lt;/strong&gt; to analyze data from multiple sources and identify possible root causes.&lt;/p&gt; 
&lt;img width="3114" alt="holmesgpt-architecture-diagram" src="https://github.com/user-attachments/assets/f659707e-1958-4add-9238-8565a5e3713a" /&gt; 
&lt;h3&gt;üîó Data Sources&lt;/h3&gt; 
&lt;p&gt;HolmesGPT integrates with popular observability and cloud platforms. The following data sources ("toolsets") are built-in. &lt;a href="https://raw.githubusercontent.com/robusta-dev/holmesgpt/master/#customizing-holmesgpt"&gt;Add your own&lt;/a&gt;.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Data Source&lt;/th&gt; 
   &lt;th&gt;Status&lt;/th&gt; 
   &lt;th&gt;Notes&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://holmesgpt.dev/data-sources/builtin-toolsets/argocd/"&gt;&lt;img src="https://raw.githubusercontent.com/robusta-dev/holmesgpt/master/images/integration_logos/argocd-icon.png" alt="ArgoCD" width="20" style="vertical-align: middle;" /&gt; &lt;strong&gt;ArgoCD&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;Get status, history and manifests and more of apps, projects and clusters&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://holmesgpt.dev/data-sources/builtin-toolsets/aws/"&gt;&lt;img src="https://raw.githubusercontent.com/robusta-dev/holmesgpt/master/images/integration_logos/aws_rds_logo.png" alt="AWS RDS" width="20" style="vertical-align: middle;" /&gt; &lt;strong&gt;AWS RDS&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;Fetch events, instances, slow query logs and more&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://holmesgpt.dev/data-sources/builtin-toolsets/confluence/"&gt;&lt;img src="https://raw.githubusercontent.com/robusta-dev/holmesgpt/master/images/integration_logos/confluence_logo.png" alt="Confluence" width="20" style="vertical-align: middle;" /&gt; &lt;strong&gt;Confluence&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;Private runbooks and documentation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://holmesgpt.dev/data-sources/builtin-toolsets/coralogix-logs/"&gt;&lt;img src="https://raw.githubusercontent.com/robusta-dev/holmesgpt/master/images/integration_logos/coralogix-icon.png" alt="Coralogix Logs" width="20" style="vertical-align: middle;" /&gt; &lt;strong&gt;Coralogix Logs&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;Retrieve logs for any resource&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://holmesgpt.dev/data-sources/builtin-toolsets/datetime/"&gt;&lt;img src="https://raw.githubusercontent.com/robusta-dev/holmesgpt/master/images/integration_logos/date_time_icon.png" alt="Datetime" width="20" style="vertical-align: middle;" /&gt; &lt;strong&gt;Datetime&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;Date and time-related operations&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://holmesgpt.dev/data-sources/builtin-toolsets/docker/"&gt;&lt;img src="https://raw.githubusercontent.com/robusta-dev/holmesgpt/master/images/integration_logos/docker_logo.png" alt="Docker" width="20" style="vertical-align: middle;" /&gt; &lt;strong&gt;Docker&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;Get images, logs, events, history and more&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://holmesgpt.dev/data-sources/builtin-toolsets/github/"&gt;&lt;img src="https://raw.githubusercontent.com/robusta-dev/holmesgpt/master/images/integration_logos/github_logo.png" alt="GitHub" width="20" style="vertical-align: middle;" /&gt; &lt;strong&gt;GitHub&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;üü° Beta&lt;/td&gt; 
   &lt;td&gt;Remediate alerts by opening pull requests with fixes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://holmesgpt.dev/data-sources/builtin-toolsets/datadog/"&gt;&lt;img src="https://raw.githubusercontent.com/robusta-dev/holmesgpt/master/images/integration_logos/datadog_logo.png" alt="DataDog" width="20" style="vertical-align: middle;" /&gt; &lt;strong&gt;DataDog&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;üü° Beta&lt;/td&gt; 
   &lt;td&gt;Fetches log data from datadog&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://holmesgpt.dev/data-sources/builtin-toolsets/grafanaloki/"&gt;&lt;img src="https://raw.githubusercontent.com/robusta-dev/holmesgpt/master/images/integration_logos/grafana_loki-icon.png" alt="Loki" width="20" style="vertical-align: middle;" /&gt; &lt;strong&gt;Loki&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;Query logs for Kubernetes resources or any query&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://holmesgpt.dev/data-sources/builtin-toolsets/grafanatempo/"&gt;&lt;img src="https://raw.githubusercontent.com/robusta-dev/holmesgpt/master/images/integration_logos/tempo_logo.png" alt="Tempo" width="20" style="vertical-align: middle;" /&gt; &lt;strong&gt;Tempo&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;Fetch trace info, debug issues like high latency in application.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://holmesgpt.dev/data-sources/builtin-toolsets/helm/"&gt;&lt;img src="https://raw.githubusercontent.com/robusta-dev/holmesgpt/master/images/integration_logos/helm_logo.png" alt="Helm" width="20" style="vertical-align: middle;" /&gt; &lt;strong&gt;Helm&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;Release status, chart metadata, and values&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://holmesgpt.dev/data-sources/builtin-toolsets/internet/"&gt;&lt;img src="https://raw.githubusercontent.com/robusta-dev/holmesgpt/master/images/integration_logos/http-icon.png" alt="Internet" width="20" style="vertical-align: middle;" /&gt; &lt;strong&gt;Internet&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;Public runbooks, community docs etc&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://holmesgpt.dev/data-sources/builtin-toolsets/kafka/"&gt;&lt;img src="https://raw.githubusercontent.com/robusta-dev/holmesgpt/master/images/integration_logos/kafka_logo.png" alt="Kafka" width="20" style="vertical-align: middle;" /&gt; &lt;strong&gt;Kafka&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;Fetch metadata, list consumers and topics or find lagging consumer groups&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://holmesgpt.dev/data-sources/builtin-toolsets/kubernetes/"&gt;&lt;img src="https://raw.githubusercontent.com/robusta-dev/holmesgpt/master/images/integration_logos/kubernetes-icon.png" alt="Kubernetes" width="20" style="vertical-align: middle;" /&gt; &lt;strong&gt;Kubernetes&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;Pod logs, K8s events, and resource status (kubectl describe)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://holmesgpt.dev/data-sources/builtin-toolsets/newrelic/"&gt;&lt;img src="https://raw.githubusercontent.com/robusta-dev/holmesgpt/master/images/integration_logos/newrelic_logo.png" alt="NewRelic" width="20" style="vertical-align: middle;" /&gt; &lt;strong&gt;NewRelic&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;üü° Beta&lt;/td&gt; 
   &lt;td&gt;Investigate alerts, query tracing data&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://holmesgpt.dev/data-sources/builtin-toolsets/opensearch-status/"&gt;&lt;img src="https://raw.githubusercontent.com/robusta-dev/holmesgpt/master/images/integration_logos/opensearchserverless-icon.png" alt="OpenSearch" width="20" style="vertical-align: middle;" /&gt; &lt;strong&gt;OpenSearch&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;Query health, shard, and settings related info of one or more clusters&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://holmesgpt.dev/data-sources/builtin-toolsets/prometheus/"&gt;&lt;img src="https://raw.githubusercontent.com/robusta-dev/holmesgpt/master/images/integration_logos/prometheus-icon.png" alt="Prometheus" width="20" style="vertical-align: middle;" /&gt; &lt;strong&gt;Prometheus&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;Investigate alerts, query metrics and generate PromQL queries&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://holmesgpt.dev/data-sources/builtin-toolsets/rabbitmq/"&gt;&lt;img src="https://raw.githubusercontent.com/robusta-dev/holmesgpt/master/images/integration_logos/rabbit_mq_logo.png" alt="RabbitMQ" width="20" style="vertical-align: middle;" /&gt; &lt;strong&gt;RabbitMQ&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;Info about partitions, memory/disk alerts to troubleshoot split-brain scenarios and more&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://holmesgpt.dev/data-sources/builtin-toolsets/robusta/"&gt;&lt;img src="https://raw.githubusercontent.com/robusta-dev/holmesgpt/master/images/integration_logos/robusta_logo.png" alt="Robusta" width="20" style="vertical-align: middle;" /&gt; &lt;strong&gt;Robusta&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;Multi-cluster monitoring, historical change data, user-configured runbooks, PromQL graphs and more&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href="https://holmesgpt.dev/data-sources/builtin-toolsets/slab/"&gt;&lt;img src="https://raw.githubusercontent.com/robusta-dev/holmesgpt/master/images/integration_logos/slab_logo.png" alt="Slab" width="20" style="vertical-align: middle;" /&gt; &lt;strong&gt;Slab&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;Team knowledge base and runbooks on demand&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;üöÄ End-to-End Automation&lt;/h3&gt; 
&lt;p&gt;HolmesGPT can fetch alerts/tickets to investigate from external systems, then write the analysis back to the source or Slack.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Integration&lt;/th&gt; 
   &lt;th&gt;Status&lt;/th&gt; 
   &lt;th&gt;Notes&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Slack&lt;/td&gt; 
   &lt;td&gt;üü° Beta&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://www.loom.com/share/afcd81444b1a4adfaa0bbe01c37a4847"&gt;Demo.&lt;/a&gt; Tag HolmesGPT bot in any Slack message&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Prometheus/AlertManager&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;Robusta SaaS or HolmesGPT CLI&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;PagerDuty&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;HolmesGPT CLI only&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpsGenie&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;HolmesGPT CLI only&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Jira&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;HolmesGPT CLI only&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;GitHub&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;td&gt;HolmesGPT CLI only&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;a href="https://holmesgpt.dev/installation/cli-installation/"&gt; &lt;img src="https://raw.githubusercontent.com/robusta-dev/holmesgpt/master/images/integration_logos/all-installation-methods.png" alt="All Installation Methods" style="max-width:100%; height:auto;" /&gt; &lt;/a&gt; 
&lt;p&gt;Read the &lt;a href="https://holmesgpt.dev/installation/cli-installation/"&gt;installation documentation&lt;/a&gt; to learn how to install HolmesGPT.&lt;/p&gt; 
&lt;h2&gt;Supported LLM Providers&lt;/h2&gt; 
&lt;a href="https://holmesgpt.dev/ai-providers/"&gt; &lt;img src="https://raw.githubusercontent.com/robusta-dev/holmesgpt/master/images/integration_logos/all-integration-providers.png" alt="All Integration Providers" style="max-width:100%; height:auto;" /&gt; &lt;/a&gt; 
&lt;p&gt;Read the &lt;a href="https://holmesgpt.dev/ai-providers/"&gt;LLM Providers documentation&lt;/a&gt; to learn how to set up your LLM API key.&lt;/p&gt; 
&lt;h2&gt;Using HolmesGPT&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;In the Robusta SaaS: Go to &lt;a href="https://platform.robusta.dev/signup/?utm_source=github&amp;amp;utm_medium=holmesgpt-readme&amp;amp;utm_content=ways_to_use_holmesgpt_section"&gt;platform.robusta.dev&lt;/a&gt; and use Holmes from your browser&lt;/li&gt; 
 &lt;li&gt;With HolmesGPT CLI: &lt;a href="https://holmesgpt.dev/ai-providers/"&gt;setup an LLM API key&lt;/a&gt; and ask Holmes a question üëá&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;holmes ask "what pods are unhealthy and why?"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also provide files as context:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;holmes ask "summarize the key points in this document" -f ./mydocument.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also load the prompt from a file using the &lt;code&gt;--prompt-file&lt;/code&gt; option:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;holmes ask --prompt-file ~/long-prompt.txt

Enter interactive mode to ask follow-up questions:
```bash
holmes ask "what pods are unhealthy and why?" --interactive
# or
holmes ask "what pods are unhealthy and why?" -i
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Also supported:&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;HolmesGPT CLI: investigate Prometheus alerts&lt;/summary&gt; 
 &lt;p&gt;Pull alerts from AlertManager and investigate them with HolmesGPT:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;holmes investigate alertmanager --alertmanager-url http://localhost:9093
# if on Mac OS and using the Holmes Docker imageüëá
#  holmes investigate alertmanager --alertmanager-url http://docker.for.mac.localhost:9093
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;b&gt;To investigate alerts in your browser, sign up for a free trial of &lt;a href="https://platform.robusta.dev/signup/?utm_source=github&amp;amp;utm_medium=holmesgpt-readme&amp;amp;utm_content=ways_to_use_holmesgpt_section"&gt;Robusta SaaS&lt;/a&gt;. &lt;/b&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;b&gt;Optional:&lt;/b&gt; port-forward to AlertManager before running the command mentioned above (if running Prometheus inside Kubernetes)&lt;/p&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;kubectl port-forward alertmanager-robusta-kube-prometheus-st-alertmanager-0 9093:9093 &amp;amp;
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;HolmesGPT CLI: investigate PagerDuty and OpsGenie alerts&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;holmes investigate opsgenie --opsgenie-api-key &amp;lt;OPSGENIE_API_KEY&amp;gt;
holmes investigate pagerduty --pagerduty-api-key &amp;lt;PAGERDUTY_API_KEY&amp;gt;
# to write the analysis back to the incident as a comment
holmes investigate pagerduty --pagerduty-api-key &amp;lt;PAGERDUTY_API_KEY&amp;gt; --update
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;For more details, run &lt;code&gt;holmes investigate &amp;lt;source&amp;gt; --help&lt;/code&gt;&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;Customizing HolmesGPT&lt;/h2&gt; 
&lt;p&gt;HolmesGPT can investigate many issues out of the box, with no customization or training. Optionally, you can extend Holmes to improve results:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Custom Data Sources&lt;/strong&gt;: Add data sources (toolsets) to improve investigations&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;If using Robusta SaaS: See &lt;a href="https://holmesgpt.dev/data-sources/custom-toolsets/"&gt;here&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;If using the CLI: Use &lt;code&gt;-t&lt;/code&gt; flag with &lt;a href="https://raw.githubusercontent.com/robusta-dev/holmesgpt/master/examples/custom_toolset.yaml"&gt;custom toolset files&lt;/a&gt; or add to &lt;code&gt;~/.holmes/config.yaml&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Custom Runbooks&lt;/strong&gt;: Give HolmesGPT instructions for known alerts:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;If using Robusta SaaS: Use the Robusta UI to add runbooks&lt;/li&gt; 
 &lt;li&gt;If using the CLI: Use &lt;code&gt;-r&lt;/code&gt; flag with &lt;a href="https://raw.githubusercontent.com/robusta-dev/holmesgpt/master/examples/custom_runbooks.yaml"&gt;custom runbook files&lt;/a&gt; or add to &lt;code&gt;~/.holmes/config.yaml&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can save common settings and API Keys in a config file to avoid passing them from the CLI each time:&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Reading settings from a config file&lt;/summary&gt; 
 &lt;p&gt;You can save common settings and API keys in config file for re-use. Place the config file in &lt;code&gt;~/.holmes/config.yaml`&lt;/code&gt; or pass it using the &lt;code&gt; --config&lt;/code&gt;&lt;/p&gt; 
 &lt;p&gt;You can view an example config file with all available settings &lt;a href="https://raw.githubusercontent.com/robusta-dev/holmesgpt/master/config.example.yaml"&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h2&gt;üîê Data Privacy&lt;/h2&gt; 
&lt;p&gt;By design, HolmesGPT has &lt;strong&gt;read-only access&lt;/strong&gt; and respects RBAC permissions. It is safe to run in production environments.&lt;/p&gt; 
&lt;p&gt;We do &lt;strong&gt;not&lt;/strong&gt; train HolmesGPT on your data. Data sent to Robusta SaaS is private to your account.&lt;/p&gt; 
&lt;p&gt;For extra privacy, &lt;a href="https://holmesgpt.dev/ai-providers/"&gt;bring an API key&lt;/a&gt; for your own AI model.&lt;/p&gt; 
&lt;h2&gt;Evals&lt;/h2&gt; 
&lt;p&gt;Because HolmesGPT relies on LLMs, it relies on &lt;a href="https://holmesgpt.dev/development/evals/"&gt;a suite of pytest based evaluations&lt;/a&gt; to ensure the prompt and HolmesGPT's default set of tools work as expected with LLMs.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://holmesgpt.dev/development/evals/"&gt;Introduction to HolmesGPT's evals&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://holmesgpt.dev/development/evals/adding-new-eval/"&gt;Write your own evals&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://holmesgpt.dev/development/evals/reporting/"&gt;Use Braintrust to view analyze results (optional)&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Distributed under the MIT License. See &lt;a href="https://github.com/robusta-dev/holmesgpt/raw/master/LICENSE.txt"&gt;LICENSE.txt&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;!-- Change License --&gt; 
&lt;h2&gt;Community&lt;/h2&gt; 
&lt;p&gt;Join our community meetings to discuss the HolmesGPT roadmap and share feedback:&lt;/p&gt; 
&lt;p&gt;üìÖ &lt;strong&gt;First Community Meeting:&lt;/strong&gt; Thursday, August 21, 2025&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Time:&lt;/strong&gt; 8:00-9:00 AM PT / 11:00 AM-12:00 PM ET / 8:30-9:30 PM IST&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Where:&lt;/strong&gt; &lt;a href="https://meet.google.com/jxc-ujyf-xwy"&gt;Google Meet&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Agenda:&lt;/strong&gt; &lt;a href="https://github.com/orgs/robusta-dev/projects/2"&gt;Roadmap discussion&lt;/a&gt;, community feedback, and Q&amp;amp;A&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://docs.google.com/document/d/1sIHCcTivyzrF5XNvos7ZT_UcxEOqgwfawsTbb9wMJe4/edit?tab=t.0"&gt;üìù Meeting Notes&lt;/a&gt; | &lt;a href="https://holmesgpt.dev/community/"&gt;üìã Full Details&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Support&lt;/h2&gt; 
&lt;p&gt;If you have any questions, feel free to message us on &lt;a href="https://bit.ly/robusta-slack"&gt;robustacommunity.slack.com&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;How to Contribute&lt;/h2&gt; 
&lt;p&gt;Please read our &lt;a href="https://raw.githubusercontent.com/robusta-dev/holmesgpt/master/CONTRIBUTING.md"&gt;CONTRIBUTING.md&lt;/a&gt; for guidelines and instructions.&lt;/p&gt; 
&lt;p&gt;For help, contact us on &lt;a href="https://bit.ly/robusta-slack"&gt;Slack&lt;/a&gt; or ask &lt;a href="https://deepwiki.com/robusta-dev/holmesgpt"&gt;DeepWiki AI&lt;/a&gt; your questions.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://deepwiki.com/robusta-dev/holmesgpt"&gt;&lt;img src="https://deepwiki.com/badge.svg?sanitize=true" alt="Ask DeepWiki" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Fosowl/agenticSeek</title>
      <link>https://github.com/Fosowl/agenticSeek</link>
      <description>&lt;p&gt;Fully Local Manus AI. No APIs, No $200 monthly bills. Enjoy an autonomous agent that thinks, browses the web, and code for the sole cost of electricity. üîî Official updates only via twitter @Martin993886460 (Beware of fake account)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AgenticSeek: Private, Local Manus Alternative.&lt;/h1&gt; 
&lt;p align="center"&gt; &lt;img align="center" src="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/agentic_seek_logo.png" width="300" height="300" alt="Agentic Seek Logo" /&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;English | &lt;a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_CHS.md"&gt;‰∏≠Êñá&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_CHT.md"&gt;ÁπÅÈ´î‰∏≠Êñá&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_FR.md"&gt;Fran√ßais&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_JP.md"&gt;Êó•Êú¨Ë™û&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_PTBR.md"&gt;Portugu√™s (Brasil)&lt;/a&gt; | &lt;a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_ES.md"&gt;Espa√±ol&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;A &lt;strong&gt;100% local alternative to Manus AI&lt;/strong&gt;, this voice-enabled AI assistant autonomously browses the web, writes code, and plans tasks while keeping all data on your device. Tailored for local reasoning models, it runs entirely on your hardware, ensuring complete privacy and zero cloud dependency.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://fosowl.github.io/agenticSeek.html"&gt;&lt;img src="https://img.shields.io/static/v1?label=Website&amp;amp;message=AgenticSeek&amp;amp;color=blue&amp;amp;style=flat-square" alt="Visit AgenticSeek" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/badge/license-GPL--3.0-green" alt="License" /&gt; &lt;a href="https://discord.gg/8hGDaME3TC"&gt;&lt;img src="https://img.shields.io/badge/Discord-Join%20Us-7289DA?logo=discord&amp;amp;logoColor=white" alt="Discord" /&gt;&lt;/a&gt; &lt;a href="https://x.com/Martin993886460"&gt;&lt;img src="https://img.shields.io/twitter/url/https/twitter.com/fosowl.svg?style=social&amp;amp;label=Update%20%40Fosowl" alt="Twitter" /&gt;&lt;/a&gt; &lt;a href="https://github.com/Fosowl/agenticSeek/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/Fosowl/agenticSeek?style=social" alt="GitHub stars" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Why AgenticSeek ?&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;üîí Fully Local &amp;amp; Private - Everything runs on your machine ‚Äî no cloud, no data sharing. Your files, conversations, and searches stay private.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üåê Smart Web Browsing - AgenticSeek can browse the internet by itself ‚Äî search, read, extract info, fill web form ‚Äî all hands-free.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üíª Autonomous Coding Assistant - Need code? It can write, debug, and run programs in Python, C, Go, Java, and more ‚Äî all without supervision.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üß† Smart Agent Selection - You ask, it figures out the best agent for the job automatically. Like having a team of experts ready to help.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üìã Plans &amp;amp; Executes Complex Tasks - From trip planning to complex projects ‚Äî it can split big tasks into steps and get things done using multiple AI agents.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üéôÔ∏è Voice-Enabled - Clean, fast, futuristic voice and speech to text allowing you to talk to it like it's your personal AI from a sci-fi movie. (In progress)&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;strong&gt;Demo&lt;/strong&gt;&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;em&gt;Can you search for the agenticSeek project, learn what skills are required, then open the CV_candidates.zip and then tell me which match best the project&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;a href="https://github.com/user-attachments/assets/b8ca60e9-7b3b-4533-840e-08f9ac426316"&gt;https://github.com/user-attachments/assets/b8ca60e9-7b3b-4533-840e-08f9ac426316&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Disclaimer: This demo, including all the files that appear (e.g: CV_candidates.zip), are entirely fictional. We are not a corporation, we seek open-source contributors not candidates.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;üõ†‚ö†Ô∏èÔ∏è &lt;strong&gt;Active Work in Progress&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;üôè This project started as a side-project and has zero roadmap and zero funding. It's grown way beyond what I expected by ending in GitHub Trending. Contributions, feedback, and patience are deeply appreciated.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Prerequisites&lt;/h2&gt; 
&lt;p&gt;Before you begin, ensure you have the following software installed:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Git:&lt;/strong&gt; For cloning the repository. &lt;a href="https://git-scm.com/downloads"&gt;Download Git&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Python 3.10.x:&lt;/strong&gt; We strongly recommend using Python version 3.10.x. Using other versions might lead to dependency errors. &lt;a href="https://www.python.org/downloads/release/python-3100/"&gt;Download Python 3.10&lt;/a&gt; (pick a 3.10.x version).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Docker Engine &amp;amp; Docker Compose:&lt;/strong&gt; For running bundled services like SearxNG. 
  &lt;ul&gt; 
   &lt;li&gt;Install Docker Desktop (which includes Docker Compose V2): &lt;a href="https://docs.docker.com/desktop/install/windows-install/"&gt;Windows&lt;/a&gt; | &lt;a href="https://docs.docker.com/desktop/install/mac-install/"&gt;Mac&lt;/a&gt; | &lt;a href="https://docs.docker.com/desktop/install/linux-install/"&gt;Linux&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Alternatively, install Docker Engine and Docker Compose separately on Linux: &lt;a href="https://docs.docker.com/engine/install/"&gt;Docker Engine&lt;/a&gt; | &lt;a href="https://docs.docker.com/compose/install/"&gt;Docker Compose&lt;/a&gt; (ensure you install Compose V2, e.g., &lt;code&gt;sudo apt-get install docker-compose-plugin&lt;/code&gt;).&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;1. &lt;strong&gt;Clone the repository and setup&lt;/strong&gt;&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;git clone https://github.com/Fosowl/agenticSeek.git
cd agenticSeek
mv .env.example .env
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. Change the .env file content&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;SEARXNG_BASE_URL="http://127.0.0.1:8080"
REDIS_BASE_URL="redis://redis:6379/0"
WORK_DIR="/Users/mlg/Documents/workspace_for_ai"
OLLAMA_PORT="11434"
LM_STUDIO_PORT="1234"
CUSTOM_ADDITIONAL_LLM_PORT="11435"
OPENAI_API_KEY='optional'
DEEPSEEK_API_KEY='optional'
OPENROUTER_API_KEY='optional'
TOGETHER_API_KEY='optional'
GOOGLE_API_KEY='optional'
ANTHROPIC_API_KEY='optional'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Update the &lt;code&gt;.env&lt;/code&gt; file with your own values as needed:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;SEARXNG_BASE_URL&lt;/strong&gt;: Leave unchanged&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;REDIS_BASE_URL&lt;/strong&gt;: Leave unchanged&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;WORK_DIR&lt;/strong&gt;: Path to your working directory on your local machine. AgenticSeek will be able to read and interact with these files.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;OLLAMA_PORT&lt;/strong&gt;: Port number for the Ollama service.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;LM_STUDIO_PORT&lt;/strong&gt;: Port number for the LM Studio service.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;CUSTOM_ADDITIONAL_LLM_PORT&lt;/strong&gt;: Port for any additional custom LLM service.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;API Key are totally optional for user who choose to run LLM locally. Which is the primary purpose of this project. Leave empty if you have sufficient hardware&lt;/strong&gt;&lt;/p&gt; 
&lt;h3&gt;3. &lt;strong&gt;Start Docker&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;Make sure Docker is installed and running on your system. You can start Docker using the following commands:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;On Linux/macOS:&lt;/strong&gt;&lt;br /&gt; Open a terminal and run:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;sudo systemctl start docker
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Or launch Docker Desktop from your applications menu if installed.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;On Windows:&lt;/strong&gt;&lt;br /&gt; Start Docker Desktop from the Start menu.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can verify Docker is running by executing:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;docker info
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you see information about your Docker installation, it is running correctly.&lt;/p&gt; 
&lt;p&gt;See the table of &lt;a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/#list-of-local-providers"&gt;Local Providers&lt;/a&gt; below for a summary.&lt;/p&gt; 
&lt;p&gt;Next step: &lt;a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/#start-services-and-run"&gt;Run AgenticSeek locally&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;See the &lt;a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/#troubleshooting"&gt;Troubleshooting&lt;/a&gt; section if you are having issues.&lt;/em&gt; &lt;em&gt;If your hardware can't run LLMs locally, see &lt;a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/#setup-to-run-with-an-api"&gt;Setup to run with an API&lt;/a&gt;.&lt;/em&gt; &lt;em&gt;For detailed &lt;code&gt;config.ini&lt;/code&gt; explanations, see &lt;a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/#config"&gt;Config Section&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Setup for running LLM locally on your machine&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Hardware Requirements:&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;To run LLMs locally, you'll need sufficient hardware. At a minimum, a GPU capable of running Magistral, Qwen or Deepseek 14B is required. See the FAQ for detailed model/performance recommendations.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Setup your local provider&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Start your local provider, for example with ollama:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;ollama serve
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See below for a list of local supported provider.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Update the config.ini&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Change the config.ini file to set the provider_name to a supported provider and provider_model to a LLM supported by your provider. We recommend reasoning model such as &lt;em&gt;Magistral&lt;/em&gt; or &lt;em&gt;Deepseek&lt;/em&gt;.&lt;/p&gt; 
&lt;p&gt;See the &lt;strong&gt;FAQ&lt;/strong&gt; at the end of the README for required hardware.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;[MAIN]
is_local = True # Whenever you are running locally or with remote provider.
provider_name = ollama # or lm-studio, openai, etc..
provider_model = deepseek-r1:14b # choose a model that fit your hardware
provider_server_address = 127.0.0.1:11434
agent_name = Jarvis # name of your AI
recover_last_session = True # whenever to recover the previous session
save_session = True # whenever to remember the current session
speak = False # text to speech
listen = False # Speech to text, only for CLI, experimental
jarvis_personality = False # Whenever to use a more "Jarvis" like personality (experimental)
languages = en zh # The list of languages, Text to speech will default to the first language on the list
[BROWSER]
headless_browser = True # leave unchanged unless using CLI on host.
stealth_mode = True # Use undetected selenium to reduce browser detection
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;The &lt;code&gt;config.ini&lt;/code&gt; file format does not support comments. Do not copy and paste the example configuration directly, as comments will cause errors. Instead, manually modify the &lt;code&gt;config.ini&lt;/code&gt; file with your desired settings, excluding any comments.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Do &lt;em&gt;NOT&lt;/em&gt; set provider_name to &lt;code&gt;openai&lt;/code&gt; if using LM-studio for running LLMs. Set it to &lt;code&gt;lm-studio&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Some provider (eg: lm-studio) require you to have &lt;code&gt;http://&lt;/code&gt; in front of the IP. For example &lt;code&gt;http://127.0.0.1:1234&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;List of local providers&lt;/strong&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Provider&lt;/th&gt; 
   &lt;th&gt;Local?&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ollama&lt;/td&gt; 
   &lt;td&gt;Yes&lt;/td&gt; 
   &lt;td&gt;Run LLMs locally with ease using ollama as a LLM provider&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;lm-studio&lt;/td&gt; 
   &lt;td&gt;Yes&lt;/td&gt; 
   &lt;td&gt;Run LLM locally with LM studio (set &lt;code&gt;provider_name&lt;/code&gt; to &lt;code&gt;lm-studio&lt;/code&gt;)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;openai&lt;/td&gt; 
   &lt;td&gt;Yes&lt;/td&gt; 
   &lt;td&gt;Use openai compatible API (eg: llama.cpp server)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Next step: &lt;a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/#Start-services-and-Run"&gt;Start services and run AgenticSeek&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;See the &lt;a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/#troubleshooting"&gt;Troubleshooting&lt;/a&gt; section if you are having issues.&lt;/em&gt; &lt;em&gt;If your hardware can't run LLMs locally, see &lt;a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/#setup-to-run-with-an-api"&gt;Setup to run with an API&lt;/a&gt;.&lt;/em&gt; &lt;em&gt;For detailed &lt;code&gt;config.ini&lt;/code&gt; explanations, see &lt;a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/#config"&gt;Config Section&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt; 
&lt;h2&gt;Setup to run with an API&lt;/h2&gt; 
&lt;p&gt;This setup uses external, cloud-based LLM providers. You'll need an API key from your chosen service.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;1. Choose an API Provider and Get an API Key:&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Refer to the &lt;a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/#list-of-api-providers"&gt;List of API Providers&lt;/a&gt; below. Visit their websites to sign up and obtain an API key.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;2. Set Your API Key as an Environment Variable:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Linux/macOS:&lt;/strong&gt; Open your terminal and use the &lt;code&gt;export&lt;/code&gt; command. It's best to add this to your shell's profile file (e.g., &lt;code&gt;~/.bashrc&lt;/code&gt;, &lt;code&gt;~/.zshrc&lt;/code&gt;) for persistence.&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;export PROVIDER_API_KEY="your_api_key_here" 
# Replace PROVIDER_API_KEY with the specific variable name, e.g., OPENAI_API_KEY, GOOGLE_API_KEY
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Example for TogetherAI:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;export TOGETHER_API_KEY="xxxxxxxxxxxxxxxxxxxxxx"
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Windows:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Command Prompt (Temporary for current session):&lt;/strong&gt; &lt;pre&gt;&lt;code class="language-cmd"&gt;set PROVIDER_API_KEY=your_api_key_here
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;PowerShell (Temporary for current session):&lt;/strong&gt; &lt;pre&gt;&lt;code class="language-powershell"&gt;$env:PROVIDER_API_KEY="your_api_key_here"
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Permanently:&lt;/strong&gt; Search for "environment variables" in the Windows search bar, click "Edit the system environment variables," then click the "Environment Variables..." button. Add a new User variable with the appropriate name (e.g., &lt;code&gt;OPENAI_API_KEY&lt;/code&gt;) and your key as the value.&lt;/li&gt; 
  &lt;/ul&gt; &lt;p&gt;&lt;em&gt;(See FAQ: &lt;a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/#how-do-i-set-api-keys"&gt;How do I set API keys?&lt;/a&gt; for more details).&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;3. Update &lt;code&gt;config.ini&lt;/code&gt;:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-ini"&gt;[MAIN]
is_local = False
provider_name = openai # Or google, deepseek, togetherAI, huggingface
provider_model = gpt-3.5-turbo # Or gemini-1.5-flash, deepseek-chat, mistralai/Mixtral-8x7B-Instruct-v0.1 etc.
provider_server_address = # Typically ignored or can be left blank when is_local = False for most APIs
# ... other settings ...
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Warning:&lt;/em&gt; Make sure there are no trailing spaces in the &lt;code&gt;config.ini&lt;/code&gt; values.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;List of API Providers&lt;/strong&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Provider&lt;/th&gt; 
   &lt;th&gt;&lt;code&gt;provider_name&lt;/code&gt;&lt;/th&gt; 
   &lt;th&gt;Local?&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;API Key Link (Examples)&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenAI&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;openai&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;No&lt;/td&gt; 
   &lt;td&gt;Use ChatGPT models via OpenAI's API.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://platform.openai.com/signup"&gt;platform.openai.com/signup&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Google Gemini&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;google&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;No&lt;/td&gt; 
   &lt;td&gt;Use Google Gemini models via Google AI Studio.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://aistudio.google.com/keys"&gt;aistudio.google.com/keys&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Deepseek&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;deepseek&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;No&lt;/td&gt; 
   &lt;td&gt;Use Deepseek models via their API.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://platform.deepseek.com"&gt;platform.deepseek.com&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Hugging Face&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;huggingface&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;No&lt;/td&gt; 
   &lt;td&gt;Use models from Hugging Face Inference API.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://huggingface.co/settings/tokens"&gt;huggingface.co/settings/tokens&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;TogetherAI&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;togetherAI&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;No&lt;/td&gt; 
   &lt;td&gt;Use various open-source models via TogetherAI API.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://api.together.ai/settings/api-keys"&gt;api.together.ai/settings/api-keys&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;em&gt;Note:&lt;/em&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;We advise against using &lt;code&gt;gpt-4o&lt;/code&gt; or other OpenAI models for complex web browsing and task planning as current prompt optimizations are geared towards models like Deepseek.&lt;/li&gt; 
 &lt;li&gt;Coding/bash tasks might encounter issues with Gemini, as it may not strictly follow formatting prompts optimized for Deepseek.&lt;/li&gt; 
 &lt;li&gt;The &lt;code&gt;provider_server_address&lt;/code&gt; in &lt;code&gt;config.ini&lt;/code&gt; is generally not used when &lt;code&gt;is_local = False&lt;/code&gt; as the API endpoint is usually hardcoded in the respective provider's library.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Next step: &lt;a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/#Start-services-and-Run"&gt;Start services and run AgenticSeek&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;See the &lt;strong&gt;Known issues&lt;/strong&gt; section if you are having issues&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;See the &lt;strong&gt;Config&lt;/strong&gt; section for detailed config file explanation.&lt;/em&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Start services and Run&lt;/h2&gt; 
&lt;p&gt;By default AgenticSeek is run fully in docker.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Option 1:&lt;/strong&gt; Run in Docker, use web interface:&lt;/p&gt; 
&lt;p&gt;Start required services. This will start all services from the docker-compose.yml, including: - searxng - redis (required by searxng) - frontend - backend (if using &lt;code&gt;full&lt;/code&gt; when using the web interface)&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;./start_services.sh full # MacOS
start start_services.cmd full # Window
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Warning:&lt;/strong&gt; This step will download and load all Docker images, which may take up to 30 minutes. After starting the services, please wait until the backend service is fully running (you should see &lt;strong&gt;backend: "GET /health HTTP/1.1" 200 OK&lt;/strong&gt; in the log) before sending any messages. The backend services might take 5 minute to start on first run.&lt;/p&gt; 
&lt;p&gt;Go to &lt;code&gt;http://localhost:3000/&lt;/code&gt; and you should see the web interface.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Troubleshooting service start:&lt;/em&gt; If these scripts fail, ensure Docker Engine is running and Docker Compose (V2, &lt;code&gt;docker compose&lt;/code&gt;) is correctly installed. Check the output in the terminal for error messages. See &lt;a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/#faq-troubleshooting"&gt;FAQ: Help! I get an error when running AgenticSeek or its scripts.&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Option 2:&lt;/strong&gt; CLI mode:&lt;/p&gt; 
&lt;p&gt;To run with CLI interface you would have to install package on host:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;./install.sh
./install.bat # windows
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Start required services. This will start some services from the docker-compose.yml, including: - searxng - redis (required by searxng) - frontend&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;./start_services.sh # MacOS
start start_services.cmd # Window
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Use the CLI: &lt;code&gt;uv run cli.py&lt;/code&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;p&gt;Make sure the services are up and running with &lt;code&gt;./start_services.sh full&lt;/code&gt; and go to &lt;code&gt;localhost:3000&lt;/code&gt; for web interface.&lt;/p&gt; 
&lt;p&gt;You can also use speech to text by setting &lt;code&gt;listen = True&lt;/code&gt; in the config. Only for CLI mode.&lt;/p&gt; 
&lt;p&gt;To exit, simply say/type &lt;code&gt;goodbye&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Here are some example usage:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;em&gt;Make a snake game in python!&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;em&gt;Search the web for top cafes in Rennes, France, and save a list of three with their addresses in rennes_cafes.txt.&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;em&gt;Write a Go program to calculate the factorial of a number, save it as factorial.go in your workspace&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;em&gt;Search my summer_pictures folder for all JPG files, rename them with today‚Äôs date, and save a list of renamed files in photos_list.txt&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;em&gt;Search online for popular sci-fi movies from 2024 and pick three to watch tonight. Save the list in movie_night.txt.&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;em&gt;Search the web for the latest AI news articles from 2025, select three, and write a Python script to scrape their titles and summaries. Save the script as news_scraper.py and the summaries in ai_news.txt in /home/projects&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;em&gt;Friday, search the web for a free stock price API, register with &lt;a href="mailto:supersuper7434567@gmail.com"&gt;supersuper7434567@gmail.com&lt;/a&gt; then write a Python script to fetch using the API daily prices for Tesla, and save the results in stock_prices.csv&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;em&gt;Note that form filling capabilities are still experimental and might fail.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;After you type your query, AgenticSeek will allocate the best agent for the task.&lt;/p&gt; 
&lt;p&gt;Because this is an early prototype, the agent routing system might not always allocate the right agent based on your query.&lt;/p&gt; 
&lt;p&gt;Therefore, you should be very explicit in what you want and how the AI might proceed for example if you want it to conduct a web search, do not say:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;Do you know some good countries for solo-travel?&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Instead, ask:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;Do a web search and find out which are the best country for solo-travel&lt;/code&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;&lt;strong&gt;Setup to run the LLM on your own server&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;If you have a powerful computer or a server that you can use, but you want to use it from your laptop you have the options to run the LLM on a remote server using our custom llm server.&lt;/p&gt; 
&lt;p&gt;On your "server" that will run the AI model, get the ip address&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;ip a | grep "inet " | grep -v 127.0.0.1 | awk '{print $2}' | cut -d/ -f1 # local ip
curl https://ipinfo.io/ip # public ip
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note: For Windows or macOS, use ipconfig or ifconfig respectively to find the IP address.&lt;/p&gt; 
&lt;p&gt;Clone the repository and enter the &lt;code&gt;server/&lt;/code&gt;folder.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;git clone --depth 1 https://github.com/Fosowl/agenticSeek.git
cd agenticSeek/llm_server/
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Install server specific requirements:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;pip3 install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Run the server script.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;python3 app.py --provider ollama --port 3333
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You have the choice between using &lt;code&gt;ollama&lt;/code&gt; and &lt;code&gt;llamacpp&lt;/code&gt; as a LLM service.&lt;/p&gt; 
&lt;p&gt;Now on your personal computer:&lt;/p&gt; 
&lt;p&gt;Change the &lt;code&gt;config.ini&lt;/code&gt; file to set the &lt;code&gt;provider_name&lt;/code&gt; to &lt;code&gt;server&lt;/code&gt; and &lt;code&gt;provider_model&lt;/code&gt; to &lt;code&gt;deepseek-r1:xxb&lt;/code&gt;. Set the &lt;code&gt;provider_server_address&lt;/code&gt; to the ip address of the machine that will run the model.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;[MAIN]
is_local = False
provider_name = server
provider_model = deepseek-r1:70b
provider_server_address = http://x.x.x.x:3333
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Next step: &lt;a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/#Start-services-and-Run"&gt;Start services and run AgenticSeek&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Speech to Text&lt;/h2&gt; 
&lt;p&gt;Warning: speech to text only work in CLI mode at the moment.&lt;/p&gt; 
&lt;p&gt;Please note that currently speech to text only work in english.&lt;/p&gt; 
&lt;p&gt;The speech-to-text functionality is disabled by default. To enable it, set the listen option to True in the config.ini file:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;listen = True
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;When enabled, the speech-to-text feature listens for a trigger keyword, which is the agent's name, before it begins processing your input. You can customize the agent's name by updating the &lt;code&gt;agent_name&lt;/code&gt; value in the &lt;em&gt;config.ini&lt;/em&gt; file:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;agent_name = Friday
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For optimal recognition, we recommend using a common English name like "John" or "Emma" as the agent name&lt;/p&gt; 
&lt;p&gt;Once you see the transcript start to appear, say the agent's name aloud to wake it up (e.g., "Friday").&lt;/p&gt; 
&lt;p&gt;Speak your query clearly.&lt;/p&gt; 
&lt;p&gt;End your request with a confirmation phrase to signal the system to proceed. Examples of confirmation phrases include:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;"do it", "go ahead", "execute", "run", "start", "thanks", "would ya", "please", "okay?", "proceed", "continue", "go on", "do that", "go it", "do you understand?"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Config&lt;/h2&gt; 
&lt;p&gt;Example config:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;[MAIN]
is_local = True
provider_name = ollama
provider_model = deepseek-r1:32b
provider_server_address = http://127.0.0.1:11434 # Example for Ollama; use http://127.0.0.1:1234 for LM-Studio
agent_name = Friday
recover_last_session = False
save_session = False
speak = False
listen = False

jarvis_personality = False
languages = en zh # List of languages for TTS and potentially routing.
[BROWSER]
headless_browser = False
stealth_mode = False
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Explanation of &lt;code&gt;config.ini&lt;/code&gt; Settings&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;[MAIN]&lt;/code&gt; Section:&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;is_local&lt;/code&gt;: &lt;code&gt;True&lt;/code&gt; if using a local LLM provider (Ollama, LM-Studio, local OpenAI-compatible server) or the self-hosted server option. &lt;code&gt;False&lt;/code&gt; if using a cloud-based API (OpenAI, Google, etc.).&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;provider_name&lt;/code&gt;: Specifies the LLM provider. 
    &lt;ul&gt; 
     &lt;li&gt;Local options: &lt;code&gt;ollama&lt;/code&gt;, &lt;code&gt;lm-studio&lt;/code&gt;, &lt;code&gt;openai&lt;/code&gt; (for local OpenAI-compatible servers), &lt;code&gt;server&lt;/code&gt; (for the self-hosted server setup).&lt;/li&gt; 
     &lt;li&gt;API options: &lt;code&gt;openai&lt;/code&gt;, &lt;code&gt;google&lt;/code&gt;, &lt;code&gt;deepseek&lt;/code&gt;, &lt;code&gt;huggingface&lt;/code&gt;, &lt;code&gt;togetherAI&lt;/code&gt;.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;provider_model&lt;/code&gt;: The specific model name or ID for the chosen provider (e.g., &lt;code&gt;deepseekcoder:6.7b&lt;/code&gt; for Ollama, &lt;code&gt;gpt-3.5-turbo&lt;/code&gt; for OpenAI API, &lt;code&gt;mistralai/Mixtral-8x7B-Instruct-v0.1&lt;/code&gt; for TogetherAI).&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;provider_server_address&lt;/code&gt;: The address of your LLM provider. 
    &lt;ul&gt; 
     &lt;li&gt;For local providers: e.g., &lt;code&gt;http://127.0.0.1:11434&lt;/code&gt; for Ollama, &lt;code&gt;http://127.0.0.1:1234&lt;/code&gt; for LM-Studio.&lt;/li&gt; 
     &lt;li&gt;For the &lt;code&gt;server&lt;/code&gt; provider type: The address of your self-hosted LLM server (e.g., &lt;code&gt;http://your_server_ip:3333&lt;/code&gt;).&lt;/li&gt; 
     &lt;li&gt;For cloud APIs (&lt;code&gt;is_local = False&lt;/code&gt;): This is often ignored or can be left blank, as the API endpoint is usually handled by the client library.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;agent_name&lt;/code&gt;: Name of the AI assistant (e.g., Friday). Used as a trigger word for speech-to-text if enabled.&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;recover_last_session&lt;/code&gt;: &lt;code&gt;True&lt;/code&gt; to attempt to restore the previous session's state, &lt;code&gt;False&lt;/code&gt; to start fresh.&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;save_session&lt;/code&gt;: &lt;code&gt;True&lt;/code&gt; to save the current session's state for potential recovery, &lt;code&gt;False&lt;/code&gt; otherwise.&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;speak&lt;/code&gt;: &lt;code&gt;True&lt;/code&gt; to enable text-to-speech voice output, &lt;code&gt;False&lt;/code&gt; to disable.&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;listen&lt;/code&gt;: &lt;code&gt;True&lt;/code&gt; to enable speech-to-text voice input (CLI mode only), &lt;code&gt;False&lt;/code&gt; to disable.&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;work_dir&lt;/code&gt;: &lt;strong&gt;Crucial:&lt;/strong&gt; The directory where AgenticSeek will read/write files. &lt;strong&gt;Ensure this path is valid and accessible on your system.&lt;/strong&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;jarvis_personality&lt;/code&gt;: &lt;code&gt;True&lt;/code&gt; to use a more "Jarvis-like" system prompt (experimental), &lt;code&gt;False&lt;/code&gt; for the standard prompt.&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;languages&lt;/code&gt;: A comma-separated list of languages (e.g., &lt;code&gt;en, zh, fr&lt;/code&gt;). Used for TTS voice selection (defaults to the first) and can assist the LLM router. Avoid too many or very similar languages for router efficiency.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;[BROWSER]&lt;/code&gt; Section:&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;headless_browser&lt;/code&gt;: &lt;code&gt;True&lt;/code&gt; to run the automated browser without a visible window (recommended for web interface or non-interactive use). &lt;code&gt;False&lt;/code&gt; to show the browser window (useful for CLI mode or debugging).&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;stealth_mode&lt;/code&gt;: &lt;code&gt;True&lt;/code&gt; to enable measures to make browser automation harder to detect. May require manual installation of browser extensions like anticaptcha.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This section summarizes the supported LLM provider types. Configure them in &lt;code&gt;config.ini&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Local Providers (Run on Your Own Hardware):&lt;/strong&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Provider Name in &lt;code&gt;config.ini&lt;/code&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;code&gt;is_local&lt;/code&gt;&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Setup Section&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;ollama&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;True&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Use Ollama to serve local LLMs.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/#setup-for-running-llm-locally-on-your-machine"&gt;Setup for running LLM locally&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;lm-studio&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;True&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Use LM-Studio to serve local LLMs.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/#setup-for-running-llm-locally-on-your-machine"&gt;Setup for running LLM locally&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;openai&lt;/code&gt; (for local server)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;True&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Connect to a local server that exposes an OpenAI-compatible API (e.g., llama.cpp).&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/#setup-for-running-llm-locally-on-your-machine"&gt;Setup for running LLM locally&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;server&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Connect to the AgenticSeek self-hosted LLM server running on another machine.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/#setup-to-run-the-llm-on-your-own-server"&gt;Setup to run the LLM on your own server&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;API Providers (Cloud-Based):&lt;/strong&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Provider Name in &lt;code&gt;config.ini&lt;/code&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;code&gt;is_local&lt;/code&gt;&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Setup Section&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;openai&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Use OpenAI's official API (e.g., GPT-3.5, GPT-4).&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/#setup-to-run-with-an-api"&gt;Setup to run with an API&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;google&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Use Google's Gemini models via API.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/#setup-to-run-with-an-api"&gt;Setup to run with an API&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;deepseek&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Use Deepseek's official API.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/#setup-to-run-with-an-api"&gt;Setup to run with an API&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;huggingface&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Use Hugging Face Inference API.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/#setup-to-run-with-an-api"&gt;Setup to run with an API&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;togetherAI&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Use TogetherAI's API for various open models.&lt;/td&gt; 
   &lt;td&gt;&lt;a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/#setup-to-run-with-an-api"&gt;Setup to run with an API&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Troubleshooting&lt;/h2&gt; 
&lt;p&gt;If you encounter issues, this section provides guidance.&lt;/p&gt; 
&lt;h1&gt;Known Issues&lt;/h1&gt; 
&lt;h2&gt;ChromeDriver Issues&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Error Example:&lt;/strong&gt; &lt;code&gt;SessionNotCreatedException: Message: session not created: This version of ChromeDriver only supports Chrome version XXX&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;Root Cause&lt;/h3&gt; 
&lt;p&gt;ChromeDriver version incompatibility occurs when:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Your installed ChromeDriver version doesn't match your Chrome browser version&lt;/li&gt; 
 &lt;li&gt;In Docker environments, &lt;code&gt;undetected_chromedriver&lt;/code&gt; may download its own ChromeDriver version, bypassing the mounted binary&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Solution Steps&lt;/h3&gt; 
&lt;h4&gt;1. Check Your Chrome Version&lt;/h4&gt; 
&lt;p&gt;Open Google Chrome ‚Üí &lt;code&gt;Settings &amp;gt; About Chrome&lt;/code&gt; to find your version (e.g., "Version 134.0.6998.88")&lt;/p&gt; 
&lt;h4&gt;2. Download Matching ChromeDriver&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;For Chrome 115 and newer:&lt;/strong&gt; Use the &lt;a href="https://googlechromelabs.github.io/chrome-for-testing/"&gt;Chrome for Testing API&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Visit the Chrome for Testing availability dashboard&lt;/li&gt; 
 &lt;li&gt;Find your Chrome version or the closest available match&lt;/li&gt; 
 &lt;li&gt;Download the ChromeDriver for your OS (Linux64 for Docker environments)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;For older Chrome versions:&lt;/strong&gt; Use the &lt;a href="https://chromedriver.chromium.org/downloads"&gt;legacy ChromeDriver downloads&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/chromedriver_readme.png" alt="Download ChromeDriver from Chrome for Testing" /&gt;&lt;/p&gt; 
&lt;h4&gt;3. Install ChromeDriver (Choose One Method)&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;Method A: Project Root Directory (Recommended for Docker)&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Place the downloaded chromedriver binary in your project root
cp path/to/downloaded/chromedriver ./chromedriver
chmod +x ./chromedriver  # Make executable on Linux/macOS
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Method B: System PATH&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Linux/macOS
sudo mv chromedriver /usr/local/bin/
sudo chmod +x /usr/local/bin/chromedriver

# Windows: Place chromedriver.exe in a folder that's in your PATH
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;4. Verify Installation&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Test the ChromeDriver version
./chromedriver --version
# OR if in PATH:
chromedriver --version
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Docker-Specific Notes&lt;/h3&gt; 
&lt;p&gt;‚ö†Ô∏è &lt;strong&gt;Important for Docker Users:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;The Docker volume mount approach may not work with stealth mode (&lt;code&gt;undetected_chromedriver&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Solution&lt;/strong&gt;: Place ChromeDriver in the project root directory as &lt;code&gt;./chromedriver&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;The application will automatically detect and use this binary&lt;/li&gt; 
 &lt;li&gt;You should see: &lt;code&gt;"Using ChromeDriver from project root: ./chromedriver"&lt;/code&gt; in the logs&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Troubleshooting Tips&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Still getting version mismatch?&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Verify the ChromeDriver is executable: &lt;code&gt;ls -la ./chromedriver&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Check the ChromeDriver version: &lt;code&gt;./chromedriver --version&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Ensure it matches your Chrome browser version&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Docker container issues?&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Check backend logs: &lt;code&gt;docker logs backend&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Look for the message: &lt;code&gt;"Using ChromeDriver from project root"&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;If not found, verify the file exists and is executable&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Chrome for Testing versions&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Use the exact version match when possible&lt;/li&gt; 
   &lt;li&gt;For version 134.0.6998.88, use ChromeDriver 134.0.6998.165 (closest available)&lt;/li&gt; 
   &lt;li&gt;Major version numbers must match (134 = 134)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Version Compatibility Matrix&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Chrome Version&lt;/th&gt; 
   &lt;th&gt;ChromeDriver Version&lt;/th&gt; 
   &lt;th&gt;Status&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;134.0.6998.x&lt;/td&gt; 
   &lt;td&gt;134.0.6998.165&lt;/td&gt; 
   &lt;td&gt;‚úÖ Works&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;133.0.6943.x&lt;/td&gt; 
   &lt;td&gt;133.0.6943.141&lt;/td&gt; 
   &lt;td&gt;‚úÖ Works&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;132.0.6834.x&lt;/td&gt; 
   &lt;td&gt;132.0.6834.159&lt;/td&gt; 
   &lt;td&gt;‚úÖ Works&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;em&gt;For the latest compatibility, check the &lt;a href="https://googlechromelabs.github.io/chrome-for-testing/"&gt;Chrome for Testing dashboard&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;Exception: Failed to initialize browser: Message: session not created: This version of ChromeDriver only supports Chrome version 113 Current browser version is 134.0.6998.89 with binary path&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;This happen if there is a mismatch between your browser and chromedriver version.&lt;/p&gt; 
&lt;p&gt;You need to navigate to download the latest version:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://developer.chrome.com/docs/chromedriver/downloads"&gt;https://developer.chrome.com/docs/chromedriver/downloads&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;If you're using Chrome version 115 or newer go to:&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://googlechromelabs.github.io/chrome-for-testing/"&gt;https://googlechromelabs.github.io/chrome-for-testing/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;And download the chromedriver version matching your OS.&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/chromedriver_readme.png" alt="alt text" /&gt;&lt;/p&gt; 
&lt;p&gt;If this section is incomplete please raise an issue.&lt;/p&gt; 
&lt;h2&gt;connection adapters Issues&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;Exception: Provider lm-studio failed: HTTP request failed: No connection adapters were found for '127.0.0.1:1234/v1/chat/completions'` (Note: port may vary)
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Cause:&lt;/strong&gt; The &lt;code&gt;provider_server_address&lt;/code&gt; in &lt;code&gt;config.ini&lt;/code&gt; for &lt;code&gt;lm-studio&lt;/code&gt; (or other similar local OpenAI-compatible servers) is missing the &lt;code&gt;http://&lt;/code&gt; prefix or is pointing to the wrong port.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Solution:&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Ensure the address includes &lt;code&gt;http://&lt;/code&gt;. LM-Studio typically defaults to &lt;code&gt;http://127.0.0.1:1234&lt;/code&gt;.&lt;/li&gt; 
   &lt;li&gt;Correct &lt;code&gt;config.ini&lt;/code&gt;: &lt;code&gt;provider_server_address = http://127.0.0.1:1234&lt;/code&gt; (or your actual LM-Studio server port).&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;SearxNG Base URL Not Provided&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;raise ValueError("SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.")
ValueError: SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.`
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;FAQ&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Q: What hardware do I need?&lt;/strong&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model Size&lt;/th&gt; 
   &lt;th&gt;GPU&lt;/th&gt; 
   &lt;th&gt;Comment&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;7B&lt;/td&gt; 
   &lt;td&gt;8GB Vram&lt;/td&gt; 
   &lt;td&gt;‚ö†Ô∏è Not recommended. Performance is poor, frequent hallucinations, and planner agents will likely fail.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;14B&lt;/td&gt; 
   &lt;td&gt;12 GB VRAM (e.g. RTX 3060)&lt;/td&gt; 
   &lt;td&gt;‚úÖ Usable for simple tasks. May struggle with web browsing and planning tasks.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;32B&lt;/td&gt; 
   &lt;td&gt;24+ GB VRAM (e.g. RTX 4090)&lt;/td&gt; 
   &lt;td&gt;üöÄ Success with most tasks, might still struggle with task planning&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;70B+&lt;/td&gt; 
   &lt;td&gt;48+ GB Vram&lt;/td&gt; 
   &lt;td&gt;üí™ Excellent. Recommended for advanced use cases.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;Q: I get an error what do I do?&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Ensure local is running (&lt;code&gt;ollama serve&lt;/code&gt;), your &lt;code&gt;config.ini&lt;/code&gt; matches your provider, and dependencies are installed. If none work feel free to raise an issue.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Q: Can it really run 100% locally?&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Yes with Ollama, lm-studio or server providers, all speech to text, LLM and text to speech model run locally. Non-local options (OpenAI or others API) are optional.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Q: Why should I use AgenticSeek when I have Manus?&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Unlike Manus, AgenticSeek prioritizes independence from external systems, giving you more control, privacy and avoid api cost.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Q: Who is behind the project ?&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;The project was created by me, along with two friends who serve as maintainers and contributors from the open-source community on GitHub. We‚Äôre just a group of passionate individuals, not a startup or affiliated with any organization.&lt;/p&gt; 
&lt;p&gt;Any AgenticSeek account on X other than my personal account (&lt;a href="https://x.com/Martin993886460"&gt;https://x.com/Martin993886460&lt;/a&gt;) is an impersonation.&lt;/p&gt; 
&lt;h2&gt;Contribute&lt;/h2&gt; 
&lt;p&gt;We‚Äôre looking for developers to improve AgenticSeek! Check out open issues or discussion.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/docs/CONTRIBUTING.md"&gt;Contribution guide&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Sponsors:&lt;/h2&gt; 
&lt;p&gt;Want to level up AgenticSeek capabilities with features like flight search, trip planning, or snagging the best shopping deals? Consider crafting a custom tool with SerpApi to unlock more Jarvis-like capabilities. With SerpApi, you can turbocharge your agent for specialized tasks while staying in full control.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://serpapi.com/"&gt;&lt;img src="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/banners/sponsor_banner_serpapi.png" height="350" alt="SerpApi Banner" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;See &lt;a href="https://raw.githubusercontent.com/Fosowl/agenticSeek/main/docs/CONTRIBUTING.md"&gt;Contributing.md&lt;/a&gt; to learn how to integrate custom tools!&lt;/p&gt; 
&lt;h3&gt;&lt;strong&gt;Patron sponsor&lt;/strong&gt;:&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://github.com/tatra-labs"&gt;tatra-labs&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Maintainers:&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;a href="https://github.com/Fosowl"&gt;Fosowl&lt;/a&gt; | Paris Time&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;a href="https://github.com/antoineVIVIES"&gt;antoineVIVIES&lt;/a&gt; | Taipei Time&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Special Thanks:&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;a href="https://github.com/tcsenpai"&gt;tcsenpai&lt;/a&gt; and &lt;a href="https://github.com/plitc"&gt;plitc&lt;/a&gt; For helping with backend dockerization&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;a href="https://www.star-history.com/#Fosowl/agenticSeek&amp;amp;Date"&gt;&lt;img src="https://api.star-history.com/svg?repos=Fosowl/agenticSeek&amp;amp;type=Date" alt="Star History Chart" /&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>1rgs/claude-code-proxy</title>
      <link>https://github.com/1rgs/claude-code-proxy</link>
      <description>&lt;p&gt;Run Claude Code on OpenAI models&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Anthropic API Proxy for Gemini &amp;amp; OpenAI Models üîÑ&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;Use Anthropic clients (like Claude Code) with Gemini, OpenAI, or direct Anthropic backends.&lt;/strong&gt; ü§ù&lt;/p&gt; 
&lt;p&gt;A proxy server that lets you use Anthropic clients with Gemini, OpenAI, or Anthropic models themselves (a transparent proxy of sorts), all via LiteLLM. üåâ&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/1rgs/claude-code-proxy/main/pic.png" alt="Anthropic API Proxy" /&gt;&lt;/p&gt; 
&lt;h2&gt;Quick Start ‚ö°&lt;/h2&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;OpenAI API key üîë&lt;/li&gt; 
 &lt;li&gt;Google AI Studio (Gemini) API key (if using Google provider) üîë&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://github.com/astral-sh/uv"&gt;uv&lt;/a&gt; installed.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Setup üõ†Ô∏è&lt;/h3&gt; 
&lt;h4&gt;From source&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Clone this repository&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;git clone https://github.com/1rgs/claude-code-proxy.git
cd claude-code-proxy
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install uv&lt;/strong&gt; (if you haven't already):&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;curl -LsSf https://astral.sh/uv/install.sh | sh
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;(&lt;code&gt;uv&lt;/code&gt; will handle dependencies based on &lt;code&gt;pyproject.toml&lt;/code&gt; when you run the server)&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Configure Environment Variables&lt;/strong&gt;: Copy the example environment file:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;cp .env.example .env
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Edit &lt;code&gt;.env&lt;/code&gt; and fill in your API keys and model configurations:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;ANTHROPIC_API_KEY&lt;/code&gt;: (Optional) Needed only if proxying &lt;em&gt;to&lt;/em&gt; Anthropic models.&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;OPENAI_API_KEY&lt;/code&gt;: Your OpenAI API key (Required if using the default OpenAI preference or as fallback).&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;GEMINI_API_KEY&lt;/code&gt;: Your Google AI Studio (Gemini) API key (Required if PREFERRED_PROVIDER=google).&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;PREFERRED_PROVIDER&lt;/code&gt; (Optional): Set to &lt;code&gt;openai&lt;/code&gt; (default), &lt;code&gt;google&lt;/code&gt;, or &lt;code&gt;anthropic&lt;/code&gt;. This determines the primary backend for mapping &lt;code&gt;haiku&lt;/code&gt;/&lt;code&gt;sonnet&lt;/code&gt;.&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;BIG_MODEL&lt;/code&gt; (Optional): The model to map &lt;code&gt;sonnet&lt;/code&gt; requests to. Defaults to &lt;code&gt;gpt-4.1&lt;/code&gt; (if &lt;code&gt;PREFERRED_PROVIDER=openai&lt;/code&gt;) or &lt;code&gt;gemini-2.5-pro-preview-03-25&lt;/code&gt;. Ignored when &lt;code&gt;PREFERRED_PROVIDER=anthropic&lt;/code&gt;.&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;SMALL_MODEL&lt;/code&gt; (Optional): The model to map &lt;code&gt;haiku&lt;/code&gt; requests to. Defaults to &lt;code&gt;gpt-4.1-mini&lt;/code&gt; (if &lt;code&gt;PREFERRED_PROVIDER=openai&lt;/code&gt;) or &lt;code&gt;gemini-2.0-flash&lt;/code&gt;. Ignored when &lt;code&gt;PREFERRED_PROVIDER=anthropic&lt;/code&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Mapping Logic:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;If &lt;code&gt;PREFERRED_PROVIDER=openai&lt;/code&gt; (default), &lt;code&gt;haiku&lt;/code&gt;/&lt;code&gt;sonnet&lt;/code&gt; map to &lt;code&gt;SMALL_MODEL&lt;/code&gt;/&lt;code&gt;BIG_MODEL&lt;/code&gt; prefixed with &lt;code&gt;openai/&lt;/code&gt;.&lt;/li&gt; 
   &lt;li&gt;If &lt;code&gt;PREFERRED_PROVIDER=google&lt;/code&gt;, &lt;code&gt;haiku&lt;/code&gt;/&lt;code&gt;sonnet&lt;/code&gt; map to &lt;code&gt;SMALL_MODEL&lt;/code&gt;/&lt;code&gt;BIG_MODEL&lt;/code&gt; prefixed with &lt;code&gt;gemini/&lt;/code&gt; &lt;em&gt;if&lt;/em&gt; those models are in the server's known &lt;code&gt;GEMINI_MODELS&lt;/code&gt; list (otherwise falls back to OpenAI mapping).&lt;/li&gt; 
   &lt;li&gt;If &lt;code&gt;PREFERRED_PROVIDER=anthropic&lt;/code&gt;, &lt;code&gt;haiku&lt;/code&gt;/&lt;code&gt;sonnet&lt;/code&gt; requests are passed directly to Anthropic with the &lt;code&gt;anthropic/&lt;/code&gt; prefix without remapping to different models.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Run the server&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;uv run uvicorn server:app --host 0.0.0.0 --port 8082 --reload
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;(&lt;code&gt;--reload&lt;/code&gt; is optional, for development)&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Docker&lt;/h4&gt; 
&lt;p&gt;If using docker, download the example environment file to &lt;code&gt;.env&lt;/code&gt; and edit it as described above.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;curl -O .env https://raw.githubusercontent.com/1rgs/claude-code-proxy/refs/heads/main/.env.example
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then, you can either start the container with &lt;a href="https://docs.docker.com/compose/"&gt;docker compose&lt;/a&gt; (preferred):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yml"&gt;services:
  proxy:
    image: ghcr.io/1rgs/claude-code-proxy:latest
    restart: unless-stopped
    env_file: .env
    ports:
      - 8082:8082
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or with a command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;docker run -d --env-file .env -p 8082:8082 ghcr.io/1rgs/claude-code-proxy:latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Using with Claude Code üéÆ&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install Claude Code&lt;/strong&gt; (if you haven't already):&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;npm install -g @anthropic-ai/claude-code
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Connect to your proxy&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;ANTHROPIC_BASE_URL=http://localhost:8082 claude
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;That's it!&lt;/strong&gt; Your Claude Code client will now use the configured backend models (defaulting to Gemini) through the proxy. üéØ&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Model Mapping üó∫Ô∏è&lt;/h2&gt; 
&lt;p&gt;The proxy automatically maps Claude models to either OpenAI or Gemini models based on the configured model:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Claude Model&lt;/th&gt; 
   &lt;th&gt;Default Mapping&lt;/th&gt; 
   &lt;th&gt;When BIG_MODEL/SMALL_MODEL is a Gemini model&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;haiku&lt;/td&gt; 
   &lt;td&gt;openai/gpt-4o-mini&lt;/td&gt; 
   &lt;td&gt;gemini/[model-name]&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;sonnet&lt;/td&gt; 
   &lt;td&gt;openai/gpt-4o&lt;/td&gt; 
   &lt;td&gt;gemini/[model-name]&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Supported Models&lt;/h3&gt; 
&lt;h4&gt;OpenAI Models&lt;/h4&gt; 
&lt;p&gt;The following OpenAI models are supported with automatic &lt;code&gt;openai/&lt;/code&gt; prefix handling:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;o3-mini&lt;/li&gt; 
 &lt;li&gt;o1&lt;/li&gt; 
 &lt;li&gt;o1-mini&lt;/li&gt; 
 &lt;li&gt;o1-pro&lt;/li&gt; 
 &lt;li&gt;gpt-4.5-preview&lt;/li&gt; 
 &lt;li&gt;gpt-4o&lt;/li&gt; 
 &lt;li&gt;gpt-4o-audio-preview&lt;/li&gt; 
 &lt;li&gt;chatgpt-4o-latest&lt;/li&gt; 
 &lt;li&gt;gpt-4o-mini&lt;/li&gt; 
 &lt;li&gt;gpt-4o-mini-audio-preview&lt;/li&gt; 
 &lt;li&gt;gpt-4.1&lt;/li&gt; 
 &lt;li&gt;gpt-4.1-mini&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Gemini Models&lt;/h4&gt; 
&lt;p&gt;The following Gemini models are supported with automatic &lt;code&gt;gemini/&lt;/code&gt; prefix handling:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;gemini-2.5-pro-preview-03-25&lt;/li&gt; 
 &lt;li&gt;gemini-2.0-flash&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Model Prefix Handling&lt;/h3&gt; 
&lt;p&gt;The proxy automatically adds the appropriate prefix to model names:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;OpenAI models get the &lt;code&gt;openai/&lt;/code&gt; prefix&lt;/li&gt; 
 &lt;li&gt;Gemini models get the &lt;code&gt;gemini/&lt;/code&gt; prefix&lt;/li&gt; 
 &lt;li&gt;The BIG_MODEL and SMALL_MODEL will get the appropriate prefix based on whether they're in the OpenAI or Gemini model lists&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For example:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;gpt-4o&lt;/code&gt; becomes &lt;code&gt;openai/gpt-4o&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;gemini-2.5-pro-preview-03-25&lt;/code&gt; becomes &lt;code&gt;gemini/gemini-2.5-pro-preview-03-25&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;When BIG_MODEL is set to a Gemini model, Claude Sonnet will map to &lt;code&gt;gemini/[model-name]&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Customizing Model Mapping&lt;/h3&gt; 
&lt;p&gt;Control the mapping using environment variables in your &lt;code&gt;.env&lt;/code&gt; file or directly:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Example 1: Default (Use OpenAI)&lt;/strong&gt; No changes needed in &lt;code&gt;.env&lt;/code&gt; beyond API keys, or ensure:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-dotenv"&gt;OPENAI_API_KEY="your-openai-key"
GEMINI_API_KEY="your-google-key" # Needed if PREFERRED_PROVIDER=google
# PREFERRED_PROVIDER="openai" # Optional, it's the default
# BIG_MODEL="gpt-4.1" # Optional, it's the default
# SMALL_MODEL="gpt-4.1-mini" # Optional, it's the default
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Example 2: Prefer Google&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-dotenv"&gt;GEMINI_API_KEY="your-google-key"
OPENAI_API_KEY="your-openai-key" # Needed for fallback
PREFERRED_PROVIDER="google"
# BIG_MODEL="gemini-2.5-pro-preview-03-25" # Optional, it's the default for Google pref
# SMALL_MODEL="gemini-2.0-flash" # Optional, it's the default for Google pref
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Example 3: Use Direct Anthropic ("Just an Anthropic Proxy" Mode)&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-dotenv"&gt;ANTHROPIC_API_KEY="sk-ant-..."
PREFERRED_PROVIDER="anthropic"
# BIG_MODEL and SMALL_MODEL are ignored in this mode
# haiku/sonnet requests are passed directly to Anthropic models
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Use case: This mode enables you to use the proxy infrastructure (for logging, middleware, request/response processing, etc.) while still using actual Anthropic models rather than being forced to remap to OpenAI or Gemini.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Example 4: Use Specific OpenAI Models&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-dotenv"&gt;OPENAI_API_KEY="your-openai-key"
GEMINI_API_KEY="your-google-key"
PREFERRED_PROVIDER="openai"
BIG_MODEL="gpt-4o" # Example specific model
SMALL_MODEL="gpt-4o-mini" # Example specific model
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;How It Works üß©&lt;/h2&gt; 
&lt;p&gt;This proxy works by:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Receiving requests&lt;/strong&gt; in Anthropic's API format üì•&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Translating&lt;/strong&gt; the requests to OpenAI format via LiteLLM üîÑ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Sending&lt;/strong&gt; the translated request to OpenAI üì§&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Converting&lt;/strong&gt; the response back to Anthropic format üîÑ&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Returning&lt;/strong&gt; the formatted response to the client ‚úÖ&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;The proxy handles both streaming and non-streaming responses, maintaining compatibility with all Claude clients. üåä&lt;/p&gt; 
&lt;h2&gt;Contributing ü§ù&lt;/h2&gt; 
&lt;p&gt;Contributions are welcome! Please feel free to submit a Pull Request. üéÅ&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>EbookFoundation/free-programming-books</title>
      <link>https://github.com/EbookFoundation/free-programming-books</link>
      <description>&lt;p&gt;üìö Freely available programming books&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;List of Free Learning Resources In Many Languages&lt;/h1&gt; 
&lt;div align="center" markdown="1"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/sindresorhus/awesome"&gt;&lt;img src="https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg?sanitize=true" alt="Awesome" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://creativecommons.org/licenses/by/4.0/"&gt;&lt;img src="https://img.shields.io/badge/License-CC%20BY%204.0-lightgrey.svg?sanitize=true" alt="License: CC BY 4.0" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/pulls?q=is%3Apr+is%3Amerged+created%3A2023-10-01..2023-10-31"&gt;&lt;img src="https://img.shields.io/github/hacktoberfest/2023/EbookFoundation/free-programming-books?label=Hacktoberfest+2023" alt="Hacktoberfest 2023 stats" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;Search the list at &lt;a href="https://ebookfoundation.github.io/free-programming-books-search/"&gt;https://ebookfoundation.github.io/free-programming-books-search/&lt;/a&gt; &lt;a href="https://ebookfoundation.github.io/free-programming-books-search/"&gt;&lt;img src="https://img.shields.io/website?style=flat&amp;amp;logo=www&amp;amp;logoColor=whitesmoke&amp;amp;label=Dynamic%20search%20site&amp;amp;down_color=red&amp;amp;down_message=down&amp;amp;up_color=green&amp;amp;up_message=up&amp;amp;url=https%3A%2F%2Febookfoundation.github.io%2Ffree-programming-books-search%2F" alt="https://ebookfoundation.github.io/free-programming-books-search/" /&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;This page is available as an easy-to-read website. Access it by clicking on &lt;a href="https://ebookfoundation.github.io/free-programming-books/"&gt;&lt;img src="https://img.shields.io/website?style=flat&amp;amp;logo=www&amp;amp;logoColor=whitesmoke&amp;amp;label=Static%20site&amp;amp;down_color=red&amp;amp;down_message=down&amp;amp;up_color=green&amp;amp;up_message=up&amp;amp;url=https%3A%2F%2Febookfoundation.github.io%2Ffree-programming-books%2F" alt="https://ebookfoundation.github.io/free-programming-books/" /&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;form action="https://ebookfoundation.github.io/free-programming-books-search"&gt; 
  &lt;input type="text" id="fpbSearch" name="search" required placeholder="Search Book or Author" /&gt; 
  &lt;label for="submit"&gt; &lt;/label&gt; 
  &lt;input type="submit" id="submit" name="submit" value="Search" /&gt; 
 &lt;/form&gt; 
&lt;/div&gt; 
&lt;h2&gt;Intro&lt;/h2&gt; 
&lt;p&gt;This list was originally a clone of &lt;a href="https://web.archive.org/web/20140606191453/http://stackoverflow.com/questions/194812/list-of-freely-available-programming-books/392926"&gt;StackOverflow - List of Freely Available Programming Books&lt;/a&gt; with contributions from Karan Bhangui and George Stocker.&lt;/p&gt; 
&lt;p&gt;The list was moved to GitHub by Victor Felder for collaborative updating and maintenance. It has grown to become one of &lt;a href="https://octoverse.github.com/"&gt;GitHub's most popular repositories&lt;/a&gt;.&lt;/p&gt; 
&lt;div align="center" markdown="1"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/EbookFoundation/free-programming-books/network"&gt;&lt;img src="https://img.shields.io/github/forks/EbookFoundation/free-programming-books?style=flat&amp;amp;logo=github&amp;amp;logoColor=whitesmoke&amp;amp;label=Forks" alt="GitHub repo forks" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/EbookFoundation/free-programming-books?style=flat&amp;amp;logo=github&amp;amp;logoColor=whitesmoke&amp;amp;label=Stars" alt="GitHub repo stars" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/graphs/contributors"&gt;&lt;img src="https://img.shields.io/github/contributors-anon/EbookFoundation/free-programming-books?style=flat&amp;amp;logo=github&amp;amp;logoColor=whitesmoke&amp;amp;label=Contributors" alt="GitHub repo contributors" /&gt;&lt;/a&gt;&lt;br /&gt; &lt;a href="https://github.com/sponsors/EbookFoundation"&gt;&lt;img src="https://img.shields.io/github/sponsors/EbookFoundation?style=flat&amp;amp;logo=github&amp;amp;logoColor=whitesmoke&amp;amp;label=Sponsors" alt="GitHub org sponsors" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/watchers"&gt;&lt;img src="https://img.shields.io/github/watchers/EbookFoundation/free-programming-books?style=flat&amp;amp;logo=github&amp;amp;logoColor=whitesmoke&amp;amp;label=Watchers" alt="GitHub repo watchers" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/archive/refs/heads/main.zip"&gt;&lt;img src="https://img.shields.io/github/repo-size/EbookFoundation/free-programming-books?style=flat&amp;amp;logo=github&amp;amp;logoColor=whitesmoke&amp;amp;label=Repo%20Size" alt="GitHub repo size" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;The &lt;a href="https://ebookfoundation.org"&gt;Free Ebook Foundation&lt;/a&gt; now administers the repo, a not-for-profit organization devoted to promoting the creation, distribution, archiving, and sustainability of free ebooks. &lt;a href="https://ebookfoundation.org/contributions.html"&gt;Donations&lt;/a&gt; to the Free Ebook Foundation are tax-deductible in the US.&lt;/p&gt; 
&lt;h2&gt;How To Contribute&lt;/h2&gt; 
&lt;p&gt;Please read &lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/CONTRIBUTING.md"&gt;CONTRIBUTING&lt;/a&gt;. If you're new to GitHub, &lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/HOWTO.md"&gt;welcome&lt;/a&gt;! Remember to abide by our adapted from &lt;img src="https://img.shields.io/badge/Contributor%20Covenant-1.3-4baaaa.svg?sanitize=true" alt="Contributor Covenant 1.3" /&gt; &lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/CODE_OF_CONDUCT.md"&gt;Code of Conduct&lt;/a&gt; too (&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/#translations"&gt;translations&lt;/a&gt; also available).&lt;/p&gt; 
&lt;p&gt;Click on these badges to see how you might be able to help:&lt;/p&gt; 
&lt;div align="center" markdown="1"&gt; 
 &lt;p&gt;&lt;a href="https://github.com/EbookFoundation/free-programming-books/issues"&gt;&lt;img src="https://img.shields.io/github/issues/EbookFoundation/free-programming-books?style=flat&amp;amp;logo=github&amp;amp;logoColor=red&amp;amp;label=Issues" alt="GitHub repo Issues" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22"&gt;&lt;img src="https://img.shields.io/github/issues/EbookFoundation/free-programming-books/good%20first%20issue?style=flat&amp;amp;logo=github&amp;amp;logoColor=green&amp;amp;label=Good%20First%20issues" alt="GitHub repo Good Issues for newbies" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/issues?q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22"&gt;&lt;img src="https://img.shields.io/github/issues/EbookFoundation/free-programming-books/help%20wanted?style=flat&amp;amp;logo=github&amp;amp;logoColor=b545d1&amp;amp;label=%22Help%20Wanted%22%20issues" alt="GitHub Help Wanted issues" /&gt;&lt;/a&gt;&lt;br /&gt; &lt;a href="https://github.com/EbookFoundation/free-programming-books/pulls"&gt;&lt;img src="https://img.shields.io/github/issues-pr/EbookFoundation/free-programming-books?style=flat&amp;amp;logo=github&amp;amp;logoColor=orange&amp;amp;label=PRs" alt="GitHub repo PRs" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/pulls?q=is%3Apr+is%3Amerged"&gt;&lt;img src="https://img.shields.io/github/issues-search/EbookFoundation/free-programming-books?style=flat&amp;amp;logo=github&amp;amp;logoColor=green&amp;amp;label=Merged%20PRs&amp;amp;query=is%3Amerged" alt="GitHub repo Merged PRs" /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href="https://github.com/EbookFoundation/free-programming-books/pulls?q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22"&gt;&lt;img src="https://img.shields.io/github/issues-pr/EbookFoundation/free-programming-books/help%20wanted?style=flat&amp;amp;logo=github&amp;amp;logoColor=b545d1&amp;amp;label=%22Help%20Wanted%22%20PRs" alt="GitHub Help Wanted PRs" /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;How To Share&lt;/h2&gt; 
&lt;div align="left" markdown="1"&gt; 
 &lt;a href="https://www.facebook.com/share.php?u=https%3A%2F%2Fgithub.com%2FEbookFoundation%2Ffree-programming-books&amp;amp;p%5Bimages%5D%5B0%5D=&amp;amp;p%5Btitle%5D=Free%20Programming%20Books&amp;amp;p%5Bsummary%5D="&gt;Share on Facebook&lt;/a&gt;
 &lt;br /&gt; 
 &lt;a href="http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://github.com/EbookFoundation/free-programming-books&amp;amp;title=Free%20Programming%20Books&amp;amp;summary=&amp;amp;source="&gt;Share on LinkedIn&lt;/a&gt;
 &lt;br /&gt; 
 &lt;a href="https://toot.kytta.dev/?mini=true&amp;amp;url=https://github.com/EbookFoundation/free-programming-books&amp;amp;title=Free%20Programming%20Books&amp;amp;summary=&amp;amp;source="&gt;Share on Mastodon/Fediverse&lt;/a&gt;
 &lt;br /&gt; 
 &lt;a href="https://t.me/share/url?url=https://github.com/EbookFoundation/free-programming-books"&gt;Share on Telegram&lt;/a&gt;
 &lt;br /&gt; 
 &lt;a href="https://twitter.com/intent/tweet?text=https://github.com/EbookFoundation/free-programming-books%0AFree%20Programming%20Books"&gt;Share on ùïè (Twitter)&lt;/a&gt;
 &lt;br /&gt; 
&lt;/div&gt; 
&lt;h2&gt;Resources&lt;/h2&gt; 
&lt;p&gt;This project lists books and other resources grouped by genres:&lt;/p&gt; 
&lt;h3&gt;Books&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-langs.md"&gt;English, By Programming Language&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-subjects.md"&gt;English, By Subject&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;Other Languages&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ar.md"&gt;Arabic / al arabiya / ÿßŸÑÿπÿ±ÿ®Ÿäÿ©&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-hy.md"&gt;Armenian / ’Ä’°’µ’•÷Ä’•’∂&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-az.md"&gt;Azerbaijani / –ê–∑”ô—Ä–±–∞—ò“π–∞–Ω –¥–∏–ª–∏ / ÿ¢ÿ∞ÿ±ÿ®ÿßŸäÿ¨ÿßŸÜÿ¨ÿß ÿØŸäŸÑŸä&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-bn.md"&gt;Bengali / ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-bg.md"&gt;Bulgarian / –±—ä–ª–≥–∞—Ä—Å–∫–∏&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-my.md"&gt;Burmese / ·Äô·Äº·Äî·Ä∫·Äô·Ä¨·Äò·Ä¨·Äû·Ä¨&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-zh.md"&gt;Chinese / ‰∏≠Êñá&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-cs.md"&gt;Czech / ƒçe≈°tina / ƒçesk√Ω jazyk&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ca.md"&gt;Catalan / catalan/ catal√†&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-da.md"&gt;Danish / dansk&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-nl.md"&gt;Dutch / Nederlands&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-et.md"&gt;Estonian / eesti keel&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-fi.md"&gt;Finnish / suomi / suomen kieli&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-fr.md"&gt;French / fran√ßais&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-de.md"&gt;German / Deutsch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-el.md"&gt;Greek / ŒµŒªŒªŒ∑ŒΩŒπŒ∫Œ¨&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-he.md"&gt;Hebrew / ◊¢◊ë◊®◊ô◊™&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-hi.md"&gt;Hindi / ‡§π‡§ø‡§®‡•ç‡§¶‡•Ä&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-hu.md"&gt;Hungarian / magyar / magyar nyelv&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-id.md"&gt;Indonesian / Bahasa Indonesia&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-it.md"&gt;Italian / italiano&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ja.md"&gt;Japanese / Êó•Êú¨Ë™û&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ko.md"&gt;Korean / ÌïúÍµ≠Ïñ¥&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-lv.md"&gt;Latvian / Latvie≈°u&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ml.md"&gt;Malayalam / ‡¥Æ‡¥≤‡¥Ø‡¥æ‡¥≥‡¥Ç&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-no.md"&gt;Norwegian / Norsk&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-fa_IR.md"&gt;Persian / Farsi (Iran) / ŸÅÿßÿ±ÿ≥Ÿâ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-pl.md"&gt;Polish / polski / jƒôzyk polski / polszczyzna&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-pt_BR.md"&gt;Portuguese (Brazil)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-pt_PT.md"&gt;Portuguese (Portugal)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ro.md"&gt;Romanian (Romania) / limba rom√¢nƒÉ / rom√¢n&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ru.md"&gt;Russian / –†—É—Å—Å–∫–∏–π —è–∑—ã–∫&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-sr.md"&gt;Serbian / —Å—Ä–ø—Å–∫–∏ —ò–µ–∑–∏–∫ / srpski jezik&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-sk.md"&gt;Slovak / slovenƒçina&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-es.md"&gt;Spanish / espa√±ol / castellano&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-sv.md"&gt;Swedish / Svenska&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-ta.md"&gt;Tamil / ‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-te.md"&gt;Telugu / ‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-th.md"&gt;Thai / ‡πÑ‡∏ó‡∏¢&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-tr.md"&gt;Turkish / T√ºrk√ße&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-uk.md"&gt;Ukrainian / –£–∫—Ä–∞—ó–Ω—Å—å–∫–∞&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/books/free-programming-books-vi.md"&gt;Vietnamese / Ti·∫øng Vi·ªát&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Cheat Sheets&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-cheatsheets.md"&gt;All Languages&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Free Online Courses&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ar.md"&gt;Arabic / al arabiya / ÿßŸÑÿπÿ±ÿ®Ÿäÿ©&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-bn.md"&gt;Bengali / ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-bg.md"&gt;Bulgarian / –±—ä–ª–≥–∞—Ä—Å–∫–∏&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-my.md"&gt;Burmese / ·Äô·Äº·Äî·Ä∫·Äô·Ä¨·Äò·Ä¨·Äû·Ä¨&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-zh.md"&gt;Chinese / ‰∏≠Êñá&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-en.md"&gt;English&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-fi.md"&gt;Finnish / suomi / suomen kieli&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-fr.md"&gt;French / fran√ßais&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-de.md"&gt;German / Deutsch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-el.md"&gt;Greek / ŒµŒªŒªŒ∑ŒΩŒπŒ∫Œ¨&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-he.md"&gt;Hebrew / ◊¢◊ë◊®◊ô◊™&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-hi.md"&gt;Hindi / ‡§π‡§ø‡§Ç‡§¶‡•Ä&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-id.md"&gt;Indonesian / Bahasa Indonesia&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-it.md"&gt;Italian / italiano&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ja.md"&gt;Japanese / Êó•Êú¨Ë™û&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-kn.md"&gt;Kannada/‡≤ï‡≤®‡≥ç‡≤®‡≤°&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-kk.md"&gt;Kazakh / “õ–∞–∑–∞“õ—à–∞&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-km.md"&gt;Khmer / ·ûó·û∂·ûü·û∂·ûÅ·üí·ûò·üÇ·ûö&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ko.md"&gt;Korean / ÌïúÍµ≠Ïñ¥&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ml.md"&gt;Malayalam / ‡¥Æ‡¥≤‡¥Ø‡¥æ‡¥≥‡¥Ç&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-mr.md"&gt;Marathi / ‡§Æ‡§∞‡§æ‡§†‡•Ä&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ne.md"&gt;Nepali / ‡§®‡•á‡§™‡§æ‡§≤‡•Ä&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-no.md"&gt;Norwegian / Norsk&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-fa_IR.md"&gt;Persian / Farsi (Iran) / ŸÅÿßÿ±ÿ≥Ÿâ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-pl.md"&gt;Polish / polski / jƒôzyk polski / polszczyzna&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-pt_BR.md"&gt;Portuguese (Brazil)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-pt_PT.md"&gt;Portuguese (Portugal)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ru.md"&gt;Russian / –†—É—Å—Å–∫–∏–π —è–∑—ã–∫&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-si.md"&gt;Sinhala / ‡∑É‡∑í‡∂Ç‡∑Ñ‡∂Ω&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-es.md"&gt;Spanish / espa√±ol / castellano&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-sv.md"&gt;Swedish / svenska&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ta.md"&gt;Tamil / ‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-te.md"&gt;Telugu / ‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-th.md"&gt;Thai / ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-tr.md"&gt;Turkish / T√ºrk√ße&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-uk.md"&gt;Ukrainian / –£–∫—Ä–∞—ó–Ω—Å—å–∫–∞&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-ur.md"&gt;Urdu / ÿßÿ±ÿØŸà&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/courses/free-courses-vi.md"&gt;Vietnamese / Ti·∫øng Vi·ªát&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Interactive Programming Resources&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-interactive-tutorials-zh.md"&gt;Chinese / ‰∏≠Êñá&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-interactive-tutorials-en.md"&gt;English&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-interactive-tutorials-de.md"&gt;German / Deutsch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-interactive-tutorials-ja.md"&gt;Japanese / Êó•Êú¨Ë™û&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-interactive-tutorials-ru.md"&gt;Russian / –†—É—Å—Å–∫–∏–π —è–∑—ã–∫&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Problem Sets and Competitive Programming&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/problem-sets-competitive-programming.md"&gt;Problem Sets&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Podcast - Screencast&lt;/h3&gt; 
&lt;p&gt;Free Podcasts and Screencasts:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-ar.md"&gt;Arabic / al Arabiya / ÿßŸÑÿπÿ±ÿ®Ÿäÿ©&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-my.md"&gt;Burmese / ·Äô·Äº·Äî·Ä∫·Äô·Ä¨·Äò·Ä¨·Äû·Ä¨&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-zh.md"&gt;Chinese / ‰∏≠Êñá&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-cs.md"&gt;Czech / ƒçe≈°tina / ƒçesk√Ω jazyk&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-nl.md"&gt;Dutch / Nederlands&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-en.md"&gt;English&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-fi.md"&gt;Finnish / Suomi&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-fr.md"&gt;French / fran√ßais&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-de.md"&gt;German / Deutsch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-he.md"&gt;Hebrew / ◊¢◊ë◊®◊ô◊™&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-id.md"&gt;Indonesian / Bahasa Indonesia&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-fa_IR.md"&gt;Persian / Farsi (Iran) / ŸÅÿßÿ±ÿ≥Ÿâ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-pl.md"&gt;Polish / polski / jƒôzyk polski / polszczyzna&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-pt_BR.md"&gt;Portuguese (Brazil)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-pt_PT.md"&gt;Portuguese (Portugal)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-ru.md"&gt;Russian / –†—É—Å—Å–∫–∏–π —è–∑—ã–∫&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-si.md"&gt;Sinhala / ‡∑É‡∑í‡∂Ç‡∑Ñ‡∂Ω&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-es.md"&gt;Spanish / espa√±ol / castellano&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-sv.md"&gt;Swedish / Svenska&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-tr.md"&gt;Turkish / T√ºrk√ße&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/casts/free-podcasts-screencasts-uk.md"&gt;Ukrainian / –£–∫—Ä–∞—ó–Ω—Å—å–∫–∞&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Programming Playgrounds&lt;/h3&gt; 
&lt;p&gt;Write, compile, and run your code within a browser. Try it out!&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-playgrounds-zh.md"&gt;Chinese / ‰∏≠Êñá&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-playgrounds.md"&gt;English&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/more/free-programming-playgrounds-de.md"&gt;German / Deutsch&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Translations&lt;/h2&gt; 
&lt;p&gt;Volunteers have translated many of our Contributing, How-to, and Code of Conduct documents into languages covered by our lists.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;English 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/CODE_OF_CONDUCT.md"&gt;Code of Conduct&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/CONTRIBUTING.md"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/HOWTO.md"&gt;How-to&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;... &lt;em&gt;&lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/README.md#translations"&gt;More languages&lt;/a&gt;&lt;/em&gt; ...&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You might notice that there are &lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/README.md#translations"&gt;some missing translations here&lt;/a&gt; - perhaps you would like to help out by &lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/docs/CONTRIBUTING.md#help-out-by-contributing-a-translation"&gt;contributing a translation&lt;/a&gt;?&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Each file included in this repository is licensed under the &lt;a href="https://raw.githubusercontent.com/EbookFoundation/free-programming-books/main/LICENSE"&gt;CC BY License&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>521xueweihan/GitHub520</title>
      <link>https://github.com/521xueweihan/GitHub520</link>
      <description>&lt;p&gt;üòò ËÆ©‰Ω†‚ÄúÁà±‚Äù‰∏ä GitHubÔºåËß£ÂÜ≥ËÆøÈóÆÊó∂ÂõæË£Ç„ÄÅÂä†ËΩΩÊÖ¢ÁöÑÈóÆÈ¢ò„ÄÇÔºàÊó†ÈúÄÂÆâË£ÖÔºâ&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;GitHub520&lt;/h1&gt; 
&lt;p align="center"&gt; &lt;a href="https://hellogithub.com/repository/d05ff820bf36470581c02cda5cbd17ea" target="_blank"&gt;&lt;img src="https://api.hellogithub.com/v1/widgets/recommend.svg?rid=d05ff820bf36470581c02cda5cbd17ea&amp;amp;claim_uid=8MKvZoxaWt" alt="FeaturedÔΩúHelloGitHub" style="width: 250px; height: 54px;" width="250" height="54" /&gt;&lt;/a&gt;&lt;br /&gt; üòò ËÆ©‰Ω†‚ÄúÁà±‚Äù‰∏ä GitHubÔºåËß£ÂÜ≥ËÆøÈóÆÊó∂ÂõæË£Ç„ÄÅÂä†ËΩΩÊÖ¢ÁöÑÈóÆÈ¢ò„ÄÇ &lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;üí∞ÊúçÂä°Âô®Â∞Ü‰∫é 2025-12-31 Âà∞ÊúüÔºåÁª≠Ë¥πÈúÄË¶Å 831 ÂÖÉ/Âπ¥ &lt;a href="https://raw.hellogithub.com/code.png"&gt;ÁÇπÂáªÊâ´Á†ÅËµûÂä©&lt;/a&gt;ÔºåÊÑüË∞¢üôè&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;‰∏Ä„ÄÅ‰ªãÁªç&lt;/h2&gt; 
&lt;p&gt;ÂØπ GitHub ËØ¥"Áà±"Â§™Èöæ‰∫ÜÔºöËÆøÈóÆÊÖ¢„ÄÅÂõæÁâáÂä†ËΩΩ‰∏çÂá∫Êù•„ÄÇ&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Êú¨È°πÁõÆÊó†ÈúÄÂÆâË£Ö‰ªª‰ΩïÁ®ãÂ∫èÔºå‰ªÖÈúÄ 5 ÂàÜÈíü„ÄÇ&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;ÈÄöËøá‰øÆÊîπÊú¨Âú∞ hosts Êñá‰ª∂ÔºåËØïÂõæËß£ÂÜ≥Ôºö&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;GitHub ËÆøÈóÆÈÄüÂ∫¶ÊÖ¢ÁöÑÈóÆÈ¢ò&lt;/li&gt; 
 &lt;li&gt;GitHub È°πÁõÆ‰∏≠ÁöÑÂõæÁâáÊòæÁ§∫‰∏çÂá∫ÁöÑÈóÆÈ¢ò&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;ËÆ©‰Ω†"Áà±"‰∏ä GitHub„ÄÇ&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Ê≥®Ôºö&lt;/em&gt; Êú¨È°πÁõÆËøòÂ§Ñ‰∫éÊµãËØïÈò∂ÊÆµÔºå‰ªÖÂú®Êú¨Êú∫ÊµãËØïÈÄöËøáÔºåÂ¶ÇÊúâÈóÆÈ¢òÊ¨¢ËøéÊèê &lt;a href="https://github.com/521xueweihan/GitHub520/issues/new"&gt;issues&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;‰∫å„ÄÅ‰ΩøÁî®ÊñπÊ≥ï&lt;/h2&gt; 
&lt;p&gt;‰∏ãÈù¢ÁöÑÂú∞ÂùÄÊó†ÈúÄËÆøÈóÆ GitHub Âç≥ÂèØËé∑ÂèñÂà∞ÊúÄÊñ∞ÁöÑ hosts ÂÜÖÂÆπÔºö&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Êñá‰ª∂Ôºö&lt;code&gt;https://raw.hellogithub.com/hosts&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;JSONÔºö&lt;code&gt;https://raw.hellogithub.com/hosts.json&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;2.1 ÊâãÂä®ÊñπÂºè&lt;/h3&gt; 
&lt;h4&gt;2.1.1 Â§çÂà∂‰∏ãÈù¢ÁöÑÂÜÖÂÆπ&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# GitHub520 Host Start
140.82.112.25                 alive.github.com
20.205.243.168                api.github.com
140.82.112.21                 api.individual.githubcopilot.com
185.199.111.133               avatars.githubusercontent.com
185.199.111.133               avatars0.githubusercontent.com
185.199.111.133               avatars1.githubusercontent.com
185.199.111.133               avatars2.githubusercontent.com
185.199.111.133               avatars3.githubusercontent.com
185.199.111.133               avatars4.githubusercontent.com
185.199.111.133               avatars5.githubusercontent.com
185.199.111.133               camo.githubusercontent.com
140.82.112.21                 central.github.com
185.199.111.133               cloud.githubusercontent.com
20.205.243.165                codeload.github.com
140.82.113.22                 collector.github.com
185.199.111.133               desktop.githubusercontent.com
185.199.111.133               favicons.githubusercontent.com
20.205.243.166                gist.github.com
3.5.9.13                      github-cloud.s3.amazonaws.com
16.15.201.119                 github-com.s3.amazonaws.com
52.216.209.193                github-production-release-asset-2e65be.s3.amazonaws.com
16.182.42.25                  github-production-repository-file-5c1aeb.s3.amazonaws.com
16.15.193.252                 github-production-user-asset-6210df.s3.amazonaws.com
192.0.66.2                    github.blog
20.205.243.166                github.com
140.82.114.18                 github.community
185.199.108.154               github.githubassets.com
151.101.193.194               github.global.ssl.fastly.net
185.199.108.153               github.io
185.199.111.133               github.map.fastly.net
185.199.108.153               githubstatus.com
140.82.113.26                 live.github.com
185.199.111.133               media.githubusercontent.com
185.199.111.133               objects.githubusercontent.com
13.107.42.16                  pipelines.actions.githubusercontent.com
185.199.111.133               raw.githubusercontent.com
185.199.111.133               user-images.githubusercontent.com
13.107.246.73                 vscode.dev
140.82.114.21                 education.github.com
185.199.111.133               private-user-images.githubusercontent.com


# Update time: 2025-08-24T08:31:57+08:00
# Update url: https://raw.hellogithub.com/hosts
# Star me: https://github.com/521xueweihan/GitHub520
# GitHub520 Host End

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;ËØ•ÂÜÖÂÆπ‰ºöËá™Âä®ÂÆöÊó∂Êõ¥Êñ∞Ôºå Êï∞ÊçÆÊõ¥Êñ∞Êó∂Èó¥Ôºö2025-08-24T08:31:57+08:00&lt;/p&gt; 
&lt;h4&gt;2.1.2 ‰øÆÊîπ hosts Êñá‰ª∂&lt;/h4&gt; 
&lt;p&gt;hosts Êñá‰ª∂Âú®ÊØè‰∏™Á≥ªÁªüÁöÑ‰ΩçÁΩÆ‰∏ç‰∏ÄÔºåËØ¶ÊÉÖÂ¶Ç‰∏ãÔºö&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Windows Á≥ªÁªüÔºö&lt;code&gt;C:\Windows\System32\drivers\etc\hosts&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Linux Á≥ªÁªüÔºö&lt;code&gt;/etc/hosts&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;MacÔºàËãπÊûúÁîµËÑëÔºâÁ≥ªÁªüÔºö&lt;code&gt;/etc/hosts&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;AndroidÔºàÂÆâÂçìÔºâÁ≥ªÁªüÔºö&lt;code&gt;/system/etc/hosts&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;iPhoneÔºàiOSÔºâÁ≥ªÁªüÔºö&lt;code&gt;/etc/hosts&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;‰øÆÊîπÊñπÊ≥ïÔºåÊääÁ¨¨‰∏ÄÊ≠•ÁöÑÂÜÖÂÆπÂ§çÂà∂Âà∞ÊñáÊú¨Êú´Â∞æÔºö&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Windows ‰ΩøÁî®ËÆ∞‰∫ãÊú¨„ÄÇ&lt;/li&gt; 
 &lt;li&gt;Linux„ÄÅMac ‰ΩøÁî® Root ÊùÉÈôêÔºö&lt;code&gt;sudo vi /etc/hosts&lt;/code&gt;„ÄÇ&lt;/li&gt; 
 &lt;li&gt;iPhone„ÄÅiPad È°ªË∂äÁã±„ÄÅAndroid ÂøÖÈ°ªË¶Å root„ÄÇ&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;2.1.3 ÊøÄÊ¥ªÁîüÊïà&lt;/h4&gt; 
&lt;p&gt;Â§ßÈÉ®ÂàÜÊÉÖÂÜµ‰∏ãÊòØÁõ¥Êé•ÁîüÊïàÔºåÂ¶ÇÊú™ÁîüÊïàÂèØÂ∞ùËØï‰∏ãÈù¢ÁöÑÂäûÊ≥ïÔºåÂà∑Êñ∞ DNSÔºö&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;WindowsÔºöÂú® CMD Á™óÂè£ËæìÂÖ•Ôºö&lt;code&gt;ipconfig /flushdns&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Linux ÂëΩ‰ª§Ôºö&lt;code&gt;sudo nscd restart&lt;/code&gt;ÔºåÂ¶ÇÊä•ÈîôÂàôÈ°ªÂÆâË£ÖÔºö&lt;code&gt;sudo apt install nscd&lt;/code&gt; Êàñ &lt;code&gt;sudo /etc/init.d/nscd restart&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Mac ÂëΩ‰ª§Ôºö&lt;code&gt;sudo killall -HUP mDNSResponder&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;TipsÔºö&lt;/strong&gt; ‰∏äËø∞ÊñπÊ≥ïÊó†ÊïàÂèØ‰ª•Â∞ùËØïÈáçÂêØÊú∫Âô®„ÄÇ&lt;/p&gt; 
&lt;h3&gt;2.2 Ëá™Âä®ÊñπÂºèÔºàSwitchHostsÔºâ&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Tip&lt;/strong&gt;ÔºöÊé®Ëçê &lt;a href="https://github.com/oldj/SwitchHosts"&gt;SwitchHosts&lt;/a&gt; Â∑•ÂÖ∑ÁÆ°ÁêÜ hosts&lt;/p&gt; 
&lt;p&gt;‰ª• SwitchHosts ‰∏∫‰æãÔºåÁúã‰∏Ä‰∏ãÊÄé‰πà‰ΩøÁî®ÁöÑÔºåÈÖçÁΩÆÂèÇËÄÉ‰∏ãÈù¢Ôºö&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Hosts Á±ªÂûã: &lt;code&gt;Remote&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Hosts Ê†áÈ¢ò: ÈöèÊÑè&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;URL: &lt;code&gt;https://raw.hellogithub.com/hosts&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Ëá™Âä®Âà∑Êñ∞: ÊúÄÂ•ΩÈÄâ &lt;code&gt;1 Â∞èÊó∂&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Â¶ÇÂõæÔºö&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/521xueweihan/GitHub520/main/img/switch-hosts.png" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;ËøôÊ†∑ÊØèÊ¨° hosts ÊúâÊõ¥Êñ∞ÈÉΩËÉΩÂèäÊó∂ËøõË°åÊõ¥Êñ∞ÔºåÂÖçÂéªÊâãÂä®Êõ¥Êñ∞„ÄÇ&lt;/p&gt; 
&lt;h3&gt;2.3 ‰∏ÄË°åÂëΩ‰ª§&lt;/h3&gt; 
&lt;h4&gt;Windows&lt;/h4&gt; 
&lt;p&gt;‰ΩøÁî®ÂëΩ‰ª§ÈúÄË¶ÅÂÆâË£Ö&lt;a href="https://gitforwindows.org/"&gt;git bash&lt;/a&gt; Â§çÂà∂‰ª•‰∏ãÂëΩ‰ª§‰øùÂ≠òÂà∞Êú¨Âú∞ÂëΩÂêç‰∏∫&lt;strong&gt;fetch_github_hosts&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-shell"&gt;_hosts=$(mktemp /tmp/hostsXXX)
hosts=/c/Windows/System32/drivers/etc/hosts
remote=https://raw.hellogithub.com/hosts
reg='/# GitHub520 Host Start/,/# Github520 Host End/d'

sed "$reg" $hosts &amp;gt; "$_hosts"
curl "$remote" &amp;gt;&amp;gt; "$_hosts"
cat "$_hosts" &amp;gt; "$hosts"

rm "$_hosts"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Âú®&lt;strong&gt;CMD&lt;/strong&gt;‰∏≠ÊâßË°å‰ª•‰∏ãÂëΩ‰ª§ÔºåÊâßË°åÂâçÈúÄË¶ÅÊõøÊç¢&lt;strong&gt;git-bash.exe&lt;/strong&gt;Âíå&lt;strong&gt;fetch_github_hosts&lt;/strong&gt;‰∏∫‰Ω†Êú¨Âú∞ÁöÑË∑ØÂæÑÔºåÊ≥®ÊÑèÂâçËÄÖ‰∏∫windowsË∑ØÂæÑÊ†ºÂºèÂêéËÄÖ‰∏∫shellË∑ØÂæÑÊ†ºÂºè&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;"C:\Program Files\Git\git-bash.exe" -c "/c/Users/XXX/fetch_github_hosts"&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;ÂèØ‰ª•Â∞Ü‰∏äËø∞ÂëΩ‰ª§Ê∑ªÂä†Âà∞windowsÁöÑtask schedularÔºà‰ªªÂä°ËÆ°ÂàíÁ®ãÂ∫èÔºâ‰∏≠‰ª•ÂÆöÊó∂ÊâßË°å&lt;/p&gt; 
&lt;h4&gt;GNUÔºàUbuntu/CentOS/FedoraÔºâ&lt;/h4&gt; 
&lt;p&gt;&lt;code&gt;sudo sh -c 'sed -i "/# GitHub520 Host Start/Q" /etc/hosts &amp;amp;&amp;amp; curl https://raw.hellogithub.com/hosts &amp;gt;&amp;gt; /etc/hosts'&lt;/code&gt;&lt;/p&gt; 
&lt;h4&gt;BSD/macOS&lt;/h4&gt; 
&lt;p&gt;&lt;code&gt;sudo sed -i "" "/# GitHub520 Host Start/,/# Github520 Host End/d" /etc/hosts &amp;amp;&amp;amp; curl https://raw.hellogithub.com/hosts | sudo tee -a /etc/hosts&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Â∞Ü‰∏äÈù¢ÁöÑÂëΩ‰ª§Ê∑ªÂä†Âà∞ cronÔºåÂèØÂÆöÊó∂ÊâßË°å„ÄÇ‰ΩøÁî®ÂâçÁ°Æ‰øù GitHub520 ÂÜÖÂÆπÂú®ËØ•Êñá‰ª∂ÊúÄÂêéÈÉ®ÂàÜ„ÄÇ&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Âú® Docker ‰∏≠ËøêË°åÔºåËã•ÈÅáÂà∞ &lt;code&gt;Device or resource busy&lt;/code&gt; ÈîôËØØÔºåÂèØ‰ΩøÁî®‰ª•‰∏ãÂëΩ‰ª§ÊâßË°å&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;cp /etc/hosts ~/hosts.new &amp;amp;&amp;amp; sed -i "/# GitHub520 Host Start/Q" ~/hosts.new &amp;amp;&amp;amp; curl https://raw.hellogithub.com/hosts &amp;gt;&amp;gt; ~/hosts.new &amp;amp;&amp;amp; cp -f ~/hosts.new /etc/hosts&lt;/code&gt;&lt;/p&gt; 
&lt;h3&gt;2.4 AdGuard Áî®Êà∑ÔºàËá™Âä®ÊñπÂºèÔºâ&lt;/h3&gt; 
&lt;p&gt;Âú® &lt;strong&gt;ËøáÊª§Âô®&amp;gt;DNS Â∞ÅÈîÅÊ∏ÖÂçï&amp;gt;Ê∑ªÂä†ÈòªÊ≠¢ÂàóË°®&amp;gt;Ê∑ªÂä†‰∏Ä‰∏™Ëá™ÂÆö‰πâÂàóË°®&lt;/strong&gt;ÔºåÈÖçÁΩÆÂ¶Ç‰∏ãÔºö&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;ÂêçÁß∞ÔºöÈöèÊÑè&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;URLÔºö&lt;code&gt;https://raw.hellogithub.com/hosts&lt;/code&gt;ÔºàÂíå‰∏äÈù¢ SwitchHosts ‰ΩøÁî®ÁöÑ‰∏ÄÊ†∑Ôºâ&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Â¶ÇÂõæÔºö&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/521xueweihan/GitHub520/main/img/AdGuard-rules.png" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;Êõ¥Êñ∞Èó¥ÈöîÂú® &lt;strong&gt;ËÆæÁΩÆ &amp;gt; Â∏∏ËßÑËÆæÁΩÆ &amp;gt; ËøáÊª§Âô®Êõ¥Êñ∞Èó¥ÈöîÔºàËÆæÁΩÆ‰∏ÄÂ∞èÊó∂‰∏ÄÊ¨°Âç≥ÂèØÔºâ&lt;/strong&gt;ÔºåËÆ∞ÂæóÂãæÈÄâ‰∏ä &lt;strong&gt;‰ΩøÁî®ËøáÊª§Âô®Âíå Hosts Êñá‰ª∂‰ª•Êã¶Êà™ÊåáÂÆöÂüüÂêç&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/521xueweihan/GitHub520/main/img/AdGuard-rules2.png" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Tip&lt;/strong&gt;Ôºö‰∏çË¶ÅÊ∑ªÂä†Âú® &lt;strong&gt;DNS ÂÖÅËÆ∏Ê∏ÖÂçï&lt;/strong&gt; ÂÜÖÔºåÂè™ËÉΩÊ∑ªÂä†Âú® &lt;strong&gt;DNS Â∞ÅÈîÅÊ∏ÖÂçï&lt;/strong&gt; ÊâçÁÆ°Áî®„ÄÇ Âè¶Â§ñÔºåAdGuard for Mac„ÄÅAdGuard for Windows„ÄÅAdGuard for Android„ÄÅAdGuard for IOS Á≠âÁ≠â &lt;strong&gt;AdGuard ÂÆ∂ÊóèËΩØ‰ª∂&lt;/strong&gt; Ê∑ªÂä†ÊñπÊ≥ïÂùáÁ±ª‰ºº„ÄÇ&lt;/p&gt; 
&lt;h2&gt;‰∏â„ÄÅÊïàÊûúÂØπÊØî&lt;/h2&gt; 
&lt;p&gt;‰πãÂâçÁöÑÊ†∑Â≠êÔºö&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/521xueweihan/GitHub520/main/img/old.png" alt="" /&gt;&lt;/p&gt; 
&lt;p&gt;‰øÆÊîπÂÆå hosts ÁöÑÊ†∑Â≠êÔºö&lt;/p&gt; 
&lt;p&gt;&lt;img src="https://raw.githubusercontent.com/521xueweihan/GitHub520/main/img/new.png" alt="" /&gt;&lt;/p&gt; 
&lt;h2&gt;TODO&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; ÂÆöÊó∂Ëá™Âä®Êõ¥Êñ∞ hosts ÂÜÖÂÆπ&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; hosts ÂÜÖÂÆπÊó†ÂèòÂä®‰∏ç‰ºöÊõ¥Êñ∞&lt;/li&gt; 
 &lt;li&gt;&lt;input type="checkbox" checked disabled /&gt; ÂØªÂà∞ÊúÄ‰ºò IP Ëß£ÊûêÁªìÊûú&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Â£∞Êòé&lt;/h2&gt; 
&lt;p&gt;&lt;a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/deed.zh"&gt;&lt;img alt="Áü•ËØÜÂÖ±‰∫´ËÆ∏ÂèØÂçèËÆÆ" style="border-width: 0" src="https://licensebuttons.net/l/by-nc-nd/4.0/88x31.png" /&gt;&lt;/a&gt;&lt;br /&gt;Êú¨‰ΩúÂìÅÈááÁî® &lt;a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/deed.zh"&gt;ÁΩ≤Âêç-ÈùûÂïÜ‰∏öÊÄß‰ΩøÁî®-Á¶ÅÊ≠¢ÊºîÁªé 4.0 ÂõΩÈôÖ&lt;/a&gt; ËøõË°åËÆ∏ÂèØ„ÄÇ&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>NVIDIA-NeMo/RL</title>
      <link>https://github.com/NVIDIA-NeMo/RL</link>
      <description>&lt;p&gt;Scalable toolkit for efficient model reinforcement&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Nemo RL: A Scalable and Efficient Post-Training Library&lt;/h1&gt; 
&lt;h2&gt;üì£ News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;[7/25/2025] &lt;a href="https://github.com/NVIDIA-NeMo/RL/releases/tag/v0.3.0"&gt;Release v0.3.0!&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;üìù &lt;a href="https://nvidia-nemo.github.io/blog/2025/07/21/nemo-rl-v0.3/"&gt;v0.3.0 Blog Post&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;üìä View the release run metrics on &lt;a href="https://colab.research.google.com/drive/15kpesCV1m_C5UQFStssTEjaN2RsBMeZ0?usp=sharing"&gt;Google Colab&lt;/a&gt; to get a head start on your experimentation.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;[5/14/2025] &lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/docs/guides/grpo-deepscaler.md"&gt;Reproduce DeepscaleR with NeMo RL!&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[5/14/2025] &lt;a href="https://github.com/NVIDIA-NeMo/RL/releases/tag/v0.2.1"&gt;Release v0.2.1!&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;üìä View the release run metrics on &lt;a href="https://colab.research.google.com/drive/1o14sO0gj_Tl_ZXGsoYip3C0r5ofkU1Ey?usp=sharing"&gt;Google Colab&lt;/a&gt; to get a head start on your experimentation.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;!-- markdown all in one --&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#nemo-rl-a-scalable-and-efficient-post-training-library"&gt;Nemo RL: A Scalable and Efficient Post-Training Library&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#-news"&gt;üì£ News&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#table-of-contents"&gt;Table of Contents&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#features"&gt;Features&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#prerequisites"&gt;Prerequisites&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#training-backends"&gt;Training Backends&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#grpo"&gt;GRPO&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#grpo-single-node"&gt;GRPO Single Node&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#grpo-multi-node"&gt;GRPO Multi-node&lt;/a&gt; 
      &lt;ul&gt; 
       &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#grpo-qwen25-32b"&gt;GRPO Qwen2.5-32B&lt;/a&gt;&lt;/li&gt; 
       &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#grpo-multi-turn"&gt;GRPO Multi-Turn&lt;/a&gt;&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#supervised-fine-tuning-sft"&gt;Supervised Fine-Tuning (SFT)&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#sft-single-node"&gt;SFT Single Node&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#sft-multi-node"&gt;SFT Multi-node&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#dpo"&gt;DPO&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#dpo-single-node"&gt;DPO Single Node&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#dpo-multi-node"&gt;DPO Multi-node&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#rm"&gt;RM&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#rm-single-node"&gt;RM Single Node&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#rm-multi-node"&gt;RM Multi-node&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#evaluation"&gt;Evaluation&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#convert-model-format-optional"&gt;Convert Model Format (Optional)&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#run-evaluation"&gt;Run Evaluation&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#set-up-clusters"&gt;Set Up Clusters&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#tips-and-tricks"&gt;Tips and Tricks&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#citation"&gt;Citation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#contributing"&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#licenses"&gt;Licenses&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Nemo RL&lt;/strong&gt; is a scalable and efficient post-training library designed for models ranging from 1 GPU to thousands, and from tiny to over 100 billion parameters.&lt;/p&gt; 
&lt;p&gt;What you can expect:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Seamless integration with Hugging Face&lt;/strong&gt; for ease of use, allowing users to leverage a wide range of pre-trained models and tools.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;High-performance implementation with Megatron Core&lt;/strong&gt;, supporting various parallelism techniques for large models (&amp;gt;100B) and large context lengths.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Efficient resource management using Ray&lt;/strong&gt;, enabling scalable and flexible deployment across different hardware configurations.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Flexibility&lt;/strong&gt; with a modular design that allows easy integration and customization.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Comprehensive documentation&lt;/strong&gt; that is both detailed and user-friendly, with practical examples.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;p&gt;‚úÖ &lt;em&gt;Available now&lt;/em&gt; | üîú &lt;em&gt;Coming in v0.4&lt;/em&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;‚úÖ &lt;strong&gt;Fast Generation&lt;/strong&gt; - vLLM backend for optimized inference.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;‚úÖ &lt;strong&gt;HuggingFace Integration&lt;/strong&gt; - Works with 1-70B models (Qwen, Llama).&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;‚úÖ &lt;strong&gt;Distributed Training&lt;/strong&gt; - Fully Sharded Data Parallel (FSDP2) support and Ray-based infrastructure.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;‚úÖ &lt;strong&gt;Environment Support&lt;/strong&gt; - Support for multi-environment training.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;‚úÖ &lt;strong&gt;Learning Algorithms&lt;/strong&gt; - GRPO (Group Relative Policy Optimization), SFT (Supervised Fine-Tuning), and DPO (Direct Preference Optimization).&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;‚úÖ &lt;strong&gt;Multi-Turn RL&lt;/strong&gt; - Multi-turn generation and training for RL with tool use, games, etc.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;‚úÖ &lt;strong&gt;Large Model Support&lt;/strong&gt; - Native PyTorch support for models up to 70B parameters.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;‚úÖ &lt;strong&gt;Advanced Parallelism&lt;/strong&gt; - PyTorch native FSDP2, TP, CP, and SP for efficient training.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;‚úÖ &lt;strong&gt;(even) Larger Model Support with Long(er) Sequences&lt;/strong&gt; - Advanced parallelisms with Megatron Core (TP/PP/CP/SP/EP).&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;‚úÖ &lt;strong&gt;Worker Isolation&lt;/strong&gt; - Process isolation between RL Actors (no worries about global state).&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;‚úÖ &lt;strong&gt;Environment Isolation&lt;/strong&gt; - Dependency isolation between components.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;‚úÖ &lt;strong&gt;Megatron Inference&lt;/strong&gt; - (static) Megatron Inference for day-0 support for new megatron models.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;‚úÖ &lt;strong&gt;MoE Models&lt;/strong&gt; - Support for DeepseekV3 and Qwen-3 MoE models&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;‚úÖ &lt;strong&gt;Sequence Packing&lt;/strong&gt; - Sequence packing in both DTensor and MCore for huge training perf gains&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üîú &lt;strong&gt;Improved Native Performance&lt;/strong&gt; - Improve training time for Native Pytorch Models.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üîú &lt;strong&gt;Megatron Inference&lt;/strong&gt; - (dynamic) Megatron Inference for fast day-0 support for new megatron models.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Prerequisites&lt;/h2&gt; 
&lt;p&gt;Clone &lt;strong&gt;NeMo RL&lt;/strong&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;git clone git@github.com:NVIDIA-NeMo/RL.git nemo-rl
cd nemo-rl

# If you are using the Megatron backend, download the pinned versions of Megatron-LM and NeMo submodules 
# by running (This is not necessary if you are using the pure Pytorch/DTensor path):
git submodule update --init --recursive

# Different branches of the repo can have different pinned versions of these third-party submodules. Ensure
# submodules are automatically updated after switching branches or pulling updates by configuring git with:
# git config submodule.recurse true

# **NOTE**: this setting will not download **new** or remove **old** submodules with the branch's changes.
# You will have to run the full `git submodule update --init --recursive` command in these situations.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you are using the Megatron backend on bare-metal (outside of a container), you may need to install the cudnn headers as well. Here is how you can check as well as install them:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Check if you have libcudnn installed
dpkg -l | grep cudnn.*cuda

# Find the version you need here: https://developer.nvidia.com/cudnn-downloads?target_os=Linux&amp;amp;target_arch=x86_64&amp;amp;Distribution=Ubuntu&amp;amp;target_version=20.04&amp;amp;target_type=deb_network
# As an example, these are the "Linux Ubuntu 20.04 x86_64" instructions
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-keyring_1.1-1_all.deb
sudo dpkg -i cuda-keyring_1.1-1_all.deb
sudo apt-get update
sudo apt-get install cudnn-cuda-12
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For faster setup and environment isolation, we use &lt;a href="https://docs.astral.sh/uv/"&gt;uv&lt;/a&gt;. Follow &lt;a href="https://docs.astral.sh/uv/getting-started/installation/"&gt;these instructions&lt;/a&gt; to install uv.&lt;/p&gt; 
&lt;p&gt;Then, initialize NeMo RL project virtual environment via:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;uv venv
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Please do not use &lt;code&gt;-p/--python&lt;/code&gt; and instead allow &lt;code&gt;uv venv&lt;/code&gt; to read it from &lt;code&gt;.python-version&lt;/code&gt;. This ensures that the version of python used is always what we prescribe.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;If working outside a container, it can help to build &lt;a href="https://github.com/Dao-AILab/flash-attention"&gt;flash-attn&lt;/a&gt; and warm the uv cache before your first run.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;bash tools/build-flash-attn-in-uv-cache.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] On the first install, &lt;code&gt;flash-attn&lt;/code&gt; can take a while to install (~45min with 48 CPU hyperthreads). After it is built once, it is cached in your uv's cache dir making subsequent installs much quicker.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] The NeMo RL Dockerfile will warm the uv cache with flash-attn. See &lt;a href="https://docs.nvidia.com/nemo/rl/latest/docker.html"&gt;https://docs.nvidia.com/nemo/rl/latest/docker.html&lt;/a&gt; for instructions if you are looking for the NeMo RL container.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;If sucessful, you should see &lt;code&gt;‚úÖ flash-attn successfully added to uv cache&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Use &lt;code&gt;uv run&lt;/code&gt; to launch all commands. It handles pip installing implicitly and ensures your environment is up to date with our lock file.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;It is not recommended to activate the &lt;code&gt;venv&lt;/code&gt;, and you should use &lt;code&gt;uv run &amp;lt;command&amp;gt;&lt;/code&gt; instead to execute scripts within the managed environment. This ensures consistent environment usage across different shells and sessions. Example: &lt;code&gt;uv run python examples/run_grpo_math.py&lt;/code&gt;&lt;/li&gt; 
  &lt;li&gt;Ensure you have the necessary CUDA drivers and PyTorch installed compatible with your hardware.&lt;/li&gt; 
  &lt;li&gt;If you update your environment in &lt;code&gt;pyproject.toml&lt;/code&gt;, it is necessary to force a rebuild of the virtual environments by setting &lt;code&gt;NRL_FORCE_REBUILD_VENVS=true&lt;/code&gt; next time you launch a run.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Reminder&lt;/strong&gt;: Don't forget to set your &lt;code&gt;HF_HOME&lt;/code&gt;, &lt;code&gt;WANDB_API_KEY&lt;/code&gt;, and &lt;code&gt;HF_DATASETS_CACHE&lt;/code&gt; (if needed). You'll need to do a &lt;code&gt;huggingface-cli login&lt;/code&gt; as well for Llama models.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Training Backends&lt;/h2&gt; 
&lt;p&gt;NeMo RL supports multiple training backends to accommodate different model sizes and hardware configurations:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;DTensor (FSDP2)&lt;/strong&gt; - PyTorch's next-generation distributed training with improved memory efficiency&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Megatron&lt;/strong&gt; - NVIDIA's high-performance training framework for scaling to large models (&amp;gt;100B parameters)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The training backend is automatically determined based on your YAML configuration settings. For detailed information on backend selection, configuration, and examples, see the &lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/docs/design-docs/training-backends.md"&gt;Training Backends documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;GRPO&lt;/h2&gt; 
&lt;p&gt;We have a reference GRPO experiment config set up trained for math benchmarks using the &lt;a href="https://huggingface.co/datasets/nvidia/OpenMathInstruct-2"&gt;OpenInstructMath2&lt;/a&gt; dataset.&lt;/p&gt; 
&lt;p&gt;You can read about the details of the GRPO implementation &lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/docs/guides/grpo.md"&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;GRPO Single Node&lt;/h3&gt; 
&lt;p&gt;To run GRPO on a single GPU for &lt;code&gt;Qwen/Qwen2.5-1.5B&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Run the GRPO math example using a 1B parameter model
uv run python examples/run_grpo_math.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;By default, this uses the configuration in &lt;code&gt;examples/configs/grpo_math_1B.yaml&lt;/code&gt;. You can customize parameters with command-line overrides. For example, to run on 8 GPUs,&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Run the GRPO math example using a 1B parameter model using 8 GPUs
uv run python examples/run_grpo_math.py \
  cluster.gpus_per_node=8
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can override any of the parameters listed in the yaml configuration file. For example,&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;uv run python examples/run_grpo_math.py \
  policy.model_name="meta-llama/Llama-3.2-1B-Instruct" \
  checkpointing.checkpoint_dir="results/llama1b_math" \
  logger.wandb_enabled=True \
  logger.wandb.name="grpo-llama1b_math" \
  logger.num_val_samples_to_print=10
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The default configuration uses the DTensor training backend. We also provide a config &lt;code&gt;examples/configs/grpo_math_1B_megatron.yaml&lt;/code&gt; which is set up to use the Megatron backend out of the box.&lt;/p&gt; 
&lt;p&gt;To train using this config on a single GPU:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Run a GRPO math example on 1 GPU using the Megatron backend
uv run python examples/run_grpo_math.py \
  --config examples/configs/grpo_math_1B_megatron.yaml
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For additional details on supported backends and how to configure the training backend to suit your setup, refer to the &lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/docs/design-docs/training-backends.md"&gt;Training Backends documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;GRPO Multi-node&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Run from the root of NeMo RL repo
NUM_ACTOR_NODES=2

# grpo_math_8b uses Llama-3.1-8B-Instruct model
COMMAND="uv run ./examples/run_grpo_math.py --config examples/configs/grpo_math_8B.yaml cluster.num_nodes=2 checkpointing.checkpoint_dir='results/llama8b_2nodes' logger.wandb_enabled=True logger.wandb.name='grpo-llama8b_math'" \
CONTAINER=YOUR_CONTAINER \
MOUNTS="$PWD:$PWD" \
sbatch \
    --nodes=${NUM_ACTOR_NODES} \
    --account=YOUR_ACCOUNT \
    --job-name=YOUR_JOBNAME \
    --partition=YOUR_PARTITION \
    --time=4:0:0 \
    --gres=gpu:8 \
    ray.sub
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The required &lt;code&gt;CONTAINER&lt;/code&gt; can be built by following the instructions in the &lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/docs/docker.md"&gt;Docker documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h4&gt;GRPO Qwen2.5-32B&lt;/h4&gt; 
&lt;p&gt;This section outlines how to run GRPO for Qwen2.5-32B with a 16k sequence length.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Run from the root of NeMo RL repo
NUM_ACTOR_NODES=32

# Download Qwen before the job starts to avoid spending time downloading during the training loop
HF_HOME=/path/to/hf_home huggingface-cli download Qwen/Qwen2.5-32B

# Ensure HF_HOME is included in your MOUNTS
HF_HOME=/path/to/hf_home \
COMMAND="uv run ./examples/run_grpo_math.py --config examples/configs/grpo_math_8B.yaml policy.model_name='Qwen/Qwen2.5-32B' policy.generation.vllm_cfg.tensor_parallel_size=4 policy.max_total_sequence_length=16384 cluster.num_nodes=${NUM_ACTOR_NODES} policy.dtensor_cfg.enabled=True policy.dtensor_cfg.tensor_parallel_size=8 policy.dtensor_cfg.sequence_parallel=True policy.dtensor_cfg.activation_checkpointing=True checkpointing.checkpoint_dir='results/qwen2.5-32b' logger.wandb_enabled=True logger.wandb.name='qwen2.5-32b'" \
CONTAINER=YOUR_CONTAINER \
MOUNTS="$PWD:$PWD" \
sbatch \
    --nodes=${NUM_ACTOR_NODES} \
    --account=YOUR_ACCOUNT \
    --job-name=YOUR_JOBNAME \
    --partition=YOUR_PARTITION \
    --time=4:0:0 \
    --gres=gpu:8 \
    ray.sub
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;GRPO Multi-Turn&lt;/h4&gt; 
&lt;p&gt;We also support multi-turn generation and training (tool use, games, etc.). Reference example for training to play a Sliding Puzzle Game:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;uv run python examples/run_grpo_sliding_puzzle.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Supervised Fine-Tuning (SFT)&lt;/h2&gt; 
&lt;p&gt;We provide an example SFT experiment using the &lt;a href="https://rajpurkar.github.io/SQuAD-explorer/"&gt;SQuAD dataset&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;SFT Single Node&lt;/h3&gt; 
&lt;p&gt;The default SFT configuration is set to run on a single GPU. To start the experiment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;uv run python examples/run_sft.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This fine-tunes the &lt;code&gt;Llama3.2-1B&lt;/code&gt; model on the SQuAD dataset using a 1 GPU.&lt;/p&gt; 
&lt;p&gt;To use multiple GPUs on a single node, you can modify the cluster configuration. This adjustment will also let you potentially increase the model and batch size:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;uv run python examples/run_sft.py \
  policy.model_name="meta-llama/Meta-Llama-3-8B" \
  policy.train_global_batch_size=128 \
  sft.val_global_batch_size=128 \
  cluster.gpus_per_node=8
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Refer to &lt;code&gt;examples/configs/sft.yaml&lt;/code&gt; for a full list of parameters that can be overridden.&lt;/p&gt; 
&lt;h3&gt;SFT Multi-node&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Run from the root of NeMo RL repo
NUM_ACTOR_NODES=2

COMMAND="uv run ./examples/run_sft.py --config examples/configs/sft.yaml cluster.num_nodes=2 cluster.gpus_per_node=8 checkpointing.checkpoint_dir='results/sft_llama8b_2nodes' logger.wandb_enabled=True logger.wandb.name='sft-llama8b'" \
CONTAINER=YOUR_CONTAINER \
MOUNTS="$PWD:$PWD" \
sbatch \
    --nodes=${NUM_ACTOR_NODES} \
    --account=YOUR_ACCOUNT \
    --job-name=YOUR_JOBNAME \
    --partition=YOUR_PARTITION \
    --time=4:0:0 \
    --gres=gpu:8 \
    ray.sub
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;DPO&lt;/h2&gt; 
&lt;p&gt;We provide a sample DPO experiment that uses the &lt;a href="https://huggingface.co/datasets/nvidia/HelpSteer3"&gt;HelpSteer3 dataset&lt;/a&gt; for preference-based training.&lt;/p&gt; 
&lt;h3&gt;DPO Single Node&lt;/h3&gt; 
&lt;p&gt;The default DPO experiment is configured to run on a single GPU. To launch the experiment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;uv run python examples/run_dpo.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This trains &lt;code&gt;Llama3.2-1B-Instruct&lt;/code&gt; on one GPU.&lt;/p&gt; 
&lt;p&gt;If you have access to more GPUs, you can update the experiment accordingly. To run on 8 GPUs, we update the cluster configuration and switch to an 8B Llama3.1 Instruct model:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;uv run python examples/run_dpo.py \
  policy.model_name="meta-llama/Llama-3.1-8B-Instruct" \
  policy.train_global_batch_size=256 \
  cluster.gpus_per_node=8
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Any of the DPO parameters can be customized from the command line. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;uv run python examples/run_dpo.py \
  dpo.sft_loss_weight=0.1 \
  dpo.preference_average_log_probs=True \
  checkpointing.checkpoint_dir="results/llama_dpo_sft" \
  logger.wandb_enabled=True \
  logger.wandb.name="llama-dpo-sft"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Refer to &lt;code&gt;examples/configs/dpo.yaml&lt;/code&gt; for a full list of parameters that can be overridden. For an in-depth explanation of how to add your own DPO dataset, refer to the &lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/docs/guides/dpo.md"&gt;DPO documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;DPO Multi-node&lt;/h3&gt; 
&lt;p&gt;For distributed DPO training across multiple nodes, modify the following script for your use case:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Run from the root of NeMo RL repo
## number of nodes to use for your job
NUM_ACTOR_NODES=2

COMMAND="uv run ./examples/run_dpo.py --config examples/configs/dpo.yaml cluster.num_nodes=2 cluster.gpus_per_node=8 dpo.val_global_batch_size=32 checkpointing.checkpoint_dir='results/dpo_llama81_2nodes' logger.wandb_enabled=True logger.wandb.name='dpo-llama1b'" \
CONTAINER=YOUR_CONTAINER \
MOUNTS="$PWD:$PWD" \
sbatch \
    --nodes=${NUM_ACTOR_NODES} \
    --account=YOUR_ACCOUNT \
    --job-name=YOUR_JOBNAME \
    --partition=YOUR_PARTITION \
    --time=4:0:0 \
    --gres=gpu:8 \
    ray.sub
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;RM&lt;/h2&gt; 
&lt;p&gt;We provide a sample RM experiment that uses the &lt;a href="https://huggingface.co/datasets/nvidia/HelpSteer3"&gt;HelpSteer3 dataset&lt;/a&gt; for preference-based training.&lt;/p&gt; 
&lt;h3&gt;RM Single Node&lt;/h3&gt; 
&lt;p&gt;The default RM experiment is configured to run on a single GPU. To launch the experiment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;uv run python examples/run_rm.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This trains a RM based on &lt;code&gt;meta-llama/Llama-3.2-1B-Instruct&lt;/code&gt; on one GPU.&lt;/p&gt; 
&lt;p&gt;If you have access to more GPUs, you can update the experiment accordingly. To run on 8 GPUs, we update the cluster configuration:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;uv run python examples/run_rm.py cluster.gpus_per_node=8
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Refer to the &lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/docs/guides/rm.md"&gt;RM documentation&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h3&gt;RM Multi-node&lt;/h3&gt; 
&lt;p&gt;For distributed RM training across multiple nodes, modify the following script for your use case:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Run from the root of NeMo RL repo
## number of nodes to use for your job
NUM_ACTOR_NODES=2

COMMAND="uv run ./examples/run_rm.py --config examples/configs/rm.yaml cluster.num_nodes=2 cluster.gpus_per_node=8 checkpointing.checkpoint_dir='results/rm_llama1b_2nodes' logger.wandb_enabled=True logger.wandb.name='rm-llama1b-2nodes'" \
CONTAINER=YOUR_CONTAINER \
MOUNTS="$PWD:$PWD" \
sbatch \
    --nodes=${NUM_ACTOR_NODES} \
    --account=YOUR_ACCOUNT \
    --job-name=YOUR_JOBNAME \
    --partition=YOUR_PARTITION \
    --time=4:0:0 \
    --gres=gpu:8 \
    ray.sub
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Evaluation&lt;/h2&gt; 
&lt;p&gt;We provide evaluation tools to assess model capabilities.&lt;/p&gt; 
&lt;h3&gt;Convert Model Format (Optional)&lt;/h3&gt; 
&lt;p&gt;If you have trained a model and saved the checkpoint in the Pytorch DCP format, you first need to convert it to the Hugging Face format before running evaluation:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Example for a GRPO checkpoint at step 170
uv run python examples/converters/convert_dcp_to_hf.py \
    --config results/grpo/step_170/config.yaml \
    --dcp-ckpt-path results/grpo/step_170/policy/weights/ \
    --hf-ckpt-path results/grpo/hf
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you have a model saved in Megatron format, you can use the following command to convert it to Hugging Face format prior to running evaluation. This script requires mcore, so make sure to launch with the mcore extra:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Example for a GRPO checkpoint at step 170
uv run --extra mcore python examples/converters/convert_megatron_to_hf.py \
    --config results/grpo/step_170/config.yaml \
    --megatron-ckpt-path results/grpo/step_170/policy/weights/iter_0000000 \
    --hf-ckpt-path results/grpo/hf
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Adjust the paths according to your training output directory structure.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;For an in-depth explanation of checkpointing, refer to the &lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/docs/design-docs/checkpointing.md"&gt;Checkpointing documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Run Evaluation&lt;/h3&gt; 
&lt;p&gt;Run evaluation script with converted model:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;uv run python examples/run_eval.py generation.model_name=$PWD/results/grpo/hf
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Run evaluation script with custom settings:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-sh"&gt;# Example: Evaluation of DeepScaleR-1.5B-Preview on MATH-500 using 8 GPUs
#          Pass@1 accuracy averaged over 16 samples for each problem
uv run python examples/run_eval.py \
    --config examples/configs/evals/math_eval.yaml \
    generation.model_name=agentica-org/DeepScaleR-1.5B-Preview \
    generation.temperature=0.6 \
    generation.top_p=0.95 \
    generation.vllm_cfg.max_model_len=32768 \
    data.dataset_name=math500 \
    eval.num_tests_per_prompt=16 \
    cluster.gpus_per_node=8
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Evaluation results may vary slightly due to various factors, such as sampling parameters, random seed, inference engine version, and inference engine settings.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Refer to &lt;code&gt;examples/configs/evals/eval.yaml&lt;/code&gt; for a full list of parameters that can be overridden. For an in-depth explanation of evaluation, refer to the &lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/docs/guides/eval.md"&gt;Evaluation documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Set Up Clusters&lt;/h2&gt; 
&lt;p&gt;For detailed instructions on how to set up and launch NeMo RL on Slurm or Kubernetes clusters, please refer to the dedicated &lt;a href="https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/docs/cluster.md"&gt;Cluster Start&lt;/a&gt; documentation.&lt;/p&gt; 
&lt;h2&gt;Tips and Tricks&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;If you forget to initialize the NeMo and Megatron submodules when cloning the NeMo-RL repository, you may run into an error like this:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;ModuleNotFoundError: No module named 'megatron'
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you see this error, there is likely an issue with your virtual environments. To fix this, first intialize the submodules:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;git submodule update --init --recursive
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;and then force a rebuild of the virtual environments by setting &lt;code&gt;NRL_FORCE_REBUILD_VENVS=true&lt;/code&gt; next time you launch a run:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-sh"&gt;NRL_FORCE_REBUILD_VENVS=true uv run examples/run_grpo.py ...
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you use NeMo RL in your research, please cite it using the following BibTeX entry:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bibtex"&gt;@misc{nemo-rl,
title = {NeMo RL: A Scalable and Efficient Post-Training Library},
howpublished = {\url{https://github.com/NVIDIA-NeMo/RL}},
year = {2025},
note = {GitHub repository},
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions to NeMo RL! Please see our &lt;a href="https://github.com/NVIDIA-NeMo/RL/raw/main/CONTRIBUTING.md"&gt;Contributing Guidelines&lt;/a&gt; for more information on how to get involved.&lt;/p&gt; 
&lt;h2&gt;Licenses&lt;/h2&gt; 
&lt;p&gt;NVIDIA NeMo RL is licensed under the &lt;a href="https://github.com/NVIDIA-NeMo/RL/raw/main/LICENSE"&gt;Apache License 2.0&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>HKUDS/DeepCode</title>
      <link>https://github.com/HKUDS/DeepCode</link>
      <description>&lt;p&gt;"DeepCode: Open Agentic Coding (Paper2Code &amp; Text2Web &amp; Text2Backend)"&lt;/p&gt;&lt;hr&gt;&lt;div align="center"&gt; 
 &lt;table style="border: none; margin: 0 auto; padding: 0; border-collapse: collapse;"&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td align="center" style="vertical-align: middle; padding: 10px; border: none; width: 250px;"&gt; &lt;img src="https://raw.githubusercontent.com/HKUDS/DeepCode/main/assets/logo.png" alt="DeepCode Logo" width="200" style="margin: 0; padding: 0; display: block;" /&gt; &lt;/td&gt; 
    &lt;td align="left" style="vertical-align: middle; padding: 10px 0 10px 30px; border: none;"&gt; &lt;pre style="font-family: 'Courier New', monospace; font-size: 16px; color: #0EA5E9; margin: 0; padding: 0; text-shadow: 0 0 10px #0EA5E9, 0 0 20px rgba(14,165,233,0.5); line-height: 1.2; transform: skew(-1deg, 0deg); display: block;"&gt;    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó
    ‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù
    ‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó
    ‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù  ‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù  ‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïù ‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù
    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë     ‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó
    ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïù      ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù&lt;/pre&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
 &lt;div align="center"&gt; 
  &lt;a href="https://trendshift.io/repositories/14665" target="_blank"&gt;&lt;img src="https://trendshift.io/api/badge/repositories/14665" alt="HKUDS%2FDeepCode | Trendshift" style="width: 250px; height: 55px;" width="250" height="55" /&gt;&lt;/a&gt; 
 &lt;/div&gt; 
 &lt;!-- &lt;img src="https://readme-typing-svg.herokuapp.com?font=Russo+One&amp;size=28&amp;duration=2000&amp;pause=800&amp;color=06B6D4&amp;background=00000000&amp;center=true&amp;vCenter=true&amp;width=800&amp;height=50&amp;lines=%E2%9A%A1+OPEN+AGENTIC+CODING+%E2%9A%A1" alt="DeepCode Tech Subtitle" style="margin-top: 5px; filter: drop-shadow(0 0 12px #06B6D4) drop-shadow(0 0 24px rgba(6,182,212,0.4));"/&gt; --&gt; 
 &lt;h1&gt;&lt;img src="https://github.com/Zongwei9888/Experiment_Images/raw/43c585dca3d21b8e4b6390d835cdd34dc4b4b23d/DeepCode_images/title_logo.svg?sanitize=true" alt="DeepCode Logo" width="32" height="32" style="vertical-align: middle; margin-right: 8px;" /&gt; DeepCode: Open Agentic Coding&lt;/h1&gt; 
 &lt;h3&gt;&lt;em&gt;Advancing Code Generation with Multi-Agent Systems&lt;/em&gt;&lt;/h3&gt; 
 &lt;!-- &lt;p align="center"&gt;
  &lt;img src="https://img.shields.io/badge/Version-1.0.0-00d4ff?style=for-the-badge&amp;logo=rocket&amp;logoColor=white" alt="Version"&gt;

  &lt;img src="https://img.shields.io/badge/License-MIT-4ecdc4?style=for-the-badge&amp;logo=opensourceinitiative&amp;logoColor=white" alt="License"&gt;
  &lt;img src="https://img.shields.io/badge/AI-Multi--Agent-9b59b6?style=for-the-badge&amp;logo=brain&amp;logoColor=white" alt="AI"&gt;
  &lt;img src="https://img.shields.io/badge/HKU-Data_Intelligence_Lab-f39c12?style=for-the-badge&amp;logo=university&amp;logoColor=white" alt="HKU"&gt;
&lt;/p&gt; --&gt; 
 &lt;p&gt; &lt;a href="https://github.com/HKUDS/DeepCode/stargazers"&gt;&lt;img src="https://img.shields.io/github/stars/HKUDS/DeepCode?color=00d9ff&amp;amp;style=for-the-badge&amp;amp;logo=star&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;img src="https://img.shields.io/badge/üêçPython-3.13-4ecdc4?style=for-the-badge&amp;amp;logo=python&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt; &lt;a href="https://pypi.org/project/deepcode-hku/"&gt;&lt;img src="https://img.shields.io/pypi/v/deepcode-hku.svg?style=for-the-badge&amp;amp;logo=pypi&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e&amp;amp;color=ff6b6b" /&gt;&lt;/a&gt; &lt;/p&gt; 
 &lt;p&gt; &lt;a href="https://discord.gg/yF2MmDJyGJ"&gt;&lt;img src="https://img.shields.io/badge/üí¨Discord-Community-7289da?style=for-the-badge&amp;amp;logo=discord&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;a href="https://github.com/HKUDS/DeepCode/issues/11"&gt;&lt;img src="https://img.shields.io/badge/üí¨WeChat-Group-07c160?style=for-the-badge&amp;amp;logo=wechat&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt;&lt;/a&gt; &lt;/p&gt; 
 &lt;div align="center"&gt; 
  &lt;div style="width: 100%; height: 2px; margin: 20px 0; background: linear-gradient(90deg, transparent, #00d9ff, transparent);"&gt;&lt;/div&gt; 
 &lt;/div&gt; 
 &lt;div align="center"&gt; 
  &lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-quick-start" style="text-decoration: none;"&gt; &lt;img src="https://img.shields.io/badge/Quick%20Start-Get%20Started%20Now-00d9ff?style=for-the-badge&amp;amp;logo=rocket&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e" /&gt; &lt;/a&gt; 
 &lt;/div&gt; 
 &lt;h3&gt;üñ•Ô∏è &lt;strong&gt;Interface Showcase&lt;/strong&gt;&lt;/h3&gt; 
 &lt;table align="center" width="100%" style="border: none; border-collapse: collapse; margin: 30px 0;"&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td width="50%" align="center" style="vertical-align: top; padding: 20px;"&gt; &lt;h4&gt;üñ•Ô∏è &lt;strong&gt;CLI Interface&lt;/strong&gt;&lt;/h4&gt; &lt;p&gt;&lt;strong&gt;Terminal-Based Development&lt;/strong&gt;&lt;/p&gt; 
     &lt;div align="center"&gt; 
      &lt;img src="https://github.com/Zongwei9888/Experiment_Images/raw/8882a7313c504ca97ead6e7b36c51aa761b6a4f3/DeepCode_images/CLI.gif" alt="CLI Interface Demo" width="100%" style="border-radius: 10px; box-shadow: 0 8px 20px rgba(45,55,72,0.3); margin: 15px 0;" /&gt; 
      &lt;div style="background: linear-gradient(135deg, #2D3748 0%, #4A5568 100%); border-radius: 12px; padding: 15px; margin: 15px 0; color: white;"&gt; 
       &lt;strong&gt;üöÄ Advanced Terminal Experience&lt;/strong&gt;
       &lt;br /&gt; 
       &lt;small&gt;‚ö° Fast command-line workflow&lt;br /&gt;üîß Developer-friendly interface&lt;br /&gt;üìä Real-time progress tracking&lt;/small&gt; 
      &lt;/div&gt; 
      &lt;p&gt;&lt;em&gt;Professional terminal interface for advanced users and CI/CD integration&lt;/em&gt;&lt;/p&gt; 
     &lt;/div&gt; &lt;/td&gt; 
    &lt;td width="50%" align="center" style="vertical-align: top; padding: 20px;"&gt; &lt;h4&gt;üåê &lt;strong&gt;Web Interface&lt;/strong&gt;&lt;/h4&gt; &lt;p&gt;&lt;strong&gt;Visual Interactive Experience&lt;/strong&gt;&lt;/p&gt; 
     &lt;div align="center"&gt; 
      &lt;img src="https://github.com/Zongwei9888/Experiment_Images/raw/8882a7313c504ca97ead6e7b36c51aa761b6a4f3/DeepCode_images/UI.gif" alt="Web Interface Demo" width="100%" style="border-radius: 10px; box-shadow: 0 8px 20px rgba(14,165,233,0.3); margin: 15px 0;" /&gt; 
      &lt;div style="background: linear-gradient(135deg, #0EA5E9 0%, #00D4FF 100%); border-radius: 12px; padding: 15px; margin: 15px 0; color: white;"&gt; 
       &lt;strong&gt;üé® Modern Web Dashboard&lt;/strong&gt;
       &lt;br /&gt; 
       &lt;small&gt;üñ±Ô∏è Intuitive drag-and-drop&lt;br /&gt;üì± Responsive design&lt;br /&gt;üéØ Visual progress tracking&lt;/small&gt; 
      &lt;/div&gt; 
      &lt;p&gt;&lt;em&gt;Beautiful web interface with streamlined workflow for all skill levels&lt;/em&gt;&lt;/p&gt; 
     &lt;/div&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
 &lt;hr /&gt; 
 &lt;div align="center"&gt; 
  &lt;h3&gt;üé¨ &lt;strong&gt;Introduction Video&lt;/strong&gt;&lt;/h3&gt; 
  &lt;div style="margin: 20px 0;"&gt; 
   &lt;a href="https://youtu.be/PRgmP8pOI08" target="_blank"&gt; &lt;img src="https://img.youtube.com/vi/PRgmP8pOI08/maxresdefault.jpg" alt="DeepCode Introduction Video" width="75%" style="border-radius: 12px; box-shadow: 0 8px 25px rgba(0,0,0,0.15); transition: transform 0.3s ease;" /&gt; &lt;/a&gt; 
  &lt;/div&gt; 
  &lt;p&gt;&lt;em&gt;üéØ &lt;strong&gt;Watch our complete introduction&lt;/strong&gt; - See how DeepCode transforms research papers and natural language into production-ready code&lt;/em&gt;&lt;/p&gt; 
  &lt;p&gt; &lt;a href="https://youtu.be/PRgmP8pOI08" target="_blank"&gt; &lt;img src="https://img.shields.io/badge/‚ñ∂Ô∏è_Watch_Video-FF0000?style=for-the-badge&amp;amp;logo=youtube&amp;amp;logoColor=white" alt="Watch Video" /&gt; &lt;/a&gt; &lt;/p&gt; 
 &lt;/div&gt; 
 &lt;hr /&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;&lt;em&gt;"Where AI Agents Transform Ideas into Production-Ready Code"&lt;/em&gt;&lt;/p&gt; 
 &lt;/blockquote&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üìë Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-key-features"&gt;üöÄ Key Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#%EF%B8%8F-architecture"&gt;üèóÔ∏è Architecture&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-quick-start"&gt;üöÄ Quick Start&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-examples"&gt;üí° Examples&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-live-demonstrations"&gt;üé¨ Live Demonstrations&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-star-history"&gt;‚≠ê Star History&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-license"&gt;üìÑ License&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üöÄ Key Features&lt;/h2&gt; 
&lt;br /&gt; 
&lt;table align="center" width="100%" style="border: none; table-layout: fixed;"&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td width="30%" align="center" style="vertical-align: top; padding: 20px;"&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;h3 style="margin: 0; padding: 0;"&gt;üöÄ &lt;strong&gt;Paper2Code&lt;/strong&gt;&lt;/h3&gt; 
    &lt;/div&gt; 
    &lt;div align="center" style="margin: 15px 0;"&gt; 
     &lt;img src="https://img.shields.io/badge/ALGORITHM-IMPLEMENTATION-ff6b6b?style=for-the-badge&amp;amp;logo=algorithm&amp;amp;logoColor=white" alt="Algorithm Badge" /&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;&lt;strong&gt;Automated Implementation of Complex Algorithms&lt;/strong&gt;&lt;/p&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 60px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;Effortlessly converts complex algorithms from research papers into &lt;strong&gt;high-quality&lt;/strong&gt;, &lt;strong&gt;production-ready&lt;/strong&gt; code, accelerating algorithm reproduction.&lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
   &lt;td width="30%" align="center" style="vertical-align: top; padding: 20px;"&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;h3 style="margin: 0; padding: 0;"&gt;üé® &lt;strong&gt;Text2Web&lt;/strong&gt;&lt;/h3&gt; 
    &lt;/div&gt; 
    &lt;div align="center" style="margin: 15px 0;"&gt; 
     &lt;img src="https://img.shields.io/badge/FRONTEND-DEVELOPMENT-4ecdc4?style=for-the-badge&amp;amp;logo=react&amp;amp;logoColor=white" alt="Frontend Badge" /&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;&lt;strong&gt;Automated Front-End Web Development&lt;/strong&gt;&lt;/p&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 60px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;Translates plain textual descriptions into &lt;strong&gt;fully functional&lt;/strong&gt;, &lt;strong&gt;visually appealing&lt;/strong&gt; front-end web code for rapid interface creation.&lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
   &lt;td width="30%" align="center" style="vertical-align: top; padding: 20px;"&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;h3 style="margin: 0; padding: 0;"&gt;‚öôÔ∏è &lt;strong&gt;Text2Backend&lt;/strong&gt;&lt;/h3&gt; 
    &lt;/div&gt; 
    &lt;div align="center" style="margin: 15px 0;"&gt; 
     &lt;img src="https://img.shields.io/badge/BACKEND-DEVELOPMENT-9b59b6?style=for-the-badge&amp;amp;logo=server&amp;amp;logoColor=white" alt="Backend Badge" /&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 80px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;&lt;strong&gt;Automated Back-End Development&lt;/strong&gt;&lt;/p&gt; 
    &lt;/div&gt; 
    &lt;div style="height: 60px; display: flex; align-items: center; justify-content: center;"&gt; 
     &lt;p align="center"&gt;Generates &lt;strong&gt;efficient&lt;/strong&gt;, &lt;strong&gt;scalable&lt;/strong&gt;, and &lt;strong&gt;feature-rich&lt;/strong&gt; back-end code from simple text inputs, streamlining server-side development.&lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;br /&gt; 
&lt;h3&gt;üéØ &lt;strong&gt;Autonomous Multi-Agent Workflow&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;The Challenges&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;üìÑ &lt;strong&gt;Implementation Complexity&lt;/strong&gt;: Converting academic papers and complex algorithms into working code requires significant technical effort and domain expertise&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üî¨ &lt;strong&gt;Research Bottleneck&lt;/strong&gt;: Researchers spend valuable time implementing algorithms instead of focusing on their core research and discovery work&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;‚è±Ô∏è &lt;strong&gt;Development Delays&lt;/strong&gt;: Product teams experience long wait times between concept and testable prototypes, slowing down innovation cycles&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üîÑ &lt;strong&gt;Repetitive Coding&lt;/strong&gt;: Developers repeatedly implement similar patterns and functionality instead of building on existing solutions&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;DeepCode&lt;/strong&gt; addresses these workflow inefficiencies by providing reliable automation for common development tasks, streamlining your development workflow from concept to code.&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;pre&gt;&lt;code class="language-mermaid"&gt;flowchart LR
    A["üìÑ Research Papers&amp;lt;br/&amp;gt;üí¨ Text Prompts&amp;lt;br/&amp;gt;üåê URLs &amp;amp; Document&amp;lt;br/&amp;gt;üìé Files: PDF, DOC, PPTX, TXT, HTML"] --&amp;gt; B["üß† DeepCode&amp;lt;br/&amp;gt;Multi-Agent Engine"]
    B --&amp;gt; C["üöÄ Algorithm Implementation &amp;lt;br/&amp;gt;üé® Frontend Development &amp;lt;br/&amp;gt;‚öôÔ∏è Backend Development"]

    style A fill:#ff6b6b,stroke:#c0392b,stroke-width:2px,color:#000
    style B fill:#00d4ff,stroke:#0984e3,stroke-width:3px,color:#000
    style C fill:#00b894,stroke:#00a085,stroke-width:2px,color:#000
&lt;/code&gt;&lt;/pre&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üèóÔ∏è Architecture&lt;/h2&gt; 
&lt;h3&gt;üìä &lt;strong&gt;System Overview&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;DeepCode&lt;/strong&gt; is an AI-powered development platform that automates code generation and implementation tasks. Our multi-agent system handles the complexity of translating requirements into functional, well-structured code, allowing you to focus on innovation rather than implementation details.&lt;/p&gt; 
&lt;p&gt;üéØ &lt;strong&gt;Technical Capabilities&lt;/strong&gt;:&lt;/p&gt; 
&lt;p&gt;üß¨ &lt;strong&gt;Research-to-Production Pipeline&lt;/strong&gt;&lt;br /&gt; Multi-modal document analysis engine that extracts algorithmic logic and mathematical models from academic papers. Generates optimized implementations with proper data structures while preserving computational complexity characteristics.&lt;/p&gt; 
&lt;p&gt;ü™Ñ &lt;strong&gt;Natural Language Code Synthesis&lt;/strong&gt;&lt;br /&gt; Context-aware code generation using fine-tuned language models trained on curated code repositories. Maintains architectural consistency across modules while supporting multiple programming languages and frameworks.&lt;/p&gt; 
&lt;p&gt;‚ö° &lt;strong&gt;Automated Prototyping Engine&lt;/strong&gt;&lt;br /&gt; Intelligent scaffolding system generating complete application structures including database schemas, API endpoints, and frontend components. Uses dependency analysis to ensure scalable architecture from initial generation.&lt;/p&gt; 
&lt;p&gt;üíé &lt;strong&gt;Quality Assurance Automation&lt;/strong&gt;&lt;br /&gt; Integrated static analysis with automated unit test generation and documentation synthesis. Employs AST analysis for code correctness and property-based testing for comprehensive coverage.&lt;/p&gt; 
&lt;p&gt;üîÆ &lt;strong&gt;CodeRAG Integration System&lt;/strong&gt;&lt;br /&gt; Advanced retrieval-augmented generation combining semantic vector embeddings with graph-based dependency analysis. Automatically discovers optimal libraries and implementation patterns from large-scale code corpus.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;üîß &lt;strong&gt;Core Techniques&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;üß† &lt;strong&gt;Intelligent Orchestration Agent&lt;/strong&gt;: Central decision-making system that coordinates workflow phases and analyzes requirements. Employs dynamic planning algorithms to adapt execution strategies in real-time based on evolving project complexity. Dynamically selects optimal processing strategies for each implementation step. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üíæ &lt;strong&gt;Efficient Memory Mechanism&lt;/strong&gt;: Advanced context engineering system that manages large-scale code contexts efficiently. Implements hierarchical memory structures with intelligent compression for handling complex codebases. This component enables instant retrieval of implementation patterns and maintains semantic coherence across extended development sessions. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;üîç &lt;strong&gt;Advanced CodeRAG System&lt;/strong&gt;: Global code comprehension engine that analyzes complex inter-dependencies across repositories. Performs cross-codebase relationship mapping to understand architectural patterns from a holistic perspective. This module leverages dependency graphs and semantic analysis to provide globally-aware code recommendations during implementation.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h3&gt;ü§ñ &lt;strong&gt;Multi-Agent Architecture of DeepCode&lt;/strong&gt;:&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üéØ Central Orchestrating Agent&lt;/strong&gt;: Orchestrates entire workflow execution and makes strategic decisions. Coordinates specialized agents based on input complexity analysis. Implements dynamic task planning and resource allocation algorithms. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üìù Intent Understanding Agent&lt;/strong&gt;: Performs deep semantic analysis of user requirements to decode complex intentions. Extracts functional specifications and technical constraints through advanced NLP processing. Transforms ambiguous human descriptions into precise, actionable development specifications with structured task decomposition. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üìÑ Document Parsing Agent&lt;/strong&gt;: Processes complex technical documents and research papers with advanced parsing capabilities. Extracts algorithms and methodologies using document understanding models. Converts academic concepts into practical implementation specifications through intelligent content analysis. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üèóÔ∏è Code Planning Agent&lt;/strong&gt;: Performs architectural design and technology stack optimization. Dynamic planning for adaptive development roadmaps. Enforces coding standards and generates modular structures through automated design pattern selection.&lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üîç Code Reference Mining Agent&lt;/strong&gt;: Discovers relevant repositories and frameworks through intelligent search algorithms. Analyzes codebases for compatibility and integration potential. Provides recommendations based on similarity metrics and automated dependency analysis. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üìö Code Indexing Agent&lt;/strong&gt;: Builds comprehensive knowledge graphs of discovered codebases. Maintains semantic relationships between code components. Enables intelligent retrieval and cross-reference capabilities. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üß¨ Code Generation Agent&lt;/strong&gt;: Synthesizes gathered information into executable code implementations. Creates functional interfaces and integrates discovered components. Generates comprehensive test suites and documentation for reproducibility.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h4&gt;üõ†Ô∏è &lt;strong&gt;Implementation Tools Matrix&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;üîß Powered by MCP (Model Context Protocol)&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;DeepCode leverages the &lt;strong&gt;Model Context Protocol (MCP)&lt;/strong&gt; standard to seamlessly integrate with various tools and services. This standardized approach ensures reliable communication between AI agents and external systems, enabling powerful automation capabilities.&lt;/p&gt; 
&lt;h5&gt;üì° &lt;strong&gt;MCP Servers &amp;amp; Tools&lt;/strong&gt;&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;üõ†Ô∏è &lt;strong&gt;MCP Server&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;üîß &lt;strong&gt;Primary Function&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;üí° &lt;strong&gt;Purpose &amp;amp; Capabilities&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üîç brave&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Web Search Engine&lt;/td&gt; 
   &lt;td&gt;Real-time information retrieval via Brave Search API&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üåê bocha-mcp&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Alternative Search&lt;/td&gt; 
   &lt;td&gt;Secondary search option with independent API access&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üìÇ filesystem&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;File System Operations&lt;/td&gt; 
   &lt;td&gt;Local file and directory management, read/write operations&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üåê fetch&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Web Content Retrieval&lt;/td&gt; 
   &lt;td&gt;Fetch and extract content from URLs and web resources&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üì• github-downloader&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Repository Management&lt;/td&gt; 
   &lt;td&gt;Clone and download GitHub repositories for analysis&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üìã file-downloader&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Document Processing&lt;/td&gt; 
   &lt;td&gt;Download and convert files (PDF, DOCX, etc.) to Markdown&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;‚ö° command-executor&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;System Commands&lt;/td&gt; 
   &lt;td&gt;Execute bash/shell commands for environment management&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üß¨ code-implementation&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Code Generation Hub&lt;/td&gt; 
   &lt;td&gt;Comprehensive code reproduction with execution and testing&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üìö code-reference-indexer&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Smart Code Search&lt;/td&gt; 
   &lt;td&gt;Intelligent indexing and search of code repositories&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üìÑ document-segmentation&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Smart Document Analysis&lt;/td&gt; 
   &lt;td&gt;Intelligent document segmentation for large papers and technical documents&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h5&gt;üîß &lt;strong&gt;Legacy Tool Functions&lt;/strong&gt; &lt;em&gt;(for reference)&lt;/em&gt;&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;üõ†Ô∏è &lt;strong&gt;Function&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;üéØ &lt;strong&gt;Usage Context&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üìÑ read_code_mem&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Efficient code context retrieval from memory&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;‚úçÔ∏è write_file&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Direct file content generation and modification&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üêç execute_python&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Python code testing and validation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üìÅ get_file_structure&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Project structure analysis and organization&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;‚öôÔ∏è set_workspace&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Dynamic workspace and environment configuration&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;üìä get_operation_history&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Process monitoring and operation tracking&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;p&gt;üéõÔ∏è &lt;strong&gt;Multi-Interface Framework&lt;/strong&gt;&lt;br /&gt; RESTful API with CLI and web frontends featuring real-time code streaming, interactive debugging, and extensible plugin architecture for CI/CD integration.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;üöÄ Multi-Agent Intelligent Pipeline:&lt;/strong&gt;&lt;/p&gt; 
&lt;div align="center"&gt; 
 &lt;h3&gt;üåü &lt;strong&gt;Intelligence Processing Flow&lt;/strong&gt;&lt;/h3&gt; 
 &lt;table align="center" width="100%" style="border: none; border-collapse: collapse;"&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td colspan="3" align="center" style="padding: 20px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 15px; color: white; font-weight: bold;"&gt; üí° &lt;strong&gt;INPUT LAYER&lt;/strong&gt;&lt;br /&gt; üìÑ Research Papers ‚Ä¢ üí¨ Natural Language ‚Ä¢ üåê URLs ‚Ä¢ üìã Requirements &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan="3" height="20"&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td colspan="3" align="center" style="padding: 15px; background: linear-gradient(135deg, #ff6b6b 0%, #ee5a24 100%); border-radius: 12px; color: white; font-weight: bold;"&gt; üéØ &lt;strong&gt;CENTRAL ORCHESTRATION&lt;/strong&gt;&lt;br /&gt; Strategic Decision Making ‚Ä¢ Workflow Coordination ‚Ä¢ Agent Management &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan="3" height="15"&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center" style="padding: 12px; background: linear-gradient(135deg, #3742fa 0%, #2f3542 100%); border-radius: 10px; color: white; width: 50%;"&gt; üìù &lt;strong&gt;TEXT ANALYSIS&lt;/strong&gt;&lt;br /&gt; &lt;small&gt;Requirement Processing&lt;/small&gt; &lt;/td&gt; 
    &lt;td width="10"&gt;&lt;/td&gt; 
    &lt;td align="center" style="padding: 12px; background: linear-gradient(135deg, #8c7ae6 0%, #9c88ff 100%); border-radius: 10px; color: white; width: 50%;"&gt; üìÑ &lt;strong&gt;DOCUMENT ANALYSIS&lt;/strong&gt;&lt;br /&gt; &lt;small&gt;Paper &amp;amp; Spec Processing&lt;/small&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan="3" height="15"&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td colspan="3" align="center" style="padding: 15px; background: linear-gradient(135deg, #00d2d3 0%, #54a0ff 100%); border-radius: 12px; color: white; font-weight: bold;"&gt; üìã &lt;strong&gt;REPRODUCTION PLANNING&lt;/strong&gt;&lt;br /&gt; Deep Paper Analysis ‚Ä¢ Code Requirements Parsing ‚Ä¢ Reproduction Strategy Development &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan="3" height="15"&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align="center" style="padding: 12px; background: linear-gradient(135deg, #ffa726 0%, #ff7043 100%); border-radius: 10px; color: white; width: 50%;"&gt; üîç &lt;strong&gt;REFERENCE ANALYSIS&lt;/strong&gt;&lt;br /&gt; &lt;small&gt;Repository Discovery&lt;/small&gt; &lt;/td&gt; 
    &lt;td width="10"&gt;&lt;/td&gt; 
    &lt;td align="center" style="padding: 12px; background: linear-gradient(135deg, #e056fd 0%, #f368e0 100%); border-radius: 10px; color: white; width: 50%;"&gt; üìö &lt;strong&gt;CODE INDEXING&lt;/strong&gt;&lt;br /&gt; &lt;small&gt;Knowledge Graph Building&lt;/small&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan="3" height="15"&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td colspan="3" align="center" style="padding: 15px; background: linear-gradient(135deg, #26de81 0%, #20bf6b 100%); border-radius: 12px; color: white; font-weight: bold;"&gt; üß¨ &lt;strong&gt;CODE IMPLEMENTATION&lt;/strong&gt;&lt;br /&gt; Implementation Generation ‚Ä¢ Testing ‚Ä¢ Documentation &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan="3" height="15"&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td colspan="3" align="center" style="padding: 20px; background: linear-gradient(135deg, #045de9 0%, #09c6f9 100%); border-radius: 15px; color: white; font-weight: bold;"&gt; ‚ö° &lt;strong&gt;OUTPUT DELIVERY&lt;/strong&gt;&lt;br /&gt; üì¶ Complete Codebase ‚Ä¢ üß™ Test Suite ‚Ä¢ üìö Documentation ‚Ä¢ üöÄ Deployment Ready &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;div align="center"&gt; 
 &lt;br /&gt; 
 &lt;h3&gt;üîÑ &lt;strong&gt;Process Intelligence Features&lt;/strong&gt;&lt;/h3&gt; 
 &lt;table align="center" style="border: none;"&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td align="center" width="25%" style="padding: 15px;"&gt; 
     &lt;div style="background: #f8f9fa; border-radius: 10px; padding: 15px; border-left: 4px solid #ff6b6b;"&gt; 
      &lt;h4&gt;üéØ Adaptive Flow&lt;/h4&gt; 
      &lt;p&gt;&lt;small&gt;Dynamic agent selection based on input complexity&lt;/small&gt;&lt;/p&gt; 
     &lt;/div&gt; &lt;/td&gt; 
    &lt;td align="center" width="25%" style="padding: 15px;"&gt; 
     &lt;div style="background: #f8f9fa; border-radius: 10px; padding: 15px; border-left: 4px solid #4ecdc4;"&gt; 
      &lt;h4&gt;üß† Smart Coordination&lt;/h4&gt; 
      &lt;p&gt;&lt;small&gt;Intelligent task distribution and parallel processing&lt;/small&gt;&lt;/p&gt; 
     &lt;/div&gt; &lt;/td&gt; 
    &lt;td align="center" width="25%" style="padding: 15px;"&gt; 
     &lt;div style="background: #f8f9fa; border-radius: 10px; padding: 15px; border-left: 4px solid #45b7d1;"&gt; 
      &lt;h4&gt;üîç Context Awareness&lt;/h4&gt; 
      &lt;p&gt;&lt;small&gt;Deep understanding through CodeRAG integration&lt;/small&gt;&lt;/p&gt; 
     &lt;/div&gt; &lt;/td&gt; 
    &lt;td align="center" width="25%" style="padding: 15px;"&gt; 
     &lt;div style="background: #f8f9fa; border-radius: 10px; padding: 15px; border-left: 4px solid #96ceb4;"&gt; 
      &lt;h4&gt;‚ö° Quality Assurance&lt;/h4&gt; 
      &lt;p&gt;&lt;small&gt;Automated testing and validation throughout&lt;/small&gt;&lt;/p&gt; 
     &lt;/div&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üöÄ Quick Start&lt;/h2&gt; 
&lt;h3&gt;üì¶ &lt;strong&gt;Step 1: Installation&lt;/strong&gt;&lt;/h3&gt; 
&lt;h4&gt;‚ö° &lt;strong&gt;Direct Installation (Recommended)&lt;/strong&gt;&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# üöÄ Install DeepCode package directly
pip install deepcode-hku

# üîë Download configuration files
curl -O https://raw.githubusercontent.com/HKUDS/DeepCode/main/mcp_agent.config.yaml
curl -O https://raw.githubusercontent.com/HKUDS/DeepCode/main/mcp_agent.secrets.yaml

# üîë Configure API keys (required)
# Edit mcp_agent.secrets.yaml with your API keys and base_url:
# - openai: api_key, base_url (for OpenAI/custom endpoints)
# - anthropic: api_key (for Claude models)

# üîë Configure search API keys for web search (optional)
# Edit mcp_agent.config.yaml to set your API keys:
# - For Brave Search: Set BRAVE_API_KEY: "your_key_here" in brave.env section (line ~28)
# - For Bocha-MCP: Set BOCHA_API_KEY: "your_key_here" in bocha-mcp.env section (line ~74)

# üìÑ Configure document segmentation (optional)
# Edit mcp_agent.config.yaml to control document processing:
# - enabled: true/false (whether to use intelligent document segmentation)
# - size_threshold_chars: 50000 (document size threshold to trigger segmentation)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;üîß &lt;strong&gt;Development Installation (From Source)&lt;/strong&gt;&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;üìÇ Click to expand development installation options&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h5&gt;üî• &lt;strong&gt;Using UV (Recommended for Development)&lt;/strong&gt;&lt;/h5&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# üîΩ Clone the repository
git clone https://github.com/HKUDS/DeepCode.git
cd DeepCode/

# üì¶ Install UV package manager
curl -LsSf https://astral.sh/uv/install.sh | sh

# üîß Install dependencies with UV
uv venv --python=3.13
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
uv pip install -r requirements.txt

# üîë Configure API keys (required)
# Edit mcp_agent.secrets.yaml with your API keys and base_url:
# - openai: api_key, base_url (for OpenAI/custom endpoints)
# - anthropic: api_key (for Claude models)

# üîë Configure search API keys for web search (optional)
# Edit mcp_agent.config.yaml to set your API keys:
# - For Brave Search: Set BRAVE_API_KEY: "your_key_here" in brave.env section (line ~28)
# - For Bocha-MCP: Set BOCHA_API_KEY: "your_key_here" in bocha-mcp.env section (line ~74)

# üìÑ Configure document segmentation (optional)
# Edit mcp_agent.config.yaml to control document processing:
# - enabled: true/false (whether to use intelligent document segmentation)
# - size_threshold_chars: 50000 (document size threshold to trigger segmentation)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h5&gt;üêç &lt;strong&gt;Using Traditional pip&lt;/strong&gt;&lt;/h5&gt; 
 &lt;pre&gt;&lt;code class="language-bash"&gt;# üîΩ Clone the repository
git clone https://github.com/HKUDS/DeepCode.git
cd DeepCode/

# üì¶ Install dependencies
pip install -r requirements.txt

# üîë Configure API keys (required)
# Edit mcp_agent.secrets.yaml with your API keys and base_url:
# - openai: api_key, base_url (for OpenAI/custom endpoints)
# - anthropic: api_key (for Claude models)

# üîë Configure search API keys for web search (optional)
# Edit mcp_agent.config.yaml to set your API keys:
# - For Brave Search: Set BRAVE_API_KEY: "your_key_here" in brave.env section (line ~28)
# - For Bocha-MCP: Set BOCHA_API_KEY: "your_key_here" in bocha-mcp.env section (line ~74)

# üìÑ Configure document segmentation (optional)
# Edit mcp_agent.config.yaml to control document processing:
# - enabled: true/false (whether to use intelligent document segmentation)
# - size_threshold_chars: 50000 (document size threshold to trigger segmentation)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h4&gt;ü™ü &lt;strong&gt;Windows Users: Additional MCP Server Configuration&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;If you're using Windows, you may need to configure MCP servers manually in &lt;code&gt;mcp_agent.config.yaml&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# 1. Install MCP servers globally
npm i -g @modelcontextprotocol/server-brave-search
npm i -g @modelcontextprotocol/server-filesystem

# 2. Find your global node_modules path
npm -g root
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then update your &lt;code&gt;mcp_agent.config.yaml&lt;/code&gt; to use absolute paths:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;mcp:
  servers:
    brave:
      command: "node"
      args: ["C:/Program Files/nodejs/node_modules/@modelcontextprotocol/server-brave-search/dist/index.js"]
    filesystem:
      command: "node"
      args: ["C:/Program Files/nodejs/node_modules/@modelcontextprotocol/server-filesystem/dist/index.js", "."]
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Replace the path with your actual global node_modules path from step 2.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;üîç &lt;strong&gt;Search Server Configuration (Optional)&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;DeepCode supports multiple search servers for web search functionality. You can configure your preferred option in &lt;code&gt;mcp_agent.config.yaml&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;# Default search server configuration
# Options: "brave" or "bocha-mcp"
default_search_server: "brave"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Available Options:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üîç Brave Search&lt;/strong&gt; (&lt;code&gt;"brave"&lt;/code&gt;):&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Default option with high-quality search results&lt;/li&gt; 
   &lt;li&gt;Requires BRAVE_API_KEY configuration&lt;/li&gt; 
   &lt;li&gt;Recommended for most users&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;üåê Bocha-MCP&lt;/strong&gt; (&lt;code&gt;"bocha-mcp"&lt;/code&gt;):&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Alternative search server option&lt;/li&gt; 
   &lt;li&gt;Requires BOCHA_API_KEY configuration&lt;/li&gt; 
   &lt;li&gt;Uses local Python server implementation&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;API Key Configuration in mcp_agent.config.yaml:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="language-yaml"&gt;# For Brave Search (default) - around line 28
brave:
  command: "npx"
  args: ["-y", "@modelcontextprotocol/server-brave-search"]
  env:
    BRAVE_API_KEY: "your_brave_api_key_here"

# For Bocha-MCP (alternative) - around line 74
bocha-mcp:
  command: "python"
  args: ["tools/bocha_search_server.py"]
  env:
    PYTHONPATH: "."
    BOCHA_API_KEY: "your_bocha_api_key_here"
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;üí° Tip&lt;/strong&gt;: Both search servers require API key configuration. Choose the one that best fits your API access and requirements.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;‚ö° &lt;strong&gt;Step 2: Launch Application&lt;/strong&gt;&lt;/h3&gt; 
&lt;h4&gt;üöÄ &lt;strong&gt;Using Installed Package (Recommended)&lt;/strong&gt;&lt;/h4&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# üåê Launch web interface directly
deepcode

# The application will automatically start at http://localhost:8501
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;üõ†Ô∏è &lt;strong&gt;Using Source Code&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;Choose your preferred interface:&lt;/p&gt; 
&lt;h5&gt;üåê &lt;strong&gt;Web Interface&lt;/strong&gt; (Recommended)&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Using UV
uv run streamlit run ui/streamlit_app.py
# Or using traditional Python
streamlit run ui/streamlit_app.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://img.shields.io/badge/Access-localhost:8501-00d4ff?style=flat-square&amp;amp;logo=streamlit&amp;amp;logoColor=white" alt="Web Access" /&gt; 
&lt;/div&gt; 
&lt;h5&gt;üñ•Ô∏è &lt;strong&gt;CLI Interface&lt;/strong&gt; (Advanced Users)&lt;/h5&gt; 
&lt;pre&gt;&lt;code class="language-bash"&gt;# Using UV
uv run python cli/main_cli.py
# Or using traditional Python
python cli/main_cli.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;div align="center"&gt; 
 &lt;img src="https://img.shields.io/badge/Mode-Interactive_Terminal-9b59b6?style=flat-square&amp;amp;logo=terminal&amp;amp;logoColor=white" alt="CLI Mode" /&gt; 
&lt;/div&gt; 
&lt;h3&gt;üéØ &lt;strong&gt;Step 3: Generate Code&lt;/strong&gt;&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;üìÑ Input&lt;/strong&gt;: Upload your research paper, provide requirements, or paste a URL&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ü§ñ Processing&lt;/strong&gt;: Watch the multi-agent system analyze and plan&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;‚ö° Output&lt;/strong&gt;: Receive production-ready code with tests and documentation&lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üí° Examples&lt;/h2&gt; 
&lt;h3&gt;üé¨ &lt;strong&gt;Live Demonstrations&lt;/strong&gt;&lt;/h3&gt; 
&lt;table align="center"&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td width="33%" align="center"&gt; &lt;h4&gt;üìÑ &lt;strong&gt;Paper2Code Demo&lt;/strong&gt;&lt;/h4&gt; &lt;p&gt;&lt;strong&gt;Research to Implementation&lt;/strong&gt;&lt;/p&gt; 
    &lt;div align="center"&gt; 
     &lt;a href="https://www.youtube.com/watch?v=MQZYpLkzsbw"&gt; &lt;img src="https://img.youtube.com/vi/MQZYpLkzsbw/maxresdefault.jpg" alt="Paper2Code Demo" width="100%" style="border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);" /&gt; &lt;/a&gt; 
     &lt;p&gt;&lt;strong&gt;&lt;a href="https://www.youtube.com/watch?v=MQZYpLkzsbw"&gt;‚ñ∂Ô∏è Watch Demo&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
     &lt;p&gt;&lt;em&gt;Transform academic papers into production-ready code automatically&lt;/em&gt;&lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
   &lt;td width="33%" align="center"&gt; &lt;h4&gt;üñºÔ∏è &lt;strong&gt;Image Processing Demo&lt;/strong&gt;&lt;/h4&gt; &lt;p&gt;&lt;strong&gt;AI-Powered Image Tools&lt;/strong&gt;&lt;/p&gt; 
    &lt;div align="center"&gt; 
     &lt;a href="https://www.youtube.com/watch?v=nFt5mLaMEac"&gt; &lt;img src="https://img.youtube.com/vi/nFt5mLaMEac/maxresdefault.jpg" alt="Image Processing Demo" width="100%" style="border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);" /&gt; &lt;/a&gt; 
     &lt;p&gt;&lt;strong&gt;&lt;a href="https://www.youtube.com/watch?v=nFt5mLaMEac"&gt;‚ñ∂Ô∏è Watch Demo&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
     &lt;p&gt;&lt;em&gt;Intelligent image processing with background removal and enhancement&lt;/em&gt;&lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
   &lt;td width="33%" align="center"&gt; &lt;h4&gt;üåê &lt;strong&gt;Frontend Implementation&lt;/strong&gt;&lt;/h4&gt; &lt;p&gt;&lt;strong&gt;Complete Web Application&lt;/strong&gt;&lt;/p&gt; 
    &lt;div align="center"&gt; 
     &lt;a href="https://www.youtube.com/watch?v=78wx3dkTaAU"&gt; &lt;img src="https://img.youtube.com/vi/78wx3dkTaAU/maxresdefault.jpg" alt="Frontend Demo" width="100%" style="border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);" /&gt; &lt;/a&gt; 
     &lt;p&gt;&lt;strong&gt;&lt;a href="https://www.youtube.com/watch?v=78wx3dkTaAU"&gt;‚ñ∂Ô∏è Watch Demo&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
     &lt;p&gt;&lt;em&gt;Full-stack web development from concept to deployment&lt;/em&gt;&lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h3&gt;üÜï &lt;strong&gt;Recent Updates&lt;/strong&gt;&lt;/h3&gt; 
&lt;h4&gt;üìÑ &lt;strong&gt;Smart Document Segmentation (v1.2.0)&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Intelligent Processing&lt;/strong&gt;: Automatically handles large research papers and technical documents that exceed LLM token limits&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Configurable Control&lt;/strong&gt;: Toggle segmentation via configuration with size-based thresholds&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Semantic Analysis&lt;/strong&gt;: Advanced content understanding with algorithm, concept, and formula preservation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Backward Compatibility&lt;/strong&gt;: Seamlessly falls back to traditional processing for smaller documents&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;üöÄ &lt;strong&gt;Coming Soon&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;We're continuously enhancing DeepCode with exciting new features:&lt;/p&gt; 
&lt;h4&gt;üîß &lt;strong&gt;Enhanced Code Reliability &amp;amp; Validation&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Automated Testing&lt;/strong&gt;: Comprehensive functionality testing with execution verification and error detection.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Code Quality Assurance&lt;/strong&gt;: Multi-level validation through static analysis, dynamic testing, and performance benchmarking.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Smart Debugging&lt;/strong&gt;: AI-powered error detection with automatic correction suggestions&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;üìä &lt;strong&gt;PaperBench Performance Showcase&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Benchmark Dashboard&lt;/strong&gt;: Comprehensive performance metrics on the PaperBench evaluation suite.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Accuracy Metrics&lt;/strong&gt;: Detailed comparison with state-of-the-art paper reproduction systems.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Success Analytics&lt;/strong&gt;: Statistical analysis across paper categories and complexity levels.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;‚ö° &lt;strong&gt;System-wide Optimizations&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Performance Boost&lt;/strong&gt;: Multi-threaded processing and optimized agent coordination for faster generation.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Enhanced Reasoning&lt;/strong&gt;: Advanced reasoning capabilities with improved context understanding.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Expanded Support&lt;/strong&gt;: Extended compatibility with additional programming languages and frameworks.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;‚≠ê Star History&lt;/h2&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt;&lt;em&gt;Community Growth Trajectory&lt;/em&gt;&lt;/p&gt; 
 &lt;a href="https://star-history.com/#HKUDS/DeepCode&amp;amp;Date"&gt; 
  &lt;picture&gt; 
   &lt;source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=HKUDS/DeepCode&amp;amp;type=Date&amp;amp;theme=dark" /&gt; 
   &lt;source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=HKUDS/DeepCode&amp;amp;type=Date" /&gt; 
   &lt;img alt="Star History Chart" src="https://api.star-history.com/svg?repos=HKUDS/DeepCode&amp;amp;type=Date" style="border-radius: 15px; box-shadow: 0 0 30px rgba(0, 217, 255, 0.3);" /&gt; 
  &lt;/picture&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h3&gt;üöÄ &lt;strong&gt;Ready to Transform Development?&lt;/strong&gt;&lt;/h3&gt; 
&lt;div align="center"&gt; 
 &lt;p&gt; &lt;a href="https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-quick-start"&gt;&lt;img src="https://img.shields.io/badge/üöÄ_Get_Started-00d4ff?style=for-the-badge&amp;amp;logo=rocket&amp;amp;logoColor=white" alt="Get Started" /&gt;&lt;/a&gt; &lt;a href="https://github.com/HKUDS"&gt;&lt;img src="https://img.shields.io/badge/üèõÔ∏è_View_on_GitHub-00d4ff?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white" alt="View on GitHub" /&gt;&lt;/a&gt; &lt;a href="https://github.com/HKUDS/deepcode-agent"&gt;&lt;img src="https://img.shields.io/badge/‚≠ê_Star_Project-00d4ff?style=for-the-badge&amp;amp;logo=star&amp;amp;logoColor=white" alt="Star Project" /&gt;&lt;/a&gt; &lt;/p&gt; 
 &lt;hr /&gt; 
 &lt;h3&gt;üìÑ &lt;strong&gt;License&lt;/strong&gt;&lt;/h3&gt; 
 &lt;img src="https://img.shields.io/badge/License-MIT-4ecdc4?style=for-the-badge&amp;amp;logo=opensourceinitiative&amp;amp;logoColor=white" alt="MIT License" /&gt; 
 &lt;p&gt;&lt;strong&gt;MIT License&lt;/strong&gt; - Copyright (c) 2025 Data Intelligence Lab, The University of Hong Kong&lt;/p&gt; 
 &lt;hr /&gt; 
 &lt;img src="https://visitor-badge.laobi.icu/badge?page_id=deepcode.readme&amp;amp;style=for-the-badge&amp;amp;color=00d4ff" alt="Visitors" /&gt; 
&lt;/div&gt;</description>
    </item>
    
  </channel>
</rss>